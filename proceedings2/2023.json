[
    {
        "title": "A Few-Shot Neural Approach for Layout Analysis of Music Score Images.",
        "author": [
            "Francisco J. Castellanos 0001",
            "Antonio Javier Gallego 0001",
            "Ichiro Fujinaga"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265233",
        "url": "https://doi.org/10.5281/zenodo.10265233",
        "abstract": "Optical Music Recognition (OMR) is a well-established research field focused on the task of reading musical notation from images of music scores. In the standard OMR workflow, layout analysis is a critical component for identifying relevant parts of the image, such as staff lines, text, or notes. State-of-the-art approaches to this task are based on machine learning, which entails having to label a training corpus, an error-prone, laborious, and expensive task that must be performed by experts. In this paper, we propose a novel few-shot strategy for building robust models by utilizing only partial annotations, therefore requiring minimal human effort. Specifically, we introduce a masking layer and an oversampling technique to train models using a small set of annotated patches from the training images. Our proposal enables achieving high performance even with scarce training data, as demonstrated by experiments on four benchmark datasets. The results indicate that this approach achieves performance values comparable to models trained with a fully annotated corpus, but, in this case, requiring the annotation of only between 20% and 39% of this data.",
        "zenodo_id": 10265233,
        "dblp_key": "conf/ismir/00010F23",
        "keywords": [
            "Optical Music Recognition",
            "layout analysis",
            "machine learning",
            "few-shot strategy",
            "partial annotations",
            "masking layer",
            "oversampling technique",
            "benchmark datasets",
            "performance values",
            "scarcity of training data"
        ],
        "ee": "https://zenodo.org/records/10265233/files/000011.pdf",
        "content": "A FEW-SHOT NEURAL APPROACH FOR LAYOUT ANALYSIS OF MUSIC\nSCORE IMAGES\nFrancisco J. Castellanos1Antonio Javier Gallego1Ichiro Fujinaga2\n1University Institute for Computing Research, University of Alicante, Spain\n2Schulich School of Music, McGill University, Montreal, Canada\n{fcastellanos, jgallego}@dlsi.ua.es, ichiro.fujinaga@mcgill.ca\nABSTRACT\nOptical Music Recognition (OMR) is a well-established\nresearch ﬁeld focused on the task of reading musical no-\ntation from images of music scores. In the standard OMR\nworkﬂow, layout analysis is a critical component for iden-\ntifying relevant parts of the image, such as staff lines, text,\nor notes. State-of-the-art approaches to this task are based\non machine learning, which entails having to label a train-\ning corpus, an error-prone, laborious, and expensive task\nthat must be performed by experts. In this paper, we pro-\npose a novel few-shot strategy for building robust mod-\nels by utilizing only partial annotations, therefore requiring\nminimal human effort. Speciﬁcally, we introduce a mask-\ning layer and an oversampling technique to train models\nusing a small set of annotated patches from the training\nimages. Our proposal enables achieving high performance\neven with scarce training data, as demonstrated by exper-\niments on four benchmark datasets. The results indicate\nthat this approach achieves performance values compara-\nble to models trained with a fully annotated corpus, but,\nin this case, requiring the annotation of only between 20%\nand 39% of this data.\n1. INTRODUCTION\nOptical Music Recognition (OMR) is a research ﬁeld ded-\nicated to developing computational methods for transcrib-\ning musical notation from document images into digital\nformats [1]. While this task could be accomplished man-\nually, the vast number and heterogeneity of music docu-\nments make this approach tedious, costly, and error-prone.\nThe development of OMR systems has the potential to\nenhance music heritage accessibility and preservation, as\nwell as enable the application of analysis algorithms to in-\ncrease knowledge about this cultural legacy.\nOMR typically follows a sequential workﬂow, which\ndivides the transcription process into simpler tasks. The\ninitial task is called Document Image Analysis (DIA),\nwhich is itself a research ﬁeld that studies how to obtain\n© F. J. Castellanos, A. J. Gallego, and I. Fujinaga. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: F. J. Castellanos, A. J. Gallego, and I. Fujinaga, “A\nFew-shot Neural Approach for Layout Analysis of Music Score Images”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.a segmented version of the image by isolating the differ-\nent layers of interest, such as staves, lyrics, instructions,\nornaments, etc [2]. In the literature, multiple strategies\ncan be found to perform this layout analysis , ranging from\nheuristic approaches that exploit speciﬁc features of the\nimages to deep learning techniques. Although heuristic\napproaches achieve high performance in controlled scenar-\nios, these solutions are poorly generalizable. To obtain bet-\nter and generalizable results, the current trend is to rely on\nmachine learning and, more speciﬁcally, on neural network\narchitectures [3].\nThe application of deep learning in layout analysis has\nbeen extensively studied, as evidenced by several state-of-\nthe-art works [4, 5]. However, a major drawback of these\nmethods is the requirement for a large amount of annotated\ndata for their training. This is particularly problematic for\nthe layout analysis of music scores since their high vari-\nability in appearance and styles makes necessary the an-\nnotation of each new application domain in order to train\nrobust models. Despite the importance of this issue, it has\nbeen overlooked in the OMR literature, with domain adap-\ntation being the only explored solution [6]. Nevertheless,\nthis technique also requires full annotations (even if it is\nfrom a different domain) and the performance obtained is\nnot good or robust enough, which also makes it an imprac-\ntical solution.\nIn this work, we propose a novel few-shot strategy for\nbuilding robust models for layout analysis by utilizing only\npartial annotations, therefore requiring minimal human ef-\nfort. Speciﬁcally, we introduce a masking layer and an\noversampling technique to train models using a small set of\nannotated patches from the training images. Our approach\naims to drastically reduce the manual workload without\ncompromising performance, making it of particular inter-\nest to real-world applications. Experiments on four bench-\nmark datasets indicate that this approach achieves perfor-\nmance comparable to models trained on a fully annotated\ncorpus—but requiring the annotation of only between 20%\nand 39% of this data depending on the layer—thus making\nit a highly efﬁcient and effective strategy.\n2. RELATED WORK\nTraditional OMR workﬂows relied on a combination of\nheuristic strategies to perform pixel-wise layout analysis\nand classify each pixel of the image according to a set106of categories [2]. A binarization process was commonly\napplied to simplify the complexity of the image to de-\ntect the ink pixels, either using generic approaches [7, 8]\nor other particular ones proposed for the musical con-\ntext [9, 10]. The recognition and isolation of the staff\nand the lyrics were then carried out using also heuristic\ntechniques [11, 12]. From these detected staves, the mu-\nsical symbols were ﬁnally processed, sometimes carry-\ning out another step to remove the staff lines, as can be\nseen in the review by Dalitz et al. [13] or in more recent\nworks [14–16].\nMore recently, all these steps were combined by means\nof machine learning techniques. Calvo-Zaragoza et al. [17]\nproposed a Convolutional Neural Network (CNN) to di-\nrectly classify each pixel of the image—performing a\npixel-wise layout analysis—which was later improved us-\ning a U-net-like architecture—referred to as Selectional\nAuto-Encoder (SAE)—to more efﬁciently classify the im-\nage by patches [18]. This later work, on which our pro-\nposal is based, trained a set of SAE specialized in the\ndetection of each layer of information—staff lines, notes,\ntext, or background.\nThe main challenge with layout analysis approaches\nthat rely on supervised learning is the large amount of an-\nnotated data needed to train the models [19, 20]. This re-\nquires the annotation at the pixel level of a reference set of\nimages, which has to be done by hand, so it is not a scalable\nsolution given the high level of detail of these annotations\nand heterogeneity in music documents. In addition, when\nthis constraint cannot be fulﬁlled, these learning-based ar-\nchitectures fail to converge to obtain a suitable model for\nthe task at hand.\nIn the literature, we can ﬁnd different proposals that\nseek to alleviate this issue [21], two of the most common\nbeing the use of regularization strategies [22] and data aug-\nmentation processes [23]. We can also ﬁnd more speciﬁc\nproposals for cases of remarkable data scarcity, i.e., with\na considerably fewer number of annotated training sam-\nples. These scenarios are known as few-shot learning [24]\nand typically employ speciﬁc neural architectures to es-\ntimate the similarity of the data [25]. Some of the most\ntypical examples of these techniques are Siamese Neural\nNetworks [26], Matching Networks [27], Prototypical Net-\nworks [28], and Relation Networks [29]. For a comprehen-\nsive review of these strategies, the reader is referred to the\nwork by Jadon [30].\nOur proposal follows a few-shot learning approach, but\ninstead of using a speciﬁc few-shot architecture, a state-\nof-the-art layout analysis model—the previously described\nSAE network—is modiﬁed to integrate a masking layer\nthat enables training with very little data. This layer is\ncomplemented by an oversampling proposal used during\nthe training process to draw samples at random positions\naround the chunks with annotated data. A mask is applied\nto these pieces and used by the added layer to avoid pro-\ncessing the non-annotated parts, which will randomly ap-\npear in different positions in each iteration, thus forcing the\narchitecture to generalize the learned weights.In the related literature, masks have been used for dif-\nferent purposes. For example, Medhat et al. [31] proposed\nthe use of binary masks for sound classiﬁcation to ﬁlter\nout certain frequency bands. It has also been explored for\nimage classiﬁcation, speciﬁcally, Suresh et al. [32] studied\nthe use of masks as a pre-processing task to ﬁlter the back-\nground of images with hand gestures, making the model\nfocus only on the gestures to be classiﬁed. However, as far\nas we know, masks have not been used either in binariza-\ntion tasks or for few-shot learning cases, so that the model\ndoes not use the unlabeled areas.\n3. METHODOLOGY\nOur approach aims to build a robust few-shot learning\nmodel for layout analysis of music score images that classi-\nﬁes each pixel of an input image into one of the following\ncategories: staff ,notes ,text , andbackground .\nIn our context, the few-shot scenario can be represented as\na manual annotation of a limited number nof portions or\npatches from a set of images I, withn≪N, whereNis\nthe total number of possible patches that could be sequen-\ntially extracted without overlapping from I. Therefore,\nwhennis small, less human effort and cost are required\nto annotate the training set.\nNote that labeling only part of the image makes the rest\nof it uninformative, even if there are ink pixels. In a typical\ntraining process, only the annotated patches would be used.\nHowever, when the amount of data is limited, this would\nlead to overﬁtting of the model. Although data augmenta-\ntion may help mitigate this problem, in a few-shot learning\nscenario, it is not very useful due to the little information\nto be altered.\nOur proposal introduces a novel approach to extract\na larger—and more varied—number of samples from the\nscarce labeled information. Speciﬁcally, it is proposed\nto extract random patches around the annotated areas—\nkeeping a minimum λ%of labeled information—to obtain\nmore varied samples, thus generating variations in the po-\nsition of the elements and their labeling. Since some parts\nof the extracted patches will fall outside the annotated area,\nit is proposed to mark those parts with a special label ( −1)\nso that they are not used during training. This approach al-\nlows us to control the number of samples to be drawn from\nthe images and get enough variability in the data to train\nthe model, as we will demonstrate in the experiments.\nFormally, letX ∈Rw×hbe a collection of patches\nof size w×hdrawn from the input set of im-\nagesI, andY ∈ { 0,1}w×hbe the corresponding\npixel-level annotation matrices extracted from the an-\nnotation setLlfor the layer to be processed l∈\n{staff,notes,text,background}, where1is used\nto label the ink of that layer and 0the rest, either back-\nground or information from another layer. Additionally,\nletS={(xi,yi) :xi∈X,yi∈Y}|S|\ni=1represent an an-\nnotated collection of data where each datum xiis related\nto labelyiby an underlying function fl:X→Y , that rep-\nresents the objective function to be learned for each layer\nl, and for which the SAE state-of-the-art architecture willProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n107be used. Note also that x∗will be used to refer to the input\npatches after applying the mask, which may contain values\nin the range [0,255], for the original pixels of the image,\nbut also the value−1as a mask to mark the parts without\nannotated information. This mask is therefore applied to\nthe input data inXand will be used by the masking layer\n(described below) added to the networks flto ignore those\nparts during the training process.\nAlgorithm 1 describes the oversampling method pro-\nposed to obtain the set Spreviously described. This\nmethod receives as input the set of images I, the set with\nthe annotated data L, the layer lto be processed, the λ%\nof minimum patch information, the total sizeof sampling\nto perform, and the set Mthat contains the list of patches\nannotated with their coordinates in the input images. The\nalgorithm ﬁrst iterates through the number of patches an-\nnotated inM(line 3 ) and for each one obtains the in-\ndexjof the image it corresponds to ( line 4 ). It then\niterates for the number of samples that have to be extracted\nfor that annotated patch ( line 5 ) and, for each one, per-\nforms the following steps: 1) randomly selects the sam-\nple coordinates pusing the mask of that patch and taking\ninto account the minimum λ%of annotated pixels allowed\n(line 6 ); 2) extracts the patch xfromIjusing the coor-\ndinatesp(line 7 ); 3) applies the mask to set a constant\nvalue (−1) in those pixels that are not part of the annotated\narea (line 8 ); 4) retrieves the layout annotations yfor\nthat sample ( line 9 ); and 5) both x∗andyare added to\nthe setS. The algorithm repeats this process until reaching\nthe requested size, ﬁnally returning the set Sobtained.\nAlgorithm 1 Random masking patches generator.\n1:function SAMPLE GENERATION (I,L,M,l,λ,size )\n2:S←∅\n3: fori←1to|Ml|do\n4: j←getPatchIndex (Ml\ni)\n5: fork←1tosize\n|Ml|do\n6: p←getRandomPosition (Ml\ni,λ)\n7: x←getWindow (Ij,p)\n8: x∗←applyMask (x,Ml\ni,p)\n9: y←getWindow (Ll\nj,p)\n10:S←S∪ (x∗,y)\n11: end for\n12: end for\n13: returnS\n14:end function\nNote that the getWindow (·) function may apply addi-\ntional data augmentation to the sample in order to further\nincrease its variability.\nThis oversampling process is complemented by the pro-\nposal of a masking layer that is added to the network archi-\ntectureflto ignore the pixels that are not annotated. This\nlayer, as indicated in Section 2, has been previously used\nin other proposals to skip time steps in sequence processes\nand to mask the background in classiﬁcation tasks. In this\nproposal, we adapt it to ignore the parts of the input with\nthis mask and also propagate the mask to the followinglayers so that the non-annotated parts are not taken into ac-\ncount during the training process. Intuitively, the masking\nlayer acts as a regularizer and data augmentation process.\nGiven that the annotated and non-annotated parts will vary\nin position and size across iterations, the network is forced\nto generalize the weights learned during training by hav-\ning to use different connections of the network and non-\nannotated pixels will not be used.\n4. EXPERIMENTAL SETUP\nThis section describes the corpora and metrics considered\nfor evaluation and the implementation details of the neural\narchitecture.1\n4.1 Corpora\nFor the experiments, we considered the following 4\ndatasets with manual pixel-wise annotations of 4 layers of\ninformation ( staff ,notes ,text , andbackground ).\nFigure 1 shows some examples for each manuscript and\nTable 1 includes a summary with their details.\n•EIN: 9 high-resolution scanned pages of neumatic\nnotation belonging to the Einsiedeln, Stiftsbiblio-\nthek, Codex 611(89), from 1314.2\n•SAL: A set of 10 high-resolution images of pages\nfrom the Salzinnes Antiphonal manuscript (CDM-\nHsmu M2149.14), in neumatic notation. It is avail-\nable in the Cantus Ultimus platform.3\n•MS73 : Selection of 10 pages of square music nota-\ntion from the miscellaneous choir book ‘ Dominican,\nCDN-Mlr MS Medieval 0073 ’ from Northern Italy,\nwritten between 13th and 15th centuries. This cor-\npus is stored in the McGill Library collection, and it\nis online available through Cantus Ultimus .4\n•CAP: A compilation of mensural notation\nmanuscripts from the 17-18th centuries belonging\nto the ‘Cathedral of Our Lady of the Pillar’ in\nZaragoza (Spain), introduced for OMR purposes\nby Calvo-Zaragoza et al. [33]. We use a subset of\nthe corpus, with 10 manually pixel-wise annotated\npages.\nIn all the cases, we used 4 images for training, 2 im-\nages for validation, and the remaining for testing. After\npreliminary experiments and also based on previous pro-\nposals, we selected a patch size of 256×256 pixels to\nextract from these images. To be fair and more realistic,\nwe use the same number of samples for the validation set\nas for the training partition. This is because, in a real case,\nit would be necessary to annotate the validation partition\nas well, so it is not fair to use the entire pages to validate\nthe models in a few-shot scenario. This does not apply to\nthe test set, for which we use all available data.\n1https://github.com/fjcastellanos/\nFewShotLayoutAnalysisMusic.git\n2http://www.e-codices.unifr.ch/en/sbe/0611/\n3https://cantus.simssa.ca/manuscript/133/\n4https://cantus.simssa.ca/manuscript/35/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n108(a) E IN(input).\n (b) E IN(ground truth).\n (c) S AL(input).\n (d) S AL(ground truth).\n(e) MS73 (input).\n (f) MS73 (ground truth).\n (g) C AP(input).\n (h) C AP(ground truth).\nFigure 1 : Examples of images extracted from the corpora described in Table 1. In the ground truth images: red pixels\nrepresent the staff lines annotation, black is used for music symbols, blue for text, and white for the background.\nCorpus # imgs Resol.Layers (%)\nBG St No Te\nEIN 9 6 496×4 872 87.9 3.5 2.7 5.9\nSAL 10 5 847×3 818 87.6 2.4 2.5 7.5\nMS73 10 6 990 ×4 797 93.4 1.8 1.8 3.0\nCAP 10 2 126×3 065 85.7 6.6 5.1 2.6\nTable 1 : Details of the corpora considered including the\nnumber of images (# imgs), the average resolution and the\nproportion of pixels for each layer of interest, with BGfor\nbackground, Stfor staff lines, Nofor notes, and Tefor text.\n4.2 Metrics\nTo evaluate the performance of our few-shot approach, we\nresorted to the F-score (F 1) ﬁgure of merit to avoid pos-\nsible biases toward any particular class given the inherent\nlabel imbalance in the datasets considered (see Table 1).\nAssuming a binary classiﬁcation scenario, this metric is\ndeﬁned as\nF1=2·TP\n2·TP+FP+FN, (1)\nwhere TP, FP, and FN denote the True Positives, False Pos-\nitives, and False Negatives, respectively.\nFinally, given the non-binary nature of the task at hand,\nwe considered the use of the macro-averaged F-score (Fm\n1)\nas the average of the F 1values computed for each layer.\nMathematically, this metric is deﬁned as\nFm\n1=/summationtext|L|\nl=1Fl\n1\n|L|, (2)\nwhere Fl\n1is the F 1calculated for the layer lassuming a\none-versus-all evaluation framework and |L|represents the\ntotal number of layers of information (in our case 4).4.3 Implementation details\nThe architecture considered is based on a previous\nwork [18], in which a framework consisting of a series\nof SAE models—one for each layer to be predicted—was\nproposed. SAE follows a U-net architecture, in which an\nimage of size w×h(in our case a 256×256pixels patch)\nis given as input, and the output is a matrix of the same\nsize that contains the conﬁdence value of pixels belonging\nto the layer of interest. In our case, we have four layers to\nbe predicted, so we will have four SAE models, each one\nspecialized in one particular layer.\nFor the experimentation, we resort to the same archi-\ntecture proposed in the original work. An encoder with\nfour blocks composed of a convolutional layer of 32 ﬁlters\nof3×3, a sub-sampling of 2×2, a batch normalization,\na Rectiﬁed Linear Unit (ReLU) activation, and a dropout\nof 0.4. On the decoder side, the blocks follow the same\nscheme except for the sub-sampling, which is replaced by\nan oversampling of the same rate. The last layer of the de-\ncoder is connected to a convolution with one 3×3ﬁlter\nand a sigmoid activation to obtain the result of the predic-\ntion with values between 0 and 1. This architecture was\nonly changed to add the masking layer after the input.\nNote that each SAE was trained using the binary cross-\nentropy loss for up to 200 epochs with a batch size of 16,\nand an early stopping criterion of 20 epochs of no improve-\nment on the validation set. Adam optimizer [34] was used\nwith a learning rate of 0.001.\nFurthermore, to favor the convergence of the model,\nthe input images were normalized in the range [0,1]. The\nmask was applied over this result, so the inputs can actu-\nally contain the values {−1}∪[0,1]. For the extraction of\npatches, a value of λof 2.5% was used, since it allowed\nobtaining chunks with sufﬁcient information. In addition,\nwe also considered standard data augmentation to increase\ndata variability by applying random rotations between -45º\nand 45º, zoom variations between 0.8x and 1.2x, and hori-\nzontal and vertical ﬂips.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1095. RESULTS\nThis section presents and discusses the results obtained\nwith the proposed method.\nFirst, a preliminary experiment was carried out to an-\nalyze the inﬂuence of the amount of oversampling. For\nthis, starting from a single annotated patch, we studied the\nresult obtained by increasing the number of randomly ex-\ntracted samples around the annotated patch using the pro-\nposed technique. Fig. 2 shows the average results of this\nexperiment in the validation set for all layers and consid-\nering both the application and non-application of data aug-\nmentation. For a small number of randomly extracted sam-\nples, the proposal achieves approximately 30% of Fm\n1. The\naverage result is improved as the number of samples ex-\ntracted increases, reaching over 70% of Fm\n1for 512 sam-\nples and barely improving for the case of 1 024 samples.\nAdditional data augmentation does not help to improve\nthe results, only for cases of sample size equal to or less\nthan 128. This may be because the proposed oversampling\nmethod can be considered as a data augmentation process,\nso that, from a given amount of sampling, there is enough\nvariability and other techniques of data augmentation may\nnot be necessary.\n 0 20 40 60 80 100\n 0  200  400  600  800  1000F1m  (%)\nNumber of extracted samplesNot augmented Augmented\nFigure 2 : Preliminary experiment to study the inﬂuence\nof the number of samples drawn randomly from one an-\nnotated patch of 256×256pixels. The result obtained in\nterms of Fm\n1(%) in the validation partition is shown, con-\nsidering both the application and the non-application of ad-\nditional data augmentation.\nBased on these results, the sampling size is set to 512\nfor the following experiments. Also, since standard data\naugmentation seems detrimental in combination with our\nproposal, we decided not to use it.\nThe selected conﬁguration was evaluated using the test\nset, carrying out an analysis of the inﬂuence of the number\nof patches annotated (from 1 to 32) and the inﬂuence of\nthese being extracted from the same page or from several\n(up to 4, which would generate more variability). Fig. 3\nshows these results compared to two baselines: an up-\nper one representing the state-of-the-art model [18] trained\nwith all available information (if the entire training set was\nannotated) and a lower bound training this model with only\none annotated patch (in both cases without applying the\nproposed masking layer). One initial observation is that the\nthree case studies (with 1, 2, or 4 pages) demonstrate com-\nparable trends. The results, as expected, show an increas-\ning trend with the number of annotated samples, from anaverage Fm\n1of 40% when training with one annotated sam-\nple to∼62% when using 32 annotated patches, and stabi-\nlizing (or improving less) from 16 to 32 annotated patches.\nIf these results are compared with the baselines, it can\nbe seen how the proposal exceeds the lower bound by 16%\nwhen training with one annotated sample and that it equals\nor even improves the upper baseline in the cases with 1\nand 2 pages from 16 annotated samples. Also, it is only\n7% worse than the state of the art for the 4-page case but\nwith a much lower annotated data requirement (32 sam-\nples, which represents 39% of the total information).\nLayer Annotated samples Baseline\nCorpus 1 2 4 8 16 32 Bt Up\n(1%) (2%) (5%) (10%) (20%) (39%) (1%) (100%)\nstaff\nEIN 10.5 39.8 64.1 62.1 83.9 78.1 0.0 87.3\nSAL 72.0 75.4 75.7 75.7 74.8 87.4 0.0 90.8\nMS73 11.3 13.9 17.7 12.9 92.8 94.1 0.0 91.4\nCAP 66.2 75.6 75.2 79.0 79.9 82.5 0.0 47.0\nAvg. 40.0 51.2 58.3 57.4 82.9 85.5 0.0 79.1\nnote\nEIN 19.0 16.7 20.7 0.0 20.4 26.3 0.0 77.8\nSAL 35.3 3.3 21.2 4.1 38.6 50.2 0.0 4.1\nMS73 0.2 3.2 6.7 7.3 7.1 7.3 0.0 2.7\nCAP 66.7 69.7 73.0 77.9 81.2 82.6 0.3 8.3\nAvg. 30.3 23.2 30.4 22.3 36.8 41.6 0.1 23.2\ntext\nEIN 22.9 15.1 17.2 67.3 31.7 37.0 11.3 11.3\nSAL 67.6 15.5 46.1 32.3 71.7 73.4 0.0 78.5\nMS73 6.3 9.4 26.2 16.7 15.3 14.3 0.0 13.5\nCAP 3.6 0.0 15.1 37.0 45.4 16.7 3.6 12.7\nAvg. 25.1 10.0 26.2 38.3 41.0 35.4 3.7 29.0\nbackground\nEIN 93.9 93.8 93.8 93.8 93.8 93.7 93.7 93.7\nSAL 93.2 93.2 93.2 93.2 97.9 99.1 93.2 98.5\nMS73 40.0 36.8 46.6 49.2 87.4 96.8 96.8 96.8\nCAP 93.6 93.6 93.7 93.6 93.6 93.6 93.6 93.6\nAvg. 80.2 79.4 81.8 82.5 93.2 95.8 94.2 95.7\nTable 2 : Average results in terms of F 1(%) for each layer\nconsidering 1 page in a few-shot evaluation. The percent-\nage of annotated information is indicated between paren-\ntheses. Btrepresents the bottom baseline, which is the\nstate-of-the-art model trained with 1 annotated sample per\npage, and Upis the upper baseline, with full pages used\nfor training. Both baselines do not apply any masking.\nFrom these results, we now analyze in detail the case of\na single page, since it represents the most extreme case as\nit has less variability available for the annotation. Table 2\nshows a summary of the results obtained individually for\neach dataset and layer considered, including the baselines\nand the percentage of the image used in each case. As\nin the previous results, it is observed that the performance\nof our approach improves as more annotated samples are\nused. In this case, we can analyze how the results vary\naccording to the layer and the corpus evaluated. In general,\nthe proposal improves the bottom baseline, in some cases,\nsuch asstaff ,notes , andtext , by a wide margin.\nHowever, note that this baseline fails to converge on most\nlayers, except for the background one. In this case, on\naverage, the proposal only improves the baseline by using\n32 samples—39% of the image. This is due to the fact\nthat for fewer annotated samples, poor overall results are\nobtained for the MS73 dataset. This may be because this\ndataset presents a greater variability of backgrounds. InProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n110 0 20 40 60 80 100\n 0  5  10  15  20  25  30F1m  (%)\nNumber of annotated samplesMasked SAE\nUpper baseline\nBottom baseline(a) 1 training page.\n 0 20 40 60 80 100\n 0  5  10  15  20  25  30F1m  (%)\nNumber of annotated samplesMasked SAE\nUpper baseline\nBottom baseline (b) 2 training pages.\n 0 20 40 60 80 100\n 0  5  10  15  20  25  30F1m  (%)\nNumber of annotated samplesMasked SAE\nUpper baseline\nBottom baseline (c) 4 training pages.\nFigure 3 : Average results in terms of Fm\n1(%) with respect to the number of annotated samples (from 1 to 32) and the\nnumber of pages (from 1 to 4). Dashed lines represent baseline results for reference. The upper reference line indicates the\nresults of the state-of-the-art model trained with fully annotated pages, while the lower reference line represents the results\nobtained when only one sample is annotated. Note that both baselines do not use the proposed masking method.\n(a) S AL(input).\n (b) Background layer.\n (c) Staff layer.\n (d) Notes layer\n (e) Text layer.\nFigure 4 : Example of the results obtained in S ALfor the four layers considered in this work. The method was trained with\n32 samples drawn from one page. White represents the detected information for the particular layer.\nfact, the rest of the layers of that corpus also obtain low-\nperformance values when the annotated data is scarce.\nRegarding the upper baseline, it can be seen how the\nproposal, on average, improves it in all layers, although it\nrequires a different number of labeled samples depending\non the layer. As stated before, on average, from 16 patches\nor 20% labeling, a better result is achieved. It is inter-\nesting that for the simplest and more homogeneous layers\n(such asstaff andbackground ), the upper baseline\nobtains a better result and it is more difﬁcult for the pro-\nposal to overcome it, while for the more difﬁcult ones that\npresent greater variability ( notes andtext ), the base-\nline obtains a worse result while the proposal achieves a\ngreater margin of improvement. This may be due to the\nfact that the proposal performs some overﬁtting in the sim-\nplest cases with less variability and, therefore, requires a\ngreater number of labeled samples to learn it.\nTo complement the quantitative results, Fig. 4 shows\nan example of prediction for S AL. As can be seen, the\nbackground and the staff layers are correctly retrieved, and\nsome false positives can be found in the notes layer. The\ntext layer seems the most challenging as it is not able to\ndifferentiate the ink of the text from other elements. How-\never, the text is recovered, and the false positives could\nbe removed by combining the predictions obtained for the\nother layers.\n6. CONCLUSIONS\nIn this work, we presented a few-shot neural approach for\npixel-wise layout analysis of music score images. The pro-\nposal includes a masking layer, which acts as a regular-izer, that is combined with an oversampling technique to\nleverage the limited annotated information available. The\noversampling technique extracts annotated parts of the im-\nages at different random positions at each training itera-\ntion, leaving annotated and non-annotated information in\ndifferent positions of the input. This strategy forces the\nneural architecture to generalize the learned weights, sim-\nilar to a data augmentation process but adapted to the case\nof few-shot and partial annotation in documents.\nThe proposal is evaluated on four benchmark datasets to\nstudy the inﬂuence of the amount of annotated data in the\nlayout analysis task. We found that the number of anno-\ntated samples is key to optimizing performance, and anno-\ntating a relatively small number of them—between 16 and\n32 samples, which represents using only between 20% and\n39% of the total information—can achieve average results\nof 65.5% of Fm\n1, which is very close to the result obtained\nby the state of the art (72%) using the entire training set\nannotated. It is also interesting to note that the proposal\nobtains similar results when labeling more pages, so it is\nenough to have a single page for training and perform a\npartial annotation of between 16 and 32 patches.\nIn general, the approach shows very competitive results\nin few-shot scenarios. Therefore, we hope this research\ncan open doors to new avenues in this line. Reducing the\namount of annotated data required for pixel-wise layout\nanalysis is essential, and techniques such as domain adap-\ntation and transfer learning may help to reduce human ef-\nfort. We plan to investigate new ways to address this prob-\nlem, including to combine domain adaptation techniques\nwith our masking proposal and studying the feasibility of\nincremental and active learning.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1117. ACKNOWLEDGMENT\nThis work was supported by the I+D+i project\nTED2021-132103A-I00 (DOREMI), funded by\nMCIN/AEI/10.13039/501100011033. This work also\ndraws on research supported by the Social Sciences\nand Humanities Research Council (895-2013-1012) and\nthe Fonds de recherche du Québec-Société et Culture\n(2022-SE3-303927).\n8. REFERENCES\n[1] D. Bainbridge and T. Bell, “The challenge of optical\nmusic recognition,” Computers and the Humanities ,\nvol. 35, no. 2, pp. 95–121, 2001.\n[2] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. S. Marçal,\nC. Guedes, and J. S. Cardoso, “Optical music recogni-\ntion: State-of-the-art and open issues,” International\nJournal of Multimedia Information Retrieval , vol. 1,\nno. 3, pp. 173–190, 2012.\n[3] J. Calvo-Zaragoza, J. H. Jr., and A. Pacha, “Under-\nstanding optical music recognition,” ACM Comput.\nSurv. , vol. 53, no. 4, Jul. 2020.\n[4] J. Calvo-Zaragoza, F. J. Castellanos, G. Vigliensoni,\nand I. Fujinaga, “Deep neural networks for document\nprocessing of music score images,” Applied Sciences ,\nvol. 8, no. 5, p. 654, 2018.\n[5] I. Fujinaga and G. Vigliensoni, “The art of teaching\ncomputers: The SIMSSA optical music recognition\nworkﬂow system,” in 27th European Signal Process-\ning Conference, EUSIPCO, A Coruña, Spain, Septem-\nber 2-6 . IEEE, 2019, pp. 1–5.\n[6] F. J. Castellanos, A. J. Gallego, and J. Calvo-Zaragoza,\n“Unsupervised domain adaptation for document anal-\nysis of music score images,” in Proceedings of the\n22nd International Society for Music Information Re-\ntrieval Conference, ISMIR 2021, Online, November 7-\n12, 2021 , 2021, pp. 81–87.\n[7] J. Sauvola and M. Pietikäinen, “Adaptive document\nimage binarization,” Pattern Recognition , vol. 33,\nno. 2, pp. 225–236, 2000.\n[8] N. R. Howe, “Document binarization with automatic\nparameter tuning,” International Journal on Document\nAnalysis and Recognition , vol. 16, no. 3, pp. 247–258,\n2013.\n[9] T. Pinto, A. Rebelo, G. A. Giraldi, and J. S. Cardoso,\n“Music score binarization based on domain knowl-\nedge,” in 5th Iberian Conference on Pattern Recogni-\ntion and Image Analysis, Las Palmas de Gran Canaria,\nSpain , 2011, pp. 700–708.\n[10] Q. N. V o, S. H. Kim, H. J. Yang, and G. Lee, “An MRF\nmodel for binarization of music scores with complex\nbackground,” Pattern Recognition Letters , vol. 69, no.\nSupplement C, pp. 88–95, 2016.[11] J. A. Burgoyne and I. Fujinaga, “Lyric extraction and\nrecognition on digital images of early music sources,”\ninProceedings of the 10th International Society for\nMusic Information Retrieval Conference , 2009, pp.\n723–728.\n[12] V . B. Campos, J. Calvo-Zaragoza, A. H. Toselli, and\nE. Vidal, “Sheet music statistical layout analysis,” in\n15th International Conference on Frontiers in Hand-\nwriting Recognition, Shenzhen, China , 2016, pp. 313–\n318.\n[13] C. Dalitz, M. Droettboom, B. Pranzas, and I. Fujinaga,\n“A comparative study of staff removal algorithms,”\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence , vol. 30, no. 5, pp. 753–766, 2008.\n[14] J. Dos Santos Cardoso, A. Capela, A. Rebelo,\nC. Guedes, and J. Pinto da Costa, “Staff detection with\nstable paths,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence , vol. 31, no. 6, pp. 1134–\n1139, 2009.\n[15] T. Géraud, “A morphological method for music score\nstaff removal,” in International Conference on Image\nProcessing , 2014, pp. 2599–2603.\n[16] A. Gallego and J. Calvo-Zaragoza, “Staff-line removal\nwith selectional auto-encoders,” Expert Systems with\nApplications , vol. 89, pp. 138–48, 2017.\n[17] J. Calvo-Zaragoza, G. Vigliensoni, and I. Fujinaga,\n“One-step detection of background, staff lines, and\nsymbols in medieval music manuscripts with convo-\nlutional neural networks,” in Proceedings of the 18th\nInternational Society for Music Information Retrieval\nConference, Suzhou, China , 2017, pp. 724–730.\n[18] F. J. Castellanos, J. Calvo-Zaragoza, G. Vigliensoni,\nand I. Fujinaga, “Document analysis of music\nscore images with selectional auto-encoders,” in\nProceedings of the 19th International Society for\nMusic Information Retrieval Conference, ISMIR 2018,\nParis, France, September 23-27, 2018 , E. Gómez,\nX. Hu, E. Humphrey, and E. Benetos, Eds.,\n2018, pp. 256–263. [Online]. Available: http:\n//ismir2018.ircam.fr/doc/pdfs/93_Paper.pdf\n[19] C. Zhang, S. Bengio, M. Hardt, B. Recht, and\nO. Vinyals, “Understanding deep learning (still) re-\nquires rethinking generalization,” Communications of\nthe ACM , vol. 64, no. 3, pp. 107–115, 2021.\n[20] J.-M. Lee and D.-s. Kang, “Improved method for learn-\ning data imbalance in gender classiﬁcation model using\nda-fsl,” Multimedia Tools and Applications , pp. 1–19,\n2021.\n[21] X. Li, L. Yu, C.-W. Fu, M. Fang, and P.-A. Heng, “Re-\nvisiting metric learning for few-shot image classiﬁca-\ntion,” Neurocomputing , vol. 406, pp. 49–58, 2020.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n112[22] M. Cogswell, F. Ahmed, R. B. Girshick, L. Zitnick,\nand D. Batra, “Reducing overﬁtting in deep networks\nby decorrelating representations,” in 4th International\nConference on Learning Representations, ICLR , 2016.\n[23] C. Shorten and T. M. Khoshgoftaar, “A survey on im-\nage data augmentation for deep learning,” Journal of\nBig Data , vol. 6, no. 1, p. 60, 2019.\n[24] Y . Wang, Q. Yao, J. Kwok, and L. M. Ni, “Generalizing\nfrom a few examples: A survey on few-shot learning,”\n2019.\n[25] C. Simon, P. Koniusz, R. Nock, and M. Harandi,\n“Adaptive subspaces for few-shot learning,” in Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2020, pp. 4136–4145.\n[26] G. Koch, R. Zemel, and R. Salakhutdinov, “Siamese\nneural networks for one-shot image recognition,” in In-\nternational Conference on Machine Learning (ICML) -\nDeep Learning workshop , vol. 2, 2015, pp. 1126–1135.\n[27] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al. ,\n“Matching networks for one shot learning,” Advances\nin neural information processing systems , vol. 29, pp.\n3630–3638, 2016.\n[28] J. Snell, K. Swersky, and R. Zemel, “Prototypical net-\nworks for few-shot learning,” in Advances in Neural\nInformation Processing Systems , 2017, pp. 4077–4087.\n[29] F. Sung, Y . Yang, L. Zhang, T. Xiang, P. H. Torr,\nand T. M. Hospedales, “Learning to compare: Rela-\ntion network for few-shot learning,” in Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition , 2018, pp. 1199–1208.\n[30] S. Jadon, “An overview of deep learning architec-\ntures in few-shot learning domain,” arXiv preprint\narXiv:2008.06365 , 2020.\n[31] F. Medhat, D. Chesmore, and J. Robinson, “Masked\nconditional neural networks for environmental sound\nclassiﬁcation,” in Artiﬁcial Intelligence XXXIV: 37th\nSGAI International Conference on Artiﬁcial Intelli-\ngence, AI 2017, Cambridge, UK, December 12-14,\n2017, Proceedings . Springer, 2017, pp. 21–33.\n[32] M. Suresh, A. Sinha, and R. Aneesh, “Real-time hand\ngesture recognition using deep learning,” International\nJournal of Innovations and Implementations in Engi-\nneering , vol. 1, 2019.\n[33] J. Calvo-Zaragoza, D. Rizo, and J. M. I. Quereda, “Two\n(note) heads are better than one: Pen-based multimodal\ninteraction with music scores,” in Proceedings of the\n17th International Society for Music InformationRe-\ntrieval Conference, ISMIR 2016, New York City, United\nStates, August 7-11, 2016 , 2016, pp. 509–514.[34] D. P. Kingma and J. Ba, “Adam: A method\nfor stochastic optimization,” in 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings , Y . Bengio and Y . LeCun, Eds., 2015.\n[Online]. Available: http://arxiv.org/abs/1412.6980Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n113"
    },
    {
        "title": "Transformer-Based Beat Tracking With Low-Resolution Encoder and High-Resolution Decoder.",
        "author": [
            "Tian Cheng 0001",
            "Masataka Goto"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265325",
        "url": "https://doi.org/10.5281/zenodo.10265325",
        "abstract": "In this paper, we address the beat tracking task which is to predict beat times corresponding to the input audio. Due to the long sequential inputs, it is still challenging to model the global structure efficiently and to deal with the data imbalance between beats and no beats. In order to meet the above challenges, we propose a novel Transformer-based model consisting of a low-resolution encoder and a high-resolution decoder. The encoder with low temporal resolution is suited to capture global features with more balanced data. The decoder with high temporal resolution is designed to predict beat times at a desired resolution. In the decoder, the global structure is considered by the cross attention between the global features and high-dimensional features. There are two key modifications in the proposed model: (1) adding 1D convolutional layers in the encoder and (2) replacing positional embedding by the upsampled encoder features in the decoder. In the experiment, we achieved the state-of-the-art performance and showed that the decoder produced more precise and stable results.",
        "zenodo_id": 10265325,
        "dblp_key": "conf/ismir/0001G23",
        "ee": "https://zenodo.org/records/10265325/files/000055.pdf",
        "content": "TRANSFORMER-BASED BEAT TRACKING WITH LOW-RESOLUTION\nENCODER AND HIGH-RESOLUTION DECODER\nTian Cheng Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\n{tian.cheng, m.goto}@aist.go.jp\nABSTRACT\nIn this paper, we address the beat tracking task which is to\npredict beat times corresponding to the input audio. Due to\nthe long sequential inputs, it is still challenging to model\nthe global structure efﬁciently and to deal with the data im-\nbalance between beats and no beats. In order to meet the\nabove challenges, we propose a novel Transformer-based\nmodel consisting of a low-resolution encoder and a high-\nresolution decoder. The encoder with low temporal reso-\nlution is suited to capture global features with more bal-\nanced data. The decoder with high temporal resolution is\ndesigned to predict beat times at a desired resolution. In\nthe decoder, the global structure is considered by the cross\nattention between the global features and high-dimensional\nfeatures. There are two key modiﬁcations in the proposed\nmodel: (1) adding 1D convolutional layers in the encoder\nand (2) replacing positional embedding by the upsampled\nencoder features in the decoder. In the experiment, we\nachieved the state-of-the-art performance and showed that\nthe decoder produced more precise and stable results.\n1. INTRODUCTION\nBeat tracking is an important task in Music Information\nRetrieval (MIR) area with a long history. The task is to\npredict beat times, a periodic sequence of time instants\nwhich people can tap along with, from musical pieces.\nThe ﬁrst attempt of beat tracking for polyphonic musical\naudio signals can date back to around 30 years ago [1].\nIn the past three decades, we see the techniques shift-\ning from signal processing to machine learning. In the\nmost recent deep-learning-based methods, sequence mod-\nels have been used to produce beat probabilities for each\ninput frame, with the ﬁnal beat times detected by the HMM\non the beat probabilities in the post-processing step. In\nthese models, various sequence models have been used, in-\ncluding Recurrent Neural Network (RNN) [2–4], Tempo-\nral Convolutional Network (TCN) [5–7], and Transform-\ners [8–10]. Convolutional Neural Networks (CNNs) are\nalso commonly combined in the models for front-end fea-\nture embedding [9, 11, 12].\n© T. Cheng and M. Goto. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nT. Cheng and M. Goto, “Transformer-based beat tracking with low-\nresolution encoder and high-resolution decoder”, in Proc. of the 24th\nInt. Society for Music Information Retrieval Conf., Milan, Italy, 2023.To produce good beat tracking results, the model needs\nto consider both local timing and global consistency. This\nbrings a contradiction on choosing the temporal resolu-\ntion. The problem of using low temporal resolution (i.e.,\nlow frame rate) is that we cannot predict beats with pre-\ncise times. On the other hand, using high temporal resolu-\ntion (i.e., high frame rate) results in long sequential inputs\nand imbalanced output labels. The current commonly-used\n10ms temporal resolution enables an easy comparison on\nthe results. With such high temporal resolution, the se-\nquential inputs are already relatively long for the RNN,\ncausing the gradient vanishing problem. Using TCN and\nTransformer helps to solve the gradient vanishing problem,\nwhile modeling long sequences can still be challenging.\nTo model long sequences more efﬁciently, more compact\nmodels have been proposed, such as dilated self-attention\n[9] and linear Transformer [10]. Another problem caused\nby the high temporal resolution is the data imbalance is-\nsue between beats and no beats. Given the same tempo,\nthe higher the temporal resolution is, the more the no-\nbeat labels exist between the beat labels. In order to solve\nthis problem, smoothed labels [7, 9, 13] and weighted loss\nfunctions designed for the data imbalance problem [14–16]\nare applied to achieve more efﬁcient training. The above\nlong sequence modelling issue and data imbalance issue\ncan be more challenging if a higher temporal resolution\nthan 10ms is needed. In fact, there are some commer-\ncial music applications that potentially require more tem-\nporally precise beat tracking for sample-wise audio edit-\ning/mixing/mashups based on beat timings and highly rigid\nmusic synchronization.\nIn order to tackle the contradiction between high and\nlow temporal resolutions, we propose a novel beat track-\ning model based on the Transformer with low-resolution\nencoder and high-resolution decoder. With the low tem-\nporal resolution, the sequential inputs become shorter and\nthe training data become more balanced, which makes the\nglobal structure easier to model by the encoder. At the\nsame time, the beat time precision in the output can still\nbe preserved by the decoder with the high temporal reso-\nlution. The Transformer is a good architecture for joining\nthe two parts because the encoder and decoder are not re-\nquired to be the same length, and features of different di-\nmensions can be jointly learned by the cross attention in\nthe decoder. We modify the original Transformer in sev-\neral ways to make it work for beat tracking with the pro-\nposed combination of the encoder and decoder. First, we466stack 2D convolutional layers for feature learning from the\nspectral inputs and 1D convolutional layers inside the en-\ncoder layers for feature smoothing and dimension adjust-\nment. Second, we use the upsampled encoder feature to\nreplace the position encoding in the decoder. In the ex-\nperiments, we produced results comparable to the state-of-\nthe-art performance. The analysis of experimental results\nshowed that the decoder not only produced more precise\nresults, but also helped to recover the missing beats and to\nﬁlter out unwanted peaks between beats, making the beat\ntracking more stable.\nThe rest of paper is organised as follows. Section 2\nsummarises the related work on Transformer-based beat\ntracking models and multi-scale models. In Section 3, we\ngive a detailed description of the proposed model, espe-\ncially focusing on the proposed modiﬁcations. Section 4\npresents the experiments with ablation study, results, and\nattention visualisation. In the last section, we conclude the\npaper and show aspects for future improvements.\n2. RELATED WORKS\n2.1 Transformer in beat tracking\nRecently, Transformers have been used for many MIR\ntasks with promising performance, such as music tran-\nscription [17–19], music tagging [20], and beat tracking\n[8–10]. In the SpectTNT model, Transformer encoders\nare used for modeling both the spectral and temporal fea-\ntures [8]. The model also combines the Temporal Con-\nvolutional Network (TCN) model for better beat tracking\nresults. Since the Transformer is computationally expen-\nsive for long sequences, the inputs are divided in 6-second\nchunks to process. For modelling the long sequences ef-\nﬁciently, more compact Transformers have been applied,\nincluding the dilated Transformer in the Beat Transformer\nmodel [9] and the linear Transformers for singing beat\ntracking [10].\nThese existing methods are based on Transformer en-\ncoders, while in our model, we use both the encoder and\ndecoder, which is an important contribution of this paper.\nBy adding the decoder layers, we can set a more reasonable\ntemporal resolution for the encoder input, more speciﬁ-\ncally, low-temporal-resolution inputs. In other words, in\nthe proposed model, the temporal resolution of the encoder\ncan be independent of the high temporal resolution of the\nbeat tracking output. With the low-resolution encoder, we\nare able to model the sequences more efﬁciently and obtain\nmore balanced training data.\n2.2 Multi-scale structure\nIn the proposed model, we leverage features at different\nscales: low-dimensional features for modeling the global\nstructure and high-dimensional features for predicting pre-\ncise beat times. Such multi-scale structure has also been\nused in related domains. In our previous work, we pro-\nposed a multi-scale beat tracking model based on the\nWave-U-Net, which learned features at different scales\nfrom waveform and spectral inputs with downsampling1D Conv\n2D Conv BlockPositional \nEncoding\n2D Conv Block\nInput_low\nInput_highMulti-Head \nAttentionAdd & NormFeed ForwardAdd & NormFeed ForwardAdd & Norm\nMulti-Head \nAttentionAdd & Norm\nMulti-Head \nAttentionAdd & NormDenseBeat Output \nMxNx\nUpsampling \nBlock\n1D Conv\nFigure 1 : The model architecture of the beat tracking\nmodel. The coloured parts indicate the modiﬁcations from\nthe original Transformer.\nand upsampling blocks [21]. Schreiber et al. achieved\ntempo estimation by concatenating multi-scale features\nlearned from a series of convolutional layers with differ-\nent ﬁlter size from 32 to 256 [22]. Sun et al. [23] propose\na multi-scale structure for tempo estimation by downsam-\npling/upsampling the feature to different scales and com-\nbining multi-scale features repeatedly.\n3. MODEL ARCHITECTURE\nThe proposed model architecture is shown in Figure 1. Our\nmodel is based on the Transformer, consisting of both en-\ncoder and decoder layers. As we have already written, the\nkey idea is to use a low-resolution encoder for modelling\nthe global structure, depicted on the left side of the ﬁg-\nure (starting from “Input_low” denoting the low-resolution\ninput), and a high-resolution decoder for predicting beats\nmore precisely, depicted on the right side of the ﬁgure\n(starting from “Input_high” denoting the high-resolution\ninput).\nTo make it work for beat tracking, we make some mod-\niﬁcations on the original Transformer. The modiﬁed parts\nare coloured in Figure 1, including adding 1D convolu-\ntional layers (“1D Conv”) in the encoder, replacing the po-\nsitional embedding by upsampled encoder features (from\n“Upsampling Block”) in the decoder, and stacking 2D con-\nvolutional layers for feature learning from the spectral in-\nputs (“2D Conv Block”) in both the encoder and decoder.\nIn the following subsections, we illustrate the proposed\nmodel in detail.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n467Network parameter Setting\nﬁlter size 3×3,3×3,3×3,1×3\nmaxpooling size 1×3,1×3,1×3\nactivation function ReLU\nTable 1 : Parameters used in the convolution block.\nEncoder\nConv Block ﬁlter number 48, 64, 72\nencoder layer number 3, 4, 5, 6\nhead number 4, 8, 12, 16\nkey dim. 8, 16, 24, 32\ninner-layer dim. in feed-forward 32, 48, 64, 72, 128\nConv 1D ﬁlter number 32, 64, 96\nConv 1D ﬁlter size 5, 15\nDecoder\nConv Block ﬁlter number 32\ndecoder layer number 1, 2\nhead number 4\nTable 2 : Hyperparameters in the proposed beat tracking\nmodel.\n3.1 Input features and 2D convolutional layers\nWe use the Mel-spectrogram as the input features. For\nthe low-resolution encoder, we computed 80-dimensional\nMel-spectrogram with a 22050 sample rate and a hop size\nof 1024, roughly corresponding to a 46 ms temporal res-\nolution (more precisely, 46.44 ms). The high-resolution\nMel-spectrogram is computed the same way but in a hop\nsize of 256, roughly corresponding to 12 ms temporal res-\nolution (more precisely, 11.61ms). The dimensions of the\ninputs are (T, 80) and (4T, 80), respectively, where T is\nthe frame length of the low-resolution input. We choose\nsuch resolutions so that the low-resolution can still distin-\nguish beat and no beat frames for fast-tempo pieces, and\nthe high-resolution outputs can be easily compared to those\nof other methods. In the 2D convolutional block, we stack\nfour 2D convolutional layers and three maxpooling layers\nfor feature embedding, with details show in Table 1.\n3.2 Encoder with 1D convolutional layers\nThe encoder consists of identical encoder layers which\nprocess the low-dimensional features. As shown on the left\nside in Figure 1, each encoder layer includes a multi-head\nattention sub-layer and a fully connected feed-forward\nnetwork with residual connections. Before the encoder,\nwe concatenate the features with the positional encod-\ning. Since in the Transformer, the input dimension is not\nchangeable within the encoder layers, we stack a 1D con-\nvolutional layer after the feed-forward network for feature\nsmoothing and channel number adjustment.3.3 Upsampling Block\nAnother important change of the proposed model is that\nwe replace the original positional encoding by upsampled\nencoder features for the decoder. In the upsampling block,\nthere are two upsampling layers with linear interpolation.\nThe upsampled features are then concatenated with the\nhigh-dimensional features. We also stack a 1D convolu-\ntional layer to re-dimension the concatenated features. In\nthe preliminary experiment, we conﬁrmed that the origi-\nnal positional encoding does not work well and the upsam-\npled features worked for indicating rough beat positions.\nThe ablation study for this replacement is presented in Sec-\ntion 4.3.\n3.4 Decoder\nThe decoder processes the high-dimensional features for\npredicting more precise beats. The decoder layer con-\nsists of three components. As shown on the right side in\nFigure 1, between the multi-head attention sub-layer and\na fully connected feed-forward network, there is another\nmulti-head attention sub-layer which computes the cross-\nattention between the low- and high-dimensional features.\nWe use the decoder as a discriminative model for predict-\ning the output based on the input, rather than a generative\nmodel as the original Transformer decoder. Hence we do\nnot need to use the causal mask in the ﬁrst multi-head at-\ntention sub-layer.\n3.5 Output Layer and Post-Processing\nAs shown in Figure 1, we stack a dense layer with the sig-\nmoid activation at the end of the decoder for producing\nbeat outputs. Then, we apply the DBN from Madmom [2]\nfor post-processing. We ﬁrst take the nearest integer of\nframe per second (fps) for the post-processing, and then\nmap the results to the original fps.\n3.6 Complete Architecture\nWe apply random search to ﬁnd the best hyperparameters\nfor the model. We set up a grid of hyperparameter values\naccording to Table 2, and randomly select a subset to com-\npare. The decoder uses the same parameters as the encoder\nif not present. The ﬁnally chosen parameters are shown in\nbold.\n4. EXPERIMENTS\n4.1 Data\nWe train, validate, and test the proposed model by using the\nstandard music datasets with beat annotations as shown in\nTable 3. For training and validation sets, all musical pieces\nare segmented into 30-second clips with 50% overlap. Seg-\nments from the same musical piece appear only in either\nthe training or validation set to ensure that there is no over-\nlap between the training and validation sets. Test results\nare obtained on the whole pieces without segmentation.\nWe do data augmentation for better tempo balance. We\nfollow the strategy in [7] to generate input features for lessProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n468Usage Datasets\ntraining only Beatles [24], Harmonix [25],\n5 RWC datasets [26, 27],\ntapcorrect [28]\n8-fold cross- Ballroom [29, 30], Hainsworth [31],\nvalidation SMC [32]\ntesting only GTZAN [33, 34]\nTable 3 : The usage of the standard datasets for beat track-\ning evaluation.\nFigure 2 : The tempo distribution before and after the data\naugmentation for the model training.\nrepresentative tempos by changing the hop size when com-\nputing the mel-spectrogram. The tempo distribution before\nand after the data augmentation is shown in Figure 2.\nInspired by [9], we also process the input mel-\nspectral features by Harmonic Percussive Source Separa-\ntion (HPSS) and obtain the original mel-spectrogram S,\nthe harmonic part H, and the percussive part P. In the pre-\nliminary experiment, we compare two way of using HPSS:\none way is as data augmentation which triples the training\ndata (i.e., we can use all of S,H, andPwith the same beat\nannotations); the other way is to concatenate three parts,\nS,H, andP, as the more informative input features. Since\nthe results showed that using HPSS as the data augmenta-\ntion works better, we decided to take that way.\n4.2 Training\nIn order to train the model effectively, we compare three\ntraining methods as shown in Table 4. The ﬁrst method\nis training the model (i.e., both the encoder and decoder)\nfrom scratch.\nFor the other two methods, we ﬁrst temporarily stack a\ndense layer at the end of the encoder and pre-train the en-\ncoder only with the low-resolution labels. Then we initial-\nize the encoder with this pre-trained model and start train-Method Initialization Encoder parameters\n1 None Trained with decoder\n2 Pre-trained encoder Not trainable\nin training decoder\n3 Pre-trained encoder Trainable\nin training decoder\nTable 4 : Three training methods (the third method was the\nbest).\ning the decoder. The second method trains the parameters\nof the decoder only, by freezing the parameters of this pre-\ntraind encoder. The third method trains the parameters of\nboth the encoder and decoder after the above initialization\nof the encoder.\nWe choose the third method for training the model be-\ncause it worked best in our preliminary experiments. The\nmodel is trained with binary cross-entropy by using the\nRMSprop optimiser [35] with a learning rate of 0.0002.\nThe batch size is set to 16 for pre-training the encoder, and\n4 for training the whole model.\n4.3 Ablation Study\nTo illustrate the inﬂuence of replacing the positional en-\ncoding by the upsampled encoder features in the decoder,\nwe show the differences on the training with and without\nthe proposed modiﬁcation (replacement) in Figure 3. We\nsee that using the upsampled encoder features decreased\nthe validation loss slightly and decreased the training loss\nin a large degree in comparison to using the positional en-\ncoding. This shows that the modiﬁed model can learn bet-\nter from the training data and generalise well in the valida-\ntion set, resulting in better beat tracking results.\n4.4 Evaluation\nWe evaluate the proposed method with three standard\nmetrics: F-measure with a tolerance window of 70ms,\ncontinuity-based metrics CMLt (tracking accuracy on the\ncorrect metrical level), and AMLt (tracking accuracy with\nalternate metrical levels allowed) [24].\n4.4.1 The proposed method\nIn order to validate our model design, besides results on the\ndecoder outputs, we also show results on the pre-trained\nencoder outputs. In Table 5, “Encoder (Th)” indicates the\nresults obtained by applying a threshold of 0.1 without us-\ning the DBN. “Encoder” and “Decoder (Proposed)” results\nare processed by the DBN in the post-processing step, with\nthe encoder outputs linear interpolated. If we compare the\nresults of “Encoder (Th)” and “Encoder”, we observe that\nthe DBN post-processing step increased the performance\nin all the four datasets, especially for the continuity-based\nresults (CMLt and AMLt). Furthermore, if we compare\nthem with the “Decoder (Proposed)” results, which corre-\nsponds to the proposed model, we see performance further\nincreased on the Ballroom, SMC, and GTZAN datasets.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n469Figure 3 : The training and validation losses for training the decoder with the original positional embedding or with upsam-\npled encoder features.\nMethod F-measure CMLt AMLt\nDataset: Ballroom\nEncoder (Th) 90.7 80.1 85.7\nEncoder 93 87.4 96.1\nDecoder (Proposed) 95 91.1 96.4\nBeat trans [9] 96.8 95.4 96.6\nTF trans [8] 96.2 93.9 96.7\nTCN [7] 96.2 94.7 96.1\nDataset: Hainsworth\nEncoder (Th) 84.4 66.7 81.8\nEncoder 88.2 81 93.4\nDecoder (Proposed) 87 76.2 93.6\nBeat trans [9] 90.2 84.2 91.8\nTF trans [8] 87.7 86.2 91.5\nTCN [7] 90.4 85.1 93.7\nDataset: SMC\nEncoder (Th) 53.9 32.9 45.6\nEncoder 55 45.8 64.1\nDecoder (Proposed) 55.4 45.1 65.6\nBeat trans [9] 59.6 45.6 63.5\nTF trans [8] 60.5 51.4 66.3\nTCN [7] 55.2 46.5 64.3\nDataset: GTZAN\nEncoder (Th) 87.1 72.8 85.5\nEncoder 87.8 78.5 93.7\nDecoder (Proposed) 88.4 80.8 94\nBeat trans [9] 88.5 80 92.2\nTF trans [8] 88.7 81.2 92\nTCN [7] 88.5 81.3 93.1\nTable 5 : Testing results for comparing the proposed\nmethod with three state-of-the-art beat tracking models\n[7–9]. The GTZAN dataset is held out for testing only;\nother datasets are used in the 8-fold cross-validation. (Th)\nmeans results obtained with a threshold of 0.1 without us-\ning the DBN post-processing step.\nIn order to understand the effect of the proposed de-\ncoder better, we show the outputs examples from the pre-trained encoder and the decoder in Figure 4. We can see\nthat the encoder outputs are large at beat times, which is\nbeneﬁt from the more balanced training data in shorter se-\nquences. On the other hand, the decoder outputs are small\nand even (i.e., more stable), as we expected. As we design,\nthe decoder basically predicts the beat times at a higher\nresolution in comparison to the encoder. The decoder also\nhelped to recover missing beats as shown in the 5th exam-\nple, where some beats are missing in the encoder output\nbut they are recovered in the decoder output. Moreover, the\ndecoder helped to ﬁlter out peaks between beats as shown\nin the 3rd example, where peaks are more regularly placed\nin the decoder output. With the above effects, using the\nproposed decoder generally improved results except for\nthe Hainsworth dataset. For the Hainsworth dataset, we\nsee the CMLt decreased, but the AMLt remained the same\nlevel, which means that the decrease on the performance is\ncaused by phase and octave errors. Since the peaks from\nthe decoder are evener, in some cases it would be more\ndifﬁcult for the DBN to exclude the peaks between beats.\n4.4.2 Comparison to state-of-the-art beat tracking\nmodels\nAs shown in Table 5, results of the proposed model (“De-\ncoder (Proposed)”) were comparable to the state-of-the-art\nresults obtained by three beat tracking models [7–9], de-\nspite not the best. Since our goal is not to achieve better\nperformances than all the state-of-the-art models, these re-\nsults are satisfactory since we can show the high perfor-\nmances of the proposed model with different temporal res-\nolutions. We see noticeable gap on the CMLt in compar-\nison to other methods, which means the proposed model\nencountered more phase and octave errors. We hope this\ncould be improved by including related topics in the multi-\ntask learning as in [7, 9]. In addition, for the testing-only\ndataset GTZAN, the F-measures achieved by thresholding\nthe encoder outputs (“Encoder (Th)”) are better than what\nwe expected, given the fact that it did not use the DBN\npost-processing step.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n470(a) Encoder outputs\n(b) Decoder outputs\nFigure 4 : Output examples from the pre-trained encoder\nand the decoder for ﬁve different pieces. The ground-truth\nbeat annotations are indicated by lines pointing down.\n4.5 Attention Visualisation\nIn order to understand how low- and high-dimensional fea-\ntures are jointly learned, we show the cross attention ma-\ntrix between low- and high-dimensional features in the sec-\nond decoder layer in Figure 5. We found that for high-\ndimension features at beat times (frames), it got attention\nat each beat on the low-dimensional features. For no-beat\ntimes (frames), all attentions were drawn to the frames af-\nter the corresponding beat times, which formed horizontal\nlines in this ﬁgure. With such attentions, the ﬁnal high-\ndimensional beat outputs were predicted with the captured\nglobal beat structure considered.\n5. CONCLUSIONS AND FUTURE WORK\nWe present a novel Transformer-based model for beat\ntracking. The proposed model consists of both en-\nFigure 5 : Cross attention matrix between low dimensional\nfeatures and high dimensional features in the decoder layer.\ncoder layers and decoder layers which work on low-\nand high-dimensional features, respectively. We obtained\nbeat tracking performances which are comparable to the\nstate-of-the-art beat tracking results. The experimen-\ntal results showed that the proposed model worked well\nas designed: with the low-dimensional (low-temporal-\nresolution) encoder for capturing the global beat structure\nand high-dimensional (high-temporal-resolution) decoder\nfor predicting more precise beats. Thus, the proposed\nTransformer-based encoder and decoder structure succeeds\nin providing a new framework for handling multi-scale fea-\ntures for beat tracking. Beyond beat tracking, the advan-\ntage of this framework can be summarized as follows.\n• The encoder and decoder do not require inputs to\nbe the same length (same temporal resolution), they\ncan be used to handling features at different scales,\nwhich enables us to sample the features with more\nreasonable time resolutions.\n• We can make use of features at different scales\njointly learned by the cross attention in the decoder.\nWe therefore believe that this framework is also adaptable\nfor other MIR tasks, such as musical structure boundary\ndetection.\nAs the analysis of our experimental results showed,\nphase and octave errors are relatively high in our results.\nAs future work, we would like to tackle the problems by\ncombining downbeat tracking and tempo estimation in the\nproposed model by using multi-task learning. In addition,\nwe also plan to use our model to produce beat outputs in\na higher temporal resolution, which is demanded by some\npractical music applications as we discussed in Section 1.\nYet another advantage of our model is that such precise\nbeats can be achieved by using transfer learning with a\nhigher-temporal-resolution input for the decoder.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4716. ACKNOWLEDGMENTS\nThis work was supported in part by JST CREST Grant\nNumber JPMJCR20D4 and JSPS KAKENHI Grant Num-\nber JP21H04917, Japan.\n7. REFERENCES\n[1] M. Goto and Y . Muraoka, “A Beat Tracking System for\nAcoustic Signals of Music,” in Proc. ACM Multimedia ,\n1994, pp. 365–372.\n[2] S. Böck, F. Krebs, and G. Widmer, “A Multi-Model\nApproach to Beat Tracking Considering Heteroge-\nneous Music Styles,” in Proc. ISMIR , 2014.\n[3] ——, “Joint Beat and Downbeat Tracking with Recur-\nrent Neural Networks,” in Proc. ISMIR , 2016.\n[4] F. Krebs, S. Böck, and G. Widmer, “Downbeat Track-\ning Using Beat-Synchronous Features and Recurrent\nNetworks,” in Proc. ISMIR , 2016.\n[5] M. E. P. Davies and S. Böck, “Temporal Convolutional\nNetworks for Musical Audio Beat Tracking,” in Proc.\nEUSIPCO , 2019.\n[6] S. Böck, M. E. P. Davies, and P. Knees, “Multi-Task\nLearning of Tempo and Beat: Learning One to Improve\nthe Other,” in Proc. ISMIR , 2019, pp. 486–493.\n[7] S. Böck and M. E. Davies, “Deconstruct, Analyse, Re-\nconstruct: How to Improve Tempo, Beat, and Down-\nbeat Estimation,” in Proc. ISMIR , 2020.\n[8] Y . Hung, J. Wang, X. Song, W. T. Lu, and M. Won,\n“Modeling beats and downbeats with a time-frequency\ntransformer,” in Proc. ICASSP , 2022, pp. 401–405.\n[9] J. Zhao, G. Xia, and Y . Wang, “Beat Transformer:\nDemixed Beat and Downbeat Tracking with Dilated\nSelf-Attention,” in Proc. ISMIR , 2022, pp. 169–177.\n[10] M. Heydari and Z. Duan, “Singing Beat Tracking With\nSelf-supervised Front-end and Linear Transformers,”\ninProc. ISMIR , 2022, pp. 617–624.\n[11] M. Fuentes, B. Mcfee, H. C. Crayencour, S. Essid, and\nJ. P. Bello, “Analysis of Common Design Choices in\nDeep Learning Systems for Downbeat Tracking,” in\nProc. ISMIR , 2018.\n[12] T. Cheng, S. Fukayama, and M. Goto, “Joint Beat\nand Downbeat Tracking Based on CRNN Models and\na Comparison of Using Different Context Ranges in\nConvolutional Layers,” in Proc. ICMC , 2020.\n[13] ——, “Convolving Gaussian Kernels for RNN-based\nBeat Tracking,” in Proc. EUSIPCO , 2018, pp. 1919–\n1923.\n[14] F. Pedersoli and M. Goto, “Dance Beat Tracking from\nVisual Information Alone,” in Proc. ISMIR , 2020, pp.\n400–408.[15] C. J. Steinmetz and J. D. Reiss, “WaveBeat: End-to-\nend beat and downbeat tracking in the time domain,”\nin151st AES Convention , 2021.\n[16] T.-P. Chen and L. Su, “Toward postprocessing-free\nneural networks for joint beat and downbeat estima-\ntion,” in Proc. ISMIR , 2022, pp. 27–35.\n[17] C. Hawthorne, I. Simon, R. Swavely, E. Manilow, and\nJ. Engel, “Sequence-to-sequence piano transcription\nwith transformers,” in Proc. ISMIR , 2021, pp. 246–\n253.\n[18] L. Oua, Z. Guo, E. Benetos, J. Han, and Y . Wang,\n“Exploring transformer’s potential on automatic piano\ntranscription,” in Proc. ICASSP , 2022, pp. 776–780.\n[19] J. Gardner, I. Simon, E. Manilow, C. Hawthorne, and\nJ. H. Engel, “Mt3: multi-task multitrack music tran-\nscription,” in Proc. of the Tenth International Confer-\nence on Learning Representations (ICLR) , 2022.\n[20] M. Won, K. Choi, and X. Serra, “Semi-supervised mu-\nsic tagging transformer,” in Proc. ISMIR , 2021, pp.\n769–776.\n[21] T. Cheng and M. Goto, “U-Beat: A multi-scale\nbeat tracking model based on Wave-U-Net,” in Proc.\nICASSP , 2023.\n[22] H. Schreiber and M. Meinard, “A single-step approach\nto musical tempo estimation using a convolutional neu-\nral network,” in Proc. ISMIR , 2018, pp. 98–105.\n[23] X. Sun, Q. He, Y . Gao, and W. Li, “Musical Tempo\nEstimation Using a Multi-scale Network,” in Proc. IS-\nMIR, 2021.\n[24] M. E. P. Davies, N. Degara, and M. D. Plumbley,\n“Evaluation Methods for Musical Audio Beat Track-\ning Algorithms,” Queen Mary University of London,\nLondon, United Kingdom, Tech. Rep. C4DM-TR-09-\n06, 2009.\n[25] O. Nieto, M. McCallum, M. Davies, A. Robertson,\nA. Stark, and E. Egozy, “The harmonix set: Beats,\ndownbeats, and functional segment annotations of\nwestern popular music,” in Proc. ISMIR , 2019.\n[26] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka,\n“RWC Music Database: Popular, Classical, and Jazz\nMusic Databases,” in Proc. ISMIR , 2002, pp. 287–288.\n[27] ——, “RWC Music Database: Music Genre Database\nand Musical Instrument Sound Database,” in Proc. IS-\nMIR, 2003, pp. 229–230.\n[28] J. Driedger, H. Schreiber, W. B. de Haas, and\nM. Müller, “Towards automatically correcting tapped\nbeat annotations for music recordings,” in Proc. ISMIR ,\n2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n472[29] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-\ntakis, C. Uhle, and P. Cano, “An Experimental Com-\nparison of Audio Tempo Induction Algorithms,” IEEE\nTrans. Audio, Speech, Language Process. , vol. 14,\nno. 5, pp. 1832–1844, 2006.\n[30] F. Krebs, S. Böck, and G. Widmer, “Rhythmic Pattern\nModeling for Beat and Downbeat Tracking in Musical\nAudio,” in Proc. ISMIR , 2013, pp. 227–232.\n[31] S. W. Hainsworth and M. D. Macleod, “Particle Filter-\ning Applied to Musical Tempo Tracking,” EURASIP\nJournal on Applied Signal Process. , vol. 15, 2004.\n[32] A. Holzapfel, M. E. P. Davies, J. R. Zapata, J. a. L.\nOliveira, and F. Gouyon, “Selective Sampling for Beat\nTracking Evaluation,” IEEE Trans. Audio, Speech,\nLanguage Process. , vol. 20, no. 9, pp. 2539–2548,\n2012.\n[33] G. Tzanetakis and P. Cook, “Musical Genre Classiﬁca-\ntion of Audio Signals,” IEEE Speech Audio Process. ,\nvol. 10, no. 5, 2002.\n[34] U. Marchand and G. Peeters, “Swing Ratio Estima-\ntion,” in Proc. DAFx , 2015.\n[35] T. Tieleman and G. Hinton, “Lecture 6.5—RMSProp:\nDivide the Gradient by a Running Average of its Re-\ncent Magnitude,” COURSERA: Neural Networks for\nMachine Learning, 2012.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n473"
    },
    {
        "title": "Automatic Piano Transcription With Hierarchical Frequency-Time Transformer.",
        "author": [
            "Keisuke Toyama 0002",
            "Taketo Akama",
            "Yukara Ikemiya",
            "Yuhta Takida",
            "Wei-Hsiang Liao",
            "Yuki Mitsufuji"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265261",
        "url": "https://doi.org/10.5281/zenodo.10265261",
        "abstract": "Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription.\nThis is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content.\nIn this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes.\nIn this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture.\nThe first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis.\nThe output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis.\nWe evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations.",
        "zenodo_id": 10265261,
        "dblp_key": "conf/ismir/0002AITLM23",
        "keywords": [
            "automatic piano transcription",
            "long-term spectral and temporal dependencies",
            "determining precise onset and offset",
            "self-attention mechanism",
            "Transformer architecture",
            "hFT-Transformer",
            "two-level hierarchical frequency-time Transformer architecture",
            "Convolutional block",
            "Transformer encoder",
            "Transformer decoder"
        ],
        "ee": "https://zenodo.org/records/10265261/files/000024.pdf",
        "content": "AUTOMATIC PIANO TRANSCRIPTION WITH HIERARCHICAL\nFREQUENCY-TIME TRANSFORMER\nKeisuke Toyama1Taketo Akama2Yukara Ikemiya1\nYuhta Takida1Wei-Hsiang Liao1Yuki Mitsufuji1\n1Sony Group Corporation, Japan\n2Sony Computer Science Laboratories, Japan\nkeisuke.toyama@sony.com\nABSTRACT\nTaking long-term spectral and temporal dependencies into\naccount is essential for automatic piano transcription. This\nis especially helpful when determining the precise onset\nand offset for each note in the polyphonic piano content.\nIn this case, we may rely on the capability of self-attention\nmechanism in Transformers to capture these long-term de-\npendencies in the frequency and time axes. In this work,\nwe propose hFT-Transformer , which is an automatic mu-\nsic transcription method that uses a two-level hierarchical\nfrequency-time Transformer architecture. The ﬁrst hier-\narchy includes a convolutional block in the time axis, a\nTransformer encoder in the frequency axis, and a Trans-\nformer decoder that converts the dimension in the fre-\nquency axis. The output is then fed into the second hi-\nerarchy which consists of another Transformer encoder in\nthe time axis. We evaluated our method with the widely\nused MAPS and MAESTRO v3.0.0 datasets, and it demon-\nstrated state-of-the-art performance on all the F1-scores of\nthe metrics among Frame ,Note ,Note with Offset , and Note\nwith Offset and Velocity estimations.\n1. INTRODUCTION\nAutomatic music transcription (AMT) is to convert music\nsignals into symbolic representations such as piano rolls,\nMusical Instrument Digital Interface (MIDI), and musical\nscores [1]. AMT is important for music information re-\ntrieval (MIR), its result is useful for symbolic music com-\nposition, chord progression recognition, score alignment,\netc. Following the conventional methods [1–15], we esti-\nmate the frame-level metric and note-level metrics as fol-\nlows: (1) Frame : the activation of quantized pitches in\neach time-processing frame, (2) Note : the onset time of\neach note, (3) Note with Offset : the onset and offset time\nof each note, and (4) Note with Offset and Velocity : the\nonset, offset time, and the loudness of each note.\n© K. Toyama, T. Akama, Y . Ikemiya, Y . Takida, W. H. Liao,\nand Y . Mitsufuji. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: K. Toyama, T. Akama,\nY . Ikemiya, Y . Takida, W. H. Liao, and Y . Mitsufuji, “Automatic Piano\nTranscription with Hierarchical Frequency-Time Transformer”, in Proc.\nof the 24th Int. Society for Music Information Retrieval Conf., Milan,\nItaly, 2023.For automatic piano transcription, it is important to an-\nalyze several harmonic structures that spread in a wide\nrange of frequencies, since piano excerpts are usually poly-\nphonic. Convolutional neural network (CNN)-based meth-\nods have been used to aggregate harmonic structures as\nacoustic features. Most conventional methods apply multi-\nlayer convolutional blocks to extend the receptive ﬁeld in\nthe frequency axis. However, the blocks often include\npooling or striding to downsample the features in the fre-\nquency axis. Such a downsampling process may reduce\nthe frequency resolution [6]. It is worth mentioning, many\nof these methods use 2-D convolutions, which means the\nconvolution is simultaneously applied in the frequency and\ntime axes. The convolution in the time axis works as a pre-\nemphasis ﬁlter to model the temporal changes of the input\nsignals.\nUp to now, recurrent neural networks (RNNs), such as\ngated recurrent unit (GRU) [16] and long short-term mem-\nory (LSTM) [17], are popular for analyzing the temporal\nsequences of acoustic features. However, recently some of\nthe works start to use Transformer [18], which is a pow-\nerful tool for analyzing sequences, in AMT tasks. Ou\net al. [2] applied a Transformer encoder along the time\naxis and suggested that using Transformer improves ve-\nlocity estimation. Hawthorne et al. [3] used a Transformer\nencoder-decoder as a sequence-to-sequence model for es-\ntimating a sequence of note events from another sequence\nof input audio spectrograms. Their method outperformed\nother methods using GRUs or LSTMs. Lu et al. [19] pro-\nposed a method called SpecTNT to apply Transformer en-\ncoders in both frequency and time axes and reached state-\nof-the-art performance for various MIR tasks such as mu-\nsic tagging, vocal melody extraction, and chord recogni-\ntion. This suggests that such a combination of encoders\nhelps in characterizing the broad-scale dependency in the\nfrequency and time axes. However, SpecTNT aggregates\nspectral features into one token, and the process in its\ntemporal Transformer encoder is not independent in the\nfrequency axis. This inspires us to incorporate Trans-\nformer encoders in the frequency and time axes and make\nthe spectral information available for the temporal Trans-\nformer encoder.\nIn addition, we usually divide the input signal into\nchunks since the entire sequence is often too long to be2152ndHierarchy 1stHierarchyInput\n2ndTransformer encoder\n(Time axis)1stTransformer encoder\n(Frequency axis)Output M+1+M\nP\nN\nN Mframe\noffsetonset\nvelocityspectrogram\nMF\nNF\nM+1+MP\nN\nP\nNP\nN NPConvolutional block\n(1-D CNN (time axis))Converter\nF P\n(Transformer\ndecoder)\nTransformer\nencoder\nFTransformer\nencoder\nTransformer\nencoderTransformer\nencoder\nTransformer\nencoder\nTransformer\nencoder\nN\nFigure 1 . hFT-Transformer (N: number of frames in each processing chunk, M: length of margin, F: number of frequency\nbins, P: number of pitches)\ndealt at once. However, this raises a problem that the es-\ntimated onset and offset accuracy ﬂuctuates depending on\nthe relative position in the processing chunk. In our obser-\nvation, the accuracy tends to be worse at both ends of the\nprocessing chunk. This motivates us to incorporate extra\ntechniques during the inference time to boost the perfor-\nmance.\nIn summary, we propose hFT-Transformer , an auto-\nmatic piano transcription method that uses a two-level hi-\nerarchical frequency-time Transformer architecture. Its\nworkﬂow is shown in Figure 1. The ﬁrst hierarchy con-\nsists of a one-dimensional (1-D) convolutional block in the\ntime axis, a Transformer encoder in the frequency axis, and\na Transformer decoder in the frequency axis. The second\nhierarchy consists of another Transformer encoder in the\ntime axis. In particular, the Transformer decoder at the\nend of the ﬁrst hierarchy converts the dimension in the\nfrequency axis from the number of frequency bins to the\nnumber of pitches (88 for piano). Regarding the issue of\nthe location dependent accuracy ﬂuctuation in the process-\ning chunks, we propose a technique which halves the stride\nlength at inference time. It uses only the result of the cen-\ntral part of processing chunks, which will improve overall\naccuracy. Finally, in Section 4, we show that our method\noutperforms other piano transcription methods in terms of\nF1 scores for all the four metrics.\nAPyTorch implementation of our method is available\nhere1.\n2. RELATED WORK\nNeural networks, such as CNNs, RNNs, generative adver-\nsarial networks (GANs) [20], and Transformers have been\ndominant for AMT. Since Sigtia et al. [4] proposed the\nﬁrst method to use a CNN to tackle AMT, CNNs have\nbeen widely used for the methods of analyzing the spec-\ntral dependency of the input spectrogram [2, 6–10, 12–15].\nHowever, it is difﬁcult for CNNs to directly capture the\nharmonic structure of the input sound in a wide range of\nfrequencies, as convolutions are used to capture features\nin a local area. Wei et al. [5] proposed a method of us-\ning harmonic constant-Q transform (CQT) for capturing\nthe harmonic structure of piano sounds. They ﬁrst ap-\nplied a 3-Dimensional CQT, then applied multiple dilated\nconvolutions with different dilation rates to the output of\n1https://github.com/sony/hFT-TransformerCQT. Because the dilation rates are designed to capture\nthe harmonics, the performance of Frame andNote accu-\nracy reached state-of-the-art. However, the dilation rates\nare designed speciﬁcally for piano. Thus, the method is\nnot easy to adapt to other instruments.\nFor analysis of time dependency, Kong et al. [6] pro-\nposed a method that uses GRUs. Howthorner et al. [7],\nKwon et al. [8], Cheuk et al. [9], and Wei et al. [5] pro-\nposed methods that use bi-directional LSTMs for analysis.\nOu et al. [2] used a Transformer encoder to replace the\nGRUs in Kong et al.’s method [6], and showed the effec-\ntiveness of the Transformer. Usually, the note onset and\noffset are estimated in each frequency and time-processing\nframe grid, then paired as a note for note-level transcrip-\ntion by post-processing algorithms such as [6]. How-\never, compared to heuristically designed algorithms, end-\nto-end data-driven methods are often preferred. For exam-\nple, Keltz et al. [10] applied a seven-state hidden Markov\nmodel (HMM) for the sequence of attack, decay, sustain,\nand release to achieve note-level transcription. Kwon et\nal. [8] proposed a method of characterizing the output of\nLSTM as a ﬁve-state statement (onset, offset, re-onset, ac-\ntivate, and inactivate). Hawthorne et al. [3] proposed a\nmethod of estimating a sequence of note events, such as\nnote pitch, velocity, and time, from another sequence of\ninput audio spectrograms using a Transformer encoder-\ndecoder. This method performs well in multiple instru-\nments with the same model [11]. Yan et al. [12] proposed\na note-wise transcription method for estimating the interval\nbetween onset and offset. This method shows state-of-the-\nart performance in estimating Note with Offset andNote\nwith Offset and Velocity . However, the performance in es-\ntimating Frame andNote is worse than that of Wei et al.’s\nmethod [5].\n3. METHOD\n3.1 Conﬁguration\nOur proposed method aims to transcribe Nframes of the\ninput spectrogram into Nframes of the output piano rolls\n(frame ,onset ,offset , and velocity ) as shown in Figure\n1, where Nis the number of frames in each processing\nchunk. Each input frame is composed of a log-mel spec-\ntrogram having size ( F,M+ 1 +M), where Fis the\nnumber of frequency bins, and Mis the size of the for-\nward margin and that of the backward margin. To obtainProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2162ndHierarchy 1stHierarchy\nposition\nembedding1st Transformer encoder 2nd Transformer encoder\nlog mel\nspectrogramconv blockposition\nembedding\n{0,1,…,F -1}linear\n{0,1,…,N -1}(B, N, F, M+1+M)(B*N, F, C*((M+1+M) -(K-1)))\n(B*N, F)(B*N, F, Z)(B*N, F, Z)\n(B*N, P, Z)\n(B*P, N, Z)\n(B*P, N)(B*P, N, Z)(B*P, N, Z)frame\nsigmoid\nlinearonset\nsigmoid\nlinearoffset\nsigmoid\nlinearvelocity\nlinearoutput_2nd\n(B, N, P, 128) (B, N, P)\nposition\nembeddingmulti -head\nattentionadd & normfeed forwardadd & norm\nadd & norm\nmulti -head\nattention\nx 3layerTransformer decoder\n{0,1,…,P -1}transpose\n(B*N, P)(B*N, P, Z)multi -head\nattentionadd & normfeed forwardadd & norm\nx 3layerframe\nsigmoid\nlinearonset\nsigmoid\nlinearoffset\nsigmoid\nlinearvelocity\nlinearoutput_1st\n(B, N, P, 128) (B, N, P)\nmulti -head\nattentionadd & normfeed forwardadd & norm\nx 3layer(B*N, F, Z)\nFigure 2 . Model architecture of hFT-Transformer\nthe log-mel spectrogram, we ﬁrst downmix the input wave-\nform into one channel and resample them to 16 kHz. Then,\nthe resampled waveform is transformed into a mel spectro-\ngram with transforms.MelSpectrogram class in\ntheTorchaudio library [21]. For the transformation, we\nusehann window, setting the window size as 2048, fast-\nFourier-transform size as 2048, Fas 256, padding mode\nasconstant , and hop-size as 16 ms. The magnitude of the\nmel spectrogram is then compressed with a log function.\n3.2 Model Architecture and Loss Functions\nThe model architecture of our proposed method is shown\nin Figure 2. We ﬁrst apply a convolutional block to the\ninput log-mel spectrogram, the size of which is ( B,N,F,\nM+1+M) whereBis the batch size. In the convolutional\nblock, we apply a 1-D convolution in the M+ 1 +M\ndimension. After this process, the data are embedded with\na linear module.\nThe embedded vector is then processed with the ﬁrst\nTransformer encoder in the frequency axis. The self-\nattention is processed to analyze the dependency between\nspectral features. The positional information is designated\nas [0,1, ...,F−1]. These positional values are then em-\nbedded with a trainable embedding. These are processed\nin the frequency axis only, thus completely independent to\nthe time axis ( Ndimension).\nNext, we convert the frequency dimension from Fto\nthe number of pitches ( P). A Transformer decoder with\ncross-attention is used as the converter. The Transformer\ndecoder calculates the cross-attention between the output\nvectors of the ﬁrst Transformer encoder and another train-\nable positional embedding made from [ 0,1, ...,P−1]. The\ndecoded vectors are then converted to the outputs of the\nﬁrst hierarchy with a linear module and a sigmoid function\n(hereafter, we call these outputs output_1st ).\nRegarding the loss calculation for the outputs, frame ,\nonset , and offset are calculated with binary cross-entropy,andvelocity is calculated with 128-category cross-entropy.\nThe losses can be summarized as the following equations:\nL<m>\nbce=N−1/summationdisplay\nn=0P−1/summationdisplay\np=0lbce(y<m>\nn,p,ˆy<m>\nn,p), (1)\nLvelocity\ncce=N−1/summationdisplay\nn=0P−1/summationdisplay\np=0lcce(yvelocity\nn,p,ˆyvelocity\nn,p), (2)\nL=Lframe\nbce+Lonset\nbce+Loﬀset\nbce+Lvelocity\ncce,(3)\nwhere<m>is the placeholder for each output ( frame ,\nonset , and offset ),lbceandlccedenote the loss function for\nbinary cross-entropy and categorical cross-entropy, respec-\ntively, and yandˆydenote the ground truth and predicted\nvalues of each output ( frame ,onset ,offset , and velocity ),\nrespectively. Although it is intuitive to apply the mean\nsquared error (MSE) for velocity , we found that using the\ncategorical cross-entropy yields much better performance\nthan the MSE from a preliminary experiment.\nFinally, the output of the converter is processed with\nanother Transformer encoder in the time axis. The self-\nattention is used to analyze the temporal dependency of\nfeatures in each time-processing frame. A third positional\nembedding made from [ 0,1, ...,N−1] is used here. Then,\nsimilar to the ﬁrst hierarchy, the outputs of the second hier-\narchy are obtained through a linear module and a sigmoid\nfunction. We call these outputs of the second hierarchy as\noutput_2nd hereafter. The losses for the output_2nd are\nevaluated in the same way as those for output_1st . These\nlosses are summed with the coefﬁcients α1standα2ndas\nfollows:\nLall=α1stL1st+α2ndL2nd. (4)\nAlthough both outputs are used for computing losses dur-\ning training, only output_2nd is used in inference. As Chen\net al. [22] suggested that the performance of their method\nof calculating multiple losses outperformed the methodProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2170 20 40 60 80 100 120\nn678error_n×103 frame\n0 20 40 60 80 100 120\nn3.03.23.4error_n×104 onset\n0 20 40 60 80 100 120\nn7.58.08.5error_n×104 offset\n0 20 40 60 80 100 120\nn4.04.14.24.3error_n×104 velocityFigure 3 . Estimation error (Eqn (5)) on location in each\ntime-processing frame\nthat uses single loss only, it hints us that utilizing both\noutput_1st andoutput_2nd in training has the potential to\nachieve better performance.\n3.3 Inference Stride\nAs mentioned in Section 1, chunk-based processing is re-\nquired because the input length is limited due to system\nlimitations, such as memory size and acceptable process-\ning delay. We found that the estimation error tends to in-\ncrease at certain part within each processing chunk. This\ncan be demonstrated by evaluating the error for each in-\nstance of time nwithin the chunks:\nerror<m>\nn=1\nIPI−1/summationdisplay\ni=0P−1/summationdisplay\np=0(y<m>\ni,n,p−ˆy<m>\ni,n,p)2,(5)\nwhere<m>is the placeholder for each output ( frame ,\nonset ,offset , and velocity ), andIis the number of pro-\ncessing chunks over the test set. The result using our pro-\nposed model trained using the MAESTRO training set (de-\nscribed in Section 4) is shown in Figure 3. Here, the error\nerror<m>\nn is calculated using the MAESTRO test set. In\nthe ﬁgure, we observe a monotonic decrease for frame and\na similar but much weaker trend for onset andoffset . How-\never, for velocity , no such trend can be observed. This hints\nus to use only the middle portion of a processing chunk as\nthe output to reduce the error rate. We call this as the half-\nstride strategy, since a 50% overlap is required for process-\ning chunks, as shown in Figure 4 (B).\n4. EXPERIMENTS\n4.1 Datasets\nWe use two well-known piano datasets for the evaluation.\nThe MAPS dataset [23] consists of CD-quality recordings\nand corresponding annotations of isolated notes, chords,\nand complete piano pieces. We use the full musical\npieces and the train/validation/test split as stated in [4, 7].\nThe number of recordings and the total duration in hours\nin each split are 139/71/60 and 8.3/4.4/5.5, respectively.\nThe MAESTRO v3.0.0 dataset [13] includes about 200\nhours of paired audio and MIDI recordings from ten years\nof the International Piano-e-Competition. We used the\n(A) Full stride\n(B) Half strideN N\nN N\nN\nNNN\nN/4 N/4 N/2\nN/4 N/2 N/4\nN/4 N/4 N/2\nN/4 N/4 N/2Figure 4 . Inference stride: (A) full stride, (B) half stride\ntrain/validation/test split conﬁguration as provided. In\neach split, the number of recordings and total duration in\nhours are 962/137/177 and 159.2/19.4/20.0, respectively.\nFor both datasets, the MIDI data have been collected by\nYamaha Disklaviers concert-quality acoustic grand pianos\nintegrated with a high-precision MIDI capture and play-\nback system.\n4.2 Model Conﬁguration\nRegarding our model architecture depicted in Figure 2, we\nsetNas 128,Mas 32,Fas 256,Pas 88, the CNN chan-\nnels (C) as 4, size of the CNN kernel ( K) as 5, and embed-\nding vector size ( Z) as 256. For the Transformers, we set\nthe feed-forward network vector size as 512, the number\nof heads as 4, and the number of layers as 3. For training,\nwe used the following settings: a batch size of 8, learn-\ning rate of 0.0001 with Adam optimizer [24], dropout rate\nof0.1, and clip norm of 1.0.ReduceLROnPlateu in\nPyTorch is used for learning rate scheduling with default\nparameters. We set α1standα2ndas 1.0, which were de-\nrived from a preliminary experiment (see Section 4.6).\nWe trained our models for 50 epochs on MAPS dataset\nand 20 epochs for MAESTRO dataset using one NVIDIA\nA100 GPU. It took roughly 140 minutes and 43.5 hours to\ntrain one epoch with our model for MAPS and MAESTRO,\nrespectively. The best model is determined by choosing the\none with the highest F1 score in the validation stage.\nIn order to obtain high-resolution ground truth for onset\nandoffset , we followed the method in Kong et al. [6]. We\nsetJ, the hyper-parameter to control the sharpness of the\ntargets, to 3. Also, the label of velocity is set only when an\nonset is present. We set the threshold as 0.5, which means\nif the onset is smaller than 0.5, the velocity is set as 0.\n4.3 Inference\nAt inference time, we use output_2nd as the ﬁnal output.\nWe set the threshold for frame as 0.5. For note-wise events\n(onset ,offset , and velocity ), the outputs in each pitch-frame\ngrid are converted to a set containing note-wise onset, off-\nset, and velocity following Kong et al.’s Algorithm 1 [6] in\nﬁve steps shown below:\nStep 1. onset detection: ﬁnd a local maximum in onset\nwith a value at least 0.5. Then calculate the precise onset\ntime using the values of the adjacent three frames [6].\nStep 2. velocity: If an onset is detected in Step 1, extract\nthevelocity value at the frame. If the value is zero, thenProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n218MethodhalfParamsFrame Note Note w/ Offset Note w/ Offset&Velocity\nstride P(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%)\nOnsets&Frames [7] 26M 88.53 70.89 78.30 84.24 80.67 82.29 51.32 49.31 50.22 35.52 30.80 35.59\nADSR [10] 0.3M 90.73 67.85 77.16 90.15 74.78 81.38 61.93 51.66 56.08 - - -\nhFT-Transformer 5.5M 83.36 82.00 82.67 86.63 83.75 85.07 67.18 65.06 66.03 48.75 47.21 47.92\nhFT-Transformer ✓ 5.5M 83.68 82.11 82.89 86.72 83.81 85.14 67.51 65.36 66.34 49.05 47.48 48.20\nTable 1 . Evaluation results on MAPS test dataset (P: precision, R: recall, bold : best score, underline : second best score)\nMethodhalfParamsFrame Note Note w/ Offset Note w/ Offset&Velocity\nstride P(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%)\nSeq2Seq [3] 54M - - - - - 96.01 - - 83.94 - - 82.75\nHPT-T [2] - - - 90.09 97.88 96.72 96.77 84.13 82.31 83.20 82.85 81.07 81.90\nSemi-CRFs [12] 9M 93.79 88.36 90.75 98.69 93.96 96.11 90.79 86.46 88.42 89.78 85.51 87.44\nHPPNet-sp [5] 1.2M 92.79 93.59 93.15 98.45 95.95 97.18 84.88 82.76 83.80 83.29 81.24 82.24\nhFT-Transformer 5.5M 92.62 93.43 93.02 99.62 95.41 97.43 92.32 88.48 90.32 91.21 87.44 89.25\nhFT-Transformer ✓ 5.5M 92.82 93.66 93.24 99.64 95.44 97.44 92.52 88.69 90.53 91.43 87.67 89.48\nTable 2 . Evaluation results on MAESTRO v3.0.0 test dataset\n②61\n③4.043\n④4.064①4.003\n0 0 61 61 61 0 0 0 0 0 velocity0.00 0.29 0.65 0.93 0.75 0.40 0.05 0.00 0.00 0.00 onset\n0.00 0.00 0.01 0.11 0.51 0.80 0.86 0.70 0.31 0.25 offset\n0.00 0.00 0.01 0.97 1.00 1.00 0.75 0.20 0.01 0.00 frame3.952 3.968 3.984 4.000 4.016 4.032 4.048 4.064 4.080 4.096 time [sec]\n{onset: 4.003, offset: 4.043, velocity: 61}\nFigure 5 . Example of conversion from grid-wise values to\nnote-wise values\ndiscard both onset and velocity at this frame.\nStep 3. offset detection with offset :ﬁnd a local maxi-\nmum in offset with a value at least 0.5. Then calculate the\nprecise offset time using the values of the adjacent three\nframes [6].\nStep 4. offset detection with frame :choose the frame\nthat is nearest to the detected onset which has a frame value\nbelow 0.5.\nStep 5. offset decision: choose the smaller value between\nthe results of Step 3 and 4.\nAn example is shown in Figure 5. The onset is 4.003,\nand the velocity is 61. For offset , the direct estimation from\noffset is 4.043, and that estimated via frame is 4.064. Thus,\nwe choose 4.043 as offset . Finally, we obtain a note with\n{onset : 4.003, offset : 4.043, velocity : 61} in the output.\n4.4 Metrics\nWe evaluate the performance of our proposed method with\nframe-level metrics ( Frame ) and note-level metrics ( Note ,\nNote with Offset , and Note with Offset & Velocity ) with the\nstandard precision, recall, and F1 scores. We calculated\nthese scores using mir_eval library [25] with its default\nsettings. The scores were calculated per recording, and the\nmean of these per-recording scores was presented as the\nﬁnal metric for a given collection of pieces, as explainedin Hawthorne et al. [7].\n4.5 Results\nTables 1 and 2 show the scores on the test sets of\nMAPS and MAESTRO datasets. The numbers of pa-\nrameters in these Tables are referred from [5, 10]. For\nthe MAPS dataset, our proposed method outperformed\nthe other methods in F1 score for all metrics. For the\nMAESTRO dataset, our proposed method outperformed\nthe other methods in F1 score for Note ,Note with Off-\nset, and Note with Offset & Velocity . Furthermore, our\nmethod with the half-stride strategy which is mentioned in\n3.3 outperformed other methods in all metrics. In contrast,\nthe two state-of-the-art methods for MAESTRO, which are\nSemi-CRFs [12] and HPPNet-sp [5], performed well only\non a subset of the metrics.\nThe results suggest that the proposed two-level hierar-\nchical frequency-time Transformer structure is promising\nfor AMT.\n4.6 Ablation Study\nTo investigate the effectiveness of each module in our pro-\nposed method, we trained various combinations of those\nmodules using the MAPS training set and evaluated them\nusing the MAPS validation set. The variations are shown\nin Table 3. In this study, we call our proposed method\n1-F-D-T , which means it consists of the 1-D convolution\nblock, the ﬁrst Transformer encoder in the Frequency axis,\nthe Transformer Decoder, and the second Transformer en-\ncoder in the Time axis. Table 4 shows evaluation results\nfor each variation.\nSecond Transformer encoder in time axis. To verify\nthe effectiveness of the second Transformer encoder, we\ncompared the 1-F-D-T and the model without the second\nTransformer encoder (1-F-D-N). For the 1-F-D-N model,\nwe use output_1st in both training and inference stages as\nthe ﬁnal output. The result indicates that the second Trans-\nformer encoder improved Note with Offset performance, inProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n219Model1st-Hierarchy 2nd-HierarchyOutputConvolutional block 1st Transformer encoder Converter 2nd Transformer encoder\n1-F-D-T† 1-D (time axis) Frequency axis Transformer Decoder Time axis output_2nd\n1-F-D-N 1-D (time axis) Frequency axis Transformer Decoder n/a output_1st\n2-F-D-T 2-D Frequency axis Transformer Decoder Time axis output_2nd\n1-F-L-T 1-D (time axis) Frequency axis Linear Time axis output_2nd\nTable 3 . Model variations for ablation study (†: the proposed method, hFT-Transformer)\nModel ParamsFrame Note Note w/ Offset Note w/ Offset&Velocity\nP(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%)\n1-F-D-T† 5.5M 93.61 88.71 91.09 98.81 94.81 96.72 86.18 82.81 84.42 77.47 74.55 75.95\n1-F-D-N 3.9M 92.85 87.49 90.09 99.01 93.24 95.95 82.67 78.06 80.23 73.89 69.90 71.78\n2-F-D-T 6.1M 75.49 61.08 67.52 97.03 19.68 31.10 64.07 13.28 20.88 42.11 8.57 13.50\n1-F-L-T 3.4M 93.71 88.42 90.99 99.11 92.90 95.79 85.77 80.56 82.98 71.66 67.32 69.34\nTable 4 . Evaluation results of ablation study on MAPS validation dataset\nwhich the F1 score is 84.42 for 1-F-D-T and 80.23 for 1-\nF-D-N. This shows the effectiveness of the second Trans-\nformer encoder as it provides an extra pass to model the\ntemporal dependency of acoustic features, which is pre-\nsumably helpful in offset estimation.\nCompelxity of convolutional block. To investigate\nhow the complexity of the convolutional block affects the\nAMT performance, we compared the 1-F-D-T model and\nthe model that replaces the 1-D convolutional block with\na 2-D convolutional block (2-F-D-T). Surprisingly, the re-\nsult shows that the performance of the 2-F-D-T model is\nsigniﬁcantly worse than that of the 1-F-D-T model. This is\nprobably because the two modules working on the spectral\ndependency do not cohere with each other. The 2-D convo-\nlutional block may over aggregate the spectral information\nthus resulting into an effectively lower frequency resolu-\ntion. Then, the Transformer encoder can only evaluate the\nspectral dependency over an over-simpliﬁed feature space,\ncausing the performance degradation.\nConverter. We used a Transformer decoder to convert\nthe dimension in the frequency axis from FtoP. In con-\ntrast, almost all of the existing methods used a linear mod-\nule to achieve this. We compared the performance of the\n1-F-D-T model to a model with the Transfomer decoder\nreplaced with a linear converter (1-F-L-T). The result in-\ndicates that the 1-F-D-T model outperformed the 1-F-L-T\nmodel in F1 score for all four metrics. Especially, the dif-\nference in Note with Offset and Velocity is large (75.95 for\nthe 1-F-D-T model and 69.34 for the 1-F-L-T model in\nF1 score). This suggests that using a Transformer decoder\nas converter is an effective way of improving the perfor-\nmance, although the side effect is the increase of model\nsize.\nWe also investigated how the coefﬁcients for the loss\nfunctions, α1standα2ndin Eqn (4), affect the perfor-\nmance. We investigated six pairs of coefﬁcients of loss\nfunctions ( α1st,α2nd) in Eqn (4), i.e., (1.8, 0.2), (1.4, 0.6),\n(1.0, 1.0), (0.6, 1.4), (0.2, 1.8), and (0.0, 2.0), for the 1-F-\nD-T model. Figure 6 shows the F1 scores of frame ,onset ,\noffset , and velocity evaluated on the MAPS validation set\nin each epoch. These results indicate that the (1.0, 1.0) pair\n10 20 30 40 50\nepoch0.850.90F1Frame\n(1.8,0.2)\n(1.4,0.6)\n(1.0,1.0)\n(0.6,1.4)\n(0.2,1.8)\n(0.0,2.0)\n10 20 30 40 50\nepoch0.850.900.95F1Note\n(1.8,0.2)\n(1.4,0.6)\n(1.0,1.0)\n(0.6,1.4)\n(0.2,1.8)\n(0.0,2.0)\n10 20 30 40 50\nepoch0.750.800.85F1Note w/ Offset\n(1.8,0.2)\n(1.4,0.6)\n(1.0,1.0)\n(0.6,1.4)\n(0.2,1.8)\n(0.0,2.0)\n10 20 30 40 50\nepoch0.600.650.700.75F1Note w/ Offset&Velocity\n(1.8,0.2)\n(1.4,0.6)\n(1.0,1.0)\n(0.6,1.4)\n(0.2,1.8)\n(0.0,2.0)Figure 6 . Performance of 1-F-D-T model trained with six-\npairs of coefﬁcients of loss functions\nyields the best score. It also shows that the training con-\nverges faster when α1stis larger than α2nd. Importantly,\nif we omit the output_1st , which is the case when training\nwith the pair (0.0, 2.0), the training loss did not decrease\nmuch. Therefore, the F1 score stays around 0% and thus\ncannot be seen in Figure 6. This suggests that it is cru-\ncial to use both losses, output_1st andoutput_2nd in our\nproposed method.\n5. CONCLUSION\nIn this work, we proposed hFT-Transformer , an automatic\npiano transcription method that uses a two-level hierarchi-\ncal frequency-time Transformer architecture. The ﬁrst hi-\nerarchy consists of a 1-D convolutional block in the time\naxis, a Transformer encoder and a Transformer decoder in\nthe frequency axis, and the second hierarchy consists of a\nTransformer encoder in the time axis. The experiment re-\nsult based on two well-known piano datasets, MAPS and\nMAESTRO, revealed that our two-level hierarchical archi-\ntecture works effectively and outperformed other state-of-\nthe-art methods in F1 score for frame-level and note-level\ntranscription metrics. For future work, we would like to ex-\ntend our method to other instruments and multi-instrument\nsettings.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2206. ACKNOWLEDGMENTS\nWe would like to thank Giorgio Fabbro and Stefan Uh-\nlich for their valuable comments while preparing this\nmanuscript. We are grateful to Kin Wai Cheuk for his ded-\nicated support in preparing our github repository.\n7. REFERENCES\n[1] E. Benetos, S. Dixon, Z. Duan, and S. Ewert, “Auto-\nmatic music transcription: An overview,” IEEE Signal\nProcessing Magazine , vol. 36, no. 1, pp. 20–30, 2019.\n[2] L. Ou, Z. Guo, E. Benetos, J. Han, and Y . Hang,\n“Exploring transformer’s potential on automatic piano\ntranscription,” in Proc. of the 47th Int. Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\n2022, pp. 776–780.\n[3] C. Hawthorne, I. Simon, R. Swavely, E. Manilow, and\nJ. Engel, “Sequence-to-sequence piano transcription\nwith transformers,” in Proc. of the 22th Int. Society for\nMusic Information Retrieval Conf. , 2021, pp. 246–253.\n[4] S. Sigtia, E. Benetos, and S. Dixon, “An end-to-end\nneural network for polyphonic piano music transcrip-\ntion,” IEEE/ACM Transactions on Audio, Speech and\nLanguage Processing (TASLP) , vol. 24, no. 5, pp. 927–\n939, 2016.\n[5] W. Wei, P. Li, Y . Yu, and W. Li, “Hppnet: Modeling the\nharmonic structure and pitch invariance in piano tran-\nscription,” in Proc. of the 23rd Int. Society for Music\nInformation Retrieval Conf. , 2022, pp. 709–716.\n[6] Q. Kong, B. Li, X. Song, Y . Wan, and Y . Wang, “High-\nresolution piano transcription with pedals by regress-\ning onsets and offsets times,” IEEE/ACM Transactions\non Audio Speech and Language Processing , vol. 29,\npp. 3707–3717, 2021.\n[7] C. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Simon,\nC. Raffel, J. Engel, S. Oore, and D. Eck, “Onsets and\nframes: Dual-objective piano transcription,” in Proc.\nof the 19th Int. Society for Music Information Retrieval\nConf. , 2018, pp. 50–57.\n[8] T. Kwon, D. Jeong, and J. Nam, “Polyphonic pi-\nano transcription using autoregressive multi-state note\nmodel,” in Proc. of the 21st Int. Society for Music In-\nformation Retrieval Conf. , 2020, pp. 454–460.\n[9] K. W. Cheuk, Y .-J. Luo, E. Benetos, and D. Herre-\nmans, “The effect of spectrogram reconstruction on au-\ntomatic music transcription: An alternative approach\nto improve transcription accuracy,” in Proc. of the\n25th International Conference on Pattern Recognition\n(ICPR) , 2020, pp. 9091–9098.\n[10] R. Kelz, S. Böck, and G. Widmer, “Deep polyphonic\nadsr piano note transcription,” in Proc. of the 44th Int.\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2019, pp. 246–250.[11] J. Gardner, I. Simon, E. Manilow, and C. H. J. Engel,\n“Mt3: Multi-task multitrack music transcription,” in\nProc. of the Int. Conference on Learning Representa-\ntions (ICLR) , 2022.\n[12] Y . Yan, F. Cwitkowitz, and Z. Duan, “Skipping the\nframe-level: Event-based piano transcription with neu-\nral semi-crfs,” in Proc. of the 36th Int. Conference\non Neural Information Processing Systems (NeurIPS) ,\n2021.\n[13] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-\nZ. A. Huang, S. Dieleman, E. Elsen, J. Engel, and\nD. Eck, “Enabling factorized piano music modeling\nand generation with the maestro dataset,” in Proc. of\nthe 7th International Conference on Learning Repre-\nsentations (ICLR) , 2019.\n[14] R. Kelz, M. Dorfer, F. Korzeniowski, S. Böck, A. Arzt,\nand G. Widmer, “On the potential of simple frame-\nwise approaches to piano transcription,” in Proc. of the\n17th Int. Society for Music Information Retrieval Conf. ,\n2016, pp. 475–481.\n[15] J. W. Kim and J. P. Bello, “Adversarial learning for im-\nproved onsets and frames music transcription,” in Proc.\nof the 20th Int. Society for Music Information Retrieval\nConf. , 2019, pp. 670–677.\n[16] C. Kyunghyun, van Merrienboer Bart, G. Caglar,\nB. Dzmitry, B. Fethi, S. Holger, and B. Yoshua,\n“Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation,” in Pro-\nceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) , 2014,\npp. 1724–1734.\n[17] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural Computation , vol. 9, pp. 1735–1780,\n1997.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. olosukhin,\n“Attention is all you need,” in Proc. of the 31st Con-\nference on Neural Information Processing Systems\n(NeurIPS) , 2017, pp. 6000–6010.\n[19] W.-T. Lu, J.-C. Wang, M. Won, K. Choi, and X. Song,\n“Spectnt: A time-frequency transformer for music au-\ndio,” in Proc. of the 22nd Int. Society for Music Infor-\nmation Retrieval Conf. , 2021, pp. 396–403.\n[20] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Ben-\ngio, “Generative adversarial nets,” in Advances in Neu-\nral Information Processing Systems , vol. 27, 2014.\n[21] Y .-Y . Yang, M. Hira, Z. Ni, A. Chourdia, A. Asta-\nfurov, C. Chen, C.-F. Yeh, C. Puhrsch, D. Pol-\nlack, D. Genzel, D. Greenberg, E. Z. Yang, J. Lian,\nJ. Mahadeokar, J. Hwang, J. Chen, P. Goldsborough,\nP. Roy, S. Narenthiran, S. Watanabe, S. Chintala,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n221V . Quenneville-Bélair, and Y . Shi, “Torchaudio: Build-\ning blocks for audio and speech processing,” arXiv\npreprint arXiv:2110.15018 , 2021.\n[22] Y .-H. Chen, W.-Y . Hsiao, T.-K. Hsieh, J.-S. R. Jang,\nand Y .-H. Yang, “Towards automatic transcription of\npolyphonic electric guitar music: A new dataset and a\nmulti-loss transformer model,” in Proc. of the 47th Int.\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2022, pp. 786–790.\n[23] V . Emiya, R. Badeau, and B. David, “Multipitch esti-\nmation of piano sounds using a new probabilistic spec-\ntral smoothness principle,” IEEE ACM Transactions on\nAudio Speech and Language Processing , vol. 18, no. 6,\npp. 1643–1654, 2009.\n[24] D. P. Kingm and J. L. Ba, “Adam: A method for\nstochastic optimization,” in Proceedings of the 3rd In-\nternational Conference on Learning Representations\n(ICLR) , 2015.\n[25] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nietro, D. Liang, and D. Ellis, “mir_eval: A trans-\nparent implementation of common mir metrics,” in\nProc. of the 15th Int. Society for Music Information Re-\ntrieval Conf. , 2014, pp. 367–372.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n222"
    },
    {
        "title": "Weakly Supervised Multi-Pitch Estimation Using Cross-Version Alignment.",
        "author": [
            "Michael Krause 0002",
            "Sebastian Strahl",
            "Meinard Müller"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265279",
        "url": "https://doi.org/10.5281/zenodo.10265279",
        "abstract": "Multi-pitch estimation (MPE), the task of detecting active pitches within a polyphonic music recording, has garnered significant research interest in recent years. Most state-of-the-art approaches for MPE are based on deep networks trained using pitch annotations as targets. The success of current methods is therefore limited by the difficulty of obtaining large amounts of accurate annotations.\nIn this paper, we propose a novel technique for learning MPE without any pitch annotations at all. Our approach exploits multiple recorded versions of a musical piece as surrogate targets. Given one version of a piece as input, we train a network to minimize the distance between its output and time-frequency representations of other versions of that piece. \nSince all versions are based on the same musical score, we hypothesize that the learned output corresponds to pitch estimates. To further ensure that this hypothesis holds, we incorporate domain knowledge about overtones and noise levels into the network.\nOverall, our method replaces strong pitch annotations with weaker and easier-to-obtain cross-version targets.\nIn our experiments, we show that our proposed approach yields viable multi-pitch estimates and outperforms two baselines.",
        "zenodo_id": 10265279,
        "dblp_key": "conf/ismir/0002SM23",
        "keywords": [
            "Multi-pitch estimation",
            "detecting active pitches",
            "polyphonic music recording",
            "deep networks",
            "pitch annotations",
            "learning MPE",
            "multiple recorded versions",
            "surrogate targets",
            "time-frequency representations",
            "domain knowledge"
        ],
        "ee": "https://zenodo.org/records/10265279/files/000033.pdf",
        "content": "WEAKLY SUPERVISED MULTI-PITCH ESTIMATION\nUSING CROSS-VERSION ALIGNMENT\nMichael Krause, Sebastian Strahl, Meinard Müller\nInternational Audio Laboratories Erlangen, Germany\n{michael.krause,sebastian.strahl,meinard.mueller}@audiolabs-erlangen.de\nABSTRACT\nMulti-pitch estimation (MPE), the task of detecting active\npitches within a polyphonic music recording, has garnered\nsigniﬁcant research interest in recent years. Most state-of-\nthe-art approaches for MPE are based on deep networks\ntrained using pitch annotations as targets. The success of\ncurrent methods is therefore limited by the difﬁculty of ob-\ntaining large amounts of accurate annotations. In this pa-\nper, we propose a novel technique for learning MPE with-\nout any pitch annotations at all. Our approach exploits\nmultiple recorded versions of a musical piece as surrogate\ntargets. Given one version of a piece as input, we train a\nnetwork to minimize the distance between its output and\ntime–frequency representations of other versions of that\npiece. Since all versions are based on the same musical\nscore, we hypothesize that the learned output corresponds\nto pitch estimates. To further ensure that this hypothesis\nholds, we incorporate domain knowledge about overtones\nand noise levels into the network. Overall, our method re-\nplaces strong pitch annotations with weaker and easier-to-\nobtain cross-version targets. In our experiments, we show\nthat our proposed approach yields viable multi-pitch esti-\nmates and outperforms two baselines.\n1. INTRODUCTION\nMusic transcription, i. e., converting music audio record-\nings into score representations, is a fundamental task in\nmusic information retrieval (MIR). As a subtask of tran-\nscription, one may estimate the pitches active at different\npoints in time throughout a recording of polyphonic mu-\nsic, yielding a piano roll representation (without consider-\ning instrumentation, note values, or other score-based in-\nformation). This goal is commonly referred to as multi-\npitch estimation (MPE). Recent years have seen signiﬁ-\ncant advances in MPE systems, mainly due to the use of\ndeep learning models [1–6]. These models are typically\ntrained with large amounts of aligned pitch annotations as\ntargets, see also Figure 1a. Creating such annotations may\ninvolve an enormous effort. In particular, manually anno-\n© M. Krause, S. Strahl, and M. Müller. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: M. Krause, S. Strahl, and M. Müller, “Weakly Supervised\nMulti-Pitch Estimation Using Cross-Version Alignment”, in Proc. of the\n24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.\nFigure 1 : Systems for multi-pitch estimation are typically\ntrained using pitch annotations (a), which are cumbersome\nto create. In this work, we propose to use different ver-\nsions of a piece as surrogate targets (b), which are much\neasier to obtain. In both scenarios, a network input ( I)\nis passed through convolutional layers, producing an out-\nput (O), which is compared to one or several targets ( T)\nusing some loss function ( L).\ntating pitch activity in every frame of an audio recording\nwould be prohibitively time consuming. Many datasets are\nthus annotated using semi-automatic methods like score–\naudio synchronization (e. g., [7]), which introduces anno-\ntation errors. Because of this, systems that can learn pitch\nestimation without large amounts of pitch annotations are\nhighly desirable.\nIn this paper, we propose a novel approach for learn-\ning MPE without pitch annotations. As our key idea, we\nuse different versions (i. e., recorded performances) of a\nmusical piece as surrogate targets. To this end, we lever-\nage cross-version music datasets, which contain several\nversions per piece. Such datasets are especially common\nfor Western classical music, where the same compositions\nare regularly performed by different musicians. Each ver-\nsion exhibits unique timing, artistic expression, and vary-\ning acoustic conditions. All versions, however, are based\non the same musical score and thus contain the same com-\nbinations of pitches. We therefore hypothesize that a deep\nnetwork may produce pitch estimates by learning the com-\nmonalities between different versions of a piece.289In our approach, we train a deep network that takes\na time–frequency representation of one version as input,\nand whose output minimizes a certain distance to time–\nfrequency representations of other versions. This core idea\nis illustrated in Figure 1b. Since versions vary in length\nand the timing of pitch events may be different, we require\na distance measure that temporally aligns the network out-\nput to the representations of other versions. To do so within\na deep learning setting, we use a differentiable variant of\ndynamic time warping called SoftDTW [8]. Apart from\nthe fundamental frequencies of pitches played, all recorded\nversions of a piece contain overtone structures and ambi-\nent noise. To increase the validity of our hypothesis and\nto encourage the network to capture nothing but pitches,\nwe incorporate knowledge about overtones and noise us-\ning additional ﬁxed processing blocks.\nOverall, our proposed approach replaces the need for\nstrong pitch annotations (which are frame-wise, binary,\nand difﬁcult-to-obtain) with weaker cross-version targets\n(not temporally aligned, real-valued, and easy-to-obtain).\nIn summary, we make the following contributions: We\npropose a novel approach for weakly supervised MPE that\ndoes not require pitch annotations, based on the hypothesis\nthat pitch estimation can be learned from multiple versions.\nWe further propose to incorporate extra layers for simulat-\ning overtones and noise levels to ensure that our hypothesis\nholds. Finally, as a proof of concept, we show qualitatively\nand quantitatively that our approach can be used for MPE\nand outperforms two baselines. To aid reproducibility, we\nrelease code and trained models for our approach.1\nThe remainder of this paper is structured as follows: In\nSection 2, we discuss related work on pitch estimation. In\nSection 3, we describe our proposed approach. Section 4\ncovers the experimental setup, while Section 5 contains our\nresults. Section 6 concludes the paper with an overview of\npossible directions for future work.\n2. RELATED WORK\nON MULTI-PITCH ESTIMATION\nThe majority of work on MPE and music transcription in\ngeneral has focused on supervised training schemes, where\na dataset of music recordings with aligned pitch annota-\ntions is given. Most recent papers utilize deep learning\nmodels that are trained with pitch targets using standard\ncross-entropy loss functions [1–5]. Often, these works fo-\ncus on piano music, where annotations can be obtained us-\ning MIDI recording technology built into certain types of\npianos [9]. We refer to [6] for an overview of music tran-\nscription research.\nSome works have explored pitch estimation from data\nwithout aligned pitch annotations. Weiß and Peeters [10]\nproposed to utilize weakly aligned annotations, where\nthere may be temporal deviations between recorded per-\nformance and annotations. This scenario is also explored\nin [11]. However, in both cases, pitch annotations are re-\nquired for the entire training dataset. Gfeller et al. [12] in-\n1https://www.audiolabs-erlangen.de/resources/\nMIR/2023-ISMIR-WeaklySupervisedMPEtroduced a self-supervised approach for pitch estimation,\nwhere a network learns to predict the relative differences\nbetween pitch-shifted, monophonic recordings. Their ap-\nproach requires only a small amount of data with pitch\nannotations, but does not deal with polyphonic scenarios.\nBerg-Kirkpatrick et al. [13] describe a system for MPE\non piano recordings that does not use pitch annotations.\nTheir approach solves an optimization problem, with con-\nstraints motivated by the sound production process in pi-\nanos. In contrast, the method we propose in this paper uti-\nlizes several versions of a musical piece and could be used\nfor recordings with arbitrary instruments.\n3. PROPOSED METHOD\nWe now describe our proposed approach for learning\nMPE using cross-version alignment. Here, we assume\nthat we have multiple corresponding recorded versions\nfor each musical piece in the training set. Let us de-\nnote the set of all corresponding versions for one piece by\nV={V1,V2,...}. Furthermore, given a version V∈ V,\nwe write InputRep (V)for the audio representation of V\nthat our network takes as input.1\nGiven an input I=InputRep(V), we formulate\nMPE as the problem of producing a binary piano roll\n˜M∈ {0,1}B×Nthat matches the pitch annotations\nA∈ {0,1}B×Nfor that input. Here, Bdenotes the num-\nber of pitch bins, while Nis the number of time frames\nin the input. In the supervised case, deep networks for\nMPE produce a real-valued output O∈[0,1]B×Nthat is\noptimized using the binary cross-entropy loss LBCE with\nT=Aas targets (where the loss is averaged over all time–\npitch bins). The ﬁnal pitch predictions ˜Mare obtained from\nOby applying a threshold τ. This threshold is often set\nto a ﬁxed value (e. g., τ= 0.4in [7]) or optimized on a\nvalidation dataset [14]. This supervised approach to MPE,\nwhich crucially relies on the aligned pitch annotations A,\nis illustrated in Figure 1a. In the following, we will refer\nto it with the shorthand Sup.\nOur proposed approach, illustrated in Figure 1b, also\ntakes an input representation I=InputRep(V)for\nsome version V∈ V . As before, our network yields\na real-valued output O∈[0,1]B×N. However, rather\nthan using pitch annotations A, we utilize a surrogate\ntargetT=TargetRep (V′)based on another version\nV′∈ V \\{V}. We choose a time–frequency representa-\ntion TargetRep (V′)∈[0,1]B×N′\nas target that is normal-\nized in the range [0,1]and has the same number of bins\nBasO, but a potentially different number of time frames\nN′, due to the temporal differences between versions.1As\nexplained in the introduction, Tcontains the same combi-\nnations of pitches as I.2Intuitively, if Ois close to the\ntarget representations of all versions V \\{V}, we hypoth-\nesize thatOmust correspond to pitch estimates for I. We\n1Details of InputRep and TargetRep are provided in Section 4.\n2Here, we assume that there are no structural differences between ver-\nsions, i. e., performers do not deviate from the score. Versions performed\nin different keys can be handled through pitch shifting, see Section 4.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n290I\nCNNM\n+Ov +B\nO\n≥ 끫븞\n�MLSoftDTW\nTFigure 2 : Detailed overview of the proposed cross-version alignment ( CVA) method, see also Figure 1b. Before applying\nthe alignment loss ( LSoftDTW ), the intermediate output of the network ( M) is optionally extended using a simple overtone\nmodel (+Ov) and a bias value ( +B) to address background noise. The ﬁnal output Ocan thus arise from different conﬁgura-\ntions (e. g., O=M,O=M+Ov,...). Importantly, the MPE output of the system ( ˜M) is computed based on the intermediate\nrepresentation M, rather than the output O.\nrefer to our proposed approach with the shorthand CVA (for\n“cross-version alignment”).\nNote that we cannot directly apply a loss on time–pitch\nbins here (as in the supervised case), since OandTare\nnot temporally aligned. For this reason, we use the dif-\nferentiable alignment loss LSoftDTW in our approach, see\nSection 3.1. Furthermore, our hypothesis may fail to ap-\nply, since recorded versions of a piece contain overtone\nstructures and background noise in addition to the pitches\nplayed. We thus extend our approach to account for these\nproperties of music recordings in Section 3.2.\n3.1 Differentiable Alignment\nIn order to perform temporal alignment between Oand a\ntarget representation Tin a differentiable fashion, we use\nthe SoftDTW loss [8]. SoftDTW is a differentiable approx-\nimation of the classical dynamic time warping algorithm\nthat is often used to align music sequences [15]. SoftDTW\nhas originally been introduced for one-dimensional time\nseries but has also been adopted for computer vision tasks\nlike action recognition in video recordings [16,17]. Within\nMIR, SoftDTW has previously been used in the context of\nmusic synchronization [18] and MPE [11]. In [11], the au-\nthors showed that SoftDTW can be used to replace strongly\naligned (i. e., frame-wise) pitch annotations with weakly\naligned pitches without a major impact on MPE perfor-\nmance. Nevertheless, their approach requires pitch anno-\ntations for training.\nIn our case, we crucially rely on the ability of SoftDTW\nto align real-valued sequences such as time–frequency rep-\nresentations of audio. In contrast, a commonly used alter-\nnative loss function called connectionist temporal classi-\nﬁcation (CTC) can only handle discrete target sequences.\nTo compute LSoftDTW , one needs to choose a local cost\nfunction (for comparing individual frames of the time–\nfrequency representations) and set a temperature hyper-\nparameter called γ(which determines the approximation\nquality of SoftDTW). Here, we use the cosine distance for\ncomparing frames, which exhibited high training stability\nin our experiments. We further set γ= 0.1, which corre-\nsponds to a good approximation of DTW.As a drawback, the time and space complexity of Soft-\nDTW is quadratic in the lengths of the input sequences.\nWe thus train on short input excerpts (see Section 4).\n3.2 Overtone and Noise Model\nAside from differentiable alignments, our proposed ap-\nproach utilizes ﬁxed processing layers that simulate over-\ntone structures and background noise. In this way, our\nmethod follows the analysis-by-synthesis paradigm [19],\nwhere one estimates parameters from an audio recording\n(pitches, in our case) by re-synthesizing the input. Choi\nand Cho [20] utilized this idea for unsupervised drum tran-\nscription. Their network consists of a transcription stage\nand a ﬁxed sample-based drum synthesizer. The transcrip-\ntion network is trained by minimizing a reconstruction loss\non the synthesizer output. In recent years, such systems\nhave become more popular due to the release of the dif-\nferentiable digital signal processing (DDSP) library [21],\nwhich has been used, e. g., in the context of unsupervised\nmonophonic pitch estimation [22]. In contrast to these\nworks, our proposed approach utilizes cross-version data.\nA full overview of our CVA approach is given in Fig-\nure 2. We explicitly add overtones (denoted by +Ov) and\nbackground noise ( +B) to an intermediate output Mof our\nnetwork via dedicated layers. In this way, the network may\nlearn a sparser and more piano roll-like representation M,\nsince overtones and noise are added afterwards. Crucially,\nthe ﬁnal MPE results ˜Mare obtained from M, before over-\ntones and noise are applied. The output O, used for align-\nment with the cross-version targets, depends on the model\nconﬁguration used. For example, O=M+Ov+B if all mod-\nules are used, O=M+Ov if only overtones are added, etc.\nIn the basic system without extensions, O=M.\nHere, we opt for very simple overtone and noise models\nthat serve to indicate the potential of our core idea. We esti-\nmate the relative amplitudes of different harmonics from a\nsmall internal dataset of single-note piano recordings. The\nresulting estimates, used for our overtone model, are illus-\ntrated in Figure 3. We keep these values ﬁxed for all sub-\nsequent experiments. To apply this ﬁxed overtone model\nwithin our network in a differentiable fashion, we sum up\npitch-shifted versions of M. For each harmonic h, we shift\nMalong the vertical axis by a number of semitones corre-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n29112345678910\nHarmonic10−1100AmplitudeFigure 3 : Amplitudes for the overtone model ( +Ov) em-\nployed in our proposed approach.\nsponding to h(e. g., 12 semitones for h= 2). We then\nweight the shifted representation with the amplitude esti-\nmated for h(see Figure 3). The ﬁnal output is obtained by\nsumming the resulting representations for all h.3To ad-\ndress the overall noise level in the target T, we add a ﬁxed\nbias term of δ= 0.2after applying the overtone model.\nAs a result of this additional processing, we may obtain\noutputs larger than 1. We therefore clip all values outside\nthe interval [0,1](corresponding to the value range of the\ntarget representations T) to get the ﬁnal output O.\n4. EXPERIMENTAL SETUP\n4.1 Model, Representations, and Training\nIn this work, we focus on demonstrating the potential of\nour cross-version approach compared to traditional, fully\nsupervised training for MPE. Thus, we do not propose\ncomplicated network architectures that require extensive\ntuning. Instead, we use a relatively small convolutional\nneural network for extracting the representation Mfrom\nI. For InputRep and TargetRep, we use time–frequency\nrepresentations based on the constant-Q transform (CQT),\nwhich provides a frequency axis corresponding to semi-\ntones. Note that we cannot train on entire (several min-\nutes long) recordings in a single step. Instead, our training\nbatches contain short input excerpts and we use state-of-\nthe-art music synchronization techniques [23] to ﬁnd the\ncorresponding sections in other versions.\nConcretely, we use the network architecture, input rep-\nresentation, and training setup from [10] (we refer to their\npaper for details). Their network consists of ﬁve convolu-\ntional layers with musically motivated kernel shapes and\nroughly 50 000 learnable parameters. The network takes\na magnitude harmonic CQT (HCQT [24]) of an audio ex-\ncerpt as InputRep, containing N=500 frames computed\nwith a hop size of 512 from waveforms at 22 050 Hz (i. e.,\nan excerpt of 11.6 seconds length). The network produces\noutputsMof the same length, with a pitch axis containing\nB= 72 bins (corresponding to the semitones from C1 to\nB6). The ﬁnal layer of the network contains a sigmoid acti-\nvation, such that all values in Mare restricted to the interval\n3Equivalently, the overtone model can be understood as a frame-wise\nconvolution in pitch direction, with a kernel based on the amplitudes in\nFigure 3.[0,1]. For TargetRep, we use magnitude CQTs where the\ncenter frequencies of different bins correspond to the same\nB= 72 semitones. Column-wise max-normalization is\napplied onT, such that the target values are also in [0,1].\nWe train our network by minimizing the SoftDTW\nloss over all training excerpts until the validation loss has\nstopped improving for 12 epochs. In each training step,\nwe compute the loss on a batch of 16 inputs. Each input\nexcerpt is based on some version V∈ V and aligned to\nthe corresponding excerpt in one randomly selected target\nversionV′∈ V \\ {V}. We use the Adam optimizer with\na learning rate of 0.001, which is reduced whenever the\nvalidation loss has not improved for three epochs. Finally,\nwe employ an efﬁcient CUDA implementation of the Soft-\nDTW recursions by Maghumi et al. [25].4\n4.2 Dataset and Split\nTo train our cross-version approach, we require a dataset\ncontaining multiple versions per piece. For testing, we ad-\nditionally require aligned pitch annotations for the record-\nings. We opt for using the Schubert Winterreise Dataset\n(SWD, [26]) for training, which contains nine versions of\nthe 24 songs in the cycle “Winterreise” composed by Franz\nSchubert (in total, roughly 11 h of audio). Each song con-\nstitutes one unique musical piece. The recordings consist\nof a tenor or baritone singer accompanied by piano. There\nare no structural differences between versions. Thus, all\nrecordings for a piece contain the same combinations of\npitches up to transposition (a global pitch shift), since some\nmusicians chose to perform some songs in different keys.\nWhen training our CVA approach, we ensure that input and\ntarget version are in the same key by appropriately shifting\nthe target CQT representation according to the key annota-\ntions given in the dataset.\nWe train and evaluate our model using a challenging\nsplit where the train and test sets contain both different\nversions and different songs. We choose songs 1–13 for\ntraining, 14–16 for validation, and 17–24 for testing. Fur-\nthermore, versions HU33 and SC06 are used for testing,\nwhile the remaining seven versions are used for training\nand validation. Such a split is also referred to as a “neither\nsplit”, since neither the same versions nor songs appear\nduring training and testing [27]. This split avoids over-\noptimistic evaluation due to confounders such as the “al-\nbum effect” [28].\n4.3 Baselines\nAside from the supervised baseline Sup, which is trained\nusing strong pitch annotations, we compare our proposed\nCVA approach to two additional baselines. With these,\nwe aim to evaluate our hypothesis that cross-version tar-\ngets are useful for learning MPE-like representations (see\n4Note that, within one batch, the targets Tmay have different lengths.\nIn order to beneﬁt from parallelization across the batch dimension, we\ntherefore rescale the targets Tto a common length N′= 500 (a trick\nreferred to as W4 in [11]). This did not affect results negatively in early\nexperiments. Note that rescaling is not equivalent to temporally aligning\ninputs and targets.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n292Scenario CS APτ= 0.4 τ=τ∗\nF Acc. F Acc.\nCQT 0.585 0.410 0.443 0.287 0.450 0.292\nAE 0.588 0.500 0.336 0.203 0.511 0.345\nCVA 0.632 0.589 0.585 0.416 0.592 0.423\nCVA+Ov 0.664 0.639 0.553 0.384 0.623 0.455\nCVA+B 0.633 0.563 0.560 0.392 0.592 0.424\nCVA+Ov+B 0.682 0.646 0.625 0.458 0.627 0.460\nSup 0.748 0.753 0.700 0.543 0.703 0.546\nTable 1 : Results for multi-pitch estimation on the Schubert\nWinterreise Dataset for the baselines and different conﬁg-\nurations of our proposed approach.\nSection 3). For the CQT baseline, we take the target rep-\nresentations of our test recordings (which are normalized\nto have values in the range [0,1]) and obtain multi-pitch\nestimates by directly thresholding these magnitude CQTs\nwithτ. This learning-free baseline was previously pro-\nposed in [10] and, like CVA, does not require pitch anno-\ntations. Furthermore, we consider a second baseline that\nis very similar to CVA but does not utilize cross-version\ntargets. Therefore, for each input excerpt, we choose the\nsame version V∈ V for bothIandT. Thus, the network\nneeds to effectively recreate its input, similar to an auto-\nencoder. We refer to this baseline with the shorthand AE.\nIntuitively, we expect CQT andAEto yield similar results.\nHowever,AEallows us to verify that any improvements\nobserved for CVA stem from the cross-version targets and\nnot from the model architecture or training setup. Note that\nSup andAEuse the same network architecture as CVA.\n4.4 Evaluation Metrics\nWe evaluate the multi-pitch estimates of our proposed ap-\nproach and all baselines using standard metrics on the test\nset. For this, we utilize the strongly aligned pitch annota-\ntions provided in the test data. As metrics, we use the co-\nsine similarity (CS) between predictions and annotations,\naveraged over all frames and ﬁles in the test set. Further-\nmore, we compare the average precision (AP, computed as\nthe area under the precision-recall curve), F-measure (F),\nand the accuracy (Acc.) metric introduced in [29]. For\nthese measures, we average over all pitches. Note that F\nand Acc. are evaluated on ˜Mand thus depend on the thresh-\noldτ, while CS and AP are threshold-free evaluation met-\nrics that directly compare MandA.\n5. RESULTS\nThe main results of our study are summarized in Table 1.\nRows correspond to different baselines or conﬁgurations of\nour proposed approach. We write +Ov when adding over-\ntones and+Bwhen including the bias term to account for\nbackground noise. Our model including all proposed mod-\nules is thus referred to as CVA+Ov+B . Columns contain\nthe evaluation metrics. For the thresholding-based metrics\nF and Acc., we provide both results based on a ﬁxed thresh-\nold (τ= 0.4) and a threshold chosen to optimize F on the\nvalidation set ( τ=τ∗).0.10.20.30.40.50.60.70.80.91.0\nτ0.30.40.50.60.70.8F-measureSup\nCVA+Ov+B\nCQT\nFigure 4 : F-measures on the test set for different MPE ap-\nproaches, depending on the choice of threshold τ. Markers\nshow the optimal threshold τ∗as determined on the vali-\ndation set.\nOur proposed approach CVA outperforms the two base-\nlinesCQT andAEacross all metrics, demonstrating the ef-\nfectiveness of using different versions of a piece to capture\npitches inM. For example, CS =0.632forCVA compared\nto CS= 0.588 forAE, and AP = 0.589 forCVA com-\npared to AP =0.410forCQT. Furthermore, our proposed\novertone and noise models are effective. By adding over-\ntones (CVA+Ov ), we can further increase AP from 0.589\nto0.639. Adding a ﬁxed bias term ( CVA+B ) does not\nyield improvements by itself. However, by combining both\nmodules (CVA+Ov+B ), we achieve the best results for our\napproach, further increasing AP to 0.646and CS to 0.682.\nDespite these encouraging results, there remains a gap\nbetween the best results for our proposed approach and\nthose for the supervised baseline Sup. We emphasize\nagain that—unlike CVA—Sup requires strong pitch anno-\ntations for training.\n5.1 Impact of Threshold τ\nWhen using the standard value of τ= 0.4for threshold-\ningM, ourCVA approach also outperforms both baselines\nin terms of F-measure and accuracy (e. g., F = 0.553for\nCVA+Ov compared to 0.443forCQT).\nA ﬁxed threshold may be sub-optimal, especially for\nmethods that are not explicitly trained for MPE. When\nevaluating using the optimized threshold τ∗, we observe\nincreased results for all approaches. CVA and its extensions\ncontinue to outperform the two baselines. The F-measure\nforCVA+Ov , for example, further increases to F =0.623.\nFor that model, the optimal threshold as determined on the\nvalidation set is τ∗= 0.28. In this case, our method re-\nquires at least a few pitch annotations to determine τ∗and\nis no longer relying solely on the cross-version targets.\nFigure 4 further demonstrates the impact of the param-\neterτ. F-measures (vertical axis) are shown for different\nMPE approaches (colored lines), depending on the choice\nofτ(horizontal axis). Markers indicate τ∗. As shown in\nthis ﬁgure, a poor choice of τmay strongly affect test re-\nsults. Moreover, τ∗as found using the validation set may\nnot always give the highest scores on the test set. For in-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n293M �M\nCVA\nCVA+Ov\nCVA+B\nCVA+Ov+B\nSupI A\nCQT\nAE\nFigure 5 : Qualitative results on a test excerpt from SWD.\nstance, a choice of τ= 0.5would yield an even higher\nF-measure of 0.634forCVA+Ov+B .\n5.2 Training Stability\nThe metrics reported in Table 1 are computed from a single\ntraining run per method. When repeating the experiment,\nresults may deviate slightly due to random network ini-\ntialization, dataset shufﬂing, or dropout. For CVA+Ov+B ,\nwe repeat the experiment ﬁve times and ﬁnd low standard\ndeviation σin results ( σ(CS) = 0.004,σ(AP) = 0.009,\nσ(F) = 0.008, andσ(Acc.) = 0.009forτ=τ∗).\n5.3 Qualitative Results\nTo complement the quantitative evaluation, we also pro-\nvide qualitative results on an exemplary excerpt in Fig-\nure 5. The ﬁrst row shows an input excerpt ( I) and corre-\nsponding pitch annotations ( A), while the remaining rowsshow multi-pitch estimates before ( M) and after threshold-\ning (˜M, computed using τ=τ∗).\nForCQT andAE, the resulting Mcorrespond to the input\nrepresentation and thus lead to poor multi-pitch estimates.\nWhen training our approach without overtones or noise\nmodel (CVA), the output representation Memphasizes the\nfundamental frequencies of many of the actual pitches be-\ning played. However, Malso contains a lot of energy from\novertone structures and background noise. As a conse-\nquence, the resulting ˜Mcontains many spurious pitch pre-\ndictions, especially for higher pitches.\nWith+Ov and+B, we see a reduced impact of over-\ntones or background noise in M, respectively. In both cases,\nmany erroneous predictions remain after thresholding. By\nincluding both modules ( CVA+Ov+B ), we obtain a promis-\ning representation that bears visual resemblance to the re-\nsults forSup. We also observe fewer spurious activations\nin˜Mcompared to the basic CVA. Overall, the proposed ex-\ntensions are effective in encouraging the model to produce\nMPE predictions in M.\n6. CONCLUSION\nIn this paper, we presented a novel approach for MPE that\ndoes not require pitch annotations for training. Instead,\nour method utilizes multiple versions of the same musical\npiece as surrogate targets. We train a network that takes a\ntime–frequency representation of one version as input and\nminimizes an alignment-based distance to time–frequency\nrepresentations of other versions. We hypothesized that\nthis would result in outputs corresponding to pitch esti-\nmates. We further incorporate knowledge about overtones\nand noise levels into our system to support this hypothesis\nand improve results. In our experiments, we showed that\nour approach outperforms two baselines and that our pro-\nposed extensions to the model are effective. Overall, our\nwork demonstrates the use of weak cross-version targets to\nreplace strong pitch annotations.\nThis paper serves as a proof of concept for our core idea,\nwhich could be extended in future work. First, better re-\nsults may be obtained by utilizing larger model architec-\ntures and bigger training datasets than in the present study.\nHere, we also abstained from excessive model and hyper-\nparameter tweaking. In the future, larger and more exten-\nsively tuned models may close the gap between fully su-\npervised approaches and the proposed cross-version train-\ning. Second, one may extend our approach to align one in-\nput excerpt to multiple versions simultaneously within the\nsame training step (rather than choosing one target version\nat a time). This may further regularize the model output.\nFinally, future work may explore more elaborate synthe-\nsis models that could replace the simplistic overtone and\nnoise models used here. For example, one may incorpo-\nrate knowledge about the sound production processes of\ndifferent instruments into the network [13]. In this con-\ntext, results might also be improved by estimating the syn-\nthesis parameters (e. g., amplitudes of the overtone model)\nfrom the input recording, rather than using ﬁxed process-\ning steps.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n294Acknowledgments: This work was supported by the\nGerman Research Foundation (DFG MU 2686/7-2,\nMU 2686/11-2). The authors are with the Interna-\ntional Audio Laboratories Erlangen, a joint institution\nof the Friedrich-Alexander-Universität Erlangen-Nürnberg\n(FAU) and Fraunhofer Institute for Integrated Circuits IIS.\nThe authors gratefully acknowledge the compute resources\nand support provided by the Erlangen Regional Computing\nCenter (RRZE).\n7. REFERENCES\n[1] R. Kelz, M. Dorfer, F. Korzeniowski, S. Böck, A. Arzt,\nand G. Widmer, “On the potential of simple framewise\napproaches to piano transcription,” in Proceedings of\nthe International Society for Music Information Re-\ntrieval Conference (ISMIR) , New York City, New York,\nUSA, 2016, pp. 475–481.\n[2] S. Sigtia, E. Benetos, and S. Dixon, “An end-to-end\nneural network for polyphonic piano music transcrip-\ntion,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , vol. 24, no. 5, pp. 927–939,\n2016.\n[3] C. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Si-\nmon, C. Raffel, J. H. Engel, S. Oore, and D. Eck, “On-\nsets and frames: Dual-objective piano transcription,”\ninProceedings of the International Society for Mu-\nsic Information Retrieval Conference, (ISMIR) , Paris,\nFrance, 2018, pp. 50–57.\n[4] K. W. Cheuk, Y . Luo, E. Benetos, and D. Herremans,\n“Revisiting the onsets and frames model with additive\nattention,” in Proceedings of the International Joint\nConference on Neural Networks (IJCNN) , Shenzhen,\nChina, 2021.\n[5] Q. Kong, B. Li, X. Song, Y . Wan, and Y . Wang, “High-\nresolution piano transcription with pedals by regress-\ning onset and offset times,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing , vol. 29,\npp. 3707–3717, 2021.\n[6] E. Benetos, S. Dixon, Z. Duan, and S. Ewert, “Auto-\nmatic music transcription: An overview,” IEEE Signal\nProcessing Magazine , vol. 36, no. 1, pp. 20–30, 2019.\n[7] J. Thickstun, Z. Harchaoui, and S. M. Kakade, “Learn-\ning features of music from scratch,” in Proceedings of\nthe International Conference on Learning Representa-\ntions (ICLR) , Toulon, France, 2017.\n[8] M. Cuturi and M. Blondel, “Soft-DTW: a differen-\ntiable loss function for time-series,” in Proceedings\nof the International Conference on Machine Learning\n(ICML) , Sydney, NSW, Australia, 2017, pp. 894–903.\n[9] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C. A.\nHuang, S. Dieleman, E. Elsen, J. H. Engel, and D. Eck,\n“Enabling factorized piano music modeling and gener-\nation with the MAESTRO dataset,” in Proceedings ofthe International Conference on Learning Representa-\ntions (ICLR) , New Orleans, Louisiana, USA, 2019.\n[10] C. Weiß and G. Peeters, “Learning multi-pitch esti-\nmation from weakly aligned score-audio pairs using\na multi-label CTC loss,” in Proceedings of the IEEE\nWorkshop on Applications of Signal Processing to Au-\ndio and Acoustics (WASPAA) , New Paltz, USA, 2021,\npp. 121–125.\n[11] M. Krause, C. Weiß, and M. Müller, “Soft dynamic\ntime warping for multi-pitch estimation and beyond,”\ninProceedings of the IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\nRhodes Island, Greece, 2023.\n[12] B. Gfeller, C. Frank, D. Roblek, M. Shariﬁ,\nM. Tagliasacchi, and M. Velimirovic, “SPICE: Self-\nsupervised pitch estimation,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing , vol. 28,\npp. 1118–1128, 2020.\n[13] T. Berg-Kirkpatrick, J. Andreas, and D. Klein, “Unsu-\npervised transcription of piano music,” in Proceedings\nof Advances in Neural Information Processing Systems\n(NIPS) , Montréal, Canada, 2014, pp. 1538–1546.\n[14] Y . Wu, B. Chen, and L. Su, “Polyphonic music tran-\nscription with semantic segmentation,” in Proceedings\nof the IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP) , Brighton,\nUK, 2019, pp. 166–170.\n[15] M. Müller, Fundamentals of Music Processing – Us-\ning Python and Jupyter Notebooks , 2nd ed. Springer\nVerlag, 2021.\n[16] I. Hadji, K. G. Derpanis, and A. D. Jepson, “Repre-\nsentation learning via global temporal alignment and\ncycle-consistency,” in IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , Virtual,\n2021, pp. 11 068–11 077.\n[17] C. Chang, D. Huang, Y . Sui, L. Fei-Fei, and J. C.\nNiebles, “D3TW: Discriminative differentiable dy-\nnamic time warping for weakly supervised action\nalignment and segmentation,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , Long Beach, CA, USA,\n2019, pp. 3546–3555.\n[18] R. Agrawal, D. Wolff, and S. Dixon, “A convolutional-\nattentional neural framework for structure-aware\nperformance-score synchronization,” IEEE Signal Pro-\ncessing Letters , vol. 29, pp. 344–348, 2021.\n[19] N. Cleju, M. G. Jafari, and M. D. Plumbley,\n“Analysis-based sparse reconstruction with synthesis-\nbased solvers,” in IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\nKyoto, Japan, 2012, pp. 5401–5404.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n295[20] K. Choi and K. Cho, “Deep unsupervised drum tran-\nscription,” in Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nDelft, The Netherlands, 2019, pp. 183–191.\n[21] J. Engel, L. Hantrakul, C. Gu, and A. Roberts, “DDSP:\nDifferentiable digital signal processing,” in Proceed-\nings of the International Conference on Learning Rep-\nresentations (ICLR) , Virtual, 2020.\n[22] J. Engel, R. Swavely, L. H. Hantrakul, A. Roberts,\nand C. Hawthorne, “Self-supervised pitch detection\nby inverse audio synthesis,” in International Confer-\nence on Machine Learning (ICML), Workshop on Self-\nSupervision in Audio and Speech , Vienna, Austria,\n2020.\n[23] M. Müller, Y . Özer, M. Krause, T. Prätzlich, and\nJ. Driedger, “Sync Toolbox: A Python package for ef-\nﬁcient, robust, and accurate music synchronization,”\nJournal of Open Source Software (JOSS) , vol. 6,\nno. 64, pp. 3434:1–4, 2021.\n[24] R. M. Bittner, B. McFee, J. Salamon, P. Li, and J. P.\nBello, “Deep salience representations for F0 tracking\nin polyphonic music,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Suzhou, China, 2017, pp. 63–70.\n[25] M. Maghoumi, E. M. Taranta, and J. LaViola, “Deep-\nNAG: Deep non-adversarial gesture generation,” in\nProceedings of the International Conference on Intel-\nligent User Interfaces (IUI) , College Station, Texas,\nUSA, 2021, pp. 213–223.\n[26] C. Weiß, F. Zalkow, V . Ariﬁ-Müller, M. Müller, H. V .\nKoops, A. V olk, and H. Grohganz, “Schubert Winter-\nreise dataset: A multimodal scenario for music anal-\nysis,” ACM Journal on Computing and Cultural Her-\nitage (JOCCH) , vol. 14, no. 2, pp. 25:1–18, 2021.\n[27] C. Weiß, H. Schreiber, and M. Müller, “Local key esti-\nmation in music recordings: A case study across songs,\nversions, and annotators,” IEEE/ACM Transactions on\nAudio, Speech, and Language Processing , vol. 28, pp.\n2919–2932, 2020.\n[28] A. Flexer, “A closer look on artist ﬁlters for musical\ngenre classiﬁcation,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Vienna, Austria, 2007, pp. 341–344.\n[29] G. E. Poliner and D. P. Ellis, “A discriminative model\nfor polyphonic piano transcription,” EURASIP Journal\non Advances in Signal Processing , vol. 2007, no. 1,\n2007.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n296"
    },
    {
        "title": "A Cross-Version Approach to Audio Representation Learning for Orchestral Music.",
        "author": [
            "Michael Krause 0002",
            "Christof Weiß",
            "Meinard Müller"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265419",
        "url": "https://doi.org/10.5281/zenodo.10265419",
        "abstract": "Deep learning systems have become popular for tackling a variety of music information retrieval tasks. However, these systems often require large amounts of labeled data for supervised training, which can be very costly to obtain. To alleviate this problem, recent papers on learning music audio representations employ alternative training strategies that utilize unannotated data.\nIn this paper, we introduce a novel cross-version approach to audio representation learning that can be used with music datasets containing several versions (performances) of a musical work. Our method exploits the correspondences that exist between two versions of the same musical section.\nWe evaluate our proposed cross-version approach qualitatively and quantitatively on complex orchestral music recordings and show that it can better capture aspects of instrumentation compared to techniques that do not use cross-version information.",
        "zenodo_id": 10265419,
        "dblp_key": "conf/ismir/0002WM23",
        "keywords": [
            "deep learning",
            "music information retrieval",
            "supervised training",
            "unannotated data",
            "cross-version approach",
            "audio representation learning",
            "correspondences",
            "instrumentation",
            "complex orchestral music recordings",
            "qualitative and quantitative evaluation"
        ],
        "ee": "https://zenodo.org/records/10265419/files/000099.pdf",
        "content": "A CROSS-VERSION APPROACH TO\nAUDIO REPRESENTATION LEARNING FOR ORCHESTRAL MUSIC\nMichael Krause1Christof Weiß2Meinard Müller1\n1International Audio Laboratories Erlangen, Germany\n2University of Würzburg, Germany\n{michael.krause,meinard.mueller}@audiolabs-erlangen.de, christof.weiss@uni-wuerzburg.de\nABSTRACT\nDeep learning systems have become popular for tackling\na variety of music information retrieval tasks. However,\nthese systems often require large amounts of labeled data\nfor supervised training, which can be very costly to ob-\ntain. To alleviate this problem, recent papers on learn-\ning music audio representations employ alternative train-\ning strategies that utilize unannotated data. In this paper,\nwe introduce a novel cross-version approach to audio rep-\nresentation learning that can be used with music datasets\ncontaining several versions (performances) of a musical\nwork. Our method exploits the correspondences that ex-\nist between two versions of the same musical section. We\nevaluate our proposed cross-version approach qualitatively\nand quantitatively on complex orchestral music recordings\nand show that it can better capture aspects of instrumenta-\ntion compared to techniques that do not use cross-version\ninformation.\n1. INTRODUCTION\nDeep learning (DL) has become a common tool for ap-\nproaching diverse tasks in music information retrieval\n(MIR). These approaches usually follow a supervised\nlearning scheme, where a neural network is trained on the\nannotations of some dataset. For many MIR tasks, how-\never, such annotations are costly to obtain. Recent work\nhas investigated alternatives that require little or no anno-\ntations and enable training on large, unannotated datasets.\nFor certain music genres, there are datasets that contain\nseveral versions (i. e., recorded performances) of a musical\nwork. For example, the same classical symphony or con-\ncerto can be performed by different orchestras, and sev-\neral commercial recordings are often available. On such\ndatasets, automatic music synchronization techniques can\nbe used to ﬁnd alignments between different versions of a\nwork, requiring minimal annotation effort [1, 2].\nIn this paper, we introduce a conceptually novel ap-\nproach to audio representation learning that exploits cross-\n© M. Krause, C. Weiß, and M. Müller. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: M. Krause, C. Weiß, and M. Müller, “A Cross-Version\nApproach to Audio Representation Learning for Orchestral Music”, in\nProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.\nFigure 1 : Visualization of our cross-version approach to\nrepresentation learning for orchestral music. An anchor\n(blue) excerpt is selected from a music recording. The pos-\nitive (green) and negative (red) excerpts are chosen from a\ndifferent version of the same musical piece. For this, an\nalignment between versions is needed (gray arrows).\nversion datasets, thus requiring only alignments between\nversions and no further human annotations. Our approach\naims at learning embeddings of audio excerpts such that\nmusically corresponding excerpts in different versions are\nmapped to close points in the embedding space (Figure 1).\nThere are several musical aspects that stay roughly con-\nstant across most versions, e. g., pitches, harmonies or\nrhythm. For orchestral music, aspects of instrumentation\n(i. e., active instruments or instrument families) are an-\nother such property. Instrumentation represents a challeng-\ning MIR scenario given the complexity of instrument tax-\nonomies and the difﬁculty of annotating instrument activ-\nity in orchestral music. In our experiments on a dataset of\ncomplex orchestral music, we show qualitatively and quan-\ntitatively that—by utilizing the correspondences between\ndifferent versions of a musical section—our proposed rep-\nresentation learning technique is better at capturing aspects\nof instrumentation and instrument texture compared to ap-\nproaches that do not exploit cross-version information.832The remainder of the paper is structured as follows:\nSection 2 covers related work on music audio representa-\ntion learning, cross-version analysis, and instrumentation\nin orchestral music. In Section 3, we introduce our pro-\nposed approach. In Section 4, we describe our experimen-\ntal setup, including datasets, our model architecture, and\nbaselines. Section 5 contains qualitative and quantitative\nresults and Section 6 concludes the paper with a discus-\nsion of possible future work.\n2. RELATED WORK\nSeveral recent contributions have explored so-called self-\nsupervised strategies for learning representations from\nunannotated music recordings. Often, in these studies, ex-\ncerpts from a music recording that are in close proximity\nare considered as positive pairs (i. e., should be mapped to\nsimilar representations) whereas excerpts that are further\napart (or from other recordings) are negative pairs (i. e.,\nshould be mapped to dissimilar representations). This idea\nis also illustrated in Figure 2. McCallum [3] originally\nconsidered this with the aim of learning features for music\nstructure analysis. Wang et al. [4] had a similar use case but\nused a supervised learning approach. Several authors em-\nployed such a strategy for learning more general purpose\nrepresentations [5–10], often applying additional augmen-\ntations. Apart from using temporal proximity, other pa-\npers on music representation learning exploit audio-visual\nor audio-text correspondences [11, 12], use classical fea-\ntures as training targets [13], exploit metadata [14], or in-\nvestigate music generation models [15].\nThe approach proposed in this paper is conceptually dif-\nferent since we utilize cross-version datasets, rather than\nrelying on temporal proximity alone. Such datasets contain\nseveral recorded versions of a musical work, which may\nvary in aspects related to musical interpretation, recording\nconditions, and timbral characteristics of the instruments\nused. These datasets have been exploited for expressive\nperformance rendering [16] or improved harmonic analy-\nsis [17]. Cross-version datasets also allow for investigat-\ning model biases and overﬁtting effects in MIR models\nthrough different dataset splits [18]. To our knowledge,\nthe only other work utilizing cross-version information for\nembedding learning is by Zalkow et al. [19], whose aim\nwas to compress chromagram excerpts for efﬁcient music\nretrieval. In contrast, we propose to learn representations\nbased on spectrogram-like input features and investigate\nthem for capturing instrument texture.\nIn the wider machine learning literature, representations\nare often learned by masking a part of an input and predict-\ning the masked content [20, 21]. Other strategies utilize\nmulti-modal datasets, e. g., containing text–image [22] or\naudio–text pairs [23].\nOrchestral music has been explored in the context of\nsource separation [24] or melody extraction [25]. The\nauthors in [26] considered instrument family recognition\nfor classical, monotimbral recordings using a supervised\nlearning approach. Other recent papers on instrument ac-\ntivity detection in music recordings [27–29] have also con-\nFigure 2 : When forming triplets of audio excerpts, the an-\nchor and positive/negative excerpts are chosen according\nto a maximum/minimum distance τp/τn.\nsidered DL-based, supervised learning approaches, but not\nwithin orchestral scenarios.\n3. CROSS-VERSION APPROACH TO AUDIO\nREPRESENTATION LEARNING\nIn this section, we formalize our proposed cross-version\napproach to representation learning. The key idea is to\nutilize correspondences between different versions (i. e.,\nrecorded performances played by different orchestras) of\nthe same musical work. We aim to learn embeddings of\naudio excerpts such that the same musical section in dif-\nferent versions is represented by neighboring points in the\nembedding space and audio excerpts for unrelated musi-\ncal sections are mapped to distant points in the embedding\nspace. To this end, inspired by [19], we sample triplets\nof audio excerpts as in Figure 1, and apply a triplet loss\nfor learning. Musical characteristics that stay roughly con-\nstant across different versions of an orchestral work in-\nclude pitches and harmonies, as well as instrumentation. In\nlater sections, we will analyze to what extent our approach\ncaptures pitches or aspects of instrumentation.\nSingle-Version Approach (SV).We begin by formalizing\na common approach to music representation learning that\nonly utilizes temporal proximity inside a single version,\nsee also Section 2 and Figure 2. Let Wbe a set of musi-\ncal works and let Vwbe the set of available versions for a\nworkw∈ W . We ﬁrst randomly select a work w∈ W\nand some version of this work v∈Vw. LetTdenote the\nlength of vin seconds. We choose an anchor excerpt by\nuniformly sampling an anchor position a∈[0,T]and ex-\ntracting the excerpt xaofvthat is centered around a. To\nobtain the positive and negative excerpts, we choose a po-\nsitionp∈[0,T]for the positive excerpt xpofvsuch that\n|a−p| ≤τp. Thus, the positive excerpt is in temporal\nproximity of the anchor excerpt—up to a threshold of τp\nseconds—and is likely to correspond to a musically similar\nsection. In the same way, we choose a position n∈[0,T]\nfor the negative excerpt xnofvsuch that |a−n| ≥τn.\nThe negative excerpt is therefore a certain minimum dis-\ntance ofτnseconds away from the anchor position, likely\ncorresponding to a musically dissimilar section.1\n1Due to repetitions and other structural similarities, there may in fact\nbe some musically related sections that are far apart temporally. In the\nmajority of cases, however, the assumption underlying positive and neg-\native sampling will hold [3].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n833Embedding Learning. We obtain embeddings by pass-\ning these excerpts through a neural network (described in\nSection 4.2), i. e.:\nY= (ya,yp,yn) = (f(xa),f(xp),f(xn)),(1)\nwherefis a neural network that embeds an audio excerpt\nxinto an embedding vector y. Using this triplet, we can\napply a standard triplet loss [30] such as:\nL(Y) = max/parenleftbig\n0,∥ya−yp∥2\n2−∥ya−yn∥2\n2+α/parenrightbig\n,(2)\nwhereα∈R≥0describes the desired minimum margin be-\ntween the distance of embeddings for anchor and positive\nversus the distance of embeddings for anchor and negative.\nCross-Version Approach (CV).For our proposed cross-\nversion approach, we sample triplets in a different fash-\nion. Since we utilize multiple versions per work, we now\nrequire|Vw| ≥2. To form a triplet of excerpts, we ran-\ndomly select some version v1∈Vwof a work w∈ W . We\nthen sample an anchor position a1∈[0,T1], whereT1is\nthe length of v1in seconds, and extract the corresponding\nexcerptxaofv1. To obtain the positive and negative ex-\ncerpts, we randomly select another version v2∈Vw\\{v1}\nofw. As before, let T2denote the length of v2in seconds.\nWe can ﬁnd the position a2∈[0,T2]inv2corresponding\nto the same musical position as the anchor a1inv1using\nmusic alignment techniques. With this, we choose a posi-\ntionp∈[0,T2]for the positive excerpt xpofv2such that\n|a2−p| ≤τp. Thus, the positive excerpt corresponds to\nthe same musical section as the anchor, up to some toler-\nance ofτpseconds (in addition to alignment inaccuracies).\nSimilarly, we sample n∈[0,T2](with|a2−n| ≥τn) and\nextractxn. Note that only xais an excerpt of the ﬁrst ver-\nsionv1, whereas both xpandxnare excerpts of the second\nversionv2. As before, we construct a triplet Yusing these\nexcerpts and apply a standard triplet loss.\n4. EXPERIMENTAL SETUP\n4.1 Dataset and Splits\nTo show the potential of our representation learning tech-\nnique, we construct a cross-version dataset of commercial\nsymphonic and opera music recordings, illustrated in Ta-\nble 1. Our dataset contains an act from an opera (the ﬁrst\nact from Richard Wagner’s “Die Walküre”) as well as or-\nchestral pieces by Beethoven, Dvorak and Tschaikowsky.\nCounting each movement as an individual work, the\ndataset contains eleven different works in total. We choose\nthe ﬁrst movement of the Beethoven Symphony, the fourth\nmovement of the Dvorak Symphony and the third move-\nment of the Tschaikowsky Concerto for testing. Since we\ndo not have multiple opera acts that could be split into\ntrain and test, we choose an excerpt of the Wagner opera\nact (measures 697 to 955, corresponding to around twelve\nminutes), omit this excerpt during training, and use it for\ntesting. We further ensure that the train and test set contain\ndifferent versions. By splitting our dataset in this fashion,Composer WorkVersions\nNum. Avg. Duration\nWagner Die Walküre, Act 1 8 1 h\nBeethoven Symph. 3, Mvmts. 1–4 6 45 min\nDvorak Symph. 9, Mvmts. 1, 2, 4 6 40 min\nTschaikowsky Violin Concerto, Mvmts. 1–3 6 35 min\nTotal duration 20 h\nTable 1 : Our cross-version dataset containing several com-\nmercial recordings of different orchestral and opera com-\npositions.\nwe aim to avoid overﬁtting to speciﬁc musical composi-\ntions or recording conditions (the latter is also referred to\nas “album effect” [31]).\nFor the cross-version approach CV, we obtain an\nalignment between versions of the same work using\nstate-of-the-art music synchronization techniques involv-\ning chroma onset features and multi-scale alignment [2].\nFor some experiments, we also require pitch-class and\ninstrument activity annotations for our dataset. To this\nend, we manually encoded a score representation of “Die\nWalküre” and obtained further scores from the Mutopia\nproject.2Again, we use music synchronization techniques\nto align score to audio and create the annotations.\n4.2 Model\nWe implement all representation learning approaches us-\ning a convolutional neural network that takes a harmonic\nCQT representation (HCQT, [32]) of an audio excerpt as\ninput and outputs a corresponding embedding vector. The\nHCQT input consists of 201 frames (at a frame rate of\n43 Hz, i. e., roughly 4.7seconds), three bins per semitone\nfrom C1 to B7 (leading to 252 bins), and ﬁve harmonics\n(with frequency multiples of [0.5,1,2,3,4]).\nThe model architecture is adapted from [33] and re-\nceives an HCQT input patch, processes it through several\nconvolution and max-pooling layers, and outputs a single\nℓ2-normalized vector (length 128) per input. We take this\noutput as the embedding vector for the center frame of the\ninput patch. In total, the architecture has roughly 1.5 mil-\nlion learnable parameters. We train our network for 200\nepochs (with 16 000 triples randomly sampled per epoch)\nusing the Adam optimizer with a learning rate of 0.002. In\nthe interest of reproducibility, we release code and trained\nmodels for our approach.3\nIn line with previous studies on audio representation\nlearning [5–7], we apply a number of augmentations to ex-\ncerpts during training, including time scaling, pitch shift-\ning, random masking, adding noise and applying random\nequalization. For all experiments, we set τp= 0.2s. With\nthis, the maximal distance between anchor and positive\nexcerpt is in the same order of magnitude as the typical\nalignment inaccuracy between versions. We further set\n2https://www.mutopiaproject.org/\n3https://www.audiolabs-erlangen.de/resources/\nMIR/2023-ISMIR-CrossVersionLearningProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n83402004006000200400600SecondsRefI\n0200400600RefH\n0200400600Sup\n0200400600CV\n0200400600SV\n400500600\nSeconds400500600Seconds\n400500600\nSeconds\n400500600\nSeconds\n400500600\nSeconds\n400500600\nSeconds\nFigure 3 : Self-similarity matrices constructed from instrument annotations ( RefI) and pitch-class annotations ( RefH), or\nobtained with a supervised learning system ( Sup), the proposed cross-version approach ( CV), and a baseline that does not\nincorporate cross-version information ( SV). The lower row shows the sections highlighted in red from above.\nτn= 10.0sandα= 1.0. We found that results are stable\nfor a broad range of settings of these parameters.\n4.3 Baselines\nTo investigate the musical properties captured by the rep-\nresentation learning approaches CVandSV, we compare\nthem to several optimistic baselines: First, we extract tradi-\ntional music audio features. We use mel-frequency cepstral\ncoefﬁcients ( MFCC ), which are known to capture aspects\nof instrumentation [34], and Chroma features, which con-\ntain the dominant pitch-classes in the recording. Here, our\ngoal is not to outperform MFCC orChroma , but to com-\npare them to our learned representations. If our learning\napproaches capture instrumentation, we expect them to be-\nhave similar to MFCCs. Likewise, in case they contain\npitch-class information, we expect them to perform like\nChroma features.\nSecond, we consider a supervised learning approach\nSup where we train a model on instrument activity anno-\ntations and use its hidden representations as features. For\nthis, we utilize the same model architecture as for CVand\nSVand only add a ﬁnal dense layer with a number of out-\nputs equal to the number of instruments to detect. Rather\nthan using the triplet loss from Section 3, we train this ap-\nproach by applying a sigmoid activation and binary cross-\nentropy loss. Note that in contrast to CVandSV, theSup\napproach requires instrument activity annotations for the\nrecordings in the training set.\n5. RESULTS\n5.1 Feature Analysis using Self-Similarity\nIn order to visualize and compare the representations\nlearned by different techniques, we employ self-similaritymatrices. Such matrices are commonly used for mu-\nsic structure analysis and allow for visualizing struc-\ntures based on repetition and homogeneity in feature se-\nquences [1]. Here, we use them to analyze our learned\nrepresentations without the need for additional ﬁne-tuning.\nThis also allows us to directly compare approaches trained\nwith a ﬁxed instrument vocabulary ( Sup) to others that are\nnot informed about instruments. We provide an alternative\nevaluation in Section 5.4.\nGiven a sequence X= (x1,...,x N)of (learned) rep-\nresentations of Naudio frames, we construct the corre-\nsponding self-similarity matrix S∈RN×Nas follows.\nWe ﬁrst normalize all representations with respect to the\nℓ2-norm, yielding ˜X= (˜x1,...,˜xN). We then compute\nS(n,m) :=⟨˜xn,˜xm⟩forn,m∈[1 :N]. Thus,Scon-\ntains the cosine similarities between elements of X, and\nall its entries lie in the interval [−1,1]. By deﬁnition, all\nentries on the diagonal of Sare equal to 1. In addition, re-\npeated subsequences appear as path-like structures and ho-\nmogeneous segments appear as block-like structures, see\nalso [1].\nWe compare the self-similarity matrices obtained from\nlearned representations to matrices created using reference\nannotations. First, we represent an instrument activity an-\nnotation as a sequence of multi-hot binary vectors (indi-\ncating the presence of instruments in different frames). By\nnormalizing and computing the dot product as before, we\nobtain a matrix corresponding to instrument texture, where\nblocks indicate segments with similar instrumentation. We\nwill refer to this matrix using the shorthand RefI. For ex-\nample, the start of the middle measure in Figure 1 would\nbe encoded as a vector (1,1,1)⊤, i. e., all instruments are\nactive, and the end of that measure would be encoded as\n(1,1,0)⊤, i. e., only horn and soprano are active. After\nnormalization, the dot product of these vectors is 0.82,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n835indicating similar instrumentation. Analogously, we con-\nstruct another matrix RefHfrom a sequence of pitch-class\nannotations. This matrix captures regions with similar har-\nmonies and pitches.\n5.2 Qualitative Results\nFigure 3 shows several self-similarity matrices obtained\nthrough reference annotations or by different representa-\ntion learning approaches. The excerpt shown in the upper\nrow is the test excerpt from “Die Walküre” (similar results\nare obtained on other inputs). The lower row shows mag-\nniﬁed sections from above. Darker color indicates higher\nsimilarity.\nIn theRefImatrix, arising from instrument annota-\ntions as explained in Section 5.1, one can observe many\nblock and checkerboard-like structures. For example,\nfrom seconds 460 to 560, different combinations of wood-\nwind instruments are playing together, creating block and\ncheckerboard-like patterns (highlighted in blue). White ar-\neas indicate S(n,m) = 0 , i. e., no common instruments\nare playing. The matrix RefH, on the other hand, indicates\nharmonic similarities which are mostly distinct from the\ninstrument similarities in RefI.\nFor theSup system, many of the patterns in RefIare\nreplicated, albeit with less detail. This is expected, since\nthis system has been trained on the same kind of anno-\ntations that have been used to create RefI. Interestingly,\nmany of the patterns present in the RefIandSup matrices\nalso appear for the proposed approach CV, which has not\nbeen trained using instrument annotations. In particular,\nthe checkerboard pattern starting at second 460 is captured\nbyCV, as well as many block structures.\nThere are fewer similarities between CVandRefH, in-\ndicating that the CVrepresentations are more likely to\ncapture instrumentation rather than pitch-class content.\nThis behavior is encouraged by our augmentation strat-\negy, where we randomly pitch-shift the anchor, positive\nand negative excerpts.\nThe matrix obtained through the SVapproach is blurry\nand, unlike the results for CV, fails to capture many of the\ncheckerboard-like patterns present in RefI. The example\nsuggests that exploiting cross-version information during\ntraining is important for capturing aspects of instrumenta-\ntion in learned representations.\n5.3 Quantitative Results\nIn order to quantify the correlation between our learned\nrepresentations and instrument texture, we now apply a\nprocedure for detecting the boundaries of block-like struc-\ntures in self-similarity matrices. We then compare block\nboundaries estimated on RefIwith boundaries from all\nother matrices. Such procedures are often used in the con-\ntext of music structure analysis [1, 35].\nTo detect block boundaries, we ﬁrst correlate a self-\nsimilarity matrix with a checkerboard kernel along the\nmain diagonal, as proposed in [35]. From this, we ob-\ntain a novelty curve. We then apply a peak picking proce-\ndure using local thresholding on this novelty curve, yield-0 10 20 30 40 50 60\nKernel size (s)0.40.60.81.0Boundary F-measureSup CV SV MFCC\nFigure 4 : Results for different representation learning ap-\nproaches when comparing estimated structure boundaries\nto boundaries from RefI.\n0 10 20 30 40 50 60\nKernel size (s)0.40.60.81.0Boundary F-measureCV SV Chroma\nFigure 5 : Results for comparing with RefH.\ning sparse positions of detected block structures. We do\nthis for all approaches and reference matrices. We ﬁnally\ncompare—with a tolerance of up to three seconds—the\ndetected boundaries for all approaches to those of RefI,\nyielding a boundary F-measure. By adjusting the size of\nthe checkerboard kernel in this procedure, we can iden-\ntify changes of instrument texture on short or larger time\nscales. For more details on the boundary detection, peak\npicking, and evaluation procedure, we refer to [1].\nFigure 4 shows the results of our quantitative evalua-\ntion for different sizes of the checkerboard kernel. The F-\nmeasures given are averaged over all recordings in the test\ndataset. We observe that the supervised approach is best at\ncapturing instrument texture (as encoded by RefI) com-\npared to all others, with the highest F-measure of 0.77 for\na kernel of eight seconds. CVandMFCC perform roughly\non par. This is surprising, since CVis trained without any\ninstrument annotations, while MFCC is known to capture\ninstrumentation. Results for SVdeteriorate with larger ker-\nnel sizes, dropping to as low as 0.28 F-measure for a kernel\nof 48 seconds. The proposed approach CVis better at cap-\nturing instrument texture than the alternative SVthat does\nnot utilize cross-version information.\nTo examine whether our representations capture infor-\nmation related with harmonies and pitches played, we per-\nform the same evaluation procedure with boundaries from\nRefH(see Figure 5). We obtain low F-measures for both\nCVandSV(dropping below 0.4for kernel sizes above\n20 seconds for both approaches). In particular, while we\nobserve an advantage of CVoverSVfor capturing in-\nstrumentation, there is no such advantage with regard toProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n836ScenarioMicro Avg. Macro Avg.\nAP AUC F1 S F1 S\nMFCC 0.777 0.780 0.600 0.890 0.450 0.847\nSV 0.708 0.735 0.590 0.871 0.407 0.820\nCV 0.753 0.795 0.657 0.872 0.514 0.835\nSup 0.838 0.881 0.772 0.894 0.714 0.874\nTable 2 : Results for different representation learning ap-\nproaches when performing instrument classiﬁcation.\npitch-classes. Additionally, standard Chroma features are\nclearly superior at capturing the structures in RefH. We\nconclude that the representations learned by our proposed\napproachCVindeed contain information about instrument\ntexture rather than pitch-classes and harmonies.\n5.4 Feature Analysis Using Classiﬁcation\nTo gain further insights into the information captured by\nour learned representations, we also perform an indirect\nevaluation as typically done in representation learning.\nPrevious studies often rely on training small classiﬁers\non top of learned representations to investigate their use-\nfulness for different downstream tasks [5, 15]. In this\nsection, we complement our self-similarity-based analysis\nwith such a classiﬁcation-based evaluation strategy.\nTo this end, we pass individual representation vectors\nthrough a small network of dense layers with 128, 64, and\n32 hidden units followed by leaky ReLU activations, re-\nspectively. The ﬁnal layer produces outputs for every in-\nstrument annotated in our dataset, followed by a sigmoid\nactivation. For each representation learning technique, we\ntrain and evaluate such a network using the dataset split\nas described in Section 4.1. Concretely, we minimize the\nmean binary cross-entropy loss over all instrument classes\non the training set, using stochastic gradient descent with\na learning rate of 10−4for 10 epochs. We ﬁnally evalu-\nate the classiﬁcation results on the test set using standard\nmetrics, including ranking-based average precision (AP),\nmean area under the ROC curves (AUC), F-measure (F1),\nand speciﬁcity (S). For F1 and S, we threshold the pre-\ndicted probabilities at 0.5 and compute both micro and\nmacro averages of the evaluation scores, where the macro\naverage is not affected by imbalance among instrument\nclasses.\nThe results of this experiment are shown in Table 2.\nWe observe similar trends as in our self-similarity-based\nevaluation. As expected, the supervised baseline again\nyields best results. Our proposed cross-version approach\nCVclearly outperforms the traditional SVacross all metrics\n(e. g., AP = 0.753as opposed to 0.708forSV). Further-\nmore,CVeven improves upon the optimistic MFCC base-\nline in terms of AUC and F-measure (e. g., micro F1 =\n0.657 instead of 0.600 forMFCC ). Finally,SVperforms\nworse than MFCC . Overall, the representations learned by\nour proposed approach CVare more effective for instru-\nment classiﬁcation compared to the standard SVapproach\nthat does not utilize cross-version information.ScenarioMicro Avg. Macro Avg.\nAP AUC F1 S F1 S\nChroma 0.802 0.854 0.591 0.964 0.586 0.963\nSV 0.427 0.568 0.001 1.000 0.001 1.000\nCV 0.430 0.584 0.021 0.994 0.018 0.994\nSup 0.457 0.612 0.137 0.959 0.122 0.958\nTable 3 : Results for pitch-class classiﬁcation using the\nlearned representations.\nWe repeat this experiment using pitch-classes as the\nclassiﬁcation targets instead of instruments. Table 3 shows\nthe results of the modiﬁed experiment, which are inline\nwith our conclusions from previous sections. Standard\nChroma features strongly outperform all learned repre-\nsentations on this task. We conclude that our proposed\napproach captures instrumentation rather than pitches.\n6. CONCLUSION\nIn this paper, we described a novel audio representation\nlearning approach for cross-version music data and investi-\ngated its application to orchestral music. Our approach uti-\nlizes the correspondences between different versions of the\nsame musical work. We showed qualitatively and quantita-\ntively that the representations learned by our approach cap-\nture aspects of instrumentation. We outperform a standard\ntraining strategy that relies on temporal proximity alone.\nOur approach can be applied to any kind of cross-\nversion music dataset where alignments between versions\ncan be obtained using standard music synchronization\ntechniques. Future work may apply our approach to other\nmusical scenarios and larger datasets, explore more com-\nplex feature extraction networks, investigate alternatives to\nour triplet loss formulation, or apply the learned represen-\ntations in the context of different downstream tasks (such\nas structure analysis). One may also study the impact of\ndesign choices such as τpandτn, the pitch shifting aug-\nmentation, or the number of versions used for training.\nAcknowledgments: This work was supported by the\nGerman Research Foundation (DFG MU 2686/7-2,\nMU 2686/11-2). The authors are with the Interna-\ntional Audio Laboratories Erlangen, a joint institution\nof the Friedrich-Alexander-Universität Erlangen-Nürnberg\n(FAU) and Fraunhofer Institute for Integrated Circuits IIS.\nThe authors gratefully acknowledge the compute resources\nand support provided by the Erlangen Regional Computing\nCenter (RRZE).\n7. REFERENCES\n[1] M. Müller, Fundamentals of Music Processing – Us-\ning Python and Jupyter Notebooks , 2nd ed. Springer\nVerlag, 2021.\n[2] M. Müller, Y . Özer, M. Krause, T. Prätzlich, and\nJ. Driedger, “Sync Toolbox: A Python package for ef-\nﬁcient, robust, and accurate music synchronization,”\nJournal of Open Source Software (JOSS) , vol. 6,\nno. 64, pp. 3434:1–4, 2021.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n837[3] M. C. McCallum, “Unsupervised learning of deep fea-\ntures for music segmentation,” in Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP) , Brighton, UK, 2019,\npp. 346–350.\n[4] J. Wang, J. B. L. Smith, W. T. Lu, and X. Song, “Su-\npervised metric learning for music structure features,”\ninProceedings of the International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , Online,\n2021, pp. 730–737.\n[5] J. Spijkervet and J. A. Burgoyne, “Contrastive learn-\ning of musical representations,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , Online, 2021, pp. 673–681.\n[6] C. Thomé, S. Piwell, and O. Utterbäck, “Musical au-\ndio similarity with self-supervised convolutional neu-\nral networks,” in Demos and Late Breaking News of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , Online, 2021.\n[7] A. Saeed, D. Grangier, and N. Zeghidour, “Contrastive\nlearning of general-purpose audio representations,” in\nProceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\nToronto, Canada, 2021, pp. 3875–3879.\n[8] M. Buisson, B. McFee, S. Essid, and H. C. Crayen-\ncour, “Learning multi-level representations for hierar-\nchical music structure analysis,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , Bengaluru, India, 2022.\n[9] M. A. V . Vásquez and J. A. Burgoyne, “Tailed U-\nNet: Multi-scale music representation learning,” in\nProceedings of the International Society for Music In-\nformation Retrieval Conference (ISMIR) , Bengaluru,\nIndia, 2022.\n[10] M. C. McCallum, F. Korzeniowski, S. Oramas,\nF. Gouyon, and A. Ehmann, “Supervised and unsuper-\nvised learning of audio representations for music un-\nderstanding,” in Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Bengaluru, India, 2022.\n[11] B. Li and A. Kumar, “Query by video: Cross-modal\nmusic retrieval,” in Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , Delft, The Netherlands, 2019, pp. 604–611.\n[12] I. Manco, E. Benetos, E. Quinton, and G. Fazekas,\n“Learning music audio representations via weak lan-\nguage supervision,” in Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP) , Singapore, 2022, pp. 456–460.\n[13] H. Wu, C. Kao, Q. Tang, M. Sun, B. McFee, J. P. Bello,\nand C. Wang, “Multi-task self-supervised pre-training\nfor music classiﬁcation,” in Proceedings of the IEEEInternational Conference on Acoustics, Speech, and\nSignal Processing (ICASSP) , Toronto, Canada, 2021,\npp. 556–560.\n[14] P. Alonso-Jiménez, X. Serra, and D. Bogdanov, “Mu-\nsic representation learning based on editorial metadata\nfrom discogs,” in Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Bengaluru, India, 2022.\n[15] R. Castellon, C. Donahue, and P. Liang, “Codiﬁed au-\ndio language modeling learns useful representations\nfor music information retrieval,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , Online, 2021, pp. 88–96.\n[16] H. Zhang, J. Tang, S. R. M. Rafee, S. Dixon, and\nG. Fazekas, “ATEPP: A dataset of automatically tran-\nscribed expressive piano performance,” in Proceedings\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , Bengaluru, India, 2022.\n[17] S. Ewert, M. Müller, V . Konz, D. Müllensiefen, and\nG. A. Wiggins, “Towards cross-version harmonic anal-\nysis of music,” IEEE Transactions on Multimedia ,\nvol. 14, no. 3-2, pp. 770–782, 2012.\n[18] H. Schreiber, C. Weiß, and M. Müller, “Local key\nestimation in classical music audio recordings: A\ncross-version study on Schubert’s Winterreise,” in Pro-\nceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\nBarcelona, Spain, 2020, pp. 501–505.\n[19] F. Zalkow and M. Müller, “Learning low-dimensional\nembeddings of audio shingles for cross-version re-\ntrieval of classical music,” Applied Sciences , vol. 10,\nno. 1, 2020.\n[20] K. He, X. Chen, S. Xie, Y . Li, P. Dollár, and R. B. Gir-\nshick, “Masked autoencoders are scalable vision learn-\ners,” in IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , New Orleans, LA, USA,\n2022, pp. 15 979–15 988.\n[21] W. Hsu, B. Bolte, Y . H. Tsai, K. Lakhotia, R. Salakhut-\ndinov, and A. Mohamed, “HuBERT: Self-supervised\nspeech representation learning by masked prediction\nof hidden units,” IEEE/ACM Transactions on Audio,\nSpeech and Language Processing , vol. 29, pp. 3451–\n3460, 2021.\n[22] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\nG. Krueger, and I. Sutskever, “Learning transferable\nvisual models from natural language supervision,” in\nProceedings of the International Conference on Ma-\nchine Learning (ICML) , Virtual, 2021, pp. 8748–8763.\n[23] A. Guzhov, F. Raue, J. Hees, and A. Dengel, “Au-\ndioCLIP: Extending clip to image, text and audio,” in\nProceedings of the IEEE International Conference onProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n838Acoustics, Speech, and Signal Processing (ICASSP) ,\nSingapore, 2022, pp. 976–980.\n[24] M. Miron, J. J. Carabias-Orti, J. J. Bosch, E. Gómez,\nand J. Janer, “Score-informed source separation for\nmultichannel orchestral recordings,” Journal of Elec-\ntrical and Computer Engineering , vol. 2016, no.\n8363507, 2016.\n[25] Z. Tang and D. A. A. Black, “Melody extraction from\npolyphonic audio of Western opera: A method based\non detection of the singer’s formant,” in Proceedings\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , Taipei, Taiwan, October\n2014, pp. 161–166.\n[26] M. Taenzer, J. Abeßer, S. I. Mimilakis, C. Weiß,\nH. Lukashevich, and M. Müller, “Investigating CNN-\nbased instrument family recognition for Western clas-\nsical music recordings,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Delft, The Netherlands, 2019, pp. 612–\n619.\n[27] Y .-N. Hung and Y .-H. Yang, “Frame-level instrument\nrecognition by timbre and pitch,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , Paris, France, 2018, pp. 135–142.\n[28] S. Gururani, C. Summers, and A. Lerch, “Instrument\nactivity detection in polyphonic music using deep neu-\nral networks,” in Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Paris, France, 2018, pp. 569–576.\n[29] Y . Han, J. Kim, and K. Lee, “Deep convolutional neu-\nral networks for predominant instrument recognition in\npolyphonic music,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , vol. 25, no. 1, pp.\n208–221, 2017.\n[30] F. Schroff, D. Kalenichenko, and J. Philbin, “FaceNet:\nA uniﬁed embedding for face recognition and cluster-\ning,” in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , Boston,\nMA, USA, 2015, pp. 815–823.\n[31] A. Flexer, “A closer look on artist ﬁlters for musical\ngenre classiﬁcation,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Vienna, Austria, 2007, pp. 341–344.\n[32] R. M. Bittner, B. McFee, J. Salamon, P. Li, and J. P.\nBello, “Deep salience representations for F0 tracking\nin polyphonic music,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Suzhou, China, 2017, pp. 63–70.\n[33] C. Weiß, J. Zeitler, T. Zunner, F. Schuberth, and\nM. Müller, “Learning pitch-class representations from\nscore–audio pairs of classical music,” in Proceedingsof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , Online, 2021, pp. 746–\n753.\n[34] H. Terasawa, M. Slaney, and J. Berger, “The thirteen\ncolors of timbre,” in Proceedings of the IEEE Work-\nshop on Applications of Signal Processing to Audio\nand Acoustics (WASPAA) , New Paltz, NY , USA, 2005,\npp. 323–326.\n[35] J. Foote, “Automatic audio segmentation using a mea-\nsure of audio novelty,” in Proceedings of the IEEE\nInternational Conference on Multimedia and Expo\n(ICME) , New York, NY , USA, 2000, pp. 452–455.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n839"
    },
    {
        "title": "Supporting Musicological Investigations With Information Retrieval Tools: An Iterative Approach to Data Collection.",
        "author": [
            "David Lewis 0003",
            "Elisabete Shibata",
            "Andrew Hankinson",
            "Johannes Kepper",
            "Kevin R. Page",
            "Lisa Rosendahl",
            "Mark Saccomano",
            "Christine Siegert"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265407",
        "url": "https://doi.org/10.5281/zenodo.10265407",
        "abstract": "Digital musicology research often proceeds by extending and enriching its evidence base as it progresses, rather than starting with a complete corpus of data and metadata, as a consequence of an emergent research need.\n\nIn this paper, we consider a research workflow which assumes an incremental approach to data gathering and annotation. We describe tooling which implements parts of this workflow, developed to support the study of nineteenth-century music arrangements, and evaluate the applicability of our approach through interviews with musicologists and music editors who have used the tools. We conclude by considering extensions of this approach and the wider implications for digital musicology and music information retrieval.",
        "zenodo_id": 10265407,
        "dblp_key": "conf/ismir/0003SHKPRSS23",
        "keywords": [
            "Digital musicology",
            "research workflow",
            "incremental approach",
            "data gathering",
            "annotation tooling",
            "nineteenth-century music",
            "music arrangements",
            "musicologists",
            "music editors",
            "extensions"
        ],
        "ee": "https://zenodo.org/records/10265407/files/000094.pdf",
        "content": "SUPPORTING MUSICOLOGICAL INVESTIGATIONS WITH\nINFORMATION RETRIEV AL TOOLS: AN ITERATIVE APPROACH TO\nDATA COLLECTION\nDavid Lewis1Elisabete Shibata2Andrew Hankinson3Johannes Kepper4\nKevin R. Page1Lisa Rosendahl2Mark Saccomano4Christine Siegert2\n1University of Oxford, UK2BeethovenHaus Bonn, Germany\n3RISM Digital Centre, Switzerland4Paderborn University, Germany\ndavid.lewis@oerc.ox.ac.uk\nABSTRACT\nDigital musicology research often proceeds by extending\nand enriching its evidence base as it progresses, rather than\nstarting with a complete corpus of data and metadata, as a\nconsequence of an emergent research need.\nIn this paper, we consider a research workﬂow which\nassumes an incremental approach to data gathering and\nannotation. We describe tooling which implements parts\nof this workﬂow, developed to support the study of\nnineteenth-century music arrangements, and evaluate the\napplicability of our approach through interviews with mu-\nsicologists and music editors who have used the tools. We\nconclude by considering extensions of this approach and\nthe wider implications for digital musicology and music\ninformation retrieval.\n1. INTRODUCTION\nDigital humanities research often extends and enriches an\nevidence base – in the form of digital, machine-accessible\ncorpora – as it progresses, mirroring a methodological pro-\ncess of evidence gathering and preparation that is common\nand accepted in analogue research. Rather than assum-\ning complete corpus encoding as a prerequisite for digital\nscholarship, we anticipate that research subjects will more\nusually be found in un-transcribed and only minimally-\ncatalogued documents. A researcher or team can thereby\nmore effectively support their work by digitising, tran-\nscribing, and annotating a corpus incrementally. Resource\nlimitations will generally mean that this is most efﬁciently\ncarried out in an incomplete way, producing partial edi-\ntions of short extracts or individual instrumental parts, in-\nstead of a complete corpus as an outcome of the investi-\ngation. To support this mode of digital scholarship, we\npropose that incremental workﬂows, which manipulate and\n© D. Lewis, E. Shibata, A. Hankinson, J. Kepper, K. Page,\nL. Rosendahl, M. Saccomano, and C. Siegert. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: D. Lewis, E. Shibata, A. Hankinson, J. Kepper, K. Page, L.\nRosendahl, M. Saccomano, and C. Siegert, “Supporting musicological\ninvestigations with information retrieval tools: an iterative approach to\ndata collection”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.analyse incomplete sources, should be an explicit consid-\neration for applied MIR assemblies.\nWe present an example of this approach, from the\nBeethoven in the House project, where musical arrange-\nments and miscellaneous music publications aimed at a\ndomestic market are the subject of the scholarship. In this\ncase, little of the music has been published in modern edi-\ntions, and no digital editions existed at the start of the re-\nsearch process. Some sources had been photographed and\npublished online before the project began, and the remain-\nder were digitised at the request of the project. Our data\nmodel abstracts the musical structures from the surface\npresented by digital representations themselves, so that our\ntools can switch transparently between working with dig-\nital scores and facsimile images, with measure detection\nsupporting the transition. We also use Linked Data and\nuser-authored, web-based storage, which supports the en-\nrichment of institutional data resources, such as library im-\nages, without requiring that scholars have write access to\nthose servers. We focus on chained components and data\ncompatibility rather than trying to build end-to-end tools.\nOur ambition is that, at the end of the process, the digital\ntools support our own research, as well as supporting re-\nusability and transparency, since the ‘working materials’\ncan be published along with the ﬁnished results.\nIn this paper, we consider a research workﬂow which\nassumes an incomplete and incremental approach to data\ngathering and annotation. We describe tooling implement-\ning this workﬂow, and evaluate the applicability of the ap-\nproach through interviews with musicologists and music\neditors who have used the tools. We conclude by consider-\ning extensions of this approach and the wider implications\nfor digital musicology and MIR.\n2. MUSICOLOGISTS AS DIGITAL\nRESEARCHERS\nMost Information Retrieval implementations are optimised\nfrom the perspective of a ‘whole’ or ‘complete’ corpus,\nproduced by some prior acts of digitisation, being interro-\ngated by a user motivated by a single, explicit information\nneed. This approach facilitates the optimisation of retrieval\ntool engineering, since the elements of the system are\nwell known, and the quality of tools can be transparently795quantiﬁed, assessed, evaluated, and compared. Meanwhile\nBates’s model of berry picking [1] is based on the observa-\ntion that information needs often develop during the user’s\ninteractions with a system, as a part of a research process\nthat takes new ﬁndings into account in the search. Dif-\nferent information-seeking strategies and their modes of\nsearch and scope of application (whether based on con-\ntent, features or metadata) are further teased out by Weigl\net al [2]. While this does not replace or reject the engineer-\ning of MIR tools based on concrete requirements, design,\nand evaluation, it does suggest we should consider such\ntools being recomposed as components within a multitude\nof individualised workﬂows – where the overall object of\nthe composite workﬂow cannot be determined a priori. We\nreﬂect that this is especially true when MIR tools are used\nas a means to undertake curiosity-driven research, as they\noften are in support of digital musicology.\nA similar pattern is identiﬁed for data as well as tools.\nFenlon et al. [3] note the role of selecting and gathering\ndata in the research process so that, as the investigation\nevolves, so may the subset of the corpus being studied.\nMore recently, Oberbichler et al. [4] have observed that the\nseparation between the management of digital materials\nand their analysis is less clear for humanities scholarship.\nThey note that the clarity and separation of workﬂows and\nresponsibilities in digitising, organising, and interrogating\ncollections that make for efﬁcient, maintainable solutions\nmay be problematic in these domains.\nFor notated music, where data entry remains expensive\nin terms of time and effort, separating between digitisa-\ntion, digital editing, metadata organisation, and research\ncan mean that much musical heritage is ruled out from\ndigital research, as digital editors become unwitting gate-\nkeepers of our history. This can have the effect of chan-\nnelling research into canonical composers and works, and\ndiverting it from less-well-represented areas and niche and\nregional music [5]. The lack of encoded corpora appropri-\nate to their research has long been identiﬁed as an impor-\ntant problem for musicologists [6, 7]. Although it is true\nthat these issues could be addressed by comprehensive and\ncomplete mass digitisation and encoding, in the absence of\nthis, an alternative strategy may be required.\nWe have seen that berry picking can be extended to ac-\ncept that the research process involves partial and chang-\ning research questions, and even that during the investi-\ngation, the researcher may add to, correct or enrich the\nmetadata [2]. An alternative interaction model might ex-\ntend berry picking to acknowledge that this is true for the\ndata itself. Clearly, this may pose problems for statisti-\ncal evaluation of IR tools, and necessitates consideration\nof alternative approaches to system and workﬂow design.\nNonetheless we can demonstrate that it is a mode of use\naligned with the needs – and limited resources – of digital\nmusicologists.\nGiven limited resources, we cannot assume that a sin-\ngle scholar, or even a funded research project, can tran-\nscribe the complete corpus of music that might be relevant\nto their investigations – including any comparison or con-trol groups – prior to research commencing, and even pro-\nducing a complete digitisation by the end of their investi-\ngations may prove impractical. Creating an expectation of\nthe prior existence of these primary objects of study may\nfeed the sense of “disconnect between this research strand\nand musicological users’ needs and requirements” identi-\nﬁed by Inskip and Wiering [8]. A better approach would\naccommodate images or partial editions – transcriptions of\nonly a few bars or one instrumental part – created incre-\nmentally as the research progresses.\nMany of the basic tools that already exist could be\nmade to accommodate this approach well, indeed the ex-\ntra information that may be available in a digital envi-\nronment at a later research stage may help them, sup-\nporting a bootstrapping approach to training or parame-\nter tuning. Without musicologist-facing, high-level tools\nbuilt on these, researchers are more likely to resort to less\nmachine-accessible approaches, such as pen and paper or\nlocal spreadsheets.\nIn this paper we explore this interaction model through\na set of prototypes. In the next section, we describe a\nworkﬂow and tooling designed to support musicological\nresearch in previously digitally unavailable music, and dis-\ncuss how an incremental approach can be supported, be-\nfore evaluating the approach in subsequent sections.\n3. SUPPORTING RESEARCH WITH\nINCREMENTAL AND INCOMPLETE CORPORA\nMusicology, and indeed research more broadly, may in-\nvolve many activities and strategies, whose selection will\nbe informed both by research topic (see [2]) and the stage\nat which the research stands. For example, a researcher\nmay start with a literature exploration, then start reviewing\nmusic scores through a catalogue, selecting a set of poten-\ntial subjects to look at more closely and then focus down\nlater. The researcher may scan through the scores, select-\ning works or passages for further consideration, and reject-\ning others. This might be followed by closer engagement\nwith the chosen texts, often relating them to extra-musical\ninformation. Finally, their investigations will be written up\nformally.\nTeasing apart the steps of this example, and when they\nare most likely to happen in a research life cycle, we can\nsee the following:\n1. Literature exploration (early phase)\n2. Catalogue exploration (early phase)\n3. Workset selection (early phase)\n4. Content exploration (mid phase)\n5. Content analysis (mid phase)\n6. Connecting music with extra-musical material (mid\nphase)\n7. Visualisation and reporting (end phase)\nThis is not intended as a complete catalogue of re-\nsearch steps, but illustrates common components, and\nhelps ground our observations. Each of these steps will\ndecompose into tasks that may or may not be carried outProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n796Phase Step Example activity Example tools and media\nEarly1. Literature exploration Makes up-to-date literature survey RILM, JSTOR\nGoogle Scholar\nPhysical browsing\n2. Catalogue exploration Explores the repertory; identiﬁes a superset for more attention Library catalogues\nRISM\nIMSLP, CPDL\nPhysical browsing\n3. Workset selection Looks at the music, scans through scores to identify works or\npassages for detailed considerationRISM\nIMSLP, CPDL\nSpecialised corpora\nPhysical sources\nImage digitisation\nMid4. Content exploration Close reading of scores, identifying distinctive parameters that\nsupport an emerging thesisOMR\nMeasure detection\nSonic visualiser\nPiano\n5. Content analysis Lists spacing and instrumentation of chords at cadences Humdrum toolkit\nMusic21\nSonic visualiser\nSpreadsheet\nPaper\n6. Making Connections Associates particular orchestration approaches with review and\ntheory textsSpreadsheet\nPaper\nEnd 7. Visualisation & reporting Writes and publishes a journal article Journal, Published edition\nRecording, Dataset\nTable 1 . A typical set of steps in a research lifecycle, with example activities and tools. Although this appears as a list,\nscholars may jump between these, or pursue several at the same time. The Beethoven in the House Annotator supports\nstages 4 and 5, producing data suitable for stage 7.\nin a digital environment, and although broadly sequential,\na musicologist may jump backwards at any point to sup-\nplement the data they already have. To support such ﬂex-\nible research patterns, we believe it is important to create\nan ecosystem of tools that read or write compatible data,\nfacilitating researcher-directed methodologies for tool se-\nlection and task ordering.\n4. THE BEETHOVEN IN THE HOUSE\nANNOTATOR: A TOOL SUPPORTING MID PHASE\nRESEARCH\nTo investigate the feasibility of an ‘incremental’ interaction\nmodel with MIR tools, we have developed a tool to sup-\nport an active musicological investigation which also em-\nbodies the ‘mid phase’ of the research life cycle described\nabove, focussing particularly on steps 4 and 5. The tool’s\nmain purpose is to bring together digitised resources in the\nform of images and digital scores, and allow a musicolo-\ngist to view them in a browser, selecting speciﬁc extracts\nfor study and then annotating those with scholarly com-\nmentary. The resulting annotations are stored, and can be\nshared and published, including references to the pertinent\nselections from the digital music resources.\nA user entering the Beethoven in the House Anno-\ntator ﬁrst selects items to explore in a ‘library’ view –\na listing which displays metadata about available musical\nworks, their arrangements and digital resources available.\nBecause the annotator is designed to handle comparisons\nof the same passage of music as it is realised in different\nversions, the selected resources are displayed one aboveanother to aid analysis.\nOnce works are selected and loaded into the display\npane, a musicologist can point and click on individual\nnotes and measures, or click and drag to select larger re-\ngions, whether the resource is a facsimile image or a ren-\ndered score encoding. Individual selections can be anno-\ntated, but also parallel passages in different versions of a\nwork (‘Musical Material’) can be identiﬁed (ﬁgure 1, left),\nand these structures themselves annotated (ﬁgure 1, right).\nPrevious annotations can also be viewed and themselves\nannotated. Thus, the tool can be used for quick browsing\nor juxtaposition of music and metadata and for detailed la-\nbelling of the content.\nThe Beethoven in the House Annotator is built as a\nweb application, and implemented as a MELD (Music En-\ncoding and Linked Data) application [9]1. As a baseline\nprovision we assume the materials underpinning the mu-\nsicologist’s investigation are available in image form via\nIIIF2as welll as digital MEI scores when these are avail-\nable. We further assume that the musicologist has the tools\nand skills to optionally transcribe whole pieces or extracts\nand save or convert them as MEI (this can be carried out\nusing music typesetting packages such as Sibelius or Mus-\n1More precisely, we use data models and the graph traversal library\nfrom MELD, with Vue-based application code.\n2The International Image Interoperability Framework provides for\nstandardised image delivery through APIs for content and presentation.\nAlthough increasingly widespread in use by research collections in par-\nticular, it is not yet comprehensively adopted. For the purposes of our\nresearch project, required digitized images were provisioned via a local\n(private) IIIF server where they were not already available over IIIF from\nthe holding collection.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n797Figure 1 . Two screenshots from the Beethoven in the House Annotator .Left: ‘Musical material’ – Parallel passages\nrecorded as occurring in two different arrangements of Beethoven’s Wellington’s Sieg (Op. 91). Selections need not\nbe contiguous or limited to a single part. The upper version here has been retrieved from an MEI ﬁle and is displayed\nusing Verovio. The lower version is from a IIIF ﬁle for which measure locations have been separately detected using the\nCartographer tool and stored, along with links to the image, in an otherwise minimal MEI ﬁle. Right: The annotation\nview, showing an observation recorded about the musical material shown left. In both cases, structures are saved to the\nmusicologists personal Solid pod, with their login shown upper right.\neScore with the help of plugins).\nWe also assume the prior existence of well-formed and\nself-describing catalogue metadata, and we base our pro-\ntotype on the Linked Open Data published by the Gemein-\nsame Normdatei (GND) of the Deutsche Nationalbiblio-\nthek. We do this with the intention that these could in fu-\nture be loaded directly where records exist.3\nDirect image annotation is possible within our tool.\nThe musicologist may prefer to use a labour-saving mea-\nsure detection tool, such as Cartographer4or MEI Friend\n[10], both of which can output MEI with empty measures\nand image co-ordinates, and which have been successfully\ntested with our tool. When provided with MEI and IIIF\nresources such as these, our annotation tool allows the re-\nsearcher to annotate the image measure by measure – giv-\ning a semantically-richer anchor for the annotation with\nrelatively low input of manual intervention (see the lower\npane in ﬁgure 1, left). If the researcher needs a ﬁner level\nof annotation, then they may ﬁll in additional music nota-\ntion in the MEI, and can indicate the selective nature of the\nencoding in the MEI header, a process supported by tools\nsuch as MEI Friend.\nOur application supports textual Web Annotations [11]\nmade onto conceptually abstracted musical extracts rather\nthan directly onto elements or regions of the image or en-\ncoding, allowing parallel material occurring in different ar-\nrangements of a work to be annotated together and, at a\nmore basic level, allowing the model to remain agnostic\nto the different types of media used as evidence (ﬁgure 1,\nright, illustrates an example of an annotation on a passage\nthat has been identiﬁed in two arrangements, in one case\nusing the MEI transcript, and in the other a IIIF image af-\nter a process of measure detection). This uses the Music\n3In practice, the GND is not currently usable for client-side applica-\ntions due to access control headers. This would still allow the use of a\nserver-cached version of the data. Where other metadata is needed, we\ndraw on the WikiData model.\n4https://cartographer-app.zenmem.deAnnotation Ontology described by Lewis et al [12].\nIn order to promote data sharing between tools rather\nthan a single monolithic application, user data is stored\nas Linked Data in Solid Pods [13], distributed online data\nstorage with ﬁne-grained access control, and for which the\nuser can choose provider. This provides a simple mecha-\nnism for data portability between applications, given com-\npatible data structures. The structures written can refer to\nresources anywhere on the web, and traversal carried out\nby the MELD library will draw them into the application.\nIn summary, the Beethoven in the House Annotator\ndescribed above supports our proposed workﬂow in sev-\neral ways. Firstly, it is conceived as part of a pipeline of\ntools publishing compatible Linked Data and MEI, and is\nalready interoperable with existing tools. Secondly, it is\nintended to provide a low barrier for including evidence\nmaterials, allowing the use of any web-published IIIF im-\nages, complete or partial MEI ﬁles, and GND metadata\nrather than requiring extensive data entry and local servers.\nThirdly, it supports the sharing of source data and meta-\ndata, along with intermediate observations, within a re-\nsearch team. Finally, as currently implemented, annota-\ntions are minimally structured. This supports an evolving\nresearch agenda, trading expressiveness against semantic\nstructures.\n5. EV ALUATING THE BEETHOVEN IN THE\nHOUSE ANNOTATOR AND ITS WIDER\nAPPLICATION\nWhilst the tool’s internal development was aimed at satis-\nfying researcher needs within our own project, two rounds\nof wider evaluation were carried out, timed to coincide\nwith two phases of application development. These eval-\nuation rounds were carried out as semi-structured inter-\nviews following shortly after a combination of a presen-\ntation about the Annotator and period of time freely ex-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n798ploring its functionality over a pre-loaded musical library.\nIn the ﬁrst round interviews were conducted with musicol-\nogists recruited via a Studienkolleg (summer school) lo-\ncated at Beethoven Haus, Bonn, in September 2022. In\nthe second round in March 2023, volunteers from staff at\nthe Beethoven Haus were interviewed. In the ﬁrst round,\nwe interviewed 9 scholars, and 7 in the second round, of\nwhom 2 had previously been interviewed. This allowed us\nto assess progress with new and returning users.\n5.1 Workﬂow as data pipeline, low barriers for\nevidence gathering\nThe application was regarded by all interviewees as use-\nful in the context of larger musicological research projects\nand editorial work. Since our interviewees were musicol-\nogists and editors rather than engineers and, since we did\nnot demonstrate or present any tools for other steps in the\nprocess, this support is based primarily on the interviewer’s\ndescription of the intended wider context for the app rather\nthan concrete experience. Interviewees did raise important\nconcerns regarding the workﬂow itself, and these are dis-\ncussed in 5.5 below, and as further work.\n5.2 Sharing of evidence and ﬁndings\nUsers that we spoke to were strongly attracted both by\nthe idea of sharing data and annotations and the option of\nkeeping these private or controlling access – either dur-\ning the research process or separating draft and publish-\nable work. They immediately identiﬁed the equivalence\nof this approach to paper based methods of publication\nand regarded using publicly shared annotations as “similar\nto quoting published books”, although there are concerns\nabout how to verify and attest its quality. It is clear that\nthese features would be easier to realise given user inter-\nfaces optimised for these tasks, since the default manage-\nment interfaces of Solid providers, our principle medium\nfor publication, generally present usability barriers to new-\ncomers. Nonetheless, one user evaluates that the applica-\ntion has the potential to “bring everything together in a\nway I haven’t experienced before” in terms of gathering\nand sharing knowledge about musical works. This would\nsupport the “Nachprüfbarkeit”, or veriﬁability (literally re-\nviewability) of a conclusion by collecting the evidence in\na single place.\nAlthough musicology can appear – at least from its out-\nputs – as the activity of lone scholars, sharing between\nscholars in an informal way is common, as is the use of stu-\ndent assistance, both of which can beneﬁt from controlled\ndata sharing. Certainly, several participants were explic-\nitly open to a wider set of contributors, one noting that,\ndepending on the quality of the community, “more knowl-\nedge can be obtained”. Beyond this, other musicological\nuse cases identiﬁed by participants are more commonly\nteam or group activities, such as scholarly music editing\nor pedagogical uses, with sharing either between teacher\nand student or between students within a class.\nThis sharing approach is well supported for our own\nLinked Data structures, but there are concerns with theboundaries of that sharing. For example, a Linked Data\nstructure that is publicly shared could annotate a part of an\nimage or score that is not itself publicly available (perhaps\nfor copyright reasons). This would not render the informa-\ntion in the Linked Data unusable, and the URI itself would\nremain uniquely identiﬁable, but for some uses would be-\ncome unavailable. There is no clear way to deduce that one\nidentiﬁed element in an MEI ﬁle occurs earlier in the piece\nthan another purely from the URI since these semantics are\nlocated in the MEI score. Our use of the Music Annotation\nOntology brings more aspects of musical selection into the\nLinked Data domain, but we do not attempt to export mu-\nsical meaning encoded in MEI into RDF.\n5.3 Minimally-structured annotations\nThe open nature of the annotations and the Beethoven in\nthe House Annotator more generally was very clearly im-\nportant in allowing the musicologists to identify a wide\nrange of contexts in which it would be useful to them.\nThese covered the full range from studying stages in\nthe development of a particular music edition (“Platten-\nstadien”), systematic musicology, historical approaches,\nphilology and pedagogy. Participants also identiﬁed the\nease of linking material, both music and annotation ma-\nterial, which is evidence that our low structure approach\nmay have reduced barriers to use. Beyond this (sometimes\nimplicit) validation of our approach, participants identiﬁed\nsome structures in annotations to support navigation and\ndiscovery.\nIn the Beethoven in the House Annotator , annota-\ntions are edited and viewed separately from the score view.\nIn our ﬁrst version, this view was purely textual, mak-\ning them harder to navigate, and placing a strong reliance\non user-provided labels. Adding musical previews for the\nsecond version enhanced ﬁndability, but multiple partic-\nipants noted that an informal taxonomic labelling, such\nas tags, would enhance this – especially where annota-\ntions are shared between users. Data currently available\nto the application includes metadata and musical locations\n(where annotations are made on transcribed sources or im-\nages on which measure detection has been run). These are\nnot currently used in the annotation listings, but could be,\nallowing the navigation by measure and source requested\nby several participants.\n5.4 Application-speciﬁc responses\nTheBeethoven in the House Annotator builds on rich un-\nderlying data models and a complex range of data sources\nand technologies. An aspect that emerged from the inter-\nviews is that the terms chosen for deﬁning key elements\nin the model did not translate well when designing a user\ninterface. Most users had difﬁculty navigating the appli-\ncation because their expectations about the terms used did\nnot match with the meaning given to them in the context\nof the model. Although the learning curve can be con-\nquered, the interviewees expressed that substituting and\nsimplifying the language (in certain cases, hiding struc-\ntures) would be more beneﬁcial to a quick acclimation intoProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n799the application. Although the general-purpose nature of\nthe tool makes the choice of task-speciﬁc language dif-\nﬁcult, use of clear domain-speciﬁc and task-appropriate\nterms would have been better received by the musicolo-\ngists and required less detailed brieﬁng.\nAlthough the functionality of the Beethoven in the\nHouse Annotator is distinctive in ways that were recog-\nnised and appreciated, those participants who have worked\nwith comparable applications commented on affordances\nthat they missed from the other tools. In particular, famil-\niarity with EDIROM tools left some participants missing\nthe more advanced navigation system, with, for example,\njumping to measure numbers.\n5.5 The workﬂow outside the application\nOur workﬂow acknowledges the poverty of encoded scores\nbut does not, currently, accommodate the lack of digitised\nimages. These, too, have been created according to partic-\nular priorities, which may not reﬂect those of researchers.\nLibraries and archives must weigh up rarity, value, ap-\npearance, physical condition, use and public impact among\nmany other factors when deciding their digitisation policy.\nInterviewees expressed particular concern for sources lo-\ncated in institutions for whom the burden posed by digiti-\nsation in the ﬁrst place and publication as IIIF in the second\nis too great, while private collectors may have no desire to\nengage in digitisation at all. Even following the suggestion\nof one interviewee, and supporting user upload of static\nimages – whether to their own Solid Pods, or some pub-\nlic IIIF server operating for the common good – could fall\nfoul of institutional restrictions. This may indicate a need\nto point our structures at musical regions even where no\ndigital proxy exists at all, something which would require\na semantic representation of musical location. Although\nsome progress has been made towards such a representa-\ntion (see, for example, [14]), further modelling is needed\nto make a robust system.\nSimilarly, interviewees speculated about how additions\nare made to the library that the application presents. Cur-\nrently, we have no application to support the selection of\nitems from a published catalogue to create and operate on\na selected workset (steps 2 and 3 in table 1) or the discov-\nery and data transformation this would require. This has\nnot been the focus of the current research, but it does mean\nthat we have relied on some manual technical interventions\nthat would be unsuitable for the sort of musicologists we\ntarget here.\n6. CONCLUSIONS AND FURTHER WORK\nThe research workﬂow we describe here is one in which\na scholar adds and edits data and metadata, and in which\nresearch priorities develop throughout. We assert this is ex-\npresses, albeit schematically, a common approach in mu-\nsicological research. Rather than trying to create tools to\nmanage the whole process, we have advocated for smaller\ntools that can comfortably handle mixed, incomplete and\npartial data, and accumulate results in a way that is data-compatible with other applications and IR tools that the\nresearcher might use.\nThe musicologists interviewed identiﬁed a range of\ncontexts for the Beethoven in the House Annotator . That\nthese went not only beyond our design for it, but also be-\nyond its capabilities provides evidence of the need for and\ndearth of tools that support such activities and the diversity\nof approaches that can and should be considered.\nOur interviews also point clearly to further work, with\nearly-phase support – in the form of digitisation, search\nand retrieval, and workset gathering – being priorities that\nwould help researchers prepare their materials for use with\nmid phase applications such as our own. Candidates for\ncomponents of such tooling, such as Cartographer, but also\nSonic Annotator and MEI Friend, often already exist, and\noften have elements that directly support their role in an\necosystem of tools, particularly in terms of data compati-\nbility, but are often seen either as entirely standalone tools\nor built into workﬂows in task-speciﬁc ways that are not\ngeneralised.\nOur investigation demonstrates that the workﬂow into\nwhich the Beethoven in the House Annotator ﬁts is\nrecognised and valued by musicologists. The ﬂexibility\nof the annotator tool, in terms of data and functionality,\npresents many opportunities in support of musicological\nresearch. Importantly, we show that it supports or replaces\nactivities currently taking place in forms – such as Word\ndocuments, spreadsheets or on paper – that provide few\nopportunities for scholars to take advantage of either MIR\ntools in data analysis on the one hand or digital trans-\nparency and sharing of results on the other. Thus, it is\nrecognised as going beyond reproducing existing methods,\nby enhancing and extending them.\n7. ACKNOWLEDGMENTS\nThis research was undertaken by the project ‘Beethoven in\nthe House: Digital Studies of Domestic Music Arrange-\nments’, supported by a UK-Germany funding initiative:\nin the UK by the Arts and Humanities Research Council\n(AHRC) project number AH/T01279X/1 and in Germany\nfunded by the Deutsche Forschungsgemeinschaft (DFG,\nGerman Research Foundation) project number 429039809;\nwith additional support from the UK Software Sustain-\nability Institute Phase 3, funded by the UK Engineering\nand Physical Sciences Research Council (EPSRC) project\nnumber EP/S021779/1.\n8. REFERENCES\n[1] M. J. Bates, “The design of browsing and berrypicking\ntechniques for the online search interface,” Online\nReview , vol. 13, no. 5, pp. 407–424, May 1989.\n[Online]. Available: https://www.emerald.com/insight/\ncontent/doi/10.1108/eb024320/full/html\n[2] D. M. Weigl, K. R. Page, P. Organisciak, and J. S.\nDownie, “Information-seeking in large-scale digital\nlibraries: Strategies for scholarly workset creation,”Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n800in2017 ACM/IEEE Joint Conference on Digital Li-\nbraries, JCDL 2017, Toronto, ON, Canada, June 19-\n23, 2017 . IEEE Computer Society, 2017, pp. 253–\n256.\n[3] K. Fenlon, M. Senseney, H. E. Green, S. Bhat-\ntacharyya, C. Willis, and J. S. Downie, “Scholar-built\ncollections: A study of user requirements for re-\nsearch in large-scale digital libraries,” in Connecting\nCollections, Cultures, and Communities - Proceed-\nings of the 77th ASIS&T Annual Meeting, ASIST\n2014, Seattle, WA, USA, October 31 - November 5,\n2014 , ser. Proc. Assoc. Inf. Sci. Technol., vol. 51,\nno. 1. Wiley, 2014, pp. 1–10. [Online]. Available:\nhttps://doi.org/10.1002/meet.2014.14505101047\n[4] S. Oberbichler, E. Boro¸ s, A. Doucet, J. Marjanen,\nE. Pfanzelter, J. Rautiainen, H. Toivonen, and\nM. Tolonen, “Integrated interdisciplinary workﬂows\nfor research on historical newspapers: Perspectives\nfrom humanities scholars, computer scientists, and\nlibrarians,” Journal of the Association for Information\nScience and Technology , vol. 73, no. 2, pp. 225–239,\n2022. [Online]. Available: https://onlinelibrary.wiley.\ncom/doi/abs/10.1002/asi.24565\n[5] A. Kijas, “What does the data tell us?:\nRepresentation, canon, and music encoding,”\n2018, Keynote at Music Encoding Confer-\nence, Maryland, 24 May 2018. [Online]. Avail-\nable: https://medium.com/@kijas/https-medium-com-\nkijas-what-does-the-data-tell-us-926ba830702f\n[6] F. Wiering, “User needs and challenges in digital\nmusicology,” 2014, Digital Music Lab Workshop\non Analysing Big Music Data, City University\nLondon, 19 March 2014. [Online]. Available: https:\n//webspace.science.uu.nl/~wieri103/presentations/\nWieringLondonDigitalMusicLabFinal.pdf\n[7] N. Cook, “Towards the compleat musicologist?” 2005,\nInvited Keynote for the 6th International Conference\non Music Information Retrieval (ISMIR), London\n2005. [Online]. Available: https://ismir2005.ismir.net/\ndocuments/Cook-CompleatMusicologist.pdf\n[8] C. Inskip and F. Wiering, “In their own words: Us-\ning text analysis to identify musicologists’ attitudes to-\nwards technology,” in Proceedings of the 16th Inter-\nnational Society for Music Information Retrieval Con-\nference, ISMIR 2015, Málaga, Spain, October 26-30,\n2015 , 2015, pp. 455–461.\n[9] D. M. Weigl and K. R. Page, “A framework for\ndistributed semantic annotation of musical score:\n\"take it to the bridge!\",” in Proceedings of the\n18th International Society for Music Information\nRetrieval Conference, ISMIR 2017, Suzhou, China,\nOctober 23-27, 2017 , S. J. Cunningham, Z. Duan,\nX. Hu, and D. Turnbull, Eds., 2017, pp. 221–228.\n[Online]. Available: https://ismir2017.smcnus.org/wp-\ncontent/uploads/2017/10/190\\_Paper.pdf[10] D. M. Weigl and W. Goebl, “Alleviating the last mile\nof encoding: The mei-friend package for the atom\ntext editor,” in Proceedings of the Music Encoding\nConference, Alicante, 2021 . Humanities Commons,\n2022. [Online]. Available: https://hcommons.org/\ndeposits/item/hc:45977/\n[11] R. Sanderson, P. Ciccarese, and B. Young, “Web\nannotation data model,” W3C, W3C Recommenda-\ntion, Feb. 2017, https://www.w3.org/TR/2017/REC-\nannotation-model-20170223/.\n[12] D. Lewis, E. Shibata, M. Saccomano, L. Rosendahl,\nJ. Kepper, A. Hankinson, C. Siegert, and K. R. Page,\n“A model for annotating musical versions and arrange-\nments across multiple documents and media,” in DLfM\n’22: 9th International Conference on Digital Libraries\nfor Musicology, Prague Czech Republic, 28 July 2022 ,\nL. Pugin, Ed. ACM, 2022, pp. 10–18.\n[13] D. M. Weigl, W. Goebl, A. Hofmann, T. Crawford,\nF. Zubani, C. C. S. Liem, and A. Porter, “Read/write\ndigital libraries for musicology,” in 7th International\nConference on Digital Libraries for Musicology,\nMontréal, Canada, October 16, 2020 . ACM, 2020,\npp. 48–52. [Online]. Available: https://doi.org/10.\n1145/3424911.3425519\n[14] R. Viglianti, “The music addressability api: A draft\nspeciﬁcation for addressing portions of music nota-\ntion on the web,” in Proceedings of the 3rd Interna-\ntional Workshop on Digital Libraries for Musicology ,\nser. DLfM 2016. New York, NY , USA: Association\nfor Computing Machinery, 2016, p. 57–60.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n801"
    },
    {
        "title": "White Box Search Over Audio Synthesizer Parameters.",
        "author": [
            "Yuting Yang 0004",
            "Zeyu Jin",
            "Connelly Barnes",
            "Adam Finkelstein"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265255",
        "url": "https://doi.org/10.5281/zenodo.10265255",
        "abstract": "Synthesizer parameter inference searches for a set of patch connections and parameters to generate audio that best matches a given target sound. Such optimization tasks benefit from access to accurate gradients. However, typical audio synths incorporate components with discontinuities – such as sawtooth or square waveforms, or a categorical search over discrete parameters like a choice among such waveforms – that thwart conventional automatic differentiation (AD). AD libraries in frameworks like TensorFlow and PyTorch typically ignore discontinuities, providing incorrect gradients at such locations. Thus, SOTA parameter inference methods avoid differentiating the synth directly, and resort to workarounds such as genetic search or neural proxies. Instead, we adapt and extend recent computer graphics methods for differentiable rendering to directly differentiate the synth as a white box program, and thereby optimize its parameters using gradient descent. We evaluate our framework using a generic FM synth with ADSR, noise, and IIR filters, adapting its parameters to match a variety of target audio clips. Our method outperforms baselines in both quantitative and qualitative evaluations.",
        "zenodo_id": 10265255,
        "dblp_key": "conf/ismir/0004JBF23",
        "keywords": [
            "Synthesizer",
            "parameter inference",
            "audio matching",
            "gradient-based optimization",
            "discontinuities",
            "automatic differentiation",
            "white box program",
            "gradient descent",
            "generic FM synth",
            "ADPs"
        ],
        "ee": "https://zenodo.org/records/10265255/files/000021.pdf",
        "content": "WHITE BOX SEARCH OVER AUDIO SYNTHESIZER PARAMETERS\nYuting Yang1Zeyu Jin2Connelly Barnes2Adam Finkelstein1\n1Princeton University2Adobe Research\n1{yutingy, af}@princeton.edu,2{zejin, cobarnes}@adobe.com\nABSTRACT\nSynthesizer parameter inference searches for a set of patch\nconnections and parameters to generate audio that best\nmatches a given target sound. Such optimization tasks ben-\neﬁt from access to accurate gradients. However, typical\naudio synths incorporate components with discontinuities\n– such as sawtooth or square waveforms, or a categorical\nsearch over discrete parameters like a choice among such\nwaveforms – that thwart conventional automatic differen-\ntiation (AD). AD libraries in frameworks like TensorFlow\nand PyTorch typically ignore discontinuities, providing in-\ncorrect gradients at such locations. Thus, SOTA parameter\ninference methods avoid differentiating the synth directly,\nand resort to workarounds such as genetic search or neu-\nral proxies. Instead, we adapt and extend recent computer\ngraphics methods for differentiable rendering to directly\ndifferentiate the synth as a white box program, and thereby\noptimize its parameters using gradient descent. We evalu-\nate our framework using a generic FM synth with ADSR,\nnoise, and IIR ﬁlters, adapting its parameters to match a va-\nriety of target audio clips. Our method outperforms base-\nlines in both quantitative and qualitative evaluations.\n1. INTRODUCTION\nSynthesizers provide musicians and sound designers with\nﬂexibility for exploring sound with various audio char-\nacteristics. However, the versatility of synths also poses\nchallenges in terms of control, because manually search-\ning over numerous parameters to seek a particular type of\nsound requires expertise, time, and effort. Synth parame-\nter inference addresses these challenges by automating this\nsearch process to ﬁnd parameters that best match a given\ntarget sound. Given a synth fwith parameters θand a tar-\ngetT, the search seeks the optimal parameters θ∗to mini-\nmize some loss Lbetween the synth output and the target.\nθ∗=argminθL(f(θ),T) (1)\nIf the synth fcan be expressed as a white box program, a\nstraightforward solution to Equation 1 would differentiate\nLwrt the parameters θ, and then minimize Lby gradi-\nent descent. However, in practice, typical synthesizers f\n© Y . Yang, Z. Jin, C. Barnes, A. Finkelstein. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Y . Yang, Z. Jin, C. Barnes, A. Finkelstein, “White\nBox Search over Audio Synthesizer Parameters”, in Proc. of the 24rd Int.\nSociety for Music Information Retrieval Conf., Milan, Italy, 2023.contain discontinuous oscillators, like square or sawtooth\nwaveforms, and discrete categorical parameters, such as\nchoosing different waveforms and modules, that thwart tra-\nditional automatic differentiation (AD).\nResearchers have developed several workarounds to\navoid directly differentiating f. For example, genetic al-\ngorithms [1, 2] approximately solve Equation 1 at the ex-\npense of greater computation and potential artifacts from\nfailures near local minima. Alternatively, Equation 1 may\nbe approximated as black box models using deep learning:\neither the synthesizer can be approximated via a differen-\ntiable neural proxy [3,4], or the entire argmin mapping can\nbe approximated by a parameter prediction network [5, 6].\nSimilarly, the parameter space can be mapped to a V AE la-\ntent space for direct control [7]. However, the ﬂexibility of\ndeep learning approaches is constrained, as data collection\nand training are typically limited to speciﬁc synthesizers\nwith ﬁxed parameter choices, making it impractical to di-\nrectly apply trained models to arbitrary synthesizers.\nGraphics researchers developed recent methods to ap-\nproximate the gradient for discontinuous white box im-\nage generation processes [8–10]. These generally integrate\nover the discontinuous function f, and approximate the\ngradient for the integral ˆf. This paper builds on A δ[10],\nwhich replaces the traditional calculus rules for AD to di-\nrectly enable backpropagation on arbitrary discontinuous\nprograms. Our method relies on the key observation that\nthe discontinuous function will eventually be band-limited\nand sampled at some rate (e.g. 48kHz). Each sample rep-\nresents an integration over a time interval that may contain\na discontinuity. However, the band-limited function ˆfis\ncontinuous so differentiation rules can be developed for it.\nOur optimization framework differentiates a pre-ﬁltered\nwhite box synth, and solves Equation 1 via gradient de-\nscent. We adapt and extend the math in A δ[10] to differ-\nentiate discontinuous and discrete synth components, and\nalso introduce heuristic methods for better convergence.\nWe evaluate on a FM synthesizer and our approach ﬁnds\nparameters that better match the target than baselines qual-\nitatively and quantitatively. Moreover, our framework al-\nlows musicians to incorporate domain expertise to ﬂexibly\nmodify and ﬁne-tune synth modules. Because our white\nbox approach does not incur training overhead, our frame-\nwork can be ﬂexibly applied to arbitrary synth programs.\n2. RELATED WORK\nResearchers have explored a variety of techniques to auto-\nmatically search for optimal synthesizer parameters with-190out having to explicitly differentiate the synthesizer. Ge-\nnetic algorithm (GA) approaches [1, 2] mutate and cross\nvariants to search over the entire program space for ar-\nbitrary synthesizers, but suffer from excessive computa-\ntion and difﬁculty in accurately converging to local min-\nima without the guidance of the gradient. On the other\nhand, deep learning models can be used to directly predict\nthe synthesizer parameters [5, 6, 11]. However, they heav-\nily rely on the annotated datasets of synthesizer presets,\ntherefore cannot be ﬂexibly generalized to anysynthesizer.\nSimilarly, each trained model can only be used for one par-\nticular synthesizer patching, greatly limiting the ﬂexibil-\nity of the method. Unlike learning methods, our approach\ndoes not rely on a dataset, and can ﬂexibly differentiate any\nwhite-box program, supporting ﬁnetuning and parameter\ntransfer between synthesizer patches. Our gradient-based\nprocess also converges more robustly than GA.\nAlternatively, synthesizers can be deﬁned by differen-\ntiable functions, therefore allowing optimal parameters to\nbe learned through gradient descent. For example, neural\naudio synthesis methods use black-box neural networks to\ngenerate audio samples [12, 13]. The neural proxies can\nbe combined with continuous synthesizer components as\nwell, such as DDSP methods that incorporate digital sig-\nnal processing modules [4, 14], and DWTS methods with\nlearnable wavetables [15]. However, because these meth-\nods use continuous proxies, they usually do not match the\nexact parameterization of complicated discontinuous syn-\nthesizers, therefore cannot be ﬂexibly used to control con-\nventional synthesizers. Moreover, the neural modules in-\ntroduce nontrivial inference overhead and are less efﬁcient\nthan synthesizers. Unlike the differentiable neural prox-\nies, our method directly differentiates a white box program\nthat can be any desired synthesizer. Therefore, it optimizes\nsemantically meaningful parameters.\nWe leverage recent ideas from differentiable rendering\nin computer graphics. Researchers developed compiler\nframeworks to systematically differentiate arbitrary dis-\ncontinuous programs [8, 10], and application-speciﬁc so-\nlutions to efﬁciently differentiate speciﬁc types of discon-\ntinuities in the rendering pipeline [9, 16]. Our method dif-\nferentiates synthesizer discontinuities by combining these\ntwo approaches: we adapt the gradient rules from A δ[10]\nfor use with discontinuous audio waveforms, and introduce\na specialized gradient rule for discrete categorical choices.\n3. METHOD\nThis section describes our optimization pipeline for synth\nparameter inference. Section 3.1 introduces our approach\nto differentiating a synth. Section 3.2 considers loss func-\ntion options. Finally Section 3.3 discusses how to explore\nthe multi-modality and avoid local minima.\n3.1 Approximating the Gradient\nWe introduce a customized gradient, which includes dif-\nferentiating at discontinuities, avoiding plateaus with zero\ngradient, and efﬁciently differentiating IIR ﬁlters. We ﬁrst\nFM\n8 float (§3.1.1)\n5 categorical (§3.1.2)Equalizers &\nFilters\n15 float (§3.1.4)ADSR\n6 - 11 float \n(§ 3.1.3)\nWhite Noise Eq & Filters ADSROutFigure 1 . Summary for our FM synthesizer and how they\nare differentiated. Dashed boxes and arrows are optional\ncomponents whose connection is decided per target.\nintroduce A δ’s [10] gradient rule for differentiating dis-\ncontinuities and discuss its usage in audio synthesizers in\nSection 3.1.1, followed by our novel synthesizer-speciﬁc\ngradient rules in Sections 3.1.2 - 3.1.4.\n3.1.1 Differentiating Discontinuous Waveforms\nWe view discontinuities as compositions of the Heaviside\nstep function H, which evaluates to 0on the one side of\na discontinuity, and 1on the other side. The discontinuity\ncan be differentiated using the gradient rules from A δ[10].\nThe key idea is to approximate the gradient as if the discon-\ntinuous function is ﬁrst convolved with a 1D box ﬁlter φ(t)\nalong the time dimension t. As an example, if His con-\ntrolled by a continuous function c, we can differentiate the\nconvolution of H(c(t,θ))withφ(t)by applying the Dirac\ndelta’s scaling property at the discontinuity td.\n∂\n∂θ/integraldisplay\nH(c(t′,θ))φ(t−t′)dt′=/integraldisplay\nδ(c)dc\ndθφ(t−t′)dt′\n=/integraldisplayδ(t′−td)dc\ndθ\n|dc\ndt|φ(t−t′)dt′=φ(t−td)dc\ndθ\n|dc\ndt||td\nThis can be approximated with two samples corresponding\nto two ends of the box kernel φ, denoted as t+andt−. The\nbox kernel φ(t−td)either evaluates to 0or1\nt+−t−, depend-\ning on whether H(c(t,θ))evaluates to the same or differ-\nent values at t+andt−. Because cis continuous, dc/dθ\ncan be computed with AD, and its evaluation on either t+\nort−approximates that of tdbecause Lipschitz continu-\nous functions are locally bounded. Finally, dc/dt is ap-\nproximated by ﬁnite difference:c(t+,θ)−c(t−,θ)\nt+−t− . Because\nthe audio signal already samples at a regular interval along\nthe time dimension (e.g. 48kHz), we conveniently set\nthe support of the box kernel to straddle the current sam-\nple and its neighbor. While the mathematical correctness\nin [10] is derived assuming a single discontinuity in the\nneighborhood, empirically the approximated gradient also\nworks well for signals with sparse multi-discontinuities,\nsuch as when both the carrier and FM modulation waves\nare square. However, if the sampling rate is too low and\ncauses aliasing, the A δrule is unable to correctly approxi-\nmate the gradient as if the signal was antialiased.\nDiscontinuous waves such as square and sawtooth can\nbe constructed as periodic compositions of H. These\ndiscontinuities are differentiated using the gradient rules\nintroduced in A δ[10] that are analogous to the equa-\ntion above, where θmight e.g., be the frequency of a\nsquare wave. The gradient for the synthesizer parame-\nters are obtained by differentiating the loss term (Sec-\ntion 3.2) using A δgradient rules, which reduce to tradi-\ntional AD for continuous parameters, combined with ourProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n191customized gradients (Sections 3.1.2 - 3.1.4). This ap-\nproach is more accurate than differentiating a discontinu-\nity naively smoothed with arbitrary linear or sigmoid tran-\nsitions, especially when discontinuities are composited –\nfor example, the composition of discontinuous modulation\nand carrier signals in an FM synthesizer.\n3.1.2 Differentiating Discrete Categorical Choices\nSection 3.1 discusses a simple scenario where the disconti-\nnuity can be sampled along the time dimension. However,\nthe challenge remains for the discrete categorical choices,\nbecause for ﬁxed parameterization, the corresponding dis-\ncontinuity Hevaluates to a constant for any time t, there-\nfore the discontinuity cannot be easily sampled.\nThis section proposes a stochastic approach to differen-\ntiate the discrete parameters. We deﬁne a categorical node\ngas taking input from a discrete parameter xwith potential\nchoicesA,B , ..., and outputs to a ﬂoating point value:\ng(x;θ) =\n\ngA(θ)ifx==A\ngB(θ)ifx==B,\n...(2)\ngA,gBare ﬂoating point functions associated with choices\nA,B respectively, such as sine or square wave equations.\nOur stochastic approach views the discrete parameter\nxas a discrete random variable Xwith different samples\nXat different time steps. Therefore g(X;θ)is a random\nvariable as well. Throughout this section, we will use low-\nercase (e.g. x) for the synth parameters that need to be\noptimized, calligraphic (e.g. X) for its corresponding ran-\ndom variables, and regular uppercase (e.g. X) for sam-\npled values from the random variable. Note when Xhas\nclose to zero variance, it consistently samples the same\nchoice for every time step, therefore Xcan be viewed as\na constant identical to x. We further model g(X;θ)sim-\nilarly to an argmax operator, where each potential choice\nA,B, ... is associated with a “score” random variable, and\nthe output of gcorresponds to the choice with the highest\n“score”. Speciﬁcally, the “score” for choice Ais modeled\nasYA=µA+σA· U, whereµA,σAare the mean and\nstandard deviation, and Uis a uniform random variable\nwith zero mean and unit variance. For any two neighboring\nsamples with disagreeing categorical choices AandB, we\nview the inconsistency as a discontinuous branching con-\nditioned on whether the sampled “score” Yfor choice A\nis greater than Bor not:g=select(YA> YB,gA,gB).\nBy forming the discontinuity this way, the gradient wrt\nµA/B,σA/B can be easily computed with the A δgradient\nrules on the time domain. At convergence, the variance to\nevery “score” variable should be reduced to a small value\nsuch that the categorical choice is sampled consistently.\nThe stochastic gradient rule works best when there is\na high correlation between the functions associated with\neach choice gA,gB, etc. Intuitively, this allows gA,gB,\n... to form a smaller convex hull for the sampled output\ng(X;θ), therefore reducing the variance of the gradient es-\ntimation. Therefore when differentiating categorical wave-form choices, we align the phase of the wave functions\nsuch that their correlation is maximized.\n3.1.3 Avoiding Zero Gradient in Plateaus\nMany synthesizer parameters have constraints on their val-\nues, such as the period for ADSR stages should be nonneg-\native, and the ﬁlters’ cutoff frequencies should be within a\nrange to avoid singularities. A typical strategy for optimiz-\ning these constrained parameters in an unconstrained prob-\nlem is to clamp the parameters: taking the min and max\nagainst their upper and lower bounds. However, clamping\nintroduces another challenge for optimization: once the pa-\nrameter clamped, the gradient wrt the parameter becomes\nzero across an entire “out of bounds” plateau in the loss\nfunction. For example,∂max(θ,0)\n∂θ= 0whenever θ <0.\nWe propose a heuristic workaround that avoids con-\nstrained parameters getting stuck at out-of-bound values,\nvia a customized gradient for the min (or max) operator f:\nf=min(θ,C)\n∂L\n∂θ=select(θ < C,dL\ndf,max(dL\ndf,0)) (3)\nHere the min operator compares with constant C, and we\nassume the gradient wrt fis already computed as dL/df .\nNote only the blue term in Equation 3 is different from\ntraditional AD. The gradient for the max operator is sim-\nilar to Equation 3, but <and max are replaced by >and\nmin respectively. Note this is only a heuristic workaround\nfor reverse-mode AD, and can not be used for forward-\nmode because it computes dL/df before differentiating f.\nIntuitively, our customized gradient will push the out-of-\nboundθback to its valid range whenever the gradient wrt\nfwishes to bring the clamped value back to valid. We only\napply this workaround when constraining parameter values\nagainst a constant, and generic min/max comparisons be-\ntween two non-constants are still differentiated by AD.\n3.1.4 Efﬁcient IIR Filter Back-propagation\nInﬁnite impulse response (IIR) ﬁlters are widely used in\nsynths to ﬂexibly control the timbre. However, differenti-\nating the IIR ﬁlter introduces performance challenges be-\ncause each output value at a certain time step recurrently\ndepends on every input/output value in previous steps, and\nnaively unrolling the gradient in the time domain is com-\nputationally expensive. We, therefore, avoid the complex\ndependency in the time domain by applying the ﬁlter in the\nfrequency domain similar to [3]. During optimization, we\nonly differentiate the multiplication between the unﬁltered\nspectrogram and the frequency response of the ﬁlters. Be-\ncause most popular ﬁlters (e.g. Biquad, Butterworth) used\nin synthesizers already have closed-form solutions for their\nfrequency responses, requiring a frequency domain proxy\ndoes not restrict the expressiveness of this approach.\n3.2 Loss Function\nUnlike supervised deep-learning methods that could rely\non losses in the parameter space at the cost of collecting\nthe preset dataset, our optimization pipeline can only relyProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n192on spectral and time domain losses. However, ﬁnding the\nideal loss that is consistent with human perception is chal-\nlenging for several reasons. Firstly, standard losses such\nas L2 on the (log mel) spectrogram only work well when\ndistances between two signals are smaller than just notice-\nable difference (JND); but this is rarely the case during\nour optimization, as we start with random initial guesses,\nand the synthesizer may never even approach JND to an\nout-of-domain target. Furthermore, although deep percep-\ntual metrics have been developed for speech signals (e.g.,\n[17]), they do not generalize well to music synths.\nWe propose a heuristic combination of several differ-\nent losses to approximate the perceptual similarity. The\nintuition is that the gradient to the majority of the losses\nshould agree with human perception even if a few of them\nare noisy. In addition to standard losses, we also include\nthe 1D Wasserstein distance [18] along the frequency di-\nmension because of its wide applicability in matching dis-\ntributions. Our ﬁnal optimization loss is a weighted com-\nbination of the Wasserstein distance, L2, log mel L2, and\na deep feature distance from the wav2clip model [19]. The\nweights are chosen such that each component has a rel-\natively equal contribution. For the losses that work on a\nspectrogram (L2, log mel L2, and Wasserstein), we use\nthree different window sizes (512, 1024, 2048) with 75%\noverlap between windows. The deep feature loss also uses\nthe same window and hop sizes, but for efﬁciency, we\nstochastically evaluate the model using one of the window\nsizes per iteration. Additionally, because the deep feature\nmodel takes time domain signal as input, we need to apply\ninverse STFT to the spectrogram because of the frequency\ndomain IIR approximation described in Section 3.1.4.\n3.3 Identifying Perceptually Similar Results\nGradient-based optimizations may converge to a variety of\nlocal minima with different perceptual similarities to the\ntarget. Our framework runs multiple random restarts to\navoid getting stuck at local minima. However, we are not\naware of a quantitative metric that reliably characterizes\nthe perceptual similarity for synthesizers [20, 21]. While\nwe use our weighted loss in Section 3.2 to provide a gradi-\nent for the optimization, its absolute value does not pre-\ncisely correspond to perceptual similarity: perceptually\ndissimilar results sometimes have lower loss than similar\nresults. Thus, manual selection is needed to choose the\nbest results. We also implement early termination to avoid\nwasting compute at local minima, and also a mechanism to\nidentify good quality results after convergence.\nOur early termination strategy is a generalization to the\nintuition that good initializations have a higher probability\nof good convergence. We generalize the heuristic to arbi-\ntrary iterations within the optimization, and terminate the\nones with bad results at the end of a sequence of predeter-\nmined iterations. Additionally, because the weighted loss\nin Section 3.2 cannot reliably characterize perceptual simi-\nlarity, we rely on the Pareto ranking [22] on multiple losses\nto identify bad results. We terminate optimizations whose\nPareto rank on every non-deep-learning loss in Section 3.2\nFigure 2 . MOS listening test preference distribution.\nis higher than ceil(0.5max_rank), where max_rank is the\nmaximum Pareto rank for the current population. Our im-\nplementation checks for early termination every 100 iter-\nations, starting at iter 200, and we run every optimization\nuntil full convergence and simulate the early termination.\nWe also note that when the optimization result is already\nclose to the target at convergence, its loss metrics calcu-\nlated from a larger window size better resemble perceptual\nsimilarity. Speciﬁcally, large L2 errors are usually bad. We\ntherefore further omit any converged result whose L2 loss\non the spectrogram with window size 2048 is 2x higher\nthan the lowest among all results, and ﬁnally rank the re-\nmaining results based on the weighted sum of Wasserstein,\nL2, and log mel L2 on the same spectrogram.\n4. V ALIDATION\nThis section validates our proposed framework by optimiz-\ning the parameters of an FM synthesizer to match various\naudio signals for musical instruments and special sound ef-\nfects. All the targets are downloaded from the web and are\ntherefore out of domain. We ﬁrst describe our FM synthe-\nsizer in Section 4.1 and evaluation setup in Section 4.2,\nthen compare our method with two baselines through a\nuser study (Section 4.3). Section 4.4 also shows the op-\ntimization convergence. Finally, Section 4.5 demonstrates\nthe ﬂexibility of our framework with a case study that mod-\niﬁes the synthesizer modules for better quality result.\n4.1 Synthesizer Model\nWe choose an FM synthesizer as in Figure 1 following\nthe recommendation from a synthesizer expert, because it\nis simple yet expressive enough to approximate most of\nour target signals. It has one carrier signal modulated by\nthe weighted sum of four different signals. Each signal\nis parameterized with a categorical choice from the four\nbase waveforms: sin, square, triangle, and sawtooth. Each\nmodulation signal is also parameterized by ratio and in-\ndex, which controls the frequency and the magnitude of the\nmodulation. The FM signal will further be ﬁltered by three\nBiquad equalizers (low/high shelf, peak) parameterized by\ntheir cutoff frequency, resonance and gain, and a pair of\nButterworth low/high pass ﬁlters parameterized by their\ncutoff, bandwidth, and attenuation. After that, the ﬁltered\nsignal is multiplied by an ADSR parameterized with the\nduration of each stage, overall volume and that of sustain,\nthe starting time of the attack, and optionally the exponen-\ntial decay of the release as well as the scale, frequency, and\nphase to an optional AM envelope applied to attack, decay,\nand sustain. Finally, ﬁltered white noise can be optionally\nadded either by sharing the original ADSR or with a differ-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n193RatingFigure 3 . MOS listening test ratings (higher is better) for each of 16 target clips, grouped in 6 categories. Error bars corre-\nspond to 2SEM (standard error of mean). To save space we shorten names: Marim(ba), Xylo(phone), and Count(down).\nent ADSR. Optional conﬁgurations are included based on\naudio characteristics. For example, sustained sounds such\nas woodwind and brass uses the AM envelope for ADSR,\nand shorter sound such as percussion includes a ﬁltered\nwhite noise with separate ADSR. The overall model in-\ncludes 40 (e.g. oboe) - 70 (e.g. crotale) parameters.\nWe implement the FM synthesizer in PyTorch to lever-\nage its AD framework. The gradient discussed in Sec-\ntion 3.3 is implemented as the cutomized backward pass,\nand AD is used for the rest of the computation (e.g. ADSR,\nSTFT). Note this could also be generated by a compiler for\narbitrary synthesizers similar to A δ[10].\n4.2 Evaluation Setup\nWe compare with two baselines: traditional AD and zeroth\norder optimization with genetic algorithm NSGA-II. AD\nbaseline uses the same optimization framework described\nin Section 3, except that the gradient described in Sec-\ntions 3.1.1 - 3.1.3 is replaced by traditional AD. The zeroth\norder baseline does not require any gradient, and instead\nuses the genetic algorithm NSGA-II [23] to search over\nthe parameter space. Because NSGA-II is multi-objective,\nit directly ﬁnds Pareto optimal solutions to the various loss\nfunctions in Section 3.2 without having to compute their\nweighted sum as in gradient-based optimization.\nWe use 16 different target sounds, including 12 musical\ninstruments and 4 special sound effects listed in Figure 3.\nFor ours and AD, we run the experiment with 100 random\nrestarts for a maximum of 2000 iterations per restart. Note\nthat because of the early termination described in Sec-\ntion 3.3, the actual number of iterations per restart varies.\nWe additionally supply the NSGA-II with a reasonable\nsample range to the parameters, and run the algorithm with\n100 population size and 2000 generations.\n4.3 MOS Listening Test\nAs mentioned in Section 3.2, we have no perceptually-\naccurate loss for comparing synth output to target audio.\nTherefore, we rely on a Mean Opinion Score (MOS) test to\nqualitatively compare results for our method and baselines.\nFor each method and target sound, we use the top 4 results\nbased on the Pareto ranking from Section 3.3 for testing,\nresulting in 12 samples across 3 methods: ours, AD, and\nNSGA-II. These clips may be heard in our supplemental\nmaterial.\nWorkers on Amazon Mechanical Turk (AMT) rate how\nsimilar each result is to the target on a scale of 1 (bad) - 5\n(identical). They are “master” workers, English-speakers\nin the US, and are paid $20 per hour. Each worker is askedto rate all 12 samples for two different targets. We further\nembed four validation tests to ﬁlter out careless ratings:\ntwo that are intentionally corrupted from the two targets to\nbe worse than any of the 12 samples to be rated, and two\nthat are identical to targets randomly chosen from all 16\ntargets. Therefore each worker rates 2×12+4 = 28 sam-\nples for each assigned task called HIT (Human Intelligence\nTask). In the end we collected 240 valid HITs where each\naudio sample gets 30 ratings from 30 different workers.\nWe compute a preference score for each worker and\neach instrument: we calculate a mean rating for each\nmethod over the 4 rated samples. If Method 1 has a higher\nscore than Method 2, we say that Method 1 is preferred by\nthis worker. Figure 2 shows the preferences among pairs of\nmethods aggregated across all workers. Our method out-\nperforms both baselines by a larger margin, but AD is pre-\nferred more than NSGA-II. We compute the p-value for the\nhypothesis: our average rating per user per instrument is\nhigher than that of the baseline. The p-value for the AD\nbaseline is 2e-8, and for the NSGA-II baseline is 3e-61.\nWe additionally report in Figure 3 the rating for each\ntarget. Ours performs best when the FM synth is a good\nemulation of the underlying instrument , such as for wood-\nwind or brass. AD has similar ratings to ours more fre-\nquently than NSGA-II, which is consistent with Figure 2.\nNote in all cases when baselines have similar or higher rat-\nings than ours, the rating difference is always within the\nerror bar, indicating the preference is not statistically sig-\nniﬁcant. We characterize the cases where ours and base-\nlines have similar ratings into two scenarios. The ﬁrst one\nis when the target is less challenging, and can be easily re-\nconstructed by various local minimums, such as Pop1 and\nPop2. The second scenario is when the FM synth cannot\nnicely emulate the instrument, such as for Piano. There-\nfore none of the methods can converge close enough to the\ntarget, resulting in similarly low ratings.\n4.4 Optimization Convergence\nThis section discusses the optimization convergence to\ndemonstrate how frequently each method converges in the\noptimization. Figure 4 demonstrates two representative re-\nsults: Horn for ours outperforms baselines and Xylophone\nfor ours performs similarly to baseline AD. In both plots,\nall 100 populations for NSGA-II converge similarly be-\ncause bad results are removed at the end of each genera-\ntion. Unlike genetic algorithms, the 100 optimizations for\nboth ours and AD have diverging performances because\ngradient-descent only explores the local parameter space\nand may be stuck at a local minimum. The early termina-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n194Weighted Loss\nSimulated TimeFigure 4 . Comparing the convergence of ours and base-\nlines for the 100 random restarts of two tasks. The x-axis\nreports simulated time: the number of function evaluations\nscaled with the actual runtime for each method. The y-axis\nreports the weighted loss for the optimization. For ours\nand AD, each transparent line corresponds to a restart. For\nNSGA-II, each transparent line plots the loss for the kth\npopulation at each generation (k ∈[1,100]). The median\nwithin all runs at a given time is shown as the solid line.\ntion described in Section 3.3 conservatively removes some\nof the local minimums, but more importantly reduces the\nnumber of evaluations toward the end of the optimization\nbecause fewer restarts are still active. Typically, the con-\nvergence plot is consistent with the listening test result in\nFigure 3, but with the exception of Oboe, where NSGA-\nII converges to the lowest error, but its listening test per-\nforms worse than ours. But this is simply due to the choice\nof weights that combine multiple losses into one scalar:\nNSGA-II converges to lower Wasserstein and higher L2\nand log mel L2, thus it is notPareto superior to ours.\n4.5 Case Study: Modify Synthesizer Modules\nThis section uses the Xylophone target as a case study\nto demonstrate that our white box method can be ﬂexi-\nbly combined with user expertise to modify the synthesizer\ncomponents to improve the quality of generated audio.\nSimilar to other targets, Xylophone is initially approxi-\nmated by the synth model described in Section 4.1. It uses\nﬁltered white noise with independent ADSR to model the\nstrike at the start of the sound. However, the optimization\nresult is not ideal, speciﬁcally, the beginning of the audio\nsounds very different from the target. This can be veriﬁed\nby Figure 5, which compares the spectrogram for the ﬁrst\n0.07s of the sound between the target (a) and the optimiza-\ntion(b): the optimization has a longer attack stage.\nWe ask a synthesizer expert to identify the potential\ncause of the inconsistency: instead of using ﬁltered white\nnoise, the beginning of the audio may be better approxi-\nmated by an impulse with IIR ﬁlters. We, therefore, use\nthe following impulse component to replace the original\nﬁltered white noise. We ﬁrst manually calibrate the starting\ntime of the Xylophone within the target audio, and set the\nimpulse at that location. Similar to the white noise, the im-\npulse is also ﬁltered by three Biquad equalizers (low/high\nshelf and peak) and a pair of low/high-pass Biquad ﬁl-\nters. Because the impulse is not static, we have to opti-\nmize the IIR parameters in the time domain rather than the\nfrequency domain as in Section 3.1.4. Therefore we avoid\nusing any Butterworth ﬁlters mentioned in Section 4.1 for\na faster backward pass. Because the original optimization\nFigure 5 . Visualizing the spectrogram for the Xylophone\ntarget (a), original optimization (b) using ﬁltered white\nnoise described in Section 4.1, and ﬁnetune result (c) using\nan impulse module described in Section 4.5. The spectro-\ngram is computed with window size 512 and hop size 128.\nnicely approximates the target except at the beginning, we\nonly compute the loss for the ﬁrst 2048 samples, and keep\nall the FM-related parameters ﬁxed to only optimize the\nnewly added IIR parameters, the scale of the impulse, and\nthe original ADSR parameters that are initialized with their\npreviously optimized values. To better characterize the ﬁl-\ntered impulse signal, we use smaller spectrogram window\nsizes: 128, 256, and 512 with 75% overlap. Figure 5(c)\nshows the spectrogram of the ﬁnetune result that indeed\nbetter matches the attack stage of the target. Perceptually\nit also sounds better: please refer to supplemental material.\nNote the ﬁnetuning process described in this section\ncannot be supported by deep learning methods without re-\ncollecting a new dataset and re-training the model for any\nchange in the synthesizer design. Because our method di-\nrectly optimizes the white-box programs, we can ﬂexibly\nadd the synthesizer components and reuse any parameters\nfrom previous optimizations.\n5. CONCLUSION AND FUTURE WORK\nThis paper proposes to ﬁnd synthesizer parameter settings\nthat best match a given target sound by directly differen-\ntiating the white-box synthesizer program. We adapt and\nextend recent methods from differentiable rendering to dif-\nferentiate the discontinuous and discrete components of the\nsynthesizer, and design an optimization pipeline to solve\nthe problem through gradient descent. We validate our\nmethod through user studies on Mechanical Turk, where\nour result is preferred over baselines by a large margin.\nWe further demonstrate the beneﬁt of differentiating white-\nbox programs through a case study, where we can ﬂexibly\nmodify and ﬁnetune synthesizer components.\nThis work suggests several directions for future re-\nsearch. Our framework only searches for synthesizer pa-\nrameters, and leaves patch connections ﬁxed. Neverthe-\nless, the gradient rules described in Section 3.1 provide a\npotential solution. It could be easily extended to optimize\nbinary connection decisions, therefore the general patch\nconnection could be optimized if viewed as compositions\nof binary choices. Additionally, because no perceptually\naccurate loss exists for music, our framework relies on a\ncombination of various loss terms (Section 3.2) together\nwith a Pareto rank based early termination strategy to im-\nprove convergence. Future work on perceptual similarity\ncould simplify and improve our process.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1956. REFERENCES\n[1] M. J. Yee-King, L. Fedden, and M. d’Inverno, “Au-\ntomatic programming of vst sound synthesizers using\ndeep networks and other techniques,” IEEE Transac-\ntions on Emerging Topics in Computational Intelli-\ngence , vol. 2, no. 2, pp. 150–159, 2018.\n[2] K. Tatar, M. Macret, and P. Pasquier, “Automatic syn-\nthesizer preset generation with presetgen,” Journal of\nNew Music Research , vol. 45, no. 2, pp. 124–144,\n2016.\n[3] N. Masuda and D. Saito, “Synthesizer sound matching\nwith differentiable dsp.” in ISMIR , 2021, pp. 428–434.\n[4] F. Caspe, A. McPherson, and M. Sandler, “Ddx7: Dif-\nferentiable fm synthesis of musical instrument sounds,”\narXiv preprint arXiv:2208.06169 , 2022.\n[5] G. Le Vaillant, T. Dutoit, and S. Dekeyser, “Im-\nproving synthesizer programming from variational au-\ntoencoders latent space,” in Proceedings of the 24th\nInternational Conference on Digital Audio Effects\n(DAFx20in21) , Sep. 2021.\n[6] O. Barkan, D. Tsiris, O. Katz, and N. Koenigstein,\n“Inversynth: Deep estimation of synthesizer parameter\nconﬁgurations from audio signals,” IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing ,\nvol. 27, no. 12, pp. 2385–2396, 2019.\n[7] P. Esling, N. Masuda, A. Bardet, R. Despres et al. ,\n“Universal audio synthesizer control with normalizing\nﬂows,” arXiv preprint arXiv:1907.00971 , 2019.\n[8] S. Bangaru, J. Michel, K. Mu, G. Bernstein, T.-M.\nLi, and J. Ragan-Kelley, “Systematically differentiat-\ning parametric discontinuities,” ACM Trans. Graph. ,\nvol. 40, no. 107, pp. 107:1–107:17, 2021.\n[9] S. Bangaru, T.-M. Li, and F. Durand, “Unbiased\nwarped-area sampling for differentiable rendering,”\nACM Trans. Graph. , vol. 39, no. 6, pp. 245:1–245:18,\n2020.\n[10] Y . Yang, C. Barnes, A. Adams, and A. Finkelstein,\n“Aδ: Autodiff for discontinuous programs - applied to\nshaders,” in SIGGRAPH, to appear , Aug. 2022.\n[11] J. Shier, G. Tzanetakis, and K. McNally, “Spiegelib:\nAn automatic synthesizer programming library,” in Au-\ndio Engineering Society Convention 148 . Audio En-\ngineering Society, 2020.\n[12] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan,\nO. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu, “Wavenet: A generative model for\nraw audio,” arXiv preprint arXiv:1609.03499 , 2016.\n[13] J. Engel, C. Resnick, A. Roberts, S. Dieleman,\nM. Norouzi, D. Eck, and K. Simonyan, “Neural audio\nsynthesis of musical notes with wavenet autoencoders,”\ninInternational Conference on Machine Learning .\nPMLR, 2017, pp. 1068–1077.[14] J. Engel, L. Hantrakul, C. Gu, and A. Roberts,\n“Ddsp: Differentiable digital signal processing,” arXiv\npreprint arXiv:2001.04643 , 2020.\n[15] S. Shan, L. Hantrakul, J. Chen, M. Avent, and\nD. Trevelyan, “Differentiable wavetable synthesis,” in\nICASSP 2022-2022 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2022, pp. 4598–4602.\n[16] T.-M. Li, M. Luká ˇc, G. Michaël, and J. Ragan-Kelley,\n“Differentiable vector graphics rasterization for editing\nand learning,” ACM Trans. Graph. (Proc. SIGGRAPH\nAsia) , vol. 39, no. 6, pp. 193:1–193:15, 2020.\n[17] P. Manocha, A. Finkelstein, R. Zhang, N. J. Bryan,\nG. J. Mysore, and Z. Jin, “A differentiable perceptual\naudio metric learned from just noticeable differences,”\ninInterspeech , Oct. 2020.\n[18] Wikipedia contributors, “Wasserstein metric —\nWikipedia, the free encyclopedia,” 2023, [On-\nline; accessed 15-April-2023]. [Online]. Avail-\nable: https://en.wikipedia.org/w/index.php?title=\nWasserstein_metric&oldid=1147354544\n[19] H.-H. Wu, P. Seetharaman, K. Kumar, and J. P.\nBello, “Wav2clip: Learning robust audio representa-\ntions from clip,” in ICASSP 2022 - 2022 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2022.\n[20] F. Pachet and J.-J. Aucouturier, “Improving timbre\nsimilarity: How high is the sky,” Journal of negative\nresults in speech and audio sciences , vol. 1, no. 1, pp.\n1–13, 2004.\n[21] K. Siedenburg and D. Müllensiefen, “Modeling timbre\nsimilarity of short music clips,” Frontiers in psychol-\nogy, vol. 8, p. 639, 2017.\n[22] P. Sitthi-Amorn, N. Modly, W. Weimer, and\nJ. Lawrence, “Genetic programming for shader\nsimpliﬁcation,” ACM Trans. Graph. , vol. 30,\nno. 6, p. 1–12, dec 2011. [Online]. Available:\nhttps://doi.org/10.1145/2070781.2024186\n[23] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A\nfast and elitist multiobjective genetic algorithm: Nsga-\nii,”IEEE transactions on evolutionary computation ,\nvol. 6, no. 2, pp. 182–197, 2002.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n196"
    },
    {
        "title": "A Dataset and Baseline for Automated Assessment of Timbre Quality in Trumpet Sound.",
        "author": [
            "Alberto Acquilino",
            "Ninad Puranik",
            "Ichiro Fujinaga",
            "Gary P. Scavone"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.8132780",
        "url": "https://doi.org/10.5281/zenodo.8132780",
        "ee": "https://doi.org/10.5281/zenodo.10265381",
        "abstract": "Accompanying dataset to the paper titled A dataset and baseline for automated assessment of timbre quality in trumpet sound published at ISMIR2023, Milano, Italy",
        "zenodo_id": 8132780,
        "dblp_key": "conf/ismir/AcquilinoPFS23",
        "keywords": [
            "ISMIR2023",
            "Milano",
            "ISMIR",
            "trumpet sound",
            "timbre quality",
            "automated assessment",
            "dataset",
            "baseline",
            "paper",
            "A dataset"
        ]
    },
    {
        "title": "Efficient Supervised Training of Audio Transformers for Music Representation Learning.",
        "author": [
            "Pablo Alonso-Jiménez",
            "Xavier Serra",
            "Dmitry Bogdanov"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265414",
        "url": "https://doi.org/10.5281/zenodo.10265414",
        "ee": "https://zenodo.org/records/10265414/files/000098.pdf",
        "abstract": "In this work, we address music representation learning using convolution-free transformers. We build on top of existing spectrogram-based audio transformers such as AST and train our models on a supervised task using patchout training similar to PaSST. In contrast to previous works, we study how specific design decisions affect downstream music tagging tasks instead of focusing on the training task. We assess the impact of initializing the training with different existing weights, using various input audio segment lengths, using learned representations from different blocks and tokens of the transformer for downstream tasks, and applying patchout at inference to speed up feature extraction. We find that 1) initializing the audio training from ImageNet or AudioSet weights and longer input segments are beneficial both for the training and downstream tasks, 2) the best representations for the downstream tasks are located in the middle blocks of the transformer, and 3) using patchout at inference allows faster processing than our convolutional baselines while maintaining superior performance. The resulting models, MAEST, are publicly available and obtain the best performance among open models in music tagging tasks.",
        "zenodo_id": 10265414,
        "dblp_key": "conf/ismir/Alonso-JimenezS23",
        "keywords": [
            "music representation learning",
            "convolution-free transformers",
            "spectrogram-based audio transformers",
            "patchout training",
            "supervised task",
            "downstream music tagging tasks",
            "design decisions",
            "specific design decisions",
            "impact of initializing",
            "input audio segment lengths"
        ],
        "content": "EFFICIENT SUPERVISED TRAINING OF AUDIO TRANSFORMERS FOR\nMUSIC REPRESENTATION LEARNING\nPablo Alonso-Jiménez Xavier Serra Dmitry Bogdanov\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona\npablo.alonso@upf.edu\nABSTRACT\nIn this work, we address music representation learning us-\ning convolution-free transformers. We build on top of ex-\nisting spectrogram-based audio transformers such as AST\nand train our models on a supervised task using patchout\ntraining similar to PaSST. In contrast to previous works, we\nstudy how speciﬁc design decisions affect downstream mu-\nsic tagging tasks instead of focusing on the training task.\nWe assess the impact of initializing the models with dif-\nferent pre-trained weights, using various input audio seg-\nment lengths, using learned representations from differ-\nent blocks and tokens of the transformer for downstream\ntasks, and applying patchout at inference to speed up fea-\nture extraction. We ﬁnd that 1) initializing the model\nfrom ImageNet or AudioSet weights and using longer in-\nput segments are beneﬁcial both for the training and down-\nstream tasks, 2) the best representations for the consid-\nered downstream tasks are located in the middle blocks of\nthe transformer, and 3) using patchout at inference allows\nfaster processing than our convolutional baselines while\nmaintaining superior performance. The resulting models,\nMAEST,1are publicly available and obtain the best per-\nformance among open models in music tagging tasks.\n1. INTRODUCTION\nThe goal of representation learning is to develop features\nthat are suitable for a variety of tasks, rather than being spe-\nciﬁc to the training objective. In the context of audio, these\nfeatures are sometimes referred to as embeddings, and they\ntypically have a much lower dimensionality than the origi-\nnal signals, making them easier to store and process. When\nthe embeddings are well-suited to a downstream task, it is\noften possible to achieve good performance using shallow\nmodels that require few resources to train and run. Ad-\nditionally, using a single embedding model to feed sev-\neral shallow classiﬁers or regressors is more efﬁcient than\n1Music Audio Efﬁcient Spectrogram Transformer. Code for training:\nhttps://github.com/palonso/MAEST . This model is part of\nEssentia models: https://essentia.upf.edu/models.html\n© P. Alonso-Jiménez, X. Serra, and D. Bogdanov. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: P. Alonso-Jiménez, X. Serra, and D. Bogdanov, “Ef-\nﬁcient Supervised Training of Audio Transformers for Music Represen-\ntation Learning”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.having individual end-to-end models, and it simpliﬁes ad-\ndressing new related tasks with minimal additional effort.\nAs a result, embedding models are valuable for a diverse\nrange of applications, from quick prototyping without re-\nquiring detailed knowledge of audio processing to large-\nscale processing of industrial audio databases.\nThe universal success of transformers in text [1], vi-\nsion [2], and audio [3] tasks motivate further research using\nthis architecture for music representation learning. How-\never, most state-of-the-art (SOTA) models are based on\nconvolutional neural networks (CNNs) [4–7]. We hypoth-\nesize that transformers are not ruling this domain yet be-\ncause they require large amounts of data and computa-\ntional power to overcome their convolutional counterparts,\nwhile such resources are not always available. To address\nthese challenges, we propose leveraging a large collection\nof 3.3 M tracks annotated with public-domain metadata\nfrom Discogs and using techniques to train transformers\nefﬁciently. Speciﬁcally, we focus on PaSST [8], a method\nthat has demonstrated remarkable performance in the Au-\ndioSet [9] benchmark. This method uses patchout, a tech-\nnique consisting of discarding parts of the input to regu-\nlarize the training process, while also allows reducing the\nGPU memory and computations required for training. In\nthis work, we investigate the effectiveness of this technique\nfor music representation learning, considering the impact\nof speciﬁc design aspects.\nWe focus on the impact of using different combinations\nof tokens from different blocks of the transformer as em-\nbeddings, starting the training from different pre-trained\nweights from publicly available models, using different in-\nput segment lengths, and using patchout at inference time\nto speed up the embedding extraction. Our experiments\nshow that the best performance is obtained by extracting\nembeddings from the middle of the transformer and ini-\ntializing it with weights pre-trained on other audio tasks.\nContrary to previous studies based on CNNs, our trans-\nformers beneﬁt from long input segments both in training\nand different downstream scenarios. Finally, we show that,\non certain patchout conditions, our transformers are able\nto double the inference speed of an EfﬁcientNet-B0 base-\nline while producing embeddings that obtain better perfor-\nmance on downstream tasks. Moreover, this approach has\nthe advantage of being entirely conﬁgurable at inference\ntime, allowing the throughput/performance tradeoff to be\nadapted to the task at hand.\nThe remainder of this paper is structured as follows: In824Section 2 we present existing works related to this study.\nThe experimental setup is presented in Section 3, and the\nproposed experiments and results are in Section 4. Finally,\nwe conclude in Section 5.\n2. BACKGROUND\nIn this section, we review the literature on music repre-\nsentation learning to motivate the selection of our training\ntask and discuss existing audio and music transformers and\njustify our architecture and training approach. Finally, we\nintroduce existing works on music representation learning\nwith transformers.\n2.1 Music representation learning\nSome authors have pursued general-purpose representa-\ntion models to address simultaneously speech, audio event,\nand music tasks, which led to the proposal of challenges\nsuch as HEAR [10] and benchmarks such as HARES [11].\nHowever, for now, there is no evidence that a single train-\ning paradigm can yield excellent performance in all the au-\ndio domains at the same time. Alternatively, audio repre-\nsentations can be optimized to a single domain leveraging\nspeciﬁc data, which tends to produce better performance.\nIn this sense, music-speciﬁc representation models are typ-\nically evaluated in music description in terms of genre,\nmood, era, rhythmic properties or arousal and valence es-\ntimation, where the annotations are generally on the track\nlevel. Additionally, music representation models can be\nevaluated in more objective tasks such as tempo or key es-\ntimation, although, speciﬁc models using domain knowl-\nedge tend to be better suited for these tasks [12].\nMusic tagging is a multi-label classiﬁcation task using a\nvocabulary that can combine multiple music notions (e.g.,\ngenre, moods, eras). Some of the most successful music\nrepresentation learning approaches are based on music tag-\nging [5, 13–15]. Other directions include training models\non editorial metadata [4,6,16–20], multi-modal correspon-\ndence [21], co-listening statistics [4], contrastive super-\nvised [7,22–24] and self-supervised [11,25–28] objectives,\nmusic generative models [29], playlist co-occurrences [20,\n24], text [7, 30], or combinations of them [4, 19, 24, 29].\nWhile self-supervised approaches have been narrowing the\ngap with their supervised counterparts, the SOTA models\nuse music tagging [4,5], or supervised contrastive learning\nin a single-domain [6] or cross-domain [7] settings. Since\nthe scope of this work is to assess the beneﬁts of transform-\ners, we ﬁx our training task to music tagging for its sim-\nplicity, popularity, and empirically shown effectiveness.\n2.2 Transformers in audio classiﬁcation tasks\nTransformers have become a popular choice for audio\ntasks due to their superior performance compared to their\nconvolutional counterparts when sufﬁcient data is avail-\nable. Lately, AudioSet, with almost 2 M audio event ex-\ncerpts, has become a popular benchmark led by trans-\nformer models. A popular approach consists of applying\nattention over small overlapping patches (e.g., 16 ×16)Model Init. GPUs Time mAP\nAST [3] ViT - - 45.9\nPaSST [8] DeiT 2 RTX 2080ti 24 h 47.6\nMaskSpec [31] FS 64 Tesla V100 36 h 47.3\nBeats [32] FS 16 - 48.7\nTable 1 . Comparison transformers from the literature in\nterms of initialization weights, number of GPUs used for\ntraining, training time, and mAP obtained in AudioSet.\nfrom the spectrogram using a classiﬁcation objective. The\nsequence of spectrogram patches is linearly projected to\na 1-D space where a trainable positional encoding signal\nis added. A trainable classiﬁcation token is appended to\nthe sequence of projections, and after a number of Trans-\nformer blocks it is used to solve the classiﬁcation task us-\ning a linear classiﬁer. This idea was ﬁrst introduced in the\nimage domain by ViT [2] and adapted to audio spectro-\ngrams in AST [3]. PaSST extends this approach by intro-\nducing patchout , a technique consisting of discarding ran-\ndom patches from the input spectrogram at training time\n(see Figure 1) [8]. This technique has two beneﬁts. First,\nby discarding input patches, the training sequence length is\nsigniﬁcantly reduced, which increases the training speed.\nSecond, it acts as a regularization technique that improves\nthe robustness of the transformer. Additionally, patchout\ncan be combined with other training methods. MaskSpec is\na self-supervised pre-training method based on an encoder-\ndecoder architecture where the decoder has to reconstruct\nthe spectrogram from a partial spectrogram altered with\npatchout [31]. Beats is a transformer trained with a super-\nvised objective and patchout where the labels come from\na codebook of randomly initialized vectors that is itera-\ntively optimized [32]. While these techniques prevent the\ntransformers from depending on initializing from weights\nof pre-trained models, such systems are signiﬁcantly more\nresource-demanding. Table 1 compares the mentioned au-\ndio transformers in terms of GPUs used for training, train-\ning duration, and mean Average Precision (mAP) on Au-\ndioSet. Remarkably, PaSST achieves an excellent trade-\noff between mAP and needed resources. Since we aim to\nuse transformer models that can be trained with a com-\nputational budget equivalent to SOTA CNNs (i.e., using\nconsumer-grade GPUs), we focus on the standard patchout\ntraining with a supervised objective.\n2.3 Music representation learning with transformers\nSome works already combined music representation learn-\ning and pure-attention-based transformers. S3T com-\nbines MoCo’s momentum-based self-supervised con-\ntrastive learning with the Swin Transformer [33] architec-\nture to learn music representations for classiﬁcation [28].\nMuLan is an audio representation model trained with\ncross-domain contrastive learning that aligns the latent rep-\nresentations of associated audio and text pairs. The au-\nthors experiment both with a ResNet50 and an AST archi-\ntecture, with the former obtaining better performance in\ndownstream music tagging tasks [7].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n825cls0Transformer block0Transformer blockNBCE lossTraining\nProjector\nFreq. encodingdist0\n…\n……Linear\n…\nTemp. encoding\nyDownstream evaluation\naverage\nxk0\n0,1Discarded patches are not \nfed to the transformer\nk0k0\n2,1\nk0\n2,0k0\nT,0k0\nT,1\nk0\n0,0k0\n3,0k0\nT,FkN\n0,0kN\nT,F\nk0\n0,0\n cls0Transformer block0Transformer blocknBCE loss\nFreq. encodingdist0\n…\n……MLP\n…\nTemp. encoding\nyaverage\nxk0\n0,1Patchout is optional on the \ndownstream evaluation\nk0k0\n2,1\nk0\n2,0k0\nT,0k0\nT,1\nk0\n0,0k0\n3,0k0\nT,Fclsndistnkn\n0,0kn\nT,F\nk0\n0,0Transformer blockN\navgnAugmentkN\n3,0clsNdistNkn\n3,0ProjectorFigure 1 . Illustration of our system at the training and downstream evaluation stages where xis the input spectrogram, k0\nis the sequence of tokens after the patchout, yis the target labels, and BCE is the binary cross-entropy loss. Trainable and\nfrozen blocks are colored green and blue respectively.\nThe limited list of studies combining transformers and\nmusic representation learning motivates further research.\nWe propose addressing this by using a simple supervised\nobjective and patchout.\n3. EXPERIMENTAL SETUP\nWe train our models using an in-house dataset with 3.3 M\ntracks mapped to the Discogs’ public metadata dump.2\nThe training task consists of a multi-label classiﬁcation\nof the top 400 music styles from Discogs’ taxonomy. We\ncompare different training conﬁgurations in several down-\nstream tasks by training Multi-Layer Perceptrons (MLP)\non representations extracted from the transformers.\n3.1 Dataset and pre-processing\nOur dataset is derived from a pool of 4 M audio tracks\nmapped to the release information from the Discogs web-\nsite’s public dump.3All release metadata, which can in-\nclude music style tags following a pre-deﬁned taxonomy,\nis submitted by the community of platform users. Master\nreleases group different versions of the same release such\nas special editions, or remasters. We obtain our training la-\nbels,y, at the master release level by ﬁrst aggregating the\nstyle tags of all the associated releases and then discard-\ning master releases with more than ﬁve style tags or with-\nout any style label among the 400 most frequent among\nour pool of tracks. We keep tracks longer than 20 sec-\nonds. Since the style annotations are done at the master re-\nlease level, the resulting track annotations are expected to\nbe noisy. We generate validation and testing subsets with\napproximately 40,000 tracks and a training set with 3.3 M\ntracks, ensuring that every artist appears on a single split.\nThis pre-processing is similar to our previous work [6], and\nadditional details and statistics about the resulting dataset\ncan be found in the repository accompanying this publi-\ncation. For now on, we refer to this internal dataset as\nDiscogs20 .\nFrom every track, we sample 30 seconds from the cen-\nter of the track and downmix it to a mono channel at 16\nkHz. We extract 96-bands mel-spectrograms, x, using 32\n2https://www.discogs.com/data/\n3In Discogs, releases include albums, EPs, compilations, etc.ms windows and a hop size of 16 ms compressed with the\nexpression log10(1 + 10000 x)similar to previous works\nin music tagging [6, 34]. The resulting representations are\nstored as half-precision ﬂoats (16 bits) resulting in 1.3 TB\nof data. Given that our dataset is in the order of magni-\ntude of AudioSet (1.8 M vs. 3.3 M) and presents similar\nlabel density (2.7 average labels in AudioSet and 2.1 in\nDiscogs20), we adopt the sampling strategy used in previ-\nous works [8]. Every epoch, we take a balanced sample of\n200,000 tracks without replacement using the inverse label\nfrequencies as sample weight. We normalize the input to\nthe mean and standard deviation of the training set.\n3.2 Model and training\nOur transformer, MAEST , has the same architecture as\nAST [3], ViT [2], or PassT [8], and features 12 blocks of\nself-attention plus a dense layer resulting in close to 87\nmillion parameters. We use 16 ×16 patches, xt,f, with\na stride of 10 ×10. Similar to PaSST, we split the posi-\ntional encoding into time/frequency encodings ( tet,fef)\nand apply patchout by randomly discarding entire rows\nand columns from the sliced spectrogram. The input se-\nquence of tokens, k0, is created as a linear projection of\nthe patches plus the correspondent time/frequency encod-\nings,k0\nt,f=P(xt,f)+tet+fef, whereP(·)is a trainable\nlinear layer.4k1tok12represent the output tokens of the\nrespective transformer blocks. Similar to DeiT [35] and\nPaSST, we extend k0with classiﬁcation ( cls0) and distil-\nlation (dist0) trainable tokens, which are initialized with\nthe DeiT or PaSST pre-trained weights in the experiments\ninvolving these models.5We take the average of cls12\nanddis12tokens to feed a linear classiﬁer targeting y.\nWe use the Adam Optimizer with a weight decay of\n1e−4and train the model for 130 epochs. We warm up the\nmodel for 5 epochs and then keep the learning rate at 1e−4\nuntil epoch 50. Then the learning rate is linearly decreased\nto1e−7during 50 additional epochs. We consider two sets\nof weights for inference: those from the last epoch and\n4Since the mel scale is not linear, we considered specialized projectors\nfor each frequency patch. However, this did not improve the performance.\n5We considered a teacher-student approach similar to DeiT by using\na pre-trained MAEST-30 to generate pseudo-labels that were targeted by\nthedist12token in the training stage. We decided to omit the experiment\ndetails since it did not achieve a signiﬁcant improvement.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n826Dataset Size Lab. Dur. Av. Split\nMTGJ-Genre 55,215 87 FT 2.44 split 0 [38]\nMTGJ-Inst 25,135 40 FT 2.57 split 0 [38]\nMTGJ-Moods 18,486 56 FT 1.77 split 0 [38]\nMTGJ-T50 54,380 50 FT 3.07 split 0 [38]\nMTT 25,860 50 29s 2.70 12-1-3 [39]\nMSDs 241,889 50 30 1.72 usual [15]\nMSDc 231,782 50 30 1.31 CALS [40]\nTable 2 . Automatic tagging datasets used in the down-\nstream evaluation. The datasets are compared in terms of\nsample size, number of labels, audio duration (Full Tracks\nor excerpts of ﬁxed duration), average labels per track, and\nthe splits used in our evaluations.\nthose obtained by taking the mean of the model’s weights\nevery 5 epochs from epoch 50 using Stochastic Weight Av-\neraging (SWA). We pre-compute the mel-spectrograms for\nefﬁciency, which limits the set of data augmentations we\ncould apply. We use mixup [36] with alpha= 0.3and\nSpecAugment [37] by masking up to 20 groups of 8 times-\ntamps and up to 5 groups of 8 frequency bands.6\nInitialization weights. Previous works showed the im-\nportance of initializing the transformer to weights pre-\ntrained on ImageNet [3]. To gain further knowledge,\nwe consider three initialization options: the DeiT B ↑384\nmodel pre-trained on ImageNet [35], the PaSST S S16\nmodel pre-trained on mel-spectrograms from AudioSet,\nand random initialization.\nSpectrogram segment length . We consider spectro-\ngram segment lengths of 5 to 30 seconds resulting in the\narchitectures MAEST-5s, MAEST-10s, MAEST-20s, and\nMAEST-30s. In all cases, we take existing PaSST fre-\nquency and temporal encodings and interpolate them to\nthe target shape as an initialization. We use patchout dis-\ncarding 3 frequency and 15 temporal patches for MAEST-\n5s and increase the temporal patchout proportionally for\nmodels with longer input sequences (e.g., 60 patches for\nMAEST-20s).\n3.3 Evaluation\nWe evaluate our models in several music automatic tag-\nging datasets covering various musical notions. We con-\nsider the popular MagnaTagATune (MTT) and the Mil-\nlion Song Dataset (MSD) with the commonly used train-\ning, validation, and testing splits used in [39] and [15] re-\nspectively. Additionally, we report the performance of our\nmodels in the CALS split, which is an artist-ﬁltered ver-\nsion of the MSD ground truth [40]. Finally, we use the\nMTG-Jamendo Dataset, a dataset of Creative Commons\nmusic containing sub-taxonomies with the tags related to\ngenre (MTGJ-Genre), moods and themes (MTGJ-Mood),\nand instrumentation (MTGJ-Inst), along with the top 50\ntags (MTGJ-T50) in the dataset. We use the ofﬁcial split\n0 for all the subsets similar to previous works [5, 30, 41].\n6We trained MAEST using 4 Nvidia 2080 RTX Ti GPUs with 12GB\nof RAM. The training takes 31 hours for MAEST-5 and 48 hours for\nMAEST-30.Table 2 summarizes these datasets in terms of size, num-\nber of labels, audio duration, average number of labels per\ntrack, and used splits.\nWe evaluate our models by extracting internal repre-\nsentations from different blocks of the transformer and\ntraining MLP classiﬁers on top. Instead of averaging the\ncls12anddist12tokens as done in the training stage, we\nconsider three types of representations, clsn,distn, and\nthe average of the tokens representing the input spectro-\ngram patches ( avgn) afterntransformer blocks. Addition-\nally, we evaluate the complementarity of these embeddings\ntraining MLP classiﬁers on stacks of the different tokens.\nTo generate the dataset of embeddings, we average the em-\nbeddings extracted from half-overlapped segments across\nthe entire audio available for the tracks in the downstream\ndatasets. The same setup is used for the training, validation\nand testing stages.\nThe downstream model is an MLP with a single-hidden\nlayer of 512 dimensions with a ReLU activation and\ndropout. In the experiments described in Sections 4.1, 4.2,\n4.3, and 4.5, we use a batch size of 128, drop out of 0.5 and\ntrain the model for 30 epochs. In the downstream evalua-\ntion from Section 4.4, we perform a grid search over the\nfollowing hyper-parameters for each task:\n•batch size : {64, 128, 256}\n•epochs : {30, 40, 50, 60, 70, 80}\n•drop out : {0.5, 0.75}\n•maximum learning rate : {1e−3,1e−4,5e−4,1e−5}\nThe MLP is trained with the binary cross-entropy loss\nusing the Adam optimizer with a weight decay of 1e−3.\nThe learning rate is exponentially raised to its maximum\nvalue during the ﬁrst 10 epochs, kept constant for the num-\nber of epochs, and linearly reduced until reaching 1e−7at\nthe end of training. After training, we report the perfor-\nmance on the testing set obtained using the weights from\nthe epoch with the highest validation ROC-AUC.\n4. EXPERIMENTS AND RESULTS\nIn this section, we present the conducted experiments and\ndiscuss the results.\n4.1 Extracting embeddings from the transformer\nWe are interested in ﬁnding the optimal representations\nfrom the transformer to be used as embeddings. To do\nthis, we extract representations clsn,distn, andavgnfrom\ndifferent transformer blocks n∈[5,12]. To measure\nthe complementarity of these features, we train MLPs fed\nwith stacks of combinations of these representations. In\nthis experiment, we use MAEST-30s intialized with PaSST\nweights and the MTT dataset.\nFigure 2 shows mAP scores obtained with different\nstacks of embeddings extracted from the different trans-\nformer blocks. In accordance with previous studies [29],\nwe ﬁnd that the embeddings with the best performance are\nfound in the middle blocks of the transformer. This con-\ntrasts with the typical behavior of CNNs, where the bestProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n82756789101112\nTransformer blockc d a cd ca cdaConcatenated tokens36.5 38.8 40.2 40.7 40.4 40.1 39.3 38.8\n36.8 39.2 40.5 40.5 40.9 40.5 39.6 39.2\n38.1 40.1 40.6 40.4 40.2 39.3 39.6 39.4\n37.3 39.6 40.9 41.0 40.7 40.2 39.3 39.3\n39.2 40.5 41.1 40.6 40.7 39.5 39.4 39.5\n39.1 40.6 41.3 41.1 40.6 39.9 39.4 39.4\n 3738394041Figure 2 . mAP scores obtained with our evaluation setup\nin the MTT dataset using embeddings extracted from dif-\nferent blocks and tokens transformer. We evaluate the cls\n(c),dist (d), andavg(a) tokens and stacks of their combi-\nnations extracted from the transformer blocks 5 to 12.\nfeatures are normally towards the last layers of the model,\nespecially, when the downstream task is well aligned with\nthe training task. Also, concatenating the features beneﬁts\nthe performance. In the remaining experiments, we ﬁx our\nembedding to the stack ( cls7,dist7,avg7).\n4.2 Impact of the initial weights\nDue to the lack of inductive biases present in architectures\nsuch as CNNs, transformers are heavily dependent on pre-\ntraining. Because of this, many audio transformers are ini-\ntialized with weights transferred from image tasks [3, 8].\nWe evaluate the impact of initializing our models from the\nweights of DeiT [35] (image input), the best single PaSST\nmodel [8] (mel-spectrogram input), and random initializa-\ntion. In this experiment, we use MAEST-10s and its ver-\nsion with SWA weights, MAEST-10s-swa. Although our\nmain focus is to evaluate MAEST on public downstream\ndatasets, we also report their performance on the training\ntask to provide additional insights.\nTable 3 shows the performance in both, the training\n(Discogs20), and a downstream (MTT) task. In both cases,\nthe scores are higher when the training is started from pre-\ntrained weights. Since the PaSST weights result in slightly\nhigher performance, we use this initialization for the re-\nmaining of this work. Regarding the SWA, we observe a\npositive effect on the training task when the model is ini-\ntialized with pre-trained weights. However, we do not ob-\nserve improvements in the downstream task.\n4.3 Effect of the input segment length\nWe train MAEST using input segment lengths ranging\nfrom 5 to 30 seconds. In our experiments, we keep the fre-\nquency patchout constant and proportionally increase the\ntemporal patchout. For our models with segment lengths\nof 5, 10, 20, and 30 seconds we discard 15, 30, 60, and 90\ntemporal patches respectively.Model RW DeiT PaSST\nPre-training task: Discogs20\nMAEST-10s 20.5 22.7 22.8\nMAEST-10s-swa 20.1 23.2 23.5\nDownstream task: MTT\nMAEST-10s 38.7 40.4 41.1\nMAEST-10s-swa 39.0 40.2 41.0\nTable 3 . mAP scores obtained in the training and down-\nstream tasks using different initializations. We considered\nRandom Weights, and pre-trained weights from DeiT and\nPaSST.\nTable 4 shows the performance of the MAEST models\nwith respect to their input spectrogram segment length in\nterms of mAP both in the training (Discogs20) and a down-\nstream (MTT) evaluation. While music tagging CNNs tend\nto reach their peak of performance with receptive ﬁelds of\n3 to 5 seconds [14], attention-based systems have shown\nthe capability to take advantage of longer temporal con-\ntexts [40]. Our models are consistent with this trend, reach-\ning their best performance when trained on segments of 30\nseconds. Although even longer segments could be beneﬁ-\ncial, we could not use them while keeping the same model\nsize due to GPU memory limitations.\n4.4 Performance in downstream tasks\nConsidering our previous ﬁndings, we extend the evalua-\ntion of MAEST to a number of downstream datasets. We\nevaluate MAEST-10s, MAEST-20s, MAEST-30s, and a\nbaseline consisting of embeddings from the penultimate\nlayer of an EfﬁcientNet-B0 (EffNet-B0) architecture [43]\ntrained in the same 400 music style tags from Discogs20\nfollowing previous work [6]. Additionally, we report the\nperformance of SOTA models from the literature consider-\ning approaches fully trained in the downstream tasks and\nbased on embeddings plus shallow classiﬁers.\nTable 4 shows the results of the different models in\nterms of ROC-AUC and mAP. We observe that all the\nMAEST models outperform the baseline in all tasks, con-\nﬁrming the superiority of the proposed approach. Addi-\ntionally, we achieve a new SOTA for the MTGJ-Genre,\nMTGJ-Inst, and MSDc datasets, although other models re-\nmain superior in the rest of the datasets. Speciﬁcally, Mu-\nLan [7] obtains higher mAP in MTT, probably because it is\nModel 5s 10s 20s 30s\nPre-training task: Discogs20\nMAEST- T 21.1 22.8 24.8 26.1\nMAEST- T-swa 21.3 23.5 25.8 27.0\nDownstream task: MTT\nMAEST- T 40.8 41.1 41.2 41.7\nMAEST- T-swa 40.9 41.0 41.2 41.5\nTable 4 . mAP scores obtained in the training and down-\nstream tasks using different spectrogram segment lengths.\nTrepresents the spectrogram segment length.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n828MTGJ-Genre MTGJ-Inst MTGJ-Mood MTGJ-T50 MTAT MSDs MSDc\nROC mAP ROC mAP ROC mAP ROC mAP ROC mAP ROC mAP ROC mAP\nState of the art\nFully-trained- - - - 77.8 15.6 83.2 29.8 90.69 38.44 92.2 38.9 89.7 34.8\n- - - - [42] [42] [34] [34] [41] [41] [40] [40] [40] [40]\nEmbeddings87.7 19.9 77.6 19.8 78.6 16.1 84.3 32.1 92.7 41.4 - - 90.3 36.3\n[6] [6] [6] [6] [5]†[5]†[5]†[5]†[7]†[5]†- - [5]†[5]†\nBaseline\nEffNet-B0 87.7 19.9 77.6 19.8 75.6 13.6 83.1 29.7 90.2 37.4 90.4 32.8 88.9 32.8\nOur models\nMAEST-10s 88.1 21.1 79.7 22.4 77.9 15.1 84.0 31.3 91.8 41.0 91.5 36.9 88.9 32.7\nMAEST-20s 88.1 21.4 79.9 22.6 77.9 15.2 84.1 31.5 91.8 41.0 92.1 39.2 89.5 34.5\nMAEST-30s 88.2 21.6 80.0 22.9 78.1 15.4 84.0 31.5 92.0 41.9 92.4 40.7 89.8 35.4\nTable 5 . ROC-AUC and mAP scores obtained in the downstream tasks. Our baseline consists of an EffNet-B0 architecture\ntrained in Discogs20. Additionally, we report the SOTA results distinguishing models with all parameters trained in the\ndownstream tasks (fully trained) and models evaluated with shallow classiﬁers. For every task, we mark in bold the best\nscore obtained by a MAEST model and highlight in grey models achieving better performance than the best open alternative.\n†Models not publicly available.\ntrained on a much larger corpus of 40 M tracks. In MTGJ-\nMoods, MTGJ-T50, MTT, and MSDs, Musicset-Sup, a\nmodel trained on a curated dataset of 1.8 M expert annota-\ntions, remains superior [5]. In both cases, the advantage is\nlikely due to the superiority of the training task. Notably,\nnone of these models is public, which makes MAEST the\nbest open music embedding extractor available.\n4.5 Faster feature extraction with inference patchout\nInferring with transformers is typically more computation-\nally expensive than with CNNs. To speed up our models,\nwe consider using two types of patchout at inference time:\nTime-wise, we keep one out of Tspectrogram patches.\nFrequency-wise, we discard speciﬁc rows of patches. We\nexperiment with temporal patchout using T∈[2,3,5,10]\nand frequency patchout of 3 and 4 patches corresponding\nto the ﬁrst and the two last blocks, and the two ﬁrst and\ntwo last blocks respectively. The embeddings obtained un-\nder different patchout settings are compared in the training\nand a downstream task following our downstream evalua-\ntion approach on the MTT dataset.\nFigure 3 shows the mAP scores on the training and\ndownstream tasks under different patchout settings. In\nthe downstream task, even under strong patchout settings,\nMAEST-30s overcomes the throughput of standard CNN\narchitectures by two to three times while keeping higher\nmAP. On the training task, this technique is not so effec-\ntive because the classiﬁer is frozen and cannot adapt to the\neffects of patchout, and also it operates on tokens from the\nlast block, which requires more computations.\n5. CONCLUSION\nIn this work, we demonstrate the beneﬁts of pure-attention-\nbased transformers for music representation learning and\nstudy how different design decisions affect the downstream\nperformance. Our experiments show that the best embed-\ndings come from a stack of features from the middle blocks\n1020mAPF0T0\nF3T0 F3T2F3T3\nF3T5\nF4T10EffNet B0\nResNet50Pre-training task: Discogs20\nMAEST-30s\nbaseline\n0 2 4 6 8 10 12\nthroughput (analyzed minutes / second)384042mAPF0T0F3T0\nF3T2F3T3F3T5\nF4T10\nEffNet B0ResNet50Downstream task: MTTFigure 3 . mAP scores against throughput for MAEST-\n30s under different amounts of frequency (F) and time (T)\npatchout. The radius is proportional to the parameter count\nand the inference is performed on the CPU.\nof the transformer, initializing from weights pre-trained\nin audio event recognition provides the best performance,\nand that longer input segments correlate with better re-\nsults. We evaluate our models in six popular music tagging\ndatasets, and experiment with patchout at inference time,\nﬁnding that it allows speeding up signiﬁcantly the trans-\nformer while producing embeddings with better perfor-\nmance/speed trade-offs than our convolutional baselines.\nFinally, we present MAEST, a family of transformers for\nmusic style tagging and embedding extraction, which are\npublicly available and achieve SOTA performance among\ncurrently available music representation models.\nIn future work, we will combine our architecture with\nadditional training objectives combining supervised and\nself-supervised paradigms. Additionally, we will experi-\nment with longer input segments and teacher-student se-\ntups suitable for noisy datasets such as ours.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8296. ACKNOWLEDGEMENTS\nThis work has been supported by the Musical AI project\n- PID2019-111403GB-I00/AEI/10.13039/501100011033,\nfunded by the Spanish Ministerio de Ciencia e Innovación\nand the Agencia Estatal de Investigación.\n7. REFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in neural infor-\nmation processing systems , 2017.\n[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” in 9th Intl.\nConf. on Learning Representations (ICLR) , 2021.\n[3] Y . Gong, Y . Chung, and J. R. Glass, “AST: audio\nspectrogram transformer,” in 22nd Annual Conf. of\nthe Intn. Speech Communication Association (Inter-\nspeech) , 2021.\n[4] Q. Huang, A. Jansen, L. Zhang, D. P. Ellis, R. A.\nSaurous, and J. Anderson, “Large-scale weakly-\nsupervised content embeddings for music recommen-\ndation and tagging,” in Intl. Conf. on Acoustics, Speech\nand Signal Processing (ICASSP) , 2020.\n[5] M. C. McCallum, F. Korzeniowski, S. Oramas,\nF. Gouyon, and A. F. Ehmann, “Supervised and un-\nsupervised learning of audio representations for music\nunderstanding,” in Intl. Society for Music Information\nRetrieval Conf. (ISMIR) , 2022.\n[6] P. Alonso-Jiménez, X. Serra, and B. Dmitry, “Mu-\nsic representation learning based on editorial metadata\nfrom Discogs,” in Intl. Society for Music Information\nRetrieval Conf. (ISMIR) , 2022.\n[7] Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y . Li, and\nD. P. Ellis, “MuLan: A joint embedding of music au-\ndio and natural language,” in Intl. Society for Music\nInformation Retrieval Conf. (ISMIR) , 2022.\n[8] K. Koutini, J. Schlüter, H. Eghbal-zadeh, and G. Wid-\nmer, “Efﬁcient training of audio transformers with\npatchout,” in 23rd Annual Conf. of the Intl. Speech\nCommunication Association (Interspeech) , 2022.\n[9] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter,\n“Audio Set: An ontology and human-labeled dataset\nfor audio events,” in Intl. Conf. on acoustics, speech\nand signal processing (ICASSP) , 2017.\n[10] J. Turian, J. Shier, H. R. Khan, B. Raj, B. W. Schuller,\nC. J. Steinmetz, C. Malloy, G. Tzanetakis, G. Velarde,\nK. McNally, M. Henry, N. Pinto, C. Nouﬁ, C. Clough,\nD. Herremans, E. Fonseca, J. H. Engel, J. Salamon,P. Esling, P. Manocha, S. Watanabe, Z. Jin, and Y . Bisk,\n“HEAR: holistic evaluation of audio representations,”\ninConf. on Neural Information Processing Systems\n(NeurIPS) , D. Kiela, M. Ciccone, and B. Caputo, Eds.,\n2021.\n[11] L. Wang, P. Luc, Y . Wu, A. Recasens, L. Smaira,\nA. Brock, A. Jaegle, J.-B. Alayrac, S. Dieleman, J. Car-\nreira et al. , “Towards learning universal audio repre-\nsentations,” in Intl. Conf. on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , 2022.\n[12] H. Schreiber and M. Meinard, “A single-step approach\nto musical tempo estimation using a convolutional neu-\nral network,” in Intl. Society for Music Information Re-\ntrieval Conf. (ISMIR) , 2018.\n[13] A. van den Oord, S. Dieleman, and B. Schrauwen,\n“Transfer learning by supervised pre-training for\naudio-based music classiﬁcation,” in Intl. Society for\nMusic Information Retrieval Conf. (ISMIR) , 2014.\n[14] K. Choi, G. Fazekas, M. Sandler, and K. Cho, “Trans-\nfer learning for music classiﬁcation and regression\ntasks,” in Intl. Society for Music Information Retrieval\nConf. (ISMIR) , 2017.\n[15] J. Lee, J. Park, K. Kim, and J. Nam, “SampleCNN:\nEnd-to-end deep convolutional neural networks using\nvery small ﬁlters for music classiﬁcation,” Applied Sci-\nences , 2018.\n[16] J. Park, J. Lee, J.-W. Ha, and J. Nam, “Representation\nlearning of music using artist labels,” in Intl. Society\nfor Music Information Retrieval Conf. (ISMIR) , 2018.\n[17] J. Kim, M. Won, X. Serra, and C. C. S. Liem, “Transfer\nlearning of artist group factors to musical genre classi-\nﬁcation,” Intl. World Wide Web Conf. , 2018.\n[18] J. Lee, J. Park, and J. Nam, “Representation learning\nof music using artist, album, and track information,”\ninIntl. Conf. on Machine Learning (ICML), Machine\nLearning for Music Discovery Workshop , 2019.\n[19] J. Kim, J. Urbano, C. C. S. Liem, and A. Hanjalic,\n“One deep music representation to rule them all? a\ncomparative analysis of different representation learn-\ning strategies,” Neural Computing and Applications ,\n2020.\n[20] P. Alonso-Jiménez, X. Favory, H. Foroughmand,\nG. Bourdalas, X. Serra, T. Lidy, and D. Bogdanov,\n“Pre-training strategies using contrastive learning and\nplaylist information for music classiﬁcation and simi-\nlarity,” in Intl. Conf. on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2023.\n[21] J. Cramer, H.-H. Wu, J. Salamon, and J. P. Bello,\n“Look, listen, and learn more: Design choices for deep\naudio embeddings,” in Intl. Conf. on Acoustics, Speech\nand Signal Processing (ICASSP) , 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n830[22] X. Favory, K. Drossos, T. Virtanen, and X. Serra,\n“COALA: co-aligned autoencoders for learning se-\nmantically enriched audio representations,” in Work-\nshop on Self-supervised Learning in Audio and Speech,\nIntl. Conf. on Machine Learning (ICML) , 2020.\n[23] ——, “Learning contextual tag embeddings for cross-\nmodal alignment of audio and tags,” in Intl. Conf. on\nAcoustics, Speech and Signal Processing (ICASSP) ,\n2021.\n[24] A. Ferraro, X. Favory, K. Drossos, Y . Kim, and D. Bog-\ndanov, “Enriched music representations with multiple\ncross-modal contrastive learning,” Signal Processing\nLetters , 2021.\n[25] J. Spijkervet and J. A. Burgoyne, “Contrastive learning\nof musical representations,” in Intl. Society for Music\nInformation Retrieval Conf. (ISMIR) , 2021.\n[26] D. Niizumi, D. Takeuchi, Y . Ohishi, N. Harada, and\nK. Kashino, “Byol for audio: Self-supervised learning\nfor general-purpose audio representation,” in 2021 Intl.\nJoint Conf. on Neural Networks (IJCNN) , 2021.\n[27] D. Yao, Z. Zhao, S. Zhang, J. Zhu, Y . Zhu, R. Zhang,\nand X. He, “Contrastive learning with positive-\nnegative frame mask for music representation,” in Intl.\nWorld Wide Web Conf. , 2022.\n[28] H. Zhao, C. Zhang, B. Zhu, Z. Ma, and K. Zhang,\n“S3t: Self-supervised pre-training with swin trans-\nformer for music classiﬁcation,” in Intl. Conf. on\nAcoustics, Speech and Signal Processing (ICASSP) ,\n2022.\n[29] R. Castellon, C. Donahue, and P. Liang, “Codiﬁed au-\ndio language modeling learns useful representations\nfor music information retrieval,” in Intl. Society for\nMusic Information Retrieval Conf. (ISMIR) , 2021.\n[30] I. Manco, E. Benetos, E. Quinton, and G. Fazekas,\n“Learning music audio representations via weak lan-\nguage supervision,” in IEEE Intl. Conf. on Acoustics,\nSpeech and Signal Processing (ICASSP) , 2022.\n[31] D. Chong, H. Wang, P. Zhou, and Q. Zeng, “Masked\nspectrogram prediction for self-supervised audio pre-\ntraining,” arXiv preprint arXiv:2204.12768 , 2022.\n[32] S. Chen, Y . Wu, C. Wang, S. Liu, D. Tompkins,\nZ. Chen, and F. Wei, “BEATs: Audio pre-training with\nacoustic tokenizers,” arXiv preprint arXiv:2212.09058 ,\n2022.\n[33] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin,\nand B. Guo, “Swin transformer: Hierarchical vision\ntransformer using shifted windows,” in Proc. of the\nIntl. Conf. on Computer Vision (ICCV) , 2021.\n[34] J. Pons and X. Serra, “musicnn: Pre-trained convolu-\ntional neural networks for music audio tagging,” Late-\nBraeking/Demo, Intl. Society for Music Information\nRetrieval Conf. (ISMIR) , 2019.[35] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-\nrolles, and H. Jégou, “Training data-efﬁcient image\ntransformers & distillation through attention,” in Intl.\nConf. on Machine Learning (ICML) , 2021.\n[36] H. Zhang, M. Cissé, Y . N. Dauphin, and D. Lopez-Paz,\n“mixup: Beyond empirical risk minimization,” in Intl.\nConf. on Learning Representations (ICLR) , 2018.\n[37] D. S. Park, W. Chan, Y . Zhang, C. Chiu, B. Zoph,\nE. D. Cubuk, and Q. V . Le, “Specaugment: A simple\ndata augmentation method for automatic speech recog-\nnition,” in Annual Conf. of the Intl. Speech Communi-\ncation Association (Interspeech) , 2019.\n[38] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and\nX. Serra, “The MTG-Jamendo dataset for automatic\nmusic tagging,” in Intl. Conf. on Machine Learning\n(ICML) , 2019.\n[39] A. van den Oord, S. Dieleman, and B. Schrauwen,\n“Transfer learning by supervised pre-training for\naudio-based music classiﬁcation,” in Conf. of the\nIntl. Society for Music Information Retrieval (ISMIR) ,\n2014.\n[40] M. Won, K. Choi, and X. Serra, “Semi-supervised mu-\nsic tagging transformer,” in Intl. Society for Music In-\nformation Retrieval Conf. (ISMIR) , 2021.\n[41] M. Won, A. Ferraro, D. Bogdanov, and X. Serra, “Eval-\nuation of cnn-based automatic music tagging models,”\ninSound and Music Computing Conf. (SMC) , 2020.\n[42] D. Knox, T. Greer, B. Ma, E. Kuo, K. Somande-\npalli, and S. Narayanan, “Mediaeval 2020 emotion and\ntheme recognition in music task: Loss function ap-\nproaches for multi-label music tagging,” in Proc. of the\nMediaEval 2020 Workshop , 2020.\n[43] M. Tan and Q. Le, “Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks,” in Intl.\nConf. on Machine Learning (ICML) , 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n831"
    },
    {
        "title": "A Semi-Supervised Deep Learning Approach to Dataset Collection for Query-by-Humming Task.",
        "author": [
            "Amantur Amatov",
            "Dmitry Lamanov",
            "Maksim Titov",
            "Ivan Vovk",
            "Ilya Makarov",
            "Mikhail A. Kudinov"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265375",
        "url": "https://doi.org/10.5281/zenodo.10265375",
        "ee": "https://zenodo.org/records/10265375/files/000077.pdf",
        "abstract": "Query-by-Humming (QbH) is a task that involves finding the most relevant song based on a hummed or sung fragment. Despite recent successful commercial solutions, implementing QbH systems remains challenging due to the lack of high-quality datasets for training machine learning models. In this paper, we propose a deep learning data collection technique and introduce Covers and Hummings Aligned Dataset (CHAD), a novel dataset that contains 18 hours of short music fragments, paired with time-aligned hummed versions. To expand our dataset, we employ a semi-supervised model training pipeline that leverages the QbH task as a specialized case of cover song identification (CSI) task. Starting with a model trained on the initial dataset, we iteratively collect groups of fragments of cover versions of the same song and retrain the model on the extended data. Using this pipeline, we collect over 308 hours of additional music fragments, paired with time-aligned cover versions. The final model is successfully applied to the QbH task and achieves competitive results on benchmark datasets. Our study shows that the proposed dataset and training pipeline can effectively facilitate the implementation of QbH systems.",
        "zenodo_id": 10265375,
        "dblp_key": "conf/ismir/AmatovLTVMK23",
        "keywords": [
            "Query-by-Humming",
            "training machine learning models",
            "Covers and Hummings Aligned Dataset (CHAD)",
            "semi-supervised model training pipeline",
            "time-aligned hummed versions",
            "music fragments",
            "cover versions",
            "QbH task",
            "benchmark datasets",
            "competitive results"
        ],
        "content": "A SEMI-SUPERVISED DEEP LEARNING APPROACH TO DATASET\nCOLLECTION FOR QUERY-BY-HUMMING TASK\nAmantur Amatov1Dmitry Lamanov2Maksim Titov2\nIvan Vovk2Ilya Makarov3Mikhail Kudinov2\n1Higher School of Economics, Moscow, Russia\n2Huawei Noah’s Ark Lab, Moscow, Russia\n3AI Center, NUST MISiS, Moscow, Russia\nABSTRACT\nQuery-by-Humming (QbH) is a task that involves ﬁnding\nthe most relevant song based on a hummed or sung frag-\nment. Despite recent successful commercial solutions, im-\nplementing QbH systems remains challenging due to the\nlack of high-quality datasets for training machine learning\nmodels. In this paper, we propose a deep learning data\ncollection technique and introduce Covers and Hummings\nAligned Dataset (CHAD), a novel dataset that contains 18\nhours of short music fragments, paired with time-aligned\nhummed versions. To expand our dataset, we employ a\nsemi-supervised model training pipeline that leverages the\nQbH task as a specialized case of cover song identiﬁca-\ntion (CSI) task. Starting with a model trained on the initial\ndataset, we iteratively collect groups of fragments of cover\nversions of the same song and retrain the model on the ex-\ntended data. Using this pipeline, we collect over 308 hours\nof additional music fragments, paired with time-aligned\ncover versions. The ﬁnal model is successfully applied to\nthe QbH task and achieves competitive results on bench-\nmark datasets. Our study shows that the proposed dataset\nand training pipeline can effectively facilitate the imple-\nmentation of QbH systems.\n1. INTRODUCTION\nQuery-by-Humming (QbH) is a well-known task in Music\nInformation Retrieval. It aims to enable users to ﬁnd a par-\nticular song within a retrieval system by providing a small\naudio segment of their voice or humming as a query. Such\nsystems rely on a large database of songs and display the\nmost similar matches to the user’s query.\nOne signiﬁcant beneﬁt of the QbH system compared to\nother music search systems [1] is that users do not have to\nplay a copy of the song or recall its lyrics. Instead, they can\nhum or sing the melody of the desired song, and the sys-\n© A. Amatov, D. Lamanov, M. Titov, I. V ovk, I. Makarov,\nM. Kudinov. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: A. Amatov, D. Lamanov,\nM. Titov, I. V ovk, I. Makarov, M. Kudinov, “A Semi-Supervised Deep\nLearning Approach to Dataset Collection for Query-by-Humming Task”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.tem will use advanced audio processing and deep learning\ntechniques to locate it.\nA similar task to QbH is Cover Song Identiﬁcation\n(CSI) task [2–4]. CSI aims to identify cover songs per-\nformed by different artists as versions of original songs\nwithin a music database. Although CSI systems often rely\non neural networks, traditional QbH systems mainly uti-\nlize audio processing and music information retrieval tech-\nniques like pitch estimation, note extraction, and time se-\nries matching [5–7]. The main reason QbH lacks deep\nlearning models is the absence of large datasets for train-\ning. This is primarily due to the high cost and limited avail-\nability of humming/singing data for QbH compared to CSI,\nwhere multiple versions of the same song are sufﬁcient\nfor training. Additionally, QbH requires the alignment of\nhumming/singing fragments with the original versions of\nthe song.\nTo overcome the challenges of limited data, we propose\na novel dataset CHAD - Covers and Hummings Aligned\nDataset. This dataset contains groups of time-aligned mu-\nsic fragments, primarily consisting of vocal segments from\npopular songs. Time alignment is the process of syn-\nchronizing a fragment from a humming or cover version\nof a song with its corresponding fragment from the orig-\ninal version to have the same temporal structure. The\ngroups are separated into two categories: one with hum-\nming fragments collected via crowdsourcing and another\nwith cover fragments collected using a semi-supervised\ntraining pipeline. We use this dataset to train our deep\nlearning model for matching audio fragments with simi-\nlar melodies using metric learning paradigm. We demon-\nstrate that these techniques can also be successfully applied\nin the QbH task, achieving results comparable to the best\nperforming scores on popular QbH benchmarks. Further-\nmore, we evaluate our model’s performance on a large in-\nternal song database, showing its ability to generalize to a\nwider range of songs.\nThe paper is structured as follows. Section 2 brieﬂy\nreviews existing approaches to the QbH task. Section 3\ndescribes the proposed deep learning model and training\nmethod for the QbH task. Section 4 outlines the dataset\nand semi-supervised data collection pipeline. Section 5\ndescribes the experiments conducted on public and private\ndata. Finally, Section 6 concludes the paper.6492. RELATED WORK\nQbH systems typically have two components: audio tran-\nscription and search modules. Many approaches in QbH\nresearch have focused on designing effective representa-\ntions of hummings that can be easily matched with MIDI\ntargets. Some standard methods include using Hidden\nMarkov Models [5] to transcribe hummings into a se-\nquence of symbols, discretizing fundamental frequency\ninto semitones [6], and transcribing hummings into a note-\nlike structure using pitch, interval, and duration features\n[7].\nOnce the humming has been transcribed into a for-\nmat that can be compared to database entries, the search\nmodule is responsible for ﬁnding the most relevant songs.\nDynamic Time Warping [6, 8] has been a popular algo-\nrithm for comparing the humming query to MIDI-audio\nentries in the database. This algorithm ﬁnds the minimum\npath between the discretized humming and the MIDI-audio\ndatabase. Another approach, top-down melody match-\ning [9], involves dynamically aligning the humming query\nwith a song from the database. A third approach, pro-\ngressive ﬁltering [10], involves multiple stages of song\nrecognition with increasingly complex recognition mecha-\nnisms. These algorithms serve to match humming queries\nwith songs in a database effectively. In the approach [11],\nauthors use melody extraction network to extract robust\nfeatures from audios and match them with songs from\ndatabase using an ensemble of melody matching algo-\nrithms.\nIn contrast to QbH, the latest research on the Cover\nSong Identiﬁcation (CSI) task has been focused on deep\nlearning-based techniques. A popular approach in CSI is to\nuse deep neural networks for audio representation and met-\nric learning for similarity search. In [12], the authors use\nConstant-Q Transform (CQT) of audio and train a mod-\niﬁed version of ResNet with two losses - triplet loss for\nintra-class compactness and classiﬁcation loss for inter-\nclass discrimination. In the next study, [13], the authors\nimprove results by integrating the PCA module into the\nfully-connected layer of ResNet. Metric learning is widely\nused in CSI. It is shown that different model and loss ar-\nchitectures like Siamese Network [3], triplet loss [12], and\ncontrastive loss [4] can produce competitive results.\nIn [2], the authors use VGG on CQT features with vari-\nable length to tackle the problem of tempo changes of the\ncover songs. In [14], the authors use an audio signal’s Mel-\nFrequency Cepstrum Coefﬁcients (MFCC) as the represen-\ntation. They build cross-similarity matrices between songs\nand collect the nearest neighbors of each song based on\nthese matrices.\nSeveral datasets are available for CSI tasks [15–18].\nThese datasets contain audio features alongside music\nmetadata and provide researchers with a way to evaluate\nand validate their models without collecting large amounts\nof audio data.\n extraction\nVocal extraction model\nInitial audio waveform\nConvolutional encoder\n projection layer...\n...\n extraction ... ...\nFigure 1 : Audio encoder model. V ocal part is extracted\nfrom the input waveform. Then, either f0orCQT features\nare calculated on the vocal part. Finally, the features are\nprocessed by a convolutional encoder model and, then, the\noutput embeddings are normalized.\n3. MODEL\nOur encoder model, M, is presented in Figure 1 and in-\nspired by [4]. The whole ﬁngerprints extraction pipeline\ncan be described in the following steps:\n1. The ﬁrst step of the encoding process is to extract\nthe vocal part of the audio waveform yusing a pre-\ntrained audio source separation model V(.). The\nmodel is applied only to cover fragments since hum-\nming fragments do not contain any accompaniments.\nWe used Spleeter [19] as a model due to its high-\nspeed performance.\n2. The vocal part of the audio is then sent to the fea-\nture extractor model. In this study, two different ex-\ntracted feature types are used: the ﬁrst is the funda-\nmental frequency ( f0) extracted using the CREPE\nmodel [20], which is considered a robust representa-\ntion of the melody. The second is Constant-Q Trans-\nform (CQT) as its faster alternative. The melody is\ncrucial for search as it contains essential song infor-\nmation while ignoring irrelevant singing person de-\ntails.\n3. The extracted feature matrix is then separated into\noverlapping segments, called analysis windows with\nlengthWand stepH, which are fed separately to a\nconvolutional encoder F(.)ResNet18 [21]. The ﬁ-\nnal layer of the encoder is a L2-normalization layer\nG(.), which normalizes the output of the encoder\nalong the embedding dimension.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n650Aligned fragments\nextraction\nalgorithmOriginal\nfragmentHumming\nfragments\nTrain model \non initial dataOriginal song\nOriginal\nfragmentsCover\nfragments\nRetrain model \non a batch of new\ndataUpdate modelCover songs\nFigure 2 : Semi-supervised training and data collection\npipeline used to train the initial model, and iteratively\ngather new aligned audio fragments and retrain the model.\n4. The output ﬁngerprints Z={zi}i=1...T, whereTis\nthe total number of ﬁngerprints for a waveform and\n128 is its dimension size.\nWe use the metric learning method similar to [4] as\na learning framework. To form a batch of audio frag-\nments for training, we randomly sample Kgroups of time-\naligned audio fragments. By group , we refer to a collection\nof original song and humming/singing fragments. Then,\nwe select nrandom audio fragments from each group and\nextract a random analysis window of size W. Since our\ndata is aligned, all windows from each group will repre-\nsent the variations of the same data. Afterward, we apply\nour model and extract in total N=K·nﬁngerprints for\nnin each group.\nOur loss is deﬁned as follows:\nℓ=−K/summationdisplay\nk=1/summationdisplay\nzi\nk,zj\nk∈Zklogexp(sim(zi\nk,zj\nk)\nτ)\n/summationtext\nzl̸∈Zkexp(sim(zi\nk,zl)\nτ),(1)\nwhereZk={z0\nk,...zn−1\nk}is the group of ﬁngerprints, zi\nk\nandzj\nkare different ﬁngerprints from Zk,zl̸∈Zkstands\nfor all ﬁngerprints not in a given group k,sim(zi,zj) =\nzT\nizjis the similarity function, and τis a temperature pa-\nrameter. The ﬁnal loss is computed across all possible pos-\nitive pairs and averaged afterward.\n4. DATASET\nThis section describes the process of collecting a dataset\nfor the QbH task, its statistics, and its limitations.\n4.1 Semi-supervised pipeline\nFigure 2 presents our proposed semi-supervised pipeline.\nThe dataset used in this study is structurally divided into\ntwo parts: HandC. The ﬁrst part, H, consists of orig-\ninal music fragments fopaired with time-aligned hum-\nming/singing fragments fh, making groups Fh. Thefo\nfragments are represented by various vocal and instrumen-\ntal parts of music clips. The fhfragments were collectedAlgorithm 1: Aligned fragments extraction algo-\nrithm of data collection pipeline.\nInput : yo- original song;\nYc- set of cover songs;\ndmin - fragment’s minimal length;\ndmax - fragment’s maximal length;\nDp- set of pause lengths;\nLdb- set of dB levels;\nαcorr - threshold value to exclude same fragments;\nOutput: F- set of groups of aligned fragments from original\nand cover songs\n1Fo,Fo\nprev←{},{};\n2M←initialize_model (.);\n3rms←rms(yo);\n4foreachdp∈Dpdo\n5 foreachldb∈Ldbdo\n6 msilence←ﬁnd_silence_mask (rms,ldb);\n7 Fo←split_by_silence (yo,msilence);\n8 Fo←merge_fragments (Fo,dp,dmin,dmax);\n9 Fo\nemb←M(Fo);\n10Acorr←build_correlation_matrix (Fo\nemb);\n11 Fo←ﬁnd_unique_fragments (Fo,Acorr,αcorr);\n12 Fo,Fo\nprev←max(Fo,Fo\nprev);\n13 end\n14end\n15F←{} ;\n16foreachfo∈Fodo\n17 foreachyc∈Ycdo\n18 fo\nemb←M(fo);\n19 yc\nemb←M(yc);\n20 Fc←cross_correlation( yc,yc\nemb,fo\nemb);\n21 Fc←ﬁlter(Fc,βrel,βirrel );\n22 F←F∪(fo,Fc)\n23 end\n24end\nusing a crowdsourcing service Yandex.Toloka. The sec-\nond part of the dataset, C, was created by collecting the\n100 most popular songs from the Billboard Charts for each\nyear from 1960 to 2020. For each song, up to 10 cover ver-\nsions were retrieved from YouTube top results using query\n\"{song name} {artist name} cover\".\nBecause Halready has groups of time-aligned frag-\nments, we can train the initial encoder model M0with\nthis data. However, Conly contains groups of full song\nversions instead of time-aligned fragments, so extracting\nfragments from these groups is necessary. We propose\nAlgorithm 1 for this task. This algorithm is designed to\nextract the maximum amount of unique fragments from\nthe original versions of the songs and ﬁnd the correspond-\ning aligned fragments from cover versions of the songs in\nC. The algorithm can be described in three stages:\nInitialization stage\n1. As input, the algorithm takes the vocal part of the\noriginal song yoand a group of cover songs Yc. Ad-\nditionally, the algorithm takes the minimal and max-\nimal length of the fragment dminanddmax, respec-\ntively, the set of dB levels Ldbby which to count the\nregion in song as silent or non-silent, the set of max-\nimal pause lengths Dpbetween adjacent fragments\nin a song separated by silence to be considered as\none fragment, and threshold values αcorr,βrel, and\nβirrel to exclude unwanted fragments from the out-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n651put set.\n2. Initialize empty sets of unique fragments Foand\nFo\nprev, the encoder model M, andrms of the wave-\nformyo(lines 1-3).\nFragmentation stage\n1. To ﬁnd the best combination of DpandLdbto yield\nFoof maximal size, start two loops by iterating over\nthese sets (lines 4-5).\n2. Compute the binary mask of non-silent regions\nmsilence usingrms andldb∈Ldband ﬁnd a set\nof initial fragments by splitting the waveform yous-\ning this mask. Then, merge adjacent fragments, the\npause between which is less than dp∈Dp. Addi-\ntionally, the length of such fragments should satisfy\nthe condition dmin<|f|< dmax,f∈Fo. (lines\n6-8).\n3. Apply model Mito the found fragments and extract\nthe ﬁngerprints. Then, build the correlation matrix\nAcorrbased on the fragments’ ﬁngerprints Fo\nemband\nexclude the ones with a correlation higher than the\nthreshold αcorr. We used the maximum of cross-\ncorrelation function to measure the correlation of\nﬁngerprints with different lengths (lines 9-11).\n4. Find the parameters of dB levels and pause lengths\nthat yield the maximum amount of unique fragments\n(line 12).\nMatching stage\n1. Once the unique fragments from the original version\nof the song Foare extracted, initialize the empty set\nFto be ﬁlled with groups of the time-aligned frag-\nments from original and cover songs and iterate over\neach found original fragment fo∈Foand each\ncover song yc∈Yc(lines 15-17).\n2. Extract ﬁngerprints from original fragment fo\nemband\ncover song yc\nembusingM. Search for the same frag-\nments in the cover song using a cross-correlation\nfunction and peak detection algorithm (lines 18-19).\n3. Filter out noise cover fragments by establishing two\nthresholds:\n(a) The cover fragments with correlation above\nβrelare considered relevant, indicating a high\nlevel of certainty that the content of the cover\nfragment is similar to that of the original frag-\nment.\n(b) The cover fragments with correlation below\nβirrel are considered irrelevant fragments and\nare excluded. Fragments with a correlation be-\ntween these two thresholds are counted as un-\ncertain and require double-checking via addi-\ntional crowdsourcing.Save the gathered groups of aligned fragments (lines\n20-22).\nWe apply this pipeline to song batches of C, which gen-\nerates new groups of aligned data. These groups are then\nadded to H, and the model, M, is retrained on the newly\ngathered data. In such a way, we ﬁrst train M0on ini-\ntial humming data, then iteratively update our model from\nMi→Mi+1and ﬁll our dataset with new data.\nFor the unique fragments extraction algorithm, we set\ndmin= 8,dmax= 20 ,Dp={0.5,1,1.5}seconds,Ldb=\n{52,56,60,64,68}dB,αcorr= 0.8. When searching for\nfragments in cover versions of songs, we set the optimal\nthresholds to βrel= 0.5andβirrel= 0.3. All fragments\nfrom the same group have equal duration to retain the time-\nalignment consistency.\n4.2 Statistics\nWe call the collected dataset Covers and Hummings\nAligned Dataset (CHAD). Here are the dataset’s statistics:\n• CHAD contains 5494 original songs, 31630 cover\nsongs, and 5164 hummings fragments.\n• The total number of audio fragments is\n81781 , which amounts to over 270 hours of\nsinging/humming audio fragments and 51hours of\noriginal song fragments. The group size varies from\n2 to 31, with an average size of 6 fragments.\n• InH, the duration of the fragments ranges from 4to\n20seconds, with a mean of 11.06±2.67seconds,\nand a total for original fragments - 2.12 hours, and\nfor humming fragments - 15.83 hours.\n• InC, the duration ranges from 8to20seconds, with\na mean of 14.66±2.03seconds, and a total for orig-\ninal fragments - 49.54 hours, and cover fragments -\n259.03 hours, where 194.53 hours are for fragments\nwith correlation above βrel, and 64.50 hours are for\nfragments with correlation between βrelandβirrel.\n• Additionally, the metadata is collected. It includes\nYouTube video ID, title, author, correlation value,\nand whether the fragment is double-checked. The\ndataset’s audio IDs, metadata, start and end times-\ntamps and data download script are available in our\nGitHub repository1.\n4.3 Limitations\nHowever, our semi-supervised pipeline has some limita-\ntions. First, it can only extract vocal data, and the algo-\nrithm needs modiﬁcation to extract instrumental segments.\nSecond, the number of covers is limited, as there are usu-\nally fewer cover versions for non-popular songs. Lastly,\nthere will still be some noisy unrelated fragments in the\nﬁnal set due to the automatic validation threshold. Future\nresearch could explore using generative networks [22] to\novercome these limitations.\n1https://github.com/amanteur/CHADProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6525. EXPERIMENTS\n5.1 Experimental setup\nInput features . We used CREPE [20] activations as f0\nfeatures, yielding output features with a size of (360,T).\nWe further enhanced the robustness of the melody fea-\nture by trimming it to include only 3octaves around its\nmean pitch, following the approach used in [25]. Addition-\nally, we downscaled this representation to the size (80,T\n4).\nHowever, we encountered issues with the slow speed of\nthe melody extraction model during evaluation, rendering\nthe overall approach unscalable. To address this, we incor-\nporatedCQT features into our model, extracted with the\nfollowing parameters: 12bins per octave with a total of\n7octaves, Hann window, hop length 512, and a sampling\nrate of16kHz.\nAugmentations . We found an optimal set of augmen-\ntations to every batch of waveform fragments, which in-\ncluded continuous pitch shifting (with a shift range of -4.0\nto 4.0 semitones and probability of 0.5), time stretching\n(with a stretch rate range of 0.8 to 1.25 and probability\nof 0.8), SpliceOut [26] (with 10 random intervals of 500\nframes and probability of 0.8), mixing with other audio\nsamples in the batch (with an SNR range of 5 to 10 dB and\nprobability of 0.8), and adding background noises (with an\nSNR range of 3 to 30 dB and probability of 0.8).\nModel . We discovered that as the length of a hummed\nor sung recording increases, the tempo/rhythm becomes\nmore mismatched from the original song. So we trained\ntwo models with different analysis window lengths ( W)\nand hop sizes ( S):Mshort for shorter recordings (up to 15\nsec) with W=3sec andS=0.25sec, andMlong for longer\nrecordings with W=8sec andS=0.64sec. Both models\nused a vanilla ResNet18 encoder model, with output em-\nbeddings of size (128,T), whereTis the number of ﬁn-\ngerprints.\nTraining setup . We trained the encoder model using the\nADAM optimizer, with a learning rate of lr= 0.001and a\nbatch size of 32for100epochs. We used the NT-Xent Loss\n[27] with a temperature of t= 0.05. We employed the\nMulti-similarity miner [28] and an adaptive batch sampler\nto improve convergence speed. The batch sampler selects\nup to4fragments with a random starting point for each\nfragments group. We trained the model under two settings:\nonly on the Cpart and on both C+Hparts of CHAD.\nModels were trained on 1 NVIDIA GeForce RTX 2080 Ti\n12 Gb.\nTo evaluate the performance of our model, we con-\nducted a series of experiments, which involved:\n• Experiments on the MIREX QbH datasets [29],\nspeciﬁcally the Roger Jang and ThinkIt datasets,\nwhere MIDI recordings were used as references.\nThe MIR-QBSH corpus of Roger Jang consists of\n4431 query hummings and 48 original MIDI ﬁles,\nwhile the Thinkit corpus contains 355 queries and\n106 original MIDI ﬁles. The song database was con-\nstructed according to MIREX QbH Challenge stan-\ndards, with 2600 MIDI ﬁles. These experimentsaimed to ﬁnd the ground-truth MIDI by a given\nquery humming.\n• Experiments on MIREX QbH datasets according to\nSubtask 2 testing protocol in MIREX evaluation sys-\ntem. In this protocol, queries are also considered as\n\"versions\" of ground truth, and the objective is to re-\ntrieve all variants related to a searched ground truth\nby given query humming.\n• Experiments on a dataset of real recordings, which\nincluded MIREX Roger Jang Dataset with all MIDI\nﬁles replaced with real recordings extracted from\nYouTube videos (Jang Real), and MTG-QbH [24]\ndataset with 118queries and 118original songs. The\nadditional database comprises 1886 random songs\nfrom the internal dataset to serve as imposter songs.\n• Experiments on a large-scale internal database\n(DB90K) containing more than 90k real song\nrecordings. For this experiment, we used two types\nof queries: 126humming fragments for 126songs\ncollected by our team as search-by-humming setup\nand2000 singing fragments from karaoke record-\nings gathered from the DAMP-VPB dataset [30] as\nsearch-by-singing setup. In the latter case, we se-\nlected5of16original songs and their sung perfor-\nmances and manually split them into fragments.\nFor all real recordings, we extract the vocal part before-\nhand. Also, we ensure that CHAD does not contain any\nsongs that are also present in the evaluation datasets. This\nwas achieved by excluding such songs from the training\nset.\nRetrieval . We use two variants of sequence matching\nmethods at the retrieval phase: maximum Pearson correla-\ntion coefﬁcient (Corr) or Dynamic Time Warping (DTW)\n[31]. For the large-scale experiment on DB90K, we use a\ntwo-step search procedure with a ﬁrst step of fast retrieval\nof preliminary candidates using the FAISS Approximate\nNearest Neighbors (ANN) algorithm with Euclidean dis-\ntance followed by a second step of reranking. After further\nanalysis, we discovered that Euclidean distance and Cosine\ndistance yielded similar results. To maintain simplicity, we\nchose to use Euclidean distance. The ANN search returns\nthe top5000 candidates, which are reranked based on the\nPearson correlation score.\nMetrics . We follow the MIREX evaluation protocols [29]\nfor the QbH task and compute the mean of the Top- nhit\nrate for every humming/singing fragment. There was only\none related song in the database for every query fragment.\n5.2 Results\nWe compare our model Mshort trained on C+Hwith 2\nbest performing methods according to the latest available\nresult of MIREX QbH Challenge [32]. The ﬁrst one [8]\nis based on f0-matching technique. The second one is a\nproprietary method for which only scores were reported in\nthe leaderboard. We use only one of our models ( Mshort\non CREPE and CQT features) in this experiment as mostProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n653MethodTop-10hit rate↑\nJang [23] Thinkit Subtask 2 Jang Real MTG-QBH [24]\nOursmetric learning(CREPE) 0.921 0.966 0.959 0.868 0.883\nmetric learning( CQT ) 0.840 0.786 0.866 0.867 0.747\nStasiak [8] f0-matching 0.948 0.907 0.968 - -\nACRCloud proprietary 0.990 0.986 0.972 - -\nTable 1 : Evaluation of model Mshort trained on dataset C+Hwith two types of features - CREPE and CQT . Evaluation\nis provided on MIREX datasets - Jang, Thinkit, and Subtask 2, and datasets - Jang Real, and MTG-QBH, which are more\napplicable to real-world scenarios.\nquery fragments are shorter than 16 seconds. We use DTW\nfor matching feature sequences on MIDI-based datasets\nand Corr for non-MIDI datasets as we found that corre-\nlation coefﬁcient gives performance improvement on real\ndata.\nThe results are summarized in Table 1. Our model\ndemonstrates competitive though slightly inferior perfor-\nmance on the given benchmarks. On real (non-MIDI) data\nour implementation of [8] produced near-random results\nwhich can be explained by the difﬁculty of tracking and\nmatching f0in real music recordings. Also, we see that\nwhile using CQT features led to a performance drop, it is\nnot prohibitively large so CQT features can be used when\ncomputing f0is infeasible.\nTable 2 shows the scalability of our approach in the ex-\nperiment with DB90k. We do not track the top-1 hit rate as\nthe database contains several versions of the same song.\nTable 2a reports the results of the search-by-humming\nsetup. We used Mshort ,Mlong, and their combination\nmodelMfused , which worked on a simple rule: Mshort\nwas used for hummings shorter than 15 seconds, while\nMlong was used otherwise. All presented models are\ntrained with CQT features. We observed that Mfused\nworked better than Mshort andMlongseparately in all sce-\nnarios. Comparing models trained on CandC+H, the\naccuracy gap suggests that training on real humming data\nis crucial for search-by-humming setup. In Table 2b, we\nreport our results for search-by-singing setup with Mfused\non DAMP-VPB. Our model, trained on both CandC+H,\nretrieves the correct songs with high precision, with no per-\nformance drop observed for the model trained on Calone,\ndue to the dominance of sung fragments in our training\ndataset.\nAdditionally, we evaluate the retrieval speed of our\nmodelsMshort andMlong on DB90k, as shown in Ta-\nble 3. We ﬁnd that Mlong performs better than Mshort\nin both search steps (ANN and Reranking) due to its abil-\nity to process longer humming recordings and thus require\nless processing of fragments. Our results demonstrate the\nscalability and efﬁciency of our search system in efﬁciently\nachieving high-precision results.\n6. CONCLUSIONS\nIn this paper, we propose a novel dataset CHAD alongside\na semi-supervised data collection and training pipeline forPartition ModelTop-nhit rate↑\n100 10 5 3\nCMshort 0.643 0.548 0.524 0.476\nMlong 0.412 0.277 0.270 0.262\nMfused 0.759 0.621 0.603 0.517\nC+HMshort 0.659 0.595 0.571 0.484\nMlong 0.595 0.508 0.413 0.389\nMfused 0.776 0.707 0.691 0.586\n(a) Results on humming queries.\nPartition ModelTop-nhit rate↑\n100 10 5 3\nCMfused0.931 0.904 0.885 0.865\nC+H 0.923 0.899 0.885 0.856\n(b) Results on singing queries.\nTable 2 : Evaluation on DB90K with humming and singing\nfragments using models Mshort ,Mlong, and their fusion\nmodelMfused trained on CandC+HwithCQT features.\nModelSearch step, s\nANN Reranking\nMshort 1.41±0.57 5.37 ±0.87\nMlong 0.52±0.11 2.39 ±0.43\nTable 3 : Query search speed on DB90K using models\nMshort andMlong trained on C+HwithCQT features\nusing 32 CPU.\na Query-by-Humming system. We show that cover songs\ncould be used to train query-by-humming models with\ncompetitive performance. Although the model trained on\nopen data performs well on sung queries, the pure search-\nby-humming setup requires adding a portion of real hum-\nming data into the training set for acceptable performance.\nThe main disadvantage of the proposed approach is that\nit cannot be used for searching instrumental tracks. One\npossible solution to this problem would lie in the ﬁeld of\ndominant melody extraction and generative networks and\nis left for future research.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6547. REFERENCES\n[1] A. Wang, “An industrial strength audio search algo-\nrithm,” in ISMIR , 2003.\n[2] Z. Yu, X. Xu, X. Chen, and D. Yang, “Learn-\ning a representation for cover song identiﬁca-\ntion using convolutional neural network,” CoRR ,\nvol. abs/1911.00334, 2019. [Online]. Available:\nhttp://arxiv.org/abs/1911.00334\n[3] G. Doras and G. Peeters, “A prototypical triplet loss for\ncover detection,” CoRR , vol. abs/1910.09862, 2019.\n[Online]. Available: http://arxiv.org/abs/1910.09862\n[4] S. Chang, D. Lee, J. Park, H. Lim, K. Lee, K. Ko,\nand Y . Han, “Neural audio ﬁngerprint for high-\nspeciﬁc audio retrieval based on contrastive learning,”\nCoRR , vol. abs/2010.11910, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.11910\n[5] E. Unal, E. Chew, P. G. Georgiou, and S. S. Narayanan,\n“Challenging uncertainty in query by humming sys-\ntems: A ﬁngerprinting approach,” IEEE Transactions\non Audio, Speech, and Language Processing , vol. 16,\nno. 2, pp. 359–371, 2008.\n[6] R. A. Putri and D. P. Lestari, “Music information re-\ntrieval using query-by-humming based on the dynamic\ntime warping,” in 2015 International Conference on\nElectrical Engineering and Informatics (ICEEI) , 2015,\npp. 65–70.\n[7] L. Lu, H. You, and H.-J. Zhang, “A new approach to\nquery by humming in music retrieval,” in IEEE Inter-\nnational Conference on Multimedia and Expo, 2001.\nICME 2001. , 2001, pp. 595–598.\n[8] B. Stasiak, “Follow that tune – adaptive approach\nto dtw-based query-by-humming system,” Archives of\nAcoustics , vol. 39, pp. 467 –, 01 2014.\n[9] X. Wu, M. Li, J. Liu, J. Yang, and Y . Yan, “A top-down\napproach to melody match in pitch contour for query\nby humming,” 2006.\n[10] J.-S. R. Jang and H.-R. Lee, “A general framework\nof progressive ﬁltering and its application to query\nby singing/humming,” IEEE Transactions on Audio,\nSpeech, and Language Processing , vol. 16, no. 2, pp.\n350–358, 2008.\n[11] M. Ulﬁ and R. Mandala, “Improving query by hum-\nming system using frequency-temporal attention net-\nwork and partial query matching,” in 2022 9th Interna-\ntional Conference on Advanced Informatics: Concepts,\nTheory and Applications (ICAICTA) , 2022, pp. 1–6.\n[12] X. Du, Z. Yu, B. Zhu, X. Chen, and Z. Ma,\n“Bytecover: Cover song identiﬁcation via multi-loss\ntraining,” CoRR , vol. abs/2010.14022, 2020. [Online].\nAvailable: https://arxiv.org/abs/2010.14022[13] X. Du, K. Chen, Z. Wang, B. Zhu, and Z. Ma, “Byte-\ncover2: Towards dimensionality reduction of latent\nembedding for efﬁcient cover song identiﬁcation,” in\nICASSP 2022 - 2022 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2022, pp. 616–620.\n[14] C. J. Tralie and P. Bendich, “Cover song iden-\ntiﬁcation with timbral shape sequences,” CoRR ,\nvol. abs/1507.05143, 2015. [Online]. Available:\nhttp://arxiv.org/abs/1507.05143\n[15] D. P. Ellis and G. E. Poliner, “Identifying ‘cover songs’\nwith chroma features and dynamic programming beat\ntracking,” in 2007 IEEE International Conference on\nAcoustics, Speech and Signal Processing - ICASSP\n’07, vol. 4, 2007, pp. IV–1429–IV–1432.\n[16] T. Bertin-Mahieux and D. Ellis, “Large-scale cover\nsong recognition using hashed chroma landmarks,” in\nProceedings of IEEE WASPAA . New Platz, NY: IEEE\nWorkshop on Applications of Signal Processing to Au-\ndio and Acoustics (WASPAA), 2011.\n[17] Y . Bayle, L. Maršík, M. Rusek, M. Robine, P. Hanna,\nK. Slaninová, J. Martinovic, and J. Pokorný, “Kara1k:\nA karaoke dataset for cover song identiﬁcation and\nsinging voice analysis,” in 2017 IEEE International\nSymposium on Multimedia (ISM) , 2017, pp. 177–184.\n[18] F. Yesiler, C. J. Tralie, A. A. Correya, D. F. Silva,\nP. Tovstogan, E. Gómez, and X. Serra, “Da-tacos: A\ndataset for cover song identiﬁcation and understand-\ning,” in ISMIR , 2019.\n[19] R. Hennequin, A. Khlif, F. V oituret, and M. Mous-\nsallam, “Spleeter: a fast and efﬁcient music source\nseparation tool with pre-trained models,” Journal\nof Open Source Software , vol. 5, no. 50, p.\n2154, 2020, deezer Research. [Online]. Available:\nhttps://doi.org/10.21105/joss.02154\n[20] J. W. Kim, J. Salamon, P. Li, and J. P. Bello, “Crepe:\nA convolutional representation for pitch estimation,”\n2018. [Online]. Available: https://arxiv.org/abs/1802.\n06182\n[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep\nresidual learning for image recognition,” CoRR , vol.\nabs/1512.03385, 2015. [Online]. Available: http:\n//arxiv.org/abs/1512.03385\n[22] C. Frank, “The machine learning behind hum\nto search,” https://ai.googleblog.com/2020/11/\nthe-machine-learning-behind-hum-to.html, accessed:\n2022-10-23.\n[23] J.-S. R. Jang, “Qbsh: A corpus for designing\nqbsh (query by singing/humming) systems.” [Online].\nAvailable: http://www.cs.nthu.edu.tw/~jangProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n655[24] J. Salamon, J. Serrà, and E. Gómez, “Tonal represen-\ntations for music retrieval: From version identiﬁca-\ntion to query-by-humming,” International Journal of\nMultimedia Information Retrieval, special issue on Hy-\nbrid Music Information Retrieval , vol. 2, pp. 45–58, 03\n2013.\n[25] G. Doras and G. Peeters, “Cover detection\nusing dominant melody embeddings,” CoRR ,\nvol. abs/1907.01824, 2019. [Online]. Available:\nhttp://arxiv.org/abs/1907.01824\n[26] A. Jain, P. R. Samala, D. Mittal, P. Jyothi, and\nM. Singh, “Spliceout: A simple and efﬁcient audio\naugmentation method,” CoRR , vol. abs/2110.00046,\n2021. [Online]. Available: https://arxiv.org/abs/2110.\n00046\n[27] K. Sohn, “Improved deep metric learning with\nmulti-class n-pair loss objective,” in Advances in\nNeural Information Processing Systems , D. Lee,\nM. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett,\nEds., vol. 29. Curran Associates, Inc., 2016. [Online].\nAvailable: https://proceedings.neurips.cc/paper/2016/\nﬁle/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf\n[28] X. Wang, X. Han, W. Huang, D. Dong, and M. R. Scott,\n“Multi-similarity loss with general pair weighting for\ndeep metric learning,” 2019.\n[29] “MIREX QBSH challenge results,” https:\n//www.music-ir.org/mirex/wiki/2021:Query_by_\nSinging/Humming, accessed: 2022-10-23.\n[30] I. Smule, “DAMP-VPB: Digital Archive of Mobile\nPerformances - Smule V ocal Performances Balanced,”\nNov. 2017. [Online]. Available: https://doi.org/10.\n5281/zenodo.2616690\n[31] M. Müller, “Dynamic time warping,” Information Re-\ntrieval for Music and Motion , vol. 2, pp. 69–84, 01\n2007.\n[32] “MIREX QBSH 2016 results,” https://www.music-ir.\norg/mirex/wiki/2016:MIREX2016_Results, accessed:\n2022-10-23.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n656"
    },
    {
        "title": "The Coordinated Corpus of Popular Musics (CoCoPops): A Meta-Corpus of Melodic and Harmonic Transcriptions.",
        "author": [
            "Claire Arthur",
            "Nathaniel Condit-Schultz"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265267",
        "url": "https://doi.org/10.5281/zenodo.10265267",
        "ee": "https://zenodo.org/records/10265267/files/000027.pdf",
        "abstract": "This paper introduces a new corpus, CoCoPops: The Coordinated Corpus of Popular Musics. The corpus can be considered a \"meta corpus\" in that it both extends and combines two existing corpora—the widely-used McGill Bill-\nboard corpus the and RS200 corpus. Both the McGill Billboard corpus and the RS200 contain expert harmonic annotations using different encoding schemes and each\nrepresent harmony in fundamentally different ways: Billboard using a root-quality representation and the RS200 using Roman numerals. By combining these corpora\ninto a unified format, using the well-known **kern and**harm representations, we aim to facilitate research in computational musicology, which is frequently burdened\nby corpora spread across multiple encoding formats. The format will also facilitate cross-corpus comparison with the large body of existing works in **kern format. For a\n100-song subset of the CoCoPops-Billboard collection, we also provide participant ratings of continuous valence and arousal ratings, along with the RMS (Root Mean Square) signal level and associated timestamps. In this paper we describe the corpus and the procedures used to create it.",
        "zenodo_id": 10265267,
        "dblp_key": "conf/ismir/ArthurC23",
        "keywords": [
            "CoCoPops",
            "Meta corpus",
            "McGill Billboard corpus",
            "RS200 corpus",
            "Harmony annotations",
            "Encoding schemes",
            "Unified format",
            "Computational musicology",
            "Cross-corpus comparison",
            "Valence and arousal ratings"
        ],
        "content": "THE COORDINATED CORPUS OF POPULAR MUSICS (COCOPOPS): A\nMETA-CORPUS OF MELODIC AND HARMONIC TRANSCRIPTIONS\nClaire Arthur\nGeorgia Institute of Technology\nSchool of Music\nclaire.arthur@gatech.eduNathaniel Condit-Schultz\nGeorgia Institute of Technology\nSchool of Music\nnatcs@gatech.edu\nABSTRACT\nThis paper introduces a new corpus, CoCoPops: The Co-\nordinated Corpus of Popular Musics. The corpus can be\nconsidered a “meta corpus” in that it both extends and com-\nbines two existing corpora—the widely-used McGill Bill-\nboard corpus the and RS200 corpus. Both the McGill Bill-\nboard corpus and the RS200 contain expert harmonic an-\nnotations using different encoding schemes and each rep-\nresent harmony in fundamentally different ways: Billboard\nusing a root-quality representation and the RS200 using\nRoman numerals. By combining these corpora into a uni-\nﬁed format, using the well-known **kern and**harm\nrepresentations, we aim to facilitate research in computa-\ntional musicology, which is frequently burdened by cor-\npora spread across multiple encoding formats. The format\nwill also facilitate cross-corpus comparison with the large\nbody of existing works in **kern format. For a 100-song\nsubset of the CoCoPops-Billboard collection, we also pro-\nvide participant ratings of continuous valence and arousal\nratings, along with the RMS (Root Mean Square) signal\nlevel and associated timestamps. In this paper we describe\nthe corpus and the procedures used to create it.\n1. INTRODUCTION\nIn 2011, Burgoyne et al. [1] introduced a dataset that\nwould have a lasting inﬂuence in the ISMIR community:\nthe McGill Billboard corpus, a set of expert harmonic\nanalyses of commercial pop songs. This dataset—and\nthe Harte [2] standard for encoding chord symbols that\nit adopted—has become a standard in the MIR commu-\nnity, for example, being used as training and testing data\nin the MIREX competition for Audio Chord Estimation\nsince 2008. Around the same time, Trevor de Clercq and\nDavid Temperley independently created another rock mu-\nsic dataset—the RS200 corpus—which would ultimately\nconsist of 200 harmonic andmelodic transcriptions [3, 4];\nThough perhaps less well known in the MIR community,\ntheir corpus has been the basis for several computational\n© C. Arthur and N. Condit-Schultz. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). Attri-\nbution: C. Arthur and N. Condit-Schultz, “The Coordinated Corpus of\nPopular Musics (CoCoPops): A Meta-Corpus of Melodic and Harmonic\nTranscriptions”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.musicology papers [4]. While other datasets of popular-\nstyle music harmony have been released (e.g., Isophon-\nics [2]), the Billboard and RS200 datasets stand out for\ntheir use of experts to encode the annotations, the rigor of\ntheir sampling methodologies, and the detail of their pro-\ncedural documentation.\nThe ﬁeld of computational musicology suffers from\nperennial data scarcity [5]; What few symbolic corpora\nexist are largely biased towards Western classical music\n[6], which is relatively easy to digitize due to its basis\nin notated scores. Unlike classical music, popular music\nmust generally be transcribed from audio recordings, with\nmelody transcription being a particularly time-consuming\ntask. Although more open-source data can be found (e.g.,\ncrowd-sourced arrangements from www.musescore.com)\nand MIR algorithms for tasks such as source separation and\nautomatic transcription are improving, both procedures are\nprone to high levels of error that is undesirable for either\ncomputational music analysis or training machine learning\nmodels [6]. The RS200 is still the only major corpus of\nexpert melodic transcriptions of popular music; the pair-\ning of these melodic transcriptions with harmonic analyses\naffords sophisticated analysis of tonality in popular music.\nIn this paper we present a corpus which extends the Bill-\nboard corpus to include expert-transcribed melodies for a\nsizable subset of the original corpus (214 songs presently).\nBy adding melodic transcriptions to an existing corpus of\nharmonic annotations (the Billboard corpus), we create a\ndataset fully comparable to the RS200. We also trans-\nlate both the Billboard and RS corpora into humdrum data\nformats, creating two comparable datasets which together\nform a super-corpus we call the Coordinated Corpus of\nPopular Music (CoCoPops). In addition to melodic and\nharmonic transcriptions, CoCoPops includes entirely new\nannotations of rhyme schemes in both subcorpora and con-\ntinuous valence and arousal ratings in a 100-song subset.\nLike the When In Rome project [7], CoCoPops aims to fa-\ncilitate musicological and MIR research by making a large\nbody of data available in a consistent, standard format. In\nthe sections that follow, we describe in detail the original\ntwo datasets that CoCoPops is built on, the procedures we\nused to generate new data, and the content of CoCoPops.239Figure 1 . Sample annotation ﬁle from the original McGill\nBillboard corpus (“Honky Tonk Woman,” The Rolling\nStones).\n2. BACKGROUND\n2.1 The McGill Billboard Corpus\nThe McGill Billboard [1] corpus contains annotations of\n7391unique songs, all sampled from the Billboard Hot\n100 charts between 1958 (when Billboard magazine be-\ngan publishing this chart) and 1991. The authors used a\nstratiﬁed sampling procedure to gather as representative a\nsample as possible, sampling a (roughly) equal number of\nsongs from each of three “eras” (60s, 70s, 80s) while also\naccounting for chart position (1–100).\nThe McGill Billboard transcription process involved\na team of more than two dozen people, included “audi-\ntions to identify musicians with sufﬁcient skill to transcribe\nreliably and efﬁciently,” and cost upwards of $20,000\n[1]. The process included creating manual annotations of\nthe chords, formal sections (e.g., verse, chorus), phrases\n(loosely deﬁned), key(s), and meter in each sampled song,\nconducted by two independent annotators. A third “meta-\nannotator” compared the two versions for differences and\ncombined them into a single, ﬁnal transcription.\nThe McGill Billboard chord annotations are encoded\nusing the representation scheme proposed by Harte in 2005\n[8] and later expanded and revised in 2010 [2]. This rep-\nresentation uses a syntax that is common in popular music\nlead sheets, where chords are represented as a root note\nwith a set of intervals above the root, with the most com-\nmon chord types given a list of shorthand symbols (e.g.,\nC:maj ,A:7). The McGill annotations are encoded in\nplain-text ﬁles with line breaks representing new phrases,\neach line tagged with the dominant instrument (or vocals)\nin that phrase. An example of an original ﬁle from the\nMcGill Billboard corpus is shown in Figure 1.2\n2.2 The RS200 Corpus\nThe Rolling Stone corpus was ﬁrst described in a paper\npublished in 2011 [3], initially dubbed the RS5x20 cor-\n1Note that a small subset has been withheld from the public to serve\nas testing data for the MIREX competition.\n2A separate set of mirex text-ﬁles includes only the chords, but with\na timestamp for every chord.pus. This original 100-song corpus (RS5x20) contained\nharmonic annotations of the top 20 songs listed, for each of\nﬁve decades from the 1950s through the 1990s, on Rolling\nStone magazine’s list of the “500 Greatest Songs of All\nTime” (as ﬁrst published in 2004). The corpus was later\nexpanded to 200 songs (the RS200 corpus), and also added\nmelodic transcriptions for each song [4], making it the ﬁrst\npublic corpus of expert melodic transcriptions of popular\nmusic. Since the remaining 400 songs on Rolling Stone’s\nlist were not chronologically balanced, the second set of\n100 songs was chosen based on rank position alone. While\nthe Billboard charts are based on commercial sales, the\nRolling Stone list was based on votes from experts (specif-\nically, “172 rock stars and leading authorities”). Although\none may suspect that these two corpora would substantially\noverlap, in fact there are only ﬁfteen songs in common.\nThe RS200 annotations are spread over multiple sep-\narate ﬁles per song: one with the timestamps, two with\nthe harmonic analyses (one per annotator), another with\nthe melody transcription, and (for an 80-song subset) a\nﬁfth with lyrics. Unlike the Billboard corpus, the RS200\nchords are annotated using Roman numerals; Similarly, the\nmelody transcriptions are encoded as scale-degree annota-\ntions, with direction markers to clarify octave and contour.\nRhythmic durations are not encoded at all, only the timing\nof note onsets: each measure of music is divided into regu-\nlar steps representing metric positions, with notes placed at\nsteps indicating onsets and dots representing empty steps.\nThe number of steps per measure is dynamic, depending\non the meter and the lowest metric position needed to rep-\nresent onsets in that measure. For instance, a measure that\ncontains only one note that arrives on the second half of the\nﬁrst beat (e.g., the “and of 1”) requires division into eighth\nnotes, so that measure will have eight steps with only a\nnote at the fourth step and the rest dots. However, a mea-\nsure with only a single note that lands on the downbeat can\nbe represented with just one token. Sample ﬁles from the\nRS200 corpus can be seen in Figure 2.\n2.3 Related Work\nThe most closely-related work to ours is another extension\nto the McGill Billboard corpus by Christopher White et\nal. [9], which adds timbral and textural annotations to the\nentire Billboard corpus. Annotators of this corpus listened\nto the songs and notated “all moments of change” within\neach track according to three broad categories: the “do-\nmain” of change (such as the instrument group, harmony,\nlyrics, texture, etc.); the “genera” of each change within the\nrelevant domain (such as a change to “solo” within a tex-\nture category); and an “event type” which solely denotes\none of three options: a change, entry, or exit. We intend to\nwork with the authors for a future release of CoCoPops to\nincorporate this textural and timbral information as well.\nA major drawback of both the Billboard and Rolling\nStone samples is their overwhelming bias towards music\nfrom before 1991. Two recent projects have sought to right\nthis imbalance by creating corpora of more modern pop-\nular music to complement the Billboard sample: White etProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n240Figure 2 . Sample annotation ﬁles from the RS200 corpus (“Whole Lotta Love,” Led Zeppelin). The image shows three\nﬁles overlaid on top of each other from left to right: timestamps of each measure, key and chord annotations, and melody\ntranscription.\nal. [10] introduce the “Millenial corpus,” a dataset of expert\nmelodic transcriptions of twenty ﬁve popular songs written\nbetween 2015 and 2019. Beach and Arthur [6] created a\nmuch larger corpus of popular songs with annotations, al-\nthough their annotations are derived algorithmically from\nthe audio, and are quite noisy.\nOur aim to combine two existing corpora into a single,\nhomogeneous dataset is inspired by Mark Gotham’s “when\nin Rome” project [7], which merges and reformats several\nexisting classical corpora with Roman numeral annotations\ninto a single collection in a common format. Our project\nto gather valence and arousal data for the Billboard sam-\nple was similarly inspired by the DEAM dataset: a dataset\ncontaining dynamic annotations of valence and arousal for\n1,809 non-copyrighted (Creative Commons license) songs\nand song excerpts [11]. The majority of these annota-\ntions are of short excerpts ( ≈45s) across numerous mu-\nsical styles (folk, world, jazz, instrumental, pop); however,\nthe dataset also includes ratings for 56 complete songs,\nwhich provide the most valuable information, according\nto the creators [11]. In addition, the quality of the audio\nrecordings (and the musical content) in the DEAM sam-\nple is highly variable, as these recordings do not represent\nprofessionally published works. Our valence- and arousal-\nratings for 100 complete, successful, commercial record-\nings will serve as a useful complement to the DEAM sam-\nple.\n3. CORPUS OVERVIEW\nThe CoCoPops corpus consists of two collections: the Bill-\nboard and RS200 subcorpora. Each collection contains one\nﬁle per song. In the CoCoPops-Billboard collection, all\n739 of the original McGill Billboard ﬁles have an equiv-\nalent humdrum ﬁle. The contents of each ﬁle, however,\nvary: All 739 ﬁles contain all the originally encoded infor-\nmation (chords, keys, formal section labels, timestamps,\nphrase information) from the original McGill dataset, but\nall converted to humdrum format, and with a signiﬁcant\nnumber of corrections (see Section 5). At present, 214 out\nof the 739 ﬁles include new expert melodic and lyric tran-scriptions, as well as an encoding of the rhyme scheme;\n100 of those 214 songs also contain continuous user rat-\nings of valence and arousal, as well as rolling RMS (root\nmean square) amplitude values of the audio, to approxi-\nmate the changing sound level of the music—both sam-\npled at a rate of 2Hz. A sample CoCoPops-Billboard ﬁle\nis shown in Figure 3. In the CoCoPops-RS200 collec-\ntion, each ﬁle contains the information originally spread\nover separate ﬁles—e.g., melody, harmony, time stamps,\nlyrics—in a single humdrum ﬁle. Unlike the original Bill-\nboard annotations which used Harte’s encoding scheme\n(i.e., root +quality), the RS200 were originally annotated\nwith Roman numerals. To facilitate analysis, we provide\nboth types of harmonic annotations in both collections. In\naddition, since the original RS200 contained two indepen-\ndent transcriptions of the harmony, each CoCoPops-RS200\nﬁle includes two Roman numeral annotations (i.e., two\n**harm spines) side-by-side. Eighty of the ﬁles also in-\nclude lyrics and syllable stress information.\nThe humdrum syntax is a plain-text format for repre-\nsenting musical information, organized into tab-delineated\ncolumns—called “spines”—representing different streams\nof data [12] (see www.humdrum.org for more informa-\ntion). Within the general humdrum syntax, various spe-\nciﬁc representation schemes can be deﬁned3: Two of the\nmost common representation schemes include the widely-\nknown**kern representation of pitch information, the\n**silbe representation of lyrics, and the **harm repre-\nsentation of harmonic information in Roman-numeral for-\nmat. Other relevant representations for the present collec-\ntions include **harte —a humdrum representation for\nroot+quality-style harmonic annotations (near-identical to\nthe original annotation scheme used in the McGill Bill-\nboard corpus. This scheme is based on the syntax proposed\nby Chris Harte [2, 8] and the humdrum representation is\ndescribed in Arthur et al. [13]); and **rhyme —a repre-\nsentation for rhyme schemes [14].\nIn the following sections we describe our procedures\nfor gathering new data (e.g., melodic transcriptions), and\n3Chapter 18 of the Humdrum User Guide illustrates how to create new\nhumdrum representations.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n241Figure 3 . Sample ﬁle from the CoCoPops corpus. This ﬁle (“Sweet Nothins,”, Brenda Lee) includes the original McGill\nBillboard information alongside new melody and lyric information. Files in the valence and arousal subset (see Section 6)\ninclude three additional spines.\nhow we converted the preexisting datasets into humdrum\nformats.\n4. MELODY TRANSCRIPTION\nIn the early stages of our project, we worked with four col-\nlaborators4to deﬁne transcription guidelines which could\nbe applied consistently. We elected to transcribe only vo-\ncal parts, with focus on the “lead” vocal melody in each\nsong—however, we agreed to encode important vocal har-\nmonies or other “backing” vocals as needed. The vo-\ncal performances in the sample are often challenging to\ntranscribe, including unpitched or quasi-pitched vocals,\n“blue” notes, glissandi, loose rhythms, and syncopation.\nOur goal was to create readable transcriptions using con-\nventional musical syntax (beat positions, durations, notes)\nrather than mechanical, empirical terms (milliseconds, F0,\netc.). This requires signiﬁcant interpretation and quantiza-\ntion; However, we took care to not over-simplify melodies\nsuch that they became melodic reductions. Our transcrip-\ntions generally interpret rhythms using a 16th-note grid,\nbut triplets and 32nd-notes are used sparingly at slow tem-\npos; Similarly, pitches are encoded in standard western\npitch categories (e.g., C#5, B4), ignoring most glissandi\nand blue notes. However, many vocal performances sim-\nply cannot be faithfully represented in traditional score cat-\negories: as such, we included provisions for indicating, as\nneeded, unpitched or approximate pitch, “free” or approxi-\nmate rhythms, glissandi, and blue notes—the complete de-\ntails of these encodings are documented directly in the Co-\nCoPops repository.\nUltimately, ten individuals contributed to our 214\nmelodic transcriptions: 94 transcriptions by the authors; 40\ntranscriptions by our four early collaborators, all graduate\nstudents in music performance or theory; 10 transcriptions\nby three (paid) undergraduate music students; and 70 tran-\n4Thanks to Hubert Léveillé Gauvin, Gary Yim, Dana DeVlieger, Lissa\nReed.scriptions by one (paid) professional jazz performer, also\na graduate student in jazz performance at the time. When\ntranscribers were uncertain of their transcriptions, a sec-\nond transcriber would collaborate on the ﬁnal version. We\ngave our paid transcribers detailed instructions and have\npersonally vetted and edited all transcriptions for consis-\ntency. The complete transcription guidelines are provided\nin the supplementary materials.\nThe exact audio ﬁles used for the original McGill tran-\nscriptions are not publicly available; for our transcriptions\nwe accessed targeted songs via YouTube, taking care to\nconﬁrm that each recording was the correct Billboard Hot\n100 single. Unfortunately, some of the original McGill\ntranscriptions do notmatch the targeted single, instead\nmatching an album version, live version, or some other\nversion of the same song; In a few cases, we could not ﬁnd\nany recording that clearly matched the transcription. To\nimprove consistency, we elected to modify the harmonic\ntranscriptions for sixteen tracks to match the correct, sin-\ngle version from the Hot 100 chart. In most cases, these\nversions were very similar but slightly longer or shorter; in\na few cases, the alternate version was in a different key or\ncontained other signiﬁcant differences. For these sixteen\naltered versions, the original timestamps were discarded\nand replaced with corrected timestamps in the correct sin-\ngle version, as available on YouTube. The CoCoPops\nrepository includes ﬁles with links to each song’s reference\nrecording on YouTube, as well as MusicBrainz MBIDs for\nour 214-song melodic transcription subset.\n5. CONVERTING EXISTING DATA\nTo create the new data, we converted the preexisting Bill-\nboard and RS data into humdrum format. During this pro-\ncess, we noted some errors in the Billboard transcriptions,\nwhich we corrected in our new data. Our expertise (edu-\ncation/credentials) in music performance and analysis are\ncomparable to the original transcribers’. Most of these er-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n242rors are unambiguous—for instance, a measure of music\nmissing or a clear change of key that is not indicated. In\nonly a few cases our “corrections” might by considered de-\nbatable. All errors and corrections are documented in our\ncorpus repository. Each ﬁle in CoCoPops also includes\na wealth of meta-data, including track information—title,\noriginal artist, release date, etc.—and sampling informa-\ntion, like the rank on the Rolling Stone 500 list and chart\nposition on the Billboard Hot 100.\n5.1 Billboard Data\nWe created a custom R script to convert the\noriginal Billboard corpus ﬁles (available at\nddmal.music.mcgill.ca/research) into a humdrum rep-\nresentation. The harmonic annotations are encoded in a\n**harte spine with the timestamps in a **timestamp\nspine. Along with the **harte representation, we\nalso include a **harm spine in each ﬁle: the humdrum\nstandard for representing Roman numerals. Whereas\nthe original harmonic transcriptions focus on the literal\npitch-content played by rhythm-section instruments (ig-\nnoring vocal parts), Roman numerals represent harmony\nat a higher level of abstraction, incorporating the broader\ntonal context. This means that, for example, open-ﬁfth\n“power chords” are interpreted as major or minor triads\n(numerals) based on the key, context, and vocal melody.\nFor illustration, the original transcription of the track “I’m\nGoing Down,” by Bruce Springsteen, consists entirely\nof two repeated patterns: A:5-E:5-F#:5-D:5 and\nA:maj-E:maj-F#:min-D:maj . We interpret both of\nthese patterns as I-V-vi-IV . To create this **harm\ninformation, we wrote an R script to parse each ﬁle and\nreplace under-speciﬁed chords (like C5) with the full triad\nexpected given the key-signature and/or explicit triads\nindicated on the same root in the same song. This process\nwas effective in the vast majority of cases; however, for\nsongs with ambiguous modality we identiﬁed the triad\nmanually. The harmonic rhythm is also indicated in\nthe**harm spine using standard humdrum rhythmic\nduration tokens.\nThe original McGill data includes two, parallel, formal\nencodings: named sections (e.g., verse, chorus) and ab-\nstract letters (e.g., AABA). These parallel encodings are\nnot redundant, as the transcribers used letters to indicate\nmore abstract repetition (mainly of chord progressions)—\nfor example, a guitar solo section which reuses the chord\nprogression from the verse will be labeled “solo”, but use\nthe same letter designation (e.g., A) as the verse. We en-\ncode both formal representations, independently, in hier-\narchical https://www.humdrum.org/guide/ch20/: Abstract\nformal labels are encoded in interpretation records of the\nform*>Letter>A ; formal names are encoded in sep-\narate records of the form *>Label>Verse . Phrases\nin the music (originally represented with line breaks) are\nindicated by the presence of the token newline in a\n**phrase spine, with a parallel **leadinstrument\nspine for lead-instrument annotations.\nTranscribers worked in music notation software oftheir choice (e.g., Musescore, Sibelius) transcribing pitch,\nrhythm, and lyrics. The transcription was then exported\ninto musicXML format. We wrote a Haskell program to\nparse musicXML scores into humdrum notation ( **kern\nfor pitch/rhythm, and **silbe for lyrics), and align this\ninformation with the already generate humdrum data de-\nscribed in the previous paragraphs. When transcribers in-\ncluded more than one vocal part for a song, each part ap-\npears as a separate pair of spines( **kern and**silbe )\nin the humdrum ﬁle.\n5.2 Rolling Stone Data\nThe RS200 dataset is available at rockcorpus.midside.com,\nwith data for each song encoded in four or ﬁve sepa-\nrate ﬁles—Figure 1 shows three such ﬁles. In addition,\nDavid Temperley provided us with ﬁles indicating the\nhierarchical structure built into their original transcrip-\ntions, which can be interpreted as formal labels. We\ncreated a Haskell program to parse these ﬁles and gen-\nerate a single humdrum-syntax ﬁle for each track.5In\nsome cases, we had to correct inconsistencies between\nharmonic and melodic transcriptions—e.g., music notated\nas 4/4 in the harmonic analysis but 12/8 in the melodic\ntranscription. Each humdrum ﬁle created includes two\n**harm spines, representing Temperley and de Clercq’s\nseparate harmonic transcriptions, labeled with comment\ntokens ‘!D.T.’ or ‘!T.d.C’ respectively. The RS200’s origi-\nnal step-sequencer-like approach to rhythm transcription is\nfaithfully encoded using humdrum’s “timebase” function\nwhere*tbinterpretations indicate the duration of each\nstep. For the 80-song subset with lyrics, a **silbe spine\nindicates the lyric alongside a **stress spine to indicate\nthree levels of lexical/prosodic stress.\nThe original RS200 transcriptions indicate only tonal\ncenter (tonic), not mode, which can be ambiguous in pop-\nular music [3]. For consistency with the Billboard data,\nthe key in each **harm spine is indicated as either major\nor minor, depending on what would be the most likely in-\nterpretation. The RS200 melodic transcriptions doinclude\nkey-signature-like indications of raised/lowered scale de-\ngrees. Using these scale indications and the humdrumR\npackage [16], we were able to convert the original scale-\ndegree representation to **kern in the ﬁnal dataset.\n6. V ALENCE AND AROUSAL SUBSET\nIn addition to the musical data itself, we gathered\ncontinuous-response ratings of perceived valence and\narousal, in a 100-song subset of the Billboard data. Va-\nlence and arousal are the two core dimensions of Russel’s\ncircumplex model of affect [17], and, while perhaps lim-\niting [18, 19], has been used widely in both music per-\nception research [18, 20] and music emotion recognition\n(MER) [21–23]. We focused on valence and arousal due\nto their simplicity (i.e., only two variables) and ubiquity\n5Though the music21 Python library [15] includes a parser for the\nRS200 harmonic transcriptions, it was easier to assure consistency and\nalignment between melodic, harmonic, lyrical, and formal information\nby using a single custom parser.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n243E2 F#2 Ab2 Bb2 C3 D3 E3 F#3 Ab3 Bb3 C4 D4 E4 F#4 Ab4 Bb4 C5 D5 E5 F#5 A b5Distribution of Pitches in CoCoPopsCount\nAbsolute Pitch01000200030004000500060007000800090001000011000120001300014000 Billboard\nRolling Stone\nBillboard\n0300060009000120001500018000\n170001400011000800050002000\n1#1 b2\n2#2\nb3\n34#4 b5\n5b6\n6b771\n#1 b22\n#2b33\n4\n#4\nb55\nb66\nb7\n7\nRolling StoneDistribution of Scale Degrees in CoCoPops\nScale DegreeCountBillboard\n01000200030004000\n4000300020001000\nIIVVbVII iviii bVIbIIIiviiiI\nIV V\nbVII\ni\nviii\nbVI bIIIiviii\nRolling StoneDistribution of Functional Harmonies in CoCoPops\nHarmonyCountFigure 4 . Left: distribution of absolute pitches in each corpus. Middle: distribution of 15 most common scale degrees in\neach corpus. Right: distribution of ten most common functional harmonies in each corpus (11 in total), sorted by rank in the\nBillboard data. (Only Temperley’s harmonic annotations are counted; Immediate repetitions of a chord are not counted.)\nin the literature, though it is acknowledged that there are\nlikely additional, overlooked dimensions such as tension\nand power [24]. Since arousal is highly correlated with\nsound level, we also include the rolling RMS values for\neach track in an **rms spine.\n6.1 Perceptual Data\nPerceptual data was gathered in a human-subject exper-\niment, approved by Georgia Tech’s Institutional Review\nBoard (protocol H22086). Eighty participants took part\nin our experiment, each paid $15 for their time. All par-\nticipants were students at Georgia Tech, and were mainly\nnon-music majors. Experiments took place in person, in\na sound-attenuated booth using professional-quality loud-\nspeakers set at the same ﬁxed sound-level for all partici-\npants. Participants used a physical slider (Monogram Cre-\native) to make their continuous ratings, with the slider po-\nsition sampled every 500ms.\nThe concepts of valence and arousal were explained to\neach participant in simple terms: arousal being how calm-\nenergetic they perceived the music to be at any given mo-\nment, and valence being the polarity (negative-positive) of\nthe music [25]. Participants were instructed to rate what\nthey perceived the music to express, not necessarily what\nthey themselves felt. Since continuously tracking valence\nand arousal simultaneously is challenging, we had partic-\nipants rate each independently—the same approach taken\nfor the DEAM dataset [11]. The authors of the DEAM\nproject also reported an increase in the usability (i.e. vari-\nation) [11] of the ratings when they used full songs as op-\nposed to shorter clips; Accordingly, participants in our ex-\nperiment listened to the full songs. To encourage sustained\nengagement and attention throughout the experiment, we\nhad each participant rate only ten songs. Participants wererandomly assigned to rate valence in ﬁve songs and arousal\nin the other ﬁve, with the order of tasks counterbalanced.\nUltimately, each of the 100 songs was independently rated\nfor valence and arousal by eight different participants (four\nfor valence and four for arousal). The full experiment took\napproximately forty minutes.\nFiles in the 100-song subset include independent\n**valence ,**arousal , and**rms spines. The four\nindependent arousal and valence ratings are encoded in the\nsame spine, in space-separated humdrum sub-tokens.\n7. SUMMARY\nThe CoCoPops corpus includes complete melodic and har-\nmonic data for 398 unique popular songs released be-\ntween 1949 and 2002. 95% of songs (379) come from\nthe years 1956–1991 with more than half (203) from the\nyears 1965–1980. The corpus includes 145,822 note on-\nsets (86,215 in the Billboard subset), 37,010 chord changes\n(19,682 in the Billboard subset), and 63,809 words in the\nlyrics (48,018 in the Billboard subset). Figure 4 shows\nthe distributions, in each subcorpus, of three fundamental\npitch parameters—absolute pitch height, scale degree, and\nthe ten most frequent Roman numerals. Though the two\nsubcorpora originate in data generated by different sam-\npling criteria and different measurement/encoding proce-\ndures (see Section 5), these distributions are nonetheless\nbroadly similar, highlighting the potential value of treating\nthese two separate subcorpora as a single united corpus.\nThe CoCoPops dataset is hosted at\ngithub.com/Computational-Cognitive-Musicology-\nLab/CoCoPops, shared under a CC-BY-4.0 license. Many\nfurther methodological and encoding details are included\nin the repository ﬁles, as well as our recommendations\nabout the usage, distribution, and citation of the data.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2448. REFERENCES\n[1] J. A. Burgoyne, J. Wild, and I. Fujinaga, “An expert\nground truth set for audio chord recognition and mu-\nsic analysis,” in Proceedings of the International So-\nciety for Music Information Retrieval , A. Klapuri and\nC. Leider, Eds., Miami, FL, 2011, pp. 633–638.\n[2] C. Harte, “Towards automatic extraction of harmony\ninformation from music signals,” Doctor of Philos-\nophy, University of London, Department of Elec-\ntronic Engineering, Queen Mary, University of Lon-\ndon, 2010.\n[3] T. De Clercq and D. Temperley, “A corpus\nanalysis of rock harmony,” Popular Music , vol. 30,\nno. 1, pp. 47–70, 2011, publisher: Cambridge\nUniversity Press. [Online]. Available: https:\n//www.cambridge.org/core/journals/popular-music/\narticle/abs/corpus-analysis-of-rock-harmony/\nC5210A8EC985DDF170B53124F4464DA4\n[4] D. Temperley and T. d. Clercq, “Statistical analysis\nof harmony and melody in rock music,” Journal of\nNew Music Research , vol. 42, no. 3, pp. 187–204,\n2013. [Online]. Available: https://doi.org/10.1080/\n09298215.2013.788039\n[5] D. Huron, “On the virtuous and the vexatious in an age\nof big data,” Music Perception: An Interdisciplinary\nJournal , vol. 31, no. 1, pp. 4–9, 2013. [Online].\nAvailable: https://www.jstor.org/stable/10.1525/mp.\n2013.31.1.4\n[6] B. Clark and C. Arthur, “Is melody ‘dead’?: A\nlarge-scale analysis of pop music melodies from 1960\nthrough 2019,” Empirical Musicology Review , In\nPress.\n[7] G. Micchi, M. Gotham, and M. Giraud, “Not\nall roads lead to rome: Pitch representation and\nmodel architecture for automatic harmonic Analysis,”\nTransactions of the International Society for Music\nInformation Retrieval (TISMIR) , vol. 3, no. 1,\np. 42, 2020. [Online]. Available: https://hal.science/\nhal-02934374\n[8] C. Harte, M. B. Sandler, S. A. Abdallah, and E. Gómez,\n“Symbolic representation of musical chords: A pro-\nposed syntax for text annotations.” in Proceedings of\nthe International Society for Music Information Re-\ntrieval , 2005, p. 66–71.\n[9] C. W. White, J. Fulmer, B. Cordova, A. Black,\nC. Danitz, W. Evans, A. Fischer, R. Greene,\nJ. He, E. Kenyon, J. Miller, M. Moylan, A. Ring,\nE. Schwitzgebel, and Y . Wang, “A new corpus of\ntexture, timbre, and change in 20th-century American\npopular music,” in Future Directions of Music\nCognition . The Ohio State University Libraries,\n2021. [Online]. Available: https://kb.osu.edu/handle/\n1811/93133[10] C. W. White, J. Pater, and M. Breen, “A comparative\nanalysis of melodic rhythm in two corpora of American\npopular music,” Journal of Mathematics and Music ,\nvol. 16, no. 2, pp. 160–182, 2022. [Online]. Available:\nhttps://doi.org/10.1080/17459737.2022.2075946\n[11] A. Aljanaki, Y .-H. Yang, and M. Soleymani,\n“Developing a benchmark for emotional analy-\nsis of music,” PLOS ONE , vol. 12, no. 3,\np. e0173392, 2017. [Online]. Available: https:\n//dx.plos.org/10.1371/journal.pone.0173392\n[12] C. S. Sapp, “Online database of scores in the humdrum\nﬁle format.” in Proceedings of the International Society\nfor Music Information Retrieval , 2005, p. 664–665.\n[13] C. Arthur, F. Lehman, and J. McNamara, “Presenting\nthe SWTC: A symbolic corpus of themes from John\nWilliams’ Star Wars episodes I–IX,” Empirical Musi-\ncology Review , In Press.\n[14] N. Condit-Schultz, “MCFlow: A digital corpus of rap\ntranscriptions,” Empirical Musicology Review , vol. 11,\nno. 2, p. 124–147, 2016.\n[15] M. S. Cuthbert and C. Ariza, “Music21: A toolkit for\ncomputer-aided musicology and symbolic music data.”\ninProceedings of the International Society for Music\nInformation Retrieval , 2010, p. 9–13.\n[16] N. Condit-Schultz and C. Arthur, “humdrumR: a new\ntake on an old approach to computational musicology,”\ninProceedings of the International Society for Music\nInformation Retrieval , 2019, p. 715–722.\n[17] J. A. Russell, “A circumplex model of affect,” Journal\nof Personality and Social Psychology , vol. 39, no. 6,\npp. 1161–1178, 1980, place: US Publisher: American\nPsychological Association.\n[18] A. Micallef Grimaud and T. Eerola, “An inter-\nactive approach to emotional expression through\nmusical cues,” Music & Science , vol. 5, p.\n20592043211061745, 2022, publisher: SAGE\nPublications Ltd. [Online]. Available: https:\n//doi.org/10.1177/20592043211061745\n[19] G. L. Collier, “Beyond valence and activity in the emo-\ntional connotations of music,” Psychology of Music ,\nvol. 35, no. 1, pp. 110–131, 2007.\n[20] T. Eerola and J. K. Vuoskoski, “A review of music\nand emotion studies: Approaches, emotion models,\nand stimuli,” Music Perception: An Interdisciplinary\nJournal , vol. 30, no. 3, pp. 307–340, 2013. [Online].\nAvailable: http://mp.ucpress.edu/cgi/doi/10.1525/mp.\n2012.30.3.307\n[21] Y . Agrawal, R. G. R. Shanker, and V . Alluri,\n“Transformer-based approach towards music emotion\nrecognition from lyrics,” in Advances in Informa-\ntion Retrieval , ser. Lecture Notes in Computer Sci-\nence, D. Hiemstra, M.-F. Moens, J. Mothe, R. Perego,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n245M. Potthast, and F. Sebastiani, Eds. Cham: Springer\nInternational Publishing, 2021, pp. 167–175.\n[22] J. de Berardinis, A. Cangelosi, and E. Coutinho,\n“The multiple voices of musical emotions: source\nseparation for improving music emotion recognition\nmodels and their interpretability,” in Proceedings of\nthe 21st International Society for Music Information\nRetrieval Conference , Online, 2020, pp. 310–317.\n[Online]. Available: https://livrepository.liverpool.ac.\nuk/3105085\n[23] D. Bogdanov, X. Lizarraga Seijas, P. Alonso-Jiménez,\nand X. Serra, “MusA V: A dataset of relative arousal-\nvalence annotations for validation of audio models,” in\nProceedings of the International Society for Music In-\nformation Retrieval . International Society for Music\nInformation Retrieval (ISMIR), 2022, pp. 650–658.\n[24] J. Cespedes-Guevara and T. Eerola, “Music communi-\ncates affects, not basic emotions – A constructionist\naccount of attribution of emotional meanings to\nmusic,” Frontiers in Psychology , vol. 9, 2018. [On-\nline]. Available: https://www.frontiersin.org/articles/\n10.3389/fpsyg.2018.00215/full\n[25] K. N. Olsen, R. T. Dean, and C. J. Stevens, “A Con-\ntinuous Measure of Musical Engagement Contributes\nto Prediction of Perceived Arousal and Valence,” Psy-\nchomusicology: Music, Mind, and Brain , vol. 24, no. 2,\npp. 147–156, 2014.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n246"
    },
    {
        "title": "Inversynth II: Sound Matching via Self-Supervised Synthesizer-Proxy and Inference-Time Finetuning.",
        "author": [
            "Oren Barkan",
            "Shlomi Shvartzman",
            "Noy Uzrad",
            "Moshe Laufer",
            "Almog Elharar",
            "Noam Koenigstein"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265371",
        "url": "https://doi.org/10.5281/zenodo.10265371",
        "ee": "https://zenodo.org/records/10265371/files/000076.pdf",
        "abstract": "Synthesizers are widely used electronic musical instruments. Given an input sound, inferring the underlying synthesizer's parameters to reproduce it is a difficult task known as sound-matching. In this work, we tackle the problem of automatic sound matching, which is otherwise performed manually by professional audio experts. The novelty of our work stems from the introduction of a novel differentiable synthesizer-proxy that enables gradient-based optimization by comparing the input and reproduced audio signals. Additionally, we introduce a novel self-supervised finetuning mechanism that further refines the prediction at inference time. Both contributions lead to state-of-the-art results, outperforming previous methods across various metrics. Our code is available at: https://github.com/inversynth/InverSynth2.",
        "zenodo_id": 10265371,
        "dblp_key": "conf/ismir/BarkanSULEK23",
        "keywords": [
            "synthesizers",
            "electronic musical instruments",
            "sound-matching",
            "automatic sound matching",
            "differentiable synthesizer-proxy",
            "gradient-based optimization",
            "self-supervised finetuning",
            "state-of-the-art results",
            "previous methods",
            "code available"
        ],
        "content": "INVERSYNTH II: SOUND MATCHING VIA SELF-SUPERVISED\nSYNTHESIZER-PROXY AND INFERENCE-TIME FINETUNING\nOren Barkan1Shlomi Shvartzman2Noy Uzrad2\nMoshe Laufer2Almog Elharar2Noam Koenigstein2\n1The Open Univesity of Israel\n2Tel Aviv University, Israel\nABSTRACT\nSynthesizers are widely used electronic musical instru-\nments. Given an input sound, inferring the underly-\ning synthesizer’s parameters to reproduce it is a difﬁ-\ncult task known as sound-matching . In this work, we\ntackle the problem of automatic sound matching, which is\notherwise performed manually by professional audio ex-\nperts. The novelty of our work stems from the introduc-\ntion of a novel differentiable synthesizer-proxy that en-\nables gradient-based optimization by comparing the in-\nput and reproduced audio signals. Additionally, we in-\ntroduce a novel self-supervised ﬁnetuning mechanism that\nfurther reﬁnes the prediction at inference time. Both con-\ntributions lead to state-of-the-art results, outperforming\nprevious methods across various metrics. Our code is\navailable at: https://github.com/inversynth/\nInverSynth2 .\n1. INTRODUCTION AND RELATED WORK\nSound synthesis has been an active research ﬁeld since the\nend of the previous century [1]. Given a query audio in-\nput, the task of crafting a speciﬁc sound is known as sound\nmatching . Synthesizer sound matching, also known as in-\nverse synthesis , involves carefully tuning parameters from\nan exponentially large number of possible conﬁgurations-\na task mostly reserved for professional audio experts. This\npaper presents a novel algorithmic approach for automated\nsound matching .\nAlgorithmic approaches for inverse synthesis can\nbe loosely categorized into search-based methods and\nmodeling-based methods [2]. Search-based methods of-\nten utilize genetic algorithms (GA) which are based on\nprinciples of Darwinian evolution to determine the opti-\nmal synthesizer conﬁgurations. For instance [3] initiated a\nset of randomly sampled conﬁgurations and used GA op-\ntimization to reconstruct the original audio signal. Other\nsearch-based methods include Particle Swarm Optimiza-\ntion (PSO) [4] and Hill-Climbing [5]. Search-based meth-\nods can employ different objectives such as mel-frequency\ncepstral coefﬁcients (MFCCs) or a combination of mul-\ntiple objectives [6, 7]. However, optimizing conﬁgura-\n© Barkan et al.. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: Barkan\net al., “InverSynth II: Sound Matching via Self-Supervised Synthesizer-\nProxy and Inference-Time Finetuning”, in Proc. of the 24th Int. Society\nfor Music Information Retrieval Conf., Milan, Italy, 2023.tions through search-based methods can be both resource-\nintensive and time-consuming for every sound sample.\nConsequently, the rise of deep learning techniques has led\nto a shift from search-based methods to model-based ones,\nwhich avoid the previously mentioned drawbacks. How-\never, search-based methods still possess a unique advan-\ntage: they can establish a loss term that directly contrasts\nthe reconstructed audio signal with the input signal.\nThe aforementioned advantage is absent in most\nmodeling-based methods, as they usually cannot propagate\ngradients through an external, commercial synthesizer. As\na result, they depend on setting an optimization goal fo-\ncused on reconstructing the parameters rather than the re-\nproduced signal. In general, modeling-based methods em-\nploy deep learning in order to predict a synthesizer’s con-\nﬁguration based on the input audio signal. For example,\n[8] employed long short-term memory (LSTM) networks\nfor predicting the parameters in FM synthesizers. Inver-\nSynth (IS) [9] employed strided convolution neural net-\nworks (CNNs) to estimate a synthesizer’s parameters as\na multi-objective classiﬁcation problem. When compared\nto the LSTMs approach of [8], IS provides improved accu-\nracy with the ability to scale for longer audio sequences.\nAnother direction involves employing variational infer-\nence [10] and normalizing ﬂows [11, 12] to automatically\ntune an open-source replica of the Yamaha DX7 synthe-\nsizer [13]. Finally, [14–16] introduce a different versions\nof audio synthesizer models for sound matching.\nA completely different direction for sound matching\nand synthesis is through neural synthesizers [17–22]. For\nexample, in [19] the authors train Generative Adversarial\nNetworks to synthesize sounds that simulate natural audio\nsamples. However, these directions are inherently different\nfrom the current line of work, as they do not deal with the\nproblem of tuning existing musical synthesizers. Instead,\nthese works suggest alternatives to familiar synthesizers,\nwhich may be useful for future applications but are less\nrelevant to mainstream musicians that use existing com-\nmercial synthesizers.\nIn this paper, we present InverSynth II (IS2) - an in-\nnovative inverse-synthesis model that introduces a differ-\nentiable synthesizer-proxy capable of learning to “imitate”\nthe behavior of any given synthesizer. This allows for a dif-\nferentiable relationship between the synthesizer’s parame-\nters and the produced audio signal. As a result, IS2 learns\nto focus on the synthesizer parameters that have more im-\npact on the reproduced signal. Our evaluations indicate\nthat this approach leads to a better reconstruction of the642original audio signal in terms of spectral loss and human\nperception.\nOur contributions are as follows: (1) We introduce IS2\nthat effectively incorporates the synthesizer’s functionality\ninto the computational graph. By learning a differentiable\nsynthesizer-proxy , IS2 facilitates self-supervision based on\nthe difference between the input and reproduced audio sig-\nnals. This is in contrast to previous model-based works\nthat optimized on the predicted synthesizer parameters\nalone [8, 9, 23]. (2) We introduce a novel self-supervised\nﬁnetuning technique that utilizes the learned synthesizer-\nproxy to further reﬁne predictions at inference time. (3) We\ncompare IS2 against the state-of-the-art methods from [10]\nand [9] on the three datasets, including the datasets from\nboth of these works. Our ﬁndings show that IS2 outper-\nforms both methods on all datasets, across all metrics.\n2. INVERSYNTH II\n2.1 Problem Setup\nLetx∈X be the audio signal i.e., raw waveform, Short-\ntime Fourier transform (STFT) spectrogram, etc. Let f:\nY → X be a synthesizer function that generates a sig-\nnalf(y)∈X according to the parameters conﬁguration\ny∈Y , whereyencodes the exact value for each of the\nconﬁgurable synthesizer parameters. For example, these\nparameters determine the oscillators’ waveform types, the\namplitudes’ values, modulation indexes, ADSR envelopes,\nﬁlter cutoff frequency, etc. The inverse-synthesis task is to\nlearn an encoder functioneθ:X→Y , parameterized by\nθ, that receives an audio x∈X and predicts the parameters\nconﬁguration eθ(x)∈Y s.t.\nf(eθ(x)) =x′≈x. (1)\n2.2 The IS Model\nThe IS model from [9] receives an input signal xand\naims at inferring a parameters conﬁguration ˆywhich best\nmatches the true yet unknown parameters conﬁguration y\nthat produced x=f(y). To this end, a dataset D=\n{(xi,yi)}N\ni=1is generated, where yiis the synthesizer’s\nconﬁguration used by fto generate the sound xi, hence\nf(yi) =xi. IS trains an encoder network eθto predict yi\nfromxiby minimizing the objective\nθ∗=argmin\nθN/summationdisplay\ni=1Lp(eθ(xi),yi), (2)\nwhereLp:Y×Y→ Ris the parameters loss that quan-\ntiﬁes the difference between the predicted conﬁguration\neθ(xi)and the ground truth conﬁguration yi. In [9], each\nsynthesizer parameter was treated as a categorical vari-\nable (continuous parameters were quantized), hence solv-\ning multiple classiﬁcation problems simultaneously (one\nfor each parameter). Accordingly, the loss Lpwas the sum\nofPcross-entropy (CE) losses, where Pis the number of\nthe synthesizer parameters.\n2.3 The IS2 Model\nIS does not optimize on the actual reproduced audio sig-\nnal. Instead, it only optimizes on the parameters con-\nﬁguration according to Eq. 2. However, minimizing Lpis just a proxy to the original task from Eq. 1 that aims\nat minimizing the difference between the original sig-\nnalxand the reproduced signal f(eθ(x)). This obser-\nvation motivates an additional self-supervised loss term\nLa:X×X→ R, namely the audio loss , that measures the\ndiscrepancy between the input signal xand the reproduced\nsignalf(eθ(x)):\nθ∗=argmin\nθN/summationdisplay\ni=1Lp(eθ(xi),yi)+λLa(f(eθ(xi)),xi),\n(3)\nwhereλis a hyperparameter. The audio loss term La\nprovides feedback on the quality of the reproduced signal\nf(eθ(xi))itself, hence better aligns with the ultimate task\nof Eq. 1.\nYet, a key challenge arises - how to backpropagate the\nerror induced byLaviaf? A naive approach may pro-\npose implementing the synthesizer fas part of the com-\nputational graph. However, this approach suffers from\nseveral limitations: First, it requires a speciﬁc implemen-\ntation per synthesizer and hence does not scale. Sec-\nond, most commercial synthesizers are not open-source,\nand even if the source code was provided, it would still\nrequire rewriting of the entire codebase to support an\nauto-differentiation platform (e.g., PyTorch). Furthermore,\nsome synthesizer functionalities are not differentiable and\nrequire workarounds that may incur discrepancies and hin-\nder gradient-based optimization.\nTo this end, IS2 introduces a synthesizer-proxy decoder\nnetworkdφ:Y→X , parameterized by φ, that serves as a\ndifferential replacement to the true synthesizer function f.\ndφis trained to minimize Law.r.t.φover the dataset D,\nwhich leads to the IS2 training objective\nΘ∗=argmin\nΘN/summationdisplay\ni=1Lp(eθ(xi),yi)+λLa(dφ(eθ(xi)),xi),\n(4)\nwithΘ ={θ,φ}.\n2.4 IS2 Training\nIS2 employs stochastic gradient descent optimization [24]\non the objective from Eq. 4 as depicted in Fig. 1(a) (the\nexact implementation and optimization details will follow\nin Secs. 2.6 and 3.2). We apply a K-fold cross-validation\nprocedure over the dataset D, where each fold deﬁnes dif-\nferent training, validation, and test sets. For each fold, we\ntrain the IS2 model on the training set and monitor the fol-\nlowing measure on the validation set V⊂{1..N}:\nLf\nV:=/summationdisplay\ni∈VLa(f(eθ(xi)),xi). (5)\nFinally, the best-performing model (in terms of Lf\nVacross\nall epochs) is used for reporting results on the test set.\nNote that the predicted parameters eθ(x)in Eq. 5\nare propagated to the true synthesizer fand not to the\nsynthesizer-proxy dφ(see Fig. 1(b)). This enables select-\ning the model that minimizes the discrepancy between x\nandf(eθ(x)), which aligns with the ultimate task of Eq. 1.\nYet,fdoes not participate in the optimization objective\n(Eq. 4) since it is not necessarily differentiable. Instead,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n643IS2 Training\n IS2 Inference\nFigure 1 : (a)-(b) depict the IS2 training phase (Sec. 2.4) that utilizes the differentiable synthesizer-proxy dφ, while monitor-\ning for the best model via the true synthesizer f. (c)-(d) depict the IS2 inference phase (Sec. 2.5) that employs ITF, utilizing\nthe optimized dφ∗for improved parameters prediction on the speciﬁc test example. Again, fis used for monitoring.\nthe audio loss term Lain Eq. 4 utilizes dφas a differen-\ntiable proxy to fin order to propagate gradients as part of\nthe optimization process.\n2.5 IS2 Inference\nA unique feature of IS2 is the ability to improve the pre-\ndictions at inference time, by employing Inference-Time\nFinetuning (ITF). Given a test input xt, we utilize the au-\ndio lossLafor leveraging self-supervision from xt, and\nreﬁne the prediction speciﬁcally for xt. To this end, we\nfreeze the trained decoder parameters φ∗(Eq. 4) and ﬁne-\ntune the trained encoder parameters θ∗to obtain ﬁnetuned\nparameters θt:\nθt=argmin\nθLt+λBLB, (6)\nwhere\nLt=La(dφ∗(eθ(xt)),xt),\nLB=1\n|B|/summationdisplay\ni∈BLp(eθ(xi),yi)+λLa(dφ∗(eθ(xi)),xi),\nandB⊂{1..N}is a subset of indexes from the training set\n(a training batch). While one could optimize only Ltw.r.t.\nθ, we found that the inclusion of LBserves as a regular-\nization (controlled by the hyperparameter λB) that leads to\nmore accurate predictions. This can be explained by the\nfact thatLBenforces the encoder to predict accurate con-\nﬁgurations for the examples in B, effectively safeguard-\ning the encoder from forgetting what it has learned during\nthe training phase (Sec. 2.4) and avoid overﬁtting the test\nexamplext. In practice, the ITF procedure alternates be-\ntween sampling a batch of examples from the training set\nB⊂{1..N}, and performing gradient descent update to\nθaccording to the objective in Eq. 6, until either conver-\ngence w.r.t.Lf\nt:=La(f(eθ(xt)),xt)is met or the number\nof alternations exceeds a prescribed threshold. ITF opti-\nmization and monitoring are depicted in Fig. 1(c)-(d).\nIt is important to clarify that ITF is applied per test\nexample, i.e., for each test example xt, we ﬁrst initializeθ←θ∗, whereθ∗are the optimal encoder parameters ob-\ntained from the IS2 training procedure (Eq. 4). Then, ITF\nalternations are employed according to Eq. 6 to obtain ﬁne-\ntuned encoder parameters θtthat might improve Lf\nt. How-\never, improvement is not guaranteed due to an inherent dis-\ncrepancy that may exist between the synthesizer-proxy de-\ncoderdφ∗(used inLt) and the synthesizer f(used inLf\nt).\nTherefore, if none of the ITF alternations yield improve-\nment toLf\nt, we fallback to the prediction obtained by the\noriginally trained encoder eθ∗(xt)(that serves as a starting\npoint for the ITF procedure).\n2.6 IS2 Implementation\nIn [9], various encoder implementations were investigated\nand the spectrogram-based strided CNN encoder stood out\nas the best performer. Following this ﬁnding, we imple-\nment the encoder eθand decoder dφas strided CNNs. Ac-\ncordingly, x∈X stands for the processed log-magnitude\nspectrogram or mel-spectrogram domain (where the spec-\ntrogram is obtained by the application of the STFT to the\nwaveform), andLais set to the Mean Absolute Error,\nhence measuring the spectral difference between the input\nand reproduced signals.\nThe synthesizer parameters conﬁguration is encoded by\na super-vector y∈Y that concatenates one-hot vectors and\nnormalized scalars representing the categorical and contin-\nuous parameters, respectively. Accordingly, the parame-\nters lossLpis set to the average of the cross-entropy and L2\nlosses for categorical and continuous parameters, respec-\ntively. The exact details of the data processing, data repre-\nsentation, and hyperparameters settings appear in Sec. 3.\n3. EXPERIMENTAL SETUP AND RESULTS\n3.1 Datasets, preprocessing, and data representation\nIn this study, we present ﬁndings from analyses conducted\non three distinct datasets. As a consequence of space con-\nstraints, it is not feasible to detail all the numerous con-\nﬁgurable parameters of every synthesizer utilized in our\nexperiments. Nevertheless, a comprehensive account ofProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n644Metric Flow IS IS2xITF IS2\nFM Dataset\nSpec (x100) 4.89 1.61 1.54 1.51\nMelspec (x100) 193.93 56.77 54.65 53.84\nMFCC (x100) 73.49 28.83 27.74 27.29\nSC 0.0941 0.0383 0.0367 0.0361\nACC (%) 93.01 93.89 93.97 94.04\nDX7 Dataset\nSpec (x100) 65.31 58.83 58.59 58.18\nMelspec (x100) 24.04 19.29 19.37 19.26\nMFCC (x100) 1502.2 1309.5 1300.4 1280\nSC 1.0472 0.8578 0.8594 0.8532\nACC (%) 85.36 86.07 86.34 86.74\nMAEparam (x100) 10.77 9.79 9.68 9.56\nTAL Dataset\nSpec (x100) 0.44 0.1809 0.177 0.173\nMelspec (x100) 106.5 68.06 67.07 64.64\nMFCC (x100) 8.95 5.85 5.72 5.8\nSC 0.51 0.512 0.467 0.424\nACC (%) 80.94 80.62 80.73 81.17\nTable 1 : Aggregated results on all datasets and metrics.\neach synthesizer parameter can be found in the supplemen-\ntary material accompanying this manuscript. The datasets\nwhich were used in this research are as follows: (1) FM: is\nbased on the FM synthesizer implementation that is avail-\nable in IS2 GitHub repository. The synthesizer is com-\nposed of a FM oscillator, AM modulation, and low-pass\nﬁlter. It includes 9 conﬁgurable parameters, each repre-\nsented by a categorical variable. Continuous parameters\nwere discretized and binned to create a ﬁnite set of values.\nA dataset of 180K audio samples (1 second, 16KHz) was\ngenerated based on a random sampling of parameter con-\nﬁgurations. Samples were transformed into 257x129 spec-\ntrograms using log-magnitude STFT (with window size\n512 and hop size 128) followed by normalization to [-\n1,1]. (2) DX7 : is the dataset from [10] which is based\non the Dexed synthesizer1which is a virtual replica of\nthe Yamaha DX7 synthesizer with 144 conﬁgurable pa-\nrameters (represented by 54 categorical and 90 continu-\nous variables). It contains 30K audio samples (3 seconds,\n22.05KHz). Each sample was transformed into a 257x347\nmel-spectrogram (257-bins) of the log-magnitude STFT\n(with window size 1024 and a hop size 256), followed by\nnormalization to [-1,1]. (3) TAL is based on the commer-\ncial synthesizer: TAL-NoiseMaker2. It consists of 180k\naudio samples generated using 9 conﬁgurable parameters\ncontrolling the oscillator, LFO1, LFO2, and cutoff param-\neters. Each sound has a duration of 1 second sampled at\n16kHz and is converted into a 257x129 spectrogram. The\nspectrograms are normalized to the range [-1, 1] using the\nsame method as the FM synthesizer dataset. The code for\nthe generation of the datasets, including the parameter dis-\ncretization process is available in our GitHub repository.\nThe above datasets encompass a broad spectrum of\nsounds that vary from basic sine waves to intricate wave-\nforms with a wealth of harmonics. The TAL Noisemaker\nand FM synthesizer datasets comprise a range of sounds in-\ncluding bass, leads, pads, plucks, and percussion, while the\nDX7 dataset comprises percussive, bell-like, and metallic\nsounds, in addition to rich pads and complex bass sounds.\n1https://github.com/asb2m10/dexed\n2https://tal-software.com/products/\ntal-noisemakerOur GitHub repository includes scripts that can reproduce\nthe these datasets.\n3.2 Evaluated methods and hyperparameters setting\nThe following models were evaluated: (1) IS2: our model\nfrom Sec. 2. eθanddφare implemented by strided and\ntransposed CNNs with 9 hidden LeakyReLU activated lay-\ners and Batch Normalization [25] (the exact hyperparam-\neters which were chosen for each layer can be seen in our\nGitHub code). The IS2 objective (Eq. 4) was optimized\nwithλ= 1using the Adam optimizer [26] with β1= 0.9,\nβ2= 0.99, batch size 64 and learning rate scheduling\nfrom10−4to10−6for 100 epochs. While training, we\nmonitoredLf\nV(Eq. 5) on the validation set, and the best-\nperforming model was selected eventually. For each test\nsample, we employed 30 ITF alternations according to the\nobjective from Eq. 6, with λB= 1, andBis a stochas-\ntic sample of 64 examples drawn randomly from the train-\ning set at each alternation. Finally, Lf\ntwas monitored\nfor selecting the best result as explained in Sec. 2.5. (2)\nIS2xITF : an ablated version of IS2, in which ITF is not\nemployed and the predictions are performed by the trained\nencodereθ∗. (3) IS: the IS method from [9]. (4) Flow : the\nmethod from [10] which is based on variational inference\nwith normalizing ﬂows. We tuned hyperparameters for all\nmodels using the validation set.\n3.3 Evaluation metrics\nWe report the average results obtained by a 5-fold cross-\nvalidation procedure with 80%-10%-10% (training, vali-\ndation, test) splits, on the following metrics: (1) Spec : the\nMean Absolute Error (MAE) between the log-magnitude\nSTFTs of a- the signal reproduced by the application of\nfto the predicted parameters conﬁguration, and b- the\nground truth conﬁguration signal. (2) Melspec : the MAE\nbetween the mel-spectrograms of aandb. (3) MFCC : the\nMAE between the 40-band MFCCs of aandb. (4) SC:\nthe Spectral Convergence [27] between aandb. Note that\nSC was found less correlated with human perception [10],\nnevertheless we report this metric for the sake of complete-\nness. (5) ACC : the accuracy of the predicted categorical\nsynthesizer parameters. (6) MAEparam : MAE between\nthe predicted and ground truth numerical synthesizer pa-\nrameter values. This metric is reported for the DX7 dataset\nonly, as all parameters in the FM and TAL dataset are mod-\neled by categorical variables. Metrics (1)-(4) measure er-\nrors in the reproduced signal f(eθ(x)), while metrics (5)-\n(6) measure accuracy / error w.r.t. the ground truth param-\neter conﬁgurations.\nTo complete our evaluations, we also present the results\nof a MOS (Mean Opinion Score) test [28] with N= 20 .\nThe MOS test involves presenting a set of synthesized\nsounds to a panel of listeners, who are then asked to rate\nthe sound quality of the reconstructed sound with respect to\nthe original sound using a standardized rating scale: [1−5].\n3.4 Quantitative Results\nTable 1 displays the results obtained by all methods\nacross all datasets and evaluation metrics. The ACC and\nMAEparam are averages across all categorical and contin-\nuous parameters, respectively. It is important to note that\nthe Flow results reported in [10] were replicated, and ourProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n645TAL Dataset FM Synth Dataset DX7 Dataset\nIS2 IS2xITF Flow IS IS2 IS2xITF Flow IS IS2 IS2xITF Flow IS\nLow (x100) 608 627 619 642 0.62 0.63 1.88 0.79 0.53 0.54 0.61 0.533\nMid (x100) 86.82 90.26 102 91.64 0.18 0.19 0.51 0.24 0.02658 0.02652 0.0338 0.0265\nHigh (x100) 5.82 6.12 7.7 6.64 0.0036 0.0037 0.0224 0.0098 0.0038 0.0037 0.0043 0.0035\nAll bands (x100) 654 675 676 691 0.63 0.64 0.19 0.8 0.53 0.538 0.6 0.537\nTable 2 : Spectral analysis for different Mel-frequency bands (x100).\nDX7 Param Type Flow IS IS2 IS2xITF\nALGORITHM cat 0.5758 0.6660 0.6676 0.6627\nFEEDBACK cat 0.6938 0.7056 0.7179 0.7151\nOSCKEYSYNC cat 0.8227 0.8269 0.8356 0.8356\nLFOSPEED num 12.5070 11.5924 11.6528 11.6337\nLFODELAY num 16.5244 15.0604 14.9376 14.8934\nLFOPMDEPTH num 13.0251 11.7224 11.5284 11.5593\nLFOAMDEPTH num 17.8000 17.7326 17.5715 17.6148\nLFOKEYSYNC cat 0.8008 0.8112 0.8163 0.8150\nLFOWA VE cat 0.7599 0.7622 0.7665 0.7618\nPMODESENS cat 0.6214 0.6473 0.6598 0.6590\nPITCHEGRATE1 num 17.6189 16.8161 16.6582 16.6706\nPITCHEGRATE2 num 17.7838 16.7808 16.8625 16.8100\nPITCHEGRATE3 num 18.2013 17.6938 17.4543 17.5203\nPITCHEGRATE4 num 19.5772 18.6485 18.7511 18.7084\nPITCHEGLEVEL1 num 6.2437 6.3104 6.3948 6.3948\nPITCHEGLEVEL2 num 6.5503 6.8803 6.8803 6.8803\nPITCHEGLEVEL3 num 6.8834 7.1543 7.1543 7.1543\nPITCHEGLEVEL4 num 6.1773 6.4220 6.4220 6.4220\nOP_EGRATE1 num 13.4020 12.2691 12.1219 12.1169\nOP_EGRATE2 num 17.7775 16.8140 16.9248 16.8298\nOP_EGRATE3 num 17.9113 17.0677 17.1096 17.0545\nOP_EGRATE4 num 12.6359 11.8326 11.8028 11.7685\nOP_EGLEVEL1 num 10.6051 10.7961 10.8860 10.9058\nOP_EGLEVEL2 num 17.9278 16.8495 16.7416 16.7842\nOP_EGLEVEL3 num 21.2140 20.3835 20.2471 20.2462\nOP_EGLEVEL4 num 12.1882 15.1754 15.1754 15.1754\nOP_OUTPUTLEVEL num 12.4545 11.5483 11.5102 11.5341\nOP_MODE cat 0.9359 0.9404 0.9446 0.9430\nOP_FCOARSE cat 0.6951 0.7486 0.7538 0.7513\nOP_FFINE num 13.8020 13.3172 13.2250 13.2555\nOP_OSCDETUNE cat 0.6578 0.7275 0.7346 0.7299\nOP_BREAKPOINT num 16.3170 15.4886 15.5501 15.4929\nOP_LSCALEDEPTH num 16.0303 16.0030 15.9739 16.0440\nOP_RSCALEDEPTH num 16.3427 15.7286 15.6248 15.6984\nOP_LKEYSCALE cat 0.8476 0.8505 0.8574 0.8526\nOP_RKEYSCALE cat 0.8533 0.8541 0.8643 0.8580\nOP_RATESCALING cat 0.7187 0.7528 0.7598 0.7579\nOP_AMODSENS cat 0.9112 0.8987 0.9185 0.9052\nOP_KEYVELOCITY cat 0.6777 0.7236 0.7290 0.7267\nMEAN CAT - 0.7551 0.7796 0.7875 0.7838\nMEAN NUM - 14.3 13.8434 13.8064 13.8067\nTable 3 : Aggregated DX7 parameters’ accuracy. The\nfunctionality of each parameter is explained in the supple-\nmentary materials (appears in our GitHub repository).\nParam TAL Flow IS IS2 IS2xITF\nx3_FilterCutoff (%) 86.08 72.01 75 72.8\nx24_Osc2Waveform (%) 99.82 99.38 99.48 99.39\nx20_Osc2Tune (%) 95 93.49 93.7 93.6\nx26_Lfo1Waveform (%) 68.28 78.33 78.8 78.38\nx28_Lfo1Rate (%) 47.54 52.93 53.34 52.97\nx30_Lfo1Amount (%) 73.37 71.74 72.19 71.78\nx27_Lfo2Waveform (%) 88.86 88.87 88.85 88.76\nx29_Lfo2Rate (%) 84.77 84.56 84.67 84.59\nx31_Lfo2Amount (%) 84.77 84.28 84.45 84.31\nMEAN (%) 80.94 80.62 81.17 80.73\nTable 4 : TAL parameters’ accuracy. The parameters are\npreﬁxed with “xAB”, where AB denotes the index of the\nparameter within the synthesizer. The functionality of each\nparameter is explained in the supplementary materials (ap-\npears in our GitHub repository).Param FM Flow IS IS2 IS2xITF\nosc1_wave (%) 99.98 99.94 99.94 99.94\nosc1_freq (%) 91.26 98.7 98.83 98.81\nosc1_mod_index (%) 93.08 96.43 96.5 96.45\nlfo1_freq (%) 99.95 99.86 99.88 99.87\nlfo1_wave (%) 99.52 98.75 98.69 98.67\nam_mod_wave (%) 67.73 71.02 71.74 71.59\nam_mod_freq (%) 86.23 82.71 82.88 82.86\nam_mod_amount (%) 99.43 97.62 97.64 97.61\nﬁlter_freq (%) 99.98 99.93 99.95 99.94\nMEAN (%) 93.02 93.89 94.01 93.97\nTable 5 : FM Synth parameters’ accuracy. The function-\nality of each parameter is explained in the supplementary\nmaterials (appears in our GitHub repository).\nDataset Flow IS IS2xITF IS2\nFM 4.7 4.85 4.6 5\nDX7 1.35 2.45 3.28 3.5\nTAL 3.87 3.37 3.85 3.95\nTable 6 : MOS test results. Scores on a scale of [1−5]\nrepresent the perceptual reconstruction quality w.r.t. the\noriginal audio.\nresults are consistent with the original ﬁndings. Table 1\ndemonstrates that our IS2 method outperforms the other\nbaselines in all metrics and datasets, except for the MFCC\nscore on the TAL datasets, where the ablated version of\nIS2, IS2xITF, outperforms it. Furthermore, the results in-\ndicate that the ablated version IS2xITF is highly effective\nin comparison to previous baselines which highlights the\ngeneral utility of the IS2 architecture even without the ITF\nphase. In the following section, we aim to provide a more\ncomprehensive analysis and interpretation of these results.\nTo provide additional perspective, we conducted the fol-\nlowing analysis: We partitioned the 257 mel-spectrogram\nbins into “Low”, “Mid”, and “High” equally sized mel-\nfrequency bands. Then, for each sound in the test set, we\ncomputed the L2 loss between the reproduced version and\nthe ground-truth of each mel-frequency band. The results\nfor the different frequency bands, including the entire mel-\nspectrogram (’All bands’) are presented in Table 2. First,\nWe observe that IS2 outperforms the other models on the\nentire mel-spectrogram (’All bands’), across all datasets,\nwhich is consistent with the results presented in Table 1\n(note that Tables 1 and 2 report different metrics, i.e, MAE\nvs. L2). Speciﬁcally, IS2 performs particularly well on low\nfrequency regime (’Low’). Arguably, this ﬁnding might be\nexplained later where we shall see that the IS2 model at-\ntains the best loss in 4 out of 6 low-frequency oscillator\n(LFO) parameters, which have a stronger impact on the\nlow bands. This calls for further research into the relation-\nship between parameter prediction accuracy and the mel-\nspectrogram error.\nIn terms of the “Mid” band, the IS2 model demon-\nstrated superior performance on the TAL and FM datasets,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n646whereas the IS model exhibited better results on the DX7\ndataset. For the “High” band, different baselines achieved\nthe best outcomes. This observation is not surprising since\nhigh frequencies typically undergo rapid changes and can\nbe less perceptible even to experienced listeners. Overall,\nour ﬁndings indicate that the IS2 approach exhibits robust\nperformance across various datasets and frequency bands,\nwith exceptional accuracy in estimating low frequencies.\nNext, we turn to evaluate the accuracy of predicting\neach parameter speciﬁcally. The DX7 synthesizer con-\nsists of two types of parameters: categorical, denoted as\n“cat”, and numerical, denoted as “num”. To evaluate\nperformance, ACC was reported for categorical parame-\nters, while MAEparam was calculated for numerical pa-\nrameters, as previously mentioned. The DX7 parameters\nare further categorized into several groups, including Al-\ngorithm, Feedback, Operators, Pitch Envelope Generator,\nLFO, and Filter, with a comprehensive explanation of each\nparameter available in the supplementary material. Ta-\nble 3 outlines the prediction results for the DX7 parame-\nters. Note that parameters beginning with the preﬁx “OP”\nare an average aggregation of the six operators of the syn-\nthesizer, as explained in the supplementary material.\nThe results reveal that the IS2 method consistently out-\nperformed other models in predicting all categorical pa-\nrameters. Furthermore, the IS2 model also outperformed\nother models in 9 out of 25 numerical parameters. No-\ntably, the IS2xITF model performed best among all mod-\nels for the “OP_EGRATEi” (i=1...4) numerical param-\neters, demonstrating superior performance even without\nﬁne-tuning (ITF). Speciﬁcally, this model exhibited bet-\nter performance in all numerical parameters and outper-\nformed other models in 5 out of 25 numerical parameters.\nIn contrast, the IS and Flow models demonstrated similarly\nrobust performance, outperforming other models in fewer\nnumerical parameters, namely 11 out of 25.\nOverall, Table 3 displays a trend where all categori-\ncal parameters are more accurately estimated by the IS2\nmodel. In terms of numerical parameters, the IS2 model\nperforms better in predicting parameters with greater per-\nceptual signiﬁcance, such as LFO. However, parameters of\nlesser perceptual signiﬁcance, such as the envelope level\nof pitch (PITCHEGLEVEL), exhibit lower estimation ac-\ncuracy. These trends are consistent with the ﬁndings pre-\nsented earlier in Table 2\nThe trends observed in Table 3 repeat themselves in the\nTAL dataset (Table 4) and the FM Synth dataset (Table 5),\nwith slightly improved accuracy for the alternative models.\nNevertheless, the IS2 model maintains the highest mean\naccuracy. IS2 does not achieve the highest accuracy in cer-\ntain parameters, such as ﬁlter cutoff, which are of lesser\nsigniﬁcance for perceived quality. For example, if the ﬁlter\ncutoff is estimated for class A instead of the correct class\nB, and A and B are neighboring classes, the impact on\nperception might be insigniﬁcant. Another set of param-\neters that demonstrate negligible differences in accuracy\nbetween models is Oscillator 2, LFO1 amount, and LFO2\nvalues. In contrast, parameters such as LFO1 waveform\nand rate play a crucial role in controlling Oscillator 2 mod-\nulation and impacting the low frequencies of the sound,\nmaking them signiﬁcant in terms of perception. Here, IS2\nachieves signiﬁcantly higher accuracies compared to other\nmodels. These ﬁndings are consistent with the low lossesof the IS2 model for low frequencies, as presented in Ta-\nble 2.\nIn Table 5, Oscillator 1 waveform, LFO1, and ﬁlter\ncutoff frequency exhibit negligible differences in accuracy\nin favor of the alternative baselines. Higher differences\nare observed in AM modulation parameters. Neverthe-\nless, these parameters primarily affect the Tremolo effect,\nwhich has a relatively no impact at all on the frequency\ncomposition, and for small changes, leading to less inﬂu-\nence on human perception and less impact on the metrics\npresented in Table 1. Compared to the Flow model, the\nparameters with the most signiﬁcant differences are Oscil-\nlator 1 frequency and modulation index, which have a sig-\nniﬁcant impact on perception by affecting the carrier fre-\nquency of the signal.\nOverall, the results in Tables 1-5 indicate that by lever-\naging information on the difference between the original\nand reproduced signal during training and inference (ITF),\nIS2 promotes accurate predictions for parameters that have\nthe most signiﬁcant impact on human perception, espe-\ncially FM modulator parameters which very challenging to\nestimate. Consequently, IS2 produces reconstructions that\nmore closely resemble the original signal, which is the ul-\ntimate goal of sound matching. In what follows, we further\nsubstantiate our ﬁndings via human subjective evaluation.\n3.5 MOS Test and Qualitative Results\nTable 6 presents MOS test results conducted using N= 20\nindividuals. Participants were asked to rate the reconstruc-\ntion score of 80random audio samples on a [1−5]scale.\nThe results are inline with those of Table 1, showing that\nthe IS2 model outperforms the other models. Furthermore,\nthe MOS test results indicate that the IS2 model has the\nhighest perception quality of all the models evaluated. The\nsamples from this test are available for listening on the\nGitHub repository.\nFinally, in the supplementary materials, we provide ex-\ntensive qualitative comparison between the ground-truth\nspectrograms and the reconstructions produced by each of\nthe evaluated methods. Additionally, the audio signals for\nthese examples are provided are available in our Google\nDrive folder3.\n4. CONCLUSION\nWe presented IS2 - a novel model for automatic synthe-\nsizer sound matching. IS2 introduces two novel contribu-\ntions: (1) a differentiable synthesizer-proxy decoder that\nenables gradient-based optimization of the reproduced au-\ndio signals, and (2) the ITF technique that enables im-\nproved model predictions at inference time. These contri-\nbutions lead to state-of-the-art results compared to existing\nmethods across multiple datasets and metrics.\n5. ACKNOWLEDGMENTS\nThis research was supported by the ISRAEL SCIENCE\nFOUNDATION (Grant No. 2243/20).\n3https://drive.google.com/drive/folders/\n1VFnE5fcEfbbmKdNT-NxMvXotSdz5mOQk?usp=sharingProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6476. REFERENCES\n[1] G. De Poli, “A tutorial on digital sound synthesis tech-\nniques,” Computer Music Journal , vol. 7, no. 4, pp.\n8–26, 1983.\n[2] J. Shier, “The synthesizer programming problem: im-\nproving the usability of sound synthesizers,” Ph.D. dis-\nsertation, 2021.\n[3] A. Horner, J. Beauchamp, and L. Haken, “Machine\ntongues xvi: Genetic algorithms and their application\nto fm matching synthesis,” Computer Music Journal ,\nvol. 17, no. 4, pp. 17–29, 1993.\n[4] S. Heise, M. Hlatky, and J. Loviscach, “Automatic\ncloning of recorded sounds by software synthesizers,”\ninAudio Engineering Society Convention 127 . Audio\nEngineering Society, 2009.\n[5] S. Luke, “Stochastic synthesizer patch exploration in\nedisyn,” in International Conference on Computational\nIntelligence in Music, Sound, Art and Design (Part of\nEvoStar) . Springer, 2019, pp. 188–200.\n[6] M. Yee-King and M. Roth, “Synthbot: An unsuper-\nvised software synthesizer programmer,” in ICMC ,\n2008.\n[7] K. Tatar, M. Macret, and P. Pasquier, “Automatic syn-\nthesizer preset generation with presetgen,” Journal of\nNew Music Research , vol. 45, no. 2, pp. 124–144,\n2016.\n[8] M. J. Yee-King, L. Fedden, and M. d’Inverno, “Au-\ntomatic programming of vst sound synthesizers using\ndeep networks and other techniques,” IEEE Transac-\ntions on Emerging Topics in Computational Intelli-\ngence , vol. 2, no. 2, pp. 150–159, 2018.\n[9] O. Barkan, D. Tsiris, O. Katz, and N. Koenigstein,\n“Inversynth: Deep estimation of synthesizer parameter\nconﬁgurations from audio signals,” IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing ,\nvol. 27, no. 12, pp. 2385–2396, 2019.\n[10] G. L. Vaillant, T. Dutoit, and S. Dekeyser, “Improv-\ning synthesizer programming from variational autoen-\ncoders latent space,” in 2021 24th International Con-\nference on Digital Audio Effects (DAFx) , 2021, pp.\n276–283.\n[11] D. Rezende and S. Mohamed, “Variational inference\nwith normalizing ﬂows,” in International conference\non machine learning . PMLR, 2015, pp. 1530–1538.\n[12] P. Esling, N. Masuda, A. Bardet, R. Despres, and\nA. Chemla-Romeu-Santos, “Flow synthesizer: Univer-\nsal audio synthesizer control with normalizing ﬂows,”\nApplied Sciences , vol. 10, no. 1, p. 302, 2019.\n[13] “Dexed github repository,” https://github.com/\nasb2m10/dexed, 2021.\n[14] N. Masuda and D. Saito, “Synthesizer sound matching\nwith differentiable dsp.” in ISMIR , 2021, pp. 428–434.[15] ——, “Improving semi-supervised differentiable syn-\nthesizer sound matching for practical applications,”\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 2023.\n[16] F. Caspe, A. McPherson, and M. Sandler, “Ddx7: Dif-\nferentiable fm synthesis of musical instrument sounds,”\narXiv preprint arXiv:2208.06169 , 2022.\n[17] J. Engel, C. Resnick, A. Roberts, S. Dieleman,\nM. Norouzi, D. Eck, and K. Simonyan, “Neural audio\nsynthesis of musical notes with wavenet autoencoders,”\ninInternational Conference on Machine Learning .\nPMLR, 2017, pp. 1068–1077.\n[18] P. Chandna, M. Blaauw, J. Bonada, and E. Gómez,\n“Wgansing: A multi-voice singing voice synthesizer\nbased on the wasserstein-gan,” in 2019 27th European\nsignal processing conference (EUSIPCO) . IEEE,\n2019, pp. 1–5.\n[19] J. Engel, K. K. Agrawal, S. Chen, I. Gulrajani, C. Don-\nahue, and A. Roberts, “Gansynth: Adversarial neu-\nral audio synthesis,” arXiv preprint arXiv:1902.08710 ,\n2019.\n[20] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z.\nTeoh, J. Sotelo, A. de Brébisson, Y . Bengio, and A. C.\nCourville, “Melgan: Generative adversarial networks\nfor conditional waveform synthesis,” Advances in neu-\nral information processing systems , vol. 32, 2019.\n[21] A. Défossez, N. Zeghidour, N. Usunier, L. Bottou, and\nF. Bach, “Sing: Symbol-to-instrument neural genera-\ntor,” Advances in neural information processing sys-\ntems, vol. 31, 2018.\n[22] J. Engel, L. Hantrakul, C. Gu, and A. Roberts,\n“Ddsp: Differentiable digital signal processing,” arXiv\npreprint arXiv:2001.04643 , 2020.\n[23] O. Barkan and D. Tsiris, “Deep synthesizer parameter\nestimation,” in ICASSP 2019-2019 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) . IEEE, 2019, pp. 3887–3891.\n[24] S. Ruder, “An overview of gradient descent optimiza-\ntion algorithms,” arXiv preprint arXiv:1609.04747 ,\n2016.\n[25] S. Ioffe and C. Szegedy, “Batch normalization: Accel-\nerating deep network training by reducing internal co-\nvariate shift,” in International conference on machine\nlearning . PMLR, 2015, pp. 448–456.\n[26] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” arXiv preprint arXiv:1412.6980 ,\n2014.\n[27] S. Ö. Arık, H. Jun, and G. Diamos, “Fast spectrogram\ninversion using multi-head convolutional neural net-\nworks,” IEEE Signal Processing Letters , vol. 26, no. 1,\npp. 94–98, 2018.\n[28] R. C. Streijl, S. Winkler, and D. S. Hands, “Mean\nopinion score (mos) revisited: methods and applica-\ntions, limitations and alternatives,” Multimedia Sys-\ntems, vol. 22, no. 2, pp. 213–227, 2016.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n648"
    },
    {
        "title": "The Music Meta Ontology: A Flexible Semantic Model for the Interoperability of Music Metadata.",
        "author": [
            "Jacopo de Berardinis",
            "Valentina Anita Carriero",
            "Albert Meroño-Peñuela",
            "Andrea Poltronieri",
            "Valentina Presutti"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265423",
        "url": "https://doi.org/10.5281/zenodo.10265423",
        "ee": "https://zenodo.org/records/10265423/files/000102.pdf",
        "abstract": "The semantic description of music metadata is a key requirement for the creation of music datasets that can be aligned, integrated, and accessed for information retrieval and knowledge discovery. It is nonetheless an open challenge due to the complexity of musical concepts arising from different genres, styles, and periods – standing to benefit from a lingua franca to accommodate various stakeholders (musicologists, librarians, data engineers, etc.). To initiate this transition, we introduce the Music Meta ontology, a rich and flexible semantic model to describe music metadata related to artists, compositions, performances, recordings, and links. We follow eXtreme Design methodologies and best practices for data engineering, to reflect the perspectives and the requirements of various stakeholders into the design of the model, while leveraging ontology design patterns and accounting for provenance at different levels (claims, links). After presenting the main features of Music Meta, we provide a first evaluation of the model, alignments to other schema (Music Ontology, DOREMUS, Wikidata), and support for data transformation.",
        "zenodo_id": 10265423,
        "dblp_key": "conf/ismir/BerardinisCMPP23",
        "keywords": [
            "semantic description",
            "music datasets",
            "alignment",
            "integration",
            "information retrieval",
            "knowledge discovery",
            "music genres",
            "styles",
            "periods",
            "lingua franca"
        ],
        "content": "THE MUSIC META ONTOLOGY: A FLEXIBLE SEMANTIC MODEL\nFOR THE INTEROPERABILITY OF MUSIC METADATA\nJacopo de Berardinis1Valentina Anita Carriero2Albert Meroño-Penuela1\nAndrea Poltronieri2Valentina Presutti2\n1King’s College London, UK\n2University of Bologna, Italy\njacopo.deberardinis@kcl.ac.uk, andrea.poltronieri2@unibo.it\nABSTRACT\nThe semantic description of music metadata is a key re-\nquirement for the creation of music datasets that can be\naligned, integrated, and accessed for information retrieval\nand knowledge discovery. It is nonetheless an open chal-\nlenge due to the complexity of musical concepts arising\nfrom different genres, styles, and periods – standing to\nbeneﬁt from a lingua franca to accommodate various stake-\nholders (musicologists, librarians, data engineers, etc.). To\ninitiate this transition, we introduce the Music Meta on-\ntology, a rich and ﬂexible semantic model to describe mu-\nsic metadata related to artists, compositions, performances,\nrecordings, and links. We follow eXtreme Design method-\nologies and best practices for data engineering, to reﬂect\nthe perspectives and the requirements of various stakehold-\ners into the design of the model, while leveraging ontology\ndesign patterns and accounting for provenance at different\nlevels (claims, links). After presenting the main features\nof Music Meta, we provide a ﬁrst evaluation of the model,\nalignments to other schema (Music Ontology, DOREMUS,\nWikidata), and support for data transformation.\n1. INTRODUCTION\nA music analyst, a computational musicologist, a music li-\nbrarian, and a data engineer are working on a joint project.\nThey need to contribute data from various musical sources,\nranging from music libraries, annotated corpora and tune\nbooks, to audiovisual archives, radio broadcasts, and music\ncatalogues. All data is eventually merged/aggregated as in-\nterconnected corpora, and linked to online music databases\n(e.g. MusicBrainz, Discogs) and knowledge bases (e.g.\nWikidata). This creates opportunities to link cultural her-\nitage artefacts to music industry data (streaming services,\nmusic professionals, etc.) and viceversa.\nThis plot subsumes a recurring challenge for musical\nheritage projects [1]. Besides the individual requirements\n© J. de Berardinis, V .A. Carriero, A. Meroño-Peñuela, A.\nPoltronieri, and V . Presutti. Licensed under a Creative Commons Attri-\nbution 4.0 International License (CC BY 4.0). Attribution: J. de Berar-\ndinis, V .A. Carriero, A. Meroño-Peñuela, A. Poltronieri, and V . Presutti,\n“The Music Meta Ontology: a ﬂexible semantic model for the interop-\nerability of music metadata”, in Proc. of the 24th Int. Society for Music\nInformation Retrieval Conf., Milan, Italy, 2023.of each stakeholder – possibly rooted in different music\ngenres, periods and datasets, a fundamental requirement is\nthe interoperability of music metadata.\nMusic metadata (alias bibliographic, or documentary\nmusic data) is used to consistently identify and describe\nmusical works, their artists, recordings, and performances.\nFor music industry, it allows for efﬁcient management and\ndistribution of music, which facilitate search and recom-\nmendation [2]. When metadata is accurate, it ensures that\nartists receive proper credit and compensation [3]. For mu-\nsical heritage, metadata allows for the preservation and dis-\nsemination of musical works and traditions, but also aid in\nthe research and study of music history and culture [4].\nWhen integrating both views, metadata can help to pro-\nmote diversity and inclusivity in the music industry by\nhighlighting lesser-known genres and artists, while inte-\ngrating information and artefacts of cultural interest [5].\nHence, a model that can consistently describe metadata\nis highly desirable – as it enables linking entities and con-\ncepts from various datasets (e.g. a composer is linked to\na tune that has no authors in another collection). Seman-\ntic Web technologies can help achieve interoperability, as\nthey facilitate data access and integration, resource discov-\nery, semantic reasoning and knowledge extraction [6]. In\nthe Resource Description Framework [7], data is described\nas<subject-predicate-object> triples using on-\ntologies, and released as Knowledge Graphs (KGs).\nTo achieve interoperability, one possibility akin to [8]\nis to let stakeholders design their own domain-speciﬁc on-\ntologies, then use alignment algorithms to ﬁnd connections\nbetween them (e.g. MusicalWork andComposition\nreferring to the same concept). However, this approach\ncomes with three major drawbacks: (i) ontology alignment\nis error-prone, hence links would still require manual in-\nspection; (ii) even when alignment is sound, the semantics\nof classes and relationships may vastly differ across do-\nmains, which in turn, may create inconsistent alignments;\n(iii) it does not address the problem in the long-term.\n1.1 Challenges and requirements for interoperability\nAnother possibility is to reuse current ontologies for mu-\nsic metadata, such as the Music Ontology (MO) [9] and\nthe DOREMUS ontology [10]. However, modelling music\nmetadata across different genres and historical periods, to859accommodate various use cases over heterogeneous data\nsources poses a number of challenges. First of all, it re-\nquires a perspective that harmonises all requirements from\ndifferent stakeholders – to design a model that can be tai-\nlored to different data sources rather than to a single type\nof dataset. We categorise the main challenges and require-\nments for metadata interoperability as follows.\n1.1.1 Domain speciﬁcity hampers interoperability\nWhen looking at current ontologies, MO leans towards\nmodelling discographic data with a focus on contempo-\nrary music, whereas DOREMUS is inherently rooted in\nclassical music. These ontologies have been demonstrated\nto model metadata from MusicBrainz and BBC Music\n[11], and from classical music libraries and radio broad-\ncasts for concerts programming [12], respectively. Their\nspeciﬁcity makes them appealing when downstream ap-\nplications show considerable overlap in terms of require-\nments and data. Examples include the reuse of MO in the\nWASABI project [13], to support the semantic annotation\nof audio music (emotions, lyrics, structures), but also for\nmusic recommendation [14] and listening [15]; and the\nadoption of DOREMUS by Philarmonie de Paris ,Bib-\nlioteque National de France , and Radio France .\nNevertheless, when drifting from discographic data and\nclassical music, or attempting to reuse both models, ad-\ndressing e.g. cultural heritage requirements while fostering\ninteroperability becomes difﬁcult. Indeed, a model reﬂect-\ning the view and the interpretations ascribable to a musical\ngenre, stakeholder, or dataset type may be difﬁcult to reuse\nand extend to other domains. For instance, a music artefact\nmay originate from oral transmission or be the result of a\ncreative process that does not necessarily entail a formal\ncomposition process. The latter is common in songwrit-\ning, but also in folk music whenever a set of tunes (col-\nlected from different manuscripts) allows for the identiﬁca-\ntion of a tune family [16]. Similarly, when expressing rela-\ntionships between musical artefacts (alias derivations), it is\nimportant not to impose any modelling bias that may con-\nstrain possible interpretations (e.g. an arrangement having\nproper musical identity vs simply providing a different in-\nstrumentation). This is commonly referred to as “domi-\nnance of concept” [12], whose deﬁnition should be left to\nusers depending on their data and domain expertise.\nRather than attempting to achieve consensus on musi-\ncal concepts and jargon, accounting for the interoperability\ncalls for an abstraction layer for music metadata (“ zoom-\nout”) that can then be specialised, extended, and adapted\nto address domain-speciﬁc requirements (“ zoom-in ”).\n1.1.2 Expressivity is needed at different levels\nAnother requirement for interoperability and reuse across\nvarious data sources is providing expressivity at different\ndegrees, i.e. the possibility to conveniently describe mu-\nsic metadata at the right level of detail. For example, one\ndata source may have granular/detailed information that\nrequires high semantic expressivity (a composition pro-\ncess spread over different time, places, and involving moreartists); whereas others may have basic (only the name of\nan artist is known) or even incomplete and uncertain infor-\nmation (a composition tentatively attributed to an artist).\nHere, the WikiProject Music1has been successful in\nproviding expressivity to represent music metadata from\ndifferent sources. As an extreme case of ontological ﬂex-\nibility, the schema underlying Wikidata – an open-ended,\nmulti-domain KG built collaboratively like Wikipedia – is\nnot speciﬁed in a previously agreed ontology, and the high\nexpressivity overly adds complexity to the model. This is\ndue to Wikidata’s scope being the most general.\n1.1.3 Provenance is fundamental for data integration\nAccounting for provenance is a central requirement for\nboth cultural heritage and music industry. This becomes\nfundamental when integrating Knowledge Graphs from\ndifferent datasets and stakeholders – as every single bit of\ndata (each triple) should be attributable to a dataset/KG.\nFurthermore, integrating provenance is also needed within\nthe context of a single dataset, at least for claims and links.\nClaims-Interpretations . Cultural heritage applications\noften require representing debatable statements or claims\n[17, 18]. These are usually the result of an interpreta-\ntion process based on factual or documentary evidence\n(a dataset, a manuscript, etc.), and following a methodol-\nogy and/or theory. Examples include personal information\n(e.g. the year/place of birth of a composer), and authorship\nclaims (e.g. a composition being attributed to an artist).\nLinks and identiﬁers . These includes links to artists’\nofﬁcial websites, fan pages, discussion forums, music re-\nviews, record shops; as well as identiﬁers from music\ndatabases (e.g. MusicBrainz, Discogs, AllMusic), stream-\ning platforms (e.g. Deezer, Spotify), and authoritative\nsources (e.g. ISNI, ISWC, ISRC). As most links and iden-\ntiﬁers are crowdsourced or automatically inferred by entity\nlinking algorithms, modelling provenance here promotes\ntraceability and accountability of data sources.\nNotably, Wikidata addresses both these requirements,\nas every triple is considered a statement per se, for which\nso-called references can be appended and ranked. Refer-\nences may include information on the source, whether a\ncomputational method was used, and a date of retrieval.\n1.2 Our contribution\nWe leverage the expertise and complementary views of\nvarious music stakeholders (musicologists, data engineers,\nmusic analysts, and heritage archivists) to contribute:\n• The Music Meta ontology, a rich ﬂexible model to\ndescribe Western music metadata and its provenance\nat different levels of granularity.\n• An example-driven validation of the model, focused\non the data elicited from four different stakeholders.\n• Code support to create Music Meta KGs without ex-\npert knowledge of the model, with automatic align-\nments to the MO, DOREMUS, and Wikidata.\n1https://en.wikipedia.org/wiki/Wikipedia:\nWikiProject_MusicProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8602. RELATED WORK\nBesides metadata, the use of Semantic Web technologies\nin the music domain has contributed several ontologies,\ncovering a variety of musical aspects and spanning both\nsymbolic and audio music.\nAmong them, the Music Theory Ontology [19] describe\ntheoretical concepts of compositions, whereas the Music\nScore [20] and Music Notation [21] propose granular on-\ntologies to represent elements of music scores. OMAC ex-\npresses features of musical entities but also musicological\nclaims [22], while the OMRAS project [23] contributed\nontologies to describe music chords as well as concepts re-\nlated to tonality and temperament.\nIn the audio domain, ontologies describe music produc-\ntion [24], audio features [25], effects [26]; an also model\nlistening habits/taste [27], music-induced emotions [28],\nmusic structure [29, 30], and musical similarities [31].\nThese ontologies have speciﬁc focus, and many were\ndeveloped as stand-alone projects, with little or no align-\nment [32]. Instead, some ontologies focus on achieving in-\nteroperability between notations, taxonomies, and formats.\nThese include the Internet of Musical Things [33], where\nheterogeneous musical objects are envisioned to coexist;\nthe Music Annotation Pattern [34] which allows to model\nmusic annotations in the JAMS format [35]; and the Hamse\nontology [36] describing musical features for musicologi-\ncal research. Similarly, [37] models abstract annotations of\nmusical works, rather than concrete encodings.\nInteroperability at the level of musical content level re-\nsulted in successful MIR applications, such as the MIDI\nLinked Data Cloud [38] – integrating MIDI music to learn\nembeddings over the resulting KG [39]; and ChoCo [40] –\na chord corpus integrating 18 chord datasets and enabling\nnovel workﬂows for computational creativity [41].\n3. THE MUSIC META ONTOLOGY\nTo derive requirements from various music stakeholders,\nwe leverage the domain expertise and views in Polifonia\n– a European H2020 project aiming to connect “music,\npeople, places and events” from the 16th century. The\ninterdisciplinarity of Polifonia, involving data engineers,\nanthropologists, ethnomusicologists, historians of music,\nlinguists, musical heritage archivists, cataloguers, and cre-\native professionals – makes it an ideal testbed for this work.\nMusic Meta is part of the Polifonia Ontology Network\n[42], from which we reuse the CORE module. This is done\nto consistently reuse general-purpose elements of design\n(e.g. Person, Time, Place) and ontology design patterns.\nThe reuse of this module also ensures alignment with other\nfoundational models (FOAF, Dublin Core, etc.).\nThe ontology (preﬁxed as mm) is available at the\nfollowing URI: https://w3id.org/polifonia/\nontology/music-meta/ , and is released as open\nsource project under the CC-BY 4.0 on GitHub2.\n2https://github.com/polifonia-project/\nmusic-meta-ontology3.1 Methodology\nThe development of Music Meta is driven by eXtreme De-\nsign (XD) [43], an agile ontology engineering methodol-\nogy that makes extensive use of ontology design patterns\n(ODPs) – small ontologies that work as reusable templates\nfor recurrent modelling problems. An ODP is intuitive and\ncompact, clearly and formally deﬁned, tackles a speciﬁc\n(sub)set of requirements, and is designed for a modular\nreuse, enabling a pragmatic cognitive analysis [44].\nIn XD, a story-based approach guides the collection of\nrequirements. A story is a framework for customers to de-\nscribe their needs, and is composed of 4 sections: (i) the\npersona, a description of a typical user; (ii) the overarching\ngoal they need to address; (iii) the scenario, describing how\nthe goal will be address; (iv) the competency questions\n(CQs) translating needs into formal requirements. Ontol-\nogy modelling starts iteratively from the CQs, and is based\non the reuse of ODPs and existent templates.\n3.1.1 From FRBR to Information Objects/Realisations\nAt the core of Music Meta lies the use of the Information-\nRealisation (IR) ODP [45]. An information object is a non-\nphysical social object carrying information that can have\none or multiple materialisations ( information realisations ).\nEach realisation is a particular physical object, or event, re-\nalising the the information object , or involving the latter as\na participant. Both information object and realisation are\nintended as information entities (IE), i.e. (social) objects\ncreated and/or used to communicate, reason, and specify\nnew entities. This allows to distinguish between a piece of\ninformation (e.g. the content of a composition) from how\nit is materialised (e.g. as a performance).\nOn the other hand, both the Music Ontology [9] and\nDOREMUS [12] are built on top of different ﬂavours of\nFRBR [46] (FRBRer and FRBRoo, respectively). FRBR\nis a conceptual model describing bibliographic resources at\nfour levels: Work ,Expression ,Manifestation , and Item. In\ncontrast, the two levels of the IR pattern map to Expression\nandItem, since Work andManifestation are said to provide\nnon-informative conceptualisations [45]. Moreover, [47]\nargues that FRBR’s Works – intended as “entities that pre-\nexist expressions”, cannot represent improvisations or tra-\nditional music, as they do not derive from a formal com-\nposition process leading to a realisation. FRBR’s Work\nis often ambiguously intended as an entity retrospectively\ncreated for grouping multiple expressions for cataloguing\nneeds. As for the Manifestation level, while its represen-\ntation is straightforward in the bibliographic domain (e.g.\nthe printed version of a book), its correspondence in the\nmusic domain is not fully intuitive, as it may relate to ei-\nther a recording, a score, a compact disc, or all the above –\nthereby introducing complexity and ambiguity.\nNevertheless, being aligned to two levels of FRBR, the\nIR ODP makes our model leaner and ﬂexible, while still\nachieving interoperability with FRBR-based (music) on-\ntologies. In fact, IE patterns are meant to boost the seman-\ntic integration of contents, tools, platforms, resources that\nare silo-ed or non-interoperable [45].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n861mm:MusicArtist\n core:AgentRole\ncore:Role\ncore:involvesAgentcore:involvesRolerdfs:subClassOf\nmm:Musician\n mm:MusicEnsemble\nrdfs:subClassOf\nmm:MusicAlgorithmcore:isMemberOf\ncore:hasMember\nmm:MusicGenremm:hasGenre\nmm:Award\nmm:nominatedForAward\nmm:receivedAward\ncore:Person\n core:Place\nmm:wasFormedIn\nxsd:dateTimecore:activityStartDate\ncore:activityEndDate\ncore:MusicDataset\nmm:isTrainedOn\ncore:Alias\ncore:hasAliascore:hasLanguage\nxsd:stringcore:name\ncore:Language\ncore:name\nmm:isInfluencedBy\nmm:hasCollaboratedWith\nmm:MusicEnsembleMembership\nmm:MusicEnsemblemm:involvesMusic\nEnsemble\nmm:MusicArtistmm:involvesMemberOf\nMusicEnsemble\ncore:TimeInterval\ncore:hasTimeInterval\ncore:Rolecore:involvesRole*\n*core:isMemberOf\ncore:hasMember*Prefixes\nrdf:\nrdfs:\nowl:\ncore:\nmm:http://www.w3.org/1999/02/22-rdf-syntax-ns#\nhttp://www.w3.org/2000/01/rdf-schema#\nhttp://www.w3.org/2002/07/owl#\nhttps://w3id.org/polifonia/ontology/core/\nhttps://w3id.org/polifonia/ontology/music-meta/\nFigure 1 . Describing music artists as musicians, music ensembles, and algorithms using the Graffoo notation ( yellow boxes\nare classes, blue/green arrows are object/datatype properties, purple circles are individuals, green polygons are datatypes).\n3.2 Main elements of design\nFrom Polifonia’s CQs3, we identiﬁed those related to\nmetadata, and aimed for a model capable to address the\nrequirements in Section 1.1. Music Meta follows a hierar-\nchical design (where each level extends the former to add\nexpressiveness) and is complemented by data transforma-\ntion rules to conveniently translate one level into another.\nTo enable data integration from existing knowledge\nbases and datasets, we align Music Meta to other ontolo-\ngies: the Music Ontology, DOREMUS, and Wikidata, after\nhaving identiﬁed common/similar classes and properties.\n3.2.1 Music artists\nTo represent music creatives the class mm:MusicArtist\ngeneralises over musicians ( mm:Musician ), ensem-\nbles (mm:MusicEnsemble ), and computational meth-\nods (mm:MusicAlgorithm ), as illustrated in Figure 1.\nMusicians are seen as a specialisation of persons who\ncan optionally be associated to a medium of performance\n(e.g. voice, guitar), and be part of a music ensemble (e.g.\nMusicGroup ,Orchestra ,Choir ). Depending on the\ndata available, the latter can be expressed either through\na membership relationship ( core:isMemberOf ), a spe-\ncialisation of the former, such as mm:isSingerOf , or\nthrough amm:MusicEnsembleMembership when the\nperiod of participation of the musician is available.\nAll music artists can be associated to (one or more)\nmm:MusicGenre (s), express inﬂuences or collabora-\ntions, and share a period of activity. Here, the start date\nrefers to the foundation for music ensembles, whereas the\nend date is used for discontinued projects for algorithms.\n3.2.2 Music inception\nThe focal point of Music Meta is the mm:MusicEntity\nclass (Figures 2 and 3). This class represents an Informa-\ntion Object, which is deﬁned as the sum of all the elements\nthat make up a piece of music. A Music Entity is com-\nposed of several components, including lyrics (generalised\n3https://github.com/polifonia-project/storiesthroughmm:Text to also account for mm:Libretto ),\nthe entailed musical content ( mm:AbstractScore ) and\nits instrumentation ( mm:Instrumentation ).\nAmm:AbstractScore provides an abstraction to\ndescribe the musical properties of an entity, such as\nthe form of a piece ( mm:FormType ), its constituents\nparts (e.g. mm:Movement ormm:Section ), and its\nkey (mm:Key ). Datatype properties also describe the\ntempo of the composition ( mm:tempo ) and its order\n(mm:orderNumber ). Amm:Instrumentation can\ninstead be formalised in a mm:Score , which can be either\ndigital or paper. Through the score, the instrumentation\ndescribes one or more mm:MediumOfPerformance ,\neach of which has a cardinality (e.g. 3 violins).\nIt is also possible to describe relationships be-\ntween different Music Entities, deﬁned by parthood\n(mm:hasPart ) and derivation ( mm:isDerivedFrom ).\nDerivations are used at the user’s discretion, based on\nthe dominance of concept [12] (whose criteria attribute\nproper identity to a musical entity) and can be of differ-\nent types: revision, transposition, cover, reconstruction,\nreduction, etc. This makes it possible to describe dif-\nferent types of compositions, rearrangements and modi-\nﬁcations of an original piece, as well as inﬂuences and\nmore complex types of derivations. For example, the\nproduction of a cover song (e.g. in a different mu-\nsical genre) may keep the lyrics and introduce a new\ncomposition and instrumentation, hence resulting in a\nnewmm:MusicEntity . In addition, Music Entities\ncan be organised in mm:Collection , according to a\nmm:CollectionConcept that binds them together.\nIn sum, the model provides ﬂexibility across periods\nand genres as the proposed classes allow generalisations to\nbe made about the text, the musical composition and its ar-\nrangement. (c.f. Section 1.1.1). Through the specialisation\nof classes, depending on the target domain/application,\nspeciﬁcity can easily be achieved (c.f. Section 1.1.2). For\nexample, a tune family can be seen as a mm:Collection\nencompassing several tunes (as music entities) based on\nspeciﬁc criteria (e.g. similarity, provenance).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n862mm:CreativeProcess\n mm:CreativeActionmm:involves\nCreativeAction\ncore:hasPlacecore:hasTime\nIntervalrdfs:subClassOf\ncore:executes\nTaskmm:LyricsWriting\nmm:MusicWriting\nmm:Instrumentation\nmm:Orchestration\nmm:MusicArtist\n core:AgentRole\ncore:Role\ncore:involvesRole\ncore:has\nAgentRolecore:isInvolvedIn\ncore:TimeIndexed\nSituationrdfs:subClassOf\ncore:hasTime\nIntervalcore:involvesAgent\nmm:MusicEntitymm:creates\ncore:Place\n core:TimeInterval\ncore:CreativeTask\ncore:Task\nmm:Remix\nmm:Rearrangementrdf:type\nFigure 2 . Abstracting music inception as an product of a creative process, involving music artists in activities (music\nwriting, instrumentation, etc.), deﬁned in time and space and according to different roles.\nmm:Lyrics\nmm:MusicEntitycore:hasPart\nmm:hasOpus\nStatementmm:hasDedication\nStatement\nmm:Text\nmm:Instrumentation\nmm:AbstractScore core:hasPart\nmm:hasInstrumentationcore:hasPart\nmm:TextFragment\nmm:Score\nmm:MusicSheetrdfs:subClassOf\nmm:DigitalScore\nxsd:integer\nmm:MediumOf\nPerformancemm:hasScore\nmm:CreativeProcess\nmm:Libretto\nmm:hasFormType\nmm:hasTextrdfs:subClassOf mm:creates\ncore:hasPart\nmm:hasKey\nxsd:stringmm:orderNumber\nmm:FormType\nrdfs:subClassOf\nrdfs:subClassOf\nmm:Composition\nPart\nmm:Key\nmm:Dedication\nStatement\nmm:OpusStatement\nxsd:integer\n rdfs:Literal\n core:Personmm:isDedicatedTo core:text\nrdfs:Literal\nmm:tempo\nmm:Movement\nmm:Section\nmm:Collection\nmm:Collection\nConcept\ncore:isDerivedFrom\ncore:isMemberOfcore:isDefinedBycore:hasSource\nmm:ScorePartmm:hasScorePart\nmm:hasMediummm:medium\nCardinalitymm:isRealisedIn\nmm:isRealisedInmm:opusNumber\nmm:opusSubNumber\nFigure 3 . Describing a music entity and the elements it contains: Text, AbstractScore and Instrumentation.\n3.2.3 From performance to recording and broadcast\nThe realisation of a mm:MusicEntity is exempli-\nﬁed bymm:MusicalPerformance , which can be\neither live ( mm:LivePerformance ) or in a studio\n(mm:StudioPerformance ). As illustrated in Fig-\nure 4, the place and time interval of a performance are\ndescribed by core:Place andcore:TimeInterval\n– involving one or more music artists (optionally, with\na speciﬁc role). A performance may also create a new\nmm:MusicEntity if, e.g., the execution differs signif-\nicantly from the original version.\nA Music Entity can also be recorded by means of\namm:RecordingProcess , which is a subclass of a\nmm:CreativeProcess . This makes it possible to de-\nscribe information about both the production (e.g., pro-\nducers) and the technical aspects of it (e.g., sound engi-\nneer, equipment used). The recording process produces a\nmm:Recording , which is contained in a mm:Release .\nInformation about the broadcasting of a recording is\nmodelled through the mm:BroadcastingSituation\nclass (an instance of the Situation ODP [48]), which de-\nscribes when and where the song was broadcast, and by\nwhich broadcaster ( mm:Broadcaster ).3.2.4 Publishing and licensing information\nThemm:PublicationSituation class describes in-\nformation about the publication of a release, which is com-\nmon to the publication of a mm:Score (c.f. Figure 4). For\nboth a release and a score, it describes when and where\nthey were published, and by a mm:Publisher .\nLicence information is described by the mm:License\nclass, which applies to records, releases and scores.\n3.2.5 Modelling links and integrating provenance\nWe propose a pattern based on RDF* [49] to describe the\nprovenance at different levels (Figure 5). The use of RDF*\nis particularly useful for this purpose, as it allows to embed\nprovenance information to every triple in the dataset. This\nsimpliﬁes and streamlines the model, eliminating the need\nfor n-ary relations or reiﬁcation for each triple.\nThe proposed pattern is straightforward and com-\nprises the class core:Reference , which describes the\nsource of the reference (using the class core:Source )\nand the method used to obtain the annotation (us-\ning the class core:SourceMethod ). Addition-\nally, the datatype properties core:confidence and\ncore:retrievedOn describe the conﬁdence of the an-\nnotation and the date it was produced, respectively.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n863rdfs:subClassOfrdfs:subClassOf\nmm:Recording\nProcess\nmm:Recordingmm:produces\nRecording\nmm:Releasemm:isPartOf\nRelease\ncore:title\ncore:hasDurationcore:titlemm:isBroadcastedIn\nmm:Broadcasting\nSituation core:hasTime\nIntervalcore:hasPlace\nmm:hasBroadcaster\ncore:TimeInterval\ncore:Place\nmm:MusicEntity\nmm:is\nRealisedBy\nmm:Musical\nPerformance\ncore:Information\nRealisation\nmm:Studio\nPerformancerdfs:subClassOf\nmm:Live\nPerformance\nmm:CreativeProcess\nmm:creates\nmm:Broadcaster\nxsd:string\nmm:MusicAnnotation\nTimeInterval\nmm:Publication\nSituation\ncore:TimeInterval\n core:Placecore:hasTime\nIntervalcore:hasPlacerdfs:subClassOf\nmm:Publisher mm:hasPublisher\ncore:Agent\nmm:Score\nmm:hasPublication\nSituationmm:hasEquipment\nmm:Recording\nEquipment\nmm:Licensemm:hasLicense\nrdfs:subClassOf\nrdfs:subClassOf\n core:Agentmm:isRecordedBy\nFigure 4 . Describing performance, recording, broadcasting, publication, and licensing.\nowl:Thing\n owl:Thing\nxsd:decimal\ncore:Referencemm:hasReference\nmm:Annotation\nMethodmm:collectedBy\ncore:Sourcemm:hasSource\nxsd:Date mm:retrievedOn\nA source can be a dataset,\na document, an annotation, etc.Crowdsourced,\ncomputationally inferredmm:confidencedisputableRelation\nFigure 5 . Our pattern to describe provenance with RDF*.\n3.3 Conversion rules and code support\nTo facilitate the reuse of Music Meta and its data con-\nversion into OWL/RDF Knowledge Graphs, we developed\nPyMusicMeta – a library to map arbitrary music meta-\ndata into RDF triples. This enables a practical and scalable\nworkﬂows for data lifting to create Music KGs without ex-\npert knowledge of our ontological model. The library is\ndeveloped in Python as an extension of RDF-Lib [50].\nWith each triple, PyMusicMeta adds alignments\nto the supported schema whenever possible. For\nexample, the pseudo triple <DavidBowieURI,\nrdf:type, mm:Musician> in Music Meta will be\ncomplemented with <DavidBowieURI, rdf:type,\nhttp://purl.org/ontology/mo/MusicArtist>\nfor Music Ontology, <DavidBowieURI, rdf:type,\nhttp://erlangen-crm.org/E21_Person> for\nDOREMUS (via the Erlangen Conceptual Reference\nModel [51]) and <DavidBowieURI, rdf:type,\nhttps://www.wikidata.org/wiki/Q639669>\nfor Wikidata; to achieve interoperability of the Music KG.\n4. V ALIDATION AND ADOPTION\nFollowing the XD methodology (c.f. Section 3.1), we val-\nidate Music Meta against the competency questions (CQs)\ndriving its design. In this context, testing consists in for-\nmulating logical statements for each competency question\n– using the ontology as a formal model. Logical state-\nments are encoded as SPARQL queries to evaluate themodel. Examples of tested CQs include “ In which time in-\nterval did the creation process take place? ” and “ Which is\nthe language of the name/alias of a music artist? ”. The\ncomplete list of CQs, together with their correspondent\nSPARQL queries can be found in the project’s repository.\nThis also contributes a test framework where the ontol-\nogy is automatically tested using the available SPARQL\nqueries [52], whenever changes occur or new requirements\nare supported in future versions of Music Meta.\nMusic Meta has already been used in ChoCo [40], the\nlargest Harmony KG to date, obtained from the integra-\ntion of 18 MIR datasets4. The ontology has also been\nspecialised for folk metadata (Tunes Ontology) and ex-\ntended to describe music datasets (CoMeta Ontology). All\nontologies are part of the Polifonia Ontology Network\n(PON) and can be found at https://github.com/\npolifonia-project/ontology-network . We\nalso provide documentation, examples, and tutorials5.\n5. CONCLUSIONS\nThe interoperability of metadata is an essential require-\nment for the integration of music datasets, which is cur-\nrently hampered by the speciﬁcity of existent ontologies.\nOur work addresses interoperability requirements for\nthe design of the Music Meta ontology – a rich and ﬂex-\nible semantic model for (Western) music metadata across\ndifferent genres and periods, for various stakeholders and\nmusic datasets. The model is based on the Information-\nRealisation ontology design pattern, allowing to reduce\ncomplexity while maintaining alignment to other ontolo-\ngies (Music Ontology, DOREMUS). We validate Music\nMeta following the XD methodology, to demonstrate the\nsupport of requirements collected from various stakehold-\ners (music analysts, archivists, musicologists, and data en-\ngineers). The model has modular design – allowing users\nto describe music data depending on their speciﬁcity and\ntype, while providing provenance support through RDF*.\nWe are extending the evaluation of Music Meta across\ncultural heritage and music industry datasets, while work-\ning with our stakeholders to specialise the model for the\nintegration and release of Music Knowledge Graphs.\n4https://github.com/smashub/choco\n5https://polifonia-project.github.io/\nontology-network/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n864Acknowledgements This project has received funding\nfrom the European Union’s H2020 research and innovation\nprogramme under grant agreement No 101004746. The\nauthors also acknowledge Philippe Rigaux, Peter van Kra-\nnenburg, Marco Gurrieri, and Mari Wigham for their sup-\nport and feedback throughout the design of Music Meta.\n6. REFERENCES\n[1] T. Bottini, V . A. Carriero, J. Carvalho, P. Cathé,\nF. Ciroku, E. Daga, M. Daquino, A. Davy-Rigaux,\nM. Guillotel-Nothmann, Gurrieri, P. van Kemenade,\nE. Marzi, A. Meroño Peñuelala, P. Mulholland,\nE. Musumeci, V . Presutti, and A. Scharnhorst, “D1.1\nroadmap and pilot requirements 1st version,” EU Com-\nmission, The Polifonia consortium, Tech. Rep., 2021.\n[2] F. Pachet, “Knowledge management and musical meta-\ndata,” Idea Group , vol. 12, 2005.\n[3] C. Sitonio and A. Nucciarelli, The impact of blockchain\non the music industry . Calgary: International\nTelecommunications Society (ITS), 2018.\n[4] S. Giannoulakis, N. Tsapatsoulis, and N. Grammalidis,\n“Metadata for intangible cultural heritage,” in Proceed-\nings of the 13th international joint conference on com-\nputer vision, imaging and computer graphics theory\nand applications (VISAPP 2018) , 2018, pp. 634–645.\n[5] B. de Miguel-Molina and R. Boix-Doménech, “Intro-\nduction: Music, from intangible cultural heritage to\nthe music industry,” Music as Intangible Cultural Her-\nitage: Economic, Cultural and Social Identity , pp. 3–8,\n2021.\n[6] T. Berners-Lee, J. Hendler, and O. Lassila, “The se-\nmantic web,” Scientiﬁc american , vol. 284, no. 5, 2001.\n[7] O. Lassila, R. R. Swick et al. , “Resource descrip-\ntion framework (RDF) model and syntax speciﬁca-\ntion,” 1998.\n[8] N. Corthaut, S. Govaerts, K. Verbert, and E. Duval,\n“Connecting the Dots: Music Metadata Generation,\nSchemas and Applications.” in ISMIR , 2008, pp. 249–\n254.\n[9] Y . Raimond, S. A. Abdallah, M. B. Sandler, and F. Gi-\nasson, “The Music Ontology.” in ISMIR , vol. 2007.\nVienna, Austria, 2007, p. 8th.\n[10] M. Achichi, R. Bailly, C. Cecconi, M. Destandau,\nK. Todorov, and R. Troncy, “Doremus: Doing reusable\nmusical data,” in ISWC: International Semantic Web\nConference , 2015.\n[11] Y . Raimond and M. Sandler, “Evaluation of the music\nontology framework,” in The Semantic Web: Research\nand Applications: 9th Extended Semantic Web Con-\nference, ESWC 2012, Heraklion, Crete, Greece, May\n27-31, 2012. Proceedings 9 . Springer, 2012.[12] P. Choffé and F. Leresche, “DOREMUS: connecting\nsources, enriching catalogues and user experience,” in\n24th IFLA World Library and Information Congress ,\n2016, pp. 1–20.\n[13] M. Buffa, E. Cabrio, M. Fell, F. Gandon, A. Gi-\nboin, R. Hennequin, F. Michel, J. Pauwels, G. Pellerin,\nM. Tikat et al. , “The WASABI dataset: cultural, lyrics\nand audio analysis metadata about 2 million popular\ncommercially released songs,” in The Semantic Web:\n18th International Conference, ESWC 2021, Virtual\nEvent, June 6–10, 2021, Proceedings 18 . Springer,\n2021, pp. 515–531.\n[14] M. Á. Rodríguez-García, L. O. Colombo-Mendoza,\nR. Valencia-García, A. A. Lopez-Lorca, and G. Bey-\ndoun, “Ontology-based music recommender system,”\ninDistributed Computing and Artiﬁcial Intelligence,\n12th International Conference . Springer, 2015, pp.\n39–46.\n[15] A. Adamou, S. Brown, H. Barlow, C. Allocca, and\nM. d’Aquin, “Crowdsourcing Linked Data on listen-\ning experiences through reuse and enhancement of li-\nbrary data,” International Journal on Digital Libraries ,\nvol. 20, no. 1, pp. 61–79, 2019.\n[16] P. van Kranenburg, B. Janssen, A. V olk et al. , “The\nMeertens tune collections: The annotated corpus (mtc-\nann) versions 1.1 and 2.0. 1,” Meertens Online Reports ,\nvol. 2016, no. 1, 2016.\n[17] M. Daquino, V . Pasqual, and F. Tomasi, “Knowledge\nRepresentation of digital Hermeneutics of archival and\nliterary Sources,” Knowledge Representation of digital\nHermeneutics of archival and literary Sources , pp. 59–\n76, 2020.\n[18] M. Daquino, V . Pasqual, F. Tomasi, and F. Vitali,\n“Expressing Without Asserting in the Arts,” in CEUR\nWORKSHOP PROCEEDINGS , vol. 3160, 2022.\n[19] S. M. Rashid, D. De Roure, and D. L. McGuinness,\n“A Music Theory Ontology,” in Proceedings of the 1st\nInternational Workshop on Semantic Applications for\nAudio and Music , ser. SAAM ’18. New York, NY ,\nUSA: Association for Computing Machinery, 2018, p.\n6–14.\n[20] J. Jones, D. de Siqueira Braga, K. Tertuliano, and\nT. Kauppinen, “MusicOWL: The Music Score Ontol-\nogy,” in Proceedings of the International Conference\non Web Intelligence , ser. WI ’17. New York, NY ,\nUSA: Association for Computing Machinery, 2017.\n[21] S. S.-s. Cherﬁ, C. Guillotel, F. Hamdi, P. Rigaux,\nand N. Travers, “Ontology-Based Annotation of Mu-\nsic Scores,” in Proceedings of the Knowledge Capture\nConference , ser. K-CAP 2017. New York, NY , USA:\nAssociation for Computing Machinery, 2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n865[22] E. M. Sanﬁlippo and R. Freedman, “Ontology for an-\nalytic claims in music,” in New Trends in Database\nand Information Systems , S. Chiusano, T. Cerquitelli,\nR. Wrembel, K. Nørvåg, B. Catania, G. Vargas-Solar,\nand E. Zumpano, Eds. Cham: Springer International\nPublishing, 2022, pp. 559–571.\n[23] G. Fazekas, Y . Raimond, K. Jacobson, and M. San-\ndler, “An overview of Semantic Web activities in the\nOMRAS2 project,” Journal of New Music Research ,\nvol. 39, 12 2010.\n[24] G. Fazekas and M. B. Sandler, “The Studio Ontology\nFramework,” in Proceedings of the 12th International\nSociety for Music Information Retrieval Conference,\nISMIR 2011, Miami, Florida, USA, October 24-28,\n2011 . University of Miami, 2011.\n[25] A. Allik, G. Fazekas, and M. B. Sandler, “An Ontol-\nogy for Audio Features,” in Proceedings of the 17th\nInternational Society for Music Information Retrieval\nConference, ISMIR 2016, New York City, United States,\nAugust 7-11, 2016 , 2016.\n[26] T. Wilmering, G. Fazekas, and M. B. Sandler, “The\nAudio Effects Ontology,” in Proceedings of the 14th\nInternational Society for Music Information Retrieval\nConference, ISMIR 2013, Curitiba, Brazil, November\n4-8, 2013 , 2013.\n[27] Òscar Celma and X. Serra, “FOAFing the music:\nBridging the semantic gap in music recommendation,”\nJournal of Web Semantics , vol. 6, no. 4, pp.\n250–256, 2008, semantic Web Challenge 2006/2007.\n[Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S1570826808000711\n[28] S. Song, M. Kim, S. Rho, and E. Hwang, “Music On-\ntology for Mood and Situation Reasoning to Support\nMusic Retrieval and Recommendation,” in 2009 Third\nInternational Conference on Digital Society , 2009, pp.\n304–309.\n[29] B. Fields, K. R. Page, D. D. Roure, and T. Craw-\nford, “The segment ontology: Bridging music-generic\nand domain-speciﬁc,” in Proceedings of the 2011 IEEE\nInternational Conference on Multimedia and Expo,\nICME 2011, 11-15 July, 2011, Barcelona, Catalonia,\nSpain . IEEE Computer Society, 2011.\n[30] N. Harley and G. Wiggins, “An ontology for abstract,\nhierarchical music representation,” in Demo at the 16th\nInternational Society for Music Information Retrieval\nConference (ISMIR 2015), Malaga, Spain , 2015.\n[31] K. Jacobson, Y . Raimond, and M. B. Sandler, “An\nEcosystem for Transparent Music Similarity in an\nOpen World,” in Proceedings of the 10th International\nSociety for Music Information Retrieval Conference,\nISMIR 2009, Kobe International Conference Center,\nKobe, Japan, October 26-30, 2009 , K. Hirata,\nG. Tzanetakis, and K. Yoshii, Eds. InternationalSociety for Music Information Retrieval, 2009, pp.\n33–38. [Online]. Available: http://ismir2009.ismir.net/\nproceedings/OS1-2.pdf\n[32] V . A. Carriero, F. Ciroku, J. de Berardinis, D. S. M.\nPandiani, A. Meroño-Peñuela, A. Poltronieri, and\nV . Presutti, “Semantic Integration of MIR Datasets\nwith the Polifonia Ontology Network,” in ISMIR Late\nBreaking Demo , 11 2021.\n[33] L. Turchet, F. Antoniazzi, F. Viola, F. Giunchiglia, and\nG. Fazekas, “The internet of musical things ontology,”\nJournal of Web Semantics , vol. 60, p. 100548, 2020.\n[34] J. de Berardinis, A. M. Penuela, A. Poltronieri, and\nV . Presutti, “The Music Annotation Pattern,” in The\nSemantic Web–ISWC 2022 21st International Semantic\nWeb Conference: 13th Workshop on Ontology Design\nand Patterns (WOP2022) , 2022.\n[35] E. J. Humphrey, J. Salamon, O. Nieto, J. Forsyth, R. M.\nBittner, and J. P. Bello, “JAMS: A JSON Annotated\nMusic Speciﬁcation for Reproducible MIR Research.”\ninISMIR , 2014, pp. 591–596.\n[36] A. Poltronieri and A. Gangemi, “The HaMSE Ontol-\nogy: Using Semantic Technologies to support Mu-\nsic Representation Interoperability and Musicological\nAnalysis,” arXiv preprint arXiv:2202.05817 , 2022.\n[37] D. Lewis, E. Shibata, M. Saccomano, L. Rosendahl,\nJ. Kepper, A. Hankinson, C. Siegert, and K. Page,\n“A model for annotating musical versions and\narrangements across multiple documents and media,”\ninProceedings of the 9th International Conference\non Digital Libraries for Musicology , ser. DLfM ’22.\nNew York, NY , USA: Association for Computing\nMachinery, 2022, p. 10–18. [Online]. Available:\nhttps://doi.org/10.1145/3543882.3543891\n[38] A. Meroño-Peñuela, R. Hoekstra, A. Gangemi,\nP. Bloem, R. de Valk, B. Stringer, B. Janssen,\nV . de Boer, A. Allik, S. Schlobach, and K. Page, “The\nMIDI Linked Data Cloud,” in The Semantic Web –\nISWC 2017 . Cham: Springer International Publish-\ning, 2017.\n[39] P. Lisena, A. Meroño-Peñuela, and R. Troncy,\n“MIDI2vec: Learning MIDI embeddings for reliable\nprediction of symbolic music metadata,” Semantic\nWeb, vol. 13, no. 3, pp. 357–377, 2022.\n[40] J. de Berardinis, A. Meroño-Peñuela, A. Poltron-\nieri, and V . Presutti, “ChoCo: a Chord Corpus and\na Data Transformation Workﬂow for Musical Har-\nmony Knowledge Graphs,” in Manuscript under re-\nview, 2023.\n[41] ——, “The Harmonic Memory: a Knowledge Graph of\nharmonic patterns as a trustworthy framework for com-\nputational creativity,” in Proceedings of the ACM Web\nConference 2023 (WWW ’23), April 30-May 4, 2023,\nAustin, TX, USA , 2023.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n866[42] J. de Berardinis, V . A. Carriero, N. Jain, N. Lazzari,\nA. Meroño-Peñuela, A. Poltronieri, and V . Presutti,\n“The polifonia ontology network: Building a semantic\nbackbone for musical heritage,” in Manuscript under\nreview , 2023.\n[43] E. Blomqvist, K. Hammar, and V . Presutti, “Engi-\nneering Ontologies with Patterns-The eXtreme Design\nMethodology.” Ontology Engineering with Ontology\nDesign Patterns , no. 25, 2016.\n[44] V . A. Carriero, M. Daquino, A. Gangemi, A. G. Nuz-\nzolese, S. Peroni, V . Presutti, and F. Tomasi, “The\nLandscape of Ontology Reuse Approaches,” in Ap-\nplications and Practices in Ontology Design, Extrac-\ntion, and Reasoning , ser. Studies on the Semantic Web,\nG. Cota, M. Daquino, and G. L. Pozzato, Eds. Ams-\nterdam: IOS Press, 2020, vol. 49, pp. 21–38.\n[45] A. Gangemi and S. Peroni, “The Information Realiza-\ntion Pattern,” in Ontology Engineering with Ontology\nDesign Patterns - Foundations and Applications , ser.\nStudies on the Semantic Web, P. Hitzler, A. Gangemi,\nK. Janowicz, A. Krisnadhi, and V . Presutti, Eds. IOS\nPress, 2016, vol. 25, pp. 299–312. [Online]. Available:\nhttps://doi.org/10.3233/978-1-61499-676-7-299\n[46] “Genius website,” https://www.iﬂa.org, accessed:\n2023-04-14.\n[47] J. Riley, “Application of the Functional Requirements\nfor Bibliographic Records (FRBR) to Music.” in IS-\nMIR, 2008, pp. 439–444.\n[48] A. Gangemi, N. Guarino, C. Masolo, A. Oltramari,\nand L. Schneider, “Sweetening ontologies with dolce,”\ninKnowledge Engineering and Knowledge Manage-\nment: Ontologies and the Semantic Web , A. Gómez-\nPérez and V . R. Benjamins, Eds. Berlin, Heidelberg:\nSpringer Berlin Heidelberg, 2002, pp. 166–181.\n[49] O. Hartig, “Foundations of RDF* and SPARQL* (An\nAlternative Approach to Statement-Level Metadata in\nRDF),” in Alberto Mendelzon Workshop on Founda-\ntions of Data Management , 2017.\n[50] C. Boettiger, rdﬂib: A high level wrapper around the\nredland package for common rdf applications , 2018.\n[Online]. Available: https://doi.org/10.5281/zenodo.\n1098478\n[51] J. Merges, M. Scholz, and G. Goerz, “Erlangen imple-\nmentation of frbroo,” in CIDOC 2012 , 2012.\n[52] A. Fernández-Izquierdo, “Ontology testing based on\nrequirements formalization in collaborative develop-\nment environments.” in DC@ ISWC , 2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n867"
    },
    {
        "title": "Exploring Sampling Techniques for Generating Melodies With a Transformer Language Model.",
        "author": [
            "Mathias Rose Bjare",
            "Stefan Lattner",
            "Gerhard Widmer"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265411",
        "url": "https://doi.org/10.5281/zenodo.10265411",
        "ee": "https://zenodo.org/records/10265411/files/000096.pdf",
        "abstract": "Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed \"typical sampling\", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.",
        "zenodo_id": 10265411,
        "dblp_key": "conf/ismir/BjareLW23",
        "keywords": [
            "sampling techniques",
            "musical qualities",
            "distribution truncation sampling",
            "nucleus sampling",
            "typical sampling",
            "conventional ancestral sampling",
            "objective evaluations",
            "subjective evaluations",
            "well-calibrated model",
            "suboptimal circumstances"
        ],
        "content": "EXPLORING SAMPLING TECHNIQUES FOR GENERATING MELODIES\nWITH A TRANSFORMER LANGUAGE MODEL\nMathias Rose Bjare1Stefan Lattner2Gerhard Widmer1,3\n1Institute of Computational Perception, Johannes Kepler University Linz, Austria\n2Sony Computer Science Laboratories (CSL), Paris, France\n3LIT AI Lab, Linz Institute of Technology, Austria\nmathias.bjare@jku.at, stefan.lattner@sony.com, gerhard.widmer@jku.at\nABSTRACT\nResearch in natural language processing has demonstrated\nthat the quality of generations from trained autoregressive\nlanguage models is signiﬁcantly inﬂuenced by the used\nsampling strategy. In this study, we investigate the impact\nof different sampling techniques on musical qualities such\nas diversity and structure. To accomplish this, we train\na high-capacity transformer model on a vast collection of\nhighly-structured Irish folk melodies and analyze the mu-\nsical qualities of the samples generated using distribution\ntruncation sampling techniques. Speciﬁcally, we use nu-\ncleus sampling, the recently proposed \"typical sampling\",\nand conventional ancestral sampling. We evaluate the ef-\nfect of these sampling strategies in two scenarios: optimal\ncircumstances with a well-calibrated model and subopti-\nmal circumstances where we systematically degrade the\nmodel’s performance. We assess the generated samples\nusing objective and subjective evaluations. We discover\nthat probability truncation techniques may restrict diver-\nsity and structural patterns in optimal circumstances, but\nmay also produce more musical samples in suboptimal cir-\ncumstances.\n1. INTRODUCTION\nIn recent years, developments in natural language mod-\nelling have also accelerated the ﬁeld of symbolic music\ngeneration. In this context, the musical events of a mu-\nsic piece are represented as a sequence of symbols or to-\nkens from a ﬁxed vocabulary, and the goal is to learn to\ngenerate new token sequences. At present, the autoregres-\nsive transformer model [1] is the basis of many symbolic\nmusic generation models [2–5]. In this context, a con-\nditional distribution is learned by solving a masked self-\nprediction task [2–5], and generation is performed with\nstochastic sampling techniques, e.g., ancestral sampling, or\nmaximization-based search techniques, e.g., beam search.\n© M. Bjare, S. Lattner, and G. Widmer. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: M. Bjare, S. Lattner, and G. Widmer, “Exploring Sam-\npling Techniques for Generating Melodies with a Transformer Language\nModel”, in Proc. of the 24th Int. Society for Music Information Retrieval\nConf., Milan, Italy, 2023.However, the choice of decoding technique has been\nshown to impact various qualitative features of generated\nsamples substantially. In [6], the authors showed that gen-\neration with nucleus sampling yields natural language sam-\nples that are more contextualized than those from conven-\ntional sampling techniques, and samples of nucleus sam-\npling score higher in human evaluations. More recently,\nthe authors of [7] propose typical sampling and show that\nit reduces degenerate sample generation while exhibiting\nperformance competitive with nucleus sampling. Typical\nsampling is based on the authors’ ﬁnding that words in hu-\nman language are typical . More speciﬁcally, the authors\nshow that most words of human language are, in fact, not\nthe most likely words (lowest information content (IC)), as\nmeasured with a language model, but rather typical words,\ni.e., they have an IC close to the conditional entropy of the\nlanguage model. Typical sampling explicitly enforces this\ncondition.\nWe hypothesize that a careful choice of sampling tech-\nnique could also improve certain aspects of music gener-\nated using language models, particularly because in [8],\nit has been shown that musical events tend to be typical.\nHowever, we ﬁnd that many music generation systems rely\non ordinary sampling techniques. In addition, studies on\nthe effect of sampling techniques on musical qualities are\nlimited.\nIn this work, we study the structural and tonal proper-\nties of music generated with different sampling techniques\napplied to a high-capacity transformer model. Speciﬁcally,\nwe measure the IC, long and short-term self-similarities\nand scale consistency of samples generated with conven-\ntional sampling, nucleus sampling, and typical sampling.\nWe test the sampling techniques for a well-calibrated\nmodel and for under-calibrated models. We support our\nﬁndings by performing a listening study. We conduct our\nexperiments on The Session dataset [9], a large dataset of\nwell-structured monophonic music in the established musi-\ncal genre of Irish traditional music. We choose this dataset\nsince we expect it to provide suitable conditions for train-\ning a well-calibrated model. Our ﬁndings suggest that trun-\ncation techniques can address inadequacies of models that\nare not well-ﬁtted to the data.8102. BACKGROUND AND RELATED WORK\nAlthough maximization-based techniques like beam search\nwork well for directed language generation tasks1(such\nas machine translation and summarization), beam search\nhas been shown to produce dull and repetitive samples for\nopen-ended language generation tasks2[6], an effect that\ncan be observed in music generation as well [10]. It is,\ntherefore, more common to use stochastic sampling tech-\nniques3for open-ended generation tasks. The most ob-\nvious method is ancestral sampling, where one token at a\ntime is sampled based on the predicted distribution, condi-\ntioned on the previously generated tokens. However, it has\nbeen shown that truncating the conditional distribution (by\nsetting the probability of speciﬁc tokens to zero, followed\nby renormalising), can lead to better sample quality than\nthe non-truncated variant. An example of distribution trun-\ncation is top- ksampling, where all but the kmost probable\ntokens are zeroed. In [12], the authors showed that top- k\nsampling generates more coherent samples than the non-\ntruncated variant. In [6], it is explained that the quality\nimprovement of top- ksampling is caused by removing un-\nreliably estimated low-probability tokens, and it is found\nthat top-ksampling mitigates the problem. However, it is\nalso shown that top- ksampling is sensitive to the distribu-\ntion’s entropy (see Section 3.3), making it hard to select a\nvalue ofkthat ﬁts both high and low certainty conditions.\nAs a solution, they propose nucleus sampling that assigns\nzero probability to the largest set of least probable tokens\nthat together have a probability below a given threshold.\nThe authors ﬁnd that the samples produced using the tech-\nnique are preferred by humans over other sampling tech-\nniques. Nucleus sampling has been used in music genera-\ntion in [13–15], but its effects are difﬁcult to quantify with-\nout comparisons to the non-truncated case. Although nu-\ncleus sampling mitigates the problem of poorly estimated\nlow-probability tokens, it does not prevent generating de-\ngenerated repetitive sequences caused by low entropy dis-\ntributions (see Section 3). As a solution, in [7], the authors\npropose typical sampling and show that this technique pre-\nvents degenerated sample generation.\n3. ANCESTRAL SAMPLING\nLetp(xt|x<t)be the conditional probability of a symbol\nxtgiven previously observed symbols x<t(i.e., the con-\ntext) and let qbe a model ﬁtted to p, e.g., a neural net-\nwork ﬁtted via likelihood maximization. Given a model\nq, ancestral sampling samples one token at a time using\nx0∼q(·),x1∼q(·|x0),...,xt∼q(·|x<t).\n1Generation with input sequence conditioning.\n2Generation without input sequence conditioning.\n3In the context of generative models, “ sampling techniques ” could re-\nfer to a multitude of aspects in the generative pipeline (e.g., Gibbs sam-\npling in restricted Boltzmann machines [11]). In our work, “ sampling\ntechniques ”, refers to techniques for obtaining samples from a trained\nlanguage model.3.1 Distribution truncation sampling techniques\nIn distribution truncation, a truncated distribution /tildewideqis ob-\ntained by zeroing the probability of a subset of tokens and\nrenormalising the resulting distribution. Formally, /tildewideqis de-\nﬁned by\n/tildewideq(xt|x<t) =/braceleftBiggq(xt|x<t)/summationtext\nv∈Vq(v|x<t)ifxt∈V\n0 otherwise, (1)\nwhereVis the set of tokens with nonzero probability in\n/tildewideq. For the remainder of this article, we use ‘ conventional\nsampling ’ to denote sampling from untruncated distribu-\ntions.\n3.2 Nucleus sampling\nIn nucleus sampling, Vis deﬁned as the smallest set such\nthat /summationdisplay\nv∈Vq(v|x<t)≥τ, (2)\nwhereτis a constant determining the number of tokens to\nbe removed.\n3.3 Typical sampling\nIn typical sampling [7], Vis deﬁned in terms of the token\ninformation content described below.\nDeﬁnition 3.1 (Conditional information content) .The\nconditional information content (IC) is given by\nIC(xt|x<t) =−logq(xt|x<t). (3)\nIn computational music perception, IC has been used to\nmodel how surprising a musical event is given the musical\ncontext [16–18].\nDeﬁnition 3.2 (Conditional entropy) .The conditional en-\ntropy is the expected conditional information content\nH(xt|x<t) =Ext∼q(·|x<t)[IC(xt|x<t)]. (4)\nThe entropy of a distribution explains how conﬁdent a\nmodel is. It ranges from 0tolognwherenis the num-\nber of symbols in the vocabulary, with 0indicating that the\ndistribution is deterministic and lognindicating that the\ndistribution is uniformly random. In typical sampling, the\nprobabilities of tokens with the highest deviation of infor-\nmation from the entropy\n|H(xt|x<t)−IC(xt|x<t)| (5)\nare set to zero. More precisely, let U=v1,v2,...,vnbe\nan ascending ordering of the vocabulary in accordance to\nEquation (5). Then Vis deﬁned as the smallest preﬁx of\nUsuch that q(U|x<t)≥τ. Equation (5) implies that V\nis restricted by a band around the entropy as shown in Fig-\nure 1. Therefore, also the most likely token under qcan\nhave zero probability in ˜q. The authors of [7] note that\nthis property, however, lowers the number of degenerately\nrepetitive samples, as opposed to nucleus sampling, with-\nout degrading preference in human evaluations.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n811domain ( )probablity (p)Typical SamplingFigure 1 : In typical sampling, a probability band around\nthe entropy (dark-grey) deﬁnes the set Vof tokens with\nnon-zero probabilities in the truncated distribution.\n4. EXPERIMENTS\nIn this section, we describe the setup for both our objective\nand subjective experiments, the used data, training details,\ndegradation scenarios, and generation details, as well as\nobjective and subjective evaluation.\n4.1 Data\nOur experiments are performed on monophonic symbolic\nmusic. Speciﬁcally, we use the midi-encoded version of\ntheThe Session dataset [9], consisting of 45,849 tradi-\ntional Irish folk tunes originally encoded in ABC notation.\nWe discard the 5%longest sequences to lower the compu-\ntational footprint of the autoregressive transformer model,\nand partition the dataset in training, validation, and test sets\nwith proportions 10/12, 1/12, and 1/12, respectively. All\nanalyses will be performed on the test set, while our gen-\nerative models will be trained and optimized on the train-\ning and validation sets, respectively. The dataset contains\ntunes with the same name, corresponding to different ver-\nsions of the same tune. We ensure that tunes with the same\nname appear in exactly one of the three sets.\nWe tokenize the sequences using a modiﬁed version of\nthe popular REMI representation [3]. REMI serializes a\nscore bar-wise from left to right. A bar is serialized as a\nsequence of tokens starting with a bar-delimiter token fol-\nlowed by a serialization of the notes within that bar. Each\nnote is serialized as three tokens indicating the onset within\nthe bar, the pitch and the duration, in that order. The posi-\ntion and duration tokens are quantized to 1/12th of a beat.\nContrary to the original REMI implementation, we omit\nvelocity, tempo change and chord symbols, since these are\nnot encoded in the original ABC ﬁles either. Similar to [4],\nwe extend the REMI representation with time-signature to-\nkens inserted immediately after the bar token. We base our\ntokenization implementation on a modiﬁed version of the\nREMI python implementation in MidiTok [19].4.2 Training\nWe train a 21-layered Transformer decoder model [20]\nwith relative attention [2, 21] in a self-supervised predic-\ntion task. We train the model using Adam optimization\n[22] with a learning rate of 10−4until no improvement\ntakes place on the validation set during 10 subsequent\nepochs. The used batch size is 16, and the input sequence\nlength is 512 tokens. Sequences shorter than 512 tokens\nare zero-padded. The negative log-likelihood (NLL) on\nthe test dataset is measured to be NLL= 0.30, which\nis similar to the result of a recent transformer-based model\ntrained on the same dataset [18]. We thus call this a well-\ncalibrated model .\n4.3 Model Degradation\nIn addition to the well-calibrated model, we consider two\nunder-calibrated models , which we achieve by intention-\nally degrading the well-calibrated model. For our ﬁrst\ndegradation, we scale the logits vector hof the transformer\nsoftmax output distribution, i.e.,\nq(xt|x<t) =Softmax(h/r), (6)\nwherer >1.0is a temperature scale. This degradation in-\ncreases the distribution’s entropy (uncertainty) while keep-\ning the relative ordering of the probabilities the same. Us-\ning temperature scaling, we deliberately increase the prob-\nability of token predictions xtthat ﬁt the token context\nx<tpoorly, thereby simulating the failure case of unreli-\nably estimated tokens reported for conventional sampling\n(see section 2), where truncation techniques are expected\nto provide better results. We empirically set rto the mini-\nmal value that leads to an audible degradation of the gen-\nerated sequences. This resulted in r= 1.5. The NLL\nof the test data under the temperature-degraded model is\nmeasured to be NLL= 0.31, which is an increase of 0.01\ncompared to the well-calibrated model.\nSecondly, we consider an unbiased degradation where\nwe perturb the network weights by adding a small amount\nof Gaussian noise. More speciﬁcally, for every weight ma-\ntrixWof the well-calibrated model, we obtain a degraded\nweight matrix W′by adding noise zWtoW\nW′=W+kzW, (7)\nwherezW∼ N(0,std(W))andkis a constant. We sam-\nple the noise vector once and keep it ﬁxed for all our ex-\nperiments. We empirically set kto be the minimal value\nwhere sample degradations are audible, which results in\nk= 0.175. The NLL of the test data under the resulting\nmodel is measured as NLL= 0.36, which is an increase\nof0.06.\n4.4 Generation\nWhen generating sequences with the learned models, for\nall models, we perform conventional sampling, nucleus\nsampling and typical sampling as described in section 3.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n812We sample until either the end-of-sequence token is en-\ncountered or a maximum length is reached. Due to com-\nputing limitations, we ﬁx the maximum sequence length\nto the 80%-quantile of the dataset song-length distribution.\nWe keep both sequences which terminate with the end-of-\nsequence token and sequences with the maximum length\nreached in our sample sets.\n4.5 Objective Evaluation\nThe objective evaluations are performed by calculating dif-\nferent statistics from the generated sequences and compar-\ning the results between different (non-)degradations, sam-\npling types and with the original reference data.\n4.5.1 Surprisal\nWe are interested in the degree of surprisal of the samples\ngenerated with the different sampling methods. Similar\nto [16–18], we measure surprisal using the IC of events.\nAs we do not have access to the data distribution, we inter-\npret the well-calibrated model to be an oracle that approxi-\nmates the data distribution. We then use the well-calibrated\nmodel to measure the mean IC of all events from a speciﬁc\nsampling method and model.\n4.5.2 Structural Consistency\nWe measure structural consistency by investigating the\nself-similarities of the generated pieces. Similar to [5],\nwe compute a self-similarity distribution from samples of\na given sampling method and contrast it with the similar-\nity distribution calculated from real data. To do so, we\nﬁrst compute the similarity between bar pairs separated by\nmeasure lags of size t. This is done for each tune xin\nsample sets Daccording to\nlx\ni,i+t=|N(i)∩N(i+t)|\n|N(i)∪N(i+t)|, (8)\nwhere the set of notes in the i-th bar is denoted as N(i),\nand two notes are deemed equal if their pitches, durations,\nand onset positions within their respective bars are identi-\ncal. The similarity score lx\ni,jbetween any two bars ranges\nfrom 0.0 to 1.0, with a score of 1.0 indicating that the two\nbars are identical. After computing the similarity for all\npossible lags in each tune of a sample set D, we calculate\nthe average similarity scores of that sample set by\nLD\nt=1\n|D|\n/summationdisplay\nx∈D/summationdisplay\nj=i+tlx\ni,j\n. (9)\nNote that eq. (9) does not deﬁne a probability distribution\nand does not, in general, sum to one. For each dataset, we\nthen calculate an overall self-similarity score\nSS(D) =1\nTT/summationdisplay\nt=1LD\nt, (10)\nwhereTis the maximum bar lag considered. SS(D)cap-\ntures both short-term self-similarities, e.g., repetitions orvariations of motives, and long-term self-similarities, e.g.,\nrepetitions or variations of musical segments. Similar to\n[5], we also consider the deviation of a sample set’s simi-\nlarity distribution LD\ntto the dataset’s similarity distribution\nLtgiven by\nSE(D) =1\nTT/summationdisplay\nt=1/vextendsingle/vextendsingleLt−LD\nt/vextendsingle/vextendsingle. (11)\nWe interpret this deviation as a measure of how closely the\nself-similarities of tunes generated with the different sam-\npling techniques follow the self-similarities of tunes found\nin the dataset. We set T= 38 in our experiments (i.e.,\nthe smallest maximum number of bars generated by any\nmethod).\n4.5.3 Tonal Consistency\nWe are furthermore interested in the tonality coherence of\nsamples generated with the sampling methods. Speciﬁ-\ncally, we investigate the scale consistency [23], i.e., the\nmaximum percentage of notes ﬁtting a diatonic scale. The\nscale consistency is therefore calculated by\nmax\nscale#pitch _in_scale(x,scale)\n#pitches(x). (12)\nA scale consistency value of 1.0 indicates that all pitches\nare within a single scale, whereas lower values indicate\nmore complex harmonic structures.\n4.6 User Study\nIn addition to the objective evaluations described above,\nwe also perform a user study to gather subjective evalu-\nations of the tunes’ musical quality, structural properties\nand complexity. For that, we hosted a website consisting\nof two pages. The ﬁrst page explains the purpose of the\nstudy, speciﬁcally that it aims to evaluate sampling tech-\nniques for neural network music generation. Furthermore,\nthe users are instructed to rate the respective tunes using\nthe attributes overall quality ,short-term structure ,long-\nterm structure andcomplexity using a 5-point Likert scale.\nThe users are also asked to use appropriate headphones or\nloudspeakers and to announce their level of musical ex-\npertise with choices { Beginner ,Intermediate ,Expert }. On\nthe second page, a list of 10audio widgets is displayed,\none for each tune. Below each widget, the Likert scales\nfor the4different attributes (as described above) are pro-\nvided for voting. In addition, the users can click on a “sheet\nlink” that opens a window displaying the tune in staff no-\ntation. The 10tunes for every user constitute the Cartesian\nproduct of all three sampling methods (i.e., conventional ,\nnucleus ,typical ) and all three model modes (i.e., well-\ncalibrated ,temperature degradation ,noise-degradation )\nplus a reference tune. It is ensured that every user obtains\nunique tunes sampled randomly from a set of 500 instances\nfor each of the 10types, presented in a random order. To\nprevent biases, every user is allowed to perform the study\nonly once.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8130.4 0.5 0.6 0.7 0.8 0.9 1.0101100IC\nInformation Content\nWELL_CONV\nWELL_NUCL\nWELL_TYP\nTEMP_CONV\nTEMP_NUCL\nTEMP_TYP\nNOISE_CONV\nNOISE_NUCL\nNOISE_TYP\nREFERENCE\nFigure 2 : Information content of generated data using dif-\nferent sampling strategies and τvalues under the well-\ncalibrated model.\n5. RESULTS AND DISCUSSION\nIn this section, we present the results of the experiments\ndescribed in Section 4. For the ﬁgures and tables, we\nuse the abbreviations WELL, NOISE and TEMP for the\nwell-calibrated, noise-degraded and temperature-degraded\nmodels, respectively. To these abbreviations, we append\nCONV , NUCL and TYP for conventional sampling, nu-\ncleus sampling and typical sampling correspondingly.\n5.1 Objective Evaluation\nIn the following section, we analyse and discuss the results\nof our objective and subjective evaluations.\n5.1.1 Surprisal\nWe report the results of the IC estimation in Figure 2 for\nthe truncation degrees τ= 0.4,...,1.0. The samples from\nthe well-calibrated model have the lowest IC and the IC\nof samples from the temperature-degraded model is higher\nthan the IC of samples from the noise-degraded model. For\nboth nucleus and typical sampling, the IC decreases with\ndecreasing τ. For typical sampling in particular, this sug-\ngests that relatively more high information than low infor-\nmation tokens are pruned, similar to what is found in [8].\nFor most degradation scenarios and sampling methods, a τ\nvalue between 0.8and0.9is shown to recover the original\ndata distribution best.\n5.1.2 Structural Consistency\nWe compute the self-similarity (see Equation (10)) for all\nmodels and sampling techniques and show the result in\nFigure 3a. Similarly, we plot the self-similarity devia-\ntion (see Equation (11)) in Figure 3b. From Figure 3a,\nwe ﬁnd that the overall self-similarity of samples pro-\nduced with typical and nucleus sampling increases as τ\ndecreases. This holds for both degraded models and the\nwell-calibrated model. However, we ﬁnd that the increase\nin self-similarity is more moderate for samples generated\nwith typical sampling than those of nucleus sampling, in-\ndicating that the removal of highly probable tokens keepsthe self-similarity at more moderate levels. In the tem-\nperature degradation scenario, we ﬁnd that moderate lev-\nels of truncation lower the self-similarity deviation for the\ntemperature-degraded model and thereby counteract the\ntemperature degradation (with an optimal τof0.8and\n0.6for nucleus sampling and typical sampling, respec-\ntively). In fact, in this scenario, the self-similarity of sam-\nples generated with nucleus and typical sampling follows\nthe self-similarity of the reference distribution closer than\nsamples generated with ordinary sampling for most tested\ntruncation strengths. This is not the case for the unbiased\nnoise degradation, where the self-similarity increases with\nhigher truncation strengths, increasing also the deviation\nfrom the reference statistics.\n5.1.3 Tonal Consistency\nWe inspect the tonal consistency by calculating the scale\nconsistency (see Equation (12)) and report the results in\nFigure 3c. For both nucleus and typical sampling, we\nﬁnd that samples generated with low values of τlead to a\nhigher degree of scale consistency. Furthermore, we ﬁnd\nfor any given τthat generations from typical sampling\nhave lower scale consistency than samples generated\nwith nucleus sampling. Especially when considering\ntemperature degradation, the scale consistency of nucleus\nsampling is almost at the level of the reference distribution\natτ= 0.9, whereas typical sampling stays low even at\nhigh levels of τ. An important observation is that (with the\nexception of typical sampling in the temperature degra-\ndation scenario) there is an optimal τfor both truncation\ntechniques that leads to a recovery of the dataset’s scale\nconsistency statistic in both degradation scenarios.\nSimilar to the ﬁndings in [6] for natural language, our\nobjective evaluations in the high-temperature scenario in-\ndicate that the musical statistics of the samples generated\nwith truncation techniques more closely match the statis-\ntics of samples from the reference distribution. This ﬁnd-\ning implies that truncation sampling techniques can be ap-\nplied to music generative language models, similar to their\napplication in natural language. This can help remove to-\nkens with unreliable probability estimates that do not ﬁt\nthe musical context well. This approach may have impli-\ncations for more complex datasets and limited resources,\nwhere obtaining a well-calibrated model can be challeng-\ning.\n5.2 User Study\nThe user study was performed by 38participants who,\naccording to their self-assessment can be divided into 8\nbeginners, 18intermediate and 12musical experts. The\npresented melodies (except the reference ) are generated\nas described in Section 4.4, with τ= 0.8for both, nu-\ncleus and typical sampling. Table 1 shows the user study\nresults. As there is a high variance for all ratings, we\nperformed for all attributes a Welch’s t-test between all\nm= 10 tune types. Using a desired signiﬁcance level of\nα= 0.05, the corresponding Bonferroni correction to theProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8140.4 0.5 0.6 0.7 0.8 0.9 1.0102101\nWELL-CONV\nWELL-NUCL\nWELL-TYP\nNOISE-TYP\nNOISE-NUCL\nNOISE-TYP\nTEMP-CONV\nTEMP-NUCL\nTEMP-TYPSelf-similarity(a)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0101Self-similarity deviation WELL-CONV\nWELL-NUCL\nWELL-TYP\nNOISE-CONV\nNOISE-NUCL\nNOISE-TYP\nTEMP-CONV\nTEMP-NUCL\nTEMP-TYP (b)\n0.4 0.5 0.6 0.7 0.8 0.9 1.00.900.920.940.960.981.00\nWELL_CONV\nWELL_NUCL\nWELL_TYP\nNOISE_CONV\nNOISE_NUCL\nNOISE_TYP\nTEMP_CONV\nTEMP_NUCL\nTEMP_TYP\nREFERENCEScale consistency (c)\nFigure 3 : Structural and tonal consistency for different model degradations, sampling strategies and τvalues. In (a) the self-\nsimilarity of sample sets generated with different sampling techniques is shown. Higher values indicate a higher degree of\nself-similarities. In (b) the deviation of the generated samples’ self-similarities to the self-similarity and the data reference\ndistribution is shown. A deviation of 0 indicates that the self-similarity of a sample set ﬁts the reference distribution exactly.\nIn (c) the scale consistency of different sample strategies and the reference dataset is shown.\nMethod QULT ST_STR LT_STR CPLX\nREFERENCE 3.7±1.0 3.8±1.0 3.7±1.1 3.6±0.8\nWELL_CONV 3.2±1.1 3.7±0.9 3.5±1.2 3.3±1.0\nWELL_NUCL 3.6±1.1 3.9±1.1 3.7±1.1 2.8±1.0\nWELL_TYP 3.4±1.2 3.6±0.9 3.7±1.0 3.3±1.0\nNOISE_CONV 2.7±1.0 3.2±0.9 3.0±1.0 2.8±0.9\nNOISE_NUCL 2.6±1.3 3.2±1.4 2.8±1.5 2.5±1.2\nNOISE_TYP 2.7±1.1 3.2±1.1 3.1±1.2 2.4±1.0\nTEMP_CONV 2.1±1.3 2.7±1.1 2.1±1.1 3.7±1.0\nTEMP_NUCL 3.4±1.2 3.6±0.9 3.4±1.3 3.4±1.1\nTEMP_TYP 2.2±1.1 2.7±0.9 2.4±1.0 3.3±0.8\nTable 1 : Results showing the mean-opinion scores of the\nuser study ±the standard deviation. QULT denotes the\noverall quality estimation, ST_STR the perceived short-\nterm structure, LT_STR the perceived long-term structure\nand CPLX the perceived complexity of the rated samples.\nmultiple comparisons problem gives a signiﬁcance level of\nα\n1\n2m(m−1)=0.05\n45= 0.001. We can see in the ﬁrst column\nthat the human-composed reference tracks have the high-\nest quality scores on average and that the perceived quality\nof the tunes tends to degrade for the noise- and tempera-\nture degradation cases as expected. The t-test shows that\nthe users’ preference for REFERENCE is signiﬁcant com-\npared to all samples of the under-calibrated models (with\np <1×10−4), except for TEMP_NUCL with p= 0.37.\nThis shows that nucleus sampling can potentially improve\nthe sample quality of low-conﬁdence models, while typi-\ncal sampling is not able to recover any degradations. Fur-\nthermore, we ﬁnd that WELL_CONV , WELL_NUCL and\nWELL_TYP differ in QULT with p= 0.07,0.67and\n0.37respectively compared to REFERENCE. This pro-\nvides some evidence that nucleus and typical sampling\nimproves the sampling quality of well-calibrated models,\nbut this effect is not signiﬁcant. While nucleus sampling\nperforms well in the temperature-degraded model, we ob-\nserve some (non-signiﬁcant) evidence of a lower complex-\nity than conventional and typical sampling in the well-\ncalibrated model (with p= 0.023andp= 0.044, respec-tively). Typical sampling (with τ= 0.8) does not cause\nsigniﬁcant differences from conventional sampling. As the\np-value between NOISE_TYP and NOISE_CONV is also\nlow (but not signiﬁcant, with p= 0.06), there is some evi-\ndence that typical sampling slightly reduces the complexity\nof outputs from under-calibrated models. This could be ex-\nplained by typical sampling pruning the higher and lower\nprobability events, overall reducing the possible number of\nevents to be sampled. The well-calibrated model performs\nwell with all sampling techniques (no signiﬁcant differ-\nences to REFERENCE), with only some non-signiﬁcant\nevidence for lower complexity with nucleus sampling.\n6. CONCLUSION\nWe investigated the effect of distribution truncation sam-\npling techniques on the musical qualities of information\ncontent, self-similarity, scale consistency and complexity\nof samples generated under different degradation scenar-\nios. Our objective evaluations show that a higher trunca-\ntion strength leads to increased self-similarity and tonal\nconsistency. This trend is more pronounced for sam-\nples generated with nucleus sampling compared to sam-\nples generated with typical sampling. For a well-calibrated\nmodel, we show that the increase in self-similarity and\nscale consistency leads to an increase in deviations of\nthese metrics from the reference distribution. However,\nfor under-calibrated models, we showed that the deviations\nfrom the original data statistics could often be reduced with\nthe correct truncation strategy and carefully selected trun-\ncation levels (where a τbetween0.8and0.9seems to be\ngood trade-off value over all experiments). While nucleus\nsampling carries the risk to reduce complexity of the out-\nputs, this trend could not be observed with typical sam-\npling.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8157. ACKNOWLEDGMENTS\nThe work leading to these results was conducted in a col-\nlaboration between JKU and Sony Computer Science Lab-\noratories Paris under a research agreement. GW’s work\nis supported by the European Research Council (ERC) un-\nder the European Union’s Horizon 2020 research and inno-\nvation programme, grant agreement 101019375 ( “Whither\nMusic?” ).\n8. REFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n“Attention is all you need,” in NIPS , 2017, pp. 5998–\n6008.\n[2] C. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer,\nC. Hawthorne, A. M. Dai, M. D. Hoffman, and D. Eck,\n“An improved relative self-attention mechanism for\ntransformer with application to music generation,”\nCoRR , vol. abs/1809.04281, 2018.\n[3] Y .-S. Huang and Y .-H. Yang, “Pop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions,” in Proceedings of the 28th ACM\nInternational Conference on Multimedia , 2020, pp.\n1180–1188.\n[4] D. von Rütte, L. Biggio, Y . Kilcher, and T. Hoff-\nman, “FIGARO: generating symbolic music with ﬁne-\ngrained artistic control,” CoRR , vol. abs/2201.10936,\n2022.\n[5] B. Yu, P. Lu, R. Wang, W. Hu, X. Tan, W. Ye, S. Zhang,\nT. Qin, and T. Liu, “Museformer: Transformer with\nﬁne- and coarse-grained attention for music genera-\ntion,” in NeurIPS , 2022.\n[6] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi,\n“The curious case of neural text degeneration,” in\nICLR . OpenReview.net, 2020.\n[7] C. Meister, T. Pimentel, G. Wiher, and R. Cotterell,\n“Typical decoding for natural language generation,”\narXiv preprint arXiv:2202.00666 , 2022.\n[8] M. R. Bjare and S. Lattner, “On the typicality of musi-\ncal sequences,” in ISMIR (Late-breaking demo) , 2022.\n[9] B. L. Sturm, J. F. Santos, O. Ben-Tal, and I. Kor-\nshunova, “Music transcription modelling and compo-\nsition using deep learning,” in Proc. Conf. Computer\nSimulation of Musical Creativity , Huddersﬁeld, UK,\n2016.\n[10] S. Dieleman, “Musings on typicality,” 2020. [On-\nline]. Available: https://benanne.github.io/2020/09/01/\ntypicality.html\n[11] R. Salakhutdinov, A. Mnih, and G. Hinton, “Restricted\nboltzmann machines for collaborative ﬁltering,” in\nProceedings of the 24th international conference on\nMachine learning , 2007, pp. 791–798.[12] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever, “Language models are unsupervised mul-\ntitask learners,” 2019.\n[13] W. Hsiao, J. Liu, Y . Yeh, and Y . Yang, “Compound\nword transformer: Learning to compose full-song\nmusic over dynamic directed hypergraphs,” in AAAI .\nAAAI Press, 2021, pp. 178–186.\n[14] B. Sturm and L. Casini, “Tradformer: A transformer\nmodel of traditional music,” in International Joint Con-\nference on Artiﬁcial Intelligence , 2022.\n[15] F. Mo, X. Ji, H. Qian, and Y . Xu, “A user-customized\nautomatic music composition system,” in 2022 In-\nternational Conference on Robotics and Automation\n(ICRA) , 2022, pp. 640–645.\n[16] L. B. Meyer, “Meaning in music and information\ntheory,” The Journal of Aesthetics and Art Criticism ,\nvol. 15, no. 4, pp. 412–424, 1957. [Online]. Available:\nhttp://www.jstor.org/stable/427154\n[17] M. Pearce, “The construction and evaluation of statis-\ntical models of melodic structure in music perception\nand composition,” Ph.D. dissertation, Department of\nComputing, City University, London, UK, 2005.\n[18] M. R. Bjare, S. Lattner, and G. Widmer, “Differentiable\nshort-term models for efﬁcient online learning and pre-\ndiction in monophonic music,” Trans. Int. Soc. Music.\nInf. Retr. , vol. 5, no. 1, p. 190, 2022.\n[19] N. Fradet, J.-P. Briot, F. Chhel, A. El Fal-\nlah Seghrouchni, and N. Gutowski, “MidiTok: A\npython package for MIDI ﬁle tokenization,” in ISMIR\n(Late-breaking demo) , 2021.\n[20] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine\ntranslation by jointly learning to align and translate,” in\nICLR , 2015.\n[21] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention\nwith relative position representations,” in NAACL-HLT\n(2). Association for Computational Linguistics, 2018,\npp. 464–468.\n[22] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” in ICLR (Poster) , 2015.\n[23] O. Mogren, “C-rnn-gan: A continuous recurrent neu-\nral network with adversarial training,” in Construc-\ntive Machine Learning Workshop (CML) at NIPS 2016 ,\n2016, p. 1.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n816"
    },
    {
        "title": "ScorePerformer: Expressive Piano Performance Rendering With Fine-Grained Control.",
        "author": [
            "Ilya Borovik",
            "Vladimir Viro"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265355",
        "url": "https://doi.org/10.5281/zenodo.10265355",
        "ee": "https://zenodo.org/records/10265355/files/000069.pdf",
        "abstract": "We present ScorePerformer, an encoder-decoder transformer with hierarchical style encoding heads for controllable rendering of expressive piano music performances. We design a tokenized representation of symbolic score and performance music, the Score Performance Music tuple (SPMuple), and validate a novel way to encode the local performance tempo in a local note time window. Along with the encoding, we extend a transformer encoder with multi-level maximum mean discrepancy variational autoencoder style modeling heads that learn performance style at the global, bar, beat, and onset levels for fine-grained performance control. To offer an interpretation of the learned latent spaces, we introduce performance direction marking classifiers that associate vectors in the latent space with direction markings to guide performance rendering through the model. Evaluation results show the importance of the architectural design choices and demonstrate that ScorePerformer produces diverse and coherent piano performances that follow the control input.",
        "zenodo_id": 10265355,
        "dblp_key": "conf/ismir/BorovikV23",
        "keywords": [
            "encoder-decoder",
            "transformer",
            "hierarchical style encoding",
            "controllable rendering",
            "expressive piano music",
            "performance music tuple",
            "tokenized representation",
            "performance tempo",
            "performance style",
            "performance control"
        ],
        "content": "SCOREPERFORMER: EXPRESSIVE PIANO PERFORMANCE\nRENDERING WITH FINE-GRAINED CONTROL\nIlya Borovik\nSkoltech, Russia\nilya.borovik@skoltech.ruVladimir Viro\nPeachnote GmbH, Germany\nvladimir@peachnote.de\nABSTRACT\nWe present ScorePerformer, an encoder-decoder trans-\nformer with hierarchical style encoding heads for control-\nlable rendering of expressive piano music performances.\nWe design a tokenized representation of symbolic score\nand performance music, the Score Performance Music tu-\nple (SPMuple), and validate a novel way to encode the\nlocal performance tempo in a local note time window.\nAlong with the encoding, we extend a transformer encoder\nwith multi-level maximum mean discrepancy variational\nautoencoder style modeling heads that learn performance\nstyle at the global, bar, beat, and onset levels for ﬁne-\ngrained performance control. To offer an interpretation of\nthe learned latent spaces, we introduce performance direc-\ntion marking classiﬁers that associate vectors in the latent\nspace with direction markings to guide performance ren-\ndering through the model. Evaluation results show the im-\nportance of the architectural design choices and demon-\nstrate that ScorePerformer produces diverse and coherent\npiano performances that follow the control input.\n1. INTRODUCTION\nMusical expression is the human touch that transforms a\nwritten piece of music into an emotionally moving experi-\nence. In musical interpretation and performance, the musi-\ncian interprets a musical score and translates the intended\nexpression through the control of the musical instrument,\nthe sound of which conveys affect and emotion to the lis-\ntener [1, 2]. However, effective control of musical instru-\nments often requires considerable expertise and training,\nmaking musical expression less accessible than it could be.\nDeep learning music performance models reduce the\nneed for musical expertise and open up new ways to cre-\nate and perform music [3, 4]. To render expressive perfor-\nmances of written music [5, 6], the models mix recurrent\nneural networks to learn temporal dependencies in music\nwith variational autoencoders to encode performance style\nand enable controllable generation [7–11]. The models are\ntrained on real and categorical score and performance fea-\ntures for aligned score and performance notes.\n© I. Borovik and V . Viro. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nI. Borovik and V . Viro, “ScorePerformer: Expressive Piano Performance\nRendering with Fine-Grained Control”, in Proc. of the 24th Int. Society\nfor Music Information Retrieval Conf., Milan, Italy, 2023.The related task of symbolic music generation is ap-\nproached differently. Transformer models [12] are primar-\nily utilized due to their ability to effectively learn long-term\ndependencies in music sequences [13–17]. The symbolic\nmusic is encoded as sequences of musical tokens, either\nindividual [15, 18, 19] or stacked into tuples [16, 20]. Sim-\nilar approaches could be applied to the task of rendering\nexpressive music performances for written compositions.\nAiming to advance the research and make musical ex-\npression more accessible, we develop ScorePerformer1,\na piano music performance rendering model with inter-\nactive ﬁne-grained performance style control. The model\ncombines encoder and decoder transformers [12] with hi-\nerarchical maximum mean discrepancy variational autoen-\ncoders [21, 22] that encode performance style representa-\ntions at the global, bar, beat, and onset levels.\nTo interpret the learned style embedding spaces, we\ntrain embedding classiﬁers that associate local perfor-\nmance contexts with written musical score direction mark-\nings. For each marking, we use the classiﬁer predictions to\ncompute the average delta vectors in the style space from\nnegatively to positively classiﬁed style embeddings. These\nvectors provide quantiﬁed model control inputs to move\nthe performance rendering per given direction marking.\nFor data encoding, we design a tokenized representa-\ntion of score and performance music, a Score-Performance\nMusic tuple (SPMuple). It introduces a local window on-\nset tempo function that produces smoother and more robust\ntempos than inset-bar, -beat, or -onset tempo functions.\nThe experiments and evaluation results show that the\nmodel trained on the designed encoding successfully cap-\ntures different performance styles, can sample diverse and\ncoherent piano performances, and can be used for expres-\nsive performance rendering with ﬁne-grained style control.\nOur main contributions are:\n1. We extend transformers for expressive piano per-\nformance rendering with hierarchical style encoding\nand control at the global, bar, beat, and onset levels;\n2. We design a tokenized encoding for aligned score\nand performance music that proposes an efﬁcient lo-\ncal tempo computation function;\n3. We introduce performance direction classiﬁers to\nprovide musical language-driven performance con-\ntrol by modifying the learned style latent spaces.\n1Source code and demo are available at: https://github.com/\nilya16/scoreperformer5882. RELATED WORK\nExpressive Music Performance: Recent expressive mu-\nsic performance rendering models mainly utilize deep\nlearning methods [3, 6]. Jeong et al. [8] and Maezawa et\nal. [9] use conditional variational autoencoders for perfor-\nmance style encoding and recurrent neural networks for\nexpressive performance rendering. Rhyu et al. [11] allow\nperformance style to be intuitively “sketched” by a set of\nlearned latent representations. We propose to use trans-\nformers with self-attention mechanisms [12] to infer pat-\nterns in music performance and model its style through hi-\nerarchical style encoding heads.\nSymbolic Music Generation: Symbolic music gen-\neration with deep learning [4] is dominated by trans-\nformers for learning long-term sequential musical patterns\n[13, 15–17, 23] and variational autoencoders for unsuper-\nvised style encoding and control [19, 23–26]. The models\noffer unconditional or priming melody-based music gen-\neration [14], global control of performance style [14, 25]\nor ﬁne-grained control of music through learned high-level\nfeatures [23, 26] and descriptions [19]. Our model is close\nto the melody-conditioned transformer autoencoder [14],\nbut introduces modiﬁcations for the task of score-based\nperformance rendering with style control.\nSymbolic Music Encoding: The simplest way to en-\ncode symbolic music is a MIDI-like encoding with note-\non, note-off, and time-shift events [13, 18]. REMI [15],\nREMI+ [19], and Compound Word [16] replace position\nshifts with absolute bar, position, and beat tempo tokens.\nOctupleMIDI [20] shortens sequence lengths by stacking\nnote attributes into tuples of 8 tokens. For expressive mu-\nsic performance rendering, it is common to mix real, cat-\negorical and pianoroll-based score and performance fea-\ntures parsed from MusicXML and MIDI ﬁles [8,9,11,27].\nTransformers work well with tokenized data [12, 28, 29].\nInspired by OctupleMIDI, we design a tuple-like token en-\ncoding that naturally ﬁts aligned score-performance data.\n3. DATA ENCODING\n3.1 Score and Performance Data Matching\nExpressive music performance rendering models require\ndatasets of aligned score and performance music [5,30]. In\nthis work, we consider piano music performances in MIDI\nformat and use the following data preparation pipeline.\nFirst, we compute alignments using Nakamura’s alignment\ntool [31]. The alignments may contain errors, such as\nalignment holes or close performance notes aligned with\ndistant and unrelated score notes. Following the litera-\nture [8, 32], we revise the alignments and ﬁlter out notes\nthat deviate from the local performance tempo. After the\ncleanup, we omit performances with less than 80% aligned\nnotes. Finally, to achieve a perfect match, we remove ex-\ntra performed notes and interpolate missing notes using the\nlocal performance tempos and dynamics, since taking only\nmatched notes and discarding score notes can result in the\nremoval of important chord and bar information.3.2 SPMuple Encoding\nWe introduce the Score-Performance Music tuple (SPMu-\nple), a tokenized representation for aligned symbolic score\nand performance music. It encodes performed notes using\ntuples of 8 score and 4 performance tokens.\nScore Tokens: a set of features extracted from the score\nMIDI. Pitch is a MIDI pitch number in the range 21 to\n108. Duration is a score note value, encoded by 128 to-\nkens with high and low resolution tokens for short and long\ndurations, respectively [20,33]. Baris an index of the mu-\nsical bar to which the note refers, ranging from 0 to the\nmaximum bar in the data. Position is the position of the\nnote in the bar, one of 128 tokens with 64th note resolution.\nTimeSignature is the time signature of the beat containing\nthe note, a set of 22 tokens for 2nd, 4th, and 8th note beat\nlengths, with a maximum bar length of 2 whole notes for\n2nd note, and 1.5 for 4th and 8th note. OnsetShift is the\npositional interval between the current and previous note\nonsets (chords). NotesInOnset andPositionInOnset are\nthe number of notes and the index of the note in the onset,\nranging from 1 to 12, notes are ordered by pitch.\nPerformance Tokens: a set of performance features\nextracted from the performance MIDI and processed us-\ning the aligned score note features. Velocity is a MIDI\nvelocity from 1 to 127. Tempo is the performance tempo\nat the bar, beat or onset level, encoded by a geometric se-\nquence of 121 tokens for beats per minute tempos from 15\nto 480. RelOnsetDeviation models the exact timing of the\nnote, encoded as the ratio of the absolute note-onset posi-\ntion deviation to the inter-onset interval scaled by the local\nonset tempo using 161 tokens for values in the range -2 to\n2.RelPerformedDuration is an articulation of the per-\nformed note, computed as the ratio of the performed dura-\ntion to the score duration, scaled by the local onset tempo,\nand encoded by 121 tokens for logarithmically distributed\nvalues between 0.1 and 3.\nThe score and performance token sequences are sorted\nby score note start position, pitch and duration.\n3.3 Local Tempo\nInset-onset tempos are noisy and have very high variance,\nwhile beat and bar tempos are smoother but still ﬂuctuate\nat beat/bar boundaries, which can lead to degraded musical\nexperience [34–36]. We design a smooth alternative, local\nonset tempos, weighted with respect to previous onsets in\nthe local onset time window.\nLet{IOIs\ni}and{IOIp\ni}be the sets of score and per-\nformance inter-onset intervals between the onset oandN\npreceding onsets oiin the time window W. The weights\nwo\nifor inter-onset temposIOIs\ni\nIOIp\niare computed as:\nwo\ni= 1−IOIp\ni\nmaxj{IOIp\nj}+10−2(1)\nThe weights give more attention to the closest preceding\nonsets, but still consider the more distant onsets to smooth\nthe local tempo. Based on the decoding quality, we set\nthe time window length Wto 8s as the optimal one. InProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n589ScorePerformer Architecture\nPerformance\nDecoderDirection\nClassifiersf\n.03 .17 .91 .75 .05p\ncrescendo\nstaccato\nritardandoDirection ClassifiersVelocityTempo\nOnsetDev\nPerfDurationInferenceTraining\nPerformance\nEncoder\nScore\nEncoder\nPast PerformanceCurrent NoteScore ContextPerformance StyleControl Input\nbaronset\nbeat\nglobalEncoder\nTransformerPerformance Style Encoder\nHead\nHeadHeadHead  Onset\n Beat\n All Bar\nFigure 1 . The overall architecture of ScorePerformer, hierarchical style encoding heads and direction classiﬁers.\naddition to the window W, we ﬁlter out the nearest onsets\nwithIOIp\ni<0.5to reduce the effect of immediate tempo\nchanges, and take at least Nmin= 8 past onsets with any\nIOIpto have enough points for smoothing ( N≥Nmin).\n4. MODEL\nWith a focus on hierarchical performance style control\nand efﬁcient training on tokenized sequences, we present\nScorePerformer , an encoder-decoder model that com-\nbines transformers [12] and maximum mean discrepancy\nvariational autoencoders (MMD-V AE) [21,22] for control-\nlable expressive rendering of piano performances for writ-\nten scores. The model is illustrated in Figure 1.\n4.1 Model Architecture\nScore Encoder is an encoder transformer that computes a\ncontextual representation of the written music. It maps a\nscore note sequence y+∈NN×10(score tokens y+ score\ntempos and velocities) to note embeddings cs∈RL×D.\nPerformance Encoder is an encoder transformer that\ncomputes performance style representations at different\nlevels of the musical hierarchy. It takes a sequence of mu-\nsic tuples of score and performance tokens m= [y,x],\ny∈NN×8,x∈NN×4, and outputs performance context\nembeddings cp∈RN×D. The embeddings are grouped\nand averaged over the entire sequence, bars, beats, and on-\nsets, and iteratively passed through conditional linear lay-\ners to compute global, bar, beat, and onset latents zG,zB,\nzb, andzo. With the idea of learning missing lower-level\ndetails hierarchically, at each step tthe latent z∗\ntdepends\non the context cp\ntand all higher-level latents containing the\nnote, e.g. zb\nt=fb\nφ(cp\nt,zG\nt,zB\nt). All note latents are stacked\nto produce note-level style embeddings z∈RN×Dz.\nThe latent spaces are ﬁt into the Gaussian distribution\nusing a maximum mean discrepancy objective:\nLMMD(p∥q) =Ep(z),p(z′)[k(z,z′)]+Eq(z),q(z′)[k(z,z′)]\n−2Ep(z),q(z′)[k(z,z′)], (2)\nwherek(z,z′) =e−∥z−z′∥2\n2σ2is a Gaussian kernel.We use MMD-V AE [21, 22] to solve issues with poste-\nrior collapse and latent space holes [37] common to con-\nventional variational autoencoders [38], especially, when\ntrained on sequential data [39].\nPerformance Decoder is a decoder transformer that\nrenders performance by sequentially predicting perfor-\nmance tokens xtfor score note tokens yt. The input token\nsequence combines two sequences: 1) a sequence ms=y\nwith the current score notes to be rendered; 2) a sequence\nm−1= [y−1,x−1]shifted one step into the past score y−1\nand rendered performance tokens x−1describing the past\nperformance history. To reuse the SPMuple token embed-\nder, the ﬁrst sequence is extended with the masked per-\nformance tokens. The two sequence embeddings are con-\ncatenated with the score context csand passed to the trans-\nformer layers together with the style embeddings z.\nWe use style-adaptive layer normalization (SALN) [40]\nand pass style embeddings zto the decoder’s layer nor-\nmalization layers, rather than concatenating the style and\ninput token embeddings, to increase the focus on the per-\nformance style at each transformer layer.\nThe performance decoder minimizes the negative log-\nlikelihood for the sequence of performance tokens x:\nLperf=−N/summationdisplay\nt=1logpθ(xt|x<t,y≤t,c≤t,z≤t) (3)\n4.2 Transformer Modiﬁcations\nDiscrete+Continuous Tokens: Discrete musical tokens\ndo not explicitly encode the absolute and relative informa-\ntion about note attributes, e.g. that pitches C2, C3, and\nC4 differ by an octave, or that velocity 80 is louder than\n60. We mix discrete and continuous tokens by summing\nlearned discrete token embeddings with delta embeddings\nprovided by a learned nonlinear mapping of the real values\nassociated with tokens to the token embedding dimensions.\nRelative Attention : We use the learned ALiBi relative\npositional bias [41] in the decoder and the learned bidi-\nrectional symmetric bias [42] in the encoder for efﬁcient\ninterpolation to sequence lengths not seen during training.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n590Other: We use single key-value attention heads [43] to\nspeed up decoding, SwiGLU activation [44,45] in feedfor-\nward layers, reuse token embedding weights between the\nencoders and decoder since they share token vocabularies,\nand tie input and output embeddings in the decoder [45].\n4.3 Performance Direction Classiﬁers\nWe provide an intuitive interpretation of the learned style\nembedding space by training performance direction clas-\nsiﬁers on the learned note style embeddings. We extract\nperformance direction markings from MusicXML ﬁles\nand associate score notes with performance direction la-\nbels where they are present. We train classiﬁers for dy-\nnamics (degrees of piano andforte ),dynamic changes\n(crescendo anddiminuendo ),tempo (adagio ,largo , etc.),\ntempo changes (accelerando ,ritardando , etc.) and note\narticulation binary classes ( staccato ,fermata , etc.).\nClassiﬁers take as input the combined note-level perfor-\nmance style embeddings z= [zG,zB,zb,zo]and output\nthe probabilities of directions being performed in a given\nperformance context. The module minimizes the sum of\ncross entropy losses for Kclassiﬁers with Ckclasses each:\nLclf=K/summationdisplay\nk=1Lk\nclf=−1\nNK/summationdisplay\nk=1N/summationdisplay\nt=1Ck/summationdisplay\nc=1dk\nt,clog(ˆdk\nt,c),(4)\nwheredk\nt,candˆdk\nt,care true and predicted labels for direc-\ntioncof the classiﬁer kat stept.\nGiven the smoothness of the learned latent space, the\ndifferences between embeddings with high and low classi-\nﬁcation scores for a given marking may provide a direction\nin the latent space to move the generation toward the mark-\ning. We compute and use mean per-marking delta embed-\ndings to control performance rendering. Since markings\nare related to deﬁned musical concepts, we can map natu-\nral language commands, such as “play more piano here”\nor“switch to largo” , to quantitative model control inputs.\n4.4 Training and Inference\nThe total loss minimized by the model during training is:\nL=Lperf+LMMD+Lclf (5)\nTo avoid overﬁtting of the decoder to lower-level perfor-\nmance embeddings during training, we drop bar, beat, and\nonset embeddings with probabilities of 0.1, 0.2, and 0.4,\nrespectively. The embeddings are dropped inclusively, i.e.\nif the bar latent is dropped out, all beat and onset latents\nare also dropped. Additionally, the classiﬁers are trained\non detached style embeddings z, as we found the model to\noverﬁt the unbalanced direction markings labels.\nDuring inference, the sampled or modiﬁed reference\nperformance embeddings can be used to control the render-\ning of the music performance. Based on the learned style\nspaces, the control can range from high-level global to low-\nlevel onset. The extracted performance direction delta em-\nbeddings can be used to provide intuitive, command-driven\nperformance manipulation. The model supports real-time\ninference on the CPU for use in interactive applications.5. EXPERIMENTS\nDatasets: For all experiments, we use the ASAP dataset of\nmatched piano scores and performances [46], preprocessed\nas described in Section 3.1. The prepared dataset repre-\nsents 212 musical compositions by 15 composers with a\ntotal of 937 performances, 79 hours of performed music.\nThe data is divided into training and evaluation sets with an\napproximate ratio of 9:1 for the number of performances in\nthe entire dataset and for each composer.\nImplementation: The SPMuple data encoding is im-\nplemented using miditok ’s [33] MIDI tokenizer inter-\nface. The encoders and decoders in all experiments have a\nhidden dimension of 256, 4 layers, and 4 attention heads,\nexcept for the score encoder, which has 2 layers. The to-\nken embedding dimension is set to 128 for each token type,\nthe projected embedding dimension for input embeddings\nis set to 256. The global, bar, beat, and onset latent dimen-\nsions are set to 32, 20, 8, and 4, respectively.\nTraining: The maximum sequence length during train-\ning is set to 256 tokens. To regularize the model and arti-\nﬁcially increase the variety of data, we augment the data\nwith sampled pitch shifts (up to ±3semitones) and ve-\nlocity shifts (up to ±12MIDI values). In addition, we\nrandomly replace real performances with deadpan perfor-\nmances with a probability of 25% to allow the model to\nlearn the style of both expressive and inexpressive music.\nWe use the ADAM optimizer [47] with an initial learning\nrate of2·10−4, decaying by 0.995after each epoch. Mod-\nels are trained for 70,000 iterations with batch size 128.\nEvaluation: We conduct three sets of experiments: 1)\nevaluation of the designed data encoding and different lo-\ncal tempo calculation functions; 2) comparison of different\nlatent style hierarchies and their impact on performance\nrendering; 3) an ablation study on the model architec-\nture design. For the metrics, we use Pearson correlation\n[9, 11, 48] and mean absolute error for performance fea-\ntures: inter-onset intervals (IOI), absolute onset deviations\n(OD), performed note durations (PD), and velocity (Vel).\nWe generate 3 samples for each performance in the evalu-\nation set and compute and average the metrics between the\nground truth and the generated performances, decoded to\nMIDI. The errors are measured in seconds, except for ve-\nlocities, which are measured in MIDI velocity values. Af-\nter the objective evaluation, we analyze the generation and\ncontrol capabilities of the designed ScorePerformer model.\n6. EV ALUATION\n6.1 Encoding and Local Tempos\nThe tokenized representation of performance is not loss-\nless, since some information is lost during feature quanti-\nzation. We evaluate the decoding quality and performance\nof ScorePerformer on sequences encoded using SPMuple\nwith different local tempo functions.\nTable 1 shows the evaluation results. The local window\nonset tempo function (Section 3.3) shows the least degra-\ndation in decoding quality for inter-onset intervals and on-\nset deviations. It captures local tempo changes and noteProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n591Decoded Generated, ∆z= 0\nError↓ Correlation ↑ Error↓ Correlation ↑\nTempo IOI OD PD IOI OD PD IOI OD PD Vel IOI OD PD Vel\nBar 0.092 0.002 0.026 0.770 0.953 0.954 0.140 0.012 0.063 2.354 0.650 0.361 0.837 0.940\nBeat 0.084 0.002 0.027 0.836 0.971 0.958 0.116 0.009 0.066 2.627 0.727 0.406 0.854 0.932\nOnset 0.019 0.001 0.006 0.921 0.977 0.982 0.124 0.011 0.056 2.856 0.709 0.339 0.890 0.932\nWindow 0.028 0.001 0.011 0.963 0.985 0.979 0.090 0.008 0.048 2.583 0.901 0.538 0.907 0.943\nTable 1 . Encoding evaluation on decoded performances and performances generated with unaltered style embeddings from\nthe performance encoder. IOI – inter-onset interval, OD – onset deviation, PD – performed duration, Vel – velocity.\nG B b o z IOI OD PD Vel\n32 20 8 4 64 0.901 0.538 0.907 0.943\n32 20 12 ✗64 0.464 0.194 0.739 0.861\n32 32 ✗ ✗ 64 0.417 0.067 0.722 0.812\n64 ✗ ✗ ✗ 64 0.327 0.066 0.658 0.576\n✗ 32 ✗ ✗ 32 0.410 0.069 0.702 0.792\n✗ ✗ 12 ✗12 0.384 0.066 0.711 0.767\n✗ ✗ ✗ 4 4 0.590 0.063 0.735 0.748\n32 20 8 ✗60 0.410 0.065 0.764 0.847\n32 20 ✗ 4 56 0.842 0.224 0.881 0.857\n32 ✗ 8 4 44 0.863 0.386 0.886 0.913\n✗ 20 8 4 32 0.890 0.485 0.904 0.939\nTable 2 . Correlation with ground truth performances for\nsamples generated by models trained with different com-\nbinations of latent hierarchies. G – global, B – bar, b –\nbeat, o – onset, and z – total latent dimensions.\ntiming more efﬁciently than bar, beat and onset tempos.\nThese ﬁndings are supported by the generation results. The\nmodel trained with local window tempo tokens renders\nsamples with smaller errors and closer to the ground truth\nthan the models trained with bar, beat, or onset tempo to-\nkens. In particular, it shows more consistency in modeling\nlocal tempo changes and note timing. For future work, the\nencoding could be further improved by incorporating ped-\nals, an essential element of piano performance [49].\n6.2 Style Embedding Hierarchies\nTable 2 shows the impact of different learned style em-\nbedding hierarchy combinations in ScorePerformer on the\nquality of performance rendering. Replacing lower-level\nlatents with higher-level ones, using only a single level,\nor omitting any level of the hierarchy leads to a decrease\nin quality for all musical features. The lower-level onset\nlatents account for most of the variation in performance\nfeatures, while the higher-level latents provide the missing\nperformance timing, articulation, and dynamics informa-\ntion at the beat, bar, and global levels. The results suggest\nthat a hierarchical style representation is advantageous for\nmodeling global and local changes in music performance.\nThe search for an optimal conﬁguration of latent dimen-\nsions is beyond the scope of this study.IOI OD PD Vel\nScorePerformer 0.901 0.538 0.907 0.943\nw/o Score Encoder 0.885 0.526 0.889 0.951\nw/o input seq. ms0.844 0.422 0.895 0.925\nw/o SALN 0.871 0.469 0.920 0.930\nw/o in-out emb. tie 0.901 0.459 0.873 0.951\nw/o Continuous Tokens 0.576 0.116 0.747 0.561\nTable 3 . Evaluation of model conﬁgurations using the cor-\nrelation between ground truth and generated performances.\n6.3 Ablation Study\nThe ablation study on the ScorePerformer model is sum-\nmarized in Table 3. Removing any of the proposed design\nchoices degrades the quality for all features in almost all\ncases. The score encoder adds a local future score context\nto the decoder and contributes to a slight quality improve-\nment. The same is true for the additional decoder input\nsequence ms, which explicitly highlights the currently ren-\ndered score notes. Without style-adaptive layer normaliza-\ntion or input-output embedding weight sharing, the corre-\nlation for timing features decreases. The most noticeable\nquality degradation occurs after using only discrete tokens\nwithout continuous input tokens, demonstrating the posi-\ntive impact of value-aware inputs on model predictions.\n6.4 Performance Embeddings Analysis\nWe explore the learned performance style spaces using the\ntrained performance direction marking classiﬁers. We take\nthe style embeddings zfor note onsets in the dataset and\nproject them into two dimensions using principal compo-\nnent analysis [50]. Figure 3 shows the projected embed-\ndings labeled by the selected dynamics, tempo, and ar-\nticulation markings classiﬁers and their ground truth la-\nbels. We can see the gradient moving from the light col-\nors (high probabilities) to the darker colors (low probabil-\nities). Despite the class imbalances and low representation\nof some labels in the dataset, the positive classiﬁer predic-\ntions match the areas of the ground truth labels shown in\nthe right plots for each marking. This suggests that the vec-\ntors for moving the performance toward the markings exist\nin the original latent style space and can be used to attempt\nto control the performance rendering through the model.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n592 3648607284pitcha) z= 0\n0 3 6 9 12 15 18 21 24 27 30 33\ntime, s3648607284pitchb) z (0, 0.52)60120180IO T empo\n0.51.01.5Articulation\n306090Velocity\n60120180IO T empo\n0.51.01.5Articulation\n0 25 50 75 100\nonset306090VelocityGT Model\n   c) bars 5-8: piano , bars 9-12: forte\n0 3 6 9 12 15 18 21 24 27 30 33\ntime, s d) bars 5-8: più mosso , bars 9-12: largo60120180IO T empo\n0.51.01.5Articulation\n306090Velocity\n60120180IO T empo\n0.51.01.5Articulation\n0 25 50 75 100\nonset306090VelocityGT Model\n   e) bars 5-10: switch cresc .  and dim.\n0 3 6 9 12 15 18 21 24 27 30 33\ntime, s f) pitches C4: staccato , <C4: fermata60120180IO T empo\n0.51.01.5Articulation\n306090Velocity\n60120180IO T empo\n0.51.01.5Articulation\n0 25 50 75 100\nonset306090p f\npiù mosso largo< > < > < >\nfermatastaccato\nVelocityGT Model\nFigure 2 . Pianorolls and performance features (inter-onset tempo, articulation, and velocity) for the ﬁrst 12 musical bars of\nBach’s “Prelude and Fugue No.19”, rendered by ScorePerformer with unconditional or conditional style control. The title\nof each plot indicates the form of the control input. Colored areas highlight the regions with the applied control.\nFigure 3 . Projected style embeddings classiﬁed by chosen\ndirection marking classiﬁers. The left and right plots for\neach marking highlight predicted and ground truth labels.\nThe direction classiﬁers can also be used to analyze\nperformance practices. For example, take all performance\ncontexts with a given notated performance direction mark-\ning and sort them by the classiﬁcation scores using the as-\nsociated direction classiﬁer. Further analysis of the score\ncontexts can provide insight into the reasons why musi-\ncians follow or interpret differently certain markings.\n6.5 Performance Rendering Control\nFor performance rendering control, we add control embed-\ndings∆zto the encoded style embeddings and pass them\nto the decoder. We analyze both uncontrolled generation\nwith sampled control embeddings and direction-based con-\ntrol using the computed delta latents for markings.\nFigure 2 shows examples of music performance render-\ning for a composition from the evaluation set. The sample\n(a) shows the successful reconstruction of the performance\nvariations from the encoded style embeddings. When gen-\nerated using sampled delta latents (b), the added noise is\ntransferred to higher variations in tempo than in articula-\ntion and dynamics. In our observations, small amounts of\ndelta noise can result in both pleasant and diverse samples.\nFrom the evaluation of the performance direction based\ncontrol, we can see that in most cases the model follows\nthe musical meaning of the marking. Example (c) showsthatpiano andforte delta embeddings lead to the expected\ndecrease and increase in dynamics. The più mosso (more\nmovement, faster) and largo (slowly and broadly) in the\nexample (d) lead to the expected changes in tempo, ar-\nticulation and dynamics. An interesting behaviour can be\nfound in example (e), where the model values one mark-\ning over the other. During the alternation of crescendo and\ndiminuendo , the model follows diminuendo more and falls\non the path of slow and quiet performance. The last exam-\nple (f) shows that the control can also be applied effectively\nto individual notes. As the deﬁnitions suggest, the staccato\non higher pitched notes makes them more abrupt, and the\nfermata on other notes holds the notes a bit longer.\nDespite the positive examples of piano performance\nrendering control, the model has some limitations. The\nproposed marking delta embeddings encode the highest\nlearned deviations between performance styles and lead to\nimmediate changes in performance, which can sound un-\nnatural. One solution is to scale or interpolate the control\ninputs for smoother performance changes. Another issue\nto be addressed is disentangling the learned latent space\nacross direction classes for a more controllable generation.\nFinally, the study was limited by low performance varia-\ntion for some markings and compositions in the dataset.\nWe believe that the proposed approach has a high potential\nfor both analytical and musical creativity applications that\ncould be fulﬁlled with orders of magnitude larger datasets.\n7. CONCLUSION\nWe presented ScorePerformer, an encoder-decoder trans-\nformer with hierarchical MMD-V AE style encoding heads\nfor ﬁne-grained controllable expressive rendering of pi-\nano music performances. We also introduced performance\ndirection classiﬁers, trained on performance style embed-\ndings, to map notated direction markings and natural lan-\nguage inputs to model control inputs. Evaluation showed\nthat the model captures performance style variations and\nfollows control intents. Future work will focus on improv-\ning the diversity of training data to enable large-scale anal-\nysis, and may include in-depth subjective evaluation of the\nproposed and existing performance rendering models.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n5938. ACKNOWLEDGEMENTS\nWe thank Dmitry Yarotsky for his valuable comments dur-\ning the model development and experimentation. We thank\nthe anonymous reviewers for their critical feedback, which\nallowed us to improve the paper. The experiments were\nperformed on the Zhores computing cluster [51].\n9. REFERENCES\n[1] C. Palmer, “Music performance,” Annual review of\npsychology , vol. 48, no. 1, pp. 115–138, 1997.\n[2] J. Rink, Musical Performance: A Guide to Under-\nstanding . Cambridge University Press, 2002.\n[3] S. Ji, J. Luo, and X. Yang, “A Comprehensive Survey\non Deep Music Generation: Multi-level Representa-\ntions, Algorithms, Evaluations, and Future Directions,”\narXiv preprint arXiv:2011.06801 , 2020.\n[4] C. Hernandez-Olivan, J. Hernandez-Olivan, and J. R.\nBeltran, “A Survey on Artiﬁcial Intelligence for Music\nGeneration: Agents, Domains and Perspectives,” arXiv\npreprint arXiv:2210.13944 , 2022.\n[5] A. Kirke and E. R. Miranda, Guide to Computing for\nExpressive Music Performance . Springer, 2013.\n[6] C. E. Cancino-Chacón, M. Grachten, W. Goebl, and\nG. Widmer, “Computational Models of Expressive\nMusic Performance: A Comprehensive and Critical\nReview,” Frontiers in Digital Humanities , vol. 5, p. 25,\n2018.\n[7] D. Jeong, T. Kwon, Y . Kim, and J. Nam, “Graph Neural\nNetwork for Music Score Data and Modeling Expres-\nsive Piano Performance,” in Proceedings of the 36th In-\nternational Conference on Machine Learning . PMLR,\n2019, pp. 3060–3070.\n[8] D. Jeong, T. Kwon, Y . Kim, K. Lee, and J. Nam,\n“VirtuosoNet: A Hierarchical RNN-based System for\nModeling Expressive Piano Performance,” in Proceed-\nings of the 20th International Society for Music Infor-\nmation Retrieval Conference , 2019, pp. 908–915.\n[9] A. Maezawa, K. Yamamoto, and T. Fujishima, “Ren-\ndering Music Performance With Interpretation Varia-\ntions Using Conditional Variational RNN,” in Proceed-\nings of the 20th International Society for Music Infor-\nmation Retrieval Conference , 2019, pp. 855–861.\n[10] H. H. Tan, Y .-J. Luo, and D. Herremans, “Gen-\nerative modelling for controllable audio synthesis\nof expressive piano performance,” arXiv preprint\narXiv:2006.09833 , 2020.\n[11] S. Rhyu, S. Kim, and K. Lee, “Sketching the Expres-\nsion: Flexible Rendering of Expressive Piano Perfor-\nmance with Self-Supervised Learning,” arXiv preprint\narXiv:2208.14867 , 2022.[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin,\n“Attention is All you Need,” in Advances in Neural In-\nformation Processing Systems , vol. 30. Curran Asso-\nciates, Inc., 2017, pp. 5998–6008.\n[13] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer,\nC. Hawthorne, A. M. Dai, M. D. Hoffman, and D. Eck,\n“Music Transformer: Generating Music with Long-\nTerm Structure,” arXiv preprint arXiv:1809.04281 ,\n2018.\n[14] K. Choi, C. Hawthorne, I. Simon, M. Dinculescu, and\nJ. Engel, “Encoding Musical Style with Transformer\nAutoencoders,” arXiv preprint arXiv:1912.05537 ,\n2019.\n[15] Y .-S. Huang and Y .-H. Yang, “Pop Music Trans-\nformer: Beat-based Modeling and Generation of Ex-\npressive Pop Piano Compositions,” arXiv preprint\narXiv:2002.00212 , 2020.\n[16] W.-Y . Hsiao, J.-Y . Liu, Y .-C. Yeh, and Y .-H. Yang,\n“Compound Word Transformer: Learning to Com-\npose Full-Song Music over Dynamic Directed Hyper-\ngraphs,” in Proceedings of the AAAI Conference on Ar-\ntiﬁcial Intelligence , vol. 35, 2021, pp. 178–186.\n[17] B. Yu, P. Lu, R. Wang, W. Hu, X. Tan, W. Ye, S. Zhang,\nT. Qin, and T.-Y . Liu, “Museformer: Transformer with\nFine-and Coarse-Grained Attention for Music Genera-\ntion,” arXiv preprint arXiv:2210.10349 , 2022.\n[18] S. Oore, I. Simon, S. Dieleman, D. Eck, and K. Si-\nmonyan, “This time with feeling: Learning expressive\nmusical performance,” Neural Computing and Appli-\ncations , vol. 32, pp. 955–967, 2020.\n[19] D. von Rütte, L. Biggio, Y . Kilcher, and T. Hoff-\nman, “FIGARO: Generating Symbolic Music with\nFine-Grained Artistic Control,” arXiv preprint\narXiv:2201.10936 , 2022.\n[20] M. Zeng, X. Tan, R. Wang, Z. Ju, T. Qin, and T.-\nY . Liu, “MusicBERT: Symbolic Music Understand-\ning with Large-Scale Pre-Training,” arXiv preprint\narXiv:2106.05630 , 2021.\n[21] S. Zhao, J. Song, and S. Ermon, “InfoV AE: Infor-\nmation Maximizing Variational Autoencoders,” arXiv\npreprint arXiv:1706.02262 , 2017.\n[22] A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf,\nand A. Smola, “A Kernel Method for the Two-Sample-\nProblem,” in Advances in Neural Information Process-\ning Systems , vol. 19. MIT Press, 2006, pp. 513—-520.\n[23] S.-L. Wu and Y .-H. Yang, “MuseMorphose: Full-Song\nand Fine-Grained Piano Music Style Transfer with One\nTransformer V AE,” arXiv preprint arXiv:2105.04090 ,\n2021.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n594[24] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and\nD. Eck, “A Hierarchical Latent Vector Model for\nLearning Long-Term Structure in Music,” in Pro-\nceedings of the International Conference on Machine\nLearning . PMLR, 2018, pp. 4364–4373.\n[25] G. Brunner, A. Konrad, Y . Wang, and R. Wattenhofer,\n“MIDI-V AE: Modeling Dynamics and Instrumenta-\ntion of Music with Applications to Style Transfer,”\narXiv preprint arXiv:1809.07600 , 2018.\n[26] H. H. Tan and D. Herremans, “Music FaderNets:\nControllable Music Generation Based On High-Level\nFeatures via Low-Level Feature Modelling,” arXiv\npreprint arXiv:2007.15474 , 2020.\n[27] D. Jeong, T. Kwon, Y . Kim, and J. Nam, “Score and\nperformance features for rendering expressive music\nperformances,” in Music Encoding Conference . Mu-\nsic Encoding Initiative Vienna, Austria, 2019, pp. 1–6.\n[28] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly et al. , “An Image is\nWorth 16x16 Words: Transformers for Image Recogni-\ntion at Scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[29] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov,\nO. Pietquin, M. Shariﬁ, O. Teboul, D. Grangier,\nM. Tagliasacchi, and N. Zeghidour, “AudioLM: a\nLanguage Modeling Approach to Audio Generation,”\narXiv preprint arXiv:2209.03143 , 2022.\n[30] C. E. Cancino-Chacón, “Computational Modeling of\nExpressive Music Performance with Linear and Non-\nlinear Basis Function Models,” Ph.D. dissertation, Jo-\nhannes Kepler University Linz, Austria, December\n2018.\n[31] E. Nakamura, K. Yoshii, and H. Katayose, “Perfor-\nmance Error Detection and Post-Processing for Fast\nand Accurate Symbolic Music Alignment,” in Interna-\ntional Society for Music Information Retrieval Confer-\nence, 2017.\n[32] G. G. Xia, “Expressive collaborative music per-\nformance via machine learning,” Ph.D. dissertation,\nCarnegie Mellon University, August 2016.\n[33] N. Fradet, J.-P. Briot, F. Chhel, A. El Fallah-\nSeghrouchni, and N. Gutowski, “MidiTok: A Python\npackage for MIDI ﬁle tokenization,” in 22nd Interna-\ntional Society for Music Information Retrieval Confer-\nence, 2021.\n[34] S. Dixon, W. Goebl, and E. Cambouropoulos, “Percep-\ntual Smoothness of Tempo in Expressively Performed\nMusic,” Music Perception , vol. 23, no. 3, pp. 195–214,\n2006.\n[35] B. H. Repp, “On Determining the Basic Tempo of an\nExpressive Music Performance,” Psychology of Music ,\nvol. 22, no. 2, pp. 157–167, 1994.[36] H. Schreiber, F. Zalkow, and M. Müller, “Modeling and\nEstimating Local Tempo: A Case Study on Chopin’s\nMazurkas,” in Proceedings of the 21st International\nSociety for Music Information Retrieval Conference ,\n2020, pp. 773–779.\n[37] J. Lucas, G. Tucker, R. B. Grosse, and M. Norouzi,\n“Understanding Posterior Collapse in Generative La-\ntent Variable Models,” in Deep Generative Models for\nHighly Structured Data, ICLR 2019 Workshop , 2019.\n[38] D. P. Kingma and M. Welling, “Auto-Encoding Varia-\ntional Bayes,” arXiv preprint arXiv:1312.6114 , 2013.\n[39] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai,\nR. Józefowicz, and S. Bengio, “Generating Sentences\nfrom a Continuous Space,” in Conference on Compu-\ntational Natural Language Learning , 2015.\n[40] D. Min, D. B. Lee, E. Yang, and S. J. Hwang, “Meta-\nStyleSpeech: Multi-Speaker Adaptive Text-to-Speech\nGeneration,” in International Conference on Machine\nLearning . PMLR, 2021, pp. 7748–7759.\n[41] O. Press, N. A. Smith, and M. Lewis, “Train\nshort, test long: Attention with linear biases en-\nables input length extrapolation,” arXiv preprint\narXiv:2108.12409 , 2021.\n[42] M. Lee, K. Han, and M. C. Shin, “LittleBird: Efﬁ-\ncient Faster & Longer Transformer for Question An-\nswering,” arXiv preprint arXiv:2210.11870 , 2022.\n[43] N. Shazeer, “Fast Transformer Decoding: One\nWrite-Head is All You Need,” arXiv preprint\narXiv:1911.02150 , 2019.\n[44] ——, “GLU Variants Improve Transformer,” arXiv\npreprint arXiv:2002.05202 , 2020.\n[45] A. Chowdhery, S. Narang, J. Devlin, M. Bosma,\nG. Mishra, A. Roberts, P. Barham, H. W. Chung,\nC. Sutton, S. Gehrmann et al. , “PaLM: Scaling\nLanguage Modeling with Pathways,” arXiv preprint\narXiv:2204.02311 , 2022.\n[46] F. Foscarin, A. Mcleod, P. Rigaux, F. Jacquemard, and\nM. Sakai, “ASAP: a Dataset of Aligned Scores and\nPerformances for Piano Transcription,” in Proceedings\nof the 21st International Society for Music Information\nRetrieval Conference , 2020, pp. 534–541.\n[47] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” arXiv preprint arXiv:1412.6980 ,\n2014.\n[48] C. E. Cancino-Chacón, T. Gadermaier, G. Widmer, and\nM. Grachten, “An evaluation of linear and non-linear\nmodels of expressive dynamics in classical piano and\nsymphonic music,” Machine Learning , vol. 106, pp.\n887–909, 2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n595[49] S. P. Rosenblum, “Pedaling the Piano: A Brief Survey\nfrom the Eighteenth Century to the Present,” Perfor-\nmance Practice Review , vol. 6, no. 2, p. 8, 1993.\n[50] M. E. Tipping and C. M. Bishop, “Probabilistic Princi-\npal Component Analysis,” Journal of the Royal Statis-\ntical Society Series B: Statistical Methodology , vol. 61,\nno. 3, pp. 611–622, 1999.\n[51] I. Zacharov, R. Arslanov, M. Gunin, D. Stefonishin,\nA. Bykov, S. Pavlov, O. Panarin, A. Maliutin, S. Ryko-\nvanov, and M. Fedorov, ““Zhores”—Petaﬂops super-\ncomputer for data-driven modeling, machine learning\nand artiﬁcial intelligence installed in Skolkovo Insti-\ntute of Science and Technology,” Open Engineering ,\nvol. 9, no. 1, pp. 512–520, 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n596"
    },
    {
        "title": "The Games We Play: Exploring the Impact of ISMIR on Musicology.",
        "author": [
            "Vanessa Nina Borsan",
            "Mathieu Giraud",
            "Richard Groult"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265345",
        "url": "https://doi.org/10.5281/zenodo.10265345",
        "ee": "https://zenodo.org/records/10265345/files/000064.pdf",
        "abstract": "Throughout history, a consistent temporal and spatial gap has persisted between the inception of novel knowledge and technology and their subsequent adoption for extensive practical utilization. The article explores the dynamic interaction and exchange of methodologies between musicology and computational music research. It focuses on an analysis of ten years' worth of papers from the International Society for Music Information Retrieval (ISMIR) from 2012 to 2021. Over 1000 citations of ISMIR papers were reviewed, and out of these, 51 later works published in musicological venues drew from the findings of 28 ISMIR papers. Final results reveal that most contributions from ISMIR rarely make their way to musicology or humanities. Nevertheless, the paper highlights four examples of successful knowledge transfers between the fields and discusses best practices for collaborations while addressing potential causes for such disparities. In the epilogue, we address the interlaced origins of the problem as stemming from the language of new media, institutional restrictions, and the inability to engage in multidisciplinary communication.",
        "zenodo_id": 10265345,
        "dblp_key": "conf/ismir/BorsanGG23",
        "keywords": [
            "novel knowledge",
            "technology adoption",
            "dynamic interaction",
            "musicology",
            "computational music",
            "ISMIR",
            "International Society for Music Information Retrieval",
            "papers review",
            "musicological venues",
            "knowledge transfers"
        ],
        "content": "THE GAMES WE PLAY: EXPLORING THE IMPACT\nOF ISMIR ON MUSICOLOGY\nVanessa Nina Borsan Mathieu Giraud\nUniv. Lille, CNRS, Centrale Lille\nUMR 9189 CRIStAL, F-59000 Lille, France\n{vanessa,mathieu}@algomus.frRichard Groult\nUniv Rouen Normandie, INSA Rouen Normandie,\nUniversité Le Havre Normandie, Normandie Univ\nLITIS UR 4108, F-76000 Rouen, France\nrichard.groult@univ-rouen.fr\nABSTRACT\nThroughout history, a consistent temporal and spatial\ngap has persisted between the inception of novel knowl-\nedge and technology and their subsequent adoption for ex-\ntensive practical utilization. The article explores the dy-\nnamic interaction and exchange of methodologies between\nmusicology and computational music research. It focuses\non an analysis of ten years’ worth of papers from the Inter-\nnational Society for Music Information Retrieval (ISMIR)\nfrom 2012 to 2021. Over 1000 citations of ISMIR papers\nwere reviewed, and out of these, 51 later works published\nin musicological venues drew from the ﬁndings of 28 IS-\nMIR papers. Final results reveal that most contributions\nfrom ISMIR rarely make their way to musicology or hu-\nmanities. Nevertheless, the paper highlights four examples\nof successful knowledge transfers between the ﬁelds and\ndiscusses best practices for collaborations while address-\ning potential causes for such disparities. In the epilogue,\nwe address the interlaced origins of the problem as stem-\nming from the language of new media, institutional restric-\ntions, and the inability to engage in multidisciplinary com-\nmunication.\n1. INTRODUCTION\nIn 2005, Cook [1] critically addressed the prospects and\ndifﬁculties of collaborations between Music Information\nRetrieval (MIR) and musicology, many of which were re-\nvisited by Downie in 2009, further examining their impli-\ncations and potential advancements [2]. With the emer-\ngence of empirical research methods and advancements in\ntechnology, music research has encompassed multiple aca-\ndemic ﬁelds, leading to a transformation in the structures\nof these disciplines, including Music Information Retrieval\n(MIR) and contemporary musicology. Given their multi-\ndisciplinary nature, the categorization of either is becom-\ning increasingly arbitrary. However, for the purpose of\n© VN. Borsan, M. Giraud, and R. Groult. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: VN. Borsan, M. Giraud, and R. Groult, “The Games We\nPlay: Exploring The Impact of ISMIR on Musicology”, in Proc. of the\n24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.the clarity of further arguments in this paper, we classify\n“traditional” and humanities-centred music research ﬁelds\n(musicology, music theory, ethnomusicology, etc.) under\nthe umbrella term “musicology.” Conversely, we use the\nterm “MIR” to encompass all ﬁelds that engage in natural-\nsciences-based (typically computational) research related\nto music, such as acoustics, informatics, physics, mathe-\nmatics, engineering, and more1.\nDespite the signiﬁcant impact of both ﬁelds in broad-\nening our understanding of music, unresolved issues high-\nlighted by Cook continue to hinder their collaboration to\nthis day [1]. In recent years, a growing number of mu-\nsicologists, along with humanities researchers in general,\nhave shown a preference for working with digital materials\nrather than physical ones [3], but the application of com-\nputation to research can be approached at various levels.\nThere are general-purpose software , such as word proces-\nsors or spreadsheet editors, and music-oriented software ,\nsuch as Sibelius, Finale, and Audacity; there are program-\nming music/MIR platforms and libraries , such as Hum-\ndrum [4], music21 [5], Librosa [6] and Essentia [7] and\nthen there are methods and algorithms as developed by the\nMIR community, for example [8–11] and others (see [12]\nfor a detailed review). While computer usage is preva-\nlent among many researchers there are fewer musicologists\nwho adopt or contribute to similar methodologies. How-\never, through new media and computational advancements,\nmusic and our relationship to it are changing [13]. Given\nthe expansion of what is deemed signiﬁcant in the “realm\nof music,” it raises the question of whether familiarity with\ncomputational languages is becoming a prerequisite for its\nexploration.\nComputational methods assist researchers in handling\nlarger and more varied datasets, but, would musicologists\nagree that “working with [these] datasets [have] open[ed]\nup new areas of musicology?” [1] Or, has this shift evoked\nnew areas of research, which are (almost) independent\nfrom the musicological domain? The goal of this paper is\nto ask to what extent the MIR contributions (in the frames\nof ISMIR) resonate throughout the musicological commu-\nnity. The very results of these particular analyses may also\n1Our labels are arbitrary categories for rough orientation, considering\ndisciplines like music cognition that ﬁt into both/neither category.545highlight some of the core issues of miscommunication be-\ntween the two domains.\nWe start by outlining the “ready-made” arguments of\ncollaboratory issues of the ﬁelds (Section 1), followed by\na methodology introduction for a bibliographical study of\nten years of ISMIR papers (2010–2021) and their cita-\ntions, where we present some empirical results (Sections 2\nand 3). As an example of “good collaboratory practice”,\nwe detail four examples, where ISMIR ﬁndings were later\nused by/for a musicological audience through datasets,\nmethodologies, and tool and/or code (Section 4). In con-\nclusion, we discuss the results, examine potential reasons\nfor the outcomes, and draw on theories of play and media\nstudies to support our ﬁndings (Section 5).\n2. THE WEB OF ARGUMENTS\nNumerous authors have explored the advantages and/or\ndrawbacks of interdisciplinary research in the realm of mu-\nsic. We acknowledge the tensions within musicology with-\nout delving into the detailed evolution of historic musicol-\nogy, ethnomusicology, and systematic (empirical) musi-\ncology, as these topics have already been extensively cov-\nered( [14]). We focus on the development of (pro and con)\narguments, generally raised in the 2000s and2010s .\n2.1 Years 2000–2010: Enticement Versus Restraint\nThe critical discussions began with the emergence of more\nempirically-centred approaches, mostly labelled as sys-\ntematic musicology. Following the iconic question “Who\nstole systematic musicology?”, Leman [15] observed, that\neven systematic musicology had no longer belonged to\n“itself.” Conversely, transdisciplinary musicology gained\ntraction among engineering departments (MIR, sound pro-\ncessing), as and neuroscientists and psychologists, who de-\nveloped a growing interest in the study of music .\nAmidst the rapid growth of music-related technology\nproduction, papers in the early 2000s addressed contem-\nporary musicology, its redeﬁnition, and future methodolo-\ngies and goals. For some, technologies were viewed as a\nnatural extension for quantitative, big-data, and empirical\nmusic analyses [1], while others thought of music research\nas an interdisciplinary ground of “somewhat equal” sub-\ndisciplines, including musicology and MIR. Addressing\nthe beneﬁts of these collaborations, [16, 17], many authors\nhighlighted the beneﬁts of multidisciplinary projects in ex-\npanding the boundaries of isolated disciplines for more\ncomprehensive outcomes. In contrast, others warned that\nin “an era in which interdisciplinarity has become a kind\nof mantra, verbally subscribed to by nearly everyone, dis-\nciplines continue to police their own boundaries [18].” A\nsimilar opinion was shared by Parncutt [14], and Leman,\nwho stressed that, even though they like talking about in-\nterdisciplinary projects, “it was very rare that researchers\nwent beyond the boundaries of their own disciplines [15].”\nAdditionally, knowledge transfers are anything but ﬂuidamong computational scientists and musicologists, thus\nthe ideas expand poorly, if at all [19], hence, they must\nbe improved [12]. The scepticism towards uncondition-\nally welcoming the emerging collaboratory changes thus\nremained. In 2009, [2] reﬂected on interdisciplinary dy-\nnamics during the ﬁrst 10 years of ISMIR, highlighting\nits shortcomings, such as the inability to communicate the\nproduced tools to the user (performer, musicologists, ...),\nfavouring low-level over high-level features and audio over\nother symbolic music representations, and so forth.\n2.2 Years 2010–Today: The Quest for Consensus\nThe scepticism and critiques were not far-fetched nor prop-\nerly addressed, as Urberg later noticed that the method-\nological visions of “fundamentally-renewed” music re-\nsearch, had “not [yet] taken over the majority of musico-\nlogical scholarship [20].” Nonetheless, he imposed that the\nmethodology of research has already shifted, as there is an\nascending trend of new research tools and digitized (mu-\nsic) data representations, a lot of them consciously used by\nmusicologists. So what seems to be the problem?\nFinding balance in methodology, data collection and in-\nterpretation. Still in the second decade of the 21st century,\nwhen the introduced arguments began to overlap, Inskip et.\nal. [25] conducted a survey in order to answer this ques-\ntion. The study suggests that “[...] efforts should be made\ninto supporting the development of their digital skills and\nproviding usable, useful and reliable software created with\na ‘musicology-centred’ design approach.” Otherwise, the\n“data richness will lead to information overload [26].” As\nDahling expressed in 2012, there are many tools for music\ncollection and analysis, of which many “suffer from var-\nious shortcomings, such as speciﬁcity to a certain reper-\ntoire or approach, lack of robustness and ﬂexibility, ﬂawed\nuser interfaces, or output is difﬁcult to interpret [26].” A\nsimilar concern has been expressed by others, such as [27]\nand [28], or, for textual analysis [29]. All of them advo-\ncate not only for a more accessible and ﬂexible computa-\ntional methods , but also express the need understand what\nthese methods do and how . Alongside epistemological\nconfusion and other (methodological) drawbacks, a similar\nproblem was stressed by Aucouturier and Bigand. Their\ndialogue-style paper revealed the ﬂaws and prospects for\ncollaborations between MIR and music research (speciﬁ-\ncally music cognition) [30]. In Drucker’s words, “the hu-\nmanities are not a mere afterthought, simply studying and\ncritiquing the effects of computational methods. [Their\ntheory] can provide ways of thinking differently [31].” In\na different light, the latter was also implied by [32].\nCyclical collaboration vs discontinuity. Following\nDownie’s call for improvements [2], some authors dis-\ncussed reﬁned measurements that need to be considered re-\ngarding data collection and interpretation, for “obtaining or\naccessing high-quality datasets remains a serious hurdle,\nespecially on a large scale [33].” These hurdles limit the\n(digital) quality of music research, but not only that. AllProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n546Claim/link to musicology ISMIR papers Examples of claim\nNone, “musicolog” is only present in\none of the references147 (42.7%)\nApplication of musicological con-\ncepts, by only explaining citation,\nor apply musicological concepts, or\nhinting towards the possibility of\nmusicological application.81 (23.6%) [21] “In order to select relevant low-level features, we refer to musicology papers such as\n[...] which suggest that arousal is related to features including rhythm density, note\ndensity, key, dynamic, tempo, etc.”\n[22] “We assume that the music tradition is known, and that the rhythm class (t ¯al.a) of the\npiece is from a set of known (from musicological literature) t ¯al.as.”\nSome claim of musicological utility. 114 (33.7%) [23] “[...] retaining the rest of the presented framework, e.g. for an analytical ontology\nof musicological terms supporting the use of digital score annotations to illustrate\npoints in scholarly musicological arguments.” (see Section 4)\n[24] “These features can serve as inputs to machine learning algorithms, or they can be\nanalyzed statistically to derive musicological insights.” (see Section 4)\nTable 1 . Links and/or claims regarding musicology in 342 ISMIR papers from 2012 to 2021 where “musicolog” occurs.\nmusic cannot be collected and/or represented in the same\nmanner, and it is not feasible to investigate and discuss it\nwithin identical methodological frameworks [28,34]. They\nbelieve that this perspective should be considered not only\nby musicologists but should also be of equal importance\nfor the ﬁeld of MIR. Schüler and Huron argued that mu-\ntual theoretical awareness is essential for musicologists\nand MIR researchers [19,35]. Methodological tools should\nnot be confused with philosophical worldviews [35], and\ndue to the importance of theory and“practice”, there must\nexist a cyclical collaboration between the disciplines [27].\nHumanities scholars express concern about detached inter-\npretation and the prioritization of “facts” and algorithmic\nsuccess in studies [28, 29]. Thus, the algorithms must be\ntransparent enough for the scholars to actively participate\nin the building blocks of their framework and methods.\n“[I]n the long run, the most ’useful’ computational anal-\nyses will be the ones which are interactive, confronting a\nhuman user with the results of computational analysis and\nallowing that user to modify or intervene in the procedure\nto arrive at an acceptable or interesting result [28].”\nFrom a more critical standpoint, Becker asks whether\n“our failure [is] due to our own shortcomings in not becom-\ning thoroughly versed in the protocols and expectations of\nanother discipline? Or, was the failure due to too strin-\ngent protocols and expectations for publication in a [...]\njournal?”, concluding that some disciplinary barriers may\nbe unbreachable due to rigid institutional formations [18].\nLeman, conversely, sees the “failure” of collaboration in\nthe notion of the absence of “concrete planned goal at long\nterm, except some vague idea of what all these research\nactivities are up to [15].” Although no ﬁrm solutions have\nbeen introduced, some humanities authors [29, 36, 37] of-\nfered partial theoretical frameworks. Our methodology, in-\nspired by the latter (e.g., Moretti’s Distant Reading ), will\nbe introduced in the following section.\n3. METHODOLOGY AND RESULTS\nIn this section, we discuss the ﬁltering process of ISMIR\n2012–21 to examine whether andhow such papers were\nused in musicological studies. We also provide statistics\nand information on data availability.3.1 Article Selection and Filtration: Which papers\nclaim to have some musicological utility?\nWe downloaded all 1055 ISMIR papers2from the past\n10 years (2012–2021)3and converted the .pdf ﬁles to\n.txt ﬁles. We retrieved 342 articles which included the\nroot “musicolog”, meaning the article contained words\nlike “musicological,” “musicology”, and “ethnomusicol-\nogist”4. Next, we reviewed these 342 papers to deter-\nmine their musicological implications, categorizing them\ninto 3 categories (see Table 1 for examples and details).\nSubsequently, we focused on the 114 ISMIR papers that\nclaimed some musicological relevance and the citations, if\nany.\n3.2 Citations Analysis: Were the papers later used “in\nmusicology”?\nTo study how and if these 114 papers may have had an\nimpact on musicology, we identiﬁed 907 citations of them\nthrough Google Scholar. The median of all citations per\ncited paper is 16. The most cited paper was cited 208\ntimes, while 10 were never cited. We retrieved almost all\nof these citations5and sorted the citing papers by these\ntwo (slightly ambivalent) categories.\n➀Is any “citator” a musicologist? As “musicologists”,\nwe classiﬁed researchers with a Master’s or PhD degree\nin a “musicological” research ﬁeld or most of their ac-\ntivity was mostly conducted in a musicological environ-\nment (see Introduction). Together, there were 210 cita-\ntions to 67 unique ISMIR papers that corresponded with\nthis category.\n➁Does the citing paper appear in a musicological jour-\nnal/conference? Here, we focus on venues instead of in-\n2https://www.ismir.net/conferences/\n3Due to time constraints, we couldn’t thoroughly analyze all ISMIR\npapers. Instead, we focused on the impact of early 2000s ideas on the\nMIR and musicology collaboration, exploring new tools, and acknowl-\nedging changes due to improved technology and online publication ac-\ncessibility.\n4We acknowledge potential exclusions of articles using terms like\n“music research,” “music theory,” or “music history”, and that ISMIR\npapers may hold musicological signiﬁcance without explicitly stating so.\n5About 20 were excluded due to inaccessibility of the article or lack\nof information, among which 6 belong to centred dataset.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n547Journal/Conference Citations\n(Cited ISMIR papers)\nDigital Libraries for Musicology (DLfM)⋆31 (15)\nJournal of New Music Research (JNMR)⋆17 (15)\nActa Musicologica 7 (7)\nFrontiers in Digital Humanities⋆6 (5)\nEmpirical Musicology Review (EMR) 6 (5)\nFolk Music Analysis (FMA) 6 (5)\n4: Musicae Scientiae; Zeitschrift der Gesellschaft für Musikthe-\norie; Digital Scholarship in the Humanities⋆; McGill University\n(Schulich School of Music, Music Technology⋆); Utrecht Uni-\nversity (MA or PhD Thesis)⋆; 3: Music Theory Online (MTO);\nComputational Music Analysis; UC San Diego⋆; 2: Compu-\ntational Phonogram Archiving: Current Research in Systematic\nMusicology⋆; The Musical Quarterly; Journal on Computing and\nCultural Heritage⋆; Digital Humanities Quarterly⋆; +33 more (ap-\npear once)\n70 citations in total\nTable 2 . Somewhat musicology-centred jour-\nnals/conferences/books/institutions, in which ISMIR\npapers were cited 143 times. The venues marked with (⋆)\nhave both, musicology/MIR goals.\ndividuals, because researchers with musicological back-\ngrounds can have a strong root in MIR as well, while\nmusicological journals mainly target and publish works\nof primarily musicologically-motivated research activ-\nity. We deﬁned “musicological venues” by their pri-\nmary motivation and targeted audience (Table 2), some\nof them also have (secondary) MIR motivations (⋆in\nTable 2). ISMIR was fully excluded, with the intention\nto show to which extent these contributions manage to\n“leave” the ISMIR community. Together, there were\n143 citations in rather musicologically relevant publi-\ncations to 55 ISMIR papers.\nFrom here, we focus on the 143 citations, as the rest\neither focused on the MIR audience only (was published\nin technical, science, MIR conference) or did not imply\nthe musicological utility.\n3.3 Filtered Citations Analysis: What is the type of\ncitation/utility?\nWe sorted the 143 citations (or 114 unique citing articles)\nof previously mentioned 55 ISMIR papers, focusing on if\nand how the ﬁrst use the latter.\n✘Only referencing the ISMIR paper . 92 citations only ref-\nerence 43 ISMIR papers. The authors referenced the ar-\nticle, because it was relevant to the topic, however, their\ncontribution was not actually used.\nThe other 51 citations cited and somewhat used 28 IS-\nMIR contributions6, split into the following types:\n✓Dataset (10 citations to 5 ISMIR papers): The author(s)\nof citation (partially) used the dataset, presented in cited\nISMIR paper.\n6Certain ISMIR papers were referenced and utilized in various con-\ntexts, and/or classiﬁed under multiple utility categories.\n20175 10 15\nISMIR year ISMIR papers citing papers20162015201420132012\n2018\n2019\n2020\n2021(a) (b) (c)10 20 5 15 25\n(f) (d) (e)Figure 1 . Distribution of the papers reported in this study\namong the years. Left. (a) 114 ISMIR papers with “musi-\ncolog” root and claiming to have some musicological util-\nity; (b) from which 43 ISMIR papers cited in musicologi-\ncal venues; (c) from which 28 ISMIR papers actually used\nat least once. Right. (d) 143 citations in 51 citing papers\n(of the 43 ISMIR papers) from musicological venues; (e)\nfrom which 87 citations (74 unique citing papers) with at\nleast one musicologist as an author; (f) from which 35 (or\n31 unique citing papers) with actual usage (of the 28 IS-\nMIR papers). Even for citations, the considered year is the\nyear of the original ISMIR paper.\n✓Methodology (22 citations to 17 ISMIR papers): The\nauthor(s) of citation (partially) used the methodology,\npresented in cited ISMIR paper.\n✓Code/Tool (19 citations to 13 ISMIR papers): The au-\nthor(s) of citation (partially) used the code and/or tool,\npresented in cited ISMIR paper.\n3.4 Statistics on these Papers and Citations\nAbout 10% of ISMIR articles mention “musicolog” every\nyear. As expected, most recent papers are not cited (Fig-\nure 1). Despite the limited 10-year time span, papers that\nreceived at least one citation showed an average gap of\nthree years between publication and the ﬁrst citation. If\nwe consider the 81 “older papers” published between 2012\nand 2018, about the third of them have been actually used\nat least once in another study.\nThe list of musicological venues is also revealing (Ta-\nble 2): The conference that most frequently included IS-\nMIR’s contribution was DLfM, a community that started\nas a satellite event of ISMIR and that “provides a fo-\nrum for musicians, musicologists, librarians, and technol-\nogists to share ﬁndings and expertise7.” It is followed by\nJNMR, which “publishes systematic, scientiﬁc and tech-\nnological research on music, musical processes and musi-\ncal behaviours, including popular, cultural and canon mu-\nsic”8. The majority of the 143 citations (see Figure 1)\nappear in journals/conferences with a clearly stated incli-\nnation to MIR and/or digital humanities ((⋆) in Table 2)\nand include several MIR scientists.\n7https://dlfm.web.ox.ac.uk\n8https://www.tandfonline.com/journals/nnmr20Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n548Among 51 citations that used ISMIR papers, 16 papers\nwere (partial) self-citations, meaning there was at least one\ncommon author. However, in 12 cases, new team members\nwere involved (often from outside the initial institution),\nand in 4 cases, a new musicologist was present.\n3.5 Data Availability\nThe annotated data on the 114 papers, which claim to\nhave some musicological utility, and the one of 143 cit-\ning papers of the 28 papers are available on a git repository\nthrough open licences (Open Database License, Database\nContents License) at algomus.fr/data .\n4. FOUR EXAMPLES OF KNOWLEDGE AND\nIDEA TRANSFERS\n51 citations in musicological venues were thus used one\nof the 28 ISMIR papers through its dataset, methodology,\ncode and/or tool. We focused on four of these stories: In\nthe qualitative observation, we picked examples that de-\nscribe the type of utility of ISMIR contribution.\nDespite some self-citations, promising collaborations\nwere observed within research teams integrating interdis-\nciplinary dynamics between musicology and MIR. These\nteams included both computer/MIR scientists and “con-\nventionally” trained musicologists.\nTool: VIS Framework. In an ISMIR 2014 paper, re-\nsearchers from the Distributed Digital Music Archives &\nLibraries Lab at McGill University introduced the VIS\nFramework, a Python library for music analysis together\nwith a case study on counterpoint patterns in symbolic mu-\nsic scores [38]. The library was further used and cited\nby the same group in “musicological” venues, such as\na study on encoding and translation issues published in\nDLfM [39]. Two PhD theses from the Schulich School\nof Music (McGill University) also used the framework.\nFirst proposed a computer-assisted approach to the study of\ninterval-succession treaties [40], while second studied the\ntonality practice of seventeenth-century Italian composers\nin trio-sonatas [41] and used VIS to extract features. The\nVIS GUI was found to be essential in making the analysis\ntask easier for non-computational scientists.\nDataset: The Story of Jingju. The Music Technology\nGroup (UPF, Barcelona, Spain) includes the ethnomusi-\ncologist, Repetto. His ISMIR 2017 paper with Serra intro-\nduced JMSC, of collection of scores or Jingju (also called\n“Beijing Opera”) [42]. Two citing DLfM 2017 papers9\nanalyzed the melodic syllabic contours in JMSC [43, 44],\neach paper including another member of the MTG joining\nthe two authors of the ISMIR paper.\nMultidisciplinary environments have been created by\nMIR and music teams globally, fostering collaboration\n9DLfM was a satellite event of ISMIR at that time, meaning the papers\nand their citations appeared (and were likely prepared) simultaneously.with external groups, attracting more scientists, and ex-\npanding opportunities for obtaining PhD positions from\nboth sides. The following story exempliﬁes how a mul-\ntidisciplinary group can attract new collaborations.\nMethodology/Tool: The Lohengrin TimeMachine. An\nISMIR 2017 paper by Weigl and Page, from the Univer-\nsity of Oxford, presented an update on the MELD frame-\nwork [23], used to encode information of and about mu-\nsic(e.g., digital representations of notation, audio, contex-\ntual information) inside MEI. MELD has been cited by 25\nother papers. One of the “MELD applications”, the Lohen-\ngrin TimeMachine was presented at DLfM in 2021 [45]\nby Lewis and Page, as well as Dreyfus, an American mu-\nsicologist who was previously not involved with the MIR\ncommunity. In his late career, he was appointed at the Uni-\nversity of Oxford – but in the music department. The ap-\nplication explored a few extracts of Wagner’s Lohengrin\nthrough scores, motives, orchestration, structure, texts, au-\ndio/video, musicological analysis, etc. It offers interesting\nrepresentations to a wider audience of both musical knowl-\nedges but also on the very methodology of the musicolog-\nical research. This citation is also a good example of the\ntime it may take to cross domains (here, 4 years).\nTool: Mindfulness and Music Performance Study . In\nISMIR 2017, researchers from IRCAM presented the PiPo\nplugin, designed for data stream processing in various do-\nmains including interactive audio processing and MIR.\nThis API-based tool facilitates the extraction of low-level\ndescriptors from audio and motion data streams [46]. A\n2021 citing paper in Psychology of Music , from a com-\npletely independent group, in Israel, examined whether\nshort-term mindfulness meditation activity would improve\nmusic performance (vocal skills) regarding pitch intona-\ntion, dynamics transmission, and vocal resonation [47].\nThey use the PiPo tool in the processing phase, using PiPo\nmodules for the automatic segmentation of markers by on-\nset (time-tagged frames) for low-level descriptor extraction\n(pitch, dynamics, timbre ...). Focusing on music psychol-\nogy, this application doesn’t qualify as a musicological\nstudy. However, it showcases how MIR methods can be\napplied to humanities-based music research. Interestingly,\nout of the 114 ISMIR papers examined, this is the only one\nreused in a “musicological” context independently of the\noriginal authors.\n5. DISCUSSION AND CONCLUSION\nWhile ISMIR is not exclusively focused on musicology,\ncertain researchers who publish at ISMIR assert their im-\npact on the ﬁeld. Our examination of the last ﬁve years as\nwell as a ten-year period of ISMIR reveals that the majority\nof these contributions seldom make their way into musico-\nlogical or humanities scholarship. Out of the 28 ISMIR pa-\npers, which have been cited and used, the majority of them\nare partly self-cited, and/or are “re-used” within the same\ngroup, lab etc. Somehow, we did not ﬁnd a single exampleProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n549of independent musicological application of ISMIR 2012-\n2021 contributions in a traditional musicological journal.\nWe are aware that our study has some biases. To broadly\nobserve how MIR and other music research interact, we\nshould explore the utility dynamics both ways (ISMIR to\nmusicology, andmusicology to ISMIR), as well as analyze\nroots other than “musicolog” in multiple venues (both MIR\nand music) and thoroughly explore the organizers, institu-\ntions, and authors. There are also time10and space11vari-\nables, which could have had an impact on the results. Re-\nsearch and collaboration cannot always be measured solely\nby points or numbers. Non-citable research and pedagogi-\ncal activities at universities are valuable components that\nmay not be easily quantiﬁable. In some cases, tools or\ndatasets may be used for inspiration without being cited\nin the ﬁnal report. Similarly, ISMIR-presented tools may\nbe employed without direct citation, with references made\nto non-ISMIR contributions or other sources.\nVarious technologies have undoubtedly made their way\nto musicologists, inspiring the creation of a quasi-common\nground with IT and other domains. However, further ef-\nforts are necessary to establish a consistent circulation of\nknowledge. While some are managing this challenge (see\nSection 4), most still struggle.\nThis struggle could be understood through theories of\nthe game (or play) by Huizinga [48] then Caillois [49].\nThey discuss how the games we play are not only those of\n“leisure” (sports, video games, ...) but also “law and order,\ncommerce and proﬁt, craft and art, [...] and science. All\nare rooted in the primaeval soil of play [48].” Caillois con-\nsiders day-to-day games people play in the light of com-\npetitive examinations and economic competition [49], and\nhis six rules very much resemble the scientiﬁc atmosphere.\nLike play, it is 1. not obligatory to participate in science,\nwhich 2. must be conducted (or “played”) in an environ-\nment, pre-deﬁned in time and space . 3. The strategy (re-\nsearch development) is left to the individual ideas 4. and\nis generally locked in an inﬁnite loop of “unproductivity”\n(meaning, it is largely being developed and executed and\nre-executed within itself). Both (games and science) fol-\nlowconventional rules and take place in 6. “make-believe”\nworld, which is accompanied by a special awareness of a\nsecond reality . For example, this may as well be the daily\nshift from one’s research to mundane events. Games (or\nscience) can only be played when all parties are in agree-\nment with the particular rules .\nSeveral of these may be incompatible between the MIR\nand musicology, one of them being, as mentioned in [50],\nthe language of new media (similar idea in [13]). As later\nelaborated by [51] and [52], this language has, “in the\nprocess of epochal technological change” never been im-\nmediate, but instead adopted “through a process of tran-\nsition [52].” Since the majority of new technologies (or\n10The contributions we examine may be applied in the future.\n11Some venues cannot be observed through Google Scholar, and some\ncontributions may not have cited the source when applying their tools or\ndatabases in their research.languages) for music analysis “skipped” the transitional\nera, and are, for an average musicologist, incomprehensi-\nble or non-intuitive (algorithmic codes), the computational\nproducts “do not manage to address them [musicologists]\nin an intelligible way.” There seems to be a “clear dis-\nconnect between how MIR tasks are designed to evalu-\nate systems, and how end users are supposed to use those\nsystems [...] [making them] difﬁcult and costly to imple-\nment [12]”. Consequently, the results, produced by such\nprocesses also become unusable, as the “involvement in\nthe wheel of algorithms is indispensable for musicological\nresearch [13, 52].” It is this kind of disruption alone, that\ncan disable the multidisciplinary game.\nReﬂecting on our discussion in Section 2, Huron, im-\nposed the obligation for both parties (MIR and musicol-\nogy) to familiarize themselves with each other’s method-\nologies [35]. Additionally, [30] highlights the importance\nof knowing which parts of whose methodology are to be\nused for a fruitful collaboration. Leman suggests solving\nthe gap by inducing multi-modality, introducing context-\nbased approaches into empiricism [15]; and a more re-\nserved Parncutt, explains that the wall is set by the feeling\nof superiority on both sides [14], and so on. Still, the rules\nof the playground must ﬁrst reach consensus (starting with\nthe transition towards a common “language”). And this\nis where these “common grounds” come to light. ISMIR\nin itself is a multidisciplinary environment, however, most\nof the participants (deriving from natural rather than hu-\nmanities or social sciences), already play by similar rules\n(or speak the same language). Consequently, the multidis-\nciplinary activity within MIR remains rather limited and,\ndespite numerous surveys [12] has yet been unable to prop-\nerly address all of the (reasons for) constraints mentioned\nby [2] about 14 years ago. As seen in 3.4, the most cited\npapers in musicological venues are derived from DLfM\nand JNMR. This is not a coincidence, as these are “institu-\ntions”, whose “rules” derive from a compromise between\nboth disciplines, as well as the majority of yearly contri-\nbutions, manage to speak the language of both. It hence\nmakes sense, that one of the mentioned papers address-\ning these matters [30] is structured as a dialogue , as it is\nexactly that, ﬁnding a practical working consent among\n(the two) sciences, that can endorse a fertile collabora-\ntion. Merely adapting to each other’s rules seems like try-\ning to simultaneously play football and handball, where\nsimilar “material” surely cannot and will not bring a con-\nsensus between the two games. The successful examples\n(Section 4) and mentioned discussions, should be consid-\nered to help us advance our fundamental goals on institu-\ntional grounds and go beyond both MIR and musicology.\nIn the process of transductive ergomimesis, “new digital\nmedia drastically reposition the people” [13] and repeat-\nedly evoke new (motor) skills and techniques, professions,\nand multidisciplinary actions (see also [53]). The change\nis hence indispensable for the two ﬁelds, “but we’ve got to\nput in place the [institutional] conditions to make it actu-\nally happen” [1]. It seems that it is, in the end, this game\n(digital) musicologists may want actually want to play.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n550Acknowledgements. We thank Ken Déguernel, Louis\nCouturier, the Algomus team, and the anonymous review-\ners for their thought-provoking remarks.\n6. REFERENCES\n[1] N. Cook, “Towards the compleat musicologist,” in\nInternational Conference on Music Information Re-\ntrieval (ISMIR 2005) , 2005.\n[2] J. S. Downie, D. Byrd, and T. Crawford, “Ten years of\nismir: Reﬂections on challenges and opportunities.” in\nISMIR , 2009, pp. 13–18.\n[3] T. C. Duguid, M. Feustle, F. Giannetti, and E. Grum-\nbach, “Music scholarship online (MuSO): a research\nenvironment for a more democratic digital musicol-\nogy,” Digital Humanities Quarterly , vol. 13, no. 1,\n2019.\n[4] D. Huron, “Music information processing using the\nHumdrum toolkit: Concepts, examples, and lessons,”\nComputer Music Journal , vol. 26, no. 2, pp. 11–26,\n2002.\n[5] M. S. Cuthbert and C. Ariza, “music21: A toolkit for\ncomputer-aided musicology and symbolic music data,”\nInternational Society for Music Information Retrieval\nConference (ISMIR 2010) , pp. 637–642, 2010.\n[6] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,\nE. Battenberg, and O. Nieto, “librosa: Audio and music\nsignal analysis in Python,” in Python in Science Con-\nference (SCIPY 2015) , vol. 8, 2015, pp. 18–25.\n[7] D. Bogdanov, N. Wack, E. Gómez Gutiérrez, S. Gulati,\nH. Boyer, O. Mayor, G. Roma Trepat, J. Salamon, J. R.\nZapata González, X. Serra et al. , “Essentia: An au-\ndio analysis library for music information retrieval,” in\nInternational Society for Music Information Retrieval\nConference (ISMIR 2013) , 2013, pp. 493–498,.\n[8] O. Lartillot, “Automated motivic analysis: An exhaus-\ntive approach based on closed and cyclic pattern min-\ning in multidimensional parametric spaces,” in Compu-\ntational Music Analysis . Springer, 2016, pp. 273–302.\n[9] M. Mongeau and D. Sankoff, “Comparison of musical\nsequences,” Computers and the Humanities , vol. 24,\nno. 3, pp. 161–175, 1990.\n[10] E. Cambouropoulos, “The local boundary detection\nmodel (LBDM) and its application in the study of\nexpressive timing,” in International Computer Music\nConference (ICMC 2001) , 2001, pp. 7–22.\n[11] C. Finkensiep, K. Déguernel, M. Neuwirth, and\nM. Rohrmeier, “V oice-leading schema recognition us-\ning rhythm and pitch features,” in International Soci-\nety for Music Information Retrieval Conference (IS-\nMIR 2020) , 2020, pp. 520–526.[12] M. Schedl, E. Gómez, J. Urbano et al. , “Music in-\nformation retrieval: Recent developments and applica-\ntions,” Foundations and Trends® in Information Re-\ntrieval , vol. 8, no. 2-3, pp. 127–261, 2014.\n[13] T. Magnusson, Sonic writing: technologies of material,\nsymbolic, and signal inscriptions . Bloomsbury Pub-\nlishing USA, 2019.\n[14] R. Parncutt, “Systematic musicology and the history\nand future of Western musical scholarship,” Journal of\ninterdisciplinary music studies , vol. 1, no. 1, pp. 1–32,\n2007.\n[15] M. Leman, “Systematic musicology at the crossroads\nof modern music research.” in Systematic and com-\nparative musicology: Concepts, methods, ﬁndings ,\nP. Lang, Ed., 2008, pp. 89–115.\n[16] K. Neubarth, M. Bergeron, and D. Conklin, “Associ-\nations between musicology and music information re-\ntrieval.” in International Society for Music Information\nRetrieval Conference (ISMIR 2011) , 2011, pp. 429–\n434.\n[17] A. V olk, F. Wiering, and P. Kranenburg, “Unfolding\nthe potential of computational musicology,” in Inter-\nnational Conference on Informatics and Semiotics in\nOrganisations (ICISO 2011) , 2011, pp. 137–144.\n[18] J. Becker, “Crossing boundaries: An introductory es-\nsay,” Empirical Musicology Review , vol. 4, no. 2, pp.\n45–48, 2009.\n[19] N. Schüler, “Reﬂections on the history of computer-\nassisted music analysis 1: Predecessors and the begin-\nnings,” Musicological Annual , vol. 41, no. 1, pp. 31–\n43, 2005.\n[20] M. Urberg, “Pasts and futures of digital humanities in\nmusicology: Moving towards a “bigger tent”,” Mu-\nsic Reference Services Quarterly , vol. 20, no. 3-4, pp.\n134–150, 2017.\n[21] H. H. Tan and D. Herremans, “Music fadernets: Con-\ntrollable music generation based on high-level features\nvia low-level feature modelling,” in International So-\nciety for Music Information Retrieval Conference (IS-\nMIR 2020) , 2020.\n[22] A. Srinivasamurthy, A. Holzapfel, and X. Serra, “In-\nformed automatic meter analysis of music recordings,”\ninInternational Society for Music Information Re-\ntrieval Conference (ISMIR 2017) , 2017, pp. 679–685.\n[23] D. Weigl and K. Page, “A framework for distributed\nsemantic annotation of musical score:\" take it to the\nbridge!\",” in International Society for Music Informa-\ntion Retrieval Conference (ISMIR 2017) , 2017, pp.\n221–228.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n551[24] C. McKay, J. Cumming, and I. Fujinaga, “jSymbolic\n2.2: Extracting features from symbolic music for use in\nmusicological and MIR research.” in International So-\nciety for Music Information Retrieval Conference (IS-\nMIR 2018) , 2018, pp. 348–354.\n[25] C. Inskip and F. Wiering, “In their own words: Us-\ning text analysis to identify musicologists’ attitudes to-\nwards technology,” in International Society for Music\nInformation Retrieval Conference (ISMIR 2015) , 2015,\npp. 455–461.\n[26] E. Dahlig-Turek, S. Klotz, R. Parncutt, and F. Wiering,\nMusicology (Re-) Mapped: Discussion Paper . Euro-\npean Science Foundation, 2012.\n[27] P. Van Kranenburg, J. Garbers, A. V olk, F. Wiering,\nL. P. Grijp, and R. C. Veltkamp, “Collaboration per-\nspectives for folk song research and music information\nretrieval: The indispensable role of computational mu-\nsicology,” Journal of Interdisciplinary Music Studies ,\nvol. 4, no. 1, pp. 17–43, 2010.\n[28] A. Marsden, “Music analysis by computer: Ontology\nand epistemology,” in Computational Music Analysis ,\n2016, pp. 3–28.\n[29] J. E. Dobson, Critical digital humanities: the search\nfor a methodology . University of Illinois Press, 2019.\n[30] J.-J. Aucouturier and E. Bigand, “Mel Cepstrum &\nAnn Ova: The difﬁcult dialog between MIR and mu-\nsic cognition.” in International Society for Music Infor-\nmation Retrieval Conference (ISMIR 2012) , 2012, pp.\n397–402.\n[31] J. Drucker, “Humanistic theory and digital scholar-\nship,” Debates in the digital humanities , vol. 150, pp.\n85–95, 2012.\n[32] F. Morreale, “Where does the buck stop? ethical and\npolitical issues with AI in music creation,” Transac-\ntions of the International Society for Music Informa-\ntion Retrieval , vol. 4, no. 1, pp. 105–113, 2021.\n[33] L. Pugin, “The challenge of data in digital musicol-\nogy,” Frontiers in Digital Humanities , vol. 2, p. 4,\n2015.\n[34] S. Münnich, “FAIR for whom? Commentary on Hof-\nmann et al. (2021),” Empirical Musicology Review ,\nvol. 16, no. 1, pp. 151–153, 2021.\n[35] D. Huron, “The new empiricism: Systematic musicol-\nogy in a postmodern age,” The 1999 Ernest Bloch Lec-\ntures , pp. 1–32, 1999.\n[36] S. Ahlbäck, “Melody beyond notes: A study of melody\ncognition,” Ph.D. dissertation, Göteborgs Universitet,\n2004.\n[37] F. Moretti, Distant reading . Verso Books, 2013.[38] C. Antila and J. Cumming, “The VIS framework: Ana-\nlyzing counterpoint in large datasets.” in International\nSociety for Music Information Retrieval Conference\n(ISMIR 2014) , 2014, pp. 71–76.\n[39] N. Nápoles, G. Vigliensoni, and I. Fujinaga, “Encoding\nmatters,” in Digital Libraries for Musicology (DLfM\n2018) , 2018, pp. 69–73.\n[40] A. Morgan, “Renaissance interval-succession theory:\nTreatises and analysis,” Ph.D. dissertation, Schulich\nSchool of Music, McGill University, 2017.\n[41] S. Howes, “Tonality and transposition in the\nseventeenth-century trio sonata,” Ph.D. disserta-\ntion, Schulich School of Music , McGill University,\n2021.\n[42] R. C. Repetto and X. Serra, “A collection of music\nscores for corpus based jingju singing research.” in\nInternational Society for Music Information Retrieval\nConference (ISMIR 2017) , 2017, pp. 46–52.\n[43] R. Gong, R. C. Repetto, and X. Serra, “Creating an\na cappella singing audio dataset for automatic Jingju\nsinging evaluation research,” in Digital Libraries for\nMusicology (DLfM 2017) , 2017, pp. 37–40.\n[44] R. C. Repetto, S. Zhang, and X. Serra, “Quantitative\nanalysis of the relationship between linguistic tones\nand melody in Jingju using music scores,” in Digital\nLibraries for Musicology (DLfM 2017) , 2017, pp. 41–\n44.\n[45] D. Lewis, K. Page, and L. Dreyfus, “Narratives and ex-\nploration in a musicology app: Supporting scholarly\nargument with the Lohengrin TimeMachine,” in Digi-\ntal Libraries for Musicology (DLfM 2021) , 2021, pp.\n50–58.\n[46] N. Schnell, D. Schwarz, J. Larralde, and R. Borghesi,\n“Pipo, a plugin interface for afferent data stream pro-\ncessing modules,” in International Society for Music\nInformation Retrieval Conference (ISMIR 2017) , 2017.\n[47] E. Ornoy and S. Cohen, “The effect of mindfulness\nmeditation on the vocal proﬁciencies of music edu-\ncation students,” Psychology of Music , vol. 50, no. 5,\n2021.\n[48] J. Huizinga, Homo ludens . Routledge, 1949.\n[49] R. Caillois, Man, Play, and Games . University of\nIllinois Press, 2001.\n[50] L. Manovich, The language of new media . MIT press:\nCambridge, USA, 2002.\n[51] P. Krašovec, Tujost kapitala . Sophia, 2021.\n[52] V . N. Borsan and L. Stefanija, “Introduction,” Musico-\nlogical Annual , vol. 58/2, pp. 10–14, 2022.\n[53] F. A. Kittler, Gramophone, ﬁlm, typewriter . Stanford\nUniversity Press, 1999.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n552"
    },
    {
        "title": "Adding Descriptors to Melodies Improves Pattern Matching: A Study on Slovenian Folk Songs.",
        "author": [
            "Vanessa Nina Borsan",
            "Mathieu Giraud",
            "Richard Groult",
            "Thierry Lecroq"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265329",
        "url": "https://doi.org/10.5281/zenodo.10265329",
        "ee": "https://zenodo.org/records/10265329/files/000056.pdf",
        "abstract": "The objective of pattern-matching topics is to gain insights into repetitive patterns within or across various music genres and cultures. This approach aims to shed light on the recurring instances present in diverse musical traditions. The paper presents a study analyzing folk songs using symbolic music representation, including melodic sequences and musical information. By examining a corpus of 400 monophonic Slovenian tunes, we are releasing annotations of structure, contour, and implied harmony. We propose an efficient algorithm based on suffix arrays and bit-vectors to match both music content (melodic sequence) and context (descriptors). Our study reveals that certain descriptors, such as contour types and harmonic \"stability\" exhibit variations based on phrase position within a tune. Additionally, combining melody and descriptors in pattern-matching queries enhances precision for classification tasks. We emphasize the importance of the interplay between melodic sequences and music descriptors, highlighting that different pattern queries may have varying levels of detail requirements. As a result, our approach promotes flexibility in computational music analysis. Lastly, our objective is to foster the knowledge of Slovenian folk songs.",
        "zenodo_id": 10265329,
        "dblp_key": "conf/ismir/BorsanGGL23",
        "keywords": [
            "pattern-matching",
            "music genres",
            "cultural insights",
            "repetitive patterns",
            "diverse musical traditions",
            "folk songs",
            "symbolic music representation",
            "melodic sequences",
            "musical information",
            "corpus of 400 monophonic Slovenian tunes"
        ],
        "content": "ADDING DESCRIPTORS TO MELODIES\nIMPROVES PATTERN MATCHING:\nA STUDY ON SLOVENIAN FOLK SONGS\nVanessa Nina Borsan Mathieu Giraud\nUniv. Lille, CNRS, Centrale Lille\nUMR 9189 CRIStAL, F-59000 Lille, France\n{vanessa,mathieu}@algomus.frRichard Groult Thierry Lecroq\nUniv Rouen Normandie, INSA Rouen Normandie,\nUniversité Le Havre Normandie, Normandie Univ\nLITIS UR 4108, F-76000 Rouen, France\n{thierry.lecroq,richard.groult}@univ-rouen.fr\nABSTRACT\nThe objective of pattern-matching topics is to gain insights\ninto repetitive patterns within or across various music gen-\nres and cultures. This approach aims to shed light on the\nrecurring instances present in diverse musical traditions.\nThe paper presents a study analyzing folk songs using sym-\nbolic music representation, including melodic sequences\nand musical information. By examining a corpus of 400\nmonophonic Slovenian tunes, we are releasing annota-\ntions of structure, contour, and implied harmony. We pro-\npose an efﬁcient algorithm based on sufﬁx arrays and bit-\nvectors to match both music content (melodic sequence)\nand context (descriptors). Our study reveals that certain\ndescriptors, such as contour types and harmonic “stabil-\nity” exhibit variations based on phrase position within a\ntune. Additionally, combining melody and descriptors in\npattern-matching queries enhances precision for classiﬁ-\ncation tasks. We emphasize the importance of the inter-\nplay between melodic sequences and music descriptors,\nhighlighting that different pattern queries may have vary-\ning levels of detail requirements. As a result, our ap-\nproach promotes ﬂexibility in computational music anal-\nysis. Lastly, our objective is to foster the knowledge of\nSlovenian folk songs.\n1. INTRODUCTION\nMusic pattern analysis in the ﬁeld of Music Information\nRetrieval (MIR) is extensively studied. The challenges of\nthis topic extend beyond algorithms, encompassing diverse\nmusic forms, representation (signal, symbolic, or textual),\nmusic content, and cultural metadata.\n© VN. Borsan, M. Giraud, R. Groult and T. Lecroq. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: VN. Borsan, M. Giraud, R. Groult and T.\nLecroq, “Adding Descriptors to Melodies Improves Pattern Matching: a\nStudy on Slovenian Folk Songs”, in Proc. of the 24th Int. Society for\nMusic Information Retrieval Conf., Milan, Italy, 2023.1.1 Content and Context in Ethnomusicology\nEthnomusicologists analyze recordings, live perfor-\nmances, and transcriptions (in various notations) to un-\nderstand the composition of music. While transcriptions\nreveal the what of the music, cultural context is essential\nfor comprehending the how andwhy behind these musical\nstructures.\nInitiated by Merriam [1], and many others [2–7], the\nmusic is to be observed inculture (in his later work, as\nculture), or as a multi-dimensional object, a direct conse-\nquence of the organization of social structures, and vice\nversa. Some studies [4, 6, 8–10] have primarily compared\nfolk tunes based on their music content. In others, includ-\ning the Slovenian Folk Song Collection [11], the catego-\nrization of the collection is organized according to the ele-\nments, such as lyrics and other textual content.\nConsidering music material , it can be explored as a gen-\neral outline for music analysis [12], or through speciﬁc\nmusic descriptors, such as melodic contour (the melodic\narch shape) [13]. Recent studies have expanded the use of\ndescriptors to analyze folk songs, incorporating a broader\nrange of attributes. De la Ossa [14] suggests basic music\ndescriptors be included, such as scale types, range, sev-\neral levels of rhythmic information, and so on. Canto-\nmetrics , introduced by Lomax [15, 16], proposed 37 de-\nscriptors, (almost) independent from usual Western mu-\nsic theory. His idea of representing datasets as a digital\n“Global Jukebox” was recently completed. [17–19]. Com-\nputational methodologies encourage us to process more\ndata, including multiple layers of music content , and con-\ntextas descriptors (music and/or metadata). Serra [20] ex-\nposed the presence of musical entities (performer, music,\ninstrument, etc.) that “are linked by various types of rela-\ntionships,” which contribute to the understanding of music\nas a whole. Conklin and Neubarth also stressed the im-\nportance of non-musical information, such as region and\ngenre [21], extended Densmore’s and observed (super)area\nand (super)type information [22]. These non-musical phe-\nnomenons, although limited, were always correlated with\ndifferent types of music content (rhythm, melody, pattern,\nand antipattern [23]) of folk songs. Although focusing\non musical content, especially on cadences, van Kranen-474burg also considers lyric, perceptual information, as well\nas other information [24–26].\n1.2 Pattern Inference and Matching\nMusic pattern searching or matching is most commonly\napproached from a music analysis perspective [12,27–30],\nby addressing music structures, such as melody, harmony,\nand rhythm. Previous contributions focused on a single\nor a couple of features or the computational representa-\ntions and matching of multi-feature music patterns. The\nMongeau-Sankoff algorithm [31] simultaneously explores\nmultidimensional music features, as it deﬁnes the distance\nbetween any two melodies depending on the pitch, tonal\ncontour and rhythmic structure. The pattern similarity is\nranked by the number of transformations, including con-\nsolidation and fragmentation.\nOther dynamic programming methods for melodic se-\nquence alignment were proposed [6, 32, 33], as well as\nother methods on the general melodic or pitch-related\nqueries [33–36]. Some research added the rhythmic [37],\nor, especially with multipart music, the harmonic informa-\ntion [38]. In another solution, Marsden adapts the hier-\narchical or tree structures for representing and comparing\nmelodies [39]. Lartillot, conversely, matched melodic se-\nquence (or motives) by using heterogeneous patterns [30],\nwhose occurrences can be located through multiple para-\nmetric dimensions – including contextual ones, such as im-\nplied harmony.\n1.3 Motivation and Contents\nThese studies indicate that a melodic pattern is not isolated\nfrom other musical elements, such as a phrase, rhythmic or\nharmonic structure, ornamentation, and so on. While most\ndistinguish between music material andcultural metadata ,\nwe instead split the ﬁrst into melodic sequence anddescrip-\ntors (see Table 1). Our objective is that melodic phrase\nshould never be detached from its context. Hence, we focus\non segmented melodic phrases that never lose their iden-\ntiﬁer (connecting them to all supplementary (meta)data),\nnor their position within a tune (ﬁrst, middle, last). This\nenables the tune description by phrase position, contours,\nlabels, and rough harmonic tendencies , and to easily ac-\ncess and apply any combination of other (meta)data infor-\nmation to the pattern-matching process.\nIn Section 2 we introduce the corpus as well as our an-\nnotation methodology on metadata and descriptors, includ-\ning structure, contour, and implied harmony. Section 3 in-\ntroduces a pattern-matching algorithm that utilizes sufﬁx\narrays (for all melodies) and bit arrays (a selection of de-\nscriptors) to return matched results based on melodic con-\ntent and controlled descriptor context. Sections 4 and 5\ndiscuss the implementation and the results of examples of\ncombined melody/descriptor queries, and Section 6 pro-\nvides concluding remarks and addresses open perspectives.2. THE ANNOTATED CORPUS\nOF SLOVENIAN FOLK SONGS\n2.1 The Corpus\nWe are expanding the digital world of folk songs ( [17,40–\n42] and others) with a limited selection of tunes from the\nlargest collection of Slovenian folk songs – SLP, Slovenske\nljudske pesmi ), which consists of 5 critically edited phys-\nical books, issued between 1970 and 2007 [11, 43–46].\nTunes belong to narrative song genre (Figure 1) or hy-\nbrids between narrative and lyric genres (resemble narra-\ntive form, but much shorter). These are divided into types\n(by lyric resemblance) and their variants.\nThe ﬁfth and last book [11] was edited and issued by the\nSlovenian Ethnomusicological Institute (Research Centre\nof the Slovenian Academy of Sciences and Arts, consecu-\ntively mentioned as GNI), which later digitized (OCR-ed\npdf, musicXML, sib) by Matija Marolt and Matevž Pesek\n(FRI, University of Ljubljana). Most tunes are transcrip-\ntions of ﬁeld recordings collected members of the insti-\ntute, and external colleagues, such as Franc Kramar (1890–\n1959), Josip Dravec (1912–1996), and Stanko Vraz (1810–\n1851). Most tunes were collected and/or transcribed be-\ntween the 1950s and 1970s. The earliest two (notated) tran-\nscriptions date back to 1819 and 1839, while the most re-\ncent transcription was completed in 2001. Together, there\nare 650 tune variants of 54 types and belong to family\nfates and conﬂicts topic, which represents about 34% of all\nSlovenian ballads [11]. Some tunes have one or two, while\nthe most popular have 100+ variants ( Infanticide Bride ,\nA Widower at His Wife’s Grave ,Step-mother and an Or-\nphan , and Convicted Infanticide ). which mostly belong to\nStyria (166), followed by Upper Carniola (133) and Lower\nCarniola (99) regions.\nOut of those, 418 are transcribed as monophonic,\n2181as homophonic, and 8 as mixed. About 70%\nwere performed by a solo female singer. Instrumentally-\naccompanied examples are very rare. All tunes were trans-\nposed to G (major/minor) by the editors. Recent anno-\ntations include special notation symbols indicating devi-\nations, such as slight disparity in pitch (higher/lower), du-\nration (shorter/longer), and more [11].\n2.2 Descriptors and Corpus Annotation\nOut of the 418 monophonic tunes, 18 were excluded due\nto incomplete score information or incompatible encoding.\nOur ﬁnal corpus contains 400 monophonic tune variants\nwith detailed manual annotations, made by the ﬁrst au-\nthor and reviewed by all contributors. An average tune has\nabout 9 bars and 30 notes. Phrase boundaries were anno-\ntated by curating the output of a simple heuristic relying\n1Some melodies may have been harmonized by the annotator, making\nit unclear which singing line represents the original tune. This is common\nin transcriptions by Franc Kramar (1890-1959). The opposite can be true\nfor monophonic transcriptions.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n475ID POS LBL CT H SHE\n239.A.9.1 F A ↗↘ T T\n239.A.9.2 L B →↘ T T\nID POS LBL CT H SHE\n244.4.1 F A ↘ D ?(D/T)\n244.4.2 M B ↗↘ T T\n244.4.3 M A’ ↗↘ ?(T) ?(D)\n244.4.4 L B’ ↘ ?(T) T\nFigure 1 . Tunes from the SLP corpus with structural, contour, and implied harmony annotations. (Top) The Death of\nthe Bride Before Her Wedding , 9th variant of tune type 239(A), transcribed by GNI in 1960. The ﬁrst phrase (F), labeled\nA has a convex ( ↗↘ ) contour, whereas the second phrase has a horizontal-descending ( →↘ ) contour as the ﬁrst pitch\nof B phrase is about at the average compared to consecutive pitches of the phrase. Starting (upbeat) and ending implied\nharmonies (H S, HE) can be clearly labeled as a tonic, even though the strong beat on the ﬁrst full measure is a D pitch.\n(Bottom) The Widower at His Wife’s Grave , 44th variant of tune type 252, transcribed by Franc Kramar in 1913. Phrases\nA and B are approximately repeated as phrases A’ and B’, with changes in melody, contours, and harmonic functions.\nHarmonically, the ﬁrst phrase starts on a clear dominant but is somewhat ambivalent at the end. The second phrase is the\nmost stable on a tonic. The rest is slightly more ambivalent between the two degrees again, while, at the very end, the tune\nconcludes on a tonic. (Right) A few descriptors that are associated with these tunes (see Table 1).\non pauses and punctuation marks, yielding 1502 phrases\n(median of 4 per tune, min. 2, max. 8).\nMusical descriptors , listed in Table 1, describe either\nthe full tune or phrases. The descriptors can carry both,\nnon-musical andmusical information.\nTune metadata4relies on transcribers’ information, and\nthe format aligns with the original sources, with some con-\nversions made for data analysis convenience [11]. Tune\nphrases were annotated with descriptors across the follow-\ning categories:\n•Phrase position. This central annotation category es-\ntablishes the relationship between other descriptors.\nEach phrase is annotated with a sequence number and\nposition, such as ﬁrst, middle, or last.\n•Structure. Each phrase is assigned a label that de-\nscribes the repetition of its melodic material within the\nverse. The ﬁrst label is always A, followed by A, A’,\nA+ (similar to B), A(X) (refrain-like A), or B. The\nsame alphabetical progression is applied to subsequent\nphrases. The tunes have an average of 2.82 different la-\nbels (1 to 6 with symbols or 4 with letters only) and in-\nfrequent repetitions (Table 2). Each label appears only\n1.34 times on average per tune.\n•Implied Harmony. Using Western harmony to describe\nfolk songs may be biased and controversial, but it is\nlikely that Slovenian folk tunes and their transcribers\nhave been exposed to the Western music system to\nsome extent [47]. Approximate functions of tonic\n(T) or dominant (D) were annotated for about 60% of\n4Metadata was collected for all 650 songs, including polyphonic com-\npositions.phrase beginnings and 50% of endings, with ambigu-\nous cases marked as “?”. To evaluate the validity of\nindividual annotations, the inclusion of scale informa-\ntion (the count of distinct pitch classes) is provided.\n•Contour . Diverging from only comparing the un-\nreliable note-to-note melodic representation of oral\nmusic tradition, we use Huron’s 9 types of melodic\narches [13] (the most frequent being the convex con-\ntour↗↘ ), where the starting and ending MIDI pitch\nvalue is compared against an average value of all inter-\nmediate MIDI pitch values.\n3. MATCHING MELODY WITH DESCRIPTORS\n3.1 Pitch and Descriptor Representation\nEach tune is subdivided into phrases aspitch sequences\nwith descriptors , which are considered as a set of n\nphrasesP={p1,p2,...,pn}, andmphrase descriptors\n∆1,...,∆m. Each descriptor ∆thas a ﬁnite set of values\nV(∆t). A phrase piis associated with a descriptor se-\nquencedi= (d1\ni,d2\ni,...,dm\ni), where each dt\niis inV(∆t).\nFor example, the following options:\n\n\n{∆1,...,∆5}={POS,LBL,CT,HS,HE}\nV(POS) ={F,L,M}V(HE) ={T,D,?}\nV(LBL) ={A,B}V(HE) ={T,D,?}\nV(CT) ={↗↘,→↘,↗,...}\ncan describe phrase 239.9.A.1 (Figure 1) as:\np239.9.A.1=gbddgdcbag d239.9.A.1= (F,A,↗↘,T,T).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n476Non-musical Metadata\ntune ID ▷ Type, variant, other\ntype title ▷ Title or label\nregion ▷,⋆Region\nannotator ▷ Name of the initial collector/transcriber\nyear ▷,⋆Year2of initial annotation.\nsinger ▷,⋆Singer sex and ensemble size\nlyric ▷,⋆First line3, ﬁrst verse, structure\nMusical Descriptors\nPOS ⋆ Phrase position ( First,Middle, Last)\nNUM ⋆ Phrase number (1, 2, 3, 4, ...)\nLBL ⋆ Phrase label (A, B, C, ...)\nSYM ⋆ Phrase symbol (A ’, A”, A+, ...)\nCT_SPEC ⋄ Huron’s contours ( ↗,↘,↗↘ ,↘↗ ,\n→,↗→ ,↘→ ,→↗ ,→↘ )\nHS ⋆ Starting harmony (T, D, ?, ...)\nHE ⋆ Ending harmony (T, D, ?, ...)\nTS ⋄ Time signature (simple duple/triple ...)\nSCALE ⋄ Scale (8, 7, 6, ...)\nMusical Content\nMEL Melodic sequence (example: gbddg )\nTable 1 . Metadata, musical descriptors, and musical con-\ntent. Manual annotations were done for phrase boundaries\nand descriptors marked with ⋆, while computed descrip-\ntors are marked with ⋄. Descriptors marked with ▷were\ncollected by the initial transcribers and/or the GNI. Other\ndescriptors, such as general contour, tone set, and leading\ntone, are also present in the dataset but not discussed here.\n3.2 Melody and Descriptor Pattern Matching\nThe goal of melody-and-descriptor matching is to ﬁnd\nall phrases (associated with their descriptors) matching in\nboth the given pitch pattern and selected variation of de-\nscriptor pattern .\nApitch pattern ppis also a sequences of pitches. It\nmatches a phrase piwhenppis a factor of pi, matching\nnote-to-note. This deﬁnition currently permits no kind of\ndeviation. For example, pp=dcbmatchesp239.9.A.1=\ngbddgdcb ag.\nAdescriptor pattern isdp= (dp1,...,dpm)∈\n(V(∆1)∪ {⋆} × ··· × V(∆m)∪ {⋆}), where⋆is a\n“don’t-care” symbol. It determines which descriptors are\nNP instances\n2 20% (78) AB (60) AA (18)\n3 6% (23) ABC (11) ABB (6)\n4 65% (261) ABCD (112) ABAB (45)\n5 2% (8) – –\n6 6% (24) ABCDCD (7) AABABA (5)\n8 1% (6) ABCBCBCB (2) –\nTable 2 . Out of 400 tunes, sorted according to the number\nof phrases (NP), the most common structure “ABCD” is\npresent in 28% of all tunes. Label variants are ignored (A’\nis considered as A). There are no tunes with 7 or more than\n8 phrases. Unique structures (–) are not reported.to be checked, and matches a descriptor sequence di=\n(d1\ni,...,dm\ni)if, for every t= 1...m , eitherdpt=⋆\nordpt=dt\ni. For instance, the descriptor pattern dp=\n(⋆,A,↗↘,⋆,⋆)checks if the phrase label matches A,\nand contour matches ↗↘ , but ignores the phrase position\nand harmonic functions. Thus dpmatchesd239.9.A.1=\n(F,A,↗↘,T,T)but does not match (F,B,↗↘,T,D).\n3.3 Algorithm\nFor this matching problem, we ﬁrst retrieve phrases and\npositions from a sufﬁx array, in linear time, then ﬁlter these\nmatches with bit-wise operators.\nPitch sequence matching with sufﬁx array. Pitch se-\nquences of Pare concatenated to one sequence, separated\nby a symbol, such as SP=p1$p2$...pn$. An index data\nstructure such as a compressed sufﬁx array is computed\nand stored to retrieve all occurrences of a pitch sequence.\nWhen a query is matched at position kinSP, the corre-\nsponding phrase piand its position in piis retrieved using\na (pre-computed) bit-vector Sp, and functions rank1(x,k)\nandselect1(x,k)(respect. the number of occurrences of 1\nin the preﬁx of length kof a bit-vector x, and the index\nof thek-th1inx). The bit-vector Sp=b1...b|Sp|is\ndeﬁned as bi=1ifSp= $, otherwise bi=0. Hence,\nthe query occurs in phrase piat position j(withinpi) with\ni=rank1(Sp,k), andj=k−select1(Sp,i). Retrieving\nthe list of phrases and positions of a query, pitch sequence\nqof length mis done in time O(m+occ), whereoccis the\nnumber of occurrences of qinSpprovided that rank and\nselect operations are performed in constant time.\nDescriptor pattern matching with bitwise operators.\nEach descriptor ∆tcan be represented by btbits, with\nbt=⌈log2|V(∆t)|⌉, and each value v∈V(∆t)is\nassociated to a bit-vector v. Each descriptor sequence\ndi= (d1\ni,d2\ni,...,dm\ni)is then stored as a bit-vector di=\nd1\ni...dm\ni. A descriptor pattern dp= (dp1,dp2,...,dpm)\nis associated to two bit-vector masks µ(dp) =µ1...µm\nandπ(dp) =π1...πm, where\n/braceleftbiggµt=πt=0...0(btbits) ifdpt=⋆\nµt=1...1andπt=dptotherwise.\nThen, a descriptor pattern dpmatches one descriptor dif\nand only if (dxorπ(dp))andµ(dp) = 0 . For example,\nifdp= (⋆,A,↗↘,⋆,⋆), then\nd239.9.1.A=00·0·0010·01·01\nµ(dp) =00·1·1111·00·00\nπ(dp) =00·0·0010·00·00\nd239.9.1.Axorπ(dp) =00·0·0000·01·01\nd239.9.1.Axorπ(dp)andµ(dp) =00·0·0000·00·00\nChecking whether a descriptor dpmatches a descrip-\ntordis done in O(1)time, provided that the bit-vectors ﬁt\nin one machine word.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n477↗↘ ↘ ↗ ↘↗ ↗→ ↘→\nFirst 24% 16% 30% 15% 6% 1.2%\nMiddle 36% 21% 15% 8% 10% 1.3%\nLast 44% 32% 2% 5% 4% 11%\nTotal 35% 23% 16% 9% 7% 1.5%\nTable 3 . We show the most frequent Huron’s contour types\naccording to phrase position. Regarding general contour\n(data not shown), ﬁrst phrases are mostly ascending (53%),\nwhile middle and last phrases are predominantly descend-\ning (63% and 89% respectively).\n4. IMPLEMENTATION AND A V AILIBILITY\nThe descriptors and the algorithm were implemented in\nPython using music21 [48] for music data manipula-\ntion,bitarray library for descriptors matching and\na C++ library sdsl-lite [49] (used in Python with\npysdsl library) for melodic matching. From the latter,\nwe usedrank andselect methods, and BitVector\nandSuffixArrayBitcompressed classes. On a\nstandard laptop from 2022, building sufﬁx arrays and bit\nvectors on 1502 phrases takes less than 0.5 s. A single\nsufﬁx array takes 49.4 KB, and bit vectors 1.7 KB. The\nlongest melody/descriptor queries take about 100 ms. The\nannotations and the code are available on a git repository\nthrough open licences (Open Database License, Database\nContents License, GPLv3+) at algomus.fr/data and\nalgomus.fr/code . We are collaborating with partners\nin Slovenia to prepare the release of 400 melodies and ad-\nditional ethnomusicological research.\n5. RESULTS AND DISCUSSION\nAlmost all tunes (84%) in our collection revolve around a\nminimum of six distinct pitch classes, indicating the likely\nutilization of Western major/minor scales and modes. Ap-\nproximately 67% of the tunes fall within the range of a\nmajor sixth (M6) to an octave (P8). Additional statistics\nin the annotated corpus examine phrase positions (Sec-\ntion 5.1). Speciﬁcally, the analysis focuses on the preva-\nlent tune subtype 286.T1, annotated for melodic similarity.\nFindings from combined melody and descriptor queries are\npresented in Section 5.2.\n5.1 Descriptors and Phrase Positions\nThe 400 tunes consist of 1502 phrases, split into ﬁrst (400),\nmiddle (702), and last (400) positions. The labels, con-\ntours, and implied harmonies are strongly inﬂuenced by\nthe phrase positions, as evident from Tables 2, 3, and 4. In\ngeneral, phrases are convex or descending (see Table 3),\nwhile the ﬁrst phrases are mostly ascending or convex,\nand less harmonically stable at their ends than beginnings.\nConversely, the last phrase is almost never ascending, and\nmore harmonically stable at its ends than beginnings. The\nmiddle phrase group is more divided and is more unstable.T D ? T ?D ?\nFirst H S 25% 54% 9% <1% 12%\nHE 22% 27% 15% 7% 29%\nMiddle H S 16% 36% 14% 6% 28%\nHE 21% 16% 18% 6% 40%\nEnd H S 19% 32% 10% 5% 34%\nHE 60% <1% 20% None 19%\nTotal H S 19% 40% 11% 5% 25%\nHE 32% 15% 18% 5% 32%\nTable 4 . Starting (H S) and ending (H E) harmonic func-\ntions in relation to phrase positions demonstrate a consis-\ntent pattern. Phrases typically initiate on a dominant (D)\nand conclude on a tonic (T). However, there is ambiguity\nwith the functions ?, ? T, and ?D, as they can be interpreted\nas either T or D, which arises from the inﬂuence of previ-\nous and following pitch values or bars, making the exact\nannotation spot unclear.\nFigure 2 . Two variants (out of 34) of the subtype\n286.T1 with similar melodies with short melodic patterns\nin coloured squares.\nWe assume that the contrasting beginnings and endings\nof each verse offer pitch orientation for the singers, given\nthe repetitive structure of narrative songs. The contour re-\nlationship between the ﬁrst, middle, and last phrases sup-\nports the notion that “what goes up is likely to come down,”\nas proposed by Huron [13].\n5.2 Case Study: Subtype 286.T1, Infanticide Bride\nOur dataset includes 103 monophonic variants of the\nwidely known “Infanticide Bride” theme in European folk\nsong tradition. Subtype 286.T1 consists of 34 tunes se-\nlected for their melodic similarity (Figure 2), which often\nexhibit similar patterns, such as the fadas a start middle\nphrase pattern or the bagas a last phrase ending pattern.\nWe have developed combined melody/descriptor\nqueries to represent certain phrases of subtype 286.T1.\nThese queries are evaluated as a binary classiﬁcation\nproblem: Can we accurately identify the 34 initial phrases\nof 286.T1 and distinguish them exclusively from others?\nPattern design and matching. Table 5 demonstrates that\nsimple melody queries with 1 to 3 notes achieve reasonable\nrecall rates (50%-80%) but limited precision. Reﬁning the\nqueries with descriptors improves precision and relevance,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n478leading to enhanced F1measures. The ddbmelody query\nalone produces 93 matches, but 75 of them are “false pos-\nitives” unrelated to the ﬁrst phrase of 286.T1 tunes. Incor-\nporating a phrase position descriptor (F, ﬁrst) improves the\nquery, while adding relevant contour ( ↗) and starting har-\nmonic information further enhances speciﬁcity. This com-\nprehensive query results in only 2 false positives, achiev-\ning a precision of up to 0.88, with minimal sensitivity loss.\nTheagpattern in the last phrase, characterized by a convex\ncontour and a harmonic ending, is a noteworthy example.\nGiven the enhanced harmonic stability typically found in\nverse endings, the inclusion of the H ST as a stable har-\nmony descriptor proves to be effective in this context. In-\ncluding too many or irrelevant descriptors leads to poor re-\nsults. For instance, the cbbpattern is primarily found at the\nend of the middle phrase. However, requiring a stable har-\nmonic framework (H ET) for middle endings reduces preci-\nsion, as it is less common in those positions (Table 4). An-\nother interesting instance is the fad, occurrences of which\nare almost evenly split into two contours. If we matched\n(fad, M,↗↘ or↗→ ), we would get 23 true positives and\na precision of 0.82 with a recall of also 0.82. The algorithm\nshould be extended to accommodate the matching of a sub-\nset of multiple descriptors within the same category, rather\nthan solely relying on one descriptor.\nPatterns as building blocks. Melody/descriptor patterns\nhave versatile applications beyond classiﬁcation. In our\ncase, the most effective queries incorporate position de-\nscriptors, indicating that we should view phrase building\nblocks as patterns. It is noteworthy, that studying the “false\npositives” (matches outside of 286.T1) is expected to yield\nintriguing results, shedding light on the transmission of\nmusic material among tunes and vice versa. For instance,\ntheagpattern in the last phrase, exhibiting a ↗↘ con-\ntour and ending with H ET, is not only speciﬁc to 286.T1\nbut also appears in 14 tunes of type 252 ( A Widower at His\nWife’s Grave ). The shared section of the melodic line in the\ntwo tunes has identical descriptors, although its positions\nmay vary (scores not shown). Comparing outcomes across\nmultiple corpora would provide insight into the unique mu-\nsical characteristics of (Slovenian) folk songs.\n5.3 Discussion\nOur study explored tune structures and melodic patterns,\nﬁnding that combining melodic content with descriptors\nprovides valuable insights into the characteristics of Slove-\nnian folk songs. However, not all descriptors ﬁt univer-\nsally to describe all content, and vice versa. In contrast\nto the usual transferability of folk song melodies, our case\nstudy indicates that the melody of 286.T1 was not easily\ntransferable, possibly due to its popularity in multiple re-\ngions. Further inter- and intra-corpus research is needed\nto investigate this distinctive characteristic. We also show,\nthat individual melodic extracts alone lack the speciﬁcity\nrequired for a comprehensive description of a full phrase\nor tune. Strong correlations were found for descriptorsQuery (melody + descriptors) TP FP FN Prec. Rec. F 1\nd None 1 (34) 28 1191 6 0.02 0.82 0.04\nd F 28 327 6 0.08 0.82 0.14\nd F, HSD 27 189 7 0.12 0.79 0.22\nd F,↗, HSD 21 48 13 0.30 0.62 0.41\nddb None 1 (34) 18 75 16 0.19 0.53 0.28\nddb F 18 32 16 0.36 0.53 0.43\nddb F, HSD 17 21 17 0.45 0.50 0.47\nddb F,↗, HSD 14 2 20 0.88 0.41 0.56\nfad None 3 (33) 24 24 9 0.50 0.73 0.59\nfad M 24 14 9 0.63 0.73 0.68\nfad M,↗↘ 11 2 22 0.85 0.33 0.48\nfad M,↗→ 12 3 21 0.80 0.36 0.50\ncbb None 3 (33) 25 71 8 0.26 0.76 0.39\ncbb M 25 39 8 0.39 0.76 0.52\ncbb M,↗→ 11 3 22 0.79 0.33 0.47\ncbb M, HET 1 3 32 0.25 0.03 0.05\nag None 4 (34) 27 481 7 0.05 0.79 0.10\nag L 27 165 7 0.14 0.79 0.24\nag L,↗↘ 23 54 11 0.30 0.68 0.41\nag L,↗↘ , HET 23 51 11 0.31 0.68 0.43\nTable 5 . Evaluation of melody/descriptor queries seen as\nclassiﬁcation queries intended to match phrases 1, 3, and 4\nof the melodic tune subtype 286.1 (34 ﬁrst and last phrases,\n33 third phrases) against all 1502 phrases of the dataset.\nWe computed True Positives (TP), False Positives (FP),\nFalse Negatives (FN), and from those, precision, recall,\nandF1-score. Bold values are discussed in the text.\nlike contour. Expanding the dataset is needed to com-\nprehensively explore the relationship between lyrics and\nmelodies, including the observed descending shape in the\nlast phrase, potentially reﬂecting or corresponding with\nspeech characteristics [50, 51].\n6. CONCLUSION AND PERSPECTIVES\nBy integrating descriptor information into melodies, we\ngain a deeper understanding of the observed music. Our\nﬁndings indicate a strong dependency of many descrip-\ntors on phrase positions, and that combining melody and\ndescriptors enhances precision compared to using melody\nalone. Our algorithm efﬁciently matches melodies and de-\nscriptors, which can be extended beyond our proposed se-\nlection. Lastly, we released annotations of Slovenian folk\nsongs, a yet underrepresented corpus in the MIR commu-\nnity.\nOur current plans primarily involve releasing the corpus\nof these tunes, accompanied by comprehensive ethnomu-\nsicological commentary. In addition, future work should\nprioritize improving the algorithm’s usability for non-\ncomputational users, expanding the existing annotations\nof descriptors, and implementing the capability to perform\ncombined query searches with approximate matching for\nmelodies and descriptors. Our study (and corpus) may be\nused as supporting data for new algorithms of phrase seg-\nmentation, tune structure analysis, and harmony tasks in-\ncluding semi-automatic annotation.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n479Acknowledgements. We extend our gratitude to Matija\nMarolt, Matevž Pesek, and current as well as past mem-\nbers of the Institute of Ethnomusicology ZRC SAZU for\ntheir pivotal role in digitizing and curating the corpus from\nﬁeld recordings, notations and notes. We also appreciate\nthe valuable input and comments provided by Dinh-Viet-\nToan Le and the Algomus team, Patrick E. Savage and the\nmembers of Comp Music Lab, and the anonymous review-\ners, which greatly contributed to the development of this\npaper.\n7. REFERENCES\n[1] A. P. Merriam, The Anthropology of Music . North-\nwestern University Press, 1964.\n[2] K. Blaukopf, Musical Life in a Changing Society: As-\npects of Music Sociology . Hal Leonard Corporation,\n1992.\n[3] I. Mills, “The heart of the folk song,” Canadian Folk\nMusic Journal , vol. 2, pp. 29–34, 1974.\n[4] S. P. Bayard, “Prolegomena to a study of the principal\nmelodic families of British-American folk song,” The\nJournal of American Folklore , vol. 63, no. 247, pp. 1–\n44, 1950.\n[5] Z. Kumer, Vloga, zgradba, slog slovenske ljudske\npesmi . Založba ZRC, 1996.\n[6] P. van Kranenburg, A. V olk, and F. Wiering, “On op-\nerationalizing the musicological concept of tune fam-\nily for computational modeling,” Proceedings of Sup-\nporting Digital Humanities: Answering the unaskable ,\n2011.\n[7] T. Rice, Modeling Ethnomusicology . Oxford Univer-\nsity Press, 2017.\n[8] J. R. Cowdery, “A fresh look at the concept of tune\nfamily,” Ethnomusicology , vol. 28, no. 3, pp. 495–504,\n1984.\n[9] C. Pendlebury, “Tune families and tune histories:\nMelodic resemblances in British and Irish folk tunes,”\nFolk Music Journal , vol. 11, no. 5, pp. 67–158, 2020.\n[10] A. V olk and P. van Kranenburg, “Melodic similarity\namong folk songs: An annotation study on similarity-\nbased categorization in music,” Musicae Scientiae ,\nvol. 16, no. 3, pp. 317–339, 2012.\n[11] M. G. Kau ˇciˇc, M. Klob ˇcar, Z. Kumer, U. Šivic, and\nM. Terseglav, Slovenske ljudske pesmi V .: Pripovedni\npesmi . Založba ZRC, 2007, vol. 5.\n[12] S. Ahlbäck, “Melody beyond notes: A study of melody\ncognition,” Ph.D. dissertation, Göteborgs Universitet,\n2004.[13] D. Huron et al. , “The melodic arch in Western folk-\nsongs,” Computing in Musicology , vol. 10, pp. 3–23,\n1996.\n[14] S. d. l. Ossa, A Basic Guide to Folksong Analysis . Bu-\ndapest: Liszt Academy of Music, 2019.\n[15] A. Lomax, Folk Song Style and Culture . Routledge,\n2017.\n[16] A. Lomax and N. Berkowitz, “The evolutionary taxon-\nomy of culture: A few behavioral factors account for\nthe regional variation and evolutionary development\nof culture,” Science , vol. 177, no. 4045, pp. 228–239,\n1972.\n[17] A. Wood, K. R. Kirby, C. Ember, S. Silbert,\nS. Passmore, H. Daikoku, J. Mcbride, F. Paulay,\nM. Flory, J. Szinger et al. , “The Global Jukebox:\nA public database of performing arts and culture,”\nPLOS One , vol. 17, no. 11, 2021. [Online]. Available:\nhttps://doi.org/10.1371/journal.pone.0275469\n[18] A. L. Wood, Songs of Earth: Aesthetic and Social\nCodes in Music . University Press of Mississippi,\n2021.\n[19] P. E. Savage, “Alan Lomax’s cantometrics project: A\ncomprehensive review,” Music & Science , vol. 1, 2018.\n[20] X. Serra, “The computational study of a musical cul-\nture through its digital traces,” Acta Musicologica ,\nvol. 89, no. 1, pp. 24–44, 2017.\n[21] K. Neubarth, I. Goienetxea, C. G. Johnson, and\nD. Conklin, “Association mining of folk music genres\nand toponyms,” International Society for Music Infor-\nmation Retrieval Conference (ISMIR 2012) , pp. 7–12,\n2012.\n[22] K. Neubarth and D. Conklin, “Contrast pattern mining\nin folk music analysis,” in Computational Music Anal-\nysis. Springer, 2016, pp. 393–424.\n[23] D. Conklin, “Antipattern discovery in folk tunes,” Jour-\nnal of New Music Research , vol. 42, no. 2, pp. 161–\n169, 2013.\n[24] P. van Kranenburg, “Computational approach to\ncontent-based retrieval of folk song melodies,” Ph.D.\ndissertation, Utrecht University, 2010.\n[25] P. van Kranenburg and F. Karsdorp, “Cadence detec-\ntion in western traditional stanzaic songs using melodic\nand textual features,” in International Society for Mu-\nsic Information Retrieval Conference (ISMIR 2014) ,\n2014, pp. 391–396.\n[26] C. McKay, J. Cumming, and I. Fujinaga, “jSymbolic\n2.2: Extracting features from symbolic music for use in\nmusicological and MIR research,” in International So-\nciety for Music Information Retrieval Conference (IS-\nMIR 2018) , 2018, pp. 348–354.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n480[27] D. Conklin and M. Bergeron, “Discovery of contrapun-\ntal patterns,” in International Society for Music Infor-\nmation Retrieval Conference (ISMIR 2010) , vol. 2010,\n2010, pp. 201–206.\n[28] I. Y . Ren, H. V . Koops, A. V olk, and W. Swierstra, “In\nsearch of the consensus among musical pattern discov-\nery algorithms,” in International Society for Music In-\nformation Retrieval Conference (ISMIR 2017) , 2017,\npp. 671–678.\n[29] E. Cambouropoulos, “Musical rhythm: A formal\nmodel for determining local boundaries, accents and\nmetre in a melodic surface,” in Joint International\nConference on Cognitive and Systematic Musicology ,\n1996, pp. 277–293.\n[30] O. Lartillot, “Automated motivic analysis: An exhaus-\ntive approach based on closed and cyclic pattern min-\ning in multidimensional parametric spaces,” in Compu-\ntational Music Analysis , 2016, pp. 273–302.\n[31] M. Mongeau and D. Sankoff, “Comparison of musical\nsequences,” Computers and the Humanities , vol. 24,\nno. 3, pp. 161–175, 1990.\n[32] P. E. Savage, “Measuring the cultural evolution of mu-\nsic: Cross-cultural and cross-genre case studies,” in\nPsyArXiv , 2020 (preprint).\n[33] P. E. Savage, S. Passmore, G. Chiba, T. E. Currie,\nH. Suzuki, and Q. D. Atkinson, “Sequence alignment\nof folk song melodies reveals cross-cultural regularities\nof musical evolution,” Current Biology , vol. 32, no. 6,\npp. 1395–1402, 2022.\n[34] T. Eerola and M. Bregman, “Melodic and contextual\nsimilarity of folk song phrases,” Musicae Scientiae ,\nvol. 11, no. 1, pp. 211–233, 2007.\n[35] C. Anagnostopoulou, M. Giraud, and N. Poulakis,\n“Melodic contour representations in the analysis of\nchildren’s songs,” in International Workshop on Folk\nMusic Analysis (FMA 2013) , 2013, pp. 40–43.\n[36] J. McBride, A. T. Tierney, P. Pfordresher, J. Six,\nS. Fuji, and P. E. Savage, “Are pitches discrete? an\ninformation-theoretic framework and a corpus study,”\ninInternational Conference on Music Perception and\nCognition and Triennial Conference of the European\nSociety for the Cognitive Sciences of Music (ICMPC-\nESCOM 2021) , 2021.\n[37] C. Finkensiep, K. Déguernel, M. Neuwirth, and\nM. Rohrmeier, “V oice-leading schema recognition us-\ning rhythm and pitch features,” in International Soci-\nety for Music Information Retrieval Conference (IS-\nMIR 2020) , 2020, pp. 520–526.\n[38] E. Cambouropoulos, “The harmonic musical surface\nand two novel chord representation schemes,” in Com-\nputational Music Analysis . Springer, 2016, pp. 31–56.[39] A. Marsden, “Representing melodic patterns as net-\nworks of elaborations,” Computers and the Humani-\nties, vol. 35, no. 1, pp. 37–54, 2001.\n[40] H. Schaffrath, “The Essen Associative Code: A code\nfor folksong analysis,” in Beyond MIDI: The handbook\nof musical codes . MIT Press, 1997, p. 343–361.\n[41] T. Eerola and P. Toiviainen, “Suomen kansan es-\nävelmät – ﬁnnish folk song database,” 1999. [Online].\nAvailable: http://esavelmat.jyu.ﬁ/\n[42] P. van Kranenburg, M. De Bruin, and A. V olk, “Doc-\numenting a song culture: the Dutch song database as\na resource for musicological research,” International\nJournal on Digital Libraries , vol. 20, no. 1, pp. 13–23,\n2019.\n[43] Z. Kumer, M. Mati ˇcetov, B. Merhar, and V . V odušek,\nSlovenske ljudske pesmi I: Pripovedni pesmi . Ljubl-\njana: CGP Delo, 1970, vol. 1.\n[44] Z. Kumer, M. Mati ˇcetov, and V . V odušek, Slovenske\nljudske pesmi II: Pripovedni pesmi . Ljubljana:\nSlovenska matica, 1981, vol. 2.\n[45] M. Terseglav, I. Cvetko, M. Golež, and J. Strajnar,\nSlovenske ljudske pesmi III: Pripovedni pesmi . Ljubl-\njana: Slovenska matica, 1992, vol. 3.\n[46] ——, Slovenske ljudske pesmi IV: Pripovedni pesmi .\nLjubljana: Slovenska matica, 1992, vol. 4.\n[47] V . V odušek, Etnomuzikološki ˇ clanki in razprave . Za-\nložba ZRC, 2003.\n[48] M. S. Cuthbert and C. Ariza, “music21: A toolkit for\ncomputer-aided musicology and symbolic music data,”\nInternational Society for Music Information Retrieval\nConference (ISMIR 2010) , pp. 637–642, 2010.\n[49] S. Gog, T. Beller, A. Moffat, and M. Petri, “From the-\nory to practice: Plug and play with succinct data struc-\ntures,” in International Symposium of Experimental Al-\ngorithms (SEA 2014) , 2014, pp. 326–337.\n[50] Y . Ozaki, A. Tierney, P. Pfordresher, J. Mcbride,\nE. Benetos, P. Proutskova, G. Chiba, F. Liu, N. Jacoby,\nS. Purdy et al. , “Similarities and differences in a global\nsample of song and speech recordings [stage 2 regis-\ntered report],” PsyArXiv , 2022 (preprint).\n[51] P. Albouy, S. A. Mehr, R. S. Hoyer, J. Ginzburg,\nand R. J. Zatorre, “Spectro-temporal acoustical mark-\ners differentiate speech from song across cultures,”\nbioRxiv , 2023 (preprint).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n481"
    },
    {
        "title": "A Repetition-Based Triplet Mining Approach for Music Segmentation.",
        "author": [
            "Morgan Buisson",
            "Brian McFee",
            "Slim Essid",
            "Hélène C. Crayencour"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265313",
        "url": "https://doi.org/10.5281/zenodo.10265313",
        "ee": "https://zenodo.org/records/10265313/files/000049.pdf",
        "abstract": "Contrastive learning has recently appeared as a well-suited method to find representations of music audio signals that are suitable for structural segmentation. However, most existing unsupervised training strategies omit the notion of repetition and therefore fail at encompassing this essential aspect of music structure. This work introduces a triplet mining method which explicitly considers repeating sequences occurring inside a music track by leveraging common audio descriptors. We study its impact on the learned representations through downstream music segmentation. Because musical repetitions can be of different natures, we give further insight on the role of the audio descriptors employed at the triplet mining stage as well as the trade-off existing between the quality of the triplets mined and the quantity of unlabelled data used for training. We observe that our method requires less non-annotated data while remaining competitive against other unsupervised methods trained on a larger corpus.",
        "zenodo_id": 10265313,
        "dblp_key": "conf/ismir/BuissonMEC23",
        "keywords": [
            "Contrastive learning",
            "structural segmentation",
            "repetition",
            "unsupervised training",
            "triplet mining",
            "audio descriptors",
            "music segmentation",
            "trade-off",
            "quality",
            "quantity"
        ],
        "content": "A REPETITION-BASED TRIPLET MINING APPROACH FOR MUSIC\nSEGMENTATION\nMorgan Buisson1Brian McFee2,3Slim Essid1Hélène C. Crayencour4\n1LTCI, Télécom Paris, Institut Polytechnique de Paris, France\n2Music and Audio Research Laboratory, New York University, USA\n3Center of Data Science, New York University, USA\n4L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France\nABSTRACT\nContrastive learning has recently appeared as a well-suited\nmethod to ﬁnd representations of music audio signals that\nare suitable for structural segmentation. However, most\nexisting unsupervised training strategies omit the notion of\nrepetition and therefore fail at encompassing this essential\naspect of music structure. This work introduces a triplet\nmining method which explicitly considers repeating se-\nquences occurring inside a music track by leveraging com-\nmon audio descriptors. We study its impact on the learned\nrepresentations through downstream music segmentation.\nBecause musical repetitions can be of different natures, we\ngive further insight on the role of the audio descriptors em-\nployed at the triplet mining stage as well as the trade-off\nexisting between the quality of the triplets mined and the\nquantity of unlabelled data used for training. We observe\nthat our method requires less non-annotated data while re-\nmaining competitive against other unsupervised methods\ntrained on a larger corpus.\n1. INTRODUCTION\nThe task of music structure analysis consists in locating\nthe boundaries between consecutive segments and group-\ning them into relevant categories, called musical sections.\nThis problem has gained attention in the ﬁeld of music in-\nformation retrieval and has numerous applications, such\nas music generation [1, 2], music recommendation [3] or\nmusic similarity estimation [4]. Structure is also strongly\nlinked to other musical elements such as harmony, melody\nand rhythm [5] and has been leveraged to address other\ntasks such as beat and downbeat tracking [6] or chord tran-\nscription [7].\nMost methods that have been proposed for the task of\nmusic structure analysis can be categorized according to\nthe structure trait they rely on, namely: homogeneity ,nov-\n© M. Buisson, B. McFee, S. Essid and H. C. Crayencour.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: M. Buisson, B. McFee, S. Essid and\nH. C. Crayencour, “A repetition-based triplet mining approach for music\nsegmentation”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.eltyandrepetition [8]. The homogeneity rule states that\nmusical attributes should be relatively homogeneous inside\nmusical segments or sections. Consequently, transitions\nfrom one segment to the next should result in points of im-\nportant changes in musical features ( i.e.novelty). The idea\nof repetition in structure assumes that sections of the same\ntype are rather similar sequences. In other words, musical\nsections are generally characterized by the degree at which\nthey repeat throughout the entire music piece, which has\nbeen the starting point of many algorithms to infer song\nstructures [9–11]. However, both the extent to which two\nsequences can be considered as repetitions, or how homo-\ngeneous a given musical segment is, imply a certain def-\ninition of similarity between time instants. Such similar-\nity criteria are usually derived from frame representations\nbased on common audio descriptors such as harmonic and\ntimbral features, or their combinations [8].\nA line of work has focused on ﬁnding better-suited au-\ndio representations so as to make sure that frames from the\nsame musical sections yield similar features and therefore,\nsharpen transitions between consecutive musical segments.\nMethods based on contrastive learning have recently been\nproposed to ﬁnd such representations [12–15], as they can\nleverage commonalities from large quantities of music data\nto learn a distance metric that complies with the aforemen-\ntioned requirements. Training such models either involves\nthe use of structural annotations [13] or some pre-deﬁned\nproxies to select frames that should be brought close to one\nanother in the latent space [12,15]. In the latter case, these\nheuristics mainly rely on the homogeneity principle and\ndiscard the notion of repetition occurring inside a track,\npreventing them from fully exploiting unlabelled data.\nThe method introduced in this work aims at bridging\nthe gap between current unsupervised deep metric learning\nmethods for music segmentation and both ideas of homo-\ngeneity and repetition that are inherent to musical struc-\nture. As in previous work [12, 15], a contrastive learning\npipeline using a triplet loss is adopted. However, triplets\nare mined by seeking repeating sequences inside the input\ntrack with respect to various hand-crafted audio features.\nIn a preliminary analysis, a qualitative evaluation of the\ntriplets generated is performed by direct comparison with\nstructural annotations of a manually annotated test dataset.\nWe then measure how these representations impact down-417stream segmentation on two datasets for music structure\nanalysis. Finally, we demonstrate that our approach re-\nquires less non-annotated data than previous similar meth-\nods. We also give further insight on how the choice of the\ninput features used to mine triplets affects training and its\nrelationship with the music genre that the resulting repre-\nsentations are tested on.\n2. RELATED WORK\nNumerous methods for music structure analysis rely on\nmeasuring similarity between every point of a music\nrecording to retrieve homogeneous segments and tran-\nsitions between them. Since music is naturally multi-\ndimensional, many factors such as harmony, timbre or in-\nstrumentation can be associated with boundaries between\nmusical sections [16]. Therefore, several strategies have\nbeen adopted to capture short-term similar regions, and it\nhas been shown that sharp timbre changes can be a good\ncue for section transitions [17–19].\nHowever, not all boundaries can be explained solely\nby such changes in musical features, as the perception of\nstructure is also greatly affected by additional character-\nistics of a music recording such as parallelism, pauses or\nmusical rules proper to the music genre considered [16].\nTherefore, other approaches tend to rely on the repetition\nprinciple to characterise the structure of a music piece.\nFor example, early work on music segmentation has at-\ntempted to ﬁnd audio representations to identify repeating\nelements inside music recordings, such as pitch estimation\nor polyphonic transcription [10]. Generally, repetition-\nbased methods rely on harmony-related information from\nthe audio, as the instrumentation or other factors are sub-\nject to variations between different occurrences of a given\nmusical section [18, 20].\nSeveral algorithms have also been proposed to unify\nthese two types of approaches by recognizing similar re-\ngions and repetitions of varying lengths. For example, inte-\ngrating structural information at different scales into frame\nrepresentations has led to considerable improvements in\nthe recognition of musical segments [21, 22].\nEven though these methods are theoretically well\ngrounded and have proven to be efﬁcient on commonly\nused datasets, the traditional hand-crafted descriptors they\nuse can fail at accommodating different structure types and\nmusic genres. On the other hand, deep learning-based\nmethods are able to extract efﬁcient features from large\nquantities of data, thus, surpassing traditional audio de-\nscriptors [12]. Approaches based on contrastive learning\nalso have the advantage to be easily incorporated into the\nclassical music structure analysis pipeline, by simply re-\nplacing the original input features by the deep embeddings\nthey learn from training data. To this end, Wang et al. [13]\nuse structural annotations from a labelled training dataset\nto ﬁnd positive and negative pairs of frames and a multi-\nsimilarity loss function [23]. They additionally employ a\nmining mechanism to further improve convergence of their\nmodel. Using structural annotations allows for explicitly\nenforcing frames of identical sections to yield similar fea-tures regardless of their appearance throughout the track.\nDespite not relying on annotations, the method in this work\nis similar to theirs, in the sense that it explicitly considers\nsection repetitions inside a music recording.\nA similar method proposed by McCallum [12] proceeds\nin an unsupervised manner with a triplet loss. This time,\npositive and negative frames are sampled using time prox-\nimity as a proxy: frames occurring within a small time\ninterval are more likely to belong to the same musical\nsections than those separated by a larger amount of time.\nWhile this assumption generally holds true, it completely\ndiscards the notion of repetition, which can limit the efﬁ-\ncacy of the approach. In the present work, this limitation is\naddressed by using pairwise frame similarity measures as\nprior information to guide the triplet sampling mechanism.\nThis temporal-based mining method [12] is used as a base-\nline in this work and referred to as temporal sampling .\n3. METHOD\nThe core of the triplet mining method proposed in this\nwork resides in the estimation of a self-similarity matrix,\nwhich should reﬂect as much as possible section label as-\nsignment corresponding to structural annotations. This\napproximation of ideal pairwise frame similarities should\nyield high values for frames belonging to the same musi-\ncal section, and low values otherwise. This self-similarity\nmatrix is used as a probability mass function according to\nwhich are sampled, for each given frame, positive and neg-\native examples across the whole input track.\n3.1 Triplet loss\nThe method proposed in this work consists in ﬁnding\ntriplets of audio feature patches (xa,xp,xn)wherexais\nthe anchor, xpis a positive example from the same mu-\nsical section and xnthe negative example sampled from\na different one without using structural annotations. The\nmodels are trained using the triplet loss, which for a given\ntripletT= (xa,xp,xn)is expressed as:\nL(T) = [d(f(xa),f(xp))−d(f(xa),f(xn))+δ]+,(1)\nwhered(x,y)is a pre-deﬁned distance metric, [.]+denotes\nthe Hinge loss, δ >0is the margin parameter, and f(x)\nis the projection of xinto the embedding space by a deep\nneural network.\n3.2 Finding repetitions\nThe choice of the input features from which frame-wise\nsimilarities are extracted greatly inﬂuences the ﬁnal triplet\nsampling mechanism. As the goal is to jointly detect ho-\nmogeneous regions and overall repetitions throughout the\ninput track, we employ a combination of timbral and har-\nmonic features as done in previous work [24, 25]. These\nfeatures are beat-synchronized beforehand, using the algo-\nrithm from Korzeniowski et al. [26] implemented in the\nmadmom package [27]. One way to emphasize repeti-\ntion is to encode features into time-delay embeddings, soProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n418that pairwise comparisons are performed over short time-\nwindows: given a sequence X={Xi}i∈{1,...,N}of fea-\nture vectors, the ith time embedding vector ˜Xiis obtained\nby stacking the mfeature vectors ranging from i−(m−1)\ntoi:\n˜Xi=/bracketleftBig\nXT\niXT\ni−1...XT\ni−(m−1)/bracketrightBigT\n, (2)\nwheremdenotes the embedding dimension, ruling how\nmuch of past information is considered. Such transforma-\ntions have successfully been used for music structure anal-\nysis [22], structure-based music similarity [4] and more\ngenerally in the ﬁeld of non-linear time series analysis\n[28]. The ﬁnal representation’s temporal dimension re-\nmainsN, asXis ﬁrst zero-padded before transformation.\nThen, a self-similarity matrix is built from the obtained se-\nquence of time-lag features such that:\nM(i,j) =/braceleftBigg\nexp/parenleftBig\n−d(˜Xi,˜Xj)\nb/parenrightBig\n,˜Xj∈NN k(˜Xi)\n0, ˜Xj/∈NN k(˜Xi)(3)\nwhered(x,y)is the euclidean distance, bthe bandwidth\nparameter, NN k(x)denotes the k-nearest neighbors of x\nandi,j= 1,...,N . The self-similarity matrix Mis then\nﬁltered with a sigmoid activation, such that:\nˆM(i,j) =σ/parenleftbiggM(i,j)\nmaxkM(i,k)/parenrightbigg\n, (4)\nwherei,j= 1,...,N and theσfunction deﬁned as:\nσ(x) =1\n1+e−α(x−β), (5)\nwhereα >0is a parameter ruling the steepness of the\ncurve and β∈[0,1]a threshold above which the compo-\nnents ofSare set to values close to 1. This process is ap-\nplied both using MFCC and chroma features, from which\nwe obtain their respective ﬁltered self-similarity matrices\nSMandSCusing Equation (4) (ﬁrst row of Figure 1). The\nmatrixSis then obtained by linear combination, such that:\nS=γSM+(1−γ)SC, (6)\nwhereγ∈[0,1]weights the contributions of each feature\ntype. The matrix S(second row, left column of Figure\n1) is row-wise min-max normalized and ﬁltered with the\nsigmoid function deﬁned in Equation (5), diagonal stripes\nindicating repeating sequences are enhanced by median ﬁl-\ntering similar to the one used by McFee et al. [18].\n3.3 Imposing segment homogeneity\nThe obtained pairwise similarity Sprovides information\nabout the repetitions present inside the input track. How-\never, using it as it is to mine positive (large S(a,p)) and\nnegative examples (small S(a,n)) would result in many\ntrivial triplets, as positives would be located at exact points\nof repetitions. Therefore, a dilation operation is applied\nto the matrix Sto enlarge these detected regions of repe-\ntition. Similar to the method by Serra et al. [22], a two-\ndimensional Gaussian kernel Gof sizeKis convolved\nwithS:\nSp=S∗G, (7)\nIntro\nRefrain\nVerse\nRefrain\nVerse\nBridge\nRefrain\nRefrainS\nBridge\nRefrain\nVerse\nRefrainSM\n SC\nIntro\nRefrain\nVerse\nRefrain\nVerse\nBridge\nRefrain\nRefrainS\nBridge\nRefrain\nVerse\nRefrainS\n Reference\nIntro\nRefrain\nVerse\nRefrain\nVerse\nBridge\nRefrain\nRefrainS\nBridge\nRefrain\nVerse\nRefrainSp\n Sn\nIntro\nRefrainVerseRefrainVerseBridgeRefrainRefrainSBridgeRefrainVerseRefrainIntro\nRefrain\nVerse\nRefrain\nVerse\nBridge\nRefrain\nRefrainS\nBridge\nRefrain\nVerse\nRefrainTp\nIntro\nRefrainVerseRefrainVerseBridgeRefrainRefrainSBridgeRefrainVerseRefrainTnFigure 1 . Example of the self-similarity approximation\nprocess for The Beatles — Baby’s In Black . Top to bot-\ntom, left to right: self-similarity lag-matrices obtained us-\ning MFCC ( SM), chroma features ( SC), median ﬁltered\ncombination ( S), reference self-similarity matrix (super-\nvised scenario), positive matrix ( Sp), negative matrix ( Sn),\npositive ( Tp) and negative ( Tn) sampling matrices using\ntemporal sampling [12]. White dotted lines denote bound-\nary instants.\nThis has the effect of blurring the regions of Saround its\ndiagonal stripes, which approximates the width of the cor-\nresponding musical segments in a more uniform manner\nthan directly using the unﬁltered matrix S. The size of the\nkernelKlogically impacts the extent to which this dilation\nis performed. It was found that setting K= 8(beats) pro-\nvided a good balance between the amount of dilation and\nits alignment with segment boundaries (third row, left col-\numn of Figure 1), as it blurs repetitions over 2bars when\nsongs follow a 4/4time signature1.\n1Such value might induce a bias towards speciﬁc western music gen-\nres. This parameter should ideally be adapted to each training track.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4193.4 Negative mining\nWhile the matrix Spguides the selection of positive ex-\namples for any frame of the input track, the triplet loss re-\nquires to ﬁnd a third point with a different label, called neg-\native example. In our case, such example should belong to\na different musical section, which could be easily solved by\nsearching for the least similar frames from the anchor ( i.e.\nusing the matrix Sn= 1−Spfor sampling). However, do-\ning so is likely to result in trivial triplets where the relative\ndifference between d(f(xa),f(xp))andd(f(xa),f(xn))\nfrom Equation (1) might already be larger than the margin\nδ, thus, yielding small gradients that prevent the network\nfrom learning features that are discriminative enough [29].\nInstead, we enforce negative examples to be chosen close\nto the anchor’s location while still avoiding homogeneous\nregions indicated by the positive matrix Sp. To this end,\nthe negative sampling matrix Snis obtained by applying\nan exponential decay to 1−Spsuch that:\nSn(i,j) = (1−Sp(i,j))e−λmax(|i−j|\nN,Sp(i,j)), (8)\nwhereλ >0is a parameter that deﬁnes the strength of\nthe smoothing. As a consequence, components near the\nmain diagonal of Sn(third row, right column of Figure 1)\nreceive greater values than those close the opposite edges,\nthus favoring frames located within consecutive segments\nof that of the anchor.\nThe ﬁnal sampling process works as follows: given an\nanchor point iachosen among the Nframes of the in-\nput track, the weight attributed to a certain index ikwhen\nsampling the positive example follows the discrete prob-\nability distribution deﬁned by the a-th row of Sp, such\nthatPr(I=ik) =Sp(a,k). The negative example is\nchosen in a similar fashion with the matrix Sn, such that\nPr(I=ik) =Sn(a,k).\n4. EXPERIMENTAL SETTING\nThis section details the experiments performed to as-\nsess the efﬁcacy of the proposed triplet mining method.\nFirst, a preliminary evaluation of the triplets generated is\ndone against structural annotations from a commonly used\ndataset for music structure analysis. Secondly, we train two\nseparate convolutional neural networks using triplets ob-\ntained by temporal sampling and those from our method.\nThe obtained embeddings are fed as input to a downstream\nmusic segmentation algorithm and performance on both\nboundary detection and structural grouping is measured.\nFinally, to gain more insight on the quality of the triplets\ngenerated, training is performed on different fractions of\nthe unlabelled training dataset.\n4.1 Datasets\nSince this work falls under the scope of unsupervised\nlearning, a non annotated external audio collection is used\nfor training. It is composed of 20,000 tracks, spanning\nvarious musical genres such as rock, popular, rap, jazz,\nelectronic or classical. These were retrieved from publiclyavailable playlists and the audio obtained from Y OUTUBE .\nCare has been taken to discard any track from this exter-\nnal collection also present in one of the following testing\ndatasets. Training is separately done on 10%,50% and\n100% of this dataset.\nSALAMI : the Structural Annotations for Large\nAmounts of Music Information (SALAMI) [30] contains\n1,359tracks ranging from classical, jazz, popular to world\nand live music. For evaluation, we use the upper anno-\ntations of a subset of 884songs labelled by two different\nannotators.\nJSD: the Jazz Structure Dataset [31] gathers 340jazz\nrecordings provided with two-level annotations: the cho-\nrus level (a full cycle of the harmonic schema, which is the\nannotation level used for evaluation) and a solo level, con-\nsisting of one more choruses. These annotations follow the\ncommon jazz structure schema that includes the introduc-\ntion of the main melody (theme), followed by alternating\nsolos from the different musicians and a ﬁnal return to-\nwards the main theme at the end of the track.\n4.2 Evaluation metrics\nCommon evaluation metrics for automatic structure analy-\nsis are employed throughout our experiments. For bound-\nary detection, we report the F-measure2of the trimmed3\nboundary detection hit-rate with a 0.5and3-second tol-\nerance windows (HR.5F, HR3F respectively). For struc-\ntural grouping, we report the F-measure of frame pair-\nwise clustering [21] (PFC), which gives another view on\nﬂat segmentation performance in terms of frame-wise sec-\ntion assignment. Additionally, the normalized conditional\nentropy score (NCE) [33] is also calculated, in order to\nindicate from a probabilistic perspective the amount of\ninformation shared between predicted label distributions\nand their corresponding reference annotations. In the case\nwhere the test dataset has more than one annotator, the best\nscore across annotators is kept, as the goal of the evaluation\nprocess is to measure how close to human ground-truth the\npredicted segmentations are. The average score obtained\nper metric is reported and the statistical signiﬁcance is as-\nsessed using a paired-sample T-test with p <0.05.\n4.3 Implementation details\nInput features : All tracks are resampled at 22.05\nkHz. We use log-scaled Mel-spectrograms as input to the\ndeep network, with a window and hop size of 2048 and\n256 respectively. We compute 60Mel-band coefﬁcients\nper frame. Feature patches are composed of 512frames\n(≃5.94s) and centered at each detected beat location.\nMining parameters : Chroma features are extracted us-\ning a minimum frequency of 27.5Hz over8octaves. 20\nMFCC coefﬁcients are calculated per frame and the very\nﬁrst one is discarded. Both are calculated with the librosa\n2All evaluations are done using the mir_eval package [32].\n3The ﬁrst and last boundaries are discarded during evaluation, as they\ncorrespond to the beginning and the end of the track and therefore, do not\nprovide any information regarding the system’s performance.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n420library [34]. The features are encoded into time-delay rep-\nresentations using context values of m= 16 andm= 8\nbeats respectively. The parameters αandβof the sigmoid\nﬁltering step are set to 60and0.85. We give equal weight\nto each feature by setting the γ= 0.5in Equation (6). Fi-\nnally, the negative matrix Snis calculated with a smooth-\ning parameter λ= 5. These parameters were found using\nsimple grid searches and visual inspections of the obtained\nself-similarity matrices.\nNetwork architecture : The encoder consists of a con-\nvolutional neural network composed of 3convolutional\nlayers, each followed by a max-pooling layer and Elu ac-\ntivation, and two fully-connected layers comprising 128\nunits with Elu and linear activations respectively. All con-\nvolutional layers use a kernel size of size (3,3)with32\nﬁlters each. The output embeddings are ℓ2-normalized be-\nfore calculating the triplet loss. The models are imple-\nmented4with Pytorch 1.7.1[35]. The SGD optimizer\nwith10−4weight decay and 0.9momentum is used, the\nmodels are trained for a maximum of 200epochs, where\neach batch is composed of 256triplets obtained from one\nsingle track. Similar to previous work [12], the margin\nparameter δis set to0.1and the embedding dimension to\nd= 128 .\nDownstream segmentation : For all experiments, the\nembeddings returned by each model are fed as input to\nspectral clustering [24], as this algorithm jointly performs\nboth boundary detection and structural grouping in an un-\nsupervised manner and has proven to be efﬁcient in previ-\nous studies [13, 14]. This also allows one to compare the\ninﬂuence of each of the tested representations into a sin-\ngle uniﬁed framework. The original algorithm takes two\ndistinct beat-synchronized audio features as input (MFCC\nand CQT). We consider this method as a second baseline\nwhich we denote as LSD (Laplacian Structural Decom-\nposition). However in our case, it is directly applied to\nthe self-similarity Spof each track. When this algorithm\nis combined with deep representations, we simply replace\nboth input features by the embedding matrix. Finally, be-\ncause spectral clustering outputs multiple levels of seg-\nmentation, only the one maximizing the considered metric\nis reported (HR.5F and HR3F for boundary detection, PFC\nand NCE for structural grouping).\n5. RESULTS\n5.1 Preliminary evaluation\nWe generate 256 triplets per track contained in the\nSALAMI dataset and report the proportions of true pos-\nitives, true negatives and correct triplets in Table 1. For\ncomparison purposes, we also provide a random base-\nline, where each anchor, positive and negative example is\nuniformly sampled over the whole track. The sampling\nmethod proposed signiﬁcantly improves the selection of\nnegative examples compared to the temporal sampling ap-\nproach. However, random negative sampling performs bet-\nter than our approach. This was to be expected, since\n4Code:github.com/morgan76/Triplet_Miningthe latter samples negatives over the whole track while\nour method greatly narrows down the number of proba-\nble candidates (see Equation (8)). Conversely, the tempo-\nral sampling returns a higher proportion of true positives\nthan ours, since these are sampled in a relatively short time\nwindow around their respective anchor, thus omitting any\nsection repetition occurring inside the input track. All in\nall, our approach returns a much higher proportion of cor-\nrect triplets than either of the comparison strategies while\nguaranteeing that positive examples are located within the\nright musical sections and the negative within a relatively\nshort time window around their anchor’s.\nSampling TP TN CT\nRandom .401±.22.595±.21.194±.06\nTemporal [12].886±.32.398±.49.325±.47\nOurs .800±.40 .583±.49 .432 ±.50\nTable 1 . Triplet mining results on upper annotation level\nof SALAMI dataset. TP, TN, CT: proportions of true pos-\nitives, true negatives and correct triplets respectively. Re-\nsults highlighted in bold denote statistically signiﬁcant im-\nprovements over temporal sampling according to a paired-\nsample T-test with p <0.05.\n5.2 Segmentation and structural grouping\nTable 2 shows the performance of our approach against\ntemporal sampling on the upper annotations of the\nSALAMI dataset. Regardless of the amount of training\ndata, our method constantly improves both boundary de-\ntection and structural grouping in a signiﬁcant manner. It\nis also interesting to see that such improvement is already\nachieved when the proposed method uses only 10% of the\ntraining dataset. This corroborates the results from Section\n5.1, showing that improving the triplets quality provides a\ncleaner training signal and makes learning more efﬁcient.\nMethod (Split) HR.5F HR3F PFC NCE\nLSD .195.486.707.682\nTemp. (10%) [12] .280.665.770.677\nOurs (10%) .291 .676 .777 .691\nTemp. (50%) [12] .288.671.773.678\nOurs (50%) .296 .682 .778 .690\nTemp. (100%) [12] .284.670.773.678\nOurs (100%) .297 .683 .781 .694\nTable 2 . Flat segmentation results on SALAMI ( upper an-\nnotations). Results in bold denote statistically signiﬁcant\nimprovement over temporal sampling on same split (de-\nnoted as Temp. ).\nFrom a more qualitative perspective, Figure 2 shows\nexamples of self-similarity matrices derived from the em-\nbeddings trained with temporal sampling and our method.\nIn the latter case, consecutive musical sections are better\ndiscriminated (clearer block structures on the main diago-\nnal). Section repetitions (visible as diagonal stripes and\noff-diagonal blocks) are more straightforward to recog-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n421nize, especially those with relatively small durations (sec-\ntions A, B or D).\nMethod (Split) HR.5F HR3F PFC NCE\nLSD .195.486.707.682\nTemp. (10%) [12] .221.568.739.745\nOurs (10%) .219 .586.744.749\nTemp. (50%) [12] .243.586.763.766\nOurs (50%) .222.583.755.758\nTemp. (100%) [12] .229.590.766.767\nOurs (100%) .225.592.754.760\nTable 3 . Flat segmentation results on JSD ( chorus annota-\ntion level). Results in bold denote statistically signiﬁcant\nimprovement over temporal sampling (denoted as Temp. )\non same split.\nResults on the JSD dataset are given in Table 3. Here,\nthe improvements made are not as consistent. However,\nwhen using only 10% of the training dataset, the perfor-\nmance of our approach remains within the same range than\nthat of the baseline when trained on larger splits. Com-\npared to the results obtained on SALAMI, the small im-\nprovements made here can be associated with the way\nstructure is deﬁned in terms of feature similarity in jazz.\nZD'ABCDABCDABC D ZZ\nD'\nA\nB\nC\nD\nA\nB\nC\nD\nA\nB\nC\nD\nZTemporal sampling\nZD'ABCDABCDABC D ZOurs\nFigure 2 . Example of self-similarity matrices for the track\nSALAMI 1380 . Left: encoder trained with temporal sam-\npling . Right: encoder trained using the proposed triplet\nmining method. White dotted lines denote boundary in-\nstants.\n5.3 Discussion on mining parameters\nImpact on triplet selection : The sampling parame-\nters could further be tuned to improve performance. More\nspeciﬁcally, the audio descriptors employed at the ﬁrst\nstage and their combination could be adapted to the train-\ning data in order to better emphasize more speciﬁc aspects\nof the audio. For example, some music genres such as\npop music or rock generally rely on the repetition of cer-\ntain chord progressions [1]. However, introducing a degree\nof timbral homogeneity allows for differentiating two sec-\ntions that are semantically similar, such as in the example\nfrom Figure 1, ’refrain’ and ’refrain-Solo’. Putting more\nemphasis on timbral features might be better adapted to\nmusic genres such as jazz, where structure is highly inﬂu-\nenced by changes in soloists. As an example, Figure 3 dis-\nplays the positive sampling matrices obtained when vary-ing theγparameter from Equation (6). It is clear to see\nthat favoring timbral similarity helps better approximating\nsegment transitions and mutual dissimilarities between the\nsuccessive solos of saxophone, piano and guitar.\nintro\ntheme_1s_saxs_piano s_guitar theme_2intro\ntheme_1\ns_sax\ns_piano\ns_guitar\ntheme_2PM\nintro\ntheme_1s_saxs_piano s_guitar theme_2PC\nFigure 3 . Example of positive sampling matrices for\nMichael Brecker — Song for Bilbao . Left: emphasis on\ntimbral content ( γ= 0.9). Right: emphasis on harmonic\ncontent (γ= 0.1). White dotted lines denote boundary\ninstants.\nImpact on segmentation : To illustrate how the bal-\nance between harmonic and timbral features impacts the\nﬁnal segmentation, the encoder is trained on the 10% and\n50% splits of the dataset with γ= 0.9, thus putting a\nstronger emphasis on the MFCC-based similarity at the\ntriplet mining stage. All other parameters are kept to their\ninitial values described in Section 4.3. The segmentation\nresults summarized in Table 4 show that the choice of the\nparameter γdoes impact the training process. In this case,\nputting more weight on timbral information seems to make\nthe representations more sensitive to timbral changes and\nimproves boundary detection (HR3F) in a signiﬁcant man-\nner compared to temporal sampling .\nMethod (Split) HR.5F HR3F PFC NCE\nTemp. (10%) [12] .221.568.739.745\nOurs (10%, γ= 0.9).223 .585.743.750\nTemp. (50%) [12] .243.586.763.766\nOurs (50%, γ= 0.9).234 .607.769.772\nTable 4 . Flat segmentation results on JSD ( chorus annota-\ntion level) with emphasis on timbral features ( γ= 0.9).\nResults in bold denote statistically signiﬁcant improve-\nment over temporal sampling (denoted as Temp. ) on same\nsplit.\n6. CONCLUSION\nThis work introduced a repetition-based triplet mining\nmechanism to learn efﬁcient audio representations prior to\nmusic segmentation, which can signiﬁcantly improve both\nboundary detection and structural grouping, while needing\nless data than previous similar methods. Complementary\nexperiments demonstrate that this sampling process can be\nfurther adapted to the ﬁnal type of segmentation desired by\neither emphasizing harmonic or timbral information from\nthe input track.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4227. ACKNOWLEDGEMENTS\nThis work was performed using HPC resources from\nGENCI-IDRIS (Grant 2022-AD011013255R1).\n8. REFERENCES\n[1] O. Nieto, G. J. Mysore, C.-i. Wang, J. B. Smith,\nJ. Schlüter, T. Grill, and B. McFee, “Audio-based mu-\nsic structure analysis: Current trends, open challenges,\nand applications,” Transactions of the International So-\nciety for Music Information Retrieval , vol. 3, no. 1,\n2020.\n[2] S. Dai, Z. Jin, C. Gomes, and R. B. Dannenberg, “Con-\ntrollable deep melody generation via hierarchical mu-\nsic structure representation,” ISMIR , 2021.\n[3] A. Bozzon, G. Prandi, G. Valenzise, M. Tagliasacchi\net al. , “A music recommendation system based on se-\nmantic audio segments similarity,” Proceeding of In-\nternet and Multimedia Systems and Applications-2008 ,\npp. 182–187, 2008.\n[4] J. P. Bello, “Measuring structural similarity in music,”\nIEEE Transactions on Audio, Speech, and Language\nProcessing , vol. 19, no. 7, pp. 2013–2025, 2011.\n[5] S. Dai, H. Zhang, and R. B. Dannenberg, “Auto-\nmatic analysis and inﬂuence of hierarchical structure\non melody, rhythm and harmony in popular music,”\n2020.\n[6] M. Fuentes, B. McFee, H. C. Crayencour, S. Essid,\nand J. P. Bello, “A music structure informed downbeat\ntracking system using skip-chain conditional random\nﬁelds and deep learning,” in ICASSP , 2019.\n[7] M. Mauch, K. C. Noland, and S. Dixon, “Using musi-\ncal structure to enhance automatic chord transcription.”\ninISMIR , 2009.\n[8] J. Paulus, M. Müller, and A. Klapuri, “State of the art\nreport: Audio-based music structure analysis.” in IS-\nMIR, 2010.\n[9] M. Goto, “A chorus section detection method for mu-\nsical audio signals and its application to a music listen-\ning station,” IEEE Transactions on Audio, Speech, and\nLanguage Processing , vol. 14, no. 5, pp. 1783–1794,\n2006.\n[10] R. B. Dannenberg and N. Hu, “Pattern discovery tech-\nniques for music audio,” Journal of New Music Re-\nsearch , vol. 32, no. 2, pp. 153–163, 2003.\n[11] M. Müller and F. Kurth, “Towards structural analysis of\naudio recordings in the presence of musical variations,”\nEURASIP Journal on Advances in Signal Processing ,\nvol. 2007, pp. 1–18, 2006.\n[12] M. C. McCallum, “Unsupervised learning of deep fea-\ntures for music segmentation,” in ICASSP , 2019.[13] J.-C. Wang, J. B. L. Smith, W. T. Lu, and X. Song, “Su-\npervised metric learning for music structure features,”\ninISMIR , 2021.\n[14] J. Salamon, O. Nieto, and N. J. Bryan, “Deep embed-\ndings and section fusion improve music segmentation,”\ninISMIR , 2021.\n[15] M. Buisson, B. McFee, S. Essid, and H. C. Crayencour,\n“Learning multi-level representations for hierarchical\nmusic structure analysis,” in ISMIR , 2022.\n[16] J. B. Smith, C.-H. Chuan, and E. Chew, “Audio prop-\nerties of perceived boundaries in music,” IEEE trans-\nactions on multimedia , vol. 16, no. 5, pp. 1219–1228,\n2014.\n[17] F. Kaiser and G. Peeters, “A simple fusion method of\nstate and sequence segmentation for music structure\ndiscovery.” in ISMIR , 2013.\n[18] B. McFee and D. P. Ellis, “Learning to segment songs\nwith ordinal linear discriminant analysis,” in ICASSP ,\n2014.\n[19] K. Ullrich, J. Schlüter, and T. Grill, “Boundary de-\ntection in music structure analysis using convolutional\nneural networks.” in ISMIR , 2014.\n[20] J. Paulus and A. Klapuri, “Music structure analysis us-\ning a probabilistic ﬁtness measure and a greedy search\nalgorithm,” IEEE Transactions on Audio, Speech, and\nLanguage Processing , vol. 17, no. 6, pp. 1159–1170,\n2009.\n[21] M. Levy and M. Sandler, “Structural segmentation of\nmusical audio by constrained clustering,” IEEE trans-\nactions on audio, speech, and language processing ,\nvol. 16, no. 2, pp. 318–326, 2008.\n[22] J. Serra, M. Müller, P. Grosche, and J. L. Arcos, “Un-\nsupervised music structure annotation by time series\nstructure features and segment similarity,” IEEE Trans-\nactions on Multimedia , vol. 16, no. 5, pp. 1229–1240,\n2014.\n[23] X. Wang, X. Han, W. Huang, D. Dong, and M. R.\nScott, “Multi-similarity loss with general pair weight-\ning for deep metric learning,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition , 2019, pp. 5022–5030.\n[24] B. McFee and D. Ellis, “Analyzing song structure with\nspectral clustering.” in ISMIR , 2014.\n[25] G. Shibata, R. Nishikimi, and K. Yoshii, “Music struc-\nture analysis based on an lstm-hsmm hybrid model.” in\nISMIR , 2020.\n[26] F. Korzeniowski, S. Böck, and G. Widmer, “Probabilis-\ntic extraction of beat positions from a beat activation\nfunction.” in ISMIR , 2014.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n423[27] S. Böck, F. Korzeniowski, J. Schlüter, F. Krebs, and\nG. Widmer, “Madmom: A new python audio and mu-\nsic signal processing library,” in Proceedings of the\n24th ACM international conference on Multimedia ,\n2016, pp. 1174–1178.\n[28] H. Kantz and T. Schreiber, Nonlinear time series anal-\nysis. Cambridge university press, 2004, vol. 7.\n[29] A. Hermans, L. Beyer, and B. Leibe, “In defense of the\ntriplet loss for person re-identiﬁcation,” arXiv preprint\narXiv:1703.07737 , 2017.\n[30] J. B. L. Smith, J. A. Burgoyne, I. Fujinaga,\nD. De Roure, and J. S. Downie, “Design and creation\nof a large-scale database of structural annotations.” in\nISMIR , 2011.\n[31] S. Balke, J. Reck, C. Weiß, J. Abeßer, and M. Müller,\n“Jsd: A dataset for structure analysis in jazz music,”\nTransactions of the International Society for Music In-\nformation Retrieval , vol. 5, no. 1, 2022.\n[32] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, D. P. Ellis, and C. C. Raffel,\n“mir_eval: A transparent implementation of common\nmir metrics,” in ISMIR , 2014.\n[33] H. M. Lukashevich, “Towards quantitative measures of\nevaluating song segmentation.” in ISMIR , 2008.\n[34] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,\nE. Battenberg, and O. Nieto, “librosa: Audio and music\nsignal analysis in python,” in Proceedings of the 14th\npython in science conference , vol. 8, 2015, pp. 18–25.\n[35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad-\nbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga et al. , “Pytorch: An imperative style, high-\nperformance deep learning library,” Advances in neural\ninformation processing systems , 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n424"
    },
    {
        "title": "FlexDTW: Dynamic Time Warping With Flexible Boundary Conditions.",
        "author": [
            "Irmak Bukey",
            "Jason Zhang",
            "T. J. Tsai 0001"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265393",
        "url": "https://doi.org/10.5281/zenodo.10265393",
        "ee": "https://zenodo.org/records/10265393/files/000087.pdf",
        "abstract": "Alignment algorithms like DTW and subsequence DTW assume specific boundary conditions on where an alignment path can begin and end in the cost matrix.  In practice, the boundary conditions may not be known a priori or may not satisfy such strict assumptions.  This paper introduces an alignment algorithm called FlexDTW that is designed to handle a wide range of boundary conditions.  FlexDTW allows alignment paths to start anywhere on the bottom or left edge of the cost matrix (adjacent to the origin) and to end anywhere on the top or right edge.  In order to properly compare paths of very different lengths, we use a goodness measure that normalizes the cumulative path cost by the path length.  The key insight of FlexDTW is that the Manhattan length of a path can be computed by simply knowing the starting point of the path, which can be computed recursively during dynamic programming.  We artificially generate a suite of 16 benchmarks based on the Chopin Mazurka dataset in order to characterize audio alignment performance under a variety of boundary conditions.  We show that FlexDTW has consistently strong performance that is comparable or better than commonly used alignment algorithms, and it is the only system with strong performance in some boundary conditions.",
        "zenodo_id": 10265393,
        "dblp_key": "conf/ismir/BukeyZ023",
        "keywords": [
            "Boundary conditions",
            "FlexDTW",
            "Dynamic programming",
            "Manhattan length",
            "Cost matrix",
            "Audio alignment",
            "Chopin Mazurka dataset",
            "Goodness measure",
            "Alignment algorithms",
            "Subsequence DTW"
        ],
        "content": "FLEXDTW: DYNAMIC TIME WARPING WITH FLEXIBLE BOUNDARY\nCONDITIONS\nIrmak Bükey\nPomona College\nibab2018@mymail.pomona.eduJason Zhang\nUniversity of Michigan\nzhangjt@umich.eduTJ Tsai\nHarvey Mudd College\nttsai@hmc.edu\nABSTRACT\nAlignment algorithms like DTW and subsequence DTW\nassume speciﬁc boundary conditions on where an align-\nment path can begin and end in the cost matrix. In prac-\ntice, the boundary conditions may not be known a priori\nor may not satisfy such strict assumptions. This paper in-\ntroduces an alignment algorithm called FlexDTW that is\ndesigned to handle a wide range of boundary conditions.\nFlexDTW allows alignment paths to start anywhere on the\nbottom or left edge of the cost matrix (adjacent to the ori-\ngin) and to end anywhere on the top or right edge. In or-\nder to properly compare paths of very different lengths,\nwe use a normalized path cost measure that normalizes the\ncumulative path cost by the path length. The key insight\nof FlexDTW is that the Manhattan length of a path can\nbe computed by simply knowing the starting point of the\npath, which can be computed recursively during dynamic\nprogramming. We artiﬁcially generate a suite of 16 bench-\nmarks based on the Chopin Mazurka dataset in order to\ncharacterize audio alignment performance under a variety\nof boundary conditions. We show that FlexDTW has con-\nsistently strong performance that is comparable or better\nthan commonly used alignment algorithms, and it is the\nonly system with strong performance in some boundary\nconditions.\n1. INTRODUCTION\nDynamic Time Warping (DTW) is a dynamic program-\nming algorithm for computing the optimal alignment be-\ntween two sequences under certain assumptions. In the\nMIR literature, it is the most widely used method for align-\ning two audio recordings of the same piece of music. One\nof its assumptions is the boundary condition of the align-\nment path: it assumes that the alignment path begins at the\norigin of the pairwise cost matrix and ends in the opposite\ncorner of the cost matrix. When working with real data\nlike (say) Youtube recordings of a piece of classical mu-\nsic, the boundary conditions are usually unknown a priori\nand may not satisfy the restrictive assumptions of standard\n© I. Bükey, J. Zhang, and T. Tsai. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: I. Bükey, J. Zhang, and T. Tsai, “FlexDTW: Dynamic Time\nWarping With Flexible Boundary Conditions”, in Proc. of the 24th Int.\nSociety for Music Information Retrieval Conf., Milan, Italy, 2023.DTW. This may be due to silence or applause at the begin-\nning or end of videos, or perhaps due to some videos con-\ntaining only one movement of a piece. This paper seeks to\ndevelop a more ﬂexible variant of DTW that can handle a\nwider range of boundary conditions.\nPrevious work . There is a very large body of work\non variations or extensions of DTW. These works gener-\nally fall into one of two categories. The ﬁrst category fo-\ncuses on mitigating the quadratic computation and mem-\nory costs of DTW. Some works approach this by speed-\ning up an exact DTW computation through the use of\nlower bounds [1, 2], early abandoning [3, 4], using mul-\ntiple cores [5, 6], or specialized hardware [7, 8]. Tralie\nand Dempsey [9] introduce a method for computing ex-\nact DTW with linear memory by processing diagonals\nrather than rows/columns. Other works propose approx-\nimations to DTW that require less computation, runtime,\nor memory. Some approaches include approximate lower\nbounds [10,11], approximations of DTW distance [12,13],\nimposing bands in the cost matrix to limit extreme time\nwarping [14,15], computing alignments at multiple resolu-\ntions [16,17], parallelizable approximations of DTW [18],\nor working with a ﬁxed amount of memory [19]. The sec-\nond category focuses on extending the behavior of DTW\nin some way. Some examples in the MIR literature include\nhandling structural differences like repeats and jumps in\nmusic [20–22], performing alignment in an online setting\n[23–25], handling partial alignments [26, 27], using multi-\nple performances to improve alignment accuracy [28], ac-\ncounting for pitch drift in a capella music [29], and align-\ning sets of source recordings and mixtures [30].\nShortcomings . Our work aims to make DTW more ﬂex-\nible by focusing on an often overlooked aspect: bound-\nary conditions. The vast majority of previous works on\nDTW or its variants focus on handling one speciﬁc type of\nboundary condition. For example, DTW (and any of its ap-\nproximations or efﬁcient implementations) assumes that an\nalignment path begins at the origin of the cost matrix and\nends in the opposite corner. Similarly, subsequence DTW\nassumes that an alignment path begins somewhere on the\nlonger edge of the cost matrix and ends on the opposite\nedge. As mentioned above, in many situations the bound-\nary conditions are unknown a priori or may be incompati-\nble with the assumptions of standard alignment methods.\nOur approach . FlexDTW is designed to be ﬂexible in\nhandling a wide range of boundary conditions. Assuming\nthat the origin of the cost matrix is in the lower left corner,733Figure 1 . Different boundary conditions for the align-\nment path between two sequences. The full match and\nsubsequence conditions are well handled by standard al-\ngorithms, but the other conditions are not.\nFlexDTW allows an alignment path to begin anywhere on\nthe left or bottom edge, and it allows the alignment path to\nend anywhere on the top or right edge. Figure 1 shows\nseveral examples of boundary conditions that FlexDTW\ncan handle. To properly compare alignment paths of very\ndifferent length, it is necessary to use a normalized path\ncost measure that normalizes the cumulative path cost by\nthe path length. While it is possible to determine the op-\ntimal alignment path ending at any position by following\nthe backpointers in the backtrace matrix, this would result\nin an impractically high computation overhead. The key\ninsight with FlexDTW is that the Manhattan length of an\nalignment path can be computed by simply knowing the\nstarting and ending location of the alignment path (without\nknowing the actual path itself). The starting location infor-\nmation can be computed in a recursive manner and stored\nduring the dynamic programming stage, making it possible\nto compute normalized path costs in an efﬁcient manner.\nContributions . This paper has three main contribu-\ntions. First, we introduce an alignment algorithm called\nFlexDTW that handles a wide range of boundary condi-\ntions. FlexDTW allows an alignment path to start any-\nwhere on the two edges of the cost matrix adjacent to the\norigin (e.g. bottom and left edge), and it allows alignment\npaths to end anywhere on the other two edges (top and right\nedge). Second, we design a suite of 16 benchmarks based\non the Chopin Mazurka dataset [31] in order to character-\nize audio alignment performance under a variety of spe-\nciﬁc boundary conditions. Third, we present experimental\nresults showing that FlexDTW has consistently strong per-\nformance across all 16 benchmarks that is comparable to or\nbetter than the best-performing system from a set of widely\nused audio alignment algorithms. We provide source code\nfor our implementation of FlexDTW, along with code for\nrunning all experiments in this paper.1\n2. SYSTEM DESIGN\nIn this section we describe the FlexDTW algorithm in de-\ntail. To make it clear how FlexDTW relates to previous\n1Code can be found at https://github.com/anonymized/ .work, we begin with a brief overview of DTW and subse-\nquence DTW.\n2.1 DTW and Subsequence DTW\nStandard DTW estimates the alignment between two fea-\nture sequences x0,x1, . . . ,xN−1andy0,y1, . . . ,yM−1.\nIt accomplishes this by using dynamic programming to\nﬁnd the optimal path through a pairwise cost matrix C∈\nRN×Munder a set of allowable transitions. DTW assumes\nthat the alignment path begins at (0,0) and ends at ( N−1,\nM−1) in the cost matrix. Subsequence DTW is a variant\nof DTW that ﬁnds the optimal alignment between a query\nsequence x0,x1, . . . ,xN−1and any subsequence within a\n(typically longer) reference sequence y0,y1, . . . ,yM−1.\nSubsequence DTW assumes that the alignment path in-\ncludes the entire query sequence but can begin and end\nanywhere in the reference sequence.\n2.2 FlexDTW: Algorithm\nFlexDTW is a variant of DTW that seeks to handle a much\nwider range of boundary conditions. It is designed to\nhandle the boundary conditions of standard DTW, subse-\nquence DTW, as well as many other conditions that are not\nhandled by DTW or subsequence DTW. We ﬁrst give an\noverview of the boundary conditions that FlexDTW is de-\nsigned to handle, describe the main challenge in allowing\nﬂexible boundary conditions, introduce a key insight, and\nthen explain the algorithm in detail.\nBoundary conditions . Figure 1 shows an overview of\nthe boundary conditions that FlexDTW is designed to han-\ndle. In the given ﬁgure, the alignment path may begin any-\nwhere along the left edge or bottom edge of the cost ma-\ntrix, and the alignment path can end anywhere along the\ntop edge or right edge.2Note that the resulting set of al-\nlowable alignment paths is a superset that contains all al-\nlowable DTW paths and all allowable subsequence DTW\npaths, in addition to many other types of alignment paths.\nChallenge . The main challenge in allowing such ﬂex-\nible boundary conditions is normalization. Because the\nset of allowable paths has such an enormous variation in\npath length, one must use a normalized path cost to fairly\ncompare one alignment path with another. (Otherwise, the\npath with minimum cumulative path cost will simply be the\nalignment path with fewest elements.) This means that our\nmetric for comparing different alignment paths must nor-\nmalize the cumulative path cost by some measure of align-\nment path length. To determine the length of an alignment\npath ending at position ( i,j), we could simply follow the\nbackpointers in the backtrace matrix, but this introduces\nan impractically high computational overhead to the algo-\nrithm.\nKey insight . The key insight with FlexDTW is that the\nManhattan length of an alignment path does not require\nknowing what the actual alignment path is. Assuming that\nthe alignment path is monotonically non-decreasing (as is\nthe case with DTW), computing the Manhattan length of\n2We exclude a buffer region near the top left and bottom right corners\nto avoid short, degenerate paths, as will be explained later.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n734an alignment path only requires knowing the starting point\nand ending point of the path. The starting location of any\noptimal alignment path can be computed recursively with\nminimal computational overhead and simply stored as an\nadditional piece of information (similar to the backtrace\ninformation). Having the starting location of all optimal\nalignment paths allows us to efﬁciently calculate normal-\nized path costs without having to perform any backtrack-\ning. We can then compare the goodness of alignment paths\nby comparing their path cost per Manhattan block.\nAlgorithm . We now describe the FlexDTW algorithm\nfor aligning two feature sequences x0,x1,. . . ,xN−1and\ny0,y1, . . . ,yM−1. Similar to DTW, one must specify a\nset of allowable transitions and corresponding transition\nweights. In addition, there is one hyperparameter buffer\nthat speciﬁes a minimum allowable path length, which\nhelps to avoid short, degenerate alignment paths. The al-\ngorithm consists of ﬁve steps, which are described below.\nThe ﬁrst step is to compute a pairwise cost matrix C∈\nRN×M, where each element C[i,j]indicates the distance\nbetweenxiandyjunder some distance metric.\nThe second step is to initialize three matrices: a cu-\nmulative cost matrix D∈RN×M, a backtrace matrix\nB∈ZN×M, and a starting point matrix S∈ZN×M. In\norder to allow alignment paths to begin anywhere in either\nsequence without penalty, we initialize D[0,j] =C[0,j],\nj= 0,1,. . . ,M−1andD[i,0] =C[i,0],i= 0,1,. . . ,\nN−1. We also initialize Sfor all valid starting points for\nalignment paths. Since the starting locations are all of the\nform (0,j) or (i,0), we can efﬁciently encode the starting\nlocations as a single integer, where positive integers indi-\ncate a starting location ( 0,j) and negative integers indicate\na starting location ( i,0). This reduces the memory over-\nhead of matrix S. Accordingly, we initialize S[0,j] =j,\nj= 0,1,. . . ,M−1andS[i,0] =−i,i= 0,1,. . . ,N−1.\nThe third step is to compute the elements in D, B, and S\nusing dynamic programming. For a given set of allowable\ntransitions {t1,t2,t3}(assumed to be {(1, 1), (1, 2), (2, 1)\n}in the equation below) and corresponding multiplicative\nweightsw1,w2,w3, the optimal transition B[i,j]can be\ncomputed with the following recursive formula:\nB[i,j] = argmin\nk=1,2,3\n\nD[i−1,j−1]+w1·C[i,j]\ni+j−|S[i−1,j−1]|ifk= 1\nD[i−1,j−2]+w2·C[i,j]\ni+j−|S[i−1,j−2]|ifk= 2\nD[i−2,j−1]+w3·C[i,j]\ni+j−|S[i−2,j−1]|ifk= 3(1)\nThe numerator elements in the equation above are cumula-\ntive path costs, and the denominator elements are the Man-\nhattan lengths of each candidate path. Once the best tran-\nsition has been determined, the value of D[i,j]can be up-\ndated as:\nD[i,j] =\n\nD[i−1,j−1]+w1·C[i,j]ifB[i,j] = 1\nD[i−1,j−2]+w2·C[i,j]ifB[i,j] = 2\nD[i−2,j−1]+w3·C[i,j]ifB[i,j] = 3\n(2)Similarly, the value of S[i,j]can be updated as:\nS[i,j] =\n\nS[i−1,j−1]ifB[i,j] = 1\nS[i−1,j−2]ifB[i,j] = 2\nS[i−2,j−1]ifB[i,j] = 3(3)\nNote that the elements in Dstill indicate unnormalized\npath costs (as in DTW), but the decision of which tran-\nsition is the best is made based on the normalized path cost\n(i.e. path cost per Manhattan block).\nThe fourth step is to identify the endpoint of the optimal\nalignment path. The candidate set of valid ending points is\ngiven byEcand={(N−1,j)|j=buffer,...,M−\n1} ∪ {(i,M−1)|i=buffer,...,N−1}, which corre-\nsponds to any location in the top or right edge in Figure 1.\nWe exclude a user-speciﬁed buffer region from the top left\nand bottom right corners, which ensures that the alignment\npath is of a certain minimum length. This buffer region\nhelps to prevent the algorithm from selecting short, de-\ngenerate alignments paths with low normalized path cost.\nGiven this set of candidate locations, we select the end-\npointEbestto be\nEbest= argmin\n(i,j)∈EcandD[i,j]\ni+j−|S[i,j]|(4)\nwhere the objective function is the path cost per Manhattan\nblock.\nThe ﬁfth step is to backtrace from the selected endpoint\nusing the backpointers in Buntil we reach an element ( 0,\nj),j= 0,1,. . . ,M−1(on the bottom edge in Figure 1) or\n(i,0),i= 0,1,. . . ,N−1(on the left edge). The resulting\nalignment path is the ﬁnal estimated alignment.\n2.3 FlexDTW: Hyperparameters\nIn this subsection we discuss the hyperparameters in\nFlexDTW and our method for setting them. As mentioned\npreviously, FlexDTW has three kinds of user-deﬁned pa-\nrameters: a set of allowable transitions, a corresponding\nset of transition weights, and a buffer hyperparameter that\nspeciﬁes a minimum path length for allowable alignment\npaths. Note that DTW also requires specifying a set of\ntransitions and transition weights, so FlexDTW has one ad-\nditional hyperparameter compared to DTW.\nTransitions & weights . A typical set of transitions for\naudio alignment tasks is {(1, 1), (1, 2), (2, 1) }, which im-\nposes a maximum warping factor of 2. This set is usually\npreferred to sets that include (0, 1) and (1, 0) transitions,\nsince these transitions can lead to degenerate alignments.\nWe will use this set of allowable transitions throughout this\npaper, unless otherwise noted. The associated transition\nweights can be set in many different ways. In FlexDTW,\nthere is one particular setting of transition weights that is\nof theoretical interest: w1= 2,w2= 3,w3= 3. This\nsetting weights each transition according to its Manhattan\ndistance. Note that in standard DTW (where the alignment\npath is assumed to start at (0, 0) and end at ( N−1,M−1)),\nevery allowable alignment path has the same ManhattanProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n735Piece Files mean std min max\nOpus 17, No 4 64 259.7 32.5 194.4 409.6\nOpus 24, No 2 64 137.5 13.9 109.6 180.0\nOpus 30, No 2 34 85.0 9.2 68.0 99.0\nOpus 63, No 3 88 129.0 13.4 96.2 162.9\nOpus 68, No 3 51 101.1 19.4 71.8 164.8\nTable 1 . Overview of the original Chopin Mazurka dataset.\nThis is used as the source data to generate the benchmark\nsuite. All durations are in seconds.\ndistance, so this setting effectively treats every path as be-\ning equally likely. It is analogous to a maximum likelihood\nformulation in which all possibilities are treated as equally\nlikely a priori, and selection is made entirely based on the\nobservations. For this reason, we recommend setting the\ntransition weights in FlexDTW as w1=W,w2= 3,\nw3= 3, whereWcan be tuned on a validation dataset.\nW= 2 corresponds to a maximum likelihood formula-\ntion, and smaller values of Wcorrespond to a bias to-\nwards diagonal alignment paths. In our experiments, we\nuseW= 1.25, which provided optimal performance on\nthe training set.\nBuffer . The purpose of the buffer is to prevent the al-\ngorithm from selecting short, degenerate alignment paths\nthat may have low normalized path cost. For example, si-\nlence at the end of one sequence may match silence at the\nbeginning of the other sequence, resulting in a very short\nalignment path with low normalized path cost. The buffer\nshould be interpreted as the minimum length along one se-\nquence that an alignment path must match in order to be\nconsidered a valid path. This could simply be set manually\nbased on knowledge of the task or data. In our case, how-\never, our suite of benchmarks spans such a wide range of\nsequence lengths and alignment path lengths that a single\nsetting is not ideal. Therefore, we determined the buffer\nhyperparameter in a data-dependent way for every individ-\nual query based on two considerations. First, when one se-\nquence is much longer than the other sequence, the desired\nbehavior is probably a subsequence alignment. In this case,\nwe want the entire shorter (query) sequence to be matched.\nSecond, when the two sequences are approximately the\nsame length, much more ﬂexibility can be afforded and\nan intuitive parameter is to deﬁne the minimum percent-\nage of either sequence that must be matched. Putting\nthese two considerations together, we recommend setting\nthe buffer hyperparameter in the following way: for align-\ning two sequences of length L1andL2, setbuffer =\nmin(L1,L2)·(1−(1−β)∗min(L1,L2)\nmax(L1,L2)). This sets the\nbuffer to a fraction of the shorter sequence length, where\nthe fraction is close to 1 when L1andL2are very differ-\nent (i.e. the subsequence case) and close to βwhenL1and\nL2are approximately the same. βcan thus be interpreted\nas the minimum fraction of either sequence that must be\nmatched when both sequences are equal in length. We\ntunedβon the training set and found β= 0.1to work\nwell.3. EXPERIMENTAL SETUP\nIn this section we describe the suite of 16 benchmarks that\nwe use to characterize the performance of alignment algo-\nrithms under a variety of boundary conditions.\nOriginal data . The raw source material for our bench-\nmarks comes from the Chopin Mazurka dataset [31]. This\ndataset consists of numerous historic recordings of ﬁve\ndifferent Chopin Mazurkas, along with beat-level annota-\ntions of each recording. All of the recordings for two of\nthe Mazurkas (Opus 17 No. 4 and Opus 63 No. 3) were\nset apart for training and development, and the recordings\nfrom the other three Mazurkas were set apart for testing.\nTable 1 provides an overview of the dataset.\nEvaluation . To evaluate alignment performance, we\nconsider every pair of recordings of the same Mazurka.\nThis results in/parenleftbig64\n2/parenrightbig\n+/parenleftbig88\n2/parenrightbig\n= 5844 training pairs and/parenleftbig64\n2/parenrightbig\n+/parenleftbig34\n2/parenrightbig\n+/parenleftbig51\n2/parenrightbig\n= 3852 testing pairs. For each pair\nof recordings AandB, we compare the estimated align-\nment path against the ground truth beat timestamps in the\nfollowing manner. At each ground truth beat timestamp in\nrecording A, we compute the alignment error between the\nestimated corresponding timestamp in recording B(based\non the predicted alignment path) and the ground truth cor-\nresponding timestamp in recording B(based on the beat\nannotations). We report aggregate alignment performance\nas an error rate indicating the percentage of alignments that\nhave an alignment error greater than a ﬁxed error tolerance.\nModiﬁcations: Overview . We generated synthetically\nmodiﬁed versions of the Mazurka dataset in order to sim-\nulate a variety of boundary conditions. Each modiﬁed ver-\nsion of the Mazurka dataset contains the exact same num-\nber of recordings, but each recording has been modiﬁed to\nstudy a particular boundary condition. Thus, the number of\ntraining pairs and testing pairs is the same as in the original\nbenchmark, but the audio data and corresponding annota-\ntions have been modiﬁed appropriately. Each benchmark\nis evaluated as described above. Below, we describe how\nwe constructed each of the 16 benchmarks.\nFull Match . The full match benchmark is the Mazurka\ndataset in its original unmodiﬁed form. This boundary con-\ndition assumes that both recordings start and end at the be-\nginning and end of the piece. In Figure 1, this corresponds\nto an alignment path that starts in the lower left corner and\nends in the upper right corner.\nSubsequence . The subsequence benchmark assumes\nthat one recording matches a subsequence in the other\nrecording. For every pair of recordings AandB, a ran-\ndomly sampled L-second interval within recording Ais\nselected and aligned against the entirety of recording B.\nWe construct three separate subsequence benchmarks with\nL= 20 ,30,40.\nPartial Start . The partial start benchmark assumes that\nboth recordings start together but that one recording ends\nearly (e.g. only contains one movement). For every pair\nof recordings AandB, we randomly sample a number in\nthe interval [0.55,0.75], select that percentage of recording\nA(starting from the beginning), and align the fragment of\nrecording Aagainst the entirety of recording B.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n736Partial End . The partial end benchmark assumes that\nboth recordings end together but that one recording starts\npart way through the piece. For every pair of recordings\nAandB, we randomly sample a number in the interval\n[0.55,0.75], select that percentage of recording Aat the\nend (i.e. starting in the middle of the recording and extend-\ning until the end), and then align the fragment of recording\nAagainst the entirety of recording B.\nPartial Overlap . The partial overlap benchmark as-\nsumes that both recordings have some temporal overlap,\nbut that one recording contains extra content before the re-\ngion of overlap and the other recording contains extra con-\ntent after the region of overlap. For every pair of recordings\nAandB, we (a) randomly sample a number in [0.55,0.75]\nand select that percentage of recording Astarting from\nthe beginning, (b) randomly sample a different number in\n[0.55,0.75]and select that percentage of recording Bat\nthe end, and then (c) align the fragment of Aagainst the\nfragment of B.\nPre. The pre benchmark assumes that both recordings\ncontain the entire piece but that one recording has a period\nof silence at the beginning. For every pair of recordings\nAandB, we prepend Lseconds of silence to recording A\nand align it to the entirety of recording B. We construct\nthree separate pre benchmarks with L= 5,10,20.\nPost. The post benchmark assumes that both recordings\ncontain the entire piece but that one recording has a period\nof silence after the piece ends. For every pair of recordings\nAandB, we append Lseconds of silence to recording A\nand align it against the entirety of recording B. We con-\nstruct three separate post benchmarks with L= 5,10,20.\nPre-Post . The pre-post benchmark assumes that both\nrecordings contain the entire piece, but that one record-\ning contains extra silence at the beginning and the other\nrecording contains extra silence at the end. For every pair\nof recordings AandB, we prepend Lseconds of silence to\nrecording A, append Lseconds to recording B, and then\nalign the two recordings. We construct three separate pre-\npost benchmarks with L= 5,10,20.\n4. RESULTS\nWe report experimental results with FlexDTW and several\nstandard alignment algorithms:\n• DTW1: Standard DTW with transitions of (1, 1), (1,\n2), (2, 1) and corresponding weights 2, 3, 3.\n• DTW2: Standard DTW with transitions of (1, 1), (1,\n2), (2, 1) and corresponding weights 1, 1, 1.\n• DTW3: Standard DTW with transitions of (1, 1), (1,\n2), (2, 1) and corresponding weights 1, 2, 2.\n• SubseqDTW1: Subsequence DTW with (query, ref-\nerence) transitions of (1, 1), (1, 2), (2, 1) and corre-\nsponding weights 1, 1, 2.\n• SubseqDTW2: Subsequence DTW with (query, ref-\nerence) transitions of (1, 1), (1, 2), (2, 1) and corre-\nsponding weights 2, 3, 3.\n• SubseqDTW3: Subsequence DTW with (query, ref-\nerence) transitions of (1, 1), (1, 2), (2, 1) and corre-\nsponding weights 1, 2, 2.• NWTW: A variant of DTW proposed in [21] that\nallows skip transitions (0, 1) and (1, 0), in addition\nto the usual (1, 1), (1, 2), (2, 1) transitions. The skip\ntransitions incur a ﬁxed penalty cost γ, which is a\nhyperparameter that we tuned on the training data.\nWe assessed the performance of a larger set of DTW ver-\nsions (with different sets of allowable transitions and cor-\nresponding transition weights), but we only include the 3\nversions with best performance to avoid overcluttering Fig-\nure 2. Of particular note, we did experiment with DTW\nversions that had (0, 1) and (1, 0) transitions, but always\nfound those versions to perform much worse. Likewise, we\nconsidered other versions of subsequence DTW but only\ninclude the top 3 versions in Figure 2. The subsequence\nDTW systems are unique in that they are not symmetric.\nFor these systems, we always assume that the alignment is\ntrying to match the shorter recording against a subsequence\nin the longer recording. Note that all of the systems above\ncan be used with any feature representation and distance\nmetric. For simplicity, we use standard chroma features\n(as computed with default parameters in librosa) and a co-\nsine distance metric for all systems.\nFigure 2 compares the performance of FlexDTW and\nthe above algorithms on our benchmark suite. For each\nsystem, we ﬁxed the hyperparameter settings and evalu-\nated its performance across all 16 benchmarks. Each panel\nin Figure 2 corresponds to one of the 16 benchmarks, and\nthe different colored bars show the error rate at 200ms tol-\nerance for different systems. On top of each colored bar,\nwe have also overlaid two black horizontal bars indicating\nthe error rate at 100ms tolerance (above) and at 500ms tol-\nerance (below).\nThere are two things to notice about the results in Figure\n2. First, the seven baseline systems only handle a subset\nof boundary conditions. In other words, each of the base-\nline systems performs well on certain benchmarks and very\npoorly on other benchmarks. For example, the DTW sys-\ntems perform well on the fully matching benchmark (for\nwhich they are designed), but they perform terribly on the\nsubsequence benchmarks and perform worse and worse as\nmore silence is prepended or appended to either record-\ning. Likewise, the subsequence DTW systems perform\nwell on the subsequence benchmarks, but they fail on the\npartial overlap benchmark and have only moderate perfor-\nmance on the pre, post, and pre-post benchmarks. NWTW\nhas strong performance across most benchmarks but fails\ncompletely on the subsequence and partial overlap bench-\nmarks. All of the baseline systems completely fail on the\npartial overlap benchmark, since none are designed to han-\ndle that boundary condition. Second, FlexDTW has con-\nsistently strong performance across all 16 benchmarks. On\nall benchmarks, it has a performance that is comparable\nto or better than the best-performing baseline system. On\nthe partial overlap benchmark, it is the only system that\nhas strong performance, with an error rate that is compa-\nrable to its performance on the other benchmarks. These\nresults demonstrate its ﬂexibility in handling a wide range\nof boundary conditions.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n737Figure 2 . Performance of alignment algorithms on the 16 boundary conditions in our benchmark suite. Colored bars\nindicate error rate at 200ms error tolerance, and the horizontal bars indicate error rates at 100ms (above) and 500ms\n(below). Error rates greater than 50% are not shown.\nSystem 1k 2k 5k 10k 20k 50k\nDTW .033 0.14 0.87 3.5 13.8 87.3\nSubseqDTW 0.04 0.15 0.96 3.82 15.3 96.8\nNWTW .037 0.16 0.97 3.93 15.8 101.1\nFlexDTW .038 0.16 1.05 4.21 16.9 111.1\nTable 2 . Average runtime to process a cost matrix of size\nN×N. Columns indicate different sizes N, and rows indi-\ncate different systems. Each reported number is an average\nover 10 trials, and times are expressed in seconds.\n5. ANALYSIS\nIn this section we conduct several analyses to provide\ndeeper insight into FlexDTW.\nTable 2 compares the runtime of FlexDTW and the\nbaseline alignment systems. We measured how long each\nalignment algorithm took to process a cost matrix of size\nN×N, whereNranges from 1k to 50k. Each number\nin the table is an average over 10 trials. FlexDTW and\nNWTW were implemented in python with numba accel-\neration, and we used the librosa implementation for DTW\nand subsequence DTW (also using numba acceleration).\nAll experiments were run on an Intel Xeon 2.40 GHz CPU.\nFor longer sequence lengths, we can see that FlexDTW\nincurs a 20-25% runtime overhead compared DTW and a\n10-15% runtime overhead compared to subsequence DTW.\nThis overhead comes primarily from needing to perform a\nﬂoating-point division to evaluate every candidate path.\nAnother drawback of FlexDTW is the additional mem-\nory overhead of storing S. We can estimate the memory\noverhead in the following manner. DTW requires allocat-\ning three matrices: the pairwise cost matrix C∈RN×M,\nthe cumulative cost matrix D∈RN×M, and the back-\ntrace matrix B∈ZN×M. Assuming that CandDare\nmatrices of 64-bit ﬂoating point numbers and Bis a ma-\ntrix of 8-bit unsigned integers, the total memory cost is\n8NM+ 8NM+ 1NM= 17NM bytes. FlexDTWrequires allocating an additional matrix Sfor storing the\nstarting point locations. If the two sequence lengths are\nless than215= 32768 , thenScan be stored as a matrix of\n16-bit integers, resulting in an extra memory overhead of\n2NM . If either sequence length is greater than 32768, then\nS must be stored as a matrix of 64-bit integers, resulting\nin an extra memory overhead of 4NM . In summary, the\nmemory overhead is2NM\n17NM≈12% for sequence lengths\nless than 32768 and4NM\n17NM≈24% for longer sequences.\nWe also investigated and identiﬁed two main failure\nmodes of FlexDTW. The ﬁrst failure mode occurs when\nthere is extreme time warping between the two recordings.\nBecause the (1, 1) transition is penalized proportionally\nless than the (2, 1) or (1, 2) transitions, the algorithm will\nsometimes take a “shortcut\" of (1, 1) transitions to/from\nan edge of the cost matrix at the beginning or end of the\nalignment path. The second failure mode occurs when al-\nternate matching paths are selected. For example, in the\nMazurka Opus 17 No. 4, the ﬁrst four measures and the last\nfour measures match, which creates an additional match-\ning alignment path under the ﬂexible boundary conditions\nof FlexDTW.\n6. CONCLUSION\nWe have introduced a time warping algorithm called\nFlexDTW that is designed to handle a wide range of\nboundary conditions. We artiﬁcially generate a suite of 16\nbenchmarks based on the Chopin Mazurka dataset, which\ncharacterizes alignment performance in a variety of bound-\nary conditions. In all 16 boundary conditions, FlexDTW\nhas strong performance that is as good or better than a set\nof widely used alignment algorithms. Compared to the\nlibrosa implementation of DTW and subsequence DTW,\nFlexDTW incurs a runtime overhead of 10-25% and a\nmemory overhead of 12% for sequences less than length\n215and 24% for longer sequences.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n7387. ACKNOWLEDGMENTS\nWe would like to thank Kate Perkins for her contributions\nin the early stages of this project. This material is based\nupon work supported by the National Science Foundation\nunder Grant No. 2144050.\n8. REFERENCES\n[1] Y . Zhang and J. Glass, “An inner-product lower-bound\nestimate for dynamic time warping,” in Proc. of the In-\nternational Conference on Acoustics, Speech, and Sig-\nnal Processing (ICASSP) , 2011, pp. 5660–5663.\n[2] E. Keogh, L. Wei, X. Xi, M. Vlachos, S.-H. Lee, and\nP. Protopapas, “Supporting exact indexing of arbitrar-\nily rotated shapes and periodic time series under eu-\nclidean and warping distance measures,” VLDB Jour-\nnal, vol. 18, no. 3, pp. 611–630, 2009.\n[3] T. Rakthanmanon, B. Campana, A. Mueen, G. Batista,\nB. Westover, Q. Zhu, J. Zakaria, and E. Keogh,\n“Searching and mining trillions of time series subse-\nquences under dynamic time warping,” in Proc. of the\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining , 2012, pp. 262–270.\n[4] J. Li and Y . Wang, “EA DTW: Early abandon to ac-\ncelerate exactly warping matching of time series,” in\nInternational Conference on Intelligent Systems and\nKnowledge Engineering , 2007.\n[5] A. Shabib, A. Narang, C. P. Niddodi, M. Das,\nR. Pradeep, V . Shenoy, P. Auradkar, T. Vignesh, and\nD. Sitaram, “Parallelization of searching and mining\ntime series data using dynamic time warping,” in IEEE\nInternational Conference on Advances in Computing,\nCommunications and Informatics (ICACCI) , 2015, pp.\n343–348.\n[6] S. Srikanthan, A. Kumar, and R. Gupta, “Implementing\nthe dynamic time warping algorithm in multithreaded\nenvironments for real time and unsupervised pattern\ndiscovery,” in International Conference on Computer\nand Communication Technology , 2011, pp. 394–398.\n[7] Z. Wang, S. Huang, L. Wang, H. Li, Y . Wang, and\nH. Yang, “Accelerating subsequence similarity search\nbased on dynamic time warping distance with FPGA,”\ninProceedings of the ACM/SIGDA International Sym-\nposium on Field Programmable Gate Arrays , 2013, pp.\n53–62.\n[8] D. Sart, A. Mueen, W. Najjar, E. Keogh, and V . Nien-\nnattrakul, “Accelerating dynamic time warping subse-\nquence search with GPUs and FPGAs,” in IEEE Inter-\nnational Conference on Data Mining , 2010, pp. 1001–\n1006.\n[9] C. J. Tralie and E. Dempsey, “Exact, parallelizable dy-\nnamic time warping alignment with linear memory,” in\nProc. of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , 2020, pp. 462–469.[10] R. Tavenard and L. Amsaleg, “Improving the efﬁciency\nof traditional DTW accelerators,” Knowledge and In-\nformation Systems , vol. 42, no. 1, pp. 215–243, 2015.\n[11] Y . Zhang and J. Glass, “A piecewise aggregate approx-\nimation lower-bound estimate for posteriorgram-based\ndynamic time warping,” in Proc. of the Annual Confer-\nence of the International Speech Communication Asso-\nciation , 2011.\n[12] A. Lods, S. Malinowski, R. Tavenard, and L. Amsa-\nleg, “Learning DTW-preserving shapelets,” in Interna-\ntional Symposium on Intelligent Data Analysis , 2017,\npp. 198–209.\n[13] G. Nagendar and C. Jawahar, “Efﬁcient word image re-\ntrieval using fast DTW distance,” in Proc. of the IEEE\nInternational Conference on Document Analysis and\nRecognition (ICDAR) , 2015, pp. 876–880.\n[14] H. Sakoe and S. Chiba, “Dynamic programming algo-\nrithm optimization for spoken word recognition,” IEEE\nTransactions on Acoustics, Speech, and Signal Pro-\ncessing , vol. 26, no. 1, pp. 43–49, 1978.\n[15] F. Itakura, “Minimum prediction residual principle ap-\nplied to speech recognition,” IEEE Transactions on\nAcoustics, Speech, and Signal Processing , vol. 23,\nno. 1, pp. 67–72, 1975.\n[16] M. Müller, H. Mattes, and F. Kurth, “An efﬁcient mul-\ntiscale approach to audio synchronization,” in Proc.\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2006, pp. 192–197.\n[17] S. Salvador and P. Chan, “FastDTW: Toward accurate\ndynamic time warping in linear time and space,” in\nProc. of the KDD Workshop on Mining Temporal and\nSequential Data , 2004.\n[18] T. Tsai, “Segmental DTW: A parallelizable alternative\nto dynamic time warping,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , 2021, pp. 106–110.\n[19] T. Prätzlich, J. Driedger, and M. Müller, “Memory-\nrestricted multiscale dynamic time warping,” in Proc.\nof the IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP) , 2016, pp.\n569–573.\n[20] C. Fremerey, M. Müller, and M. Clausen, “Handling\nrepeats and jumps in score-performance synchroniza-\ntion,” in Proc. of the International Society for Music\nInformation Retrieval Conference (ISMIR) , 2010, pp.\n243–248.\n[21] M. Grachten, M. Gasser, A. Arzt, and G. Widmer, “Au-\ntomatic alignment of music performances with struc-\ntural differences,” in Proc. of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2013.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n739[22] M. Shan and T. Tsai, “Improved handling of repeats\nand jumps in audio-sheet image synchronization,” in\nProc. of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , 2020, pp. 62–69.\n[23] S. Dixon, “Live tracking of musical performances us-\ning on-line time warping,” in Proc. of the International\nConference on Digital Audio Effects , 2005, pp. 92–97.\n[24] S. Dixon and G. Widmer, “MATCH: A music align-\nment tool chest,” in Proc. of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2005, pp. 492–497.\n[25] R. Macrae and S. Dixon, “Accurate real-time win-\ndowed time warping,” in Proc. of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2010, pp. 423–428.\n[26] M. Müller and D. Appelt, “Path-constrained partial\nmusic synchronization,” in Proc. of the International\nConference on Acoustics, Speech, and Signal Process-\ning (ICASSP) , 2008, pp. 65–68.\n[27] M. Müller and S. Ewert, “Joint structure analysis with\napplications to music annotation and synchronization,”\ninProc. of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , 2008, pp. 389–394.\n[28] S. Wang, S. Ewert, and S. Dixon, “Robust and efﬁcient\njoint alignment of multiple musical performances,”\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , vol. 24, no. 11, pp. 2132–2145,\n2016.\n[29] S. Waloschek and A. Hadjakos, “Driftin’ down the\nscale: Dynamic time warping in the presence of pitch\ndrift and transpositions,” in Proc. of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , 2018, pp. 630–636.\n[30] D. Yang, K. Ji, and T. Tsai, “Aligning unsynchronized\npart recordings to a full mix using iterative subtrac-\ntive alignment,” in Proc. of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2021, pp. 810–817.\n[31] C. Sapp, “Hybrid numeric/rank similarity metrics for\nmusical performance analysis,” in Proc. of the Inter-\nnational Conference for Music Information Retrieval\n(ISMIR) , 2008, pp. 501–506.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n740"
    },
    {
        "title": "Modeling Harmonic Similarity for Jazz Using Co-occurrence Vectors and the Membrane Area.",
        "author": [
            "Carey Bunks",
            "Tillman Weyde",
            "Simon Dixon",
            "Bruno Di Giorgi"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265400",
        "url": "https://doi.org/10.5281/zenodo.10265400",
        "ee": "https://zenodo.org/records/10265400/files/000090.pdf",
        "abstract": "In jazz, measuring harmonic similarity is complicated by the common practice of reharmonization -- the altering or substitution of chords without fundamentally changing the piece's harmonic identity. This is analogous to natural language processing tasks where synonymous terms can be used interchangeably without significantly modifying the meaning of a text.  Our approach to modeling harmonic similarity borrows from NLP techniques, such as distributional semantics, by embedding chords into a vector space using a co-occurrence matrix.  We show that the method can robustly detect harmonic similarity between songs, even when reharmonized.  The co-occurrence matrix is computed from a corpus of symbolic jazz-chord progressions, and the result is a map from chords into vectors. A song's harmony can then be represented as a piecewise-linear path constructed from the cumulative sum of its chord vectors.  For any two songs, their harmonic similarity can be measured as the minimal surface membrane area between their vector paths.  Using a dataset of jazz contrafacts, we show that our approach reduces the median rank of matches from 318 to 18 compared to a baseline approach using pitch class vectors.",
        "zenodo_id": 10265400,
        "dblp_key": "conf/ismir/BunksWDG23",
        "keywords": [
            "harmonic similarity",
            "reharmonization",
            "chords",
            "vector space",
            "co-occurrence matrix",
            "distributional semantics",
            "symbolic jazz-chord progressions",
            "piecewise-linear path",
            "minimal surface membrane area",
            "dataset of jazz contrafacts"
        ],
        "content": "MODELING HARMONIC SIMILARITY FOR JAZZ USING\nCO-OCCURRENCE VECTORS AND THE MEMBRANE AREA\nCarey Bunks1,2Tillman Weyde2Simon Dixon1Bruno Di Giorgi3\n1Queen Mary University of London, UK\n2City, University of London, UK\n3Apple, UK\nABSTRACT\nIn jazz, measuring harmonic similarity is complicated by\nthe common practice of reharmonization – the altering or\nsubstitution of chords without fundamentally changing the\npiece’s harmonic identity. This is analogous to natural\nlanguage processing tasks where synonymous terms can\nbe used interchangeably without signiﬁcantly modifying\nthe meaning of a text. Our approach to modeling har-\nmonic similarity borrows from NLP techniques, such as\ndistributional semantics, by embedding chords into a vec-\ntor space using a co-occurrence matrix. We show that the\nmethod can robustly detect harmonic similarity between\nsongs, even when reharmonized. The co-occurrence ma-\ntrix is computed from a corpus of symbolic jazz-chord pro-\ngressions, and the result is a map from chords into vectors.\nA song’s harmony can then be represented as a piecewise-\nlinear path constructed from the cumulative sum of its\nchord vectors. For any two songs, their harmonic simi-\nlarity can be measured as the minimal surface membrane\narea between their vector paths. Using a dataset of jazz\ncontrafacts, we show that our approach reduces the median\nrank of matches from 318 to 18 compared to a baseline ap-\nproach using pitch class vectors.\n1. INTRODUCTION\nMeasuring similarity between songs is important for many\nmusic information retrieval tasks, for example, recom-\nmendation systems, copyright infringement detection, and\ngenre classiﬁcation systems. Many different types of fea-\ntures can be used to compare songs, but the speciﬁc focus\nof this paper is on jazz harmony as represented by the sym-\nbolic chord progressions found on leadsheets.\nThe analysis of harmonic similarity has been studied us-\ning N-grams [1], parse trees [2, 3], and NLP methods such\nas TF-IDF, Latent Semantic Analysis (LSA), and Doc2Vec\n[4]. The approach taken in this paper is based on embed-\nding chord symbols into a vector space through the compu-\ntation of a co-occurrence matrix [5]. As will be seen when\n© C. Bunks, T. Weyde, S. Dixon, and B. Di Giorgi. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: C. Bunks, T. Weyde, S. Dixon, and B.\nDi Giorgi, “Modeling Harmonic Similarity for Jazz Using Co-occurrence\nVectors and the Membrane Area”, in Proc. of the 24th Int. Society for\nMusic Information Retrieval Conf., Milan, Italy, 2023.we describe the data in Section 2, many chord symbols oc-\ncur only rarely. To reduce computational problems due to\nsparsity, the dimensionality of chord space should be re-\nduced [6]. A typical machine learning approach for this\nmight use an algorithm such as truncated singular value\ndecomposition after vectorization [7]. In this work, how-\never, we use music theory to reduce the number of effective\nchord symbols prior to vectorization, which in turn reduces\nthe chord space dimensionality. In the ensuing sections\nwe describe the data, explain our approach to dimension-\nality reduction, and give computational details of how we\ncompute the co-occurrence matrix. We then explain how\nthe chord vectors generated from the co-occurrence matrix\nare used to represent chord progressions, and we present a\nnovel harmonic-similarity metric, the membrane area .\nThe experimental part of our paper is based on analyz-\ning contrafacts. In jazz, a contrafact is a song whose har-\nmony is similar to that of another song, but which has a\ndifferent melody [8]. The tune I Got Rhythm , by George\nGershwin (1930), is a well-known source of many con-\ntrafacts,1and there are numerous other examples [9–11].\nIn addition to the difference in melody, contrafact chord\nprogressions often feature reharmonization, a common\npractice in jazz that makes chord substitutions in a song\nwhile maintaining its harmonic identity [12]. Reharmo-\nnization is a core characteristic of jazz – so much so that\nthere are typically reharmonizations from chorus to chorus\neven in a single performance of a jazz song.\n2. THE DATA\nThe data used in this paper is a corpus of symbolic chord\nprogressions similar to those found in jazz fake books,\nsuch as the Real Book [13]. The progressions are mainly\nfrom jazz standards, but also include some blues, jazz-\nblues, modal jazz, and jazz versions of pop tunes. The\ncorpus is derived from a collection distributed with Impro-\nVisor , an open-source music notation program.2Our\nmodiﬁcations remove control information used by the\nImpro-Visor application, retaining the musical content and\nsong-speciﬁc metadata. We have performed numerous\nquality checks on the data, have made corrections where\nrequired, and have enriched some of the metadata. The re-\n1https://en.wikipedia.org/wiki/Rhythm_changes\n2https://www.cs.hmc.edu/~keller/jazz/\nimprovisor/757sulting corpus and the code we used to generate our ex-\namples is available on GitHub.3The Impro-Visor cor-\npus provides chord progressions for 2,612 songs, and is\nthe largest digital collection of jazz chord progressions we\nknow of. For comparison, the applications iRealPro4and\nBand-in-a-Box5contain chord progressions for roughly\n1400 and 226 jazz standards, respectively. The Weimar\nJazz Database contains chords for 456 jazz songs.6\nOf the 134,182 chord symbol instances in the corpus,\nthere are 1,542 unique symbols, of which many are rare,\nwith 20% occurring just once, and 50% fewer than six\ntimes. As the corpus consists mainly of jazz standards,\nthere is a preponderance of 7thchords, comprising at least\nthe root, 3rd, 5th, and 7thnotes. These types of chords of-\nten have additional extensions (9th, 11th, 13th) and chro-\nmatic alterations ( ♭9,♯9,♭5,♯5). A common variation of\njazz chords replaces the 7thwith a 6thfor major7 and mi-\nnor7 chords. As 7thchords are the basic harmonic unit\nin jazz [14], and make up 77% of our corpus, they are\nthe focus of our approach to dimensionality reduction de-\nscribed in the next section. Of the remaining chords, 16%\nare three-note chords (triads), and 7% are drawn from a va-\nriety of special types, as shown in Table 1, which provides\na list of all the types and their frequencies.\nType Percentage\n7thchords (and extensions) 76.939%\nmajor triads 11.484%\nslash chords 4.781%\nminor triads 4.320%\nsus chords 1.364%\nno chord 0.458%\naugmented triads 0.392%\nmajor triads add9 0.127%\ndiminished triads 0.095%\npower chords 0.031%\npolychords 0.009%\nTable 1 . Corpus chord types and their frequencies\n3. DIMENSIONALITY REDUCTION\nOur approach to reducing dimensionality is based on map-\nping chords to a reduced vocabulary of functionally equiv-\nalent symbols (similar to [15]). This is important because\n20% of the chords in the corpus occur only a single time\n(known as hapax legomena ), and without additional pro-\ncessing, these types of terms would provide no predic-\ntive value [16]. Many techniques are used in NLP to bet-\nter leverage hapax legomena. For example, stemming,\nlemmatization, and thesauri are all useful. This paper\n3https://github.com/carey-bunks/\nJazz-Chord-Progressions-Corpus\n4https://www.irealb.com/forums/showthread.php?\n12753-Jazz-1350-Standards\n5https://members.learnjazzstandards.com/sp/\nbiab-jazzstandards/\n6https://jazzomat.hfm-weimar.de/dbformat/\ndbcontent.htmltakes a similar approach for harmony, making use of mu-\nsic theory to reduce the dimensionality of chord space. Our\nmethod is akin to lemmatization, applying concepts from\nfunctional harmony to group similar chords into classes\n(for example, see [17]). Based on standard practices in\njazz [12,18,19], we reduce the set of 1,542 chord symbols\nto 61 chord classes, as detailed in the following sections.\n3.1 7thChord Types\nOur choice of base chord types is built on the four-note\n7thchords diatonically generated from the major scale, and\nmaking up 77% of our corpus. These are the major7 (M),\nminor7 (m), dominant7 (7), and minor7 ♭5(h), where the\nsymbols shown in parentheses are abbreviations we use in\nthis paper. To these we add a ﬁfth base chord type, the\ndiminished7 (o). Combining the ﬁve types with the root\nnotes from the 12 pitch classes yields 60 chord classes.\nInstances of these classes can occur with extensions or al-\nterations, and we map these to the base class without ex-\ntension/alteration. For example, we map the symbols Cm9\nand Cm11 to the Cm7 class; C7 ♭9, C7♯5, and C13 to the\nC7 class; and CM7 ♯11 to the CM7 class. In addition, in ac-\ncordance with reharmonization practices, we assign chords\nsuch as CmM7 to the Cm7 class and C6 to the CM7 class.\nWe also include the symbol NC(no chord) to account for\nthe absence of harmony (0.5% of the corpus).\n3.2 Other Chord Type Mappings\nIn the following discussion, we describe a rationale for\nmapping the remaining 22.5% of the symbols into classes\nof the ﬁve base types deﬁned above. The mapping choices\ndescribed in the following discussion are imperfect, but\nthey are simple to implement, and we show they are ad-\nequate for our application.\n3.2.1 Triads\nTriads represent 16% of the corpus. As they do not con-\ntain a 7thnote, mapping them to the base chord types can\nbe ambiguous. For example, a C major triad shares all of\nits notes with both the CM7 and C7 chords. We attempt to\nresolve triad ambiguities using principles from tonal har-\nmony and the local harmonic context. Based on the chord\nfollowing a triad, we decide whether it has a subdominant,\ndominant, or tonic function [19]. For example, for a ma-\njor triad, if the root of the following chord is a ﬁfth down\nand a member of the major7 or minor7 classes we assign\nthe triad to the dominant7 class with the same root. Other-\nwise, we assign it to its corresponding major7 class. Major\ntriads with an added 9thare handled in the same way. Aug-\nmented triads share their notes with dominant7 ♯5 chords,\nan alteration of the dominant, and so we map these to the\ndominant7 class with the same root. Finally, we map all the\nminor and diminished triads to their corresponding minor7\nand diminished7 classes, respectively.\n3.2.2 Sus Chords and Slash Chords\nSus chords also have a harmonic function that depends on\ncontext [18]. When followed by a dominant7 chord withProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n758the same root, they act like a subdominant and we opt to\nmap them to a minor7 class with a root a ﬁfth above. For\nexample, a G7sus4 would map to a Dm7. Otherwise, they\nact like a dominant and we map them to the dominant7\nclass with the same root. Slash chords are chords played\nover a speciﬁc bass note, for example C/G or Dm7/G,\nwhere the symbol above (to the left of) the slash is the\nchord and below is the bass note. If the bass note belongs\nto the chord above the slash (for example, C/G), it is an in-\nversion. For such cases, we map it to the class of the chord\nabove the slash. Slash chords are also commonly used to\nrepresent sus chords. For example, Dm7/G is harmonically\nequivalent to G9sus4. We map these according to the pro-\ncess for sus chords. For all other slash chords, we map the\nchord as if the bass note were an extension or alteration of\nthe chord above the slash.\n3.2.3 Power Chords and Polychords\nPower chords consist of just two notes, a root and a ﬁfth.\nAs they have no 3rdor 7th, they are harmonically ambigu-\nous. With only 42 instances in our corpus, we have opted\nto map these chords to the no-chord class. With only 12\ninstances, polychords are also rare. These chords, used\nmainly by pianists, consist of a lower triad and an upper\ntriad or 7thchord. We map polychords according to their\nlower structure, interpreting the upper structure as a col-\nlection of extensions or alterations.\n4. KEY SIGNATURE BASED REPRESENTATION\nTo make distributional semantics more effective, we trans-\npose all songs to a common key, and represent them in\nRoman numeral notation. However, transposition requires\nknowing the correct key of each song, and from extensive\nmanual checking, we know that our database contains a\nfair number of songs for which the stated key signature is\nin error. For this reason, we introduce a key signature esti-\nmation algorithm, as described in the following section.\n4.1 Key Signature Estimation Algorithm\nSeveral authors have proposed key estimation algorithms\nfor music information retrieval tasks [20–24]. However,\nour objective is not to estimate the key that is cognitively\nperceived by a listener, but rather a simpler problem, the\nkey signature that minimizes the number of accidentals\nneeded when writing out the song’s chords. Some prior\nwork exists for this [25], however, it is based on machine\nlearning models applied to MIDI data for classical music.\nOur algorithm selects the key signature most consistent\nwith the chord progression. For each chord in a progres-\nsion, we map it to one of the described 61 classes, and\nidentify all the major scales it could belong to (excluding\ndiminished7 and no chord classes). The major scale that\naccumulates the most beats is the resulting estimate of the\nkey signature for that song.\nFigure 1 provides a concrete illustration of how the key\nestimation algorithm works for the case of a short chord\nprogression: A7-Dm7-G7-CM7-CM7. Each column of thetable represents one measure, and in this example, there is\none chord per measure. The column labels correspond to\nthe chords, and each row label is a key signature whose\nmajor scale diatonically contains one or more of the chords\nin the progression. As shown, the A7 chord belongs to D\nmajor; the Dm7 chord belongs to B ♭, C, and F major; G7\nbelongs to C major; and CM7 belongs to both C and G ma-\njor. Presuming four beats per measure, C accumulates the\nmost beats (16), and is the resulting key signature estimate.\nFigure 1 . Illustration of key signature estimation\n4.2 Algorithm Evaluation\nAs already mentioned, there are quite a few songs in our\ncorpus where the key signature is incorrect or in doubt.\nNevertheless, it is worthwhile comparing the outputs of\nour key estimation algorithm with the keys recorded in the\ncorpus. Of the 2,612 songs, the algorithm concurs with\nthe database for 1,763 (67.5%) of them. For the 849 songs\nwith database key signatures that do not agree with our\nestimates, we use the Circle of Fifths as a distance met-\nric to evaluate the magnitude of differences between the\ntwo. Adjacent key signatures on the circle of ﬁfths cor-\nrespond to major scales that differ in a single pitch class.\nTable 2 shows the distribution of circle-of-ﬁfths distances\nbetween estimated and database key signatures for all of\nthe songs in the corpus. The ﬁrst row is the distance in\nnumber of sharps or ﬂats from the estimated to the database\nkey, where 0corresponds to agreement. The last column\nof Table 2 is labelled “Amb.” for ambiguous. There are\n123 songs in the database for which the key estimation al-\ngorithm returns a non-unique result, ﬁnding two or more\nequally good major scales. This occurs for 4.7% of the\nsongs in the corpus, and when it does our estimation algo-\nrithm defaults to the database key.\nDx 6♭5♭4♭3♭2♭1♭01♯2♯3♯4♯5♯Amb.\nFrq 10223355993041763 1832225121123\nTable 2 . Key signature estimation statistics with the circle\nof ﬁfths distance Dxby the frequency of occurrence Frq\n4.3 Mapping to Roman Numeral Notation\nOnce a song’s key has been estimated, all the chords in\nits progression can be mapped to Roman numeral notation.\nTable 3 shows the Roman numerals corresponding to chord\nroots for C major. As an example, the sequence of chords\nA7-Dm-G7-CM maps to vi7-iim-v7-iM. In our system, weProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n759represent minor keys by their relative major, so the relative\nminor cadence, Bm7b5-E7-Am7, maps to viih-iii7-vim.\nRoot CD♭DE♭EFG♭GA♭AB♭B\nRN i♭iiii♭iiiiiiiv♭vv♭vivi♭viivii\nTable 3 . Roman numeral notation: chord roots in C major\n5. VECTOR REPRESENTATION\nSections 3 and 4 described our approach for reducing the\ndimensionality of chord space, distilling the 1,542 chord\nsymbols in our corpus to 61 classes. In this section we de-\nscribe our method for embedding the chord classes into a\nvector space. Our design objective is that common rehar-\nmonizations be close to each other in cosine similarity, and\nit is known that the co-occurrence matrix can capture this\ntype of characteristic [5, 26–28].\nGiven a corpus of Dchord progressions, with progres-\nsiond∈ {1,2,...,D}containing Ndchords with in-\ndices1,2...,Nd, we can represent the corresponding se-\nquence of chord symbols as sd,1,sd,2,...,s d,Nd. We de-\nﬁne the symmetric, sliding context window, Wk,d, of nom-\ninal width Nwwith the indices Wk,d= [wl,...,(k−\n1),(k+1),...,w r], where the left and right endpoints are\nwl= max(k−Nw,1)andwr= min(k+Nw,Nd), re-\nspectively. With these deﬁnitions, the (i,j)thelement of\nthe co-occurrence matrix, Ci,jis computed by\nCi,j=D/summationdisplay\nd=1Nd/summationdisplay\nk=1/summationdisplay\nw∈Wk,d/braceleftBigg\n1,ifsd,k=ciandsd,w=cj\n0,otherwise\n(1)\nThis produces a square, symmetric matrix whose row Ci\n(or alternatively, column) is a vector representations of the\nithchord class ci. As it will be useful in the following,\nwe normalize each row to have unit length. Because co-\noccurrence matrices capture contextual information, the\nvectors of chord classes that have similar harmonic func-\ntion are expected to be close to each other with respect to\nthe cosine similarity measure, and this seems to be borne\nout by an inspection of certain chord vectors. For exam-\nple, of 60 chord classes, the closest vector to the v7 is its\ntritone substitute, the ♭ii7, and the closest to the iim is the\niih, a common substitute from the parallel minor scale (see\nmodal interchange in [19]).\n6. MEMBRANE-AREA DISTANCE METRIC\nWe use the co-occurrence vectors to represent chord pro-\ngressions in a way that represents each chord type, du-\nration, and metric position, while being robust to rehar-\nmonizations. The normalized chord vectors derived from\nthe co-occurrence matrix can be used to plot the path of\na song’s progression through 61-dimensional space. Start-\ning from the origin, the sequence of chord vectors can be\nconcatenated from head to tail, beginning with the ﬁrst,\nand terminating with the last vector (see Figure 2). Each\nunit vector is scaled by the number of beats of the chordit represents, and the result is a piecewise linear function\nthroughR61. The comparison of two songs in this space\ncan be formulated as a trajectory comparison problem, for\nwhich there are many existing techniques [29]. The most\npopular ones, however, are not well adapted to our prob-\nlem. The Fréchet distance, dynamic time warping, longest\ncommon subsequence, and the edit distance are all based\non matching and comparing points, and would not directly\nfactor in information about reharmonized chords embodied\nin the co-occurrence vectors. For this reason, we introduce\na new metric that accounts for reharmonizations by com-\nputing the membrane area between the paths of two songs.\nExpressed formally, we represent song vector paths by\npiecewise linear functions of the form f(t)∈R61, where\nt∈[0,1]is a parametric variable representing the number\nof normalized beats traversed in the song. We can move\nalong the entire length of fin discrete, equal increments,\ndt, where the starting point of the function, f(0)att= 0\nis the origin, and the end point of the function is at t= 1.\nGiven two songs and their corresponding piecewise linear\nfunctions, f(t)andg(t), and letting K= 1/dt, we can\ndeﬁne a distance metric between them as the area of a 2D\nmembrane, M, stretched between the two paths. Mis cal-\nculated as the integral obtained in the limit of\nM(f,g) = lim\ndt→0K/summationdisplay\nk=0∥f(kdt)−g(kdt)∥dt, (2)\nwhere∥ · ∥ is the Euclidean norm. The piecewise linear\nfunctions for two identical chord progressions would, natu-\nrally, overlay each other, yielding a membrane area of zero.\nTwo harmonically similar songs should trace out similar\npaths keeping the membrane area small. For example,\ntwo chord progressions that differ in just a tritone substitu-\ntion will only slightly perturb the path and the membrane\narea between songs. Figure 2 is a notional illustration of\nhow the measure in Equation 2 is evaluated. The red and\nblue paths represent two different songs, each having three\nchords. Each song begins at the origin, and the chord vec-\ntors are added head-to-tail to trace out a piecewise linear\npath. The membrane area metric is approximated by sum-\nming the lengths of the Nequally spaced black line seg-\nments drawn between the two songs. Note that this way of\nrepresenting the harmony of a song accounts for positions\nand durations of each chord in the progression, as well as\ncapturing harmonic similarities of chord transitions.\n7. EXPERIMENTS\nWe have designed some experiments based on a set of jazz\ncontrafacts listed in a Wikipedia article.7The list has 252\njazz songs whose harmonies are known to be based on\nother songs (see also [30]). A subset of 91 contrafacts are\navailable in our corpus, but for 11 of them, only a section\nof the harmony is borrowed, and we remove these from the\nlist. The basic structure of all of our experiments is the\nsame: for each contrafact, we compute the membrane area\n7https://en.wikipedia.org/wiki/List_of_jazz_\ncontrafactsProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n760Figure 2 . Conceptual illustration of the membrane-area\ndistance metric for two, 3-chord sequences\ndistance between it and each of the other 2,611 songs in\nour corpus. We then sort the songs from smallest mem-\nbrane area to largest, and note the original song’s rank in\nthat list. Because of reharmonizations, we don’t expect the\nmembrane area to be zero for all contrafact-original pairs,\nbut matches should rank high in the list. Original songs\noften inspire multiple contrafacts, and some may be closer\nto each other than to the original. For these reasons, we\nuse the histogram of original song rankings to present the\noverall performance of our method, and we use the median\nrank as a method of comparison between approaches.\n7.1 Using Co-Occurrence Vectors\nWe evaluated six variants of our approach using co-\noccurrence chord vectors. The ﬁrst three were based on\nthe context window widths Nw= [1,2,3]. The second\nthree variants used the same context window values, but\napplied to a ﬁltered version of the chord progressions. For\neach chord progression, the ﬁlter collapses adjacent iden-\ntical chords to a single instance. For Nw= 1, this has\nthe effect of eliminating the co-occurrence of chords with\nthemselves, making the diagonal of the co-occurrence ma-\ntrix zero. Of the six versions, the best result was obtained\nfor the ﬁltered chord progressions with the context window\nwidthNw= 1. Figure 3 shows the histogram of original\nsong rankings for this case. The median rank is 18, mean-\ning that half of the original songs rank in the top 0.7% in\nharmonic similarity to their contrafacts. As there is some\nhistogram mass out to rank 1,382, the histogram makes\nuse of a log-scale on the x-axis. It is likely that some of the\nsongs ranking better than the original are also contrafacts,\nas the Wikipedia list is far from exhaustive, but it would\nrequire substantial effort and expertise to evaluate this.\nAs noted, some original songs have inspired many con-\ntrafacts. As an example of this in our corpus, there are four\nknown contrafacts of the song All the Things You Are . The\nranks and membrane areas of the original song for each\ncontrafact are shown in Table 4. The original ranks highly\nfor three of the four contrafacts in the table. As the chord\nprogressions for Prince Albert andAll the Things You are\nare identical, their membrane area is zero. The contrafacts\nFigure 3 . Histogram of original song ranks for 80 con-\ntrafacts (median rank = 18)\nAblution andBoston Bernie have some chord substitutions,\nand the original song ranks highly for both of them. The\nsong I Want More , however, does quite poorly, with a rank\nof 758thout of the 2,611 songs in our corpus.\nContrafact Rank Membrane Area\nPrince Albert 1 0.00\nAblution 1 6.72\nBoston Bernie 2 7.72\nI Want More 758 26.89\nTable 4 . Rank and membrane area for All the Things You\nAreagainst its four contrafacts\nTo investigate, we use the jazz harmony visualization\ntool described in [31] to display the chord progressions for\nthese two songs. The visualization shows a tabular format\nwith each rectangle representing a measure. Figures 4 and\n5 show All the Things You Are andI Want More , respec-\ntively. The background colors indicate the key the chords\nbelong to. Red is for the main key, which is A ♭for both\nsongs. Other colors indicate modulations. Some chords are\nembedded in a geometric shape to indicate they are toni-\ncizations: diamonds are secondary dominants, pentagons\nare borrowed chords. As the ﬁgures illustrate, the two\nsongs have some similar chords, however, the sequences\nof modulations are completely different. Whereas All the\nThings You Are modulates through the tonal centers of C\nmajor, E♭major, G major, and E major, I Want More mod-\nulates to D ♭major and C minor. After verifying the latter’s\nchord progression,8we conclude that, harmonically, these\ntwo songs have very little in common, and we question the\nannotation of this song as a contrafact.\n7.2 Using Pitch-Class Vectors\nTo evaluate the effect of using co-occurrence vectors, we\ncompare with a baseline vector embedding scheme based\n8Jamey Aebersold play-along book, volume 82, Dexter GordonProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n761Figure 4 . Chord Progression for All the Things You Are\nFigure 5 . Chord progression for I Want More\non converting chord symbols to their pitch-class vectors.\nThis is similar to the starting point of the approach used\nin [32]. We begin by applying the key estimation algorithm\ndescribed in Section 4.1 to transpose all chords in our cor-\npus to the key of C. Subsequently, each chord in the corpus\nis converted to a 12-dimensional binary pitch-class vector,\nwith ones in positions corresponding to pitch classes be-\nlonging to the chord, and zeroes elsewhere. Thus, for a C7\nchord with the notes C, E, G, and B ♭, the corresponding\npitch-class vector is [1,0,0,0,1,0,0,1,0,0,1,0].\nFollowing a similar schema as for the previous exper-\niment, the pitch-class vectors can be used to construct\npiecewise linear paths, however, now they are constructed\nin a 12-dimensional space. We use the membrane area as\npreviously to rank songs by harmonic similarity. Table 5\ncompares the performance of co-occurrence vectors for the\nbest case (chord progression ﬁltering with a window size\nofNw= 1) versus pitch-class vectors using three metrics:\nmedian rank, mean rank, and mean reciprocal rank. Co-\noccurrence vectors outperform the pitch-class vectors by alarge margin for each of these criteria.\nVector Type Median Mean MRR\nCo-occurrence 18 222 0.305\nPitch-class 318 457 0.200\nTable 5 . Comparison of median rank, mean rank, and\nmean reciprocal rank (MRR) for the ﬁltered-progression,\nco-occurrence vectors ( Nw= 1) and pitch-class vectors\n8. DISCUSSION AND CONCLUSIONS\nWe showed how co-occurrence vectors can be used to\nmodel harmonic similarity, and introduced the membrane\narea as a evaluation metric that is well-adapted for handling\nreharmonizations. We use music theory to reduce the di-\nmensionality of chord space, and provide a comprehensive\nmap of all 1,542 chord symbols in our corpus to 61 classes.\nThe results are used to compute a dense co-occurrence ma-\ntrix without needing to resort to non-parametric approxi-\nmations such as truncated SVD or gradient descent. Using\nthe cosine similarity measure, we show that the rows of\nthe co-occurrence matrix embody some characteristics of\ncommon reharmonizations. Using the normalized rows of\nthe matrix as vector embeddings of chord classes, we mod-\neled songs as piecewise linear paths in R61. A novel dis-\ntance metric, the membrane area, was introduced and used\nas a measure of harmonic similarity between songs. We\nshowed that the similarity metric can be used to retrieve\ncontrafacts from a database of jazz standards, and that it\nperforms signiﬁcantly better than a baseline system using\nbinary pitch-class vectors as chord embeddings.\nAlthough our approach is successful for contrafact de-\ntection, there are several weaknesses that require future\nwork. Our key detection algorithm is simple and static,\ndespite the fact that jazz harmony exhibits many local key\nchanges (e.g. see Figures 4 and 5). We also treat minor\nkeys as equivalent to their relative major, which is not\nstrictly correct. The chord mapping scheme is limited in its\nability to distinguish common progressions such as triad\nprogressions i-iv and v-i. A richer chord vocabulary or\nlocal key estimation could disambiguate such situations.\nOur song-level similarity assumes only minor structural\ndifferences between pieces. Modifying it to perform sub-\nsequence matching would overcome this limitation.\nWe believe that the methods discussed in this paper\nhave many additional applications, such as those in eval-\nuating harmonic complexity [33] and in musicology [34].\nWe intend to investigate whether our harmonic similarity\nmeasure can be used to cluster jazz songs by composer or\ndecade of publication. Although our focus has been on\njazz, chords have similar functions across much of West-\nern tonal harmony. For this reason, we believe that this\nwork can be adapted to other genres such as classical, rock,\nand pop. Furthermore, as our methods are based on captur-\ning the distributional semantics of harmony, the approach\nmay also be useful in discovering harmonic relationships\nin non-Western music genres.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n7629. ACKNOWLEDGEMENTS\nThe ﬁrst author is a research student at the UKRI Centre\nfor Doctoral Training in Artiﬁcial Intelligence and Music,\nsupported by UK Research and Innovation [grant number\nEP/S022694/1].\n10. REFERENCES\n[1] D. Ponsford, G. Wiggins, and C. Mellish, “Statistical\nlearning of harmonic movement,” Journal of New Mu-\nsic Research , vol. 28, no. 2, pp. 150–177, 1999.\n[2] W. B. De Haas, “Music information retrieval based on\ntonal harmony,” Ph.D. dissertation, Utrecht University,\n2012.\n[3] M. Rohrmeier, “The syntax of jazz harmony: Diatonic\ntonality, phrase structure, and form,” Music Theory and\nAnalysis (MTA) , vol. 7, no. 1, pp. 1–63, 2020.\n[4] D. Zahnd, “Similarity analysis of jazz tunes with vec-\ntor space models,” Ph.D. dissertation, Hochschule für\nMusik, Freiburg, 2022.\n[5] K. Lund and C. Burgess, “Producing high-dimensional\nsemantic spaces from lexical co-occurrence,” Behavior\nResearch Methods, Instruments, & Computers , vol. 28,\nno. 2, pp. 203–208, 1996.\n[6] E. Bruni, N.-K. Tran, and M. Baroni, “Multimodal dis-\ntributional semantics,” Journal of Artiﬁcial Intelligence\nResearch , vol. 49, pp. 1–47, 2014.\n[7] C. O. S. Sorzano, J. Vargas, and A. P. Montano, “A\nsurvey of dimensionality reduction techniques,” arXiv\npreprint arXiv:1403.2877 , 2014.\n[8] B. D. Kernfeld, The New Grove Dictionary of Jazz .\nGrove’s Dictionaries Incorporated, 2002, vol. 2.\n[9] H. Martin, Charlie Parker, Composer . Oxford Uni-\nversity Press, USA, 2020.\n[10] D. H. Rosenthal, Hard Bop: Jazz and Black Music\n1955-1965 . Oxford University Press, 1994.\n[11] T. Owens, Bebop: The Music and its Players . Oxford\nUniversity Press, 1996.\n[12] D. Berkman, The Jazz Harmony Book: A Course in\nAdding Chords to Melodies . Sher Music Co., 2013.\n[13] H. Leonard, The Real Book . Hal Leonard Publishing\nCorporation, 2016.\n[14] R. Rawlins and N. E. Bahha, Jazzology: The Encyclo-\npedia of Jazz Theory for All Musicians . Hal Leonard\nCorporation, 2005.\n[15] X. Serra, “Audio-aligned jazz harmony dataset for\nautomatic chord transcription and corpus-based re-\nsearch,” in in Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2018, pp. 483–490.[16] J. Pierrehumbert and R. Granell, “On hapax legom-\nena and morphological productivity,” in Proceedings\nof the Fifteenth Workshop on Computational Research\nin Phonetics, Phonology, and Morphology , 2018, pp.\n125–130.\n[17] D. Harasim, C. Finkensiep, P. Ericson, T. J. O’Donnell,\nand M. Rohrmeier, “The jazz harmony treebank,” in\nin Proceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR) , 2020, pp.\n207–215.\n[18] M. Levine, The Jazz Theory Book . Sher Music Co.,\n1995.\n[19] J. Mulholland and T. Hojnacki, The Berklee Book of\nJazz Harmony . Berklee Press, 2013.\n[20] M. Mauch and S. Dixon, “Simultaneous estimation of\nchords and musical context from audio,” IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\nvol. 18, no. 6, pp. 1280–1289, 2009.\n[21] J. Pauwels and J.-P. Martens, “Combining musicologi-\ncal knowledge about chords and keys in a simultaneous\nchord and local key estimation system,” Journal of New\nMusic Research , vol. 43, no. 3, pp. 318–330, 2014.\n[22] T. Rocher, M. Robine, P. Hanna, L. Oudre, Y . Gre-\nnier, and C. Févotte, “Concurrent estimation of chords\nand keys from audio,” in in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , 2010, pp. 141–146.\n[23] K. C. Noland and M. B. Sandler, “Key estimation us-\ning a hidden Markov model,” in in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2006, pp. 121–126.\n[24] E. Benetos, A. Jansson, and T. Weyde, “Improving au-\ntomatic music transcription through key detection,” in\nAudio Engineering Society Conference . Audio Engi-\nneering Society, 2014.\n[25] F. Foscarin, N. Audebert, and R. Fournier-S’Niehotta,\n“PKSpell: Data-driven pitch spelling and key signature\nestimation,” arXiv preprint arXiv:2107.14009 , 2021.\n[26] S. Bordag, “A comparison of co-occurrence and simi-\nlarity measures as simulations of context,” in Interna-\ntional Conference on Intelligent Text Processing and\nComputational Linguistics . Springer, 2008, pp. 52–\n63.\n[27] A. Globerson, G. Chechik, F. Pereira, and N. Tishby,\n“Euclidean embedding of co-occurrence data,” Ad-\nvances in Neural Information Processing Systems ,\nvol. 17, 2004.\n[28] L. Leydesdorff and L. Vaughan, “Co-occurrence matri-\nces and their applications in information science: Ex-\ntending ACA to the web environment,” Journal of the\nAmerican Society for Information Science and Tech-\nnology , vol. 57, no. 12, pp. 1616–1628, 2006.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n763[29] K. Toohey and M. Duckham, “Trajectory similarity\nmeasures,” Sigspatial Special , vol. 7, no. 1, pp. 43–50,\n2015.\n[30] F. Tirro, “The silent theme tradition in jazz,” The Mu-\nsical Quarterly , vol. 53, no. 3, pp. 313–334, 1967.\n[31] C. Bunks, T. Weyde, A. Slingsby, and J. Wood, “Vi-\nsualization of tonal harmony for jazz lead sheets,” in\n24th EG Conference on Visualization (EuroVis) Short\nPapers , 2022, pp. 109–113.\n[32] S. Madjiheurem, L. Qu, and C. Walder, “Chord2vec:\nLearning musical chord embeddings,” in Proceedings\nof the Constructive Machine Learning Workshop at\n30th Conference on Neural Information Processing\nSystems (NeurIPS) , 2016.\n[33] B. Di Giorgi, S. Dixon, M. Zanoni, and A. Sarti, “A\ndata-driven model of tonal chord sequence complex-\nity,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , vol. 25, no. 11, pp. 2237–2250,\n2017.\n[34] A. Moore, “Patterns of harmony,” Popular Music ,\nvol. 11, no. 1, pp. 73–106, 1992.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n764"
    },
    {
        "title": "Measuring the Eurovision Song Contest: A Living Dataset for Real-World MIR.",
        "author": [
            "John Ashley Burgoyne",
            "Janne Spijkervet",
            "David J. Baker"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265415",
        "url": "https://doi.org/10.5281/zenodo.10265415",
        "ee": "https://zenodo.org/records/10265415/files/000097.pdf",
        "abstract": "Every year, several dozen, primarily European, countries, send performers to compete on live television at the Eurovision Song Contest, with the goal of entertaining an international audience of more than 150 million viewers. Each participating country is able to evaluate every other country's performance via a combination of rankings from professional jurors and telephone votes from viewers. Between fan sites and the official Song Contest organisation, a complete historical record of musical performances and country-to-country contest scores is available, back to the very first edition in 1956, and for the most recent contests, there is also information about each individual juror's rankings. In this paper, we introduce MIRoVision, a set of scripts which collates the data from these sources into a single, easy-to-use dataset, and a discrete-choice model to convert the raw contest scores into a stable, interval-scale measure of the quality of Eurovision Song Contest entries across the years. We use this model to simulate contest outcomes from previous editions and compare the results to the implied win probabilities from bookmakers at various online betting markets. We also assess how successful content-based MIR could be at predicting Eurovision outcomes, using state-of-the-art music foundation models. Given its annual recurrence, emphasis on new music and lesser-known artists, and sophisticated voting structure, the Eurovision Song Contest is an outstanding testing ground for MIR algorithms, and we hope that this paper will inspire the community to use the contest as a regular assessment of the strength of modern MIR.",
        "zenodo_id": 10265415,
        "dblp_key": "conf/ismir/BurgoyneSB23",
        "keywords": [
            "Eurovision Song Contest",
            "live television",
            "entertainment",
            "international audience",
            "professional jurors",
            "telephone votes",
            "historical record",
            "complete dataset",
            "discrete-choice model",
            "simulated contest outcomes"
        ],
        "content": "MEASURING THE EUROVISION SONG CONTEST:\nA LIVING DATASET FOR REAL-WORLD MIR\nJohn Ashley Burgoyne\nUniversity of Amsterdam\nj.a.burgoyne@uva.nlJanne Spijkervet\nByteDance\njanne.spijkervet@gmail.comDavid John Baker\nUniversity of Amsterdam\nd.j.baker@uva.nl\nABSTRACT\nEvery year, several dozen, primarily European, countries,\nsend performers to compete on live television at the Euro-\nvision Song Contest, with the goal of entertaining an inter-\nnational audience of more than 150 million viewers. Each\nparticipating country is able to evaluate every other coun-\ntry’s performance via a combination of rankings from pro-\nfessional jurors and telephone votes from viewers. Between\nfan sites and the ofﬁcial Song Contest organisation, a com-\nplete historical record of musical performances and country-\nto-country contest scores is available, back to the very ﬁrst\nedition in 1956, and for the most recent contests, there\nis also information about each individual juror’s rankings.\nIn this paper, we introduce MIRoVision, a set of scripts\nwhich collates the data from these sources into a single,\neasy-to-use dataset, and a discrete-choice model to convert\nthe raw contest scores into a stable, interval-scale measure\nof the competitiveness of Eurovision Song Contest entries\nacross the years. We use this model to simulate contest\noutcomes from previous editions and compare the results\nto the implied win probabilities from bookmakers at vari-\nous online betting markets. We also assess how success-\nful content-based MIR could be at predicting Eurovision\noutcomes, using state-of-the-art music foundation models.\nGiven its annual recurrence, emphasis on new music and\nlesser-known artists, and sophisticated voting structure, the\nEurovision Song Contest is an outstanding testing ground\nfor MIR algorithms, and we hope that this paper will inspire\nthe community to use the contest as a regular assessment of\nthe strength of modern MIR.\n1. INTRODUCTION\nThe Eurovision Song Contest (ESC) is an annual event\nwherein several, primarily European, countries compete\nagainst one another by performing original, live songs dur-\ning an internationally televised event. The contest began\nin 1956 and is typically held in the country of the previous\nyear’s winner in the spring.\n© J. A. Burgoyne, J. Spijkervet, and D. J. Baker. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC\nBY 4.0). Attribution: J. A. Burgoyne, J. Spijkervet, and D. J. Baker,\n“Measuring the Eurovision Song Contest: A Living Dataset for Real-World\nMIR”, in Proc. of the 24th Int. Society for Music Information Retrieval\nConf., Milan, Italy, 2023.The content of the musical acts performed during the\nEurovision Song Contest is always novel and notably di-\nverse. Contestants are allowed to sing in whichever lan-\nguage they choose, often electing to sing in English to\ncommunicate the meaning of their song to a larger base, but\nsome countries (notably France) have historically preferred\nto sing in their national language. According to the ofﬁcial\nEurovision rules, all musical acts must perform an original\nsong that is no more than three minutes in length, with the\nlead vocals performed live, and acts are limited to only six\nperformers being on stage at any given moment during the\nperformance [1].\nWithin these constraints, the musical acts of Eurovision\nare known for their ostentatious performances and camp\naesthetics, which are often accompanied with visual spec-\ntacles from lightening to elaborate dance. As the contest\nis an international stage, the musical acts have also been a\nmeans in which countries are able to provide meta-political\ncommentary on either national or global events [2, 3]. The\ncontest has been noted as serving as an important platform\nfor global LGBTQ+ visibility, which featured openly gay\nand transgender performers as early as the 1990s [4].\nThe winner of the contest is determined as a combination\nof both expert and panel voting, with no set criteria stated\nas to what should constitute a winning performance. A\ncombination of the song’s content, the visual performance,\nand the performer’s ability to relate to the zeitgeist are\nall presumed to play an important role in determining the\nwinner. Indeed, the Eurovision Song Contest can be and has\nbeen analysed from a variety of dimensions, summarised by\nWolther as the media, the musical, the musical-economical,\nthe political, the national-cultural, the national-economic,\nand the competitive [5].\nWe next detail the rules of the contest before introducing\nthe MIRoVision data set, which contains a multi-faceted\ncollection of historical data that could be used to predict\nthe contest’s winner and enable researchers to make deeper\ninquiries into the history and music of the contest.\n1.1 Rules of Eurovision\nIn order to participate in the Eurovision Song Contest, parti-\ncipating countries work in coordination with the European\nBroadcasting Union. While each participating country – or\nmore speciﬁcally the country’s partnered national broad-\ncaster – is allowed to decide for themselves which act to\nsend to participate, the results of the Eurovision Song Con-\ntest are determined by voting over three events. These817three events referred to as the First Semi-Final, the Second\nSemi-Final, and the Grand Final. It is the Grand Final that\ntypically receives the vast majority of the attention and\nviewership.\nAs described on the ofﬁcial Eurovision website1, all\nparticipating countries qualify for two semi-ﬁnal shows in\nthe week leading up to the Grand Final, which only a subset\nof the total countries will perform. France, Germany, Italy,\nSpain and the United Kingdom are automatically included\nin the Grand Final and are referred to as the ’Big Five’.\nAfter a country has performed, each other country gives\ntwo sets of votes for the performance. The ﬁrst set of\nvotes comes from an expert panel of music industry pro-\nfessionals from within that country. Starting in 2016, the\nofﬁcial Eurovision website has published the individual\ndata of each juror from each participating country. The\nsecond set of votes comes from viewers from the of the\nperforming country. The votes represent points that are\nadded together and each country can use their set of points,\n{12,10,8,7,6,5,4,3,2,1}for one and only one country.\nNo juror or television vote can be cast for one’s own coun-\ntry. In the semi-ﬁnals, voting is limited to only countries\nparticipating in their respective show, whereas in the Grand\nFinal, any country is allowed to vote. The Grand Final tele-\nvision show is also characterised by great fanfare surround-\ning each national jury’s announcement of which country\nthey chose to award ‘ douze points ’.\nNo explicit criteria are given as how any vote should be\ndecided. Said another way, it should not be assumed that all\nparticipants attempt to vote for a measure of musical quality.\nMany factors have been discussed in academic literature\non the topic, that suggest there are both geographic and\npolitical factors that can play into how countries decide to\ncast their votes [6–10].\n2. MIROVISION DATASET\nData that comprises the MIRoVision dataset originates from\nthree primary sources. The ﬁrst is the ofﬁcial Eurovison\nwebsite (https://eurovision.tv/), the second is the Eurovision\nWorld fan website (https://eurovisionworld.com), the third\nare audio features taken directly from the YouTube videos\nlinked in the contestant metadata. The dataset contains ﬁve\nprimary types of data: (1) contest meta-data; (2) contest\nresults; (3) voting data; (4) audio features extracted from\nrecorded performances of the musical acts and (5) betting\nofﬁce data. All data for each Eurovision Song Contest is\navailable each year since the year 1956 until present day\nwith the exception of 2020 when the contest was cancelled\ndue to the global COVID-19 pandemic. As of 2016, the\nofﬁcial Eurovision website has published data detailing how\neach of the ﬁve jurors from the expert panel have voted on\nall three nights of the contest. The current release of the\ndata set contains the contestant metadata, contest ranking\nand voting data of 1719 entries. The dataset is hosted on a\nGitHub repository.2\n1https://eurovision.tv/about/how-it-works\n2https://github.com/Spijkervet/eurovision-dat\nasetIn total, 56 countries are represented in the dataset,\nwhich includes countries that have been dissolved, renamed,\nor merged since the inception of the contest in 1956. V oting\ndata for the contest is stored in three tables: (1) votes; (2)\ncontestants; and (3) jurors.\nThevotes table contains data from the contest’s begin-\nning in 1956 and indicates how each country’s aggregated\njury and televoting points were distributed to each other\nparticipating country.\nThe contestants table contains all metadata regarding\neach song entry, such as the artist’s name and song title,\nlyrics, composers and lyricists, the running order and the\ntotal points awarded by the jury and televoters in the Semi-\nFinal and Final Rounds respectively. This table also in-\ncludes links to YouTube videos of live performances from\nthe televised Finals or Semi-Finals, as maintained by the\nEurovision World team.\nThejurors table contains data beginning from the year\n2016 and indicates how the ﬁve anonymous jurors (desig-\nnated with letter names A through E) voted for each other\ncountry and in which night of the contest. As noted above,\ncountries are unable to vote for themselves, are only able to\nvote within the Semi-Final they are participating in, whereas\nall countries are able to vote in the Grand Final.\nIn addition to the voting tables, the betting-ofﬁces table\nprovide tables of historical bookmakers’ odds for the contest\nwinners, as collected by Eurovision World. The Eurovision\nSong Contest is a popular target for online betting. Day-\nof-contest odds are available for 2016 and 2017, and daily\nodds up to six months prior to the contest are available from\n2018 onward, for 10 to 20 betting ofﬁces.\n3. A PREFERENCE MODEL FOR EUROVISION\nThe Eurovision Song Contest voting system is iconic, but\nbecause the number of contestants varies, it is not possible\nto use contest scores to make comparisons across years.\nMoreover, the contest scores do not operate on an inter-\nval level of measurement: even within a particular year, a\ndifference of ﬁve or ten points may mean something quite\ndifferent at the top end of the score range than it does at the\nbottom. With the rich data in the MIRoVision set, however,\nit is possible to ﬁt statistical models with parameters that\ncorrespond monotonically to actual contest results but that\ndobehave on an interval scale. Such an interval scale is\nnot only interesting musicologically and sociologically, but\nalso for machine-learning applications, as most common\nloss functions for training implicitly assume interval-scale\noutcomes. In short, we are looking for a true measure of\ncompetitiveness in the Eurovision Song Contest, and one\nthat applies stably across years.\nIn order to achieve these desiderata, the contest results\nmust be sufﬁcient statistics for the model parameters of\ninterest. If we make the stronger assumption that there be\nonly a ﬁnite number of sufﬁcient statistics beyond these,\nthen by the Pitman–Koopman–Darmois theorem [11], the\nmodel must be a member of the exponential family. That\nleaves a surprisingly small class of plausible models.\nThe simplest model requires no sufﬁcient statistics otherProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n818than the scores themselves. Under such a model, the prob-\nability of the set of scores from any particular country’s jury\nor televoters\nPr[ranking]∝exp(s1β1+s2β2+···+sNβN),(1)\nwhere the coefﬁcients sn∈ {12,10,8,7,6,5,4,3,2,1}\nare the scores awarded from that jury or televoter group to\ncontestant nand theβnare the model’s competitiveness\nparameters for contestant n. The normaliser Z0(β)for this\ndistribution is the sum of these terms for any valid assign-\nment of scores under the Eurovision system. After Mjuries\nand televote groups combine their scores independently to\ndetermine a winner, the combined probability\nPr[contest] =exp(s1β1+s2β2+···+sNβN)\nZ0(β)M,(2)\nwheres1,s2,...,s Nnow represent the total scores awar-\nded to each contestant. The trouble with this model is that\nfor a typical Eurovision show of 26 contestants, the normal-\niser contains 26P10≈19 trillion terms. The model is thus\ninfeasible in practice, despite its theoretical simplicity.\nMost alternatives to this model lose their exponential-\nfamily properties. There is, however, an interesting alternat-\nive if we are willing to consider Eurovision contest scores\nfrom juries and televoters to be ratings instead of rankings.\nSpeciﬁcally, assume that for each song, juries must award\na scores in the set {12,10,8,7,6,5,4,3,2,1,0}, but that\nthere is no restriction on how many times they can use each\nscore. While the numerator of such a model remains the\nsame as (1) and (2), its normaliser\nZ(β) =N/productdisplay\nn=1/summationdisplay\nk∈{12,10,8,7,6,5,4,3,2,1,0}exp(kβn),(3)\nwhich can be computed easily. Although there are funda-\nmental conceptual and mathematical differences between\nrankings and ratings [12], if we restrict the outcome space\nof rating model (3)to allow only outcomes that would also\nbe valid in the ranking model (2), the models are equival-\nent [13]. Moreover, we can add an extra set of score-level\nparameters ξkto allow (3)to better approximate (2)without\nsacriﬁcing equivalency on the restricted outcome space:\nPr[ratings] =exp(/summationtext\nnsnβn)·exp(/summationtext\nkξk)/producttext\nn/summationtext\nkexp(kβi+ξk),(4)\nwheresnare again the scores from a particular jury or tele-\nvoter group and k∈ {12,10,8,7,6,5,4,3,2,1,0}. This\nmodel is known in the psychometric literature as the partial-\ncredit model [14] and is one of the standard mathematical\ntools used for assessing the reliability of rubrics, Likert\nscales, and educational test items with partial credit.\n4. FITTING THE PREFERENCE MODEL\nWe ﬁt the partial-credit model (4)to the MIRoVision data\nfor all Song Contests since 1975, the year that the {12, 10,\n8, 7, 6, 5, 4, 3, 2, 1, 0} scoring system was instituted. We0100200300400500\n-2 -1 0 1\nSong Quality (cantobels)Final Eurovision Score\nFigure 1 . Correspondence between song competitiveness\n(in cantobels) and ﬁnal Eurovision Song Contest scores in\n2019. The pattern in this year is typical of all other years,\nwith a relatively slow increase in points as competitiveness\nimproves up to about 0.5 cantobels, followed by a rapid\nincrease. Because of the semi-ﬁnal rounds, the relationship\nbetween competitiveness and ﬁnal score is not a strictly\nmonotonic as in years without semi-ﬁnals, but it is still\nnearly monotonic.\nconsidered every vote available as an individual observa-\ntion: every country’s jury, every country’s televotes in years\nthat those votes were counted separately from juries, and all\nvotes from semi-ﬁnal rounds when they occurred. We made\nthe important but unavoidable assumption that the average\ncompetitiveness of a Eurovision entry has remained con-\nstant over time, as there are no cross-year comparisons that\nwould make it possible to estimate the model otherwise.\nWe ﬁt the joint probability model using the Bayesian\nprobabilistic programming language Stan, with normal pri-\nors on average country competitiveness and song competit-\niveness and a multivariate normal prior on ξfor each contest.\nThe complete model code is available in the supplemental\nmaterial. For interpretive purposes, we ﬁxed the mean of\nthe song competitiveness parameters βto 0 and report them\non a10log10scale, analogous to the decibel. In honour of\nthe singing at the contest, we deem this unit the cantobel .\nAn increase of one cantobel in song competitiveness means\nthat a song improves its chances of receiving one extra point\nfrom any given jury by 101\n10≈1.26. Like the decibel scale,\nan increase of 3 cantobels means that a song approximately\ndoubles its chances of receiving one extra point.\nFigure 1 illustrates the typical correspondence between\ncompetitiveness in cantobels and actual song contest res-\nults. After a slow increase, the slope rapidly increases for\nhighly competitive entries. The Eurovision Song Contest\nscoring system compresses differences between relatively\nuncompetitive entries and dramatically exaggerates small\ndifferences at the top. While this surely contributes to the\nexciting television, cantobels are a better scale to use for\nscientiﬁc purposes.\nFigures 2 and 3 reveal the heart of the model. The ﬁrst\nshows the average song quality, as perceived by the Eurovi-\nsion Song Contest juries and televoters, over the period from\n1975 to 2022. Ukraine, Russia, Italy, and Sweden standProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n819BECH\nDEESFIFRGB\nIE\nILIT\nLU\nMCMT\nNOPTSE\nTRYUAT\nBECH\nDE\nESFIFR\nGRIEIL\nIT\nLUMC\nNL\nNOPT\nYUATBECH\nDE\nES\nFIGB\nGRIE\nIL\nIT\nLUMC\nNL\nNO\nPT\nSEATBE\nCHDE\nDKES\nFIFR\nGBGRIE\nITLUMC\nNL\nNOPTSE\nTRATBECHDE\nDKES\nFIFR\nGB\nGRIE\nITLU\nMCNLNOPT\nSEAT\nBECHDE\nDKES\nFIFRGB\nGRIT\nLU\nMANL\nNOPT\nSE\nTR\nATBECH\nCYDE\nDK\nES\nFIFR\nGRIE\nIL\nLUNL\nNOPTSE\nTRYUATBECH\nCY\nDKES\nFIGB\nIEIL\nLU NL\nNO\nPTSE\nTRYUAT\nBECH\nCYDE\nDK\nESFIFRGB\nGRIL\nITNL\nNO\nPTSE\nTRYU\nATBE\nCHCYDEDKES\nFIFRGBIE\nIT\nLUNL\nNOPTTR\nYUAT\nBECH\nCYDE\nDK\nESFIFRGB\nGRIEIL\nIT\nLU\nPTSE\nTR\nATCH\nCYDEDK\nES\nFI\nFRGBIE\nILISLU NL\nNO\nPTSE\nTR\nYU\nATBE\nCHCYDE\nDK\nESFIFRGBGRIL\nISIT\nLUNL\nNO\nPTSE\nTRYU\nATBEDEDK\nES\nFIFRGB\nGRIEIL\nISITLU NL NO\nPTSE\nTRYUAT\nBECHCY\nDEDK\nES\nFI\nFRGB\nGR\nIEIL\nISIT\nLUNL\nNOPTSE\nTRAT\nBECH\nCYDEDKES\nFIFR\nGB\nGRIE\nILIS\nLU NL\nNOPTSE\nTRYU\nATBECH\nCY\nDE\nDKES\nFIGB\nGRIEIL\nISIT\nLUMT\nNOPT\nTR\nYUAT\nBECHCY\nDEDK\nES\nFIFRGB\nGR\nIL\nISIT\nLUMT\nNL\nNOPT\nSETRYU\nAT\nBA\nBECH\nCYDE\nDKES\nFIFRGB\nGR\nHR\nILISIT\nLUMTNLNO\nPTSE\nSITRATBA\nCHCYDE\nEEES\nFIFR\nGB\nGR\nHRHU\nIS\nLTMT\nNLNOPL\nPT\nRORU\nSE\nSKAT\nBA\nBECY\nDEDKES\nFR\nGB\nGRHR\nHUIEIL\nISMT\nPL\nPTRUSE\nSI\nTRAT\nBABECHCYEE\nES\nFIFRGB\nGRHR\nISMTNLNO\nPLPTSE\nSISKTR\nATBA\nCHCY\nDEDKEEESFR\nGR\nHRHUIE IS\nIT\nMT\nNL\nNOPL\nPTRUSESITR BE\nCHCYDE\nEE\nESFI\nFRGB\nGRHR\nHUIE\nMKMT\nNL\nNO\nPLPT\nROSE\nSI\nSKTRATBA\nBE\nCYDE\nDKEE\nESFRGBHR\nIEILIS\nLTMTNL\nNO\nPL\nPTSI\nTRAT\nBECH\nCYDEEE\nES\nFI\nFRGBHRIE\nILISLV\nMKMT\nNLNO\nRORU\nSE\nTR\nBADEDK\nESFR\nGBGR\nHR\nIEIL\nISLT\nLVMT\nNL\nNOPLPTRUSE\nSI\nTR\nATBABE\nCHCY\nDE\nDKEE\nES\nFIFRGB\nGRHR\nIL\nLTMKMT\nRO\nRUSE\nSI\nTRAT\nBABE\nCYDE\nEEES\nFR\nGBGRHRIE\nILIS\nLVMTNLNO\nPL\nPTRORU\nSE\nSIUA\nADAL\nATBA\nBE\nBY\nCHCS CY\nDE\nDKEEES\nFIFR\nGBGR\nHR\nIEIL\nISLT\nLV\nMCMKMTNL\nNOPL\nPT\nRORUSE\nSITR\nADAL\nATBA\nBEBGBYCHCS CY\nDEDK\nEEESFI\nFRGBHR\nHU\nIEIL\nIS\nLTLV\nMCMD\nMKMT\nNLNO\nPL\nPTRO\nRU\nSESITR\nUA\nADALAMBA\nBE\nBG\nBYCHCY\nDE\nDKEE\nES\nFRGBGR\nHRIE\nILISLT\nLV\nMCMDMK\nMTNLNOPL\nPTRORU\nSE\nSITRUA\nAD\nALAM\nATBA\nBEBGBY\nCHCY\nCZDE\nDK\nEEESFI\nFRGBGEGR\nHRHU\nIEILIS\nLTLV\nMD\nMEMK\nMTNLNOPLPTRORU\nSESITRUA\nADALAM\nAZ\nBA\nBEBG\nBYCH\nCY\nCZDEDK\nEEESFI\nFR\nGBGEGR\nHR\nHUIEIL\nIS\nLTLV\nMD\nMEMK\nMT\nNLNO\nPLPT\nRORS\nSE\nSI\nSMTRUA\nADALAMAZ\nBA\nBEBGBY\nCHCY\nCZDEDKEE\nESFIFRGB\nGR\nHR\nHUIEILIS\nLT\nLVMD\nMEMKMT\nNLPLPT\nRORSRU\nSE\nSI\nSKTR\nUA\nALAMAZ\nBABE\nBGBY\nCHCYDK\nEEES\nFIFR\nGBGEGR\nHRIEILIS\nLT\nLVMD\nMKMT\nNLNO\nPLPTRO\nRSRU\nSE\nSISKTR\nUA\nALAMATBA\nBE\nBG\nBY\nCH\nCYDEDK\nEEESFIFRGB\nGEGR\nHRHUIE\nILISIT\nLT\nLVMD\nMKMT\nNLNO\nPLPTRO\nRS\nRUSE\nSI\nSK\nSMTRUAAL\nATAZ\nBA\nBEBG\nBYCHCYDE\nDKEE\nES\nFIFR\nGBGEGR\nHR\nHUIE\nILISIT\nLT\nLVMD\nMEMK\nMT\nNL\nNOPTRORSRU\nSI\nSKSMTR\nUA\nALAM\nATAZ\nBE\nBGBY\nCH\nCYDEEE\nESFI\nFRGBGEGR\nHRHU\nIEILISIT\nLT\nLVMD\nME\nMKMT\nNLNO\nRO\nRSRU\nSE\nSISMUA\nALAM\nAZ\nBEBYCH\nDEDK\nEEESFI\nFRGB\nGEGRHU\nIE\nILIS\nIT\nLTLV\nMDME\nMKMTNL\nNO\nPL\nPTRO\nRUSE\nSISMUA\nALAM\nATAU\nAZBE\nBY\nCHCY\nCZ\nDEDKEE\nES\nFIFRGBGE\nGR\nHU\nIEIL\nISIT\nLTLV\nMDME\nMKMT\nNLNO\nPL\nPTRO\nRSRU\nSI\nSMALAM\nATAU\nAZ\nBABEBG\nBY\nCHCY\nCZ\nDEDK\nEEES\nFIFR\nGBGE\nGRHRHU\nIEIL\nISITLT\nLV\nMDMEMKMTNL\nNOPL\nRSRU\nSE\nSI\nSMALAM\nATAU\nAZBEBG\nBY\nCHCY\nCZ\nDEDK\nEE\nESFIFR\nGB\nGEGRHRHU\nIEIL\nISIT\nLT\nLVMD\nMEMK\nMTNLNO\nPLRO\nRSSE\nSI\nSMUAAL\nAMAT\nAU\nAZ\nBEBG\nBYCHCY\nCZDE\nDKEE\nESFIFR\nGB\nGEGR\nHRHUIE ISIT\nLT\nLVMD\nME\nMKMTNLNO\nPL\nPTRORS\nRUSE\nSI\nSMUA\nAL\nAM\nATAUAZ\nBEBYCH\nCYCZ\nDEDKEE\nES\nFIFR\nGBGEGR\nHR\nHU\nIEILISIT\nLT\nLVMD\nMEMK\nMTNO\nPL\nPTRORSRUSE\nSI\nSM\nAL\nAT\nAUAZ\nBEBGCH\nCY\nCZ\nDEDK\nEE\nESFIFR\nGBGEGR\nHR\nIEILIS\nLT\nLVMD\nMKMT\nNLNO\nPLPT\nRORSRU\nSE\nSISMUA\nALAM\nATAU\nAZBE\nBGCH\nCYCZ\nDEDKEEES\nFI\nFRGB\nGEGR\nHR\nIEILISIT\nLT\nLVMD\nMEMK\nMTNL\nNO\nPLPT\nRORSSE\nSISMNLGB\nFRIL\nILIE\nGBDE\nLUSE\nNOBE\nIE\nCH\nYUITFRSE IEIEIE\nNOIEGB\nIL SEDKEE\nLV\nTRUA\nGRFI\nRS\nRUNO\nDE\nAZSE\nDKATSE\nUAPT\nILNLITUA\n–3–2–10123\n1980 1990 2000 2010 2020aa\naaaa\naaaa Northern EuropeEastern EuropeSouthern EuropeWestern EuropeNon-European\nFigure 2 . Historical competitiveness of Eurovision Song Contest entries (in cantobels). Countries are coloured by their geographic region as deﬁned in the United Nations M49\nstandard. Winners are boxed. The standard error of estimates is roughly 0.5 cantobel in early years and roughly 0.3 after the institution of semi-ﬁnal rounds in 2004; as such, difference\nof approximately 1.0 cantobels are likely statistically signiﬁcant. After a period when Northern and Western Europe exchanged victories, there was a period of Northern European\ndominance; recent years have been characterised by a good geographic diversity of winners.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n820ADALAM\nATAUAZ\nBA\nBEBG\nBYCHCS\nCY\nCZDEDK\nEE\nES\nFIFRGB\nGEGR\nHR\nHUIE\nIL\nISIT\nLTLU\nLVMAMCMD\nMEMKMT\nNL\nNO\nPL\nPTRORSRU\nSE\nSI\nSK\nSMTRUA\nYU\n–1.0 0.0 1.0a\na\na\na\naNorthern Europe\nEastern Europe\nSouthern Europe\nWestern Europe\nNon-European\nFigure 3 . Median competitiveness of countries’ Eurovi-\nsion Song Contest entries, 1975–2022, in cantobels with\n90% credible intervals. Countries are coloured by their\ngeographic region as deﬁned in the United Nations M49\nstandard. Ukraine, Russia, Italy, and Sweden stand out\nas having sent contestants of exceptional competitiveness,\nalthough Azerbaijan, the United Kingdom, and Greece’s\ncredible intervals are also strictly greater than zero.\nout as having been particularly successful, even though\nthey have suffered almost-wins instead of victories in many\nyears. On average, songs from these countries have been a\nhalf cantobel above the average. But the ﬁrst ﬁgure shows\nthat there are dramatic swings from year to year underneath\nthese averages. Even one the most convincing victories\nfrom one of the historically strongest countries – Måns\nZelmerlöw’s ‘Heroes’, Sweden’s 2015 entry – was preceded\nand succeeded by much less appreciated acts.\n4.1 Jury Model\nJury scores at the Eurovision Song Contest are determined\nby combining rankings from ﬁve independent jurors from\neach country, each of whom must make a complete ranking\nof contestants at a show, from best to worst. After aver-\naging these ranks, they are converted to the better-known\n{12,10,8,7,6,5,4,3,2,1,0}system that is reported on\ntelevision. Since 2016, the European Broadcasting Union024681012\n1 6 11 16 21 26\nRankIdeal Score\nFigure 4 . Ideal scores for averaging ranks within juries,\naccording to a generalised partial-credit model, with 90%\ncredible intervals. In recent years, the Eurovision Song\nContest has used an exponential weighting scheme, but\nthese results suggest that a linear scheme with a small bonus\nfor the top-ranked entry would be sufﬁcient.\nhas made not only the ﬁnal scores but also these individual\nrankings public. They have also publicised that they con-\ntinue to experiment with the proper way to average the\nranks across jurors, currently using exponential decay.3\nThe theory of partial-credit models offers an alternative,\nmore empirical solution. Rather than taking the scoring rule\nin(4)as ﬁxed, the generalised partial-credit model con-\nsiders an optimal scoring rule that would lead the model to\nmake the best predictions. Concretely, that would mean con-\nsidering alternatives to the {12,10,8,7,6,5,4,3,2,1,0}\nrule for the main contest, and by extension, to the simpler\n{1,2,...,N}rule for jury members making a full ranking.\nThe MIRoVision dataset includes these jury scores, and\nwe ﬁt a generalised partial-credit model to them analogous\nto the model we ﬁt for the contest overall. The code is\navailable in the supplemental material. Figure 4 shows the\nresults. Like the European Broadcasting Union’s current\nrule, we arbitrarily ﬁx the maximum score to 12. It seems\nthat rather than replacing the former linear scheme with the\ncurrent exponential one may be a more effective simply to\ngive a small ﬁxed bonus to each juror’s top-ranked entry.\nSuch a solution would also solve the core issue motivating\nthe exponential weighting, namely that it it undesirable for\none juror to have unilateral power to spoil the chances of\nsome other juror’s favourite.\n5. PREDICTING WINNERS\nThe Eurovision Song Contest is also notorious for attracting\nonline and ofﬂine bets on the outcome. Since 2015, the\nEurovisionWorld web site has been collecting the odds\nposted at a large number of online betting ofﬁces, for each\nday leading up to the contest. These odds can be converted\ninto implicit probabilities of winning, and there is often\nmuch discussion in the weeks leading up to the contest\nabout which acts the bookmakers are favouring.\n3https://eurovision.tv/story/subtle-significa\nnt-ebu-changes-weight-individual-jury-rankingsProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n821Year Country Actual Bookmakers\n2018 Israel .87 .24\n2018 Cyprus .12 .37\n2018 Germany .01 .09\n2019 Netherlands .53 .51\n2019 Italy .45 .09\n2019 Switzerland .01 .09\n2019 Russia .01 .02\n2021 Italy .63 .26\n2021 France .35 .22\n2021 Switzerland .02 .05\n2022 Ukraine .98 .62\n2022 Sweden .01 .14\n2022 United Kingdom .01 .06\n2022 Spain .01 .06\nTable 1 . Probability of winning the Eurovision Song Con-\ntest, 2018–2022, given the partial-credit model and perfect\ninformation about jurors’ and televoters’ preferences, com-\npared to bookmakers’ implied win probabilities immedi-\nately prior to the contest ﬁnal.\nWe can use our model ﬁts to compare the bookmakers’\npredictions to the actual probabilities countries had to win\ngiven jurors’ and televoters’ preferences and the assump-\ntions of the partial-credit model. To compute these probab-\nilities, we reshufﬂed the draws from our Bayesian samples\nindependently for each country and tallied how often these\nwould have been the highest, taking advantage of the fact\nthat competitiveness in cantobels is a sufﬁcient statistics for\nactual contest outcomes. Table 1 presents the results. Both\n2019 and 2021 were rather close contests, whereas 2018 and\n2022 had clearer frontrunners. The bookmakers markedly\nmis-called 2018, but have been more accurate since. If one\nhad been able to place stakes at the online betting ofﬁces\nwith perfect knowledge of the jurors’ and televoters’ prefer-\nences, one would have quadrupled one’s stake on average\n(before paying out the bookmakers’ sometimes shockingly\nhigh margins on Eurovision odds).\n6. CONTENT-BASED CONTEST PREDICTIONS\nPerfect information is of course never available, but per-\nhaps deep learning and content-based MIR offer some-\nthing? Self-supervised music representation learning has\nadvanced considerably in recent years. It has successfully\nbeen applied to many downstream tasks, including music\ntagging [16], genre classiﬁcation, key detection and emo-\ntion recognition [17, 18]. These foundation models are\ngenerally pre-trained in an unsupervised, end-to-end fash-\nion on raw audio samples. By deﬁning an auxiliary loss\nobjective on large quantities of music and using data per-\nturbations, models are able to learn effective and robust\nrepresentations.\nTo evaluate whether a pre-trained foundation model is\nable to predict preferences, we extracted embeddings on all\nsong entries using the TUNe+ [19] and MERT [18] models.\nOn every window of 2 seconds, an embedding vector of 512\nfeature dimensions is computed for the TUNe+ model. TheModel L1 L2\nTUNe+ [19] 0.828 (0.039) 1.063 (0.052)\nMERT [18] 0.820 (0.019) 1.025 (0.027)\nTable 2 . L1 (MAE) and L2 (RMSE) losses and their stand-\nard deviations after training two state-of-the-art audio em-\nbeddings to predict the competitiveness of Eurovision Song\nContest entries from 1975–2022, in cantobels.\nMERT model returns 25 representation layers, and 1024\nfeature dimensions on 5-second windows. For every song\nentry between 1975 and 2022, a single embedding vector\nis calculated by taking the arithmetic mean along the time\ndimension for TUNe+ and along the representation layers\nfor MERT respectively. This results in 1 261 embeddings\nin total. For every song entry, we took 4 000 draws from\nthe ﬁtted model for song competitiveness (in cantobels) and\ntreated these as our targets Y; using 4 000 draws instead of a\nsingle point estimate more accurately averages over our un-\ncertainty about song competitiveness, given the inherently\nlimited number of rankings available for any single edition\nof the contest. We freeze the pre-trained TUNe+ and MERT\nmodels and perform a linear probe using the mean-squared\nerror between (ˆy,y). We use 5-fold cross-validation and\nsample all song entries from two years within each decade\nbetween 1975 and 2023 as our validation set.\nOur results in Table 2 show that we can achieve RMSE of\n1.025 cantobels by way of training a linear layer on embed-\ndings extracted from a pre-trained foundation model. These\nmodels are not speciﬁcally trained or designed for our down-\nstream task of preference prediction, e.g., features extracted\nby the different layers in MERT vary in their downstream\ntask performance, and we leave further improvements to\nfuture work. But to contextualise the result, the overall\nstandard deviation of our Eurovision competitiveness rat-\nings is 1.064 cantobels, which means that state-of-the-art\nMIR audio embeddings are able to predict 7.2% of the\nvariance in Eurovision Song Contest competitiveness.\n7. CONCLUSION\nWe present MIRoVision, a collection of data and tools for\nstudying the Eurovision Song Contest and applying music\ninformation retrieval to several types of data generated from\nthe contest. One of our key results is a model for converting\nthe highly non-linear contest scores into a well-behaved\ninterval-scale measurement we dub the cantobel . Cantobels\nfacilitate understanding of ﬂuctuations in the contest over\ntime and more accurately represent both the competitive-\nness and the uncertainty surrounding the competitiveness of\nEurovision Song Contest entries. They also behave better\nwith the standard loss functions used in machine learning\nsystems, and allow us to predict a small but meaningful\nportion of variance in contest outcomes. We hope this result\nis sufﬁciently tantalising to encourage the community to try\ntheir own models – the Eurovision Song Contest offers a\nfresh set of contestants every year – and to ﬁnd their own\ncreative uses for this rich musicological data source.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8228. REFERENCES\n[1]“How the Eurovision Song Contest works,” Jul 2022.\n[Online]. Available: https://eurovision.tv/about/how-i\nt-works\n[2]C. Baker, “Wild dances and dying wolves: Simulation,\nessentialization, and national identity at the Eurovision\nSong Contest,” Popular Communication , vol. 6, no. 3,\npp. 173–189, 2008.\n[3]J. K. O’Connor, The Eurovision Song Contest: The\nOfﬁcial History . Carlton, 2010.\n[4]C. Baker, “The gay olympics? the Eurovision Song\nContest and the politics of LGBT/European belonging,”\nEuropean Journal of International Relations , vol. 23,\nno. 1, pp. 97–121, 2017.\n[5]I. Wolther, “More than just music: The seven dimen-\nsions of the Eurovision Song Contest,” Popular Music ,\nvol. 31, no. 1, pp. 165–171, 2012.\n[6]G. Yair, “‘Unite Unite Europe’: The political and cul-\ntural structures of europe as reﬂected in the Eurovi-\nsion Song Contest,” Social Networks , vol. 17, no. 2, pp.\n147–161, 1995.\n[7]G. Yair and D. Maman, “The persistent structure of\nhegemony in the Eurovision Song Contest,” Acta Soci-\nologica , vol. 39, no. 3, pp. 309–325, 1996.\n[8]D. Fenn, O. Suleman, J. Efstathiou, and N. F. John-\nson, “How does Europe make its mind up? Connec-\ntions, cliques, and compatibility between countries in\nthe Eurovision Song Contest,” Physica A , vol. 360, pp.\n576–598, 2006.\n[9]V . Ginsburgh and A. G. Noury, “The Eurovision Song\nContest: Is voting political or cultural?” European\nJournal of Political Economy , vol. 24, no. 1, pp. 41–52,\n2008.\n[10] M. Blangiardo and G. Baio, “Evidence of bias in the\nEurovision Song Contest: Modelling the votes using\nBayesian hierarchical models,” Journal of Applied Stat-\nistics , vol. 41, no. 10, pp. 2312–2322, 2014.\n[11] B. O. Koopman, “On distributions admitting a sufﬁcient\nstatistic,” Transactions of the American Mathematical\nSociety , vol. 19, pp. 399–409, 1936.\n[12] S. J. Brams and P. C. Fishburn, “V oting procedures,” in\nHandbook of Social Choice and Welfare , K. J. Arrow,\nA. Sen, and K. Suzumura, Eds. Elsevier, 2002, vol. 1,\npp. 173–236.\n[13] D. Andrich, “Understanding the response structure and\nprocess in the polytomous Rasch model,” in Handbook\nof Polytomous Item Response Theory Models , M. L.\nNering and R. Ostini, Eds. New York: Routledge,\n2010, pp. 123–152.[14] G. N. Masters, “A Rasch model for partial credit scor-\ning,” Psychometrika , vol. 47, no. 2, pp. 149–174, 1982.\n[15] United Nations Statistics Division, “Standard country\narea codes for statistical use (M49),” 2021. [Online].\nAvailable: https://unstats.un.org/unsd/methodology/\nm49/\n[16] J. Spijkervet and J. A. Burgoyne, “Contrastive learning\nof musical representations,” in Proceedings of the 22nd\nSociety for Music Information Retrieval Conference ,\n2021.\n[17] R. Castellon, C. Donahue, and P. Liang, “Codiﬁed au-\ndio language modeling learns useful representations for\nmusic information retrieval,” Proceedings of the 22nd\nSociety for Music Information Retrieval Conference ,\n2021.\n[18] Y . Li, R. Yuan, G. Zhang, Y . Ma, C. Lin, X. Chen,\nA. Ragni, H. Yin, Z. Hu, H. He et al. , “Large-scale\npretrained model for self-supervised music audio rep-\nresentation learning,” Presentation at the Digital Music\nResearch Network, 2022.\n[19] M. A. Vélez Vásquez and J. A. Burgoyne, “Tailed U-\nnet: Multi-scale music representation learning,” in Pro-\nceedings of the 23rd International Society for Music\nInformation Retrieval Conference , 2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n823"
    },
    {
        "title": "Passage Summarization With Recurrent Models for Audio - Sheet Music Retrieval.",
        "author": [
            "Luís Carvalho",
            "Gerhard Widmer"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265385",
        "url": "https://doi.org/10.5281/zenodo.10265385",
        "ee": "https://zenodo.org/records/10265385/files/000083.pdf",
        "abstract": "Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo deviations. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio - sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet music. We conduct a number of experiments on synthetic and real piano data and scores, showing that our proposed recurrent method leads to more accurate retrieval in all possible configurations.",
        "zenodo_id": 10265385,
        "dblp_key": "conf/ismir/CarvalhoW23",
        "keywords": [
            "cross-modal music retrieval",
            "joint embedding space",
            "deep neural networks",
            "audio and sheet music",
            "tempo deviations",
            "recurrent network",
            "longer passages",
            "weakly aligned data",
            "non-linearities",
            "accurate retrieval"
        ],
        "content": "PASSAGE SUMMARIZATION WITH RECURRENT MODELS\nFOR AUDIO – SHEET MUSIC RETRIEV AL\nLuís Carvalho1Gerhard Widmer1,2\n1Institute of Computational Perception &2LIT Artiﬁcial Intelligence Lab\nJohannes Kepler University Linz, Austria\n{luis.carvalho, gerhard.widmer}@jku.at\nABSTRACT\nMany applications of cross-modal music retrieval are re-\nlated to connecting sheet music images to audio record-\nings. A typical and recent approach to this is to learn, via\ndeep neural networks, a joint embedding space that corre-\nlates short ﬁxed-size snippets of audio and sheet music by\nmeans of an appropriate similarity structure. However, two\nchallenges that arise out of this strategy are the requirement\nof strongly aligned data to train the networks, and the in-\nherent discrepancies of musical content between audio and\nsheet music snippets caused by local and global tempo dif-\nferences. In this paper, we address these two shortcomings\nby designing a cross-modal recurrent network that learns\njoint embeddings that can summarize longer passages of\ncorresponding audio and sheet music. The beneﬁts of our\nmethod are that it only requires weakly aligned audio –\nsheet music pairs, as well as that the recurrent network\nhandles the non-linearities caused by tempo variations be-\ntween audio and sheet music. We conduct a number of\nexperiments on synthetic and real piano data and scores,\nshowing that our proposed recurrent method leads to more\naccurate retrieval in all possible conﬁgurations.\n1. INTRODUCTION\nThe abundance of music-related content in various dig-\nital formats, including studio and live audio recordings,\nscanned sheet music, and metadata, among others, calls\nfor efﬁcient technologies for cross-linking between docu-\nments of different modalities. In this work, we explore a\ncross-modal task referred to as audio – sheet music passage\nretrieval. We deﬁne it as follows: given an audio fragment\nas a query, search within an image database and retrieve the\ncorresponding sheet music passage; or vice versa, ﬁnd the\nappropriate recording fragment given a query in the form\nof some snippet of (scanned) sheet music.\nA fundamental step in audio–sheet music retrieval con-\ncerns deﬁning a suitable shared representation that per-\nmits the comparison between items of different modalities\n© L. Carvalho and G. Widmer. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: L. Carvalho and G. Widmer, “Passage summarization with recur-\nrent models for Audio – Sheet Music Retrieval”, in Proc. of the 24th Int.\nSociety for Music Information Retrieval Conf., Milan, Italy, 2023.\nFigure 1 : Distribution of system durations in around\n40,000 examples from the MSMD. More than 25% of the\npassages are longer than ten seconds.\nin a convenient and effective way. The conventional ap-\nproaches for linking audio recordings to their respective\nprinted scores are based on handcrafted mid-level repre-\nsentations [1, 2]. These are usually pitch-class proﬁles,\nlike chroma-based features [3,4], symbolic ﬁngerprints [5],\nor the bootleg score [6, 7], which is a coarse mid-level\ncodiﬁcation of the main note-heads in a sheet music im-\nage. However extracting such representations requires a\nseries of pre-processing stages that are prone to errors,\nfor example optical music recognition on the sheet mu-\nsic side [8–10], and automatic music transcription on the\naudio part [11–13].\nA promising approach [14, 15] has been proposed to\neliminate these problematic pre-processing steps by learn-\ning a shared low-dimensional embedding space directly\nfrom audio recordings and printed scores. This is achieved\nby optimizing a cross-modal convolutional network (CNN)\nto project short snippets of audio and sheet music onto a la-\ntent space, in which the cosine distances between semanti-\ncally related snippets are minimized, whereas non-related\nitems of either modality are projected far from each other.\nThen the retrieval procedure is reduced to simple nearest-\nneighbour search in the shared embedding space, which is\na simple and fast algorithm.\nA ﬁrst limitation of this strategy relates to its super-\nvised nature: it requires strongly-aligned data in order to\ngenerate matching audio–sheet snippet pairs for training,\nwhich means ﬁne-grained mappings between note onsets\nand corresponding note positions in the score. Obtaining\nsuch annotations is tedious and time-consuming, and also700Figure 2 : Diagram of the proposed network. Two independent pathways are trained to encode sheet music (a) and audio\n(b) passages by minimizing a contrastive loss function (c).\nrequires specialized annotators with musical training. As\na result, embedding learning approaches have been trained\nwith synthetic data, in which recordings, sheet music im-\nages, and their respective alignments are rendered from\nsymbolic scores. This leads to poor generalization in sce-\nnarios with real music data, as shown in [16].\nMoreover, the snippets in both modalities have to be\nﬁxed in size, meaning that the amount of actual musical\ncontent in the fragments can vary considerably depend-\ning on note durations and the tempo in which the piece\nis played. For example, a sheet excerpt with longer notes\nplayed slowly would correspond to a considerably larger\nduration in audio than one with short notes and a faster\ntempo. This leads to generalization problems caused by\ndifferences between what the model sees during training\nand test time; [17] attempted to address this limitation by\nintroducing a soft-attention mechanism to the network.\nIn this paper we address the two aforementioned lim-\nitations by proposing a recurrent cross-modal network\nthat learns compact, ﬁxed-size representations from longer\nvariable-length fragments of audio and sheet music. By\nremoving the ﬁxed-size fragment constraint, we can ad-\njust the lengths of fragments during training so that cross-\nmodal pairs can span the same music content, leading\nto a more robust representation. Moreover, by operating\nwith longer music passages, it is possible to rely solely on\nweakly-annotated data for training, since we now require\nonly the starting and ending positions of longer-context\nmusic fragments within music documents, in order to ex-\ntract audio–sheet passages to prepare a train set. This is a\nremarkable advantage compared for example to other ap-\nproaches based on [14], where ﬁne-detailed alignments are\nindispensable to generate short audio–sheet snippet pairs.\nThe rest of the paper is structured as follows. In Sec-\ntion 2 we describe the model proposed to learn joint repre-sentations from cross-modal passages. Section 3 presents\na series of experiments on artiﬁcial and real data and Sec-\ntion 4 summarizes and concludes the work.\n2. AUDIO–SHEET PASSAGE RETRIEV AL\nFor the purposes of this paper, and in order to be able to\nuse our annotated corpora for the experiments, we deﬁne\na \"passage\" as the musical content corresponding to one\nline of sheet music (also known as a \"system\"). System-\nlevel annotation of scores are much easier to come by than\nnote-precise score-recording alignments, making it rela-\ntively easy to compile large collections of training data for\nour approach. Our deﬁnition of passages resembles that\nof \"musical themes\", which has been used under a cross-\nmodal retrieval scenario with symbolic queries in a num-\nber of previous works [18, 19]. To illustrate the temporal\ndiscrepancies between passages, we show in Figure 1 the\ndistribution of time duration of the systems from all pieces\nof the MSMD dataset [14] (later we will elaborate more\non this database). In this dataset, we observe that systems\ncan cover from less than ﬁve to more than 25 seconds of\nmusical audio.\nThis important temporal aspect motivates us to propose\nthe network depicted in Figure 2 to learn a common latent\nrepresentation from pairs of audio–sheet passages. The\narchitecture has two independent recurrent-convolutional\npathways, which are responsible for encoding sheet music\n(Figure 2a) and audio (Figure 2b) passages. The key com-\nponent of this approach is the introduction of two recurrent\nlayers that, inspired by traditional sequence-to-sequence\nmodels [22], are trained to summarize a variable-length se-\nquences into context vectors, that we conveniently refer to\nas embedding vectors.\nDeﬁning a pair of corresponding passages in the\nform of image (sheet music) and log-magnitude spectro-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n701Audio CNN encoder Sheet-Image CNN encoder\ninput:92×20 input:160×180\n2x Conv(3, pad-1)-24- BN 2x Conv( 3, pad-1)-24- BN\nMaxPooling(2) MaxPooling(2)\n2x Conv(3, pad-1)-48- BN 2x Conv( 3, pad-1)-48- BN\nMaxPooling(2) MaxPooling(2)\n2x Conv(3, pad-1)-96- BN 2x Conv( 3, pad-1)-96- BN\nMaxPooling(2) MaxPooling(2)\n2x Conv(3, pad-1)-96- BN 2x Conv( 3, pad-1)-96- BN\nMaxPooling(2) MaxPooling(2)\nConv(1, pad-0)-32- BN Conv( 1, pad-0)-32- BN\nFC(32) FC( 32)\nTable 1 : Overview of the two convolutinal encoders. Each\nside is responsible for their respective modality. Conv( 3,\npad-1)-24: 3×3 convolution, 24 feature maps and zero-\npadding of 1. BN: Batch normalization [20]. We use ELU\nactivation functions [21] after all convolutional and fully-\nconnected layers.\ngram (audio) as XandY, respectively, two sequences\n(x1,x2,...,xN)and(y1,y2,...,yM)are generated by\nsequentially cutting out short snippets from XandY. The\nshapes of the short sheet and audio snippets are respec-\ntively160×180(pixels)1and92×20(frequency bins ×\nframes), which corresponds to one second of audio. After\nthat, each individual snippet is encoded by a VGG-style\nCNN [23] into a 32-dimensional vector, as shown in Fig-\nure 2, generating two sequences of encoded snippets, one\nfor the audio passage, and the other for the sheet passage\n(note that each modality has its own dedicated CNN en-\ncoder). The architecture of the CNN encoders are detailed\nin Table 1.\nThen each sequence is fed to a recurrent layer in order\nto learn the spatial and temporal relations between subse-\nquent snippets, which are inherent in music. After exper-\nimenting with two typical simple recurrent layers, namely\nlong short-term memory cells (LSTM) [24] and gated re-\ncurrent units (GRU) [25], we observed on average better\nresults with GRUs, and we decided for the latter for our\narchitecture. Each of the two GRUs is designed with 128\nhidden units, where the hidden state of each GRU after\nthe last step is the context vector that summarizes the pas-\nsages. Finally a fully connected layer (FC) is applied over\neach context vector, in order to encode the ﬁnal passage\nembeddings (xemb,yemb)with the desired dimension.\nDuring training, a triplet (contrastive) loss function [26]\nis used to minimize the distances between embeddings\nfrom corresponding passages of audio and sheet music and\nmaximize the distance between non-corresponding ones.\nDeﬁning d(·)as the cosine distance, the loss function is\ngiven by:\nL=K/summationdisplay\nk=1max/braceleftBig\n0,α+d/parenleftBig\nxemb,yemb/parenrightBig\n−d/parenleftBig\nxemb,yk\nemb/parenrightBig/bracerightBig\n,\n(1)\nwhereyk\nembfork∈1,2,...,K are contrastive (nega-\ntive) examples from Knon-matching passages in the same\n1In our approach, all sheet music pages are initially re-scaled to a\n1181×835resolutiontraining mini-batch. This contrastive loss is applied to\nall(xemb,yemb)pairs within each mini-batch iteration.\nThe margin parameter α∈R+, in combination with the\nmax{·}function, penalizes matching snippets that were\npoorly embedded.\nFor the sake of simplicity, we leave the remaining de-\ntails concerning the design of the networks, such as learn-\ning hyper-parameters, to our repository where our method\nwill be made publicly available,2as well as the trained\nmodels derived in this work.\n3. EXPERIMENTS\nIn this section we conduct experiments on different audio–\nsheet music scenarios. We ﬁrst elaborate on the main\ndataset used for training and evaluation and deﬁne the steps\nof the passage retrieval task. Then we select four experi-\nment setups and present the results.\nWe train our models with the Multi-Modal Sheet Music\nDataset (MSMD) [14], which is a collection of classical\npiano pieces with multifaceted data, including score sheets\n(PDF) engraved via Lilypond3and corresponding audio\nrecordings rendered from MIDI with several types of pi-\nano soundfonts. With over 400 pieces from over 50 com-\nposers, including Bach, Beethoven and Schubert, and cov-\nering more than 15 hours of audio, the MSMD has audio–\nsheet music alignments which allow us to obtain corre-\nsponding cross-modal pairs of musical passages. From\nthe MSMD we were able to derive roughly 5,000 audio–\nsheet passages for training, which is scaled up to around\n40,000 different pairs after data augmentation: audios are\nre-rendered with different soundfonts and have their tempo\nchanged between 90% and 110%. Then we generate a test\nset of 534 pairs from a separate set of music pieces, that\nwere rendered with a soundfont that was not seen during\ntraining. Later, in 3.2, we will also consider real scanned\nscores and real audio recordings.\nTo perform cross-modal passage retrieval, we ﬁrst em-\nbed all audio–sheet pairs in the shared space using our\ntrained model depicted in Figure 2. Then the retrieval\nis conducted by using the cosine distance and nearest-\nneighbor search within the space. For example, in case\nof using an audio passage as a query to ﬁnd the appropri-\nate sheet music fragment, the pairwise cosine distances be-\ntween the query embedding and all the sheet music passage\nembeddings are computed. Finally, the retrieval results are\nobtained by means of a ranked list through sorting the dis-\ntances in ascending order.\nAs for evaluation metrics, we look at the Recall@k\n(R@k), Mean Reciprocal Rank (MRR) and the Median\nRank (MR). The R@k measures the ratio of queries which\nwere correctly retrieved within the top kresults. The MRR\nis deﬁned as the average value of the reciprocal rank over\nall queries. MR is the median position of the correct match\nin the ranked list.\n2https://github.com/luisfvc/lcasr\n3http://www.lilypond.orgProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n702Figure 3 : Mean Reciprocal Rank (MRR) for different em-\nbedding dimensions, evaluated in both search directions.\n3.1 Experiment 1: Embedding dimension\nIn the ﬁrst round of experiments, we investigate the effect\nof the ﬁnal embedding dimension in the retrieval task. We\nconsider the values in {16,32,64,128,256,512,1024}\nand train the model of Figure 2 with the same hyperparam-\neters. Then we perform the retrieval task in both search\ndirections: audio-to-sheet music (A2S) and sheet music-\nto-audio (S2A).\nFigure 3 presents the MRR of the snippet retrieval re-\nsults evaluated on the 534 audio–sheet music passage pairs\nof the MSMD testset. A ﬁrst and straightforward observa-\ntion is that in all cases the S2A direction indicates better\nretrieval quality. We observe the performance increasing\ntogether with the embedding dimensionality until it stag-\nnates at 64-D, and the MRR does not improve on aver-\nage for higher-dimensional embeddings. For this reason,\nwe select the model that generates 64-dimensional embed-\ndings as the best one, which will be evaluated more thor-\noughly in the next experiments.\n3.2 Experiment 2: Real data and improved models\nIn this section, we conduct an extensive series of ex-\nperiments comparing our proposed recurrent network and\nsome improved models thereof with baseline methods, and\nextend the evaluation to real-world piano data.\nGiven that our training data are entirely synthetic, we\nwish to investigate the generalization of our models from\nsynthetic to real data. To this end, we evaluate on three\ndatasets: on a (1) fully artiﬁcial one, and on datasets con-\nsisting (2) partially and (3) entirely of real data. For (1) we\nuse the test split of MSMD and for (2) and (3) we combine\nthe Zeilinger and Magaloff Corpora [27] with a collection\nof commercial recordings and scanned scores that we have\naccess to. These data account for more than a thousand\npages of sheet music scans with mappings to both MIDI\nﬁles and over 20 hours of classical piano recordings. Then,\nbesides the MSMD (I), we deﬁne two additional evalua-\ntion sets: (II) RealScores_Synth : a partially real set, with\nscanned (real) scores of around 300 pieces aligned to syn-\nthesized MIDI recordings. And (III) RealScores_Rec : an\nentirely real set, with scanned (real) scores of around 200pieces and their corresponding real audio recordings.\nAs a baseline (BL), we implement the method from [14]\nand adapt their short-snippet-voting strategy to identify\nand retrieve entire music recordings and printed scores so it\ncan operate with passages.4In essence, short snippets are\nsequentially cut out from a passage query and embedded,\nand are compared to all embedded snippets which were\nselected from passages in a search dataset of the counter-\npart modality, resulting in a ranked list based on the co-\nsine distance for each passage snippet. Then the individual\nranked lists are combined into a single ranking, in which\nthe passage with most similar snippets is retrieved as the\nbest match.\nAdditionally, we investigate whether our models can\nbeneﬁt from pre-trained cross-modal embeddings. Since\nboth CNN encoders of our proposed network architecture\n(see Figure 2) are the same as in [14], we re-designed the\nbaseline cross-modal network to accommodate our snip-\npet dimensions ( 160×180and92×20, for sheet and au-\ndio, respectively) and trained a short-snippet embedding\nmodel also with the MSMD, as a pre-training step, and\nthen loaded the two CNN encoders of our recurrent net-\nwork with their respective pre-trained weights before train-\ning. Our hypothesis is that, by initializing the CNN en-\ncoders with parameters that were optimized to project short\npairs of matching audio–sheet snippets close together onto\na common latent space, models with better embedding ca-\npacity can be obtained. After loading the two CNNs with\npre-trained weights, we can either freeze (FZ) them during\ntraining or just ﬁne-tune (FT) on them. Therefore, in our\nexperiments, we refer to these modiﬁcations of our pro-\nposed vanilla recurrent network (RNN) as RNN-FZ and\nRNN-FT, respectively.\nMoreover, an additional CCA (canonical correlation\nanalysis) layer [28] is used in [14] to increase the corre-\nlation of corresponding pairs in the embedding space. This\nCCA layer is reﬁned in a post-training step, and we inves-\ntigate whether this reﬁnement process is beneﬁcial to our\nnetwork. In our experiments we refer to models that were\ninitialized with pre-trained parameters from networks that\nhad their CCA layer reﬁned as RNN-FZ-CCA and RNN-\nFT-CCA.\nTable 2 presents the results for all data conﬁgurations\nand models deﬁned previously. To keep our experiments\nconsistent and the comparison fair, we randomly select 534\npassage pairs from sets (II) and (III) to create the retrieval\nscenario for their respective experiments.\nAn evident observation from the table is the consider-\nable performance drop as we transition from synthetic to\nreal music data. For all the models, the MRR drops at least\n4The reasons we did not use the attention-based method from [17] as\na baseline comparison are twofold. First we intend to compare the ex-\nact original snippet embedding architecture with and without a recurrent\nencoder, and adding the attention mechanism to a baseline model would\nintroduce a signiﬁcant number of additional trainable parameters, mak-\ning the comparison unfair. Second, the purpose of the attention model is\nto compensate the musical content discrepancy between audio and sheet\nsnippets, which is not the case for musical passages as deﬁned here: pairs\nof audio–sheet music passages comprise the exact musical content (that\nis the reason why fragments are not ﬁxed in time).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n703Table 2 : Results of audio–sheet music passage retrieval, performed in both search directions, and evaluated in three types\nof data: (I) fully synthetic, (II) partially real and (III) entirely real. Boldfaced rows represent the best performing model per\ndataset.\nAudio-to-Score (A2S) Score-to-Audio (S2A)\nR@1 R@10 R@25 MRR MR R@1 R@10 R@25 MRR MR\nI MSMD (Fully synthetic)\nBL 47.56 81.68 90.80 0.592 1 51.37 83.51 92.59 0.628 1\nRNN 51.12 84.46 92.88 0.627 1 54.30 85.95 94.94 0.670 1\nRNN-FT 55.27 87.98 95.02 0.651 1 56.32 87.12 96.44 0.697 1\nRNN-FT-CCA 60.04 89.66 97.73 0.692 1 62.11 91.44 98.41 0.734 1\nRNN-FZ 50.76 84.20 92.11 0.619 1 52.90 85.21 94.12 0.658 1\nRNN-FZ-CCA 52.67 86.46 92.88 0.635 1 55.67 86.30 95.34 0.682 1\nII RealScores_Synth (Sheet music scans and synthetic recordings)\nBL 20.19 55.47 74.99 0.343 7 25.15 70.27 83.11 0.391 5\nRNN 25.09 61.24 78.27 0.374 5 30.15 72.47 86.89 0.439 3\nRNN-FT 28.87 66.41 81.32 0.447 4 33.98 75.47 88.51 0.462 2\nRNN-FT-CCA 33.36 69.49 83.88 0.481 3 37.35 79.22 89.95 0.538 1\nRNN-FZ 25.83 62.02 79.74 0.376 5 31.45 74.87 87.26 0.442 3\nRNN-FZ-CCA 26.82 63.33 80.19 0.391 5 33.55 75.71 88.79 0.467 2\nIII RealScores_Rec (Sheet music scans and real recordings)\nBL 15.67 31.46 48.12 0.226 29 18.30 36.71 54.94 0.266 18\nRNN 19.11 35.98 53.65 0.278 21 22.76 39.95 57.47 0.303 15\nRNN-FT 22.39 39.53 57.19 0.338 18 26.76 42.77 59.38 0.371 7\nRNN-FT-CCA 26.62 44.81 60.01 0.362 7 29.84 46.71 60.88 0.435 4\nRNN-FZ 17.65 33.12 52.98 0.252 22 19.13 37.51 55.57 0.277 17\nRNN-FZ-CCA 18.38 35.81 54.51 0.279 21 22.30 38.95 58.82 0.285 16\n0.2 points to a partially real test set, and drops more than\n0.3 points when moving to the entirely real data. More-\nover, as mentioned in Subsection 3.1, the passage retrieval\nmetrics of the S2A direction are better than those of A2S\nfor all models and scenarios.\nOur recurrent model RNN and its variants outperform\nthe baseline approach in all retrieval scenarios for all eval-\nuation metrics. In our ﬁndings, we did not see notice-\nable improvements when the pre-loaded encoders were\nfrozen during training. In fact, for some conﬁgurations\n(scenarios I and III) the evaluation metrics were slightly\nworse than those from the vanilla RNN model. When\nthe CNN encoders are pre-loaded and enabled for ﬁne-\ntuning, we observe the largest improvements over RNN\nand subsequently over BL. Moreover, the models initial-\nized with pre-trained weights from CCA-reﬁned networks\n(RNN-FT-CCA) achieved the best overall results, for all\ntest datasets and search directions.\nIn addition to the overall absolute improvements, we\nobserve that the performance drop between synthetic and\nreal datasets shrinks with our proposed models, specially\nwith RNN-FT-CCA. In comparison with the baseline, the\nI-to-III MRR gap is reduced by 0.036 and 0.06 points in\nthe directions A2S and S2A, respectively.\nThe results we obtained and summarized in Table 2 in-\ndicate that introducing a recurrent layer to learn longer\ncontexts of musical content is beneﬁcial in our cross-modalretrieval problem. However the real-data generalization\nproblem is still evident, and in Section 4 we discuss po-\ntential solutions to address such issues.\n3.3 Experiment 3: Global tempo variations\nIn this experiment, we investigate the robustness of our\nsystem to global tempo changes. To this end, the pieces\nof the MSMD test dataset are re-rendered with different\ntempo ratios ρ∈ {0.5,0.66,1,1.33,2}(ρ= 0.5means\nthe tempo was halved and ρ= 2 stands for doubling the\noriginal tempo). A similar study was conducted in [17] for\nretrieval of short audio–sheet snippets.\nTable 3 summarizes the MRR values obtained for each\ntempo re-rendering, where the baseline method is com-\npared with our proposed recurrent model. We notice the\ngeneral trend that the MRR gets worse as the tempo ratio\nis farther from ρ= 1 (original tempo). This behavior is\nsomehow expected because the new tempo renditions are\nmore extreme than the tempo changes the model has seen\nduring training.\nBesides the better MRR values of the proposed network,\nan important improvement concerns the performance drop\nwhen changing from ρ= 1 toρ= 0.5(slower rendi-\ntions). The MRR gap between these tempo ratios drops\nfrom 0.12 to 0.1 and from 0.09 to 0.07 points for the A2S\nand S2A directions, respectively, when comparing our net-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n704Model ρ= 0.5ρ= 0.66ρ= 1ρ= 1.33ρ= 2\nBL 0.47 0.54 0.59 0.52 0.40\nRNN 0.53 0.59 0.63 0.58 0.43\n(a) A2S search direction.\nModel ρ= 0.5ρ= 0.66ρ= 1ρ= 1.33ρ= 2\nBL 0.54 0.59 0.63 0.56 0.48\nRNN 0.60 0.64 0.67 0.61 0.50\n(b) S2A search direction.\nTable 3 : MRR for different tempo renderings of the test\npieces of MSMD in both (a) audio-to-sheet and (b) sheet-\nto-audio retrieval directions. We evaluate both baseline and\nRNN models.\nFigure 4 : Cosine distance in the embedding space in rela-\ntion to the respective audio passage duration of 534 pairs\nfrom the MSMD test set. The cosine distances were com-\nputed with the RNN model.\nwork with the baseline. This indicates that the recurrent\nmodel is more robust to global tempo variations and can\noperate well with longer audio passages.\n3.4 Experiment 4: Qualitative analysis\nTo get a better understanding of the behavior of our pro-\nposed network, in this last experiment we take a closer\nlook at the shared embedding space properties. Figure 4\nshows the distribution of the pairwise cosine distances be-\ntween the passage pairs from the MSMD test set, in rela-\ntion to the duration (in seconds) of their respective audio\npassages. Moreover, we scale the point sizes in the plot\nso they are proportional to their individual precision val-\nues (inverse of the rank values), when considering the S2A\nexperimental setup.\nAn interesting behavior in this visualization is the size\nof the points increasing as the cosine distance decreases.\nIt is expected that passage pairs with smaller distances be-\ntween them, meaning that they are closer together in the\nembedding space, would be lead to better retrieval ranks.\nAnother interesting aspect of this distribution concerns\nthe proportion of larger cosine distances as the audio dura-\ntion of the passages increases. For example, between ﬁve\nand ten seconds, there are more large points observed than\nsmaller ones, while between 20 and 25 seconds, the pro-portion is roughly equal. This indicates that, in our test\nset, embeddings from shorter passages of audio are still lo-\ncated closer to their sheet counterparts in comparison with\nlonger audio passages, despite our efforts to design a recur-\nrent networks that learns from longer temporal contexts.\n4. CONCLUSION AND FUTURE WORK\nWe have presented a novel cross-modal recurrent network\nfor learning correspondences between audio and sheet mu-\nsic passages. Besides requiring only weakly-aligned music\ndata for training, this approach overcomes the problems of\nintrinsic global and local tempo mismatches of previous\nworks that operate on short and ﬁxed-size fragments. Our\nproposed models were validated in a series of experiments\nunder different retrieval scenarios and generated better re-\nsults when comparing with baseline methods, for all pos-\nsible conﬁgurations.\nOn the other hand, a serious generalization gap to real\nmusic data was observed, which points us to the next stages\nof our research. A natural step towards making deep-\nlearning-based cross-modal audio–sheet music retrieval\nmore robust would be to include real and diverse data that\ncan be used for training models. However such data with\nsuitable annotations are scarce, and recent advances in end-\nto-end full-page optical music recognition [29] can be a\npossible solution to learn correspondences on the score\npage level. Moreover, the powerful transformers [30] are\npotential architectures to learn correspondences from even\nlonger audio recordings, accommodating typical structural\ndifferences between audio and sheet music, such as jumps\nand repetitions.\n5. ACKNOWLEDGMENTS\nThis work is supported by the European Research Council\n(ERC) under the EU’s Horizon 2020 research and innova-\ntion programme, grant agreement No. 101019375 ( Whither\nMusic? ), and the Federal State of Upper Austria (LIT AI\nLab).\n6. REFERENCES\n[1] M. Müller, A. Arzt, S. Balke, M. Dorfer, and G. Wid-\nmer, “Cross-modal music retrieval and applications:\nAn overview of key methodologies,” IEEE Signal Pro-\ncessing Magazine , vol. 36, no. 1, pp. 52–62, 2019.\n[2] Ö. Izmirli and G. Sharma, “Bridging printed music and\naudio through alignment using a mid-level score repre-\nsentation,” in Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nPorto, Portugal, 2012, pp. 61–66.\n[3] C. Fremerey, M. Clausen, S. Ewert, and M. Müller,\n“Sheet music-audio identiﬁcation,” in Proceedings of\nthe International Conference on Music Information Re-\ntrieval (ISMIR) , Kobe, Japan, Oct. 2009, pp. 645–650.\n[4] F. Kurth, M. Müller, C. Fremerey, Y . ha Chang, and\nM. Clausen, “Automated synchronization of scannedProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n705sheet music with audio recordings,” in Proceedings of\nthe International Conference on Music Information Re-\ntrieval (ISMIR) , Vienna, Austria, Sep. 2007, pp. 261–\n266.\n[5] A. Arzt, S. Böck, and G. Widmer, “Fast identiﬁca-\ntion of piece and score position via symbolic ﬁnger-\nprinting,” in Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nPorto, Portugal, 2012, pp. 433–438.\n[6] T. J. Tsai, “Towards linking the Lakh and IMSLP\ndatasets,” in Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning (ICASSP) , 2020, pp. 546–550.\n[7] D. Yang, T. Tanprasert, T. Jenrungrot, M. Shan, and\nT. J. Tsai, “MIDI passage retrieval using cell phone\npictures of sheet music,” in Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , 2019, pp. 916–923.\n[8] J. Calvo-Zaragoza, J. H. Jr., and A. Pacha, “Under-\nstanding optical music recognition,” ACM Computing\nSurveys , vol. 53, no. 77, 2021.\n[9] J. C. López-Gutiérrez, J. J. Valero-Mas, F. J. Castel-\nlanos, and J. Calvo-Zaragoza, “Data augmentation for\nend-to-end optical music recognition,” in Proceedings\nof the 14th IAPR International Workshop on Graphics\nRecognition (GREC) . Springer, 2021, pp. 59–73.\n[10] E. van der Wel and K. Ullrich, “Optical music recogni-\ntion with convolutional sequence-to-sequence models,”\ninProceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR) , Suzhou,\nChina, 2017, pp. 731–737.\n[11] S. Böck and M. Schedl, “Polyphonic piano note tran-\nscription with recurrent neural networks,” in IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , 2012, pp. 121–124.\n[12] C. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Si-\nmon, C. Raffel, J. Engel, S. Oore, and D. Eck, “On-\nsets and frames: Dual-objective piano transcription,”\ninProceedings of the International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , Paris,\nFrance, 2018, pp. 50–57.\n[13] S. Sigtia, E. Benetos, and S. Dixon, “An end-to-end\nneural network for polyphonic piano music transcrip-\ntion,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , vol. 24, no. 5, pp. 927–939,\n2016.\n[14] M. Dorfer, J. Haji ˇc jr., A. Arzt, H. Frostel, and G. Wid-\nmer, “Learning audio–sheet music correspondences for\ncross-modal retrieval and piece identiﬁcation,” Trans-\nactions of the International Society for Music Informa-\ntion Retrieval , vol. 1, no. 1, 2018.[15] M. Dorfer, A. Arzt, and G. Widmer, “Learning audio-\nsheet music correspondences for score identiﬁcation\nand ofﬂine alignment,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Suzhou, China, 2017, pp. 115–122.\n[16] L. Carvalho, T. Washüttl, and G. Widmer, “Self-\nsupervised contrastive learning for robust audio–sheet\nmusic retrieval systems,” in Proceedings of the\nACM International Conference on Multimedia Systems\n(ACM-MMSys) , Vancouver, Canada, 2023.\n[17] S. Balke, M. Dorfer, L. Carvalho, A. Arzt, and G. Wid-\nmer, “Learning soft-attention models for tempo-\ninvariant audio-sheet music retrieval,” in Proceedings\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , Delft, Netherlands, 2019,\npp. 216–222.\n[18] F. Zalkow and M. Müller, “Using weakly aligned\nscore–audio pairs to train deep chroma models for\ncross-modal music retrieval,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , Montréal, Canada, 2020, pp.\n184–191.\n[19] S. Balke, V . Ariﬁ-Müller, L. Lamprecht, and\nM. Müller, “Retrieving audio recordings using musi-\ncal themes,” in Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning (ICASSP) , Shanghai, China, 2016, pp. 281–285.\n[20] S. Ioffe and C. Szegedy, “Batch normalization: Accel-\nerating deep network training by reducing internal co-\nvariate shift,” in Proceedings of the 32nd International\nConference on International Conference on Machine\nLearning (ICML) , Lille, France, 2015, pp. 448–456.\n[21] D. Clevert, T. Unterthiner, and S. Hochreiter, “Fast and\naccurate deep network learning by exponential linear\nunits (ELUs),” in International Conference on Learn-\ning Representations, (ICLR) , 2016.\n[22] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to se-\nquence learning with neural networks,” in Proceedings\nof the 27th International Conference on Neural Infor-\nmation Processing Systems , 2014, pp. 3104–3112.\n[23] K. Simonyan and A. Zisserman, “Very deep convolu-\ntional networks for large-scale image recognition,” in\nInternational Conference on Learning Representations\n(ICLR) , 2015.\n[24] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural Computation , vol. 9, no. 8, pp. 1735–\n1780, 1997.\n[25] K. Cho, B. van Merriënboer, C. Gulcehre, D. Bah-\ndanau, F. Bougares, H. Schwenk, and Y . Bengio,\n“Learning phrase representations using RNN encoder–\ndecoder for statistical machine translation,” in Pro-\nceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) . Doha,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n706Qatar: Association for Computational Linguistics, Oct.\n2014, pp. 1724–1734.\n[26] R. Kiros, R. Salakhutdinov, and R. S. Zemel,\n“Unifying visual-semantic embeddings with mul-\ntimodal neural language models,” arXiv preprint\n(arXiv:1411.2539) , 2014. [Online]. Available: http:\n//arxiv.org/abs/1411.2539\n[27] C. E. Cancino-Chacón, T. Gadermaier, G. Widmer, and\nM. Grachten, “An evaluation of linear and non-linear\nmodels of expressive dynamics in classical piano and\nsymphonic music,” Machine Learning , vol. 106, no. 6,\npp. 887–909, 2017.\n[28] M. Dorfer, J. Schlüter, A. Vall, F. Korzeniowski,\nand G. Widmer, “End-to-end cross-modality retrieval\nwith CCA projections and pairwise ranking loss,”\nInternational Journal of Multimedia Information Re-\ntrieval , vol. 7, no. 2, pp. 117–128, Jun 2018. [Online].\nAvailable: https://doi.org/10.1007/s13735-018-0151-5\n[29] A. Ríos-Vila, J. M. Iñesta, and J. Calvo-Zaragoza,\n“End-to-end full-page optical music recognition for\nmensural notation,” in Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , Bengaluru, India, 2022, pp. 226–232.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin,\n“Attention is all you need,” in Advances in Neural In-\nformation Processing Systems , vol. 30. Curran Asso-\nciates, Inc., 2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n707"
    },
    {
        "title": "Decoding Drums, Instrumentals, Vocals, and Mixed Sources in Music Using Human Brain Activity With fMRI.",
        "author": [
            "Vincent K. M. Cheung",
            "Lana Okuma",
            "Kazuhisa Shibata",
            "Kosetsu Tsukuda",
            "Masataka Goto",
            "Shinichi Furuya"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265257",
        "url": "https://doi.org/10.5281/zenodo.10265257",
        "ee": "https://zenodo.org/records/10265257/files/000022.pdf",
        "abstract": "Brain decoding allows the read-out of stimulus and mental content from neural activity, and has been utilised in various neural-driven classification tasks related to the music information retrieval community. However, even the relatively simple task of instrument classification has only been demonstrated for single- or few-note stimuli when decoding from neural data recorded using functional magnetic resonance imaging (fMRI). Here, we show that drums, instrumentals, vocals, and mixed sources of naturalistic musical stimuli can be decoded from single-trial spatial patterns of auditory cortex activation as recorded using fMRI. Comparing classification based on convolutional neural networks (CNN), random forests (RF), and support vector machines (SVM) further revealed similar neural encoding of vocals and mixed sources, despite vocals being most easily identifiable. These results highlight the prominence of vocal information during music perception, and illustrate the potential of using neural representations towards evaluating music source separation performance and informing future algorithm design.",
        "zenodo_id": 10265257,
        "dblp_key": "conf/ismir/CheungOSTGF23",
        "keywords": [
            "Brain decoding",
            "stimulus and mental content",
            "neural activity",
            "music information retrieval",
            "instrument classification",
            "fMRI",
            "drums",
            "instrumentals",
            "vocals",
            "mixed sources"
        ],
        "content": "DECODING DRUMS, INSTRUMENTALS, VOCALS, AND MIXED\nSOURCES IN MUSIC USING HUMAN BRAIN ACTIVITY WITH FMRI\nVincent K.M. Cheung1Lana Okuma2Kazuhisa Shibata2\nKosetsu Tsukuda3Masataka Goto3Shinichi Furuya1\n1Sony Computer Science Laboratories, Tokyo, Japan\n2RIKEN Center for Brain Science, Japan\n3National Institute of Advanced Industrial Science and Technology (AIST), Japan\nABSTRACT\nBrain decoding allows the read-out of stimulus and men-\ntal content from neural activity, and has been utilised\nin various neural-driven classiﬁcation tasks related to the\nmusic information retrieval community. However, even\nthe relatively simple task of instrument classiﬁcation has\nonly been demonstrated for single- or few-note stimuli\nwhen decoding from neural data recorded using functional\nmagnetic resonance imaging (fMRI). Here, we show that\ndrums, instrumentals, vocals, and mixed sources of nat-\nuralistic musical stimuli can be decoded from single-trial\nspatial patterns of auditory cortex activation as recorded\nusing fMRI. Comparing classiﬁcation based on convolu-\ntional neural networks (CNN), random forests (RF), and\nsupport vector machines (SVM) further revealed similar\nneural encoding of vocals and mixed sources, despite vo-\ncals being most easily identiﬁable. These results highlight\nthe prominence of vocal information during music percep-\ntion, and illustrate the potential of using neural represen-\ntations towards evaluating music source separation perfor-\nmance and informing future algorithm design.\n1. INTRODUCTION\nThe goal of brain decoding is to infer mental states and\nperceptual information from neural activity [1, 2]. Com-\nmon neuroimaging techniques such as functional mag-\nnetic resonance imaging (fMRI) and electroencephalogra-\nphy (EEG) allow data acquisition in a non-invasive man-\nner, which has resulted in rapid developments in brain-\ncomputer interfaces (BCI) [3, 4].\nAlthough fMRI- and EEG-based models both make use\nof neural activity for decoding, the form of information\nretrieved is substantially different. That is because fMRI\noffers (sub-)millimetre spatial resolution at the cost of low\n© V .K.M. Cheung, L. Okuma, K. Shibata, K. Tsukuda, M.\nGoto, and S. Furuya. Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: V .K.M. Cheung,\nL. Okuma, K. Shibata, K. Tsukuda, M. Goto, and S. Furuya, “Decoding\ndrums, instrumentals, vocals, and mixed sources in music using human\nbrain activity with fMRI”, in Proc. of the 24th Int. Society for Music\nInformation Retrieval Conf., Milan, Italy, 2023.\nFigure 1 . We compared the decoding performance of\nconvolutional neural networks (CNN), random forests\n(RF), and support vector machines (SVM) in classifying\ndrums, instrumentals, vocals, and mixed naturalistic mu-\nsical sources based on human auditory cortex activation\n(highlighted in green) as recorded using fMRI.\ntemporal resolution, whilst EEG provides a millisecond-\nlevel temporal resolution at the expense of poor spatial\nresolution [2, 5]. Consequently, fMRI-based decoders typ-\nically rely on spatial representations of neural activation as\nfeatures, whilst EEG-based decoders exploit the temporal\ndynamics of neural activity.\nIn the context of music information retrieval (MIR),\nboth fMRI- and EEG-based decoders have been employed\nfor a variety of classiﬁcation/estimation tasks, such as\ngenre [6–9], pitch [6, 10–12], rhythm [13, 14], musical\nemotion classiﬁcation [15–21], song identiﬁcation [22–\n25], music composition [26], beat and note onset detec-\ntion [27,28], and acoustic feature extraction [29], as well as\nreconstruction from heard and imagined melodies [30–35].\nHowever, a problem that has remained under-studied is\nthe decoding of different instruments within a song based\non brain activity. This is despite its intimate relation to the\nstandard MIR task of music source separation, which seeks\nto decompose a musical sound mixture into a linear sum of\ninstrumental sources [36, 37]. Although music and speech\nsource separation share the same goals, the key difference\nis that sound sources from multiple musical instruments\nare more correlated in music than in speech [36].\nThe most relevant literature on neural-driven music\nsource separation is the work by Cantisani et al. [38, 39].\nTheir initial work showed that EEG can be used to decode\nlisteners’ attention deployment to a particular instrument197from naturalistic polyphonic music mixtures [38]. The\napproach was to ﬁrst record listeners’ EEG as they were\npresented with solo instrumental sources. A temporal re-\nsponse function was then trained to reconstruct the solo\ninstrumental sources from EEG. This response function\nwas later applied to the EEG signal when subjects listened\nto the polyphonic mixtures. The attended instrument was\nidentiﬁed as the one that showed the highest correlation\nwith the reconstructed source. In their subsequent work\n[39], they showed that the reconstructed sources from their\nEEG attention decoding model can be used as contrastive\npriors to inform a non-negative matrix factorisation-based\nsource separation model.\nOn the other hand, relevant work based on fMRI data\nseems to be lacking. While existing studies have identi-\nﬁed the role of the auditory cortex in processing timbre\n[40, 41] via correlational approaches, even the relatively\nsimple task of decoding musical instrument category has\nonly been restricted to single- or few-note stimuli [19, 42].\nIn this paper we address this gap by showing that dis-\ntinct musical sources, namely drums, instrumentals, and\nvocals from naturalistic musical stimuli, as well as their\nmixtures, can be decoded from spatial representations of\nneural activation recorded using fMRI on the single-trial\nlevel. We report that decoding performance was the high-\nest when detecting the presence of vocal information in the\nauditory stimulus, and we explain our model decisions in\nterms of patterns of neural activations. Importantly, un-\nlike most existing decoding studies which have relied on a\nsingle classiﬁcation algorithm, we additionally compared\nperformance across three decoders, convolutional neural\nnetworks (CNN), random forests (RF), and support vec-\ntor machines (SVM), to enhance the generalisability of our\nﬁndings. In the last section of this paper, we also discuss\nhow brain activity could be used in the future to evaluate\nmusic source separation and inform algorithm design.\n2. METHODS\n2.1 Experimental stimuli\nExperimental stimuli consisted of 15-second musical audio\nexcerpts derived from the beginning of the chorus section\nof 24 unreleased pop and rock songs within an in-house\nmusic dataset created by professional musicians.\nFour versions of each song— drums ,instrumentals ,vo-\ncals, and mixed —were compiled, resulting in a total of\n96 stimuli. The versions were produced by ﬁrst separat-\ning the original song into bass ,drums ,other , and\nvocals using a state-of-the-art music source separation\nmodel, Demucs-v4 [43]. Due to the frequency response of\nMRI-compatible noise-isolating earphones (Sensimetrics\nS15),bass andother were linearly combined to form\naninstrumentals version. A 100-ms fade-out was then ap-\nplied to the drums ,instrumentals , and vocals versions, fol-\nlowed by loudness normalisation to the EBR U 128 stan-\ndard. Finally, the normalised drums ,instrumentals , and\nvocals versions of each song were linearly combined to\nform the mixed version, which was also normalised forloudness. We chose to use the mixed version rather than\nthe original song to ensure that decoding was not biased\nby differences in loudness from the underlying versions.\nWe made sure that each song actually included drums, vo-\ncals, and other instruments before source separation, and\nwe checked our resulting stimuli after source separation to\nensure that they were free from audible artefacts and sep-\naration errors, and that they did not contain silences at the\nstart and end that would shorten the stimuli.\n2.2 Data acquisition\nData were collected from 24 healthy, normal-hearing\nadults aged between 19-34 with their written informed\nconsent. The 96 music stimuli were presented over\neight runs whilst functional gradient echo planar images\n(TR/TA/TE = 2/2/0.025 s, voxel size = 3×3×3 mm3, 33\nslices, ﬂip angle = 77◦, 188 volumes per run) were ac-\nquired using a Siemens Prisma 3T MRI scanner. Each run\nlasted approximately 6 minutes, and was separated by a\nshort break of around one minute. Stimulus presentation\nwas counter-balanced across runs, with the constraint that\neach run contained three samples of the four versions, all\nstimuli came from different songs, and that each song (re-\ngardless of version) appeared only once every other run.\nStimulus presentation within a run was randomised. To\nmaintain attention, subjects were also asked to rate their\npreference on a 1-9 scale within a 4-second time window\nusing a button box after each stimulus presentation. Our\nstudy was approved by the Ethics Committee at RIKEN.\n2.3 fMRI data preprocessing\nFunctional MRI data for each subject were preprocessed\nusing fMRIprep [44]. Functional images were ﬁrst cor-\nrected for slice-timing differences, motion artefacts, and\nsusceptibility distortions, then co-registered to subjects’\nanatomical image, and then normalised to standard MNI-\nspace using the ICBM 152 Nonlinear Asymmetrical tem-\nplate. Next, for each subject, we ﬁtted a general linear\nmodel in each voxel to estimate the blood oxygen level-\ndependent (BOLD) response on the single-trial level using\nSPM [45] following a ‘least-squared all’ approach [46]:\neach stimulus was modelled as a separate regressor in the\ndesign matrix, and a parametric modulator that varied by\nsubjects’ stimulus rating was also added to control for dif-\nferences in preference. Another regressor was included\nto account for variance during the rating period. These\nregressors were modelled as boxcar functions and con-\nvolved with the canonical haemodynamic response func-\ntion (HRF). Six motion, one cardiac, and one respiratory\nregressors were further added to the design matrix to con-\ntrol for motion- and physiology-induced artefacts. Model\nparameters were estimated using restricted maximum like-\nlihood, and the resulting parameter estimates at each voxel\nprovided a spatial representation (i.e., beta maps ) of neural\nactivations for each stimulus separately, which we used for\nsubsequent decoding.\nAs we were interested in stimulus differences in the\nneural-perceptual level, we considered voxels in the hu-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n198Figure 2 . Architecture of our CNN-based decoder.\nman auditory cortex (see Figure 1) as decoding features.\nThese were obtained by applying a mask to the bilateral\nearly-auditory andauditory-associative\nregions in the HCP-MM1 brain atlas [47,48], and then ﬂat-\ntened into a 1D-vector using nilearn [49].\n2.4 Decoding analyses\nWe performed two decoding analyses. The ﬁrst was a four-\nway classiﬁcation task, whose goal was to classify which\nof the four versions a stimulus belonged to based on sub-\njects’ brain activation as summarised by its beta map. The\nsecond was a binary recognition task, whose goal was to\ndetect the presence of drums, instrumentals, or vocals in\nthe stimulus from brain activation. As an example, for\ndrum-recognition, drums andmixed versions would be as-\nsigned a positive label, whilst instrumentals and vocals\nversions would be assigned a negative label.\nWe also examined whether decoding performance de-\npended on neural information encoded in the left, right, or\nboth auditory cortices. This was motivated by neurosci-\nentiﬁc ﬁndings suggesting a right-lateralised hemispheric\ndominance to musical stimuli [50], and that the left audi-\ntory cortex may be more sensitive to rapid temporal fea-\ntures in an auditory stimulus whilst the right may be more\nsensitive towards spectral features [51].\nTo enhance the generalisability of our ﬁndings, we per-\nformed leave-one-subject-out cross-validation, where each\ndecoder was trained on data from 23 subjects and tested\non 1 remaining subject. Note that brain decoding between\nsubjects is generally harder than decoding within subjects,\nbecause the decoder must additionally overcome individ-\nual differences in structural and functional organisation of\nthe brain when predicting on an unseen subject [52].\n2.5 Implementation\nWe trained three types of classiﬁers—convolutional neural\nnetworks (CNN), random forests (RF), and support vec-\ntor machines (SVM)—for our two decoding tasks. While\nclassical approaches such as SVMs and RFs remain pop-\nular [53], CNNs have also been recently used to decode\nvisual objects [54], vocal emotions [55], and musical pitch\n[10] from fMRI data. We implemented CNN decoders on\nTensorFlow2, whilst RF and SVM were implemented on\nscikit-learn1.\nTraining data were ﬁrst put through a variance thresh-\nold to remove features that gave identical outputs (e.g., at\n1For data, code, and Supplementary Information, please refer to\nhttps://github.com/vkmcheung/neuromusic-decoding/the boundary of the brain), and then scaled using a robust\nscaler, before decoders were ﬁtted.\nOur CNN decoders (see Figure 2) were inspired by\nConvNeXt [56], which is a family of purely convolu-\ntional neural networks that recently achieved state-of-the-\nart performance in image classiﬁcation. Input features\nﬁrst passed through a 1D-convolution layer (96 units, ker-\nnel size = 4), and a ConvNeXt-like residual block. This\nblock comprised a 1D-convolution layer (96 units, kernel\nsize = 7), followed by layer normalisation, 1D-convolution\n(384 units, kernel size = 1), GELU activation, another 1D-\nconvolution (96 units, kernel size = 1), and a residual con-\nnection layer followed by ReLU activation. Outputs of\nthe residual block then passed through three dense layers\n(1024, 512, and 256 units, respectively), a ﬂattening layer,\nand ﬁnally a dense layer with softmax output. All convo-\nlution layers had a stride length of 1 (except for the ﬁrst,\nwhich had a length of 4) and same -padding. Each model\nwas trained to minimise categorical entropy loss for 200\nepochs, and early-stopped if validation performance did\nnot improve after 25 epochs (with best weights restored).\nData from two random subjects ( ∼10%) in the training set\nwere held-out for validation, and we selected a batch size\nof 512, and an AdamW optimiser [57] with learning rate =\n0.001 and weight decay = 0.0001 for training.\nRF decoders were trained with bootstrapping using 100\ntrees in the forest, at least 1 sample per leaf, and 2 samples\nper split. Quality of split was assessed with Gini impurity.\nSVM decoders were trained using regularisation param-\neter of 1 with squared-L2 penalty and a linear kernel for a\nmaximum of 10,000 iterations.\n3. RESULTS AND DISCUSSION\n3.1 Four-way classiﬁcation\nTable 1 and Figure 3 show the leave-one-subject-out cross-\nvalidation performance of CNN, RF, and SVM decoders in\nclassifying whether a stimulus belonged to drums ,instru-\nmentals ,vocals , ormixed versions of a song, given audi-\ntory cortex activation. To test the statistical signiﬁcance\nof our results (see Table 2), we ﬁtted linear mixed models\nwith the interaction between classiﬁer and hemisphere (and\nlower order terms) as ﬁxed effects and a maximal random\neffects structure with subject as a grouping factor.\nAll classiﬁers showed signiﬁcantly above-chance per-\nformance (all p < 2.2 ×10−16, see Supplementary Infor-\nmation1) in decoding accuracy and Area Under the Re-\nceiver Operating Characteristic Curve (ROC AUC), sug-\ngesting that despite the slow temporal resolution of fMRI,\ncorrelated sources of the same song can be decoded and\nclassiﬁed from spatial representations of brain activation.\nFor all classiﬁers, accuracy and ROC AUC were highest\nwhen decoding from the bilateral auditory cortex, followed\nby the right and left hemispheres. Resolving signiﬁcant in-\nteraction effects between classiﬁer and hemisphere for ac-\ncuracy and ROC AUC furthermore revealed that accuracy\nwas signiﬁcantly worse when decoding from the left com-\npared to the right and bilateral auditory cortices for CNN,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n199CNN RF SVM\nacc auc acc auc acc auc\nFour-way classiﬁcation\nl AC .506 .799 .523 .802 .524 .810\nr AC .588 .858 .536 .817 .563 .843\nl+r AC .604 .863 .554 .824 .597 .863\nl+r PV .301 .560 .319 .554 .253 .510\nl+r SM .304 .547 .332 .550 .276 .554\nDrums recognition\nl AC .595 .638 .603 .637 .550 .559\nr AC .622 .677 .586 .638 .559 .591\nl+r AC .630 .683 .599 .655 .553 .601\nl+r PV .507 .505 .528 .533 .526 .531\nl+r SM .517 .544 .530 .545 .490 .500\nInstrumentals recognition\nl AC .656 .726 .638 .688 .615 .679\nr AC .642 .723 .666 .727 .627 .687\nl+r AC .680 .762 .657 .712 .652 .703\nl+r PV .577 .593 .585 .611 .495 .509\nl+r SM .558 .576 .580 .600 .517 .553\nVocals recognition\nl AC .794 .891 .799 .913 .746 .847\nr AC .816 .926 .841 .937 .836 .936\nl+r AC .839 .946 .829 .936 .843 .950\nl+r PV .527 .527 .525 .541 .495 .502\nl+r SM .516 .544 .563 .581 .516 .552\nTable 1 . Mean brain decoding performance with leave-\none-subject-out cross-validation. acc= accuracy; auc=\nROC AUC; l/r/l+r = left/right/bilateral; AC= auditory, PV\n= primary visual, SM= somatosensory-motor cortices.\nFigure 3 . Box plots showing four-way classiﬁcation per-\nformance when decoding from voxels in the left and/or\nright auditory cortex using CNN, RF, and SVM. Light cir-\ncles indicate test performance on each held-out subject.\nFilled circles indicate mean. Dashed lines indicate chance.\nand compared to the right for SVM. Likewise, ROC AUC\nwas signiﬁcantly lower when decoding from the left com-\npared to the bilateral auditory cortex for all classiﬁers, and\ncompared to the right for CNN and SVM. Although these\nresults corroborate previous ﬁndings (e.g., [50]) that sup-\nport a dominant role of the right auditory cortex in pro-\ncessing musical stimuli, they nevertheless show that both\nauditory cortices were engaged and provided useful infor-\nmation for decoding.Four-way classiﬁcation χ2df p\nAccuracy\nhemisphere 37.3 2 8.08 ×10−9***\nclassiﬁer 6.81 2 .0331 *\nhemisphere:classiﬁer 14.6 4 .00551 **\nROC AUC\nhemisphere 55.3 2 9.58 ×10−13***\nclassiﬁer 11.8 2 .00278 **\nhemisphere:classiﬁer 14.3 4 .00625 **\nRecognition task χ2df p\nAccuracy\nhemisphere 3.30 2 .192\nclassiﬁer 14.5 2 .000720 ***\ntask 59.9 2 9.78 ×10−14***\nhemisphere:classiﬁer 3.60 4 .463\ntask:classiﬁer 9.76 4 .0447 *\nhemisphere:task 2.90 4 .574\nhemisphere:classiﬁer:task 11.3 8 .184\nROC AUC\nhemisphere 4.89 2 .0866\nclassiﬁer 13.6 2 .00114 **\ntask 94.2 2 < 2.2 ×10−16***\nhemisphere:classiﬁer 2.07 4 .722\ntask:classiﬁer 10.9 4 .0275 *\nhemisphere:task 2.68 4 .612\nhemisphere:classiﬁer:task 9.54 8 .299\nTable 2 . ANOV A table evaluating the statistical signiﬁ-\ncance of hemisphere and classiﬁer in four-way classiﬁca-\ntion and recognition task performance. * = p < .05; ** = p\n< .01; *** = p < .001.\nConfusion matrices in Figure 4(A) provide further in-\nsight on decoding performance. We notice that across the\nthree decoders trained on both hemispheres, recall was the\nhighest for drums and the lowest for mixed . The high re-\ncall for drums could be because they were the most tem-\nporally regular and had limited pitch possibilities. Further-\nmore, mixed andvocals , as well as drums andinstrumen-\ntals, were often misclassiﬁed as the other. These suggest\na similar neural representation between mixed andvocals ,\nas well as drums andinstrumentals . Whether this pairing\nis contingent on the stimulus set, the part of a song used\n(here, our stimulus excerpts were taken from the begin-\nning of the chorus section), and the experimental design\nremains to be veriﬁed in future studies.\n3.2 Neural representations\nTo explain the impact of each voxel towards classiﬁcation,\nwe turned to SHapley Additive exPlanations (SHAP) [58].\nSHAP decomposes a model prediction into the additive\ncontribution of each feature from the mean using game the-\nory. Figure 4(B) shows the mean-averaged contribution of\nvoxels in the bilateral auditory cortex towards classifying a\nstimulus as belonging to the four versions in Subject 4 us-\ning a CNN. We notice that the pattern of contributions were\nquite similar for mixed andvocals , which could explain the\nmisclassiﬁcation of the two labels observed above. Fur-\nthermore, there were substantial contributions from bothProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n200Figure 4 . (A) Confusion matrix (normalised along rows) for each decoder when trained on the bilateral auditory cortices,\npooled across all subjects. Notice that drums recall was highest, and a consistent misclassiﬁcation between mixed and\nvocals , as well as drums andinstrumentals . (B) Mean additive contribution of each voxel in the bilateral auditory cortex\ntowards classifying a given label for one subject using a CNN decoder derived using SHAP [58].\nauditory cortices, again indicating a bilateral engagement\nduring music processing.\n3.3 Recognition task\nWe next tested whether decoding performance in recognis-\ning the presence of drums, vocals, or instrumentals varied\nfrom the left and/or right auditory cortex. Results from\nleave-one-subject-out cross-validation are summarised in\nFigure 5 and Tables 1 and 2.\nResolving signiﬁcant main effect of tasks for accuracy\nand ROC AUC revealed substantially higher decoding per-\nformance across CNN, RF, and SVM in recognising vocals\ncompared to drums and instrumentals (see Supplementary\nInformation1). That the presence of vocal information\nwas most robustly encoded from neural activation patterns\nis very interesting, as it suggests that listeners show an en-\nhanced sensitivity towards perceiving human voice in mu-\nsic. This ﬁnding is in line with the view that singing vo-\ncals play a prominent and powerful role in communicating\nand expressing meaning and emotion during music listen-\ning [59, 60]. We speculate that the presence of vocal in-\nformation might have additionally engaged neural popula-\ntions involved in language processing, which consequently\nincreased its dissimilarity amongst other labels.\nSigniﬁcant main effects of classiﬁer for accuracy and\nROC AUC also indicated superior performance of CNN\nand RF over SVM when averaged across recognition\ntasks. However, signiﬁcant task-by-classiﬁer interactions\nfor both measures suggest that performance varied accord-\ning to recognition task. Resolving the interaction revealedsigniﬁcantly lower accuracy and ROC AUC for SVM com-\npared to CNN and RF in drums recognition. Signiﬁcantly\nhigher ROC AUC was also observed for CNN compared to\nSVM in recognising instrumentals. Nevertheless, we were\nnot able to detect any meaningful differences in laterality\nacross tasks or classiﬁers.\n3.4 Feature-encoding speciﬁcity\nThus far, we relied on neural activations in the auditory\ncortex as input features for our decoding models. To as-\nsess the speciﬁcity of information encoding, we repeated\nthe above analyses in two other sensory processing brain\nregions, namely the bilateral primary-visual , and the\nsomatosensory-and-motor regions as derived from\nthe HCP-MM1 brain atlas [47,48]. As before, we assessed\nthe statistical signiﬁcance of decoding performance using\nlinear mixed models. However, rather than comparing the\neffects of hemisphere within the auditory cortex, we now\ncompare performance across the bilateral auditory, primary\nvisual, as well as somatosensory-motor cortices.\nIn the four-way classiﬁcation task, we observe in Table\n1 and the Supplementary Information1that decoding from\nthe bilateral auditory cortex resulted in signiﬁcantly higher\naccuracy and ROC AUC compared to the two other sensory\ncortices across all classiﬁers (all p < 2.2 ×10−16).\nInterestingly, decoding accuracy and ROC AUC were\nalso signiﬁcantly above chance when CNNs and RFs were\ntrained using features from the visual and somatosensory-\nmotor regions (with no signiﬁcant differences between\nthese two regions). Furthermore, resolving signiﬁ-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n201Figure 5 . Box plots comparing performance in recognising the presence of drums ,instrumentals , orvocals in a musical\nstimulus using left and/or right auditory cortex activation as decoding features. Signiﬁcantly higher decoding performance\nwas detected in the vocals recognition. Dashed lines indicate chance performance.\ncant cortex-by-classiﬁer interactions showed signiﬁcantly\nlower accuracy and ROC AUC when decoding from the\nprimary visual cortex using SVM compared to CNN and\nRF, and from the auditory cortex using RF compared to\nCNN, as well as lower accuracy when decoding from the\nsomatosensory-motor cortex using SVM compared to RF.\nA similar picture could be seen in recognition per-\nformance. Signiﬁcant main effects of cortex for recog-\nnition accuracy and ROC AUC indicate superior perfor-\nmance when decoding from the auditory compared to\nvisual or somatosensory-motor regions. Resolving sig-\nniﬁcant cortex-by-task interactions further revealed that\nthe signiﬁcantly higher performance in recognising vocals\ncompared to drums or instrumentals was speciﬁc to the au-\nditory cortex. By contrast, accuracy and ROC AUC for\ninstrumentals were signiﬁcantly higher than drums in the\nauditory and somatosensory-motor areas, as well as in the\nprimary visual cortex (ROC AUC only).\nEngagement of the primary visual cortex during music\nhas been suggested to be related to mental imagery [61,62],\nwhich is thought to be an important way through which\nmusic evokes emotions [63]. Likewise, the somatosensory\ncortex has been said to encode the emotional percept or\nfeeling states associated with music [15], whilst auditory-\nmotor interactions during music perception is thought to be\nrelated to the integration and updating of hierarchical pre-\ndictions of the musical beat [64, 65]. Combined with the\nsubstantially higher performance observed when decoding\nfrom the auditory cortex, these suggest that while musical\nsources could be decoded from visual and somatosensory-\nmotor regions, the information encoded is unlikely to be\nrelated to the auditory content itself. Rather, such repre-\nsentations might encode affective or metrical information\nfrom associated cognitive processes that arise when per-\nceiving the four different musical sources.\n4. CONCLUSION AND FUTURE PERSPECTIVES\nIn this paper, we demonstrated that drums, instrumentals,\nvocals, and mixed sources of naturalistic music can be\ndecoded from human auditory cortex fMRI data on thesingle-trial, between-subject level. While decoding per-\nformance was the highest for CNN, performance across\nall classiﬁers—CNN, RF, and SVM—were above chance\nand suggested similar neural representations for vocals and\nmixed sources. An especially high performance in vocals\nrecognition across all classiﬁers further pointed towards an\nenhanced perceptual sensitivity towards vocal information\nduring music listening. Taken together, our results show\nthat despite the low temporal resolution of fMRI, the high\nspatial resolution it offers could still provide relevant in-\nformation for decoding in neural-driven MIR tasks.\nAlthough our speciﬁcity analyses highlighted the au-\nditory cortex in encoding stimulus-relevant information\ncompared to other sensory areas, the perception of dif-\nferent musical sources is a hierarchical process that en-\ngages higher-order brain regions in the prefrontal cortex\nvia dorsal and ventral pathways [66, 67]. Future work\ncould examine differences in representations along these\ntwo pathways to shed light on neural mechanisms involved\nin auditory-object processing.\nIn the context of music source separation, one future\npossibility is to use neural data for evaluation. While cur-\nrent subjective evaluation of music source separation algo-\nrithms typically rely on explicit ratings such as MUSHRA\nor mean opinion scores, ratings are known to be prone to\nresponse biases [68–70] and might consequently fail to ad-\nequately reﬂect subjects’ perception. This could be over-\ncome by directly evaluating performance on the neural-\nperceptual level. Future work could, for example, com-\npare the neural representations of source-separated stimuli\nfrom different algorithms or hyperparameters. Separation\nquality could be determined by identifying the algorithm\nthat maximises dissimilarity in neural activation across the\ndifferent sources. Another possibility is to assess sensitiv-\nity to each instrument by examining neural activation in\nresponse to different mixing proportions. This would pro-\nvide perceptual priors that could be used to constrain the\nparameter space in future music source separation algo-\nrithms. While these prospects may seem too challenging\nat this time, we envision that our work will help pave the\nway in that direction.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2025. ACKNOWLEDGMENTS\nThis work was supported in part by JST CREST Grant\nNumber JPMJCR20D4 and JSPS KAKENHI Grant Num-\nber JP21H04917, Japan.\n6. REFERENCES\n[1] N. Kriegeskorte and P. K. Douglas, “Interpreting en-\ncoding and decoding models,” Current Opinion in Neu-\nrobiology , vol. 55, pp. 167–179, 2019.\n[2] B. Kaneshiro and J. P. Dmochowski, “Neuroimaging\nmethods for music information retrieval: Current ﬁnd-\nings and future prospects,” in Proceedings of the 16th\nInternational Society for Music Information Retrieval\nConference , ser. ISMIR 2015, 2015, pp. 538–544.\n[3] M. Zhuang, Q. Wu, F. Wan, and Y . Hu, “State-of-the-\nart non-invasive brain–computer interface for neural\nrehabilitation: A review,” Journal of Neurorestoratol-\nogy, vol. 8, no. 1, pp. 12–25, 2020.\n[4] R. Sitaram, A. Caria, R. Veit, T. Gaber, G. Rota,\nA. Kuebler, and N. Birbaumer, “fMRI brain-computer\ninterface: a tool for neuroscientiﬁc research and treat-\nment,” Computational Intelligence and Neuroscience ,\nvol. 2007, 2007.\n[5] B. He and Z. Liu, “Multimodal functional neuroimag-\ning: Integrating functional MRI and EEG/MEG,” IEEE\nReviews in Biomedical Engineering , vol. 1, pp. 23–40,\n2008.\n[6] M. A. Casey, “Music of the 7ts: Predicting and\ndecoding multivoxel fMRI responses with acoustic,\nschematic, and categorical music features,” Frontiers\nin Psychology , vol. 8, p. 1179, 2017.\n[7] T. Nakai, N. Koide-Majima, and S. Nishimoto, “En-\ncoding and decoding of music-genre representations in\nthe human brain,” in Proceedings of the 2018 IEEE In-\nternational Conference on Systems, Man, and Cyber-\nnetics , ser. SMC 2018, 2018, pp. 584–589.\n[8] T. Nakai, N. Koide-Majima, and S. Nishimoto, “Corre-\nspondence of categorical and feature-based representa-\ntions of music in the human brain,” Brain and Behav-\nior, vol. 11, no. 1, p. e01936, 2021.\n[9] J. S. Rahman, T. Gedeon, S. Caldwell, and R. Jones,\n“Brain melody informatics: Analysing effects of music\non brainwave patterns,” in Proceedings of the 2020 In-\nternational Joint Conference on Neural Networks , ser.\nICJNN 2020, 2020, pp. 1–8.\n[10] V . K. Cheung, Y .-P. Peng, J.-H. Lin, and L. Su, “De-\ncoding musical pitch from human brain activity with\nautomatic voxel-wise whole-brain fMRI feature selec-\ntion,” in Proceedings of the 2023 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning, ser. ICASSP 2023, 2023.[11] K. Tsekoura and A. Foka, “Classiﬁcation of EEG sig-\nnals produced by musical notes as stimuli,” Expert Sys-\ntems with Applications , vol. 159, p. 113507, 2020.\n[12] V . De Angelis, F. De Martino, M. Moerel, R. Santoro,\nL. Hausfeld, and E. Formisano, “Cortical processing of\npitch: Model-based encoding and decoding of auditory\nfMRI responses to real-life sounds,” NeuroImage , vol.\n180, pp. 291–300, 2018.\n[13] P.-C. Chang, J.-R. Chang, P.-Y . Chen, L.-K. Cheng, J.-\nC. Hsieh, H.-Y . Yu, L.-F. Chen, and Y .-S. Chen, “De-\ncoding neural representations of rhythmic sounds from\nmagnetoencephalography,” in Proceedings of the 2021\nIEEE International Conference on Acoustics, Speech\nand Signal Processing , ser. ICASSP 2021, 2021, pp.\n1280–1284.\n[14] S. Stober, D. J. Cameron, and J. A. Grahn, “Classifying\nEEG recordings of rhythm perception,” in Proceedings\nof the 15th International Society for Music Information\nRetrieval Conference , ser. ISMIR 2014, 2014, pp. 649–\n654.\n[15] S. Koelsch, V . K. Cheung, S. Jentschke, and J.-D.\nHaynes, “Neocortical substrates of feelings evoked\nwith music in the acc, insula, and somatosensory cor-\ntex,” Scientiﬁc Reports , vol. 11, no. 1, p. 10119, 2021.\n[16] M. E. Sachs, A. Habibi, A. Damasio, and J. T. Kaplan,\n“Decoding the neural signatures of emotions expressed\nthrough sound,” NeuroImage , vol. 174, pp. 1–10, 2018.\n[17] V . Putkinen, S. Nazari-Farsani, K. Seppälä, T. Kar-\njalainen, L. Sun, H. K. Karlsson, M. Hudson, T. T.\nHeikkilä, J. Hirvonen, and L. Nummenmaa, “Decoding\nmusic-evoked emotions in the auditory and motor cor-\ntex,” Cerebral Cortex , vol. 31, no. 5, pp. 2549–2560,\n2021.\n[18] I. Daly, D. Williams, F. Hwang, A. Kirke, E. R. Mi-\nranda, and S. J. Nasuto, “Electroencephalography re-\nﬂects the activity of sub-cortical brain regions during\napproach-withdrawal behaviour while listening to mu-\nsic,” Scientiﬁc Reports , vol. 9, no. 1, p. 9415, 2019.\n[19] S. Paquette, S. Takerkart, S. Saget, I. Peretz, and P. Be-\nlin, “Cross-classiﬁcation of musical and vocal emo-\ntions in the auditory cortex,” Annals of the New York\nAcademy of Sciences , vol. 1423, no. 1, pp. 329–337,\n2018.\n[20] X. Cui, Y . Wu, J. Wu, Z. You, J. Xiahou, and\nM. Ouyang, “A review: Music-emotion recognition\nand analysis based on EEG signals,” Frontiers in Neu-\nroinformatics , vol. 16, p. 997282, 2023.\n[21] D. S. Naser and G. Saha, “Inﬂuence of music liking\non EEG based emotion recognition,” Biomedical Sig-\nnal Processing and Control , vol. 64, p. 102251, 2021.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n203[22] S. Hoeﬂe, A. Engel, R. Basilio, V . Alluri, P. Toiviainen,\nM. Cagy, and J. Moll, “Identifying musical pieces from\nfMRI data using encoding and decoding models,” Sci-\nentiﬁc Reports , vol. 8, no. 1, pp. 1–13, 2018.\n[23] D. Sonawane, K. P. Miyapuram, B. Rs, and D. J. Lo-\nmas, “Guessthemusic: Song identiﬁcation from elec-\ntroencephalography response,” in Proceedings of the\n3rd ACM India Joint International Conference on Data\nScience & Management of Data (8th ACM IKDD\nCODS & 26th COMAD) , ser. CODS-COMAD ’21,\n2021, pp. 154–162.\n[24] P. Pandey, G. Sharma, K. P. Miyapuram, R. Subra-\nmanian, and D. Lomas, “Music identiﬁcation using\nbrain responses to initial snippets,” in Proceedings in\nthe 2022 IEEE International Conference on Acous-\ntics, Speech and Signal Processing , ser. ICASSP 2022,\n2022, pp. 1246–1250.\n[25] R. S. Schaefer, J. Farquhar, Y . Blokland, M. Sadakata,\nand P. Desain, “Name that tune: decoding music from\nthe listening brain,” NeuroImage , vol. 56, no. 2, pp.\n843–849, 2011.\n[26] D. Wu, C. Li, Y . Yin, C. Zhou, and D. Yao, “Mu-\nsic composition from the brain signal: Representing\nthe mental state by music,” Computational Intelligence\nand Neuroscience , vol. 2010, 2010.\n[27] S. Stober, T. Prätzlich, and M. Müller, “Brain beats:\nTempo extraction from EEG data.” in Proceedings of\nthe 17th International Society for Music Information\nRetrieval Conference , ser. ISMIR 2016, 2016, pp. 276–\n282.\n[28] I. Sturm, M. Treder, D. Miklody, H. Purwins, S. Dähne,\nB. Blankertz, and G. Curio, “Extracting the neural rep-\nresentation of tone onsets for separate voices of en-\nsemble music using multivariate EEG analysis.” Psy-\nchomusicology: Music, Mind, and Brain , vol. 25, no. 4,\np. 366, 2015.\n[29] N. Gang, B. Kaneshiro, J. Berger, and J. P. Dmo-\nchowski, “Decoding neurally relevant musical features\nusing canonical correlation analysis,” in Proceedings\nof the 18th International Society for Music Informa-\ntion Retrieval Conference , ser. ISMIR 2017, 2017, pp.\n131–138.\n[30] I. Daly, “Neural decoding of music from the EEG,” Sci-\nentiﬁc Reports , vol. 13, no. 1, pp. 1–17, 2023.\n[31] L. May, A. R. Halpern, S. D. Paulsen, and M. A. Casey,\n“Imagined musical scale relationships decoded from\nauditory cortex,” Journal of Cognitive Neuroscience ,\nvol. 34, no. 8, pp. 1326–1339, 2022.\n[32] S. Ntalampiras and I. Potamitis, “A statistical inference\nframework for understanding music-related brain ac-\ntivity,” IEEE Journal of Selected Topics in Signal Pro-\ncessing , vol. 13, no. 2, pp. 275–284, 2019.[33] G. M. Di Liberto, G. Marion, and S. A. Shamma,\n“Accurate decoding of imagined and heard melodies,”\nFrontiers in Neuroscience , vol. 15, p. 673401, 2021.\n[34] S. Stober, A. Sternin, A. M. Owen, and J. A. Grahn,\n“Towards music imagery information retrieval: Intro-\nducing the OpenMIIR dataset of EEG recordings from\nmusic perception and imagination.” in Proceedings of\nthe 16th International Society for Music Information\nRetreival Conference , ser. ISMIR 2015, 2015, pp. 763–\n769.\n[35] A. Ofner and S. Stober, “Modeling perception with\nhierarchical prediction: Auditory segmentation with\ndeep predictive coding locates candidate evoked po-\ntentials in EEG,” in Proceedings of the 21th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ser. ISMIR 2020, 2020, pp. 566–573.\n[36] E. Cano, D. FitzGerald, A. Liutkus, M. D. Plumbley,\nand F.-R. Stöter, “Musical source separation: An intro-\nduction,” IEEE Signal Processing Magazine , vol. 36,\nno. 1, pp. 31–40, 2019.\n[37] E. Manilow, P. Seetharman, and J. Salamon, “Open\nsource tools & data for music source separation,”\n2020. [Online]. Available: https://source-separation.\ngithub.io/tutorial\n[38] G. Cantisani, G. Trégoat, S. Essid, and G. Richard,\n“MAD-EEG: an EEG dataset for decoding auditory at-\ntention to a target instrument in polyphonic music,” in\nProceedings of the Speech, Music and Mind (SMM),\nSatellite Workshop of Interspeech 2019 , 2019.\n[39] G. Cantisani, S. Essid, and G. Richard, “Neuro-steered\nmusic source separation with eeg-based auditory at-\ntention decoding and contrastive-nmf,” in Proceedings\nof the 2021 IEEE International Conference on Acous-\ntics, Speech and Signal Processing , ser. ICASSP 2021,\n2021, pp. 36–40.\n[40] E. J. Allen, M. Moerel, A. Lage-Castellanos,\nF. De Martino, E. Formisano, and A. J. Oxenham, “En-\ncoding of natural timbre dimensions in human auditory\ncortex,” NeuroImage , vol. 166, pp. 60–70, 2018.\n[41] V . Alluri, P. Toiviainen, I. P. Jääskeläinen, E. Glerean,\nM. Sams, and E. Brattico, “Large-scale brain networks\nemerge from dynamic processing of musical timbre,\nkey and rhythm,” NeuroImage , vol. 59, no. 4, pp. 3677–\n3689, 2012.\n[42] M. Ogg, D. Moraczewski, S. E. Kuchinsky, and\nL. R. Slevc, “Separable neural representations of sound\nsources: Speaker identity and musical timbre,” Neu-\nroImage , vol. 191, pp. 116–126, 2019.\n[43] S. Rouard, F. Massa, and A. Défossez, “Hybrid trans-\nformers for music source separation,” in Proceedings\nof the 2023 IEEE International Conference on Acous-\ntics, Speech and Signal Processing , ser. ICASSP 2023,\n2023.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n204[44] O. Esteban, C. J. Markiewicz, R. W. Blair, C. A.\nMoodie, A. I. Isik, A. Erramuzpe, J. D. Kent,\nM. Goncalves, E. DuPre, M. Snyder, H. Oya, S. S.\nGhosh, J. Wright, J. Durnez, R. A. Poldrack, and\nK. J. Gorgolewski, “fmriprep: a robust preprocessing\npipeline for functional MRI,” Nature Methods , vol. 16,\nno. 1, pp. 111–116, 2019.\n[45] W. D. Penny, K. J. Friston, J. T. Ashburner, S. J. Kiebel,\nand T. E. Nichols, Statistical parametric mapping: the\nanalysis of functional brain images . Elsevier, 2011.\n[46] J. A. Mumford, T. Davis, and R. A. Poldrack, “The im-\npact of study design on pattern estimation for single-\ntrial multivariate pattern analysis,” NeuroImage , vol.\n103, pp. 130–138, 2014.\n[47] M. F. Glasser, T. S. Coalson, E. C. Robinson, C. D.\nHacker, J. Harwell, E. Yacoub, K. Ugurbil, J. Anders-\nson, C. F. Beckmann, M. Jenkinson, S. M. Smith, and\nD. C. Van Essen, “A multi-modal parcellation of hu-\nman cerebral cortex,” Nature , vol. 536, no. 7615, pp.\n171–178, 2016.\n[48] A. Horn, “HCP-MMP1.0 projected on MNI2009a\nGM (volumetric) in NIfTI format,” 2016. [Online].\nAvailable: https://ﬁgshare.com/articles/dataset/\nHCP-MMP1_0_projected_on_MNI2009a_GM_\nvolumetric_in_NIfTI_format/3501911\n[49] A. Abraham, F. Pedregosa, M. Eickenberg, P. Gervais,\nA. Mueller, J. Kossaiﬁ, A. Gramfort, B. Thirion, and\nG. Varoquaux, “Machine learning for neuroimaging\nwith scikit-learn,” Frontiers in Neuroinformatics , p. 14,\n2014.\n[50] S. Koelsch, “Neural substrates of processing syntax\nand semantics in music,” Current Opinion in Neuro-\nbiology , vol. 15, no. 2, pp. 207–212, 2005.\n[51] R. J. Zatorre, P. Belin, and V . B. Penhune, “Structure\nand function of auditory cortex: music and speech,”\nTrends in Cognitive Sciences , vol. 6, no. 1, pp. 37–46,\n2002.\n[52] Y . Zhang, H. Ruan, Z. Yuan, H. Du, X. Gao, and\nJ. Lu, “A learnable spatial mapping for decoding the\ndirectional focus of auditory attention using eeg,” in\nProceedings of the 2023 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing , ser.\nICASSP 2023, 2023.\n[53] A. Floren, B. Naylor, R. Miikkulainen, and D. Ress,\n“Accurately decoding visual information from fMRI\ndata obtained in a realistic virtual environment,” Fron-\ntiers in Human Neuroscience , vol. 9, p. 327, 2015.\n[54] T. Horikawa and Y . Kamitani, “Generic decoding of\nseen and imagined objects using hierarchical visual\nfeatures,” Nature Communications , vol. 8, no. 1, p.\n15037, 2017.[55] Y .-T. Wu, H.-Y . Chen, Y .-H. Liao, L.-W. Kuo, and\nC.-C. Lee, “Modeling perceivers neural-responses us-\ning lobe-dependent convolutional neural network to\nimprove speech emotion recognition.” in Proceed-\nings of the 18th Annual Conference of the Interna-\ntional Speech Communication Association , ser. IN-\nTERSPEECH 2017, 2017, pp. 3261–3265.\n[56] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell,\nand S. Xie, “A convnet for the 2020s,” in Proceedings\nof the IEEE/CVF conference on computer vision and\npattern recognition , 2022, pp. 11 976–11 986.\n[57] I. Loshchilov and F. Hutter, “Decoupled weight decay\nregularization,” in Proceedings of the 7th International\nConference on Learning Representations , ser. ICLR\n2019, 2019.\n[58] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach\nto interpreting model predictions,” in Proceedings of\nthe 31st International Conference on Neural Infor-\nmation Processing Systems , ser. NIPS’17, 2017, p.\n4768–4777.\n[59] E. J. Humphrey, S. Reddy, P. Seetharaman, A. Kumar,\nR. M. Bittner, A. Demetriou, S. Gulati, A. Jansson,\nT. Jehan, B. Lehner, A. Krupse, and L. Yang, “An intro-\nduction to signal processing for singing-voice analysis:\nHigh notes in the effort to automate the understanding\nof vocals in music,” IEEE Signal Processing Magazine ,\nvol. 36, no. 1, pp. 82–94, 2018.\n[60] C. Gupta, H. Li, and M. Goto, “Deep learning ap-\nproaches in topics of singing information processing,”\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , vol. 30, pp. 2422–2451, 2022.\n[61] W. Trost, T. Ethofer, M. Zentner, and P. Vuilleumier,\n“Mapping aesthetic musical emotions in the brain,”\nCerebral Cortex , vol. 22, no. 12, pp. 2769–2783, 2012.\n[62] S. Koelsch and S. Skouras, “Functional centrality of\namygdala, striatum and hypothalamus in a “small-\nworld” network underlying joy: An fmri study with\nmusic,” Human Brain Mapping , vol. 35, no. 7, pp.\n3485–3498, 2014.\n[63] P. N. Juslin, “From everyday emotions to aesthetic\nemotions: Towards a uniﬁed theory of musical emo-\ntions,” Physics of Life Reviews , vol. 10, no. 3, pp. 235–\n266, 2013.\n[64] R. J. Zatorre, J. L. Chen, and V . B. Penhune, “When\nthe brain plays music: auditory–motor interactions in\nmusic perception and production,” Nature reviews neu-\nroscience , vol. 8, no. 7, pp. 547–558, 2007.\n[65] C. L. Gordon, P. R. Cobb, and R. Balasubramaniam,\n“Recruitment of the motor system during music listen-\ning: An ale meta-analysis of fmri data,” PloS ONE ,\nvol. 13, no. 11, p. e0207213, 2018.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n205[66] J. K. Bizley and Y . E. Cohen, “The what, where and\nhow of auditory-object perception,” Nature Reviews\nNeuroscience , vol. 14, no. 10, pp. 693–707, 2013.\n[67] V . K. M. Cheung and S. Sakamoto, “Separating uncer-\ntainty from surprise in auditory processing with neu-\nrocomputational models: Implications for music per-\nception,” Journal of Neuroscience , vol. 42, no. 29, pp.\n5657–5659, 2022.\n[68] S. K. Zieli ´nski, P. Hardisty, C. Hummersone, and\nF. Rumsey, “Potential biases in MUSHRA listening\ntests,” in Proceedings of the 123rd Audio Engineering\nSociety Convention , ser. AES 123, 2007.\n[69] A. Furnham, “Response bias, social desirability and\ndissimulation,” Personality and Individual Differences ,\nvol. 7, no. 3, pp. 385–400, 1986.\n[70] G. Kalton and H. Schuman, “The effect of the question\non survey responses: A review,” Journal of the Royal\nStatistical Society: Series A (General) , vol. 145, no. 1,\npp. 42–57, 1982.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n206"
    },
    {
        "title": "Towards a New Interface for Music Listening: A User Experience Study on YouTube.",
        "author": [
            "Ahyeon Choi",
            "Eunsik Shin",
            "Haesun Joung",
            "Joongseek Lee",
            "Kyogu Lee"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265333",
        "url": "https://doi.org/10.5281/zenodo.10265333",
        "ee": "https://zenodo.org/records/10265333/files/000058.pdf",
        "abstract": "In light of the enduring success of music streaming services, it is noteworthy that an increasing number of users are positively gravitating toward YouTube as their preferred platform for listening to music. YouTube differs from traditional music streaming services in that they provide a diverse range of music-related videos as well as soundtracks. However, notwithstanding the surge in the platform's utilization as a music consumption tool, there is a lack of thorough research on the phenomenon. To investigate its usability and interface satisfaction as a music listening tool, we conducted semi-structured interviews with 27 users who listen to music through YouTube more than three times a week. Our qualitative analysis found that YouTube has five main meanings for users as a music streaming service: 1) exploring musical diversity, 2) sharing unique playlists, 3) providing visual satisfaction, 4) facilitating user interaction, and 5) allowing free and easy access. We also propose wireframes of a video streaming service for better audio-visual music listening in two stages: search and listening. By these wireframes, we offer practical solutions to enhance user satisfaction with YouTube for music listening. It has implications not only for YouTube but also for other streaming services for music.",
        "zenodo_id": 10265333,
        "dblp_key": "conf/ismir/ChoiSJLL23",
        "keywords": [
            "Music streaming services",
            "YouTube",
            "increasing number of users",
            "music consumption tool",
            "research on the phenomenon",
            "qualitative analysis",
            "five main meanings",
            "video streaming service",
            "audio-visual music listening",
            "practical solutions"
        ],
        "content": "TOWARDS A NEW INTERFACE FOR MUSIC LISTENING:\nA USER EXPERIENCE STUDY ON YOUTUBE\nAhyeon Choi Eunsik Shin Haesun Joung Joongseek Lee Kyogu Lee\nDepartment of Intelligence and Information, Seoul National University\n{chah0623, esshin, gotjs3841, joonlee8, kglee}@snu.ac.kr\nABSTRACT\nIn light of the enduring success of music streaming ser-\nvices, it is noteworthy that an increasing number of users\nare positively gravitating toward YouTube as their pre-\nferred platform for listening to music. YouTube differs\nfrom typical music streaming services in that they provide\na diverse range of music-related videos as well as sound-\ntracks. However, despite the increasing popularity of us-\ning YouTube as a platform for music consumption, there is\nstill a lack of comprehensive research on this phenomenon.\nAs independent researchers unafﬁliated with YouTube, we\nconducted semi-structured interviews with 27 users who\nlisten to music through YouTube more than three times a\nweek to investigate its usability and interface satisfaction.\nOur qualitative analysis found that YouTube has ﬁve main\nmeanings for users as a music streaming service: 1) ex-\nploring musical diversity, 2) sharing unique playlists, 3)\nproviding visual satisfaction, 4) facilitating user interac-\ntion, and 5) allowing free and easy access. We also propose\nwireframes of a video streaming service for better audio-\nvisual music listening in two stages: search and listening.\nBy these wireframes, we offer practical solutions to en-\nhance user satisfaction with YouTube for music listening.\nThese ﬁndings have wider implications beyond YouTube\nand could inform enhancements in other music streaming\nservices as well.\n1. INTRODUCTION\nIn recent years, the music streaming industry has witnessed\na signiﬁcant surge in popularity, with market leaders such\nas Spotify, Apple Music, and Amazon Music dominating\nthe market [1]. Alongside this trend, YouTube has solidi-\nﬁed its position as a prominent platform for diverse video\ncontent, including documentaries, daily vlogs, entertain-\nment shows, and more. As users ﬂocked to YouTube for\nvarious types of content, the platform naturally became\na hub for music-related videos as well. Users now have\neasy access to a wide range of music video content on\n© Ahyeon Choi, Eunsik Shin, Haesun Joung, Joongseek\nLee, and Kyogu Lee. Licensed under a Creative Commons Attribution\n4.0 International License (CC BY 4.0). Attribution: Ahyeon Choi, Eun-\nsik Shin, Haesun Joung, Joongseek Lee, and Kyogu Lee, “Towards a New\nInterface for Music Listening: A User Experience Study on YouTube”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.YouTube, contributing to the growing trend of consuming\nmusic through video formats [2].\nIndeed, YouTube delivers a distinctive multi-sensory\nexperience by showcasing a vast variety of music-related\nvideos such as music videos, live performances, curated\nplaylists with visual artworks, and cover performances,\nenabling users to enjoy music through a fusion of visual\nand auditory elements. Despite Spotify’s global promi-\nnence based on subscribers, YouTube has seen an increas-\ning number of users turning to its platform for music con-\nsumption [1, 3]. This trend is evident in regions like South\nKorea [4, 5] and Latin America [6], where YouTube domi-\nnates as a preferred music platform.\nGiven YouTube’s current dominance in music con-\nsumption, there’s a need for a more comprehensive investi-\ngation into this behavior and patterns. Earlier studies have\nexplored YouTube’s role as a streaming service [7], com-\npared its usability with Spotify [3], and analyzed music\nconsumption behavior on YouTube [8]. However, the el-\nements contributing to YouTube’s rise as a primary music\nplatform and the actual levels of user satisfaction are still\nnot fully understood, indicating a need for further user-\nfocused research.\nThus, this study aims to conduct in-depth interviews\nwith music consumers on YouTube, examining their be-\nhavior, comparing the advantages and disadvantages of us-\ning YouTube as a music consumption tool with other music\nstreaming services, and reevaluating YouTube’s standing\nas a tool for music consumption. Additionally, we pro-\npose a new interface design that enhances the usability of\nmusic-related searches and listening. This research was\nconducted independently by our team, with no ﬁnancial\nbacking or data provided by YouTube or any associated or-\nganization. With our study, we aim to contribute to the on-\ngoing conversation on YouTube’s role as a music platform\nand offer insights into developing an innovative interface\nthat elevates the user’s music listening experience.\n2. RELATED WORK\nIn the ﬁeld of music information retrieval (MIR), research\non music streaming services includes studies on improving\nrecommendation algorithms [9–11], understanding user\nbehavior and patterns of use [12–16], and studying user\nexperiences and interfaces [17–21]. These studies aimed\nto enhance overall user satisfaction and engagement with\nmusic streaming services by providing personalized rec-\nommendations, improving the user interface, and identify-492ing the factors that inﬂuenced user behaviors and prefer-\nences.\nCompared to other music streaming services, research\non music consumption through YouTube has only recently\ngained attention due to the platform’s relatively late recog-\nnition as a music consumption platform. Early studies\non YouTube’s music videos have revealed that music is\nthe most consumed content category on YouTube, and re-\nsearchers have classiﬁed the types of YouTube’s music\ncontent while analyzing their differences [7]. Furthermore,\n[3] reported that YouTube is used as frequently as Spotify\nand is perceived as superior to Spotify in terms of its share-\nability and accessibility.\nAs YouTube’s inﬂuence in music consumption grows,\nrecent research has examined three types of online mu-\nsic practices according to the role YouTube plays: default,\nsoundtracking, and complementary platforms [8]. Authors\nreport that one of the main results is that YouTube’s music\nvideos are listened to, rather than watched. However, the\nsigniﬁcance of visual elements in music listening can dif-\nfer based on the genre or content. Additionally, it is worth\nmentioning that the participants in the study reported only\noccasional use of YouTube for music, which may limit the\ngeneralizability of the ﬁndings to other contexts, such as\nfrequent YouTube users.\nTherefore, this study aims to examine the usage behav-\nior of users who use YouTube more than three times a\nweek in everyday situations, report on the characteristics\nof the subject group, and classify the content used. In ad-\ndition, we draw out advantages and disadvantages through\nusability tests to newly consider the role of YouTube as\na music-listening tool. Moreover, the study proposes in-\nterface improvement measures to ﬁll the research gap on\n\"how to improve the music listening environment through\nYouTube.\" Considering the diverse range of devices used\nto access YouTube, including mobile devices, PCs, tablets,\nand TVs, we primarily focus on the mobile device, taking\ninto account its widespread usage among participants.\n3. METHODS\n3.1 Participant\nWe recruited 27 Seoul National University students (12\nmales, 15 females) aged 18 or older (mean=23.40,\nsd=3.13). Our recruitment focused on participants who\nlisten to music on YouTube at least three times a week\nwhile excluding those who rely solely on YouTube Music\nwithout using YouTube. This approach allowed us to con-\ncentrate on the distinct characteristics of consuming music\nthrough videos on YouTube, which encompass both visual\nelements and audio. Participants were compensated with\na cash payment of KRW 10,000. Ethics approval was ob-\ntained from the Institutional Review Board of SNU.\n3.2 Study Design\nInformed by previous studies’ methodologies and the spe-\nciﬁc needs of our research, we designed our interview in\ntwo stages: a preliminary questionnaire [22, 23], followedPhase Requirements\nA Verbal\nInterviewAsking about participants’ music\nlistening habits and preferences along\nwith the motivation to use YouTube.\nUsability\nTestComparison of YouTube and other\nmusic streaming services and\nfeedback on the interface of YouTube\nfor searching and listening to music.\nUI proposalPropose YouTube interface design for\nmusic listening freely, and explain\nyourself.\nTable 1 . Three steps of semi-structured interview\nby a semi-structured interview [22–24] that includes a brief\nice-breaking session [25]. The preliminary questionnaire\ncollects demographic data and music consumption habits\nof the participants, such as their academic majors, rela-\ntionship with music, frequency and duration of YouTube\nuse for music, and speciﬁc contexts of YouTube music\nconsumption (excluding YouTube Music). Additionally,\nwe also sought information regarding their subscription to\nYouTube Premium or usage of YouTube Music.\nFollowing an ice-breaking session, the semi-structured\ninterview proceeded with three main segments (Table 1).\nFirst, we explored participants’ regular music consump-\ntion habits, such as frequency, platform preference, and\ncontent preferences. Second, participants were asked to\ndemonstrate the process of searching and listening to mu-\nsic on YouTube, which allowed for a natural exploration\nof the platform’s advantages and disadvantages in com-\nparison to other music streaming services. Third, partic-\nipants utilized empty interface templates on iPad to design\na new interface for music searching and listening, enabling\nthem to customize the screen ratio, functions, buttons, and\nmore. Each interview, lasting roughly 30-40 minutes, was\nrecorded and transcribed using NA VER Clova Note, with\nparticipant consent.\n3.3 Analysis\nWe identiﬁed the overarching themes and trends of the par-\nticipants’ responses and organized the data accordingly.\nThe data were categorized into the following topics: pri-\nmary streaming service, weekly listening time, music lis-\ntening type, preferred music genres or content on YouTube,\nsituations YouTube is used for music listening, reasons for\nusing YouTube as a music consumption tool, music search\nmethods on YouTube, criteria for video selection, advan-\ntages and disadvantages of YouTube compared to other ser-\nvices, and a summary of interface proposal sessions.\nWe generated a list of keywords for the qualitative anal-\nysis, which includes the advantages and disadvantages of\nusing YouTube for music listening and user interface pro-\nposals from interviewees. To validate our classiﬁcations\nand identify commonalities, we repeated the process of\nanalysis and consensus-building three times among the re-\nsearchers similar to the analysis process in [22, 26, 27].\nGrounded theory [28] and content analysis [29] were alsoProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n493used as a guide throughout the process of keyword genera-\ntion. We also referred to previous qualitative studies in the\nﬁeld of MIR [8,15,24,26,27] to guide our data analysis, as\nwell as to ensure consistency in our reporting and citation\npractices. Finally, we thoroughly reviewed the keyword\nlists to extract the main ﬁndings of how YouTube is used\nas a music consumption tool by the participants based on\nthe method of theme analysis [30].\nTo better understand the participants’ interface design\nproposals, we compared the proposals from the partici-\npants and reviewed the summary of the interface designing\nsessions. From this process, we synthesized useful design\nimplications and arrived at wireframe designs for the mu-\nsic search and listening screens.\n4. RESULT\n4.1 Behavior and Characteristics of Music\nConsumption on YouTube\nAs the current study investigates interview data from a\nsample of 27 users, it is important to take into account the\nunique characteristics of this group. Therefore, informa-\ntion concerning the participants’ music consumption be-\nhaviors and preferences was gathered through preliminary\nsurveys and interviews. The results showed that partici-\npants typically used YouTube about ﬁve times per week\n(mean = 4.89, sd = 1.93), for a total of approximately\nﬁve hours (mean = 5.35, sd = 3.76), to listen to music\nwhile engaging in various activities, such as studying, re-\nlaxing, commuting, and exercising. No one specialized\nin music. The majority of participants used the free ver-\nsion of YouTube and did not subscribe to YouTube Pre-\nmium. Additionally, some participants supplemented their\nmusic listening with other platforms such as YouTube Mu-\nsic, Melon, Spotify, and Genie.\nParticipants enjoyed a diverse range of music genres on\nYouTube. The top ﬁve genres mentioned most frequently\nwere OST (original soundtrack of movies or dramas, 13\ntimes), pop (12 times), K-pop (11 times), classical music\n(7 times), and indie music (7 times). Other genres men-\ntioned in order of frequency include J-pop, ballads, old-\nfashioned music (mid-20th-century Korean pop and bal-\nlads), jazz, rock, band music (with live instrumentation and\nelements of rock, pop, and indie), new age, hip-hop, EDM,\nand R&B. Music content can be broadly divided into three\ncategories: 1) Ofﬁcial music content such as music videos,\n2) Live music content such as performances, concerts, fes-\ntivals, and 3) User-generated content such as playlists and\ncover videos. In terms of frequency of mention, the order\nwas 3-2-1 (27 times, 25 times, 8 times) respectively.\n4.2 Advantages of using YouTube for music listening\nAlongside our anticipation that YouTube serves as an\naudiovisual music listening tool, we found that YouTube\npossesses various strengths compared to other streaming\nservices (Table 2). Musical diversity was the most fre-\nquently mentioned category, with two main points: the\navailability of non-ofﬁcial music in addition to ofﬁcialCategory Keyword Freq Total\nMusical Diversityofﬁcial soundtrack + α 1423playlist 9\nConveniencefamiliarity 4\n15accessibility 4\nsubscription fee 4\nCustomizing 3\nUser Interactionrecommendation 1013comments 3\nVisual Contentsthumbnail 610video 4\netc. etc. 1 1\nTable 2 . Pros. keywords of usability test\nreleases, and the diversity of playlist content compared to\nother streaming services.\nWith streaming services, I can only listen to ofﬁcial re-\nleases, but with YouTube, I can listen to not only ofﬁcial\nreleases but covers and other user-generated content.\n(P11)\nUnlike other services, YouTube’s diverse playlists prevent\nrepetitive listening by offering a wide range of songs\nwithin similar genres. (P26)\nAlso, convenience was mentioned as an advantage,\nwith familiarity, accessibility, no subscription fee, and\nuser customization.\nI use it because I’m used to it. I’ve used Melon and\nYouTube Music before, but I settled with YouTube be-\ncause it was more convenient. (P12)\nSince YouTube is free, there’s no need to pay for other\nservices. (P8)\nAs for user interaction, most users mentioned the\nrecommendation algorithm itself and the ability to view\nother users’ opinions through comments.\nThe recommendation algorithm is good. I often ﬁnd great\nnew songs through it. (P17)\nIt’s good to be able to see other people’s opinions and\nsympathize by reading comments. (P4)\nLastly, the visual content of thumbnails and videos was\nmentioned as an advantage.\nI can use both sight and sound when listening to music\nwith videos. (P4)\nWhen I play playlists with thumbnails, like at a house-\nwarming party, it adds to the atmosphere, and it’s good\nfor interior purposes too. (P6)\nWhile we initially expected the inclusion of visual ele-\nments to be a signiﬁcant advantage of YouTube, the par-\nticipants’ usage patterns proved more diverse. Some ap-\npreciated the visual components, while others turned to\nYouTube strictly for audio during activities like work or\nsleep (P2, P5, P22, P23, P24). These observations align\nwith prior research [8], showing the varied ways users uti-\nlize YouTube for music. Although some mentioned listen-\ning to audio with the screen off (P7, P23, P24), we ex-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n494Category Keyword Freq Total\nUser Interactioncomments 1221recommendation 9\nManipulationbutton / tap 1017display ratio 7\nPlaylistplaylist contents 4\n10 making playlist 4\nmix playlist 2\nSection Searchtimestamp 69playback bar 3\nLack of Info.song information 58log information 3\nUnderutilizationreplay 58volume control 3\nContents Qualitysound quality 56video quality 1\netc.(video) data size 46etc. 2\nTable 3 . Cons. keywords of usability test\ncluded this aspect from our analysis as it’s a feature exclu-\nsive to YouTube Premium subscribers.\n4.3 Disadvantages of using YouTube for music\nlistening\nThe inconveniences and disadvantages of listening to\nmusic on YouTube were categorized into seven major\nthemes (Table 3). The most frequently mentioned in-\nconvenience was related to user interaction, with many\ncomplaints about the inconvenience of ﬁltering the desired\ninformation while exploring recommended videos and\ncomments.\nIn other music streaming services, genre separation is\nclearly done, but YouTube recommends based on the\nvideos you watch, so there is a tendency to lean towards\na speciﬁc genre. (P26)\nWhen watching music videos and reading comments, it’s\nhard to ﬁnd South Korean users’ reactions when most\ncomments are in foreign languages. (P11)\nThe second most frequently mentioned disadvantage\nwas related to screen manipulation, such as ﬁxed thumb-\nnails, the ratio of videos, and accidental button presses.\nIt would be nice if I could reduce the screen ratio. I want\nto watch the small screen when exercising or doing other\nthings. (P8)\nThere are cases where I accidentally press the Shorts\nbutton and the music stops. (P22)\nRegarding playlists, users complained about not having\ntimestamps for individual songs, the content of playlists\nmade by others, the process of creating playlists them-\nselves, and the mixes provided by YouTube.\nIt’s inconvenient to switch to another song if there is no\ntimestamp in the playlist. (P18)\nSince playlists are made by others, there are few cases\nwhere all songs suit my taste, and there are mediocresongs in between. (P18)\nIt’s inconvenient to save songs one by one in my li-\nbrary. It feels slow every time I press the save button, and\nit is a hassle to press the button several times to save. (P27)\nLastly, some users mentioned the lack of information\nabout album or song information and lyrics, as well as the\nlack of log information about previously watched videos\nas a disadvantage.\nIt’s hard to ﬁnd album or song information, and it’s\nfrustrating not knowing the information of the concert I\nam watching. (P7)\nWhen I use the autoplay function, it is hard to ﬁnd which\nsong I thought I liked. (P17)\nDespite the existence of autoplay and volume control\nfeatures on YouTube, user complaints arose from a lack\nof information about these functions. Some users viewed\nthem as drawbacks, unaware of their existence or location.\nSpeciﬁcally, enabling autoplay requires navigating to the\nsettings, while ﬁne-tuning volume necessitates physical\ndevice button use. This complexity may have heightened\nuser frustration and dissatisfaction.\nI wish there is a autoplay button. (P12)\nI want to make minute adjustments, but even if I increase\nthe volume level by just one, the volume suddenly\nbecomes too loud. (P16)\nThe quality of the content is related to the audio or\nvideo quality. Some responses showed low reliability in\naudio quality when used for music listening.\nThere are cases where the sound quality is poor in content\nuploaded by individual users. (P12)\nAside from that, there were four mentions of concerns\nabout mobile data usage due to large video data size (P2,\nP9, P11, P25), one mention of discomfort with provocative\ntitles (P23) and experiencing an error when randomly play-\ning saved videos (P14). There were nine mentions related\nto ads or background playback (P2, P5, P6, P8, P9, P12,\nP17, P19, P25), but these were excluded from the analy-\nsis since they can be resolved with a YouTube premium\nsubscription.\n4.4 User Feedback for Interface Improvements\nWe analyzed users’ explanations and drawings of the\nsearching and listening screens, categorizing their de-\nmands for interface improvement into three categories: ad-\ndition, modiﬁcation, and deletion. These categories, along\nwith relevant quotes, provide speciﬁc descriptions of users’\ninterface improvement suggestions.\nThe ﬁrst is to request the addition of new information\nor functions that are currently absent on YouTube, such as\na new button or tab, new sorting and ﬁltering criteria, or\nmore information about contents and songs.\nIt would be great if there was a detailed search button\nunder the search bar , where you could search by year,\nalbum, composer, etc. (P7)Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n495It would be nice if I can choose options between \"all\nvideos recommended\" and \"music-related recommenda-\ntions\" in the recommendation section. (P9)\nThe second is to modify the existing functions or\nconﬁguration of YouTube to increase operability and\nefﬁciency when searching and listening to music, such as\nchanging the ratio of various spaces on the interface or\nchanging the positions of existing buttons and information.\nIt would be great if the thumbnail (album cover) could be\nsmaller, and the title, artist, etc. could be displayed next\nto it. (P12)\nIt would be nice to adjust the ratio of the comment box\nand recommended videos so that you can view them\ntogether. (P4)\nLastly, there were cases where demands were made to\nremove things from the existing YouTube interface that\nare not directly related to music searching or listening.\nWe don’t need the buttons for uploading videos on the\nbottom menu bar. It would be great if we could freely\nconﬁgure this menu bar. (P15)\nIf we could hide the buttons we don’t use often and press\nthe detailed button to show them, it would be neat. (P19)\n5. FINDINGS AND DICSUSSION\n5.1 Role of YouTube as a music streaming service\nUsers desire an improved interface for YouTube to maxi-\nmize its potential as a music consumption tool. We have\nidentiﬁed ﬁve key roles that YouTube plays in music lis-\ntening, and based on this, we propose design implications\nto enhance the user experience.\n5.1.1 Exploring musical diversity\nYouTube offers users a wide variety of musical genres,\nartists, and songs to discover and explore, including rare or\nunreleased music not found on other streaming services.\nUsers can also enjoy various versions of the same song\nthrough covers or live performances by different artists.\nDesign Implication: To improve search efﬁciency, music\ncontent should be categorized by genre, artist, and mood,\nand album information such as lyrics should be provided\nto reduce the need to search for information on other plat-\nforms.\n5.1.2 Sharing unique playlists\nYouTube creators can create and share playlists, simplify-\ning the search process and enabling them to select playlists\nbased on keywords like mood or activity (e.g. warm spring\nday, driving playlist). Design Implication: Playlists\nshould provide song and timeline information, and allow\nusers to switch to the next song with a button. Allow-\ning users to customize songs within the playlist, such as\nadding or removing them, and saving these changes, would\nenhance the playlist’s functionality.5.1.3 Providing visual satisfaction\nBy offering sensory satisfaction beyond just videos,\nYouTube’s visual content enhances the music listening ex-\nperience. Users appreciate being able to observe the mu-\nsicians’ expressions, gestures, and style, and sometimes\neven watch music videos solely for visual gratiﬁcation like\nrepetitive animations or thumbnail images paired with the\nmusic. Design Implication: The screen size and ratio of\nthe video should be customizable based on the content’s\ncharacteristics and users’ listening environment. For ex-\nample, users would like the option to decrease the video\nscreen size in public settings or enlarge it to focus on a\nparticular idol member or musician’s ﬁnger movements.\n5.1.4 Facilitating user interaction\nYouTube’s likes, dislikes, subscriptions, and comments\nfeatures enable users to interact with the platform and fos-\nter a sense of community, resulting in a more engaging\nmusic listening experience. Additionally, the recommen-\ndation algorithm lets users explore new content and see\nhow others react to music, which is a key motivation for\nusers to use YouTube. Design Implication: Users should\nbe able to sort and ﬁlter recommendations and comments\nbased on various criteria, such as timeline, keyword, lyrics\nor the most frequently mentioned, to expand YouTube’s so-\ncial function. Pinning speciﬁc comments that users like or\nrefer to frequently could also reduce search time.\n5.1.5 Allowing free and easy access\nYouTube’s accessibility, cost-effectiveness, and cross-\ndevice compatibility make it a convenient option for users\nto listen to music in various situations. This versatility\nhas led some users to cease subscribing to other stream-\ning services. Primarily, the appeal lies in the free access\nto a diverse library of music videos, live performances,\ncovers, and user-generated content, resonating with users\ndisinclined to pay for music subscriptions. Design Im-\nplication: While device-speciﬁc interfaces are important,\nconsistent usability is crucial to prevent user confusion or\ninconvenience.\nIt is crucial to acknowledge that while some of the\nproposed features (e.g., album and artist ﬁlters, lyrics,\nsmaller screen mode) have already been implemented in\nthe YouTube Music App, users still rely on YouTube to ac-\ncess a diverse range of music videos that are not available\non the YouTube Music App. Therefore, our design impli-\ncations hold the potential to differentiate YouTube from\nYouTube Music by catering to the experience of video\nstreaming alongside music consumption.\n5.2 UI for Audio-Visual Music Streaming Platform\nTaking into account the role of YouTube as a music stream-\ning service, the needs of its users, and the interface designs\nof typical music streaming services, we have developed an\nideal wireframe for an audio-visual music listening plat-\nform. It consists of two stages: (a) searching and (b) lis-\ntening screens (Figure 1).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n496(a) Searching\n (b) Listening\nFigure 1 . A Wireframe of youtube UI for music listening\n(a) Searching To display diverse music content tailored\nto users’ interests, we added 1) advanced search function-\nality to the top keyword search bar, allowing users to ﬁl-\nter by era, genre, artist, and other details. Additionally,\nwe added 2) a button to easily add multiple videos to a\nuser’s playlist, and 3) reduced the thumbnail size to show\nmore videos on one screen. Next to the thumbnail, we in-\ncluded 4) information about the songs in the video, and if\nthe video is a playlist, we added 5) a timeline and infor-\nmation about the included songs. Finally, we made 6) the\nbottom menu buttons customizable, allowing users to re-\nmove buttons when they feel unnecessary and create their\nown menu.\n(b) Listening While maintaining the current structure\nof the interface, we adjusted the layout and added new\nfeatures to enhance the music listening experience. 1)\nAdding a toggle button that allows users to exchange be-\ntween video watching and music listening. Users can use\ntheir ﬁngers to 2) zoom in or out of the video to adjust\nits size. Previously, users had to click the video to access\nplayback and skip buttons, but we located 3) the playback\nbar and related functions at the bottom of the video. We\nalso made the 4) repeat button more visible. We added\n5) a toggle button to expand or shorten album information\nor lyrics, and made 6) comments expandable in a similar\nmanner, with a function for users to pin comments they\nwant to keep visible. We added 7)a ﬁltered recommenda-\ntion feature to suggest reduced-size videos based on spe-\nciﬁc user-selected ﬁlters. This allows for easier exploration\nof related content through horizontal scrolling.\nThe ﬁndings of this research hold potential for applica-\ntion across a variety of streaming services. Features such\nas advanced search functions, customizable menus, and en-\nhanced playlist capabilities can improve user engagement\nand satisfaction. Effective presentation of music-related\ninformation enriches the listening experience, while ad-\nditional functionalities such as video zoom or commentpinning foster a personalized user experience. These ﬁnd-\nings can signiﬁcantly beneﬁt YouTube, as well as aid other\nmusic streaming platforms like Spotify, Apple Music, and\nAmazon Music, and video streaming services including\nmusic videos, like Bilibili and Vimeo, in optimizing their\ninterfaces according to their unique characteristics and\nusers’ needs.\n6. CONCLUSION\nThis study explored the music listening behaviors of\nYouTube users and analyzed the advantages and disadvan-\ntages of YouTube as a music streaming service. We pro-\nposed new interface wireframes to improve usability and\nre-examined YouTube’s role as a tool for music listening.\nUndoubtedly, there are constraints in actualizing the pro-\nposed interface fully on YouTube. Nevertheless, some sug-\ngestions on improving visual satisfaction, comment explo-\nration, and toggle button to exchange the interface between\nvideo watching and music listening mode could be consid-\nered in designing the overall interface of video streaming\nplatforms.\nOur study has limitations depending on the small sam-\nple size of Korean users, and it is essential to consider\nseveral important factors. Firstly, our interface design pri-\nmarily focused on mobile environments, which may limit\nits direct applicability to other devices like PCs and TVs.\nSecondly, the relatively narrow age range and educational\nlevels of our participants may affect the generalizability of\nour ﬁndings. Thirdly, the absence of comparative studies\non similar video platforms and services hinders our under-\nstanding of YouTube’s performance as a video streaming\nservice. However, these limitations present opportunities\nfor future research to explore and address the diverse needs\nof users across different devices, demographics, and video\nservices. Overall, our study provides valuable insights and\npaves the way for further advancements in user-centered\ndesign for music streaming services.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4977. REFERENCES\n[1] “Business of apps, music app report 2023,” https://\nwww.businessofapps.com/data/music-app-report/, ac-\ncessed: 2023-04-14.\n[2] C. Vernallis, Unruly media: YouTube, music video, and\nthe new digital cinema . Oxford University Press,\n2013.\n[3] L. A. Liikkanen and P. Åman, “Shufﬂing services: Cur-\nrent trends in interacting with digital music,” Interact-\ning with Computers , vol. 28, no. 3, pp. 352–371, 2016.\n[4] “Statista, most frequently used music streaming or\ndownload services south korea 2022,” https://zrr.kr/\n1xyb, accessed: 2023-04-14.\n[5] “Korea create content agency (kocca), survey of music\nusers 2022,” https://zrr.kr/BHhX, accessed: 2023-04-\n14.\n[6] “Statista, top music streaming platforms\nin selected countries latin america 2021,”\nhttps://www.statista.com/statistics/1291284/\nmusic-streaming-platforms-latin-america/, accessed:\n2023-04-14.\n[7] L. A. Liikkanen and A. Salovaara, “Music on youtube:\nUser engagement with traditional, user-appropriated\nand derivative videos,” Computers in human behavior ,\nvol. 50, pp. 108–124, 2015.\n[8] J.-S. Beuscart, S. Coavoux, and J.-B. Garrocq, “Listen-\ning to music videos on youtube. digital consumption\npractices and the environmental impact of streaming,”\nJournal of Consumer Culture , p. 14695405221133266,\n2022.\n[9] S. Freeman, M. Gibbs, and B. Nansen, “‘don’t mess\nwith my algorithm’: Exploring the relationship be-\ntween listeners and automated curation and recommen-\ndation on music streaming services,” First Monday ,\nvol. 27, no. 1, 2022.\n[10] Z. Cheng, J. Shen, L. Zhu, M. S. Kankanhalli, and\nL. Nie, “Exploiting music play sequence for music\nrecommendation.” in IJCAI , vol. 17, 2017, pp. 3654–\n3660.\n[11] D. Sánchez-Moreno, A. B. Gil González, M. D.\nMuñoz Vicente, V . López Batista, and M. N. Moreno-\nGarcía, “Recommendation of songs in music streaming\nservices: dealing with sparsity and gray sheep prob-\nlems,” in Trends in Cyber-Physical Multi-Agent Sys-\ntems. The PAAMS Collection-15th International Con-\nference, PAAMS 2017 15 . Springer, 2018, pp. 206–\n213.\n[12] B. Zhang, G. Kreitz, M. Isaksson, J. Ubillos, G. Ur-\ndaneta, J. A. Pouwelse, and D. Epema, “Understanding\nuser behavior in spotify,” in 2013 Proceedings IEEE\nINFOCOM . IEEE, 2013, pp. 220–224.[13] J. Fuller, L. Hubener, Y .-S. Kim, and J. H. Lee, “Eluci-\ndating user behavior in music services through persona\nand gender.” in ISMIR , 2016, pp. 626–632.\n[14] N. Montecchio, P. Roy, and F. Pachet, “The skipping\nbehavior of users of music streaming services and its\nrelation to musical structure,” Plos one , vol. 15, no. 9,\np. e0239418, 2020.\n[15] J. H. Lee and R. Price, “Understanding users of com-\nmercial music services through personas: Design im-\nplications.” in ISMIR , 2015, pp. 476–482.\n[16] M. L. Barata and P. S. Coelho, “Music streaming ser-\nvices: understanding the drivers of customer purchase\nand intention to recommend,” Heliyon , vol. 7, no. 8, p.\ne07783, 2021.\n[17] B. Ferwerda, E. Yang, M. Schedl, and M. Tkalcic,\n“Personality and taxonomy preferences, and the inﬂu-\nence of category choice on the user experience for mu-\nsic streaming services,” Multimedia tools and applica-\ntions , vol. 78, pp. 20 157–20 190, 2019.\n[18] M. Mäntymäki and A. Islam, “Gratiﬁcations from us-\ning freemium music streaming services: Differences\nbetween basic and premium users,” 2015.\n[19] A. N. Hagen, “The playlist experience: Personal\nplaylists in music streaming services,” Popular Music\nand Society , vol. 38, no. 5, pp. 625–645, 2015.\n[20] J. H. Lee and R. Price, “User experience with commer-\ncial music services: An empirical exploration,” Journal\nof the Association for Information Science and Tech-\nnology , vol. 67, no. 4, pp. 800–811, 2016.\n[21] D. M. Weigl and C. Guastavino, “User studies in the\nmusic information retrieval literature.” in ISMIR , 2011,\npp. 335–340.\n[22] J. M. Morse, “Approaches to qualitative-quantitative\nmethodological triangulation,” Nursing research ,\nvol. 40, no. 2, pp. 120–123, 1991.\n[23] S. Bunian, K. Li, C. Jemmali, C. Harteveld, Y . Fu, and\nM. S. Seif El-Nasr, “Vins: Visual search for mobile\nuser interface design,” in Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems ,\n2021, pp. 1–14.\n[24] J. H. Lee and A. T. Nguyen, “How music fans shape\ncommercial music services: A case study of bts and\narmy.” in ISMIR , 2020, pp. 837–845.\n[25] L. Kindbom, “How does interface design and recom-\nmendation system in video streaming services affect\nuser experience?: A study on netﬂix ui design and rec-\nommendation system and how it shapes the choices\nyoung adults between the ages 18 and 26 make.” 2022.\n[26] J. H. Lee, A. Bhattacharya, R. Antony, N. K. Santero,\nand A. Le, “â?? ﬁnding homeâ?: Understanding how\nmusic supports listenersâ?? mental health through a\ncase study of bts.” in ISMIR , 2021, pp. 358–365.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n498[27] J. H. Lee, B. Bare, and G. Meek, “How similar is too\nsimilar?: Exploring users’ perceptions of similarity in\nplaylist evaluation.” in ISMIR , vol. 11, 2011, pp. 109–\n114.\n[28] B. G. Glaser and A. L. Strauss, Discovery of grounded\ntheory: Strategies for qualitative research . Routledge,\n2017.\n[29] A. Luo, “What is content analysis and how can you\nuse it in your research,” Retrived from https://www.\nscribbr. com/methodology/content-analysis/on Novem-\nber, vol. 4, p. 2020, 2019.\n[30] V . Clarke, V . Braun, and N. Hayﬁeld, “Thematic anal-\nysis,” Qualitative psychology: A practical guide to re-\nsearch methods , vol. 3, pp. 222–248, 2015.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n499"
    },
    {
        "title": "Timbre Transfer Using Image-to-Image Denoising Diffusion Implicit Models.",
        "author": [
            "Luca Comanducci",
            "Fabio Antonacci",
            "Augusto Sarti"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265271",
        "url": "https://doi.org/10.5281/zenodo.10265271",
        "ee": "https://zenodo.org/records/10265271/files/000029.pdf",
        "abstract": "Timbre transfer techniques aim at converting the sound of a musical piece generated by one instrument into the same one as if it was played by another instrument, while maintaining as much as possible the content in terms of musical characteristics such as melody and dynamics. Following their recent breakthroughs in deep learning-based generation, we apply Denoising Diffusion Models (DDMs) to perform timbre transfer. Specifically, we apply the recently proposed Denoising Diffusion Implicit Models (DDIMs) that enable to accelerate the sampling procedure. \nInspired by the recent application of DDMs to image translation problems we formulate the timbre transfer task similarly, by first converting the audio tracks into log mel spectrograms and by conditioning the generation of the desired timbre spectrogram through the input timbre spectrogram.  \nWe perform both one-to-one and many-to-many timbre transfer, by converting audio waveforms containing only single instruments and multiple instruments, respectively.\nWe compare the proposed technique with existing state-of-the-art methods both through listening tests and objective measures in order to demonstrate the effectiveness of the proposed model.",
        "zenodo_id": 10265271,
        "dblp_key": "conf/ismir/ComanducciAS23",
        "keywords": [
            "Timbre transfer",
            "Deep learning-based generation",
            "Denoising Diffusion Models",
            "DDMs",
            "Denoising Diffusion Implicit Models",
            "DDIMs",
            "Log mel spectrograms",
            "Conditioning",
            "Audio waveforms",
            "Objective measures"
        ],
        "content": "TIMBRE TRANSFER USING IMAGE-TO-IMAGE DENOISING\nDIFFUSION IMPLICIT MODELS\nLuca Comanducci Fabio Antonacci Augusto Sarti\nDipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Italy\nluca.comanducci@polimi.it, fabio.antonacci@polimi.it, augusto.sarti@polimi.it\nABSTRACT\nTimbre transfer techniques aim at converting the sound of\na musical piece generated by one instrument into the same\none as if it was played by another instrument, while main-\ntaining as much as possible the content in terms of musical\ncharacteristics such as melody and dynamics. Following\ntheir recent breakthroughs in deep learning-based gener-\nation, we apply Denoising Diffusion Models (DDMs) to\nperform timbre transfer. Speciﬁcally, we apply the recently\nproposed Denoising Diffusion Implicit Models (DDIMs)\nthat enable to accelerate the sampling procedure. Inspired\nby the recent application of DDMs to image translation\nproblems we formulate the timbre transfer task similarly,\nby ﬁrst converting the audio tracks into log mel spectro-\ngrams and by conditioning the generation of the desired\ntimbre spectrogram through the input timbre spectrogram.\nWe perform both one-to-one and many-to-many timbre\ntransfer, by converting audio waveforms containing only\nsingle instruments and multiple instruments, respectively.\nWe compare the proposed technique with existing state-of-\nthe-art methods both through listening tests and objective\nmeasures in order to demonstrate the effectiveness of the\nproposed model.\n1. INTRODUCTION\nTimbre is an extremely important perceptual aspect of mu-\nsic, yet it is hard to both model and deﬁne. The concept of\nmusical timbre can be deﬁned as the perceived character-\nistics of a musical sound that are different from pitch and\namplitude contours [1].\nTimbre Transfer concerns the task of converting a musi-\ncal piece from one timbre to another while preserving the\nother music-related characteristics. While this operation\nis not trivial, it is of extreme interest for several applica-\ntions, from the development of plugins to be used in Digi-\ntal Audio Workstations (DAW) to enabling the possibility\nof playing sounds corresponding to not widely available\nmusical instruments.\n© L. Comanducci, F. Antonacci, and A. Sarti. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: L. Comanducci, F. Antonacci, and A. Sarti, “Timbre\nTransfer using Image-to-Image Denoising Diffusion Implicit Models”, in\nProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.In this paper, we present DiffTransfer, a technique for\ntimbre transfer which is tested both between single and\nmultiple instruments and is based on a continuous De-\nnoising Diffusion Implicit Model (DDIM) with determin-\nistic sampling [2], a modiﬁed version of Denoising Diffu-\nsion Probabilistic Models (DDPMs) that are trained using\nthe same procedure, but allow for faster sampling times.\nSpeciﬁcally, in [2] it was empirically shown that DDIMs\nallow for 10×−50×faster wall-clock time performances\nwith respect to DDPMs.\nIn order to be able to convert one timbre into another,\nwe use a procedure similar to the recently proposed image-\nto-image technique Palette [3]. Speciﬁcally, we use as in-\nput to the diffusion model the noise and condition it with\nthe chosen input timbre spectrogram, then, through the de-\nnoising procedure, the model learns to reconstruct spec-\ntrograms of the desired timbre. We consider the scenario\nwhere the timbre-transfer task is paired , which means\nthat the desired and input spectrograms have the same\nmelodic/harmonic content, but differ in terms of timbre.\nWe experiment both with the possibility of convert-\ning between tracks containing only single instruments and\nalso mixtures of instruments, with no prior separation step,\nwhile making no modiﬁcations to the model in order to\ntake into account both conﬁgurations.\nIn order to demonstrate the effectiveness of the pro-\nposed model, we compare DiffTransfer with state-of-\nthe-art techniques, both through objective measures and\nby performing a user-based listening test. The source\ncode and audio excerpts can be found at https://\nlucacoma.github.io/DiffTransfer/ .\n2. RELATED WORK\nSeveral types of timbre Transfer techniques have been pro-\nposed in the literature. In [4] a CycleGAN [5] is applied in\norder to perform an unpaired transfer using the Constant-\nQ transform and the audio is then recovered through a\nWaveNet [6] model. In [7] an attention-based architecture\nis applied in order to convert mel spectrograms, which are\nthen inverted through a MelGAN architecture [8]. Gaus-\nsian mixture-based variational autoencoders are applied [9]\nin order to learn a latent space where pitch and timbre rep-\nresentations are disentangled.\nAnother class of methods, instead, extracts musical pa-\nrameters such as pitch and loudness from the input audio\ntracks and performs the transfer by resynthesizing sound257Diffusion\nDecoder\nConcatenate\nL1 \nLossConditioning\nInstrument\nSinusoidal\nEmbedding\nDiffusion\nNoisePredicted\nNoise\n!!\"##$%\"&'\nTarget\nInstrument\nFigure 1 : Training scheme of the proposed DiffTransfer technique. The target instrument spectrogram is summed with\nnoise following a simpliﬁed cosine schedule. The decoder, conditioned on the conditioning instrument spectrogram and\non the sinusoidal embedding representing the current time instant estimates the added noise. The decoder parameters are\nestimated by computing the L1 loss between the ground truth and the estimated diffusion noise.\nthrough a network that has learned to generate tracks with\nthe desired timbre. The most known example of these\ntechniques is the Differentiable Digital Signal Processing\n(DDSP) [10] model. Other similar techniques were pro-\nposed such as [11], where a hierarchical model is used\nin order to reconstruct the signal at increasing resolu-\ntions. Recently there have been proposed also models\nthat directly work on the audio waveform such as [12],\nwhere music pieces are translated to speciﬁc timbre do-\nmains. The only model that, to the best of our knowl-\nedge and except for the one proposed in this paper, is\ntested on multi-instrument timbre transfer without any\nsource separation pre-processing is the Music-STAR net-\nwork, presented in [13]. In Music-STAR a WaveNet au-\ntoencoder [14] is trained by applying teacher-forcing [15]\nto the decoders in order to recover the desired timbre.\nDenoising Diffusion Probabilistic Models (DDPMs)\n[16] have recently become the latest state-of-the-art for\nwhat concerns deep learning-based generation fastly re-\nplacing Generative Adversarial Networks (GANs) [17] and\nVariational Autoencoders [18], due to their easier training\nprocedure and increased quality of the produced results.\nDDPMs have been successfully applied to a wide va-\nriety of image-related tasks such as generation [19] and\ntranslation [3].\nMore recently, DDPMs have been also used for audio-\nrelated tasks. In [20] a diffusion model is applied in order\nto convert midi tracks to spectrograms, while in [21] a text-\nto-music diffusion model is proposed. DDPMs have also\nbeen applied to symbolic music generation [22], speech\nsynthesis [23] and singing voice extraction [24].\nWhile DDPMs have extremely powerful generation ca-\npabilities they suffer from slow sampling times. To amelio-rate this issue, recently Denoising Diffusion Implicit Mod-\nels (DDIMs) [2], which allow for faster sampling times and\nwere recently applied to image inpainting [25].\n3. PROPOSED MODEL\nIn this section, we describe the proposed DiffTransfer tech-\nnique for timbre transfer. Instead of working directly with\nraw audio signals, we convert them into log mel-scaled\nspectrograms, due to their easier handling by deep learn-\ning models. We then propose a model that, given as input\nthe spectrogram corresponding to the conditioning instru-\nment, generates the corresponding target spectrogram that\nwould have been obtained by playing the same piece of\nmusic with the target instrument. Operatively we achieve\nthis through a conditional continuous-time DDIM, which\nlearns to denoise the target instrument spectrogram, while\nconditioned on the input instrument spectrogram, as de-\npicted in Fig. 1. At inference time, the model is fed with\nthe input conditioning instrument concatenated with Gaus-\nsian noise and generates the corresponding target spectro-\ngram. We retrieve the audio signal by applying to the log\nmel spectrograms the SoundStream1model [26], provided\nby [20] where it was trained on a custom music dataset.\nIn the following, we’ll provide a brief overview of the\nDDIM framework and notation used in this paper, in order\nto make the tractation as compact as possible, for addi-\ntional and more thorough formulations, we refer the reader\nto [2] and [3]. We aim at giving a general overview of the\nprocess and we’ll use a slight abuse of notation to describe\nthe diffusion process using the continuous time framework,\n1https://tfhub.dev/google/soundstream/mel/\ndecoder/music/1Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n258in order to make it more similar to the more common liter-\nature regarding DDPMs and DDIMs.\n3.1 Diffusion Decoder\nWe adopt a procedure similar to the Palette [3] image-to-\nimage translation technique in order to train the timbre\ntransfer decoder as a Denoising Diffusion Implicit Model\n(DDIM) [2]. Broadly speaking, DDIMs work by learn-\ning how to generate data from noise in a two-part pro-\ncedure. The ﬁrst part is denoted as the forward process ,\nwhere Gaussian noise γ∼ N(0,1)is subsequently added\nto the input until it is indistinguishable from the former.\nThe second part consists of the reverse process where a de-\ncoder learns how to invert the forward process, effectively\nreconstructing data from the noise. DDIMs can be seen as\na generalization of DDPMs that shares the same training\nprocedure, however, they differ in the modeling of the re-\nverse process, by using a non-markovian diffusion process,\nwhich allows for faster generation times.\n3.1.1 Forward Process\nLet us deﬁne XandYas the log mel spectrograms cor-\nresponding to the conditioning and target instruments, re-\nspectively. We choose a continuous diffusion time [27–\n29]in order to be able to change the number of desired\nsampling steps. If we consider Tsteps, then the diffusion\ntime can be deﬁned as t∈ {0,1}, where consecutive times\nare separated by ∆t= 1/T. Then, the forward process is\ndeﬁned similarly to the case of DDPMs by subsequently\nadding noise to the target spectrogram for Tsteps\nq(Yt|Yt−∆t) =N(Yt,/radicalbig\n(αt)Yt−∆t,βtI),\nq(Y1:T|Y0) =T/productdisplay\nt=1q(Yt−∆t)(1)\nwhereαandβare parameters deﬁned by a simpliﬁed co-\nsine schedule [30].\n3.1.2 Reverse Process\nIn the case of DDIMs, the reverse diffusion process is op-\nerated by introducing an additional distribution pθ, where\na sample Yt−∆tcan be generated from a sample Ytas\nYt−∆t=/radicalbig\nβt−∆t/parenleftBigg\nc−√βtγ(t)\nθ(Yt,X)/radicalbig\n(αt)/parenrightBigg\n+\n/radicalbig\n1−αt−∆t·γ(t)\nθ(Yt,X),(2)\n, whereγis the noise estimated by a network with param-\netersθ. The noise at time t γ(t)\nθis estimated by a network\nthat is conditioned also on the input timbre spectrogram X,\nsimilarly to the formulation proposed in Palette [3].\n3.1.3 Training Procedure\nThe denoising process is operated through a U-Net archi-\ntecture which is conditioned on Xand trained to predict\nthe added noise in order to minimize the L1 loss\nE=||γ(t)\nθ(Yt,X)−γ||1\n1, (3)whereγis the true perturbation, while γ(t)\nθ(Yt,X)is the\nestimate of the noise added to the target spectrogram at\ntimet, conditioned on the input spectrogram X.\n3.2 Architecture\nThe decoder architecture is based on a U-Net model. The\nbuilding element is made of residual blocks, in each of\nthese the input is processed by (i) a 2D convolutional layer\nwith swish activation, followed by batch normalization and\nby (ii) a convolutional layer with no activation. Both con-\nvolutional layers have kernel size 3. The output of this\nprocedure is then summed with the residual, which is ob-\ntained by processing the input with a convolutional layer\nwith kernel size 1.\nThe encoder part of the network consists of 3downsam-\npling blocks, each consisting of 4residual blocks having\nﬁlter sizes 64,128,256. The output of each downsampling\nblock is followed by average pooling, with pool size 2in\norder to compress the dimension of the spectrograms. The\nlast block of the encoder is followed a self-attention block.\nThe bottleneck obtained through the encoder is pro-\ncessed by a residual block with 512ﬁlters and is then pro-\ncessed by the decoder, which is a specular version of the\nencoder. The only difference lies in the use of transposed\nconvolutions in order to create upsampling layers needed\nto increase the dimension of the features.\nThe last downsampling layer of the encoder, the bot-\ntleneck and the ﬁrst upsampling layer of the decoder are\nfollowed by self-attention.\n3.3 Deployment\nThe proposed model takes as input spectrograms of a ﬁxed\nsize, therefore audio tracks longer than the ones used for\ntraining need to be sliced accordingly.\nThe decoder takes as input the conditioning spectro-\ngramXand the diffusion noise and retrieves an estimate of\nthe latter, which can then be subtracted in order to obtain\nan estimate of the desired output timbre spectrogram ˆY.\nThe output waveform ycan then be obtained by feeding\nthe pre-trained SoundStream model with ˆY.\n4. EXPERIMENTS\nIn this section, we describe experiments performed with\nthe aim of demonstrating the capabilities of the proposed\nDiffTransfer technique both in the single-instrument and\nmulti-instrument application scenarios.\nIn Fig. 3 we show an example of input, generated and\nground-truth spectrograms, obtained via the DiffTransfer\nmodel when converting from a Clarinet to Strings.\n4.1 Dataset\nIn order to train the model we considered the StarNet\ndataset [31], which contains a set of tracks that are\nplayed with two timbre-domains, namely strings-piano and\nvibraphone-clarinet. The dataset consists of roughly 22\nhours of audio. We used the reduced version of the dataset,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n259Diffusion\nDecoder\nConcatenate\nConditioning\ninstrument\nPredicted\nNoise\nDiffusion\nNoise\nPredicted\nTarget InstrumentPredicted\nTarget Instrument\nAudio\nSoundStream\nFigure 2 : Deployment scheme of the proposed DiffTransfer technique. The decoder is fed with Gaussian noise and with the\nconditioning instrument spectrogram. The noise estimate provided by the decoder is then subtracted from the input noise\nin order to provide an estimate of the desired target spectrogram, from which the audio is estimated via the SoundStream\nmodel [20, 26].\nwhere tracks are resampled to 16000 Hz and converted\nthem to mono. In order to perform the evaluation, we use\nthe same ten tracks considered in [13], in order to ease the\ncomparison with their model.\n4.2 Techniques Under Comparison\nWe consider two baselines in order to compare the per-\nformances of the proposed DiffTransfer architecture. For\nwhat concerns the single-instrument timbre transfer task,\nwe consider the Universal Network [12] ﬁne-tuned on the\nStarNet dataset as done in [13]. For what concerns the\nmulti-timbre task, we consider the mixture-supervised ver-\nsion of the Music-STAR network proposed in [13]. We\nperform three different types of timbre transfer tasks: sin-\ngle, where only single instruments are converted, sin-\ngle/mixed where the separate conversions of single instru-\nments are mixed in order to create the desired mixture\ntrack and mixture , where the mixture is directly converted.\nThese nomenclatures are used just to ease the presentation\nof the results, we would like to point out that, for what con-\ncerns the DiffTransfer architecture, no speciﬁc changes are\nrequired for the various types of applications, except for\nthe choice of desired input data.\n4.3 Experiment Setup\nThe Universal Network and Music-STAR architectures are\ntrained with the procedure described in [13]. The Diff-\nTransfer network is trained for 5000 epochs using a batch\nsize of16, with the AdamW optimizer [32] with learning\nrate2e−5and weight decay 1e−4. The epoch that min-\nimizes the L1noise prediction loss is chosen in order to\nretain the model used to compute the results. We train a\ntotal of six models, performing the following timbre trans-\nfer conversions: vibraphone to piano, piano to vibraphone,clarinet to strings, strings to clarinet vibraphone/clarinet to\npiano/strings and piano/strings to vibraphone/clarinet.\nThe network input features are computed by ﬁrst apply-\ning the Short-Time Fourier Transform (STFT) with a Hann\nwindow of size 0.020 s and50% overlap to normalized\naudio tracks. Then the log mel spectrogram is computed\nover128bins corresponding to the range of 0−16000Hz .\nWe do not feed the entire audio tracks as input to the net-\nwork, instead, during each epoch we extract 128frames\nfrom the log mel spectrogram, corresponding to ≈2 s.\nEach spectrogram slice is normalized between −1and1\nbefore being given as input to the network and the out-\nput spectrograms are denormalized before being fed to the\nSoundStream model in order to recover the audio wave-\nform. Since the tracks considered for the test are of length\n10sand the model gets as input a ﬁxed 128frames spectro-\ngram we slice the conditioning spectrogram before feed-\ning into the model and we keep the input noise ﬁxed for\nall slices, in order to ensure consistency in the generation.\nAll spectrogram slices are normalized in the range [−1,1]\nand denormalized before being fed to the SoundStream de-\ncoder.\n4.4 Objective Evaluation\nWe evaluate the model objectively in order to analyze the\nperceptual similarity and content preservation capabilities\nof the generated tracks with respect to the ground truth au-\ndio.\nIn order to evaluate the perceptual similarity, we com-\npute the Fréchet Audio Distance (FAD) [33] using the VG-\nGish embeddings [34], through a PyTorch implementa-\ntion2. FAD is a reference-free metric for music enhance-\n2https://pypi.org/project/\nfrechet-audio-distance/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2600 2 4 6 8 10\nTime [s]012345678Frequency [kHz]\n(a)\n0 2 4 6 8 10\nTime [s]012345678Frequency [kHz]\n(b)\n0 2 4 6 8 10\nTime [s]012345678Frequency [kHz]\n(c)\nFigure 3 : Example of Timbre Conversion log mel Spectro-\ngrams using the DiffTransfer architecture, obtained when\nconverting Clarinet (a) to Strings (b). The ground truth\nStrings spectrogram is shown in (c).\nment algorithms, which views the embeddings as a conti-\nnous multivariate Gaussian and is computed between the\nreal and generated data as\nFAD =||µr−µg||2+tr(Σ r+µg−2/radicalbig\nΣrΣg),(4)\nwhere(µr,Σr)and(µg,Σg)are the mean and covariances\nof the embeddings corresponding to the real and generated\ndata, respectively. Similarly to [20], we compute FAD in\norder to analyze the perceptual similarity between the gen-\nerated audios with respect to the ground truth one, corre-\nsponding to the original StarNet dataset.\nTo understand the content-preservation capabilities of\nthe model, following [35], we compute how the pitch con-\ntours of generated ground truth audio tracks are dissimilar,\nby calculating the mismatch between two sets of pitches A\nandBthrough the Jaccard Distance\nJD(A,B) = 1−|A∩B|\n|A∪B|, (5)\nwhere a lower value corresponds to a lower mismatch and\nthus to a higher degree of similarity between the gener-\nated pitch contours. Pitch contours are computed using a\nmulti-pitch version of the MELODIA [36] as implemented\nin the Essentia library [37], rounding pitches to the nearest\nsemitone. We report the values obtained by computing the\nmetrics on the test dataset in Table 1.Objective Evaluation\nMethod FAD↓JD↓\nUniversal Network (single) 7.09 0.53\nDiffTransfer (single) 2.58 0.28\nUniversal Network (single/mixed) 10.47 0.64\nDiffTransfer (single/mixed) 4.73 0.46\nMusic-STAR (mixture) 8.93 0.57\nDiffTransfer (mixture) 4.37 0.38\nTable 1 : Objective Evaluation of the proposed DiffTrans-\nfer Method compared to the baselines, in terms of Fréchet\nAudio Distance (FAD) and Jaccard Distance (JD). Results\nare averaged over all participants and over all the tracks\nconsidered for each part of the test.\n4.5 Subjective Evaluation\nIn order to evaluate subjectively the timbre transfer capa-\nbilities, we perform a listening test with 18 human partici-\npants. The web page of the test is available at3. The test\nwas split into two parts corresponding to the single and\nmultiple instrument application scenarios, respectively.\nDuring the single instrument part of the test, the users\nlistened to four tracks, corresponding to the four types of\nconversions performed, namely: clarinet to strings, strings\nto clarinet, piano to vibraphone, vibraphone to piano. Each\nexample consisted of two conditions, one obtained via the\nDiffTransfer model and the other through the Universal\nNetwork.\nIn the second part of the test, concerning multiple in-\nstrument timbre transfer, a total of four tracks were consid-\nered, two for the conversion from vibraphone/strings to pi-\nano/strings waveforms and two for the reverse conversion.\nEach example consisted of four conditions, namely DiffS-\ntar (single/mix), Universal Network (single/mix), DiffStar\n(mixture) and Music-STAR (mixture).\nBoth the order of conditions and the order of examples\nin each separate part of the test were randomized.\nThe participants were asked to rate the conditions in\nterms of similarity with respect to the reference track on\na 5 elements Likert scale where 1corresponds to bad and\n5to excellent. We report the results obtained through the\nlistening test in Table 2.\n4.6 Discussion\nBy brieﬂy inspecting both the objective and subjective re-\nsults, reported in Table 1 and 2, respectively, it is clear how\nthe proposed DiffTransfer model outperforms the Univer-\nsal Network and Music-STAR baselines both for what con-\ncerns the single and multiple timbre transfer tasks.\nWhen considering single timbre results, DiffTransfer is\nable to achieve signiﬁcantly better performances in terms\nof FAD, Jaccard Distance and Perceived Similarity, with\nrespect to the Universal network. The gap between the two\nmethods becomes even more evident when considering the\n3https://listening-test-ismir-ttd.\n000webhostapp.com/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n261Subjective Evaluation\nMethod Similarity\nUniversal Network (single) 1.82\nDiffTransfer (single) 3.68\nUniversal Network (single/mixed) 1.69\nDiffTransfer (single/mixed) 3.78\nMusic-STAR (mixture) 2.89\nDiffTransfer (mixture) 3.80\nTable 2 : Objective Evaluation of the proposed DiffTrans-\nfer Method compared to the baselines, in terms of per-\nceived similarity with respect to the ground truth on a Lik-\nert scale from 1 (Bad) to 5 (Excellent). Results are aver-\naged over all test tracks.\nsingle/mixed case, i.e. when single timbre transfer tracks\nare mixed in order to form the desired mixture audio.\nFor what concerns the Music-STAR method, the gap\nwith respect to DiffTransfer remains high in terms of FAD,\nbut becomes less noticeable when considering JD and the\nperceived subjective similarity.\n5. CONCLUSION\nIn this paper, we have presented DiffTransfer a technique\nfor both single- and multi-instrument timbre transfer using\nDenoising Diffusion Implicit models. The novelty of the\nproposed approach lies in the fact that in addition to be-\ning, to the best of our knowledge, the ﬁrst application of\ndiffusion models to timbre transfer, it is the ﬁrst model to\nbe tested in order to perform single and multi-timbre trans-\nfer, without varying the architecture depending on which\napplication is chosen. We compared the proposed model\nwith state-of-the-art Universal Network and Music-STAR\nbaselines through both objective evaluation measures and\na listening test, demonstrating the better capabilities of the\nproposed DiffTransfer approach.\nFuture works will involve increasing the audio quality\nof the generated audio, by taking into account the consis-\ntency of subsequent generated spectrograms. Furthermore,\nwe plan on modifying the model in order to be able to\nperform unpaired timbre transfer, which greatly eases the\ndataset requirements and applicability of the technique.\n6. REFERENCES\n[1] J. T. Colonel and S. Keene, “Conditioning autoencoder\nlatent spaces for real-time timbre interpolation and syn-\nthesis,” in 2020 International Joint Conference on Neu-\nral Networks (IJCNN) . IEEE, 2020, pp. 1–7.\n[2] J. Song, C. Meng, and S. Ermon, “Denoising diffu-\nsion implicit models,” in International Conference on\nLearning Representations , 2021.\n[3] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Sal-\nimans, D. Fleet, and M. Norouzi, “Palette: Image-to-\nimage diffusion models,” in ACM SIGGRAPH 2022\nConference Proceedings , 2022, pp. 1–10.[4] S. Huang, Q. Li, C. Anil, X. Bao,\nS. Oore, and R. B. Grosse, “Timbretron: A\nwavenet(cycleGAN(CQT(audio))) pipeline for\nmusical timbre transfer,” in International Conference\non Learning Representations , 2019.\n[5] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired\nimage-to-image translation using cycle-consistent ad-\nversarial networks,” in Proc. of the IEEE international\nconference on computer vision , 2017, pp. 2223–2232.\n[6] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan,\nO. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu, “Wavenet: A generative model for\nraw audio,” arXiv preprint arXiv:1609.03499 , 2016.\n[7] D. K. Jain, A. Kumar, L. Cai, S. Singhal, and V . Ku-\nmar, “Att: Attention-based timbre transfer,” in 2020\nInternational Joint Conference on Neural Networks\n(IJCNN) . IEEE, 2020, pp. 1–6.\n[8] K. Kumar, R. Kumar, T. De Boissiere, L. Gestin, W. Z.\nTeoh, J. Sotelo, A. de Brébisson, Y . Bengio, and A. C.\nCourville, “Melgan: Generative adversarial networks\nfor conditional waveform synthesis,” Advances in neu-\nral information processing systems , vol. 32, 2019.\n[9] Y .-J. Luo, K. Agres, and D. Herremans, “Learning dis-\nentangled representations of timbre and pitch for mu-\nsical instrument sounds using gaussian mixture varia-\ntional autoencoders,” in 20th International Society for\nMusic Information Retrieval (ISMIR2019) , 2019.\n[10] J. Engel, L. H. Hantrakul, C. Gu, and A. Roberts,\n“Ddsp: Differentiable digital signal processing,” in In-\nternational Conference on Learning Representations ,\n2020.\n[11] M. Michelashvili and L. Wolf, “Hierarchical timbre-\npainting and articulation generation,” in 21st Inter-\nnational Society for Music Information Retrieval (IS-\nMIR2020) , 2020.\n[12] A. P. Noam Mor, Lior Wold and Y . Taigman, “A univer-\nsal music translation network,” in International Con-\nference on Learning Representations (ICLR) , 2019.\n[13] M. Alinoori and V . Tzerpos, “Music-star: a style trans-\nlation system for audio-based re-instrumentation,” in\n21st International Society for Music Information Re-\ntrieval (ISMIR2022) , 2022.\n[14] J. Engel, C. Resnick, A. Roberts, S. Dieleman,\nM. Norouzi, D. Eck, and K. Simonyan, “Neural audio\nsynthesis of musical notes with wavenet autoencoders,”\ninInternational Conference on Machine Learning .\nPMLR, 2017, pp. 1068–1077.\n[15] R. J. Williams and D. Zipser, “A learning algorithm for\ncontinually running fully recurrent neural networks,”\nNeural computation , vol. 1, no. 2, pp. 270–280, 1989.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n262[16] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion\nprobabilistic models,” Advances in Neural Information\nProcessing Systems , vol. 33, pp. 6840–6851, 2020.\n[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio,\n“Generative adversarial networks,” Communications of\nthe ACM , vol. 63, no. 11, pp. 139–144, 2020.\n[18] A. Roberts, J. Engel, and D. Eck, “Hierarchical varia-\ntional autoencoders for music,” in NIPS Workshop on\nMachine Learning for Creativity and Design , vol. 3,\n2017.\n[19] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang,\nE. Denton, S. K. S. Ghasemipour, R. Gontijo-Lopes,\nB. K. Ayan, T. Salimans, J. Ho, D. J. Fleet, and\nM. Norouzi, “Photorealistic text-to-image diffusion\nmodels with deep language understanding,” in Ad-\nvances in Neural Information Processing Systems ,\nA. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds.,\n2022.\n[20] C. Hawthorne, I. Simon, A. Roberts, N. Zeghi-\ndour, J. Gardner, E. Manilow, and J. Engel, “Multi-\ninstrument music synthesis with spectrogram diffu-\nsion,” in Ismir 2022 Hybrid Conference , 2022.\n[21] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y . Zou,\nand D. Yu, “Diffsound: Discrete diffusion model for\ntext-to-sound generation,” 2022.\n[22] G. Mittal, J. Engel, C. Hawthorne, and I. Simon, “Sym-\nbolic music generation with diffusion models,” in Proc.\nof the 22nd International Society for Music Informa-\ntion Retrieval Conference , 2021.\n[23] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catan-\nzaro, “Diffwave: A versatile diffusion model for au-\ndio synthesis,” in International Conference on Learn-\ning Representations , 2021.\n[24] G. Plaja-Roglans, M. Miron, and X. Serra, “A\ndiffusion-inspired training strategy for singing voice\nextraction in the waveform domain,” in International\nSociety for Music Information Retrieval (ISMIR) Con-\nference , 2022.\n[25] G. Zhang, J. Ji, Y . Zhang, M. Yu, T. Jaakkola, and\nS. Chang, “Towards coherent image inpainting using\ndenoising diffusion implicit models,” arXiv preprint\narXiv:2304.03322 , 2023.\n[26] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and\nM. Tagliasacchi, “Soundstream: An end-to-end neu-\nral audio codec,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , vol. 30, pp. 495–\n507, 2021.\n[27] A. Campbell, J. Benton, V . De Bortoli, T. Rainforth,\nG. Deligiannidis, and A. Doucet, “A continuous time\nframework for discrete denoising models,” Advances\nin Neural Information Processing Systems , vol. 35, pp.\n28 266–28 279, 2022.[28] Y . Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar,\nS. Ermon, and B. Poole, “Score-based generative mod-\neling through stochastic differential equations,” in In-\nternational Conference on Learning Representations ,\n2021.\n[29] S. Rouard and G. Hadjeres, “Crash: raw audio\nscore-based generative modeling for controllable high-\nresolution drum sound synthesis,” in 22nd Interna-\ntional Society for Music Information Retrieval (IS-\nMIR2021) , 2021.\n[30] A. Q. Nichol and P. Dhariwal, “Improved denoising\ndiffusion probabilistic models,” in International Con-\nference on Machine Learning . PMLR, 2021, pp.\n8162–8171.\n[31] M. Alinoori and V . Tzerpos, “Starnet,” Aug. 2022.\n[Online]. Available: https://doi.org/10.5281/zenodo.\n6917099\n[32] I. Loshchilov and F. Hutter, “Decoupled weight decay\nregularization,” in International Conference on Learn-\ning Representations , 2019.\n[33] K. Kilgour, M. Zuluaga, D. Roblek, and M. Shariﬁ,\n“Fréchet audio distance: A reference-free metric for\nevaluating music enhancement algorithms.” in INTER-\nSPEECH , 2019, pp. 2350–2354.\n[34] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gem-\nmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt,\nR. A. Saurous, B. Seybold et al. , “Cnn architectures\nfor large-scale audio classiﬁcation,” in 2017 ieee in-\nternational conference on acoustics, speech and signal\nprocessing (icassp) . IEEE, 2017, pp. 131–135.\n[35] O. Cífka, A. Ozerov, U. ¸ Sim¸ sekli, and G. Richard,\n“Self-supervised vq-vae for one-shot music style trans-\nfer,” in ICASSP 2021-2021 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2021, pp. 96–100.\n[36] J. Salamon and E. Gómez, “Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics,” IEEE transactions on audio, speech, and lan-\nguage processing , vol. 20, no. 6, pp. 1759–1770, 2012.\n[37] D. Bogdanov, N. Wack, E. Gómez Gutiérrez, S. Gulati,\nH. Boyer, O. Mayor, G. Roma Trepat, J. Salamon, J. R.\nZapata González, X. Serra et al. , “Essentia: An au-\ndio analysis library for music information retrieval,” in\nBritto A, Gouyon F , Dixon S, editors. 14th Conference\nof the International Society for Music Information Re-\ntrieval (ISMIR); 2013 Nov 4-8; Curitiba, Brazil.[place\nunknown]: ISMIR; 2013. p. 493-8. International So-\nciety for Music Information Retrieval (ISMIR), 2013.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n263"
    },
    {
        "title": "Comparing Texture in Piano Scores.",
        "author": [
            "Louis Couturier",
            "Louis Bigo",
            "Florence Levé"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265337",
        "url": "https://doi.org/10.5281/zenodo.10265337",
        "ee": "https://zenodo.org/records/10265337/files/000060.pdf",
        "abstract": "In this paper, we propose four different approaches to quantify similarities of compositional texture in symbolically encoded piano music. A melodic contour or harmonic progression can be shaped into a wide variety of different rhythms, densities, or combinations of layers. Instead of describing these textural organizations only locally, using existing formalisms, we question how these parameters may evolve throughout a musical piece, and more specifically how much they change. Hence, we define several distance functions to compare texture between two musical bars, based either on textural labels annotated with a dedicated syntax, or on symbolic scores. We propose an evaluation methodology based on textural heterogeneity and contrasts in classical Thema and Variations using the TAVERN dataset. Finally, we illustrate use cases of these tools to analyze long-term structure, and discuss the impact of these results on the understanding of musical texture.",
        "zenodo_id": 10265337,
        "dblp_key": "conf/ismir/CouturierBL23",
        "keywords": [
            "compositional texture",
            "symbolically encoded piano music",
            "melodic contour",
            "harmonic progression",
            "rhythms",
            "densities",
            "layers",
            "distance functions",
            "TAVERN dataset",
            "long-term structure"
        ],
        "content": "COMPARING TEXTURE IN PIANO SCORES\nLouis Couturier1Louis Bigo2Florence Levé1,2\n1MIS, Université de Picardie Jules Verne, Amiens, France\n2Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France\n{louis.couturier, florence.leve}@u-picardie.fr, louis.bigo@univ-lille.fr\nABSTRACT\nIn this paper, we propose four different approaches to\nquantify similarities of compositional texture in symbol-\nically encoded piano music. A melodic contour or har-\nmonic progression can be shaped into a wide variety of\ndifferent rhythms, densities, or combinations of layers. In-\nstead of describing these textural organizations only lo-\ncally, using existing formalisms, we question how these\nparameters may evolve throughout a musical piece, and\nmore speciﬁcally how much they change. Hence, we de-\nﬁne several distance functions to compare texture between\ntwo musical bars, based either on textural labels annotated\nwith a dedicated syntax, or directly on symbolic scores.\nWe propose an evaluation methodology based on textural\nheterogeneity and contrasts in classical Thema and Vari-\nations using the TA VERN dataset. Finally, we illustrate\nuse cases of these tools to analyze long-term structure, and\ndiscuss the impact of these results on the understanding of\nmusical texture.\n1. INTRODUCTION\nThe term texture is used at various levels of description in\nthe music domain. Initially related to the description of\nsound features, it is also used in symbolic representations\nof music to describe musical streams through a variety of\nconcepts characterizing the volume and the organization\nof basic score elements such as notes and voices, which\nencompass high-level concepts such as monophony and\npolyphony1[1–4]. Between these two extremes, elements\nof musical texture include layers, voices, melodic or rhyth-\nmic patterns, articulation and instrumentation [2,5]. Huron\ninterestingly summarizes it in three main ideas [3]: (1) the\ndensity of musical elements, (2) the diversity or inhomo-\ngeneity of elements, and (3) the overall sonic activity. The\nﬁrst two can be included in the notion of compositional\n1Polyphony, as a type of texture, has a stronger meaning than “the\nsimultaneous presence of possibly more than one note”. Here, it implies\n“two or more lines moving independently” [1]. Similarly, monophonic\ntexture is not restricted to single melodies, but designates the presence of\na unique musical line – possibly with note doubling or parallel motions.\n© L. Couturier, L. Bigo, and F. Levé. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: L. Couturier, L. Bigo, and F. Levé, “Comparing Texture\nin Piano Scores”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.texture , as opposed to orchestral (or timbral) texture [6].\nCompositional texture, which is the object of study of the\npresent work, is mostly embedded in the symbolic score.\nIt is worth noting that some models of texture only focus\non a particular musical dimension. Nordgren’s categoriza-\ntion for instance deals with the vertical dimension only,\nwith note doubling and spacing [7]. Conversely, other ap-\nproaches focus on the time dimension, as a complement\nto harmony, as in [8], or [9] for style-transfer. Figure 1\nshows multiple versions of the same musical theme, which\nis shaped into different compositional textures.\nWe aim at quantifying the differences of compositional\ntexture in piano music. Previous studies provided local de-\nscriptions of texture [10–12]. Here, we question how textu-\nral dimensions may evolve through a whole piece of music.\nThis objective requires the elaboration of dedicated tools to\ncompare textures, more precisely to assess the distance, or\ndissimilarity, between two given textural conﬁgurations.\nA number of Music Information Retrieval (MIR) tasks\ninvolve the search of similarities at various scales, from\npattern detection [13, 14] to genre classiﬁcation [15, 16].\nIn the audio domain, music similarity lies at the center\nof content-based recommender systems [17, 18]. At the\nlevel of the musical score, music similarity has also been\nextensively studied on speciﬁc musical notions such as\nmelody [13–15,19–22], rhythmic pattern [23,24], or chord\nand harmonic progressions [25–27]. Classical approaches\nfor computing similarities include edit-distance on string-\nbased representation, or geometric distance on pianoroll-\nlike representations [28]. New latent embeddings of music\nalso emerged from development of deep neural networks,\nas well as metric learning methods, like [29]. Although\nmeasures of music similarity may ultimately reﬂect some\nsimilarities in the perception of music [30,31], we compare\ntextural information based on symbolic music scores only.\nIn particular, we focus here on studying different represen-\ntations of texture in order to build interpretable dissimilar-\nity measures.\nIn this work, we propose distance functions to quan-\ntify textural dissimilarity between musical bars from piano\nscores. We ﬁrst detail four types of textural distance (Sec-\ntion 2). Then, we introduce estimators of textural hetero-\ngeneity and contrast, for longer musical extracts, and pro-\npose a dedicated methodology to evaluate our distances,\nusing a dataset of Thema and Variations (Section 3). Fi-\nnally, we provide use cases of such distances, especially in\nthe context of form or structure analysis (Section 4).508Figure 1 : Examples of different textures from Ten Variations in G on ‘Unsere dummer Pöbel meint’ by W. A. Mozart\n(K. 455, 1784). A textural annotation, following the syntax deﬁned in [10], is provided for each example. The melodic\ncontour (circled in red) is shared among the variations, but the overall compositional texture changes. a. The theme is\nintroduced in monophonic texture: three voices merge into a single musical idea, in parallel octave motions; b. There\nare now only two notes sounding at the same time: the vertical density decreases. But horizontal density is increased by\nsixteenth notes; c. A more homophonic texture: three or four threads, mostly synchronous; d. Here, we identify two layers\nof melody and accompaniment. In these last three bars, the harmony changes, but compositional texture is exactly the same.\n2. DEFINING DISTANCES FOR\nCOMPOSITIONAL TEXTURE\nThe distances that are designed in this paper aim at com-\nparing compositional texture at the scale of individual mu-\nsical bars. We focus on (polyphonic) piano scores of the\nWestern Classical repertoire, with no voice separation.\n2.1 Distances based on textural labels\nTextural annotations have been produced in [6] for piano\nmusic, on Mozart’s sonatas. For each annotated bar, a la-\nbel enclose two levels of textural information: on the one\nhand, a set of keywords that indicate the presence of certain\nproperties of the overall textural conﬁguration, or in one of\nits layer (like parallelism, melodic or harmonic roles...);\non the other hand, a vertical structuration of the musical\ncontent into main textural layers and sublayers [10]. We\npropose two distance functions based on this information.\n2.1.1 Distance between textural elements\nThe ﬁrst distance is based on a set of binary textural ele-\nments which have been deﬁned in [6, section 3.2]. These\nindicators express the presence of atomic textural charac-\nteristics in a musical bar. They include speciﬁc functions\nof the musical layers: melodic ( M), harmonic ( H), or static\n(S), relationships between voices: homorhythmy ( h), par-\nallel (p) or octave ( o) motions, as well as characteristic\nmusical ﬁgures such as sustained ( t) or repeated ( r) notes,\nscale motives ( s), oscillations ( b), sparse horizontal den-\nsity (_) and neat changes of texture in the bar ( ,).\nA musical bar ais therefore abstractly represented by\na vector texel(a) which comprised of the 12 textural ele-\nments from its label. The distance function dtexelreturns\nthe Hamming distance between the vectors. It is an inte-\nger between zero and 12 that corresponds to the number of\ntextural elements that differ between two bar annotations:dtexel(a,b) =12/summationdisplay\ni=1|texeli(a)−texeli(b)|\nwhereaandbare two musical bars, and texel i(·)the binary\nvalue of the ithtextural element of a given bar.\n2.1.2 Textural diversity and density\nAt a higher level of description, textural annotation of pi-\nano scores mainly focus on grouping threads2of notes\ninto distinct musical layers. Examples 1.a and 1.d both\nhave three threads, but they are organized differently. In\n1.a, they merge into a single layer. On the contrary, in 1.d,\nthe threads are divided in 2main textural layers: its texture\nis more diverse than 1.a without being thicker.\nThis grouping of threads is formalized in [10] under\nthe terms of density (number of threads, of simultaneous\nsounding notes; the thickness or density-number in [2])\nanddiversity (number of distinct layers). These two di-\nmensions allow to embed any textural label in the planar\ntextural space represented in Figure 2 (left).\nThedensity-Diversity distanceddDseparating two bars\nis deﬁned as the Euclidean distance between their labels in\nthis space. In previous examples, the ﬁrst bar of 1.a and\n1.d respectively have density-diversity coordinates of (3,1)\nand (3,2), resulting in a distance of 1.\nNote that this distance only takes into account the verti-\ncal dimension of compositional texture (as in [7]). A draw-\nback of restricting textural analysis to only two dimensions\nis that it is less sensitive to small textural ﬂuctuation: as an\nexample, the 1164 labels released by [6] only use 17 dis-\ntinct combinations of density and diversity values. Never-\ntheless, this condensed description allows an interpretable\n2The term ‘thread’ designates the most atomic elements that can be\ncombined into musical ‘layers’ [5, p.65].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n509Figure 2 : Schematic representations of textural spaces\nproposed respectively by Couturier et al. [6] (left) and\nHuron [3] (right). In both case, areas of the space are\nmatched to main types of texture [1, 4]. The boundaries\nare not strict, though. The four examples of Figure 1 are\nalso represented in these spaces.\napproach for high-level analysis, as it reﬂects main textural\nstrategies (see Figure 2).\n2.2 Score-based distances\nThe distances deﬁned in Section 2.1 are based on\nmanually-annotated textural labels. Such textural annota-\ntions are however rarely available as their production re-\nquires substantial time and expert knowledge. In contrast,\nthis section presents two distance functions that can sys-\ntematically be computed on encoded musical scores.\n2.2.1 Adapting Huron’s textural space\nAnother two-dimensional textural space has been proposed\nby Huron in [3]. It is used in this article to catego-\nrize full musical pieces among main types of texture (see\nalso [1]): polyphony, monophony, homophony and het-\nerophony, represented on Figure 2. Instead of analyzing\nthe quantity and grouping of musical threads (see Section\n2.1.2), it relies on the relationships between them: the pro-\nportions of onset synchronization andsemblant motions .\nThe original study used a pre-existing separation of\nvoices in the pieces (such as Bach’s Inventions and Sin-\nfonias) to compute these features. For both of them, we\nprovide an estimation of the value in the interleaved poly-\nphonic case – i.e. without separation of voices. Note that\nit is not always possible to ﬁnd a valid and unique voice\nseparation in piano scores [32]. The details of the imple-\nmentation, out of the scope of the article, can be found in\nthe dedicated repository1. They are:\n• The ratio of onset synchrony quantiﬁes the degree\nof homorhythmy of the note onsets. A value of 1\nindicates a perfect synchronization of note onsets,\nwhich is the case in monophonic (see Figure 1.a)\nor homophonic (chordal or hymnal) textures. This\nvalue decreases if note onsets happens while other\nnotes are sustained. For example, 1.b has a value of\n0.25: in this case, only one onset over four is fully\nsynchronous.\n1Available at http://algomus.fr/code .• The ratio of semblant motions estimates to what\nextent the directions of pitch motions are similar.\nThese feature has its maximal value in the case of\nmonophony, once again, whereas the presence of\nmultiple concurrent layers with opposite motions\nwill reduce its value.\nWe use these dimensions to build a new distance\ndhuron(theHuron distance) between two bars, which is ob-\ntained by summing their differences of onset synchrony\nvalues and semblant motions values, using our implemen-\ntation in the polyphonic interleaved case. This corresponds\nto the Manhattan distance between their respective coordi-\nnates in this textural space.\n2.2.2 Features of density\nWe present a last set of three distances based on low-level\ntextural features, focusing on vertical and horizontal den-\nsity. On the one hand, vertical density refers to the thick-\nness of the texture, the number of simultaneous notes –\nsimilarly to the density evoked in Section 2.1.2. On the\nother hand, horizontal density describes the volume of suc-\ncessive notes, and their position in time. For both dimen-\nsions, we use a value of volume and a value of dispersion:\n•vert_avg : average thickness, in number of notes.\nAfter slicing the bar into successive pitch sets, we\ncount the number of pitches in each slice, weighted\nby their duration.\n•vert_std : standard deviation of the number of\npitches in each onset of one or more notes.\n•horiz_avg : average number of onsets per beat.\nThe duration of one beat is inferred from the bar time\nsignature.\n•horiz_std : standard deviation of the regularity\nof onsets, i.e. around the average duration between\nsuccessive onsets.\nWe use Manhattan distance to compare two vectors of\nfeatures, computed on two target bars. We deﬁne and test\nthree variants of this distance: based on the two horizontal\nfeatures only ( dh), on the two vertical ones ( dv), or on all\nthe four (dhv).\n2.3 Implemention details and release\nThe features are extracted from the musical scores using\nintermediate Tab-Separated Values (TSV) ﬁles, which con-\ntain a list of notes (see [33]). The code1, in Python,\nincludes a converter to this format from both Hum-\ndrum **kern [34] and musicXML formats, using music21\nPython library [35].\n3. EV ALUATING TEXTURAL DISTANCES\n3.1 Dataset\nTo evaluate the relevance of the distances proposed in\nthe previous section, we use bars from classical ThemaProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n510and variations . [36] emphasizes the links between musical\nvariations in general, and musical similarity. In the genre\nof Thema and variations, a theme is reproduced in short\nsections with various changes of (textural) parameters, but\nin a way that allow to recognize the original melodic con-\ntour and/or harmony; as in Figure 1. This structure has the\nadvantage of providing both dissimilar examples (in dis-\ntinct variations), and similar examples (in the same varia-\ntion).\nAlthough no explicit mention of textural homogene-\nity within variations has been found in musicological lit-\nerature, authors more often insist on the higher contrast\nbetween distinct variations [37, p.570]. The genre of\nThema and variations provides “the largest esthetic spec-\ntrum” [38], and this variety of content is valuable in our\ncase. We rely on this fundamental assumption for the rest\nof the paper: on average, a musical bar is more similar –\nin texture – to a bar from the same musical phrase, than to\nany other bar in another variation or piece .\nWe use the TA VERN dataset [39], which consists in 27\nsets of thema and variations by Mozart (10) and Beethoven\n(17). The variations are already segmented into structural\nphrases, totalling 1060 of them in the whole dataset. We\ntake those phrases as structural units in which we use the\nscore-based distances deﬁned in section 2.2. Further anno-\ntations of texture would be required to apply label-based\ndistances on this dataset.\nRemark. The texture of phrases can vary within the\nsame variation, to a lesser extent – this is generally the\ncase in bipartite or tripartite variations, which is a common\nstructure in this context [37, 38]. Changes of mode (ma-\njor/minor) often occur, in general at least once per set of\nvariations. This change is not considered as textural, but it\nis often accompanied by changes of other musical param-\neters that are in the scope of texture, so it would still add\nvaluable information.\n3.2 Heterogeneity and contrast\nTo evaluate textural dissimilarities on full musical extracts,\nwe introduce two indicators:\n•heterogeneity : the heterogenity ( hd) within a single\nset of bars corresponds to the average distance be-\ntween pairs of distinct bars from the set , for a given\ndistance function d.\n•contrast : the contrast ( cd) between two sets of bars is\ndeﬁned as the average distance value between pairs\nof bars from the two extracts , for a given distance\nfunctiond.\nMore formally, we have:\nhd(S) =avg\n∀(mi,mj)∈S2,i̸=jd(mi,mj)\ncd(S1,S2)=avg\n∀mi∈S1,∀mj∈S2d(mi,mj)\nwhereavg is the arithmetic mean operator, S,S1andS2\nare sets of bars, and mi,mjdenote bars/measures in those\nsets.\nFigure 3 : Schematic representation of the computation of\nheterogeneity of a speciﬁc phrase (arcs above) and contrast\n(links between the bars of phrase P and all the other bars\noutside P, in the corpus T). Our evaluation metric is the\naverage value of this ratio for all the phrases of the corpus.\nThe heterogeneity is a measure of dispersion: a lower\nvalue means that samples in the extract are more similar\nbetween each other (given a distance d). We speciﬁcally\nignore the comparisons of a bar with itself to reduce the\ninﬂuence of the size of S.\nLet us illustrate those two indicators for the descriptor-\nbased distance dhv(Section 2.2.2) using examples Fig-\nure 1.a and Figure 1.d. We note Sa={bars of Fig-\nure 1.a}={a1,a2}andSd={bars of Figure 1.d }=\n{d1,d2,d3}. InSa, the heterogeneity is simply equal to\nthe distance between its two bars: small differences occur\nin horizontal density, but not in vertical density. We ob-\ntain a value of 0.6. To compute the heterogeneity inSd,\nwe have three possible unordered pairs of bars to com-\npare ({d1,d2},{d1,d3}and{d2,d3}); however, the tex-\nture in these bars is precisely the same regarding dhv, re-\nsulting a in value of zero of heterogeneity. The inequality\nhd(Sa)> hd(Sd)can be interpreted as “ Sais more tex-\nturally heterogeneous than Sd, with regards to distance d”.\nThe contrast cdbetweenSaandSdis 1.825. In general,\nthe inequality hd(Sa)< cd(Sa,Sd)means that a bar in Sa\nis, on average, more similar to other bars in Sathan bars in\nSd.\n3.3 Evaluation methodology\nWe evaluate how heterogeneous the texture is within each\nphrases of the TA VERN dataset, compared to the rest of\nthe corpus. Under the assumption exposed in Section 3.1,\nwe assess the quality of a textural distance dby looking\nfor the lowest average Relative Heterogeneity on TA VERN\nphrases (T):\naRHT(d) =avg\n∀Pi∈T/parenleftBig\nhd(Pi)\ncd(Pi,T\\Pi)/parenrightBig\nwhereTis the set of all the phrases in TA VERN dataset, Pi\nis theithphrase of the dataset, and dis a textural distance\nfunction between individual musical bars.\nThis process is schematized in Figure 3. For a given\nphrase, if the ratio between intra-phrase heterogeneity and\ninter-phrases contrast is very low, it means that the extract\nis rather homogeneous, and that this texture – or whatever\nthe distance drepresents – is rather speciﬁc to this extract\ncompared to the rest of the corpus. If this ratio is above 1, it\nmeans that the bars in this phrase are more similar to otherProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n511Distance d aRHT(d)\nHoriz. and Vert. density features dhv 0.51\nHorizontal density features dh 0.39\nVertical density features dv 0.64\nHuron’s textural space dhuron 0.72\nComparison: Pitch class content dpc 0.80\nTable 1 : Evaluation of textural distances using the Average\nRelative Heterogenity on phrases of the TA VERN dataset\n(aRHT(d)), to minimize.\nbars outside the phrases than between themselves. Put dif-\nferently: a value below 1 show that intra-phrase distances\n(heterogeneity) are smaller than inter-phrases comparison\n(contrast with the corpus). The value of aRH T(d)is the\naverage of this ratio on all the phrases of the corpus.\nRemarks. We could directly compute values of contrast\norheterogeneity on reference data, using different textural\ndistancediand opt for the most convincing values. How-\never, these values are not directly comparable if they are\nbased on different distances: they are average values of\nspeciﬁc distances, and thus follow their respective – and\npossibly very different – order of magnitude. Also note\nthat the contrast is not a distance function (or metric) be-\ncause the contrast between the same set of bars could be\ndifferent from zero – if its bars that are not all the same.\nThe functions presented in Section 2 aremetrics, applied\nto different representations of texture in a musical bar.\n3.4 Results\nThe results, for all score-based distances, are shown in\nTable 1. Using the distance based on all density features\n(dhv), the aRH Tof 0.51 indicates that a musical bar is, on\naverage, a twice more similar to bars in the same phrase\nthan to the rest of the corpus. The use of horizontal density\nfeatures alone ( dh) improves this value (0.39), highlight-\ning the importance of the time dimension to discriminate\nbetween textures.\nFor comparison, we integrate an additional distance\n(dpc) that describe not textural but harmonic content – com-\nputing Euclidean distance between pitch-classes proﬁle.\nIts aRH Tof 0.80 is still below 1, which means that intra-\nphrasedpcvalues (heterogeneity) are smaller than inter-\nphrase comparison (contrast with the corpus); this is not\nsurprising in tonal music. But most importantly, this eval-\nuation metric value is higher than for all other textural dis-\ntances. This gap contributes to validate the use of Thema\nand Variations as a source of empiric ground truth exam-\nples of textural similarities.\n3.5 Links between distances\nIn Table 2, we display correlations between all the dis-\ntances deﬁned in Section 2. They are computed on all dis-dhvdhdvdhuronddDdtexel\ndhv 1.00 \" \" \" \" \"\ndh 0.95 1.00 \" \" \" \"\ndv 0.33 0.05 1.00 \" \" \"\ndhuron 0.10 0.10 0.05 1.00 \" \"\nddD 0.19 0.03 0.53 0.03 1.00\ndtexel 0.10 0.06 0.14 0.01 0.20 1.00\nTable 2 : Spearman correlation between textural distances\nas deﬁned in Section 2, evaluated on all pair of bars in three\nMozart piano sonatas (K. 279, K. 280, K. 283).\ndhvdhdvdhuronddDdtexel\nHorizontal density\n(time dimension)× × ×\nVertical density\n(thickness)× × × ×\nSemblant motions,\nparallelism× ×\nRoles of layers\n(melody, acc. ...)×\nMain types of\ntexture (see Fig.2)× ×\nComputed on\nsymbolic scores× × × ×\nComputed on\nannotated labels× ×\nTable 3 : Summary of the distances deﬁned in Section 2,\nand the different dimensions of compositional texture that\nthey take into account.\ntinct pairs of bars among 1160 from Mozart piano sonatas\n(K. 279, K. 280, K. 283), for which we have both textural\nannotations [6] and encoded scores [33]. We use Spearman\ncorrelation, that depicts similarities of rankings of these\nvalues.\nHuron-space distance ( dhuron) and texel distance ( dtexel)\nseem independant from other distances. We explain it by\nthe fact that our distances focus on different dimensions of\ntexture, summarized in Table 3. In particular, dtexelcovers a\nwide range of abstract concept and is the only function that\ndeals with the roles of layers, which is difﬁcult to approxi-\nmate using low-level features. Otherwise, we ﬁnd that us-\ning horizontal density features only ( dh) gives a very simi-\nlar behavior than using all four density features ( dhv) – with\na correlation of 0.95. Although Density-Diversity distance\n(ddD) and vertical-feature distance ( dv) deal with very dif-\nferent level of abstraction, they correlates positively (0.53),\nas they both focus on the vertical dimension of texture.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n512Figure 4 : Textural dissimilarities between the phrases of\nTen Variations in G on ‘Unsere dummer Pöbel meint’\nby W. A. Mozart (K. 455, 1784). Intersections are col-\nored according to contrast values using dhvdistance (Sec-\ntion 2.2.2), and heterogeneity of phrases on the diagonal\n(Section 3.2). The phrases are scaled according to their\nsize in number of bars (totalling 338 in the whole piece).\nThe most similar extracts are shown in dark blue, whereas\nlight green indicates higher dissimilarity. – We identify\nblocks of consecutive similar variations, such as (1,2,3) or\n(5,6,7); inner structure of varations may reveal contrasting\nsegments in the case of Variation 4; Variation 9 is very con-\ntrasted due to the alternation between chordal texture and\nfast melodic lines; the penultimate phrase comes back to\nthe original texture of the Thema.\n4. USE CASES FOR STRUCTURE ANALYSES\n4.1 Long-term textural dissimilarities\nThecontrast deﬁned in Section 3.2 can be used as a dissim-\nilarity measure between any sets of bars, from individual\npieces to entire corpora. It may also emphasize the rela-\ntionships between sections, or phrases, of a given piece of\nmusic. Figure 4 shows an example of self-similarity ma-\ntrix based on textural contrast between phrases of a piece\nin Thema and Variations form. Beyond the case of Thema\nand Variations, the contrast measure gives an overview of\nthe piece macrostructure, and may even link thematic ma-\nterial up to transposition, such as recapitulation parts in\nsonata form. More generally, the proposed distances can\nlead to promising and original approaches for automatic\nstructure segmentation.\n4.2 Short-term textural changes\nIn this paper, we assume lower textural heterogeneity\nwithin phrases of thema and variations. But in the gen-\neral case, changes of textures may occur in the middle\nof a phrase. Following the intuition that in-phrase texture\nchanges mostly occur in openings and endings of phrases\nin the TA VERN dataset, we evaluate dhvusing the same\nmethodology as in Section 3.3, but systematically ignorethe last bar of each phrase of the corpus. We ﬁnd that\naRHT(dhv)decreases from 0.51 to 0.42. When removing\neach ﬁrst bar instead, it drops to 0.34. In comparison, re-\nmoving the second or third bar of the phrases increases the\noriginal value of aRH T(dhv)to respectively 0.55 and 0.54.\nThis shows that the ‘core’ of phrases have slightly more\ntextural homogeneity, and most importantly that openings\nand endings are less similar to the middle of phrases. Typ-\nical examples are transitional melodies and ﬁnal chords\nin cadences – which often constrast with the rest of the\nphrase. We believe that our distance can be used to study\nmore precisely these local changes of texture within short\nsections.\n5. CONCLUSION AND FURTHER WORKS\nThe textural distances proposed in this paper give promis-\ning perspectives for the computation of multi-level simi-\nlarities in symbolic music. On the one hand, comparing\ntextural labels allows to rely on expert data, which is al-\nready known as texturally meaningful. This information\nalready carries a lot of abstraction, but it is costly to pro-\nduce in practice and can lead to a certain amount of sub-\njectivity [40]. Moreover, the low amount of available an-\nnotations hinders our ability to evaluate the quality of these\ndistances. On the other hand, using symbolic features that\ncan be computed automatically is more practical, and also\nmore objective. In further work, we plan to investigate the\nbest features to use at a more global scale, as well as their\nrelative contibution.\nAlthough the proposed distances are drawn at the level\nof musical bars, we elaborated a more global dissimilar-\nity measure to compare sets of several bars, and highlight\ntextural contrast between and within structural sections of\nmusical pieces. This measure made possible a quantitative\nevaluation of textural distances on a corpus of Thema and\nVariations, based on the assumption that texture is more\ndissimilar between two distinct variations, and more ho-\nmogeneous within single variations.\nOur distances capture different facets of compositional\ntexture, at different levels of abstraction (see Table 3). Fo-\ncusing on more atomic and independant textural aspects\ncan enhance the precision and the interpretability of our\nanalyses. However, ensuring a proper disentanglement of\nsuch dimensions remains a major challenge. Integrating\nThema and variations in the evaluation methodology is a\nstep further to link theoratical models of texture to con-\ncrete, and somewhat intuitive, examples. It contributes to\na better understanding of some models of texture, but also\nof musical texture itself.\nA potential continuation of this work is to broaden the\nscope of our experiments to other repertoires. We believe\nthat the tools introduced in this paper are easily extendable\nto other styles of written polyphonic music, or to other in-\nstruments. In the meantime, the present experiments on\nWestern classical piano music already offer promising op-\nportunities of quantitative analyses of texture with regards\nto genre, style, form or harmony.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n5136. ACKNOWLEDGMENT\nWe want to thank Jean-Paul Chehab for fruitful discus-\nsion at the beginning of the project. We also thank Dinh-\nViet-Toan Le, Alexandre D’Hooge and the rest of the\nAlgomus team, as well as the anonymous reviewers for\ntheir constructive feedback. This research is partly funded\nby Région Hauts-de-France and by Agence Nationale de\nla Recherche (ANR), project TABASCO ANR-22-CE38-\n0001.\n7. REFERENCES\n[1] B. Benward and M. Saker, Music: In Theory and Prac-\ntice, Vol. I. Eigth Edition. McGraw-Hill, 2008, ch. 7,\np. 145–162.\n[2] W. Berry, Structural functions in music: A Probing Ex-\nploration of Conceptual Bases and Techniques for the\nAnalysis of Tonality, Texture, and Rhythm. Prentice-\nHall, 1976.\n[3] D. Huron, “Characterizing musical textures,” in Inter-\nnational Computer Music Conference (ICMC 1989) ,\n1989, pp. 131–134.\n[4] J. Dunsby, “Considerations of texture,” Music & Let-\nters, vol. 70, no. 1, pp. 46–57, 1989.\n[5] D. M. de Sousa, “Textural design: A compositional\ntheory for the organization of musical texture,” Ph.D.\ndissertation, Universidade Federal do Rio de Janeiro,\n2019.\n[6] L. Couturier, L. Bigo, and F. Levé, “A dataset of tex-\nture annotations in Mozart piano sonatas,” in Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR 2022) , 2022.\n[7] Q. R. Nordgren, “A measure of textural patterns and\nstrengths,” Journal of Music Theory , vol. 4, no. 1, pp.\n19–31, 1960.\n[8] Z. Wang, D. Wang, Y . Zhang, and G. Xia, “Learning in-\nterpretable representation for controllable polyphonic\nmusic generation,” in International Society for Music\nInformation Retrieval Conference (ISMIR 2020) , 2020.\n[9] O. Cífka, U. ¸ Sim¸ sekli, and G. Richard,\n“Groove2Groove: one-shot music style transfer\nwith supervision from synthetic data,” IEEE/ACM\nTransactions on Audio, Speech, and Language\nProcessing , vol. 28, pp. 2638–2650, 2020.\n[10] L. Couturier, L. Bigo, and F. Levé, “Annotating sym-\nbolic texture in piano music: a formal syntax,” in\nSound and Music Computing Conference (SMC 2022) ,\n2022.\n[11] M. Giraud, F. Levé, F. Mercier, M. Rigaudière, and\nD. Thorez, “Towards modeling texture in symbolic\ndata,” in International Society for Music Information\nRetrieval Conference (ISMIR 2014) , 2014, pp. 59–64.[12] D.-V .-T. Le, M. Giraud, F. Levé, and F. Maccarini, “A\ncorpus describing orchestral texture in ﬁrst movements\nof classical and early-romantic symphonies,” in Digital\nLibraries for Musicology (DLfM 2022) , 2022, pp. 22–\n35.\n[13] D. Conklin, “Pattern in music,” Journal of Math-\nematics and Music , vol. 15, no. 2, pp. 95–98,\n2021. [Online]. Available: https://doi.org/10.1080/\n17459737.2021.1947404\n[14] B. Janssen, W. B. de Haas, A. V olk, and P. van Kra-\nnenburg, “Finding repeated patterns in music: State of\nknowledge, challenges, perspectives,” in International\nSymposium on Computer Music and Multidisciplinary\nResearch (CMMR 2013) , 2013, pp. 277–297.\n[15] A. Ferraro and K. Lemström, “On large-scale genre\nclassiﬁcation in symbolically encoded music by auto-\nmatic identiﬁcation of repeating patterns,” in Proceed-\nings of the 5th International Conference on Digital Li-\nbraries for Musicology , 2018, pp. 34–37.\n[16] J. Ens and P. Pasquier, “Quantifying musical style:\nRanking symbolic music based on similarity to a style,”\narXiv preprint arXiv:2003.06226 , 2020.\n[17] P. Knees and M. Schedl, Music similarity and re-\ntrieval: an introduction to audio-and web-based strate-\ngies. Springer, 2016, vol. 9.\n[18] M. Schedl, P. Knees, B. McFee, and D. Bogdanov,\n“Music recommendation systems: Techniques, use\ncases, and challenges,” in Recommender Systems\nHandbook . Springer, 2021, pp. 927–971.\n[19] T. Collins, J. Thurlow, and R. Laney, “A comparative\nevaluation of algorithms for discovering translational\npatterns in baroque keyboard works,” in International\nSociety for Music Information Retrieval Conference\n(ISMIR 2010) , 2010, pp. 3–8.\n[20] V . Velardo, M. Vallati, and S. Jan, “Symbolic melodic\nsimilarity: State of the art and future challenges,” Com-\nputer Music Journal , vol. 40, no. 2, pp. 70–83, 2016.\n[21] F. Karsdorp, P. van Kranenburg, and E. Manjavacas,\n“Learning similarity metrics for melody retrieval.” in\nISMIR , 2019, pp. 478–485.\n[22] S. Park, T. Kwon, J. Lee, J. Kim, and J. Nam, “A\ncross-scape plot representation for visualizing sym-\nbolic melodic similarity.” in ISMIR , 2019, pp. 423–\n430.\n[23] F. Bruford, O. Lartillot, S. McDonald, and M. San-\ndler, “Multidimensional similarity modelling of com-\nplex drum loops using the groovetoolbox,” in Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR 2020) , 2020, pp. 207–215.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n514[24] D. Cocharro, G. Bernardes, G. Bernardo, and\nC. Lemos, “A review of musical rhythm representation\nand (dis) similarity in symbolic and audio domains,”\nPerspectives on Music, Sound and Musicology: Re-\nsearch, Education and Practice , pp. 189–208, 2021.\n[25] T. Rocher, M. Robine, P. Hanna, and M. Desainte-\nCatherine, “A survey of chord distances with compari-\nson for chord analysis,” in ICMC , 2010.\n[26] W. B. De Haas, M. Robine, P. Hanna, R. C. Veltkamp,\nand F. Wiering, “Comparing approaches to the similar-\nity of musical chord sequences,” in Exploring Music\nContents: 7th International Symposium, CMMR 2010,\nMálaga, Spain, June 21-24, 2010. Revised Papers 7 .\nSpringer, 2011, pp. 242–258.\n[27] D. Bountouridis, H. V . Koops, F. Wiering, and R. C.\nVeltkamp, “A data-driven approach to chord similarity\nand chord mutability,” in 2016 IEEE Second Interna-\ntional Conference on Multimedia Big Data (BigMM) .\nIEEE, 2016, pp. 275–278.\n[28] K. Lemström and A. Pienimäki, “On comparing edit\ndistance and geometric frameworks in content-based\nretrieval of symbolically encoded polyphonic music,”\nMusicae Scientiae , vol. 11, no. 1_suppl, pp. 135–152,\n2007.\n[29] J. Lee, N. J. Bryan, J. Salamon, Z. Jin, and J. Nam,\n“Disentangled multidimensional metric learning for\nmusic similarity,” in ICASSP 2020-2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2020, pp. 6–10.\n[30] S. McAdams, S. Vieillard, O. Houix, and R. Reynolds,\n“Perception of musical similarity among contempo-\nrary thematic materials in two instrumentations,” Mu-\nsic Perception , vol. 22, no. 2, pp. 207–237, 2004.\n[31] E. Cambouropoulos, “How similar is similar?” Musi-\ncae Scientiae , vol. 13, no. 1_suppl, pp. 7–24, 2009.\n[32] C. Finkensiep and M. A. Rohrmeier, “Modeling and\ninferring proto-voice structure in free polyphony,” in\nInternational Society for Music Information Retrieval\nConference (ISMIR 2021) , 2021, pp. 189–196.\n[33] J. Hentschel, M. Neuwirth, and M. Rohrmeier, “The\nannotated Mozart sonatas: Score, harmony, and ca-\ndence,” Transactions of the International Society for\nMusic Information Retrieval , vol. 4, no. 1, pp. 67–80,\n2021.\n[34] D. Huron, “Music information processing using the\nhumdrum toolkit: Concepts, examples, and lessons,”\nComputer Music Journal , vol. 26, no. 2, pp. 11–26,\n2002.\n[35] M. S. Cuthbert and C. Ariza, “music21: A toolkit\nfor computer-aided musicology and symbolic music\ndata,” in International Society for Music Information\nRetrieval Conference (ISMIR 2010) , 2010, pp. 637–\n642.[36] A. V olk, W. B. de Haas, and P. Van Kranenburg, “To-\nwards modelling variation in music as foundation for\nsimilarity,” in Proceedings of the 12th International\nConference on Music Perception and Cognition and\nthe 8th Triennial Conference of the European Society\nfor the Cognitive Sciences of Music . School of Music\nStudies, Aristotle University of Thessaloniki, 2012, pp.\n1085–1094.\n[37] W. E. Caplin, Analyzing Classical Form: An approach\nfor the classroom . Oxford University Press, 2013.\n[38] E. De Montalembert and C. Abromont, Guide des gen-\nres de la musique occidentales . Paris: Fayard Henry\nLemoine, 2010.\n[39] J. Devaney, C. Arthur, N. Condit-Schultz, and\nK. Nisula, “Theme and variation encodings with roman\nnumerals (tavern): A new data set for symbolic music\nanalysis,” in International Society for Music Informa-\ntion Retrieval Conference (ISMIR 2015) , 2015.\n[40] Y . Ni, M. McVicar, R. Santos-Rodriguez, and\nT. De Bie, “Understanding effects of subjectivity in\nmeasuring chord estimation accuracy,” IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\nvol. 21, no. 12, pp. 2607–2615, 2013.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n515"
    },
    {
        "title": "Modeling Bends in Popular Music Guitar Tablatures.",
        "author": [
            "Alexandre D&apos;Hooge",
            "Louis Bigo",
            "Ken Déguernel"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265396",
        "url": "https://doi.org/10.5281/zenodo.10265396",
        "ee": "https://zenodo.org/records/10265396/files/000088.pdf",
        "abstract": "Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends. This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard.\n In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 and a limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of\nnon-guitar music into guitar tablatures.",
        "zenodo_id": 10265396,
        "dblp_key": "conf/ismir/DHoogeBD23",
        "keywords": [
            "tablature notation",
            "popular music",
            "guitar musical content",
            "tablatures",
            "performance gesture information",
            "finger positions",
            "guitar-specific playing techniques",
            "bends",
            "pitch shifting",
            "discrete fretted fingerboard"
        ],
        "content": "MODELING BENDS IN POPULAR MUSIC GUITAR TABLATURES\nAlexandre D’Hooge Louis Bigo Ken Déguernel\nUniv. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France\nalexandre.dhooge@algomus.fr\nABSTRACT\nTablature notation is widely used in popular music to tran-\nscribe and share guitar musical content. As a comple-\nment to standard score notation, tablatures transcribe per-\nformance gesture information including ﬁnger positions\nand a variety of guitar-speciﬁc playing techniques such as\nslides ,hammer-on/pull-off orbends . This paper focuses\non bends, which enable to progressively shift the pitch of\na note, therefore circumventing physical limitations of the\ndiscrete fretted ﬁngerboard. In this paper, we propose a\nset of 25 high-level features, computed for each note of the\ntablature, to study how bend occurrences can be predicted\nfrom their past and future short-term context. Experiments\nare performed on a corpus of 932 lead guitar tablatures\nof popular music and show that a decision tree success-\nfully predicts bend occurrences with an F 1score of0.71\nand a limited amount of false positive predictions, demon-\nstrating promising applications to assist the arrangement of\nnon-guitar music into guitar tablatures.\n1. INTRODUCTION\nThe guitar, whether acoustic or electric, is (in most cases)\na fretted instrument which enforces the playing of discrete\npitch values on a chromatic scale. This constraint can be\nbeneﬁcial, as it limits the risk of playing out-of-tune. How-\never, it also prevents microtonal experiments or continuous\npitch shifts, which can be a powerful means of musical ex-\npressiveness. To overcome this limitation, guitarists can\nalter the string tension with their fretting hand [1] to reach\na completely new pitch, up to several semitones higher.\nThis technique is called string bending, or just bends, and\nis an important part of guitar playing in blues, rock or pop\nmusic. Even though bending a string can only increase\nthe pitch of a note, a variety of bend types are used. While\nguitar players mostly agree over the existing variations, the\nnames used can differ. In this paper, we consider the ﬁve\nbend types described in Gomez’s work [2]: Basic upward ,\nHeld ,Reverse ,Up & Down , and Complex bends for bends\nthat do not belong to any of the previous categories.\nGuitar tablatures, compared to standard staff notation,\ninclude ﬁngering information on where to play a note with\n© A. D’Hooge, L. Bigo and K. Déguernel. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: A. D’Hooge, L. Bigo and K. Déguernel, “Modeling bends\nin popular music guitar tablatures”, in Proc. of the 24th Int. Society for\nMusic Information Retrieval Conf., Milan, Italy, 2023.a given pitch on the fretboard. Tablatures are therefore an\neffective notation to display playing techniques, the posi-\ntion of the fretting hand being critical to know how to per-\nform a bend. Examples of bends are shown in a tablature\nin Figure 1, different bend types are represented with dif-\nferently shaped arrows. Knowing where and when to use\nbends is part of the idioms of guitar pop music and an im-\nportant part of learning this style. However, the variety of\nbend types can make it difﬁcult to choose how to use them.\nFor instance, a guitarist playing a score that was composed\nfor another instrument may want to add expressiveness us-\ning bends, and could need help on deciding which notes to\nbend and which bend to use. Moreover, a tool suggesting\nbends could also improve the quality of online tablatures\nthat sometimes do not have guitar techniques annotations.\nCould bends be inferred from musical context? Are\nthey correlated to other elements of a score or a tablature\nsuch as pitch, rhythm, or hand position? In this paper, we\npropose to model bent notes and their context through tem-\nporal, pitch and tablature related information. Such a rep-\nresentation could be used to predict which notes are bent\nfrom a score or a tablature. Our contribution is three-fold:\n(1) we deﬁne a set of high-level features to model bent\nnotes and their context, (2) we conduct a statistical study\non bends based on those features, and (3) we propose a\nmethod for predicting bends from tablatures.\nThe rest of this paper is organized as follows: after dis-\ncussing related works in Section 2, we introduce our mod-\neling choices in Section 3. The dataset and its statistical\nstudy are presented in Section 4. We introduce our pre-\ndiction algorithm in Section 5 and reﬂect on this work in\nSection 6.\n2. RELATED WORKS\nGuitar tablatures are often studied in the audio realm for\nguitar music transcription [3, 4]. In particular, automatic\ntranscription of playing techniques from audio has also\nbeen studied, for instance with hexaphonic microphones\n[5] or deep learning techniques [6]. Of course, this task\nis not restricted to guitar and has been applied to other in-\nstruments such as the Chinese guqin , which also features\nseveral string bending techniques [7].\nAnother related task is the generation of tablatures,\nwhich has been studied increasingly in the last decade.\nPlaying techniques can be included in generation frame-\nworks to obtain results closer to actual human perfor-\nmance. The Transformer-XL model presented in [8, 9]\ncan for instance generate tokens representing playing tech-741Figure 1 : Excerpt from Lynyrd Skynyrd’s Free Bird solo. In the ﬁrst measure, the ﬁrst two bends are basic upward bends ,\nthe remaining ones are held bends . In the second measure, the ﬁrst bend is a reverse bend , and the other ones are up and\ndown bends . While all bends of this example are whole tones (denoted by full), the amplitude of a bend can vary. String\nmovements are labeled above, according to the representation presented in Section 3. An audio rendering of this excerpt is\navailable on the accompanying repository .\nniques. Chen et al. [10] also consider Technique events, but\nonly for picking-hand techniques, since the generation fo-\ncuses on ﬁngerstyle guitar. McVicar et al. present in [11] a\nmethod to generate tablatures of guitar solo phrases, and\nfretting-hand techniques are added in a post-processing\nstep. Beyond the case of guitar, playing techniques model-\ning includes symbolic piano music generation with sustain\npedal information [12].\nAnother large part of guitar tablature MIR research fo-\ncuses on ﬁngering prediction, where a model is designed to\npredict the pitch/fret combination for each note of a musi-\ncal score. This problem has been studied with path-ﬁnding\nalgorithms [13] and minimax techniques [14]. More re-\ncently, Cheung et al. [15] used a deep learning model to\ngenerate ﬁngering annotations, though not for guitar but vi-\nolin. Those instruments nonetheless share some properties,\nand a study of ﬁngering prediction on string instruments\nlike the violin or the guitar, compared to other instruments,\ncan be found in [16]. However, the task of using symbolic\nmusic to study occurrences of playing techniques is rarely\nstudied. Xie and Li [17] propose to predict playing tech-\nniques as a tagging task on symbolic bamboo ﬂute music\nbut, to the best of our knowledge, no such work has been\nproposed for guitar music.\n3. MODELING BENDS IN TABLATURE\nTo formulate bend prediction as a machine learning task,\nwe adopt a representation for bent notes consisting of four\ndifferent labels. We also need to pre-process our data to\nobtain bend-less scores, and musical features that could be\nused to train a machine learning model. Those considera-\ntions are presented hereafter.\n3.1 Labeling\nIn the introduction, we presented the different types of\nbends that can be encountered in guitar tablatures. Based\non this taxonomy, we deﬁne 4 labels that represent the mo-\ntion and current state of the played string:\n•∅— the string is not bent;•↑— the string is bent, causing the pitch to go up;\n•→— the string was bent previously and is plucked\nagain in that state. The pitch is constant, but is not\nthe one expected from the note’s string/fret position;\n•↓— the string was bent and is released, making the\npitch go down accordingly.\nWe deﬁne those labels to circumvent the issue with up\n& down andcomplex bends that are not transcribed con-\nsistently. Labeling notes with the associated string move-\nments therefore permits representing bends accurately,\nwithout loss of generality. Using these labels, all non-bent\nnotes will be labeled ∅,basic upward bends are labeled ↑,\nheld bends correspond to →, and reverse bends are↓. To\nﬁt with this representation, up & down bends are split into\ntwo notes of equal duration, the ﬁrst one being labeled with\n↑, and the second one with ↓. An example of such labeling\non an actual guitar track is shown on top of Figure 1.\nWhile bends can be of different amplitude, we do not\ninclude that information in our labeling, as we do not aim\nat predicting it in this work but only focus on predicting\nwhen bends occur. Similarly, we do not distinguish single\nnotes from chords when predicting bends. This means that\nwhen the system predicts the presence of a bend in a chord,\nit does not specify on which string it occurs. The impact of\nthis simpliﬁcation is however limited, as bends rarely occur\ninside chords (12% of bend events are found in chords in\nour corpus).\n3.2 Deriving a bend-less score simpliﬁcation\nSince our goal is to predict whether a note is played with\na bend, we must start from a simpliﬁed tablature that does\nnot have any bend information. Removing bend annota-\ntions from a tablature is however not a straightforward task\nsince this technique affects the pitch of the performed note.\nWe design the following procedure when translating bent\nnotes from the fret/string space to the pitch space:\n• If a note is labeled by ∅, its pitch is directly obtained\nfrom the string/fret combination;Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n742Figure 2 : Excerpt of Watermelon in Easter Hay , Frank\nZappa, as transcribed [18] by Steve Vai in standard nota-\ntion (top). Below are two possible tablature representations\nof this excerpt with (middle), or without (bottom) bends.\n• Otherwise, the pitch is the one of the bend arrival\nnote. In particular:\n–if the label is ↑or→, the arrival pitch is the\npitch of the string/fret combination, to which\nthe bend amplitude is added;\n–if the label is ↓, the arrival pitch is the one cor-\nresponding to the string/fret position, because\nthe string is released to its default state.\nThis approach is the one chosen by Steve Vai when tran-\nscribing Frank Zappa’s melodies to standard notation, as\nwe illustrate in Figure 2. The middle tablature shows\nhow this excerpt is actually played (based on a live perfor-\nmance) and the bottom tablature illustrates how the same\nexcerpt might be played without bends. While it is not\nan issue in this example, there is an uncertainty regard-\ning where a bent note would be played on the fretboard,\nwithout a bend (keeping its destination pitch but losing the\ntechnique). A guitar player might indeed choose to play\na note on a higher string, if remaining on the same string\ncalled for an uncomfortably large hand span. Because de-\nciding arbitrarily of a hand position could introduce bias\ninto our model, we choose not to include any string/fret\ninformation concerning the current note in the proposed\nfeatures for our classiﬁcation task, as explained hereafter.\n3.3 Selected Features\nTo predict whether a note is bent, we propose an intermedi-\nate representation as a set of high-level features, presented\nin Table 1. Some descriptors focus on the event under\nscrutiny, while others provide short-term context informa-\ntion, both from the past and the future. Part of the fea-\ntures are derived from standard staff notation and convey\ntemporal and pitch information, while others are related to\nposition and the tablature space. If the studied event is a\nchord, the pitch, fret, and string values are averaged over\nall its constituting notes. While the average might seem an\noverly simple statistic, experiments with other functionalsTemporalDuration\nBeat Strength\nLonger than previous\nShorter than previous\nSame duration as previous\nPitchNumber of notes\nPitch(j)\nPitch jump(n±k)\nAccidentals\nPitch-class w.r.t scale root\nPositionFret(n±k)\nString(n±k)\nFret jump(n±2)\nString jump(n±2)\nTable 1 : List of high-level features extracted from the\nnote events. Let ndenote the current note index, an expo-\nnent on a feature tells on which neighbors it is computed.\nk∈ {1,2}because we discard any positional information\nrelated to the current note, and j∈/llbracketn−2,n+2/rrbracket. Other\nfeatures are only computed on n.\nsuch as min, max orstandard deviation did not improve the\nresults. We have discarded any open strings for the fret-\nboard features so that the average fret and string represents\nthe actual position of the fretting hand. Apart from abso-\nlute/relative duration values, temporal features include the\nbeat strength , which is a value between 0 and 1 suggesting\nhow strong the beat of a note is. We obtain this value from\nthe default implementation of the music21 library [19].\nThese beat strength values have been designed for Western\nclassical music, and therefore may be debatable for pop\nand rock music. However, they are mostly used here to\nrepresent onset times independently of the time signature,\nwhile grouping notes that share rhythmic properties.\nIn addition to the features on the current note, we extract\na context of two past and two future note events, as pre-\nliminary experiments did not show any beneﬁts of longer\ncontexts. Additional boolean features are provided to re-\ncall if a neighboring note event is missing, when a note is\npreceded or followed by a whole rest for instance. When\na note is missing, all corresponding features are set to 0.\nFrom this context, we compute the pitch jump between\nneighboring notes as well as the string and fret jumps when\nthey are deﬁned, i.e.,not with respect to the current note –\nbecause we do not know where the guitarist would play\nthe note if they were to bend it. We expect these features\nto help our algorithm derive the hand position on the fret-\nboard, which would be useful since bends are more likely\nto occur on certain spots of the fretboard, as will be shown\nsubsection 4.2. Furthermore, we add information about the\nkey signature through the number of accidentals (positive\nfor sharps, negative for ﬂats). From those accidentals, we\nderive the root note of the corresponding pentatonic minor\nscale (that scale encompassing much of guitar popular mu-\nsic [20]) and store the position of each note on this scale.\nFor example, one sharp would make the root E, and an A\nwould be numbered 5 since it is 5 semitones above E.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n743∅ ↑ → ↓ Total\n123231 9627 1270 3314 137442\nTable 2 : Number of notes per label in our dataset.\ne\nB\nG\nD\nA\nEStringAll notes\n0 5 10 15 20\nFret numbere\nB\nG\nD\nA\nEStringBent notes\n0.00.20.40.60.81.0\nNormalized activation\nFigure 3 : Normalized Heatmaps of all notes (Top) and\nbent notes (Bottom). The letters refer to the open string\npitch in standard tuning with ebeing the high E string.\n4. DATASET\n4.1 Guitar Tablature Corpus\nOur experiments are performed on the proprietary corpus\nMySongBook composed of 2247 guitar tablatures accu-\nrately transcribed by professional musicians in the .gp\nGuitarPro format. A subset of 932 tracks estimated as lead\nguitar – totaling more than 130000 notes – was extracted\nby applying the classiﬁcation technique from [21]. Our\nexperiments focus on lead guitar parts, as they were felt to\nfeature heavier use of playing techniques. In contrast with\nthe whole corpus, which includes 2.5%of bent notes, our\nlead guitar sub-corpus indeed contains 10% of bent notes,\nslightly mitigating the observed class imbalance.\nOur work is implemented in Python and usesmusic21\n[19] andscikit-learn [22] libraries. To foster repro-\nducibility, all our code is made publicly available (parsing\nof.gp ﬁles, extraction of features, training, and evalua-\ntion of bend classiﬁcation models). We also release the\ncomplete set of features extracted on each note of our cor-\npus, plus corresponding labels at:\nhttp://algomus.fr/code/ .\n4.2 Statistical study\nTable 2 reports the distributions of bend labels in our cor-\npus. The distribution of bent notes on the fretboard, com-\npared to all notes, is shown in Figure 3. We observe that\nmost bends occur on the top 3 strings in the middle area\nof the fretboard. This observation differs from notes in\ngeneral that are played on all strings, and especially on\nthe two middle ones and around the 7thfret. While it is\npossible that the obtained heatmaps are biased by an over-\nrepresentation of certain key signatures in the dataset –\n43% of the tracks are in G major/E minor or C major/A mi-\nnor – this bias should affect both heatmaps equally, so their\nmutual comparison is still possible. Because bent notes are\nfound on both higher strings and higher frets than all notes,\ntheir pitch is similarly higher on average, as it can be ob-\nserved in Figure 4a.The distribution of beat strength values is shown in Fig-\nure 4b. Because most beats and sub-beats in a measure\nhave a beat strength of 0.25or below, no label is mostly\nplayed on strong beats.An interesting result is that ↑and\n↓labels appear more often on stronger beats than ∅and\n→. This apparent correlation of ↑and↓labels with the\nmeter might suggest a link between note expressiveness\nand accentuation in performance, which would need to be\ninvestigated further. In contrast, the →label is most of-\nten encountered on weaker beats. This observation can be\nlinked to the fact that this technique is often used as a quick\nrepetition of the previous note and will thus be played on\nthe next offbeat, like in Figure 1.\nThe comparison of the duration of notes with or with-\nout bends (Figure 4c) conﬁrms that ↑and↓labels share\nsome essential properties. Both labels have a proportion-\nally higher tendency to be found on notes with longer du-\nration, even though eighth note is the most common dura-\ntion for all classes. This ﬁgure also conﬁrms that ∅and\n→classes share some context properties. Figure 4d shows\na strong tendency of ↑labels to appear on notes with longer\nduration than their predecessor. This further supports the\nhypothesis that bends could be used to emphasize signiﬁ-\ncant notes in a lead guitar part. That result could also be\nrelated to the substantial physical effort required to bend\na string on short duration notes. The accompanying code\nprovides interactive computation of the distribution of the\nother features.\n5. CLASSIFICATION RESULTS\nA decision tree [23] was trained to predict the bend label of\na note from its feature representation. We choose this high-\nlevel approach to facilitate the interpretation of the results\nas well as the analysis of the contribution of the features.\nIn addition to the elaboration of a predictive model, con-\nducting our experiments in an explainable AI framework\nallows us to improve our understanding of the use of bends\nin this repertoire. We hope that the use of light models\nenabled by highly expressive musical representations will\nalso contribute to promoting low energy consumption ap-\nproaches in machine learning for MIR.\n5.1 Model performance\nOur classiﬁer is trained on 75% of the dataset, and evalu-\nated on the remaining 25%. To avoid some leakage from\nthe training set to the test set, we ensure the split does not\nseparate notes from a same track. We also ensure that the\nclass imbalance is similar in both sets. Duplicate feature\nvectors are removed track-wise to avoid overﬁtting due to\nrepeated riffs/patterns. But, we acknowledge the fact that\nidentical feature vectors can be found in different tracks\nand thus keep duplicates when found in different ﬁles.\nThe confusion matrix of Figure 5 shows the results\nof the multi-class classiﬁcation on the joint prediction of\nall labels. Because the dataset is highly unbalanced, our\nmodel is naturally biased towards the ∅label. However,\nit successfully identiﬁes more than half of the ↑and↓la-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n74430 40 50 60 70 80 90 100\nMIDI Pitch Value0.000.010.020.030.040.05Density\n(a) Pitch1/16 1/8 1/4 1/2 1\nBeat Strength0.000.050.100.150.200.250.30Frequency\n(b) Beat StrengthDuration0.00.10.20.30.40.5Frequency½  \n  \n  .\n    \n(c) DurationBend Type0.00.20.40.60.81.0Distribution of duration differencesLonger\nEqual\nShorter\n(d) Duration diff. with previous\nFigure 4 : Distribution of four of the extracted features, normalized per bend class.\nPredicted label\nAccuracy=0.591True label28566\n94%1237\n4%179\n< 1%513\n2%\n786\n36%1298\n59%94\n4%23\n1%\n133\n47%56\n20%83\n29%14\n5%\n312\n40%37\n5%3\n< 1%424\n55%\n0.20.40.60.8\nFigure 5 : Confusion matrix obtained for classifying each\nnote event to one of the bend class. This matrix was ob-\ntained on a split with average performance.\nbels. Samples labeled as →are often misidentiﬁed as ↑but\nthis result still shows, presumably, that the model captures\nthe difference between ∅and→labels. We tried apply-\ning SMOTE oversampling [24] to the training data and ob-\nserved that it doubles the number of correctly identiﬁed\n→notes and increases the ratio of well-classiﬁed ↓notes\nby approximately 10%. Nevertheless, ↑notes True Pos-\nitives (TP) ratio is about the same while the quantity of\n↑notes misidentiﬁed as →or↓increases. Similarly, TP\nratio of∅notes drops by 5 p.p., so 2000 more notes are\nwrongly predicted as bent. Because we observed that bent\nnotes are sparse in guitar tracks, we consider that precision\nis more important than recall and do not use any oversam-\npling for the rest of our analysis.\n5.2 Feature importance\nTo assess the contribution of each feature, we conduct an\nall bend binary classiﬁcation experiment where ↑,→,↓\nare merged into a single class, versus the ∅class. Ta-\nble 3 shows the importance of the eight most contributing\nfeatures, computed using the random feature permutation\ntechnique introduced in [25] and monitoring its impact on\nthe F1score of our model. Temporal and pitch features ap-\npear to have a higher impact on classiﬁcation than position-\nrelated features, an observation conﬁrmed by training the\nbinary classiﬁer on selected subsets of features. The re-\nsults in Figure 6 conﬁrm the dominant inﬂuence of pitch\nfeatures. However, adding gesture and temporal informa-\ntion noticeably improve the results. This result suggestsFullFull+ Pi+T\nPi+Pos Pos+TPiPos0.00.10.20.30.40.50.60.70.8F1score\nFigure 6 : Average F 1-scores from 4 different train/test\nsplits for the binary classiﬁcation task. The leftmost part\nshows the performance of the decision tree trained on all\nfeatures, with (Full+) or without (Full) SMOTE oversam-\npling. The rightmost part corresponds to decision trees\ntrained with a reduced set of features. Tstands for tem-\nporal , Pifor pitch and Posfor position features.\nthat, while fret context contributes to induce bent notes,\na large part of the prediction can be done from the strict\nmusical content as notated in musical scores.\n6. PREDICTION ANALYSIS\nIn addition to the quantitative results presented in the last\nsection, we present a qualitative analysis of selected pre-\ndictions. Figure 7a shows one bent note wrongly identiﬁed\nas∅and, conversely, one non-bent note identiﬁed as ↑.\nFollowing the decision path provided by the decision tree,\nwe can gain some insight on what feature differences have\nFeature Importance\nPitch 0.20\nPitch jump(n+1)0.17\nPitch jump(n−1)0.16\nDuration 0.14\nSame dur. as previous 0.07\nFret jump(n−2)0.07\nString(n+1)0.05\nPitch(n+1)0.05\nTable 3 : Feature importance of the 8 most signiﬁcant fea-\ntures for the decision tree. Standard deviation of any fea-\nture importance is never above 0.005.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n745(a) Excerpt from Highway Star , Deep Purple.\n(b) Excerpt from Jailbreak , AC/DC.\nFigure 7 : Examples of predictions obtained with our Full Tree model on two different excerpts. Labels shown represent\nthe predicted label for the current note. Only wrong predictions are shown for clarity. All other notes are labeled correctly.\ncaused those wrong predictions. Both notes actually have\nmore than half their decision path in common and split\non their Pitch jump(n+2)value, suggesting that the ﬁrst\ndiscrepancy was due to future context. In particular, the\nsecond false prediction did not use any features related to\npast context. This might also explain this error because the\npitch could be obtained by bending on the 10thfret by one\nsemitone – an information that could be derived from fu-\nture context – but continuity with the previous notes called\nfor playing the note without bend on the 11thfret – an in-\nformation that should have been derived from past context.\nAnother observation is that, in spite of similar context, the\nsecond bent note was misidentiﬁed whereas the fourth bent\nnote was not. While those two notes look very similar at\nﬁrst glance, the latter has a longer duration because it is\ntied to the following eighth note, which illustrates the im-\nportance of the duration featureAn analysis of the decision\npaths indeed shows a divergence from the second decision\nrule, based on that feature. This highlights the presumably\nstrong inﬂuence of rhythm in the classiﬁcation of the ﬁrst\nfour bent notes, which bypasses pitch features.\nFigure 7b also shows a regular note wrongly tagged\nwith a↑label. The decision path for this prediction does\nnot consider any feature related to the next note. It does\nhowever use many features concerning the second next\nnote, which was correctly classiﬁed as ∅, most likely be-\ncause of its lower duration. The lack of information about\nthe current note’s position was probably critical in that\ncase. The second error on that tablature is an up & down\nbend that was not identiﬁed, probably because of the low\nduration of the involved notes. Nevertheless, this example\nsuggests that our method to obtain a bend-less transcrip-\ntion from an up & down bend might be detrimental to the\nalgorithm performance. Indeed, our procedure has an im-\npact on duration andpitch jump(n±1)which are among the\nmost useful features to our algorithm. We observe however\nthat our algorithm predicted correctly six bend labels in theselected examples with a limited amount of false positives.\nThese encouraging results suggest that our method could\nbe used as a suggestion tool for the idiomatic use of bends.\n7. CONCLUSION\nIn this paper, we proposed a model of guitar bends and\ndiscussed how these expressive playing techniques relate\nto both tablature and score content. Introducing a set of\nhigh-level features, we showed that a decision tree can suc-\ncessfully predict bend occurrences with satisfactory preci-\nsion, in spite of the difﬁculty of the task due to the low\nproportion of bent notes in guitar music. In particular, the\nlow performance on predicting →labels suggests that our\nmodeling choices could be improved and that held bends\nmight not be considered as an expressiveness technique\nbut rather another way of playing regular notes. An ad-\nvantage of our approach is the use of a lightweight and\nexplainable algorithm, facilitating its use in an assisted-\ncomposition context. In future work, this approach could\nbe extended to other guitar playing techniques, and might\nbeneﬁt from adding more context information like the\nchord being played over a bar, using rhythm guitar parts\naligned with lead guitar. Because bends are arguably more\neasily performed with the ring ﬁnger and little ﬁnger than\nother ﬁngers of the fretting-hand, combining our work with\nﬁnger prediction technique [16] might also improve pre-\ndiction performance. Finally, our modeling strategy could\nalso be used to study the playing style of speciﬁc guitarists,\nand evaluate the potential of bends for automatic guitarist\nidentiﬁcation. Indeed, our approach supposes that bends\ncan be explained by general musical features regardless\nof the artist. This is debatable since famous players can\nbe identiﬁed by their solos (without considering audio nor\nany playing technique) [26]. It would be interesting to see\nif bends are artist-dependent and, if so, to develop a model\nthat predicts bends in the style of a speciﬁc guitarist.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n746Acknowledgements. This work is made with the sup-\nport of the French National Research Agency, in the frame-\nwork of the project TABASCO (ANR-22-CE38-0001).\nThe authors would like to thank Arobas Music for provid-\ning the dataset and colleagues from the Algomus team for\ntheir thorough proofreading and insightful comments.\n8. REFERENCES\n[1] D. R. Grimes, “String Theory - The Physics of String-\nBending and Other Electric Guitar Techniques,” Public\nLibrary of Science One , vol. 9, no. 7, Jul. 2014.\n[2] P. J. Gomez, “Modern Guitar Techniques; a view of\nHistory, Convergence of Musical Traditions and Con-\ntemporary Works (A guide for composers and gui-\ntarists),” Ph.D. dissertation, UC San Diego, 2016.\n[3] Q. Xi, R. M. Bittner, J. Pauwels, X. Ye, and J. P.\nBello, “Guitarset: A Dataset for Guitar Transcription,”\ninProc. of the 19th International Society for Music In-\nformation Retrieval Conference , Paris, France, 2018.\n[4] A. Wiggins and Y . Kim, “Guitar Tablature Estimation\nwith a Convolutional Neural Network,” in Proc. of the\n20th International Society for Music Information Re-\ntrieval Conference , Delft, The Netherlands, 2019.\n[5] L. Reboursière, S. Dupont, O. Lähdeoja, C. Picard-\nLimpens, T. Drugman, and N. Riche, “Left and\nright-hand guitar playing techniques detection,” NIME ,\n2012.\n[6] S.-H. Chen, Y .-S. Lee, M.-C. Hsieh, and J.-C. Wang,\n“Playing Technique Classiﬁcation Based on Deep Col-\nlaborative Learning of Variational Auto-Encoder and\nGaussian Process,” in 2018 IEEE International Con-\nference on Multimedia and Expo (ICME) , Jul. 2018,\npp. 1–6.\n[7] Yu-Fen Huang, Jeng-I Liang, I-Chieh Wei, and Li Su,\n“Joint analysis of mode and playing technique in Guqin\nperformance with machine learning,” in Proc. of the\n21st International Society for Music Information Re-\ntrieval Conference , Montréal, Canada, 2020.\n[8] P. Sarmento, A. Kumar, C. J. Carr, Z. Zukowski,\nM. Barthet, and Y .-H. Yang, “DadaGP: A Dataset of\nTokenized GuitarPro Songs for Sequence Models,” in\nProc. of the 22nd Int. Society for Music Information\nRetrieval Conf. , Online, 2021.\n[9] P. Sarmento, A. Kumar, Y .-H. Chen, C. Carr,\nZ. Zukowski, and M. Barthet, “GTR-CTRL: Instru-\nment and Genre Conditioning for Guitar-Focused Mu-\nsic Generation with Transformers,” in Artiﬁcial Intel-\nligence in Music, Sound, Art and Design , ser. Lecture\nNotes in Computer Science, C. Johnson, N. Rodríguez-\nFernández, and S. M. Rebelo, Eds. Cham: Springer\nNature Switzerland, 2023, pp. 260–275.[10] Y .-H. Chen, Y .-H. Huang, W.-Y . Hsiao, and Y .-H.\nYang, “Automatic Composition of Guitar Tabs by\nTransformers and Groove Modeling,” in Proc. of the\n21st International Society for Music Information Re-\ntrieval Conference , Montréal, Canada, 2020.\n[11] M. McVicar, S. Fukayama, and M. Goto, “AutoLead-\nGuitar: Automatic generation of guitar solo phrases in\nthe tablature space,” in 2014 12th International Con-\nference on Signal Processing (ICSP) . Hangzhou, Zhe-\njiang, China: IEEE, Oct. 2014, pp. 599–604.\n[12] J. Ching and Y .-H. Yang, “Learning To Generate Piano\nMusic With Sustain Pedals,” in Extended Abstracts for\nthe Late-Breaking Demo Session of the 22nd Int. Soci-\nety for Music Information Retrieval Conf. , 2021.\n[13] S. I. Sayegh, “Fingering for String Instruments with the\nOptimum Path Paradigm,” Computer Music Journal ,\nvol. 13, no. 3, pp. 76–84, 1989.\n[14] G. Hori and S. Sagayama, “Minimax Viterbi algorithm\nfor HMM-based Guitar ﬁngering decision,” in Proc. of\nthe 17th International Society for Music Information\nRetrieval Conference , 2016.\n[15] V . K. M. Cheung, H.-K. Kao, and L. Su, “Semi-\nsupervised violin ﬁngering generation using variational\nautoencoders,” in Proc. of the 22nd International Soci-\nety for Music Information Retrieval Conference , On-\nline, 2021.\n[16] G. Hori, “Three-Level Model for Fingering Decision of\nString Instruments,” in Proc. of the 15th International\nSymposium on CMMR , Online, 2021.\n[17] Y . Xie and R. Li, “Symbolic Music Playing Techniques\nGeneration as a Tagging Problem,” Oct. 2020, preprint.\n[18] F. Zappa and S. Vai, The Frank Zappa Guitar Book .\nHal Leonard Corporation, 2017.\n[19] M. S. Cuthbert and C. Ariza, “music21: A Toolkit\nfor Computer-Aided Musicology and Symbolic Mu-\nsic Data,” in Proc. of the 11th International Society for\nMusic Information Retrieval Conference , 2010.\n[20] D. Temperley, The Musical Language of Rock . Oxford\nUniversity Press, Jan. 2018.\n[21] D. Régnier, N. Martin, and L. Bigo, “Identiﬁcation of\nrhythm guitar sections in symbolic tablatures,” in Proc.\nof the 22nd International Society for Music Informa-\ntion Retrieval Conference , Online, 2021.\n[22] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and Duches-\nnay, “Scikit-learn: Machine Learning in Python,” Jour-\nnal of Machine Learning Research , vol. 12, no. 85, pp.\n2825–2830, 2011.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n747[23] L. Breiman, Classiﬁcation and regression trees . Rout-\nledge, 2017.\n[24] N. V . Chawla, K. W. Bowyer, L. O. Hall, and\nW. P. Kegelmeyer, “SMOTE: Synthetic Minority Over-\nsampling Technique,” Journal of Artiﬁcial Intelligence\nResearch , vol. 16, pp. 321–357, Jun. 2002.\n[25] L. Breiman, “Random Forests,” Machine Learning ,\nvol. 45, no. 1, pp. 5–32, Oct. 2001.\n[26] O. Das, B. Kaneshiro, and T. Collins, “Analyzing and\nclassifying guitarists from rock guitar solo tablature,”\ninProceedings of the Sound and Music Computing\nConference , Limassol, Chypre, 2018.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n748"
    },
    {
        "title": "SingStyle111: A Multilingual Singing Dataset With Style Transfer.",
        "author": [
            "Shuqi Dai",
            "Yuxuan Wu",
            "Siqi Chen",
            "Roy Huang",
            "Roger B. Dannenberg"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265401",
        "url": "https://doi.org/10.5281/zenodo.10265401",
        "ee": "https://zenodo.org/records/10265401/files/000091.pdf",
        "abstract": "There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.",
        "zenodo_id": 10265401,
        "dblp_key": "conf/ismir/DaiWCHD23",
        "keywords": [
            "publicly accessible",
            "diversity of languages",
            "performance styles",
            "large studio-quality",
            "singing dataset",
            "multiple languages",
            "different singing styles",
            "80 songs",
            "professional singers",
            "12.8 hours"
        ],
        "content": "SINGSTYLE111: A MULTILINGUAL SINGING DATASET\nWITH STYLE TRANSFER\nShuqi Dai1Yuxuan Wu1Siqi Chen2Roy Huang1Roger B. Dannenberg1\n1Computer Science Department, Carnegie Mellon University , USA\n2University of Southern California, USA\nshuqid@cs.cmu.edu, rbd@cs.cmu.edu\nABSTRACT\nThere has been a persistent lack of publicly accessible\ndata in singing voice research, particularly concerning\nthe diversity of languages and performance styles. In\nthis paper, we introduce SingStyle111, a large studio-\nquality singing dataset with multiple languages and differ-\nent singing styles, and present singing style transfer exam-\nples. The dataset features 111 songs performed by eight\nprofessional singers, spanning 12.8 hours and covering En-\nglish, Chinese, and Italian. SingStyle111 incorporates dif-\nferent singing styles, such as bel canto opera, Chinese folk\nsinging, pop, jazz, and children. Speciﬁcally, 80 songs\ninclude at least two distinct singing styles performed by\nthe same singer. All recordings were conducted in profes-\nsional studios, yielding clean, dry vocal tracks in mono for-\nmat with a 44.1 kHz sample rate. We have segmented the\nsinging voices into phrases, providing lyrics, performance\nMIDI, and scores with phoneme-level alignment. We also\nextracted acoustic features such as Mel-Spectrogram, F0\ncontour, and loudness curves. This dataset applies to vari-\nous MIR tasks such as Singing V oice Synthesis, Singing\nV oice Conversion, Singing Transcription, Score Follow-\ning, and Lyrics Detection. It is also designed for Singing\nStyle Transfer, including both performance and voice tim-\nbre style. We make the dataset freely available for research\npurposes. Examples and download information can be\nfound athttps://shuqid.net/singstyle111 .\n1. INTRODUCTION\nIn recent years, deep learning technologies have signiﬁ-\ncantly advanced the ﬁeld of Artiﬁcial Intelligence Genera-\ntive Content (AIGC) [1], leading to breakthroughs in Com-\nputer Vision for image synthesis and manipulation [2–5],\nNatural Language Processing (NLP) for text generation\nand summarization [6–8], and audio signal processing for\nText-to-Speech (TTS) generation [9–11]. In particular,\nadvanced generative models such as Variational Autoen-\n© S. Dai, Y . Wu, S. Chen, R. Huang and R. B. Dannen-\nberg. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: S. Dai, Y . Wu, S. Chen, R. Huang\nand R. B. Dannenberg, “SingStyle111: A Multilingual Singing Dataset\nWith Style Transfer”, in Proc. of the 24th Int. Society for Music Informa-\ntion Retrieval Conf., Milan, Italy, 2023.coders (V AEs) [12–14], Generative Adversarial Networks\n(GANs) [15, 16], Transformer-based models [17, 18], and\nDiffusion Models [19, 20] resulted in a series of excep-\ntional TTS models that achieve not only realistic results\n[9–11, 21] but also explore stylistic and emotional speech\nsynthesis [22, 23] in a more controllable way. However,\nthe development of singing tasks such as Singing V oice\nSynthesis (SVS) [24–28] and Singing V oice Conversion\n(SVC) [29] have yet to progress as fast as TTS. One pri-\nmary reason is the lack of data on several key aspects:\n• Lack of high-quality data. Tasks such as SVS and SVC\nrequire monophonic, clean, and dry sound singing data\nwith studio quality. Unfortunately, due to the limi-\ntations of Source Separation and Denoising technolo-\ngies [30–33], as well as copyright issues, most available\ncover songs online cannot meet these quality require-\nments. Datasets recorded with studio quality are pre-\ndominantly composed of amateur performances, which\noften exhibit off-key and cracking issues that could mis-\nlead the generative models and diminish their quality.\n• Lack of diversity. Most available singing datasets cover\nonly one language, resulting in a severely imbalanced\nlanguage distribution. For example, there is a fair\namount of Chinese singing data, while clean English\ndata is very scarce. In addition, most datasets only focus\non one pop singing style, and the distributions of differ-\nent singing styles and vocal ranges are too narrow.\n• Lack of annotations. Many datasets lack proper phrase-\nlevel segmentation, lyrics, and scores, and are not\naligned at the phoneme level, making it impossible\nto conduct score-based SVS and more detailed perfor-\nmance control.\n• Lack of large-scale data. The current data volume of\nhigh-quality singing is still insufﬁcient for deep genera-\ntive models.\nFurthermore, current SVS results are primarily conﬁned\nto modeling the timbre of singing voices. While there\nare several good vocoders [11, 21, 34] and acoustic mod-\nels [10, 35] for SVS based on Ground-Truth control sig-\nnals (e.g., inputting F0 control signals to the model), the\ntruly creative and artistic aspects of singing, such as ex-\npressive performance control, singing styles, vocal tech-\nniques, and creative improvisation, have yet to be explored.\nAgain, data limitations play a signiﬁcant role in this, as\nmost datasets consist of amateur performances or have not765Dataset Language Style #Hour #Singer Quality Musicality ScoreAlign-\nmentStyle\nTransfer\nOpencpop [41] Chinese Pop 5.25 1 Studio Ama.Perform.\nMIDI✓ ✗\nM4Singer [42] Chinese Pop 29.77 20 Studio50% Ama.\n50% Prof.Perform.\nMIDI✓ ✗\nChildren\nSong [43]Korean\nEnglishChildren 4.86 1 StudioProf.\nbut plainPerform.\nMIDIword ✗\nTohoku\nKiritan [44]Japanese Pop 0.95 1 Studio Prof. Score ✓ ✗\nPopCS [28] Chinese Pop 5.89 6Not\nCleanAma. ✗ ✗ ✗\nOpen-\nSinger [35]Chinese Pop 50 66 Studio Ama. ✗ ✗ ✗\nV ocalSet [45]\nAnnotated [46]Five\nV owelsOpera 10.1 20 Studio Prof. Score ✓technique\ntransfer\nNHSS [47] English Pop 3.5 10 Studio Ama. ✗ ✓ ✗\nNUS-48E [48] EnglishPop\nChildren1.41 12 Studio Ama. ✗ ✓ ✗\nRWC [49]Japanese\nEnglishPop 4 27Not\nSoloProf. Both ✗ ✗\nTONAS [50] Spanish Flamenco 0.34 > 40Not\nCleanProf. ✗ ✗ ✗\nV ocadito [51]Seven\nLanguagesPop\nChildren0.23 29Not\nCleanAma. ✗ ✗ ✗\nMIR-1K [52] Chinese Pop 2.22 19Not\nSoloAma. ✗ ✗ ✗\nStyleSing111\n(Ours)English\nChinese\nItalianOpera\nPop\nFolk\nJazz etc.12.8 8 Studio Prof. Both ✓ ✓\nTable 1 . A comparison of existing singing datasets. Score means if there is score or performance MIDI ﬁle provided.\n“Perform. MIDI” stands for “Performance MIDI”. “Both” means both performance MIDI ﬁles synchronized with the\nsinging audio and sheet music scores are provided. Alignment means whether or not there is duration annotation at the\nphoneme level for lyrics. “Ama.” stands for “Amateur,” and “Prof.” stands for “Professional.”\nyet begun to address the issue of artistic expression.\nFor example, Style Transfer [36, 37] is a popular tech-\nnique in deep learning that combines the content of one im-\nage or sound with the style of another. For audio process-\ning, some researchers [38,39] have recently transferred the\ntimbre from one audio source to another while preserving\nthe speech content (similar to SVC). However, the transfer\nof expressive performance styles embedded below the tim-\nbre level remains elusive, mainly because (1) disentangling\nperformance style is much more challenging than timbre\nfeatures [40] and (2) the scarcity of relevant datasets pro-\nviding examples of performance styles.\nTo help address these issues, we introduce a new\nsinging corpus, SingStyle111. We summarize the main\ncontributions as follows:\n(1) SingStyle111 is a large and high-quality singing\ndataset. It contains 111 songs performed by eight pro-\nfessional singers, spanning 12.8 hours of clean mono-\nphonic vocal recordings in studio quality.\n(2) It is a diverse dataset with creative singing. It covers En-\nglish, Chinese, and Italian songs and incorporates var-ious singing styles, such as bel canto opera, Chinese\nfolk, pop, jazz, and children. Some performances are\ncreative improvisations based on the original score.\n(3) It demonstrates style transfer in both performance and\ntimbre levels. 80 songs contain at least two distinct\nsinging styles performed by the same singer.\n(4) It includes proper annotations and extracted features.\nWe manually segmented voices into phrases, labeled\nPerformance MIDI ﬁles and music score notes and\naligned them with the phonemes of lyrics, extracted\nacoustic features such as Mel-Spectrogram, F0 contour,\nand loudness curves.\n(5) It applies to different MIR tasks such as SVS, SVC,\nSinging Transcription, Score Following, Expressive\nPerformance, Lyrics Detection, Singing Style Transfer.\n(6) It is publicly available for research purposes for free.\nThe rest of this paper is organized as follows: after a\nbrief review of related works, we describe how we collect\nand process the dataset (Section 3) and show the annota-\ntions and analysis (Section 4). Finally, we discuss potential\napplications in Section 5 followed by conclusions.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n7662. RELATED WORK\nExisting singing voice datasets still have many limitations\nin fulﬁlling the requirements for singing research tasks\nsuch as Singing V oice Synthesis (SVS) [24, 26–28] and\nSinging V oice Conversion (SVC) [29]. Table 1 provides\nan overview of the available public datasets. Datasets such\nas MIR-1K [52], TONAS [50], and V ocadito [51] are re-\nstricted by the absence of separated solo vocal tracks or\nsuffer from subpar recording environments with noise, re-\nverberation, and other interferences. These issues hinder\ntheir usability in SVS-related tasks. While NHSS [47] and\nOpenSinger [35] contain clean and dry human vocals, they\nlack essential musical scores or phoneme-level duration\nalignment. Consequently, these datasets are unsuitable for\ntraining end-to-end synthesis models that convert scores\nto vocals. Moreover, datasets such as Opencpop [41] and\nM4singer [42] offer good annotations and recording qual-\nity but primarily focus on Mandarin songs and a limited\nrange of pop styles. Additionally, the singing proﬁciency\nof performers is inconsistent, with many being amateurs,\nwhich affects the overall quality of the dataset.\nAnother issue that has long been overlooked and misun-\nderstood in singing voice datasets is the difference between\nPerformance MIDI and the actual sheet music score. In\nTable 1, only Tohoku Kiritan [44], V ocalset [45, 46] and\nRWC [49] have music scores, while other datasets claimed\nto have scores that are indeed performance MIDI ﬁles. Per-\nformance MIDI features expressive performance timings\nrather than score timings with regular note durations in\nbeats. The melodic pitches in performance MIDI can also\ndiffer from those in score melody. Utilizing performance\nMIDI for singing voice synthesis and claiming it as score-\nbased is, in reality, a deceptive approach that takes advan-\ntage of real singing data.\nAs for the Style Transfer task, V ocalset [45, 46] pro-\nvides relevant examples, but its scope is limited to singing\ntechnique transfer within the bel canto singing style. Fur-\nthermore, the dataset predominantly consists of scale exer-\ncises using only ﬁve vowels and includes only three short\nsongs, which restricts its applicability. Given the limi-\ntations of existing datasets, there is a need for a large-\nscale, high-quality, professional, multilingual, and diverse\nsinging dataset that caters to various styles and includes\nstyle transfer examples. In this paper, we introduce a novel\ndataset designed to address these requirements and facili-\ntate research in SVS-related tasks and style transfer.\n3. DATASET DESCRIPTION\n3.1 Overview\nSingStyle111 is a multilingual singing dataset with style\ntransfer demonstrations. Figure 1 illustrates the data col-\nlection pipeline. Following the completion of the recording\nprocess, we post-process all recordings and retain all high-\nquality segments. Thus, our dataset offers two versions:\nthe ﬁrst version consists of edited full-length songs, and\nthe second version comprises all usable, high-quality vocal\nsegments, incorporating redos from the recording process.\nFigure 1 . Data collection pipeline.\nFigure 2 . Distribution of songs according to languages and\nthe number of style demonstrations. For example, English\nsongs have 18 songs with only one style version, 25 songs\nwith two different styles, six songs with three styles, and\nthree songs with four styles.\nWe preserve these redos for two primary reasons. First,\nduring recording, singers often need to restart due to minor\nerrors, resulting in many redos that far exceed the quantity\nrequired for a single song. The high-quality vocals in these\nredo segments are perfect for segmented training in deep\nlearning and effectively augmenting the dataset. Second,\neven when the same singer performs the same song using\nthe same style, each rendition exhibits subtle differences.\nCapturing these variations provides valuable training data\nfor learning multi-modes in singing performance and dis-\nentangling a singer’s style with music content. This paper\nfocuses on describing the second version of the dataset.\nUpon obtaining the clean and dry vocal segments in au-\ndio, we manually annotate them into phrases (music sen-\ntences), provide lyrics and score alignment with audio at\nthe phoneme level. We then extract acoustic attributes such\nas F0 contour, loudness curve, and Mel-spectrogram. Fi-\nnally, we partition and package the data, incorporating rel-\nevant attributes. Section 4 describes this process in detail.\nIn the following subsections, we delve into the dataset’s\nrepertoire and styles, singer proﬁles, recording environ-\nments, and post-production methods, accompanied by per-\ntinent statistics.\n3.2 Repertoire and Style\nSingStyle111 comprises 111 songs, of which 80 have at\nleast two different versions performed in distinct styles by\nthe same singer, resulting in a total of 224 song versions.\nThe dataset encompasses three languages: English (372\nminutes), Chinese Mandarin (307 minutes), and Italian (88\nminutes). Figure 2 illustrates the number of song versions\nfor each language. During song selection, we sought to di-\nversely represent various styles, singing techniques, tem-\npos, and eras.\nFigure 3 presents the styles of the original songs and allProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n767Figure 3 . Distribution of song styles. Chart(a) describes\nthe original style of the 111 songs, while chart(b) indicates\nthe 224 different style interpretations in the dataset.\nstyle demonstrations. We consolidated several sub-genres\ninto seven broader styles to streamline the pie chart. For\ninstance, Country, Western folk, Chinese pop, and other\npop styles were combined into a single pop genre. Like-\nwise, the Rock category contains Soft Rock, Hard Rock,\nAlternative Rock, etc.\nThroughout the data collection process, we instructed\nsingers to exhibit signiﬁcant differences in style transfer.\nSometimes they made appropriate adaptations or improvi-\nsations to the original song for better style transfer while\npreserving the original lyrics, melody, and structure. For\nexample, it is easier for singers to transfer vocal timbres\nwhen the key changes. Also, tempo changes and rhythmic\nvariations can dramatically help alter styles, such as trans-\nferring a fast and happy song into a slow and melancholic\none. Converting singing techniques or adding ornamen-\ntations are also prevalent in our style transfer examples.\nFor instance, the dataset includes many demonstrations in-\nterchanged among pop, bel canto, and Chinese traditional\nfolk singing; or singing the same song in the distinct pop\nstyles of Adele Adkins and Teresa Teng. In addition, some\nstyles include deliberate emotional changes, for example,\ncontrasting a \"plain and lyrical style\" with an \"exaggerated\nand highly emotional style.\"\n3.3 Singers\nWe paid eight professional singers (Table 2) to sing the\nsongs. They have diverse vocal ranges, singing styles, and\nvocal techniques. They are aged 20 to 63, and all have re-\nceived formal musical training for more than six years. Six\nof them are graduates or current students in the voice ma-\njor at music conservatories. “Male1” is a native American\nEnglish speaker, and all the others are Chinese. “Female1”\nhas lived in the US for more than ﬁve years and received\nformal English singing training at a music academy. We\nalso removed the English song phrases that have strong for-\neign accents. All singers have signed agreements to release\nthe dataset for research purposes.\n3.4 Recording\nWe recorded the songs in a professional recording studio\nwith little reverberation or noise. We use a Shure ModelSinger Language Style #Hour Range\nFemale1 en, cnP. C. O. R.\nF. M. J.3.73 F#3-A5\nFemale2 it, en, cn O. F. M. 1.24 E4-C6\nFemale3 cn, enP. C. O. R.\nF. M. J.1.58 F#3-F5\nFemale4 cn, en P. 1.63 D3-C5\nMale1 en P. R. M. 0.59 D2-G4\nMale2 cn, en P. M. J. 1.35 A2-C5\nMale3 it, cn O. M. 1.16 C4-G5\nMale4 cn P. O. F. 1.51 D#3-A4\nTable 2 . Singer Information. Here the vocal range is the\nused range in the dataset. en: English, cn: Chinese, it:\nItalian, P: Pop, C: Children, R: Rock, O: Opera, F: Chinese\nTraditional Folk, M: Musical, J: Jazz.\nSM81-LC microphone, an Apollo X8 Thunderbolt 3 audio\ninterface, Heritage Audio 73jr as the pre-ampliﬁer, and Pro\nTools Studio as DAW software. All singings are pure vocal\nonly and recorded at 44,100 Hz sampling rate with 24 bits\nper sample in wav format.\nIn most recording sessions, singers wear headphones\nto listen to the accompaniment. However, in some style\ntransfer demonstrations, accompaniments and headphones\nmay not always be used. Despite this, singers must ensure\nthey maintain the correct key and consistently stay within\nit throughout the performance.\n3.5 Production\nWe employed several essential post-production techniques\nto reﬁne and clean the recorded data. First, we edited\nthe raw recordings to retain only high-quality clips, ﬁlter-\ning out noisy sections, mistakes, and mispronunciations.\nA small portion of singer Male3’s singing clips were fur-\nther edited with pitch-tuning. To achieve a consistent vol-\nume balance, we applied different gain levels in each clip.\nMoreover, we incorporated a compressor for all recording\nclips to prevent extreme dynamic ﬂuctuations. Lastly, we\nmaximized the output volume using a limiter, setting the\noutput ceiling at -0.6 dB. After production, we obtained\nclean and dry vocal tracks with similar output volumes.\n4. ANNOTATION AND ANALYSIS\nThis section presents the annotation process, including\nboth manual annotation and automatic analysis. We ﬁrst\nsegment the audio clips into music phrases, for which\nwe then manually identify corresponding lyrics and music\nscores. By combining automatic algorithms and manual\nefforts, we align lyrics phonemes and score notes to their\ncorresponding audio. Next, we utilize algorithms to extract\nacoustic attributes such as F0 contour, loudness curve, and\nMel-spectrogram. Finally, we highlight the key attributes\nand explain dataset partitioning and packaging.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n768Figure 4 . An example of phoneme-level annotation using Audacity. The lyrics word, IPA phoneme, and pitch label tracks\nare aligned with the corresponding audio. “AP” here stands for “aspirate”.\n4.1 Phrase-level Segmentation\nWe further divide the audio segments into smaller musical\nphrases for two reasons. First, this additional segmentation\naccelerates model training. Second, from a music perspec-\ntive, phrases serve as one of the basic music structure units,\nwith emotional expressions and performance controls be-\ning highly related to phrase-level structure. Inappropriate\nsegmentation might compromise musical expression due\nto insufﬁcient phrase structure information. Given the low\naccuracy of automatic algorithms for phrase segmentation,\nwe manually label them. The result shows that most seg-\nmented phrases have lengths between 3 and 12 seconds\nwith one to three breaths. We obtain 6588 phrases in to-\ntal. No silence is included at the beginning or end of the\nphrases, except for breath events.\n4.2 Lyrics and Score Alignment at Phoneme Level\nIn this subsection, we describe the lyrics and score annota-\ntion process.\nLyrics annotation We ﬁrst manually ﬁnd lyrics for\neach song online and then segment and align the lyrics\nwith each phrase. We manually correct the lyrics to match\nthe actual singing in the data. Secondly, we employ al-\ngorithms to translate the lyrics into phonemes. For En-\nglish1and Italian2, we utilize tools to translate them into\nInternational Phonetic Alphabet (IPA) phonemes [53]. For\nMandarin, we use Pinyin3for phoneme-level alignment\nand provide a mapping of Pinyin to IPA phonemes for later\nphoneme-set processing for model training. We did not di-\nrectly convert Chinese to IPA due to annotation complex-\nity. Thirdly, we obtain an approximate phoneme alignment\nwith audio using the Montreal Forced Aligner [54] and out-\nput it into TextGrid ﬁles. Finally, we (1) use Praat soft-\nware [55], or (2) convert the TextGrid into txt ﬁles and in-\nput them to Audacity [56] for further manual adjustment of\nphoneme text and boundaries, as well as breath and silence\nevent annotation (Figure 4).\nPerformance MIDI and Score annotation We anno-\ntated the performance MIDI ﬁle and music score for each\n1https://github.com/mphilli/English-to-IPA\n2https://espeak.sourceforge.net/\n3https://github.com/mozillazg/python-pinyinsinging phrase in the dataset as follows:\n(1) We manually input performance MIDI ﬁles that strictly\nalign to singing audio using MIDI piano, including mul-\ntiple rounds of correction.\n(2) We automatically align MIDI notes with phonemes\nbased on their corresponding time stamps in the audio.\n(3) We search online for music score MIDI ﬁles; if no reli-\nable sources are found, we quantize and derive the score\nfrom annotated performance MIDI ﬁle.\n(4) For online score ﬁles, we develop an algorithm that\nautomatically matches each singing phrase’s perfor-\nmance MIDI data to the corresponding phrase in the\nscore MIDI ﬁle. Manual matching is required for non-\noriginal-style style transfer versions.\n(5) We use the Dynamic Time Warping algorithm to match\nthe performance MIDI data with the score MIDI ﬁle\nwithin each phrase. We manually verify the mapping\nresults for non-original-style style transfer versions.\nAll these above steps allow us to annotate the lyrics,\nperformance MIDI, and music score at the phoneme level\nfor our singing voice dataset, ensuring accurate and com-\nprehensive representations of the musical content.\n4.3 Acoustic Feature Extraction\nF0, or fundamental frequency, is the lowest frequency of a\nperiodic waveform. F0 contour is critical in singing syn-\nthesis as it determines the pitch variations of singing per-\nformance and largely inﬂuences singing quality. It can cap-\nture pitch modulations in various singing techniques, such\nas vibrato, ornaments, and glissando. Many current SVS\nsystems still require the input of ground-truth F0 as a con-\ndition to guide the synthesis process. To ensure accurate F0\nextraction, we employ a combination of two widely-used\nmodels, pYIN [57] and PENN [58]. First, we use pYIN\nalgorithm to identify unvoiced parts, including breaths, si-\nlence, and consonants. Then, the PENN algorithm is ap-\nplied to extract F0 for the voiced parts.\nLoudness represents the energy of a sound. It is crucial\nin singing performance since it largely reﬂects the dynamic\nand emotional changes that contribute to the expressive-\nness of the singing voice. To extract loudness, we ﬁrst cal-\nculate the root-mean-square (RMS) amplitude values from\naudio and then convert them to decibels. We further ap-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n769ply a moving average window of frame size 30 to obtain a\nsmoother loudness curve.\nFinally, we use the Short-Time Fourier Transform\n(STFT) with a window size of 1024, FFT size of 1024,\nand hop size of 256 to extract the mel-spectrogram, which\nshares the same settings with loudness extraction.\n5. POTENTIAL APPLICATIONS\nThis dataset is intended to promote research into a number\nof different MIR tasks. We consider a variety of interesting\nrelevant problems in this section.\n5.1 Singing Style Transfer\nStyle transfer has to do with music interpretation. Here,\n“style” refers to performance details that are not con-\nstrained by symbolic representations such as traditional no-\ntation. If notation gives a song its “identity,” styles are\nperformance characteristics that are shared across perfor-\nmances of different songs. Styles are often associated with\ngenre, e.g., a song can be interpreted in rock, pop, or jazz\nstyles. Styles can be more or less speciﬁc than genre, e.g.,\nthe style of Louis Armstrong (more speciﬁc) or symphonic\n(less speciﬁc). Style transfer is a process of identifying the\nstyle of one or more performances and applying it to a new\nsong to create a stylistic performance. SingStyle111 con-\ntains many performances where a single singer performs\nin multiple styles, offering the potential to abstract styles\nfrom other information (singer identity, melodies) which is\nheld constant. In the multi-style recordings, singers were\nasked to exaggerate differences, which should help to learn\nfeatures that characterize different styles.\n5.2 Singing Voice Synthesis\nA large motivation for SingStyle111 is the difﬁculty of\nﬁnding high-quality musical examples of singing. In par-\nticular, the presence of accompaniment and reverberation\ncomplicate the process of learning to create the sound of\nsinging voices. Furthermore, lower recording and singing\nquality are a barrier to learning high-ﬁdelity sounds of pro-\nfessional singing. In addition, SingStyle111 also provides\nnecessary phoneme-level annotations for score-based SVS.\n5.3 Singing Voice Conversion\nIn SVC, we hope to substitute the sound of one voice with\nthe sound of another while maintaining the same melody\nand style. To promote progress in this area, SingStyle111\nhas performances of the same song by multiple singers,\nincluding male and female voices. Since we have perfor-\nmances of the same song in the same style, SVC can be\ncast as a sequence-to-sequence problem analogous to many\nother machine learning tasks such as language translation.\n5.4 Expressive Performance\nExpressive performance is the general problem of creating\na musical performance given a symbolic description such\nas a melody in common music notation. Notation omitsmany details, including loudness, vibrato, pitch variations,\nchanges in vocal timbre, the details of pronouncing lyrics\nwithin pitch and rhythmic constraints, and breathiness. Of-\nten, connections and transitions from one note to the next\nare as important as how notes are performed. To learn ex-\npressive performance, it helps to have symbolic notation,\nwhich can be considered as input constraints, context, or\nconditioning. In addition, it helps if the notated events\nare aligned with corresponding time points in the audio.\nSingStyle111 includes symbolic representations (perfor-\nmance MIDI ﬁles and music scores) aligned with audio.\nThe data is especially designed to support machine learn-\ning using sequence-to-sequence models from notation to\ncontrol signals such as pitch contours, loudness, spectro-\ngrams, or directly to audio.\n5.5 Automatic Singing Transcription\nSinging transcription can be regarded as the inverse of ex-\npressive performance control: Rather than converting no-\ntation to sound, we wish to convert sound into music no-\ntation. With transcriptions for all of the singing examples,\nSingStyle111 provides a wealth of transcription examples\nfor training and evaluating transcription models.\n5.6 Score Alignment and Following\nScore following [59] is the problem of aligning an au-\ndio performance to symbolic notation. V ocal score fol-\nlowing is particularly difﬁcult because, unlike most other\ninstruments, voices do not have keys, valves, or frets, so\nsinging cannot be easily reduced to a sequence of distinct\ndiscrete states corresponding to musical notes [60]. Real-\ntime score following is the ﬁrst step in the task of com-\nputer accompaniment, in which a computer synchronizes\na pre-composed accompaniment to a live performance by\na soloist. Score following has also been used for auto-\nmatic page turning, delivering synchronized comments via\nmobile phones to symphony orchestra audiences, and as\na data collection method for learning music segmentation\nand other tasks. SingStyle111 contains accurate align-\nments for learning and evaluation of automatic alignment\nand real-time score following.\n5.7 Lyrics Detection\nThe common task of understanding lyrics is one that even\nhumans struggle with. SingStyle111 includes the lyrics\nused by the singers, and lyrics are aligned to the audio\ndown to the phoneme level, facilitating learning and eval-\nuation of various lyrics transcription and alignment tasks.\n6. CONCLUSION\nIn conclusion, we introduce SingStyle111, a large-scale,\nhigh-quality, multilingual singing voice dataset that caters\nto various styles and includes style transfer examples. We\nprovided detailed annotations of lyrics and scores at the\nphoneme level, together with extracted acoustic features.\nWe will make the dataset freely available for research pur-\nposes to facilitate relevant MIR tasks.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n7707. REFERENCES\n[1] Y . Cao, S. Li, Y . Liu, Z. Yan, Y . Dai, P. S. Yu, and\nL. Sun, “A comprehensive survey of ai-generated con-\ntent (aigc): A history of generative ai from gan to chat-\ngpt,” arXiv preprint arXiv:2303.04226 , 2023.\n[2] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark\net al. , “Learning transferable visual models from natu-\nral language supervision,” in International conference\non machine learning . PMLR, 2021, pp. 8748–8763.\n[3] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss,\nA. Radford, M. Chen, and I. Sutskever, “Zero-shot\ntext-to-image generation,” in International Conference\non Machine Learning . PMLR, 2021, pp. 8821–8831.\n[4] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam,\nP. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen,\n“Glide: Towards photorealistic image generation and\nediting with text-guided diffusion models,” in Inter-\nnational Conference on Machine Learning . PMLR,\n2022, pp. 16 784–16 804.\n[5] R. Rombach, A. Blattmann, D. Lorenz, P. Esser,\nand B. Ommer, “High-resolution image synthesis\nwith latent diffusion models,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , 2022, pp. 10 684–10 695.\n[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sas-\ntry, A. Askell et al. , “Language models are few-shot\nlearners,” Advances in neural information processing\nsystems , vol. 33, pp. 1877–1901, 2020.\n[7] OpenAI, “Gpt-4 technical report,” 2023.\n[8] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wain-\nwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray et al. , “Training language models to follow in-\nstructions with human feedback,” Advances in Neural\nInformation Processing Systems , vol. 35, pp. 27 730–\n27 744, 2022.\n[9] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly,\nZ. Yang, Z. Chen, Y . Zhang, Y . Wang, R. Skerrv-Ryan\net al. , “Natural tts synthesis by conditioning wavenet\non mel-spectrogram predictions,” in 2018 IEEE inter-\nnational conference on acoustics, speech and signal\nprocessing (ICASSP) . IEEE, 2018, pp. 4779–4783.\n[10] Y . Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and\nT.-Y . Liu, “Fastspeech 2: Fast and high-quality end-\nto-end text to speech,” in International Conference on\nLearning Representations .\n[11] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catan-\nzaro, “Diffwave: A versatile diffusion model for au-\ndio synthesis,” in International Conference on Learn-\ning Representations .[12] D. P. Kingma and M. Welling, “Auto-Encoding Vari-\national Bayes,” in 2nd International Conference on\nLearning Representations, ICLR 2014, Banff, AB,\nCanada, April 14-16, 2014, Conference Track Pro-\nceedings , 2014.\n[13] D. J. Rezende, S. Mohamed, and D. Wierstra,\n“Stochastic backpropagation and approximate infer-\nence in deep generative models,” in International con-\nference on machine learning . PMLR, 2014, pp. 1278–\n1286.\n[14] A. Van Den Oord, O. Vinyals et al. , “Neural discrete\nrepresentation learning,” Advances in neural informa-\ntion processing systems , vol. 30, 2017.\n[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio,\n“Generative adversarial networks,” Communications of\nthe ACM , vol. 63, no. 11, pp. 139–144, 2020.\n[16] T.-C. Wang, M.-Y . Liu, J.-Y . Zhu, A. Tao, J. Kautz,\nand B. Catanzaro, “High-resolution image synthesis\nand semantic manipulation with conditional gans,” in\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition , 2018, pp. 8798–8807.\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in neural infor-\nmation processing systems , vol. 30, 2017.\n[18] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding,” in Proceedings of NAACL-HLT ,\n2019, pp. 4171–4186.\n[19] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion\nprobabilistic models,” Advances in Neural Information\nProcessing Systems , vol. 33, pp. 6840–6851, 2020.\n[20] J. Song, C. Meng, and S. Ermon, “Denoising diffu-\nsion implicit models,” in International Conference on\nLearning Representations .\n[21] J. Kong, J. Kim, and J. Bae, “Hiﬁ-gan: Generative ad-\nversarial networks for efﬁcient and high ﬁdelity speech\nsynthesis,” Advances in Neural Information Processing\nSystems , vol. 33, pp. 17 022–17 033, 2020.\n[22] K. Akuzawa, Y . Iwasawa, and Y . Matsuo, “Expressive\nspeech synthesis via modeling expressions with varia-\ntional autoencoder,” Proc. Interspeech 2018 , pp. 3067–\n3071, 2018.\n[23] V . Aggarwal, M. Cotescu, N. Prateek, J. Lorenzo-\nTrueba, and R. Barra-Chicote, “Using vaes and nor-\nmalizing ﬂows for one-shot text-to-speech synthesis of\nexpressive speech,” in ICASSP 2020-2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2020, pp. 6179–6183.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n771[24] M. Blaauw and J. Bonada, “A neural parametric\nsinging synthesizer modeling timbre and expression\nfrom natural songs,” Applied Sciences , vol. 7, no. 12,\np. 1313, 2017.\n[25] M. Bi ´nkowski, J. Donahue, S. Dieleman, A. Clark,\nE. Elsen, N. Casagrande, L. C. Cobo, and K. Si-\nmonyan, “High ﬁdelity speech synthesis with adversar-\nial networks,” in International Conference on Learning\nRepresentations .\n[26] P. Lu, J. Wu, J. Luan, X. Tan, and L. Zhou, “Xi-\naoicesing: A high-quality and integrated singing voice\nsynthesis system,” Proc. Interspeech 2020 , pp. 1306–\n1310, 2020.\n[27] Y . Gu, X. Yin, Y . Rao, Y . Wan, B. Tang, Y . Zhang,\nJ. Chen, Y . Wang, and Z. Ma, “Bytesing: A chi-\nnese singing voice synthesis system using duration\nallocated encoder-decoder acoustic models and wav-\nernn vocoders,” in 2021 12th International Symposium\non Chinese Spoken Language Processing (ISCSLP) .\nIEEE, 2021, pp. 1–5.\n[28] J. Liu, C. Li, Y . Ren, F. Chen, and Z. Zhao, “Diffsinger:\nSinging voice synthesis via shallow diffusion mecha-\nnism,” in Proceedings of the AAAI Conference on Ar-\ntiﬁcial Intelligence , vol. 36, no. 10, 2022, pp. 11 020–\n11 028.\n[29] Z. Li, B. Tang, X. Yin, Y . Wan, L. Xu, C. Shen,\nand Z. Ma, “Ppg-based singing voice conversion with\nadversarial representation learning,” in ICASSP 2021-\n2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 2021,\npp. 7073–7077.\n[30] A. Jansson, E. Humphrey, N. Montecchio, R. Bittner,\nA. Kumar, and T. Weyde, “Singing voice separation\nwith deep u-net convolutional networks,” 2017.\n[31] D. Stoller, S. Ewert, and S. Dixon, “Adversarial semi-\nsupervised audio source separation applied to singing\nvoice extraction,” in 2018 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2018, pp. 2391–2395.\n[32] Y .-J. Lu, Z.-Q. Wang, S. Watanabe, A. Richard,\nC. Yu, and Y . Tsao, “Conditional diffusion proba-\nbilistic model for speech enhancement,” in ICASSP\n2022-2022 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) . IEEE,\n2022, pp. 7402–7406.\n[33] J. Su, Z. Jin, and A. Finkelstein, “Hiﬁ-gan: High-\nﬁdelity denoising and dereverberation based on speech\ndeep features in adversarial networks,” Proc. Inter-\nspeech 2020 , pp. 4506–4510, 2020.\n[34] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel wave-\ngan: A fast waveform generation model based on\ngenerative adversarial networks with multi-resolutionspectrogram,” in ICASSP 2020 - 2020 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2020, pp. 6199–6203.\n[35] R. Huang, F. Chen, Y . Ren, J. Liu, C. Cui, and Z. Zhao,\n“Multi-singer: Fast multi-singer singing voice vocoder\nwith a large-scale corpus,” in Proceedings of the 29th\nACM International Conference on Multimedia , 2021,\npp. 3945–3954.\n[36] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired\nimage-to-image translation using cycle-consistent ad-\nversarial networks,” in Proceedings of the IEEE in-\nternational conference on computer vision , 2017, pp.\n2223–2232.\n[37] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style\ntransfer using convolutional neural networks,” in Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 2414–2423.\n[38] K. Qian, Y . Zhang, S. Chang, X. Yang, and\nM. Hasegawa-Johnson, “Autovc: Zero-shot voice style\ntransfer with only autoencoder loss,” in International\nConference on Machine Learning . PMLR, 2019, pp.\n5210–5219.\n[39] K. Qian, Z. Jin, M. Hasegawa-Johnson, and G. J.\nMysore, “F0-consistent many-to-many non-parallel\nvoice conversion via conditional autoencoder,” in\nICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2020, pp. 6284–6288.\n[40] S. Dai, Z. Zhang, and G. G. Xia, “Music style transfer:\nA position paper,” arXiv preprint arXiv:1803.06841 ,\n2018.\n[41] Y . Wang, X. Wang, P. Zhu, J. Wu, H. Li, H. Xue,\nY . Zhang, L. Xie, and M. Bi, “Opencpop: A high-\nquality open source chinese popular song corpus for\nsinging voice synthesis,” Interspeech 2022 , 2022.\n[42] L. Zhang, R. Li, S. Wang, L. Deng, J. Liu, Y . Ren,\nJ. He, R. Huang, J. Zhu, X. Chen et al. , “M4singer:\nA multi-style, multi-singer and musical score provided\nmandarin singing corpus,” Advances in Neural Infor-\nmation Processing Systems , vol. 35, pp. 6914–6926,\n2022.\n[43] S. Choi, W. Kim, S. Park, S. Yong, and J. Nam, “Chil-\ndren’s song dataset for singing voice research,” in In-\nternational Society for Music Information Retrieval\nConference (ISMIR) , 2020.\n[44] I. Ogawa and M. Morise, “Tohoku kiritan singing\ndatabase: A singing database for statistical parametric\nsinging synthesis using japanese pop songs,” Acousti-\ncal Science and Technology , vol. 42, no. 3, pp. 140–\n145, 2021.\n[45] J. Wilkins, P. Seetharaman, A. Wahl, and B. Pardo,\n“V ocalset: A singing voice dataset.” in ISMIR , 2018,\npp. 468–474.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n772[46] B. Faghih and J. Timoney, “Annotated-vocalset: A\nsinging voice dataset,” Applied Sciences , vol. 12,\nno. 18, p. 9257, 2022.\n[47] B. Sharma, X. Gao, K. Vijayan, X. Tian, and H. Li,\n“Nhss: A speech and singing parallel database,”\nSpeech Communication , vol. 133, pp. 9–22, 2021.\n[48] Z. Duan, H. Fang, B. Li, K. C. Sim, and Y . Wang, “The\nnus sung and spoken lyrics corpus: A quantitative com-\nparison of singing and speech,” in 2013 Asia-Paciﬁc\nSignal and Information Processing Association Annual\nSummit and Conference . IEEE, 2013, pp. 1–9.\n[49] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka,\n“Rwc music database: Popular, classical and jazz mu-\nsic databases.” in Ismir , vol. 2, 2002, pp. 287–288.\n[50] J. Mora, F. Gomez Martin, E. Gómez, F. J. Escobar-\nBorrego, and J. M. Díaz-Báñez, “Characterization and\nmelodic similarity of a cappella ﬂamenco cantes.” In-\nternational Society for Music Information Retrieval\nConference, ISMIR, 2010.\n[51] R. M. Bittner, K. Pasalo, J. J. Bosch, G. Meseguer-\nBrocal, and D. Rubinstein, “vocadito: A dataset of solo\nvocals with f_0, note, and lyric annotations,” 2021.\n[52] C.-L. Hsu and J.-S. R. Jang, “On the improvement of\nsinging voice separation for monaural recordings us-\ning the mir-1k dataset,” IEEE transactions on audio,\nspeech, and language processing , vol. 18, no. 2, pp.\n310–319, 2009.\n[53] I. P. Association, Handbook of the International Pho-\nnetic Association: A guide to the use of the Inter-\nnational Phonetic Alphabet . Cambridge University\nPress, 1999.\n[54] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and\nM. Sonderegger, “Montreal Forced Aligner: Trainable\nText-Speech Alignment Using Kaldi,” in Proc. Inter-\nspeech 2017 , 2017, pp. 498–502.\n[55] P. Boersma, “Praat, a system for doing phonetics by\ncomputer,” Glot International 5:9/10, 341-345 , 2001.\n[56] D. Mazzoni and R. Dannenberg, “Audacity [soft-\nware],” The Audacity Team, Pittsburg, PA, USA , vol.\n328, 2000.\n[57] M. Mauch and S. Dixon, “pyin: A fundamental fre-\nquency estimator using probabilistic threshold distribu-\ntions,” in 2014 ieee international conference on acous-\ntics, speech and signal processing (icassp) . IEEE,\n2014, pp. 659–663.\n[58] M. Morrison, C. Hsieh, N. Pruyne, and B. Pardo,\n“Cross-domain neural pitch and periodicity estima-\ntion,” in Submitted to IEEE Transactions on Audio,\nSpeech, and Language Processing , TODO 2023.[59] R. B. Dannenberg and C. Raphael, “Music score align-\nment and computer accompaniment,” Communications\nof the ACM , vol. 49, no. 8, pp. 38–43, August 2006.\n[60] L. Grubb, “A probabilistic method for tracking a vocal-\nist,” PhD thesis, Carnegie Mellon University, 1998.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n773"
    },
    {
        "title": "Audio Embeddings as Teachers for Music Classification.",
        "author": [
            "Yiwei Ding",
            "Alexander Lerch 0001"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265353",
        "url": "https://doi.org/10.5281/zenodo.10265353",
        "ee": "https://zenodo.org/records/10265353/files/000068.pdf",
        "abstract": "Music classification has been one of the most popular tasks in the field of music information retrieval. With the development of deep learning models, the last decade has seen impressive improvements in a wide range of classification tasks. However, the increasing model complexity makes both training and inference computationally expensive. In this paper, we integrate the ideas of transfer learning and feature-based knowledge distillation and systematically investigate using pre-trained audio embeddings as teachers to guide the training of low-complexity student networks. By regularizing the feature space of the student networks with the pre-trained embeddings, the knowledge in the teacher embeddings can be transferred to the students. We use various pre-trained audio embeddings and test the effectiveness of the method on the tasks of musical instrument classification and music auto-tagging. Results show that our method significantly improves the results in comparison to the identical model trained without the teacher's knowledge. This technique can also be combined with classical knowledge distillation approaches to further improve the model's performance.",
        "zenodo_id": 10265353,
        "dblp_key": "conf/ismir/Ding023",
        "keywords": [
            "Music classification",
            "deep learning models",
            "increasing model complexity",
            "transfer learning",
            "feature-based knowledge distillation",
            "pre-trained audio embeddings",
            "student networks",
            "regularization",
            "knowledge transfer",
            "music auto-tagging"
        ],
        "content": "AUDIO EMBEDDINGS AS TEACHERS FOR MUSIC CLASSIFICATION\nYiwei Ding\nMusic Informatics Group\nGeorgia Institute of Technology\nyding402@gatech.eduAlexander Lerch\nMusic Informatics Group\nGeorgia Institute of Technology\nalexander.lerch@gatech.edu\nABSTRACT\nMusic classiﬁcation has been one of the most popular tasks\nin the ﬁeld of music information retrieval. With the devel-\nopment of deep learning models, the last decade has seen\nimpressive improvements in a wide range of classiﬁcation\ntasks. However, the increasing model complexity makes\nboth training and inference computationally expensive. In\nthis paper, we integrate the ideas of transfer learning and\nfeature-based knowledge distillation and systematically in-\nvestigate using pre-trained audio embeddings as teachers to\nguide the training of low-complexity student networks. By\nregularizing the feature space of the student networks with\nthe pre-trained embeddings, the knowledge in the teacher\nembeddings can be transferred to the students. We use\nvarious pre-trained audio embeddings and test the effec-\ntiveness of the method on the tasks of musical instrument\nclassiﬁcation and music auto-tagging. Results show that\nour method signiﬁcantly improves the results in comparison\nto the identical model trained without the teacher’s knowl-\nedge. This technique can also be combined with classical\nknowledge distillation approaches to further improve the\nmodel’s performance.\n1. INTRODUCTION\nThe classiﬁcation of music has always been a widely popu-\nlar task in the ﬁeld of Music Information Retrieval (MIR).\nMusic classiﬁcation serves as an umbrella term for a variety\nof tasks, including music genre classiﬁcation [1], musical\ninstrument classiﬁcation [2], and music auto-tagging [3].\nThe last decade has seen dramatic improvements in a wide\nrange of such music classiﬁcation tasks due to the increas-\ning use of artiﬁcial neural networks [4–7].\nOne major contributing factor to these impressive ac-\ncomplishments is the increased algorithmic complexity of\nthe machine learning models which also means that the\ntraining process requires an increased amount of data. As\nnot all tasks have this abundance of annotated data, trans-\nfer learning has been widely and successfully applied to\nvarious music classiﬁcation tasks [8]. In transfer learning,\na model is ﬁrst pre-trained on a large-scale dataset for a\n© Y . Ding and A. Lerch. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution: Y .\nDing and A. Lerch, “Audio Embeddings as Teachers for Music Classiﬁ-\ncation”, in Proc. of the 24th Int. Society for Music Information Retrieval\nConf., Milan, Italy, 2023.(source) task that is somewhat related to the (target) task\nand then ﬁne-tuned with a comparably smaller dataset of\nthe target task [9]. This enables knowledge to be transferred\nacross datasets and tasks. Transfer learning has been repeat-\nedly shown to result in state-of-the-art performance for a\nmultitude of MIR tasks [10–12].\nAnother side effect of the increasing model complex-\nity is the slow inference speed. One way to address this\nissue is model compression by means of knowledge distilla-\ntion. Here, a low-complexity (student) model is trained\nwhile leveraging the knowledge in the high-complexity\n(teacher) model [13, 14]. The teacher-student paradigm\nhas met with considerable success in reducing the model\ncomplexity while minimizing performance decay [15, 16].\nIn this study, we integrate ideas and approaches from\nboth transfer learning and knowledge distillation and apply\nthem to the training of low-complexity networks to show the\neffectiveness of knowledge transfer for music classiﬁcation\ntasks. More speciﬁcally, we utilize pre-trained audio em-\nbeddings as teachers to regularize the feature space of low-\ncomplexity student networks during the training process.\nThus, the main contributions of this paper are a systematic\nstudy of\n•the effectiveness of various audio embeddings as\nteachers for knowledge transfer,\n•different ways to apply the knowledge transfer from\nteachers to students, and\n•the impact of data availability on the performance of\nthe investigated systems.\nThe models and experiments are publicly available as open-\nsource code.1\n2. RELATED WORK\nThis section ﬁrst brieﬂy introduces transfer learning and\nknowledge distillation, which are both often used to transfer\nknowledge between tasks and models, respectively, and then\nsurveys the application of feature space regularization in\nthe training of neural networks.\n2.1 Transfer Learning\nIn transfer learning approaches, a model is pre-trained on a\nsource task with a large dataset and subsequently ﬁne-tuned\non a (different but related) target task with a (typically\n1https://github.com/suncerock/\nEAsT-music-classification . Last accessed on June 21,\n2023.579smaller) dataset [9]. By utilizing the knowledge learned\nfrom the source task, models trained following the trans-\nfer learning paradigm can often achieve signiﬁcantly better\nresults than the same models trained directly on the target\ntask [17]; this is especially the case if these models have a\nlarge number of parameters and the training data for the tar-\nget task is limited. In the case where ﬁne-tuning the whole\nmodel might be too computationally expensive, another\nway to do transfer learning is to use the pre-trained embed-\ndings and train only the classiﬁcation head. This allows for\na separation of the tasks of computing the embeddings and\nthe classiﬁcation itself.\nTransfer learning has been successfully applied to a wide\nvariety of areas ranging from computer vision [18, 19] to\nnatural language processing [20]. In MIR, transfer learning\nhas been used for a multitude of target tasks [8, 10, 11, 21].\nBesides ﬁne-tuning the whole model, pre-trained embed-\ndings such as VGGish [22] and Jukebox [23] have also\nshown good performance on many tasks including auto-\ntagging [12,24], instrument classiﬁcation [4,12], and music\nemotion recognition [12, 24–26].\nOne disadvantage of transfer learning is the slow infer-\nence speed. In most cases, the model has a large number of\nparameters, which means that both ﬁne-tuning (if done on\nthe whole model) and inference potentially lead to a high\ncomputational workload.\n2.2 Knowledge Distillation\nApproaches for knowledge distillation aim at model com-\npression, i.e., reducing the complexity of the network.\nThe knowledge of a (usually high-complexity) pre-trained\nnetwork (the teacher) is transferred to a different (low-\ncomplexity) network (the student) during the training phase,\nin which the student not only learns from the ground truth la-\nbels but also from the teacher predictions. This is achieved\nby adding a “distillation loss” term to the student’s loss\nfunction to learn from the teacher’s prediction [13, 14].\nThe most popular distillation loss is the Kullback-Leibler\ndivergence between the logits of the student and the teacher,\nwith a hyperparameter called temperature to soften the\nprobability distribution of the teacher’s prediction over\nclasses [13]. The soft target provides more “dark” knowl-\nedge than the ground truth hard label [27, 28]. The Pearson\ncorrelation coefﬁcient has also been proposed as a distance\nmeasure between the logits as an alternative to the Kullback-\nLeibler divergence [29].\nBesides learning from logits, the student network can\nalso try to learn from the feature map from the intermedi-\nate layers of the teacher network [30 –32]. As the feature\nmaps of the student and teacher do not necessarily share\nthe same dimension and the same size, a variety of ways to\nmatch the feature space of the student and the teacher have\nbeen proposed [31, 33, 34]. Therefore, feature-based knowl-\nedge distillation has more ﬂexibility than the logits-based\ntraditional approach, which, at the same time, also makes\nit more challenging to ﬁnd the best way of matching the\nfeature space [35, 36].2.3 Feature Space Regularization\nFeature-based knowledge distillation is a technique of reg-\nularizing the feature space of the network during training.\nBesides knowledge distillation, there exists a wide vari-\nety of other ways to implement regularization. One exam-\nple is contrastive learning, which aims at contrasting the\nfeatures of instances with positive labels against negative\nlabels [37, 38]. Contrastive learning has been shown to\nimprove the performance of neural networks on music auto-\ntagging [39, 40] and music performance assessment [41].\nRegularizing the feature space using pre-trained audio\nembeddings has also been reported to be effective in music\nclassiﬁcation [42] and music source separation [43], where\nHung and Lerch proposed to use pre-trained embeddings\nto help structure the latent space during training. This tech-\nnique is similar to but different from both transfer learning\nand knowledge distillation. In transfer learning, the same\nmodel is used on two different datasets, and a typical setting\nis that knowledge from the large dataset will be transferred\nto the small dataset. In knowledge distillation, only one\ndataset is used and the typical setting is that the knowledge\nwill be transferred from a large model to a small model. In\ncomparison, regularizing the feature space using embed-\ndings requires neither the dataset nor the model to be the\nsame, yet still allows to transfer knowledge learned by the\nteacher model from a large dataset to the low-complexity\nstudent network for a different (small) dataset.\n3. METHODS\nInspired by the promising preliminary results of prior work\n[42], we integrate the idea of transfer learning and knowl-\nedge distillation by using pre-trained audio embeddings\nas teachers to regularize the feature space of the student\nnetwork during training. The overall pipeline is illustrated\nin Figure 1.\n3.1 Loss Function\nSimilar to knowledge distillation [13], we rewrite our loss\nfunction as\nL= (1−λ)Lpred+λLreg (1)\nwhereLpred is the loss function for conventional neural\nnetwork training, Lregis the loss function that measures\nthe distance between the student network’s feature map and\nthe pre-trained embeddings, and λ∈[0,1]is a weighting\nhyper-parameter.\n3.2 Regularization Location\nDifferent stages in a neural network output different fea-\nture maps, and the optimal location to apply regularization\ncontinues to be controversially discussed in feature-based\nknowledge distillation [36]. In this study, we investigate\neither regularizing only the ﬁnal feature map before the\nclassiﬁcation head as shown in Figure 1 or regularizing the\nfeature maps at all stages of the student network.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n580CNN Blocks\nCNN Blocks\nCNN Blocks\nClassification \nHead\nPrediction\n=+\nStudentPre-trained TeacherBlocks\nBlocks\nBlocks...\nBlocks\nEmbeddings\n(Extracted beforehand)Blocks\nDistance\nMeasurement\nFigure 1 : Overall pipeline of training a model by using pre-trained embeddings as teachers. The training loss is a weighted\nsum (weighting factor omitted in the ﬁgure) of prediction loss and regularization loss. The regularization loss measures the\ndistance between pre-trained embedding and the output feature map after the feature alignment. During inference, only the\nbottom part with the blue background is used.\n3.3 Feature Alignment\nTo measure the distance between the student feature map\nl∈RTs×Csand the pre-trained teacher embeddings v∈\nRTt×Ctwhich might have different numbers of time frames\n(i.e.,Ts̸=Tt), we ﬁrst align the intermediate feature map\nwith the pre-trained embeddings in time by repeating the\none with fewer time frames, then compute the distance for\neach frame and ﬁnally average them along the time axis.\n3.4 Distance Measure\nConsidering that pre-trained embeddings and feature maps\nhave often different dimensionalities, the use of distance\nmeasures that are independent of dimensionality allows for\neasier application.\n3.4.1 Cosine Distance Difference\nCosine distance difference2as proposed in previous work\n[42, 43] measures the difference in the cosine distance\nbetween pairs of samples. Given npairs of samples of\nsingle-time-frame features l1,l2,...,lnand pre-trained em-\nbeddings v1,v2,...,vn, the cosine distance difference for\none pair is\nDij=|dcos(li,lj)−dcos(vi,vj)|, (2)\nand the distance for this time frame is averaged among all\npairs.\n3.4.2 Distance Correlation\nDistance correlation was proposed as a generalization of\nclassical correlation to measure the independence between\ntwo random vectors in arbitrary dimensions [44]. It is\ncapable of handling features of different dimensionality;\nfurthermore, correlation-based distance measures have been\n2has been referred to in previous work as Distance-based Regulariza-\ntion (Dis-Reg) [42, 43].shown to be effective in knowledge distillation [29, 32].\nUsing the same notation as above, we deﬁne\naij=∥li−lj∥, (3)\n¯ai.=1\nnn/summationdisplay\nj=1aij,¯a.j=1\nnn/summationdisplay\ni=1aij,¯a..=1\nn2/summationdisplay\ni,j=1aij\n(4)\nAij=aij−¯ai.−¯a.j+¯a.. (5)\nwherei,j∈ {1,2,...,n}, and similarly, bij=∥vi−vj∥\nandBij=bij−¯bi.−¯b.j+¯b...3The distance for the time\nframe is then\nLreg= 1−R2\nn(l,v) = 1−V2\nn(l,v)/radicalbig\nV2n(l,l)V2n(v,v)(6)\nwhere\nV2\nn(l,l) =1\nn2n/summationtext\ni,j=1A2\nij,V2\nn(v,v) =1\nn2n/summationtext\ni,j=1B2\nij,\nV2\nn(l,v) =1\nn2n/summationtext\ni,j=1AijBij.\nNote that V2\nn(l,l)andV2\nn(v,v)will be0if and only if\nall thensamples of features (or embeddings) within one\nbatch are identical [44], which we assume not to occur here.\nTo optimize both distance measures during training,\nblock stochastic gradient iteration is used, which means\nthat the distance is computed over mini-batches instead of\nthe whole dataset [45, 46]. With stochastic approximation,\nthe computational complexity of the distance measure for n\nsamples is reduced from O(n2)toO(mn)wheremis the\nbatch size.\nIt is worth mentioning that both distance measures en-\nsure that if the distance is zero, the feature maps would\n3Eq. (3) uses 2-norm following the implementation in https:\n//github.com/zhenxingjian/Partial_Distance_\nCorrelation .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n581differ from the pre-trained embeddings by only an orthogo-\nnal linear transformation, which can be modeled in a single\nlinear layer. Therefore, if the regularization loss is zero, the\nstudent would have the same performance as the teacher in\nclassiﬁcation.\n4. EXPERIMENTAL SETUP\nWe test the effectiveness of using pre-trained embeddings\nas teachers on two different tasks, datasets, and models with\nfour different pre-trained embeddings as follows.\n4.1 Tasks, Datasets, Models, and Metrics\n4.1.1 Musical Instrument Classiﬁcation with OpenMIC\nMusical instrument classiﬁcation is a multi-label classiﬁ-\ncation problem. We use the OpenMIC dataset [2], which\nprovides weakly labeled audio snippets of length 10 s. Fol-\nlowing prior work [4, 49], we use the suggested test set and\nrandomly split 15% of the training data as the validation set,\nresulting in 12,692 training observations, 2,223 validation\nobservations, and 5085 test observations. To ensure a con-\nsistent sample rate, the audio is resampled to 32 kHz [5,49].\nAs the dataset is not completely labeled, i.e., parts of the\nlabels are missing and not labeled as positive or negative,\nthe missing labels are masked out when computing the loss\nfunction as suggested in previous work [5, 10, 49].\nWe use receptive ﬁeld regularized ResNet (CP-ResNet)\n[5] for this task, as it reaches state-of-the-art performance\nwhen trained only on the OpenMIC dataset (i.e., neither\ntrained with transfer learning nor trained with any knowl-\nedge distillation). CP-ResNet has a ResNet-like struc-\nture [19] with an added hyper-parameter ρto control the\nmaximum receptive ﬁeld of the ResNet. We set ρ= 7\nto match the setting which provides the best results in the\noriginal work [5].\nThe results are reported with the metrics mean Average\nPrecision (mAP) and F1-score. The F1-score is calculated\nin a macro fashion, which means that for each instrument,\nthe F1-score is computed for both the positive labels and the\nnegative labels and then averaged, and the ﬁnal F1-score is\nthe mean of the F1-scores of all instruments. The detection\nthreshold for the prediction is set to 0.4 following previous\nwork [5].\n4.1.2 Music Auto-Tagging with MagnaTagATune\nSimilar to musical instrument classiﬁcation, music auto-\ntagging is also a multi-label classiﬁcation problem. We\nuse the MagnaTagATune dataset [3] for this task, which\ncomes with audio clips of approximately 29.1 s. Following\nprevious work, we use only the top 50 labels and exclude all\nthe songs without any positive label from the dataset [7,50].\nFor comparability, the data split is adopted from previous\nwork, with audio ﬁles in the directories ’0’ to ’b’ being\nthe training set, ’c’ being the validation set, and ’e’ and ’f’\nbeing the test set [48, 51], resulting in 15,247 training clips,\n1,529 validation clips, and 4,332 test clips.\nWe apply a modiﬁed fully convolutional neural network\n(FCN) [6] to this task. It is the simplest model among thebenchmark models for the MagnaTagATune dataset [48]\nand consists of several convolution and max-pooling layers.\nTo further reduce the complexity of the model, we apply\nthe MobileNet-like modiﬁcation [52] to the network by\nbreaking the 3×3convolutions into depth-wise separable\nconvolutions and 1×1convolutions.\nThe results are evaluated with mAP and ROC-AUC.\n4.2 Pre-trained Embeddings\n4.2.1 VGGish\nVGGish [22] is a widely used embedding in MIR, with\na VGG network [53] being trained on a large number of\nYoutube videos. The open-source PyTorch implementation\nis used to extract VGGish features4which by default ex-\ntracts 128 principle components and then quantizes them to\n8 bit. The time resolution is 960 ms.\n4.2.2 OpenL3\nThe OpenL3 embedding [54,55] is trained on a music subset\nof AudioSet [56] in a self-supervised paradigm. The audio\nembeddings are extracted using the open-source Python\npackageOpenL35with the dimensionality being 512. To\nkeep consistent with VGGish, the time resolution is set to\n960 ms.\n4.2.3 PaSST\nPaSST [10] is a 7-layer transformer trained on AudioSet\nfor acoustic event detection. It applies the structure of a\nvision transformer [16, 57] and proposes the technique of\nPatchout to make the training efﬁcient. We use the open-\nsource code6released by the authors to extract the 768-\ndimensional embeddings. The time resolution is also set to\n960 ms.\n4.2.4 PANNs\nPANNs [11] include several convolutional neural networks\nand are also trained on AudioSet for acoustic event detec-\ntion. We use the default CNN14 model from the ofﬁcial\nrepository7. The embedding dimensionality is 2048. Dif-\nferent from other embeddings, PANNs provide only one\nglobal embedding for each clip of audio. Pilot experiments\nhave shown that extracting the embeddings for short seg-\nments and concatenating them does not improve perfor-\nmance.\n4.3 Systems Overview\nThe following systems are evaluated for comparison:\n•Baseline: CP ResNet (on OpenMIC) and Mobile\nFCN (on MagnaTagATune) trained without any extra\nregularization loss.\n4https://github.com/harritaylor/torchvggish .\nLast accessed on April 4, 2023.\n5https://github.com/marl/openl3/tree/main . Last\naccessed on April 4, 2023\n6https://github.com/kkoutini/PaSST/tree/main .\nLast accessed on April 4, 2023.\n7https://github.com/qiuqiangkong/audioset_\ntagging_cnn . Last accessed on April 4, 2023.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n582OpenMICNone VGGish OpenL3 PaSST PANNs\nmAP F1 mAP F1 mAP F1 mAP F1 mAP F1\nCP ResNet* [5] .819 .809 - - - - - - - -\nSS CP ResNet* [5] .831 .822 - - - - - - - -\nTeacher LR - - .803 .799 .803 .798 .858 .837 .853 .834\nKD (w/ mask) ** - - .829 .820 .823 .813 .851 .834 .848 .823\nEAsT Cos-Diff - - .838 .824 .838 .820 .837 .822 .836 .814\nEAsT Final - - .842 .828 .835 .822 .847 .830 .849 .828\nEAsT All - - .836 .823 .835 .822 .845 .827 .845 .827\nEAsT KD - - .836 .825 .836 .821 .852 .834 .857 .831\nMagnaTagATuneNone VGGish OpenL3 PaSST PANNs\nmAP AUC mAP AUC mAP AUC mAP AUC mAP AUC\nFCN†[6] .429 .900 - - - - - - - -\nMobile FCN .437 .905 - - - - - - - -\nTeacher LR - - .433 .903 .403 .890 .473 .917 .460 .911\nKD - - .447 .911 .439 .907 .454 .912 .448 .909\nEAsT Cos-Diff - - .446 .906 .438 .907 .453 .912 .453 .911\nEAsT Final - - .454 .912 .447 .910 .459 .912 .449 .909\nEAsT All - - .455 .911 .452 .911 .458 .913 .457 .911\nEAsT KD - - .441 .908 .437 .904 .461 .915 .459 .912\nTable 1 : Results on OpenMIC (above) and MagnaTagATune (below) dataset for different models regularized with different\npre-trained embeddings. Best performances are in bold, and best results excluding the teachers are underlined. *Reported\nresults [5], SS means being trained with shake-shake regularization [47]. **When using KD, the missing labels in OpenMIC\nwere masked to avoid potentially adding more training data.†Results from the open-source re-implementation [48].\n•Teacher LR: logistic regression on the pre-trained em-\nbeddings (averaged along the time axis), which can\nbe seen as one way to do transfer learning by freezing\nthe whole model except for the classiﬁcation head.\n•KD: classical knowledge distillation where the soft\ntargets are generated by the logistic regression.\n•EAsT Cos-Diff (for Embeddings-As-Teachers): feature\nspace regularization as proposed by Hung and Lerch\nthat uses cosine distance difference and regularizes\nonly the ﬁnal feature map [42].\n•EAsT Finaland EAsT All: proposed systems based on\ndistance correlation as the distance measure, either\nregularizing only at the ﬁnal stage or at all stages,\nrespectively.\n•EAsT KD: a combination of classical knowledge distil-\nlation and our method of using embeddings to regular-\nize the feature space. The feature space regularization\nis done only at the ﬁnal stage.\nWe perform a search of λfor each of the EasT systems and\nchoose the best-performing value on the validation set.8\n5. RESULTS\nThis section presents the results of different systems and\ntheir performance in the case of limited training data.\n8For all the hyperparameters, please refer to the conﬁg ﬁles in our\nGitHub.5.1 Results on OpenMIC and MagnaTagATune\nTable 1 shows the results on the OpenMIC and the Mag-\nnaTagATune datasets.\nWe can observe that the models trained with the extra reg-\nularization loss consistently outperform the non-regularized\nones on both datasets, with all features, and all regular-\nization methods. This means that the knowledge in the\nembeddings is successfully transferred to the student net-\nworks and consistently enhances the performance.\nAlthough EAsT Finalappears to give better results on the\nOpenMIC dataset while EAsT Allseems to have slightly\nbetter performance on the MagnaTagATune dataset, the dif-\nference between them is very small, meaning that the model\ndoes not beneﬁt signiﬁcantly from being regularized by pre-\ntrained embeddings at earlier stages where the feature maps\nare still relatively low-level.\nThe results for the teacher systems show that the older\nVGGish and OpenL3 embeddings are clearly outperformed\nby the more recently proposed embeddings PaSST and\nPANNs. In fact, the teacher systems for the newer em-\nbeddings perform so strongly that the students can rarely\noutperform them, while the student systems trained with\nVGGish and OpenL3 provide better results than the corre-\nsponding teachers. We can see that whether the teachers\nthemselves have an excellent performance or not, students\nbeneﬁt from learning the additional knowledge from these\nembeddings, and the students’ upper limit is not bounded\nby the performance of teachers.\nComparing KD and the EAsT Finalor EAsT Allsystems,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n583Model Parameters (M) Iteration / s\nVGGish 72.1 172.2\nOpenL3 4.69 117.9\nPaSST 86.1 18.7\nPANNs 79.7 70.6\nMobile FCN 0.34 319.3\nCP ResNet 5.52 205.3\nTable 2 : Comparison of the model complexity.\nwe can see that with VGGish and OpenL3 embeddings,\nregularizing the feature space provides better results than\nsimply using the teachers’ soft targets. On the other hand,\nfor the PaSST and PANNs embeddings, classical knowl-\nedge distillation provides competitive results. The possible\nreason is that the soft targets given by “weak” teachers\nmight have provided too much incorrect information to the\nstudents while the high-quality soft targets generated by the\n“strong” teachers provide good guidance for the students’\ntraining.\nThe combination system EAsT KDgives us better results\nwith PaSST and PANNs embeddings (with the exception\nof no noteworthy improvement with the PaSST embedding\non the OpenMIC dataset) while for VGGish and OpenL3\nembeddings, the performance is not as good as EAsT Finalor\nEAsT Allin most cases. This observation is in accordance\nwith our speculation that traditional knowledge distillation\nperforms best with a “strong” teacher. While learning from\naudio embeddings beneﬁts a student network even more in\nthe presence of a “strong” teacher, learning from “weak”\nembeddings can still improve the model’s performance.\n5.2 Comparison of Model Complexity\nTable 2 lists the number of parameters as well as rough\ninference speed measurements9of the models.\nThe numbers of parameters only take the backbone struc-\nture (i.e., excluding the ﬁnal classiﬁcation head) into ac-\ncount so that it does not vary across datasets with different\nnumbers of classes. Iterations per second are tested with\n128×1000 input spectrograms.\nWe can see that Mobile FCN and CP ResNet are much\nfaster in inference than pre-trained models.\n5.3 Limited Training Data\nTo investigate the impact of limited training data on our\nmethods, we present the system performances for reduced\ntraining data, i.e., for 25%, 50%, and 75% of the original\ntraining data. The results are shown in Figure 2 . We use\nVGGish and PaSST as the pre-trained embeddings.\nWe can observe that limiting the training data has the\ngreatest impact on the baseline systems, which show the\nbiggest performance drop.\nOn the OpenMIC dataset, EAsT Cos-Diff and EAsT Final\nhave similar decreases in mAP, and the KD system is less\n9reference GPU: NVIDIA 2070 Super25 50 75 100\nT raining data (%)0.760.780.800.820.84mAPmAP on O enMIC (VGGish)\nBaseline\nKD\nEAsT\nCos − Diff\nEAsT\nFinal\n25 50 75 100\nT raining da a (%)0.760.780.800.820.84mAPmAP on OpenMIC (P aSST)\nBaseline\nKD\nEAsT\nCos − Diff\nEAsT\nFinal\n25 50 75 100\nT raining data (%)0.380.400.420.44mAPmAP on MagnaT agA T  ne (VGGish)\nBaseline\nKD\nEAsT\nCos − Diff\nEAsT\nFinal\n25 50 75 100\nT raining data (%)0.380.400.420.440.46mAPmAP on MagnaT agA T une (P aSST)\nBaseline\nKD\nEAsT\nCos − Diff\nEAsT\nFinal\nFigure 2 : Results with limited training data on two datasets.\naffected. An interesting ﬁnding is that when the VGGish\nembedding is used, KD shows better performance for lim-\nited data amounts while it is outperformed by EAsT Cos-Diff\nand EAsT Finalwhen the whole OpenMIC dataset is avail-\nable. This means using embeddings as teachers might still\nrequire a sufﬁcient amount of data to have good guidance\non the student models.\nOn the MagnaTagATune dataset, however, the\nEAsT Cos-Diff and EAsT Finalsystems show less performance\ndecay than either KD or the baseline when the training data\nis limited. This suggests that in our training settings, there\nis no certain answer to which method is least affected by the\nlack of training data, and the answer might be dependent\non speciﬁc tasks, models, and data.\n6. CONCLUSION AND FUTURE WORK\nIn this paper, we explored the use of audio embeddings as\nteachers to regularize the feature space of low-complexity\nstudent networks during training. We investigated several\ndifferent ways of implementing the regularization and tested\nits effectiveness on the OpenMIC and MagnaTagATune\ndatasets. Results show that using embeddings as teachers\nenhances the performance of the low-complexity student\nmodels, and the results can be further improved by com-\nbining our method with a traditional knowledge distillation\napproach.\nFuture work will investigate the performance of our\nmethod on a wider variety of downstream tasks and embed-\ndings. Moreover, as there have been a wide variety of mod-\nels to extract audio and music embeddings, we speculate\nthat using an ensemble of different pre-trained embeddings\nalso has considerable potential. Finally, the ﬂexibility of\nfeature-based knowledge distillation offers a wide range of\npossible algorithmic modiﬁcations. Our focus will be on\nevaluating different distance measures and regularizing the\nnetwork using features from different stages of the teacher\nnetwork instead of using only the output embeddings.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n5847. REFERENCES\n[1]G. Tzanetakis and P. Cook, “Musical genre classiﬁca-\ntion of audio signals,” IEEE Transactions on Speech and\nAudio Processing , vol. 10, no. 5, pp. 293–302, 2002.\n[2]E. Humphrey, S. Durand, and B. McFee, “Openmic-\n2018: An open data-set for multiple instrument recog-\nnition,” in Proceedings of the International Society for\nMusic Information Retrieval Conference (ISMIR) , 2018,\npp. 438–444.\n[3]E. Law, K. West, M. I. Mandel, M. Bay, and J. S.\nDownie, “Evaluation of algorithms using games: The\ncase of music tagging,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , 2009, pp. 387–392.\n[4]S. Gururani, M. Sharma, and A. Lerch, “An attention\nmechanism for musical instrument recognition,” in Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , 2019, pp. 83–90.\n[5]K. Koutini, H. Eghbal-zadeh, and G. Widmer, “Recep-\ntive ﬁeld regularization techniques for audio classiﬁ-\ncation and tagging with deep convolutional neural net-\nworks,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , vol. 29, pp. 1987–2000, 2021.\n[6]K. Choi, G. Fazekas, and M. Sandler, “Automatic tag-\nging using deep convolutional neural networks,” in Pro-\nceedings of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , 2016, pp. 805–811.\n[7]T. Kim, J. Lee, and J. Nam, “Sample-level cnn archi-\ntectures for music auto-tagging using raw waveforms,”\ninProceedings of the IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2018, pp. 366–370.\n[8]K. Choi, G. Fazekas, M. Sandler, and K. Cho, “Transfer\nlearning for music classiﬁcation and regression tasks,”\ninProceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR) , 2017, pp.\n141–149.\n[9]S. J. Pan and Q. Yang, “A survey on transfer learning,”\nIEEE Transactions on Knowledge and Data Engineer-\ning, vol. 22, no. 10, pp. 1345–1359, 2010.\n[10] K. Koutini, J. Schlüter, H. Eghbal-zadeh, and G. Wid-\nmer, “Efﬁcient training of audio transformers with\npatchout,” in Proceedings of INTERSPEECH 2022 ,\n2022, pp. 2753–2757.\n[11] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D.\nPlumbley, “Panns: Large-scale pretrained audio neural\nnetworks for audio pattern recognition,” IEEE/ACM\nTransactions on Audio, Speech, and Language Process-\ning, vol. 28, pp. 2880–2894, 2020.\n[12] M. C. McCallum, F. Korzeniowski, S. Oramas,\nF. Gouyon, and A. F. Ehmann, “Supervised and un-\nsupervised learning of audio representations for musicunderstanding,” in Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2022, pp. 256–263.\n[13] G. Hinton, O. Vinyals, and J. Dean, “Distilling\nthe knowledge in a neural network,” 2015. [Online].\nAvailable: http://arxiv.org/abs/1503.02531\n[14] J. Ba and R. Caruana, “Do deep nets really need to be\ndeep?” in Proceedings of Advances in Neural Informa-\ntion Processing Systems (NeurIPS) , 2014.\n[15] L. Yu, V . O. Yazici, X. Liu, J. v. d. Weijer, Y . Cheng, and\nA. Ramisa, “Learning metrics from teachers: Compact\nnetworks for image embedding,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (ICCV) , 2019, pp. 2902–2911.\n[16] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-\nrolles, and H. Jégou, “Training data-efﬁcient image\ntransformers & distillation through attention,” in Pro-\nceedings of the International Conference on Machine\nLearning (ICML) , 2021, pp. 10 347–10 357.\n[17] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung,\nS. Gelly, and N. Houlsby, “Big transfer (bit): General vi-\nsual representation learning,” in Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV) , 2020,\npp. 491–507.\n[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\nL. Fei-Fei, “Imagenet: A large-scale hierarchical im-\nage database,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) ,\n2009, pp. 248–255.\n[19] K. He, X. Zhang, S. Ren, and J. Sun, “Deep resid-\nual learning for image recognition,” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2016, pp. 770–778.\n[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n“Bert: Pre-training of deep bidirectional transformers\nfor language understanding,” in Proceedings of the Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL) , 2019, pp.\n4171–4186.\n[21] P. Alonso-Jiménez, D. Bogdanov, and X. Serra, “Deep\nembeddings with essentia models,” in Late Breaking\nDemo of the International Society for Music Information\nRetrieval Conference (ISMIR) , 2020.\n[22] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke,\nA. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A.\nSaurous, B. Seybold et al. , “Cnn architectures for large-\nscale audio classiﬁcation,” in Proceedings of the IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , 2017, pp. 131–135.\n[23] P. Dhariwal, H. Jun, C. Payne, J. W. Kim,\nA. Radford, and I. Sutskever, “Jukebox: A generative\nmodel for music,” 2020. [Online]. Available: http:\n//arxiv.org/2005.00341Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n585[24] R. Castellon, C. Donahue, and P. Liang, “Codiﬁed au-\ndio language modeling learns useful representations\nfor music information retrieval,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2021, pp. 88–96.\n[25] E. S. Koh and S. Dubnov, “Comparison and analysis of\ndeep audio embeddings for music emotion recognition,”\ninAAAI Workshop on Affective Content Analysis , 2021.\n[26] D. Bogdanov, X. Lizarraga Seijas, P. Alonso-Jiménez,\nand X. Serra, “Musav: a dataset of relative arousal-\nvalence annotations for validation of audio models,” in\nProceedings of the International Society for Music In-\nformation Retrieval Conference (ISMIR) , 2022, pp. 650–\n658.\n[27] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and\nZ. Wojna, “Rethinking the inception architecture for\ncomputer vision,” in Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR) , 2016, pp. 2818–2826.\n[28] R. Müller, S. Kornblith, and G. E. Hinton, “When does\nlabel smoothing help?” in Proceedings of Advances\nin Neural Information Processing Systems (NeurIPS) ,\n2019.\n[29] T. Huang, S. You, F. Wang, C. Qian, and C. Xu, “Knowl-\nedge distillation from a stronger teacher,” in Proceed-\nings of Advances in Neural Information Processing Sys-\ntems (NeurIPS) , 2022.\n[30] A. Romero, N. Ballas, S. E. Kahou, A. Chassang,\nC. Gatta, and Y . Bengio, “Fitnets: Hints for thin deep\nnets,” in Proceedings of the International Conference\non Learning Representations (ICLR) , 2015.\n[31] B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, and J. Y .\nChoi, “A comprehensive overhaul of feature distillation,”\ninProceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV) , 2019, pp. 1921–1930.\n[32] B. Peng, X. Jin, J. Liu, D. Li, Y . Wu, Y . Liu, S. Zhou,\nand Z. Zhang, “Correlation congruence for knowledge\ndistillation,” in Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV) , 2019,\npp. 5007–5016.\n[33] J. Yim, D. Joo, J. Bae, and J. Kim, “A gift from knowl-\nedge distillation: Fast optimization, network minimiza-\ntion and transfer learning,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recogni-\ntion (CVPR) , 2017, pp. 4133–4141.\n[34] J. Kim, S. Park, and N. Kwak, “Paraphrasing complex\nnetwork: Network compression via factor transfer,” in\nProceedings of Advances in Neural Information Pro-\ncessing Systems (NeurIPS) , 2018.\n[35] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowl-\nedge distillation: A survey,” International Journal of\nComputer Vision , vol. 129, pp. 1789–1819, 2021.[36] L. Wang and K.-J. Yoon, “Knowledge distillation and\nstudent-teacher learning for visual intelligence: A re-\nview and new outlooks,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , vol. 44, pp. 3048–\n3068, 2022.\n[37] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and\nT. Brox, “Discriminative unsupervised feature learning\nwith convolutional neural networks,” in Proceedings\nof Advances in Neural Information Processing Systems\n(NeurIPS) , 2014.\n[38] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A\nsimple framework for contrastive learning of visual\nrepresentations,” in Proceedings of the International\nConference on Machine Learning (ICML) , 2020, pp.\n1597–1607.\n[39] J. Spijkervet and J. A. Burgoyne, “Contrastive learn-\ning of musical representations,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2021, pp. 673–681.\n[40] P. Alonso-Jiménez, X. Serra, and D. Bogdanov, “Music\nrepresentation learning based on editorial metadata from\ndiscogs,” in Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2022, pp. 825–833.\n[41] P. Seshadri and A. Lerch, “Improving music perfor-\nmance assessment with contrastive learning,” in Pro-\nceedings of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , 2021, pp. 634–641.\n[42] Y .-N. Hung and A. Lerch, “Feature-informed embed-\nding space regularization for audio classiﬁcation,” in\nProceedings of the European Signal Processing Confer-\nence (EUSIPCO) , 2022, pp. 419–423.\n[43] ——, “Feature-informed latent space regularization for\nmusic source separation,” in Proceedings of the Inter-\nnational Conference on Digital Audio Effects (DAFx) ,\n2022.\n[44] G. J. Székely, M. L. Rizzo, and N. K. Bakirov, “Measur-\ning and testing dependence by correlation of distances,”\nThe Annals of Statistics , vol. 35, no. 6, pp. 2769–2794,\n2007.\n[45] Y . Xu and W. Yin, “Block stochastic gradient iteration\nfor convex and nonconvex optimization,” SIAM Journal\non Optimization , vol. 25, no. 3, pp. 1686–1716, 2015.\n[46] X. Zhen, Z. Meng, R. Chakraborty, and V . Singh, “On\nthe versatile uses of partial distance correlation in deep\nlearning,” in Proceedings of the European Conference\non Computer Vision (ECCV) , 2022, pp. 327–346.\n[47] X. Gastaldi, “Shake-shake regularization of 3-branch\nresidual networks,” in Workshop Track of the Interna-\ntional Conference on Learning Representations (ICLR) ,\n2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n586[48] M. Won, A. Ferraro, D. Bogdanov, and X. Serra, “Evalu-\nation of cnn-based automatic music tagging models,” in\nProceedings of the Sound and Music Computing (SMC) ,\n2020, pp. 331–337.\n[49] H.-H. Chen and A. Lerch, “Music instrument classiﬁca-\ntion reprogrammed,” in Proceedings of the International\nConference on Multimedia Modeling (MMM) , 2023, pp.\n345–357.\n[50] M. Won, S. Chun, and X. Serra, “Toward interpretable\nmusic tagging with self-attention,” 2019. [Online].\nAvailable: http://arxiv.org/1906.04972\n[51] M. Won, S. Chun, O. Nieto, and X. Serrc, “Data-driven\nharmonic ﬁlters for audio representation learning,” in\nProceedings of the IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\n2020, pp. 536–540.\n[52] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko,\nW. Wang, T. Weyand, M. Andreetto, and H. Adam,\n“Mobilenets: Efﬁcient convolutional neural networks for\nmobile vision applications,” 2017. [Online]. Available:\nhttp://arxiv.org/abs/1704.04861\n[53] K. Simonyan and A. Zisserman, “Very deep convolu-\ntional networks for large-scale image recognition,” in\nProceedings of the International Conference on Learn-\ning Representations (ICLR) , 2015.\n[54] J. Cramer, H.-H. Wu, J. Salamon, and J. P. Bello, “Look,\nlisten, and learn more: Design choices for deep audio\nembeddings,” in Proceedings of the IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , 2019, pp. 3852–3856.\n[55] R. Arandjelovic and A. Zisserman, “Look, listen and\nlearn,” in Proceedings of the IEEE International Confer-\nence on Computer Vision (ICCV) , 2017, pp. 609–617.\n[56] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter,\n“Audio set: An ontology and human-labeled dataset for\naudio events,” in Proceedings of the IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , 2017, pp. 776–780.\n[57] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” in Proceedings\nof the International Conference on Learning Represen-\ntations (ICLR) , 2021.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n587"
    },
    {
        "title": "How Control and Transparency for Users Could Improve Artist Fairness in Music Recommender Systems.",
        "author": [
            "Karlijn Dinnissen",
            "Christine Bauer 0001"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.8121152",
        "url": "https://doi.org/10.5281/zenodo.8121152",
        "ee": "https://zenodo.org/records/8121152/files/DinnissenBauer_2023_Questionnaire.pdf",
        "abstract": "Supplementary material\nHere, we offerthe supplementary material that was used during the study referenced below.\nFor details on the provided materials, please refer to the README.txt file.\n\nPaper title\nHow Control and Transparency for Users Could Improve Artist Fairness in Music Recommender Systems.\n\nPaper abstract\nAs streaming services have become a main channel for music consumption, they significantly impact various stakeholders: users, artists who provide music, and other professionals working in the music industry. Therefore, it is essential to consider all stakeholders goals and values when developing and evaluating the music recommender systems integrated into these services. One vital goal is treating artists fairly, thereby giving them a fair chance to have their music recommended and listened to, and subsequently building a fan base. Such artist fairness is often assumed to have a trade-off with user goals such as satisfaction. Using insights from two studies, this work shows the opposite: some goals from different stakeholders are complementary. Our first study, in which we interview music artists, demonstrates that they often see increased transparency and control for users as a means to also improve artist fairness. We expand with a second study asking other music industry professionals about these topics using a questionnaire. Its results indicate that transparency towards users is highly valued and should be increased.\n\nCitation\nKarlijn Dinnissen and Christine Bauer. 2023. How Control and Transparency for Users Could Improve Artist Fairness in Music Recommender Systems. In Proceedings of the 24thInternationalSociety for Music Information Retrieval Conference (ISMIR 23), June 2629, 2023, Milan, Italy.10pages.",
        "zenodo_id": 8121152,
        "dblp_key": "conf/ismir/Dinnissen023",
        "content": "Dinnissen & Bauer (2023) \nImprovement of Music Recommender Systems Informed Consent Agreement      Before taking part in this study, please read the following information and indicate below whether you consent to the conditions of participation.   As part of a research project focusing on music recommender systems, we would like to understand how they affect artists and other professionals with a music industry-related role. Additionally, we want to identify how these systems could be improved. For this purpose, we are asking you to contribute your perspective through participating in a survey. Your input will be used to gain more insight in the impact of music recommender systems, and to inform future research that aims to improve these systems.   Participation is completely voluntary. You can decide to withdraw at any time and for any reason, including after participating. The survey will take approximately 10-15 minutes, and will be in the form of closed questions with optional comment fields.    We intend to report the results of this survey in publications and/or presentations. We also intend to share collected survey data with the broader research community, following FAIR (Findable, Accessible, Interoperable, Reusable) data principles. Any information from the optional comment fields that could identify you as an individual, will be replaced or removed before publishing the survey data. Personal characteristics are only collected in broad categories. Therefore, your identity as a participant will remain confidential at all times, and risk of participation will be minimal.     This project has been approved by Utrecht University’s Science-Geo Ethics Review Board, and falls under fundamental research without any commercial purpose nor external stakeholders or partners.    For more information regarding this project, please contact: Karlijn Dinnissen (PhD Student) Human Centered Computing Group Utrecht University k.dinnissen@uu.nl   Dr. Christine Bauer (Assistant Professor) Human Centered Computing Group Utrecht University     c.bauer@uu.nl       For more information regarding the ethical review process or privacy, please contact Utrecht University’s Data Protection Officer: privacy@uu.nl   \nDinnissen & Bauer (2023) \nAgreement o I am 18 years or older.  o I have read the information above, and understand the nature and goal of this research project.  o I understand my participation is voluntary, and that I may withdraw from the study at any time.  o I understand that any information I enter may be shared with the broader research community, and may be reported in scientific publications. Any information that could identify me as an individual will be replaced or removed.  o I agree to participate in the research project as described above.         \nDinnissen & Bauer (2023) \nConcept clarification We will use the following concepts throughout this survey:       Music streaming service (also: streaming service) A service offering its users the possibility to digitally search for and stream music (and possibly other content) on-demand. It typically incorporates one or more music recommender systems.   Music recommender system A system that aims to help users discover music they may enjoy, based on their listening history and other factors. Some automated functionalities powered by recommender systems are: personalized playlists, 'similar artists' recommendation, autoplay, etc.              Points of view We will ask your perspective from different points of view. They are defined and indicated in the following way:        You as a professional Concerning you in your professional life.   Music providers Concerning all artists and other music industry professionals that make and/or provide the music that is consumed on streaming services.       Users / music consumers Concerning all individuals using a service to consume music.          \n \nDinnissen & Bauer (2023) \n  Questions: Professional  The following questions are about your experiences as a professional (not in your personal life).   In which role(s) or sector(s) do you currently work? It is possible to select multiple answers.  \u0000 Artist - e.g., composer, performing artist, DJ  \u0000 Artist Representation - e.g., artist management  \u0000 Bookings - e.g., festival, venue, agent, promotor  \u0000 Consultancy & Associations - e.g., export office, interest group  \u0000 Distribution - e.g., label, publisher, streaming service, collecting society  \u0000 Education - e.g., university, talent development institution  \u0000 Event Production - e.g., logistics, sustainability, event technology  \u0000 Finance - e.g., sales, sponsorships, accountant  \u0000 Legal & Policy - e.g., government, subsidiaries, lobbying  \u0000 Marketing & PR - e.g., label, venue, artist, brand, promotor  \u0000 Media - e.g., radio, press, TV  \u0000 Recording - e.g., label, record company, studio  \u0000 Technology - e.g., software / web development, ticketing  \u0000 Other: __________________________________________________ \u0000 None / would prefer not to say       \n \nDinnissen & Bauer (2023) \nFor your work, which music streaming service(s) do you regularly (more than once per month) use or come into contact with, if any?   ▢  Amazon Music   ▢  Deezer   ▢  Qobuz  ▢  Soundcloud  ▢  Apple Music  ▢  Idagio  ▢  QQ Music  ▢  Tidal ▢  Bandcamp   ▢  Pandora  ▢  Spotify  ▢  Youtube (Music)  ▢  Other(s): __________________________________________________ ▢  None       Regarding music streaming services, ...   Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Don't know / prefer not to answer I understand how their integrated recommender systems work  o  o  o  o  o  o  I know how to get insight in stream counts and audience stats  o  o  o  o  o  o  They provide me with enough information to make strategic decisions  o  o  o  o  o  o  I am in contact or would want to be in contact with their representatives  o  o  o  o  o  o    \nDinnissen & Bauer (2023) \nCompared to before the COVID-19 pandemic, the role of ... in my professional life increased:   Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Don't know / prefer not to answer Social media  o  o  o  o  o  o  Music streaming services  o  o  o  o  o  o  Algorithms  o  o  o  o  o  o      Comments or suggestions on these topics (optional): ________________________________________________________________ ________________________________________________________________ ________________________________________________________________   \nDinnissen & Bauer (2023) \nDiscovering or reviewing new artists or music is a part of my job: ◯  Never ◯  Sometimes   ◯  Frequently   ◯  Prefer not to answer   -  Please skip the questions on this page if you indicated Never or Prefer not to answer  -    Which channel(s) do you use to discover or review new artists/music?  \u0000 Word of mouth / informal channel  \u0000 Suggestions/plugs that were sent to me or my company  \u0000 Live shows/showcases  \u0000 Music streaming service(s)  \u0000 Written media (magazine, blog, newspaper etc.)  \u0000 Social media (TikTok, Facebook, Instagram, Reddit, etc.)  \u0000 Other media (TV, radio, podcast etc.)  \u0000 Other(s): __________________________________________________    Which places/pages on music streaming services do you use to discover or review new artists/music?    \u0000 What is shown on my home/front page  \u0000 'Related artists/songs' sections  \u0000 Discovery-oriented playlists that are personalized (= tailored specifically to me)  \u0000 Artist/song/album-themed playlists that are personalized  \u0000 I search for music fitting to a general mood, style, era, etc.  \u0000 Other: __________________________________________________ \u0000 None     Comments or suggestions on this topic (optional): ________________________________________________________________  \nDinnissen & Bauer (2023) \nQuestions: Providing music on streaming services  The following questions are about music providers - artists or parties who provide music - and their interaction with streaming services.     Transparency   For music providers on streaming services, I feel like...   Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Don't know / prefer not to answer It is clear for which reason(s), when, and to whom their music is recommended  o  o  o  o  o  o  It should be made more clear for which reason(s), when, and to whom their music is recommended  o  o  o  o  o  o       Control    On streaming services, I am happy with the extent to which music providers can influence...   Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Don't know / prefer not to answer Which of their music is recommended (which songs)  o  o  o  o  o  o  To whom their music is recommended (to which users)  o  o  o  o  o  o  In which context their music is recommended (which playlist, related artist page, etc.)  o  o  o  o  o  o   \n \nDinnissen & Bauer (2023) \nContact   For music providers, I feel like...   Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Not applicable / prefer not to answer It is clear how to contact streaming services with questions or suggestions  o  o  o  o  o  o  It is easy to contact streaming services with questions or suggestions  o  o  o  o  o  o      Comments or suggestions on these topics (optional): ________________________________________________________________ ________________________________________________________________ ________________________________________________________________   \nDinnissen & Bauer (2023) \n Making, releasing or providing music on streaming platforms is, (in)directly, a part of my job: ◯  Never  ◯ Sometimes   ◯  Frequently   ◯  Prefer not to answer    -  Please skip the questions on this page if you indicated Never or Prefer not to answer  -     The following questions are about your experiences as a professional and as a music provider.  They are shown as you indicated you make, release or provide music on streaming platforms.     Control   If I would have control over which of my/our songs are recommended, I would mostly recommend...  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Don't know / prefer not to answer My most popular songs  o  o  o  o  o  o  My latest releases  o  o  o  o  o  o  My personal favorites  o  o  o  o  o  o      (Optional:) I would mostly recommend songs based on other song criteria, namely:  ________________________________________________________________ ________________________________________________________________ _______________________________________________________________ \n \nDinnissen & Bauer (2023) \n-  Please skip the questions on this page if you do not make, release or provide music -  If I would have control over to whom my/our songs are recommended, I would recommend...  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Don't know / prefer not to answer To users that are familiar with my music  o  o  o  o  o  o  To users that are geographically close to where I'm from  o  o  o  o  o  o  To users that like specific other artists I associate with  o  o  o  o  o  o  To users that like a specific genre I associate with  o  o  o  o  o  o   (Optional:) I would mostly recommend songs based on other user criteria, namely: ________________________________________________________________ ________________________________________________________________    Strategy  When releasing music, on which channels do you normally focus?   \u0000 Physical medium (CD, vinyl record, cassette tape etc.)  \u0000 Digital libraries (downloads, Bandcamp, iTunes, etc.)  \u0000 Music streaming service(s)  \u0000 Analogue radio channel(s)  \u0000 Online radio channel(s) or live stream  \u0000 Podcast(s)  \u0000 Social media (TikTok, Facebook, Instagram, Reddit, etc.)  \u0000 Other: __________________________________________________ \u0000 Don't know / prefer not to answer    \nDinnissen & Bauer (2023) \n-  Please skip the questions on this page if you do not make, release or provide music -  For curated playlists (created or compiled by a human editor) on music streaming platforms, I feel like...  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Not applicable / prefer not to answer It is important that my/our music is included  o  o  o  o  o  o  It is clear how I can get my/our music included  o  o  o  o  o  o  I succeed in having my/our music included  o  o  o  o  o  o      For algorithm-based recommendations (personalized playlists, auto-play etc.) on music streaming platforms, I feel like...  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Not applicable / prefer not to answer It is important that my/our music is included  o  o  o  o  o  o  It is clear how I can get my/our music included  o  o  o  o  o  o  I succeed in having my/our music included  o  o  o  o  o  o     Comments or suggestions on this topic (optional): ________________________________________________________________ ________________________________________________________________ ________________________________________________________________  \nDinnissen & Bauer (2023) \nQuestions: Recommendation  The following questions concern decisions on which artists to recommend in music recommender systems.    For these statements, please circle a value on a scale of 0 to 100%. The statements are individual, so for each statement, you can answer anything between 0 and 100%.   Out of all algorithmically recommended songs, this % should be from artists who are...   Actively releasing new music 0%  10%  20%  30%  40%  50%  60%  70%  80%  90%  100% Established (not recently started) 0%  10%  20%  30%  40%  50%  60%  70%  80%  90%  100% Popular 0%  10%  20%  30%  40%  50%  60%  70%  80%  90%  100%    Diversity and Inclusion  • Diversity is the presence of differences that may include, e.g., gender, religion, sexual orientation, ethnicity, and language. For music, it may also include characteristics such as genre and popularity. • Inclusion is an outcome to ensure those that are diverse actually feel and/or are welcomed.    With respect to diversity and inclusion in music recommendations, I feel that music streaming services...  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Don't know / prefer not to answer Care about it  o  o  o  o  o  o  Have an impact on it in the music industry  o  o  o  o  o  o  Have a responsibility to increase it in the music industry  o  o  o  o  o  o  Should actively make users' recommendations more diverse  o  o  o  o  o  o   \n \nDinnissen & Bauer (2023) \nI feel that music streaming services should increase diversity and inclusion by actively recommending music from ... more often:  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Not applicable / prefer not to answer New (recently started) artists  o  o  o  o  o  o  Lesser-known (but not new) artists  o  o  o  o  o  o  Artists who make non-mainstream music  o  o  o  o  o  o  Artists that are from the users' local area  o  o  o  o  o  o  Artists from an underrepresented gender  o  o  o  o  o  o  Artists from an underrepresented ethnicity  o  o  o  o  o  o  Artists from an underrepresented nationality  o  o  o  o  o  o    (Optional:) Music streaming services should actively recommend music from another group more often, namely: It is possible to fill in more than 1 group ________________________________________________________________ ________________________________________________________________ ________________________________________________________________   (Optional:) Music streaming platforms could also increase diversity and inclusion in the music industry by: ________________________________________________________________ ________________________________________________________________ ________________________________________________________________   \nDinnissen & Bauer (2023) \nQuota One way to increase diversity and inclusion, is through enforcing quota: e.g., at minimum, 10% of all music recommendations should be from artists who are [...].  I feel that enforcing quota could be a suitable way to increase diversity and inclusion: o Disagree  o Neither agree nor disagree  o Agree  o Don't know / prefer not to answer   -  Please skip the questions below if you indicated Disagree -    Enforcing a quota on automatic recommendations would be a good way to promote lesser-known artists (e.g., recently started or with a small listener base):    o No, they should not be specifically promoted  o No, they should be promoted but not through quota  o Yes, but I don't know which %  o Yes, and it should be this % of lesser-known artists: _______________ o Don't know / prefer not to answer    Enforcing a quota on automatic recommendations would be a good way to promote local artists (e.g., from the same country as the user):    o No, they should not be specifically promoted  o No, they should be promoted but not through quota  o Yes, but I don't know which %  o Yes, and it should be this % of local artists: _______________ o Don't know / prefer not to answer     \nDinnissen & Bauer (2023) \n-  Please skip the questions on this page if you think quota are not a suitable solution  -   Enforcing a quota on automatic recommendations would be a good way to promote women or non-binary artists:     o No, they should not be specifically promoted  o No, they should be promoted but not through quota  o Yes, but I don't know which %  o Yes, and it should be this % of women/non-binary artists: _______________ o Don't know / prefer not to answer    (Optional:) Enforcing a quota on automatic recommendations would be a good way to promote another group of artists, namely: It is possible to fill in more than 1 group ________________________________________________________________ ________________________________________________________________ ________________________________________________________________      \nDinnissen & Bauer (2023) \nQuestions: Listening to music on streaming services    The following questions are about users (consumers of music) on streaming services, and their interaction with these services.     Variety   With respect to variety in users' music recommendations, I feel that music streaming services should...  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Don't know / prefer not to answer Take users out of their comfort zone  o  o  o  o  o  o  Actively try to make users' recommendations more varied  o  o  o  o  o  o  Actively try to make users' listening behavior more varied  o  o  o  o  o  o  Actively try to make users' general taste more varied  o  o  o  o  o  o     Transparency    For users of streaming services, I feel like...  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Don't know / prefer not to answer It is clear for which reason(s) specific music is recommended to them  o  o  o  o  o  o  It is important to make it more clear for which reason(s) specific music is recommended to them  o  o  o  o  o  o   \n \nDinnissen & Bauer (2023) \nControl    For users of streaming services, I am happy with the extent to which they can influence which music is in...  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree Not applicable / prefer not to answer Their general recommendations  o  o  o  o  o  o  Their personalized playlists  o  o  o  o  o  o      Comments or suggestions on these topics (optional): ________________________________________________________________ ________________________________________________________________ ________________________________________________________________  \nDinnissen & Bauer (2023) \nAdditional information      The following information will be used to describe your general demographics, without explicitly identifying you. You are free to skip any question.   What is your age? ◯ 18-25   ◯  26-35  ◯ 36-45  ◯ 46-55  ◯ 56-65  ◯ 66-75  ◯ 76+     How do you identify? ◯ Woman  ◯ Man   ◯ Non-binary   ◯ Prefer to self-describe: ______________    What is your nationality? ________________________________________________________________    How many years have you been active in the music industry?  o 1-5 years  o 6-10 years  o 10-15 years  o 16-20 years  o 21-25 years  o >25 years  o Not applicable     \nDinnissen & Bauer (2023) \nAre you affiliated with any record label (as an artist, employee, freelancer or in any other capacity)? It is possible to select more than 1 answer  ▢   Yes, major label (Sony, Universal, Warner)  ▢   No ▢   Yes, independent / indie label    ▢   Would prefer not to say    -  Please skip these questions if you are not a music artist -   How many albums* have you released in total (solo or as a member of an act/band), throughout your musical career?  *or equivalent in the form of EP's or singles o Some singles/1 EP  o 1 album*  o 2 albums*  o 3-5 albums*  o 6-10 albums*  o 10+ albums*    Which music genre(s) do you most identify with as an artist, if any? ________________________________________________________________   In which language(s) are your songs' lyrics written?  It is possible to select multiple answers  ▢ English  ▢ Other language(s): ___________________ ▢ My music has no lyrics   Which of these categories corresponds best to you as an artist and/or musical act? Newcomer / locally known  \u0000 Nationally known  \u0000 Internationally known  \u0000 Globally known  \u0000 Other: __________________________________________________"
    },
    {
        "title": "LP-MusicCaps: LLM-Based Pseudo Music Captioning.",
        "author": [
            "Seungheon Doh",
            "Keunwoo Choi",
            "Jongpil Lee",
            "Juhan Nam"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265311",
        "url": "https://doi.org/10.5281/zenodo.10265311",
        "ee": "https://zenodo.org/records/10265311/files/000048.pdf",
        "abstract": "Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing music-language datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We term it Large Language Model based Pseudo music caption dataset, shortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale music captioning dataset with various quantitative evaluation metrics used in the field of natural language processing as well as human evaluation. In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings. The results demonstrate that our proposed approach outperforms the supervised baseline model.",
        "zenodo_id": 10265311,
        "dblp_key": "conf/ismir/DohCLN23",
        "keywords": [
            "Automatic music captioning",
            "natural language descriptions",
            "music tracks",
            "significant potential",
            "enhancing understanding",
            "organizing musical data",
            "costly and time-consuming",
            "existing music-language datasets",
            "limited in size",
            "use of large language models"
        ],
        "content": "LP-MusicCaps: LLM-BASED PSEUDO MUSIC CAPTIONING\nSeungHeon Doh♭Keunwoo Choi♮Jongpil Lee♯Juhan Nam♭\n♭Graduate School of Culture Technology, KAIST, South Korea\n♮Gaudio Lab, Inc., South Korea\n♯Neutune, South Korea\n{seungheondoh, juhan.nam}@kaist.ac.kr, keunwoo@gaudiolab.com, jongpillee@neutune.com\nABSTRACT\nAutomatic music captioning, which generates natural lan-\nguage descriptions for given music tracks, holds signif-\nicant potential for enhancing the understanding and or-\nganization of large volumes of musical data. Despite its\nimportance, researchers face challenges due to the costly\nand time-consuming collection process of existing music-\nlanguage datasets, which are limited in size. To address\nthis data scarcity issue, we propose the use of large lan-\nguage models (LLMs) to artiﬁcially generate the descrip-\ntion sentences from large-scale tag datasets. This results\nin approximately 2.2M captions paired with 0.5M audio\nclips. We term it Large Language Model based Pseudo mu-\nsic caption dataset, shortly, LP-MusicCaps . We conduct\na systemic evaluation of the large-scale music captioning\ndataset with various quantitative evaluation metrics used in\nthe ﬁeld of natural language processing as well as human\nevaluation. In addition, we trained a transformer-based mu-\nsic captioning model with the dataset and evaluated it un-\nder zero-shot and transfer-learning settings. The results\ndemonstrate that our proposed approach outperforms the\nsupervised baseline model.1\n1. INTRODUCTION\nMusic captioning is a music information retrieval (MIR)\ntask of generating natural language descriptions of given\nmusic tracks. The text descriptions are usually sentences,\ndistinguishing the task from other music semantic under-\nstanding tasks such as music tagging. Recently, there have\nbeen some progress in music captioning including track-\nlevel captioning [1, 2] and playlist-level captioning [3–6].\nThese approaches usually utilize a deep encoder-decoder\nframework which is originally developed for neural ma-\nchine translation [7]. Choi et al. [3] used a pre-trained\nmusic tagging model as a music encoder and an RNN\n1Our dataset and codes are available at https://github.com/\nseungheondoh/lp-music-caps\n© SeungHeon Doh, Keunwoo Choi, Jongpil Lee, Juhan\nNam. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: SeungHeon Doh, Keunwoo Choi,\nJongpil Lee, Juhan Nam, “LP-MusicCaps: LLM-Based Pseudo Music\nCaptioning”, in Proc. of the 24th Int. Society for Music Information Re-\ntrieval Conf., Milan, Italy, 2023.\nFigure 1 . The generation process of pseudo captions by\nfeeding a large language model with instructions and\nmanually-annotated labels.\nlayer initialized with pre-trained word embeddings for text\ngeneration. Manco et al. [1] introduced a temporal atten-\ntion mechanism for alignment between audio and text by\npairing a pre-trained harmonic CNN encoder [8] with an\nLSTM layer. Gabbolini et al. [5] generated playlist titles\nand descriptions using pre-trained GPT-2 [9].\nCurrently, the primary challenge of track-level music\ncaptioning is the scarcity of large-scale public datasets.\nManco et al. [1] used private production music datasets.\nHuang et al. [10] also used a private dataset with 44M\nmusic-text pairs on YouTube, but this approach is hardly\nreproducible or affordable for other researchers. To address\nthis data issue, a community-driven data collection initia-\ntive has been proposed [11]. As of now, the only publicly\navailable dataset for track-level music captioning is Music-\nCaps [12], which includes high-quality music descriptions\nfrom ten musicians. However, it is limited to 5521 music-\ncaption pairs as it was originally created as an evaluation\nset for a text-prompt music generator.\nWith the scale of the aforementioned datasets, it re-\nmains difﬁcult to train a music captioning model success-\nfully. A workaround for this situation is to use music tag-\nging datasets and generate sentences with tag concatena-\ntion [2,13] or prompt template [14]. As relying on tagging\ndatasets, however, the tag-to-sentence approaches would\nhave the same limitation tagging datasets have. For exam-\nple, high false-negative rates of tagging datasets [15]. Tag-409ging datasets also has some typical issues text data have,\nfor example, synonyms, punctuation, and singular/plural\ninconsistencies. Without proper treatment, these can limit\nthe performance of the corresponding music captioning\nmodels.\nA potential solution is to use strong language models,\ni.e., large language models (LLMs). LLMs refer to the re-\ncent large-scale models with over a billion parameters that\nexhibit strong few-shot and zero-shot performance [9, 16].\nLarge language models are usually trained with text data\nfrom various domains such as Wikipedia, GitHub, chat\nlogs, medical articles, law articles, books, and crawled\nweb pages [17]. When successfully trained, they demon-\nstrate an understanding of words in various domains [9].\nThere have been similar and successful use cases of LLMs\nfor general audio understanding [18] and music genera-\ntion [19].\nMotivated by the recent success of LLMs, we propose\ncreating a music captioning dataset by applying LLMs\ncarefully to tagging datasets. Our goal is to obtain cap-\ntions that are i) semantically consistent with the provided\ntags, ii) grammatically correct, and iii) with clean and en-\nriched vocabulary. This dataset-level approach is rather\npragmatic than sophisticated; it alleviates the difﬁculty of\nmusic captioning tasks not by theory or model, but by\ndata. The aforementioned ambiguous aspects of the mu-\nsic captioning task are addressed by the powerful LLMs\nthat cost reasonably [20], considering the training cost mu-\nsic researchers would spend otherwise. Once the creation\nis complete, it is straightforward to train some music cap-\ntioning models by supervised learning.\nThere are some existing works in the pseudo-labeling\nusing language models. Huang et al. [19] introduced the\nMuLaMCap dataset, which consists of 400k music-caption\npairs generated using the large language model and the\nmusic-language joint embedding model. They utilized a\nlarge language model (LaMDA [21]) to generate 4M sen-\ntences using 150k song metadata as input in the format\nof{title} by {artist} . Then the text and music-\naudio joint embedding model, MuLan, calculates the sim-\nilarity between music and generated captions, annotating\npairs with high similarity [10]. However, it is not possi-\nble to reproduce or evaluate this work as the adopted lan-\nguage model as well as the ﬁnal music-audio embedding\nmodel are not publicly available. Moreover, using metadata\nhas some issues – a popularity-biased, limited coverage\nand a low reliability – as we discuss later in Section 2.1.\nWuet al. [22] introduce keyword-to-caption augmentation\n(K2C Aug) to generate captions based on the ground truth\ntags of audio clips in AudioSet. They used a pre-trained\nT5 model without any instruction. Finally, Mel et al. . [18]\nintroduce WavCaps, a 400k audio captioning dataset us-\ning ChatGPT [23]. However, previous approaches only re-\nported task performance and did not directly evaluate the\nquality of generated captions.\nWe propose a solution in this paper with three-fold key\ncontribution. First, we propose an LLM-based approach to\ngenerate a music captioning dataset, LP-MusicCaps . Sec-ond, we propose a systemic evaluation scheme for mu-\nsic captions generated by LLMs. Third, we demonstrate\nthat models trained on LP-MusicCaps perform well in both\nzero-shot and transfer learning scenarios, justifying the use\nof LLM-based pseudo-music captions.\n2. PSEUDO CAPTION GENERATION USING\nLARGE LANGUAGE MODELS\nIn this section, we introduce how music-speciﬁc pseudo\ncaptions are created using a large language model in the\nproposed method.\n2.1 Large Language Model for Data Generation\nWe ﬁrst take multi-label tags from existing music tagging\ndatasets. The list of tags are appended with a carefully writ-\nten task instruction as an input (prompt) to a large language\nmodel. The model then generates and returns sentences\nthat (may) describe the music in a way the task instruc-\ntion conditions. Table 1 shows examples of generated cap-\ntions according to multi-label tags and task instructions.\nFor the language model, we choose GPT-3.5 Turbo [23] for\nits strong performance in various tasks. During its training,\nit was ﬁrst trained with a large corpus and immense com-\nputing power, then ﬁne-tuned by reinforcement learning\nwith human feedback (RLHF) [24] for better interaction\nwith given instruction. As a result, GPT-3.5 Turbo demon-\nstrates state-of-the-art zero-shot abilities in understanding,\nreasoning, and generating human-like responses to natural\nlanguage inputs.\nSince LLMs contain a wide range of information, music\ncaptions may be generated based on some famous musical\nentities such as the artist name or album name. However,\nLLMs may generate inaccurate text in a conﬁdent tone\nwhich is hard to detect without ground truth. This issue,\nknown as hallucination, can be a fun aspect when using\nLLMs for creative purposes [25]. However, hallucination\nshould be avoided in an application like ours as the result-\ning captions should be factual. Therefore, we do not use\nany metadata unlike a previous work [19]. We also added\na question to measure hallucination in the proposed evalu-\nation scheme.\n2.2 Task Instruction Design\nOur proposed caption generation follows the formulation:\n˜ycap=fLLM(ytag,i), whereytagand˜ycaprefer to the multi-\nlabel tag and the generated caption, respectively, and iis\nthe task instruction provided. Given that the output can\nvary based on the task instruction, even with the same\nmodel and input, task instructions become a crucial as-\npect of data generation. Therefore, we deﬁne four different\ntasks and generate captions accordingly.\nWriting :Write a song description sentence including the\nfollowing attributes. {input tags}\nSummary :Write a single sentence that summarizes a song\nwith the following attributes. Don’t write the artist name or\nalbum name. {input tags}Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n410Input tags\nvideo game theme, no singer, instrumental, analog sounding, small keyboard, beatboxing, playful, cheerful, groovy\nResult of \"Writing\" Instruction\nThis instrumental track has a joyful and playful vibe, perfect for a video game theme. With no singer, the analog-sounding music features a\nsmall keyboard and beatboxing, creating a groovy and cheerful atmosphere.\nResult of \"Summary\" Instruction\nA cheerful and groovy instrumental track with playful beatboxing and an analog sound, featuring a small keyboard and video game-inspired\ntheme.\nResult of \"Paraphrase\" Instruction\nThis upbeat instrumental tune captures the essence of a fun-ﬁlled video game theme, incorporating analog-sounding beats,\nplayful beatboxing rhythms, and a groovy small keyboard melody, all without the need for a singer.\nResult of \"Attribute Prediction\" Instruction\n{\"new_attribute\": [\"8-bit sound\", \"chiptune style\", \"retro vibe\"],\n\"description\": \"This instrumental tune is straight out of a video game with its analog sounding melodies and small keyboard tinkles.\nBeatboxing adds a playful element to the groovy, cheerful vibe. Reminiscent of classic 8-bit sound and chiptune style, this retro vibe\nis sure to put a smile on your face.\"}\nGround Truth\nThis is a video game theme cover. The theme belongs to the Super Mario franchise. The main theme is being played on an analog sounding\nsmall keyboard. There is an added rhythmic background of beatboxing in this version. The atmosphere is playful. This piece could be used\nin the background of arcade gaming social media content.\nTable 1 . An example of generated captions from MusicCaps dataset.\nParaphrase :Write a song description sentence including\nthe following attributes. Creative paraphrasing is accept-\nable.{input tags}\nAttribute Prediction :Write the answer as a Python dic-\ntionary with new_attribute and description as keys. For\nnew_attribute, write new attributes that show high co-\noccurrence with the following attributes. For description,\nwrite a song description sentence including the following\nattributes and new attributes. {input tags}\nIn every instruction, we add ‘include / with the follow-\ning attributes’ to prevent hallucination. The “Writing” task\ninstruction is a simple prompt that uses tags to generate a\nsentence. The “Summary” task instruction aims to com-\npress information into a short length. The “Paraphrase”\ntask instruction expands the vocabulary. Finally, the “At-\ntribute Prediction” task instruction predicts new tags based\non tag co-occurrence in large corpora (i.e. the training data\nof GPT-3.5 Turbo), which is expected to address the is-\nsue of high false-negative rates in existing tagging datasets\nwhile mitigating the risk of hallucination. In this instruc-\ntion, ‘new attributes’ exists to bridge the description and\nthe input, and we only use the ‘description’ as caption.\n3. EV ALUATION OF PSEUDO CAPTIONS\nIt is crucial to ensure the quality of generated captions, es-\npecially since they are supposed to be used as ground truth.\nIn this section, we introduce a holistic evaluation scheme\nthat includes objective and subjective assessment – and its\nresult on the captions from the proposed method.\n3.1 Objective Evaluation\nWe conduct evaluation on the generated captions using\nMusicCaps dataset [12]. It has audio ( x), tag list ( ytag), and\nground truth caption ( ycap). The pseudo captions ( ˜ycap) are\ngenerated with four pre-deﬁned instructions as explainedin Section 2.2 for all items in the evaluation split. During\nthe evaluation, the generated captions are compared to the\nground truth captions with respect to n-gram, neural met-\nrics. We also report diversity metrics.\nFollowing the previous work [5], we measure four n-\ngram metrics [26–28]: BLEU1 to 4 (B1, B2, B3, B4), ME-\nTEOR (M), and ROUGE-L (R-L). They are all based on\nn-gram precision and recall between the ground truth and\ngenerated captions. These metrics capture different aspects\nof the caption quality. BLEU and METEOR focus on n-\ngram overlap between the generated and ground truth cap-\ntions, while ROUGE-L measures the longest common sub-\nsequence between the two.\nIn addition, we use BERT-Score (BERT-S) based on\npre-trained BERT embeddings to represent and match the\ntokens in the ground truth with respect to the generated\ncaption [29]. By computing the similarity between the\nBERT embeddings of each token, BERT-Score can better\ncapture the semantic similarity between the generated and\nground truth captions than n-gram metrics; as it is more ro-\nbust to synonyms, paraphrasing, and word order variations.\nFinally, we evaluate the diversity of the generated cap-\ntions by measuring how many different words are used.\nnovelvindicates the percentage of new vocabulary in gen-\nerated captions that are not among the training vocabulary.\nVocab is the number of unique words used in all the gener-\nated captions. It is worth noting that diversity metrics are\ngenerally considered as subsidiaries and do not capture the\noverall quality of the generated captions.\n3.2 Subjective Evaluation\nFollowing the previous work [12], we set up an A-vs-B hu-\nman rating task, in which a participant is presented with a\n10-second single music clip and two text descriptions. We\nrandomly selected 240 music samples from the MusicCaps\nevaluation dataset. Since the research goal is to generateProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n411Supervised Metrics Diversity Metrics Length\nMethods LM Params B1 ↑ B2↑ B3↑ B4↑ M↑ R-L↑BERT-S↑ V ocab↑Novel v↑ Avg.Token\nBaseline\nTag Concat [2, 13] - - 20.25 13.57 8.64 5.42 23.24 19.52 86.24 3506 46.92 20.6 ±11.2\nTemplate [14] - - 25.41 16.15 10.00 6.15 25.57 21.36 87.92 3507 46.93 25.6 ±11.2\nK2C Aug. [22] T5 220M 6.07 3.01 1.58 0.85 14.23 17.92 86.33 3760 67.66 14.7 ±5.1\nProposed Instruction\nWriting GPT3.5 175B+ 36.84 19.85 11.37 6.74 31.44 25.36 89.26 5521 56.17 44.4 ±17.3\nSummary GPT3.5 175B+ 26.12 14.58 8.80 5.52 27.58 25.83 89.88 4198 49.52 28.6 ±10.7\nPharapase GPT3.5 175B+ 36.51 18.73 10.33 5.87 30.36 23.40 88.71 6165 59.95 47.9 ±18.7\nAttribute Prediction GPT3.5 175B+ 35.26 18.16 9.69 5.41 34.09 23.19 88.56 6995 63.16 66.2 ±21.6\nTable 2 . Performance of existing pseudo caption generation methods and the proposed method. LM stand for the language\nmodel. Avg.Token stand for the average number of token per caption.\nFigure 2 . A-vs-B test results. Each method is compared to ground truth in terms of having more true positives and fewer\nfalse positives. The proposed methods (b, c, d, e) show comparable win+tie performance to ground truth.\nmusic captions that can be used as pseudo-ground truth,\none description is always ﬁxed to the ground truth and the\nother is chosen from 5 types of generated captions includ-\ning the K2C Augmentation [22] and the four proposed in-\nstruction methods. This yields up to 1200 (= 240 x 5) ques-\ntions. We hired 24 participants who are music researchers\nor professionals in the music industry. Each of them rated\n20 randomly selected questions. As a result, we collected a\ntotal of 480 ratings. The rater was asked to evaluate caption\nquality on two different aspects: (Q1) More True Positive :\nwhich caption describes the music with more accurate at-\ntributes? (Q2) Less False Positive : which caption describes\nthe music less wrong? For example, if a method produces\nlong and diverse sentences with many music attributes, it\nmay be advantageous for Q1 but disadvantageous for Q2.\nConversely, if a method conservatively produces short sen-\ntences with few music attributes, it may be advantageous\nfor Q2 but disadvantageous for Q1. We determine the rank-\ning of conditions by counting the number of wins, ties, and\nloses in the pairwise tests.\n3.3 Results\nWe compare our LLM-based caption generation with two\ntemplate-based methods (tag concatenation, prompt tem-\nplate2) and K2C augmentation [22]. In Table 2, we present\nthe captioning result for MusicCaps [12] evaluation set.\nWhen comparing our proposed method with existing meth-\n2Template example: the music is characterized by {input tags}ods, we observe signiﬁcant differences in n-gram metrics.\nThis is because the tag concatenation fails to complete the\nsentence structure. In the case of K2C Augmentation, due\nto the absence of instruction, the input tag is excluded from\nthe generated caption, or a sentence unrelated to the song\ndescription sentence is created. In contrast, the template-\nbased model shows improved performance as the musi-\ncal context exists in the template. We next consider diver-\nsity metric with BERT-Score. Our proposed method shows\nhigher values in BERT-Score while generating diverse vo-\ncabularies. This indicates that the newly created vocabu-\nlary does not harm the music semantics.\nComparing within the proposed different task instruc-\ntions, we can observe that each instruction performs a dif-\nferent role. “Writing” shows a high n-gram performance\nas it faithfully uses input tags to generate captions. “Sum-\nmary” has the smallest average number of tokens due to\nits compression of information, but it shows competitive\nperformance in ROUGE-L which is specialized to summa-\nrizing, as well as the highest BERT-Score. “Paraphrase”\ngenerates many synonyms, resulting in a large vocabulary\nsize and the use of novel vocabulary. “Attribute Prediction”\npredicts new tags based on the co-occurrence of tags. This\ninstruction shows lower performance in BLEU but compet-\nitive results in METEOR, which utilizes a thesaurus, such\nas WordNet, to consider the accuracy scores of words with\nsimilar meanings, indicating that newly predicted tags have\nsimilar semantic with ground truth.\nFigure 2 shows the subjective A-vs-B test results. EachProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n412Dataset # item Duration (h) C/A Avg. Token\nGeneral Audio Domain\nAudioCaps [30] 51k 144.9 1 9.0 ±N/A\nLAION-Audio [22] 630k 4325.4 1-2 N/A\nWavCaps [18] 403k 7568.9 1 7.8 ±N/A\nMusic Domain\nMusicCaps [12] 6k 15.3 1 48.9 ±17.3\nMuLaMCap∗[19] 393k 1091.0 12 N/A\nLP-MusicCaps-MC 6k 15.3 4 44.9 ±21.3\nLP-MusicCaps-MTT 22k 180.3 4 24.8 ±13.6\nLP-MusicCaps-MSD 514k 4283.1 4 37.3 ±26.8\nTable 3 . Comparison of audio-caption pair datasets. C/A\nstands for the number of caption per audio. *Although we\ninclude MuLaMCap in the table for comparison, it is not\npublicly accessible.\nmethod is compared to the ground truth in terms of having\nmore true positives (Q1) and fewer false positives (Q2). For\nthe ﬁrst question, compared to the baseline K2C augmen-\ntation, the proposed methods using the instructions show\nan overwhelmingly higher win+tie score. This indicates\nthe importance of music-speciﬁc instructions when utiliz-\ning LLM. In particular, “Paraphrase” and “Attribute Pre-\ndiction” achieve high winscores by incorporating new in-\nformation that is different from the existing vocabulary. In\nthe second question, all caption generation methods except\n“Attribute Prediction” show higher win+tie scores than\nlose scores. This advocates the trustworthiness of LLM-\nbased caption generation as it shows a similar or less false-\npositive rate to the ground truth. With its longest aver-\nage length, “Attribute Prediction” turns out to be ‘too cre-\native’ and shows a slightly higher false-positive rate than\nthe ground truth.\n4. DATASET: LP-MusicCaps\nBased on the proposed pseudo caption generation method,\nwe introduce LP-MusicCaps, an LLM-based Pseudo mu-\nsic caption dataset. We construct the music-to-caption pairs\nusing three existing multi-label tag datasets and four task\ninstructions. The data sources are MusicCaps [12], Mag-\nnatagtune [31], and Million Song Dataset [32] ECALS\nsubset [13]. We respectively refer to them as MC, MTT,\nand MSD. MC contains 5,521 music examples,3each of\nwhich is labeled with 13,219 unique aspects written by mu-\nsic experts. MTT [31] consists of 26k music clips from\n5,223 unique songs including genre, instrument, vocal,\nmood, perceptual tempo, origin, and sonority features. We\nused the full 188 tag vocabulary and did not generate cap-\ntions for tracks that do not have associated tags (decreased\nto 22k). MSD consists of 0.52 million 30-second clips and\n1054 tag vocabulary [13]. The tag vocabulary covers var-\nious categories including genre, style, instrument, vocal,\nmood, theme, and culture. Each dataset uses an average of\n10.7 / 3.3 / 10.2 labels per music clip for generating pseudo\ncaptions, respectively.\nTable 3 provides a comparison of statistics between\nthe LP-MusicCaps family and other audio-caption pair\n3We only use 5495 out of the total due to the loss of 26 data samples.\nFigure 3 . A cross-modal encoder-decoder architecture.\ndatasets. When comparing the two domains, Audio-\nCaps [30] and MusicCaps have high-quality human anno-\ntated captions, but they have fewer captions with shorter\naudio duration. When comparing large-scale datasets, the\nmusic domain lacks available datasets compared to the\ngeneral audio domain (such as LAION-Audio [22] and\nWavCaps [18]). Although MuLaMCap has an overwhelm-\ning amount of annotated captions, it is not publicly avail-\nable. In contrast, LM-MusicCaps is publicly accessible\nand provided with various scales. LP-MusicCaps-MC has\na similar caption length to manually written captions\nwhile having four times more captions per audio. LP-\nMusicCaps-MTT is a medium-sized dataset with audio\ndownload link, and LP-MusicCaps-MSD has the largest\naudio duration among various captions in the music do-\nmain caption dataset.\n5. AUTOMATIC MUSIC CAPTIONING\nWe trained a music captioning model and evaluated it un-\nder zero-shot and transfer-learning settings. This section\nreports the experimental results.\n5.1 Encoder-Decoder Model\nWe used a cross-modal encoder-decoder transformer ar-\nchitecture that has achieved outstanding results on various\nnatural language processing tasks [33], lyrics interpreta-\ntion [34], and speech recognition [35], as shown in Fig-\nure 3. Similar to Whisper [35], the encoder takes a log-\nmel spectrogram with six convolution layers with a ﬁlter\nwidth of 3 and the GELU [36] activation function. With\nthe exception of the ﬁrst layer, each convolution layer has\na stride of two. The output of the convolution layers is\ncombined with the sinusoidal position encoding and then\nprocessed by the encoder transformer blocks. Following\nthe BART basearchitecture, our encoder and decoder both\nhave 768 widths and 6 transformer blocks. The decoderProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n413Supervised Metrics Diversity Metrics Length\nModel B1 ↑ B2↑ B3↑B4↑ M↑ R-L↑BERT-S↑ V ocab↑Novel v↑Novel c↑ Avg.Token\nBaseline\nSupervised Model 28.51 13.76 7.59 4.79 20.62 19.22 87.05 2240 0.54 69.00 46.7 ±16.5\nZeroshot Captioning\nTag Concat [2, 13] 4.33 0.84 0.26 0.00 3.10 2.01 79.30 802 46.38 100.00 23.8 ±12.1\nTemplate [14] 7.22 1.58 0.46 0.00 5.28 6.81 81.69 787 45.24 100.00 25.8 ±12.4\nK2C-Aug [22] 7.67 2.10 0.49 0.10 7.94 11.37 82.99 2718 81.97 100.00 19.9 ±7.6\nLP-MusicCaps (Ours) 19.77 6.70 2.17 0.79 12.88 13.03 84.51 1686 47.21 100.00 45.3 ±28.0\nTansfer Learning\nTag Concat [2, 13] 28.65 14.68 8.68 5.82 21.88 21.31 87.67 1637 3.30 96.07 41.8 ±14.3\nTemplate [14] 28.41 14.49 8.59 5.78 21.88 21.25 87.72 1545 3.62 96.77 41.1 ±13.2\nK2C-Aug [22] 29.50 14.99 8.70 5.73 21.97 20.92 87.50 2259 1.42 84.95 44.1 ±15.0\nLP-MusicCaps (Ours) 29.09 14.87 8.93 6.05 22.39 21.49 87.78 1695 1.47 96.06 42.5 ±14.3\nTable 4 . Music captioning results on the MusicCaps eval-set. Avg.Token stands for the average number of token per caption.\nprocesses tokenized text captions using transformer blocks\nwith a multi-head attention module that includes a mask\nto hide future tokens for causality. The music and caption\nrepresentations are fed into the cross-modal attention layer,\nand the head of the language model in the decoder predicts\nthe next token autoregressively using the cross-entropy\nloss, formulated as: L=−/summationtextT\nt=1logpθ(yt|y1:t−1,x)\nwherexis the paired audio clip and ytis the ground truth\ntoken at time tin a caption with length T.\n5.2 Experimental Setup\nTo evaluate the impact of the proposed dataset on the music\ncaptioning task, we compare a supervised model trained on\nthe MusicCaps [12] training split and a pre-trained model\ntrained on an LP-MusicCaps-MSD dataset. For the pre-\ntrained model, we perform both a zero-shot captioning task\nthat does not use any MusicCaps [12] dataset and a ﬁne-\ntuning task that updates the model using MusicCaps [12]\ntraining split. For comparison with other pseudo caption\ngeneration methods, we report results on baseline models\ntrained with the same architecture and amount of audio,\nbut different pseudo captions. In addition to all the metrics\nwe used in Section 3.1, we compute Novelc, the percentage\nof generated captions that were not present in the training\nset [37]. It measures whether the captioning model is sim-\nply copying the training data or not.\nFor all the experiments, the input of the encoder is a\n10-second audio signal at 16 kHz sampling rate. It is con-\nverted to a log-scaled mel spectrogram with 128 mel bins,\n1024-point FFT with a hann window, and a hop size of\n10 ms. All models are optimized using AdamW with a\nlearning rate of 1e-4. We use a cosine learning rate decay\nto zero after a warmup over the ﬁrst 1000 updates. For the\npre-training dataset, we use 256 batch-size and the models\nare trained for 32,768 updates. We adopt a balanced sam-\npling [38], which uniformly samples an anchor tag ﬁrst and\nthen selects an annotated item. For supervised and transfer\nlearning, we use a 64 batch size, 100 epochs. We use beam\nsearch with 5 beams for the inference of all models.\n5.3 Results\nWhen comparing within zero-shot captioning models, the\nmodel trained on the proposed LP-MusicCaps datasetshows a strong performance in general. The model using\ntag concatenation shows the lowest performance as it fails\nto generate musical sentences. In case of the model using a\nprompt template, it demonstrates a slightly higher BERT-\nScore, while still exhibiting poor performance in terms of\nn-gram metrics due to its limited vocabulary. The model\nusing K2C augmentation outperforms the other two meth-\nods but still falls short due to its lack of a musical context.\nIn general, zero-shot models does not perform as well as\nthe supervised baseline in most of the metrics with few ex-\nceptions.\nAmong the transfer captioning models, the model with\nLP-MusicCaps pre-training achieves strong performance\noverall by winning in the BERT-Score and most of the\nn-gram metrics. It is noteworthy that our proposed model\nshows a meaningful increase in BERT-Score compared to\nthe supervised model. This improvement is likely a result\nof successful semantic understanding rather than word-to-\nword matching. Moreover, by the improvement of Novel c,\nthe LP-MusicCaps model demonstrates that it can generate\nnew captions instead of repeating the phrases in the train-\ning dataset. This advantage is observed in both the zero-\nshot and supervised tasks in transfer learning models.\n6. CONCLUSION\nWe proposed a tag-to-pseudo caption generation approach\nwith large language models to address the data scarcity is-\nsue in automatic music captioning. We conducted a sys-\ntemic evaluation of the LLM-based augmentation, result-\ning in the creation of the LP-MusicCaps dataset, a large-\nscale pseudo-music caption dataset. We also trained a mu-\nsic captioning model with LP-MusicCaps and showed im-\nproved generalization. Our proposed approach has the po-\ntential to signiﬁcantly reduce the cost and time required\nfor music-language dataset collection and facilitate further\nresearch in the ﬁeld of connecting music and language,\nincluding representation learning, captioning, and gener-\nation. However, further collaboration with the community\nand human evaluation is essential to enhance the quality\nand accuracy of the generated captions. Additionally, we\nbelieve that exploring the use of LLMs for other topics un-\nder music information retrieval and music recommenda-\ntion could lead to novel and exciting applications.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4147. REFERENCES\n[1] I. Manco, E. Benetos, E. Quinton, and G. Fazekas,\n“Muscaps: Generating captions for music audio,” in\nInternational Joint Conference on Neural Networks\n(IJCNN) . IEEE, 2021.\n[2] T. Cai, M. I. Mandel, and D. He, “Music autotagging\nas captioning,” in Proceedings of the 1st Workshop on\nNLP for Music and Audio (NLP4MusA) , 2020.\n[3] K. Choi, G. Fazekas, B. McFee, K. Cho, and M. San-\ndler, “Towards music captioning: Generating music\nplaylist descriptions,” in International Society for Mu-\nsic Information Retrieval Conference (ISMIR), Late-\nBreaking/Demo , 2016.\n[4] S. Doh, J. Lee, and J. Nam, “Music playlist title genera-\ntion: A machine-translation approach,” in Proceedings\nof the 2nd Workshop on NLP for Music and Spoken Au-\ndio (NLP4MuSA) , 2021.\n[5] G. Gabbolini, R. Hennequin, and E. Epure, “Data-\nefﬁcient playlist captioning with musical and linguis-\ntic knowledge,” in Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , 2022.\n[6] H. Kim, S. Doh, J. Lee, and J. Nam, “Music playlist\ntitle generation using artist information,” in Proceed-\nings of the AAAI-23 Workshop on Creative AI Across\nModalities , 2023.\n[7] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine\ntranslation by jointly learning to align and translate,” in\nProceedings of the International Conference on Learn-\ning Representations (ICLR) , 2014.\n[8] M. Won, S. Chun, O. Nieto, and X. Serrc, “Data-driven\nharmonic ﬁlters for audio representation learning,” in\nICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\n2020.\n[9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sas-\ntry, A. Askell et al. , “Language models are few-shot\nlearners,” in Proceedings of the Advances in neural in-\nformation processing systems (NeurIPS) , 2020.\n[10] Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y . Li, and\nD. P. Ellis, “MuLan: A joint embedding of music audio\nand natural language,” in International Conference on\nMusic Information Retrieval (ISMIR) , 2022.\n[11] I. Manco, B. Weck, P. Tovstogan, M. Won, and D. Bog-\ndanov, “Song describer: a platform for collecting tex-\ntual descriptions of music recordings,” in International\nConference on Music Information Retrieval (ISMIR),\nLate-Breaking/Demo session , 2022.\n[12] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel,\nM. Verzetti, A. Caillon, Q. Huang, A. Jansen,A. Roberts, M. Tagliasacchi et al. , “MusicLM:\nGenerating music from text,” arXiv preprint\narXiv:2301.11325 , 2023.\n[13] S. Doh, M. Won, K. Choi, and J. Nam, “Toward uni-\nversal text-to-music retrieval,” in IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2023.\n[14] T. Chen, Y . Xie, S. Zhang, S. Huang, H. Zhou, and\nJ. Li, “Learning music sequence representation from\ntext supervision,” in IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2022.\n[15] K. Choi, G. Fazekas, K. Cho, and M. Sandler, “The\neffects of noisy labels on deep convolutional neural\nnetworks for music tagging,” IEEE Transactions on\nEmerging Topics in Computational Intelligence , 2018.\n[16] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer,” J. Mach. Learn. Res. , vol. 21, no. 1, jan\n2020.\n[17] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe,\nC. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,\nS. Presser, and C. Leahy, “The pile: An 800gb dataset\nof diverse text for language modeling,” 2020.\n[18] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao,\nM. D. Plumbley, Y . Zou, and W. Wang, “WavCaps:\nA ChatGPT-assisted weakly-labelled audio caption-\ning dataset for audio-language multimodal research,”\narXiv preprint arXiv:2303.17395 , 2023.\n[19] Q. Huang, D. S. Park, T. Wang, T. I. Denk,\nA. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu,\nC. Frank et al. , “Noise2music: Text-conditioned mu-\nsic generation with diffusion models,” arXiv preprint\narXiv:2302.03917 , 2023.\n[20] F. Gilardi, M. Alizadeh, and M. Kubli, “ChatGPT\noutperforms crowd-workers for text-annotation tasks,”\narXiv preprint arXiv:2303.15056 , 2023.\n[21] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer,\nA. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker,\nY . Du et al. , “Lamda: Language models for dialog ap-\nplications,” arXiv preprint arXiv:2201.08239 , 2022.\n[22] Y . Wu, K. Chen, T. Zhang, Y . Hui, T. Berg-Kirkpatrick,\nand S. Dubnov, “Large-scale contrastive language-\naudio pretraining with feature fusion and keyword-\nto-caption augmentation,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) , 2023.\n[23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wain-\nwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray et al. , “Training language models to follow\ninstructions with human feedback,” in Proceedings ofProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n415the Advances in neural information processing systems\n(NeurIPS) , 2022.\n[24] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg,\nand D. Amodei, “Deep reinforcement learning from\nhuman preferences,” in Proceedings of the Advances\nin neural information processing systems (NeurIPS) ,\n2017.\n[25] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii,\nY . J. Bang, A. Madotto, and P. Fung, “Survey of hal-\nlucination in natural language generation,” ACM Com-\nputing Surveys , 2023.\n[26] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu:\na method for automatic evaluation of machine trans-\nlation,” in Proceedings of the 40th annual meeting of\nthe Association for Computational Linguistics (ACL) ,\n2002.\n[27] S. Banerjee and A. Lavie, “Meteor: An automatic met-\nric for mt evaluation with improved correlation with\nhuman judgments,” in Proceedings of the ACL work-\nshop on intrinsic and extrinsic evaluation measures for\nmachine translation and/or summarization , 2005.\n[28] C.-Y . Lin, “Rouge: A package for automatic evalua-\ntion of summaries,” in Text summarization branches\nout, 2004.\n[29] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and\nY . Artzi, “Bertscore: Evaluating text generation with\nbert,” in International Conference on Learning Repre-\nsentations (ICLR) , 2020.\n[30] C. D. Kim, B. Kim, H. Lee, and G. Kim, “AudioCaps:\nGenerating captions for audios in the wild,” in Pro-\nceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies , 2019.\n[31] E. Law, K. West, M. I. Mandel, M. Bay, and J. S.\nDownie, “Evaluation of algorithms using games: The\ncase of music tagging.” in International Conference on\nMusic Information Retrieval (ISMIR) , 2009.\n[32] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and\nP. Lamere, “The million song dataset,” in International\nConference on Music Information Retrieval (ISMIR) ,\n2011.\n[33] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V . Stoyanov, and L. Zettlemoyer,\n“Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension,” in Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL) , 2020.\n[34] Y . Zhang, J. Jiang, G. Xia, and S. Dixon, “Interpret-\ning song lyrics with an audio-informed pre-trained lan-\nguage model,” in International Conference on Music\nInformation Retrieval (ISMIR) , 2022.[35] A. Radford, J. W. Kim, T. Xu, G. Brockman,\nC. McLeavey, and I. Sutskever, “Robust speech recog-\nnition via large-scale weak supervision,” arXiv preprint\narXiv:2212.04356 , 2022.\n[36] D. Hendrycks and K. Gimpel, “Gaussian error lin-\near units (GELUs),” arXiv preprint arXiv:1606.08415 ,\n2016.\n[37] M. Stefanini, M. Cornia, L. Baraldi, S. Cascianelli,\nG. Fiameni, and R. Cucchiara, “From show to tell:\na survey on deep learning-based image captioning,”\nIEEE transactions on pattern analysis and machine in-\ntelligence , 2022.\n[38] M. Won, S. Oramas, O. Nieto, F. Gouyon, and X. Serra,\n“Multimodal metric learning for tag-based music re-\ntrieval,” in ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2021.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n416"
    },
    {
        "title": "Collaborative Song Dataset (CoSoD): An Annotated Dataset of Multi-Artist Collaborations in Popular Music.",
        "author": [
            "Michèle Duguay",
            "Kate Mancey",
            "Johanna Devaney"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265225",
        "url": "https://doi.org/10.5281/zenodo.10265225",
        "ee": "https://zenodo.org/records/10265225/files/000007.pdf",
        "abstract": "The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist collaborations from the 2010–2019 Billboard \"Hot 100\" year-end charts. The corpus is annotated with formal sections, aspects of vocal production (including reverberation, layering, panning, and gender of the performers), and relevant metadata. CoSoD complements other popular music datasets by focusing exclusively on musical collaborations between independent acts. In addition to facilitating the study of song form and vocal production, CoSoD allows for the in-depth study of gender as it relates to various timbral, pitch, and formal parameters in musical collaborations. In this paper, we detail the contents of the dataset and outline the annotation process. We also present an experiment using CoSoD that examines how the use of reverberation, layering, and panning are related to the gender of the artist. In this experiment, we find that men's voices are on average treated with less reverberation and occupy a more narrow position in the stereo mix than women's voices.",
        "zenodo_id": 10265225,
        "dblp_key": "conf/ismir/DuguayMD23",
        "keywords": [
            "Collaborative Song Dataset (CoSoD)",
            "331 multi-artist collaborations",
            "Billboard Hot 100 year-end charts",
            "formal sections",
            "vocal production",
            "reverberation",
            "layering",
            "panning",
            "gender of the performers",
            "metadata"
        ],
        "content": "COLLABORATIVE SONG DATASET (COSOD): AN ANNOTATED\nDATASET OF MULTI-ARTIST COLLABORATIONS IN POPULAR MUSIC\nMichèle Duguay1Kate Mancey1Johanna Devaney2\n1Department of Music, Harvard University\n2Brooklyn College and Graduate Center, City University of New York\nmduguay@fas.harvard.edu, johanna.devaney@brooklyn.cuny.edu\nABSTRACT\nThe Collaborative Song Dataset (CoSoD) is a corpus of\n331 multi-artist collaborations from the 2010–2019 Bill-\nboard “Hot 100” year-end charts. The corpus is anno-\ntated with formal sections, aspects of vocal production (in-\ncluding reverberation, layering, panning, and gender of the\nperformers), and relevant metadata. CoSoD complements\nother popular music datasets by focusing exclusively on\nmusical collaborations between independent acts. In addi-\ntion to facilitating the study of song form and vocal pro-\nduction, CoSoD allows for the in-depth study of gender\nas it relates to various timbral, pitch, and formal parame-\nters in musical collaborations. In this paper, we detail the\ncontents of the dataset and outline the annotation process.\nWe also present an experiment using CoSoD that examines\nhow the use of reverberation, layering, and panning are re-\nlated to the gender of the artist. In this experiment, we ﬁnd\nthat men’s voices are on average treated with less reverber-\nation and occupy a more narrow position in the stereo mix\nthan women’s voices.\n1. INTRODUCTION\nAs far back as the 1960s, Billboard charts have featured\ncollaborations between independent acts. In recent years,\nhowever, the number of songs featuring a collaboration be-\ntween artists has skyrocketed [1]. Part of this is due to the\nrising popularity of hip-hop in the 1980s, in which col-\nlaboration between different artists is a ﬁxture. The 1986\nversion of “Walk This Way” by Aerosmith and Run DMC\nis an oft-cited example of such a collaboration. As Rose\nnotes, the success of a collaboration between a hip-hop\ngroup (Run DMC) and a rock group (Aerosmith) “brought\n[hip-hop’s] strategies of intertextuality into the commercial\nspotlight” [2, p. 51–52]. The 1990 success of “She Ain’t\nWorth It” by Glenn Medeiros ft. Bobby Brown marked the\nﬁrst time a sung and rapped collaboration reached #1 on\n© M. Duguay, K. Mancey, and J. Devaney. Licensed un-\nder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: M. Duguay, K. Mancey, and J. Devaney, “Collabora-\ntive Song Dataset (CoSoD): An annotated dataset of multi-artist collab-\norations in popular music”, in Proc. of the 24th Int. Society for Music\nInformation Retrieval Conf., Milan, Italy, 2023.Billboard ’s “Hot 100.” Molanphy notes that during this pe-\nriod, multi-artist collaborations crystallized into two differ-\nent frameworks: the “featured bridge rapper,” and the “fea-\ntured hook singer” [3]. Subsequently, tracks with one or\nmore guest artist(s) have become a mainstay on the charts.\nBy 2021, over a third (39%) of the songs in Billboard ’s\n“Hot 100” year-end chart credited more than one artist.\nConsider for instance “Save Your Tears,” by singers The\nWeeknd & Ariana Grande, which occupied second place\non the chart. A solo version of the song originally ap-\npeared on The Weeknd’s album After Hours (2020). While\nthis version achieved commercial success, the remix with\nAriana Grande became a #1 single on the Billboard “Top\n100” in May 2021 and became the longest-charting col-\nlaboration in Billboard “Hot 100” history. In the remix,\nGrande performs approximately half of the vocals, trans-\nforming the solo song into a dialogue between two charac-\nters. The collaboration between the two artists is responsi-\nble for the popularity of the remix, inviting both Grande’s\nand The Weeknd’s fans to stream, buy, and otherwise en-\ngage with the song. Several musicological studies have ex-\namined this relationship between collaborative songs and\ncommercial success [4–6]. Other work has provided in-\ndepth explorations of the musical characteristics of collab-\norative songs, with a particular focus on hip-hop [7–9].\nGiven the popularity of multi-artist collaborations, a\nmore systematic exploration of their musical features is\nwarranted. In this paper, we introduce the Collaborative\nSong Dataset (CoSoD), an annotated dataset that facilitates\nthe study of various musical features in multi-artist collab-\norations. CoSoD provides metadata and analytical data for\n331 multi-artist collaborations appearing on the Billboard\n“Hot 100” year-end charts between 2010 and 2019. The\ndataset also provides timed annotations on the song’s for-\nmal structure, artists’ gender, vocal delivery and pitch, and\nvocal production (reverberation, panning, and layering).\nAs detailed in Section 2, the range of features included in\nthe dataset makes it more broadly applicable for MIR re-\nsearch tasks. These include structural segmentation, vocal\nmixing, automatic music production, and examinations of\ngender in popular music. After outlining the contents of\nthe dataset and the annotation methodology in Section 3,\nwe present an experiment in Section 4 that examines the\nrelationship between vocal production parameters and the\ngender of the performer in a subset of CoSoD.712. RELATED WORK\nCoSoD complements the growing list of annotated datasets\nthat provide information on song structure in various pop-\nular music genres, e.g., [10–17], and is the ﬁrst dataset to\nexclusively contain data on collaborative songs between\nindependent acts. It can thus be used for training and eval-\nuating structural segmentation tasks and for studying the\nspeciﬁc structural characteristics of collaborative songs.\nCoSoD also complements existing datasets for multi-track\nmixing/analysis [18–23] and vocal analysis [24–26] by\nproviding analytical annotations on the treatment of the\nvoice in a mix.\nIn recent years, several studies have proposed tools and\nmethods to automate the mixing of multi-track recordings\n[27, 28]. Such automatic production methods have various\nartistic and creative applications. One framework has been\nsuggested to remix early jazz recordings, which are pre-\nprocessed using source separation then remixed with auto-\nmatic production tools [29]. [30] proposes a prototype for\nan automatic DJ mixing system allowing for cross-fading\nvia beat and tempo adjustment between songs. Studies on\nautomatic mixing can be enhanced by knowledge of com-\nmon mixing practices for speciﬁc instruments or sound\nsources. For instance, one study uses mixing practices that\nare consistent between mixing engineers to create a model\nthat automatically mixes multiple drum tracks [31]. By fo-\ncusing on vocals, which are a salient component of the mix\nin popular music [32], CoSoD provides a complementary\napproach to these studies on automated production. By\nproviding annotations based on close listening of speciﬁc\nvocal mixing parameters in the different formal sections of\na song, the dataset allows for the identiﬁcation of trends in\npanning, layering, and use of artiﬁcial reverberation as they\nare applied to vocals in commercially successful post-2010\npopular music. It enables the direct comparison of how\nvarious mixing parameters are applied to individual artists’\nvoices within and across songs. In addition to facilitating\nthe modeling of voice mixing, CoSoD also allows musicol-\nogists to ask questions about the way different voice types\nand individuals are mixed.\nFinally, CoSoD facilitates the study of the relation-\nship between gender and popular music. A number of\nprevious studies have examined music programming and\nstreaming services, exploring for instance how listeners\ntend to stream male artists more than women and mixed-\ngender groups [33]. Watson discusses gender inequality\nand low programming of women’s music in country mu-\nsic radio [34]. Other work addresses how a listener’s de-\nclared gender impacts automatic music recommendation\n[35] and musical preferences [36]. Additionally, various\nstudies have addressed race and gender, along with sexist\nand racist discourses and practices, as they impact the mu-\nsic industry in general and the Billboard charts in particu-\nlar [37–43]. By providing data on musical features, gender,\nand the role of these parameters within the formal structure\nof a song, CoSoD offers a new and complementary angle\nfor the study of gender as it directly relates to the musical\ncontent of post-2010 popular collaborations.3. COLLABORATIVE SONG DATASET (COSOD)\nCoSoD1consists of metadata and analytical data of a\n331-song corpus comprising all multi-artist collaborations\non the Billboard “Hot 100” year-end charts published be-\ntween 2010 and 2019. Each song in the dataset is asso-\nciated with two CSV ﬁles: one for metadata and one for\nanalytical data. We assembled the corpus by identifying\nevery song on the charts that featured collaborations be-\ntween two or more artists who usually perform indepen-\ndently from one another.\n3.1 Annotation of Musical Features\nThe following analytical data is provided for each song in\nthe dataset:\n1.Index number: 1 to 33\n2.Time stamps: In seconds (start of new section)\n3.Formal section label: Introduction, Verse, Pre-chorus,\nChorus, Hook, Dance Chorus [44], Link, Post-chorus,\nBridge, Outro, Refrain orOther\n4.Name of artist(s): Full name of the artist performing\nin each section. If all artists credited on the Billboard\nlisting perform in a section, the label both orallis used.\nSongs were assigned at random to one of two annota-\ntors, who generated time stamps at the onset of each formal\nsection with Sonic Visualiser.2The annotators provided\nformal labels according to their analysis of the song. In\ncase of ambiguity in the formal sections, both annotators\ndiscussed the analysis and agreed upon an interpretation.\nFor each formal section performed by one artist only ,\nthe following analytical data on the voice is provided:\n1.Gender of artist: M(Man), W(Woman), NB(Non-\nbinary)\n2.Function of artist: Feat (Featured artist), Main (Main\nartist), Neither ,Uncredited\n3.Style of vocal delivery: R(Rapped vocals), S(Sung\nvocals), Spoken\n4.Minimum pitch value: In Hz\n5.First quartile pitch value: In Hz\n6.Median pitch value: In Hz\n7.Third quartile pitch value: In Hz\n8.Maximum pitch value: In Hz\n9.Environment value: On a scale of E1 to E5\n10.Layering value: On a scale of L1 to L5\n11.Width (panning) value: On a scale of W1 to W5\n1https://github.com/duguay-michele/CoSoD\n2The ﬁrst annotator (ﬁrst author) has a doctorate in music theory,\nwhile the second (second author) is a doctoral candidate in the same ﬁeld.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n72The annotators determined the name of the artist(s) per-\nforming in each section by ear, and using song lyric web-\nsite Genius.com to validate their hearing. In cases where\nan artist only provides minimal background vocals (a few\nwords) in a particular formal section, their name is not in-\ncluded. One annotator then provided analytical data on\neach formal section performed by one artist only. Data\non gender was gathered from media interviews and social\nmedia statements from the artists, and matches the artist’s\ngender identity at the time of the dataset creation. This\nmethodology yielded three categories: man, non-binary,\nand woman. We understand these labels as umbrella terms\nthat encompass a variety of lived experiences that inter-\nsect with race, sexuality, and other power structures. The\nstyle of vocal delivery was determined by ear. The dis-\ntinction between rapping and singing is porous, with many\nvocalists adopting ambiguous modes of vocal delivery. We\nconsider any formal section containing a melodic line per-\nformed with sustained pitches as sung.\nThe pitch data was obtained by ﬁrst isolating the vocals\nfrom the full mix using Open-Unmix [45] and then running\nthe pYIN Smoothed Pitch Track transform [46] on the iso-\nlated vocal ﬁle. The minimum, ﬁrst quartile, median, third\nquartile, and maximum pitch points in each formal section\nwere calculated and recorded in the dataset.3\nThe Environment, Layering, and Width values were\ndetermined by the ﬁrst annotator to ensure consistency.\nRather than attempting to reconstruct the mixing process\nitself, the annotations for these parameters represent the\nway a listener might perceive the ﬁnal mix upon listening\nto it on stereo speakers. The Environment of a voice is the\nspace in which the voice reverberates. Environment values\nwere determined via an aural analysis of the full track by\nusing the following scale4:\nE1: The voice’s environment sounds ﬂat. There might be\nminimal ambiance added to the voice, but there is no\naudible echo or reverberation.\nE2: The last word or syllable of most musical phrases is\nrepeated through an echo or reverberation effect.\nE3: The vocal line is repeated in one clear layer of echo.\nThis added layer may be dry or slightly reverberant\nand has a lower amplitude than the main voice.\nE4: The main voice is accompanied by a noticeable\namount of reverberation. There is no clear echo layer,\nbut rather a sense that the main voice is being rever-\nberated across a large space.\nE5: The main voice is accompanied by two or more lay-\ners of echo. The echo layers may be noticeably re-\nverberant, similar in amplitude to the main voice, and\ndifﬁcult to differentiate from one another.\n3The accuracy of the F 0estimates used to calculate this feature is im-\npacted by the quality of the vocal source separation. A more accurate\nisolated vocal ﬁle would allow for more precise pitch data. Additionally,\nsince pYIN Smoothed Pitch Track can only track a single melodic line,\nthe accuracy of the pitch data is lessened in sections that feature multiple\nvocal layers with different pitch content.\n4The scales were initially published in [9].The Layering of a voice refers to the additional vocal\ntracks that are dubbed over a single voice. Layering values\nwere determined via an aural analysis of the full track by\nusing the following scale:\nL1: The voice is presented as solo. Occasionally, a few\nwords may be doubled with another vocal track for\nemphasis. Double-tracking is often used in the mix-\ning process to create a fuller sound, with a ﬁnal result\nsounding like a single vocal layer. Such cases fall into\nthis category.\nL2: The voice is presented as solo, but additional vocal\nlayers are added at the end of musical phrases for em-\nphasis.\nL3: The main voice is accompanied by one or two layers.\nLayers might provide minimal harmonies or double\nthe main voice. The layers have a noticeably lower\namplitude than the main voice.\nL4: The main voice is accompanied by two or more lay-\ners. These layers are close copies of the main voice,\nsharing the same pitch and similar amplitude.\nL5: The main voice is accompanied by two or more lay-\ners. These layers add harmonies to the main voice,\ncreating a thick and multi-voiced texture.\nThe Width of a voice refers to the breadth it occupies on\nthe stereo stage. The Width was analyzed aurally with the\naid of panning visualisation tool MarPanning [47]. The an-\nnotator simultaneously listened to the isolated vocal audio\nand observed the MarPanning visualization generated from\nthe isolated vocals to determine the Width value. Since\nOpen-Unmix occasionally omits reverberated components\nof the voice from the isolated ﬁle, the analyst then listened\nto the full track to conﬁrm the Width value. Width values\nwere determined according to the following scale:\nW1: The voice occupies a narrow position in the center of\nthe stereo stage.\nW2: The voice occupies a slightly more diffuse position\nin the center of the stereo stage.\nW3: The main voice occupies a narrow position in the cen-\nter of the stereo stage, but some of its components\n(echo, reverberation, and/or additional vocal tracks)\nare panned toward the sides. These wider components\nhave a lower amplitude than the main voice.\nW4: The main voice occupies a slightly more diffuse po-\nsition in the center of the stereo stage, and some of\nits components (echo, reverberation, and/or additional\nvocal tracks) are panned toward the sides. These\nwider components have a lower amplitude than the\nmain voice.\nW5: The main voice and its associated components (echo,\nreverberation, and/or additional vocal tracks) are\npanned across the stereo stage. All components have\na similar amplitude.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n733.2 Metadata\nThe following metadata is provided for each song in the\ndataset:\n1.Index number: From 1 to 331\n2.Year of ﬁrst appearance on Billboard “Hot 100”\nyear-end charts\n3.Chart position: As it appears on the Billboard “Hot\n100” year-end charts\n4.Song title: As it appears on the Billboard “Hot 100”\nyear-end charts\n5.Name of artists: As it appears on the Billboard “Hot\n100” year-end charts\n6.Collaboration type:\n•Lead/featured: Collab. with lead artist(s) and fea-\ntured artist(s)\n•No lead/featured: Collab. with no determined lead\n•DJ/vocals: Collab. between a DJ and vocalist(s)\n7.Gender of artists:\n•Men: Collab. between two or more men\n•Women: Collab. between two or more women\n•Mixed: Collab. between two or more artists of\ndifferent genders\n8.Collaboration type + gender:\n•Collab M: Collab. between men, no determined\nlead\n•Collab M and W: Collab. between men and\nwomen, no determined lead\n•Collab NB and W: Collab. betwen women and\nnon-binary artists, no determined lead\n•Collab W: Collab. between women, no determined\nlead\n•DJ with M: Collab. between male DJ and male\nvocalist\n•DJ with Mix: Collab. between male DJ and\nmixed-gender vocalists\n•DJ with NB: Collab. between male DJ and non-\nbinary vocalist\n•DJ with W: Collab. between male DJ and female\nvocalist\n•M ft. M: Men featuring men\n•M ft. W: Men featuring non-binary artist(s)\n•W ft. M: Women featuring men\n•W ft. W: Women featuring women\n9.MusicBrainz URL: Link to the song on open music\nencyclopedia MusicBrainz\nEach song in the dataset is labeled with an index number\nfrom 1 to 331. Songs are numbered in reverse chronologi-\ncal order, beginning with the 2019 charts and ending with\n2010. One annotator obtained the metadata on year, chart383656118\n0 20 40 60 80 100 120M ft. NBW ft. WW ft. MM ft. WM ft. M(i) Lead/featured\n122234\n0 20 40 60 80 100 120Collab\nNB & WCollab.\nWCollab. MCollab. M\n& W(ii) No lead/featured \n271923\n0 20 40 60 80 100 120DJ(M)\nwith NBDJ(M)\nwith MixDJ(M)\nwith MDJ(M)\nwith W(iii) DJ/vocals\nFigure 1 . Summary of the gender distribution across\ndifferent types of multi-artist collaborations. Subplot (i)\nshows gender counts for collaborations with lead and fea-\ntured artists, subplot (ii) shows collaborations with no de-\ntermined lead or featured artist, and subplot (iii) shows col-\nlaborations between DJs and vocalist(s).\nposition, title, and artists from the information available\non the Billboard charts. Within years, songs are organized\naccording to their position on the chart, from highest to\nlowest. Some songs appear on the charts two years in a\nrow. In such cases, we only include the data for the earliest\nappearance.\n3.3 Corpus Statistics\nThe dataset can be divided into three categories (shown in\nFigure 1): (i) collaborations between the lead artist(s) and\nfeatured artist(s), which account for 221, or 66.7% of the\ntracks, (ii) collaborations with no determined lead or fea-\ntured artist, which account for 59, or 17.8%, of the tracks,\nand (iii) collaborations between a DJ and a vocalist, which\naccount for 51, or 15.4% of the tracks. In category (i), the\nlead artist usually performs the majority of vocals. For ex-\nample, in “No Limit” (2018) by G-Eazy ft. A$AP Rocky\n& Cardi B, G-Eazy performs most of the vocals. A$APProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n740 200 400 600 800OtherOutroBridgePost-chorusChorusPre-chorusVerseIntroAll sections\nM NB W\nFigure 2 . Number of formal sections performed by a sin-\ngle artist (main, featured, or neither), categorized accord-\ning to formal section type\nRocky accompanies him in the chorus and Cardi B raps the\nsecond verse. In category (ii), the performance of the vo-\ncals is often more equally distributed. Such collaborations\nare often billed as “duets,” and the artists’ names are sepa-\nrated by a “+”, a “&”, or a comma on the Billboard charts.\nFor example,“Something’ Bad” (2014) is labeled as a “Mi-\nranda Lambert Duet With Carrie Underwood.” Both vocal-\nists perform approximately equal portions of the song. In\ncategory (iii), the DJ does not provide vocals. In “Sweet\nNothing” (2012), for instance, only the featured Florence\nWelch sings. The voice of DJ Calvin Harris is not heard.\nMixed-gender collaborations (including any combina-\ntion of non-binary, women, and men artists) frequently ap-\npear on the Billboard charts and account for 162, or 49%,\nof the tracks in the dataset. Collaborations between two or\nmore men account for 159 tracks, or 48% of the dataset.\nFinally, collaborations between women account for 10, or\n3%, of the tracks. In six of the ten years under study–2011,\n2012, 2015, 2017, 2018, and 2019–no collaborations be-\ntween women reached the Billboard “Hot 100” year-end\nchart. Conversely, songs with two or more male vocal-\nists were a consistent ﬁxture on the charts. Mixed-gender\ncollaborations, with any combination of men, women, and\nnon-binary artists within the same track, also frequently\nappear on the charts.\nFigure 2 shows the number and type of sections per-\nformed by individual artists in the corpus, categorized ac-\ncording to gender. This ﬁgure includes identical sections\n(such as choruses) that are repeated within a song. Sections\nin which more than one artist performs are not included.\nMore sections are performed by men than by women and\nnon-binary artists, which is to be expected given the over-\nrepresentation of men in the dataset as a whole (Figure\n1). Figure 3 displays the number and type of sections per-\nformed by featured artists only.0 100 200 300 400OtherOutroBridgePost-chorusChorusPre-chorusVerseIntroSections performed by featured artists\nM NB W\nFigure 3 . Number of formal sections performed by fea-\ntured artists, categorized according to formal section type\n4. EXPERIMENT: VOCAL PRODUCTION\nFEATURES AND GENDER\nThis section examines the relationship between the gen-\nder of an artist and the treatment of their voice, as char-\nacterized by three of the annotated musical features in the\ndataset: Environment, Layering, and Width. For the pur-\nposes of statistical power in the experiment, only songs\nwith men and/or women artists were included. We only\nincluded tracks that contained verse and chorus sections to\nremove section types that occur in only a few tracks. In or-\nder to avoid over-representations of tracks with repeated\nsections (i.e., several instances of the same chorus), we\nsampled the ﬁrst verse and chorus performed by a single\nartist from each track.5This method resulted in the inclu-\nsion of two sections from 287 of the 331 dataset tracks in\nthe experiment.\nWe analyzed the data with three separate logistic\nregressions–one for each feature–using the statsmodels\npackage in Python. We encoded the different levels of the\nparameter scales (deﬁned in Section 3.1) with one-hot en-\ncoding in order to allow us to examine whether there is\na correspondence between speciﬁc parameter scale levels\nand gender.\nOf the three logistic regressions, Environment\n(R2McFadden (4,N= 574) = 0.028, p< 0.0001) and Width\n(R2McFadden (4,N= 574) = 0.035, p< 0.0001) were statisti-\ncally signiﬁcant, while Layering ( R2McFadden (4,N= 574)\n= 0.0036, p= 0.64) was not. The McFadden R2values\nfor both Environment and Width were very low. This\nwas not surprising since we did not anticipate that these\nfeatures, particularly in isolation, would be explanatory.\nWe were instead interested in exploring whether there is\na signiﬁcance between these features with respect to the\nman/woman gender binary in these collaborations.\nFor Environment, there were signiﬁcant effects ( p<\n5If the ﬁrst verse of a song was performed by two artists simultane-\nously, while the second verse was only performed by one, we sampled the\nsecond verse.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n750.0001) for E1 (β=-1.18, 95%CI [-1.49, -0.87]), E2 ( β=-\n1.12, 95%CI [-1.56, -0.69]), and E3 ( β=–0.78, 95%CI [-\n1.14, -0.42). There was a signiﬁcant negative effect for the\nlower/mid-level environment values and gender, meaning\nthat men’s voices are more likely to be set in less rever-\nberant spaces than women’s voices. For Width, there were\nsigniﬁcant effects at all of the levels: W1 ( β=-1.84, 95%CI\n[-2.50, -1.17]), W2 ( β=-1.58, 95%CI [-2.39, -0.77]), W3\n(β=–1.13, 95%CI [-1.51, -0.75]), W4 ( β=-0.47, 95%CI [-\n0.77, -0.17), and W5 ( β=-0.60, 95%CI [-0.95, -0.25]).\nThe Width results are harder to interpret than the En-\nvironment ones because the coefﬁcient values are smaller\nand all negative. This is likely due to the imbalance be-\ntween men and women in featured artist roles, both in the\ndataset (see Figure 1) overall and in the sample used in\nthis experiment (404 of the included sections featured men\nwhile only of 170 featured women). However, the over-\nall trend is similar to the one in the Environment experi-\nment: lower-level values are more common for men than\nwomen. Men’s voices are more likely to occupy a nar-\nrow, centered position on the stereo stage, while women’s\nvoices are more likely to occupy a wider space. These\nresults were expected given that high Environment values\ntend to be associated with high Width values, as the rever-\nberated components of a voice are generally panned across\nthe stereo stage.\nThe lack of signiﬁcant results for Layering indicates\nthat there are no differences in the ways in which this pa-\nrameter is applied to men’s and women’s voices. Since\ntextural variation (such as the addition of vocal layers) is\na standard feature of verse-chorus form, it is possible that\nLayering is linked to the type of formal section rather than\nto the gender of the vocalist. The signiﬁcant results for the\nEnvironment and Width parameters can be interpreted in\nlight of Brøvig-Hanssen’s and Danielsen’s work on techno-\nlogical mediation [48]. The authors establish a distinction\nbetween transparent and opaque technological mediation\nin recorded music. Transparent mediation, on one hand,\nis meant to create a recorded product that sounds natural\nand unaltered. Low Environment and Width values, for\ninstance, are closer to transparent mediation because they\nsound closer to a real-life performance that is unmediated\nwith artiﬁcial reverb or panning. Opaque mediation, on the\nother hand, highlights the use of technology by making it\nobvious to the listener. High Width and Environment val-\nues, with their clearly audible artiﬁcial reverberation and\nwide panning, are examples of opaque mediation. The re-\nsults of the experiment therefore suggest that men’s voices\nare more likely to be mixed to sound “transparent” and nat-\nural while women’s voices are more likely to be mixed to\nsound “opaque” and technologically mediated.\nOverall, this experiment demonstrates that within verse\nand chorus sections in CoSoD, there is a signiﬁcant differ-\nence between the treatment of men’s and women’s vocals\nin terms of Environment and Width. This suggests that\nsome mixing parameters contribute to the sonic differenti-\nation of men’s and women’s voices in popular music.5. CONCLUSION\nCoSoD is a 331-song corpus of all multi-artist collabora-\ntions for faciliating appearing on the 2010–2019 Billboard\n“Hot 100” charts. Each song in the dataset is annotated\nwith metadata, formal sections, and aspects of vocal pro-\nduction (including reverberation, layering, panning, and\ngender of the artists). As outlined in Section 2, CoSoD has\nseveral implications for MIR research. It provides anno-\ntated data for structural segmentation tasks and a listener-\ncentered perspective on vocal mixing that could be useful\nfor automatic music mixing tasks. The dataset could also\nbe used to determine how these parameters interact with\nsong form. Further study could also examine the relation-\nship between the vocal range of an artist in a given section,\ntheir type of vocal delivery (rapped, spoken, or sung), and\nmixing parameters. Finally, the dataset also allows for the\nexamination of the ways in which Environment, Layering,\nand Width values tend to be grouped together to create spe-\nciﬁc vocal production effects.\nThe dataset also facilitates musicological study of\nmulti-artist collaborations post-2010 and gender norms.\nThe experiment in Section 4 demonstrates this, as its re-\nsults suggest that, for the chorus and verse data sampled\nfrom 287 songs in the dataset, men’s voices are more likely\nto be narrow and less reverberated than women’s. Opportu-\nnities for future research include examining whether there\nis a signiﬁcant difference in the way Environment, Width,\nLayering, or other parameters are applied to women’s and\nmen’s voices within collaborations that feature mixed- and\nsame-gender vocalists. In other future work, we plan on\nexpanding the annotations in the dataset with time-aligned\nlyrics, harmonic analyses, and additional performance data\nfor the voice extracted using AMPACT [49,50]. These an-\nnotations will include both spectral features and seman-\ntic descriptors, and the data will be encoded in relation\nto vocal-line transcriptions, where possible [51]. We also\nplan on providing annotations on vocal production param-\neters in sections performed by multiple artists and examin-\ning how vocal production parameters correlate with mixing\nparameters such as panning.\nFinally, while our dataset focuses on gender, we are also\ninterested in encoding other aspects of identity, such as\nrace, in order to provide an intersectional perspective on\nartists’ identities. However, categorizing artists accord-\ning to race proves to be more problematic than gender.\nMatthew D. Morrison writes that “white (and other non-\nblack) people freely express themselves through the con-\nsumption and performance of commodiﬁed black aesthet-\nics without carrying the burden of being black under white\nsupremacist structures” [52, p. 791]. In other words, white\nand non-Black artists–such as rappers Iggy Azalea and G-\nEazy, or singer Bruno Mars–often assume particular sonic\ncharacteristics that implicitly associate them with com-\nmodiﬁed notion of Blackness. By categorizing all white\nartists together, for instance, we would ignore this phe-\nnomenon and the way it is sonically realized. Further work\nneeds to be done to understand how to best expand on\nCoSoD, or datasets in general, to account for this dynamic.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n766. REFERENCES\n[1] Anonymous, “In popular music, collaborations rock,”\nThe Economist , February 2018. [Online]. Avail-\nable: https://www.economist.com/business/2018/02/\n03/in-popular-music-collaborations-rock\n[2] T. Rose, Black Noise: Rap Music and Black Culture\nin Contemporary America . Hanover, NH: Wesleyan\nUniversity Press, 1994.\n[3] C. Molanphy, “Feat. don’t fail me now: The rise of the\nfeatured rapper in pop music,” Slate , July 2015.\n[4] A. Ordanini, J. C. Nunes, and A. Nanni, “The featuring\nphenomenon in music: How combining artists of dif-\nferent genres increases a song’s popularity,” Marketing\nLetters , vol. 29, no. 4, pp. 485–499, Dec 2018.\n[5] M. Silva and M. Moro, “Causality analysis between\ncollaboration proﬁles and musical success,” in Pro-\nceedings of the 25th Brazillian Symposium on Multi-\nmedia and the Web , 10 2019, pp. 369–376.\n[6] G. P. Oliveira, M. O. Silva, D. B. Seuﬁtelli, A. Lacerda,\nand M. M. Moro, “Detecting collaboration proﬁles in\nsuccess-based music genre networks,” in Proceedings\nof the 21st International Society for Music Information\nRetrieval Conference , 2020, pp. 726–732.\n[7] R. Komaniecki, “Analyzing collaborative ﬂow in rap\nmusic,” Music Theory Online , vol. 23, no. 4, 12 2017.\n[8] B. Duinker, “Song form and the mainstreaming of hip-\nhop music,” Current Musicology , vol. 107, pp. 93–135,\n1 2020.\n[9] M. Duguay, “Analyzing vocal placement in recorded\nvirtual space,” Music Theory Online , vol. 28, no. 4,\n2022.\n[10] A. Berenzweig, B. Logan, D. Ellis, B. Whitman, and\nC. A, “A large-scale evaluation of acoustic and subjec-\ntive music similarity measures,” Computer Music Jour-\nnal, vol. 28, 11 2003.\n[11] C. Harte, “Towards automatic extraction of harmony\ninformation from music signals,” Ph.D. dissertation,\nQueen Mary, University of London, august 2010.\n[12] J. Burgoyne, J. Wild, and I. Fujinaga, “An expert\nground truth set for audio chord recognition and mu-\nsic analysis.” 01 2011, pp. 633–638.\n[13] J. Smith, J. Burgoyne, I. Fujinaga, D. De Roure,\nand J. Downie, “Design and creation of a large-scale\ndatabase of structural annotations,” in Proceedings of\nthe 12th International Society for Music Information\nRetrieval Conference , Miami, FL, USA, 01 2011, pp.\n555–560.\n[14] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and\nP. Lamere, “The million song dataset,” in Proceedings\nof the 12th International Conference on Music Infor-\nmation Retrieval (ISMIR 2011) , 2011.[15] T. de Clerq and D. Temperley, “A corpus analysis of\nrock harmony,” Popular Music , vol. 30, no. 1, pp. 47–\n70, 2011.\n[16] F. Bimbot, G. Sargent, E. Deruty, C. Guichaoua, and\nE. Vincent, “Semiotic description of music structure:\nan introduction to the quaero/metiss structural annota-\ntions,” in Proceedings of the AES International Con-\nference , London, UK, 01 2014.\n[17] O. Nieto, M. McCallum, M. Davies, A. Robertson,\nA. Stark, and E. Egozy, “The Harmonix Set: Beats,\nDownbeats, and Functional Segment Annotations of\nWestern Popular Music,” in Proceedings of the 20th\nInternational Society for Music Information Retrieval\nConference , Delft, The Netherlands, 2019, pp. 565–\n572.\n[18] C.-L. Hsu and J.-S. R. Jang, “On the improvement of\nsinging voice separation for monaural recordings us-\ning the mir-1k dataset,” IEEE Transactions on Audio,\nSpeech, and Language Processing , vol. 18, no. 2, pp.\n310–19, 2010.\n[19] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-\nnam, and J. Bello, “Medleydb: A multitrack dataset\nfor annotation-intensive mir research,” in Proceedings\nof the 15th International Society for Music Information\nRetrieval Conference , Taipei, Taiwan, 10 2014.\n[20] T.-S. Chan, T.-C. Yeh, Z.-C. Fan, H.-W. Chen, L. Su,\nY .-H. Yang, and R. Jang, “V ocal activity informed\nsinging voice separation with the ikala dataset,” in\n2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , Brisbane,\nAustralia, 2015, pp. 718–722.\n[21] R. Bittner, J. Wilkins, H. Yip, and J. Bello, “Medleydb\n2.0: New data and a system for sustainable data col-\nlection,” in Extended abstracts for the Late-Breaking\nDemo Session of the 17th International Society for Mu-\nsic Information Retrieval Conference , New York, NY ,\nUSA, 2016.\n[22] B. D. Man and J. D. Reiss, “The mix evaluation\ndataset,” in Proceedings of the 20th International Con-\nference on Digital Audio Effects , Edinburgh, UK, 9\n2017, pp. 436–442.\n[23] Z. Raﬁi, A. Liutkus, F.-R. Stöter, S. I. Mimilakis, and\nR. Bittner, “The MUSDB18 corpus for music separa-\ntion,” Dec. 2017.\n[24] R. Gong, R. C. Repetto, and X. Serra, “Creating an\na cappella singing audio dataset for automatic jingju\nsinging evaluation research,” in Proceedings of the 4th\nInternational Workshop on Digital Libraries for Musi-\ncology , Shanghai, China, 2017, pp. 37–40.\n[25] C.-i. Wang and G. Tzanetakis, “Singing style investi-\ngation by residual siamese convolutional neural net-\nworks,” in 2018 IEEE International Conference onProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n77Acoustics, Speech and Signal Processing (ICASSP) ,\nCalgary, Canada, 2018, pp. 116–120.\n[26] J. Wilkins, P. Seetharaman, A. Wahl, and B. Pardo,\n“V ocalset: A singing voice dataset,” in Proceedings of\nthe 19th International Society for Music Information\nRetrieval Conference , Paris, France, 2018, pp. 468–\n474.\n[27] B. D. Man and J. D. Reiss, “Ten years of automatic\nmixing,” in Proceedings of the 3rd Workshop on Intel-\nligent Music Production , Salford, UK, 2017.\n[28] M. A. Martínez-Ramírez, W.-H. Liao, G. Fabbro,\nS. Uhlich, C. Nagashima, and Y . Mitsufuji, “Automatic\nmusic mixing with deep learning and out-of-domain\ndata,” in Proceedings of the 23rd International Society\nfor Music Information Retrieval Conference , 2022.\n[29] D. Matz, E. Cano, and J. Abeßer, “New sonorities\nfor early jazz recordings using sound source separa-\ntion and automatic mixing tools,” in Proceedings of the\n16th International Society for Music Information Re-\ntrieval Conference , Málaga, Spain, 2015, pp. 749–755.\n[30] H. Ishizaki, K. Hoashi, and Y . Takishima, “Full-\nautomatic dj mixing system with optimal tempo adjust-\nment based on measurement function of user discom-\nfort.” in Proceedings of the 10th International Soci-\nety for Music Information Retrieval Conference , Kobe,\nJapan, 01 2009, pp. 135–140.\n[31] J. J. Scott and Y . E. Kim, “Instrument identiﬁcation in-\nformed multi-track mixing,” in Proceedings of the 14th\nInternational Society for Music Information Retrieval\nConference , Curitiba, Brazil, 2013, pp. 305–310.\n[32] A. L. Knoll and K. Siedenburg, “The optimal mix?\npresentation order affects preference ratings of vocal\namplitude levels in popular music,” Music & Science ,\nvol. 5, pp. 1–12, 12 2022.\n[33] A. Epps-Darling, H. Cramer, and R. Takeo Bouyer,\n“Artist gender representation in music streaming,” in\nProceedings of the 21st International Society for Music\nInformation Retrieval Conference , 10 2020, pp. 248–\n254.\n[34] J. Watson, “Programming inequality: Gender represen-\ntation on canadian country radio (2005-2019),” in Pro-\nceedings of the 21st International Society for Music In-\nformation Retrieval Conference , 2020, pp. 392–399.\n[35] G. Vigliensoni and I. Fujinaga, “Automatic music rec-\nommendation systems: Do demographic, proﬁling,\nand contextual features improve their performance?”\ninProceedings of the 17th International Society for\nMusic Information Retrieval Conference , New York,\nNY , USA, 2016, pp. 94–100.\n[36] A. Laplante, “Improving music recommender systems:\nWhat can we learn from research on music tastes?” inProceedings of the 15th International Society for Mu-\nsic Information Retrieval Conference , Taipei, Taiwan,\n2014, pp. 451–456.\n[37] C. L. Keyes, “Empowering self, making choices, creat-\ning spaces: Black female identity via rap music perfor-\nmance,” The Journal of American Folklore , vol. 113,\nno. 449, pp. 255–69, 2000.\n[38] T. J. Dowd and M. Blyler, “Charting race: The success\nof black performers in the mainstream recording mar-\nket, 1940 to 1990,” Poetics , vol. 30, pp. 87–110, 2002.\n[39] R. N. Bradley, Barbz and Kings: Explorations of Gen-\nder and Sexuality in Hip Hop . Cambridge, England:\nCambridge University Press, 2015, pp. 181–191.\n[40] M. Lafrance, C. Scheibling, L. Burns, and J. Durr,\n“Race, gender, and the billboard top 40 charts between\n1997 and 2007,” Popular Music and Society , vol. 41,\nno. 5, pp. 522–538, 2018.\n[41] K. J. Lieb, Gender, Branding, and the Modern Music\nIndustry: The Social Construction of Female Popular\nMusic Stars , 2nd ed. New York, NY: Routledge, 2018.\n[42] J. E. Watson, “Gender on the billboard hot country\nsongs chart, 1996–2016,” Popular Music and Society ,\nvol. 42, no. 5, pp. 538–60, 2002.\n[43] C. Bauer and J. Devaney, “Constructing gender in au-\ndio: exploring how the curation of the voice in mu-\nsic and speech inﬂuences our conception of gender\nidentity,” in Mediale Stimmentwürfe: perspectives of\nmedia voice designs , ser. Schriftenreihe zur digitalen\nGesellschaft NRW, M. Erbe, A. Rifﬁ, and W. Zielinski,\nEds. Munich, Germany: kopaed Verlag, 2022, vol. 7,\npp. 83–100.\n[44] A. Barna, “The dance chorus in recent top-40 music,”\nSMT-V , vol. 6, no. 4, June 2020. [Online]. Available:\nhttp://doi.org/10.30535/smtv.6.4\n[45] F.-R. Stöter, S. Uhlich, A. Liutkus, and Y . Mitsufuji,\n“Open-unmix - a reference implementation for music\nsource separation,” Journal of Open Source Software ,\n2019.\n[46] M. Mauch and S. Dixon, “pyin: A fundamental fre-\nquency estimator using probabilistic threshold distribu-\ntions,” in 2014 Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning (ICASSP) , Florence, Italy, 2014, pp. 659–663.\n[47] K. McNally, G. Tzanetakis, and S. R. Ness, “New tools\nfor use in the musicology of record production,” Un-\npublished Paper, University of Victoria. , 2009.\n[48] R. Brøvig-Hanssen and A. Danielsen, The Impact of\nDigitization on Popular Music Sound . Cambridge,\nMA, USA: The MIT Press, 2016.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n78[49] J. Devaney, M. I. Mandel, and I. Fujinaga, “A study\nof intonation in three-part singing using the automatic\nmusic performance analysis and comparison toolkit\n(ampact).” in Proceedings of the 13th International\nSociety for Music Information Retrieval Conference ,\n2012, pp. 511–516.\n[50] J. Devaney and M. Mandel, “Score-informed estima-\ntion of performance parameters from polyphonic au-\ndio using ampact,” in Extended abstracts for the Late-\nBreaking Demo Session of the 17th International Soci-\nety for Music Information Retrieval Conference , 2016.\n[51] J. Devaney, “Using note-level music encodings to facil-\nitate interdisciplinary research on human engagement\nwith music,” Transactions of the International Society\nfor Music Information Retrieval , vol. 3, no. 1, 2020.\n[52] M. D. Morrison, “Race, blacksound, and the\n(re)making of musicological discourse,” Journal of the\nAmerican Musicological Society , vol. 72, no. 3, pp.\n781–823, 12 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n79"
    },
    {
        "title": "TriAD: Capturing Harmonics With 3D Convolutions.",
        "author": [
            "Miguel Pérez Fernández",
            "Holger Kirchhoff",
            "Xavier Serra"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265215",
        "url": "https://doi.org/10.5281/zenodo.10265215",
        "ee": "https://zenodo.org/records/10265215/files/000002.pdf",
        "abstract": "Thanks to advancements in deep learning (DL), automatic music transcription (AMT) systems recently outperformed previous ones fully based on manual feature design. Many of these highly capable DL models, however, are computationally expensive. Researchers are moving towards smaller models capable of maintaining state-of-the-art (SOTA) results by embedding musical knowledge in the network architecture. Existing approaches employ convolutional blocks specifically designed to capture the harmonic structure. These approaches, however, require either large kernels or multiple kernels, with each kernel aiming to capture a different harmonic. We present TriAD, a convolutional block that achieves an unequally distanced dilation over the frequency axis. This allows our method to capture multiple harmonics with a single yet small kernel. We compare TriAD with other methods of capturing harmonics, and we observe that our approach maintains SOTA results while reducing the number of parameters required. We also conduct an ablation study showing that our proposed method effectively relies on harmonic information.",
        "zenodo_id": 10265215,
        "dblp_key": "conf/ismir/FernandezKS23",
        "keywords": [
            "advancements",
            "deep learning",
            "automatic music transcription",
            "computational expense",
            "smaller models",
            "network architecture",
            "convolutional blocks",
            "harmonic structure",
            "unequally distanced dilation",
            "harmonic information"
        ],
        "content": "TRIAD: CAPTURING HARMONICS WITH 3D CONVOLUTIONS\nMiguel Perez♯♭\nHuawei, Munich Research Center♯\nmiguel.perez.fernandez@huawei.comHolger Kirchhoff♯Xavier Serra♭\nMTG, Universitat Pompeu Fabra♭\nxavier.serra@upf.edu\nABSTRACT\nThanks to advancements in deep learning (DL), auto-\nmatic music transcription (AMT) systems recently outper-\nformed previous ones fully based on manual feature de-\nsign. Many of these highly capable DL models, however,\nare computationally expensive. Researchers are moving\ntowards smaller models capable of maintaining state-of-\nthe-art (SOTA) results by embedding musical knowledge\nin the network architecture. Existing approaches employ\nconvolutional blocks speciﬁcally designed to capture the\nharmonic structure. These approaches, however, require\neither large kernels or multiple kernels, with each kernel\naiming to capture a different harmonic. We present TriAD,\na convolutional block that achieves an unequally distanced\ndilation over the frequency axis. This allows our method to\ncapture multiple harmonics with a single yet small kernel.\nWe compare TriAD with other methods of capturing har-\nmonics, and we observe that our approach maintains SOTA\nresults while reducing the number of parameters required.\nWe also conduct an ablation study showing that our pro-\nposed method effectively relies on harmonic information.\n1. INTRODUCTION\nWhen a note is played, a set of strongly related frequen-\ncies start to sound leading to a pitch sensation for the lis-\ntener. These strongly related frequencies are what we call\ntheharmonic spectrum , in which we distinguish two parts:\nthe fundamental frequency ( f0) and the harmonics. The\nfundamental is the frequency associated with the pitch, and\nthe harmonics are integer multiples of f0. Different in-\nstruments reinforce different harmonics, achieving differ-\nent timbres; but the underlying structure created by f0and\nits harmonics remain present.\nTraditional Automatic music transcription (AMT) sys-\ntems based on manual feature design employed this prop-\nerty to look for harmonic patterns given an observed spec-\ntrogram [1]. When DL became more popular, many re-\nsearchers refrained from incorporating expert knowledge\ninto their model architectures, but relied on generic models\nin combination with large amounts of task-speciﬁc train-\ning data. Even though these systems signiﬁcantly outper-\n© M. Perez, H. Kirchhoff, and X. Serra. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: M. Perez, H. Kirchhoff, and X. Serra, “TriAD: Capturing\nharmonics with 3D convolutions”, in Proc. of the 24th Int. Society for\nMusic Information Retrieval Conf., Milan, Italy, 2023.formed traditional approaches, models utilized large num-\nbers of parameters. [2, 3].\nThe number of parameters plays an important role, as\nmore parameters can help capture the harmonic pattern\nbetter; in exchange, larger models require more comput-\ning resources as the number of operations grows. Many\nDL practitioners do not always have access to large GPU\nclusters, and might not be able to train such large models.\nMoreover, many portable devices such as phones have lim-\nited battery and memory, and such large models in those\ndevices will either quickly drain their battery or be directly\nimpossible to employ. Part of the research focused on re-\nducing the number of models’ parameters without harming\nthe transcription’s accuracy. This was achieved in many\ncases through the incorporation of pitch expert knowledge\nwithin the architecture neural network (NN) [4–9].\nThe main challenge resides in the unequal distances be-\ntween harmonics in the spectrum, so previous approaches\nemploy either large kernels or several ones running in\nparallel. This paper introduces a tridimensional kernel\nharmonically dilated (TriAD), a neural block that captures\nmusic intervals and is capable of observing multiple har-\nmonics while using a single yet small kernel.\nThe rest of the paper is divided into the following sec-\ntions: Section 2 gives more details about prior work captur-\ning harmonics from the spectrum. Section 3 describes our\nmethod, including the processing of the signal and the de-\nsign of the kernels. The experimental setting is described\nin Section 4. We present the results for these experiments\nas well as an ablation study in Section 5. Finally, Section\n6 contains our conclusions for this paper and future work.\n2. RELATED WORK\nAs mentioned in Section 1, harmonics played an important\nrole in the ﬁrst AMT systems. For example, [1] creates\na dictionary of sets of expected harmonics for each fun-\ndamental. These ideal patterns were then matched to the\nspectrograms used as input for the system using the non-\nnegative least squares (NNLS) algorithm. The result is an\nestimation of fundamental frequencies that along with their\nrespective harmonics, would resemble the input’s spectro-\ngram.\nFor AMT systems using DL, prior work has incorpo-\nrated domain-speciﬁc knowledge in two ways: 1. by\nchoosing a custom input representation that allows the\nmodel to detect harmonic structures [4, 10, 11]; 2. by em-\nploying speciﬁc network architectures to search for pat-29terns in a given feature map obtained at any point of the\nnetwork [6–8, 12]. Within the ﬁrst category, one of the\nmost popular approaches is the harmonic constant Q trans-\nform (HCQT) [4], a feature that extends the constant Q\ntransform (CQT) [13]. The standard CQT returns a log-\nfrequency representation of the spectrum, where the nth\nbin is associated with the frequency fn=fmin·2n/p\nwherefmin is the minimum frequency to be considered,\nandpis the number of bins per octave. The magnitude of\nCQT spectrogram is a representation containing a single\nchannel,Fbins frequency bins, for Tframes; its shape is\n[1,Fbins,T]. The HCQT extends the CQT the channel di-\nmension, where now Hharmonics are aligned, resulting in\na tensor with dimensions [H,Fbins,T]. This extension is\ndone by stacking a number of HCQTs through the chan-\nnel dimension. Each one of these HCQTs is a regular\none whose fminhas been scaled by a harmonic factor h:\nfn=h·fmin; the CQTs with h= 1will refer to the fun-\ndamental, h= 2 will refer to the ﬁrst harmonic, h= 3 to\nthe third harmonic, etc. up to Hdifferent values. Similarly,\nsub-harmonics can be added by making h= 0.5,0.25,etc.\nIn a nutshell, the HCQT facilitates information about the\nfundamentals directly at the network’s input.\nAs mentioned, other works incorporated the harmonic\nknowledge within the architecture of NNs, e.g. [6] ex-\ntended the idea of frequency-shifted representations, for\nthe internal feature maps obtained inside NNs. The au-\nthors named this method multiple rates dilated harmonic\ncausal convolution (MRDC-Conv). Let Xdenote a feature\nmap, with shape [Cin,Fbins,T]at an arbitrary point of the\nnetwork. The number of channels for that map is Cin. In\na CQT spectrum, the distance dnbetween the fundamental\nfrequency and the nthharmonic is given by:\ndn=round(p·log2(n)) (1)\nWherepis a parameter that determines the number of bins\nper octave in the CQT spectra. To capture kharmonics\nwith MRDC-Conv, the feature map Xis convolved with k\ndifferent kernels in parallel, resulting in koutputs. Each of\nthe outputs is shifted following the harmonic factors given\nby Equation 1. E.g. to capture the ﬁrst three harmon-\nics, three different kernels are required, thus, producing\nthree different outputs. In the case of p= 12 and follow-\ning Equation 1, the shifts associated with the 2nd,3rdand\n4thharmonics are 12, 19, and 24. The sum across the k\noutputs is taken, leading to a single ﬁnal output of shape\n[Cout,Fbins,T], whereCoutis the number of output chan-\nnels. This method is illustrated in Figure 1a. MRDC-Conv\nachieves a convolution able to observe the input at the pre-\ncise position of the harmonics; its drawback is that for each\nof the harmonics, a different kernel is needed, thus requir-\ning a different feature map stored in memory for each of\nthekharmonics before they can be aggregated.\nSome other authors embedded harmonic knowledge\nwithin the convolutional kernels rather than in the manipu-\nlation of their inputs/outputs. In [12] the authors use sparse\nconvolutions so that only relevant parts of the spectrum are\nconsidered. Sparse convolutions allow the kernels to “ig-\nnore” certain parts of the input, so they do not contributeHarmonics Music Interval pitc class distance\n2,4,8,16 octave b·12\n17 minor second b·1\n9,18 major second b·2\n19 minor third b·3\n5,10,20 major third b·4\n21 perfect fourth b·5\n11,22 augmented fourth b·6\n3,6,12,24 perfect ﬁfth b·7\n25 minor sixth b·8\n27 major sixth b·9\n7,14,28 minor seventh b·10\n15,30 major seventh b·11\nTable 1 : The harmonics of the ﬁrst 3 octaves, and their as-\nsociated music intervals. The rightmost column indicates\nthe distance in bins associated with each interval, where b\nis the number of bins per semitone.\neither to the output or to backpropagation during train-\ning [14]. According to [15], the harmonics are positive\nindicators that a certain pitch is present, but some frequen-\ncies indicate that the pitch might not be present at all. The\nlatter are called negative indicators. The sparse convolu-\ntions from [12] are used in such a way that only positive\nandnegative indicators deﬁned in [15] are taken into ac-\ncount. Sparse convolutions require nonetheless using large\nkernels to cover relevant parts of the spectrum, i.e. [12] re-\nsulted in around 650k parameters exclusively for harmonic\nprocessing, accounting for the major portion of the model’s\nparameters.\nIn [8], dilated convolutions are used to capture the har-\nmonics from the spectrum, with a method named harmonic\ndilated convolution (HD-Conv). Dilated convolutions are\na special kind of convolution, where the kernels’ inputs are\nspaced by a ﬁxed amount. An example of dilated convo-\nlutions can be seen in Figure 1b. By controlling the di-\nlation size, the authors space the kernels’ inputs, so each\nkernel obtains a speciﬁc harmonic. The outputs of the dif-\nferent kernels are aggregated by summing across the ker-\nnels’ outputs as shown in Figure 1b. The size of the dila-\ntions is given by Equation (1). E.g. for p= 12 , the second\nharmonic is separated from the fundamental by d2= 12\nbins, the third one by d3= 19 ; to capture both the sec-\nond and the third harmonic, we would need to create two\nconvolutional kernels with a dilation size of 12and19at\nthe frequency dimension. This method has the same draw-\nback as MRDC-Conv, as different harmonics also require\na different kernel.\n3. OUR METHOD\nSimilarly to [8], our method uses dilated convolutions to\ncapture the harmonics of the spectrum. As mentioned be-\nfore, a constant dilation can not capture multiple harmon-\nics given the logarithmic nature of these. If it was possible\nto use different dilations for the same kernel, this problemProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n30(a) MRDC-Conv\n (b) HD-Conv [8]\nFigure 1 : Figure (a) An example of MRDC-Conv [6]. Two kernels are applied to the same input. The fundamental fis\nseparated from the harmonic nbydnbins. One output gets shifted by dn, and sofandnget aligned. Figure (b) An example\nof HD-Conv [8], with two kernels applied to the same input, each one with a different dilation (3, and 2 respectively).\nwould have been already solved, but currently, DL frame-\nworks support only dilations with constant spacing. Our\nmethod is able to partially overcome this technical limita-\ntion and achieve a convolution at the frequency axis with\ndifferent dilation rates; thanks to this, our proposed method\ncaptures multiple harmonics by just using a single kernel.\nWe named our method TriAD , and it involves a series\nof steps. The ﬁrst step is to split the frequency dimension\ninto two new ones, each representing different octaves and\npitch classes. We call this representation the pitch/octave\nspectrogram. Next, we create the kernels for our method.\nPrevious works used kernels spanning 2 dimensions: fre-\nquency and time; our method’s kernels however span 3 di-\nmensions: octave, pitch class, and time. An arbitrary num-\nber ofmdifferent kernels can be created, each one cap-\nturing a different music interval. The mkernels are con-\nvolved with the previously described pitch/octave spectro-\ngram, resulting in mdifferent outputs. Finally, these out-\nputs are aggregated by taking the sum across them. The\nconsecutive steps are illustrated in Figure 2.\nSubsection 3.1 details the procedure followed to con-\nvert a log-frequency spectrogram onto a pitch/octave spec-\ntrogram. Subsection 3.2 explains how our convolutional\nkernels are created and the difference they have with the\nmethod described in [8]. At the end of that subsection, we\ndescribe a special kind of padding used in our technique,\nthe octave-circular padding.\n3.1 The pitch/octave spectrogram\nLetXCin×Fbins×Tbe a feature map, with Fbinslogarith-\nmically spaced frequency bins, Tframes, and Cinchan-\nnels. Our goal is to separate octave and pitch class infor-\nmation. We split the Fbinsbins into two dimensions repre-\nsenting the octave ( o) and pitch class ( p) information. The\nnumber of pitch classes is simply the number of bins per\noctave used, and the number of octaves can be obtained by\no=Fbins\np. Note that omust be an integer, and so when\nthis condition is not met, we pad the upper part of X’s fre-\nquency dimension with the minimum amount of zeros that\nsatisﬁes the condition. The result is the pitch/octave spec-\ntrogramYCin×o×p×T, a view of XwhereFbinshas been\nseparated into its octave and pitch class information.3.2 The harmonic convolutions\nOur aim is to compare two pitch classes across multi-\nple octaves to capture harmonically related information.\nAs shown in Table 1, harmonics and music intervals are\nclosely related. Comparing two pitch classes separated by\na certain interval at multiple octaves simultaneously will\neffectively obtain the harmonics associated with that mu-\nsic interval.\nAs previously mentioned, our kernels have 3 dimen-\nsions:Kko×kp×kt, related to the octaves ( ko), pitch classes\n(kp), and frames ( kt) of the pitch/octave spectrogram; this\nmeans that our method uses 3D convolutions1. By chang-\ning the convolution dilation at the pitch class dimension we\ncontrol which interval we capture, and consequently its as-\nsociated harmonics. Since our goal is to compare the same\ntwo pitch classes, our method has a ﬁxed kp= 2, but the\nsizes ofkoandktcan be varied, spanning many octaves\nand timesteps. The effect of dilation exclusively on pitch\nclasses is what achieves the aforementioned non-constant\ndilation at the frequency dimension. E.g. Let p= 12\nand a kernel Kwithko= 3 and a perfect ﬁfth dilation\nat the pitch class dimension, in a certain position, this ker-\nnel would see C1,G1,C2,G2,C3,G3simultaneously. The\ndistance from each Cto the next Gis 7 bins, but the dis-\ntance from each Gto the next Cis 5 bins. Our method is to\nthe best of our knowledge, the only one capable of achiev-\ning that effect in dilation. In the same scenario using linear\ndilations [8], a kernel with the same size and dilation of\na perfect ﬁfth would see instead C1,G1,D2,A2,E3,B3.\nUsing our method, a single kernel with ko= 3and a dila-\ntion of perfect ﬁfths at the kpdimension capture 5 of the\nﬁrst 7 harmonics (see Table 1).\nAs can be observed in Figure 2, the inputs and outputs\nof the convolutions have the same size, which is achieved\nby padding the pitch/octave spectrogram. The values used\nto pad follow the values of the continuous log-frequency\nspectrogram. E.g. given p= 12 , to pad above B1, we\nuse the values of the bins C2,C♯2,etc. In contrast, values\nabove the highest octave of the pitch/octave spectrogram\nwill be padded with zeros. We call this method circular-\noctave padding .\n1Whenkt= 1, our method can be implemented with 2D convolutions\nby stacking frames across the batch dimension. 3D is just the general case\nfor an arbitrary ktProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n31Figure 2 : An overview of TriAD. The channel dimension has been omitted in the image. The ﬁrst stage converts a log-\nfrequency spectrogram onto a pitch/octave one. We apply mof our harmonically motivated kernels to the pitch/octave\nspectrogram. Each kernel captures different harmonics, depending on the dilation at the pdimension. The kernels’ outputs\nare aggregated by summing the moutputs.Bstands for the batch dimension.\n4. EXPERIMENTS\nWe test the performance of our method on AMT for the\nsubtask of piano transcription. Our method is compared\nwith other SOTA approaches of capturing the harmonic\nspectrum within the architecture itself; concretely, we used\nthe harmonic blocks MRDC-Conv [6], and HD-Conv [8].\nWe do not include input manipulations such as the HCQT,\nsince these are input manipulations rather than network-\ninternal musically motivated convolutional operations, and\na fair comparison is not straightforward.\n4.1 Datasets\nWe used two datasets in our experiments: MIDI and\naudio edited for synchronous track and organization\n(MAESTRO) [16], and MIDI aligned piano sounds\n(MAPS) [17]. MAESTRO contains about 200 hours of\naudio for complex piano performances precisely aligned\nto note labels. Some compositions appear multiple times,\neach played by a different interpreter. In the paper where\nMAESTRO is presented, an ofﬁcial train/validation/test\nconﬁguration was also proposed so that compositions\nplayed by different interpreters are in the same split group.\nWe use the latest version of this dataset, version 3, in our\nexperiments. MAPS is another popular dataset used in pi-\nano transcription. In contrast to MAESTRO that contains\nonly complete piano pieces, this dataset also contains iso-\nlated notes and chords.\nFollowing the practice used in previous works [7,8,16],\nwe use the train and validation splits from MAESTRO\nto train our NNs, and the test sets of MAESTRO and\nMAPS for testing the trained models. Chunks of audio\nof 20 seconds and a sample rate of 16.000Hz were used\nand transformed into a CQT spectrogram, with 352bins,\nfmin= 32.070Hz, and a resolution of 4bins per semi-\ntone. A hop size of 320 samples is employed, resulting in\na time resolution of 20 milliseconds.4.2 The model\nWe use the HPPNet-base model from [8] for our experi-\nments. This model consists of a backbone and 4 differ-\nent heads; each head is in charge respectively of predict-\ning which notes are present in each frame, its velocity and\nwhether there is an onset or offset happening. Figure 3\nshows an overview of the network. The backbone con-\nsists of multiple convolutional layers, and it is divided into\nthree main sections. The ﬁrst section consists of 3 blocks\nwith 2D convolutions, whose kernels are squarely shaped\n(7×7)and perform initial processing of the CQT spectro-\ngram. The second section is in charge of doing the back-\nbone’s harmonic processing; this is where either HD-Conv,\nMRDC-Conv, or TriAD will be placed. The last block con-\nsists of 5 2D convolutional layers with ﬁlter shape (1×5),\nspanning across the time dimension2.\nThe output of the backbone is then used as input for\nthe four heads. Each head consists of a bidirectional\nlong short-term memory (LSTM) [18] and a dense layer.\nLSTMs model sequential data, which are the features as-\nsociated with each output bin in this case. The dense layer\ntakes the features outputted by the LSTM and produces a\nsingle value for each of the 88notes of a piano. Details\nabout the design choices of HPPNet can be found in [8].\nWe run our experiments by comparing the model’s per-\nformance when the backbone’s harmonic processing is\ndone either by our method (TriAD), MRDC-Conv [6], or\nHD-Conv [8]. We use those methods as employed in their\nrespective papers: 12kernels of shape (1×1)in the case\nof [6], and 8 kernels with shape (3×1)in the case of [8].\nFor our method, we use just two kernels, one dilated for\nperfect ﬁfths, and another one for major thirds; these are\n2The third block differs from the original paper description; following\ntheir description, that block of the backbone alone has 983.040parame-\nters, whereas the paper speciﬁes that the backbone contains 421K param-\neters. We used the network as implemented in the ofﬁcial repo, which\nmatches the number of parameters and replicates their reported resultsProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n32the intervals with the most associated harmonics. Our ker-\nnels span 3 octaves ( ko= 3) and a single frame ( kt= 1).\nThe code for MRDC-Conv and HD-Conv can be found in\ntheir ofﬁcial repositories3 4. We do not train a version\nof the model with a “harmonically agnostic” block, as [8]\nalready shows in an ablation study that the model’s perfor-\nmance drops signiﬁcantly in that case.\nAs optimizer, ADAM [19] with a learning rate of 6·\n10−3was used. We trained all the models for 200.000\nsteps, where each step consists of a batch size of 4 chunks\nof audio. The evaluation was done on MAESTRO’s evalu-\nation dataset every 500steps, to check for possible cases\nof overﬁtting. The models were trained 3times, each\none with a random weight initialization. All the harmonic\nblocks take a similar time to train, around 24h to complete\nin a V100 GPU.\nThe employed loss is the same one as in HPPNet’s pa-\nper [8], a combination of individual losses for the frame,\nonset, offset, and velocity heads. Weighted binary cross\nentropy (see Equation 2) was used as loss for the frame,\nonset and offset heads. This loss is used since there are few\npositive onset labels, yet predicting onsets is necessary to\ndistinguish consecutive notes. The parameter wcontrols\nthe relevance of positive labels in the loss and is chosen\nasw= 1 for offsets and frames, and w= 2 for onsets.\nThe loss for the velocity head is the mean squared error\nbetween the expected and estimated velocities of each in-\ndividual note.\nlbce(y,ˆy) =−wy·log(ˆy)−(1−y)·log(1−ˆy)(2)\n5. RESULTS\nThe metrics reported follow the convention described in\n[20]. These metrics report different aspects of the tran-\nscription. The frame metric operates at the frame level,\nwhile the other three operate at the note level. Within the\nnote level, three different metrics exist, considering off-\nsets and/or velocity. This is due to the partially subjective\nnature of this task. The onset (referred to as the moment\nwhen a certain note starts to sound) is not very subjective\ngiven the sharp attack of the piano [21]. In contrast, off-\nset (the moment when a certain note stops sounding) and\nvelocity are less objective aspects of the transcription. An\nestimate of a note is assumed to be correct if its onset is\nwithin±50ms of the reference, and its pitch is correct.\nWhen contemplating offsets, in addition to the previous\nrequisites, the estimation’s offset should also be within a\ncertain range; this range is either ±50ms or20% of the\nreference note’s duration, whatever is larger.\nVelocity estimation is more intricate, as depending on\nthe microphone position a note played with a certain ve-\nlocity can sound louder or quieter. We use the procedure\ndescribed in [2], which involves rescaling velocities and\nusing linear regression to account for the aforementioned\n3https://github.com/WX-Wei/HarmoF0\n4https://github.com/WX-Wei/HPPNet\nFigure 3 : A diagram of HPPNet. The brackets’ numbers\nrepresent the sizes of the channel, frequency, and frame\ndimensions. Letter dindicates the dilation rate.\ndifference in loudness. All the metrics were calculated us-\ningmir_eval [22].\nThe scores for Section 4 experiments are in Table 2. We\nalso report the results of some larger models of the SOTA\nas reference. Onsets & Frames [2] is among the most\nwell-known DL models for piano transcription. Semi-\nCRFs [3] is a method designed to improve the predictions\nmade about the offsets. These are large and capable mod-\nels, but the ones using harmonic knowledge also manage\nto achieve similar results with notably fewer parameters.\nBoth TriAD and HD-Conv blocks achieve similar results,\nin pair with large models. The MRDC-Conv block uses\nfewer parameters than HD-Conv, but in exchange drops in\nperformance. Noticeably, the model using TriAD has the\nsame number of parameters as the one MRDC-Conv, yet it\ndoes not drop in performance.\n5.1 Kernel dilation relevance\nIn music theory, some intervals are more important than\nothers. Equally, some music intervals have more harmon-\nics associated with them than others, as shown in Table\n1. It could be expected, that using a kernel dilated with a\nhighly relevant interval yields better results than a kernel\nassociated a with less relevant interval. We tested whether\nthis assumption held or not in our method; instead of using\nmultiple kernels as previously described, our block con-\nsists of a single kernel for these experiments. We used 2\nrelevant intervals (perfect ﬁfth, major third), and 2 lesser\nrelevant intervals (minor second, major seventh) to test the\naforementioned assumption. These kernels span 3 octaves\n(ko= 3) and a single frame ( kt= 1), as in the previous ex-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n33Model # ParametersFRAME F1 NOTE F1 NOTE W/OFFSET F1 NOTE W/OFFSET & VEL. F1\nMAESTRO\nOnsets & Frames [2]* 26M 89.68% 95 .22% 79 .44% 78 .85%\nSemi-CRFs [3] 9M 90.75% 96 .11% 88.42% 87.44%\nHPPNet + HD-Conv 820K 91.62%(±.02) 96.14%(±.01) 82 .91%(±.02) 80 .91%(±.02)\nHPPNet + MRDC-Conv 780K 78.69%(±.01) 84.71%(±.01) 58 .77%(±.01) 52 .15%(±.03)\nHPPNet + TriAD (ours) 780K 91.50%(±.02) 96.16%(±.01) 82 .62%(±.02) 80 .76%(±.01)\nMAPS\nHPPNet + HD-Conv 820K 72.45%(±.02) 86.09%(±.01) 42.77%(±.02) 40 .11%(±.02)\nHPPNet + MRDC-Conv 780K 63.25%(±.01) 73.87%(±.02) 32 .68%(±.02) 32 .68%(±.01)\nHPPNet + TriAD (ours) 780K 72.39%(±.03) 85.06%(±.02) 42 .41%(±.02) 40.17%(±.02)\nTable 2 : Results for the experiments described in Section 4. In our experiments, each model was trained three different\ntimes. The metrics here reported are the average across these runs and in parenthesis the variance. * Results from [8].\nModelMajor third Perfect ﬁfth Minor second Major seventh\nMAESTRO MAPS MAESTRO MAPS MAESTRO MAPS MAESTRO MAPS\nHPPNet + TriAD 90.14%(±.02) 71.58%(±.01) 90.23%(±.02) 71.98%(±.01) 83.16%(±.01) 68.53%(±.01) 83.36%(±.01) 69.19%(±.02)\nHPPNet + HD-Conv 84.89%(±.01) 69.96%(±.02) 85.98%(±.02) 70.50%(±.03) 84.23%(±.03) 67.86%(±.03) 84.79%(±.01) 68.69%(±.02)\nTable 3 : F1 framewise results for the single kernel experiments described at section 5.1. Our method obtains worse results\nif a “less relevant” music interval is chosen. HD-Conv achieves more similar results regardless of the dilation, with just a\nsmall improvement for the case of the perfect ﬁfth (where it employs two kernels).\nperiment. We also used the method with constant dilations\ni.e. HD-Conv from [8], equally using single kernels except\nfor the case of the perfect ﬁfth. There are two harmonics\nassociated with the perfect ﬁfth within the ﬁrst 3 octaves,\nso we employ two rather than a single kernel. The constant\ndilations capture in this case major third: 5thharmonic;\nperfect ﬁfths, 3rdand6thharmonics; minor second, 17th\nharmonic; and major seventh 30thharmonic. We noticed\nthat after 50.000steps, the speed at which the loss dimin-\nished slowed down sensibly, and therefore, we reduced the\nnumber of training steps for this experiment and trained for\n70.000steps in each run.\nThe results can be seen in Table 3. HD-Conv [8] ob-\ntains slightly better results for the perfect ﬁfth kernels, but\nsimilar results for other cases. Our method (TriAD) has\na distinguishable performance gap depending on the inter-\nval. Results are worse for minor second and major seventh\nintervals, compared to the cases of the major third and the\nperfect ﬁfth. Moreover, in those two cases, our method\nachieves notably better results than HD-Conv.\n6. CONCLUSIONS\nIn this paper, we presented TriAD, a novel convolutional\nblock for NNs capable of capturing the harmonics related\nto music intervals. To obtain such information, we sepa-\nrate octave and pitch class dimensions from log-frequency\nspectrograms and create convolutional kernels speciﬁcally\ndesigned to process this disentangled representation. We\ntested and compared our method with other ones designed\nto capture harmonic information, in the task of piano-\nAMT. We also compared how our model performed when\nonly a single kernel was employed. To the best of our\nknowledge, our method is the only one capable of achiev-\ning dilated convolutions which are not “equally spaced”along the frequency axis, allowing our model to capture\nmultiple harmonics using a small kernel. To achieve this\neffect, other approaches require applying different convo-\nlutional layers to the same input [6, 8] or using large ker-\nnels [12].\nOur method is still capable of reaching the performance\nachieved by other harmonic blocks while making use of\nfewer parameters, showing the effectiveness of our ap-\nproach. Furthermore, the results from the experiment de-\nscribed in Subsection 5.1 show that our method’s perfor-\nmance highly depends on the dilation choice, thus hinting\nthat our method is indeed using the harmonics to determine\nwhich pitches are present. Moreover, with an appropriate\ndilation choice our model outperforms other methods also\nusing a single kernel.\nHarmonic series are relevant for other tasks beyond\nAMT, for example, instrument recognition. Some works\nhave found that the harmonics and their respective am-\nplitudes are crucial to correctly classifying instruments\n[23, 24]. Our method could be employed to capture the\namplitude of different harmonics and learn speciﬁc pat-\nterns for each instrument. In future work, we will use “har-\nmonically designed” networks in other AMT related tasks.\nRecent advances in AMT such MT3 [25] demonstrate that\nwith the current DL techniques is possible to transcribe an\narbitrary number of instruments from a piece of music au-\ndio instead of just piano as shown here. Since the harmon-\nics are relevant for instrument recognition, we hypothesize\nusing harmonic blocks such as the ones presented here, the\naccuracy with which notes are assigned to each instrument\nin systems like MT3 could improve. We release code for\nreproducibility experimentation5.\n5https://github.com/migperfer/TriAD-ISMIR2023Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n347. REFERENCES\n[1] M. Mauch and S. Dixon, “Approximate note transcrip-\ntion for the improvedidentiﬁcation of difﬁcult chords,”\ninProceedings of the 11th International Society for\nMusic Information Retrieval Conference , 2010.\n[2] C. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Simon,\nC. Raffel, J. Engel, S. Oore, and D. Eck, “Onsets and\nFrames: Dual-Objective Piano Transcription,” in Pro-\nceedings of the 19th International Society for Music\nInformation Retrieval Conference , 2018.\n[3] Y . Yan, F. Cwitkowitz, and Z. Duan, “Skipping the\nframe-level: Event-based piano transcription with neu-\nral semi-crfs,” in Advances in Neural Information\nProcessing Systems , M. Ranzato, A. Beygelzimer,\nY . Dauphin, P. Liang, and J. W. Vaughan, Eds., vol. 34,\n2021.\n[4] R. M. Bittner, B. McFee, J. Salamon, P. Li, and J. P.\nBello, “Deep Salience representations for F0 estima-\ntion in polyphonic music,” in Proceedings of the 18th\nInternational Society for Music Information Retrieval\nConference , 2017.\n[5] Ji ˇrí Balhar and Jan Haji ˇc jr., “Melody extraction us-\ning a harmonic convolutional neural network,” MIREX\nMelody Extraction Report, Tech. Rep., 2019.\n[6] W. Wei, P. Li, Y . Yu, and W. Li, “HarmoF0: Logarith-\nmic Scale Dilated Convolution for Pitch Estimation,”\nin2022 IEEE International Conference on Multimedia\nand Expo (ICME) , Jul. 2022.\n[7] X. Wang, L. Liu, and Q. Shi, “Enhancing Piano Tran-\nscription by Dilated Convolution,” in 2020 19th IEEE\nInternational Conference on Machine Learning and\nApplications (ICMLA) , Dec. 2020.\n[8] W. Wei, P. Li, Y . Yu, and W. Li, “HPPNet: Model-\ning the Harmonic Structure and Pitch Invariance in Pi-\nano Transcription,” in Proceedings of the 23th Interna-\ntional Society for Music Information Retrieval Confer-\nence, 2022.\n[9] R. M. Bittner, J. J. Bosch, D. Rubinstein, G. Meseguer-\nBrocal, and S. Ewert, “A Lightweight Instrument-\nAgnostic Model for Polyphonic Note Transcription and\nMultipitch Estimation,” in ICASSP 2022 - 2022 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , May 2022.\n[10] V . Lostanlen and C. Carmine-Emanuele, “Deep convo-\nlutional networks on the pitch spiral for musical instru-\nment recognition,” in Proceedings of the 18th Interna-\ntional Society for Music Information Retrieval Confer-\nence, 2017.\n[11] J.-F. Ducher and P. Esling, “Folded cqt rcnn for real-\ntime recognition of instrument playing techniques,” in\nProceedings of the 20th International Society for Mu-\nsic Information Retrieval Conference , 2019.[12] X. Wang, L. Liu, and Q. Shi, “Harmonic Structure-\nBased Neural Network Model for Music Pitch Detec-\ntion,” in 2020 19th IEEE International Conference on\nMachine Learning and Applications (ICMLA) , Dec.\n2020.\n[13] J. C. Brown, “Calculation of a constant Qspectral\ntransform,” The Journal of the Acoustical Society of\nAmerica , vol. 89, no. 1, Jan. 1991.\n[14] B. Graham, M. Engelcke, and L. v. d. Maaten, “3d se-\nmantic segmentation with submanifold sparse convo-\nlutional networks,” in 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2018.\n[15] A. Elowsson, “Polyphonic pitch tracking with deep\nlayered learning,” The Journal of the Acoustical\nSociety of America , vol. 148, no. 1, 2020. [Online].\nAvailable: https://doi.org/10.1121/10.0001468\n[16] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon,\nCheng-Zhi, A. Huang, S. Dieleman, E. Erich, J. Engel,\nand D. Eck, “Enabling factorized piano music model-\ning and generation with the maestro dataset,” in Pro-\nceedings of the 7th International Conference on Learn-\ning Representations , 2019.\n[17] V . Emiya, R. Badeau, and B. David, “Multipitch esti-\nmation of piano sounds using a new probabilistic spec-\ntral smoothness principle,” IEEE Transactions on Au-\ndio, Speech, and Language Processing , vol. 18, no. 6,\n2010.\n[18] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural Computation , vol. 9, no. 8, 1997.\n[19] D. P. Kingma and J. Ba, “Adam: A method for\nstochastic optimization,” in International Conference\non Learning Representations (ICLR) , 2015.\n[20] J. Salamon, “Melody extraction from polyphonic mu-\nsic signals,” Ph.D. dissertation, Universitat Pompeu\nFabra, Barcelona, Spain, 2013.\n[21] A. Ycart, L. Liu, E. Benetos, and M. T. Pearce, “In-\nvestigating the perceptual validity of evaluation metrics\nfor automatic piano music transcription,” Transactions\nof the International Society for Music Information Re-\ntrieval , vol. 3, no. 1, 2020.\n[22] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P.W., “mir_eval: A transpar-\nent implementation of common mir metrics,” in Pro-\nceedings of the 15th International Society for Music\nInformation Retrieval Conference , 2014.\n[23] Y . Mo, “Music timbre extracted from audio signal\nfeatures,” Mobile Information Systems , vol. 2022,\nJun 2022. [Online]. Available: https://doi.org/10.1155/\n2022/1349935Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n35[24] A. Livshin and X. Rodet, “The signiﬁcance of the non-\nharmonic \"noise\" versus the harmonic series for musi-\ncal instrument recognition,” in Proceedings of the 7th\nInternational Society for Music Information Retrieval\nConference , 2006.\n[25] J. Gardner, I. Simon, E. Manilow, C. Hawthorne, and\nJ. Engel, “MT3: Multi-task multitrack music transcrip-\ntion,” in International Conference on Learning Repre-\nsentations (ICLR) , 2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n36"
    },
    {
        "title": "Contrastive Learning for Cross-Modal Artist Retrieval.",
        "author": [
            "Andres Ferraro",
            "Jaehun Kim",
            "Sergio Oramas",
            "Andreas F. Ehmann",
            "Fabien Gouyon"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265303",
        "url": "https://doi.org/10.5281/zenodo.10265303",
        "ee": "https://zenodo.org/records/10265303/files/000044.pdf",
        "abstract": "Music retrieval and recommendation applications often rely on content features encoded as embeddings, which provide vector representations of items in a music dataset. Numerous complementary embeddings can be derived from processing items originally represented in several modalities, e.g., audio signals, user interaction data, or editorial data. However, data of any given modality might not be available for all items in any music dataset. In this work, we propose a method based on contrastive learning to combine embeddings from multiple modalities and explore the impact of the presence or absence of embeddings from diverse modalities in an artist similarity task. Experiments on two datasets suggest that our contrastive method outperforms single-modality embeddings and baseline algorithms for combining modalities, both in terms of artist retrieval accuracy and coverage. Improvements with respect to other methods are particularly significant for less popular query artists. We demonstrate our method successfully combines complementary information from diverse modalities, and is more robust to missing modality data (i.e., it better handles the retrieval of artists with different modality embeddings than the query artist's).",
        "zenodo_id": 10265303,
        "dblp_key": "conf/ismir/FerraroKOEG23",
        "keywords": [
            "music retrieval",
            "content features",
            "embeddings",
            "complementary embeddings",
            "contrastive learning",
            "artist similarity task",
            "modalities",
            "artist retrieval accuracy",
            "coverage",
            "missing modality data"
        ],
        "content": "CONTRASTIVE LEARNING FOR CROSS-MODAL ARTIST RETRIEV AL\nAndres Ferraro Jaehun Kim Sergio Oramas\nAndreas Ehmann Fabien Gouyon\nPandora-SiriusXM, Oakland\nandres.ferraro@siriusxm.com\nABSTRACT\nMusic retrieval and recommendation applications often\nrely on content features encoded as embeddings, which\nprovide vector representations of items in a music dataset.\nNumerous complementary embeddings can be derived\nfrom processing items originally represented in several\nmodalities, e.g., audio signals, user interaction data, or ed-\nitorial data. However, data of any given modality might\nnot be available for all items in any music dataset. In this\nwork, we propose a method based on contrastive learning\nto combine embeddings from multiple modalities and ex-\nplore the impact of the presence or absence of embeddings\nfrom diverse modalities in an artist similarity task. Experi-\nments on two datasets suggest that our contrastive method\noutperforms single-modality embeddings and baseline al-\ngorithms for combining modalities, both in terms of artist\nretrieval accuracy and coverage. Improvements with re-\nspect to other methods are particularly signiﬁcant for less\npopular query artists. We demonstrate our method success-\nfully combines complementary information from diverse\nmodalities, and is more robust to missing modality data\n(i.e., it better handles the retrieval of artists with different\nmodality embeddings than the query artist’s).\n1. INTRODUCTION AND RELATED WORK\nThe MIR community has dedicated signiﬁcant effort\nto deﬁning and computing music similarity in the last\n20years. Music similarity can be used in multiple down-\nstream tasks, from playlist continuation, music visualiza-\ntion/navigation, music categorization for organizing cata-\nlogs, or for personalized recommendations. The notion of\nsimilarity is subjective and there is no consensus on how to\ndeﬁne and evaluate it [1]. To evaluate the performance of a\nmusic similarity algorithm, some previous works either fo-\ncus on content-based aspects, such as melody or harmony.\nOther works measure similarity based on cultural aspects ,\nsuch as based on the co-occurrence of items in playlists or\non editorial data –this is the approach of our work.\n© A. Ferraro, J. Kim, S. Oramas, A. Ehmann and F.\nGouyon. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: A. Ferraro, J. Kim, S. Ora-\nmas, A. Ehmann and F. Gouyon, “Contrastive learning for cross-modal\nartist retrieval”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.Multiple methods have been proposed to compute mu-\nsic similarity based on a variety of data types related to the\nmusic, e.g., based on audio descriptors [2], document sim-\nilarity [3], or graphs of musical connections [4, 5]. Some\nrelatively recent works propose ways to produce embed-\ndings –that can be used to compute music similarity– in\na supervised or unsupervised way, by training models on\nlarge amounts of data (such as audio, text or image). Such\npre-trained models, which are often released publicly, may\nproduce feature representations –i.e. embeddings– that are\neffective for previously unseen tasks. Such embeddings\ncan be computed from diverse types of modalities related\nto music such as audio [6–8], tags [9], album covers im-\nages [10], or biographies [4]. The multiple modalities of\ndata that can describe a music item –such as audio, tags,\nor listening interactions– may contain complementary in-\nformation. For example, the quality and scale of audio vs\ncollaborative data has been shown to have signiﬁcant inﬂu-\nence in autotagging tasks [11]. It therefore appears bene-\nﬁcial to combine diverse complementary modalities to ob-\ntain a more informative representation of music items. In\nfact, recent research identiﬁes the combination of diverse\nsources of data as specially promising for mitigating limi-\ntations and issues in music recommendation research [12].\nAnother aspect to take into account is that in any given\nmusic dataset, data of diverse modalities might be avail-\nable for different subsets of items. Therefore, when query-\ning with an item represented in a given modality, the max-\nimum coverage for retrieval is limited to items for which\nthat same modality is available, leaving out a potentially\nsigniﬁcant –and relevant– part of the dataset. For example,\nthe availability of listening interactions or users’ explicit\nfeedback is highly dependent on item popularity. There-\nfore, for artists with very little listening and user feed-\nback, it may not be possible to obtain embeddings from\nthat modality. Embeddings from other modalities may suf-\nfer from the same issue, either because there is no data\navailable to produce an embedding or because the quality\nof the available information is very low. For instance in\nthe case of a model trained on tag annotations to produce\nartist embeddings, where the output embedding may not\nbe very informative for those artists that have a single or\nfew tag annotations. Such issues are particularly common\nand problematic emerging or more underground artists, for\nwhich the available information is more limited.\nIn order to mitigate the issue of availability of some\nmodalities, it is important to combine and take full ad-375vantage of all information available so that when querying\nwith an artist that has only one modality available, we can\nalso retrieve artists for which we have a different modal-\nity information. Therefore, the focus of this work is to\ncombine diverse modalities into a common shared space\nthat is beneﬁcial for 1) leveraging each modality informa-\ntion from the artists, and 2) allowing to operate on a single\nspace that covers the full population of artists, ensuring\nthat whether or not an artist is retrieved for another does\nnot depend on the number of modalities available.\nThe problem of combining embeddings from diverse\nmodalities in a shared representation has received some at-\ntention in the last few years. In the music domain, there\nhave been some works on combining embeddings by sim-\nple concatenation [13] or predicting one modality from an-\nother [14]. Contrastive learning techniques go beyond sim-\nple concatenation or prediction, trying to learn a shared\nrepresentation between embeddings from different modal-\nities. Some examples of research related to multimodal\ncontrastive learning can be found in [10], where embed-\ndings from a shared multimodal space are used as ad-\nditional features for classiﬁcation, or in [15, 16] where,\ne.g., music audio can be retrieved from natural language\ndescriptions. In this work, we propose to apply a con-\ntrastive learning method that maps embeddings from di-\nverse modalities to a shared embedding space, extending\nthe advantages of multiple modalities to populations that\nwould not be covered otherwise.\nIn summary, in this work we propose an approach to\ncombine the multiple encoders of a contrastive learning\nmethod, showcasing several improvements over baselines\nand single-modality approaches in an artist similarity task.\nWe show under two different contexts –using an open and\nan in-house dataset– that our proposed approach:\n• achieves higher performance in terms of accuracy\nand coverage of retrieved artists (§ 3.1),\n• successfully combines complementary information\nfrom diverse modalities (§ 3.2),\n• is more robust to missing modality data (§ 3.3),\n• particularly increases the performance for less pop-\nular query artists (§ 3.4).\n2. METHODOLOGY\n2.1 Single-Modality Embeddings and Contrastive\nMethod\nIn this work, we use three modalities, namely: tags, user-\nlistening interactions (i.e. collaborative ﬁltering data, re-\nferred to as CF), and audio information. In all cases, we\nuse pre-trained models to obtain embeddings for each of\nthe modalities. We evaluate artist similarity performance\nusing the embeddings from the pre-trained models directly,\nand compare to the performance when using the embed-\ndings produced by our contrastive method which is trained\nwith the same embeddings from pre-trained models.\nIn these experiments we apply a contrastive learning\nloss based on InfoNCE [17]. Speciﬁcally, we deﬁne the\ncontrastive loss between two modalities, ψψψaandψψψb, as:Lψψψa,ψψψb=M/summationtext\ni=1−logΞ(ψψψi\na,ψψψi\nb,τ)\n2M/summationtext\nk=1⊮[k̸=i]Ξ(ψψψia,ζζζk,τ), where M is the\nbatch size and τis the temperature parameter. We deﬁne\nΞ(a,b,τ) = exp( cos(a,b)τ−1), based on the cosine sim-\nilarity.ζζζkis deﬁned as ψψψk\na, ifk≤Mand elseψψψk−M\nb.\nThis loss function attempts to minimize the distance be-\ntween the modalities of the same artist while maximizing\nthe distance with any modality from other artists.\nWe use three encoders –one for each modality– that\nwill produce three representations in our shared space for\neach artist. During training1we minimize the sum of the\npairwise losses between each of the modalities as in [18]:\nLtot=LAudio-Tag+LAudio-CF+LTag-CF\nOnce the model is trained with the contrastive method\nand we want to use it for inference, for a given artist, we\naggregate the output of each internal encoder by averaging\nall available information.\n2.2 Training Data\nIn order to investigate the effectiveness of our contrastive\nmethod under different situations, we train our model us-\ning two independent datasets: We use a dataset based on\npublic data to facilitate the reproducibility of some of the\nresults. And we also use an in-house dataset that contains\nmultimodal information for a larger set of artists.\nTraining our model requires full coverage of the three\nmodalities for all artists –tag-based embeddings, CF em-\nbeddings, and audio embeddings. For the public dataset,\nwe use the Million Song Dataset (MSD) [19] and its con-\nnections with other datasets to collect tags, audio and CF\nembeddings. We collected audio track embeddings using\nthe public unsupervised model from [6] to extract embed-\ndings from MSD audio previews, then we averaged all au-\ndio tracks embeddings for each artist. The tagging data\nwas collected from the MSD500 dataset [11] and embed-\ndings were computed using PMI factorization [13] of 500\ntags. The CF embeddings were obtained using weighted\nmatrix factorization [20] based on the Echonest Proﬁle\ndataset,2with Gaussian process-based Bayesian hyperpa-\nrameter tuning [21]. We gathered information from the\nthree modalities for 17,478artists.\nFor the in-house dataset (hereafter, OWN) we collected\ntags, CF, and audio information for 38,301artists. This\ndataset is larger than MSD and includes what we believe\nishigher-quality tags and CF data , which allows us to\ncompare the performance of our approach in a different\nsetting. The CF information is computed from very large\namounts of user-listening interactions on a streaming plat-\nform. The audio embeddings are computed using the su-\npervised model3described in [6]. The tag embeddings are\n1For both datasets we use Adam optimization with a learning rate of\n0.0001 and temperature of 0.1. We use a fully connected layer of 256 for\nthe CF encoder, two layers with 512 and 256 for the Audio encoder and\n4 attention heads of 256 for the tag encoder. The learned space has 200\ndimentions. Batch size for COWN is 2048 and for CMSD is 128.\n2Speciﬁcally, we aggregated the per-song listening counts correspond-\ning artists such that we obtain the ‘user-artist’ listening matrix.\n3i.e. a different model for audio embeddings than when training on\nMSD.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n376computed using PMI factorization from a total of 6,421\ndifferent tags, which are a combination of manual and au-\ntomatic annotations. Since our pre-trained models for au-\ndio and CF are at the track level, we compute artist embed-\ndings by averaging over artist track embeddings.\nIn the remainder of this work, we refer to the model\ntrained with the contrastive method with in-house data as\nCOWN and the model trained with public data as CMSD .\n2.3 Evaluation Dataset\nThe ground truth for artist similarity is deﬁned herein by\nthe OLGA public dataset [22], containing artist similarity\ninformation collected from AllMusic. Our evaluations are\ntherefore based on a cultural ground-truth, following [5].\nWe collected data from the MSD dataset for the orig-\ninal17,646artists in OLGA. We obtained tag data from\nthe MSD500 for 10,971 (62%) artists, user interaction\ndata from the Echonest Proﬁle dataset for 15,389(87%)\nartists, and audio embeddings using MSD audio previews\nfor100% of the artists.4\nWe also create a subset of OLGA where allartists con-\ntain complete tags, user interaction, and audio information\nfrom MSD. We refer to this subset as OLGA Full Modal-\nity Coverage (FMC), which contains 9,474artists and it\nis also mapped to our internal dataset. The OLGA FMC\nsubset is used to compare the results of multiple methods\npre-trained on different and independent datasets.\n2.4 Evaluation Conditions\nIn order to provide insights on the performance of the con-\ntrastive method, we conduct analyses under 3 different sit-\nuations, varying the degree of availability of the different\nmodalities in the evaluation data:\nRaw evaluation dataset: In one condition, we compare\nthe methods using all the artists in the OLGA dataset. In\nthis case, we are interested to understand performance in\na scenario of a real –uncontrolled– evaluation dataset, ac-\ncounting for some organic imbalance of the availability of\ndata in different modalities.\nFull Modality Coverage: In another condition, we use the\nOLGA FMC subset where all artists contain CF, tags, and\naudio embeddings in both MSD and OWN datasets. In this\ncase, we want to understand performance while factoring\nout the potential inﬂuence of one or another modality being\nonly partially available in evaluation.\nSystematic variation of modality coverage: We also\nperform multiple comparisons by grouping artists from\nOLGA depending on how many modalities are available.\nHere, we want to look at how much the contrastive method\nand the baselines are capable of doing cross-modality re-\ntrieval when using different modalities as input. In par-\nticular, we want to see whether or not they are capable of\nretrieving artists that have different modality information\n4Note that we don’t control for artist separation between MSD, OWN\nand OLGA. But even if some artists may be present in both train and\ntest sets, the artist similarity information from OLGA is only used for\nevaluation, and is never used during the training of the single-modality\nembeddings nor the contrastive models on either MSD or OWN.available compared to the query artists. Therefore, in this\npart, we create 7groups of artists –at random– of equal size\nwith each group containing one, two, or three modalities\n(namely, CF, audio, tag, CF+audio, CF+tag, audio+tag,\naudio+CF+tag). We refer to these groups as ‘Modality\nGroups’. It is important to highlight the artiﬁciality of this\nsetting. We are considering an extreme case only to eval-\nuate cross-modality retrieval capabilities of the methods.\nWe are not considering here the accuracy of these results\nsince it is already evaluated in the other analyses.\n2.5 Baseline multimodal approaches\nFor multi-modal baselines, we employ two conventional\nmodels: PCA, and Gaussian random projection [23,\n24] (which we refer to as Rand).5For ﬁtting these models,\nwe consider artists who have access to all modalities. Their\nmultimodal embeddings are concatenated and treated as a\nsingle feature vector. It yields a dimensionality of 2,063\nfor the MSD dataset, and 2,528for the OWN dataset. We\nset the reduced dimensionality to 200, which is the same\nsize as the embeddings of the contrastive model. If an artist\nhas a missing modality in the prediction phase, we employ\nthe global mean embedding of the missing modality.6\n2.6 Metrics\nAccuracy: We consider nDCG@200 to measure how ac-\ncurate the retrieved artists are compared to the ground truth\nwhile taking into account the position in the ranking of the\nretrieved artists, a metric considered robust to missing rel-\nevance information [26].7\nDistribution: We also compute the Gini@200 index, mea-\nsuring the distribution of the top 200 retrieved artists in\neach experimental condition across the whole set of artists.\nA lower value of Gini indicates that the recommendations\nacross artists are more uniformly distributed –covering\nmore artists retrieved– while a higher value of Gini indi-\ncates that the recommendations are focused on only the\nfew same artists.\nWe compute the conﬁdence interval using the bootstrap\nmethod [27] on the evaluation artist population. We report\nthem in Figure 1 at 95% conﬁdence level.\nExpected Contrastive Loss: We propose an additional\nmetric that we named Expected Contrastive Loss (ECL).\nWe use this measure to analyze to what extent an artist\nis coherent with respect to their multimodal representa-\ntions. From how we deﬁned the loss in Section 2.1, a\nhigh loss value implies that the artist is relatively difﬁ-\ncult to be distinguished from other artists. Once the train-\ning is reasonably progressed, we employ ECL to quan-\ntify how “coherent” the artist is with respect to their in-\nternal representations obtained from the different modali-\nties, which is deﬁned as: ECL(i,u,v) =duv\nii−Ej\\i[duv\nij],\n5For both algorithms, we employ the standard implementation pro-\nvided from scikit-learn [25].\n6This does not happen in FMC\n7We focus on nDCG@200 in this work, as we experimentally ob-\nserved high correlation with other retrieval metrics such as precision, re-\ncall, and R-Precision.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n377whereiandjdenote artist index, while uandvrefer to\nthe modality index. duv\nijmeans the cosine distance be-\ntween artist ifrom modality uand artistsjfrom modal-\nityv. Taking expectation over all the possible modal-\nity pairs leads to the ﬁnal coherency measure for artist i:\nECL(i) =Eu,v\\u[ECL(i,u,v)].\nClustering: We further analyze the multimodal embed-\nding space of the contrastive model, by investigating how\nwell the artist embeddings are clustered. The contrastive\nmethod essentially can be seen as a “supervised” cluster-\ning task, where we minimize the distance among “posi-\ntive points” (i.e., multimodal embeddings from an artist)\nand maximize the distance between those to the “negative\npoints” (i.e., embeddings belonging to the other artists). It\nimplies that an artist will get a higher training loss when\nthe embeddings are dispersed and overlapped with the em-\nbedding cluster of other artists, while the opposite cases\nwill get lower values. The model will ﬁt the multimodal\nembedding space such that the artist embeddings poorly\nclustered initially have more concentrated and distant clus-\nters. While the contrastive learning implements this natu-\nrally by its loss function, there are other well-known mea-\nsures for the validation of the clustering methods, such as\nintra-cluster distance ( CDintra )indicating how an artist\nembeddings are well clustered together, and inter-cluster\ndistance (CDinter )indicating how an artist-speciﬁc em-\nbedding cluster is far and distinct from others’.8\n3. RESULTS\n3.1 Performance comparison of contrastive method\nWe now look at the performance of the contrastive method\nwhen some modality information is missing in the evalu-\nation dataset (using the raw OLGA dataset) and when all\nmodalities are available for each artist (FMC subset). We\nalso compare the performance of the contrastive method to\nthe baseline methods and to single-modality embeddings.\n3.1.1 Performance with incomplete modality information\nFocusing on the different combinations of input modali-\nties to the contrastive method, we can see in Table 1 that\nthe highest nDCG result is obtained when combining all\nmodalities as input. We therefore focus only on this model\nfor the remainder of the work.\nFigure 1a shows the results for all artists in OLGA.\nWe can see that when using features from MSD, the con-\ntrastive method outperforms the baselines and the original\nembeddings in all the metrics. The contrastive method al-\nways gives a better Gini compared to the other methods\n–which means that the distribution of retrieved artists is\nmore uniform– while outperforming the other models in\nnDCG.9\n8we compute CDintra as the mean cosine distance between multi-\nmodal embeddings of an artist to their centroid in the multimodal space\nof learned contrastive model. CDinter is computed as the mean distance\nbetween the centroids of target artist and of all the other artists.\n9OWN-OGLA is omitted since we observe a similar behaviour.CFTagAudioRandPCA\nCA+CF+T0.150.20\n0.250.300.350.400.45\n(a) MSD-OLGACFTagAudioRandPCA\nCA+CF+T0.200.250.300.35\n0.20.30.4\n(b) MSD-FMCnDCG@200 Gini@200\nCFTagAudioRandPCA\nCA+CF+T\n(c) OWN-FMC\nFigure 1 : Performance comparison between contrastive\nand other methods. Training with MSD (a and b) or with\nOWN (c), Evaluation on OLGA (a) or on FMC (b and c).\nOLGA FMC\nnDCG@200 Gini nDCG@200 Gini\nCA+CF+T0.2387 0.2264 0.3560 0.1666\nCA+T0.2282 0.2035 0.3407 0.1559\nCA+CF0.1381 0.3425 0.2319 0.1873\nCCF+T0.1781 0.3467 0.3082 0.1917\nCA0.2338 0.1857 0.3471 0.1353\nCT0.1232 0.4939 0.2554 0.1745\nCCF0.1381 0.3425 0.2319 0.1873\nTable 1 : Evaluation of the contrastive method trained with\nMSD data using all combinations of modalities for OLGA\ndataset and FMC subset.\n3.1.2 Performance with complete modality information\nWhen we look at the results with Full Modality Coverage\n(Figures 1b and 1c), the contrastive method outperforms\nthe baselines and the pre-trained models in all the metrics\nboth when trained with MSD data or with OWN data.\nWhen looking at baseline performance between OLGA\nand FMC (Figures 1a and 1b), we can see that in the\nlatter, baselines are relatively close to the best single-\nmodality embeddings, but in the former (i.e. with incom-\nplete modality information) their performance drops sig-\nniﬁcantly lower than the best single-modality embeddings.\nThis is something we do not observe with the contrastive\nmethod, which suggests that the baseline models are more\nlimited in the capabilities of retrieving artists that miss\nsome of the modalities from the query artist, while our con-\ntrastive method may be more robust to missing modality\ninformation. We investigate this further in Section 3.3.\n3.2 Combining complementary modality information\nIf we focus only on the single-modality approaches, and\nMSD pre-training, Audio gives the best single-modality\nperformance in both OLGA and FMC (Figure 1a and 1b).\nOn the other hand, when pre-trained with OWN, CF is\nslightly better than Audio and Tag (Figure 1c). These re-\nsults suggest that performance is highly dependent on the\nquality of the data used to pre-train the single-modality\nembeddings. Results from Figure 1b and 1c also sug-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n378Audio CF Tag Rand PCACMSDCOWN\nEntropy 0.76 0.79 0.79 0.73 0.96 1.86 1.59\nTable 2 : Entropy of each model for Modality Groups.\nHigher values indicate better distributed retrieved artists.\n(a) Contrastive - MSD training\n (b) Contrastive - OWN training\n(c) PCA - MSD training\n (d) PCA - OWN training\nFigure 2 : Analysis of modality-group dependency ratio\nwhen restricting the information available for each group\nto one, two, or three modalities. Rows indicate the groups\nused to make the queries and the columns are the groups of\nretrieved artists. Darker green indicates a higher concen-\ntration of the retrieved artists in that cell. The color scale\nis normalized across all ﬁgures. Groups of artists are ran-\ndomized, so an ideal situation is a homogeneous color in\nthe full matrix.\ngest that, whichever single-modality embedding is best,\nour contrastive method is able to successfully build on top\nof it and still gain in performance by combining comple-\nmentary information from other embeddings.\n3.3 Robustness to missing modality data\nIn this subsection, we further analyze how the contrastive\nmethod would be able to retrieve artists depending on the\navailable information for the query artists and the candi-\ndates for retrieval. In Figure 2, we can see how artists\nare retrieved from each of the Modality Groups when only\nconsidering the top 5results for each query artist. Typ-\nically we see that with the contrastive method, the same\ngroup used for query comprises between 15-38% of the\nretrieved artists. We see however an exception for the\nCF group which obtains a larger portion of the retrieved\nartists (59%) when using OWN data to train the models.\nWhen we do a similar comparison for the PCA baseline\nmethod, we see in Figure 2 that there are higher percent-\n−0.8−0.6−0.4−0.20.0\n5 10\nPopularity MeasureECL\n0.000.250.500.751.00nDCG@200\nFigure 3 : Scatter plot of artists based on the popular-\nity proxy measure and the ECL. Each point represents an\nartist, where the color brightness represents the per-artist\nretrieval performance (nDCG@200). It is computed on the\nFMC subset with MSD data.\nFMC OLGAMSD OWN\n5 10 15 20 5 10 15 20123\n0.91.11.31.5\nArtist Popularity QuantileRelative Improvement to [CF] nDCG@200Audio Rand PCA CA+CF+T\nFigure 4 : Relative retrieval improvement against CF\nmodality. The xaxis represents the grouped popularity\nquantile in 20 levels, meaning the ﬁrst group includes\nartists whose popularity is under 5% percentile, while the\ntop 5% popular artists belongs to the last group. The yaxis\nis proportional improvement of nDCG@200 compared to\nthe CF embedding model. The dotted horizontal line indi-\ncates the retrieval performance of CF modality. FMC and\nOLGA are evaluation datasets. MSD and OWN are train-\ning conditions.\nages in the diagonal of the matrix. This indicates that most\nof the retrieved artists are concentrated in the same modal-\nity group used to make the query. Therefore, these results\nhighlight the difﬁculty for the PCA baseline method to re-\ntrieve artists beyond the query artist’s modality.\nIn Table 2 we compare the entropy of each model for the\nModality Groups. A higher entropy indicates that retrieved\nartists are better distributed across the different modality\ngroups, i.e. that retrieval is less biased by the query modal-\nity –or more robust to partial modality data in the query.\nWe can see that the contrastive model is more robust to\nmissing modality data than the single-modality embed-\ndings and the baseline approaches to combining modali-\nties. This is true when trained with MSD or with OWN.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3793.4 Effect of Popularity\nArtist popularity may be a deterministic factor in artist re-\ntrieval, both for training and evaluation. Intuitively, we\nlikely have more data about popular artists, which implies\nmore multimodal data is available for training. At the same\ntime, the scale of evaluation metric themselves can be in-\nﬂated as more popular artists would have more ground\ntruths (annotated as ‘similar artists’). To conﬁrm this, we\ncompute a proxy measure for the artist popularity (POP) as\nPOP(artist) =log(#listen+1) ,10and then further com-\npare it to other training and evaluation measures.\nFirstly, we compare POP with ECL and the retrieval\nperformance. Figure 3 shows that there is correlation\namong POP, ECL, and nDCG. In particular, ECL has a\nnegative correlation with nDCG. This is a desirable out-\ncome as a model that minimizes the contrastive loss rec-\nommends “similar” artists even though such a model is not\nbeing explicitly shown artist-relatedness ground truth dur-\ning training. Meanwhile, POP also correlates with nDCG,\nwhich demonstrates the confounding effect of popularity\nto the task itself.\nFurther, we investigate how multimodal models interact\nwith artists with different popularities. One of the beneﬁts\nof employing multiple modalities is the potential mitiga-\ntion of the information void for “cold-start” artists from\ntheir music audio data. For MIR applications, audio is\nlikely accessible even when some of the other modalities\nare not readily available. For instance, the CF modality\nis not available before artists’ songs are consumed by the\nlisteners. To conﬁrm whether the audio and further multi-\nmodal embedding models would beneﬁt less popular artists\nvia multimodality, we divided the artists in 20groups by\npopularity quantiles. For each group, we further compute\nthe relative improvement of retrieval performance (nDCG)\ncompared to the CF single modality model.\nFigure 4 suggests that the original audio embedding\nachieves better performance for the less popular artists\nin all training and evaluation conditions. The contrastive\nmodel shows improvements for the majority of the groups\ncompared to the audio, while it may have smaller or no\nimprovement over audio in the least popular group for the\nMSD dataset. In the OWN dataset, a similar trend is ob-\nserved where the contrastive model shows a small decline\nfor the most popular groups compared to the original CF\nembeddings. The two baseline models indicate relatively\nﬂat results except in the case of the MSD-FMC subset,\nwhich implies that their prediction may be more reliant on\nthe CF modality. For the MSD-FMC subset, both baselines\nfollow similar trends to the audio and contrastive model.\n3.5 Multimodal Embedding Space Analysis\nWe conduct a correlation study of multiple measures\nwhere, for each artist, we compute clustering measures\nand other key indicators such as contrastive loss ECL, re-\ntrieval performance (nDCG@200), and ﬁnally the popular-\n10#listen denotes the total listening count of the artist, computed from\nthe MSD-Echonest Proﬁle dataset.0.29−0.31 −0.34−0.25 −0.3 0.80.13 0.05 0.03 0.23\nPOPECLCD intraCD inter\nnDCG@200POP ECLCDintra−1.0−0.50.00.51.0\nFigure 5 : Correlation (Kendall’s τ) among variables of in-\nterest. Each cell indicates value of τbetween two associ-\nated variables. POP denotes the popularity measure.\nity measure. In this way, we expect to obtain a better un-\nderstanding of what contrastive learning achieves in terms\nof clustering of embeddings, and how they are connected\nto retrieval performance and popularity.\nThe result of the correlation study can be found in Fig-\nure 5. We see that the ECL is highly correlated to CDintra ,\nwhile almost independent to CDinter . Notably, in terms\nof magnitude, all other measures (ECL, CDintra , and\nPOP) are relatively more correlated to nDCG compared to\nCDinter , and also correlated to each other.11\nThese relations suggest that our contrastive learning\nmethod aims at producing an artist embedding space where\nthe diverse modalities of an artist occupy a coherent region,\nbut not necessarily a region that is unique to the artist.\nCDinter shows lower correlation with most of the other\nmeasures, which conﬁrms its relatively small connection to\nthe contrastive learning and the artist retrieval downstream\ntask. We hypothesize that this is because the maximization\nofCDinter is constrained by the artist similarity inherent\nin the multimodal information and ultimately preserved.\nThis is desirable if the ultimate goal is a representation that\ncan measure artist similarity.\n4. CONCLUSION AND FUTURE WORK\nIn this work, we propose a method based on contrastive\nlearning to combine multiple artist modalities into a sin-\ngle representation. In an artist similarity task, we show\nour method yields clear improvements over other methods\nin terms of retrieval accuracy and coverage, and success-\nfully combines complementary information from diverse\nmodalities. In particular, we investigate retrieval bias to-\nwards the query’s modality. Although our method exhibits\na slight bias towards retrieving artists with similar modal-\nity to the query, we show it handles cross-modal retrieval\nbetter than other methods. Future work may be dedicated\nto further mitigate this bias. Additionally, we show that our\nmethod is particularly beneﬁcial for less popular artists.\nOur method appears to generate an artist representation\nspace with high local coherence for intra-artist modalities,\nbut at the cost of inter-artist separation. Depending on the\nﬁnal application, this is a property that could perhaps be\nmanaged by iterating on the contrastive learning method,\nfor instance, by adapting the loss function or by adapting\nthe size of the training sample batch as suggested in [28].\n11We focus on the magnitude, as the goal of this study is to investi-\ngate the degree to which some of the key indicators are associated with\nclustering quality measures in absolute mannerProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3805. ACKNOWLEDGEMENT\nWe would like to express special thanks to Matt McCallum\nfor the help collecting audio features and Sam Sandberg for\nhis valuable comments.\n6. REFERENCES\n[1] D. P. Ellis, B. Whitman, A. Berenzweig, and\nS. Lawrence, “The quest for ground truth in musical\nartist similarity,” in ISMIR , 2002.\n[2] T. Pohle, D. Schnitzer, M. Schedl, P. Knees, and\nG. Widmer, “On rhythm and general music similarity.”\ninISMIR , 2009, pp. 525–530.\n[3] M. Schedl, D. Hauger, and J. Urbano, “Harvesting\nmicroblogs for contextual music similarity estimation:\na co-occurrence-based framework,” Multimedia Sys-\ntems, vol. 20, pp. 693–705, 2014.\n[4] S. Oramas, M. Sordo, L. Espinosa-Anke, and X. Serra,\n“A semantic-based approach for artist similarity,” in IS-\nMIR, 2015.\n[5] F. Korzeniowski, S. Oramas, and F. Gouyon, “Artist\nsimilarity for everyone: A graph neural network ap-\nproach,” Transactions of the International Society for\nMusic Information Retrieval , vol. 5, no. 1, 2022.\n[6] M. C. McCallum, F. Korzeniowski, S. Oramas,\nF. Gouyon, and A. F. Ehmann, “Supervised and un-\nsupervised learning of audio representations for music\nunderstanding,” in ISMIR , 2022.\n[7] P. Alonso-Jiménez, D. Bogdanov, J. Pons, and\nX. Serra, “Tensorﬂow audio models in Essentia,” in\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2020, pp. 266–270.\n[8] P. Alonso-Jiménez, X. Serra, and D. Bogdanov, “Mu-\nsic representation learning based on editorial metadata\nfrom Discogs,” in ISMIR , 2022.\n[9] S. Dieleman, P. Brakel, and B. Schrauwen, “Audio-\nbased music classiﬁcation with a pretrained convolu-\ntional network,” in ISMIR , 2011, pp. 669–674.\n[10] S. Oramas, F. Barbieri, O. Nieto Caballero, and\nX. Serra, “Multimodal deep learning for music genre\nclassiﬁcation,” Transactions of the International Soci-\nety for Music Information Retrieval. 2018; 1 (1): 4-21. ,\n2018.\n[11] M. Won, S. Oramas, O. Nieto, F. Gouyon, and X. Serra,\n“Multimodal metric learning for tag-based music re-\ntrieval,” in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , 2021,\npp. 591–595.\n[12] A. Ferraro, “Music cold-start and long-tail recommen-\ndation: Bias in deep representations,” in Proceedings\nof the 13th ACM Conference on RecommenderSystems , 2019, p. 586–590. [Online]. Available:\nhttps://doi.org/10.1145/3298689.3347052\n[13] S. Oramas, O. Nieto, F. Barbieri, and X. Serra, “Multi-\nlabel music genre classiﬁcation from audio, text, and\nimages using deep features,” in ISMIR , 2017.\n[14] A. Van den Oord, S. Dieleman, and B. Schrauwen,\n“Deep content-based music recommendation,” Ad-\nvances in neural information processing systems ,\nvol. 26, 2013.\n[15] Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y . Li, and\nD. P. Ellis, “Mulan: A joint embedding of music audio\nand natural language,” in ISMIR , 2022.\n[16] I. Manco, E. Benetos, E. Quinton, and G. Fazekas,\n“Contrastive audio-language learning for music,” in IS-\nMIR, 2022.\n[17] A. v. d. Oord, Y . Li, and O. Vinyals, “Representa-\ntion learning with contrastive predictive coding,” arXiv\npreprint arXiv:1807.03748 , 2018.\n[18] A. Ferraro, X. Favory, K. Drossos, Y . Kim, and D. Bog-\ndanov, “Enriched music representations with multiple\ncross-modal contrastive learning,” IEEE Signal Pro-\ncessing Letters , vol. 28, pp. 733–737, 2021.\n[19] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and\nP. Lamere, “The million song dataset,” in ISMIR , 2011.\n[20] Y . Hu, Y . Koren, and C. V olinsky, “Collaborative ﬁlter-\ning for implicit feedback datasets,” in Proceedings of\nthe 8th IEEE International Conference on Data Min-\ning (ICDM 2008) , 2008, pp. 263–272.\n[21] T. Head, MechCoder, G. Louppe, I. Shcherbatyi,\nfcharras, Z. Vinícius, cmmalone, C. Schröder, nel215,\nN. Campos, T. Young, S. Cereda, T. Fan, rene rex,\nK. K. Shi, J. Schwabedal, carlosdanielcsantos, Hvass-\nLabs, M. Pak, SoManyUsernamesTaken, F. Callaway,\nL. Estève, L. Besson, M. Cherti, K. Pfannschmidt,\nF. Linzberger, C. Cauet, A. Gut, A. Mueller, and\nA. Fabisch, “scikit-optimize/scikit-optimize: v0.5.2,”\nMar. 2018. [Online]. Available: https://doi.org/10.\n5281/zenodo.1207017\n[22] F. Korzeniowski, S. Oramas, and F. Gouyon, “Artist\nsimilarity with graph neural networks,” in ISMIR ,\n2021.\n[23] E. Bingham and H. Mannila, “Random projection in\ndimensionality reduction: applications to image and\ntext data,” in Proceedings of the seventh ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining , 2001, pp. 245–250.\n[24] W. B. Johnson and J. Lindenstrauss, “Exten-\nsions of Lipschitz mappings into a Hilbert\nspace,” Contemporary Mathematics , vol. 26,\npp. 189–206, 1984. [Online]. Available: https:\n//doi.org/10.1090/conm/026/737400Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n381[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay, “Scikit-learn: Machine learning in Python,” Jour-\nnal of Machine Learning Research , vol. 12, pp. 2825–\n2830, 2011.\n[26] D. Valcarce, A. Bellogín, J. Parapar, and P. Castells,\n“Assessing ranking metrics in top-n recommendation,”\nInformation Retrieval Journal , vol. 23, pp. 411–448,\n2020.\n[27] B. Efron and R. Tibshirani, An Introduction to\nthe Bootstrap . Springer, 1993. [Online]. Available:\nhttps://doi.org/10.1007/978-1-4899-4541-9\n[28] C. Chen, J. Zhang, Y . Xu, L. Chen, J. Duan, Y . Chen,\nS. Tran, B. Zeng, and T. Chilimbi, “Why do we need\nlarge batchsizes in contrastive learning? a gradient-\nbias perspective,” in Advances in Neural Information\nProcessing Systems , vol. 35, 2022, pp. 33 860–33 875.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n382"
    },
    {
        "title": "Repetition-Structure Inference With Formal Prototypes.",
        "author": [
            "Christoph Finkensiep",
            "Matthieu Haeberle",
            "Friedrich Eisenbrand",
            "Markus Neuwirth",
            "Martin Rohrmeier"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265305",
        "url": "https://doi.org/10.5281/zenodo.10265305",
        "ee": "https://zenodo.org/records/10265305/files/000045.pdf",
        "abstract": "The concept of form in music encompasses a wide range of musical aspects, such as phrases and (hierarchical) segmentation, formal functions, cadences and voice-leading schemata, form templates, and repetition structure. In an effort towards a unified model of form, this paper proposes an integration of repetition structure\n  (i.e., which segments of a piece occur several times) and formal templates (such as AABA). While repetition structure can be modeled using context-free grammars,\n  most prior approaches allow for arbitrary grammar rules. Constraining the structure of the inferred rules to conform to a small set of templates (meta-rules) not only reduces the space of possible rules that need to be considered but also ensures that the resulting repetition grammar remains interpretable in the context of musical form.\n  The resulting formalism can be extended to cases of varied repetition and thus constitutes a building block for a larger model of form.",
        "zenodo_id": 10265305,
        "dblp_key": "conf/ismir/FinkensiepHENR23",
        "keywords": [
            "form",
            "music",
            "phrases",
            "segmentation",
            "formal functions",
            "cadences",
            "voice-leading",
            "form templates",
            "repetition structure",
            "repetition grammar"
        ],
        "content": "REPETITION-STRUCTURE INFERENCE WITH FORMAL PROTOTYPES\nChristoph Finkensiep1,2Matthieu Haeberle1Friedrich Eisenbrand1\nMarkus Neuwirth3Martin Rohrmeier1\n1École Polytechnique Fédérale de Lausanne, Switzerland\n2University of Amsterdam, The Netherlands (corresponding author: c.finkensiep@uva.nl )\n3Anton Bruckner Privatuniversität Linz, Austria\nABSTRACT\nThe concept of form in music encompasses a wide range\nof musical aspects, such as phrases and (hierarchical) seg-\nmentation, formal functions, cadences and voice-leading\nschemata, form templates, and repetition structure. In an\neffort towards a uniﬁed model of form, this paper proposes\nan integration of repetition structure (i.e., which segments\nof a piece occur several times) and formal templates (such\nas AABA). While repetition structure can be modeled us-\ning context-free grammars, most prior approaches allow\nfor arbitrary grammar rules. Constraining the structure of\nthe inferred rules to conform to a small set of templates\n(meta-rules) not only reduces the space of possible rules\nthat need to be considered but also ensures that the result-\ning repetition grammar remains interpretable in the context\nof musical form. The resulting formalism can be extended\nto cases of varied repetition and thus constitutes a building\nblock for a larger model of form.\n1. INTRODUCTION\nRepetition is one of the most central aspects of music [1]\nand constitutes a constant across almost all cultures, styles\nand genres. The repetition of material is one of the major\ncompositional devices for the arrangement of parts in over-\narching musical form [2, 3, 4], be it a folksong, a minuet,\na sonata, a jazz standard, or a pop song. In general, mu-\nsical form could be characterized in terms of exhaustive\nsegmentation, hierarchical grouping structure, rhythmic-\nhypermetrical structuring, the form functionality of seg-\nments [3], and repetition structure. For the purpose of this\npaper, three aspects of form are considered: a hierarchi-\ncal organization [5], which is also reﬂected in hierarchical\nharmonic structure [6]; repetition of formal constituents,\nwhich is one of the most prominent and salient features of\nform perception in human music cognition [1]; and pro-\ntotypes of formal organization (such as AABA) which can\ncharacterize classical forms [3] but are also common struc-\ntures in pop, jazz, and folk songs.\n© C. Finkensiep, M. Haeberle, F. Eisenbrand, M. Neuwirth,\nM. Rohrmeier. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: C. Finkensiep, M. Hae-\nberle, F. Eisenbrand, M. Neuwirth, M. Rohrmeier, “Repetition-Structure\nInference with Formal Prototypes”, in Proc. of the 24th Int. Society for\nMusic Information Retrieval Conf., Milan, Italy, 2023./noteheads.s0/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s1/noteheads.s1 /noteheads.s2 /noteheads.s1 /clefs.G /noteheads.s1/timesig.C44A\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s25\n/clefs.GB\n/noteheads.s2 /noteheads.s2/noteheads.s1/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s0 /noteheads.s1/noteheads.s19A\n/clefs.G /noteheads.s2/noteheads.s1/noteheads.s1\nFigure 1 : A German 19th-century folksong melody on the\nlyrics “Stille, stille, kein Geräusch gemacht” or “Bier her,\nBier her, oder ich fall um.”\nThe GTTM [5] deﬁnes grouping structure in terms of\na tree of hierarchical containment relations that provides\nan exhaustive segmentation of the piece. GTTM’s prefer-\nence rules for grouping structure include Gestalt principles\n[7] as well as repetition. In addition to grouping structure,\nrepetition structure is deﬁned as a hierarchical grouping\ntree that captures (optimal) reuse of material (exact or in\nvariation) in terms of groups of musical units and recur-\nsive groups of groups. Repetition structure provides a full\ngrouping of a piece, however, it may potentially result in\na different tree than what is obtained by a general formal\nanalysis of a piece (see Figure 6). For human judgement of\nform in general, repetition is not the only factor, as features\nof (hyper-)metrical structure, form functions, or harmony\nmay play a role as well (see also below in section 4.2). Ac-\ncordingly, the objective of a computational model of repe-\ntition structure as an aspect of musical form may ultimately\nrequire to take such aspects into account as well.\nRepetition structure plays a role within a single piece as\nwell as over a corpus of pieces since abstract repetition pat-\nterns generalize over a whole dataset or style. The melody\nshown in Figure 1, for example, exhibits repetition of parts\non several levels: On the highest level, the melody follows\nan ABA form, as the ﬁrst four measures are literally re-\npeated at the end (mm. 9-12). The B part (mm. 5-8) itself\nconsists of a repetition of a two-measure phrase (yellow).\nSimilarly, the ﬁrst measure of the A part is repeated in the\nsecond measure (blue). Even on the level of individual\nnotes, the direct repetition of a note is a prominent feature\nof mm. 5 and 7.\nIn the context of form, repetition structure refers to the\nre-occurrence of formal constituents (such as phrases and\nsections) that form a hierarchical segmentation structure,383Piece\nA\n. . .B\nG\nK\ng2 L\nf4 e4H\nJ\nf4 f4I\nd4 d4G\nK\ng2 L\nf4e4H\nJ\nf4 f4I\nd4d4A\nD\nF\ne1 g4E\nf4e4f4C\ne2 g2C\ne2 g2\nFigure 2 : A possible repetition tree of the melody in Figure 1. The leaves of the tree encode pitch and duration of the\nmelody notes. The second occurence of part A is identical to the ﬁrst (not shown here).\nas opposed to motivic or thematic material, for example.\nAn example of such a segmentation structure for the ex-\nample piece (Figure 1) is shown in Figure 2. Every formal\nsegment of the piece corresponds to a subtree in the repeti-\ntion tree. Note that all occurrences of the same segment\nshare the same label and have exactly the same subtree\nstructure. For this reason, the repetition structure shown\nin the tree can be more compactly described as a restricted\ncontext-free grammar (CFG) which has one non-terminal\nsymbol for each segment (with terminal symbols for the\nnotes of the piece) and exactly one rule for each symbol,\nencoding the decomposition of the corresponding segment.\nThis relationship between the repetition structure of a piece\nand a compact representation of the piece as a CFG has\nbeen utilized for compression-based pattern discovery al-\ngorithms such as S EQUITUR [8].\nAnother aspect of the repetition tree shown in Figure 2\nis that its rules use a limited set of formal prototypes, such\nasαβα (e.g., Piece−→ABA),ααβ (A−→CCD), or αα\n(B−→GG). In order to avoid confusion with the letters for\nspeciﬁc form parts, we denote these form templates with\ngreek letters, e.g., αβα ,ααβ , orαα. A concrete instance\nof a form template is denoted by applying the template to\nspeciﬁc segments: CCD =ααβ(C,D). While the rules\nin a piece’s repetition grammar are speciﬁc to a particular\nsegment in that particular piece, the form templates estab-\nlish a relation between different rules with the same shape,\nwithin the same piece or across different pieces. We there-\nfore call them meta rules .\nThis paper is a contribution towards an integrated com-\nputational model of musical form, combining two impor-\ntant aspects of form: repetition and formal prototypes. The\nmodel characterizes the relationship between meta rules\nand hierarchical repetition structure and provides a proof\nof concept algorithm and evaluation for repetition structure\ninference based on minimal description length [9].\n2. RELATED WORK\nIdentiﬁcation of repetition structure is closely related to\ncompression, as identiﬁcation of redundant information is\nimportant to achieve shorter encodings. An early exam-\nple of grammar-based compression is S EQUITUR , an al-\ngorithm that infers a (not globally optimal) grammar fora given sequence in linear time [8, 10]. For an overview\nof approximate grammar-based compression, see [11, 12].\nBesides inference of segmentation structure, grammar-\nbased compression algorithms have been used for tasks\nsuch as error detection and tune classiﬁcation [12, 13].\nThe principle of minimum description length has also been\nused outside of grammar-based approaches, e.g., in com-\nbination with hidden Markov models [14]. The approach\npresented in this paper differs from previous smallest-\ngrammar approaches in two ways: the shape of the gram-\nmar rules is not arbitrary but constrained to a set of for-\nmal prototypes, and this constrained model is evaluated by\ninferring the global optimum instead of an approximation,\nwhich is generally NP-hard and thus only feasible for short\nsequences.\nThe segmentation structure of a piece can also be in-\nferred based on criteria other than repetition. The GTTM\n[5] deﬁnes grouping structure based on a set of well-\nformedness and preference rules for recursively combining\nevents into larger segments. In the MIR community, the\nanalysis of musical form is known as music structure anal-\nysis(MSA) [15, 16, 17, 18, 19, 20, 21, 22]. MSA comes\nin a variety of tasks, involving boundary detection, (hi-\nerarchical) segmentation, the identiﬁcation of segment la-\nbels and relations, and combinations of these tasks. While\nMSA uses a wide spectrum of supervised and unsupervised\nmethods, from matrix factorization to deep learning, the\ndeﬁnition of musical form in this context is usually given\nimplicitly in the form of a dataset (e.g., [21, 22]) on which\nthe model may be trained, and on which it is evaluated.\nThe present paper, in contrast, presents a theoretical contri-\nbution towards an explicit deﬁnition of musical form, and\nthe resulting model is not intended as a solution to a com-\nputational problem, such as performing a general segmen-\ntation and labeling task. As a consequence, our evalua-\ntion focuses on exploring the characteristic properties of\nthe model.\n3. METHODS AND DATA\n3.1 Problem Description\nA speciﬁc repetition structure for a given piece can be\ncharacterized through a piece-speciﬁc context-free gram-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n384m1:αα m 2:αβ\nm3:ααα m 4:αβα\nm5:ααβ m 6:αββ\nm7:ααβα m 8:αββα\nTable 1 : The set of meta rules used in this paper.\nmar that generates exactly one string — the piece. We call\nsuch a grammar a local grammar for the piece. It consists\nof:\n• a set of terminal symbols T, corresponding to the\nunique atomic segments of the piece;1\n• a set of non-terminal symbols N, corresponding to\nthe unique composite segments of the piece;\n• a starting symbol Pthat stands for the full piece;\n• a set of production rules R.\nSince each non-terminal symbol stands for a speciﬁc seg-\nment,Rcontains exactly one rule for each non-terminal\nsymbol, signifying the decomposition of the segment and\nenforcing that all occurrences of the segment are decom-\nposed identically. As a consequence, the rules are not al-\nlowed to be (mutually) recursive since a segment cannot\ncontain itself as a proper subsegment.2\nIn order to establish a relation between local repetition\ngrammars and general formal prototypes, the right-hand\nside (RHS) of each rule must be an instance of a meta rule .\nMeta rules are generally of the shape {α,β,γ,...}+and\nare instantiated by creating a bijective mapping between\nletters and speciﬁc non-terminal symbols. For example,\nthe meta rule ααβα encodes the formal prototype AABA\nand can be instantiated as\nααβα(S,T) =ααβα{α/mapsto→S,β/mapsto→T}=SSTS (1)\nwhereS̸=T. Thus, a local grammar can express that a\npiece has an overarching AABA structure by using a rule\nP−→ααβα(S,T) (2)\nthat takes the starting symbol Pto an instance of ααβα\nwithα=Sandβ=T. The set of meta rules can be cho-\nsen freely to encode a set of typical formal prototypes. The\nmeta rules used in the following experiments are shown in\nTable 1.\nThe goal of repetition structure inference is to ﬁnd a\nlocal grammar for a given piece according to some op-\ntimality criterion, such as musical plausibility, probabil-\nity, or description length (DL). In accordance with prior\napproaches that use repetition grammars, our proof-of-\nconcept implementation searches for local grammars with\n1In the case of melodies, these atomic segments correspond to notes\nand rests, but they could also correspond to polyphonic events (slices), or\npreviously annotated elementary phrases.\n2A unary identity rule (e.g. X−→X) is not permitted. Other unary\nrules are not possible because of the one-to-one correspondence between\nsegments and grammar symbols.minimum description length, deﬁned in analogy to [13] by\ncounting the symbols needed to encode the grammar:\nDL(R) =/summationdisplay\nr∈R2+|params(r)| (3)\nwhere params (r)denotes the parameters of the meta rule\non the RHS of rule r. That is, for each rule we count one\nsymbol for the meta rule, one symbol for each parameter\nof the meta rule, and one separator symbol3marking the\nend of the rule. For example, the rule in Equation 2 has\na description length of 4: one meta-rule symbol ( ααβα\norm7)4, two parameters ( SandT) and the separator. It\nis not necessary to encode the left-hand side (LHS) of a\nrule since there exists a canonical order of rules, starting\nwith the rule for Pand then listing the rules in the order\nin which their LHS symbols are introduced on the RHS of\nother rules.\n3.2 Algorithm\nThe minimal grammar for a given piece is found in a two-\nstage process. First, a set of possible rules for each unique\nsegment of the piece is computed. Second, a set of rules\nis selected from these candidates, ensuring that the result-\ning grammar is consistent and minimizing the cost of the\nselected rules.\nAlgorithm 1 Enumerating all rule candidates.\n1:function PARSE (input )\n2:subs←uniqueSubsequences (input)\n3:chart←{}\n4: forseq∈sortByLength (subs)do\n5: forsfrom1to|seq|−1do\n6: cs←COMPLETE (seq[:s],seq[s+1 :])\n7: chart[seq]←cs\n8: returnchart\n9:function COMPLETE (left,right )\n10:il←chart[left]incomplete\n11:ir←chart[right]incomplete\n12:bs←binaryRules (left,right)\n13:is←incompleteConstituents (left,right,il,ir)\n14:ns←nAryRules (left,right,il,ir)\n15: return(complete =bs∪ns,incomplete =is)\nThe ﬁrst stage (Algorithm 1) begins with collecting all\nunique subsegments of the piece. For each of these subseg-\nments, all possible decompositions according to the meta\nrules are computed using dynamic programming, analo-\ngous to the CYK algorithm: The segment is split at ev-\nery possible split point (l. 5), generating two subsegments\nleft and right of the split point. For binary meta rules\n(ααandαβ), an instance of the rule can be identiﬁed di-\nrectly by comparing the subsegments (l. 12). Meta rules\n3The separator is not strictly necessary since the length of the rule\nis known from the meta rule, but it is included here to stay as close as\npossible to [13].\n4ααβα is counted as one symbol since the set of meta rules is as-\nsumed to be ﬁxed and cannot be freely extended.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n385of higher arity are decomposed into binary parts. For ex-\nample, the meta rule αβα can be decomposed into an in-\ncomplete constituent αβ∗and another α. When a segment\nS=S1S2has a decomposition into αβ(S1,S2), it ad-\nditionally stores an αβ∗(S1,S2)item (l. 13). At a later\npoint, a larger segment T=SS3=S1S2S3retrieves the\nitemS−→αβ∗(S1,S2), checks whether S3=S1, and\naccordingly stores a rule T−→αβα(S1,S2). Similarly,\nall larger meta rules are constructed from incomplete con-\nstituents such as αβ∗andαα∗(l. 14).\nOnce the possible decompositions of each subsequence\nare known, the second stage of the algorithm converts the\nset of possible rules into an integer linear program (ILP)\nwhich then extracts a set of rules with minimal cost. Each\nsubsequence of the input of lenght ≥2corresponds to\na potential non-terminal symbol, so one binary indicator\nvariablessymb for each symbol symb encodes the inclu-\nsion of the symbol in the grammar. Similarly, the inclu-\nsion of each candidate rule is indicated by a binary variable\nsrule. The optimization problem is then given by\nmin\nr∈{0,1}|rules|\ns∈{0,1}|symbols |/summationdisplay\nrule∈rulesrrule·DL(rule)\ns.t.ssymb=/summationdisplay\nrule∈rules\nLHS(rule)=symbrrule\nrrule≤/summationdisplay\nsymb∈RHS(rule)ssymb\n|RHS(rule)|\nsstart= 1.(4)\nTwo constraints deﬁne the relationship between symbols\nand rules: each included symbol requires exactly one cor-\nresponding rule; and each included rule requires the sym-\nbols on its right-hand side.5A third constraint requires\nthe presence of the starting symbol which corresponds to\nthe full input sequence. The rules and symbols are then\nselected by minimizing the total cost of the included rules\nas deﬁned in Equation 3.\n4. RESULTS AND DISCUSSION\n4.1 Quantitative Evaluation on a Dataset\nFor evaluating the above approach, we infer the minimal\ngrammars (under the meta rules from Table 1) for the 298\nshortest melodies from the Essen folksong collection [23],\nwith a length of 8 to 24 notes. The melodies are repre-\nsented as sequences of notes (including rests), consisting\nof pitch (or a rest symbol) and duration. Other aspects,\nsuch as the position of a note in a measure, are not taken\ninto account. The minimization algorithm is implemented\nin Julia and is available online.6For ILP optimization\nwe use the JuMP framework [24] together with the Gurobi\nsolver backend.7\n5The logical conjunction of the RHS symbols is expressed as a nor-\nmalized sum instead of a product in order to maintain linear relationships\nbetween the variables in the program.\n6https://github.com/DCMLab/form-repetition-ismir23\n7Gurobi requires a license, which is provided freely for academic pur-\nposes. Alternatively, the JuMP framework supports using different solver(a) Grammar size vs. input length.\n(b) Optimal grammar size vs. Monte-Carlo minimum.\nFigure 3 : Comparison of the description length of the min-\nimal local grammars to (a) the input sequence length and\n(b) local grammars obtained through Monte-Carlo mini-\nmization.\nSince the local repetition grammar formalism is not de-\nsigned to obtain optimal compression of the input sequence\n(but uses description length as a rather arbitrary proxy for\nthe plausibility of a speciﬁc segmentation), we cannot ex-\npect very good compression rates. Indeed, when compar-\ning the length of the input sequences to the total description\nlength of the corresponding minimal grammar, the gram-\nmars are usually larger than the original piece (Figure 3a)\nwith a average ratio of 2.47 (geometric mean). This indi-\ncates that restricting the grammars to a small set of meta\nrules is not sufﬁcient to achieve an actual compression of\nthe dataset, at least when only considering exact repetition.\nSince ﬁnding the global minimum is expensive (see be-\nlow), most grammar-based compression algorithms only\nattempt to approximate the global optimum [8, 12, 13].\nWe estimate the payoff of inferring the global optimum\nby comparing the optimal description lengths to approx-\nimate solutions obtained by a Monte-Carlo minimization\nprocess: Beginning with the start symbol, the rule for each\nrequired symbol is chosen randomly from the set of possi-\nble rules, and the corresponding RHS symbols are added to\nthe list of required symbols. This process is repeated until\nall required symbols are covered. Out of 10,000 randomly\nsampled grammars for each piece, the smallest grammar\nis selected. The results are shown in Figure 3b. For the\nbackends.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n386(a) Runtime relative to input length.\n(b) Runtime relative to possible rules.\nFigure 4 : The measured runtime of the optimization step\nrelative to (a) the length of the input sequence and (b) the\nnumber of possible rules for the sequence. Note that in\nboth cases, the time axis is scaled logarithmically, so the\nﬁtted exponential curves appear as straight lines.\ngiven dataset, the Monte-Carlo minimum is on average\n1.08 times longer than the true grammar. In many cases,\nthe Monte-Carlo process ﬁnds a true optimum, since the\nsample size of 10,000 is large enough to ﬁnd an optimal\nsolution by chance. However, with growing input size,\nthe range of possible grammars grows exponentially, in the\nworst case.8So, while a Monte-Carlo estimate can be a\nuseful approximation on short sequences, it cannot keep\nup with the size of the search space for longer sequences,\nunless the sample size is increased exponentially as well.\nThe runtime behavior of the optimization problem is\nshown in Figure 4. The problem of ﬁnding an unrestricted\nminimal CFG is known to be NP-hard [13]. The runtime\nfor the restricted case relative to the input length is shown\nin Figure 4a with logarithmic scaling. Since the actual size\nof the optimization problem depends not only on the in-\nput length but also on the amount of redundancy within\nthe sequence, Figure 4b shows the runtime relative to the\nnumber of possible rules obtained in the ﬁrst stage of the\nalgorithm. In both cases, the runtime grows approximately\nexponentially with the number of rules. This is supported\nby an exponential regression in both ﬁgures, ﬁt as a linear\nfunction in logarithmic space which minimizes the squared\n8The number of subsequences grows quadratically, and the number of\npossible grammars is a product over all substrings.(a) Overall meta rule usage.\n(b) Meta rules used at the top of the form tree.\nFigure 5 : The meta rules used in the inferred minimal\ngrammars for the melodies in the dataset.\nratio between measured and predicted runtime instead of\nthe squared difference.\nThe distribution of meta rules in the inferred grammars\nis shown in Figure 5a. By far the most common rule type\nisαβ, which is not surprising since it is the only rule type\nthat does not require any form of repetition. The rule type\nααis used very infrequently, which may seem surprising\ndue to its simplicity. However, all other rules (except for\nαββα andααα which are similarly rare) have one part\nthat does not need to be repeated and are thus applicable\nto a wider range of situations. The distribution of starting-\nrule types is shown in Figure 5b. These rule types cor-\nrespond to the overarching form of the melody in terms of\nexact repetition. The even stronger prevalence of αβin this\ncase indicates that there is very little exact repetition on the\nhighest form level in the given dataset of melodies, which\nmight be biased due to the focus on short melodies. On the\nother hand, this lack of repetition indicates that a model of\nformal segmentation cannot be exclusively based on repe-\ntition but needs to take into account at least the possibility\nof varied repetition, as well as other markers of form such\nas cadences and meter.\n4.2 Qualitative Evaluation on an Example Melody\nTable 2 displays the minimal grammar for the example\npiece in Figure 1. Compared to the overall distribution of\nmeta rules, the grammar uses many repeating rules, which\nreveals that the piece features an unusual amount of inter-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n387S198\nS277\n. . .S398\ng2 S347\nS136\nf4 e4 f4 f4d4 d4S398\ng2 S347\nS136\nf4e4 f4 f4d4 d4S277\nS164\ne1 g4S98\nS18\nf4 e4 f4S173\ne2 g2S173\ne2 g2\nFigure 6 : The minimal tree for the example piece in Figure 1.\nrule meta rule cost\nr1S198−→S277S398αββα(S277,S398)4\nr2S277−→S98S164αβ(S98,S164) 4\nr3S398−→S347g2αβ(S347,g2) 4\nr4S98−→S173S18ααβ(S173,S18) 4\nr5S164−→g4e1αβ(g4,e1) 4\nr6S347−→d4S136ααβ(d4,S136) 4\nr7S173−→g2e2αβ(g2,e2) 4\nr8S18−→f4e4 αβα(f4,e4) 4\nr9S136−→f4e4ααβα(f4,e4) 4\nTable 2 : The minimal grammar for the example piece in\nFigure 1.\nnal repetition. As the derivation tree in Figure 6 shows, the\noptimal solution found by the algorithm captures many as-\npects of the human intuition. Similar to the hand-annotated\nsegmentation in Figure 2, the minimal tree captures the\noverarching repetition of mm. 1-4 and mm. 9-12 as well as\nmm. 5-6 and mm. 7-8. However, whereas the human intu-\nition groups the single note repetitions together and splits\nnon-repeating segments according to bar units (mm. 5, 7),\nthe algorithm ﬁnds that other groupings provide an even\nmore economic description length in terms of rule usage,\nwhich leads to a somewhat counter-intuitive dangling half\nnote g at the end of the phrase. This illustrates that human\ndecisions in terms of repetition structure do not purely opti-\nmize repetition, but that they take rhythmic-metric bound-\naries into account. Therefore, the objective of a model of\nrepetition structure that captures or comes close to the hu-\nman intuition needs to be further developed to also incor-\nporate such features. A candidate model may be the hier-\narchical model of rhythmic structure as a formal grammar\n[25].\n5. CONCLUSION\nIn this paper we have presented a computational model of\nmusical repetition structure as an aspect of musical form.\nSince repetition structure is an aspect of human music cog-\nnition, the overarching objective of our approach is to ap-\nproach human listening. The model captures repetition\nstructure with a special form of context-free grammar, in\nwhich the rewrite of each category is only deﬁned oncesuch that it captures a unique repeating fragment of a given\npiece. A set of meta-rules deﬁnes the generic types of rep-\netition patterns that could occur within a piece. The model\nis very generic and can also be applied to more complex\ntextures as well as music of all styles and cultures, as long\nas a representation as a sequence of symbols is meaningful.\nInferring the optimal grammar with respect to a suitable\nobjective criterion (such as description length) is able to ef-\nfectively capture the repetition structure in a piece. Other\nobjective criteria (e.g., prior probabilities of meta rules)\ncan be used in a similar way since the algorithm does not\ndepend on a ﬁxed cost function. On the other hand, maxi-\nmizing the redundancy that is captured by a segmentation\ndoes not ensure that the segmentation is a good analysis of\nthe form of a piece. For one, not all repetition and reuse of\nmaterial is exact, which is evident from the low proportion\nof repeating meta rules used in the example dataset. Sim-\nple forms of varied repetition could be integrated in our\nmodel relatively easily: given a suitable measure of sim-\nilarity, not only identical segments are grouped together\nbut also sufﬁciently similar segments. More sophisticated\nversions of this model could capture how variations are\nproduced through the generative process of the grammar\n(e.g., by making different decisions in different subtrees),\nor how only certain aspects of a segment are repeated while\nothers change (e.g., using the same rhythm with a differ-\nent melodic contour). Furthermore, even when all repeti-\ntions are exact (as in the example piece), capturing repeti-\ntion is not the only criterion for grouping tokens into for-\nmal segments, as other criteria such as cadences, rhythm\nand meter, formal function, or harmonic and contrapuntal\nschemata interact with grouping as well.\nThe runtime complexity of ﬁnding the smallest gram-\nmar for a given piece is generally exponential. For sufﬁ-\nciently short pieces, exact inference can be approximated\nprobabilistically, but there is no guarantee that the result-\ning suboptimal grammars resemble the true optimum. For\nlarger inputs, the search space grows exponentially, so\nnaive Monte-Carlo approximation can become arbitrarily\nbad. This indicates that further research is required to\nﬁnd plausible estimates of formal structure, integrating the\ntechnical aspect of optimization with the musical problem\nof deﬁning what constitutes a plausible analysis.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3886. ACKNOWLEDGEMENTS\nThis project has received funding from the European Re-\nsearch Council (ERC) under the European Union’s Hori-\nzon 2020 research and innovation programme under grant\nagreement No 760081 – PMSB. This research was sup-\nported by the Swiss National Science Foundation within\nthe project “Distant Listening – The Development of\nHarmony over Three Centuries (1700–2000)” (Grant no.\n182811). The authors thank Mr. Claude Latour for gener-\nously supporting this research.\n7. REFERENCES\n[1] E. H. Margulis. On Repeat: How Music Plays the\nMind . Oxford, New York: Oxford University Press,\nFeb. 6, 2014. 224 pp.\n[2] F. Diergarten and M. Neuwirth. Formenlehre. Ein\nLese- Und Arbeitsbuch Zur Instrumentalmusik Des\n18. Und 19. Jahrhunderts . 2nd ed. Laaber-Verlag,\n2020.\n[3] W. E. Caplin. Classical Form: A Theory of For-\nmal Functions for the Instrumental Music of Haydn,\nMozart, and Beethoven . Oxford University Press,\nMay 14, 1998. 320 pp.\n[4] Y . Greenberg. How Sonata Forms: A Bottom-Up Ap-\nproach to Musical Form . Oxford Studies in Mu-\nsic Theory. Oxford, New York: Oxford University\nPress, June 10, 2022. 264 pp.\n[5] F. Lerdahl and R. Jackendoff. A Generative Theory\nof Tonal Music . MIT press, 1983.\n[6] M. Rohrmeier. “The Syntax of Jazz Harmony: Dia-\ntonic Tonality, Phrase Structure, and Form”. In: Mu-\nsic Theory and Analysis (MTA) 7.1 (Apr. 30, 2020),\npp. 1–63. DOI:10.11116/MTA.7.1.1 .\n[7] M. Wertheimer. “Laws of Organization in Percep-\ntual Forms”. In: A source book of Gestalt Psychol-\nogy1 (1923).\n[8] C. G. Nevill-Manning and I. H. Witten. “Identify-\ning Hierarchical Structure in Sequences: A Linear-\nTime Algorithm”. In: Journal of Artiﬁcial Intelli-\ngence Research 7 (Sept. 1, 1997), pp. 67–82. DOI:\n10.1613/jair.374 .\n[9] D. J. C. MacKay. Information Theory, Inference and\nLearning Algorithms . Cambridge University Press,\nSept. 25, 2003. 694 pp.\n[10] C. G. Nevill-Manning and I. H. Witten. “Com-\npression and Explanation Using Hierarchical Gram-\nmars”. In: The Computer Journal 40 (2_and_3 Jan.\n1997), pp. 103–116. DOI:10.1093/comjnl/\n40.2_and_3.103 .\n[11] E. Lehman and A. Shelat. “Approximation Algo-\nrithms for Grammar-Based Compression”. In: Pro-\nceedings of the Thirteenth Annual ACM-SIAM Sym-\nposium on Discrete Algorithms . Society for Indus-\ntrial and Applied Mathematics., 2002, pp. 205–212.[12] K. A. Sidorov, A. Jones, and A. D. Marshall. “Mu-\nsic Analysis as a Smallest Grammar Problem.” In:\nISMIR . 2014, pp. 301–306.\n[13] D. Humphreys, K. Sidorov, A. Jones, and D. Mar-\nshall. “An Investigation of Music Analysis by the\nApplication of Grammar-Based Compressors”. In:\nJournal of New Music Research 50.4 (Aug. 8, 2021),\npp. 312–341. DOI:10.1080/09298215.2021.\n1978505 .\n[14] P. Mavromatis. “Minimum Description Length\nModelling of Musical Structure”. In: Jour-\nnal of Mathematics and Music 3.3 (Nov. 1,\n2009), pp. 117–136. DOI:10 . 1080 /\n17459730903313122 .\n[15] M. Buisson, B. Mcfee, S. Essid, and H.-C. Crayen-\ncour. “Learning Multi-Level Representations for Hi-\nerarchical Music Structure Analysis”. In: Interna-\ntional Society for Music Information Retrieval (IS-\nMIR). Dec. 4, 2022.\n[16] B. McFee. “Audio-Based Music Structure Analy-\nsis: Current Trends, Open Challenges, and Applica-\ntions”. In: 3.1 (1 Dec. 11, 2020), pp. 246–263. DOI:\n10.5334/tismir.54 .\n[17] B. McFee and D. Ellis. “Analyzing Song Struc-\nture with Spectral Clustering.” In: ISMIR . Citeseer,\n2014, pp. 405–410.\n[18] F. Kaiser and T. Sikora. “Music Structure Discovery\nin Popular Music Using Non-negative Matrix Fac-\ntorization.” In: ISMIR . 2010, pp. 429–434.\n[19] M. Levy and M. Sandler. “Structural Segmenta-\ntion of Musical Audio by Constrained Clustering”.\nIn:IEEE Transactions on Audio, Speech, and Lan-\nguage Processing 16.2 (Feb. 2008), pp. 318–326.\nDOI:10.1109/TASL.2007.910781 .\n[20] M. C. McCallum. “Unsupervised Learning of Deep\nFeatures for Music Segmentation”. In: ICASSP\n2019 - 2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nICASSP 2019 - 2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP). May 2019, pp. 346–350. DOI:10 .\n1109/ICASSP.2019.8683407 .\n[21] J. B. L. Smith, J. A. Burgoyne, I. Fujinaga, D. De\nRoure, and J. S. Downie. “Design and Creation of\na Large-Scale Database of Structural Annotations.”\nIn:ISMIR . V ol. 11. Miami, FL, 2011, pp. 555–560.\n[22] C.-i. Wang, G. J. Mysore, and S. Dubnov. “Re-\nVisiting the Music Segmentation Problem with\nCrowdsourcing.” In: ISMIR . 2017, pp. 738–744.\n[23] H. Schaffrath. “The Essen folksong collection in the\nHumdrum Kern Format (D. Huron, Ed.)” In: Menlo\nPark, CA: Center for Computer Assisted Research\nin the Humanities (1995).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n389[24] M. Lubin, O. Dowson, J. D. Garcia, J. Huchette, B.\nLegat, and J. P. Vielma. “JuMP 1.0: Recent improve-\nments to a modeling language for mathematical op-\ntimization”. In: Mathematical Programming Com-\nputation (2023). In press.\n[25] M. Rohrmeier. “Towards a Formalization of Mu-\nsical Rhythm”. In: Proceedings of the 21th Inter-\nnational Society for Music Information Retrieval\nConference, ISMIR 2020, Montreal, Canada, Oc-\ntober 11-16, 2020 . Ed. by J. Cumming, J. H. Lee,\nB. McFee, M. Schedl, J. Devaney, C. McKay, E.\nZangerle, and T. de Reuse. 2020, pp. 621–629.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n390"
    },
    {
        "title": "Predicting Music Hierarchies With a Graph-Based Neural Decoder.",
        "author": [
            "Francesco Foscarin",
            "Daniel Harasim",
            "Gerhard Widmer"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265315",
        "url": "https://doi.org/10.5281/zenodo.10265315",
        "ee": "https://zenodo.org/records/10265315/files/000050.pdf",
        "abstract": "This paper describes a data-driven framework to parse musical sequences into dependency trees, which are hierarchical structures used in music cognition research and music analysis. The parsing involves two steps. First, the input sequence is passed through a transformer encoder to enrich it with contextual information. Then, a classifier filters the graph of all possible dependency arcs to produce the dependency tree.\nOne major benefit of this system is that it can be easily integrated into modern deep-learning pipelines. Moreover, since it does not rely on any particular symbolic grammar, it can consider multiple musical features simultaneously, make use of sequential context information, and produce partial results for noisy inputs. We test our approach on two datasets of musical trees -- time-span trees of monophonic note sequences and harmonic trees of jazz chord sequences -- and show that our approach outperforms previous methods.",
        "zenodo_id": 10265315,
        "dblp_key": "conf/ismir/FoscarinHW23",
        "keywords": [
            "data-driven framework",
            "musical sequences",
            "dependency trees",
            "transformer encoder",
            "classifier",
            "dependency arcs",
            "music cognition research",
            "music analysis",
            "integration into deep-learning pipelines",
            "multiple musical features"
        ],
        "content": "PREDICTING MUSIC HIERARCHIES WITH A\nGRAPH-BASED NEURAL DECODER\nFrancesco Foscarin1Daniel Harasim2Gerhard Widmer1\n1Institute of Computational Perception, Johannes Kepler University Linz, Austria\n2Independent Researcher\nfrancesco.foscarin@jku.at\nABSTRACT\nThis paper describes a data-driven framework to parse mu-\nsical sequences into dependency trees, which are hierarchi-\ncal structures used in music cognition research and music\nanalysis. The parsing involves two steps. First, the input\nsequence is passed through a transformer encoder to enrich\nit with contextual information. Then, a classiﬁer ﬁlters the\ngraph of all possible dependency arcs to produce the depen-\ndency tree. One major beneﬁt of this system is that it can\nbe easily integrated into modern deep-learning pipelines.\nMoreover, since it does not rely on any particular symbolic\ngrammar, it can consider multiple musical features simulta-\nneously, make use of sequential context information, and\nproduce partial results for noisy inputs. We test our ap-\nproach on two datasets of musical trees – time-span trees\nof monophonic note sequences and harmonic trees of jazz\nchord sequences – and show that our approach outperforms\nprevious methods.1\n1. INTRODUCTION\nTree-like representations are a powerful tool in many ap-\nproaches to music analysis, such as Schenkerian Theory\nand the Generative Theory of Tonal Music (GTTM). In\nthe Music Information Retrieval (MIR) literature, we ﬁnd\ntree models of melodies [1 –4], chord progressions [5 –8],\nand rhythm [9 –13]. Parallels between aspects of music and\nlanguage are often drawn, as these have similar hierarchical\nproperties and their underlying cognitive mechanisms could\nbe closely related [14]. However, with a few exceptions,\nsuch as instrument grouping and metrical information in\nscores, music is generally encoded sequentially without\nexplicit information about its hierarchical organisation. The\ntask of creating such hierarchies from a sequential represen-\ntation is called parsing and it is an active object of study in\nthe MIR community [3, 7, 11, 15].\n1All our code and data are publicly available at https://github.\ncom/fosfrancesco/musicparser\n© F. Foscarin, D. Harasim, and G. Widmer. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: F. Foscarin, D. Harasim, and G. Widmer, “Predicting Music\nHierarchies With a Graph-Based Neural Decoder”, in Proc. of the 24th\nInt. Society for Music Information Retrieval Conf., Milan, Italy, 2023.Current parsing approaches are based on generative\ngrammars , typically context-free-grammars (CFG) or simi-\nlar related mechanisms, which can be fundamentally seen as\na set of expansion rules generating a tree from the top (the\nroot) to the elements that compose the sequence (the leaves).\nGrammar rules can be enriched with a probability model\nthat permits the ranking of different parses by plausibility.\nWhen a grammar is available, parsing can be achieved with\ngrammar-based parsing algorithms, typically variants of\nthe Cocke–Younger–Kasami (CYK) algorithm [16]. While\nthe grammar rules are most often built by hand, by rely-\ning on musicologists’ knowledge, the probabilities can be\nlearnt from data if sufﬁcient amounts of musical sequences\nwith ground-truth tree annotations are available. The gram-\nmar approach has the strong advantage of leveraging an\ninterpretable and cognitively plausible mechanism. Still, it\nhas the following limitations: it is hard to achieve robust-\nness against noisy data, which can cause a complete failure\nwith no output in case the sequence cannot be produced\nby the grammar rules; it requires a high degree of domain\nknowledge; it is challenging to account for multiple musical\ndimensions in a single grammar rule; and parsing is usually\nso slow for long sequences that heavy pruning is necessary\n(CYK-parsing complexity is cubic in the length of the se-\nquence, parallelisation does not help much, and there is no\nactive research in developing dedicated hardware).\nInspired by recent research in the ﬁeld of natural lan-\nguage processing (NLP), we propose a novel, grammar-less\napproach that requires little domain knowledge (only for the\nfeature extraction phase), can easily consider multiple mu-\nsical features and sequential information, produces partial\nresults for noisy input, and is potentially scalable to longer\nsequences and larger datasets (since its components are\nproven to succeed in such scenarios). Our system works by\npredicting dependency trees which consist of dependency\narcs between the input sequence elements. Such a struc-\nture can be used as-is or later be converted into constituent\ntrees which are typically used to model music hierarchies\n(see Figure 1). The probability of each dependency arc is\npredicted in parallel (i.e., without considering other depen-\ndencies during prediction) by leveraging the rich contextual\ninformation produced by a transformer encoding of the in-\nput sequence. This set of probabilities is then run through\na post-processing algorithm to ensure a valid tree structure\n(i.e., no cycles of dependency arcs).\nWe pair our Music Dependency Parser MuDeP with a425Figure 1 . The tree harmonic analysis of the A Section of\n“Take the A Train” in three different representations. Top:\ndependency tree, Left: GTTM-style constituent tree. Right:\nCFG-style constituent tree.\nprocedure that enables its usage from constituent trees, and\ntest it on two tree datasets: the time-span treebank from\nthe GTTM database [17], which expresses subordinate rela-\ntions between notes in monophonic melodies; and the Jazz\nHarmony Treebank (JHT), a set of harmonic analyses for\nchord sequences [18]. We compare the results of our system\nwith the best-performing available approaches and obtain\nnew state-of-the-art results.\n2. RELATED WORK\nMusic Trees and Music Parsing. Trees of musical\nsequences have traditionally been notated as constituent\ntrees [1 –3, 5–13, 19], with few exceptions, such as the us-\nage of a dependency-based evaluation metric [20], and the\ncomputation of pairwise voice dependencies [4, 21].\nA system for parsing jazz chord sequences into harmonic\nanalyses has been proposed by Harasim et al. [7] and later\nevaluated on a larger dataset [20]. We compare our results\nto this approach below. Automatic grammar-based parsing\nof time-span GTTM trees has been attempted by Hamanaka\net al. [22, 23] and Nakamura et al. [2]. The latter obtained\ncomparable results with an approach that doesn’t require\nmanual parameter tuning, and we compare our system with\nit. More recently, deep-learning-based approaches were\nalso proposed [3, 24, 25] but the ﬁrst two focus only on\nGTTM metrical and grouping information, and the latter\nfocus mainly on evaluating the usage of time-span trees for\nmelodic morphing and we could not reproduce their results.\nNatural Language Parsing. Our model architecture is\ninspired by the graph-based dependency parser of Dozat\nand Manning [26, 27]. This model, extended with second-\norder dependency predictions [28] and pretrained language\nmodels [29], is still the state of the art for NLP sentence\nparsing [30]. Still, we make some substantial changes: the\nembedding layer is adapted to work from musical input, the\nencoder is a transformer instead of an LSTM, and, instead\nof the bilinear layer for arc prediction, we use a linear layer.\nAll these choices are motivated by ablation studies.\n3. TREE FORMATS FOR MUSIC ANALYSES\nIn this section, we detail the types of tree used in this pa-\nper, highlight their differences, and propose algorithms to\ntranslate between them.\nFigure 2 . A dependency tree with double-sided dependen-\ncies (left). It corresponds to two possible constituent trees\n(middle and right).\n3.1 Constituent vs Dependency Trees\nA tree can be deﬁned recursively as a node with an arbitrary\nnumber (including 0) of children that are also trees. The\nnode that is not a child of another node in the tree is called\nroot, the nodes that do not have children are called leaves ,\nand the remaining nodes are called internal nodes . When\na tree is used to model some relations of the elements of a\nsequence there are two possible conﬁgurations: dependency\ntrees , where each node (leaf, internal, and root) represents\none and only one element of the sequence; and constituent\ntrees where all elements of the sequence are represented\nin the leaves, and root and internal nodes represent nested\ngroupings of such elements.\nAmong the constituent trees there exist different repre-\nsentations. The bottom part of Figure 1 shows the two kinds\nwe consider in this paper: the one introduced by Lerdahl\nand Jackendoff [31] in their Generative Theory of Tonal\nMusic (GTTM), and the one built from the Context-Free-\nGrammar (CFG) of jazz harmony by Harasim et al. [7]. The\ntwo representations convey almost the same information:\nthey are both binary trees (i.e., every node has either 0 or\n2 children), the internal nodes are denoted by line intersec-\ntions on the ﬁrst, and by explicit labels on the second; they\nboth specify an order of importance among the children\n(i.e., the choice of a primary andsecondary child) by the\nstraight line continuation, or by labelling the node with the\nlabel of the primary child. However, this latter mechanism\ncannot differentiate between primary and secondary when\nboth children have the same label; therefore, the GTTM\nrepresentation is slightly more informative.\nOur approach does not directly treat constituent trees\nbut considers dependency trees. Each child in such a tree\nis called dependent , and the node of which it is a child\nis called the head . Dependency trees can represent the\nsame information as the binary constituent trees described\nabove. Indeed, a dependent-head arc is equivalent to a head-\nlabelled constituent node with two children: the primary is\nagain the head, and the secondary is the dependent. There\nis only one ambiguity: the dependency tree does not encode\na splitting order in the case of double-sided dependencies ,\na conﬁguration in which one head has dependents on both\nsides. This makes the dependency-to-constituent transfor-\nmation not unique (see Figure 2). This conﬁguration is\nnever present in our datasets (i.e., the root is always the\nleft-most or right-most element in the sequence) thus we\ndon’t handle it. For more general datasets, one could add a\nbinary classiﬁer that predicts the splitting order.\nThe dependency trees built from the constituent treesProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n426areprojective , i.e., for all their arcs xdep→xhead, there is a\npath from the head to every element xthat lies between the\nhead and the dependent in the sentence [32]. This means\nthat there are no “crossing arcs”, e.g., x1− →x3,x2− →x4.\nBefore proceeding with the paper, we introduce some\nnotation we will use in the next sections. We denote\nthe sequence that constitutes the input of our system as\nx= [x1,...,x λ], whereλis the sequence’s length. We\nrepresent the dependency tree over xas the set of dependent-\nhead2indices that corresponds to each arc xdep→xhead:\ny={(dep,head)|dep,head∈[1,...,λ]} (1)\n3.2 Tree Conversion Algorithms\nSince the ground-truth annotations in our datasets are con-\nstituent trees, we translate them into dependency trees for\ntraining. We also translate tree predictions back to con-\nstituent trees to run constituent-based evaluation metrics,\nand when we are interested in using such a representation\nas input for further applications. We assume our constituent\ntrees to be binary trees and not contain double-sided depen-\ndencies. For simplicity, we consider CFG-style constituent\ntrees with labels in their internal nodes.\n3.2.1 Dependency to Constituent Tree\nExisting NLP implementations of this transformation are\nunnecessarily complicated for our scenario because they\nconsider compound node labels and double-sided depen-\ndencies [33]. Instead, we present a recursive top-down\nalgorithm which yields a unique constituent solution for\nevery single-sided dependency tree.\nThe algorithm takes a fully formed dependency tree and\nstarts with the root of the (to-be-built) constituent tree. At\neach step, it removes one dependency and adds two new\nconstituent nodes. The recursive function takes as input\na dependency tree node and a constituent tree node, both\nlabelled with the same sequence element. The constituent\nnode gets assigned two children: the primary is labelled\nwith the element of the input nodes, and the secondary\nis labelled with the dependent that is further away in the\nsequence. The choice of which is the left and the right child\nrespects their label position in the sequence. The considered\ndependency is removed from the tree and the recursive\nfunction is called two times, once for each constituent child\n(with the corresponding dependency node). The process\nstops when the dependency tree node has no dependents.\n3.2.2 Constituent to Dependency Tree\nThis algorithm was used in the literature (e.g., [18]). It starts\nfrom a fully formed constituent tree and a dependency tree\nwithout any dependency arcs, consisting only of the nodes\nlabelled with sequence elements. The algorithm groups all\ninternal tree nodes with their primary child (which all have\nthe same label) and uses all secondary child relations origi-\nnating from each group to create dependency arcs between\nthe group label and the secondary child label.\n2We indicate dependency arcs as arrows pointing in the direction of the\nhead. Note that in other (NLP) papers, the opposite convention is used.4. PARSING TECHNIQUE\nOur goal is to predict a dependency tree yfor a given mu-\nsical sequence x. Our pipeline consists of three steps: fea-\nture extraction from x; prediction of dependency relations;\nand postprocessing to ensure that the output is a valid tree\nstructure. In the training phase, the output (before postpro-\ncessing) is compared with the ground truth dependency tree\nand a loss is computed to update the model parameters via\nbackpropagation.\n4.1 Feature Extraction\nFor each input element, xi∈x, we produce three groups\nof features. The ﬁrst is a “static” description of the element\n(i.e., without any temporal information), the second encodes\nthe element’s duration, and the third encodes the element’s\nmetrical position, i.e., its position in the measure relative\nto the hierarchy induced by the time signature. The static\ndescription is built differently for chords and notes, while\nthe other two are independent of the input type. Note that,\ndue to our model architecture (see next section), we need\ncategorical features and it is not primarily important to keep\ntheir number small or to have them ordered.\nFor note sequences, the static description of each ele-\nment is a single integer corresponding to either the MIDI\npitch of the element in [0,...,127] if it is a note or with the\nvalue128if the element is a rest. For chord sequences, we\nuse three integers. The ﬁrst in [0,...,11]encodes the pitch-\nclass of the chord root. The second in [0,...,5]speciﬁes\nthe basic form of the chord among major, minor, augmented,\nhalf-diminished, diminished, and suspended (sus). The last\nin[0,1,2]denotes a chord extension among 6, minor 7, or\nmajor 7. The chord labels were simpliﬁed by the author of\nthe dataset to only include these extensions, but in a more\ngeneral scenario, a larger set of integers could be used. The\nchord sequences do not contain rests.\nWe represent the durations of the elements with discrete\nvalues normalised by the duration of the measure. We pre-\ncollect the list of all durations occurring in the dataset and\nencode each element’s duration as an index on that list. For\nthe GTTM dataset, this would be an integer in [0,...,44],\nwhile for the JHB dataset, it is an integer in [0,...,5]. The\nnumber of possibilities is very different, since the tempo-\nral position of chords follows much simpler rules, mostly\noccurring only at the beginning or in the middle of a bar\nfor simple time signatures and at three bar positions for\ncompound time signatures. For tied notes, we consider a\nsingle note with the total duration, and notes can last more\nthan one measure. This is different from the annotations in\nthe JHT in which each measure opens a new chord symbol,\neven if the same chord is repeated in consecutive measures.\nTo represent the metrical position , we use an inverse\nmeasure of metrical strength, encoded with a single integer\nin[0,...,5]. This integer is computed as a function of the\nnormalised temporal position in the measure t∈[0,1[, and\nthe time signature numerator. Each time-signature numer-\nator is associated with a template of metrical divisions m,\nas proposed by Foscarin [10] and here extended to more\ntime signatures. For example, a time signature with a nu-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n427Figure 3 . Left: metrical divisions mfor different time\nsignature numerators. Right: visualisation of metrical divi-\nsions for a measure with time signature 12/8.\nmerator12(e.g.,12/8or12/4) will have metrical divisions\nm= [1,2,2,3,2], i.e., the whole measure at level 0is di-\nvided into two parts at level 1, each resulting part is divided\nin 2 at level 2, then 3 at level 3, and 2 at level 4. Table 3\nreports metrical divisions for all numerators we consider.\nEach level lin the metrical division deﬁnes a temporal\ngrid with step δl= 1//producttextl\nl=0ml, and the inverse metrical\nstrength is deﬁned as the lowest level for which the note\nposition falls on the temporal grid, minl(l|t/δl∈N).\nFor example, a time signature 6/8deﬁnes grids with steps\n[1,1\n2,1\n6,1\n12,1\n24], and the notes of the measure |ˇ“ˇ“(ˇ“‰|will\nhave normalised temporal position [0,2\n6,1\n2]and inverse\nmetrical strength [0,2,1]. If the note doesn’t align with\nany temporal grid, then its inverse metrical strength is the\nmaximum, 5 in our settings. Using metrical strength as in-\nput to our system may seem overly complicated. However,\ngiven the small size of our datasets and the high variety of\ntime signatures, we need a mechanism to encode metrical\ninformation generalisable across different time signatures.\nIt is to be expected that with a larger dataset size, this fea-\nture could be discarded, as the model could learn similar\ninformation from the list of notes with duration.\nThe feature extraction process lets us build the input\nmatrixX ∈Nλ×φwhereλis the sequence length, and φis\nthe number of features for each element: 3 for the GTTM\ndataset, and 5 for the JHT dataset. Before moving on, it\nhas to be noted that there exist other more general ways\nof transforming symbolic music into convenient inputs for\ndeep learning models, notably the tokenisation techniques,\ne.g., [34, 35] inspired by NLP research. However, given\nthe small dimension of our dataset and the fact that our\nmelodies are strictly monophonic, we prefer to use a more\ncompact, ad-hoc input representation. Our parsing frame-\nwork remains general and usable with other techniques.\n4.2 Model\nOur model consists of two parts: an encoder and an arc\npredictor (see Figure 4). The goal of the encoder is to\nenrich the input features Xwith contextual information.\nThe arc predictor uses the enriched sequence features to\npredict whether each possible pair of elements in the input\nsequence should be connected by a dependency arc.\nThe ﬁrst part of the encoder is an embedding layer, a\nlearnable look-up table which maps our collection of cat-\negorical features (each integer) to points in a continuous\nmultidimensional space. Speciﬁcally, we use φembedding\nFigure 4 . Architecture of our model. The input displayed is\nan example of a chord sequence, but the same architecture\nis used for note sequences.\nlayers (one for each input feature), which work indepen-\ndently, and map all values that the feature can have to a\nvector of a ﬁxed embedding dimension. All vectors are\nthen summed together to obtain a unique representation\nwhile keeping the input size small (see [36] for an explana-\ntion of why summing is better than concatenating). After\nthe embedding layer, we have the encoder part of a trans-\nformer [37] with relative position representations [38]. It\noutputs a matrix with the same number of rows as the in-\nput matrix X(one for each sequence element) but with a\n(possibly) different number of hidden-feature columns h.\nOnto this, we concatenate a new learnable single row that\nacts as the head of the root node. The result is a new matrix\nH ∈Q(λ+1)×h.\nThe arc predictor part of our model is a multilayer per-\nceptron (MLP) that performs the binary classiﬁcation task\nof deciding whether each pair (xhead,xdep)in the Carte-\nsian product of the input elements, i.e., {(head,dep)|\n∀head,dep∈[1,...,λ]}, should be connected by a de-\npendency arc. Depending on the input representation and\nthe speciﬁc task we are targeting, there may be some pairs\nthat are not connectable by a dependency arc, for example,\npairs where head=dep. For the GTTM input, pairs for\nwhich at least one element is a rest are also not connectable.\nTherefore, the binary classiﬁcation is performed only on a\nsubset of all pairs Λthat we call potential arcs . For every po-\ntential arc (xdep,xhead), we predict the probability ˆydep,head\nof a dependency arc by concatenating the two rows of H\nthat correspond to the head’s and the dependent’s index into\na single vector of length 2hand giving it as input to the\nMLP. We concatenate the two inputs instead of summing or\nmultiplying them because our arcs are directed, so we need\nto preserve the order when aggregating the two embeddings.\nMoreover, despite the bilinear layer being a major sellingProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n428point of Dozat’s paper [26], we ﬁnd that the concatenation\napproach yields better results. We can collect the output for\nall potential arcs into a weighted graph-adjacency matrix\nˆY, which is a λ×λmatrix with entries ˆyhead,depat the cor-\nresponding indices. We assign a probability 0 to the matrix\nentries that correspond to arcs /∈Λ.\n4.3 Training Loss\nIn the training phase, we use the sum of the binary-cross-\nentropy (BCE) loss and the (multiclass) cross-entropy (CE)\nloss. The BCE loss is computed independently for each po-\ntential arc and measures the difference between the ground-\ntruth label (0 or 1) and the predicted probability. We also\nuse the CE loss because our problem can be framed as a\nmulticlass classiﬁcation problem where for each element we\npredict his head among λ+1possibilities (each sequence\nelement plus a dummy element for the root and rests ele-\nments). The CE loss is therefore applied column-wise to\nthe adjacency matrix ˆYpredicted by our model.\nIn NLP, the BCE loss was used by [27] while the CE\nloss is used more generally, for example, by [26, 39]. We\nexperimentally found that the sum of the two losses yields\nthe best results.\n4.4 Postprocessing\nSince the prediction of our model is made independently\nfor each potential arc, simply taking the row-wise maxi-\nmum of the weighted adjacency matrix to select which head\nto assign to each element of the sequence could produce\ndependency cycles and, therefore, not yield a tree struc-\nture. We use a maximum-spanning-tree algorithm to ﬁnd\nthe tree over ˆYwith the highest weight. Since our depen-\ndency trees are projective, we use the Eisner algorithm [40]\nwhich solves this problem using bottom–up dynamic pro-\ngramming with a time complexity of O(λ3). For applica-\ntions involving non-projective trees other post-processing\napproaches such as Chu-Liu/Edmonds [41, 42] ( O(λ2)) are\nimplemented in our framework.\n5. EXPERIMENTS\nBelow, we describe the datasets, evaluation metrics, and\nexperimental settings for the two kinds of trees we consider.\n5.1 Datasets and preprocessing\nWe obtain the melodic time-span trees from the GTTM\ndatabase [17], which contains MusicXML encodings of\nmonophonic scores and a dedicated XML-based encoding\nof the constituent time-span trees (among other trees that we\ndon’t consider in this paper). We extract the note features\nwith the Python library Partitura [43]. Some pieces have two\ndifferent trees, and we keep only the ﬁrst. We also discard\n4 pieces that we could not import due to inconsistencies in\nthe XML ﬁle encoding. In total, we have 296 melodies of\nlengths between 10 and 127 (notes + rests). For training,\nwe augment the dataset by considering all transpositions\nbetween one octave higher and one octave lower.We obtain the chord analyses from the Jazz Harmony\nTreebank (JHT) [9], which encodes both chord labels and\nharmonic analyses as constituent trees, in JSON format. As\ndiscussed in Section 3 this format does not distinguish be-\ntween the primary and the secondary child when both have\nthe same chord label. In this case, we assume by default\nthat the right is the primary. The dataset contains two kinds\nof trees: open and complete constituents. We use the former\nfor comparison reasons since the results are reported only\nfor those [20]. In total, we have 150 sequences of lengths\nbetween 11 and 38 chords. For training, we augment the\ndataset by considering all 12 possible transpositions of the\nchord roots.\n5.2 Evaluation metrics\nThe papers we compare use different metrics, and we im-\nplement all of them. The work of Harasim [20] uses two\nmetrics, one more relevant for dependency trees and the\nother for constituent trees. The ﬁrst is the arc accuracy ,\ni.e., the normalised cardinality of the intersection between\nthe set of predicted arcs and the ground truth arcs. The\nsecond is the span accuracy , computed as the normalised\ncardinality of the intersection between all the spans of the\npredicted constituent tree (i.e., the pair of the leftmost and\nrightmost leaf that is part of the subtree rooted at any non-\nleaf node) and the spans of the ground truth tree (see [20]\nfor a more detailed explanation). Nakamura et al. [2] use\nthenode accuracy metric, i.e., the normalised cardinality of\nthe intersection between nodes in the predicted and ground\ntruth trees, where two nodes are considered equal if the\nlabels of the parent and children (or a dummy label if the\nnode have no parent or children) are equal.\nWe also report another metric, the head accuracy , com-\nputed as the multiclass classiﬁcation accuracy on the in-\ndices of the predicted heads, ordered by their dependent.\nFor example for the dependency tree of Figure 1, this would\ncorrespond to the accuracy computed on the sequence\n[4,2,3,4,−1], where−1indicates the root (which has no\nhead). This is similar to the arc accuracy but enforces the\npresence of a dependency head for each sequence element\n(which may not be the case for a generic system), and gives\nmore weight to the correct root prediction. It is also faster\nto compute and commonly used in NLP, so we include it\nto set a metric for future research. Note that all metrics\npresented above don’t consider the nodes corresponding to\nrests, since they are only part of the input sequence, but not\npart of the tree.\n5.3 Results\nFor our experiments, we set the hyperparameters of our en-\ncoder to an embedding size of 96, and 2 transformer layers\nof hidden size 64. The arc predictor (MLP) has 2 linear\nlayers with the same hidden size. We use the GeLU activa-\ntion [44] and the AdamW optimiser [45], with a learning\nrate of 0.0004 and weight decay of 0.05. We train with\nlearning rate warm-up [46] of 50 steps and cosine annealing\nto limit the problem of high variance in the initial and ﬁnal\nstages of training. The latter was particularly importantProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n429Figure 5 . Boxplots of three accuracy metrics (higher is\nbetter) computed with leave-one-out cross-validation and\ntheir average. For Nakamura et al. [2], we report the average\nfrom their paper, so there is no deviation information.\nsince we did not use a validation set to perform early stop-\nping due to the small size of our datasets. We train for 60\nand 20 epochs for the JHT and GTTM datasets, respectively,\nsince the latter is bigger and the data augmentation yields\ntwice as many pieces in total). The training time is roughly\nthe same, around 1 hour on a GPU RTX 1080.\nWe compare the results of our MuDeP on the JHT with\nHarasim [20] and on the GTTM with Nakamura et al. [2].\nWe use leave-one-out cross-validation, i.e., for a dataset\nwithNpieces, we run our system Ntimes, by training on\nN−1pieces and evaluating with the remaining one. As\nshown in Figure 5, MuDeP outperforms previous methods.\nBy comparing the head accuracy between JHT and\nGTTM (79.2% vs 57.9%), it is clear that time-span pre-\ndiction is a much harder problem than the chord analysis\nproblem, despite the dataset being bigger. Another inter-\nesting result is that the span accuracy is lower than head\naccuracy for the JHT dataset (63.1%), but higher for the\nGTTM(64.8%). Apparently, the main problem for JHT is\nto select which two chords to connect, but the arc direction\n(i.e., which is the head and which is the dependent) is al-\nmost always correctly inferred; conversely, for the GTTM\ndataset, the system often connects the correct notes, but\nin the wrong direction. And this type of misprediction is\npunished in the head accuracy, but not in the span accuracy.\nThe full piece-wise statistics on all metrics, a graphical\nrendering of all our predicted trees, and the qualitative\nevaluation of some examples are available in our repository.\n5.4 Ablation study\nWe report the difference in head accuracy averaged over 10\nruns with 90/10 random train/test split for the JHT dataset.\nRegarding the loss, sole usage of the (multiclass) CE loss\nreduced the accuracy by 0.3%, and only using the binary\nCE loss reduced the accuracy by 4.1%. The use of a bilinear\nlayer in the decoder reduced the accuracy by 1.2%. The ab-\nsence of post-processing did not reduce the accuracy (when\nthe network is fully trained, otherwise the reduction is very\nevident). This is promising but it does not automaticallyimplies that the network is producing correctly formed trees\nsince dependency loops could still be present in the output.\nThere are also cases when the postprocessing is reducing\nthe accuracy, by incorrectly deciding which arc to remove\nin a dependency loop.\n6. CONCLUSION AND FUTURE WORK\nWe presented MuDeP , a system for the dependency parsing\nof music sequences, and a procedure to make it applicable to\nconstituent trees. MuDeP improves upon previous methods,\nby incorporating the ability to consider multiple musical\nfeatures simultaneously, taking advantage of sequential con-\ntext, and handling noisy inputs robustly. Moreover, since it\nis based on widely researched deep learning components,\nit has the potential to scale to large datasets and longer\nsequences. The bottleneck for such scalability is the post-\nprocessing algorithm with cubic complexity. Two solutions\nexist to this problem: if one is interested in non-projective\ntrees, algorithms with a square complexity are available.\nApart from that, our system is already having good accu-\nracy without the postprocessing phase, as highlighted in\nthe ablation study. Therefore, a faster heuristic may suf-\nﬁce to correct the few problematic dependencies without\ndecreasing the performance.\nSince our deep learning model is a black box, it is no-\ntably complicated to ﬁnd a human-understandable expla-\nnation of its functioning. Although work in this direction\nexists [47, 48], it is still very limited [49]. Therefore, our\nmodel is mainly intended for scenarios in which one is\nonly interested in obtaining the parsing trees, for example,\nto use them as input for another MIR task. Conversely,\nthis paper might have limited utility if one’s goal is to\nmodel music understanding and interpretation by humans.\nGrammar-based models are much more suitable for this\ngoal, although there is a (somewhat speculative) possibility\nthat the dependency-arc probabilities in our approach relate\nto ﬁrst-guess heuristics.\nAs research on the deep learning components we use\nis rapidly evolving, any new discovery is likely to beneﬁt\nour system. Self-supervised pretraining on larger datasets\nof monophonic music or chord sequences, for example by\npredicting next or masked tokens, could also improve the\nperformance, as already proved for language parsing. While\nthe goal of this paper was to present a general framework,\nwe can also think about several domain-speciﬁc improve-\nments, for example, training the GTTM time-span parser\nwith a multi-target approach to predict at the same time\nthe metrical, time-span, and prolongation structure. We\nhope that this work will motivate the development of more\ndatasets of hierarchical music analyses, including datasets\nof dependency trees, which may be a valid alternative to\nconstituent structures, and even open up more possibili-\nties due to the missing projectivity constraints. Finally, we\nintend to explore in future research how the knowledge\nencoded in our model could be reused to guide other tasks,\nfor example, automatic chord recognition from audio ﬁles.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4307. ACKNOWLEDGEMENTS\nThis work is supported by the European Research Council\n(ERC) under the EU’s Horizon 2020 research & innovation\nprogramme, grant agreement No. 101019375 (“Whither\nMusic?”), and the Federal State of Upper Austria (LIT AI\nLab).\n8. REFERENCES\n[1]S. Abdallah, N. Gold, and A. Marsden, “Analysing\nsymbolic music with probabilistic grammars,” Compu-\ntational music analysis , pp. 157–189, 2015.\n[2]E. Nakamura, M. Hamanaka, K. Hirata, and K. Yoshii,\n“Tree-structured probabilistic model of monophonic\nwritten music based on the generative theory of tonal\nmusic,” in Proceedings of the International Conference\non Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2016, pp. 276–280.\n[3]M. Hamanaka, K. Hirata, and S. Tojo, “Time-span tree\nleveled by duration of time-span,” in Proceedings of the\nInternational Symposium on Computer Music Multidis-\nciplinary Research (CMMR) , 2021, pp. 155–164.\n[4]C. Finkensiep and M. A. Rohrmeier, “Modeling and\ninferring proto-voice structure in free polyphony,” in\nProceedings of the International Society for Music In-\nformation Retrieval Conference (ISMIR) , 2021, pp. 189–\n196.\n[5]M. Rohrmeier, “Towards a generative syntax of tonal\nharmony,” Journal of Mathematics and Music , vol. 5,\nno. 1, pp. 35–53, 2011.\n[6]M. Granroth-Wilding and M. Steedman, “A robust\nparser-interpreter for jazz chord sequences,” Journal\nof New Music Research , vol. 43, no. 4, pp. 355–374,\n2014.\n[7]D. Harasim, M. Rohrmeier, and T. J. O’Donnell, “A\ngeneralized parsing framework for generative models of\nharmonic syntax.” in Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , 2018, pp. 152–159.\n[8]O. Melkonian, “Music as language: putting probabilis-\ntic temporal graph grammars to good use,” in Proceed-\nings of the ACM SIGPLAN International Workshop on\nFunctional Art, Music, Modeling, and Design , 2019, pp.\n1–10.\n[9]D. Harasim, T. J. O’Donnell, and M. A. Rohrmeier,\n“Harmonic syntax in time: rhythm improves grammati-\ncal models of harmony,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , 2019, pp. 335–342.\n[10] F. Foscarin, F. Jacquemard, and P. Rigaux, “Modeling\nand learning rhythm structure,” in Proceedings of the\nSound and Music Computing Conference (SMC) , 2019.[11] F. Foscarin, F. Jacquemard, P. Rigaux, and M. Sakai, “A\nparse-based framework for coupled rhythm quantization\nand score structuring,” in Proceedings of the Mathemat-\nics and Computation in Music International Conference\n(MCM) . Springer, 2019, pp. 248–260.\n[12] F. Foscarin, R. Fournier-S’Niehotta, and F. Jacquemard,\n“A diff procedure for xml music score ﬁles,” in Pro-\nceedings of the International Conference on Digital\nLibraries for Musicology (DLfM) , 2019.\n[13] M. Rohrmeier, “Towards a formalization of musical\nrhythm.” in Proceedings of the International Society for\nMusic Information Retrieval Conference (ISMIR) , 2020,\npp. 621–629.\n[14] W. T. Fitch and M. D. Martins, “Hierarchical process-\ning in music, language, and action: Lashley revisited,”\nAnnals of the New York Academy of Sciences , vol. 1316,\nno. 1, pp. 87–104, 2014.\n[15] M. Tsuchiya, K. Ochiai, H. Kameoka, and S. Sagayama,\n“Probabilistic model of two-dimensional rhythm tree\nstructure representation for automatic transcription of\npolyphonic midi signals,” in Proceedings of the Asia-\nPaciﬁc Signal and Information Processing Association\nAnnual Summit and Conference . IEEE, 2013, pp. 1–6.\n[16] I. Sakai, “Syntax in universal translation,” in Proceed-\nings of the International Conference on Machine Trans-\nlation and Applied Language Analysis , 1961.\n[17] M. Hamanaka, K. Hirata, and S. Tojo, “Musical struc-\ntural analysis database based on gttm,” in Proceedings\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2014, pp. 325–330.\n[18] D. Harasim, C. Finkensiep, P. Ericson, T. J. O’Donnell,\nand M. Rohrmeier, “The jazz harmony treebank,” in\nProceedings of the International Society for Music In-\nformation Retrieval Conference (ISMIR) , 2020, pp. 207–\n215.\n[19] M. Rohrmeier, “The syntax of jazz harmony: diatonic\ntonality, phrase structure, and form,” Music Theory and\nAnalysis (MTA) , vol. 7, no. 1, pp. 1–63, 2020.\n[20] D. Harasim, “The learnability of the grammar of jazz:\nBayesian inference of hierarchical structures in har-\nmony,” Ph.D. dissertation, EPFL, 2020.\n[21] C. Finkensiep, “The structure of free polyphony,” Ph.D.\ndissertation, EPFL, 2023.\n[22] M. Hamanaka, K. Hirata, and S. Tojo, “Implementing\n“a generative theory of tonal music”,” Journal of New\nMusic Research , vol. 35, no. 4, pp. 249–277, 2006.\n[23] ——, “FATTA: Full automatic time-span tree analyzer,”\ninInternational Computer Music Conference (ICMC) ,\nvol. 1, 2007, pp. 153–156.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n431[24] ——, “deepGTTM-III: Multi-task Learning with\nGrouping and Metrical Structures,” in Proceedings of\nthe International Symposium on Computer Music Mul-\ntidisciplinary Research (CMMR) , 2018, pp. 238–251.\n[25] Y .-R. Lai and A. W.-Y . Su, “Deep learning based de-\ntection of GPR6 GTTM global feature rule of music\nscores,” in Proceedings of the International Conference\non New Music Concepts , vol. 56, 2021.\n[26] T. Dozat and C. D. Manning, “Deep biafﬁne attention\nfor neural dependency parsing,” in Proceedings of the\nInternational Conference on Learning Representations\n(ICLR) , 2017.\n[27] ——, “Simpler but more accurate semantic dependency\nparsing,” in Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics , 2018.\n[28] X. Wang, J. Huang, and K. Tu, “Second-order semantic\ndependency parsing with end-to-end neural networks,”\n2019, pp. 4609—-4618.\n[29] H. He and J. D. Choi, “Establishing strong baselines\nfor the new decade: Sequence tagging, syntactic and\nsemantic parsing with bert,” in Proceedings of the Inter-\nnational Florida Artiﬁcial Intelligence Research Society\nConference , 2019, pp. 228–233.\n[30] M. Zhang, “A survey of syntactic-semantic parsing\nbased on constituent and dependency structures,” Sci-\nence China Technological Sciences , vol. 63, no. 10, pp.\n1898–1920, 2020.\n[31] F. Lerdahl and R. S. Jackendoff, A generative theory of\ntonal music . MIT press, 1985.\n[32] J. Daniel, M. James H et al. ,Speech and language pro-\ncessing: An introduction to natural language process-\ning, computational linguistics, and speech recognition .\nPrentice Hall series in artiﬁcial intelligence, 2007.\n[33] L. Kong, A. M. Rush, and N. A. Smith, “Transforming\ndependencies into phrase structures,” in Proceedings\nof the Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies , 2015, pp. 788–798.\n[34] N. Fradet, J.-P. Briot, F. Chhel, A. El Fal-\nlah Seghrouchni, and N. Gutowski, “MidiTok: A python\npackage for MIDI ﬁle tokenization,” in Late-Breaking\nDemo Session of the International Society for Music\nInformation Retrieval Conference (ISMIR) , 2021.\n[35] N. Fradet, J.-P. Briot, F. Chhel, A. E. F. Seghrouchni,\nand N. Gutowski, “Byte pair encoding for symbolic\nmusic,” arXiv preprint arXiv:2301.11975 , 2023.\n[36] EuroCC National Competence Center Sweden\n(ENCCS), “Graph neural networks and trans-\nformer workshop,” https://enccs.github.io/gnn_\ntransformers/notebooks/session_1/1b_vector_sums_\nvs_concatenation/, 2022.[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in neural informa-\ntion processing systems , vol. 30, 2017.\n[38] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention\nwith relative position representations,” in Proceedings\nof the North American Chapter of the Association for\nComputational Linguistics , 2018.\n[39] D. Fernández-González and C. Gómez-Rodríguez,\n“Transition-based semantic dependency parsing with\npointer networks,” Proceedings of the Annual Meeting\nof the Association for Computational Linguistics , 2020.\n[40] J. M. Eisner, “Three new probabilistic models for depen-\ndency parsing: An exploration,” in Proceedings of the\nInternational Conference on Computational Linguistics\n(COLING) , 1996.\n[41] J. Edmonds, “Optimum branchings,” Journal of Re-\nsearch of the national Bureau of Standards , vol. 71,\nno. 4, pp. 233–240, 1967.\n[42] Y .-J. Chu, “On the shortest arborescence of a directed\ngraph,” Scientia Sinica , vol. 14, pp. 1396–1400, 1965.\n[43] C. E. Cancino-Chacón, S. D. Peter, E. Karystinaios,\nF. Foscarin, M. Grachten, and G. Widmer, “Partitura:\nA Python Package for Symbolic Music Processing,” in\nProceedings of the Music Encoding Conference (MEC) ,\nHalifax, Canada, 2022.\n[44] D. Hendrycks and K. Gimpel, “Gaussian error lin-\near units (GELUs),” arXiv preprint arXiv:1606.08415 ,\n2016.\n[45] I. Loshchilov and F. Hutter, “Decoupled weight decay\nregularization,” in International Conference on Learn-\ning Representations (ICLR) , 2019.\n[46] X. S. Huang, F. Perez, J. Ba, and M. V olkovs, “Improv-\ning transformer optimization through better initializa-\ntion,” in Proceedings of the International Conference\non Machine Learning (ICML) , 2020, pp. 4475–4483.\n[47] S. Mishra, B. L. Sturm, and S. Dixon, “Local Inter-\npretable Model-agnostic Explanations for Music Con-\ntent Analysis,” in Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2017, pp. 537–543.\n[48] F. Foscarin, K. Hoedt, V . Praher, A. Flexer, and G. Wid-\nmer, “Concept-based techniques for \"musicologist-\nfriendly\" explanations in a deep music classiﬁer,” in\nProceedings of the International Society for Music In-\nformation Retrieval Conference (ISMIR) , 2022.\n[49] V . Praher, K. Prinz, A. Flexer, and G. Widmer, “On the\nveracity of local, model-agnostic explanations in audio\nclassiﬁcation: targeted investigations with adversarial\nexamples,” in Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2021.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n432"
    },
    {
        "title": "Impact of Time and Note Duration Tokenizations on Deep Learning Symbolic Music Modeling.",
        "author": [
            "Nathan Fradet",
            "Nicolas Gutowski",
            "Fabien Chhel",
            "Jean-Pierre Briot"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265229",
        "url": "https://doi.org/10.5281/zenodo.10265229",
        "ee": "https://zenodo.org/records/10265229/files/000009.pdf",
        "abstract": "Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways, and recent research has focused on developing more efficient methods. However, the key differences between these methods are often unclear, and few studies have compared them. In this work, we analyze the current common tokenization methods and experiment with time and note duration representations. We compare the performance of these two impactful criteria on several tasks, including composer classification, emotion classification, music generation, and sequence representation. We demonstrate that explicit information leads to better results depending on the task.",
        "zenodo_id": 10265229,
        "dblp_key": "conf/ismir/FradetGCB23",
        "keywords": [
            "Symbolic music",
            "deep learning tasks",
            "generation",
            "transcription",
            "synthesis",
            "Music Information Retrieval (MIR)",
            "discrete models",
            "tokenization",
            "time duration",
            "note duration"
        ],
        "content": "IMPACT OF TIME AND NOTE DURATION TOKENIZATIONS ON DEEP\nLEARNING SYMBOLIC MUSIC MODELING\nNathan Fradet1,2Nicolas Gutowski3Fabien Chhel3,4Jean-Pierre Briot1\n1Sorbonne University, CNRS, LIP6, F-75005 Paris\n2Aubay, Boulogne-Billancourt, France\n3University of Angers, LERIA, 49000 Angers, France\n4ESEO-TECH / ERIS, 49100 Angers, France\nnathan.fradet@lip6.fr\nABSTRACT\nSymbolic music is widely used in various deep learning\ntasks, including generation, transcription, synthesis, and\nMusic Information Retrieval (MIR). It is mostly employed\nwith discrete models like Transformers, which require mu-\nsic to be tokenized, i.e., formatted into sequences of dis-\ntinct elements called tokens. Tokenization can be per-\nformed in different ways, and recent research has focused\non developing more efﬁcient methods. However, the key\ndifferences between these methods are often unclear, and\nfew studies have compared them. In this work, we analyze\nthe current common tokenization methods and experiment\nwith time and note duration representations. We compare\nthe performance of these two impactful criteria on several\ntasks, including composer classiﬁcation, emotion classiﬁ-\ncation, music generation, and sequence representation. We\ndemonstrate that explicit information leads to better results\ndepending on the task.\n1. INTRODUCTION\nMost tasks involving using deep learning with symbolic\nmusic [1] are performed with discrete models, such as\nTransformers [2]. To use these models, the music must\nﬁrst be formatted into sequences of distinct elements, com-\nmonly called tokens. For instance, a token can represent a\nnote attribute or a time event. The set of all known tokens\nis commonly called the vocabulary, and each token is as-\nsociated to a unique integer id. These ids are used as input\nand output of models.\nCompared to text, tokenizing music provides greater\nﬂexibility, as a musical piece can be played by different\ninstruments and composed of multiple simultaneous notes,\neach having several properties such as pitch, duration and\nvelocity. As a result, it is necessary to represent these ele-\nments in conjunction with the time dimension. To achieve\n© F. Author, S. Author, and T. Author. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: F. Author, S. Author, and T. Author, “Impact of time and\nnote duration tokenizations on deep learning symbolic music modeling”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.this, researchers have developed various methods of tok-\nenizing music, which are introduced in the next section.\nWhile these works offer model performance compar-\nisons between tokenization strategies, their main differ-\nences or similarities are not always clearly stated. Few\nexperiments have been conducted to compare model per-\nformances using different tokenization strategies. Addi-\ntionally, these studies mostly focus on music generation,\nfor which evaluations are performed on results obtained\nautoregressively, which accumulates biases [3] and is ar-\nguably difﬁcult to evaluate [4].\nThis paper’s primary contribution is a thorough and\nwell-designed comparison of common tokenization tech-\nniques. Our focus is on two critical aspects: the repre-\nsentation of time and note duration. We believe that they\nare signiﬁcant and impactful design choices for any mu-\nsic tokenization approach. Through experiments on com-\nposer classiﬁcation, emotion classiﬁcation, music gener-\nation, and sequence representation, we demonstrate that\nthese design choices produce varying results depending\non the task, model type, and inference process. Autore-\ngressive generation beneﬁts from explicit note duration\nand time shift tokens, while explicit note offset is more\ndiscriminating better suited for contrastive learning ap-\nproaches.\nWe present next the related works, followed by an anal-\nysis of music tokenization, experimental results, and ﬁ-\nnally a conclusion. The source code is available for re-\nproducibility.1\n2. DECOMPOSING MUSIC TOKENIZATION\n2.1 Related works\nEarly works using discrete models for symbolic music,\nsuch as DeepBach [5] or FolkRNN [6], rely on speciﬁc\ntokenizations often tied to their training data. Since then,\nresearchers introduced more general representations appli-\ncable to any kind of music. The most commonly used are\nMidi-Like [7] and REMI [8]. The former tokenizes music\nby representing tokens as the same types of events from\nthe MIDI protocol, while the latter represents time with\nBar andPosition tokens and note durations with explicit\n1https://github.com/Natooz/time-duration-music-modeling89Time Note duration\nTokenization TimeShift Bar +Pos. Duration NoteOff\nMIDI-Like [7]√- -√\nREMI [8] -√ √-\nStructured [17]√-√-\nTSD [15]√-√-\nOctuple [10] -√ √-\nTable 1 : Time and note duration representations of com-\nmon tokenizations. Pos. stands for Position.\nDuration tokens. Additionally, REMI includes tokens with\nadditional information such as chords and tempo.\nMore recently, researchers have focused on improv-\ning the efﬁciency of models with new tokenizations tech-\nniques: Compound Word [9],Octuple [10] and PopMAG\n[11] merge embedding vectors before passing them to the\nmodel; 2) LakhNES [12] and [13], SymphonyNet [14]\nand [15] use tokens combining several values, such as pitch\nand vocabulary.\n2.2 Music tokenization design\nWhen analyzing the possible designs of music tokeniza-\ntion, we can distinguish seven key dimensions:\n•Time : Type of token representing time, either\nTimeShift indicating time movements, or Bar and\nPosition indicating new bars and the positions of the\nnotes within them. We can also consider the unit of\nTime-Shift tokens, either in beats or in seconds.2\n•Notes duration : How notes durations are repre-\nsented, with either Duration orNoteOff tokens.\n•Pitch : Most works use tokens representing absolute\npitch values, although recent work shed light on the\nexpressiveness gain of representing as intervals in-\nstead [16];\n•Multitrack representation : The representation of\nseveral music tracks in a sequence, i.e., how are the\nnotes linked to their associated track.\n•Additional information : Any additional informa-\ntion such as chords, tempo, rests, note density. Ve-\nlocity can also falls in this category;\n•Downsampling : How \"continuous-like\" features\nare downsampled into discrete sets, e.g. the 128 ve-\nlocity values reduced to 16 values;\n•Sequence compression : Methods to reduce the se-\nquence lengths, such as merging tokens and embed-\nding vectors.\nAs time and note duration can both be represented in\ntwo different ways, existing tokenizations can be easily\nclassiﬁed based on these dimensions, as shown in table 1.\n2In this paper we only treat of the beat unit. The MIDI protocol repre-\nsents time in tickunit, which value is proportional to the time division (in\nticks per beat) and tempo. Hence, working with seconds would require a\nconversion from ticks.However, other dimensions offer a broader spectrum of po-\ntential designs.\nFor instance multitrack can be represented by\nProgram tokens3preceding notes as in FIGARO [18],\ndistinct tracks sequences separated by Program tokens\nas in MMM [19], combined note and instrument tokens as\nLakhNes [12] and MuseNet [13], or merging Program\nembeddings with the associated note tokens (MMT [20],\nMusicBert [10]). One could even infer each sequence sep-\narately and lately model their relationships with operations\naggregating their hidden states an in ColBERT [21].\nThe MIDI protocol supports a set of effects and meta-\ndata that can also be represented when tokenizing symbolic\nmusic, such as tempo, time signature, sustain pedal or con-\ntrol changes. Some works also include explicit Chord\ntokens, detected with rule-based methods. Nevertheless,\nonly a few works experimented with such additional to-\nkens so far ( [8, 22]).\nPrevious works have mainly compared tokenization\nstrategies by evaluating models with automatic and some-\ntimes subjective (human) metrics, but often do not proceed\nto comparisons between the ways to represent one of the\ndimensions we introduced previously. [8] compared results\nfor the generation task, for the use of Bar andPosition\ntokens versus TimeShift in seconds and beats.\nTo the best of our knowledge, no comprehensive work\nand empirical analysis have fairly compared these possi-\nble tokenization choices. Conducting such an assessment\nwould require an extensive survey. In this paper, we specif-\nically focus on the time and note duration representations,\nas they are the two main characteristics present in every\ntokenization.\nWe want to highlight the importance of the explicit in-\nformation carried by the token types, as they directly im-\npact the performances of models. TimeShift tokens\nrepresent explicit time movements, and especially the time\ndistances between successive notes. On the other hand,\nBar andPosition tokens bring explicit information on\nthe absolute positions (within bars) of the notes, but not\nthe onset distances between notes. One could assume that\nthe former might help to model melodies, and the lat-\nter rhythm and structure. For note duration, Duration\ntokens intuitively express the absolute durations of the\nnotes, while NoteOff tokens explicitly indicates the off-\nset times. With NoteOff , a model would have to model\nnote durations from the combinations of previous time to-\nkens.\nOur experiments aim to demonstrate the impact of dif-\nferent combinations of time and note duration tokens on\nmodel performance and which combinations are suitable\nfor different tasks. Next, we introduce our methodology.\n3. METHODOLOGY\n3.1 Models and trainings\nFor all experiments, we use the Transformer architecture\n[2], with the same model dimensions: 12 layers, with di-\n3Following the conventional programs from the MIDI protocol.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n90mension of 768 units, 12 attention heads and inner feed-\nforward layers of 3072.\nFor classiﬁcation and sequence representation, it is ﬁrst\npretrained on 100k steps and a learning rate of 10−4, then\nﬁnetuned on 50k steps and a learning rate of 3×10−5,\nwith a batch size of 48 examples. An exception is made\nfor the EMOPIA dataset, for which we set 30k pretraining\nsteps and 15k ﬁnetuning steps, as it is fairly small. These\nmodels are based on the BERT [23] implementation of the\nTransformers library [24]. We use the same pretraining\nthan the original BERT: 1) from 15% of the input tokens,\n80% is masked with a special MASK token, and 20% is\nrandomized; 2) half of the inputs have 50% of their tokens\n(starting from the end) shufﬂed and separated with a spe-\ncialSEP token, and the model is trained to detect if the\nsecond part is the next of the ﬁrst.\nFor generation, the model is based on the GPT2 im-\nplementation of the Transformers library [24]: it uses\na causal attention mask, so that for each element in\nthe sequence, the model can only attend to the current\nand previous elements. The training is performed with\nteacher forcing, the cross-entropy loss is deﬁned as: ℓ=\n−/summationtextn\nt=1logpθ(xt|x≤n).\nAll trainings are performed on V100 GPUs, using auto-\nmatic mixed precision [25], the Adam optimizer [26] with\nβ1= 0.9,β2= 0.999andϵ= 10−8, and dropout, weight\ndecay and a gradient clip norm of respectively 10−1,10−2\nand3. Learning rates follow a warm-up schedule: they are\ninitially set to 0, and increase to their default value during\nthe ﬁrst 30% of training, then slowly decrease back to 0.\n10% of the data is used for validation during training,\nand 15% to test models. Inputs contains 384 to 512 tokens,\nand begin with a BOS (Beginning of Sequence) token and\nend with aEOS (End of Sequence) one.\n3.2 Tokenizations\nWe investigate here the four combinations of possible time\nand note duration representation. In the results, we re-\nfer to them as TS(TimeShift ),Pos(Position ),Dur\n(Duration ) and NOff (NoteOff ). It is worth noting\nthat TS+Dur is equivalent to TSD [15] and Structured\n[17], TS+NOff is equivalent to MIDI-Like [7], and Pos+\nDur is equivalent to REMI (without additional tokens for\nchords and tempo).\nWe apply different resolutions for Duration and\nTimeShift token values: those up to one beat are down-\nsampled to 8 samples per beat (spb), those from one to\ntwo beats to 4 spb, those from two to four beats to 2 spb,\nand those from four to eight beats to 1 spb. Thus, short\nnotes are represented more precisely than longer ones.\nPosition tokens are downsampled to 8 spb, resulting\nin 32 different tokens as we only consider the 4/* time\nsignature. This allows to represent the 16thnote. We\nonly consider pitches within the recommended range for\npiano (program 0) speciﬁed in the General MIDI 2 speci-\nﬁcations4: 21 to 108. We then deduplicate all duplicated\n4Available on the MIDI Manufacturers Association website.notes. Velocities are downsampled to 8 distinct values. No\nadditional token (e.g., Chord ,Tempo ) is used.\nWe perform data augmentation by creating variations of\nthe original data with pitches increased and decreased by\ntwo octaves, and velocity by one value. Finally, following\n[15], we use Byte Pair Encoding to build the vocabularies\nup to 2k tokens for generation and 5k for other tasks. All\nthese preprocessing and tokenization steps were performed\nwith MidiTok [27].\n4. GENERATION\nFor the generative task, we use the POP909 dataset [28].\nThe models start with prompt made of between 384 to 512\ntokens, then autoregressively generate 512 additional to-\nkens. Evaluation of generated results remains an open is-\nsue [4]. Previous work often perform measures of similar-\nity of certain features such as pitch range or class, between\nprompts and generated results, alongside human evalua-\ntions. Feature similarity is however arguably not very in-\nsightful: a generated result could have very similar features\nto its prompts while being of poor quality. Human evalu-\nations, while being more reliable on the quality can also\ninduce biases. Besides, [8] already shows results on an ex-\nperiment similar to ours.\nHence we choose to evaluate results on the ratios of pre-\ndiction errors: Token Syntax Error (TSE) [15]. This met-\nric is bias-free and directly linked to the design choices of\nthe tokenizations. It allows us to measure how a model\nachieves to make reliable predictions based on the input\ncontext and the knowledge it learned.\nWe use the categories from [15]:\n•TSEtype: an error of type, e.g., when the model\npredicts a token of an incompatible type with the\nprevious one.\n•TSEtime: a wrong predicted Position value,\nthat goes back or stay in time.\n•TSEdupn (duplicated note): a note predicted\nwhereas it was already being played at the current\ntime being.\n•TSEnnof (no NoteOff): a NoteOn token been pre-\ndicted with no following NoteOff token to end it.\n•TSEnnon (no NoteOn): NoteOff token predicted\nwhereas this note was not being played.\nFor each generated token, a rule-based function ana-\nlyzes its type and value to determine if both are valid, or\nwhich type of error was made otherwise. The overall num-\nber of errors is normalized by the number of predicted to-\nkens.\nThe results are reported in table 2. We ﬁrst observe that\nthe type error ratios are lower than in other categories. This\nis excepted since it is less computationally demanding to\nmodel the possible next types depending solely on the last\none, rather than on the value of the predicted token, forProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n910 8 16 24 31\nPosition0.000.010.020.030.040.050.06Probability\n0 8 16 24 31\nPosition0.0000.0050.0100.0150.0200.0250.0300.035Probability\n0 8 16 24 31\nPosition0.000.010.020.030.040.05Probability\n0 8 16 24 31\nPosition0.000.010.020.030.040.050.060.07Probability\n0 8 16 24 31\nPosition0.0000.0050.0100.0150.0200.0250.0300.035Probability\n0 8 16 24 31\nPosition0.0000.0050.0100.0150.0200.0250.0300.035Probability\n0 8 16 24 31\nPosition0.0000.0050.0100.0150.0200.0250.030Probability\n0 8 16 24 31\nPosition0.000.010.020.030.04Probability\n0 8 16 24 31\nPosition0.000.010.020.030.040.05Probability\n0 8 16 24 31\nPosition0.0000.0050.0100.0150.0200.0250.0300.035Probability\n1 2 3 45678\nBeat0.000.050.100.150.200.25Probability\n(a)Ts+Dur\n1 2 3 45678\nBeat0.000.050.100.150.20Probability\n(b)Ts+NOff\n1 2 3 45678\nBeat0.000.050.100.150.200.25Probability\n(c)Pos+Dur\n1 2 3 45678\nBeat0.000.020.040.060.080.10Probability\n(d)Pos+NOff\n1 2 3 45678\nBeat0.000.020.040.060.080.100.120.140.16Probability\n(e) POP909 dataset\nFigure 1 : Histograms of the note onset positions within bars (top-row), note offset positions within bars (middle-row)\nand note durations (bottom-row) of the generated notes. There are 32 possible positions within a bar, numerated from 0\n(beginning of bar) to 31 (last 32thnote). The durations are expressed in beats, ranging from a 32thnote to 8 beats.\nPitch Velocity Duration TimeShiftNext token\nPitch\nVelocity\nDuration\nTimeShift\n0.00.20.40.60.81.0\n(a)Ts+Dur\nNoteOn Velocity TimeShift NoteOffNext token\nNoteOn\nVelocity\nTimeShift\nNoteOff0.20.40.60.8\n(b)Ts+NOff\nBar Position Pitch Velocity DurationNext token\nBar\nPosition\nPitch\nVelocity\nDuration\n0.00.20.40.60.81.0\n(c)Pos+Dur\nBar Position NoteOn Velocity NoteOffNext token\nBar\nPosition\nNoteOn\nVelocity\nNoteOff\n0.00.20.40.60.81.0\n(d)Pos+NOff\nFigure 2 : Token type succession heatmaps of the gener-\nated results. The horizontal axis denotes the next token\ntype per from the ones on the vertical axis. Each row is\nnormalized to a sum of 1.\nwhich the validity depends on a the whole previous con-\ntext.\nPosition tokens bring almost no type errors, but a\nnoticeable proportion of time errors. When decoding to-\nkens to notes, this means that the time may go backward,\nand resulting in sections of overlapping notes.\nAlthoughDuration tokens seem to bring slightly\nmore note duplication errors, the use of NoteOn and\nNoteOff tokens results in a considerable proportion ofTokenization TSEtype↓TSEtime↓TSEdupn↓TSEnnon↓TSEnnof↓\nTS+Dur <10−3- 0.014 - -\nTS+NOff <10−3- 0.001 0.109 0.040\nPos+Dur 0.002 0.113 0.032 - -\nPos+NOff 0.002 0.127 0.005 0.095 0.066\nTable 2 : Prediction error ratios when performing autore-\ngressive generation. - symbol stands for not concerned,\nand can be interpreted as 0.\nnote prediction errors. NoteOff tokens predicted while\nthe associated note was not being played ( TSEnnon ) does\nnot have undesirable consequences when decoding tokens\nto notes, but it pointlessly extends the sequence, reducing\nthe efﬁciency of the model, and may mislead the next to-\nken predictions. Additionaly, NoteOn tokens predicted\nwithout associated NoteOff (TSEnnof) result in notes\nnot properly ended. This error can only be handled by ap-\nplying a maximum note duration after decoding. Explicit\nDuration tokens allows to specify in advance this in-\nformation, for both short and long notes. Conversely, with\nNoteOff tokens, the note duration information is implicit\nand inferred by the combinations of NoteOn ,NoteOff\nand time tokens. This can be interpreted as an extra ef-\nfort for the model. Consequently, some uncertainty on\nthe duration accumulates over autoregressive steps dur-\ning generation. Based on these results, the best tradeoff\nensuring good predictions seems to represent time with\nTimeShift tokens and note duration with Duration\ntokens.\nIn ﬁg. 1 we observe the positions within bars and dura-\ntions of the generated notes. In all cases, onset positions\nare more distributed at the beginning of the bars. This is\nespecially the case with Bar andPosition tokens, for\nwhich we may ﬁnd unexpected rests at the end of bars,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n92whenBar tokens are predicted during the generation be-\nfore that the current bar is completed. The TS+Dur com-\nbination places note onsets much more on even positions.\nThe probability mass of TimeShift tokens (especially for\nshort values) seems to be much higher. However, this is not\nthe case for the TS+NOff combination, as TimeShift\ntokens have to be predicted to move the time on odd po-\nsitions of note offsets. As shown in ﬁg. 2, right after the\nmodel is likely to predict a next note, resulting in evenly\ndistributed onset distribution.\nFinally, the use of NoteOff tokens tends to produce\nlonger note durations, especially when combined with\nPosition tokens. In this last case, we can assume\nthat the model might \"forget\" the notes currently being\nplayed, and that it struggles more to model their durations\nthat have to be implicitly deduced from the past Bar and\nPosition tokens.\nTokenization Top-20 composers ↑Top-100 composers ↑Emotion↑\nTS+Dur 0.973 0.941 0.983\nTS+NOff 0.962 0.930 0.962\nPos+Dur 0.969 0.927 0.963\nPos+NOff 0.963 0.925 0.956\nTable 3 : Accuracy on classiﬁcation tasks.\n5. CLASSIFICATION\nFor some classiﬁcation tasks, symbolic music is arguably\nbetter suited than audio or piano roll. This is particu-\nlarly true for classical music feature classiﬁcation, such\nas composer [29]. Mono-instrument music with complex\nmelodies and harmonies and no particular audio effect ben-\neﬁt from being represented as discrete for classiﬁcation\nand modeling tasks. Given this, it felt important to us to\nconduct experiments on such task.\nWe choose to experiment with the GiantMIDI [30]\ndataset for composer classiﬁcation and the EMOPIA [31]\ndataset for emotion classiﬁcation. The results, as shown in\ntable 3, indicate that there is very little difference between\nthe various tokenization methods. However, the combina-\ntion ofTimeShift andDuration consistently outper-\nforms the others by one point\nThe classiﬁcation task involves modeling the patterns\nfrom data that are characteristic to composers or emotions.\nHere, it seems that the time distance between notes, and\ntheir explicit duration play a role in these task, more than\nnote offsets or onset positions. This comes with no sur-\nprise for the composer classiﬁcation task, considering that\nthe data is largely composed of complex music with dense\nmelodies and harmonies, featuring mostly short succes-\nsive notes. Intuitively, patterns of note successions and\nchords are more easily distinguishable with explicit dura-\ntions. With implicit note durations, the overall patterns\nmust be deduced by the combinations of NoteOn and\nNoteOff tokens while keeping track of the time.6. SEQUENCE REPRESENTATION\nThe last task that we wished to explore is sequence repre-\nsentation. It consists in obtaining a ﬁxed size embedding\nrepresentation of an input sequence of tokens pθ:VL/ma√sto→\nRd. HereV⊂Ndenotes the token ids of the vocabulary\nV,Lis the variable input sequence length, and dthe size of\nembeddings. In other words, the model learns to project an\ninput token sequence into a embedding space, thus provid-\ning a universal representation. We ﬁnd this task interesting\nand well-suited to assess model performances as it directly\ntrains it to model the relationships between tokens within\nthe input sequence and between different representations\nthemselves. While the real-world applications of this task\nfor symbolic music are currently limited, it serves as a use-\nful benchmarking technique for measuring how tokeniza-\ntion impacts the learning of models.\nThis task has previously been addressed in natural lan-\nguage processing by SentenceBERT [32] or SimCSE [33].\nWe adopted the approach of the latter, which uses con-\ntrastive learning to train the model to learn sequence rep-\nresentations, for which similar inputs have higher cosine\nsimilarities. The sequence embedding is obtained by per-\nforming a pooling operation on the output hidden states\nof the model. We decided to use the last hidden state of\ntheBOS token position, as it yielded good results with\nSimCSE [33]5. We trained the models with the dropout\nmethod: during training, a batch of nsequences X=\n{xi}n\ni=0is passed twice to the model, but with different\ndropout masks, resulting in different output sequence em-\nbeddings Z={zi}N\ni=0and¯Z={¯zi}N\ni=0. Although the\ndropout altered the outputs, most of the input information\nis still accessible to the model. Hence, we expect pairs\nof sequence embeddings (zi,¯zi)to be similar, so having a\nhigh cosine similarity. To achieve this objective, we train\nthe model with a loss function deﬁned by the cross-entropy\nfor in-batch pairwise cosine similarities ( sim):\nℓi=−logesim(zi,¯zi)/τ\n/summationtextN\nj=1esim(zi,¯zj)/τ(1)\nAs a result, the model will effectively learn to cre-\nate similar sequence embeddings for similar inputs, while\npushing apart those with dissimilarities. We kept a 0.1\ndropout value to train the models, and used the GiantMIDI\ndataset [30].\nEvaluation of sequence representation is intuitively per-\nformed by measuring the distances and similarities of pairs\nof similar sequences. We resort to data augmentation by\nshifting the pitch and velocity of the sequences in order to\nget pairs of similar music sequences. The augmented data\nkeeps most of the information of the original data. As such,\nthe models are expected to produce similar embeddings for\npairs of original-augmented sequence. Ideally, the cosine\nsimilarity should be high, yet not to be equal to 1, as this\nwould indicate that the model fails to capture the differ-\nences between the two sequences. The results, presented\nin ﬁg. 3, indicate that Position -based tokenizations per-\n5SimCSE uses a CLS token which is equivalent to BOS in our case.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n93TS+Dur\nTS+NOff\nPos+Dur\n0.2 0.0 0.2 0.4 0.6 0.8 1.0Pos+NOff\n(a) Pitch +1TS+Dur\nTS+NOff\nPos+Dur\n0.2 0.0 0.2 0.4 0.6 0.8 1.0Pos+NOff\n(b) Pitch +12\nTS+Dur\nTS+NOff\nPos+Dur\n0.2 0.0 0.2 0.4 0.6 0.8 1.0Pos+NOff\n(c) Velocity +1TS+Dur\nTS+NOff\nPos+Dur\n0.2 0.0 0.2 0.4 0.6 0.8 1.0Pos+NOff\n(d) Pitch +12 and Velocity +1\nFigure 3 : Density plots of cosine similarities between pairs of original and augmented token sequences.\nform slightly better. Therefore, it appears that explicit note\nonset and offset positions information facilitates models to\nobtains a universal musical representation.\nUnlike classiﬁcation, the contrastive learning objective\nmodels the similarities and dissimilarities between exam-\nples in the same batch. In this context, note onset and offset\npositions appear to be helpful for the models to distinguish\nmusic.\nWe also note the contrasting results when augmenting\nthe velocity. Increasing it by one unit, which would be\nequivalent to playing just a little bit louder, have arguably a\nvery small impact. As a result, the models mostly produces\nembeddings that are almost identical for the original and\nthe augmented sequences, but also exhibits uncertainty for\na notable proportion of samples.\nTo complement these results, we estimated the isotropy\nof sets of sequence embeddings. Isotropy measures the\nuniformity of the variance of a set points in a space.\nMore intuitively, in an isotropic space, the embeddings\nare evenly distributed. It has been associated with im-\nproved performances in natural language tasks [34–36],\nbecause embeddings are more equally distant proportion-\nally to the density of their area, and are in turn more dis-\ntinct and distinguishable. We choose to estimate it with\nthe intrinsic dimension of the sets of embeddings. In-\ntrinsic dimension is the number of dimensions required to\nrepresent a set of points. It can be estimated by several\nmanners [37]. We choose Principal Component Analysis\n(PCA) [38], method of moments (MOM) [39], Two Near-\nest Neighbors (TwoNN) [40] and FisherS [41]. The re-\nsults, reported in table 4, show that the embeddings cre-\nated from the Pos+Dur combination tend to occupy more\nspace across the dimension of the model, and are poten-\ntially better distributed.\n7. CONCLUSION\nWe have discussed the importance of different aspects of\nsymbolic music tokenization, and focused on two major\nones: the time and note duration representations. We\nshowed that different tokenization strategies can lead toTokenization lPCA ↑MOM↑TwoNN↑FisherS↑\nTS+Dur 213 42.6 34.3 17.5\nTS+NOff 161 43.7 32.7 17.5\nPos+Dur 146 39.1 33.1 17.1\nPos+NOff 177 45.2 35.6 17.8\nTable 4 : Intrinsic dimension of sequence embeddings, as\nan estimation of isotropy.\ndifferent model performances due to the explicit informa-\ntion carried by tokens, depending on the task at hand.\nExplicitly representing note duration leads to better\nclassiﬁcation accuracy as it helps the models to capture\nthe melodies and harmonies of a music. Modeling dura-\ntions, when represented implicitly, adds an extra effort to\nthe model. However, the note offset position information\nit brings have been found to be more discriminative and\neffective in our contrastive learning experiment.\nFor music generation, the time representation plays a\nsigniﬁcant role, for which the note onset and offsets distri-\nbutions vary due to the successions of token types. Implicit\nnote durations are less suited for the autoregressive nature\nof this task, from a prediction error perspective, and some-\ntimes \"forgetting\" notes being played resulting in higher\ndurations.\nWe did not explore music transcription, for which we\ncan assume that implicit note durations (note onset and\noffset) might be better suited. When training with chunks\nof log-scaled mel-spectrograms as done by [42, 43], these\nmay contain frequencies of unended or not begun notes.\nSpecifying their original durations might approximate on-\nsets might alter model performances.\nFuture research will further explore the other dimen-\nsions of music tokenization, such as multitrack or meta-\ndata, on transcription and other tasks analogous to natural\nlanguage understanding.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n948. REFERENCES\n[1] J.-P. Briot, G. Hadjeres, and F.-D. Pachet, Deep\nLearning Techniques for Music Generation , ser. Com-\nputational Synthesis and Creative Systems. Springer\nInternational Publishing, 2020. [Online]. Available:\nhttps://www.springer.com/gp/book/9783319701622\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkor-\neit, L. Jones, A. N. Gomez, L. u. Kaiser, and\nI. Polosukhin, “Attention is all you need,” in Ad-\nvances in Neural Information Processing Systems ,\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, Eds.,\nvol. 30. Curran Associates, Inc., 2017. [Online].\nAvailable: https://proceedings.neurips.cc/paper/2017/\nﬁle/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[3] A. Holtzman, J. Buys, L. Du, M. Forbes, and\nY . Choi, “The curious case of neural text degen-\neration,” in International Conference on Learn-\ning Representations , 2020. [Online]. Available:\nhttps://openreview.net/forum?id=rygGQyrFvH\n[4] L.-C. Yang and A. Lerch, “On the evaluation of\ngenerative models in music,” Neural Comput. Appl. ,\nvol. 32, no. 9, p. 4773–4784, 5 2020. [Online].\nAvailable: https://doi.org/10.1007/s00521-018-3849-7\n[5] G. Hadjeres, F. Pachet, and F. Nielsen, “DeepBach:\na steerable model for Bach chorales generation,” in\nProceedings of the 34th International Conference\non Machine Learning , ser. Proceedings of Machine\nLearning Research, D. Precup and Y . W. Teh,\nEds., vol. 70. PMLR, 8 2017, pp. 1362–1371.\n[Online]. Available: https://proceedings.mlr.press/v70/\nhadjeres17a.html\n[6] B. L. Sturm, J. F. Santos, and I. Korshunova, “Folk\nmusic style modelling by recurrent neural networks\nwith long short-term memory units,” in Extended\nabstracts for the Late-Breaking Demo Session of\nthe 16th International Society for Music Information\nRetrieval Conference , 2015. [Online]. Available:\nhttps://ismir2015.ismir.net/LBD/LBD13.pdf\n[7] S. Oore, I. Simon, S. Dieleman, D. Eck, and\nK. Simonyan, “This time with feeling: Learning\nexpressive musical performance,” Neural Computing\nand Applications , vol. 32, p. 955–967, 2018. [Online].\nAvailable: https://link.springer.com/article/10.1007/\ns00521-018-3758-9\n[8] Y .-S. Huang and Y .-H. Yang, “Pop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions,” in Proceedings of the 28th ACM\nInternational Conference on Multimedia , ser. MM ’20.\nNew York, NY , USA: Association for Computing\nMachinery, 2020, p. 1180–1188. [Online]. Available:\nhttps://doi.org/10.1145/3394171.3413671[9] W.-Y . Hsiao, J.-Y . Liu, Y .-C. Yeh, and Y .-H. Yang,\n“Compound word transformer: Learning to compose\nfull-song music over dynamic directed hypergraphs,”\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence , vol. 35, no. 1, pp. 178–186, 5 2021.\n[Online]. Available: https://ojs.aaai.org/index.php/\nAAAI/article/view/16091\n[10] M. Zeng, X. Tan, R. Wang, Z. Ju, T. Qin, and T.-Y .\nLiu, “MusicBERT: Symbolic music understanding\nwith large-scale pre-training,” in Findings of the\nAssociation for Computational Linguistics: ACL-\nIJCNLP 2021 . Online: Association for Computational\nLinguistics, 8 2021, pp. 791–800. [Online]. Available:\nhttps://aclanthology.org/2021.ﬁndings-acl.70\n[11] Y . Ren, J. He, X. Tan, T. Qin, Z. Zhao, and T.-Y . Liu,\n“Popmag: Pop music accompaniment generation,”\ninProceedings of the 28th ACM International Con-\nference on Multimedia . Association for Computing\nMachinery, 2020, p. 1198–1206. [Online]. Available:\nhttps://doi.org/10.1145/3394171.3413721\n[12] C. Donahue, H. H. Mao, Y . E. Li, G. W. Cottrell,\nand J. J. McAuley, “Lakhnes: Improving multi-\ninstrumental music generation with cross-domain\npre-training,” in Proceedings of the 20th International\nSociety for Music Information Retrieval Conference,\nISMIR 2019, Delft, The Netherlands, November 4-\n8, 2019 , 2019, pp. 685–692. [Online]. Available:\nhttp://archives.ismir.net/ismir2019/paper/000083.pdf\n[13] C. Payne, “Musenet,” 2019. [Online]. Available:\nhttps://openai.com/blog/musenet\n[14] J. Liu, Y . Dong, Z. Cheng, X. Zhang, X. Li, F. Yu,\nand M. Sun, “Symphony generation with permutation\ninvariant language model,” in Proceedings of the 23rd\nInternational Society for Music Information Retrieval\nConference . Bengalore, India: ISMIR, Dec. 2022.\n[Online]. Available: https://arxiv.org/abs/2205.05448\n[15] N. Fradet, J.-P. Briot, F. Chhel, A. E. F. Seghrouchni,\nand N. Gutowski, “Byte Pair Encoding for symbolic\nmusic,” 2023. [Online]. Available: https://arxiv.org/\nabs/2301.11975\n[16] M. Kermarec, L. Bigo, and M. Keller, “Improving to-\nkenization expressiveness with pitch intervals,” in Ex-\ntended Abstracts for the Late-Breaking Demo Session\nof the 23nd International Society for Music Informa-\ntion Retrieval Conference , 2022. [Online]. Available:\nhttps://ismir2022program.ismir.net/lbd_369.html\n[17] G. Hadjeres and L. Crestel, “The piano inpainting\napplication,” 2021. [Online]. Available: https://arxiv.\norg/abs/2107.05944\n[18] D. von Rütte, L. Biggio, Y . Kilcher, and\nT. Hofmann, “FIGARO: Controllable music gen-\neration using learned and expert features,” inProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n95The Eleventh International Conference on Learn-\ning Representations , 2023. [Online]. Available:\nhttps://openreview.net/forum?id=NyR8OZFHw6i\n[19] J. Ens and P. Pasquier, “Mmm : Exploring conditional\nmulti-track music generation with the transformer,”\n2020. [Online]. Available: https://arxiv.org/abs/2008.\n06048\n[20] H.-W. Dong, K. Chen, S. Dubnov, J. McAuley, and\nT. Berg-Kirkpatrick, “Multitrack music transformer,”\ninICASSP 2023 - 2023 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2023, pp. 1–5.\n[21] O. Khattab and M. Zaharia, “Colbert: Efﬁcient\nand effective passage search via contextualized late\ninteraction over bert,” in Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval , ser.\nSIGIR ’20. New York, NY , USA: Association for\nComputing Machinery, 2020, p. 39–48. [Online].\nAvailable: https://doi.org/10.1145/3397271.3401075\n[22] J. Ching and y.-h. Yang, “Learning to generate piano\nmusic with sustain pedals,” in Extended Abstracts for\nthe Late-Breaking Demo Session of the 22nd Interna-\ntional Society for Music Information Retrieval Confer-\nence, 2021. [Online]. Available: https://archives.ismir.\nnet/ismir2021/latebreaking/0000017.pdf\n[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n“BERT: Pre-training of deep bidirectional transformers\nfor language understanding,” in Proceedings of the\n2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers) . Minneapolis, Minnesota: Association for\nComputational Linguistics, Jun. 2019, pp. 4171–\n4186. [Online]. Available: https://aclanthology.org/\nN19-1423\n[24] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jernite,\nJ. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. M. Rush, “Transformers: State-of-\nthe-art natural language processing,” in Proceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations .\nOnline: Association for Computational Linguistics,\nOct. 2020, pp. 38–45. [Online]. Available: https:\n//www.aclweb.org/anthology/2020.emnlp-demos.6\n[25] P. Micikevicius, S. Narang, J. Alben, G. Diamos,\nE. Elsen, D. Garcia, B. Ginsburg, M. Houston,\nO. Kuchaiev, G. Venkatesh, and H. Wu, “Mixed\nprecision training,” in International Conference on\nLearning Representations , 2018. [Online]. Available:\nhttps://openreview.net/forum?id=r1gs9JgRZ[26] D. P. Kingma and J. Ba, “Adam: A method\nfor stochastic optimization,” in 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings , 2015. [Online]. Available: http:\n//arxiv.org/abs/1412.6980\n[27] N. Fradet, J.-P. Briot, F. Chhel, A. El Fal-\nlah Seghrouchni, and N. Gutowski, “MidiTok: A\npython package for MIDI ﬁle tokenization,” in Ex-\ntended Abstracts for the Late-Breaking Demo Session\nof the 22nd International Society for Music Informa-\ntion Retrieval Conference , 2021. [Online]. Available:\nhttps://github.com/Natooz/MidiTok\n[28] Z. Wang, K. Chen, J. Jiang, Y . Zhang, M. Xu,\nS. Dai, G. Bin, and G. Xia, “Pop909: A pop-\nsong dataset for music arrangement generation,” in\nProceedings of 21st International Conference on\nMusic Information Retrieval, ISMIR , 2020. [Online].\nAvailable: https://arxiv.org/abs/2008.07142\n[29] Q. Kong, K. Choi, and Y . Wang, “Large-scale\nmidi-based composer classiﬁcation,” 2020. [Online].\nAvailable: https://arxiv.org/abs/2010.14805\n[30] Q. Kong, B. Li, J. Chen, and Y . Wang, “Giantmidi-\npiano: A large-scale midi dataset for classical piano\nmusic,” in Transactions of the International Society\nfor Music Information Retrieval , vol. 5, 2021, pp.\n87–98. [Online]. Available: https://transactions.ismir.\nnet/articles/10.5334/tismir.80/#\n[31] H. Hung, J. Ching, S. Doh, N. Kim, J. Nam,\nand Y . Yang, “EMOPIA: A multi-modal pop piano\ndataset for emotion recognition and emotion-based\nmusic generation,” in Proceedings of the 22nd\nInternational Society for Music Information Retrieval\nConference, ISMIR 2021, Online, November 7-12,\n2021 , J. H. Lee, A. Lerch, Z. Duan, J. Nam,\nP. Rao, P. van Kranenburg, and A. Srinivasamurthy,\nEds., 2021, pp. 318–325. [Online]. Available: https:\n//archives.ismir.net/ismir2021/paper/000039.pdf\n[32] N. Reimers and I. Gurevych, “Sentence-bert: Sentence\nembeddings using siamese bert-networks,” in Proceed-\nings of the 2019 Conference on Empirical Methods in\nNatural Language Processing . Association for Com-\nputational Linguistics, 11 2019. [Online]. Available:\nhttps://arxiv.org/abs/1908.10084\n[33] T. Gao, X. Yao, and D. Chen, “SimCSE: Simple\ncontrastive learning of sentence embeddings,” in\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing . Online\nand Punta Cana, Dominican Republic: Association\nfor Computational Linguistics, Nov. 2021, pp. 6894–\n6910. [Online]. Available: https://aclanthology.org/\n2021.emnlp-main.552Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n96[34] T. Wang and P. Isola, “Understanding contrastive\nrepresentation learning through alignment and uni-\nformity on the hypersphere,” in Proceedings of the\n37th International Conference on Machine Learn-\ning, ser. Proceedings of Machine Learning Research,\nH. D. III and A. Singh, Eds., vol. 119. PMLR,\n13–18 Jul 2020, pp. 9929–9939. [Online]. Available:\nhttps://proceedings.mlr.press/v119/wang20k.html\n[35] D. Bi ´s, M. Podkorytov, and X. Liu, “Too much\nin common: Shifting of embeddings in transformer\nlanguage models and its implications,” in Proceedings\nof the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies . Online: Association\nfor Computational Linguistics, Jun. 2021, pp. 5117–\n5130. [Online]. Available: https://aclanthology.org/\n2021.naacl-main.403\n[36] Y . Liang, R. Cao, J. Zheng, J. Ren, and L. Gao,\n“Learning to remove: Towards isotropic pre-trained\nbert embedding,” in Artiﬁcial Neural Networks and\nMachine Learning – ICANN 2021: 30th International\nConference on Artiﬁcial Neural Networks, Bratislava,\nSlovakia, September 14–17, 2021, Proceedings, Part\nV. Berlin, Heidelberg: Springer-Verlag, 2021, p.\n448–459. [Online]. Available: https://doi.org/10.1007/\n978-3-030-86383-8_36\n[37] J. Bac, E. M. Mirkes, A. N. Gorban, I. Tyukin,\nand A. Zinovyev, “Scikit-dimension: A python\npackage for intrinsic dimension estimation,” Entropy ,\nvol. 23, no. 10, 2021. [Online]. Available: https:\n//www.mdpi.com/1099-4300/23/10/1368\n[38] K. Fukunaga and D. Olsen, “An algorithm for ﬁnding\nintrinsic dimensionality of data,” IEEE Transactions\non Computers , vol. C-20, no. 2, pp. 176–183,\n1971. [Online]. Available: https://ieeexplore.ieee.org/\nabstract/document/1671801\n[39] L. Amsaleg, O. Chelly, T. Furon, S. Girard, M. E.\nHoule, K.-I. Kawarabayashi, and M. Nett, “Extreme-\nvalue-theoretic estimation of local intrinsic dimension-\nality,” Data Mining and Knowledge Discovery , vol. 32,\nno. 6, pp. 1768–1805, 11 2018. [Online]. Available:\nhttps://hal.archives-ouvertes.fr/hal-01864580\n[40] E. Facco, M. d’Errico, A. Rodriguez, and A. Laio, “Es-\ntimating the intrinsic dimension of datasets by a min-\nimal neighborhood information,” Scientiﬁc Reports ,\nvol. 7, no. 1, p. 12140, 9 2017. [Online]. Available:\nhttps://doi.org/10.1038/s41598-017-11873-y\n[41] L. Albergante, J. Bac, and A. Zinovyev, “Estimating\nthe effective dimension of large biological datasets\nusing ﬁsher separability analysis,” in International\nJoint Conference on Neural Networks (IJCNN) , 7\n2019, pp. 1–8. [Online]. Available: https://arxiv.org/\nabs/1901.06328[42] C. Hawthorne, I. Simon, R. Swavely, E. Manilow, and\nJ. H. Engel, “Sequence-to-sequence piano transcription\nwith transformers,” in Proceedings of the 22nd\nInternational Society for Music Information Retrieval\nConference, ISMIR 2021, Online, November 7-\n12, 2021 , 2021, pp. 246–253. [Online]. Available:\nhttps://archives.ismir.net/ismir2021/paper/000030.pdf\n[43] J. P. Gardner, I. Simon, E. Manilow, C. Hawthorne,\nand J. Engel, “MT3: Multi-task multitrack mu-\nsic transcription,” in International Conference on\nLearning Representations , 2022. [Online]. Available:\nhttps://openreview.net/forum?id=iMSjopcOn0pProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n97"
    },
    {
        "title": "VampNet: Music Generation via Masked Acoustic Token Modeling.",
        "author": [
            "Hugo Flores García",
            "Prem Seetharaman",
            "Rithesh Kumar",
            "Bryan Pardo"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265299",
        "url": "https://doi.org/10.5281/zenodo.10265299",
        "ee": "https://zenodo.org/records/10265299/files/000042.pdf",
        "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. \nWe use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
        "zenodo_id": 10265299,
        "dblp_key": "conf/ismir/GarciaSKP23",
        "keywords": [
            "VampNet",
            "masked acoustic token modeling",
            "music synthesis",
            "compression",
            "inpainting",
            "variation",
            "non-autoregressive",
            "bidirectional transformer",
            "coherent high-fidelity",
            "flexible prompting capability"
        ],
        "content": "V AMPNET: MUSIC GENERATION VIA\nMASKED ACOUSTIC TOKEN MODELING\nHugo Flores García1,2Prem Seetharaman1Rithesh Kumar1Bryan Pardo2\n1Descript Inc.\n2Northwestern University\nhugofg@u.northwestern.edu\nABSTRACT\nWe introduce VampNet, a masked acoustic token mod-\neling approach to music synthesis, compression, inpaint-\ning, and variation. We use a variable masking schedule\nduring training which allows us to sample coherent mu-\nsic from the model by applying a variety of masking ap-\nproaches (called prompts) during inference. VampNet is\nnon-autoregressive, leveraging a bidirectional transformer\narchitecture that attends to all tokens in a forward pass.\nWith just 36 sampling passes, VampNet can generate co-\nherent high-ﬁdelity musical waveforms. We show that by\nprompting VampNet in various ways, we can apply it to\ntasks like music compression, inpainting, outpainting, con-\ntinuation, and looping with variation (vamping). Appropri-\nately prompted, VampNet is capable of maintaining style,\ngenre, instrumentation, and other high-level aspects of the\nmusic. This ﬂexible prompting capability makes VampNet\na powerful music co-creation tool. Code3and audio sam-\nples4are available online.\n1. INTRODUCTION\nIn recent years, advances in discrete acoustic token mod-\neling have resulted in signiﬁcant leaps in autoregressive\ngeneration of speech [1, 2] and music [3]. Meanwhile, ap-\nproaches that use non-autoregressive parallel iterative de-\ncoding have been developed for efﬁcient image synthe-\nsis [4, 5]. Parallel iterative decoding promises to allow\nfaster inference than autoregressive methods and is more\nsuited to tasks like inﬁll, which require conditioning on\nboth past and future sequence elements.\nIn this work, we combine parallel iterative decoding\nwith acoustic token modeling, and apply them to music\naudio synthesis. To the best of our knowledge, ours is the\nﬁrst1extension of parallel iterative decoding to neural au-\ndio music generation. Our model, called VampNet, can be\n1While our work was under peer review, Google released SoundStorm\n[6], which leverages a similar parallel iterative decoding approach to ours.\n© H. Flores García, P. Seetharaman, R. Kumar, and B.\nPardo. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: H. Flores García, P. Seetharaman,\nR. Kumar, and B. Pardo, “VampNet: Music Generation via Masked\nAcoustic Token Modeling”, in Proc. of the 24th Int. Society for Music\nInformation Retrieval Conf., Milan, Italy, 2023.\nFigure 1 . VampNet overview. We ﬁrst convert audio into\na sequence of discrete tokens using an audio tokenizer. To-\nkens are masked, and then passed to a masked generative\nmodel, which predicts values for masked tokens via an efﬁ-\ncient iterative parallel decoding sampling procedure at two\nlevels. We then decode the result back to audio.\nﬂexibly applied to a variety of applications via token-based\nprompting. We show that we can guide VampNet’s gener-\nation with selectively masked music token sequences, ask-\ning it to ﬁll in the blanks. The outputs of this procedure can\nrange from a high-quality audio compression technique to\nvariations on the original input music that match the orig-\ninal input music in terms of style, genre, beat and instru-\nmentation, while varying speciﬁcs of timbre and rhythm.\nUnlike auto-regressive music models [2, 3], which can\nonly perform music continuations – using some preﬁx au-\ndio as a prompt, and having the model generate music that\ncould plausibly come after it – our approach allows the\nprompts to be placed anywhere. We explore a variety of\nprompt designs, including periodic, compression, and mu-\nsically informed ones (e.g. masking on the beat). We ﬁnd\nthat our model responds well to prompts to make loops and\nvariations, thus the name VampNet2. We make our code\nopen source3and highly encourage readers to listen to our\naudio samples4.\n2To vamp is to repeat a short passage of music with variation.\n3https://github.com/hugofloresgarcia/vampnet\n4audio samples: https://tinyurl.com/bdfj7rdx3592. BACKGROUND\nTwo-stage approaches to generative modeling have gained\ntraction in image [4, 5, 7, 8] and audio [2, 3, 6, 9] synthe-\nsis, largely in part due to their computational efﬁciency. In\nthe ﬁrst stage, a discrete vocabulary of “tokens” is learned\nfor the domain of interest. The input is put through an en-\ncoder to obtain these tokens, which can be converted back\ninto the input domain via a corresponding decoder. In the\nsecond stage, a model is trained to generate tokens, and is\noptionally given some conditioning (e.g. previous tokens,\na text description, a class label) to guide generation.\n2.1 Stage 1: Tokenization\nIn images, visual tokenization has been leveraged for state-\nof-the-art classiﬁcation [10] and synthesis [4,7,8,11]. The\nmost popular approach is to use vector quantization on a la-\ntent space. Similar approaches have been explored for au-\ndio [12], but until recently such approaches have been re-\nstricted to low sampling rates (e.g. 16khz), or have been re-\nstricted to speech audio. The “sampling rate” of the latent\nspace (the number of latent vectors required every second\nto represent audio) is a critical aspect of the tokenization\nscheme. The lower the sampling rate of the latent space,\nthe easier the next stage (generation) will be to accom-\nplish. Recently, methods based on residual vector quan-\ntization [13,14] have been proposed for audio tokenization\nat high compression rates with good reconstruction quality\nof high-sample-rate audio.\nThe primary work we leverage for audio tokenization is\nthe Descript Audio Codec (DAC) [15]. With DAC, audio is\nencoded into a sequence of tokens via a fully convolutional\nencoder. The output of this encoder is then quantized us-\ning a hierarchical sequence of vector-quantizers [11]. Each\nquantizer operates on the residual error of the quantizer be-\nfore it. Because of this residual vector quantization, DAC\nis able to reconstruct audio with very high quality, at a high\ncompression ratio. It, along with its predecessors [13, 14],\nare instrumental in enabling audio language models like\nAudioLM [2], MusicLM [3], and V ALL-E [1]. While we\nlater brieﬂy describe our tokenizer, the key contributions\nof our work are applicable to the output of any audio tok-\nenizer and our speciﬁc audio tokenizer is not the focus of\nthis work.\n2.2 Stage 2: Generation\nGiven audio encoded as tokens, the common approach is to\nuse an autoregressive model [16] for generation. State-of-\nthe-art (SOTA) audio generation approaches like AudioLM\n[2], MusicLM [3], and JukeBox [17] use this approach,\ngenerating each acoustic token in the sequence in a step-\nby-step fashion using transformer-based [18] decoder-only\nmodels. Autoregressive sampling is slow in nature due to\nthe high number of steps required at inference time [4].\nFurther, autoregressive models inherently restrict down-\nstream applications, as each generated token is only condi-\ntioned on the previous tokens. For an autoregressive modelto perform tasks like inpainting (“ﬁlling in the middle”),\none must re-arrange the data during training [19].\nIn language, masked modeling has been used exten-\nsively as a pre-training procedure for high-quality seman-\ntic representations [20]. This procedure has also been ex-\ntended for representation learning in images [21] and au-\ndio [22]. Masked modeling for representation learning\ngenerally has a constant mask probability. For example,\nin BERT [20], tokens are masked 15% of the time during\ntraining. It has been shown that this approach is equiva-\nlent to a single-step discrete diffusion model [23], that uses\nmasking for its noising procedure. Therefore, we can ex-\ntend masked modeling to masked generative modeling by\nvarying the probability of masking a token during training.\nThis was done for image generation in MaskGIT [4], and\nin language [23]. Similar to diffusion modeling [24, 25],\nwhich seeks to synthesize data starting from random noise\nthrough a series of denoising steps, masked generative\nmodeling seeks to synthesize data starting from completely\nmasked data through a series of “unmasking” steps.\nKey to the efﬁciency of MaskGIT and related ap-\nproaches is a parallel iterative decoding procedure . In par-\nallel iterative decoding, the model predicts every token in\nthe output sequence in a single forward pass. However,\nafter just one forward pass of the model, the output often\ndoes not have high quality. The output of the ﬁrst sam-\npling step is re-masked, with a lower masking probability,\nand then put through the model again. In this way, masked\ngenerative models can efﬁciently reﬁne their output, result-\ning in high quality generation.\nIn unconditional generation tasks, the model is asked\nto generate a realistic sample from the target data distribu-\ntion from scratch, without any guidance. This is a difﬁcult\nproblem, as many target data distributions are highly multi-\nmodal. Unconditional generative models are susceptible to\nmode collapse [26], blurry samples, mode averaging, and\nother issues [27]. Therefore, some conditioning is helpful\nas it provides some signal for the model to resolve the mul-\ntimodality. Conditioning is also a commonly used method\nto guide the output of the system towards desired content.\nConditioning can take the form of a class label, a genre\ntag or lyrics [17], or an associated text description [3,8,28].\nConditioning can also be applied at every timestep, like\nthe semantic tokens of AudioLM [2], or aligned text or\nphonemes for text-to-speech generation [1].\nIn this work,we adopt a masked generative modeling\napproach with a parallel iterative decoding procedure, in-\nspired by work in vision such as MaskGIT [4] and Paella\n[5], as illustrated in Figure 1. We do not apply any con-\nditioning beyond that provided by the unmasked tokens in\nour encoded audio. As we show later, different approaches\nto masking, applied at inference time, can be used to steer\ngeneration in useful and artistic ways.\nIn training, tokens are masked randomly throughout the\nsequence. The model is then asked to predict the value of\neach of the masked tokens in a single forward pass, but it\nis conditioned on all of the unmasked tokens, both in the\nfuture as well as in the past. We vary the number of tokensProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n360Figure 2 . Training, sampling, and prompting VampNet. Training : we train VampNet using Masked Acoustic Token\nModeling, where we randomly mask a portion of a set of input acoustic tokens and learn to predict the masked set of\ntokens, using a variable masking schedule. Coarse model training masks coarse tokens. Coarse-to-ﬁne training only masks\nﬁne tokens. Sampling : we sample new sequences of acoustic tokens from VampNet using parallel iterative decoding,\nwhere we sample a subset of the most conﬁdent predicted tokens each iteration. Prompting : VampNet can be prompted in\na number of ways to generate music. For example, it can be prompted periodically, where every Pth timestep in an input\nsequence is unmasked, or in a beat-driven fashion, where the timesteps around beat markings in a song are unmasked.\nthat are masked during training, allowing us to generate\naudio at inference time through a sampling procedure. We\nnow describe our method in more detail.\n3. METHOD\nWe adapt the procedure of Masked Visual Token Modeling ,\nproposed in MaskGIT [4] to audio, accounting for several\nkey differences between the vision and audio domain. We\ncall our approach Masked Acoustic Token Modeling .\n3.1 Masked Acoustic Token Modeling\nWe ﬁrst train an audio tokenizer based on the techniques\ndescribed in DAC [15]. Unlike the visual tokens of\nMaskGIT, our acoustic tokens are hierarchical in nature\ndue to residual vector quantization. As a ﬁrst step, the au-\ndio signal xis encoded at each time step tas a aDdi-\nmensional latent vector Z. We then quantize ZusingN\nvector quantizers. Quantizer 1 produces ˆZ1, a quantized\napproximation of Zthat has residual error R1=Z−ˆZ1.\nThereafter, the residual from each quantizer iis passed to\nthe next quantizer i+ 1, which produces a quantized ap-\nproximation of the remaining residual error: Ri≈ˆZi+1.\nVectorZis reconstructed by summing the output of the N\nquantizers: Z=/summationtextN\ni=1ˆZi.\nSince the encoded signal is represented as a quantized\nvector of Ndiscrete tokens at each timestep, we have N\ntokens that can be masked or unmasked at each timestep.\nRather than attempt to generate all tokens at once, we in-\nstead split the Ntokens into Nc“coarse” tokens, and Nf\n“ﬁne” tokens, as in AudioLM. We then train two generative\nmodels: one that generates the ﬁne tokens given the coarse\ntokens as conditioning, and one that generates the coarse\ntokens given a sequence of coarse tokens. To generate asample (Figure 1), we chain the two models together. First,\nwe apply the coarse model to generate a sequence of coarse\ntokens. Then, we apply the coarse-to-ﬁne model to gener-\nate the ﬁne tokens. We decode the tokens to a 44.1khz\nwaveform using the decoder of our audio tokenizer.\n3.2 Training procedure\nLetY∈RT×Nbe a matrix representing the output of the\nencoder for some audio segment. Each element yt,ninY\nis a token from the nth level codebook at timestep t. Let\nYMbe the set of all masked tokens in YandYUbe the\nset of all unmasked tokens in Y. The model generates a\nprobability distribution over the set of possible codebook\nvalues for each token y∈YM, given the unmasked tokens\nand the model parameters θ. The training objective is to\nmaximize the probability of the true tokens. This corre-\nsponds to minimizing the negative log likelihood.\nL=−/summationdisplay\n∀y∈YMlogp(y|YU,θ) (1)\nTo predict the masked tokens, we use a multi-layer bidi-\nrectional transformer, which predicts the probabilities of\neach possible token at every timestep, for every quantizer.\nIf each quantizer has a codebook size of Cpossible values,\nand there are Nquantizers, then the last layer of the net-\nwork will be a fully connected layer of shape (E,CN),\nwhereEis the dimensionality of the output of the last\nlayer. We then reshape this output into (EN,C), and com-\npute the cross-entropy loss between the ground-truth one-\nhot token and the predicted token. Because the transformer\nis bidirectional, it can attend to all tokens in the input se-\nquence to optimize the loss for each token.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n361For the coarse-to-ﬁne generative model, the input se-\nquence always contains Nccoarse tokens, and the masking\noperation is restricted to the Nfﬁne tokens. The last layer\nof this network only predicts masked ﬁne tokens. Other-\nwise, the training procedure for both models is identical.\n3.3 Sampling\nWe follow the same iterative conﬁdence-based sampling\napproach used in MaskGIT. More concretely, given YMas\nthe set of masked tokens and YUas the set of unmasked\ntokens, do:\n1.Estimate. For each masked token yinYM, estimate\nthe conditional probability distribution over its vo-\ncabulary of codebook values V.\n2.Sample. For each masked token, sample from the\ndistribution to generate an associated token estimate\nˆy∈V. We don’t use any sampling tricks in this\nstep, sampling from the categorical probability dis-\ntribution for each token as-is.\n3.Rank by Conﬁdence. Compute a conﬁdence mea-\nsure for each of the sampled tokens by taking their\nprediction log-probabilities and adding temperature-\nannealed Gumbel noise to them:\nconfidence (ˆyt) =log(p(ˆyt))+temp·gt(2)\nwhereˆytis a token estimate at timestep t,gtis\nan i.i.d sample drawn from Gumbel(0,1) [29], and\ntemp is a hyperparameter that is linearly annealed to\n0 over the number of sampling iterations. Then, sort\nthe set of sampled token estimates by the conﬁdence\ncomputed above. We ﬁnd that high temperature val-\nues (e.g.>6.0) result in higher quality samples.\n4.Select. Pick the number of tokens to mask at the\nnext sampling iteration, k, according to the mask-\ning schedule5. Take the klowest conﬁdence es-\ntimates and toss them out, re-masking their tokens.\nPlace the remaining high-conﬁdence token estimates\ninYU, removing their tokens from YM.\n5.Repeat Return to step 1 until the number of itera-\ntions has been reached.\n3.4 Prompting\nInteractive music editing can be enabled by incorporating\nhuman guidance in the sampling procedure through the\nconditioning prompt of unmasked tokens. Because our ap-\nproach isn’t conditioned on any signal other than the in-\nput audio itself, we ﬁnd that various types of prompts are\nuseful for obtaining coherent samples, as they lower the\namount of multimodality when sampling from the model.\nLike AudioLM, we can prompt our model with preﬁx au-\ndio of some duration (usually between 1 and 4 seconds),\nand it will provide a continuation of that audio. Unlike Au-\ndioLM, and other auto-regressive approaches, we can also\nprompt our model with sufﬁx audio, and it will generate\n5k=γ(t\ntT)D, wheretis the current iteration, tTis the total number\nof iterations, and Dthe total number of tokens in the sequence. The\nscheduling function γis a cosine schedule.audio that leads up into that sufﬁx. We can provide preﬁx\nand sufﬁx audio, and the model will generate the remaining\naudio, such that it is appropriate, giventhe speciﬁed preﬁx\nand sufﬁx.\nWe can also apply a “periodic” prompt, where all but\neveryPth timestep are masked.The lower Pis, the more\nthe generated audio will sound like the original, as the\nmodel is highly conditioned. For example if P= 2, then\nthe model is essentially behaving like a upsampler, imput-\ning the tokens for every other timestep. As Pincreases,\nthe model shifts from behaving in a compression mode to\nagenerative mode, creating variations that match the style\nof the original.\nAnother useful style of prompt are “compression”\nprompts, where all codebooks other than the most coarse-\ngrained are masked. This gives the model strong condi-\ntioning on every timestep, so the model is likely to produce\naudio that closely matches the original. We can combine\nthis prompt with a periodic prompt with low Pfor even\nmore extreme compression ratios. Given the bitrate of the\ncodecB, which has number of codebooks N, a downsam-\npling rate Pfor the periodic prompt, and a number of kept\ncodebooks Nk, we can achieve a bitrate of B/P(N−Nk).\nFinally, we can design music-speciﬁc prompts, which\nexploit knowledge about the structure of the music. More\nconcretely, we explore beat-driven prompting, where\ntimesteps that fall on or around the beat are left unmasked.\nThe model is left to create music between these beats,\nresulting in interesting variations on the original music.\nThese prompts can all be combined to create a very use-\nful music creation tool. In concert with a well designed\nuser interface, VampNet shows promise as the basis for a\nnext-generation music editing and creation suite.\n4. EXPERIMENTS\nOur experiments aim to evaluate VampNet’s capability\nto both compress and generate music, given the various\nprompting strategies described in Section 3.4. For our ob-\njective audio quality measures, we use a multiscale mel re-\nconstruction error and the Fréchet Audio Distance (FAD).\nMel-reconstruction error is deﬁned as the L1distance be-\ntween log-mel spectrograms at various time-scales,\nDF,M=||ˆSF,M−SF,M||1 (3)\nwhereFis the FFT size of each spectrogram, and\nMis the number of mel-frequency bins. We use F∈\n[2048,512] andM∈[150,80], with a hop size of1\n4the\nFFT size. Mel-reconstruction is valuable as a metric for\ncompression quality, but not for generation quality, since\nit is likely that models produce audio that does not match\none to one with the original target audio. For generation\nquality, we use FAD, which measures the overlap between\ndistributions of real and generated audio. Unlike mel-\nreconstruction, FAD is geared more towards evaluating if\nsample quality falls within the data distribution of the real\naudio, and can be used to evaluate generation quality.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n362Figure 3 . Mel reconstruction error (top) and Fréchet Au-\ndio Distance (FAD, bottom) for VampNet samples taken\nwith varying numbers of sampling steps, taken using a pe-\nriodic prompt of P= 16 . The samples were generated\nby de-compressing tokens at an extremely low bitrate (50\nbps), effectively generating variations of the input signals.\n4.1 Dataset\nSimilar to JukeBox [17], we collect a large dataset of pop-\nular music recordings. Our dataset consists of 797k tracks,\nwith a sampling rate of 32 khz. These tracks are resam-\npled to 44.1kHz to make compatible with our tokenizer.\nOur dataset contains music from thousands of artists across\ngenres described in Echo Nest’s Every Noise at Once6.\nWe use a subset of 2k tracks for validation, and another\nsubset of 2k tracks for testing. We ensure that there is no\nartist overlap between train, validation, and test tracks. In\naddition, we collect a set of music and non-music data\n(speech, environmental sound), which we used to train\nour tokenizer, using the datasets described in DAC [15].\nAll audio is normalized to -24dbFS. We do not use any\nmetadata about these ﬁles during training, as our model is\ntrained unconditionally.\n4.2 Network Architecture and Hyperparameters\nThe audio tokenizer model we use takes as input 44.1kHz\naudio, and compresses it to a bitrate of 8kbps using 14\ncodebooks, with a downsampling rate of 768x. The latent\nspace therefore is at 57Hz, with 14 tokens to predict at ev-\nery timestep. We designate 4 of these tokens as the coarse\ntokens, and the remaining 10 as the ﬁne tokens. Refer to\nthe Descript Audio Codec [15] for details on the tokenizer\narchitecture. We train the tokenizer for 250k steps.\nThe VampNet architecture (for both coarse and coarse-\nto-ﬁne models) consists of a bidirectional transformer [18]\nwith relative attention [30] and an embedding dimension\nof 1280 and 20 attention heads. The coarse model has 20\nattention layers, while the coarse-to-ﬁne model has 16. We\ntrain the coarse and coarse-to-ﬁne model for 1M and 500k\nsteps, respectively. We train with the AdamW optimizer\n[31] with β1andβ2set to 0.9 and 0.999, respectively. We\n6https://everynoise.com/engenremap.html\nFigure 4 . Multiscale Mel-spectrogram error (top) and\nFréchet Audio Distance (FAD, bottom) for VampNet 10s\nsamples taken with a different types of prompts.\nuse the learning rate scheduler introduced by Vaswani et\nal [18] with a target learning rate of 0.001 and 10k warmup\nsteps. We use a dropout of 0.1, and a batch size of 25, with\na GPU memory budget of 72GB.\n4.3 Efﬁciency of VampNet\nWe ﬁrst validate that VampNet can generate realistic music\naudio in a low number of steps. To do this, we run Vamp-\nNet using one of our prompts (the periodic prompt, with\nP= 16 ) on our test set, on 10-second excerpts. We vary\nthe number of sampling steps in [1,4,8,12,36,64,72], and\nreport metrics for each sampling step.\n4.4 Effect of prompts\nWe seek to understand how VampNet responds to different\nprompts, as discussed in Section 3.4. The prompts range\nfrom “compression” prompts, which compress music to a\nlow bitrate, to more creative “generative” prompts. We ex-\namine whether compression and generative prompts exist\non a continuum, and whether decompression from low bi-\ntrates results in generative behavior.\nWe draw 2000 10-second examples from our evaluation\ndataset, encode them into token streams with our audio to-\nkenizer, and manipulate the token streams in four ways:\n1. Compression prompt: Ccodebooks are left un-\nmasked, starting from the coarsest codebook. All\nother tokens are masked. We set Nk= 1.\n2. Periodic prompt: every Pth timestep is left un-\nmasked. In an unmasked timestep, tokens from ev-\nery codebook are unmasked. All other tokens (e.g.\ntokens in timesteps that do not correspond to the pe-\nriodP) are masked. We set P∈[8,16,32].\n3. Preﬁx and sufﬁx (inpaint) prompts: a segment at the\nbeginning and at the end of the sequence is left un-\nmasked. All other tokens are masked. This prompt\nis parameterized by a context length in seconds. We\nset the context to be either 1 second or 2 seconds,\nwhich corresponds to 57 or 114 timesteps.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3634. Beat-driven prompt: we ﬁrst process the audio wave-\nform with a beat tracker [32]. Then, around each de-\ntected beat, we unmask timesteps to the right of the\nbeat. We examine a 75ms unmasked section around\neach beat, which is about 4 timesteps per beat.\nAfter manipulating the input token streams with our\nprompts, we generate new musical signals from these\nmasked token streams using VampNet, and compute FAD\nand mel-reconstruction error between the generated signals\nand the input signals from our music dataset. We include\na noisy token stream baseline, where a portion (as dictated\nby mask ratio r) of the tokens in the input token stream are\nreplaced with random tokens. We also include as baseline\nthe codec by itself, as well as the coarse-to-ﬁne model.\nFinally, we examine how these prompts can be com-\nbined - speciﬁcally the compression and periodic prompts.\nBy manipulating the hyperparameters of these prompts ( C\nandP), we can shift the model behavior from compression\nto generation. As more timesteps are masked, the model\nmust generate plausible musical excerpts that connect the\nunmasked timesteps, that may not match the input music.\n5. RESULTS AND DISCUSSION\nResults for our experiment varying the number of sam-\npling steps used to generate samples with VampNet are\nshown on Figure 3. We ﬁnd that VampNet achieves the\nlowest FAD with 36 sampling steps, although 12 sampling\nsteps achieves comparable performance. In practice, we\nﬁnd that samples taken with 24 steps achieve a fair trade-\noff between generation quality and compute speed, with\n10-second samples taking around 6 seconds to sample on\nan NVIDIA RTX3090. In contrast, to generate 10 seconds\nof audio with an autoregressive model would require 574\nsteps, which would take around 1 min to generate 10 sec-\nonds of audio, given an autoregressive model with the same\nnumber of parameters as ours, and the same tokenizer.\nResults for our study on the effect of each prompt are\nshown in Figure 4. First, we note that while the noisy token\nbaseline has comparable mel reconstruction to all prompts,\nit performs very poorly in terms of FAD. This indicates that\nwhile our prompting strategies may result in audio that is\nnot a perfect match to the original input audio, it still falls\ninside the distribution of plausible music.\nOf our proposed prompts, we ﬁnd that beat-driven\nprompts perform best, achieving the lowest FAD of all\nprompts. A notable result here is that the periodic prompt\nwithP= 16 (35 conditioning timesteps) performs on par\nwith inpainting with 1 second of context (57 conditioning\ntimesteps). Therefore, prompt techniques that spread out\nthe conditioning tokens throughout the sequence (periodic\nprompts) are able to use fewer conditioning timesteps to\ngenerate samples of comparable quality to those generated\nby sampling techniques that place all of the conditioning\ntokens at the start and end of the sequences (inpainting).\nQualitatively, we also ﬁnd that beat-driven prompts can\nkeep a steadier tempo than other prompts, though their out-\nputs tend to resemble the original music closer than peri-\nFigure 5 . Mel-spectrogram error (top) and Fréchet Audio\nDistance (FAD) (bottom) for VampNet samples at varying\nbitrates. A baseline is provided by replacing tokens in the\ninput sequence with random tokens, per noise ratio r.\nodic prompts. In practice, a mix of beat-driven, periodic,\nand inpainting prompts can be employed to steer of Vamp-\nNet in creative ways. To illustrate, we highly encourage\nthe reader to listen to the accompanying sound samples7.\nWe then combined periodic and compression prompting\nto show how the model’s behavior shifts between recon-\nstruction and generation tasks, as more tokens are masked\naway. Results for this experiment are shown in Figure 5.\nAt higher bitrates, (600 bps and above), VampNet is able\nto accurately reconstruct the original music signal, achiev-\ning low mel-spectrogram error and FAD values with re-\nspect to the evaluation music audio. At bitrates of 200bps\nand below, VampNet has comparable reconstruction qual-\nity to the noisy token baselines, indicating that the sam-\npled VampNet signals no longer resemble the input audio\nin terms of ﬁne-grained spectral structure. However, the\nFAD for VampNet samples at low bitrates is much lower\nthan the FAD for noisy baselines. This indicates that even\nthough VampNet isn’t able to reconstruct the input music\nsignal at low bitrates, it is still able to generate coherent\naudio signals with musical structure, that are closer to the\ndistribution of “real music” than our noisy baseline.\n6. CONCLUSION\nWe introduced VampNet, a masked acoustic token mod-\neling approach to music generation. VampNet is bidirec-\ntional, and can be prompted a variety of ways using an\ninput audio ﬁle. Through different prompting techniques,\nVampNet can operate in a continuum between music com-\npression and generation, and is an excellent tool for gener-\nating variations on a piece of music. With VampNet, a mu-\nsician could record a short loop, feed it into VampNet, and\nhave VampNet create musical variations on the recorded\nidea every time the looped region repeats. In future work,\nwe hope to investigate the interactive music co-creation po-\ntential of VampNet and its prompting techniques, as well as\nexplore the representation learning capabilities of masked\nacoustic token modeling.\n7audio samples: https://tinyurl.com/bdfj7rdxProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3647. REFERENCES\n[1] C. Wang, S. Chen, Y . Wu, Z. Zhang, L. Zhou, S. Liu,\nZ. Chen, Y . Liu, H. Wang, J. Li et al. , “Neural codec\nlanguage models are zero-shot text to speech synthe-\nsizers,” arXiv preprint arXiv:2301.02111 , 2023.\n[2] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov,\nO. Pietquin, M. Shariﬁ, O. Teboul, D. Grangier,\nM. Tagliasacchi, and N. Zeghidour, “Audiolm: a lan-\nguage modeling approach to audio generation,” arXiv\npreprint arXiv:2209.03143 , 2022.\n[3] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel,\nM. Verzetti, A. Caillon, Q. Huang, A. Jansen,\nA. Roberts, M. Tagliasacchi et al. , “Musiclm:\nGenerating music from text,” arXiv preprint\narXiv:2301.11325 , 2023.\n[4] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T.\nFreeman, “Maskgit: Masked generative image trans-\nformer,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2022, pp.\n11 315–11 325.\n[5] D. Rampas, P. Pernias, E. Zhong, and M. Aubre-\nville, “Fast text-conditional discrete denoising on\nvector-quantized latent spaces,” arXiv preprint\narXiv:2211.07292 , 2022.\n[6] Z. Borsos, M. Shariﬁ, D. Vincent, E. Kharitonov,\nN. Zeghidour, and M. Tagliasacchi, “Soundstorm: Ef-\nﬁcient parallel audio generation,” 2023.\n[7] P. Esser, R. Rombach, and B. Ommer, “Taming trans-\nformers for high-resolution image synthesis,” in Pro-\nceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition , 2021, pp. 12 873–12 883.\n[8] R. Rombach, A. Blattmann, D. Lorenz, P. Esser,\nand B. Ommer, “High-resolution image synthesis\nwith latent diffusion models,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition. , 2022.\n[9] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Syn-\nnaeve, Y . Adi, and A. Défossez, “Simple and control-\nlable music generation,” 2023.\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly et al. , “An image is\nworth 16x16 words: Transformers for image recogni-\ntion at scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[11] A. Van Den Oord, O. Vinyals et al. , “Neural discrete\nrepresentation learning,” Advances in neural informa-\ntion processing systems , vol. 30, 2017.\n[12] C. Gârbacea, A. van den Oord, Y . Li, F. S. Lim,\nA. Luebs, O. Vinyals, and T. C. Walters, “Low bit-rate\nspeech coding with vq-vae and a wavenet decoder,”\ninICASSP 2019-2019 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2019, pp. 735–739.\n[13] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and\nM. Tagliasacchi, “Soundstream: An end-to-end neu-\nral audio codec,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , vol. 30, pp. 495–\n507, 2021.\n[14] A. Défossez, J. Copet, G. Synnaeve, and Y . Adi, “High\nﬁdelity neural audio compression,” arXiv preprint\narXiv:2210.13438 , 2022.\n[15] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and\nK. Kumar, “High-ﬁdelity audio compression with im-\nproved rvqgan,” 2023.\n[16] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever\net al. , “Improving language understanding by genera-\ntive pre-training,” 2018.\n[17] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,\nand I. Sutskever, “Jukebox: A generative model for\nmusic,” arXiv preprint arXiv:2005.00341 , 2020.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in neural infor-\nmation processing systems , vol. 30, 2017.\n[19] M. Bavarian, H. Jun, N. Tezak, J. Schulman,\nC. McLeavey, J. Tworek, and M. Chen, “Efﬁcient train-\ning of language models to ﬁll in the middle,” arXiv\npreprint arXiv:2207.14255 , 2022.\n[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n“Bert: Pre-training of deep bidirectional trans-\nformers for language understanding,” arXiv preprint\narXiv:1810.04805 , 2018.\n[21] K. He, X. Chen, S. Xie, Y . Li, P. Dollár, and R. Gir-\nshick, “Masked autoencoders are scalable vision learn-\ners,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022, pp.\n16 000–16 009.\n[22] Y .-A. Chung, Y . Zhang, W. Han, C.-C. Chiu, J. Qin,\nR. Pang, and Y . Wu, “W2v-bert: Combining con-\ntrastive learning and masked language modeling for\nself-supervised speech pre-training,” in 2021 IEEE Au-\ntomatic Speech Recognition and Understanding Work-\nshop (ASRU) . IEEE, 2021, pp. 244–250.\n[23] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and\nR. van den Berg, “Structured denoising diffusion mod-\nels in discrete state-spaces,” Advances in Neural In-\nformation Processing Systems , vol. 34, pp. 17 981–\n17 993, 2021.\n[24] Y . Song and S. Ermon, “Generative modeling by esti-\nmating gradients of the data distribution,” Advances in\nneural information processing systems , vol. 32, 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n365[25] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion\nprobabilistic models,” Advances in Neural Information\nProcessing Systems , vol. 33, pp. 6840–6851, 2020.\n[26] A. Srivastava, L. Valkov, C. Russell, M. U. Gutmann,\nand C. Sutton, “Veegan: Reducing mode collapse in\ngans using implicit variational learning,” Advances in\nneural information processing systems , vol. 30, 2017.\n[27] T. Salimans, I. Goodfellow, W. Zaremba, V . Cheung,\nA. Radford, and X. Chen, “Improved techniques for\ntraining gans,” Advances in neural information pro-\ncessing systems , vol. 29, 2016.\n[28] H. Chang, H. Zhang, J. Barber, A. Maschinot,\nJ. Lezama, L. Jiang, M.-H. Yang, K. Murphy, W. T.\nFreeman, M. Rubinstein et al. , “Muse: Text-to-image\ngeneration via masked generative transformers,” arXiv\npreprint arXiv:2301.00704 , 2023.\n[29] E. J. Gumbel, “Statistical theory of extreme values\nand some practical applications; a series of lectures.”\nWashington, 1954.\n[30] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention\nwith relative position representations,” arXiv preprint\narXiv:1803.02155 , 2018.\n[31] I. Loshchilov and F. Hutter, “Fixing weight decay\nregularization in adam,” CoRR , vol. abs/1711.05101,\n2017. [Online]. Available: http://arxiv.org/abs/1711.\n05101\n[32] C. J. Steinmetz and J. D. Reiss, “WaveBeat: End-to-\nend beat and downbeat tracking in the time domain,”\nin151st AES Convention , 2021.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n366"
    },
    {
        "title": "Chromatic Chords in Theory and Practice.",
        "author": [
            "Mark Gotham"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265275",
        "url": "https://doi.org/10.5281/zenodo.10265275",
        "ee": "https://zenodo.org/records/10265275/files/000031.pdf",
        "abstract": "\"Chromatic harmony\" is seen as a fundamental part of (extended) tonal music in the Western classical tradition (c.1700–1900). It routinely features in core curricula. Yet even in this globalised and data-driven age, 1) there are significant gaps between how different national \"schools\" identify important chords and progressions, label them, and shape the corresponding curricula; 2) even many common terms lack robust definition; and 3) empirical evidence rarely features, even in in discussions about \"typical\", \"representative\" practice. This paper addresses those three considerations by: 1) comparing English- and German-speaking traditions as an example of this divergence; 2) proposing a framework for defining common terms where that is lacking; and 3) surveying the actual usage of these chromatic chord categories using a computational corpus study of human harmonic analyses.",
        "zenodo_id": 10265275,
        "dblp_key": "conf/ismir/Gotham23",
        "keywords": [
            "Chromatic harmony",
            "Western classical tradition",
            "core curricula",
            "national schools",
            "significant gaps",
            "common terms",
            "robust definition",
            "empirical evidence",
            "computational corpus study",
            "human harmonic analyses"
        ],
        "content": "CHROMATIC CHORDS IN THEORY AND PRACTICE\nMark R. H. Gotham\nDurham University\nABSTRACT\n‘Chromatic harmony’ is seen as a fundamental part of\n(extended) tonal music in the Western classical tradition\n(c.1700–1900). It routinely features in core curricula. Yet\neven in this globalised and data-driven age, 1) there are\nsigniﬁcant gaps between how different national ‘schools’\nidentify important chords and progressions, label them,\nand shape the corresponding curricula; 2) even many com-\nmon terms lack robust deﬁnition; and 3) empirical evi-\ndence rarely features, even in discussions about ‘typical’,\n‘representative’ practice. This paper addresses those three\nconsiderations by: 1) comparing English- and German-\nspeaking traditions as an example of this divergence; 2)\nproposing a framework for deﬁning common terms where\nthat is lacking; and 3) surveying the actual usage of these\nchromatic chord categories using a computational corpus\nstudy of human harmonic analyses.\n1. INTRODUCTION\nDifferent traditions for teaching music theory come with\ndivergent terminology. These gaps often correspond to na-\ntional trends (or ‘schools’) and to the different languages\nused. As always with language, these gaps can take several\nforms. Some terms may be shared by the two languages,\nso no translation is needed. Other times, a term is present\nin one language only ; this inclusion may indicate an im-\nportance for the term/concept on one side of this divide\nand not the other. More complex still, two languages may\nhave some terms with partially overlapping meaning.\nThere are signiﬁcant gaps between English- and\nGerman-speaking terminology for chromatic harmony, de-\nspite so much shared historical heritage. Even the dis-\ntinction of ‘chromatic’ from ‘diatonic’ betrays an English-\nlanguage stance. Section §1.1 introduces something of a\nframe for this comparison and §2 discusses three inter-\nesting case-studies of ‘canonical’ terms. The focus is on\nchords that are either intrinsically chromatic (Augmented\nSixths, §2.2), or chromatic against their diatonic context\n(Neapolitan Sixths, §2.1; Modal Mixture, §2.3). We leave\nto one side what is sometimes called ‘functional chromati-\ncism’ (the ‘secondary’/‘applied’ chords involved in tonici-\nsation/modulation – see [1, Part 5]) though the ﬁnal section\n(§5) brieﬂy considers some relevant chord progressions .\n© M. Gotham. Licensed under a Creative Commons Attri-\nbution 4.0 International License (CC BY 4.0). Attribution: M. Gotham,\n“Chromatic Chords in Theory and Practice”, in Proc. of the 24th Int.\nSociety for Music Information Retrieval Conf., Milan, Italy, 2023.E B F♯ C♯ G♯ D♯ A♯\nC G D A E B F♯ C♯\nE♭ B♭ F C G D A\nC♭ G♭ D♭ A♭ E♭ B♭ F C\nFigure 1 . A ‘Tonnetz’ diagram of tonal space. Major and\nminor triads in the key of C major are grey; those in C\n(natural) minor are in blue, and the ‘Neapolitan’ is purple.\nMoreover, a closer look reveals that even some of the\napparently core concepts in chromatic harmony are only\nvaguely deﬁned. For example, although ‘modal mixture’ is\ncommon (at least in US-English music theory), no source\nsets out comprehensive criteria for inclusion in this cate-\ngory. Section §3 addresses this, seeking to establish not so\nmuch a single, deﬁnitive answer, but a framework to deal\nwith the various issues involved.\nFinally, having established the (range) of terms that\nGerman- and English- speaking scholars elevate as impor-\ntant, and clariﬁed the meaning of some, §4 provides an\ninitial overview of the relative usage of these chords in the\n‘When in Rome’ repository: a meta-corpus of all Roman\nnumeral analyses that human annotators have encoded in\ncomputational formats [2]. In doing so, we gain insight\ninto how common these chords are, at least in the reper-\ntoires covered and the view of those human analysts.\nClearly, sheer usage is not the only relevant consider-\nation for the signiﬁcance of a chordal category — many\nsubjects are interesting partly because of their rarity. In\nany case, all such discussions, and any claims about ‘gen-\neral practice’, need a basis in this kind of empirical ev-\nidence. The clarity that evidence brings may prompt a\nreview of our existing practice (how we categorise these\nchords, and/or how much time we devote to them in our\ncurricula and wider musical practice). It may also clarify\nthe extent to which that attention is based on the frequency\nof occurrence as opposed to some other factor, like how\nexplainable the concept is in terms of a particular theory.\n1.1 Textbooks, Terminology, and Tradition\nWe begin with that slippery notion of a ‘tradition’. While\nit is hard to pin down exactly what this means in prac-\ntice,1the contents of widely circulated textbooks provide\n1For more on the question of ‘representativeness’, see [3].272one kind of insight into what is ‘typically taught and com-\nmonly known’ in a given context. Among the issues here\nis the privileging of contexts in which textbooks are com-\nmon (broadly speaking, the US), and lack of sensitivity to\nmore ﬂexibly amassed materials, particularly in a changing\nworld with ever more materials shared ever more accessi-\nbly online.2Then again, many of these online materials\nand apps continue to reﬂect what is described here in terms\nof textbooks. And I implicate myself in this: see, for ex-\nample, the chapter listings and content of the ‘Open Music\nTheory’ (OMT) textbook [5] which (incidentally) serves\nthroughout this paper as a go-to resource for further read-\ning, with links to relevant chapters provided.\n1.1.1 English-language (hereafter ‘Anglophone’)\nOn the Anglophone side we beneﬁt from two surveys of\nthe ‘core curriculum’ in American music theory teach-\ning, including information about the textbooks typically\nused [6, 7]. The more recent of these surveys ﬁnds that\n91.89% (238/259) respondents include ‘Chromatic har-\nmony’ in their core curriculum (see Table IV-1), with 1\nor 2 semesters being the ‘most commonly reported lengths\nof time for teaching’ this content (p.202, Table IV-9), and\nthat 79.92% use textbooks/anthologies (Table IV-10).\nThese surveys also appear to indicate that the preference\nforwhich textbook to use changes quickly,3but that what\nthose textbooks cover remains largely the same: they con-\nsistently cover the same canonical collection including at\nleast the so-called ‘Neapolitan sixths’, ‘Augmented sixths’,\nand ‘Modal Mixture’.4\nClick on those terms above for OMT chapters about\nthem, and click here for a summary of these chords in a\nmusical score that you can view, play and more online (no\nlogin required). That rendering is relatively typical of the\nsimple, purportedly ‘prototypical’ ways these chords are\nset out in textbooks. (Naturally, we will discuss here just\nhow prototypical they really are.)\n1.1.2 German-language (hereafter ‘DACH’)\nAs no equivalent survey existed for the DACH side,5we\nconducted one anew in mid-2022.6Speciﬁcally, we asked\nanyone teaching chromatic harmony at a German-speaking\ntertiary education institution to answer basic questions\nabout the textbooks and terminology they know and use.\nPlease refer to that study for a thorough report on the\nmethod and results of the survey. This paper refers to only\nthe most salient results as relevant for present purposes, as\ndiscussed in the following sections.\n2On the growing adoption of technological alternatives see the ‘What\nDo Musicologists Do All Day’ (WDMDAD) surveys (2014-15, [4] and\n2021-22, forthcoming) which investigate ‘the use of technology in the\nwork of music researchers in the widest sense’ (including teaching).\n3I.e., there is little overlap between the 2000 and 2017 results.\n4Increasingly, many also refer to the common-tone diminished sev-\nenths, (for which see §5) though they often package this more deeply\ne.g., within the ‘Rise of Symmetrical Harmony in Tonal Music’ [1].\n5‘DACH’ is an abbreviation/acronym for Germany, Austria and\nSwitzerland. These are the main areas of German-speaking today and\nwhere all the institutions approached for the survey are situated.\n6The written report is forthcoming (Feilen, Schnauss and Gotham).2. THREE CANONICAL CATEGORIES\n2.1 Similar usage: the ‘Neapolitan sixth’\nThe ‘Neapolitan sixth’ appears routinely in both lan-\nguages. It is interesting to note the status of this chord in\nrelation to the Funktions- andStufentheorie approaches to\nharmony which capture much of the core divide between\nDACH and Anglophone approaches (respectively).\nThe Neapolitan can be seen as a simple, one-semi-tone\nmodiﬁcation to the minor subdominant.7InFunktions-\ntheorie , such small transformations typically indicate close\nharmonic relations, leading to maps of tonal space like\ntheTonnetz of ﬁgure 1 which shows how the Neapolitan\nsits alongside diatonic chords, especially in minor.8(We\nwill return to the minor-speciﬁc aspect in practice in §4).\nStufentheorie , by contrast, typically describes the Neapoli-\ntan in terms of a modiﬁcation of the second degree ( ♭II6).\nThis is clearly relegated to a subsidiary position, a ‘chro-\nmatic’ chord outside the main, ‘diatonic’ set.9\nNotwithstanding the different theoretical frames, the\nNeapolitan presents relative close Anglophone-DACH\nagreement: not only is there agreement on which pitches\nare involved, but both typically relate this chord to the\n‘subdominant’ (both), or more loosely to ‘predominant’\nfunction (Anglophone). Despite the Anglophone notion of\n♭II6, the close relation to ‘iv’ (‘s’) is often emphasised, and\nlikewise it is common in DACH to eschew the possible\nFunktions -only explanation in favour of the symbol snthat\nfurther emphasises the proximity to the subdominant.\nAnglophone and DACH traditions also share most of\nthe deﬁnitional incompleteness, notably terms of whether\nto admit: other inversions (e.g., 53) and other tones (e.g.,\nseventh chords such as 653). DACH theory often does\nadmit the 53 conﬁguration of this chord, and reserves a\nspecial name for it: the verselbständigter . It is notewor-\nthy that, despite being rather sparing in its use of special\nterms for individual chords, DACH considers the Neapoli-\ntan worthy not only of one term, but two.10Both Anglo-\nphone and DACH theories lack an explicit consensus on\nwhether the Neapolitan may have a seventh.\n2.2 Divergent terms: Augmented-sixth chords\nThe Anglophone convention for teaching Augmented-\nsixth chords identiﬁes (at least) three forms that have been\ngiven spurious national labels: the ‘Italian sixth’ (63), the\n‘French’ (643), and the ‘German’ (653). Those labels seem\n7This can be viewed as the Mollsubdominantgegenklang (sG), though\nsee the following text on sn. The -gegenklang transformation is the same\nas the -gegenparallel and better known in Anglophone contexts as the\nleittonwechsel or ‘leading-tone exchange’.\n8Although this common visual analogy for tonal ‘space’ is familiar to\nAnglophone music theory, is much more closely related to the Funktions\nmentality. The earliest, recognisable form seems to be from Euler [8]\n(yes, the mathematician) but the best known exposition of this idea and\n‘space’ is that of Riemann [9] (no, not the mathematician).\n9DACH can also express this chromatic alteration ( hoch- andtief-\nalterierte ), but usually does so a last resort where other theory fails.\n10One of the earliest recorded Anglophone uses of this term treads a\nmiddle line in which the chord is explicitly built on the subdominant scale\ndegree (‘Fa’, i.e., ˆ4) and ‘is never inverted’, apparently meaning that, un-\nusually, this 63 form is not to be considered ‘inverted’ [10].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n273to have originally been proposed (c.1800) based on their\nusage in the repertoire. For example, [11] explicitly links\nthese chords to the music of those nations.11\nLeaving until §4 the question of whether these national\nlabels have anything to do with repertoire usage, there is\nan Anglophone/DACH division in the terms themselves\nwhich may perhaps be telling. DACH emphasises a sin-\ngle category for which the recognised term is übermäßiger\nQuintsextakkord . This explicitly refers to the 653 form —\nthe one that Anglophone theory calls the ‘German’ sixth.\nThis also indicates opposing ways of handling aug-\nmented sixth chord categories: Anglophone traditions not\nonly use 3 categories, but tend to start with the ‘Italian’\n(63) as the prototype (at least in the pedagogical sense)\nand then addtones to build the French (6 43) and German\n(653); DACH, by contrast, starts with the 653 and would\nneed to remove or modify from there.12\nThese differences aside, there is broad Anglophone-\nDACH agreement on the composition of the chords. The\neponymous augmented sixth interval is needed (and spelt\nas such), and there is a strong focus on both the inversion\nthat sees the lower note of that interval in the bass and the\nvoice-leading whereby this interval expands ‘out’ to a per-\nfect octave on the dominant ( ˆ5).\n2.3 Anglophone only: ‘Modal Mixture’\nMost Anglophone textbooks offer a short deﬁnition of\n‘modal mixture’ (a.k.a. ‘borrowing’) as the use of a chord\nthat is not diatonic in the key speciﬁed, but would be in the\nparallel (German: variant ) major / minor and can therefore\nbe thought of as a ‘mixture’ of major and minor modes, or\na ‘borrowing’ from the one to the other. Some coverage of\nthis topic is present in all the Anglophone textbooks sur-\nveyed, usually with a dedicated chapter.\nNo DACH equivalent appears in German text-\nbooks. Equivalents do sporadically appear in DACH\nanalysis scholarship with terms such as Dur-moll-\nAustauschbarkeit , (or simply Austauschbarkeit , literally\n‘exchange’), but this term cannot be assumed knowledge\nin the classroom or beyond.13\nDespite the ubiquity of the term ‘mixture’ in Anglo-\nphone textbooks, it is particularly under-deﬁned and never\nfully unpacked to account for all in-/exclusions. This is\nperhaps understandable in a pedagogical context where the\nincreased clarity must be weighted against the correspond-\ning complexity, but as a ﬁeld, we clearly need a framework\nfor robust deﬁnition. The following, dedicated section §3\nprovides such a framework.\n11Callcott appears to have inherited the term ‘Italian’, noting that it\n‘has been termed’ the Italian. There’s no direct reference, though nearby\nmention of Rousseau suggests that may be at least one of his sources.\nCallcott seems to introduce the other two ‘nationalities’.\n12Click here for a modern, online example of this DACH pattern, and\nsee also Biamonte’s account of this chord, including DACH sources dat-\ning back to Marpurg 1755 [12].\n13Incidentally, it is not self-evident that this ‘mixture’ is indeed a mix-\nture of distinct parts, as opposed to a uniﬁed entity. For instance, another\nschool of thought (historically of German-origin, now more common in\nRussian music theory) sees the major mode with ♭ˆ6as a single ‘harmonic\nmajor’ scale. See [13] for the progress of this idea from Hauptmann, via\nIogansen, Liadov, and Rimsky-Korsakov to modern Russian theory.3. DEFINING MODAL MIXTURE\nIn a major context, the subdominant is also major (‘IV’ or\n‘S’). Probably the most common chord identiﬁed in terms\nof modal mixture is the minor variant of this subdominant\n(‘iv’ or ‘s’). So in C major, for example, we would have\n<F-A♭-C> in place of <F-A-C>.\nBut what if this mixture chord had a seventh, so not sim-\nply <F-A ♭-C>, but <F-A ♭-C-E♭>, or <F-A ♭-C-E>? The\nﬁrst case, <F-A ♭-C-E♭> seems like a very good candi-\ndate: the additional borrowing from the minor of E ♭fur-\nther strengthens the case for mixture. The same can’t be\nsaid for <F-A ♭-C-E> as E belongs to C major exclusively\nand arguably counts against the notion of mixture.14So\nshould cases of clear non-mixture be excluded ?\nIf we admit the <F-A ♭-C-E> as a case of modal mixture,\nthen what do we have to say about the case of <F-A-C-\nE♭>? Is that equivalent? Now the E ♭is borrowed, but the\nA is arguably not depending on the type of minor mode.\nWhat minor form are we talking about when we speak\nof mixture? Some accounts seem to hint at the natural mi-\nnor, but then every raised leading-tone chord (V , V7, viio,\n. . . ) would count as cases of mixture in minor.\nShould the case of <F-A-C-E ♭> depend on whether\nit is cast as IV ♭7 or as V/ ♭VII? That is, should sec-\nondary/applied chords be handled differently as a case\nof ‘functional’ chromaticism or (put another way) as dia-\ntonic elements in a new key area? Does this depend on\nwhether that secondary tonality is realised by a subsequent\ntonicisation or modulation ? This question opens a sec-\nond set of possible criteria: in addition to questions about\nthe chord’s content , we now must also consider its context .\nSpeaking of context, does the so-called ‘Picardy\nThird’ count?15And arguably related to both content\nand context, (and certainly relevant to applied chords) is\nthe question: does pitch spelling matter? Were our <F-\nA-C-E♭> chord spelt as <F-A-C-D ♯>, apart from poten-\ntially leading analysts to describe it differently, should that\nspelling itself have a bearing on the status of mixture? Is\nthe minor third mixed only when spelt as such, or is it to be\nhandled as a pitch class, and thus admitting the enharmonic\nequivalent of a raised second degree ( ♯ˆ2)?\nAltogether, these musical questions capture something\nof the ambiguity in deﬁning modal mixture, and the need\nfor greater clarity in what ‘counts’. They also suggest\nthe need to create a framework for category membership,\nrather than clear-cut rules applicable in all contexts.\nRealising this, functionality at ‘When in Rome’ enables\nuser-deﬁned answers to any of the questions raised above,\nwhile also providing default settings and proposing a sys-\ntem for grading the relative strength of mixture, both in\nterms of the chord content and of the surrounding chord\ncontext .\n14Note we are talking speciﬁcally about how relatively mixed these\nchords are, not how chromatic .\n15This term stands for the practice of ending a minor key passage with a\nmajor tonic as the ﬁnal chord. (Click here for the modal mixture chapter\nof OMT, including an example of the ‘Picardy Third’.) It is extremely\ncommon, at least in some repertoire contexts.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2743.1 Which pitches, which minor?\nIn working towards a relative gradation of mixture (which\nmay be paired with strict requirements/exclusions), we be-\ngin with an account of how each pitch can add to or detract\nfrom the mixture status. This necessarily also involves the\nquestion of ‘which minor’, a conundrum which often com-\nplicates matters of deﬁnition in tonal music.\nMany deﬁnitions of modal mixture restrict themselves\nto natural minor speciﬁcally (minor ˆ6andˆ7), yet they do\nnot describe V in minor as a mixture, despite the raised ˆ7\nclearly belonging to major and not the chosen minor form.\nTones’ mixture status can be organised in a few categories:\nclearly indicative of one mode and not the other; possi-\nbleindication of mixture; and neutral/shared . The follow-\ning categorisation is logically guided, but only one (set of)\nopinion(s). Users of this framework are free to re-allocate\nthe status of these pitches (within reason).\n3.1.1 Clear (non-)mixture: m3, M3, m6\nTones strongly indicate (non-)mixture when they clearly\nbelong in either the host mode or the parallel mode but not\nboth. The clearest example is scale degree ˆ3. The minor\nthird (m3) is a clear case of mixture when it appears in ma-\njor (hereafter min →maj mixture) and non-mixture when\nin minor. Likewise, vice versa, for the major M3: this is\na clear case of mixture in minor (hereafter maj →min) and\nnon-mixture in major. (Again, these comments are sep-\narate from the context caveats discussed elsewhere, e.g.,\nconcerning the ‘Picardy Third’.)\nThe minor sixth (m6) in major is almost as clear: it is\nnot in the major scale and does belong to both natural and\nharmonic minors, as well as the descending melodic minor\nform. Only the ascending melodic minor misses this pitch.\nWhen in Rome defaults suggest the inclusion of m6 as a\ncase of clear mixture, in the deﬁnition framework, while\nenabling theorists to categorise it instead as a case of pos-\nsible mixture if they prefer for a speciﬁc repertoire/task.\n3.1.2 Possible mixture: M6, m7, M7\nSome tones offer a lower level of possible mixture due to\ntheir considerably more ambiguous status. When in Rome\nproposes the major sixth degree (M6) for this category as\nit is more strongly associated with major, though it can be\nreached in one melodic minor form (ascending). M6 may\ntherefore indicate possible maj→min mixture.\nLikewise, the minor seventh degree (m7) may indicate\nmin→maj mixture: it does not feature in major, but it is\nalso not as strongly indicative of minor mode as m6 is,\nappearing only in natural and descending melodic forms.\nFinally, as discussed, the major seventh degree (M7) ar-\nguably indicates maj →min mixture, though raised leading-\ntones are too common in tonal music to support this as a\nchromatic category.\n3.1.3 Neutral (1, 2, 4, 5) and ‘chromatic’ ( ♯1,♯4)\nNeutral tones belong to both major and minor forms\nequally. This group includes scale degrees 1, 2, 4, and 5\nalong with any tones excluded from the above categories.That leaves tones which may be called ‘chromatic’ in\nthe sense that they do not belong to either mode. We can\nconﬁdently populate this category with degrees ♯ˆ1and♯ˆ4.\nIf the user asserts that spelling matters, then the chromatic\ncategory also hosts enharmonics (like ♯ˆ2, discussed above).\n3.2 Metrics and/or Categories\nIf we accept the notion some chords are more strongly in-\ndicative of mixture than others, largely because of the rel-\native status of the tones, then we may wish to explicitly\nweight that relative strength, note by note. For example,\nclearly mixed tones might attract twice the weight (2) of\npossible mixture (1), with neutral values at 0. Chromatic\ntones are perhaps the most ambiguous. When in Rome\ndefaults to a value of −1, because their clearly chromatic\nstatus often detracts from their candidacy for mixture.\nFor instance, to return to the above example cases of\nmin→maj mixture: the strength of mixtures like ♭VI,♭VI7,\nand iv7 derives from that fact that they all feature the m6\nand m3, and all avoid any detractions. The weighting val-\nues above would grade each of these at 4, twice the strength\nof chords like iv with only the m6 (no m3, but also no de-\ntractions) at 2. The pros and cons of an ambiguous chords\ncombining m6 and M3 would effectively balance out.\nOne asset of this weighting-by-tone metric is its ﬂexi-\nbility: it enables any chord to be assessed, including mod-\niﬁcations like added/altered tones, and it can handle the\nenharmonic question separately. Context can be handled\neither categorically (e.g., excluding all secondaries) and/or\nwith further weightings. For instance, the status of mix-\nture may be enhanced when it is bookended by clearly\nnon-mixed chords as in I-iv-I (T-s-T). Again, see ‘When\nin Rome’ for a demonstration.\n4. IN PRACTICE (CORPUS STUDY)\nAll of the above discussion – ‘national’ category variants,\ngraded deﬁnitions, and more – would beneﬁt from com-\nparison with the actual usage in practice. For instance, if\na chord is notcommonly used in a particular style but is\ncommonly taught in courses purporting to represent that\nstyle, then we need to be clear on the reasons why.\nPart of the difﬁculty of establishing robust deﬁnitions\nof the chords above comes from the fact that a robust def-\ninition of the ‘chord’ itself is challenging. Western clas-\nsical notation includes information about which pitches to\nplay, and when, but has no explicit statements on how they\nconnect as chords .16It differs in this (and other) respects\nfrom leadsheets, for example, where it istypical to include\nchord symbols.17While many explicit algorithms for au-\ntomatic harmonic analysis have been proposed, none really\napproaches the quality of a human expert. And arguably\nthe best automatic analysis systems to have emerged in re-\ncent years are those based on machine learning, which de-\nrive, in turn, learns from the computer encodings of human\nexpert analyses discussed here [14].\n16Baroque ﬁgured bass is arguably a partial exception: given the bass\nnote and ﬁguring, you have something like a chordal analysis.\n17Though they are not key-relative like Roman numerals.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n275The assessment of chordal usage ‘in practice’ here is\nbased on that data, and speciﬁcally the ‘When in Rome’\nrepository, which provides a synthesis of all those com-\nputer encodings of human analyses for Western classical\nmusic using Roman numerals.\nAs with all analysis, this is inherently subjective; while\nthe score source material may have editorial ambiguities\nthat evade the notion of ‘ground truth’, this is all the more\nso in analytical commentary on that source. Then again,\nthe harmonic analyses are our subject of interest, and so\nthis subjectivity is not only inevitable, but also desirable.\nOnce the analyses have been encoded in a suitable for-\nmat (legible to human and machine alike), although there\nare still operational decisions to make, the process of ex-\ntracting them is readily implemented and interpreted. The\noperational decisions include ﬁlters for more or less de-\ntailed versions of the chords used as best beﬁts the research\nquestion at hand. For example, it is sometimes necessary\ntoretain inversion information, while at other times it is\nbest to report on aggregated data excluding inversion.\nEvery such option is fully, openly implemented in ex-\ntensively documented and tested code at ‘When in Rome’\nto allow maximum re-use and adaptation for future re-\nsearch. Moreover, that repository presents the percentage\nusage per basic chord type in dedicated ﬁles, separated\nboth by sub-corpus and for major- versus -minor contexts.\nFrom this alone, we can assess the relative usage of our\n‘canonical’ chords. Any such survey highlights the ex-\ntreme predominance of basic tonic and dominant function\nchords (c.75% of the total). Chromatic chords are certainly\nmarginal in relation to this, but we are more concerned\nhere with how common the chromatic chords are relative\nto each other . Figure 2 provides an example of the summa-\ntive data and visualisations provided on ‘When in Rome’,\nin this case for the example of Augmented Sixth chords in\nthe lieder sub-corpus, divided (as discussed) into separate\ndata for major- versus -minor tonal contexts.\nIn addition to the source repo, anyone interested can in-\nteract with this data on OMT’s chromatic harmony anthol-\nogy (click here) where instances of these chromatic chord\ntypes can be browsed in sortable tables, in their full score\ncontext, and in few-bar excerpts.\n4.1 Results for the Three Chromatic Categories\nFor each of the three ‘canonical’ chromatic categories dis-\ncussed above, this section provides some high-level obser-\nvations from the evidence of the corpus, and it considers\nthe implications these observations might have for review-\ning our attitudes to those chords.\nTheNeapolitan sixth is used relatively little. The main\nuse cases in the lieder sub-corpus are ‘ ♭II6’ and ‘ ♭II65’\nin minor (c.0.5%). Another c.0.4% accounts for the other\nNeapolitan candidates in minor, and there is very little use\nin major contexts at all. Other corpora broadly bear out\nthis trend, and with even less usage of the seventh chords.\nEven here in the lieder, many of the ‘ ♭II65’ sevenths cases\noccur in progressions against an inverted pedal, potentially\nsuggesting a possible sub-category for this speciﬁc device.Ger It Fr\nAugmented 6th by so-called \"nationality\" (any inversion)0.00.20.40.60.81.01.21.4Frequency (%)1.54\n0.42\n0.370.72\n0.050.17minor major\nFigure 2 . Augmented sixths chords in the Lieder corpus.\nThe fact that Neapolitans are so commonly taught is\nsomewhat is contrary to the evidence of usage, perhaps\nprompting a review of the importance attributed to them,\nespecially in the ‘category light’ DACH tradition.\nAugmented sixth chords are much more common. For\ninstance, in the lieder the ‘German’ (653) in minor alone\naccounts for over 1.5%, and thus more than all the pos-\nsible Neapolitans in both modes. Within the augmented\nsixth category, it is notable that the ‘German’ (653) is so\nmuch more common than the other forms, and that all\nforms are much more common in the minor mode con-\ntext. The DACH practice of concentrating teaching and\nterminology of augmented sixths on the 653 form arguably\nreceives support from this usage-in-practice evidence.\nModal mixture is much more common still, but very\nunevenly so. The kinds of strong candidates for mix-\nture described above occur relatively infrequently, for in-\nstance, with only ‘ ♭VI’ making a short-list of top-10 cases\n(c.0.1%). Much (c.10x) more common are moderate mix-\ntures like i (c.1.2%) and iv (c.1.0%). This extremely varied\nextent of usage reinforces the need for a distinction be-\ntween types or grades of relative mixture strength.\nIt is perhaps also notable that the ‘other’ chromatic\nchord categories discussed (Augmented and Neapolitan\nsixths) feature among the most common cases of possi-\nble ‘mixture’. They all pose a strong case for mixture,\n(especially the ‘German’ which features both of the main\nmixed tones), but they also have the detraction of chro-\nmatic notes (at least ♯ˆ4for the Augmented Sixths; ♭ˆ2for\nNeapolitans). This may prompt a review or clariﬁcation\nof categories which, in turn, speaks to wider issues such\nas the ‘French’ sixth’s status in relation to tritone substitu-\ntion (again, see [12]), the ‘bebop’ dominant seventh with\ndiminished ﬁfth, and even some secondary dominants.\n5. PROGRESSING TO CHORD PROGRESSIONS\nThis brief paper has set out some of the musical, compu-\ntational, and even national/institutional issues at stake in\ndeﬁning chromatic chords and commenting on their use in\npractice, focussing on three individual chromatic chords.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n276Clearly this is only an initial step towards developing rec-\nommendations for how to deﬁne chords and describe ‘gen-\neral’ practice in a given repertoire.\nThere is plenty of opportunity for future work, not\nleast in growing the datasets (their sheer scale, repertoire\ncoverage, and range of analytical perspectives), and in\nwidening the range of both chordal categories and the lan-\nguages/‘schools’ considered. Another clear next step is to\nexpand the remit from individual chords to chord progres-\nsions . This is not so clear-cut a distinction as it may seem.\nWe close with some examples. Once again, all of the\nlogic discussed here is implemented in the When in Rome\nrepository, and examples are presented in both the ‘An-\nthology’ section of that repo, and in the more browsing-\nfriendly format of the OMT harmony anthology.\n5.1 Chord or progression? The Case of the ‘Cto7’\nSome chromatic cases sit ambiguously between ‘chord’\nand ‘progression’. As discussed, mixture is arguably an\nexample: we certainly have to take account of the modal\ncontext (iv is diatonic in minor but mixture in major) and\nwe may also chose to have additional contextual require-\nments such as the elimination of secondary dominants that\nresolve, and/or of the ‘Picardy Third’ endings.\nThe common-tone diminished seventh chord (‘Cto7’)\npresents an example that nudges further into the realm\nof progressions. Once again, we describe a single chord,\nthough certainly need a wider contextual view. Here the\nchord’s construction as a fully diminished seventh is re-\nquired, but only a small part of the deﬁnition which other-\nwise relies on the context of at least the following chord.\nAlmost certainly required is a common-tone with the fol-\nlowing chord . . . which is not a suspension.18Not usu-\nally required, (though potentially strengthening the case) is\nuse of a common-tone with the preceding chord. And the\ncase is arguably stronger still if the preceding and follow-\ning chords are the same, indicating more of a prolongation .\n5.2 Anglophone/DACH Progressions\nThe comparison of Anglophone and DACH traditions can,\nof course, continue to chord progressions. The ‘Cto7’\ndoes not feature in DACH traditions, though a related form\nknown in Anglophone circles as the ‘Omnibus’ does have\na relative in the DACH concept of the Teufelsmühle [15].\nNot yet at the textbook level, Lewandowki [16] recently\nproposed a category pair for fallender Quintanstieg (here-\nafter, fQ) and aufsteigender Quintfall (hereafter, aQ) both\nof which see pairs of ﬁfth in the same direction, separated\nby a step in the opposite direction. For example, D-A-C-G\nwould be an instance of the fQ, while G-C-A-D would be\na case of the aQ.\nInstances of these progressions can be extracted by\nany corpus, functional or otherwise.19Filtering When in\n18The progression of viio7/V to the cadential 64 is common, but a weak\ncandidate for the Cto. It is excluded by most deﬁnitions (e.g., on OMT),\nthough it may be signiﬁcant as an historical origin for this progression.\n19As the labels are not dependent on key-context or RN labelling, it\nis reasonable to include pop examples here (as Lewandowki does). Forascending descending aufsteigender fallender0100200300400500600Count\n0158\n62\n2 2458\n185\n113623\n260\n1733511\n309\n32OpenScore-LiederCorpus\nPiano_sonatas\nKeyboard_Other\nEarly_Choral\nFigure 3 . Fifth progressions by category and corpus.\nRome for the minimal, 4-chord instances of each shows\nthat the aQ is much (c.20x) more common than the fQ\nacross all corpora (RHS of ﬁg.3). The better known pattern\nof rising/falling cycles of ﬁfths are related in that they also\nfeature pairs of ﬁfths a step apart. Filtering for 4-grams\nof these progressions reveals a similar, and even more ex-\ntreme (c.50x) preference for one direction (falling) over the\nother (rising). So once again, while we may seem to have\na class of equal schema in theory , the usage in practice\nhighlights an imbalance that arguably needs including at\nthe outset of teaching these materials.\n5.3 Beyond the Anglophone-DACH Comparison\nWe close with an example progression originating in an-\nother language, and with the additional constraint of hav-\ning an expected position for its usage, thus further expand-\ning the context we need to assess.\n‘Partimenti’ treatises originating in 18th-century Italy\nhave enjoyed a renewed interest from music theorists in\nrecent years [17]. This method centres on prototypical,\nschematic patterns that can serve as the basis of compo-\nsition (including improvisation). The schema are typically\ndeﬁned by their bass and melodic lines, their harmony, and\ntheir position both in relation to the metre and the large-\nscale form.20Harmonic analysis data captures all of this\nexcept the melodic line. For example, most aspects of\ntheQuiescenza are captured by progressions like I-V7/IV-\nIV-V-I,21and by the expected position at the end(coda)\nof a work. These textbooks provide repertoire examples,\nand there are certainly cases in the meta-corpus (which in-\ncludes a sub-corpus of Corelli Trio Sonatas) that ﬁt.\nHowever, counter-examples are also easy to ﬁnd and\nan initial survey of overall usage ﬁnds no tendency to-\nwards end-section emphasis in any sub-corpus, including\nthe Corelli.22Once again, the data suggests that it is\ntime for a thorough re-evaluation of schematic associations\npassed pedagogically from one generation to the next.\nexample, the fQ (D-A-C-G) is the chord progression of TLC’s Waterfalls .\n20Click here for examples in the relevant section of OMT.\n21Again, the code sets out how to catch all and only the relevant cases.\n22The code includes functionality for plotting usage-by-position.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2776. ACKNOWLEDGEMENTS\nThanks to all who have contributed to the (rather long)\ngestation of these ideas. Clearly the international aspect\nreﬂects my time working in several countries. Thanks to\nall colleagues and students for the exchanges! For mix-\nture especially, I have canvassed opinions from the (com-\nputational) music theory community since around 2020, on\nmusic21 (as part of creating the .isMixture() method\nhere), on Twitter (here), and elsewhere (several public\ntalks). Thanks to all who engaged with this . . . or even sim-\nply heard me out!\n7. REFERENCES\n[1] S. G. Laitz, The Complete Musician: An Integrated\nApproach to Theory, Analysis, and Listening , 4th ed.\nNew York: Oxford University Press, 2016.\n[2] M. Gotham, G. Micchi, N. Nápoles-López, and\nM. Sailor, “When in Rome: a meta-corpus of func-\ntional harmony,” Transactions of the International So-\nciety for Music Information Retrieval , expected 2023.\n[3] J. London, “Building a Representative Corpus of Clas-\nsical Music,” Music Perception: An Interdisciplinary\nJournal , vol. 31, no. 1, pp. 68–90, 2013, publisher:\nUniversity of California Press. [Online]. Available:\nhttp://www.jstor.org/stable/10.1525/mp.2013.31.1.68\n[4] C. Inskip and F. Wiering, “In Their Own Words: Using\nText Analysis to Identify Musicologists’ Attitudes To-\nwards Technology,” in Proceedings of the 16th Interna-\ntional Society for Music Information Retrieval , 2015.\n[5] M. Gotham, K. Gullings, C. Hamm, B. Hughes,\nB. Jarvis, M. Lavengood, and J. Peterson,\nOpen Music Theory , 2nd ed. VIV A Pressbooks,\n2021. [Online]. Available: https://viva.pressbooks.\npub/openmusictheory/\n[6] R. B. Nelson, “The College Music Society Music\nTheory Undergraduate Core Curriculum Survey -\n2000,” College Music Symposium , vol. 42, pp. 60–75,\n2002, publisher: College Music Society. [Online].\nAvailable: https://www.jstor.org/stable/40374423\n[7] B. Murphy and B. McConville, “Music Theory\nUndergraduate Core Curriculum Survey: a 2017\nUpdate,” Journal of Music Theory Pedagogy , vol. 31,\nno. 1, Jan. 2017. [Online]. Available: https://\ndigitalcollections.lipscomb.edu/jmtp/vol31/iss1/9\n[8] L. Euler, Tentamen novae theoriae musicae ex certis-\nsismis harmoniae principiis dilucide expositae . Saint\nPetersburg Academy, 1739.\n[9] H. Riemann, Vereinfachte Harmonielehre; oder, Die\nLehre von den tonalen Funktionen der Akkorde [Har-\nmony Simpliﬁed: Or, the Theory of the Tonal Functions\nof Chords] . London: Augener, 1893.[10] W. Crotch, Elements of musical composition :\ncomprehending the rules of thorough bass, and\nthe theory of tuning . London : Printed for\nLongman, Hurst, Rees, Orme, & Brown by Nathaniel\nBliss, Oxford, 1812. [Online]. Available: http:\n//archive.org/details/elementsofmusica00crot\n[11] J. W. Callcott, A musical grammar, in four parts: 1.\nNotation; 2. Melody; 3. Harmony; 4. Rhythm. Lon-\ndon: Printed by B. Macmillan for R. Birchall, 1806,\nopen Library ID: OL14794362M.\n[12] N. Biamonte, “Augmented-Sixth Chords vs. Tritone\nSubstitutes,” Music Theory Online , vol. 14, no. 2, Jun.\n2008. [Online]. Available: https://mtosmt.org/issues/\nmto.08.14.2/mto.08.14.2.biamonte.html\n[13] P. Ewell, “Harmonic Functionalism in Russian Music\nTheory: A Primer,” Theoria , vol. 26, 2020.\n[14] N. Nápoles López, M. Gotham, and I. Fujinaga, “Aug-\nmentedNet: A Roman Numeral Analysis Network\nwith Synthetic Training Examples and Additional\nTonal Tasks,” in Proceedings of the 22nd International\nSociety for Music Information Retrieval Conference .\nOnline: ISMIR, Nov. 2021, pp. 404–411. [Online].\nAvailable: https://doi.org/10.5281/zenodo.5624533\n[15] M.-A. Dittrich, “›Teufelsmühle‹ und ›Omnibus‹,”\nZeitschrift der Gesellschaft für Musiktheorie [Jour-\nnal of the German-speaking Society of Music\nTheory] , vol. 4, no. 1–2, pp. 107–121, 2007,\npublisher: Gesellschaft für Musiktheorie. [On-\nline]. Available: https://www.gmth.de/zeitschrift/\nartikel/247.aspx#abstract\n[16] S. Lewandowski, “›Fallende Quintanstiege‹,” ZGMTH ,\nvol. 7, no. 1, pp. 85–97, 2010. [Online]. Available:\nhttps://www.gmth.de/zeitschrift/artikel/508.aspx\n[17] R. O. Gjerdingen, Music in the Galant Style . Oxford\n; New York: Oxford University Press, 2007.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n278"
    },
    {
        "title": "Towards Building a Phylogeny of Gregorian Chant Melodies.",
        "author": [
            "Jan Hajic Jr.",
            "Gustavo A. Ballen",
            "Klára Hedvika Mühlová",
            "Hana Vlhová-Wörner"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10340442",
        "url": "https://doi.org/10.5281/zenodo.10340442",
        "ee": "https://zenodo.org/records/10340442/files/000067.pdf",
        "abstract": "The historical development of medieval plainchant melodies is an intriguing musicological topic that invites computational approaches to study it at scale. Plainchant melodies can be represented as strings from a limited alphabet, hence making it technically possible to apply bioinformatic tools that are used to study the relationships of biological sequences. We show that using phylogenetic trees to study relationships of plainchant sources is not merely possible, but that it can indeed produce meaningful results. We develop a simple plainchant substitution model for Multiple Sequence Alignment, adapt a Bayesian phylogenetic tree building method, and demonstrate the promise of this approach by validating the resultant phylogenetic tree built from a set of Divine Office sources for the Christmas Vespers against musicological knowledge.",
        "zenodo_id": 10340442,
        "dblp_key": "conf/ismir/HajicBMV23",
        "keywords": [
            "medieval plainchant melodies",
            "bioinformatic tools",
            "relationships of biological sequences",
            "Multiple Sequence Alignment",
            "Bayesian phylogenetic tree building",
            "Christmas Vespers",
            "musicological knowledge",
            "adapt a Bayesian phylogenetic tree building method",
            "divine office sources",
            "phylotree"
        ],
        "content": "TOWARDS BUILDING A PHYLOGENY OF GREGORIAN CHANT\nMELODIES\nJan Haji ˇc jr.1Gustavo A. Ballen2Klára Hedvika Mühlová3\nHana Vlhová-Wörner1\n1Masaryk Institute and Archive, Czech Academy of Sciences, Czechia\n2School of Biological and Behavioural Sciences, Queen Mary University of London, UK\n3Faculty of Arts, Masaryk University, Czechia\nhajic@mua.cas.cz\nABSTRACT\nThe historical development of medieval plainchant\nmelodies is an intriguing musicological topic that invites\ncomputational approaches to study it at scale. Plainchant\nmelodies can be represented as strings from a limited\nalphabet, hence making it technically possible to apply\nbioinformatic tools that are used to study the relationships\nof biological sequences. We show that using phylogenetic\ntrees to study relationships of plainchant sources is not\nmerely possible, but that it can indeed produce meaningful\nresults. We develop a simple plainchant substitution model\nfor Multiple Sequence Alignment, adapt a Bayesian phylo-\ngenetic tree building method, and demonstrate the promise\nof this approach by validating the resultant phylogenetic\ntree built from a set of Divine Ofﬁce sources for the Christ-\nmas Vespers against musicological knowledge.\n1. INTRODUCTION\nGregorian chant is the universal sacred liturgical monody\nof the Roman Catholic church, which exerts strong con-\ntrol over this musical tradition. There is an authoritative\nedition of chant: if singers from multiple countries and\ncontinents sing together, each from their print of liturgical\nbooks, they should encounter no conﬂicts in performance.\nHowever, this was not always so. During the ﬁve hundred\nyears of notated Gregorian chant manuscript culture, be-\ntween Guidonian staff notation and the introduction of the\npost-Tridentine printed liturgical books, rarely was a chant\nmelody written exactly the same in two sources.1Despite\nits stated role as a unifying element of the Roman Catholic\nchurch, Gregorian chant was a diverse tradition.\nThe diversity of Gregorian chant, both in terms of reper-\ntoire and melody, has been a staple of musicological study\n1See cf. a sample of melodies of an antiphon:\nhttps://cantusindex.org/id/004237\n© J. Haji ˇc jr., G. A. Ballen, K. H. Mühlová, and H. Vlhová-\nWörner. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: J. Haji ˇc jr., G. A. Ballen, K.\nH. Mühlová, and H. Vlhová-Wörner, “Towards Building a Phylogeny of\nGregorian Chant Melodies”, in Proc. of the 24th Int. Society for Music\nInformation Retrieval Conf., Milan, Italy, 2023.of plainchant [1, 2]. Already the relative importances of\nchronology, geography, and cursus2are, aside from select\ntopics such as the Cistercian reform, not well understood.\nRecent chant scholarship thrives on the large-scale digiti-\nzation effort centered around the Cantus Index network of\ndatabases [3], and there are ongoing efforts to apply digital\nmethods to the problem of chant transmission such as the\nDACT project.3\nIn this pilot study, we present a novel pipeline to model\nthe relationships between chant sources using tools from\nbioinformatics: we adapt multiple sequence alignment and\nphylogenetic tree inference for chant melodies. We qual-\nitatively evaluate the method on a dataset of sources for\nChristmas Eve vespers.4\n2. RELATED WORK\nThe study of melodic dialects of chant has a long tradi-\ntion, most prominently in the distinction proposed between\n“West Frankish” and “East Frankish” chant [4], as has the\ntheory of chant melody in general (i.a. the centonization\nhypothesis [5, 6] and its criticism [7], [8, pp. 74-75]), but\nhas not yet been performed with computational models at\nthe scales that these enable. The fact that the diversity\nwithin chant melodies is a subject worthy of study is fur-\nther reinforced by the debate on early chant as an orally\ntransmitted tradition [9, 10], justifying an ethnomusico-\nlogical perspective [11], although the extent of orality of\nthe tradition has since been contested [12]. The formu-\nlaic structure of great responsories has been studied in de-\ntail [13], even in the pre-computer era [14].\nWork on larger-scale computational analysis of chant\nmelodies has recently been done in the area of melody seg-\nmentation [15], measurement of the melodic arch hypothe-\nsis [16] and of the relationship between antiphons and dif-\nferentiae across modes [16]. Importantly, these works also\nprovide the Cantus Corpus v0.2 database, which presents\nthe contents of the Cantus Database in a manner ready\n2The ecclesiastical environment of a mansucript, such as a monastery\nof a speciﬁc order, a city church, a cathedral.\n3https://dact-chant.ca/\n4Data is available at github.com/Genome-of-Melody/\nchristmas/releases/tag/ISMIR2023 , tree inference code at\ngithub.com/Genome-of-Melody/mrbayes_volpiano .571for further processing.5Cantus Index6also provides the\nCantus Analysis tool,7which is however built solely for\nanalyzing repertoire, not melodies.\nIn MIR, the potential for applying bioinformatic tools\nas string processing models has been previously noted in\nthe context of music similarity search. Tune family iden-\ntiﬁcation using Multiple Sequence Alignment (MSA) has\nbeen tried [17, 18], and MSAs and and BLAST search has\nbeen used for melody classiﬁcation and fast melody re-\ntrieval [19], with mixed results.\nMore closely related to this work in terms of scientiﬁc\ngoals, the ﬁeld of cultural evolution has also been map-\nping patterns of musical diversity [20], with roots in the\nCantometrics project [21]. Most notably, the evolution of\nfolk melodies in English/US and Japanese traditions has\nbeen found to exhibit similar properties, using MSAs [22],\nand phylogeny of electronic music has been mapped us-\ning dynamic community detection rather than phylogenetic\ntrees [23], citing limitations of the tree model in light of\nhorizontal cultural transmission. Cultural and biological\nevolution was correlated in a study comparing populations\nin terms of genetics and their folk music [24].\nFew works in MIR go beyond leveraging MSA as a tool\nfor melodic similarity applications, and in one instance\nalso on chant [25]. From the cultural evolution ﬁeld have\nused some phylogenetic models to study music, but so far,\nnot on chant.\n3. METHOD\nWe model the relationships among melodies from a set of\nchant sources as a phylogenetic tree. The leaves of the\nchant sources, which carry (artiﬁcially ordered) melodies\nof the selected Cantus IDs in an analogy to how living\nspecies carry genes. Each instance of a chant with a certain\nCantus ID in each source is a homologous sequence; the\ncollection of melodies from one Cantus ID across sources\nis here termed a locus . The pipeline consists of the follow-\ning steps:\n1. Concatenate cleaned melodies per source (in an ar-\nbitrary but ﬁxed order of Cantus IDs)\n2. Compute a (partitioned) multiple sequence align-\nment (MSA) of the concatenated melodies\n3. Infer a phylogenetic tree over the MSA\nAn overview of the pipeline is shown in Fig. 1.\nIn the Cantus network of databases, chant melodies are\ntranscribed as strings using V olpiano [26]. V olpiano is both\na standard for encoding chant melodies in a plain text for-\nmat,8and a font that renders these strings.9The encoding\nuses several non-tone characters, such as hyphens to in-\ndicate boundaries between neumes, syllables, and words,\nor barline characters to indicate sections. For our exper-\niments, we have removed non-note characters (retaining\n5https://github.com/bacor/cantuscorpus\n6https://cantusindex.org/\n7https://cantusindex.org/analyse\n8https://cantus.uwaterloo.ca/sites/default/\nfiles/documents/2.\\%20Volpiano\\%20Protocols.pdf\n9http://www.fawe.de/volpiano/syllable and word separators did not have an appreciable\neffect on alignment, and would thus unnecessarily compli-\ncate the state space).\nAny string distance metric can then be used to model\nbetween two melodies, and between any two sources (by\naggregating the distances between melodies). However,\nwe speciﬁcally chose Bayesian phylogenetic trees as the\nmodel because (1) their inference procedure can distin-\nguish between similarities that are substantial and those\nthat are the product of chance, (2) the resulting trees, while\nperhaps not ideal as a model of transmission itself, are\noptimal results in terms of a clearly deﬁned probabilistic\nmodel, and thus have a probabilistic interpretation that di-\nrectly allows testing hypotheses about the dataset, rather\nthan post-inference normalization of arbitrary similarity\nscores, (3) the software tools are readily available.10\n3.1 MSA and Score Matrix\nMultiple sequence alignment (MSA) was carried out with\nMAFFT v7.505 [27]. Mafft is used for the alignment of\nmelodies because it is a state-of-the-art MSA tool that al-\nlows aligning arbitrary text using custom score matrices,\nthus allowing to process data which are not standard bi-\nological sequences such as DNA and aminoacids. (It has\nalready been used in MIR, precisely for these advantages\n[28].) with our custom score matrix described below. We\nused a maximum of 1000 iterations and global pairing.\nBy default, Mafft aligns arbitrary text by checking\nwhether a given symbol is equal or not to other entries\nin the alignment. This is not a good model for melodies,\nas substitutions are notequally likely. The default choice\nfor melodic distance, Mongeau-Sankoff distance [29] ad-\ndresses the unequal costs of substituting different steps of\nthe scale, but it and others are designed for tonal music,\nwhich chant predates by several hundred years. We have\nnot in fact found sufﬁcient music-theoretical understand-\ning of chant melodies (and mode) to design a similar scor-\ning function. Therefore, we resort to a basic physical re-\nality: the cost of a substitution is the number of steps be-\ntween the two notes, thus crudely mimicking how physi-\ncally different the position in the melody might feel for a\nsinger familiar with the alternative (see Fig. 2). The ap-\nplication of B ﬂats were assigned a low cost because they\nwere commonly applied without modifying considerably\nthe melody. Liquescences were treated as regular notes\nof the same pitch. We stress that this is by no means a\ndeﬁnitive chant scoring matrix, but rather a starting point\nto search for one.\n3.2 Bayesian Inference of Phylogenetic Trees\nAphylogenetic tree (hereafter: tree) is a graph represent-\ning the evolutionary relationships between the objects of\nstudy. These trees have long been used in evolutionary\nbiology as a means to depict evolutionary relationships\n10This article is not meant to inspire the impression that a large amount\nof technical work was performed: rather, we ﬁnd it notable that already\nwith a limited amount of technical work, this method already seems to\nobtain plausible results.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n572Figure 1 . Outline of the pipeline for inference of phylogenetic trees of sources from melodies.\nFigure 2 . Instances of scores from a reference pitch D. Ar-\nrows represent the ﬁnal pitch and numbers under note bod-\nies represent the score in the matrix. For MAFFT, scores\nare positive: the furthest distance between pitches is from\nG3 to D6, which has a score of 1, and the unison has the\nlargest score, which is 19.\namong species, and we are here using it to represent the\nevolution of sources containing melodies in a similar way.\nTrees are composed of sleaf nodes of degree 1, called\ntipsorterminals , and up to s−1internal nodes of degree 3\nwhich represent ancestors. Trees may have a root node: an\ninternal node of degree 2. If a tree is unrooted, there are up\ntos−2internal nodes. Edges are called branches, and they\nhave lengths that represent some amount of evolutionary\nchange – usually (as in our case) the expected number of\nchanges per site.\nBayesian inference has been applied to phylogenetic\ntree estimation since the 1990s [30] and consists of cal-\nculating the posterior probability of the tree parameters\n(topology and branch lengths) for a given tree τ[31]:\nf(τi,Q|X) =f(X|τi,Q)f(τi)f(Q)\n/summationtextB(s)\nj=1f(X|τj,Q)f(τj)f(Q)(1)\nIn this model, f(X|τi)is called the phylogenetic like-\nlihood function, which gives the likelihood of observing\nthe alignment data given the model of evolution parame-\ntersQand a particular tree τi[32, 33]. Both f(τi)and\nf(Q)are priors for the topology and model of evolution.\nThe topology prior is usually set to be uninformative (uni-\nform over all possible trees). The prior f(Q)is derived\nfrom the Mkv+G model, which is the state of the art for\nmorphology-based phylogeny.11As can be anticipated\n11As opposed to phylogenies built from sequences of nucleotides\n(DNA/RNA) or amino-acids (proteins), morphology-based phylogeny\nmodels mostly model transition probabilities from one to a different char-\nacter as equally likely.by the fast-growing number of possible trees for a given\nset ofsterminals B(s) =(2s−3)!\n2n−2(n−2), this problem can-\nnot be solved by visiting all possible topologies in order to\ncalculate the normalising constant in the Bayes’ equation,\nwhich also, cannot be analytically solved even for a sin-\ngle topology. Therefore, MCMC sampling is used to con-\nstruct the posterior densities for all the parameters of the\nmodel. The output of MCMC consists of inferred parame-\nter values (mostly branch lengths), sampled trees, and the\nsummary tree.The summary tree summarizes the posterior\ndensity of topologies in using a majority-rule consensus: it\nincludes all bi-partitions which are at least in 50% of the\nsampled topologies. Node posterior probabilities are cal-\nculated from the relative frequency of bipartitions in the\nposterior tree density. Inferred trees are in principle un-\nrooted.12\nUnfortunately, all existing software for Bayesian phy-\nlogenetics restricts the input data to some sort of biolog-\nical data, be it DNA, aminoacid, or morphological data;\ntherefore, we had to adapt an existing tool to process\nV olpiano-encoded chants. We chose to modify MrBayes\nv3.2.7a [34]. We call our fork mrbayes_volpiano , in\norder to make clear that it is intended for use only with\ndata in volpiano format.13mrbayes_volpiano ac-\ncepts V olpiano-encoded chant melodies as input and anal-\nyse them using a Markov model of evolution for an arbi-\ntrary number of the discrete character states [35]. It uses\nall the tools from MrBayes available for standard coding,\nwhich is the one applied to melody sequence data and\ncan carry out inference of both single-partition or con-\ncatenated settings composed of multiple partitions. It pro-\ncesses alignments in nexus format and can be run both in\ninteractive and scripting mode.\n4. CHRISTMAS VESPERS DATASET\nIn order to test the ability of our pipeline to resolve sub-\nstantial relationships between chant sources, we apply it\non a dataset of Christmas divine ofﬁce, speciﬁcally Ves-\npers for Vigilia Nativitatis Domini. The dataset was orig-\ninally collected in order to study relationships between\n12They can be rooted for visualisation e.g. using FigTree ( https:\n//github.com/rambaut/figtree ).\n13The source code is available at https://github.com/\ngaballench/mrbayes_volpianoProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n573late medieval Bohemian sources with the data includ-\ning transcribed melodies available in the Hymnologica\ndatabase,14and we combined this data with all further\nmelodies available for Vig. Nat. Domini vespers from the\nCantus Index interface, in order to cover a broader Euro-\npean context.\nThe combined dataset contained 14 sources, and a to-\ntal of 78 chants falling under 6 distinct Cantus IDs. Be-\ncause the repertoire in ofﬁce sources is not entirely con-\nsistent across sources and our system aligns melodies di-\nrectly, we had to select a subset of chants contained in\nas many sources as possible, and then reduce the set of\nsources to those that contained as many of these chants as\npossible. The resulting dataset contained 14 sources , each\nof which had fully transcribed melodies for the following\nCantus IDs: 001737, 002000, 003511, 004195, 007040a,\nand 605019.15\nSome sources contained multiple instances of chants of\none Cantus ID: In that case, we retained the version with\nthe most complete version of the melody (as repeated in-\nstances of the same chant are sometimes only written as\nincipits in the sources), and if multiple full melodies were\navailable, we selected the melody that was directly in the\nVig. Nat. Dom. section (see Tab. 1).\nWhy use such a limited dataset, when the entirety of\nthe Cantus Database is available? We originally intended\nto use the CantusCorpus dataset [15] of Ofﬁce melodies.\nHowever, the authors of the Cantus Database preferred\ntranscribing entire sources, so while there are more than\n13000 fully transcribed antiphons in CantusCorpus v0.2 ,\nthe vast majority comes from less than 20 sources. This\nis further compounded by the surprising diversity of of-\nﬁce repertoire. Thus, in the entirety of CantusCorpus, it\nis only possible to ﬁnd 10 different sources that have tran-\nscribed melodies for 5 antiphons. Hence, we decided to use\nthe Christmas dataset, with its advantage of having been\ncollected speciﬁcally in order to make the comparison be-\ntween different sources possible.16\n4.1 Sources and Evaluation\nOur methodology differs from machine learning experi-\nments in when data is used. The phylogenetic tree model\nis selected and parametrized a priori , and only then we\nuse a dataset to validate the model: is the tree inferred on\nthe dataset plausible according to musicological expecta-\ntions? Given that chant transmission provides few hard\npredictions, these expectations are notexpressed in terms\nof target values. The evaluation of a tree’s plausibility is\nqualitative.\nSince we base the claim of valid results on comparing\nthe inferred tree against known relationships between the\nsources, we must give an overview of the 14 sources in\nterms of their placement along the three major dimensions\nof chant culture: place, time, and liturgical context. This\n14http://hymnologica.cz/jistebnice\n15All available via https://cantusindex.org/id/(...) .\n16This quest for data also highlights the major limitation of our pipeline\nso far: we need comparable melodies from each source.section essentially describes our “evaluation data”.\nA-Wn 1799**. A 13th century Cistercian antiphoner\nfrom the Rein monastery in Austria.\nA-VOR Cod. 259/I. A 14th century antiphoner of the\ncollegiate chapter church of Vyšehrad, Prague. In the early\n15th century, it was moved to V orau because of Hussite\nwars. In 1490-1500, it was adapted for Salzburg liturgy.17\nCDN-Hsmu M2149.L4. Cistercian antiphoner from\nthe Abbey of Salzinnes, Namur, in the Diocese of Liège,\ncentral Belgium, completed in 1554-1555.18\nCH-E 611. A 14th-century antiphoner from the Bene-\ndictine monastery of Einsiedeln, central Switzerland.\nCZ-HKm II A 4. An antiphoner from the 1470s, be-\nlonging to the municipal Church of the Holy Spirit in\nHradec Králové, eastern Czechia.19\nCZ-PLm 504 C 004. A late antiphonary from the\nSt. Bartholomew municipal church in Pilsen, western\nCzechia, from 1616.20\nCZ-Pu XVII E 1. A mixed Latin and Czech an-\ntiphonary from the early 16th century, of Czech (but fur-\nther unspeciﬁed) provenance.21\nCZ-Pn XV A 10. Late 15th century notated breviary\nfrom the cathedral cursus in Prague, Czechia.22\nCZ-Pu I D 20. An antiphonary from the Augustinian\nmonastery in T ˇreboˇn, southern Czechia, created in the 2nd\nhalf of the 14th century.23\nD-KA Aug. LX. A complex 12th-century antiphoner,\nof which the musical notation was almost completely\nrewritten in the 13th or 14th centuries. From the Zwiefal-\nten Benedictine monastery in southwestern Germany,\nmoved to the abbey of Reichenau in the 15th century.24\nD-KNd 1161. A late 12th- and early 13th-century Cis-\ntercian antiphoner, possibly written for use by the female\nabbey of Saint Mechtern in Cologne, western Germany, re-\nnamed Saint Apern in 1477.25\nF-Pn lat. 12044. An early 12th-century antiphoner\nfrom the Benedictine abbey of St.-Maur-de-Fossés, close\nto Paris, France.26\nF-Pn lat. 15181. An early 14th-century notated bre-\nviary belonging to the Notre Dame cathedral in Paris,\nFrance.27\nNL-Uu 406. A 12th-century antiphonary from St.\nMary’s church in Utrecht, Netherlands. Later 13th-15th-\ncentury changes. Complex source that has multiple ver-\nsions of some melodies.28\nWhat results should one expect from a phylogeny of\nthese chant sources? The three major dimensions of “ex-\n17https://manuscripta.at/hs_detail.php?ID=6267\n18https://cantus.uwaterloo.ca/source/123723\n19http://hun-chant.eu/source/1481?page=1\n20https://rukopisy.zcm.cz/view.php?ID=504C004\n21https://www.manuscriptorium.com/apps/index.\nphp?direct=record&pid=AIPDIG-NKCR__XVII_E_1___\n_32Y2B65-cs#search\n22http://hymnologica.cz/source/47\n23http://hymnologica.cz/source/10721\n24https://cantus.uwaterloo.ca/source/123612\n25https://cantus.uwaterloo.ca/source/601861\n26https://cantus.uwaterloo.ca/source/123628\n27https://cantus.uwaterloo.ca/source/123631\n28https://cantus.uwaterloo.ca/source/123641Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n574Source Provenance Date Cursus 605019 001737 002000 003511 004195 007040a\nA-Wn 1799** Rein 1200s Cistercian 1 NA 1 1 1 1\nA-VOR Cod. 259/I Prague 1360 Secular 1 2 1 1 1 1\nCDN-Hsmu M2149.L4 Salzinnes 1554 Cistercian 1 NA 1 1 1 1\nCH-E 611 Einsiedeln 1300s Benedictine 1 3 1 1 1 1\nCZ-HKm II A 4 Hr. Král. 1400s Secular 1 1 1 1 1 1\nCZ-PLm 504 C 004 Plsen 1616 Secular 1 1 1 1 1 1\nCZ-Pu XVII E 1 Bohemia 1516 Unknown 1 NA 1 1 NA 1\nCZ-Pn XV A 10 Prague 1300s Secular 1 1 1 1 1 1\nCZ-Pu I D 20 Passau 1300s Augustinian 1 1 1 1 1 1\nD-KA Aug. LX Zwiefalten 1100s Benedictine 1 1 1 1 1 1\nD-KNd 1161 Köln 1200s Cistercian 1 NA 1 1 1 1\nF-Pn lat. 12044 Paris 1100s Benedictine 1 1 1 1 2 1\nF-Pn lat. 15181 Paris 1300s Secular 1 NA 1 1 2 1\nNL-Uu 406 Utrecht 1150 Secular 1 2 1 3 2 1\nTable 1 . Sources of the Christmas Vespers dataset with their provenance, approximate date, cursus, and presence of the\nchant in each source (1 or more instances per source). NA represents chants not present in a given source.\nternal” similarity between chant sources, in terms of how\nsimilar the segments of culture represented in these sources\nare expected to be, are geography, chronology, and cursus\n– space, time, and the liturgical context within which the\nbooks were used. It is not entirely clear in chant scholar-\nship how strongly each of these factors should inﬂuence\nchant melodies (the exception where cursus is clearly ex-\npected to dominate other factors is that of the Cistercian\norder, which mandated that all monasteries must have iden-\ntical liturgical books [36, p. 99]), but these organizing prin-\nciples should be observed in the resulting tree.\n5. EXPERIMENTS AND RESULTS\nFor all our experiments, we set up Bayesian inference\nusing an Mkv model of evolution with options +I+G.\nMetropolis-Hastings MCMC sampling was carried out\nwith four independent runs, each with four chains (one\ncold and three hot), with 10.000.000 generations, sam-\npling each 1000 generations. Parameter and tree sum-\nmaries were generated combining the four trace ﬁles after\na burn-in of 50 % was applied to each. Parameter conver-\ngence was assessed by examining the potential scale re-\nduction factor (PSRF) [37] which should approach 1.0 as\nruns converge, and the average standard deviation of split\nfrequencies (ASDPF) [38] which should be below 0.01 for\ntopological convergence. Other parameters had effective\nsample size (ESS) values above 600. We do not root the\nsummary trees, because there is no clear outgroup in our\ndataset.\n5.1 Single-locus tree inference\nWe ﬁrst computed a tree for sets of melodies under each of\nthe six Cantus IDs separately. In this setting, we examine\nwhether the model can resolve the structure of diversity of\nindividual melodies.\nFor each cantus ID, we aligned the sets of melodies to\nobtain a nexus matrix that is then used as input for tree in-\nference. This resulted in six different summary trees, one\nfor each chant. We found varying but overall low degrees\nof resolution in topology. Some chants had nearly no vari-\nation and consequently the majority-rule consensus tree is\nalmost a complete polytomy29(003511, 004195). Other\n29Star graph: a tree with only one internal node.chants had several internal nodes resolved, therefore rep-\nresenting some degree of information contained in a single\nmelodic line which shows changes across sources How-\never, at the scale of individual melodies, there was insufﬁ-\ncient signal for the model to ﬁnd meaningful differences.\n5.2 Multi-locus tree inference\nA concatenated experiment, in which the set of 14 sources\nwas chosen to represent the terminals, was then conducted.\nWe prepared individual alignments for each of the loci so\nthat the boundaries for the same locus (Cantus ID) in the\nresulting nexus matrix were in ﬁxed positions. Here, a\ntree was resolved (Fig. 3) that exhibits several properties\nthat we believe make it a plausible model of how chant\nmelodies in these sources are related.\nFirst, cursus. All the Cistercian manuscripts (“white\nmonks”) are grouped tighlty together, with the lowest prob-\nability of differences – regardless of geographical area and\ncentury of origin. This is not entirely the case for the\nBenedictine manuscripts: the tree does keep together a\nS. German and a N. Swiss source, but the French Bene-\ndictine source is grouped with a French cathedral source.\nThe probability of changes (expressed as branch length) is\nalso much greater between the two closely related Bene-\ndictine manuscripts. Finally, there is an interesting case\nof the Augustinian CZ-Pu I D 20 manuscript and A-\nVOR Cod. 259/1. The latter is not from an Augustinian\nmonastery, but belonged to a community of canon regu-\nlars – a type of clerical community from which the Augus-\ntinian order was organized in 1244. They are not particu-\nlarly close – they do not have an extra internal node like\ne.g. the French manuscripts – but they are not separated by\none, either, and they lie in between the rest of the Czech\ngroup and the rest of the tree.\nSecond, geography. If one brieﬂy disregards the Cis-\ntercian branch, the topology of the rest of the manuscripts\ndoes roughly correspond to their geographical distribution,\nfrom the French group in the west to the Czech group in\nthe east. Note also that while there is some resolution in\nthe group of Czech secular manuscripts, it is barely there:\nthe internal nodes occur at most in six out of ten MCMC\nsamples.\nFinally, chronology seems to exert a relatively weak in-\nﬂuence, but the dataset is not well suited to study the de-\nvelopment of chant melodies in time, as most of the CzechProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n575Figure 3 . Main experimental result: summary of the posterior tree density as an unrooted majority-rule consensus tree\nfor the concatenated dataset where each chant is a partition. All bipartitions present in at least 50% of the posterior trees\nare shown as internal nodes, with their nodal posterior probability. Terminals – tree leaves – are sources. Length of edges\ncorresponds to probability of mutation; scale bar (bottom left) for 1 % expected mutation rate. Flags indicate geographical\nprovenance, icons indicate cursus (black monks – Benedictines, white monks – Cistercians, heart – Augustinian, church\nbuilding – secular cursus). Century (or half-century) indicated directly; some sources (D-KA Aug. LX, NL-Uu 406) have\ncomplex histories – see sec. 4.1.\nsources are later than most other sources, so it is not clear\nhow to distinguish geographical and chronological factors,\nand there is only one non-Cistercian clearly pre-1300 old\nsource (F-Pn lat. 12044) that was not modiﬁed in the later\ncenturies (which is the case both with D-KA Aug. LX and\nNL-Uu 406).\n6. CONCLUSIONS AND FUTURE WORK\nThe proposed chant phylogeny pipeline produced a mu-\nsicologically plausible model of the melodic diversity\nwithin the Christmas chant dataset. We do not claim that\nthe resulting tree in Fig. 3 is the only orbest possible\nway to model the relationships between the sources from\nour Christmas Vigil dataset; however, while further work\nshould primarily focus on assembling a larger dataset and\ndesigning a more robust validation procedure, we believe\nthat based on the current result, the proposed method can\nmeaningfully enrich digital chant scholarship.\nA major limitation is that the model requires homolo-\ngous melodies (indicated by a shared Cantus ID). For the\nstudy of melodies to bypass this limitation, “morpholog-\nical” features derived from melodies would be needed, so\nthat we can process sources that do not share as many (tran-\nscribed) melodies. This is especially important for Ofﬁce\nsources, where repertoire is strongly differentiated.\nProvided one is not interested in the of melodic diver-sity but only in repertoire structures of chant culture, one\ncan build trees from binary features representing the pres-\nence/absence of Cantus IDs at given liturgical positions,\nusing the same Bayesian model but with an alphabet of\ntwo rather than 19 characters.\nAnother limitation is that the current method does not\nmodel chronology: it is not yet a model of chant melody\nevolution through time . This complicates interpreting the\ntree: one potentially attractive idea is that the internal\nnodes correspond to likely manuscript copying events, but\nwithout a more explicit chronology, this remains specula-\ntive. Chronology can be incorporated by using Bayesian\ndivergence time estimation (BDTE), an extension of topol-\nogy inference that produces branch lengths in absolute\ntime rather than the expected number of substitutions per\nsite by using time priors for either nodes or terminals. Fur-\nthermore, BDTE could infer a posterior distribution for\nnodes without observed time values, and thus we could es-\ntimate e.g. the times of origin of different layers of a more\ncomplex source (such as D-KA Aug. LX or NL-Uu 406)\nby using time priors rather than precise time values.\nMany methodological choices merit further exploration\n(such as the alignment scoring matrix, choice of tree\nmodel, or different ways of combining individual chants).\nHowever, based on the already plausible results of this pi-\nlot study, we are conﬁdent that chant phylogeny is a viable\nand exciting opportunity for digital chant scholarship.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n5767. ACKNOWLEDGMENTS\nThis publication was made possible through the support\nof a grant from the John Templeton Foundation (project\nGenome of Melody). The opinions expressed in this pub-\nlication are those of the author(s) and do not necessarily\nreﬂect the views of the John Templeton Foundation. Jan\nHajiˇc and Hana Vlhová-Wörner further acknowledge sup-\nport from the Czech Science Foundation project no. 19-\n28306X Old Myths, New Facts.\n8. REFERENCES\n[1] D. Hiley, Western Plainchant: A Handbook , ser.\nClarendon Paperbacks Series. Clarendon Press, 1995.\n[2] W. Apel, Gregorian chant . London: Burns & Oates,\n1958, vol. 601.\n[3] D. Lacoste, “The Cantus Database and Cantus Index\nNetwork,” in The Oxford Handbook of Music and\nCorpus Studies . Oxford University Press, 2022.\n[Online]. Available: https://doi.org/10.1093/oxfordhb/\n9780190945442.013.18\n[4] P. Wagner, “Germanisches und romanisches im früh-\nmittelalterlichen kirchengesang,” in Bericht über den\nI. Musikwissenschaftlichen Kongreß der deutschen\nMusikgesellschaft in Leipzig 1925 , Leipzig, Germany,\n1925, pp. 21–34.\n[5] ——, Einführung in die gregorianischen Melodien:\nt. Gregorianische Formenlehre; eine choralische\nStilkunde . Breitkopf & Härtel, 1921, vol. 3.\n[6] P. Ferretti, Estetica gregoriana: ossia, Trattato\ndelle forme musicali del canto gregoriano , ser.\nEstetica gregoriana: ossia, Trattato delle forme\nmusicali del canto gregoriano. Pontiﬁcio istituto di\nmusica sacra, 1934, no. sv. 1. [Online]. Available:\nhttps://books.google.cz/books?id=vOWCnQEACAAJ\n[7] L. Treitler, “Centonate\" chant:\" übles ﬂickwerk\" or\" e\npluribus unus?” Journal of the American Musicologi-\ncal Society , vol. 28, no. 1, pp. 1–23, 1975.\n[8] D. Hiley, Western plainchant: a handbook . Oxford,\nUnited Kingdom: Clarendon Press, 1993.\n[9] H. Hucke, “Toward a new historical view of gregorian\nchant,” Journal of the American Musicological Society ,\nvol. 33, no. 3, pp. 437–467, 1980.\n[10] L. Treitler, “The early history of music writing in the\nwest,” Journal of the American Musicological Society ,\nvol. 35, no. 2, pp. 237–279, 1982. [Online]. Available:\nhttp://www.jstor.org/stable/831146\n[11] P. Jeffery, Re-Envisioning Past Musical Cultures:\nEthnomusicology in the Study of Gregorian Chant ,\nser. Chicago Studies in Ethnomusicology. University\nof Chicago Press, 1992. [Online]. Available: https:\n//books.google.cz/books?id=2pfB84aGUnMC[12] D. G. Hughes, “Evidence for the traditional view of the\ntransmission of gregorian chant,” Journal of the Ameri-\ncan Musicological Society , vol. 40, no. 3, pp. 377–404,\n1987.\n[13] K. Helsen, “The use of melodic formulas in respon-\nsories: constancy and variability in the manuscript tra-\ndition,” Plainsong & Medieval Music , vol. 18, no. 1,\npp. 61–76, 2009.\n[14] W. H. Frere, Antiphonale Sarisburiense: a reproduc-\ntion in facsimile of a manuscript of the 13th century,\nwith a dissertation and analytical index . Gregg Press\nLimited, 1901.\n[15] B. Cornelissen, W. H. Zuidema, J. A. Burgoyne et al. ,\n“Mode classiﬁcation and natural units in plainchant.”\ninProceedings of the 21st Int. Society for Music Infor-\nmation Retrieval Conf. , Montreal, Canada, 2020, pp.\n869–875.\n[16] B. Cornelissen, W. Zuidema, and J. A. Burgoyne,\n“Studying large plainchant corpora using chant21,” in\n7th International Conference on Digital Libraries for\nMusicology , 2020, pp. 40–44.\n[17] P. E. Savage and Q. D. Atkinson, “Automatic tune fam-\nily identiﬁcation by musical sequence alignment,” in\nProceedings of the 16th ISMIR Conference , vol. 163,\n2015.\n[18] D. Bountouridis, D. G. Brown, F. Wiering, and R. C.\nVeltkamp, “Melodic similarity and applications using\nbiologically-inspired techniques,” Applied Sciences ,\nvol. 7, no. 12, 2017. [Online]. Available: https:\n//www.mdpi.com/2076-3417/7/12/1242\n[19] D. Bountouridis, D. Brown, H. V . Koops, F. Wiering,\nand R. C. Veltkamp, “Melody retrieval and classiﬁca-\ntion using biologically-inspired techniques,” in Com-\nputational Intelligence in Music, Sound, Art and De-\nsign, J. Correia, V . Ciesielski, and A. Liapis, Eds.\nCham: Springer International Publishing, 2017, pp.\n49–64.\n[20] P. E. Savage, “Cultural evolution of music,” Palgrave\nCommunications , vol. 5, no. 1, pp. 1–12, 2019.\n[21] A. Lomax, “Factors of musical style,” in Theory &\npractice: Essays presented to gene weltﬁsh . Mouton\nThe Hague, 1980, pp. 29–58.\n[22] P. E. Savage, S. Passmore, G. Chiba, T. E.\nCurrie, H. Suzuki, and Q. D. Atkinson, “Sequence\nalignment of folk song melodies reveals cross-\ncultural regularities of musical evolution,” Current\nBiology , vol. 32, no. 6, pp. 1395–1402.e8, 2022.\n[Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S0960982222000926\n[23] M. Youngblood, K. Baraghith, and P. E. Savage,\n“Phylogenetic reconstruction of the cultural evolutionProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n577of electronic music via dynamic community detec-\ntion (1975–1999),” Evolution and Human Behavior ,\nvol. 42, no. 6, pp. 573–582, 2021.\n[24] H. Pamjav, Z. Juhász, A. Zalán, E. Németh, and\nB. Damdin, “A comparative phylogenetic study of\ngenetics and folk music,” Molecular Genetics and\nGenomics , vol. 287, no. 4, pp. 337–349, Mar.\n2012. [Online]. Available: https://doi.org/10.1007/\ns00438-012-0683-y\n[25] K. Szabová, Analytical tools for Gregorian chant .\nBachelor thesis. Univerzita Karlova, Matematicko-\nfyzikální fakulta, 2021.\n[26] K. Helsen and D. Lacoste, “A report on the encoding\nof melodic incipits in the cantus database with the mu-\nsic font ‘volpiano’,” Plainsong amp; Medieval Music ,\nvol. 20, no. 1, p. 51–65, 2011.\n[27] K. Katoh and D. M. Standley, “MAFFT Multiple Se-\nquence Alignment Software Version 7: Improvements\nin Performance and Usability,” Molecular Biology and\nEvolution , vol. 30, no. 4, pp. 772–780, 01 2013.\n[28] D. Bountouridis et al. , “Music information retrieval us-\ning biologically inspired techniques,” Ph.D. disserta-\ntion, Utrecht University, 2018.\n[29] M. Mongeau and D. Sankoff, “Comparison of musical\nsequences,” Computers and the Humanities , vol. 24,\npp. 161–175, 1990.\n[30] Z. Yang and B. Rannala, “Bayesian phylogenetic in-\nference using DNA sequences: a Markov Chain Monte\nCarlo Method.” Molecular Biology and Evolution ,\nvol. 14, no. 7, pp. 717–724, 07 1997.\n[31] J. P. Huelsenbeck and F. Ronquist, “MRBAYES:\nBayesian inference of phylogenetic trees ,” Bioinfor-\nmatics , vol. 17, no. 8, pp. 754–755, 08 2001.\n[32] J. Felsenstein, “Maximum-likelihood estimation of\nevolutionary trees from continuous characters.” Amer-\nican Journal of Human Genetics , vol. 25, no. 5, pp.\n471–492, Sep. 1973.\n[33] ——, “Evolutionary trees from gene frequencies and\nquantitative characters: Finding maximum likelihood\nestimates,” Evolution , vol. 35, no. 6, pp. 1229–1242,\n1981.\n[34] F. Ronquist and J. P. Huelsenbeck, “MrBayes 3:\nBayesian phylogenetic inference under mixed models,”\nBioinformatics , vol. 19, no. 12, pp. 1572–1574, 08\n2003.\n[35] P. O. Lewis, “A likelihood approach to estimating phy-\nlogeny from discrete morphological character data,”\nSystematic biology , vol. 50, no. 6, pp. 913–925, 2001.\n[36] J. Glasenapp, To Pray without Ceasing: A Diachronic\nHistory of Cistercian Chant in the Beaupré An-\ntiphoner (Baltimore, Walters Art Museum, W. 759–\n762). Columbia University, 2020.[37] A. Gelman and D. B. Rubin, “Inference from iterative\nsimulation using multiple sequences,” Statistical\nScience , vol. 7, no. 4, pp. 457–472, 1992. [Online].\nAvailable: http://www.jstor.org/stable/2246093\n[38] F. Ronquist, P. van der Mark, and J. P. Huelsen-\nbeck, Bayesian phylogenetic analysis using MRBAYES ,\n2nd ed. Cambridge University Press, 2009, p.\n210–266.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n578"
    },
    {
        "title": "TapTamDrum: A Dataset for Dualized Drum Patterns.",
        "author": [
            "Behzad Haki",
            "Blazej Kotowski",
            "Cheuk Lun Isaac Lee",
            "Sergi Jordà"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265237",
        "url": "https://doi.org/10.5281/zenodo.10265237",
        "ee": "https://zenodo.org/records/10265237/files/000012.pdf",
        "abstract": "Drummers spend extensive time practicing rudiments to develop technique, speed, coordination, and phrasing. These rudiments are often practiced on \"silent\" practice pads using only the hands. Additionally, many percussive instruments across cultures are played exclusively with the hands. Building on these concepts and inspired by Einstein's probably apocryphal quote, \"Make everything as simple as possible, but not simpler,\" we hypothesize that a dual-voice reduction could serve as a natural and meaningful compressed representation of multi-voiced drum patterns. This representation would retain more information than its corresponding monotonic representation while maintaining relative simplicity for tasks such as rhythm analysis and generation. To validate this potential representation, we investigate whether experienced drummers can consistently represent and reproduce the rhythmic essence of a given drum pattern using only their two hands. We present TapTamDrum: a novel dataset of repeated dualizations from four experienced drummers, along with preliminary analysis and tools for further exploration of the data.",
        "zenodo_id": 10265237,
        "dblp_key": "conf/ismir/HakiKLJ23",
        "keywords": [
            "drummers",
            "rudiments",
            "technique",
            "speed",
            "coordination",
            "phrasing",
            "percussive instruments",
            "hand-playing",
            "Einsteins quote",
            "dual-voice reduction"
        ],
        "content": "TAPTAMDRUM: A DATASET FOR DUALIZED DRUM PATTERNS\nBehzad Haki Bła ˙zej Kotowski Cheuk Lun Isaac Lee Sergi Jordà\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\nbehzad.haki@upf.edu, blazej.kotowski@upf.edu, clilee@connect.ust.hk, sergi.jorda@upf.edu\nABSTRACT\nDrummers spend extensive time practicing rudiments to\ndevelop technique, speed, coordination, and phrasing.\nThese rudiments are often practiced on \"silent\" practice\npads using only the hands. Additionally, many percus-\nsive instruments across cultures are played exclusively\nwith the hands. Building on these concepts and inspired\nby Einstein’s probably apocryphal quote, \"Make every-\nthing as simple as possible, but not simpler,\" we hypoth-\nesize that a dual-voice reduction could serve as a natu-\nral and meaningful compressed representation of multi-\nvoiced drum patterns. This representation would retain\nmore information than its corresponding monotonic rep-\nresentation while maintaining relative simplicity for tasks\nsuch as rhythm analysis and generation. To validate this\npotential representation, we investigate whether experi-\nenced drummers can consistently represent and reproduce\nthe rhythmic essence of a given drum pattern using only\ntheir two hands. We present TapTamDrum: a novel dataset\nof repeated dualizations from four experienced drummers,\nalong with preliminary analysis and tools for further ex-\nploration of the data.\n1. INTRODUCTION\n1.1 Motivation\nMusic is a fundamental aspect of human culture, and\nrhythm is a central element of musical expression. Many\ndifferent cultures have developed complex and sophisti-\ncated rhythmic traditions that are deeply rooted in their\nhistory, language, and social structures [1]. Representing\nmusic, and more speciﬁcally rhythm, in a symbolic form\nis crucial for a wide range of purposes, including com-\nmunication, and preservation of musical traditions. More-\nover, symbolic representations of music are essential for\nenabling the efﬁcient processing and manipulation of mu-\nsical data by computers, enabling new possibilities for mu-\nsic analysis and creation. However, representing complex\nrhythmic patterns in a notation system that accurately cap-\ntures their essence and feeling can be a challenging task,\n© F. Author, S. Author, and T. Author. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: F. Author, S. Author, and T. Author, “TapTamDrum: A\nDataset for Dualized Drum Patterns”, in Proc. of the 24th Int. Society for\nMusic Information Retrieval Conf., Milan, Italy, 2023.particularly when the rhythms are highly complex or in-\nvolve multiple layers of interlocking patterns.\nSome attempts on representing percussion in a domain\nspeciﬁc-way have been made, initially focusing on a sin-\ngle stream of onsets [2]. In [3], Toussaint offers a holis-\ntic analysis of rhythmic patterns, with an approach largely\ncentred around monotonic representations. Representing\na rhythmic pattern in a monotonic stream of offsets has\nindeed proven to be successful for some tasks like trans-\nforming a sequence of taps to a fully-orchestrated percus-\nsive pattern in GrooV AE [4]. However, while a rhythmic\npattern reduced to a monotonic stream of its onsets re-\ntains part of its horizontal structure related to temporality,\nit also loses its vertical quality, which is related to the in-\nterplay between different voices [3]. As a result, a mono-\ntonic pattern transformation fails to capture the complete\nessence of a multi-voice rhythm, as noticed by Lartillot and\nBruford [5]. Inspired by Einstein’s probably apocryphal\nquote, \"Make everything as simple as possible, but not sim-\npler,\" in this work we hypothesise that a dual-voice reduc-\ntion could serve as a more natural and meaningful com-\npressed representation of multi-voiced drum patterns than\nits monotonic equivalent, and that this representation could\npreserve some quality related to the interaction or tension\nbetween instruments, while maintaining relative simplicity\nfor tasks such as rhythm analysis and generation.\nThe rationale behind this approach can also be related to\nthe role of the human motor system in rhythm perception.\nThe importance of the motor system in rhythm creation and\nperception is rooted in the fact that many musical rhythms\nare based on movements that involve two limbs. It was\npreviously documented that the synchronisation of move-\nment to a musical pulse happens automatically, whether of\nthe hand movements [6, 7], walking [8], or dancing [9]. A\nnumber of studies prove that motor areas of the brain play\na signiﬁcant role in beat perception and synchronisation.\nAs Patel and Iversen argue in [10], the ability to coordi-\nnate two limbs in a synchronised and precise manner is\nessential for playing musical instruments and dancing to\nmusic. This is exempliﬁed by the tradition of drumming,\nwhich typically involves striking a drum with two hands.\nThe renowned drummer Jaki Liebezeit, even simpliﬁed his\ndrum kit to only require two hands to play, and developed\na rhythmic notation system called E-T that employs only\ntwo symbols to represent any rhythm, accommodating any\nmusical situation [11]. Thus, the use of two limbs in mu-\nsic and rhythm is not only a fundamental aspect of motor114coordination, but also a practical consideration in the rep-\nresentation and communication of musical rhythms.\nA bell pattern is a two-voiced pattern frequently used\nin West African music, which is usually used as a key pat-\ntern to suggest a temporal organisation for different instru-\nments and musicians [12]. This pattern is used from the\nSub-Sahara, to West Africa, to the central lands of Congo\nand the Nyusa lands in Southeast Africa [13]. Agawu\n[14] claims that the notoriously complex rhythms in West\nAfrican music can be represented with a bell pattern, which\ncould usually be played with two-voiced percussion instru-\nments. This African tradition migrated to the new world\nduring the colonial period. Rooted in the Caribbeans, the\nClave cross-rhythm also splits the beat into two voices, one\nregular beat and another irregular [15].\nAdditionally, subjective rhythmization is a widely rec-\nognized phenomenon in auditory perception [16], which\noccurs when individuals are exposed to monotonous au-\nditory stimuli such as the ticking of a clock. Rather\nthan hearing a simple \"tick-tick-tick\" pattern, our brains\ntransform the sound into a more complex rhythmic se-\nquence, such as \"tick-tock-tick-tock,\" comprising two dis-\ntinct parts. This naturally occurring cognitive process en-\nhances the subjective perception of rhythm and makes the\nstimulus more engaging and dynamic to the listener. The\nconcept of rhythm streams, which is linked to the theory of\nauditory streaming, is examined by Witek et al. [17]. Their\nresearch reveals that the addition of a single instrumental\ncomponent to a monotonic rhythmic pattern can greatly in-\nﬂuence its perceived rhythm, whereas the addition of an-\nother component to a two-instrument pattern has a dis-\ncernible impact only in certain instrumentation contexts.\nFinally, Lartillot and Bruford [5] argue that any rhythm can\nbe reduced to an oscillation between two states: high and\nlow. Their rule-based system transforms multi-voice pat-\nterns into a monotonic stream of timed events representing\nthe toggles between the states and their accentuations.\nAll the aforementioned evidences provide support for\nthe initial hypothesis that simpliﬁed, dual-voice represen-\ntations of multi-voice rhythmic patterns may be adequate\nin communicating both the vertical and horizontal charac-\nteristics of rhythm.\n1.2 Rhythm Pattern Dualization\nTo formalize this idea, we introduce the novel concept of\ndualization of rhythm patterns. The task of rhythm dualiza-\ntion could be deﬁned as the transformation of any multi-\nvoice rhythmic pattern to another pattern composed of a\nmaximum of two voices, while preserving the coherence\nand the perceptual essence of the original rhythm as much\nas possible. Dualization involves simplifying and high-\nlighting the most essential features of complex rhythmic\npatterns. This dualized representation can be viewed as a\nform of abstraction, in which the most essential features\nof a rhythm are distilled and represented in a way that is\neasier to process, grasp, and perform.\nDualization could also enhance the creative and ex-\npressive potential of contemporary musicians, by provid-ing them with a tool for exploring and adapting traditional\nrhythmic patterns in new and innovative ways. Moreover,\nthe study of dualization can shed light on the cognitive and\nneural mechanisms involved in rhythm perception and per-\nformance, as well as the cultural and historical factors that\nshape rhythmic traditions. By introducing this novel con-\ncept, we hope to stimulate further research and innovation\nin the ﬁeld of rhythm notation, and to advance our under-\nstanding of the cognitive, cultural, and creative processes\ninvolved in rhythmic expression.\nIn the next section, we present the dualization exper-\niments we have conducted with the participation of four\nhighly skilled professional drummers to validate our hy-\npothesis. We introduce the dataset that emerged from these\nexperiments, which serves as a valuable resource for our\nanalysis. In Section 3, we delve into a detailed analysis\nof this dataset, shedding light on key ﬁndings and insights.\nSubsequently, in Section 4, we highlight potential applica-\ntions and promising avenues for future research.\n2. METHODOLOGY\nWhile monotonic representations of multi-voiced rhythms\ncan be obtained by simply ﬂattening any rhythmic pat-\ntern, the process of dualizing a multi-voiced rhythm ap-\npears to be less straightforward. To further support our\nhypothesis that dualized patterns can serve as a natural\nand more meaningful compressed representation of multi-\nvoiced drum patterns compared to their monotonic coun-\nterparts, we aimed to investigate whether there is a level\nof consensus or consistency in how multi-voiced patterns\ncan be dualized. To this end, we recruited 4 professional\ndrummers and conducted various dualization exercises, ex-\nploring different approaches and gathering insights into the\nprocess of dualization.\n2.1 Preparation of Rhythmic Material\nTo ensure a diverse range of rhythms for dualization, we\nutilized Magenta’s Groove MIDI Dataset [4]. This dataset\ncomprises of 13.6 hours of live recordings performed on a\nRoland TD-11 electronic drum kit, each labeled with beat\ntype (i.e. \"beat\" or \"ﬁll\"), time signature, and style. The\nrecordings are not quantized (neither in time nor velocity)\nand vary in duration and styles. For our work, we focused\non the \"beat\" subset of the dataset resulting in 503 original\nrecordings. In this subset, only twelve performances were\nnot in 4/4 meter, and subsequently, we dismissed them. Ta-\nble 1 summarizes the distributions of styles within the ﬁnal\nselected samples.\nRock Funk Latin Jazz Hip-hop Other\nCount 124 42 41 34 25 79\nPercentage 36% 12% 12% 10% 7% 23%\nTable 1 . Style distributions within the 345 2-bar samples\nselected from GMD\nThe original GMD recordings vary from several sec-\nonds to a few minutes. As a result, for each session weProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n115selected a single 2-bar segment with the highest total co-\nsine similarity with every other 2-bar segments within the\nsession (similarity was calculated between fully quantized\npatterns). This approach ensured that we had a diverse and\nrepresentative set of 2-bar rhythms from different sessions\nand styles for our dualization experiments. These repre-\nsentative segments were further processed to exclude pat-\nterns that contained only 2 or less voices (which would\nhave made the dualization task trivial). This left us with a\nﬁnal set of 345 individual meaningful and non-trivial 2-bar\nloops, suitable for our dualization experiments.\n2.2 Data Collection Sessions\nWe hired 4 drummers with expertise in Western drumming\ntradition, two of them with conservatory experience, as\nshown in Table 2. Each drummer participated in several\nindividual sessions of approximately one hour each. The\nexperiments consisted of playing 2-bar multi-voice drum\nloops to the participants and asking them to concurrently\nperform dualized versions using only drum sticks on a\nRoland HandSonic HPD-15 MIDI drum pad. Participants\nwere instructed to only hit the left region of the drum pad\nwith the left hand and vice-versa.\nP1 P2 P3 P4\nAge 41 20 24 22\nExperience (Years) 25 11 10 9\nDominant Hand Left Left Right Right\nTable 2 . Demographics of the participants\nFor each of the data collection sessions, an Ableton\nLive1project was prepared in advance, containing the\ndrum patterns to be dualized at their original tempo, with-\nout any velocity or timing quantization. As shown in Fig-\nure 1, the sessions were set up such that the participant\nwould listen to a looping 2-bar pattern and after 1 or 2 rep-\netitions would start to concurrently play their dualized in-\nterpretations of the pattern. The participants were allowed\nto continue their dualizations for as many repetitions as\ndesired until they were conﬁdent in the accuracy of the du-\nalization. All the drum patterns in all of the sessions were\nsynthesized using a single sound source.2No auditory\nfeedback of dualizations were provided to the participants\nexcept for the acoustic sound of the drum pad.\nFigure 1 . Set up of the dataset collection sessions\n2.3 Post Session Questionnaire and Interviews\nAfter the ﬁrst recording session, each participant was given\na questionnaire and an open interview was also conducted.\n1www.ableton.com\n2Neutral preset of Addictive Drums 2’s Fairfax sound packThe questionnaire comprised of three sections: (1) general\ninformation about the participant, (2) assessment of the in-\ntuitiveness of the task and conﬁdence in their performance,\nand (3) exploration of how various rhythmic factors and\nmetrics (e.g., number of instruments, tempo, genre, den-\nsity, syncopation, mostly extracted from [18, 19]) could\npotentially inﬂuence the dualization process. Parts 2 and 3\nof the questionnaire utilized a 7-point Likert scale ranging\nfrom 0 to 6. Results from these two parts are summarized\nin Table 3.\nP1 P2 P3 P4 Avg.\nGeneral Impressions\nIntuitiveness 4 5 6 5 5\nConﬁdence 6 3 4 4 4.25\nInﬂuence of Rhythmic Features*\nNo. of Instruments 0 6 5 5 4\nBeat Division 0 4 3 5 3\nTempo 0 4 3 2 2.25\nStyle 6 3 5 6 5\nFamiliarity with Style 6 6 6 5 5.75\nSyncopation-ness 6 5 5 1 4.25\nDynamics 6 4 3 4 4.25\nNote Density 6 4 5 4 4.75\nLMH Distribution 6 3 2 5 4\nLMH Syncopation-ness 6 2 4 1 3.25\nLMH Dynamics 6 4 3 4 4.25\nLMH Density 3 4 4 6 4.25\nTable 3 . Summary of the questionnaires.\n* refers to the perceived importance that different rhythmic\nfeatures have on the dualizations. LMH refers to the Low,\nMid and High frequency regions. Descriptors taken from\n[18, 19].\nAdditionally, after the questionnaire, open interviews\nwere conducted with each participant. In the following\nsections, we provide a summary with some of the more\nrelevant an recurrent topics discussed.\n2.3.1 Meaningfulness, Replicability and Universality\nAll four participants unanimously agreed that the concept\nof dualization is valid and meaningful. Despite one of them\nadmitting to not having thought much about it before, they\nall found the concept intuitive and useful after the session.\nDespite that the patterns in the introductory sessions were\nrandomly repeated without notifying the participants, they\nwere able to notice the duplication of the same 2-bar tracks\nin the session, and they strongly believed that even though\ntheir corresponding dualized patterns might not be identi-\ncal, they would likely share a high similarity. While the re-\nsults were not entirely conclusive (as discussed in the next\nsection), all participants also expressed conﬁdence in hav-\ning similar mindsets as their peers while dualizing the same\nrhythm. Participant 1 justiﬁed this idea by referencing the\ndevelopment of the Western percussive tradition from the\nsnare drum, stating that fundamentally, drummers listen to\nkey elements represented by the snare.\n2.3.2 The Effects of Style and Repetition\nOne participant stated that genre greatly inﬂuences which\nvoices to focus on; for example, in the case of rock, the\nsnare and the bass drum would more often be followed,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n116whereas in jazz-inﬂuenced styles, the hi-hat and the ride\ncymbal could probably be included in the reduction, as\nthey emphasise lay back and swing. However, not all par-\nticipants agreed on interpreting dualization solely as the\nreplication of the two most prominent voices. Some be-\nlieved that they needed to ﬁrst digest and understand each\nrhythm, before extracting its essential form for dualization.\nTwo participants also mentioned that listening to the same\npattern repeatedly in the experiment promoted a \"dualiza-\ntion reﬁnement\", listening more deeply to each iteration\nand thus reﬁning the extraction of the pattern’s rhythmic\nessence.\n2.3.3 Problems with the Recording Sessions\nParticipants reported some issues during the recording ses-\nsions, such as using always the same VSTI for all pat-\nterns, independently of their musical style. Also, the use\nof Roland HPD-15, which has a single surface with mul-\ntiple pads, seems to encourage some drummer habits such\nas paradiddles, and some participants would have preferred\na separated two-pad device. Some tracks were inade-\nquately selected for a 4/4 based recording as Participant\n1 interpreted them as a 6/8 time signature, even though\nGrooveMIDI labelled them as in 4/4. For some swing and\nshufﬂed rhythms sometimes the last semiquaver was cut.\nParticipants were also aware of the style bias and of the\nabundance of Western inﬂuenced styles speciﬁcally rooted\nfrom a snare drum tradition.\n3. DATASET AND ANALYSIS\nIn this section, we will start with providing an overview of\nthe dataset. Subsequently, we will provide a preliminary\nanalysis of the collected dualizations. The objective of this\nanalysis is to establish whether there is any validity to the\nhypothesis that professional drummers are able to dualize\ndrum patterns with some level of consistency, so as to es-\ntablish whether further research on this topic is warranted.\nDuring the dataset collection sessions, we had a limited\ntime span during which all four participants were available.\nFor the initial session, we had access to all four partici-\npants. This session was set up in a manner that would allow\nus to investigate two main questions: (1) whether a single\ndrummer dualizes a given pattern consistently at different\ntimes (Intra Participant consistency) and (2) how consis-\ntently different drummers dualize the same drum pattern\n(Inter Participant consistency). As such, we selected 24\ndrum patterns to be presented randomly three times to each\nof the four drummers without notifying them about the rep-\netitions (Subset A1). During the ﬁrst stage of the data\ncollection sessions, Participant 1 was able to participate\nlonger, as a result, 48 more drum patterns were dualized\nby him in the same manner (Subset A2).\nDuring the A2 sessions, Participant 1 notiﬁed us that\nhe was aware that we were testing a single drum pattern\nmultiple times. Following this comment, he told us that,\nif needed, he can dualize the drum patterns in two differ-\nent ways, in his own terms, in a “simple” and a “complex”\nmanner. Inspired by this comment, we conﬁrmed with theother participants about this view of dualization. Subse-\nquently, we modiﬁed the remaining sessions so as to ex-\nplore whether there is any consistencies among the “sim-\nple” and “complex” dualizations. We were able to partially\nconduct these sessions with Participants 1 and 2 (Subset\nB1), while the remaining sessions were only conducted\nwith Participant 1. Table 4 summarizes the collected data.\nThe ﬁnal dataset consists of 1116 dualizations obtained\nfrom the participants. These dualizations were obtained\nfrom the set of 345 unique drum patterns. Moreover, each\nof the drum patterns were used in a single dualization test,\nthat is, for example, a drum pattern used in Subset A1 was\nnot reused in the A2, B1 and B2 subsets. The decision to\nnot reuse the drum patterns in different subsets was made\nto maximize the number of drum patterns for which at least\none set of dualizations were available in the ﬁnal dataset.\nIn Section 3.1, we will use the collected dataset to pro-\nvide a preliminary analysis on whether there are any in-\ntra/inter participant consistencies between the dualizations\nobtained at random from all four participants (Subset A1)\n- see Figure 2. Lastly, in Section 3.2, we will compare\nsimple dualizations with their complex counterparts from\nboth intra- and inter-participant perspectives. The analysis\npresented in this section has been done on a binary repre-\nsentation of the dualizations, fully quantized to a 16th note\ngrid. Moreover, it should be noted that, as conﬁrmed with\nthe participants, a given dualization can be reproduced us-\ning an inverse hand combination. As such, to compare\nthe rhythmic similarity between the dualizations, a hand-\nagnostic measure should be employed (e.g. L0R0LLand\nR0L0RR patterns should be treated as identical - LandR\nrefer to Left andRight hand hits at a 16th note time step\nand 0 refers to silence).\nFigure 2 . Inter-/Intra- pairs. For each drum pattern, intra-\npairs are selected from each of the participants. Inter-pairs\nare selected by pairing a participant’s repetition with all\nother repetitions corresponding to the same test.\nTo this end, we focused our analysis on the ﬂattened\nversions of the dualizations, that is, the left and right hand\npatterns were superimposed onto a single sequence (e.g.\n101011 ). While this approach does not explore the func-\ntion of each of the dualized streams, we believe that it is\nvalid as a preliminary investigation, because if the ﬂattened\npatterns show no rhythmic consistency, further analysis on\nthe function of each hand may not be warranted. In other\nwords, the validity of our hypothesis is contingent upon\nthe presence of rhythmic consistency in the dualized pat-\nterns, and further analysis may be needed to determine the\nfunctional aspects of each hand in the dualizations.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n117Subset Tested Drum Patterns Repetitions Per Test Total Dualizations\nP1 P2 P3 P4\nThree Repetitions (A) Multi-Participant (A1) 24 3 3 3 3 288 (24×3×4)\nSingle Participant (A2) 48 3 - - - 144 (48×3×1)\nSimple vs Complex (B) Multi-Participant (B1) 69 2 2 - - 276 (69×2×2)\nSingle Participant (B2) 204 2 - - - 408 (204×2×1)\nTotal 345 762 210 72 72 1116\nTable 4 . Summary of the collected dataset\n3.1 Three Repetitions (A1 Session, Participants 1-4)\nIn Subset A1, we presented 24 drum patterns3to each\nof the four participants three times in a random order, re-\nsulting in a total of 288 obtained dualizations. We ﬁrst\nexamine the consistency of each participant’s dualizations\nover the three repetitions (intra-participant analysis), and\nthen we investigate the consistency of each of dualizations\nwith their counterparts4from other participants (inter-\nparticipant analysis). To establish the similarity between\ntwo given patterns, we used the Jaccard similarity measure,\ndeﬁned as the ratio of the overlap of two sequences divided\nby the union. Moreover, to establish the perceptual simi-\nlarity of the dualizations, we used Edit Distance [20, 21],\ndeﬁned as the minimum number of operations (insertions,\ndeletions, or substitutions) required to transform one se-\nquence into another. Figures 3 and 4 summarize the results\nof inter/intra-participant analysis using a pair-wise com-\nparison. In both cases, in order to establish a baseline com-\nparison, we also calculate the Edit and Jaccard values for\nthe same number of pairs randomly selected (each random\npair comes from dualizations obtained from two randomly\nselected participants and are ensured not to be associated\nwith the same drum pattern).\nFigure 3 . Intra-Participant Analysis of Subset A1\nThe results of the intra-participant distributions (Figure\n3) show that the Edit distances are smaller for any of the\nfour participants’ repetitions compared to the distance be-\ntween randomly paired dualizations. Similarly, the Jaccard\nsimilarities are also higher than the random pairs.\nUnlike the intra-participant distributions, the edit dis-\ntances for inter-participant dualizations have some overlap\nwith the random pairs. This overlap is also observed in the\nJaccard similarity values. However, despite this overlap,\nthe inter-participant distributions still show a trend towards\nhigher similarity values compared to the random pairs. The\nlower consistency between inter-participant dualizations,\ncompared to the intra-participant dualizations may be an\nindicator that experienced drummers have a consistent du-\n3latin: 4, hiphop: 3 , jazz: 3, rock: 3, funk: 2, soul: 2, afrobeat: 1,\nafrocuban: 1, dance: 1, neworleans: 1, pop: 1, punk: 1, reggae: 1\n4dualizations obtained from the same drum pattern tested\nFigure 4 . Inter-Participant Analysis of Subset A1\nalized interpretation of rhythms, however, these interpre-\ntations vary to some extent compared to other drummers.\nWhile this is a possibility, we believe more comprehen-\nsive analysis is required prior to making this conclusion as\nthere are a number of limitations to the approach taken in\nthis paper.\nThe current state of our analysis imposes several restric-\ntions and simpliﬁcations that limit the depth of our study\nof the dataset. Firstly, it is constrained to the ﬂattened ver-\nsions of the dualizations. Secondly, it dismisses (quan-\ntizes) the velocity and micro-timing information, which\nare important factors in drumming that can affect nuances\nand groove. Experienced drummers often utilize velocity\nand micro-timing to add expressiveness to their playing,\npotentially leading to differences in the perceived rhythm.\nTherefore, a comprehensive investigation of the dualiza-\ntions should incorporate these dimensions for a more nu-\nanced understanding.\n3.2 Simple/Complex (B Sessions, Participant 1)\nAs mentioned previously, Participant 1 pointed in the open\ninterview that a dualization can be done in different man-\nners: simple and complex. For further exploring this idea,\nin a second phase of the dataset collection sessions, two\nparticipants were asked to ﬁrst dualize a drum pattern in a\nsimple manner, and also immediately after, re-dualize the\nsame pattern in a more complex manner. These sessions\nwere partially conducted with Participants 1 and 2 (session\nB1), and more tests were done using Participant 1 (session\nB2). For the sake of brevity, we focus the analysis of this\nsubset on Participant 1 (a total of 273 paired simple and\ncomplex dualizations).\nFigure 5 shows that a major distinction between the\nsimple and complex dualizations of Participant 1 is that\nthe simple versions are considerably less active than their\ncomplex counterparts. One interesting observation is that\nthe activity level of the dualizations obtained from Partic-\nipant 1 during the ﬁrst session (A1) (in which this sim-\nple/complex distinction had not been introduced) are more\non par with the complex dualizations. This observationProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n118Figure 5 . Step Densities. Left two distributions corre-\nspond to the 273 simple and complex dualizations obtained\nfrom P1 in sessions B1 and B2; remaining four correspond\nto 72 dualizations per each participant in A1\nalong with the step density distributions for Participants\n2-4 raises a question that perhaps, unless restricted, the\ndrummers default to more active dualizations. This ten-\ndency may be explainable from two perspectives: (1) in\nmost styles, the hands are generally highly active as they\nare responsible for playing the majority of the drum kit,\nand (2) the dualizations may be by default more biased\ntowards the rudiments that many drummers strenuously\npractice using their hands.\nTo further analyze the simple and complex dualizations,\nwe calculated the edit distance and Jaccard distance be-\ntween each pair of dualizations (see Figure 6). Similar to\nsection 3.1, to establish a baseline comparison, we used\nan equal number of uncorrelated simple and complex pairs\nfrom Participant 1 dualizations. The results here show that,\nwhile lower in general, the distribution of the edit distances\nbetween the simple and complex pairs are partially over-\nlapping. Moreover, this distribution is higher than the in-\ntra repetition distribution in Subset A1. These trends are\nalso evident in the Jaccard similarity distributions. These\ndifferences relative to the intra distances in subset A1 are\nfully expected knowing that the dualizations are intention-\nally varied in this experiment.\nFigure 6 . Distributions of distances/similarities between\nall of the Participants simple dualizations and their corre-\nsponding complex dualizations\n4. APPLICATIONS AND FUTURE WORK\nThe majority of our work has been focused on data collec-\ntion and curation, as well as preparing tools to allow the\ncommunity to easily explore and study the data. All the\ncollected data have been carefully processed and organized\nfor both A and B subsets. Moreover, we have prepared an\naccompanying website that enables researchers to listen to\nall paired repetitions while also visualizing the piano rolls.\nLastly, we have also developed an open-source API that\nallows for easy access to the data, visualization, synthesis,and analysis using both pre-implemented and third-party\ntools of interest. All these resources are publicly available\nathttps://taptamdrum.github.io .5\nWe envision that the dataset can be used in a variety of\nstudies. Rhythm reduction studies could use it to exam-\nine the simpliﬁcation of multi-voiced patterns into dual-\nvoiced ones. The dataset can also be used to develop\ncomputational models for drum pattern reduction. These\nmodels would be highly valuable as they allow for eas-\nier study of polyphonic drum patterns. Moreover, know-\ning that the dualizations of a single participant for a given\ndrum pattern are highly correlated (i.e. they are percep-\ntual interpretations of the same pattern), researchers can\nvalidate whether the features extracted from the dualiza-\ntions are also highly correlated. This would provide a\nway to evaluate the effectiveness of rhythm feature ex-\ntraction algorithms and potentially improve them. Sim-\nilarly, to establish the perceptual relevance of rhythmic\ndistance/similarity measures, researchers can use the sim-\nple/complex subset of the dataset to ensure that the pro-\nposed measures result in reasonable distances that correlate\nwith the perceptual re-interpretations of a given pattern.\nLastly, the dataset can also be used for drum generation\ntasks. For instance, a generative model could be devel-\noped so as convert a dual sequence into a full drum pat-\ntern (similar to single voice rhythm into multi-voice drum\ngenerators [4, 22]). Such generative models can be used\nin many creative ways as during the inference stage, each\nof the left/right streams fed into the drum generator can\nbe extracted from separate instruments/sources. Moreover,\nthe random repetitions and the simple/complex repetitions\ncan be used in developing deep metric learning models that\nrely on paired training samples.\n5. CONCLUSIONS\nIn this work, we presented TapTamDrum, a novel dataset\nconsisting of 1116 dualizations of drum patterns performed\nby four experienced drummers, covering 345 unique drum\npatterns selected from Magenta’s GrooveMIDI dataset.\nThe analysis conducted in section 3.1 provides valuable in-\nsights into the dataset. Firstly, it shows that there are intra-\nparticipant consistencies in the dualizations. That said, the\ninter-participant analysis are less deﬁnitive and require fur-\nther detailed investigation. Moreover, the simple/complex\ncomparisons (section 3.2) show that the complex dualiza-\ntions are signiﬁcantly more active than the simple ones\nwhile adhering to some level of rhythmic consistency with\ntheir simple counterparts. The analysis conducted in this\nwork was preliminary and limited and has not explored the\nfull potential of the dataset. The main focus of this work\nwas to collect, curate, organize the dataset and also pro-\nvide resources for prompt exploration of the data. To this\nend, we have prepared an accompanying website and an\nopen-source API. Finally, this dataset can be used in a va-\nriety of rhythm related studies ranging from perception to\ngeneration.\n5https://github.com/taptamdrum/datasetProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1196. ACKNOWLEDGMENTS\nThis research was partly funded by the Ministry of Sci-\nence and Innovation of the Spanish Government. Agen-\ncia Estatal de Investigación (AEI). (Reference: PID2019-\n111403GB-I00).\n7. REFERENCES\n[1] J. Powell, “What is temporal art? a persistent question\nrevisited,” Contemporary Aesthetics , vol. 13, 2015.\n[2] W. T. Berry, Structural functions in music. Courier\nCorporation, 1987.\n[3] G. T. Toussaint, The geometry of musical rhythm: What\nmakes a \"Good\" rhythm good? CRC Press, 2016.\n[4] J. Gillick, A. Roberts, J. Engel, D. Eck, and D. Bam-\nman, “Learning to groove with inverse sequence trans-\nformations,” in Proc. of the 36th International Con-\nference on Machine Learning , California, USA, May\n2019, pp. 2269–2279.\n[5] O. Lartillot and F. Bruford, “Bistate reduction and\ncomparison of drum patterns,” in Proc. of the 21st\nInternational Society for Music Information Retrieval\nConference . Montreal, Canada: ISMIR, Oct. 2020,\npp. 318–324.\n[6] B. H. Repp and A. Penel, “Auditory dominance in\ntemporal processing: New evidence from synchroniza-\ntion with simultaneous visual and auditory sequences.”\nJournal of Experimental Psychology: Human Percep-\ntion and Performance , vol. 28, no. 5, pp. 1085–1099,\n2002.\n[7] P. J. Treffner and M. Turvey, “Handedness and the\nasymmetric dynamics of bimanual rhythmic coordina-\ntion.” Journal of Experimental Psychology: Human\nPerception and Performance , vol. 21, no. 2, p. 318,\n1995.\n[8] L.-A. Leow, C. Rinchon, and J. Grahn, “Familiarity\nwith music increases walking speed in rhythmic audi-\ntory cuing,” Annals of the New York Academy of Sci-\nences , vol. 1337, no. 1, pp. 53–61, 2015.\n[9] M. Leman, D. Moelants, M. Varewyck, F. Styns, L. van\nNoorden, and J.-P. Martens, “Activating and relaxing\nmusic entrains the speed of beat synchronized walk-\ning,” PLoS ONE , vol. 8, no. 7, p. e67932, Jul. 2013.\n[10] A. D. Patel and J. R. Iversen, “The evolutionary neuro-\nscience of musical beat perception: the action simula-\ntion for auditory prediction (ASAP) hypothesis,” Fron-\ntiers in Systems Neuroscience , vol. 8, May 2014.\n[11] J. Podmore, Jaki Liebezeit: The Life, Theory And Prac-\ntice Of A Master Drummer . Unbound, 2020.[12] E. D. Novotney, “The 3:2 relationship as the founda-\ntion of timelines in West African musics,” Ph.D. dis-\nsertation, University of Illinois at Urbana-Champaign,\n1998.\n[13] G. Kubik, Africa and the Blues . University Press of\nMississippi, 1999.\n[14] K. Agawu, The African Imagination in Music . Oxford\nUniversity Press, 04 2016.\n[15] D. Peñalosa and P. Greenwood, The Clave Matrix:\nAfro-Cuban Rhythm : Its Principles and African Ori-\ngins, ser. Unlocking clave. Bembe Books, 2012.\n[16] R. Bååth, “Subjective Rhythmization: A Replication\nand an Assessment of Two Theoretical Explanations,”\nMusic Perception , vol. 33, no. 2, pp. 244–254, 12 2015.\n[17] M. A. G. Witek, E. F. Clarke, M. L. Kringelbach, and\nP. Vuust, “Effects of Polyphonic Context, Instrumenta-\ntion, and Metrical Location on Syncopation in Music,”\nMusic Perception , vol. 32, no. 2, pp. 201–217, 12 2014.\n[18] D. Gómez-Marín, “Similarity and style in electronic\ndance music drum rhythms,” Ph.D. dissertation, Pom-\npeu Fabra University, Spain, 2018.\n[19] D. Gómez-Marín, S. Jordà, and P. Herrera, “Drum\nrhythm spaces: From polyphonic similarity to gener-\native maps,” Journal of New Music Research , vol. 49,\nno. 5, pp. 438–456, 2020.\n[20] O. Post and G. Toussaint, “The edit distance as a mea-\nsure of perceived rhythmic similarity,” Empirical Mu-\nsicology Review , vol. 6, no. 3, pp. 164–179, 2011.\n[21] M. Moritz, M. Heard, H.-W. Kim, and Y . S. Lee, “In-\nvariance of edit-distance to tempo in rhythm similar-\nity,” Psychology of Music , vol. 49, no. 6, pp. 1671–\n1685, 2021.\n[22] B. Haki, M. Nieto, T. Pelinski, and S. Jordà,\n“Real-Time Drum Accompaniment Using Transformer\nArchitecture,” in Proceedings of the 3rd Conference\non AI Music Creativity . AIMC, Sep. 2022. [Online].\nAvailable: 10.5281/zenodo.7088343Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n120"
    },
    {
        "title": "Finding Tori: Self-Supervised Learning for Analyzing Korean Folk Song.",
        "author": [
            "Danbinaerin Han",
            "Rafael Caro Repetto",
            "Dasaem Jeong"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265319",
        "url": "https://doi.org/10.5281/zenodo.10265319",
        "ee": "https://zenodo.org/records/10265319/files/000052.pdf",
        "abstract": "In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.",
        "zenodo_id": 10265319,
        "dblp_key": "conf/ismir/HanRJ23",
        "keywords": [
            "computational analysis",
            "field recording dataset",
            "Korean folk songs",
            "non-expert musicians",
            "convolutional neural network",
            "pitch contour",
            "tori classification",
            "scale",
            "ornamental notes",
            "idiomatic melodic contour"
        ],
        "content": "FINDING TORI: SELF-SUPERVISED LEARNING FOR ANALYZING\nKOREAN FOLK SONG\nDanbinaerin Han1Rafael Caro Repetto2Dasaem Jeong1\n1Department of Art & Technology, Sogang University2Kunstuniversität Graz\nnaerin71@sogang.ac.kr, rafael.caro-repetto@kug.ac.at, dasaemj@sogang.ac.kr\nABSTRACT\nIn this paper, we introduce a computational analysis of\nthe ﬁeld recording dataset of approximately 700 hours of\nKorean folk songs, which were recorded around 1980-\n90s. Because most of the songs were sung by non-expert\nmusicians without accompaniment, the dataset provides\nseveral challenges. To address this challenge, we utilized\nself-supervised learning with convolutional neural network\nbased on pitch contour, then analyzed how the musical\nconcept of tori, a classiﬁcation system deﬁned by a speciﬁc\nscale, ornamental notes, and an idiomatic melodic contour,\nis captured by the model. The experimental result shows\nthat our approach can better capture the characteristics of\ntori compared to traditional pitch histograms. Using our\napproaches, we have examined how musical discussions\nproposed in existing academia manifest in the actual ﬁeld\nrecordings of Korean folk songs.\n1. INTRODUCTION\nFolk songs are considered as a musical language. Not only\nthat, but they are also regarded as the root of all traditional\nmusic. Folk songs are potent embodiments of a region’s\ncultural and linguistic characteristics, serving as a founda-\ntion for all artistic and musical developments since their in-\nception. It is believed that research for folk music can pro-\nvide new inspiration for existing art music research, and\nfacilitate an interdisciplinary approach that encompasses\nboth music and other ﬁelds.\nIn Korean musicology, there have been ongoing discus-\nsions aimed at identifying regional characteristics in folk\nmusic. Through numerous debates, toriis used as the most\nrepresentative theory in many studies. Even though the use\nof tori as a term to describe the musical characteristics of\nKorean folk music is widespread and its utility is well ac-\nknowledged, the existing tori classiﬁcation methods were\nunable to fully explain the features of the actual music.\nVarious scholars have deﬁned the different tori according\nto musical characteristics such as scales (intervals between\nnotes), primary notes, ornamentation, ending note and id-\n© D. Han, R. Caro Repetto, and D. Jeong. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: D. Han, R. Caro Repetto, and D. Jeong, “Finding Tori:\nSelf-supervised Learning for Analyzing Korean Folk Song”, in Proc. of\nthe 24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.iomatic melodic patterns in folk songs, leading to a reﬁne-\nment process for the classiﬁcation of tori. So far, there are\nstill ongoing opinions, controversies, and discussions con-\ncerning the existing tori [1, 2].\nIn order to conduct valuable discussions on analyz-\ning folk songs, the relationships within the audio must\nbe examined and systematically shared. However, check-\ning numerous audio ﬁles individually is extremely time-\nconsuming and difﬁcult, so previous research has focused\non analyzing small amounts of audio. The most com-\nmon method used by musicologists has been transcription,\nwhich involves listening to the music and notating it in a\nspeciﬁc music notation system. However, the task of tran-\nscribing orally transmitted music into a music notation sys-\ntem has inherent limitations [3], such as quantizing the\npitch and rhythmic features of folk songs.\nFor these reasons, there have been several research on\nanalyzing folk music empowered with methods derived\nfrom music information retrieval (MIR) on audio record-\ning. For example, Indian raga music has been analyzed\nbased on ﬁrst-order pitch distrubutions [4], and for tradi-\ntional three-part Georgian singing, F0-based tonal analy-\nsis [5] or development of tuning systems [6] has been intro-\nduced. [7] also presented analysis using audio-signal pro-\ncessing on Flamenco and Arab-Andalusian vocal music.\nIn this paper, we take a computational approach to an-\nalyze a vast corpus of Korean folk songs, utilizing deep-\nlearning-based methodologies. Our primary aim is to in-\nvestigate the connection between conventional musicolog-\nical classiﬁcations of Korean folk songs, tori, and the ac-\ntual ﬁeld recordings. Through a comparison of our ana-\nlytical results with established musicological frameworks,\nour objective is to offer insights on identifying meaningful\nclustering or distinguishable features among Korean folk\nsongs. We also share our code and metadata that contains\nlinks to original audio with our manual tori labels for 218\nsongs1.\n2. KOREAN FOLK SONGS AND TORI\n2.1 Dataset\nThe “Anthology of Korean Traditional Folksongs” is an\naudio collection consisting solely of traditional Korean\nfolk songs [8]. As part of a cultural project conducted\nby Munhwa Broadcasting Corporation (MBC) from 1989\n1https://github.com/danbinaerinHan/ﬁnding-tori440to 1995, the folk song collection was compiled under\nthe direction of Sang-il Choi. The available audio con-\ntains 15,861 songs, with approximately 700 hours in total\nlength.\nThe audio was ﬁeld-recorded across 153 city/county\nand 1,010 villages in South Korea. The metadata accom-\npanying the recordings includes title, machine-readable\nlyrics, regions, recording dates, and the singer’s name and\nage. The region represents the administrative districts of\nSouth Korea at the time of recording, which consists of\nnine categories in total. Due to its extensive audio corpus\nand detailed accompanying metadata, this dataset has be-\ncome a crucial resource for research on Korean folk mu-\nsic within the domestic academic community [9–11]. We\nobtained the audio material through crawling the origi-\nnal website,2which has been hosted by MBC since April\n2022, where every audio and metadata is openly published.\nWe received ofﬁcial approval for using these data for re-\nsearch purposes from MBC.\n2.2 Tori\nTori was proposed by a Korean musicologist named Bo-\nhyeong Lee in the early 1980s to explain the musical char-\nacteristics of regional folk songs [12, 13]. Prior to the de-\nvelopment of tori theory, Korean music academia used\nthe terms joandcheong to explain the musical features\nof folk songs, based on the tonal center and intervals be-\ntween notes [14]. However, Bo-hyeong Lee argued that jo\nandcheong are insufﬁcient in capturing the unique mu-\nsical characteristics of Korean folk music. Therefore, he\nproposed the tori classiﬁcation system as an alternative ap-\nproach, which covers primary notes, ornamentation, end-\ning note and idiomatic melodic patterns.\nThe most widely-used tori classiﬁcation method divides\nsongs into four categories. We encourage readers to refer\nFigure 3, which presents scale of three tori with staff nota-\ntion. The pitch name in this section is used as an conven-\ntional representation, not absolute pitches. Gyung-tori uti-\nlizes ﬁve notes and is mainly sung in the capital region. It\noften uses a gentle vibrato overall and ﬁnessed ornamental\nmelody. Menari-tori is widely distributed throughout the\neastern regions and the entire Korean Peninsula. When the\nmelody ascends, it leaps through notes, while during the\ndescending melody, it comes down through passing notes.\nYukjabaegi-tori is commonly found in the southern re-\ngions of the Korean Peninsula. It is characterized by a thick\nand vibrant vibrato in G note and passing brieﬂy through\nEZin a descending melody from E Zto D. Sushimga-tori\ncommonly appears in the northwestern region of the Ko-\nrean Peninsula, characterized by its unique vibrato inﬂect-\ning upward.\n2.3 Tori annotation\nAs the dataset did not include any tori labels, one of the au-\nthors, who has over 10 years of experience in Korean tradi-\ntional music, selected a subset of 218 high-quality record-\n2http://urisori.co.kr/urisori-en/doku.php/\nFigure 1 . Example of pitch contour from the dataset ex-\ntracted by CREPE. F0 value under conﬁdence of 0.8 was\nmasked out\nings and manually annotated them in terms of tori classiﬁ-\ncation.\nTo create this tori-subset, we focused on identifying\nclear musical characteristics present in the tori classiﬁ-\ncation method, such as pitch scale, ornamentation, and\nidiomatic melodies, rather than considering the audio’s\nrecorded region. We found that there are not many in-\nstances of sushimga-tori in the audio dataset, as the dataset\npredominantly consists of folk songs from the central and\nsouthern regions of the Korean Peninsula. Also, we ob-\nserved that songs belonging to menari-tori appeared not\nonly in the eastern region but also nationwide. In conclu-\nsion, we mainly focused on the remaining three types of\ntori, gyung-tori ,yukjabaegi-tori , and menari-tori . Finally,\naudio recordings that were closer to speech or chanting\n(non-musical) and did not exclusively belong to any other\nsubdivided tori types were labeled as ‘others’. We have la-\nbeled totally 218 songs: 65 gyung, 73 menari, 49 yukja,\nand 31 others.\n3. METHODOLOGY\nIn this study, we take an approach of representing the mu-\nsical characteristics of a given folk song using a high-\ndimensional single vector, which can also be called an em-\nbedding of the song. If this embedding accurately repre-\nsents the musical characteristics of tori, it can be utilized\nfor various purposes including tori classiﬁcation, song sim-\nilarity searches and clustering songs in the corpus based on\ntori similarity.\nEven though the dataset is provided with audio record-\nings, we focus on the contour of fundamental frequency\n(F0) rather than using audio directly. By doing so, our em-\nbedding could better capture the melodic features of the\nsongs rather than timbral characterstics such as the dialect\nof the singer. To extract the F0 contour from each audio\nrecording, we utilized CREPE [15], a widely-used CNN-\nbased model. It extracts F0 value for every 10 ms as well\nas conﬁdence score ranging from 0 to 1. Figure 1 illustrates\nan example of an extracted pitch contour.\n3.1 Pitch Histogram\nSince the characteristics of a musical system can be largely\nstudied from the pitch distribution of its melodies, thereProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n441has been numerous research using pitch histograms to an-\nalyze traditional musics from different culture, such as for\nIndian Carnatic music [16], Turkish Makam [17], Arab-\nAndalusian music [18], or Iranian dastg ¯ahimusic [19].\nTherefore, we applied a similar approach to Korean folk\nmusic.\nOne of the important features one has to know to exploit\npitch histogram is the tonic of the song so that one can\nnormalize the pitch histograms of each song into relative\npitch, rather than directly using absolute pitch value. Dur-\ning our preliminary experiment, we found that the tonic of\neach recording in the dataset is usually the most frequently\nappearing pitch in the song. Therefore, we determined the\ntonic by counting the number of appearances of each pitch\nin terms of time frames, utilizing a 100 cents range to count\npitch (i.e. MIDI pitch 60.49 and 59.51 are counted as the\nsame pitch). As the center pitch may differ from the typ-\nical 440 Hz tuning, we identiﬁed the best matching pitch\nby adjusting the pitch center by increments of 10 cents and\nselecting the pitch that showed the maximum pitch count.\n3.2 CNN contour encoder\nWhile the pitch histogram can offer a comprehensible rep-\nresentation of each song, it cannot fully capture the con-\ncept of tori as it does not consider the relationship between\neach note in the scale. Additionally, because most of the\nrecordings were sung by non-professional singers without\ninstrumental accompaniment, the intervals of singing and\ntonic frequency has signifcant amount of noise. Therefore,\nwe employ a convolutional neural network (CNN) to learn\nthe representation of a given pitch contour. This approach\nallows us to better capture the underlying musical charac-\nteristics of each song in relation to tori classiﬁcation.\nThe problem is setting the training objective of the CNN\nmodel. One option is to train the model in a supervised\nmanner, as typically done in other MIR tasks, such as clas-\nsiﬁcation tasks with an annotated dataset. In our dataset,\nregion labels, which represents the administrative region\nwhere the recording took place, can be used for training\nlabels. It can be regarded as a reasonable approach con-\nsidering that tori has strong correlation with the regional\ncharacteristics of each folk song. However, we discovered\nseveral different types of tori in a single recorded region,\nwhich would make it difﬁcult to learn distinguishable mu-\nsical characteristics only using the region label.\nTo address this issue, we propose to adopt a self-\nsupervised representation learning, so that we can obtain\na representation of given pitch contour that is consistent\nwithin a song without extra annotated labels as shown in\nFig. 2. This approach of exploiting intra-song similarity\nhas been widely used for music audio representation learn-\ning [20,21]. We use triplet loss with hinge margin as Equa-\ntion 1,\nL= max(0 ,m−Sim(va,vp)+Sim(va,vn)) (1)\nwheremdenotes hinge margin, Sim (va,vp,n)represents\ncosine similarity between anchor vector vaand positive\nvectorvpor negative vector vn.\nFigure 2 . Self-supervised learning with triplet loss using\ncosine similarity, with four layers of convolutional neural\nnetwork\nAs presented in Fig. 2, the model consists of 4 layers\nof 1D convolution layer, each with a kernel size of three\nand channel size of 64, 128, 256, and 256, respectively,\neach followed by batch normalization layer. We used a max\npooling layer between each convolutional layer with size 5\nfor the ﬁrst layer and 4 for the second and third layers.\nOn top of the convolutional stack, we used context atten-\ntion to summarize the arbitrary length of vectors, which\nwas initially proposed in hierarchical document classiﬁca-\ntion [22], with a slight modiﬁcation of multi-head atten-\ntion weight [23] instead of single-head. Context attention\nis a type of attention mechanism that uses an independent\nlearnable parameter named context vector as a query while\nusing the input vectors as key and value. The ﬁnal embed-\nding size of the model was 256.\nThe input is a pitch contour in the shape of T×2,\nwhereTdenotes the number of time step of pitch contour.\nThroughout the experiment, we used the frame rate of 20\nHz, which means that 30 seconds of pitch contour is con-\nverted to 600 time steps. During the training, we randomly\nslice the contour to have 30 seconds length, while using the\nentire contour in the test or visualization. Two channels are\ntonic-normalized MIDI pitch and conﬁdence of the F0 es-\ntimation at that time frame. Since the automatic F0 estima-\ntion includes noise, especially in the unsung silent part, we\nmasked the estimated pitch value to zero if the conﬁdence\nis lower than 0.8.\n4. EXPERIMENTS\n4.1 Evaluation Design\nIn the experiments, we explored the quantitative rela-\ntion between tori and different embedding schemes to see\nwhich type of embedding can explain the concept of toriProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n442better. To evaluate the correlation between tori and the\ngiven embedding scheme, we exploit our tori-labeled sub-\nset with two metrics.\nThe ﬁrst metric is the ranking of cosine similarity. If the\nembedding shares important characteristics with the con-\ncept of tori, we can expect that songs with high cosine\nsimilarity in the given embedding space share the same\ntori with the query song. Therefore, we calculated the co-\nsine similarity between each song in the tori-subset, and\nobtained normalized discounted cumulative gain (nDCG),\nwhich is a frequently used metric to evaluate the quality of\nsearch results. If all the other nsongs with the same tori are\nranked within n-th order in the similarity, nDCG becomes\none.\nThe second metric is the tori classiﬁcation accuracy us-\ning a random forest classiﬁer. If the embedding includes\nessential characteristics that deﬁne tori, one can expect that\nit can be exploited to classify the tori for a given song.\nAmong many options, we used a simple random forest\nclassiﬁer. The random forest classiﬁer with 100 trees was\ntrained with 75% of the tori-set and tested with the re-\nmaining 25%. For each embedding scheme, we repeated\nthe procedure 30 times with different train/test split and\nreported mean and deviation of accuracy to ensure that the\nresults were not dependent on the speciﬁc dataset split.\n4.2 Training\nWhile a pitch histogram can be obtained from a given\nsong without additional training data, CNN models require\ntraining procedures. We employ F0 contour extracted from\nthe “Anthology of Korean Traditional Folksongs” to train\nour model.\nSome of the songs in the dataset were recorded with\nmultiple singers or percussion accompaniment. Because\nthese can degrade the performance of F0 estimation, we\nexcluded them while training the CNN model. Instead of\nmanually ﬁltering the dataset, we used a CNN-based sound\nevent detection model [24] to calculate the activation of\n‘choir’ and ‘percussion,’ which are included among the\nmodel’s event vocabulary. We ﬁltered the song by the max-\nimum activation value exceeding a certain threshold, which\nwas manually decided by observing the activation distribu-\ntion across the dataset.\nThe CNN model was trained for 25,000 updates. For the\nself-supervised learning using triplet loss, we used eight\nnegative samples with one positive sample, and hinge loss\nusing a margin of 0.4. For the region-supervised training,\nwe use cross-entropy loss with class weight to address the\nclass imbalance issue. The architecture of the CNN re-\nmained the same, but we added a fully connected layer to\nproject the embedding to logits for nine different region\ncategories. We used the Adam optimizer with an initial\nlearning rate of 0.001 and batch size of 128. The entire\ntraining process was conducted on a single RTX A5000,\nwhich took approximately 10 minutes to complete 25,000\nupdates. We validated the model’s training procedure and\nhyperparameters using a speciﬁc split of the tori-subset and\nchose the simplest setting to avoid overﬁtting the hyperpa-Similarity Ranking Random Forest Classiﬁer\nEmbedding nDCG Accuracy\nHist. (25 bin) 0.783 0.744±0.058\nHist. (124 bin) 0.777 0.722±0.054\nCNN (region ) 0.792 0.634±0.055\nCNN (SSL) 0.853 0.848±0.039\nTable 1 . Experiment results on the tori subset. Hist. de-\nnotes normalized pitch histogram, speciﬁed with the num-\nber of bins to cover two-octave range. region and SSL de-\nnotes the region-supervised and self-supervised learning.\nrameters to the small subset.\n4.3 Results\nThe evaluation result of each embedding scheme is pre-\nsented in Table 1. As tori is closely related to the use of\npitch, the pitch histogram showed about 74 % accuracy\nwhen combined with a random forest classiﬁer, and 0.783\nor 0.777 nDCG in the similar song search. The result shows\nthat increasing the resolution of the histogram does not\nachieve better performance. The performance of CNN em-\nbeddings trained with region classiﬁcation was worse than\nthe pitch histogram in classiﬁcation accuracy. However,\nthe CNN embedding trained with self-supervised learning\nshowed the best nDCG and classiﬁcation accuracy, even\nthough both CNN shares the same architecture except the\nﬁnal projection layer.\nThe performance gain compared to the pitch histogram\ncan be caused by the fact the concept of tori includes not\nonly the pitch scale the song uses but also ornamentation\nand idiomatic phrases it, which can be easily captured by\ncontour CNN.\nThe result also shows that the region label did not help\nto learn tori-related characteristics. Even though the con-\ncept of tori is strongly related to each region, folk songs\nwere widely spread nationwide at the time of the record-\ning, which made it difﬁcult to learn coherent musical char-\nacteristics from the region label.\nIt is also worth noting that self-supervised learning did\nnot use any other musical knowledge that is related to tori,\nexcept that it was trained to extract constant embedding\nthroughout a given song regardless of the segment position.\nFrom a machine learning point of view, the model has to\nextract an embedding that can explain the entire song or\ndistinguish it from other songs from a given fragment. The\nhigh correlation between the trained embedding and tori\nlabels shown in the evaluation implies that the concept of\ntori is clearly related to the distinguishable characteristics\nof each Korean folk song.\n5. ANALYSIS AND DISCUSSION\nIn this section, we introduce interesting musicological\nﬁndings that we have found from the pitch histogram and\nembedding learned from self-supervised training.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n443Figure 3 . Average pitch histogram of each tori obtained from the tori-subset and corresponding scale description.\n5.1 Pitch Histogram Analysis\nTo explain the scale and characteristics alongside with\npitch histogram of each tori, we present the staff notation\nof each tori in Figure 3 based on various studies describing\ntori [2, 25, 26]. In the score, the cheong (a main note that\ngenerally appears most frequently) was ﬁxed as the C5.\nThe unﬁlled notes are primary tones, and the ﬁlled notes\nare passing tones. In cases where there is a vibrato mark\nbelow the note, it is called yoseong , meaning the pitch gen-\nerally oscillates up and down. In addition, the arrow on the\nleft side of the note indicates that the pitch is slightly raised\nor lowered compared to the equal-tempered pitch. The slur\nmark ﬂowing down to the right of the note represents twoe-\nseong , which means the pitch slides down.\nWhile menari-tori and yukjabaegi-tori remain within a\nperfect ﬁfth range from the center, in gyung-tori, there is a\nlarger proportion of high notes (G5-A5) or more from the\ncenter. This indicates that the distribution of notes from\nlow to high in gyung-tori is evenly spread. This could be\ndue to the fact that folk songs of gyung-tori are more com-\nmonly found in popular folk songs than in those that ap-\npear nationwide. Perfect 4th below the center tone is com-\nmon in all Korean music. However for example in gyung-\ntori, there is vibrato in the G4 note, and the A4 note has\na low pitch, resulting in a distinct distribution in that part.\nIn menari-tori, it can be observed that the B Z4 note is used\nas a passing tone between C5 and G4. Yukjabaegi-tori is\ncharacterized by a vibrant vibrato in the G4, resulting in a\ngentle distribution in the nearby pitch than other tori.\nWhether a major third or a minor third is used in thescale is an important factor in distinguishing the regional\nmusical characteristics of the eastern and western parts\nof the Korean Peninsula in the traditional method. Subtle\npitch differences can be observed in all three tori. In gyung-\ntori, the D5 note is clearly distributed lower and the E5\nnote is deﬁnitely higher than the line appearing the equal-\ntempered tune. In yukjabaegi-tori, it is also conﬁrmed that\nthe EZnote is slightly higher than the minor third note’s\nThrough the above analysis, we found that pitch his-\ntograms can be utilized to roughly identify musical char-\nacteristics, such as the range, interval from tones and sub-\ntle pitches, etc. However, we also acknowledged that this\napproach solely exhibits the constituent pitches and their\nfrequencies, without accounting for other aspects like uses\nof melody and ornamentation and more.\n5.2 UMAP visualization\nWe visualized the embeddings learned through self-\nsupervised learning in 2D using UMAP, a dimension re-\nduction technique that is frequently used for visualization\nof high-dimensional vectors in Figure 4. In the ﬁrst ﬁgure,\nwe demonstrated the distribution of four different tori la-\nbels. In addition, by examining the metadata information\nof the embeddings, we ﬁgured out several interesting top-\nics from this visualization. We also implemented a web\ndemo using this visualization where one can directly ac-\ncess the corresponding audio recording3.\nMenari-tori has a much wider distribution area and en-\n3https://danbinaerinhan.github.io/korean-folksong-visualization/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n444Figure 4 . UMAP visualization of folk song embeddings obtained from our self-supervised-trained CNN model. Note that\nno tori label was used to train the model.\ncompasses a broader musical range. In Figure 4 (A), the\nmenari-tori distribution can be seen to be wider com-\npared to the other tori. We mentioned that several subdi-\nvided tori of it have been identiﬁed by scholars. Among\nthem, eosayong-tori is a representative example of reﬁn-\ning the musical characteristics of eastern folk songs [11].\nEosayong-tori, a lamenting song sung by lumberjacks, was\nsuggested to have distinct musical characteristics com-\npared to menari-tori. For instance, in eosayong-tori, the\nlowest pitch is a semitone higher, and it concludes on the\nlowest pitch of the scale instead of the ﬁnal pitch and the\nmiddle pitch found in menari-tori. We examined embed-\ndings corresponding to eosayong-tori in the metadata’s title\ninformation, and they were clearly gathered in a different\nspace than the area annotated as menari-tori, as presented\nin Figure 4 (B).\nSimilar results can be observed in the following cases\nwith arari .Arirang , having repetitive refrain lyrics such as\n“arirang” or “arari”, is the most representative Korean folk\nsong, with countless versions appearing in various regions.\nAmong them, arari , another name for Jeongseon arirang ,\nwas sung primarily in Gangwon province before spreading\nnationwide in the 1930s [27,28]. Arari is also regarded as a\nseparated tori from menari-tori for some researchers [29],\nwith its own slower tempo, monotonous skeleton tones,\nand the use of decorative tones. In Figure 4 (C), we can\nsee the clustering of entities with the title arari extracted\nfrom the metadata.\nAnother interesting example is songs from Jeju. Folk\nsongs sung from Jeju island, which is the largest island in\nKorea, have fewer research results compared to other re-\ngions. In Jeju folk songs, the interval between the pitchesis narrower compared to other provinces’. Due to these rea-\nsons, the traditional method of transcribing into staff nota-\ntion was unsuitable for capturing the characteristics of Jeju\nmusic [30–32]. However, using our embeddings, we can\nidentify three clusters as in Figure 4 (D). It implies the po-\ntential usefulness of our approach to uncover the unique\ncharacteristics of folk songs in Jeju Island.\nWe also checked the UMAP of the pitch histograms but\ncould not ﬁnd clear clustering as in the presented examples.\n6. CONCLUSIONS\nIn this paper, we have presented our computational ap-\nproach to obtaining a high-dimensional embedding vector\nfor a given pitch contour, which allows us to analyze a vast\namount of Korean folk songs. Using these embeddings and\na manually labeled subset, we have examined how musical\ndiscussions proposed in existing academia are manifested\nin our dataset. As our results cover various music charac-\nteristics, the learned embeddings can be utilized as a mon-\nitoring aid when dealing with numerous undeﬁned data to\nreview and ﬁnd the tori. Also, we have discussed the meth-\nods and possibilities for utilizing and interpreting exper-\nimental results in the future research. Through this mate-\nrial, we can review and reﬁne our understanding of the con-\ncept of the tori, and provide easily accessible resources and\nutilize them as appropriate evidence. Furthermore, there is\npotential to clarify the musical characteristics of regions\nthat are distinct from other regions, like Jeju, about which\nexisting research has not been as active. Ultimately, we ex-\npect this research to be valuable in illuminating the rela-\ntionships and transformations of folk songs over time.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4457. ACKNOWLEDGEMENT\nWe wish to express our deepest gratitude to Sang-il Choi\nand his team for their invaluable work on the “Anthology of\nKorean Traditional Folksongs”. Their effort to collect more\nthan ten thousand unique recordings from across Korea has\nnot only signiﬁcantly contributed to the preservation of the\nrich tapestry of Korean folk music but has also created a\nmonumental resource for academic research. Without their\nextraordinary efforts, our work would not have been possi-\nble. We sincerely thank them for laying such a robust foun-\ndation and for enabling future researchers and us to build\nupon it. We also appreciate MBC for kindly allowing us to\nutilize the dataset for this research.\nThe web demo was implemented with the kind help of\nDongmin Kim.\n8. REFERENCES\n[1] E. J. Shin, “Reconsideration on the modes(tori) of ko-\nrean folk song(한국민요선법(토리)의연구성과검\n토및논점),”The Society Of Korean Folk Song( 한국민\n요학, vol. 46, pp. 153–195, 2016.\n[2] G. R. Sung, Controversial Topics in the Research\nof Korean Music Theory, 한국음악이론연구의 쟁\n점. Publishing department of the academy of Korean\nstudies(한국학중앙연구원출판부), 2020.\n[3] H. J. Kim, Transcription and Interpretation of Korean\nFolk Songs(민요의채보와해석). Publish Company\nMinsokwon(민속원), 2013.\n[4] G. K. Koduri, S. Gulati, P. Rao, and X. Serra, “R ¯aga\nrecognition based on pitch distribution methods,” Jour-\nnal of New Music Research , vol. 41, no. 4, pp. 337–\n350, 2012.\n[5] S. Rosenzweig, F. Scherbaum, and M. Müller, “Detect-\ning stable regions in frequency trajectories for tonal\nanalysis of traditional georgian vocal music.” in Proc.\nof Int. Society of Music Information Retrieval Conf.\n(ISMIR) , 2019, pp. 352–359.\n[6] F. Scherbaum, N. Mzhavanadze, S. Rosenzweig, and\nM. Müller, “Tuning systems of traditional georgian\nsinging determined from a new corpus of ﬁeld record-\nings,” Musicologist , vol. 6, no. 2, pp. 142–168, 2022.\n[7] N. Kroher, E. Gómez, A. Chaachoo, M. Sordo, J.-\nM. Díaz-Báñez, F. Gómez, and e.-R. Mora, Joaquin\",\nComputational Ethnomusicology: A Study of Fla-\nmenco and Arab-Andalusian Vocal Music\", bookTi-\ntle=\"Springer Handbook of Systematic Musicology .\nBerlin, Heidelberg: Springer Berlin Heidelberg, 2018,\npp. 885–897.\n[8] S. I. Choi, “Articles on recordings : ‘anthology of\nkorean traditional folksongs’ about the project and\nthe records published( 창간10주년기념호:음반;’\n한국민요대전’사업과음반발간),”Korean Recording\nStudies(한국음반학), vol. 10, pp. 459–480, 2000.[9] I. G. Bae, “Musicological study of the tori of\nchungcheong province : focusing on the rice ﬁeld\nweeding folksongs contained in 『mbc hankuk minyo\ndaejeon (the compilation of korean folksongs) 』(충청\n도토리를찾기위한시론:[mbc한국민요대전]수록\n논매기소리를중심으로),”The Society Of Korean Folk\nSong(한국민요학, vol. 12, pp. 159–184, 2003.\n[10] N. kyong Choi, “Distribution of folk songs and its\nmusical characteristics(1) ( 민요의종류와음악적특\n성의분포연구(1)-모심는소리,노젓는소리,만선\n풍장소리를중심으로),”Research Institute of Korean\nStudies(민족문화연구), vol. 41, pp. 85–127, 2004.\n[11] Y . W. Kim, “\"a study of the yoengnam folk song,\neosayong(嶺南民謠어사용의音組織硏究),”The So-\nciety Of Korean Folk Song(The Society Of Korean Folk\nSong(한국민요학, vol. 6, pp. 45–131, 1999.\n[12] B. H. Lee, “Music and folk song of central and western\nregion and tori(경서토리권의무가,민요),”Collection\nof Korean Music Studies( 나운영박사회갑기념한국음\n악논총), 1982.\n[13] ——, “Music culture of the menari-tori folk song( 메나\n리토리무가민요권의음악문화),”Koanthro(한국문\n화인류학), vol. 15, pp. 233–249, 1983.\n[14] D. W. Baek, “The deﬁnition and interpretation of ter-\nminology of korean musicology, 한국음악학에서의\n용어개념정의와해석,”National Folk Museum of\nKorea(민속학연구), no. 14, pp. 193–230, 2004.\n[15] J. W. Kim, J. Salamon, P. Li, and J. P. Bello, “Crepe:\nA convolutional representation for pitch estimation,”\nin2018 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 2018,\npp. 161–165.\n[16] G. K. Koduri, J. Serrà Julià, and X. Serra, “Characteri-\nzation of intonation in carnatic music by parametrizing\npitch histograms,” in Proc. of the 13th Int. Society for\nMusic Information Retrieval Conf. International So-\nciety for Music Information Retrieval (ISMIR), 2012.\n[17] A. C. Gedik and B. Bozkurt, “Pitch-frequency\nhistogram-based music information retrieval for turk-\nish music,” Signal Processing , vol. 90, no. 4, pp. 1049–\n1063, 2010.\n[18] N. Pretto, B. Bozkurt, R. Caro Repetto, and X. Serra,\n“Nawba recognition for arab-andalusian music using\ntemplates from music scores,” in Proc. of the 15th\nSound and Music Computing Conference (SMC2018) .\nCyprus University of Technology, 2018, pp. 394–99.\n[19] B. Nikzat and R. Caro Repetto, “KDC: An open corpus\nfor computational research of dastg ¯ahimusic,” in Proc.\nof the 23rd Int. Society for Music Information Retrieval\nConf. International Society for Music Information\nRetrieval (ISMIR), 2022, pp. 321–328.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n446[20] J. Lee, J. Park, and J. Nam, “Representation learning\nof music using artist, album, and track information,”\ninMachine Learning for Music Discovery Workshop,\nInternational Conference on Machine Learning , 2019.\n[21] J. Spijkervet and J. A. Burgoyne, “Contrastive learn-\ning of musical representations,” in Proc. of the 22nd\nInt. Society for Music Information Retrieval (ISMIR) ,\n2021.\n[22] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and\nE. Hovy, “Hierarchical attention networks for docu-\nment classiﬁcation,” in Proc. of the 2016 conf. of the\nNorth American chapter of the association for com-\nputational linguistics: human language technologies ,\n2016, pp. 1480–1489.\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in neural infor-\nmation processing systems , vol. 30, 2017.\n[24] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and\nM. D. Plumbley, “Panns: Large-scale pretrained au-\ndio neural networks for audio pattern recognition,”\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , vol. 28, pp. 2880–2894, 2020.\n[25] B. H. Lee, “Joga jisihaneun seonbeobgwa torieui\ngaenyeom(조(調)가지시하는선법과토리의개념),”\n한국음악연구, vol. 51, pp. 245–271, 2012.\n[26] Y . W. Kim, Introduction to Korean Traditional\nMusic(국악개론). Eumaksekye(음악세계), 2020.\n[27] D. H. Kang, “A study on the folksong’s ecology and\ncultural meaning of chongson arari in korean folksong\narirang(정선아라리의민요생태와문화적의미),”The\nSociety Of Korean Folk Song(The Society Of Korean\nFolk Song(한국민요학, vol. 23, pp. 257–288, 2008.\n[28] Y . W. Kim, “A study of the formation of arirang(< 아리\n랑>형성과정에대한음악적연구),”한국문학과예\n술, vol. 7, pp. 5–55, 2011.\n[29] ——, “Reexamination of eastern folk song tori －ne-\ncessity to set the lower category of menari-tori\nand proposal(동부민요토리의재검토-동부민요하층\n위설정의필요성을중심으로),”Studies in Korean\nmusic(한국음악연구), vol. 61, pp. 35–64, 2017.\n[30] Y . B. Cho, “A study of the musical style of jeju folk\nsongs,濟州道民謠의音樂樣式硏究,” Ph.D. disser-\ntation, the academy of Korean studies( 한국정신문화연\n구원한국학대학원), 1997.\n[31] B. H. Lee, “Research methodology of jejudo tori\nbased on traditional tori features( 전통적토리특성에\n의한제주도토리연구방법론),”Korean Recording\nStudies(한국음반학), vol. 28, pp. 105–130, 2018.[32] E. J. Shin, “A review and proposal of studies of the\nmode of the folk songs in cheju island( 제주민요음\n조직에대한연구검토및제언),”The Society for\nKorean History Musicology( 한국음악사학회), vol. 62,\npp. 169–206, 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n447"
    },
    {
        "title": "Introducing DiMCAT for Processing and Analyzing Notated Music on a Very Large Scale.",
        "author": [
            "Johannes Hentschel",
            "Andrew McLeod",
            "Yannis Rammos",
            "Martin Rohrmeier"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265339",
        "url": "https://doi.org/10.5281/zenodo.10265339",
        "ee": "https://zenodo.org/records/10265339/files/000061.pdf",
        "abstract": "As corpora of digital musical scores continue to grow, the need for research tools capable of manipulating such data efficiently, with an intuitive interface, and support for a diversity of file formats, becomes increasingly pressing. In response, this paper introduces the Digital Musicology Corpus Analysis Toolkit (DiMCAT), a Python library for processing large corpora of digitally encoded musical scores. Equally aimed at music-analytical corpus studies, MIR, and machine-learning research, DiMCAT performs common data transformations and analyses using dataframes. Dataframes reduce the inherent complexity of atomic score contents (e.g., notes), larger score entities (e.g., measures), and abstractions (e.g., chord symbols) into easily manipulable computational structures, whose vectorized operations scale to large quantities of musical material. The design of DiMCAT's API prioritizes computational speed and ease of use, thus aiming to cater to machine-learning practitioners and musicologists alike.",
        "zenodo_id": 10265339,
        "dblp_key": "conf/ismir/HentschelMRR23",
        "keywords": [
            "Digital Musicology Corpus Analysis Toolkit (DiMCAT)",
            "Python library",
            "processing large corpora",
            "data transformations",
            "dataframes",
            "computational structures",
            "vectorized operations",
            "machine-learning research",
            "music-analytical corpus studies",
            "ease of use"
        ],
        "content": "INTRODUCING DiMCAT FOR PROCESSING AND ANALYZING NOTATED\nMUSIC ON A VERY LARGE SCALE\nJohannes Hentschel1Andrew McLeod2Yannis Rammos1\nMartin Rohrmeier1\n1Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland\n2Fraunhofer IDMT, Ilmenau, Germany\njohannes.hentschel@epfl.ch\nABSTRACT\nAs corpora of digital musical scores continue to grow, the\nneed for research tools capable of manipulating such data\nefﬁciently, with an intuitive interface, and support for a\ndiversity of ﬁle formats, becomes increasingly pressing.\nIn response, this paper introduces the Digital Musicol-\nogy Corpus Analysis Toolkit ( DiMCAT ), a Python library\nfor processing large corpora of digitally encoded musical\nscores. Equally aimed at music-analytical corpus stud-\nies, MIR, and machine-learning research, DiMCAT per-\nforms common data transformations and analyses using\ndataframes. Dataframes reduce the inherent complexity\nof atomic score contents (e.g., notes), larger score enti-\nties (e.g., measures), and abstractions (e.g., chord symbols)\ninto easily manipulable computational structures, whose\nvectorized operations scale to large quantities of musical\nmaterial. The design of DiMCAT ’s API prioritizes com-\nputational speed and ease of use, thus aiming to cater to\nmachine-learning practitioners and musicologists alike.\n1. INTRODUCTION\nGiven the proliferation of large corpora of digital scores\n(e.g., [1–4]), the computational challenges of analyzing\nsymbolically encoded staff notation loom large in Digi-\ntal Musicology and MIR. In principle, any symbolic music\nencoding is equally amenable to algorithmic processing,\nto the extent that it is consistent and comprehensive. In\npractice, however, the visual efﬁciency of staff notation—\nwhich conglomerates tonal, rhythmic, metric, articula-\ntory, and other musical parameters in context-dependent\nand position-dependent symbols—is inversely related to\nitscomputational efﬁciency. Analyzing large collections\nof digital scores is thus hindered not only by the sheer vol-\nume of data involved, but also by the intrinsic complexity\nof the representations comprising musical structures. [5,6]\nTo address such challenges, we present the Digital Mu-\nsicology Corpus Analysis Toolkit ( DiMCAT ), which uses\n© J. Hentschel, A. McLeod, Y . Rammos, and M.\nRohrmeier. Licensed under a Creative Commons Attribution 4.0 Inter-\nnational License (CC BY 4.0). Attribution: J. Hentschel, A. McLeod,\nY . Rammos, and M. Rohrmeier, “Introducing DiMCAT for processing and\nanalyzing notated music on a very large scale”, in Proc. of the 24th Int.\nSociety for Music Information Retrieval Conf., Milan, Italy, 2023.dataframes [7–9] to disentangle pertinent score features\nwithin tabular representations, providing an interface for\nprocessing and analyzing large collections of dataframe-\nstructured score data. DiMCAT supports MusicXML, MEI,\nHumdrum, and MuseScore (see Section 3.1), among other\nformats, and provides an expandable range of music anal-\nysis functionalities, including feature extraction, similarity\nanalysis, and visualization. Addressed to Digital Musicol-\nogy and MIR communities alike, its purpose is to provide\na user-friendly interface for “distant-reading” staff-notated\nscore corpora, and for utilizing score data in machine-\nlearning pipelines. Efﬁciency at scale was among our pri-\nmary design goals, an aspect which is only growing in im-\nportance as corpus sizes have continued to grow (e.g., [3]),\nwith scores, rather than MIDI encodings, increasingly used\nto train large computational models (e.g., [10]).\nIn this paper, we describe the design and implementa-\ntion ofDiMCAT and argue for its usefulness through trials\nwith various corpora. First, in Section 2, we outline a ratio-\nnale for the representation of staff notation as dataframes,\nand for the underlying data-relational mindset. Section 3\npresents the library design from a user’s perspective, cover-\ning such topics as data loading (Section 3.1) and the slice/-\ngroup/analysis pipeline (Section 3.2). Evidence of ease-of-\nuse in musicological research is provided in Section 4, and\na comparison to extant libraries is made in Section 5.\n2. UTILIZING DATAFRAMES TO REPRESENT\nSCORES\nDataframes were ﬁrst introduced in 1990 as part of the\nstatistical programming language S [7] and later ported\nto its descendant R [11]. Since then, they have become\nubiquitous in the data science and machine learning com-\nmunities, with a multitude of supplementary frameworks\nreleased across the spectrum of programming languages,\noften aiming to overcome performance problems associ-\nated with large dataframes (e.g., modin [12]). The wide\nadoption of dataframes can be attributed to their versatil-\nity, convenience, and operational principles, which resem-\nble those of relational databases, spreadsheets, and nested\narrays [9, 11–13]. DiMCAT encodes all score information\nwithin dataframe objects provided by either pandas or\nmodin , with support for additional libraries (which we re-\nfer to as “backends”) planned in future versions.51645 44\n\n   \n    \n\n\n   \n\n\nABBCDECFFGHIJKLMHABBCDEG\n4 5qstamp mc mn mc_onset mn_onset duration duration_q timesig staff voice name midi tpc octave\ninterval fraction int int fraction fraction fraction ﬂoat str int int str int int int\n[87.0, 87.25) 87 44 44 1/4 1/4 1/16 0.25 2/4 4 1 F2 41 -1 2\n[87.0, 87.25) 87 44 44 1/4 1/4 1/16 0.25 2/4 3 1 A3 57 3 3\n[87.0, 87.25) 87 44 44 1/4 1/4 1/16 0.25 2/4 2 1 C4 60 0 4\n[87.0, 87.25) 87 44 44 1/4 1/4 1/16 0.25 2/4 1 1 F4 65 -1 4\n[87.25, 87.5) 349/4 45 44 0 3/16 1/16 0.25 3/8 1 1 F4 65 -1 4\n[87.25, 87.5) 349/4 45 44 0 3/16 1/16 0.25 3/8 2 1\n[87.25, 87.5) 349/4 45 44 0 3/16 1/16 0.25 3/8 3 1\n[87.25, 87.5) 349/4 45 44 0 3/16 1/16 0.25 3/8 4 1\n[87.5, 87.75) 175/2 45 44 1/16 1/4 1/16 0.25 3/8 1 1 D5 74 2 5\n[87.5, 88.0) 175/2 45 44 1/16 1/4 1/8 0.5 3/8 2 1\n[87.5, 88.0) 175/2 45 44 1/16 1/4 1/8 0.5 3/8 3 1\n[87.5, 88.0) 175/2 45 44 1/16 1/4 1/8 0.5 3/8 4 1\n[87.75, 88.0) 351/4 45 44 1/8 5/16 1/16 0.25 3/8 1 1 C5 72 0 5\nTable 1 : Ludwig van Beethoven, String quartet op. 18/6 , 4th movement (“La Malinconia”), measure number ( mn) 44.\nThe measure contains the section break after the slow introduction and is composed of two incomplete measure units with\ncounts ( mc) 44 and 45. The new 3/8 time signature ( timesig ) of the latter is introduced by a 3/16 upbeat, mathematically\ncompleting the 2/4 meter of the former. The dataframe represents notes and rests from beat 2 onwards. Its index (bold\nvalues on the left) comprises left-closed, right-open intervals which express the start and end points of each event on the\nscore’s timeline, measured in quarter notes. Each column has a name ( bold ) and a data type ( italic ). The ﬁrst eight columns\ncontain temporal information (see Section 2.1). The columns staff andvoice determine a notational layer. The last four\ncolumns express pitch-related information ( tpcis tonal pitch class, expressed as the distance from C measured in perfect\nﬁfths) and are empty for rows representing rests. Special columns are omitted (e.g., ties, tremolos, or grace notes).\n2.1 Representing staff notation as dataframes\nMost encoding standards symbolically represent staff no-\ntation in hierarchical fashion. This includes most non-\nXML plaintext formats—at least those capable of encod-\ning multiple staves, such as Lilypond, ABC, or Humdrum’s\n**kern—as well as XML-based standards. Table 2 shows a\nselection of tags in order of hierarchical nesting, from out-\nermost to innermost, for three common XML-based stan-\ndards. The table reveals that these standards recognize al-\nmost the same types of score elements, albeit located at\ndifferent levels within the document tree. Among these\nelements are staves, measures, textural layers (‘voices’),\nchords (understood as groups of notes sharing a stem) and,\nﬁnally, notes. For certain features, such as tempo and\ndynamic markings, the choice of hierarchical anchor is\nto some extent arbitrary: a tempo marking, for instance,\nmight be attached to a speciﬁc measure or to a chord within\nthat measure. DiMCAT ’s approach to the uniﬁed modeling\nof diverse hierarchical representations consists in travers-\ning them and grouping score elements of the same type in\nthe same dataframe. This obviates mapping the particulari-\nties of each standard into a common score model, a process\nwhich would either inherit a degree of arbitrariness, or re-\nsort to error-prone estimations in order to eradicate it.1\nDiMCAT disentangles the underlying score hierarchy by\ngrouping elements in ﬁve distinct categories, which we re-\nfer to as “facets”. These are:\n• notes and rests (“events”, including ties, tremolos,\ngrace notes, etc.)\n• performance details (“control events”; tempo, dy-\nnamics, slurs, lyrics, articulation, etc.);\n• measures (“ﬂow control”; measure durations, staves,\nrepeat indications, ﬁne, etc.);\n1Such a mapping, employed for example by the music21 score pro-\ncessing library [14], is rather suitable when a complete model of the score\nneeds to be maintained for further processing.MuseScore MEI musicXML\n<Score> <music> ··· <score-partwise> <score-timewise>\n<Staff> (<part>···) <part> <measure>\n<Measure> <measure> ··· <measure> <part>\n<V oice> <staff>··· <note>\n<Chord> <layer>··· <staff>, <voice>\n<Note> (<chord> ···)\n<note>\nTable 2 : Synopsis of XML tag hierarchies in three\nwidespread XML-based score models. Models differ\nmainly in the placement and naming of score elements\n(<layer> being equivalent to <voice>). In the MEI col-\numn, ellipses ( ···) suggest that any number of hierarchical\nlevels may be nested, and parentheses mark optional lay-\ners. MusicXML has two distinct organizational strategies\n(partwise or scorewise), which converge at the note level.\n• analytical annotations (“labels”; chord changes,\nform labels, algorithmic outputs, etc.);\n• metadata.2\nA facet is the raw, original representation of a category\nof score elements from which speciﬁc, homogeneous “fea-\ntures” can be derived. In its simplest form, a feature is\na subset of a facet in terms of rows and/or columns. For\nexample, the NotesAndRests facet (shown in Table 1)\ncomprises the Rests feature from which all rows and\ncolumns about notes have been removed. Other features\noffer variants requiring simple transformations. For in-\nstance, theNotes feature may be requested with tied note\nheads fused into single note events, with metrical weights\nadded, or with pitches expressed as scale degrees (for\nan example, see Listing 2). Other features require more\nsubstantial computational analysis on a set of features or\n2Note that visual details such as beaming are not loaded by default\nwhenever they are deemed irrelevant for a distant-listening setting.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n517facets, and necessitate the invocation of an Analyzer\n(see Section 3.2.3).\nOur approach thus projects different hierarchical score\nrepresentations into a paradigm similar to that of relational\ndatabases. Structural relations previously expressed by the\nunderlying score hierarchy are now expressed via IDs (for\nexample, the columns ‘staff’, ‘voice’, and ‘mc’ in Table 1).\nIn addition, all objects (except metadata) are unambigu-\nously located on the score’s musical timeline by means of\ntimestamps. As Table 1 shows, each facet and feature in-\ncludes ﬁve columns expressing timestamps in three par-\ntially redundant ways. Timestamps expressed by means of\n‘mc’ (the strict count of measure-like units from the begin-\nning of the piece, regardless of their actual length or dis-\nplayed measure number), along with ‘mc_onset’ (the loca-\ntion within a measure-like unit, represented as a fraction\nof a whole note) serve a crucial function. Given the actual\ndurations of the measure-like units, ‘mc’ and ‘mc_onset’\ndetermine ‘qstamp’, an object’s offset from the beginning\nof the piece in quarter notes. In addition, DiMCAT pro-\nvides ‘mn’, the measure numbers actually found in score\nengravings, which are in principle non-unique and based\non longstanding editorial conventions [15]. Analogously\nto ‘mc_onset’, these units warrant ‘mn_onset’ positions,\nrequired for computing metrical weights consistent with\nthe respective meter (in the column ‘timesig’).\n2.2 Operations on DimcatResource objects\nTo facilitate the processing and analysis of potentially large\ncollections of notated music, DiMCAT aggregates facets\n(as well as features or analysis results) drawn from multi-\nple pieces in a single dataframe, a DimcatResource .\nThis approach enables vectorized operations on entire\ndatasets, thus achieving higher performance in compari-\nson to an equivalent sequence of single-dataframe oper-\nations. For additional speed when using very large cor-\npora,DiMCAT can delegate dataframe operations to a\ndistributed-computing backend such as modin , which al-\nlows for automatic partitioning and parallel processing\n[12].DimcatResource s natively serialize into ZIP\narchives accompanied by a Frictionless descriptor ﬁle [16]\nallowing for type-safe data validation and loading with ex-\nternal tools. Furthermore, the “frictionless” design allows\nDiMCAT to treat a descriptor ﬁle with its included meta-\ndata (column descriptions, ﬁle genesis, versioning infor-\nmation) as if it was the described resource itself, and to\nload the actual data into memory no earlier than required.\nThe loaders described in the following section have the\npurpose of pre-processing the data to be analyzed, and\nstoring it in a self-contained format that can also be eas-\nily served on the web.\n3. LIBRARY DESIGN\nThis section describes DiMCAT ’s design and API,3which\nparallel familiar routines of musicological research: con-\n3The complete API and documentation can be found at https://\ngithub.com/DCMLab/dimcat . In addition to pydocs, we provide\nJupyter Notebook–based interactive tutorials.structing a corpus (loading and ﬁltering, Section 3.1), or-\nganizing relevant corpus data (slicing and grouping), and\nrunning algorithms on this selection (analyzing, and plot-\nting (Section 3.2).\n3.1 Loading Data\nDiMCAT deﬁnes loaders which parse and store score data\nfor a variety of symbolic encoding standards with the aid of\nexternal libraries.4This is typically achieved by discover-\ning the relevant ﬁles on disk or (from the web) and produc-\ning the homogeneous representation (the dataframes pre-\nsented in Section 2.1) in parallelized fashion, while also\ncompressing and storing the loaded data on disk for later\nuse. Once data has been pre-processed and stored along\nwith its metadata, DiMCAT ’s default loader is capable\nof determining which score features are present, and of\n“lazily” loading them into memory whenever needed for\nprocessing. Apart from decreasing the memory footprint,\nthis principle makes it possible to verify, before proceed-\ning, whether the features required for a processing pipeline\nare actually available (see below). The extracted Facet\nobjects remain by design as faithful as possible to the orig-\ninal data in terms of presence and naming of detected el-\nements. Names and types of facet ﬁelds are standardized\nonly in relation to the above-mentioned timeline columns,\nwhich are necessary for alignment. Feature objects, on\nthe other hand, are comprehensively standardized upon ex-\ntraction to guarantee type safety. Less specialized ana-\nlyzers such as Counter s also allow for the processing\nofFacet s, and users who frequently work with custom\nFeature s drawn from nonstandard elements may con-\ntribute appropriate extensions to the codebase in the spirit\nof community-driven development.\nOnce loaded, the data is represented internally by the\nDataset class and its various subclasses (see the com-\nplete documentation for details). Dataset objects are\nDiMCAT ’s main drivers and the object type users interact\nwith the most. They grant centralized access to all avail-\nable dataframes ( Facet s,Feature s, andResult s),\ndepending on the current stage of computation. These ob-\njects in turn enable type-speciﬁc transformations and visu-\nalizations.\n3.2 The Analysis Pipeline\nConceptually, every action performed by DiMCAT is a se-\nquence ofPipelineStep objects which, having been\nchained together, accept a Dataset object, perform a\ntransformation or analysis on it, and return a new data\nobject. In practice, this chaining is not entirely arbi-\ntrary, since some computations require data in speciﬁc for-\nmats (for example, the CrossEntropy analyzer requires\nequal-shaped probability vectors). All pipeline steps can\nbe expressed as, and instantiated from, associative arrays\nof typeDimcatConfig which are stored together with\naDataset ’s descriptor to make the pipeline generation\nreproducible.\n4These currently include ms3 [17] andmusic21 [14], with\nVerovio [18] planned to be added in the future.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n518Each step in a Pipeline is fundamentally an instance\nof one of the following three classes5:Slicer s accept\ndata and partition it into chunks of various sizes—for ex-\nample, sections between repeat signs, segments under the\nsame guitar chord, or 8th-note-long slices. Slices never\ncross the boundaries of a piece of music. Grouper s ac-\ncept data and group it in formally speciﬁed categories—for\nexample, segments with the same chord label, or pieces\ncomposed in the same decade. Unlike slices, groups often\ncontain information from across the corpus. Analyzer s\nare the heart of the library, performing the actual computa-\ntion once the data has been sliced and grouped.\nMany questions in music corpus studies involve com-\nparisons between groups with a degree of commonality,\ne.g., between groups of pieces by the same composer, or\nof segments such as sonata development sections [19].\nBy combining slicers, groupers and analyzers, music re-\nsearchers will ﬁnd in DiMCAT an intuitive language for\naddressing pressing questions in the ﬁeld.\n3.2.1 Slicers\nSlicer objects invoke one particular feature to compute\nsegmentation boundaries. For example, a NoteSlicer\ninvokes the Notes feature and stores its timestamps (e.g.,\nnote onset positions) as slice markers within the resulting\nSlicedDataset , interpreting them as time intervals.\nThe newly returned dataset will slice any facet or feature\nsubsequently requested, inserting an additional index level\nor column for slice boundaries (any element spanning over\na slice boundary will be split or duplicated). In principle,\nany feature (e.g., double bar lines, dynamic indications, or\nthe results of a key ﬁnder) can serve as the slicing crite-\nrion. The properties of the feature used as criterion can\nthen be used for grouping the resulting slices (e.g., slicing\na dataset using the results of a key-ﬁnding algorithm en-\nables the subsequent grouping of slices by mode; see the\nfollowing section).\n3.2.2 Groupers\nApplying a Grouper to a dataset is tantamount to\nbinning pieces or slices based on a membership crite-\nrion. As a result, any facet or feature requested from a\nGroupedDataset is provided with a prepended index\nlevel of group identiﬁers. This enables both choosing a\nlarger unit of analysis (by analyzing entire groups rather\nthan each contained piece or slice, see the following sec-\ntion) and comparing groups of analysis results (for an ex-\nample, see Section 4.2).\nFrequently used groupers include the\nCorpusGrouper , theStaffGrouper , and the\nModeGrouper (grouping pieces, events, and key\nslices, respectively). Groupers may also use metadata\nas criterion: for instance, the YearGrouper groups\npieces based on their composition dates. Grouping is a\ncomputationally cheap operation because it is performed\nusing dataframe indices.\n5That is without considering auxiliary pipeline steps such as Writer s\nwhich never result in a different dataset type.3.2.3 Analyzers\nAnalyzer s are at the heart of the DiMCAT library. Based\non its conﬁguration, an analyzer will take one particular\nor all available features from the Dataset , perform the\nanalysis on the minimal unit provided by the Dataset\n(slice or piece), and return an AnalyzedDataset .\nResults can be Feature objects (with timestamps) or\nResult objects (without timestamps), both of which are\nDimcatResource s (see Section 2.2) and provide suit-\nable methods for retrieving, displaying, transforming, and\nplotting analysis results. In addition, they allow the com-\nbination of piece or piece-slice analysis results into those\ncorresponding to higher units of analysis, e.g., piece re-\nsults into group results. This makes it possible, by apply-\ning several groupers to the same AnalyzedDataset ,\nto regroup and recombine individual results. DiMCAT\ncurrently uses the plotly library for creating interac-\ntive plots and provides reasonable (but non-binding) de-\nfaults for combining grouped results in one ﬁgure. The\nmain types of analyzers are Counter s,Comparison s\nandClusterAnalyzer s,Transformation s, and\nRangeAnalyzer s.\nThe baseAnalyzer class is designed to be easily ex-\ntensible; additional analyzers can be created by the com-\nmunity without knowledge of the deeper layers of the code.\nContributors only need to understand the structure of the\nfeatures that the new analyzer accepts as input, and select\nor implement the appropriate result type. Thereafter they\nimplement the new object’s serialization Schema6and\none of the methods that performs the actual analysis on\na slice-, piece-, or group-speciﬁc dataframe. The method\ncombine() , used for aggregating two result objects into\none—for example, by adding result vectors—only needs\nto be implemented if no superclass is available to inherit\nit from. Optional methods include check() (for reject-\ning a dataset or feature if it doesn’t fulﬁll certain criteria),\npre_process() (for performing analyzer-speciﬁc fea-\nture transformations), and post_process() (for clean-\ning up the results object, for example by ﬁlling in missing\nvalues). A new analyzer constructed in this fashion is guar-\nanteed to work with DiMCAT ’s pipeline architecture.\nThe basicCounter counts the number of rows of any\nfacet or feature (e.g., notes or chord labels). More ver-\nsatile counters aggregate counts or durations based on the\nvalues contained in a given column (e.g., pitch classes),\nvalue combinations between several columns (e.g., pitch\nclass–duration pairs), or n-grams (e.g., pairs of successive\ndynamic indications). Results can be transformed (e.g., by\nnormalizing), various properties can be calculated and re-\nturned (e.g., the distributions’ entropies), and plots can be\ngenerated.\nComparison analyzers perform pairwise compar-\nisons on the slices, pieces, or groups represented by a\ngiven feature or result, such as the sliding-window auto-\ncorrelation of a feature’s inter-onset-intervals, the Jaccard\nsimilarity between chord vocabularies, or the cross entropy\nbetween key proﬁles. They typically store their result as a\n6See details in the documentation.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n519confusion matrix, plotted by default as a heatmap. The\nresults of comparisons lend themselves to subsequent ap-\nplication of ClusterAnalyzer s, which use common\nalgorithms such as k-means to compute groups that can\nreveal relations between the features under comparison\n[20, 21].\nTransformation s apply a function or ﬁt a model\nin order to translate a feature into a different representa-\ntion. Examples include analyzers that ﬁt a Gaussian mix-\nture model to a distribution, tokenize pitch events for use\nin a neural network, or transform pitch-class proﬁles into\nFourier coefﬁcients (as demonstrated in Section 4.1).\nRangeAnalyzer s are useful in cases where only\nminima and maxima (or the range) of numerical features\nare relevant. Examples include the line-of-ﬁfths segment\ncovered by a pitch class distribution [22] or the historical\ntimespan covered by a dataset based on composition dates.\nFinally, there are many speciﬁc analyzers, such as the\nPitchClassVectors analyzer featured in Section 4.1;\nthey perform an analysis or transformation (here, aggre-\ngating durations) on one particular feature (here, Notes )\nunder a range of speciﬁc conﬁguration values (here, for\nexample, type and format of pitch classes). A full list of\nanalyzers is available in the documentation.\n4. EXAMPLES\nIn this section, we present a few examples of musicolog-\nical questions that can be easily answered using DiMCAT\n(provided the requisite data is available).\n4.1 Fourier analysis of pitch class vectors\n\u0007 \u0004\n1D = Dataset.load( \"debussy_piano\" )\n2D_analyzed = Pipeline(\n3[WindowSlicer(quarters_per_slice=1.0),\n4 PitchClassVectors(),\n5 DiscreteFourierTransform()]\n6).process(D)\n7df = D_analyzed.get_result()\n8df.sample(5)\u0006 \u0005\nListing 1 : Pipeline for slicing a dataset by quarter-note\nwindows, computing pitch class vectors and applying the\nDiscrete Fourier Transform.\nThe Discrete Fourier Transform has seen frequent ap-\nplications to musical structures, in particular pitch-class\nsets [23–28]. It belongs in a broader class of techniques\nwhich require, in our terms, a “slicing” of the score, such\nas a chordal reduction or a ﬁxed-window segmentation.\nFor example, as part of a corpus study using the Discrete\nFourier Transform [29], DiMCAT was used to create en-\nharmonic pitch class vectors for all 2-hand piano com-\npositions by Claude Debussy. Listing 1 demonstrates the\nsimplicity with which this analysis can be expressed as a\nDiMCAT pipeline. In the ﬁrst line, the data is loaded from a\nlocal directory. Then the pipeline is created and processed.\nIn the pipeline, a slicer ﬁrst slices all pieces at every quarter\nnote, then an analyser creates vectors of aggregated pitch\nclass durations for each slice. Finally, the DFT analyser is\nrun on each vector and the result obtained. A sample ofthe results, with coefﬁcients 0 through 6 given as complex\nnumbers, is shown in Table 3.\n4.2 Evaluating key segments\n\u0007 \u0004\n1D = Dataset.load( \"dcml_corpora.datapackage.json\" )\n2D_sliced = Pipeline(\n3[KeySlicer(),\n4 ModeGrouper()]\n5).process(D)\n6F = DimcatConfig( \"Notes\",format=SCALE_DEGREES)\n7D_sliced.get_feature(F).plot_grouped() # plot 1\n8D_grouped = CorpusGrouper().process(D_sliced)\n9D_grouped.plot_grouped() # plot 2\u0006 \u0005\nListing 2 : Plotting common dataset transformations\n(plotting parameters omitted). Plots shown in Figure 1.\nGiven a score dataset with local-key annotations cre-\nated by human analysts or an automatic key ﬁnder, a re-\nsearcher might wonder how key segments in the major and\nminor mode are distributed over the corpora contained in\nthe dataset, and how the tonal pitch-class proﬁles com-\npare between the two modes. This second example relies\non a dataset that includes key annotations7and demon-\nstrates the power and ease-of-use of DiMCAT pipelines,\neven without using any analyzers. The KeySlicer used\nin Listing 2 is set up by default to slice the dataset by an-\nnotated modulations, and warns the user about pieces for\nwhich no local key information is available. The pipeline\nproceeds by applying the ModeGrouper to create one\ngroup per mode which, in the present dataset, amounts to a\nminor-key and a major-key group. Requesting the Notes\nfeature from a Dataset processed in this fashion, we may\nretrieve and plot a representation that reﬂects the group-\ning, as shown in the upper bar chart in Figure 1. The lower\nplot demonstrates that the slicing criterion itself may also\nprovide meaningful insights into a dataset. It can be pro-\nduced by applying a CorpusGrouper to the processed\nDataset and plotting the groups. (Without the corpus\ngrouper, the plot would show the major-minor ratio for the\nentire dataset—that is, the result of the mode grouper).\n5. COMPARISON WITH OTHER LIBRARIES\nOther analysis libraries also lend themselves to the analy-\nsis of datasets of symbolic music encodings. In this section\nwe compare DiMCAT to other open-source libraries which\nmaintain note names (pitch spelling) and support multiple\nstaffs and analytical annotations. Several among them like-\nwise utilize dataframes.\nThe Humdrum Toolkit was one of the ﬁrst frameworks\nfor computer-aided music analysis, and is still used for the\nanalysis of Humdrum and kern ﬁles across the range of\nprogramming languages to which it has been ported. The R\npackagehumdrumR [30] ports the Humdrum Toolkit into\nR. While it provides support for computationally efﬁcient\ndataframes, and includes R’s inherent plotting capabilities,\nlike Humdrum itself it cannot import more modern, and\narguably more common, symbolic-encoding formats such\nas musicXML without the use of error-prone converters.\n7https://github.com/DCMLab/dcml_corporaProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n520corpus fname 1.0q_slice 0 1 2 3 4 5 6\ndebussy_other_piano_pieces l068_reverie [351.0, 352.0) 2.00+0.00j 0.32-0.18j -0.50-0.87j -1.50+0.50j -1.00+0.00j 1.18+0.68j 1.00+0.00j\ndebussy_childrens_corner l113-03_childrens_serenade [27.0, 28.0) 2.25+0.00j 0.92-0.47j 0.37-0.22j 0.75-0.50j -1.12-0.65j -1.67-0.03j -0.75+0.00j\ndebussy_preludes l123-12_preludes_feux [176.0, 177.0) 6.62+0.00j 0.53-1.57j -1.94-0.11j 0.00+4.12j 2.69-2.27j 2.47-3.30j -2.12+0.00j\ndebussy_etudes l136-04_etudes_sixtes [53.5, 54.5) 4.00+0.00j -0.12+0.37j -0.75+0.00j -0.75-0.25j -1.25+0.87j 1.62-1.37j 1.50+0.00j\ndebussy_deux_arabesques l066-02_arabesques_deuxieme [137.0, 138.0) 4.00+0.00j 0.25-0.30j -0.25-1.30j -2.00-1.00j 1.75+1.30j 0.25+2.30j 2.00+0.00j\nTable 3 : Sample rows from a dataframe containing the seven ﬁrst DFT coefﬁcients gained from quarter-note-window pitch\nclass vectors. The ﬁrst three columns represent a multi-index indicating corpus, ﬁle name, and slice interval (expressed as\nqstamp , see Table 1); they make it possible to trace the result back to the score.\nbbb7bb4bb1bb5bb2bb6bb3bb7b4b1b5b2b6b3b74152637#4#1#5#2#6#3#7##4##1##5##2##6##3##7###4###1###5 00.050.10.150.2mode\nmajor\nminor\nNotes transposed to the local key, as major-scale degreesnormalized duration\n70.1 %\n66.3 %\n58.7 % 53.7 %\n36.1 % 66.9 %60.5 % 70.4 %\n40.0 %81.9 %\n74.9 % 60.9 %33.7 %\n41.3 %46.3 %\n63.9 % 33.1 %39.5 %29.6 %60.0 %18.1 %\n25.1 %39.1 %\nBeethoven String Quartets Beethoven Sonatas Chopin Mazurkas Corelli Trio Sonatas Debussy Suite Bergamasque Dvořák Silhouettes Grieg Lyric Pieces Liszt Années Medtner Tales Mozart Piano Sonatas R Schumann Kinderszenen Tchaikovsky Seasons020k40k mode\nmajor\nminor\nKey segments grouped by corpusduration in 𝅘𝅥\nFigure 1 : The two plots produced by the code shown in Listing 2.\nMusic21 [14] is a large Python library capable of im-\nporting all relevant music formats, transforming them into\na comprehensive hierarchical model of the score. Rely-\ning on an elaborate object model, it provides methods for\ncreating and manipulating the elements of a music score.\nHowever, its design renders it computationally demand-\ning [30, 31] for large corpora, and it provides only few\nmethods designed speciﬁcally for corpus analysis.\nSeveral Python libraries follow a similar approach to\nDiMCAT’s, analyzing and making available score infor-\nmation in the form of dataframes. These include the\nVIS-framework [32] andCRIM intervals [33]\n(both focusing on intervallic successions and sonorities),\nCAMAT [31] (basic pitch statistics), and musif [34] (with\na focus on global features of entire scores). Among them,\nonly [31] introduces its own score parser (for MusicXML),\nwith the remaining ones invoking music21 . [34] also in-\ncludes the MuseScore parser ms3 [17] and therefore ex-\nposes an architecture that is as easily extensible as ours.\nAlthoughDiMCAT can, in principle, provide any algo-\nrithm that operates on successions or sets of pitch events,\nits focus on “distant listening” makes it less suited for\nclose-reading studies than some of the aforementioned al-\nternatives. Indeed, its distinguishing feature is the newly\nintroduced Slice-Group-Analyze paradigm, designed for\ninquiries in which the corpus, rather than the individual\nscore, is the primary research object. To this end, DiMCATprovides mechanisms for studying the statistical properties\nof potentially very large amounts of musical material by\niteratively applying sequences of segmentation and group-\ning algorithms with a high degree of combinatorial free-\ndom. From this point of view, the “slice” serves as an\nadditional operational level between “note” [31–33] and\n“piece” [34].\n6. CONCLUSION\nIn this paper we have introduced DiMCAT , a Python li-\nbrary capable of parsing, transforming, and analyzing an-\nnotated music score data in a range of symbolic-encoding\nformats, and to do so efﬁciently at scale. The library stores\ndata as dataframes, a ubiquitous structure in the ﬁelds of\ndigital humanities and data science. DiMCAT emphasizes\ntraceability (results can reliably lead to the original score\nelements) and reproducibility (version identiﬁers are sys-\ntematically applied to code and data). Thanks to an in-\nterface that masks its inner workings, the functionality of\nthe library is usable and extensible by musicologists with\nlimited programming experience.\nDiMCAT is released under the GPL-3.0-or-later Li-\ncense, and we intend to continue adding further music ana-\nlyzers, inviting feedback, requests, and contributions from\nthe community.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n5217. ACKNOWLEDGEMENTS\nThis research was supported by the Swiss National Sci-\nence Foundation within the project “Distant Listening\n– The Development of Harmony over Three Centuries\n(1700–2000)” (Grant no. 182811). This project is being\nconducted at the Latour Chair in Digital and Cognitive Mu-\nsicology, generously funded by Mr. Claude Latour.\n8. REFERENCES\n[1] C. S. Sapp, “Online database of scores in the hum-\ndrum ﬁle format,” in Proceedings of the Proceedings\nof the 9th International Society for Music Information\nRetrieval Conference (ISMIR) , Philadelphia, 2008.\n[2] M. Gotham, P. Jonas, B. Bower, W. Bosworth,\nD. Rootham, and L. VanHandel, “Scores of Scores: An\nOpenScore project to encode and share sheet music,”\ninProceedings of the 5th International Conference on\nDigital Libraries for Musicology - DLfM ’18 . Paris,\nFrance: ACM Press, 2018, pp. 87–95.\n[3] F. Foscarin, A. McLeod, P. Rigaux, F. Jacquemard, and\nM. Sakai, “ASAP: A dataset of aligned scores and per-\nfor mances for piano transcription,” in Proceedings of\nthe 21st International Society for Music Information\nRetrieval Conference (ISMIR) , 2020.\n[4] J. Hentschel, Y . Rammos, M. Neuwirth, F. C. Moss,\nand M. Rohrmeier, “An annotated corpus of tonal pi-\nano music from the long 19th century,” Empirical Mu-\nsicology Review , forthcoming.\n[5] J. Devaney, “Using note-level music encodings to facil-\nitate interdisciplinary research on human engagement\nwith music,” Transactions of the International Society\nfor Music Information Retrieval , vol. 3, no. 1, pp. 205–\n217, Oct. 2020.\n[6] F. Foscarin, P. Rigaux, and V . Thion, “Data quality\nassessment in digital score libraries: The GioQoso\nProject,” International Journal on Digital Libraries ,\nvol. 22, no. 2, pp. 159–173, Jun. 2021.\n[7] J. Chambers, T. Hastie, and D. Pregibon, “Statistical\nmodels in S,” in Proceedings in Computational Statis-\ntics, K. Momirovi ´c and V . Mildner, Eds. Heidelberg:\nPhysica-Verlag HD, 1990, pp. 317–321.\n[8] W. McKinney, “Data structures for statistical comput-\ning in python,” in Proceedings of the 9th Python in Sci-\nence Conference , Austin, Texas, 2010, pp. 56–61.\n[9] D. Petersohn, “Dataframe systems: Theory, architec-\nture, and implementation,” Ph.D. dissertation, Univer-\nsity of California, Berkeley, 2021.\n[10] A. Mcleod and M. A. Rohrmeier, “A modular system\nfor the harmonic analysis of musical scores using a\nlarge vocabulary,” in Proceedings of the 22nd Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) . Online: ISMIR, Nov. 2021, pp. 435–\n442.[11] D. Petersohn, S. Macke, D. Xin, W. Ma, D. Lee, X. Mo,\nJ. E. Gonzalez, J. M. Hellerstein, A. D. Joseph, and\nA. Parameswaran, “Towards scalable dataframe sys-\ntems,” Proceedings of the VLDB Endowment , vol. 13,\nno. 12, pp. 2033–2046, Aug. 2020.\n[12] D. Petersohn, D. Tang, R. Durrani, A. Melik-\nAdamyan, J. E. Gonzalez, A. D. Joseph, and A. G.\nParameswaran, “Flexible rule-based decomposition\nand metadata independence in modin: A parallel\ndataframe system,” Proceedings of the VLDB Endow-\nment , vol. 15, no. 3, pp. 739–751, Nov. 2021.\n[13] Y . Wu, “Is a dataframe just a table?” in 10th Work-\nshop on Evaluation and Usability of Programming\nLanguages and Tools (PLATEAU 2019) , ser. OpenAc-\ncess Series in Informatics (OASIcs), S. Chasins, E. L.\nGlassman, and J. Sunshine, Eds., vol. 76. Dagstuhl,\nGermany: Schloss Dagstuhl–Leibniz-Zentrum fuer In-\nformatik, 2020, pp. 6:1–6:10.\n[14] M. S. Cuthbert, “Music21: A toolkit for computer-\naided musicology and symbolic music data,” in Pro-\nceedings of the 11th International Society for Music\nInformation Retrieval Conference (ISMIR) , 2010, pp.\n637–642.\n[15] E. Gould, Behind Bars: The Deﬁnitive Guide to Music\nNotation . London: Faber Music, 2011.\n[16] D. Fowler, J. Barratt, and P. Walsh, “Frictionless data:\nMaking research data quality visible,” International\nJournal of Digital Curation , vol. 12, no. 2, pp. 274–\n285, May 2018.\n[17] J. Hentschel and M. Rohrmeier, “Ms3: A parser for\nMuseScore ﬁles, serving as data factory for annotated\nmusic corpora,” Journal of Open Source Software ,\n2023.\n[18] L. Pugin, R. Zitellini, and P. Roland, “Verovio: A li-\nbrary for engraving MEI music notation into SVG,”\ninPugiProceedings of the 15th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2014, pp. 107–112.\n[19] C. White, The Music in the Data: Corpus Analysis,\nMusic Analysis, and Tonal Traditions , 1st ed. New\nYork: Routledge, Nov. 2022.\n[20] R. Cilibrasi, P. Vitanyi, and R. de Wolf, “Algorithmic\nclustering of music,” arXiv:cs/0303025 , Mar. 2003.\n[21] E. Anzuoni, S. Ayhan, F. Dutto, A. McLeod, F. C.\nMoss, and M. Rohrmeier, “A historical analysis of\nharmonic progressions using chord embeddings,” in\nSound and Music Computing Conference (SMC) , 2021,\npp. 284–291.\n[22] F. C. Moss, M. Neuwirth, and M. Rohrmeier, “The line\nof ﬁfths and the co-evolution of tonal pitch-classes,”\nJournal of Mathematics and Music , pp. 1–25, Mar.\n2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n522[23] D. Lewin, “Re: Intervallic relations between two col-\nlections of notes,” Journal of Music Theory , vol. 3,\nno. 2, pp. 298–301, Nov. 1959.\n[24] I. Quinn, “A uniﬁed theory of chord quality in equal\ntemperaments,” Ph.D. dissertation, Eastman School of\nMusic, Rochester, New York, 2004.\n[25] D. Tymoczko, “Set-class similarity, voice eading, and\nthe Fourier transform,” Journal of Music Theory ,\nvol. 52, no. 2, pp. 251–272, Sep. 2008.\n[26] J. Yust, “Applications of DFT to the theory of\ntwentieth-century harmony,” in Mathematics and Com-\nputation in Music , T. Collins, D. Meredith, and\nA. V olk, Eds. Cham: Springer International Publish-\ning, 2015, vol. 9110, pp. 207–218.\n[27] E. Amiot, Music through Fourier Space , ser. Computa-\ntional Music Science. Cham: Springer International\nPublishing, 2016.\n[28] J. D. Harding, “Applications of the Discrete Fourier\nTransform to music analysis,” Ph.D. dissertation,\nFlorida State University, 2021.\n[29] S. Laneve, L. Schaerf, G. Cecchetti, J. Hentschel, and\nM. Rohrmeier, “The diachronic development of De-\nbussy’s musical style: A corpus study with Discrete\nFourier Transform,” Humanities and Social Sciences\nCommunications , vol. 10, no. 1, p. 289, Jun. 2023.\n[30] N. Condit-Schultz and C. Arthur, “humdrumR: A new\ntake on an old approach to computational musicol-\nogy,” in Proceedings of the 20th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nDelft, 2019.\n[31] E. Poliakov and C. R. Nadar, “CAMAT: Computer As-\nsisted Music Analysis Toolkit,” in Proceedings of the\nDigital Music Research Network One-day Workshop\n(DMRN+16) , 2021, p. 12.\n[32] C. Antila and J. Cumming, “The VIS framework. An-\nalyzing counterpoint in carge datasets,” in Proceedings\nof the 15th International Society for Music Information\nRetrieval Conference (ISMIR) , 2014.\n[33] R. Freedman, A. Morgan, Gould, O. Shostak,\nT. Dang, A. Janco, D. Russo-Batterham, E. Leon, and\nH. West, “CRIM intervals,” 2023. [Online]. Available:\nhttps://github.com/HCDigitalScholarship/intervals\n[34] A. Llorens, F. Simonetta, M. Serrano, and Á. Torrente,\n“Musif: A Python package for symbolic music feature\nextraction,” in Sound and Music Computing Confer-\nence (SMC) . arXiv, Jul. 2023.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n523"
    },
    {
        "title": "Visual Overviews for Sheet Music Structure.",
        "author": [
            "Frank Heyen",
            "Quynh Quang Ngo",
            "Michael Sedlmair"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265383",
        "url": "https://doi.org/10.5281/zenodo.10265383",
        "ee": "https://zenodo.org/records/10265383/files/000082.pdf",
        "abstract": "We propose different methods for alternative representation and visual augmentation of sheet music that help users gain an overview of general structure, repeating patterns, and the similarity of segments. To this end, we explored mapping the overall similarity between sections or bars to colors. For these mappings, we use dimensionality reduction or clustering to assign similar segments to similar colors and vice versa. To provide a better overview, we further designed simplified music notation representations, including hierarchical and compressed encodings. These overviews allow users to display whole pieces more compactly on a single screen without clutter and to find and navigate to distant segments more quickly. Our preliminary evaluation with guitarists and tablature shows that our design supports users in tasks such as analyzing structure, finding repetitions, and determining the similarity of specific segments to others.",
        "zenodo_id": 10265383,
        "dblp_key": "conf/ismir/HeyenNS23",
        "keywords": [
            "alternative representation",
            "visual augmentation",
            "sheet music",
            "general structure",
            "repeating patterns",
            "segment similarity",
            "color mapping",
            "dimensionality reduction",
            "clustering",
            "simplified music notation"
        ],
        "content": "VISUAL OVERVIEWS FOR SHEET MUSIC STRUCTURE\nFrank Heyen Quynh Quang Ngo Michael Sedlmair\nVISUS, University of Stuttgart, Germany\n{frank.heyen, quynh.ngo, michael.sedlmair}@visus.uni-stuttgart.de\nABSTRACT\nWe propose different methods for alternative represen-\ntation and visual augmentation of sheet music that help\nusers gain an overview of general structure, repeating pat-\nterns, and the similarity of segments. To this end, we ex-\nplored mapping the overall similarity between sections or\nbars to colors. For these mappings, we use dimensionality\nreduction or clustering to assign similar segments to simi-\nlar colors and vice versa. To provide a better overview, we\nfurther designed simpliﬁed music notation representations,\nincluding hierarchical and compressed encodings. These\noverviews allow users to display whole pieces more com-\npactly on a single screen without clutter and to ﬁnd and\nnavigate to distant segments more quickly. Our prelimi-\nnary evaluation with guitarists and tablature shows that our\ndesign supports users in tasks such as analyzing structure,\nﬁnding repetitions, and determining the similarity of spe-\nciﬁc segments to others.\n1. INTRODUCTION\nCommon music notation can be considered as a special vi-\nsual encoding to convey music, including instructions on\nhow to perform it. Despite its compactness and detailed\ninformation, a music sheet is hard to analyze for novice\nmusicians [1]. Moreover, it contains lots of information\nthat is hard to display at once without visual clutter or get-\nting too small – getting an overview is tricky. When pieces\ncontain repeating sections such as a chorus, much infor-\nmation is redundant. Even with abbreviations that denote\nrepetitions, such as a double bar with colon, da capo, or al\nsegno, a complex structure can lead to tedious navigation.\nRecent work [1–3] strove to enrich notation to better\nconvey music-theoretical information and patterns. In this\npaper, we instead focus on quickly gaining an overview\nof structures such as similarities and repetitions. This\noverview is meant to support learning, or teaching a music\npiece, as musicians often have to remember which segment\nof a piece they have to play when and how often, informa-\ntion that can be obscured in classical sheet music notation.\nAccording to the visualization principle “eyes beat mem-\n© F. Heyen, Q. Q. Ngo, and M. Sedlmair. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: F. Heyen, Q. Q. Ngo, and M. Sedlmair, “Visual Overviews\nfor Sheet Music Structure”, in Proc. of the 24th Int. Society for Music\nInformation Retrieval Conf., Milan, Italy, 2023.ory” [4], we argue that the current notation leaves room for\nfurther optimization.\nToward this goal, we propose to visually enrich sheet\nmusic by mapping calculated similarities among segments\nof the sheet music, such as sections or bars (measures), to\ncolors. To ﬁt the notes of a whole piece onto a screen while\nremaining legible, we propose compact alternative encod-\nings to common music notation that allow for overview\nand easier navigation without having to scroll or change\npages. This work focuses on guitar tablature of Western\nmusic, which is easier to represent compactly and often\nfeatures more repeating parts than other kinds of sheet mu-\nsic. However, we argue that our general method of map-\nping similarities to color can also help with other kinds of\nmusic. We conducted a preliminary qualitative evaluation\nthrough pair analytics with four guitarists. The results in-\ndicate that our design supports tasks such as summarizing\nstructure, ﬁnding repetitions, and analyzing similarity.\nIn summary, we contribute 1) the exploration of novel\nrepresentations of sheet music for easier overviews, specif-\nically a method for mapping similarity among components\nof a music sheet to color, and 2) a preliminary pair analyt-\nics study with four guitarists. We further provide source\ncode and a web app where users can try their own sheet\nmusic in MusicXML [5] at visvar.github.io/sheetmusic-\noverviews.\n2. RELATED WORK\nSimilarity in music concerns many dimensions such as\ncognition, perception, tempo, pitch, and more. There-\nfore, existing metrics use different approaches, including a\ncontinuous representation of notes [6], a geometrical met-\nric [7], shapes of curves [8], and a graph-based metric for\nharmony [9]. Janssen et al. [10] evaluated melodic sim-\nilarity metrics using human annotators and a survey [11]\ndeﬁned eight criteria for symbolic melodic similarity. The\noverall aim of the above work is to query pieces in a\ndatabase. In contrast, our work focuses on supporting the\nstructural analysis of a single piece, by visualizing simi-\nlarities within it. While our design allows integrating any\nmetric, its main purpose is demonstrating how visualiza-\ntion can support sheet music analysis. We thus use a sim-\npler symbolic metric to instantiate our design.\nThere is a broad range of music-related visualiza-\ntion [12, 13], including structure [14–22]. However, some\nvisual encodings, such as one based on Tonnetz [21], re-692Figure 1 : Screenshot of our design with all views (cut): a) data and visualization options, b) view selection, c) player, d)\ninstrument/track overview, e) structure hierarchy, f) compressed repetitions, g) compact sheet, and h) complete score.\nquire knowledge of music theory. Similar to our approach,\nMoshViz [17] focuses on visual analysis in an overview-\ndetail fashion, but without considering perceptual or cog-\nnitive aspects. Our design allows encoding any similarity\nmetric (mathematical or perceptual) with colors, to make\nsheet music easier to understand. Closest to our work is\na structure visualization that uses dimensionality reduction\n(DR) to map audio features to color [18], which inspired\nus to design a similar mapping for sheet music.\nAugmented sheet music adds visual components to\ncommon music notation to increase expressiveness. Re-\nlated work augments a music piece with radial note his-\ntograms, to facilitate analyzing harmonic patterns [2],\nor visualizes rhythm through color-coded sunbursts [1].\nMiller et al. [3] combined both approaches, but do not ad-\ndress supporting performance preparation tasks. Only lit-\ntle research supports instrument learning and composition.\nBunks et al. [23] use color for reference keys on a tabular\nlayout to support jazz improvisation. Others augment sheet\nmusic with lines and ellipses to support error detection in\ncomposition [24] or pianists in identifying mistakes while\npracticing [25, 26]. In this work, we also support learning\nby aiding music reading before and during practice.\n3. DESIGN\nWe ﬁrst introduce the tasks we want to support with our\napproach. Then, we describe how we compute similarities\nand map them to colors and how we represent sheet music\nvisually (Figure 1).3.1 User Tasks\nOur overall goal is to improve the efﬁciency of reading\nsheet music, by sparing users the need to search for cer-\ntain segments or memorize patterns. We want to reveal\npotentially interesting patterns that are hard to infer from\nthe bare sheet music itself but could be helpful for bet-\nter understanding or practicing a piece. More speciﬁcally,\nwe want to support the following tasks: ( T1) understand\ntheoverall structure of a piece, ( T2) detect repetitions,\nwhich means to spot where something repeats how often ,\nand detect repetitions nested within repetitions, ( T3)com-\npare multiple segments regarding their similarity.\n3.2 Color Mappings\nSimilarity metrics . Our approach works with any metric\nthat takes notes and returns a scalar similarity score. We\ncompare non-overlapping segments of the piece, which can\nbe bars, pre-deﬁned sections read from the MusicXML ﬁle,\nor the result of an automatic segmentation (the latter is not\nimplemented).\nIn our related work section, we discussed existing sim-\nilarity metrics for symbolic music. Some of these metrics\ndo not support polyphony or require complete scores or\nadditional annotations (such as chords) or assumptions on\nmusical meaning. Metrics that are based on western tonal\nharmony [27, 28] would also not generalize to various cul-\ntures and genres. Therefore, we designed the following\nsimple but robust algorithm: First, the notes of a segmentProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n693are sorted by their start time and those with equal start time\nby pitch ascending. Mapping each note to its pitch then re-\nsults in one sequence of integers for each segment. We\nthen compute a similarity matrix by calculating the Lev-\nenshtein distance [29] for all possible pairs of segments,\nequals the minimum number of pitches one would have to\ninsert, delete, or replace, to transform the ﬁrst sequence\ninto the second.\nWe further compute similarities between all sets of\nnotes that have the same start time, which we refer to as\nharmonies. For these sets, we only use the notes’ pitch\nclass (disregarding octaves) and compute the Jaccard in-\ndex[30], which equals the ratio of intersection over union.\nMapping . Once we have a similarity matrix, we can cre-\nate a color mapping that respects these similarities. We\nexplored three alternative methods that use either a one-\nto-many comparison, dimensionality reduction, or hier-\narchical clustering. Figure 2 shows an example for our\nsimilarity-based color mapping, Figure 3 summarizes our\ndifferent mapping strategies.\nThe ﬁrst method colors bars by their similarity to a se-\nlected bar. This selection is made by the user or automat-\nically when playing the piece, where the currently played\nbar is selected. To obtain colors, we linearly map the simi-\nlarities to a color scale. Another mode only colors bars that\nthe metric considers identical to the selected one, allowing\nusers to quickly spot where and how often it repeats.\nOur second method uses dimensionality reduction\n(DR), a method commonly used to transform data from a\nhigh-dimensional space to a lower-dimensional one. Usu-\nally, the target space is two- or three-dimensional, such that\ndata points can be shown on a screen. We instead project\nonto a one-dimensional space that we can then linearly\nmap to a color scale. As we do not have concrete posi-\ntions in a space, but only the similarities between them,\nwe chose multi-dimensional scaling (MDS) [31] that ac-\ncepts a similarity matrix as input. Furthermore, MDS op-\ntimizes the computed projection to preserve these similar-\nities, leading to a coloring optimized for these.\nAs an alternative to MDS, we designed a method that\nclusters similar segments together and then assigns each\ncluster one color such that similar clusters have similar\ncolors. Using our similarity matrix, we compute hierar-\nchical agglomerative clustering , which gives us a binary\ntree. We then sort the tree’s leaves from left to right, as\nleaves that are closer together are more similar, and map\nthem in this order to a color scale. Compared to the above\nmethod using MDS, the resulting colors are easier to dis-\ntinguish but represent similarities less accurately. Using a\nsimilarity threshold, users can steer the number of clusters\nand therefore colors, to choose a trade-off between detail\nand overview.\nColor scales . Research on perception proposed a range of\ncolor scales speciﬁcally designed for visualization. Since\nthere are different irreconcilable goals, no scale is appro-\npriate for all tasks. While multi-hue scales such as rain-\nbows have been criticized [32], they have been shown to\nwork well for some circumstances [33–35]. For users\nFigure 2 : Example of similarity-based color mappings.\nTop: rectangles represent all bars of a piece, from left to\nright in the order they occur. Bottom: a color scale. Curves\nconnect each bar to its color. In this ﬁgure, the curves of\na single color are highlighted through a stronger opacity to\nshow how they connect to identical bars.\nFigure 3 : We compute similarity-based colors for ex-\ntracted segments via direct comparison, dimensionality re-\nduction, or clustering. A segment can be any sequence of\nnotes in the piece, such as a bar or a pre-deﬁned section.\nTheselected segment is chosen by the user to compare it\nto all others.\nwith a color vision deﬁciency, scales with fewer hues and,\ntherefore, less discernible colors can be used, such as ci-\nvidis [36]. When color is used to compare different val-\nues or intervals, a color scale needs to accurately represent\nsimilarities between values. For this task, single-hue scales\nor interpolations between two hues are appropriate but fur-\nther reduce the number of discernible colors. Although the\nnumber of distinguishable colors is small, there are enough\nfor our use case, as the number of different segments in a\npiece is limited. Since colors are distributed by similarity,\nindistinguishable colors should only be assigned to very\nsimilar segments. To accommodate different user needs,\nwe choose a broad multi-hue scale (spectral) as default but\nalso provide more accessible ones; for direct comparison,\nwe choose a single hue scale (blues) (Figure 4).\nFigure 4 : Some of the included color scales, taken from\nD3[37]. The choice depends on the current task and indi-\nvidual limitations of the user’s color vision.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n694Figure 5 : A pattern of repetitions with different endings.\nBar 16 is colored blue, 24 and 40 are the same yellow, and\nbar 32 is green.\n3.3 Visual Encodings\nLayout enrichment for common music notation . We de-\nsigned several visual encodings to address different tasks\nand reveal different kinds of patterns. The most straight-\nforward encoding is to display the full sheet music as the\ncommon notation that is familiar to musicians and repre-\nsents the complete information. We enrich this display by\nadding colored, semi-transparent rectangles on top of the\nsegments, for example, one for each bar (Figure 5). The\nreduced opacity makes colors brighter than in other views\nbut improves the readability of the notation. This color-\ning helps more quickly see where a bar repeats ( T2), as\nthe user only has to compare bars with similar colors ( T3).\nEven when two different bars were assigned similar col-\nors, this process allows for ruling out many others. This\nencoding suffers from the same limitations as non-colored\nsheet music. Due to its highly detailed nature, ﬁtting the\ncomplete piece on the screen at once would lead to small\nand illegible visuals. Therefore, we designed simpliﬁed,\nﬁltered, and compressed alternatives, which we explain in\nthe following paragraphs.\nNote display . In most views, we represent notes by blocks\nthat are positioned horizontally by start time and have a\nwidth proportional to the note’s duration, to visually in-\ndicate timing and rhythm (Figure 6). The ﬁrst mode dis-\nplays the notes as triangles in a piano roll, allowing it to\nrepresent music for any instrument, but less readable than\nother encodings. A second mode displays guitar tablature,\nwhere each row stands for one string, and the third adds\nfret numbers for more detail. We focus primarily on gui-\ntar tablature in this work, but new encodings resembling\nother instruments’ common notations could extend our ap-\nproach. Depending on their size, our encodings become\nhard to read but still reveal coarse patterns more clearly\nthan detailed notation. In order to show the whole piece\nat once (T1), without ﬁltering or compression, we display\nthe full score with the above encodings (Figure 1g).\nHierarchy . Most music pieces have a hierarchical struc-\nture in the form of sections such as verse and chorus, span-\nning multiple bars, each with none to a few notes, which\nmight be grouped in harmonies (notes played at the same\ntime). We visualize this structure as a tree, where users\ncan select a node to show only its children in the level be-\nlow (Figure 1e). This representation supports gaining an\n(a) Piano roll.\n (b) Tab (simple).\n (c) Tab with frets.\nFigure 6 : Note display: a) Piano rolls can represent any\nmusic but lack additional information. b, c) Tablature ei-\nther simpliﬁed or with fret numbers. Notes are drawn in\nblack or white depending on the background’s luminosity.\noverview ( T1) and allows navigating the sheet music more\nconveniently. The colors are level-speciﬁc, such that they\nonly represent similarities within, not between, the levels.\nNotes have their own color map that is not based on simi-\nlarity but still allows to spot repetitions or patterns such as\nsequences of increasing pitch ( T2).\nCompressed multi-level repetitions . Music pieces might\nhave another hierarchical structure regarding repetitions\nwhen a repeating segment contains repeating sub-segments\n(T1,2). Similar to data compression, this allows us to cre-\nate a more compact representation, by only displaying a\nrepeating segment once and annotating it with its number\nof repetitions. Doing this recursively results in a tree where\neach leaf is a bar of the music piece, and each inner node\ncontains the following information: A pre-ﬁx child, a re-\npeated child with its repetition count, and a post-ﬁx child,\nwhere pre- and post-ﬁx can be empty. The visual encod-\ning we chose for this data structure uses our compact note\nencoding (Figure 6) for the leaves and brackets for the in-\nner nodes (Figure 7). Numbers above the bars denote the\nindex of their ﬁrst occurrence, allowing to spot recurring\nones that are farther apart.\nFigure 7 : Our compressed view shows repetitions as\nnested blocks (cut). Note, that bar 51 appears two times,\nas it equals bar 53.\nWorkﬂow and interaction . We envision musicians using\nour interface primarily while learning a new piece, where\nthey ﬁrst get an overview and then take a closer look at\nthe detailed sheet music. During navigation, reading, and\nplaying, they could use overviews as ‘minimaps’ that pro-\nvide context for what they are currently focusing on. As all\nviews are linked, clicking on a bar in any view allows users\nto highlight or jump to a certain bar in all other views.\n4. EV ALUATION\nWe chose a pair analytics setup over a comparative user\nstudy, as most related work focuses on different tasks or\ndata, which does not allow fair comparison. Furthermore,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n695instead of quantitative usability ratings and time measure-\nments, we were more interested in qualitative feedback on\nhow guitarists would use our design and what limitations\nthey encounter. In pair analytics [38], designers and partic-\nipants collaboratively analyze the participants’ data, allow-\ning designers to evaluate a design without needing to teach\nparticipants how to use it, saving them time and ensuring\nthey use all features appropriately.\nOur participants (P1–P4) have experience with reading\nguitar tablature. P1 has played guitar for 15 years and reg-\nularly teaches it and P2 has played drums for 5 years and\nguitar occasionally. P3 and P4 have played guitar for 16\nand 12 years. P1 and P3 have full color vision, P2 and P4\nhave a slight red-green deﬁciency. All but P3 were famil-\niar with visualization. We met with each participant for\nroughly 1.5 hours. After an introduction to our interface,\nwe looked at guitar tablature of their choice together, en-\ncouraging them to use different features and think aloud.\nOur participants found the coloring generally helpful:\n“I have played classical pieces with 8 or 12 pages ... you\nsearched, with your teacher, made annotations with a pen\n... ‘here it’s that part again’ ... if it’s only black-on-white,\nyou’re blind at some point” (P3) . They were able to detect\nvarious patterns: “the color indicates a new segment” (P1)\n(T1),“always two bars one note, two bars the other note,\n... ” (P1) (Figure 8). One interesting example was a pattern\nwhere the same segment was repeated four times, with a\ndifferent ending each time, except for the fourth that equals\nthe second (Figure 5, T2,3).\nFigure 8 : A simple alternating pattern with bars that repeat\nasAABB multiple times.\nIn some cases, our current similarity metric did not\nwork well: “here, I’m sure it’s different, but the color is\nnot quite different ... if you check the color carefully, I\nthink you can see it” (P1) . We proposed coloring anno-\ntated sections of the sheet music by their similarity: “That\nis already very useful, because ... when I’m [teaching] and\nwant to show segments, then I always have to mark them\nby hand. This is doing it for me” (P1) . While we currently\ncolorize either by section or bar, P3 suggested alternatively\ncoloring by sequences of bars that occur together multiple\ntimes, such as riffs or motifs. Interestingly, our coloring\nallows users to quickly guess the effort needed to learn a\npiece: “The colors show what you already learned” (P3) ,\n“For me it looks like I practice this purple bar ... and then\nI practice these yellow and green bars and then I can play\n90 percent of the song” (P1) (T1,2). P4 suggested ignor-\ning bars with only a single note or chord when coloring, as\nthese are less interesting. Instead, they proposed coloring\ndifferently transposed versions of the same pattern more\nsimilarly and allowing users to manually adjust the color\nof a set of identical bars.\nWe also compared coloring via DR versus via cluster-ing. When trying clustering, P3 ﬁrst pondered “I think\n[coloring via clustering] is easier to understand ... as\nyou can really see that it’s different” but then concluded\nthat “it’s difﬁcult, both have pros and cons ... now if\nI would search [by color], it would be more difﬁcult to\nspot” (T2,3). P4 suggested to additionally vary the color’s\nbrightness for different segments in the same cluster, to\nalso reveal similarity within clusters ( T3).\nMost patterns were visible with all color scales, al-\nthough less clearly with those using fewer hues, so users\nwith color vision deﬁciencies can also beneﬁt from our de-\nsign. Even though P4 has a slight red-green deﬁciency,\nthey wanted to use the default spectral scale for most of\nthe study. When trying out other scales, they told us that it\nindeed makes a good default, as it has fewer hues than rain-\nbow scales. P4 further preferred viridis over cividis :“here\nI see better” . When turning of colors, P3 was astonished:\n“here you see how white it is! When looking at colors for\nso long, you see for the ﬁrst time how ugly white it is” .\nThe simpliﬁed and full tablature encodings still reveal\ncharacteristics: “These are power chords, right?” (P2) ,\n“This is a power chord on the second fret ... and that\nshould be A minor” (P3) . For our hierarchy view, P4 found\nthat“the tab encoding doesn’t help much, the simpliﬁed\ntab and piano roll work much better” . Especially, since\nwith the piano roll “you can see well which bars have sim-\nilar notes” (P4) (T3).\nIn our hierarchy view, clicking on different sections al-\nlows users to quickly compare them: “Main riff and verse\nis almost the same, it’s labeled as ‘main riff’ because it is\n... without singing” (P1) (T3). This also shows a draw-\nback of sheet music, where repetition signs often apply\nfor all instruments at once, so if one repeats and another\ndoes not, the ﬁrst instrument’s notes will show up redun-\ndant. During the comparison, we found that P1 labeled the\nsections incorrectly, as one had a few more bars that actu-\nally belonged to the following section: “we found that we\nlabeled it wrong, that’s good!” (P1) (T3). Switching be-\ntween different sections allows comparing them: “here’s\nagain a verse, but a little different ... back there, this bar\nis repeated ... this chorus is much longer” (P3) (T2,3).\nDuring our study with P4, we saw that a whole bar con-\nsisted of almost only the note E, as indicated by identical\ncolor, except for a single note with different color – “I also\nwouldn’t have noticed that [without color]” (P4) . As we\nonly support highlighting sections and bars for now, P3\nmissed being able to click on single harmonies and notes\nto highlight them in other views.\nThe compressed encoding (Figure 7) was P1’s favorite:\n“This is the feature I’m most excited about for showing\npeople the song structure because this is something I just\ncan’t do with a [music notation software]” (T1). For\nplaying a song with students where each plays a differ-\nent instrument of the piece, it “would be great if every-\none would have something like this” to have a compact\nsummary of the part they should play. Our participants\nproposed features we could add to this view, such as re-\nducing or disabling nested repetitions: P1 found “it wouldProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n696Figure 9 : Our algorithm might not create the same struc-\nture as a musician: The sequence 2,3,2,5appears twice\nin a row, but since the following bars of the second appear-\nance form a longer repetition, the sequence was included\nthere instead.\nbe great if you had the option to simplify it” and P2 sug-\ngested adding a slider for the compression level. They also\ntold us that this view could be extended to support anno-\ntation: “That would be great, to be able to go here and\nannotate some things” (P1) . Our compression sometimes\nleads to unexpected results as it tries to ﬁnd the longest\nrepetitions, which might not be how musicians would com-\npress a piece: “I would expect that this is in here two times,\nbut somehow it’s here – it makes sense, it’s just two differ-\nent ways of describing it” (P1) (Figure 9). P3 and P4 did\nnot consider the compressed view useful ( “I would not use\ncompressed much, maybe once when ﬁrst looking at a new\nsong” (P3) ) and P4 proposed merging it with the compact\nview by optionally drawing repeated segments only once\nwith the brackets used in this view.\nAs our compact sheet music view shows the whole\npiece at once, it allows users to spot global patterns, such as\nhow often a bar appears ( T1,2):“The compact view shows\nhow things repeat” (P4) ,“If you go here and go for [the\ncolor mode] ‘Identical’, you can see that there is a lot of\nthis” (P1) . P4 wished for an alignment, to have repeating\npatterns exactly below each other: “it could auto-align it\nfor me” . We then asked what they think about manually\ninserting line breaks, whereto P4 responded: “this would\nbe the most important to me – if this should help me learn\nor read or play, I have to be able to customize and save it” .\nWhen asked for general feedback, P1 told us they “like\nit a lot, because it’s always hard to seewhat is similar\nto something else ... I think it’s very important that you\n[know] not just what is played, but also if there is a con-\nnection to other segments” (T1,2,3). P1 wished to be able\nto directly compare bars or sections ( T3):“That would be\ngreat if you could select two and then see the difference\nbecause I always click [back and forth]” .\nWhen asked for use cases, our participants told us they\nwould use our interface for learning, for example by play-\ning along. Both P3 and P4 further imagined using our com-\npact view as a cheat sheet during a performance: “[For\nsongs with chords and simpler rhythm] you could print this\nand give it to someone ... and they could play the song ...\nor you use a tablet\"” (P3) . P4 hid all views but tracks and\ncompact, thereby maximizing the latter: “like this, I see\nthe whole song at once ... convenient as a memory aid. As-\nsuming I know the song already ... I see how often I have toplay everything” (T1,2). As another use case, P4 wished\nto be able to see multiple instruments at once to be able to\ncompare them visually.\n5. LIMITATIONS AND DISCUSSION\nWe mainly focus on guitar tablature, which is easier to rep-\nresent compactly and often features more repeating seg-\nments than other kinds of sheet music. However, we argue\nthat our general method of mapping similarities to color\ncan also help with other kinds of music. With new, special-\nized similarity metrics and note encodings, our approach\ncould support non-western kinds and even music without\ndiscrete notes, as long as a piece can be segmented. Our\nexample design for guitar tablature and with a simple sim-\nilarity metric allowed us to stay within a reasonable scope\nand matched our own musical expertise.\nHuman color vision is limited, even more with color vi-\nsion deﬁciencies. Our approach can add value compared to\nnon-colored sheet music for everyone, although accessible\ncolor scales reveal less detail. In our study, some patterns\nwere clearly visible while some were harder to spot – still,\nthey were easier to spot than without any color. Coloring\nby similarity works well for pieces with a few different\nsegments that are repeated, as fewer colors are necessary,\nbut will not work as well for others.\nAs our current approach depends on dimensionality re-\nduction and clustering, it inherits the limitation of these\ntechniques, such as distortion and artifacts. We chose MDS\nand hierarchical agglomerative clustering to preserve sim-\nilarities as well as possible, but other algorithms or ap-\nproaches might further reduce these limitations.\nIn our current interface, the participants missed being\nable to directly compare two selections of bars, align bars\nautomatically or through line breaks, and assign custom\ncolors and labels. Our evaluation only included four par-\nticipants. While such a number is typical for pair analyt-\nics, real-world acceptance can only be evaluated through\nlongitudinal ﬁeld studies, where a larger number of users\nregularly use a product in their daily life.\n6. CONCLUSION\nWe designed multiple methods to ease the detection of re-\npeating structures in sheet music. Our evaluation provided\na ﬁrst qualitative indication of the effectiveness of our ap-\nproach. Therefore, we are conﬁdent that extensions to our\ndesign can turn our work into a helpful tool for musicians.\nFuture work includes further similarity metrics and vi-\nsual encodings better suited for different tasks, sheet music\ncharacteristics, instruments, and music genres. Adding la-\nbels and exporting them would allow musicians and teach-\ners to save and share their results. Showing multiple in-\nstruments of a piece at once would allow comparing them,\nfor example, to quickly see where two guitars play similar\nnotes. We plan to let more musicians actively use our de-\nsign during learning, playing, and teaching over months to\ntest real-world usage and acceptance longitudinally.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6977. ACKNOWLEDGMENTS\nThis work was funded by the Cyber Valley Research Fund\nand by the Deutsche Forschungsgemeinschaft (DFG, Ger-\nman Research Foundation) – Project-ID 251654672 – SFB\nTRR 161, project A08.\n8. REFERENCES\n[1] D. Fürst, M. Miller, D. A. Keim et al. , “Augment-\ning sheet music with rhythmic ﬁngerprints,” in 5th\nWorkshop Visualization for the Digital Humanities\n(VIS4DH) . IEEE, 2020, pp. 14–23.\n[2] M. Miller, A. Bonnici, and M. El-Assady, “Augment-\ning music sheets with harmonic ﬁngerprints,” in Proc.\nACM Symp. Document Engineering (DocEng) . ACM,\n2019.\n[3] M. Miller, D. Fürst, H. Hauptmann et al. , “Augmenting\ndigital sheet music through visual analytics,” Computer\nGraphics Forum (CGF) , vol. 41, no. 1, pp. 301–316,\n2022.\n[4] T. Munzner, Visualization analysis and design . CRC\npress, 2014.\n[5] M. Good et al. , “MusicXML: An internet-friendly for-\nmat for sheet music,” in XML Conf. and expo . Cite-\nseer, 2001, pp. 03–04.\n[6] R. Valle and A. Freed, “Symbolic music similarity us-\ning neuronal periodicity and dynamic programming,”\ninMathematics and Computation in Music (MCM) ,\nT. Collins, D. Meredith, and A. V olk, Eds. Cham:\nSpringer, 2015, pp. 199–204.\n[7] W. B. de Haas, F. Wiering, and R. C. Veltkamp, “A\ngeometrical distance measure for determining the sim-\nilarity of musical harmony,” Int. Journal of Multimedia\nInformation Retrieval (IJMIR) , vol. 2, no. 3, pp. 189–\n202, 2013.\n[8] J. Urbano, J. Lloréns, J. Morato et al. , “Using the\nShape of Music to Compute the Similarity between\nSymbolic Musical Pieces,” in Int. Symposium on Com-\nputer Music Modeling and Retrieval (CMMR) , 2010,\npp. 385–396.\n[9] F. Simonetta, F. Carnovalini, N. Orio et al. , “Sym-\nbolic music similarity through a graph-based represen-\ntation,” in Proc. of the Audio Mostly 2018 on Sound in\nImmersion and Emotion . ACM, 2018.\n[10] B. Janssen, P. van Kranenburg, and A. V olk, “A com-\nparison of symbolic similarity measures for ﬁnding oc-\ncurrences of melodic segments,” in 16th Int. Society for\nMusic Information Retrieval Conf. (ISMIR) , 2015.\n[11] V . Velardo, M. Vallati, and S. Jan, “Symbolic melodic\nsimilarity: State of the art and future challenges,” Com-\nputer Music Journal (CMJ) , vol. 40, no. 2, pp. 70–83,\n2016.[12] R. Khulusi, J. Kusnick, C. Meinecke et al. , “A survey\non visualizations for musical data,” Computer Graph-\nics Forum (CGF) , 2020.\n[13] H. B. Lima, C. G. R. D. Santos, and B. S. Meiguins, “A\nsurvey of music visualization techniques,” ACM Com-\nput. Surv. , vol. 54, no. 7, 2021.\n[14] F. Watanabe, R. Hiraga, and I. Fujishiro, “Brass: Visu-\nalizing scores for assisting music learning,” in Proc.\nInternational Computer Music Conference (ICMC) ,\n2003.\n[15] M. Wattenberg, “Arc Diagrams: Visualizing structure\nin strings,” in IEEE Symp. Information Visualization\n(INFOVIS) . IEEE, 2002, pp. 110–116.\n[16] J. Li, “Music analysis through visualization,” in Proc.\nof the Int. Conf. on Technologies for Music Notation\nand Representation , 2016, pp. 220–225.\n[17] G. D. Cantareira, L. G. Nonato, and F. V . Paulovich,\n“MoshViz: A detail+overview approach to visualize\nmusic elements,” IEEE Trans. on Multimedia , vol. 18,\nno. 11, pp. 2238–2246, 2016.\n[18] J. Savelsberg, “Visualizing music structure using Spo-\ntify data,” in Extended Abstracts for the Late-Breaking\nDemo Session of the 22nd Int. Society for Music Infor-\nmation Retrieval Conf. (ISMIR EA) , 2021.\n[19] M. Müller and N. Jiang, “A scape plot representation\nfor visualizing repetitive structures of music record-\nings.” in 13th Int. Society for Music Information Re-\ntrieval Conf. (ISMIR) , 2012.\n[20] A. Hayashi, T. Itoh, and M. Matsubara, “Colorscore –\nvisualization and condensation of structure of classical\nmusic,” in 2011 15th Int. Conf. on Information Visual-\nisation (IV) , 2011, pp. 420–425.\n[21] T. Bergstrom, K. Karahalios, and J. C. Hart, “Iso-\nchords: Visualizing structure in music,” in Proc.\nGraphics Interface . ACM, 2007, p. 297–304.\n[22] J. Snydal and M. Hearst, “ImproViz: Visual explo-\nrations of Jazz improvisations,” in Extended Abstracts\non Human Factors in Computing Systems (CHI EA) ,\nser. CHI EA ’05. ACM, 2005, p. 1805–1808.\n[23] C. Bunks, T. Weyde, A. Slingsby, and J. Wood,\n“Visualization of tonal harmony for jazz lead sheets,”\ninEuroVis 2022 - Short Papers . The Eurographics\nAssociation, 2022, pp. 109–113. [Online]. Available:\nhttps://doi.org/10.2312/evs20221102\n[24] R. D. Prisco, D. Malandrino, D. Pirozzi et al. , “Un-\nderstanding the structure of musical compositions: Is\nvisualization an effective approach?” Information Vi-\nsualization , vol. 16, no. 2, pp. 139–152, 2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n698[25] S. Asahi, S. Tamura, Y . Sugiyama et al. , “Toward a\nhigh performance piano practice support system for be-\nginners,” in Asia-Paciﬁc Signal and Information Pro-\ncessing Association Annual Summit and Conf. (AP-\nSIPA ASC) , 2018, pp. 73–79.\n[26] M. Hori, C. M. Wilk, and S. Sagayama, “Piano prac-\ntice evaluation and visualization by HMM for arbitrary\njumps and mistakes,” in 2019 53rd Annual Conf. Infor-\nmation Sciences and Systems (CISS) , 2019, pp. 1–5.\n[27] W. B. De Haas, M. Rohrmeier, R. C. Veltkamp\net al. , “Modeling harmonic similarity using a gener-\native grammar of tonal harmony,” in Proc. of the Tenth\nInt. Conf. on Music Information Retrieval (ISMIR) ,\n2009.\n[28] W. B. De Haas, J. Rodrigues Magalhães, R. C.\nVeltkamp et al. , “HarmTrace: Improving harmonic\nsimilarity estimation using functional harmony analy-\nsis,” in Proc. of the 12th Int. Conf. on Music Informa-\ntion Retrieval (ISMIR) , 2011.\n[29] V . I. Levenshtein, “Binary codes capable of correcting\ndeletions, insertions, and reversals,” in Soviet Physics\nDoklady , vol. 10, no. 8. Soviet Union, 1966, pp. 707–\n710.\n[30] P. Jaccard, “The distribution of the ﬂora in the alpine\nzone,” New Phytologist , vol. 11, no. 2, pp. 37–50,\n1912.\n[31] J. B. Kruskal, “Multidimensional scaling by optimiz-\ning goodness of ﬁt to a nonmetric hypothesis,” Psy-\nchometrika , vol. 29, no. 1, pp. 1–27, 1964.\n[32] D. Borland and R. M. Taylor Ii, “Rainbow color map\n(still) considered harmful,” IEEE Computer Graphics\nand Applications (CG&A) , vol. 27, no. 2, pp. 14–17,\n2007.\n[33] K. Reda and D. A. Szaﬁr, “Rainbows revisited: Model-\ning effective colormap design for graphical inference,”\nIEEE Trans. Visualization and Computer Graphics\n(TVCG) , vol. 27, no. 2, pp. 1032–1042, 2021.\n[34] K. Reda, A. A. Salvi, J. Gray et al. , “Color nameability\npredicts inference accuracy in spatial visualizations,”\nComputer Graphics Forum (CGF) , vol. 40, no. 3, pp.\n49–60, 2021.\n[35] M. Wattenberg, F. B. Viégas, and K. Hollenbach, “Vi-\nsualizing activity on Wikipedia with chromograms,”\ninHuman-Computer Interaction – INTERACT 2007 .\nSpringer, 2007, pp. 272–287.\n[36] J. R. Nuñez, C. R. Anderton, and R. S. Renslow, “Op-\ntimizing colormaps with consideration for color vision\ndeﬁciency to enable accurate interpretation of scientiﬁc\ndata,” PLOS ONE , vol. 13, no. 7, pp. 1–14, 2018.[37] M. Bostock, V . Ogievetsky, and J. Heer, “D3: Data-\ndriven documents,” IEEE Trans. on Visualization and\nComputer Graphics (TVCG) , vol. 17, no. 12, pp. 2301–\n2309, 2011.\n[38] R. Arias-Hernandez, L. T. Kaastra, T. M. Green et al. ,\n“Pair analytics: Capturing reasoning processes in col-\nlaborative visual analytics,” in 44th Hawaii Int. Conf.\nSystem Sciences (HICSS) , 2011, pp. 1–10.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n699"
    },
    {
        "title": "BPS-Motif: A Dataset for Repeated Pattern Discovery of Polyphonic Symbolic Music.",
        "author": [
            "Yo-Wei Hsiao",
            "Tzu-Yun Hung",
            "Tsung-Ping Chen",
            "Li Su 0004"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265277",
        "url": "https://doi.org/10.5281/zenodo.10265277",
        "ee": "https://zenodo.org/records/10265277/files/000032.pdf",
        "abstract": "Intra-opus repeated pattern discovery in polyphonic symbolic music data has  challenges in both algorithm design and data annotation. To solve these challenges, we propose BPS-motif, a new symbolic music dataset containing the note-level annotation of motives and occurrences in Beethoven's piano sonatas. The size of the proposed dataset is larger than previous symbolic datasets for repeated pattern discovery. We report the process of dataset annotation, specifically a peer review process and discussion phase to improve the annotation quality. Finally, we propose a motif discovery method which is shown outperforming baseline methods on repeated pattern discovery.",
        "zenodo_id": 10265277,
        "dblp_key": "conf/ismir/HsiaoHC023",
        "keywords": [
            "Intra-opus",
            "repeated pattern discovery",
            "polyphonic symbolic music data",
            "algorithm design",
            "data annotation",
            "BPS-motif",
            "symbolic music dataset",
            "note-level annotation",
            "Beethovens piano sonatas",
            "dataset annotation"
        ],
        "content": "BPS-MOTIF: A DATASET FOR REPEATED PATTERN DISCOVERY OF\nPOLYPHONIC SYMBOLIC MUSIC\nYo-Wei Hsiao1Tzu-Yun Hung1,2Tsung-Ping Chen1Li Su1\n1Academia Sinica, Taiwan2National Taiwan Normal University, Taiwan\nlisu@iis.sinica.edu.tw\nABSTRACT\nIntra-opus repeated pattern discovery in polyphonic sym-\nbolic music data has challenges in both algorithm design\nand data annotation. To solve these challenges, we pro-\npose BPS-motif, a new symbolic music dataset contain-\ning the note-level annotation of motives and occurrences\nin Beethoven’s piano sonatas. The size of the proposed\ndataset is larger than previous symbolic datasets for re-\npeated pattern discovery. We report the process of dataset\nannotation, speciﬁcally a peer review process and discus-\nsion phase to improve the annotation quality. Finally, we\npropose a motif discovery method which is shown outper-\nforming baseline methods on repeated pattern discovery.\n1. INTRODUCTION\nRepetition is ubiquitous in music. Computational discov-\nery of repeated patterns in music data has been long dis-\ncussed in the ﬁeld of music information retrieval (MIR).\nAside from its importance in music analysis [1], the role\nof repeated pattern discovery has also been noticed in mu-\nsic classiﬁcation [2,3] and generation [4,5]. The deﬁnition\nof a pattern is multi-fold. Generally speaking, a pattern\nrefers to a group of notes that serves a musically impor-\ntant role and occurs multiple times in a piece of music.\nRepeated patterns are known by various names, such as\nmotifs, themes, phrases and sections, depending on their\nspeciﬁc musical function. The goal of the repeated pattern\ndiscovery problem is then to ﬁnd the relevant patterns (de-\npending on the intended task) and all of their occurrences\nwithin the provided musical data.\nCompared to other music analysis tasks (e.g., harmony\nanalysis) on polyphonic symbolic music data, repeated pat-\ntern discovery is relatively less discussed due to mainly\ntwo challenges. First, searching for all the possible can-\ndidates of repeated patterns is costly and redundant [6].\nThe computational complexity of the algorithm is high,\nwhile the discovered patterns often have little musical sig-\nniﬁcance [7]. Second, repetition is a non-exact attribute\nof music. A large pattern can be potentially divided into\n© Y .-W. Hsiao, T.-Y . Hung, T.-P. Chen and L Su. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: Y .-W. Hsiao, T.-Y . Hung, T.-P. Chen and L Su, “BPS-\nMotif: A Dataset for Repeated Pattern Discovery of Polyphonic Symbolic\nMusic”, in Proc. of the 24th Int. Society for Music Information Retrieval\nConf., Milan, Italy, 2023.small ones; whether a note group constitutes a meaningful\nrepeated pattern also depends on the subjective views re-\ngarding repetition, similarity, and musical importance. As\na result, human-annotated datasets that comprehensively\nidentify all the available patterns and all of their occur-\nrences remains in a quite limited scale.\nIn this paper, we propose a new dataset, BPS-motif,\nto improve the scalability of music pattern discovery re-\nsearch. The BPS-motif dataset contains the note-level an-\nnotation of motives and their occurrences in the ﬁrst move-\nments of Beethoven’s Piano Sonatas (BPS). This is an ex-\ntension of the many previous musical annotations on BPS,\nsuch as the functional harmony, phrase and section anno-\ntation provided in the Beethoven Piano Sonata Functional\nHarmony (BPS-FH) dataset [8]. We are speciﬁcally inter-\nested in annotating the motivic units in the melody parts\nof each piece of music, which could be complementary to\nthe more thematic annotation (e.g., phrases and sections)\nprovided in the BPS-FH dataset. We expect that the pro-\nposed dataset can enrich not only multi-task MIR research\nbut also novel computational music analysis tasks.\nBesides, as another contribution of this paper, we also\npropose a simple yet effective algorithm for repeated pat-\ntern discovery. Different from previous works which em-\nphasized the equal translations among notes, we empha-\nsize the contextual relationships among short segments\nof notes. We demonstrate that the proposed algorithm\nnot only outperform several baselines on the BPS-motif\ndataset, but also on the JKU-PDD dataset [9], the most\nwidely used dataset for the discovery of repeated themes\nand sections. In other words, the proposed algorithm is\ncompetitive for ﬁnding both motivic and thematic patterns.\nThe rest of this paper is organized as follows. Section\n2 gives a background introduction and a survey of previ-\nous works on the datasets and methods for repeated pattern\ndiscovery. In Section 3, we introduce the dataset and our\nproposed annotation process. In Section 4, we introduce\nthe proposed motif discovery algorithm and demonstrate\nits evaluation results. Conclusions are made in Section 5.\n2. RELATED WORK\n2.1 Repeated pattern discovery datasets\nThe datasets for repeated pattern discovery are built mostly\nfor the interest in computational music analysis research.\nComplete annotation of repeated patterns should incorpo-\nrate all the note groups (each note in pitch-onset part) that281constitute 1) the patterns of interest and 2) the occurrences\nof each pattern. Usually, a music piece contains more\nthan one pattern, and each pattern should repeat (i.e., oc-\ncur more than twice). The occurrences of a pattern may\nnot be the same; one occurrence can be an exact copy of,\nor a variation from another occurrence that belongs to the\nsame pattern. The music data can be either monophonic or\npolyphonic, and can be in either symbolic or audio format.\nThe annotation can be either intra-opus orinter-opus [10].\nIn the former case, the analysis focuses on how a piece of\nmusic is broken down into pattern occurrences by having\noccurrences of a pattern within one music piece [7, 10].\nIn the latter case, the analysis focuses on the evolution of\ncommon elements in a corpus, by having occurrences of a\npattern in different pieces of music. It should be noted that\nthe annotation in inter-opus datasets can be limited to only\na small set of patterns, while intra-opus datasets need a\ncomprehensive set of patterns and occurrences and is hard\nto build; see Table 1 and the discussion below.\nTable 1 presents the datasets for both inter-opus and\nintra-opus pattern discovery. In the Saraga dataset, Srini-\nvasamurthy et al. annotated 4,571 temporal occurrences\nfrom 1,067 characteristic melodic phrases , a musical unit\nrelated to the r¯aga, over 170 audio recordings [11]. Krause\net al. performed large-scale leitmotif classiﬁcation in au-\ndio recordings by annotating the time intervals of 10 leit-\nmotifs in Richard Wagner’s four-opera cycle Der Ring des\nNibelungen and achieve a large scale of occurrence over 16\nversions of recordings [12].1In the MTC-ANN dataset,\nKranenburg et al. categorized 93 patterns in 360 mono-\nphonic folk tunes and annotated 1,657 occurrences [13].\nFinkensiep et al. considered 20 types of schemata and\nannotated 244 events in Mozart’s piano sonatas [14]. In\nthe MIREX campaign of Discovery of Repeated Patterns\nand Themes , Collins et al. ﬁrstly compiled an organized,\nopen-source intra-opus pattern discovery dataset contain-\ning 165 occurrences in ﬁve pieces and this dataset has\nbeen widely discussed in the follow-up research works. It\nshould be noted that, among these datasets, only the JKU-\nPDD dataset is for intra-opus pattern discovery but its size\nis smallest among all (only ﬁve pieces of music).\nAside from the above-mentioned datasets, it is still\nworth mentioning the datasets for pattern matching [15],\nsuch as the Dig That Lick dataset for Jazz music [16] and\nthe Theme Finder for Classical music [17]. These datasets\nsupport pattern retrieval tasks with known query, but they\nneither support pattern discovery research nor provide the\nannotation of pattern occurrences explicitly.\n2.2 Repeated pattern discovery methods\nFor symbolic music data, there are three major approaches\nto implementing the repeated pattern discovery algorithms:\n1) string-based approach which represents music data as\none-dimensional pitch sequence and ﬁnds repeated pat-\nterns with sub-string matching [18, 19]; 2) geometry-\nbased approach which represents music data as multi-\ndimensional point sets (usually onset-pitch pairs in two-\n1There are in total 38,448 occurrences if counting the 16 versions.format usage #ps #ptns #ocrs\n[11] poly audio inter 170 1,067 4,571\n[12] poly audio inter 11 10 2,403\n[13] mono symbolic inter 360 93 1,657\n[14] poly symbolic inter 54 20 244\n[9] poly symbolic intra 5 32 165\nOurs poly symbolic intra 32 263 4,944\nTable 1 : Comparison of several open-source musical re-\npeated pattern datasets including the saraga dataset [11],\nThe Ring (one performance version) [12], MTC-ANN\n[13], Schemata [14], JKU-PDD [9], and BPS-motif (ours).\nThe number of pieces (#ps), the number of individual pat-\nterns (#ptns), and the number of occurrences (#ocrs) are\nlisted. The data formats can be monophonic (mono) or\npolyphonic (poly), audio or symbolic. The type of annota-\ntion can be inter-opus (inter) or intra-opus (intra).\ndimensional space) and retrieves the translatable subsets\n(see discussion below) as repeated patterns [20–22]; 3)\nfeature-based approach which extracts or learns features\nfrom music data, and retrieves patterns with clustering or\nclassiﬁcation of the features [14, 23–25].\nWhile the string-based approach falls limited in repre-\nsenting polyphonic music [22], research efforts on pattern\ndiscovery have been more emphasized on the geometry-\nbased approach. In the geometry-based approach, we con-\nsider a music piece DwithNnotes and ddenotes a note.\nWe haveD:={di}N\ni=1, wheredi:= (oi,pi)denotes the\nith note, and oi,pidenote its onset and pitch value, re-\nspectively. In the discussion of the structure induction al-\ngorithm with translational equivalence classes (SIATEC)\n[20], two subsets (i.e., two patterns) mandninDare\ntranslatable (denoted as n≡m) if there exists a vector v\nsuch that the translation function f(d,v) :m→n;d/mapsto→\nd+vis bijective. All the patterns translatable with respect\ntomform a translational equivalence class (TEC) of min\nD, that means\nTEC(m,D) :={n:n≡m,n⊆D}. (1)\nAmaximal translatable pattern (MTP) is the largest\npattern translatable by a translatable vector v[20]:\nMTP(v,D) := max\n|d|{d:d∈Dandd+v∈D},(2)\nwhere|d|is the number of notes in d. SIATEC is then an\nalgorithm which ﬁnds all the TEC of the available MTPs in\nD. A survey and comparative study can be found in [26].\nIn the feature-based approach, machine learning tech-\nniques are usually applied; features are processed by clus-\ntering for the pattern discovery task (when a query is not\ngiven), and by classiﬁcation for the pattern matching task\n(when a query is given) [15]. For example, in [23], ag-\nglomerative clustering over the wavelet transform of the\npitch sequence data was used for pattern discovery in\nmelodies. In [14], music schema recognition was per-\nformed by extracting the schema candidates using a skip-\ngram model and then a binary classiﬁcation on the rhythmProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n282and pitch features over the candidates. It is also noted that\nthe feature-based approach has also been widely discussed\nin the repeated pattern discovery of audio. In [24], Nuttall\net al. adopted matrix proﬁle, a time-series-based motif dis-\ncovery method [27], on the predominant pitch contours to\nextract the characteristic melodic phrases from audio [11].\nKrause et al. utilized recurrent neural networks (RNN) to\nclassify over 30,000 leitmotifs over differnt performances\nofDer Ring des Nibelungen [25].\n3. DATASET\n3.1 Overview\nThe BPS-motif dataset contains the annotation of motives\nin the ﬁrst movements of Beethoven’s 32 piano sonatas.\nAn annotation unit contains a group of motif notes and\nthe corresponding motif label. The motif labels are sorted\nin alphabetical order: the motif that occurs ﬁrst in the\nmusic piece is labeled as A, the secondly occurred mo-\ntif is labeled as B, the thirdly occurred one is C, and\nso on. The group of notes which are the jth occurrence\nof the motif AinDis annotated as mA,j,mA,j⊂D,\nj∈Z≥0. All the occurrences of this motif are anno-\ntated with A. Further information, such as the start time\nand end time of each motif occurrence, and the non-motif\nnotes (i.e., the notes which do not belong to any motif) can\nbe directly derived. The dataset is available at: https:\n//github.com/Wiilly07/Beethoven_motif .\nOver the 32 music pieces, we labeled 263 distinct mo-\ntives with 4,944 occurrences in total (see Table 1). These\noccurrences contain 36,652 notes, which is 28.87% of the\ntotal number of 126,943 notes. For each piece of music,\nthe number of motives ranges from 2 to 13 (average 8.22\nmotives), and the number of occurrences ranges from 41 to\n290 (average 154.5 occurrences). On average, a motif con-\ntains 7.41 notes and spans 5.30 crochet beats. The pitch\nranges of the motives are mostly within two octaves.\nTo facilitate the annotation process, we only consider\nthe repeated patterns in melodic notes; that means, all the\nannotated motives are constrained to be a monophonic note\nsequence. For example, in Figure 1a, although the ﬁrst beat\nof the ﬁrst measure contains three notes (i.e., B3, D4, G4),\nonly G4 is included in the the annotated motif A0. How-\never, there can be multiple motives which are fully or partly\noverlapped in time; see the demonstration in Figure 1c (the\nred and blue boxes represent two overlapped motives).\n3.2 Data format\nWe basically followed the data format adopted in the BPS-\nFH dataset. First, all the articulation symbols and grace\nnotes were omitted (see Figure 1a). Second, pickup was\nﬁlled when needed (see Figure 1b and the following dis-\ncussion). Repeat signs are also unfolded when needed.\nWe take a crotchet beat as the unit time step (i.e., the\nduration of a crotchet is 1 in our note annotation and is\n1 second in MIDI) to represent the data. Two types of\ntimestamps are recorded. The score time takes the pickup\nmeasure as negative while the MIDI time ﬁlls the pickup\n(a) Grace note removal/ taking the monophonic motif\n(b) Filling the pickup measure\n(c) Annotating overlapped motives\nFigure 1 : Examples of annotated motives. From (a) to\n(c), the three demonstrated excerpts are from Beethoven’s\nPiano Sonata No. 20, No. 1, and No. 5, respectively. The\nnotes bounded by a colored box form a motif occurrence.\nmeasure and deﬁnes the beginning of the measure as 0.\nFor example, in Figure 1b, the score time of the C4 note\nat the beginning is -1 while the MIDI time is 3. Both the\nscore time and the MIDI time unfolds the repeat signs so\nthe timestamps increase monotonically. Similarly, at the\nmeasure level, the score measure number is the measure\nnumber counted on the score sheet (the pickup measure\nis measure 0, with repeat signs), while the MIDI measure\nnumber takes the pickup measure (if there is) as measure\n1 and unfolds the repeat signs. Two types of pitch number\nare recorded: the MIDI pitch (in MIDI number) and the\nmorphetic pitch number [28].\nFor each piece of music, we provide annotation data in\ndifferent formats for users to retrieve the motif events in\ndifferent ways. The ﬁle formats include:\n1. A multi-track MIDI ﬁle that records the motif notes.\nTemporally overlapped motives are recorded in dif-\nferent tracks. There are at most four tracks in our\nannotation of this dataset.\n2. A list of all the notes. Each note has the labels of\n1) onset time (in score time), 2) MIDI pitch num-\nber, 3) morphetic pitch number, 4) note duration (inProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n283Figure 2 : Motives and occurrences labeled in Beethoven’s Piano Sonata No.1 in F minor. From top to bottom shows the\nannotation of section intervals, subsection intervals, phrase intervals, the time when a new motif occurs (with motif labels),\nand the piano roll of the music piece marked with motif and non-motif notes. In the bottom subﬁgure, different motives are\nspeciﬁed by different colors. Motif occurrences are marked with a black bounding box. Non-motif notes are in gray color.\nFigure 3 : Assessment results (Q1 to Q4) regarding the data\nannotation from the seven reviewers. The results were col-\nlected before the discussion phase.\ncrotchet beats), 5) staff number (integers from zero\nfor the top staff), 6) MIDI measure number, and 7)\nmotif (e.g., a note is annotated as Aif it is part of A).\nThe notes without motif labels are non-motif notes.\n3. Individual note lists of each motif occurrence. These\nlists are provided for users to better retrieve each oc-\ncurrence. The labels in these lists are the same as the\nones in the list of all notes.\n4. A list describing the properties of motif occurrences.\nEach motif occurrence has the labels of 1) the start\ntime and end time (in both score time and MIDI\ntime), 2) the duration of the occurrence, 3) the mea-\nsure number where the motif start (in both score\nmeasure and MIDI measure), 4) the “start beat” of\nthe motif start, and 5) time signature.\nWe also provide the PDF scoresheets with the annota-\ntor’s manual annotations and notes. These scoresheets are\nfor reference only because they are the raw annotations and\nmay not be the same as the annotation of our ﬁnal version;\nsee Section 3.3 for more details about the annotation pro-\ncess. The note lists and the score time are compatible with\nthe BPS-FH dataset, therefore annotation of more thematic\nunits (e.g., theme, sub-section and phrase) can be retrieved\nfrom the BPS-FH dataset. To better see our annotation re-\nsult, Figure 2 illustrates the hierarchical musical structureswith motives of Beethoven’s Piano Sonata No. 1, combin-\ning the section, subsection and phrase labels in BPS-FH,\nand the motif labels in BPS-motif.2\n3.3 Annotation process\nThere are a few challenges in the data annotation process.\nFirst, as mentioned, identifying of musical motifs and their\nrepetitions or variations in a piece of music is not straight-\nforward. Ambiguity arises from multiple factors. For ex-\nample, some repeated patterns may not be considered as\nvalid musical motifs, a motif may not always be the small-\nest unit of a repeated pattern, and the similarity or differ-\nence between two such sequences can also be subject to hu-\nman interpretation. Besides, while experienced musicians\ncan read the scoresheet and mark the motives directly by\nhand on it, converting such hand-drawing annotations into\ndatabase formats still requires lots of efforts.\nOur proposed approach to build the BPF-motif dataset\nincorporates three parts: annotation, review, and score typ-\ning. First, two annotators (the ﬁrst and the second authors)\nmanually annotate the motives on the scoresheet. Each\npiece is annotated by one annotator. Then, we invite ex-\nternal reviewers to review annotated scoresheets. Also, the\nreviewer helps us digitize the manual annotation. In the re-\nview process, we design a review form to let the reviewers\nassess the overall quality of annotation and also provide\ntheir suggested annotation if they hold different opinions.\nThe review form contains the following questions:\n1. (Q1) Are the annotations reasonable? (3: totally rea-\nsonable; 2: mostly reasonable; 1: unreasonable)\n2. (Q2) Are the annotations coherent with your opin-\nion? That means, if you were the annotator, will you\n2It should be noted that there are still some annotation inconsistency\nbetween the BPS-FH and BPS-motif datasets. For example, in Figure 2,\nthe phrase cis constructed only with the motif D, while the phrase c′′′is\nconstructed only with the motif H. This means that while the annotator\nof BPS-FH considered c′′′as simply a variation of c, the annotator of\nBPS-motif considered them being different (and are thereon constructed\nwith different motives).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n284Figure 4 : Two segments (light gray and light purple re-\ngions) and their common structure. The crosses indicate\nnotes, and the set of vectors in a segment represents its\nstructure. Blue vectors denote the common structure which\nexists in both segments. The dashed gray arrows represents\nnon-motivic notes within the two segments.\nalso have the same annotations as ours? (3: totally\ncoherent; 2: mostly coherent; 1: incoherent)\n3. (Q3) Are the annotations consistent (i.e., did we hold\nconsistent criteria annotating the data)? (3: totally\nconsistent; 2: mostly consistent; 1: inconsistent)\n4. (Q4) In which way your opinions are different from\nours? (a: we took multiple motives into one ( un-\ndersegmentation ); b: we divided a motif into many\n(oversegmentation ); c: we took some non-motif\npatterns as motives ( overlabeling ); d: we omitted\nsome musically important motives ( underlabeling ))\nChoose one even you totally agree to our annotation.\n5. (Q5) If you hold different opinions on our annotation\nand think we should revise them, leave your com-\nments explicitly. Your comments can be, for exam-\nple, “the motif Ein Sonata No. xshould be further\ndivided into FandG” (describe what FandGare);\n“the motif Hin Sonata No. ycan be considered as a\nvariation of Band should be merged,” etc.\nSeven reviewers were invited to review the annotations.\nThe reviewers are all from composition background and\nare good at using computer scorewriters. Each reviewer\nwas assigned from 3 to 7 pieces (according to the length\nof the music piece) for review, then they answered the\nabove questions and provided their suggested annotations\non a co-edited document. During the review and discus-\nsion phase, the reviewers also need to convert the manual\nannotation on the score into the symbolic form using the\nscorewriter MuseScore. This conﬁrms that they had care-\nfully read the annotation, and also speed up the process of\nbuilding the dataset. After the reviewers typed the scores of\nthe annotated motives, we can directly convert it to MIDI\nand the ﬁnal annotation data.\nThe reviewer’s assessment results are shown in Fig. 3.\nFrom Q1 to Q3, it is shown that no reviewer reported our\nannotation as unreasonable, incoherent to their thoughts,\nor self-inconsistent. However, over half of the review-\ners did point out a few annotation they considered prob-\nlematic. We discussed with the reviewers regarding those\nissues and revised them such that all the annotations areAlgorithm 1 Find Common Structure\n1:function COMMON STRUCTURE (D,∆t)\n2:S←∅\n3: fori←1toN−1do\n4: Ci←∅\n5: forj←i+1toNdo\n6: ifoj−oi<∆tthen\n7: adddjtoCi\n8: end if\n9: end for\n10: Si←{dj−di,dj∈Ci}\n11: addSitoS\n12: end for\n13:\n14:M←∅\n15: fori←1toN−1do\n16: ˆS←∅\n17: forj←i+1toNdo\n18: add{Si∩Sj}toˆS\n19: end for\n20: addMOST _COMMON( ˆS)toM\n21: end for\n22: returnM\n23:end function\nacceptable for the reviewer. The result of Q4 shows that\nreviewers tend to say our annotations are oversegmented.\nThis however ﬁts our needs because doing this provides ex-\ntra ﬂexibility to the dataset; researchers who are interested\nin longer repeated patterns can simply merge our annota-\ntions. On the other hand, it is hard to retrieve short motivic\npatterns from undersegmented annotation.\n4. MOTIF DISCOVERY\n4.1 Algorithm\nWe regard a motif as a short pattern recurring with little\nchange in its structure. In other words, the relative posi-\ntions of the notes in a motif will be almost ﬁxed. We there-\nfore ﬁnd motifs by detecting common structures in short\nmusical segments. The idea of the proposed algorithm is\npresented in Figure 4 and Algorithm 1. Formally, let ∆t\ndenote a threshold of time interval, and D:={di}N\ni=1a\nmusical piece composed of Nnotes sorted in ascending\norder, with di= (oi,pi)being a two-dimensional vector\nindicating the onset and pitch number of the ith note. For\ndi, we ﬁrst aggregate its context Ciand create a segment\nSi. The derived segments are then compared pairwisely to\nobtain common structures. By representing a segment as a\nset of vectors (see Figure 4 and Line 3–12 in Algorithm 1),\nthe common structure of any two segments (i.e., the blue\narrows in Figure 4) can be obtained by collecting vectors\nwhich exist in both segments (Line 18).\nAs the pairwise comparisons between segments (Line\n15-19) will result in various types of common structures,\nwe retrieve a representative pattern and all its occurrences\nby ﬁnding the “most common” structure (i.e., the com-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n285Algorithm P est Rest Fest Pocc Rocc Focc Pthr Rthr Fthr Runtime\nSIATEC 0.1804 0.6444 0.2803 0.2102 0.2771 0.2235 0.0408 0.2994 0.0713 28.5082\nCOSIATEC 0.2118 0.4557 0.2863 0.2769 0.1282 0.1548 0.0489 0.1601 0.0743 208.4119\nSIATECCompress 0.2136 0.4326 0.2835 0.1430 0.1121 0.1103 0.0579 0.1703 0.0856 636.6930\nProposed 0.5709 0.8339 0.6733 0.1491 0.4174 0.2002 0.1222 0.2644 0.1646 119.5330\n(a) Motif discovery on the proposed dataset\nAlgorithm P est Rest Fest Pocc Rocc Focc Pthr Rthr Fthr Runtime\nSIATEC 0.1238 0.4630 0.1920 0.5248 0.3970 0.4437 0.0706 0.4006 0.1176 1.5099\nCOSIATEC 0.1140 0.2530 0.1491 0.1305 0.0870 0.1044 0.0740 0.2042 0.1027 6.0167\nSIATECCompress 0.1807 0.2849 0.2181 0.1778 0.0889 0.1185 0.1117 0.2202 0.1470 34.6371\nProposed 0.2649 0.5002 0.3406 0.4208 0.5105 0.3948 0.1096 0.3003 0.1561 4.0546\n(b) Repeated pattern discovery on the JKU-PDD dataset\nTable 2 : Evaluation of pattern discovery algorithms. The subscripts est,occ, and thrindicate the establishment ,occurrence ,\nandthree-layer measurements, respectively. The averaged runtime is in minutes.\nmon structure that occurs the most times) in ˆSwith the\nMOST _COMMON operation (Line 20). Finally, motifs\nare acquired by ﬁltering out non-motivic patterns in M\nheuristically. In this work, we set ∆t=12 crotchet beats.\nThe proposed algorithm differs from the SIA family in\ntwo aspects. First, the SIA family aggregates notes of a\npattern by detecting equal translations among notes, while\nour algorithm ﬁnds patterns by identifying common struc-\ntures, or contextual relationships , among small segments.\nSecond, the SIA family computes maximal translatable\npatterns (MTP) and subsequently ﬁnd their occurrences,\nwhereas our algorithm establishes a small pattern and all\nits occurrences at the same time. Our approach is promis-\ning in that the contextual comparisons between segments\nhelp identify motifs which are small and recurring. The\ncode of the proposed algorithm is available at https://\ngithub.com/Tsung-Ping/motif_discovery .\n4.2 Evaluation\nWe evaluate the motif discovery algorithm on the proposed\ndataset (with an averaged number of 3937 notes per piece)\nas well as the JKU-PDD dataset (1284 notes in average) [9]\nusing standard metrics for pattern discovery. The estab-\nlishment measurement (est) shows the capability of an al-\ngorithm to recognize patterns rather than to ﬁnd all occur-\nrences of a pattern. The occurrence measurement (occ), on\nthe contrary, emphasizes the ability to ﬁnd all occurrences\nof a pattern. The three-layer measurement (thr) is a com-\nprehensive evaluation combining aspects of both the estab-\nlishment and occurrence measurements. Each of the three\nmeasurements are speciﬁed in terms of precision, recall,\nand F1 score.3Theaveraged runtime on each dataset will\nalso be measured to give a rough sketch of the time com-\nplexity. We compare the proposed algorithm with three\nmethods from the SIA family, i.e., SIATEC [20], COSI-\n3For more detailed deﬁnitions of the three evaluation measure-\nments, refer to https://www.music-ir.org/mirex/wiki/\n2017:Discovery_of_Repeated_Themes_\\%26_Sections .ATEC [21], and SIATECCompress [21].4All algorithms\nwere implemented in Python programming language.\nThe evaluation results are summarized in Table 2. Gen-\nerally, our algorithm performs consistently across datasets\ndespite that the two datasets are composed of distinct types\nof musical patterns, i.e., motivic (the proposed) versus the-\nmatic (the JKU-PDD), which differ with each other mainly\nin the size. Our algorithm is superior to the baselines in\nall the three establishment measures, indicating that our\nmethod can identify more existences of the ground-truth\npatterns than the other algorithms. Besides, our algorithm\nis competent in the other two measurements, with at least\none best performance in each measurement. Speciﬁcally,\nthe R occmeasure shows that the patterns retrieved by our\nalgorithm are more complete (i.e., discovering more occur-\nrences of a pattern) with respect to the ground-truth pat-\nterns, and the F thrmeasure suggests that our method has\nbetter capability to recognize salient patterns in music, es-\npecially the motivic ones. Finally, the runtime measure-\nment indicates that our algorithm can achieve a better per-\nformance on the pattern discovery tasks at a moderate com-\nputational cost, which is 4.2 (resp. 2.7) times slower than\nthe SIATEC on the proposed (reps. JKU-PDD) dataset.\n5. CONCLUSION\nWe have demonstrated a dataset for repeated pattern dis-\ncovery of polyphonic symbolic data and a motif discovery\nalgorithm. Our data annotation clearly demonstrates the\nhierarchical structure of music. The proposed motif dis-\ncovery algorithm has been shown outperforming the base-\nline methods on various repeated pattern discovery prob-\nlems. These ﬁndings suggest a direction for developing re-\npeated pattern discovery algorithms, and also evoke further\ninvestigation on music structure analysis, novelty analysis,\nand repeated pattern discovery algorithms.\n4For the three baseline algorithms, we use the implementa-\ntions available at https://github.com/wsgan001/repeated_\npattern_discovery .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2866. ACKNOWLEDGEMENT\nThe contribution of each author is as follows. Yo-Wei\nHsiao performed data annotation and compiled the whole\ndataset. Tzu-Yun Hung performed data annotation and\nthe data review process. Tsung-Ping Chen developed the\nmotif discovery algorithm. Lastly, Li Su contributed in\nproject supervision and paper writing. Also, the authors\nwould like to thank Li-Rong Huang, Hsing-Chen Lin, Yu-\nFang Hsu, Po-Hsuan Huang, Joseph-On-King Lau, Pei-\nLing Kuo, and Chia-Han Li from the Department of Mu-\nsic, National Taiwan Normal University, for their efforts in\nreviewing and digitizing our data annotation.\n7. REFERENCES\n[1] D. Meredith, Ed., Computational music analysis .\nSpringer Cham, 2016.\n[2] P. Boot, A. V olk, and W. B. de Haas, “Evaluating the\nrole of repeated patterns in folk song classiﬁcation and\ncompression,” Journal of New Music Research , vol. 45,\nno. 3, pp. 223–238, 2016.\n[3] C. Louboutin and D. Meredith, “Using general-\npurpose compression algorithms for music analysis,”\nJournal of New Music Research , vol. 45, no. 1, pp. 1–\n16, 2016.\n[4] Y .-J. Shih, S.-L. Wu, F. Zalkow, M. Müller, and Y .-H.\nYang, “Theme transformer: Symbolic music genera-\ntion with theme-conditioned transformer,” IEEE Trans-\nactions on Multimedia , March 2022.\n[5] Z. Hu, X. Ma, Y . Liu, G. Chen, and Y . Liu, “The\nbeauty of repetition in machine composition scenar-\nios,” in Proceedings of the 30th ACM International\nConference on Multimedia , Lisboa, Portugala, 2022,\npp. 1223–1231.\n[6] D. Meredith, “RECURSIA-RRT: Recursive translat-\nable point-set pattern discovery with removal of re-\ndundant translators,” in International Workshops of\nthe European Conference on Machine Learning and\nPrinciples and Practice of Knowledge Discovery in\nDatabases (ECML PKDD) 2019 , Würzburg, Germany,\n2019, pp. 485–493.\n[7] O. Björklund, “Siatec-c: Computationally efﬁcient re-\npeated pattern discovery in polyphonic music,” in Pro-\nceedings of the 23rd International Society for Music\nInformation Retrieval Conference (ISMIR) , Bengaluru,\nIndia, 2022, pp. 59–66.\n[8] T.-P. Chen and L. Su, “Functional harmony recogni-\ntion of symbolic music data with multi-task recurrent\nneural networks,” in Proceedings of the 19th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Paris, France, 2018, pp. 90–97.[9] T. Collins. 2013:Discovery of Repeated\nThemes & Sections. [Online]. Avail-\nable: https://www.music-ir.org/mirex/wiki/2013:\nDiscovery_of_Repeated_Themes_%26_Sections\n[10] D. Conklin and C. Anagnostopoulou, “Representation\nand discovery of multiple viewpoint patterns,” in Pro-\nceedings of the 2001 International Computer Music\nConference (ICMC) , Havana, Cuba, 2001, pp. 479–\n485.\n[11] A. Srinivasamurthy, S. Gulati, R. C. Repetto, and\nX. Serra, “Saraga: Open datasets for research on in-\ndian art music,” Empirical Musicology Review , vol. 16,\nno. 1, pp. 85–98, December 2021.\n[12] M. Krause, F. Zalkow, J. Zalkow, C. Weiß, and\nM. Müller, “Classifying leitmotifs in recordings of op-\neras by Richard Wagner,” in Proceedings of the Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , Montréal, Canada, 2020, pp. 473–\n480.\n[13] P. van Kranenburg, B. Janssen, and A. V olk, “The\nmeertens tune collections: The annotated corpus (mtc-\nann) versions 1.1 and 2.0.1,” Meertens Online Reports ,\nvol. 2016, no. 1, 2016.\n[14] C. Finkensiep, K. Déguernel, M. Neuwirth, and\nM. Rohrmeier, “V oice-leading schema recognition us-\ning rhythm and pitch features,” in Proceedings of the\n21st International Society for Music Information Re-\ntrieval Conference (ISMIR) , Montreal, Canada, 2020,\npp. 520–526.\n[15] B. Janssen, W. B. De Haas, A. V olk, and P. Van Kra-\nnenburg, “Finding repeated patterns in music: State of\nknowledge, challenges, perspectives,” in Sound, Mu-\nsic, and Motion: 10th International Symposium on\nComputer Music Multidisciplinary Research (CMMR) ,\nMarseille, France, 2014, pp. 277–297.\n[16] K. Frieler, F. Höger, M. Pﬂeiderer, and S. Dixon, “Two\nweb applications for exploring melodic patterns in jazz\nsolos,” in Proceedings of the 19th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nParis, France, 2018, pp. 777–783.\n[17] D. Huron. Theme ﬁnder. [Online]. Available: http:\n//www.themeﬁnder.org/\n[18] J.-L. Hsu, A. L. Chen, and C.-C. Liu, “Efﬁcient repeat-\ning pattern ﬁnding in music databases,” in Proceedings\nof the 7th international conference on Information and\nknowledge management , Maryland, USA, 1998, pp.\n281–288.\n[19] E. Cambouropoulos, M. Crochemore, C. Iliopoulos,\nL. Mouchard, and Y . Pinzon, “Algorithms for comput-\ning approximate repetitions in musical sequences,” In-\nternational Journal of Computer Mathematics , vol. 79,\nno. 11, pp. 1135–1148, 2002.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n287[20] D. Meredith, K. Lemström, and G. A. Wiggins, “Algo-\nrithms for discovering repeated patterns in multidimen-\nsional representations of polyphonic music,” Journal\nof New Music Research , vol. 31, no. 4, pp. 321–345,\n2002.\n[21] D. Meredith, “COSIATEC and SIATECCompress:\nPattern discovery by geometric compression,” in Music\nInformation Retrieval Evaluation eXchange (MIREX) ,\nCuritiba, Brazil, 2013.\n[22] ——, “Point-set algorithms for pattern discovery\nand pattern matching in music,” in Dagstuhl Semi-\nnar Proceedings on Content-Based Retrieval , Schloss\nDagstuhl, Germany, 2006.\n[23] G. Velarde, D. Meredith, and T. Weyde, “A wavelet-\nbased approach to pattern discovery in melodies,”\nD. Meredith, Ed. Springer Cham, 2016, pp. 303–333.\n[24] T. Nuttall, G. Plaja, L. Pearson, and X. Serra, “The\nmatrix proﬁle for motif discovery in audio-an exam-\nple application in carnatic music,” in Proceedings of\nthe 15th International Symposium on Computer Mu-\nsic Multidisciplinary Research (CMMR) , Online, 2021,\npp. 109–118.\n[25] M. Krause, F. Zalkow, J. Zalkow, C. Weiß, and\nM. Müller, “Classifying leitmotifs in recordings of op-\neras by richard wagner,” in Proceedings of the 21st\nInternational Society for Music Information Retrieval\nConference (ISMIR) , Montreal, Canada, 2020, pp.\n473–480.\n[26] I. Ren, A. V olk, W. Swierstra, and R. C. Veltkamp, “A\ncomputational evaluation of musical pattern discovery\nalgorithms,” arXiv preprint arXiv:2010.12325 , 2020.\n[27] C.-C. M. Yeh, Y . Zhu, L. Ulanova, N. Begum, Y . Ding,\nH. A. Dau, D. F. Silva, A. Mueen, and E. Keogh,\n“Matrix proﬁle i: all pairs similarity joins for time se-\nries: a unifying view that includes motifs, discords and\nshapelets,” in 2016 IEEE 16th international conference\non data mining (ICDM) , Barcelona, Spain, 2016, pp.\n1317–1322.\n[28] D. Meredith, “Computing pitch names in tonal music:\na comparative analysis of pitch spelling algorithms,”\nPh.D. dissertation, St. Anne’s College, University of\nOxford, 2007.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n288"
    },
    {
        "title": "The Batik-Plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations.",
        "author": [
            "Patricia Hu",
            "Gerhard Widmer"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265283",
        "url": "https://doi.org/10.5281/zenodo.10265283",
        "ee": "https://zenodo.org/records/10265283/files/000034.pdf",
        "abstract": "We present the Batik plays Mozart Corpus, a piano performance dataset\ncombining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored Bösendorfer grand piano, and are available both as MIDI files and audio recordings. They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can\nfurther be connected to the musicological annotations (harmony, cadences,\nphrases) on these scores that were recently published by [1].\n\nThe result is a high-quality, high-precision corpus mapping scores and musical\nstructure annotations to precise note-level professional performance information.\nAs the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects.\n\nIn the paper, we outline the curation process of the alignment and conduct two\nexploratory experiments to demonstrate its usefulness in analyzing expressive performance.\n\n[1] Hentschel, J., Neuwirth, M.,  Rohrmeier, M. (2021). The Annotated Mozart Sonatas: Score, Harmony, and Cadence. Transactions of the International Society for Music Information Retrieval (TISMIR), Vol. 4, No. 1, pp. 67-80.",
        "zenodo_id": 10265283,
        "dblp_key": "conf/ismir/HuW23",
        "keywords": [
            "piano performance dataset",
            "professional Mozart piano sonata performances",
            "expert-labelled scores",
            "note-precise level",
            "computer-monitored Bösendorfer grand piano",
            "MIDI files",
            "audio recordings",
            "precise alignment",
            "New Mozart Edition",
            "musicological annotations"
        ],
        "content": "THE BATIK-PLAYS-MOZART CORPUS: LINKING\nPERFORMANCE TO SCORE TO MUSICOLOGICAL ANNOTATIONS\nPatricia Hu1Gerhard Widmer1,2\n1Institute of Computational Perception, Johannes Kepler University Linz, Austria\n2LIT AI Lab, Linz Institute of Technology, Austria\nfirstname.lastname@jku.at\nABSTRACT\nWe present the Batik-plays-Mozart Corpus, a piano per-\nformance dataset combining professional Mozart piano\nsonata performances with expert-labelled scores at a note-\nprecise level. The performances originate from a record-\ning by Viennese pianist Roland Batik on a computer-\nmonitored Bösendorfer grand piano, and are available both\nas MIDI ﬁles and audio recordings. They have been pre-\ncisely aligned, note by note, with a current standard edition\nof the corresponding scores (the New Mozart Edition) in\nsuch a way that they can further be connected to the mu-\nsicological annotations (harmony, cadences, phrases) on\nthese scores that were recently published by [1].\nThe result is a high-quality, high-precision corpus map-\nping scores and musical structure annotations to precise\nnote-level professional performance information. As the\nﬁrst of its kind, it can serve as a valuable resource for\nstudying various facets of expressive performance and\ntheir relationship with structural aspects.\nIn the paper, we outline the curation process of the\nalignment and conduct two exploratory experiments to\ndemonstrate its usefulness in analyzing expressive perfor-\nmance.\n1. INTRODUCTION\nMusic performance is a complex and nuanced activity that\ninvolves the interplay of various expressive features such\nas timing, dynamics, and articulation. Expressive per-\nformance research in music information retrieval (MIR)\nfocuses on modeling expressive aspects of music perfor-\nmance by analyzing how performers use nuances in tim-\ning, dynamics, articulation, and other expressive features\nto convey their musical intentions, with the aim of devel-\noping computational models that can analyze, recognize,\nor synthesize expressive performances [2].\nRecent research in this ﬁeld for Western classical pi-\nano has focused on data-driven approaches both for per-\nformance generation [3,4] and data creation in the form of\n© P. Hu and G. Widmer. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nP. Hu and G. Widmer, “The Batik-plays-Mozart Corpus: Linking Perfor-\nmance to Score to Musicological Annotations”, in Proc. of the 24th Int.\nSociety for Music Information Retrieval Conf., Milan, Italy, 2023.large-scale MIDI performance data transcribed from au-\ndio recordings [5, 6]. While such data corpora can be use-\nful for comparative performance analyses and related tasks\n(e.g., performer identiﬁcation, performance style transfer),\nthey lack the necessary precision and alignment informa-\ntion (with the underlying musical score) required to pre-\ncisely map expressive intentions and parameters to under-\nlying score features.\nCompared to these large-scale transcribed MIDI\ndatasets, precise MIDI data (as recorded on computer con-\ntrolled grand pianos such as the Yamaha Disklavier or\nBoesendorfer SE/Ceus series) along with their correspond-\ning score alignment is somewhat limited in quantity and\nsize [7–9]. The performances in such datasets are typically\nsourced from advanced piano students or piano competi-\ntions, whereas the digital scores are often obtained from\nopen-source, user-curated online libraries such as Mus-\neScore1.\nRegarding the performance-to-score alignment, one\nwould ideally want to have note-by-note correspondence\ninformation; unfortunately, in the case of the largest of\nthese datasets [7], score-performance alignments are only\ngiven at a rather coarse level of beats. Score annotations\nconveying structural information such as underlying har-\nmony or phrases are even more scarce.\nTo address these limitations, we introduce the Batik-\nplays-Mozart dataset2, in which we provide a set of ex-\npert performances of 12 complete Mozart piano sonatas\n(36 distinct movements) in MIDI format by concert pi-\nanist Roland Batik, precisely aligned, at a note-by-note-\nlevel, to a standard edition (the New Mozart Edition) of\nthe score, thereby linking the performance information to\na previously published dataset [1] of expert annotations of\nthe scores in terms of harmony, cadence, and phrase struc-\nture. To the best of our knowledge, this is the ﬁrst corpus of\nits kind, combining high quality digital score and structural\nannotations with expert performances in recorded MIDI\nformat. We report two preliminary experiments to demon-\nstrate the beneﬁts of having precise performance–score–\nstructure annotation alignments.\nThe remainder of this paper is organised as follows:\nSection 2 presents a list of comparable expressive perfor-\nmance datasets currently publicly available. Section 3 de-\n1https://musescore.com/sheetmusic\n2https://github.com/huispaty/batik_plays_\nmozart297Size Modality Annotations\nDataset Pieces Performances MIDI Score Alignment Other\nASAP [7] 222 1,068 recorded MusicXML beat time and key signature\nVienna4x22 [8] 4 88 recorded MusicXML note -\nCrestMuse PEDB [9] 35 411 recorded MusicXML note phrase\nMazurkaBL [10] 44 2,000 - MusicXML beat dynamics, tempo markings\nBatik-plays-Mozart 36 36 recorded MusicXML note phrase, harmony, cadence\nTable 1 . An overview of publicly available comparable piano performance datasets for which precise recorded MIDI data,\nscore-performance alignments and/or musicological annotations are available.\nscribes the data origins, the used data formats, and the cu-\nration process. Section 4 gives an overview of the dataset,\nand Section 5 describes two preliminary experiments to\ndemonstrate the beneﬁts of performance–score–structure\nannotation alignments. Finally, Section 6 concludes the\npaper with some remarks for future work.\n2. RELATED WORK\nSeveral piano performance datasets have been published\nin the context of expressive performance analysis and per-\nformance rendering. While recently published datasets are\nconsiderably larger than Batik-plays-Mozart , they provide\nperformance recordings solely in the form of MIDI tran-\nscribed from audio recordings [5, 6] or do not include a\nhigh-quality digital score ground truth [11]. Despite the\nencouraging results demonstrated by recent transcription\nmodels, they often introduce inaccuracies, such as incor-\nrect note fragmentation, missed note onsets, and falsely\nidentiﬁed notes [12]. Similarly, certain expressive perfor-\nmance aspects such as (micro-)timing and tempo can only\nbe measured given either a temporal or note-wise score-\nperformance mapping [2]. Nevertheless, these datasets re-\nmain useful for various related tasks such as symbolic mu-\nsic generation, music transcription and tagging, or high-\nlevel comparative performance analysis.\nTable 1 presents an overview of comparable piano per-\nformance datasets currently publicly available, for which\nprecise (recorded) MIDI data, score-performance align-\nments and/or musicological annotations are available.\nAmong these datasets, ASAP [7] stands out as the most\nextensive one, both in terms of musical pieces and per-\nformer range, with 1,068 performances beat-aligned to 222\nscores, each annotated with key and time signature. In\ncomparison to ASAP, all other publicly accessible datasets\nare signiﬁcantly smaller: The Vienna 4x22 corpus [8] con-\ntains 22 different performances for excerpts of four dif-\nferent pieces, each aligned on a note level and provided\nin MusicXML3, MIDI and audio format. The CrestMuse\nPEDB v2.0 [9] provides 35 pieces note-aligned to 411 per-\nformances, with scores provided in MusicXML and MIDI\nand performances in MIDI and WA V . The dataset also con-\ntains phrase structure annotations, however, merely in the\nformat of PDF and plain text ﬁles, somewhat limiting their\n(re)usability.\nThe MazurkaBL dataset [10] consists of a corpus of 44\n3https://www.musicxml.com/Chopin Mazurkas with MusicXML scores that have been\nbeat-aligned to 2000 performances. The performances\nthemselves are not provided (neither as MIDI nor as au-\ndio); only beat positions and corresponding loudness val-\nues are given, along with the positions of tempo/dynamics\nmarkings in the score.\n3. CURATION PROTOCOL AND FILE FORMATS\n3.1 File origins\nThe MIDI performance ﬁles originate from a performance\nof twelve Mozart piano sonatas by Viennese concert pi-\nanist Roland Batik on a computer-controlled Bösendor-\nfer SE290 grand piano, the predecessor of the CEUS\nmodel. The Bösendorfer SE series measures each indi-\nvidual keystroke and pedal movement precisely, with on-\nset and offset times being captured at a time resolution of\n1.25ms. Hammer velocity values are captured in a propri-\netary ﬁle format, and converted and mapped to the 128 dy-\nnamics MIDI values (see [13] for conversion details). The\naudio recordings corresponding to those MIDI ﬁles can be\npurchased commercially4.\nThese MIDI performance data were originally aligned\nmanually, on a note-to-note level, to a symbolic encod-\ning of the score produced by our team [14, 15]. In or-\nder to make it possible to link the performance data in\nan unequivocal way to the musicological score annota-\ntions provided in the Annotated Mozart Sonatas dataset by\nHentschel et al. [1], we decided to replace our score encod-\ning in the alignments entirely by the score notes as given in\nthe their dataset, which link to their annotations directly via\nabsolute temporal score position. The scores in the Anno-\ntated Mozart Sonatas dataset conform to the New Mozart\nEdition5and are given in MuseScore format, with the har-\nmony, phrase and cadence label annotations provided in\ntabular format, as tab-separated values (TSV) ﬁles.\n3.2 The match alignment format\nWe provide the alignment between the above-mentioned\nscore and performance ﬁles in the match ﬁle format [16],\na ﬁle format for symbolic music alignment in a human-\nunderstandable textual form. It is structured sequentially,\nand the alignment information is given at the level of indi-\nvidual notes.\n4https://www.gramola.at/products/9003643987012\n5https://dme.mozarteum.at/DME/nma/start.php?l=\n2Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n298Figure 1 . Visual illustration of the alignment process.\nEach step in the alignment process is numbered accord-\ning to the textual description in Section 3.3. Steps marked\n* indicate manual correction / post-processing. Elements\nhighlighted in green are combined in the new alignment\nmatch ﬁles.\nThe encoded alignment is complete in the sense that all\nperformance and all score notes are captured. Each per-\nformance and each score note is represented with their\nrespective note ID, and their respective alignment can\nbe recorded with one out of three potential tuples: 1.\nAmatch between score note and performance note, i.e.,\n(score_id, performance_id) , 2. a deleted score\nnote(score_id, ) which represents a score note omit-\nted in the performance, or 3. an inserted performance note\n( , performance_id) , which marks a performed\nnote for which there is no corresponding score note.\nFollowing this alignment encoding, each line in a match\nﬁle corresponds to either a match , adeletion or an inser-\ntion. Additional lines express (sustain or soft) pedal in-\nformation, or encode meta information about the musical\npiece and performer. While the performance part in match\ncorresponds to a lossless encoding of a corresponding per-\nformance in MIDI format, the score part captures essential\ninformation including onset, offset and duration in beats,\nand pitch, pitch spelling, and octave information for each\nscore note.\n3.3 Curation protocol\nTo create note-level score-to-performance alignments, en-\ncoded in the match ﬁle format, between the performance\nMIDI data by pianist Roland Batik and scores and musico-\nlogical annotations by Hentschel et al. [1], we follow the\nworkﬂow as outlined below (see Fig. 1):\n1.Retrieve information from old alignment. Given\nan old alignment ﬁle, we use partitura [17] to re-trieve a score and performance representation which\nwe parse into score and performance note arrays,\nsna_o andpna_o , to sequentially capture each\n(notated and performed) note with a unique note ID.\nIn addition we retrieve a score-to-performance align-\nment,align_o , in the encoding format explained\nabove (i.e., a list of note ID tuples expressing either\na match, deletion or insertion).\n2.Retrieve score note array from MusicXML. In the\nnext step, we convert the annotated MuseScore for-\nmat scores provided by Hentschel et al. [1] to Mu-\nsicXML, assign unique note IDs to each note, and\nconvert this score representation into a second score\nnote array ( sna_s ).\n3.Unfold score note array. We update the score note\narray obtained from MusicXML, sna_s , by unfold-\ning it in accordance to the repetition structure found\nin the performance note array, pna_o .6\n4.Create score-score alignment. In this step, we\ncreate a score-to-score alignment ( align_s ) by\nmatching each note in the two score note arrays\nsna_o andsna_s using its pitch, onset and du-\nration information in beats. Any notes in sna_o\nandsna_s not matched automatically need to be\naligned manually. Missed alignments at this stage\ncan occur due to:\n•Score mistakes . These reﬂect mistakes in the\nscore (e.g., a missing note, incorrect pitch, oc-\ntave, missing modiﬁer, missing repetition or\nending markings) and require a manual correc-\ntion of the score ﬁle.\n•Differing score versions . For certain sonata\nmovements, the notated score provides an al-\nternative score version reﬂecting the ﬁrst edi-\ntion (“Erstdruck”) for certain segments of a\npiece, expressing the composer’s impromptu\nornamentation.7For the current dataset, such\nornamented versions exist in K.284iii, K.332ii,\nK.457iii.\n•Double-voiced score notes . These occur fre-\nquently in notated music, and describe a score\nnote that is notated doubly in two different\nvoices but corresponds to one performed note.\n•Grace notes . Grace notes in notated music\ncan occur in multiple forms to reﬂect differ-\nent types of ornaments such as trills, acciac-\ncature, mordents, turns etc. Depending on the\nornament type and the underlying score encod-\ning format, this may result in several notes oc-\ncurring at the same (notated) onset (and hence\n6To reﬂect the same note occuring in a repeated segment, a sufﬁx is\nadded to the ID to reﬂect the number of occurrence, i.e. for a note with ID\nn14, the repeat structure unfolding is expressed as n14-1 for the ﬁrst,\nandn14-2 for the second occurrence, respectively.\n7https://www.henle.de/en/music-column/\nmozart-piano-sonatas/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n299Figure 2 . An example of a cadenza within a piano sonata\nstarting in measure 198 in KV333, 3rd movement.\nwith zero duration) to ensure a regular measure\naccording to the time signature of that piece.\nWithout onset and duration information, these\nnotes must then be manually aligned to their\ncorresponding performed notes.\n•Cadenza and ad libitum measures. Both ca-\ndenza measures and those marked ad libitum\ncorrespond to irregular measures, that is, mea-\nsures that contain more beats than indicated in\nthe time signature (see Fig. 2). Digitally en-\ncoded, the notes in such measures are com-\nmonly notated without duration to allow for\nerror-free parsing, and thus share the same beat\nonset and need to be aligned manually.\n5.Update score-performance alignment. Here we\nupdate the score note IDs in the old alignment\n(align_o ) according to the score-score alignment\n(align_s ) to create new score-performance align-\nments,align_n . For each alignment in align_o ,\nwe then need to ensure the validity of the original\nalignment type (match, insertion or deletion). In\nparticular, for notes in the original score note ar-\nray (sna_o ) that could not be aligned to notes in\nthe MusicXML-based score note array ( sna_s ), we\nconsider two cases:\n• If the note in sna_o corresponds to type\n‘match’ in align_o , the alignment type for\nthe formerly matched performance note is\nchanged accordingly into an insertion.\n• If the note in sna_o corresponds to type ‘dele-\ntion’ inalign_o (i.e., a score note that was\nnot performed), it is is discarded in align_n .\nNotes insna_s that could not be aligned with notes\ninsna_o , on the other hand, are recorded as type\n‘deletion’ in align_n .\n6.Create match ﬁles. Using the updated\nperformance-to-score alignment align_n , wecreate new match ﬁles, and manually add attri-\nbutional information (e.g., ‘diff_score_version’,\n‘voice_overlap’) to score notes to reﬂect edge cases\ndescribed in step 4.\n4. DATASET OVERVIEW\nTheBatik-plays-Mozart dataset contains performances by\npianist Roland Batik of twelve Mozart sonatas (see Table\n2 for the list of sonatas), corresponding to approx. 102,400\nplayed notes and 223 minutes of music, for which the per-\nformances are provided in MIDI, musical scores in Mu-\nsicXML, and the alignment in match ﬁle format. Ap-\nproximately 98,300 (95.36%) of all performed notes are\naligned with a corresponding score note, the remaining\n4,100 (4.44%) represent insertions (reﬂecting mostly or-\nnaments). Roughly 200 score notes have been omitted in\nthe performances.\nFor each performance, we also provide the performance\nnote arrays, which capture each played note with its note\nID along with onset and duration information in seconds\nand MIDI ticks, as well as velocity and pitch informa-\ntion. Likewise, the dataset includes the score note array\n(unfolded according to the repeats as played by the pianist\nand reﬂected in the alignment), which captures each score\nnote with its (MusicXML) note ID (including repeat suf-\nﬁces, where applicable), onset and duration information in\nterms of beats (reﬂecting the time signature), and quarter\nnotes (reﬂecting a “normalized” score time unit), and pitch\nand voice information.\nWe link our aligned score note arrays to the musico-\nlogical annotations in [1] via their temporal position in\nthe following way: In the second version8of the dataset,\neach annotation label for harmonies, cadences, and phrases\nis unequivocally referenced to a temporal score position\nrepresented in terms of quarterbeats and measure number,\nwhere the ﬁrst expresses the distance of the label from the\nbeginning of the piece in quarter note units. We leverage\nthese two temporal parameters to link each note-aligned\nscore note array by ﬁrst reducing it to its shortest form\n(without any unfolded repeats), aligning it temporally with\nthe musicological annotations, and eventually unfolding it\naccording to the performed repetition structure.\n5. DATASET DEMONSTRATIONS\nThis section presents two simple examples of the kinds of\nstudies that are made possible by our dataset. The ﬁrst\nis motivated by a directly related study in the Annotated\nMozart Sonata corpus paper [1]; the second shows how\nprecise performance alignments permit more detailed in-\nvestigations relating to cadences and their performance.\n5.1 Global tempo and harmonic density\nIn a ﬁrst study, we replicate the second experiment in\nHentschel et al. [1], aimed at investigating the relationship\n8https://github.com/DCMLab/mozart_piano_\nsonatasProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n300Sonata Performed Notes Duration (min) Match Notes % Insertion Notes % Deletion Notes %\nKV279 7,789 16 .21 7,385 94 .087 404 5 .780 11 0 .130\nKV280 6,277 14 .69 6,070 95 .793 207 3 .983 13 0 .223\nKV281 7,030 14 .43 6,396 90 .450 634 9 .393 11 0 .160\nKV282 5,761 14 .77 5,552 96 .197 209 3 .467 20 0 .337\nKV283 8,231 17 .39 7,915 95 .657 316 4 .233 9 0 .107\nKV284 13,386 25 .92 12,691 93 .763 695 6 .033 27 0 .203\nKV330 7,869 18 .47 7,589 96 .857 280 3 .047 7 0 .100\nKV331 11,760 22 .64 11,595 98 .283 165 1 .370 45 0 .347\nKV332 9,013 17 .84 8,660 93 .417 353 6 .210 24 0 .373\nKV333 9,137 20 .40 8,827 96 .723 310 3 .120 16 0 .157\nKV457 7,290 18 .24 7,022 96 .043 268 3 .843 9 0 .110\nKV533 8,878 22 .12 8,616 97 .027 262 2 .837 15 0 .137\nTotal 102,421 223 .12 98,318 95 .358 4,103 4 .443 207 0 .199\nTable 2 . List of sonatas in the Batik-plays-Mozart dataset. The bottom row represents the sum in all columns except for\nthose expressing percentages, for which the mean is shown.\nbetween tempo and harmonic change rate. The basic ques-\ntion asked in [1] was whether the rate at which the harmony\nchanges in a piece is correlated with the piece’s typical per-\nformance tempo. Their study involved determining the av-\nerage (median) performance duration of each sonata move-\nment from 6 complete commercial sonata recordings, and\ncorrelating harmonic label density (rate of harmonic labels\nin their annotations, per performance time unit) with av-\nerage overall performance tempo (number of quarter notes\nper performance time unit). We repeat the same experi-\nment with our pianist’s performances and our alignment\nﬁles instead of 6 pianists’ audio recordings.\nWe apply the same procedure as in [1], unfolding the\nscore according to the repeat structure of the piece in or-\nder to calculate the actual piece length (in terms of quarter\nnotes). The only difference is that we do this according\nto the repeats actually performed by the pianist (which are\nexpressed in our match ﬁles, thus omitting the need for a\ndedicated “unfolding\" step), whereas [1] seem to have as-\nsumed that all repeats were played by all pianists.\nComparing our results (Fig. 3) to Fig. 10 in [1], we\nsee a similar general trend, in the form of a roughly linear\nincrease in harmonic label density with performed tempo\n(slope = .43, r= .75, compared to .48 and .80, respectively,\nin [1]).9However, we also immediately see a marked\ndifference in the performance tempo distribution: in [1],\nFig. 10, there is a relatively large cloud of points (sonata\nmovements) with conspicuously high tempos of 180–200\n(quarters per minute), which does not appear in our plot,\nand which we believe may point to a systematic problem\nin their way of estimating playing tempo: assuming that\nall notated repeats are played out by the performers leads\nthem to overestimate the tempo in all cases where some or\na majority skipped some repeats.10\n9Note that we have a somewhat smaller set of points, because we only\nhave 12 of the 18 sonatas in our dataset.\n10Of course, the authors explicitly acknowledge the problem: “Also,\nsome of the initial assumptions might have to be revisited. For example,\nthe extreme outlier suggesting a tempo of 239 quarter notes per minute\nis due to the fact that for this particular piece – the ﬁrst movement of K.\n533/494 – there seems to be a convention among pianists to repeat the ﬁrst\npart of the piece, but not the second (as the score would suggest), which\nof course reduces the performance duration.” [1] (p.76), but a comparison\nFigure 3 . Correlation between global tempo (as measured\nin quarter notes per minute) and harmony label density\nWe thus see an immediate advantage of our more pre-\ncise performance-aligned corpus: the match ﬁles naturally\ngive correct tempo and score duration information, being\nbased as they are on score-performance alignments that re-\nﬂect the actual repeat structure played by our performer.\nStill, we can say that our results support and conﬁrm the\noverall hypotheses proposed there, showing a more or less\nlinear relationship between harmonic label density and\nglobal performance tempo.\n5.2 Performance of different cadence types\nOur data permits much more detailed investigations into\nrelationships between structural aspects of a piece, and\nhow these are translated into performance decisions by a\npianist. As a simple example, we investigate variations in\nlocal tempo before various types of cadences. Speciﬁcally,\nwe compare the local tempo prior to a cadence annotation\nacross different tempo classes for authentic (perfect and\nimperfect, i.e., PAC and IAC) and half cadences (HC), and\ndifferentiate between the cases when a cadence falls on ei-\nther a downbeat or a weak beat. The hypothesis to be tested\nwith our distribution implies it might be more severe than expected.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n301Figure 4 . Comparison of local timing strategies one quarter note before and after authentic and half cadence labels, over\ndifferent tempo classes (in increasing tempo from top to bottom), for cadences falling on a downbeat (left) or weak beat\n(right). Colour identiﬁes cadence type, line style notated tempo class.\nhere is that a performer will tend to shape cadences dif-\nferently, in terms of tempo, depending on their type and\ndegree of ‘ﬁnality’.\nTo compute the local tempo curves, we consider a uni-\nform window spanning one quarter note each preceding\nand following a cadence label.11For each score-note-\naligned performed note in that window, we deﬁne the lo-\ncal tempo via the beat period (BP) , which we calculate\nas the ratio of the inter-onset-interval (IOI) between the\ncurrent performed onset and the subsequent one, and the\nIOI between the current notated onset and subsequent one.\nWe exclude grace notes and their corresponding performed\nnotes from this calculation in order to remove outliers.\nNext, we perform time-wise interpolation on these\ntempo curves to obtain beat period values at eighth note\nintervals within the window. Given that we are most in-\nterested in the local timing strategy immediately before a\nlabel (that is, an eighth note before the label position), we\ndiscard those curves where that particular time point is in-\nterpolated. Following this procedure, we obtain a total of\n3,540 local tempo values (corresponding to 708 curves), of\nwhich 251 (7.09%) values are interpolated.\nFigure 4 shows the mean of local tempo curves across\ndifferent tempo classes, for cadence labels annotated on a\ndownbeat (left) and on a weak beat (right), respectively.\nFor both authentic and half cadence types, the differences\nin local tempo diminish with increasing global tempo for\nboth downbeat and weak beat cadences. Likewise, the\ntempo proﬁles tend to ﬂatten out with increasing global\ntempo, suggesting that the pianist takes more liberty, in\nterms of expressive timing, in slow pieces. For this rea-\nson, we focus our analysis on the adagio tempo class, the\nslowest tempo (the solid line plots in Fig. 4) .\nThe inﬂuence of the beat level on the local tempo for\nhalf cadences seems to be negligible, with the local beat\nperiod decreasing slightly prior to the cadence (causing an\n11In the Annotated Mozart Sonatas Corpus [1], cadence labels are\nplaced at the onset of the ﬁnal target harmony (e.g., I/i for authentic ca-\ndences).increase in local tempo, i.e. an accelerando ), regardless of\nwhether it falls on a downbeat or weak beat. For authentic\ncadences, we can see a substantial difference in expres-\nsive tempo depending on whether or not the label falls on\na downbeat: for authentic cadences falling on a downbeat,\nthe mean tempo curve for the adagio tempo class corre-\nsponds mostly to what one would expect (i.e., a very clear\nritardando in preparation of the cadence) based on the un-\nderlying harmonies and their notion of tension and release.\nInterestingly, this ritard seems to continue somewhat after\nthe resolution into the tonic, suggesting a lengthening of\nthe tonic arrival. For weak-beat authentic cadences, a sim-\nilar signiﬁcant preparation or anticipation is largely miss-\ning.\n6. CONCLUSION AND FUTURE WORK\nWe have presented Batik-plays-Mozart , a piano perfor-\nmance dataset linking professional Mozart piano sonata\nperformances to expert-labelled musical scores, at the level\nof notes. The resulting dataset is the ﬁrst of its kind to com-\nbine professional performances in precise, recorded MIDI\nwith curated musical scores and expert musicological and\nstructural annotations [1] at this level of detail.\nWe presented two preliminary experiments, intended\nto demonstrate the beneﬁts of having such precise, note-\naligned performance–score–structure annotation data for\nstudying expressive features and their relation to the un-\nderlying musical structure.\nOur plan for future work includes the transcription of\nthe remaining six sonatas of the Mozart piano sonatas cor-\npus from audio recordings by the same pianist, and their\nsubsequent alignment to the musical scores using state-of-\nthe-art transcription and alignment models. By doing so,\nwe hope to advance our understanding of the differences\nbetween transcribed and recorded MIDI, and to evaluate\nthe potential beneﬁts of incorporating an alignment step to\nimprove the quality of transcription.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3027. ACKNOWLEDGMENTS\nWe wish to express our gratitude to pianist Roland Batik\nfor his gracious permission to publish the detailed mea-\nsurements of his performances. We also want to thank the\nauthors of the Annotated Mozart Sonatas Corpus for their\ntremendous efforts, and for permitting us to link our data to\ntheirs. This work receives funding from the European Re-\nsearch Council (ERC), under the European Union’s Hori-\nzon 2020 research and innovation programme, grant agree-\nment No. 101019375 ( Whither Music? ). The LIT AI Lab\nis supported by the Federal State of Upper Austria.\n8. REFERENCES\n[1] J. Hentschel, M. Neuwirth, and M. Rohrmeier,\n“The Annotated Mozart Sonatas: Score, Harmony,\nand Cadence,” Transactions of the International\nSociety for Music Information Retrieval (TISMIR) ,\nvol. 4, no. 1, pp. 67–80, 2021. [Online]. Available:\nhttps://doi.org/10.5334/tismir.63\n[2] C. E. Cancino-Chacón, M. Grachten, W. Goebl, and\nG. Widmer, “Computational Models of Expressive\nMusic Performance: A Comprehensive and Critical\nReview,” Frontiers in Digital Humanities , p. 25, 2018.\n[3] D. Jeong, T. Kwon, Y . Kim, K. Lee, and J. Nam,\n“VirtuosoNet: A Hierarchical RNN-based System for\nModeling Expressive Piano Performance.” in Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , 2019, pp. 908–915.\n[4] A. Maezawa, K. Yamamoto, and T. Fujishima, “Ren-\ndering Music Performance with Interpretation Varia-\ntions using Conditional Variational RNN,” in Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , 2018.\n[5] H. Zhang, J. Tang, S. R. M. Rafee, and S. D. G.\nFazekas, “ATEPP: A Dataset of Automatically Tran-\nscribed Expressive Piano Performance,” in Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , 2022.\n[6] Q. Kong, B. Li, J. Chen, and Y . Wang, “GiantMIDI-\nPiano: A Large- Scale MIDI Dataset for Classical Pi-\nano Music,” Transactions of the International Society\nfor Music Information Retrieval (TISMIR) , May 2022.\n[7] F. Foscarin, A. Mcleod, P. Rigaux, F. Jacquemard, and\nM. Sakai, “ASAP: A Dataset of Aligned Scores and\nPerformances for Piano Transcription,” in Proceedings\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2020, pp. 534–541.\n[8] W. Goebl. (1999) The Vienna 4x22 Piano Corpus.\n[Online]. Available: http://dx.doi.org/10.21939/4X22\n[9] M. Hashida, E. Nakamura, and H. Katayose, “Crest-\nMusePEDB 2nd Edition: Music Performance Database\nwith Phrase Information,” in Proceedings of the 15th\nSound and Music Computing (SMC) Conference , 2018.[10] K. Kosta, O. F. Bandtlow, and E. Chew, “MazurkaBL:\nScore-Aligned Loudness, Beat, Expressive Markings\nData for 2000 Chopin Mazurka Recordings,” in Pro-\nceedings of the Fourth International Conference on\nTechnologies for Music Notation and Representation\n(TENOR) (Montreal, QC) , 2018, pp. 85–94.\n[11] C. Hawthorne, A. Stasyuk, A. Roberts, I. Si-\nmon, C.-Z. A. Huang, S. Dieleman, E. Elsen,\nJ. Engel, and D. Eck, “Enabling Factorized Piano\nMusic Modeling and Generation with the MAE-\nSTRO Dataset,” in International Conference on\nLearning Representations , 2019. [Online]. Available:\nhttps://openreview.net/forum?id=r1lYRjC9F7\n[12] A. Ycart, L. Liu, E. Benetos, and M. Pearce, “Inves-\ntigating the Perceptual Validity of Evaluation Metrics\nfor Automatic Piano Music Transcription,” Transac-\ntions of the International Society for Music Informa-\ntion Retrieval (TISMIR) , 2020.\n[13] W. Goebl and R. Bresin, “Measurement and Reproduc-\ntion Accuracy of computer-controlled Grand Pianos,”\nThe Journal of the Acoustical Society of America , vol.\n114, no. 4, pp. 2273–2283, 2003.\n[14] G. Widmer, “Discovering simple rules in complex data:\nA meta-learning algorithm and some surprising musi-\ncal discoveries,” Artiﬁcial Intelligence , vol. 146, no. 2,\npp. 129–148, 2003.\n[15] E. Cambouropoulos, “From MIDI to traditional mu-\nsical notation,” in Proceedings of the AAAI Workshop\non Artiﬁcial Intelligence and Music: Towards Formal\nModels for Composition, Performance and Analysis ,\nvol. 30, 2000.\n[16] F. Foscarin, E. Karystinaios, S. D. Peter, C. Cancino-\nChacón, M. Grachten, and G. Widmer, “The match\nﬁle format: Encoding Alignments between Scores and\nPerformances,” in Proceedings of the Music Encoding\nConference (MEC) , 2022.\n[17] C. Cancino-Chacón, S. D. Peter, E. Karystinaios,\nF. Foscarin, M. Grachten, and G. Widmer, “Parti-\ntura: A Python Package for Symbolic Music Process-\ning,” in Proceedings of the Music Encoding Conference\n(MEC) , 2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n303"
    },
    {
        "title": "Expert and Novice Evaluations of Piano Performances: Criteria for Computer-Aided Feedback.",
        "author": [
            "Yucong Jiang"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.8392772",
        "url": "https://doi.org/10.5281/zenodo.8392772",
        "abstract": "This dataset was collected to compare performance assessment criteria used by experts and novices. Beyond the 83 amateur piano performance audio recordings (in the WAV format) and the 803 evaluation entries of these performances, the dataset also contains each player&#39;s self-identified skill level, seven digital scores (in the MusicXML format), and 83 audio-to-score alignment files indicating the starting time in the audio of each musical note in the scores.\n\nThe included CSV file contains 83 rows, each row representing one piano performance. Each performance was evaluated by four professional piano instructors, and by five or six randomly chosen peer players (one of whom could be the performer him/herself as the player information was never shared). The columns in the CSV file specify each performances scorename, its player (identified by a number from 1 to 21), as well as the numerical and textual evaluations from the four instructors and from the peer players.\n\nMore information can be found onthe project website: https://facultystaff.richmond.edu/~yjiang3/papers/ismir23/.",
        "zenodo_id": 8392772,
        "dblp_key": "conf/ismir/Jiang23",
        "keywords": [
            "dataset",
            "performance audio recordings",
            "evaluation entries",
            "self-identified skill level",
            "digital scores",
            "audio-to-score alignment files",
            "CSV file",
            "performance score",
            "player information",
            "project website"
        ],
        "ee": "https://zenodo.org/records/8392772/files/Recordings and Alignments.zip"
    },
    {
        "title": "Musical Micro-Timing for Live Coding.",
        "author": [
            "Max Johnson",
            "Mark Gotham"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265231",
        "url": "https://doi.org/10.5281/zenodo.10265231",
        "ee": "https://zenodo.org/records/10265231/files/000010.pdf",
        "abstract": "Micro-timing is an essential part of human music-making, yet it is absent from most computer music systems. Partly to address this gap, we present a novel system for generating music with style-specific micro-timing within the Sonic Pi live coding language. We use a probabilistic approach to control the exact timing according to patterns discovered in new analyses of existing micro-timing data (jembe drumming and Viennese waltz). This implementation also required the introduction of musical metre into Sonic Pi. The new metre and micro-timing systems are inherently flexible, and thus open to a wide range of creative possibilities including (but not limited to): creating new micro-timing profiles for additional styles; expanded definitions of metre; and the free mixing of one micro-timing style with the musical content of another. The code is freely available as a Sonic Pi plug-in and released open source at https://github.com/MaxTheComputerer/sonicpi-metre.",
        "zenodo_id": 10265231,
        "dblp_key": "conf/ismir/JohnsonG23",
        "keywords": [
            "micro-timing",
            "computer music systems",
            "novel system",
            "Sonic Pi live coding language",
            "probabilistic approach",
            "new analyses",
            "existing micro-timing data",
            "musical metre",
            "freely available",
            "open source"
        ],
        "content": "MUSICAL MICRO-TIMING FOR LIVE CODING\nMax Johnson1, Mark Gotham1,2\n1Department of Computer Science and Technology, University of Cambridge\n2Department of Computer Science, Durham University\nABSTRACT\nMicro-timing is an essential part of human music-making,\nyet it is absent from most computer music systems. Partly\nto address this gap, we present a novel system for gen-\nerating music with style-speciﬁc micro-timing within the\nSonic Pi live coding language. We use a probabilistic\napproach to control the exact timing according to pat-\nterns discovered in new analyses of existing micro-timing\ndata (jembe drumming and Viennese waltz). This imple-\nmentation also required the introduction of musical me-\ntre into Sonic Pi. The new metre and micro-timing sys-\ntems are inherently ﬂexible, and thus open to a wide range\nof creative possibilities including (but not limited to):\ncreating new micro-timing proﬁles for additional styles;\nexpanded deﬁnitions of metre; and the free mixing of\none micro-timing style with the musical content of an-\nother. The code is freely available as a Sonic Pi plug-in\nand released open source at https://github.com/\nMaxTheComputerer/sonicpi-metre .\n1. INTRODUCTION\n1.1 Metre Versus Rhythm\nMetre is distinct from rhythm in that it primarily concerns\na kind of mental representation for processing events in\nmusical time; a common analogy casts metre as a “grid” or\n“template” for categorising rhythmic events [1–3].\nAlthough metre often involves familiar notions such as\n“the beat”, and deﬁnitions often emphasise intuitive ideas\nlike regular periodicity, a clear-cut deﬁnition of metre is\nsurprisingly hard to pin down. This is especially so when\ntrying to capture the extremely wide range of musical-\ncultural contexts for which some concept of metre might\nbe relevant. Nevertheless, notwithstanding the complex-\nities of these terms, and putting any more speciﬁc deﬁni-\ntion of these terms to one side, it is reasonable to argue that\nsome form of both “rhythm” and “metre” feature in almost\nall known musics: “rhythm” in the sense of events occur-\nring in time, and “metre” in some form of semi-regular\ncycle of event expectation.\n© M. Johnson and M. Gotham. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: M. Johnson and M. Gotham, “Musical Micro-Timing for Live\nCoding”, in Proc. of the 24th Int. Society for Music Information Retrieval\nConf., Milan, Italy, 2023.1.2 On Micro-Timing in Theory and Practice\n“Micro-timing”, in turn, refers to the speciﬁc timing of\nboth those actual rhythmic “events” and of the metrical\n“grid” positions. While rhythm and metre are sometimes\nmodelled in terms of a completely regular underlying pulse\n(e.g., 1-1-1-1-) and small integer combinations thereof\n(e.g., 2-1-1-), it is impossible in practice for a human per-\nformer to play with the mechanical precision of identical\ngaps between successive events.1Moreover, musicians\nmake a virtue of this. A close look at the micro-timings\nin human performance reveals deeply sophisticated, style-\nspeciﬁc micro-timing strategies, a.k.a. “groove”.2\nEven when distinguishing between a mental “grid” for\nevents (metre) and the actual placement of those events\n(rhythm), it is appropriate to discuss micro-timing for both\nthe rhythm and the metre. This distinction is sometimes\ncast in terms of a difference between “categorical” and “ex-\npressive” timing where events may be expressively altered\nfrom their expected (categorical) position [5], but note that\nthe “categorical” position itself is also subject to micro-\ntiming strategies because the “expected” positions are not\nspaced with equal, 1:1 regularity. In short, although micro-\ntiming is sometimes described in terms of “small devia-\ntions” from simple (natural number) pulse relations, it is\nnecessary also to consider micro-timing as part of the me-\ntre itself. To continue the “metre as grid” analogy: the gaps\nbetween grid lines are not evenly spaced.\nIn practice, these micro-timing durations are too short\nto learn in a declarative fashion (verbal or mathemati-\ncal). Partly for that reason, they are also typically ne-\nglected by notation systems (including Western staff no-\ntation). Nonetheless, these micro-timing strategies clearly\naretaught and learned in the way that most music has been\npassed on: through listening, playing, and embodiment.\nComputers enable us to achieve a level of timing reg-\nularity beyond our human capability. As ever, the tech-\nnology not only extends what we can do, but invites us to\nconsider new techniques, questions, and aesthetics. And\nthe human’s micro-timing can of course be combined with\nthe computer’s extreme timing precision, as when an MC\nraps over a beat. Computers are also used to perform the\nmicro-timing analysis discussed here. However, comput-\ners are currently less exploited as a tool to help us engage\nincreative uses of micro-timing strategies.\n1This has been systematically studied since Seashore [4].\n2See, for example, Justin London’s “many metres” [2].982. RELATED WORK\nA substantial research ﬁeld has grown to analyse the\nmicro-timing strategies in different musical styles. This\nhas included work on the Viennese waltz (from Bengts-\nson and Gabrielsson’s landmark 1977 work to Yang’s 2022\nanalysis of ‘The Blue Danube’ [6, 7]), jembe music from\nMali (notably through Rainer Polak’s career-long focus on\nthis repertoire, [8–10]), and a recent surge of work on Afro-\nCuban and Latin musics (see, for instance, [11, 12]).\nCreating music with synthetic micro-timing has also\nbeen explored as part of the broad ﬁeld of MIR, but\nhas concentrated on attempting to model human-like ex-\npressive timing [13]. For example, Flossman et al. use\nprobabilistic models for expressive performance render-\ning [14, 15]. There has been very little academic work\non integrating style-speciﬁc timing analyses into compu-\ntational settings. The closest examples are in the commer-\ncial sphere: Ableton Live, for instance, features “grooves”\nwhich shift MIDI events from quantised positions accord-\ning to micro-timing styles, including a probabilistic ele-\nment and the option to create new “grooves” from any hu-\nman performances (via MIDI).\nMusical live coding is a way of creating and perform-\ning music by writing and modifying code in real time.\nWhile there may or may not be pre-made materials, live\nmanipulation of the material is a given. Given the in-\nherent “liveness” of live coding,3it is arguably an ideal\npart of the computer music pantheon to integrate human\nmicro-timing. Yet most live coding languages lack not\nonly micro-timing functionality, but even a full represen-\ntation of musical metre, typically encoding only events in\ntime, or at most an anaemic representation for beats and/or\ntime signatures. For example, the commercial Max/MSP\nlanguage uses its transport object to allow access to\nbar and beat numbers for the current time signature. An ex-\nception is McLean’s open-source Tidal Cycles which uses\na cyclic notion of time that can be subdivided to achieve\nmore complex hierarchies [17].\nIn summary, although there has been much research into\nthe analysis of micro-timing in different musical styles,\nand the application of expressive timing to computer-\ngenerated music, we still lack implementations of style-\nspeciﬁc micro-timing in most computer-music software,\nand even foundational notions of musical metre in most\nlive coding environments. This project seeks to address\nthose issues through an implementation of both metre and\nstyle-speciﬁc micro-timing for Sonic Pi: a popular live\ncoding language and IDE designed to support a range of\ncreative possibilities while being simple enough to use as\nan educational tool for use in schools [18].4We aim to im-\nprove not only how “life-like” the generated music sounds\nin general (and thus, arguably the “liveness” of that live\ncoding), but also to do this in a style-speciﬁc way. In this\npaper we report on implementation of two case studies as\nwell as a general framework for integrating further styles.\n3This is discussed in Chapter 5 of [16], for instance.\n4Sonic Pi’s domain-speciﬁc language is written in Ruby and uses the\nSuperCollider sound synthesis server to produce sounds [19].Metre\nd=2\n4\nMetreTree\nd=1\n4\nMetreLeaf\nd=1\n8MetreLeaf\nd=1\n8MetreTree\nd=1\n4\nMetreLeaf\nd=1\n8MetreLeaf\nd=1\n8\nFigure 1 : An example of how MetreTree and MetreLeaf\nobjects are nested to construct a metrical hierarchy for24.\nThe total duration dof each node is also displayed, and the\nduration of a parent node is the sum of the durations of its\nchildren [22, 23].\n3. IMPLEMENTING METRE\nThis section describes the model of metre we have imple-\nmented for Sonic Pi. We argue that this is useful for a\nrange of applications including (but not limited to) use as\na basis for micro-timing as described below (§4). As dis-\ncussed, metre is a very widespread phenomenon in gen-\neral but speciﬁc aspects differ. We can broadly distinguish\nhere between the speciﬁcally hierarchical aspects (impor-\ntant for some styles but not all) and the more general notion\nofcategorical positions in a metrical cycle (much more\nwidespread, and axiomatic for the kind of micro-timing\nsystems discussed and implemented here).\n3.1 Modelling Metrical Hierarchy with Trees\nA favoured method for encoding metrical hierarchy in a\ndata structure is through trees. See Forth [20] for a de-\ntailed mathematical treatment of trees used in this context\nand themusic21 Python library [21] for a popular imple-\nmenation. Our approach shares some high-level ideas with\nmusic21 (and indeed Forth, and others), but differs in the\nspeciﬁc implementation.\nThe tree structure is implemented by the MetreLeaf\nandMetreTree classes. Figure 1 shows the default tree\nstructure formed by these objects and their durations for\na Western24time signature. Note how the duration of a\nparent node is the sum of the durations of its children.\nTo model tree data structures of any depth with a suc-\ncinct, ﬁnite representation, we follow the Western nota-\ntional assumption of diving each MetreLeaf into two equal\nparts to get the next level where not speciﬁed otherwise.5\nUsers can specify the full depth of a tree as necessary\nagainst this assumption.\n3.2 The MetreLeaf Class\nA MetreLeaf object is the leaf node of the metrical tree\nstructure. It has an instance variable fraction which\nrepresents the duration of the MetreLeaf as a fraction of a\nwhole note. For example, a leaf node with the duration of\none quarter note will have the value1\n4.\n5See [24] for discussion of this point, of metrical “well-formedness”,\nand the notion of “binary”, “ternary” and wider metrical structures.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n99The class contains a subdivide() method, which di-\nvides the MetreLeaf by two a given number of times, s. It\nreturns a new MetreTree with 2sMetreLeaf children, each\nof valuef/2swherefis the fraction of the original Me-\ntreLeaf.\n3.3 The MetreTree Class\nA MetreTree object represents the hierarchical tree or sub-\ntree of a metre. The instance variable sequence is an\nordered list representing this node’s children and contains\nany combination of MetreLeaf objects and other MetreTree\nobjects. For example, the hierarchy in Figure 1 could also\nbe written in list form as:\n/bracketleftbigg/bracketleftbigg1\n8,1\n8/bracketrightbigg\n,/bracketleftbigg1\n8,1\n8/bracketrightbigg/bracketrightbigg\nEach list is a MetreTree, and each fraction is a MetreLeaf.\nThe MetreTree class contains several methods for manip-\nulating and extracting information from the metrical hier-\narchy it represents. The two most important of these are\nexplained in more detail below.\n3.3.1 Getting Metrical Levels\nMetrical level refers to the depth level of a metrical hier-\narchy. Here, we base our representation on the beat level ,\nwhich is divided to get division levels , and grouped to get\ngrouping levels .6Theget_level() method allows a\nuser to “ﬂatten” the tree structure to a speciﬁed depth, ac-\ncessing the sequence of events at a given metrical level.\nFor ﬂattening to a division level ( l >0) or to the beat\nlevel (l= 0), we perform a recursive depth-ﬁrst search\non the tree. For each child in the sequence list, if it is a\nMetreTree, the method is recursively called until the base\ncase ofl= 0 is reached. At this point, all the children\nof that node are combined into one MetreLeaf equal to the\nsum of their durations. If the child is instead a MetreLeaf,\nit is subdivided ltimes to reach the desired metrical level.\nFor a grouping level ( l >0), we ﬁnd an estimate of\nthe structure of higher metrical levels by clustering nodes\ntogether. It is an estimate because this information is not\nin the MetreTree’s representation of the metre, so is just\none possibility for the higher structure. The algorithm re-\ncursively clusters nodes until the desired metrical level lis\nreached. The number of nodes combined in each cluster is\ndetermined by the smallest prime factor of the number of\nnodes at the level below. For example, if level l+ 1 has\nfour nodes, they will be clustered in groups of two. If it\nhas nine nodes, they will be clustered in groups of three.\nSome examples of the output of the ﬂattened tree for the\nfollowing complex hierarchy are shown in Table 1:\n/bracketleftbigg/bracketleftbigg1\n8,1\n8/bracketrightbigg\n,/bracketleftbigg1\n16,3\n16/bracketrightbigg\n,1\n8,/bracketleftbigg1\n4,/bracketleftbigg5\n16,3\n16/bracketrightbigg/bracketrightbigg/bracketrightbigg\n6Centring the beat level in this way reﬂects the psychology of metre\nbetter than alternative “top down” and “bottom up” approaches.l get_level (l)\n−2/bracketleftbigg11\n8/bracketrightbigg\n−1/bracketleftbigg1\n2,7\n8/bracketrightbigg\n0/bracketleftbigg1\n4,1\n4,1\n8,3\n4/bracketrightbigg\n1/bracketleftbigg1\n8,1\n8,1\n16,3\n16,1\n16,1\n16,1\n4,1\n2/bracketrightbigg\nTable 1 : Examples of the output of get_level (l)at dif-\nferent metrical levels lfor an example hierarchy. Note how\nlevell=−1is formed by the clustering of level l= 0.\nLevel 0\nLevel 1\nLevel 21\n4\n1\n8\n1\n161\n161\n8\n1\n161\n16x\n1\n4\n1\n8\n1\n161\n161\n8\n1\n161\n16y\nFigure 2 : An example metrical hierarchy for24showing\nthose metrical events at each level which coincide with off-\nsetsxandy.\n3.3.2 Getting Exact Metrical Events\nWe deﬁne an offset as a position in the metric cy-\ncle represented by the quarter length duration to have\nelapsed since the beginning of the cycle.7The\nmetrical_level_indices() method of the Metre-\nTree class ﬁnds any metrical events occurring at a given\noffset, and returns their index.\nConsider the example shown in Figure 2. Offset xoc-\ncurs on the ﬁrst event of all three levels, so the function\nwould return L0(x) =L1(x) =L2(x) = 0 , whereLl(x)\nis the index of an event at level lthat offset xoccurs on.\nOffsetyoccurs only on the last event of Level 1 and the\nsecond-to-last event of Level 2, so the function would re-\nturnL1(y) = 3,L2(y) = 6 .\nThis method is important because it is used later to de-\ntermine the “categorical” position to link an event to and,\nthrough that, which micro-timing probability distribution\nto apply.\n3.4 Bar Class\nThe Bar class is a representation of a single metrical cy-\ncle,8and each instance of it has an associated metre. A\nBar object is responsible for:\n7“Quarter length” is a semi-standard measurement for a length of time\nin symbolic values where the unit length is one “quarter note” duration\n(UK: “crotchet”).\n8A more precise deﬁnition would account for hypermetre where bars\noccur at the beat level [25], but the simple deﬁnition is sufﬁcient for our\npurposes. Note that “bar” is the UK English equivalent of “measure”\n(USA).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n100play:C4\nsleep(1)\nplay:E4\nsleep(1)\nplay:G4\nsleep(0.5)\nplay:E4\nsleep(0.5)\nplay:C4\n(a) Old (above left)use_metre '4/4'\nbardo\nadd_note :C4,0,1\nadd_note :E4,0,1\nadd_note :G4,1,1\nadd_note :E4,1,1\nadd_note :C4,0,1\nend\n(b) New (above right)\n(c) Western musical notation:\n 44566666\nFigure 3 : A bar of music represented by (a) the original\nSonic Pi syntax, (b) our new metre commands, and (c) tra-\nditional Western music notation. Note how the original\nSonic Pi syntax loses information about the metre. The\nsecond and third arguments to add_note are the metrical\nlevel and duration ( landdin §3.4).\n• Keeping track of playback position during the cycle.\n• Converting a note length given as a metrical level\nand a duration into a quarter length.\n• Checking if a note or rest ﬁts in the remaining time\nin the cycle, and updating the bar’s playback position\naccordingly.\nA note’s length is speciﬁed by a metrical level and a\nduration, where the duration is in the units of an event at\nthe speciﬁed metrical level and acts as a multiplier. For\nexample, if a note’s length is deﬁned as (l,d) = (0,3), its\nunit length is the duration of an event at level l= 0, and it\nlasts ford= 3of these units.\nTheadd_note() method checks if a note ﬁts into\nthe bar’s remaining time; if it cannot, an exception is\nraised. This ensures the total (“actual”) duration of the bar\nmatches its metre’s (“nominal”) duration.\n3.5 Playing Music\nThis framework for musical metre enabled the creation of\nnew Sonic Pi commands. Figure 3 shows a comparison\nbetween the original Sonic Pi commands, our alternative\ncommands, and traditional Western music notation.\nThere are two main commands for metre. The ﬁrst\nisuse_metre( m), which changes the current thread’s\nmetre to m(using a thread-local variable). The second\nisbardo...end, which creates a new Bar object,\nstores this to a thread-local variable, then executes a block\nof user code.\nA user can use add_note to play a note on the cur-\nrent synthesiser. This works by ﬁrst getting the current Bar\nobject from the thread-local variables and calling the Bar’s\nadd_note() method to check if the note will ﬁt in the\nbar. It then passes the note pitch to Sonic Pi’s play func-\ntion which creates the sound, and ﬁnally applies sleep\nfor the remaining duration of the note.4. MICRO-TIMING\n4.1 Storing Micro-Timing Information\nIn order to add micro-timing functionality to our imple-\nmentation, we ﬁrst needed a way of representing and\nstoring the micro-timing information for different musical\nstyles. We implement this by storing each event in the met-\nrical cycle, the theoretical (isochronous) position of that\nevent in the cycle (e.g., 1), and the typical displacement\nof the from this position (the µof the micro-timing, e.g.,\n+0.004). Actual event occurrence is modelled by normal\nprobability distribution around these µvalues.\nSamples can then be drawn from these distributions\nusing the Box-Muller transform [26] on uniform random\nsamples from Sonic Pi’s random number generator. Sonic\nPi’s generator produces a deterministic, repeatable se-\nquence of pseudorandom numbers, which means the out-\nput of a Sonic Pi program sounds the same each time it is\nrun [27].\n4.2 Applying Micro-Timing\nWhen a user sets a metre with the use_metre command,\nthey can optionally specify a style as well. This causes all\nmusic played with that metre to use the micro-timing of the\nchosen style.\nAt the start of each new bar, the Metre object sam-\nples new values from the Style ’s probability distribu-\ntions. When a note is played inside the bar, the add_note\ncommand requests the timing shift that should be applied\nto the note from the Metre. To calculate this, the Metre ob-\nject calls its metrical_level_indices() method\nto determine which timing values from each level to use.\nThe individual timing contributions of each metrical level\nare summed to produce an overall timing shift for the note.\nA positive value means the note should be played slightly\nlate; a negative value means slightly early. This is returned\ntoadd_note which then uses Sonic Pi’s time_warp\nfunction to adjust the timing of the call to play .\nFor example, if the sampled timings, Tl, for each level,\nl, are:\nT0= [0,0.1]\nT1= [0.03,0,0,−0.02]\nand the metrical level indices, Ll, for each level, l, are:\nL0= 1\nL1= 3\nthen the timing shift, t, would be calculated by:\nt=/summationdisplay\ni∈T.keysTi[Li]\n=T0[L0]+T1[L1]\n= 0.1+(−0.02)\n= 0.08\nTherefore, the note will be played 0.08 quarter lengths after\nthe reference value.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1015. CASE STUDIES\nCreating music with realistic micro-timing using the im-\nplementation we have described requires a set of proba-\nbility distributions which accurately characterise a style of\nmusic. Clearly this is best implemented with data derived\nfrom real-life examples of the musical style in question.\nThis project uses two different styles of music as contrast-\ning case studies for evaluation: jembe (or “djembe”) drum\nmusic from Mali and Viennese waltz music. These two\nstyles both have robust, well-known micro-timing charac-\nteristics. The distributions derived here form the “preset”\nstyles included in our Sonic Pi plugin.\n5.1 Jembe Data Analysis\nJembe is a style of West African music involving a small\nensemble of drummers (typically 3–4). It provides an ideal\ncase study for our purposes because it has a highly con-\nsistent micro-timing strategy [8]. Malian drummers have\nbeen shown to exhibit some of the most consistent tim-\ning (lowest levels of variability) between performers in the\nworld [28].\nMoreover, jembe music is relatively constrained in\nterms of its pitch, timbre, and number of instruments. This\nalso helps by enabling a clear focus on timing. Exten-\nsive research into the micro-timing of jembe music has in-\ncluded the release of high-quality datasets of processed live\nrecordings [8–10].\nThe ﬁrst dataset is from Jacoby et al. [10] and consists\nof 11 processed recordings of a piece called ‘Suku’, which\nis a very commonly played piece in this style. The second\ndataset is from the “Interpersonal Entrainment in Music\nPerformance” (IEMP) Data Collection [29, 30]. This con-\nsists of 15 recordings across three different pieces: ‘Man-\njanin’, ‘Maraka’, and ‘Woloso’. Both datasets here use\nrecordings made by Rainer Polak in Mali. The datasets\nsupply the following information:\n• Onset of the drum stroke in seconds since the start;\n• Phase: beats since the start of the current cycle ;\n• Cycle (bar) number: a natural number count;\n• Categorical metrical position within the cycle asso-\nciated with this event (integer, 0–11).\n5.1.1 Micro-Timing Estimation\nThe pieces of jembe music in the dataset use a metre with\nfour beats,9each of which divides into three, for a total of\n12 metrical events at the ﬁrst division level (similar to128\nin Western classical notation). It is at this level, referred to\nas the “pulse”, that the main micro-timing occurs.\nRecall that the probability distributions described in\n§4.1 store the displacement of each event. We calculate\nthis from the phase given by the datasets with the follow-\ning equation:\ndisplacement =(phase×beat division )−metric position\n2\n9See Polak [8] for an ethnographically sensitive discussion of the ex-\ntent to which metre applies in this context.The phase is multiplied by the beat division (in this case,\n3) to convert it into pulse units. The metric position at the\npulse level is subtracted to get the displacement. The ﬁnal\ndivision by 2 converts the displacement into quarter lengths\n(because each pulse unit is an eighth length).\nFor example, if an onset has metric position = 6 and\nphase= 2.01, the displacement would be calculated by:\ndisplacement =(2.01×3)−6\n2= 0.015quarter lengths.\nOnce the displacement has been calculated for each\ndrum stroke, we were then able to estimate the distribution\nof displacements for each of the twelve metric locations\nusing maximum likelihood estimation (MLE).\n5.1.2 Tempo Estimation\nGenerating a synthetic piece of jembe music requires anal-\nysis of other musical features as well as the micro-timing to\nsound realistic. One of these is the tempo , which in jembe\npieces of music typically increases substantially over the\nduration of the performance [10], with the last 15 seconds\nor so showing the tempo increasing at a much faster rate.\nTheinter-beat interval (IBI) is deﬁned as the time be-\ntween two consecutive beats in a piece of music, from\nwhich the instantaneous tempo can be calculated [31]. A\nmoving average can be applied to the instantaneous tempo\nto obtain an estimate of the global tempo.\nFor the jembe data, we ﬁrst ﬁltered all the onsets to in-\nclude just those played by Jembe 2 (because it plays on\nevery beat as discussed in [10]), then ﬁltered these to only\nconsider onsets on the beats. We then calculated the inter-\nbeat interval in bpm and applied a moving average with\nwindow size 10 to smooth the tempo estimate.\nInspection of the smoothed tempo graphs (§6.2) showed\na logarithmic trend for the ﬁrst ~ 95% of the piece. A\nsharper increase follows this which was modelled by a\nquadratic curve. To ﬁt curves to the data, we used\ntheoptimize.curve_fit function from the SciPy\nPython library, which uses a non-linear least squares\nmethod [32]. The parameters estimated by the curve ﬁt-\nting are then used in Sonic Pi to control the tempo of a\nsynthetic jembe piece during playback.\n5.2 Waltz Data Analysis\nThe Viennese waltz is a style of fast waltz notated in34(but\noften counted in 1), originally intended for ballroom danc-\ning, and now often performed in concerts by Western clas-\nsical orchestras.\nThe Viennese waltz provides a useful comparison to\nMalian jembe in evaluating this project’s micro-timing im-\nplementation. The fast three beats (34) and typical hyper-\nmetrical grouping in 2s and 4s make that metrical struc-\nture somewhat similar to that of jembe music, but with\na very different micro-timing proﬁle. Distinctive micro-\ntiming can be observed on (at least) the beat level, where it\nhas a characteristic short-long-medium pattern [6, 33].\nAt the time this work was carried out, the micro-timing\nin Viennese waltz had not been studied in as much detail orProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n10201234567891011\nPhase0246Density\nFigure 4 : A histogram plot of the positions of each\npulse within the cycle for Suku. Dashed lines show the\nisochronous division of the cycle for reference. Black\ncurves show the PDF of the MLE-derived probability dis-\ntributions and the colours distinguish to the four beat.\nas recently as jembe and there were no existing datasets of\nViennese waltz performances with micro-timing. There-\nfore, we constructed a new dataset comprising of 30-\nsecond samples from seven waltz recordings performed by\nthe Vienna Philharmonic Orchestra, all of which have no-\nticeable and statistically signiﬁcant micro-timing.\nWe then performed automatic beat tracking on this\ndataset using the libfmp Python library [34] (a dynamic\nprogramming approach introduced by Müller [35]), with\nsome small manual corrections. Since the beat level is\nwhere the primary micro-timing in the Viennese waltz oc-\ncurs, no additional onset detection was necessary.\nCalculating the micro-timing displacement of each beat\nfrom onset times alone involves ﬁrst identifying the start\nand end of each cycle, estimating the onset of each beat\nas if they were isochronous, then ﬁnding the difference be-\ntween this and the actual onset to get the displacement.\nOnce the displacements were derived, maximum likeli-\nhood estimation was again used to ﬁt the probability distri-\nbutions as discussed above for the jembe case.\n6. RESULTS\n6.1 Micro-Timing Estimation\nFigure 4 shows the results of the micro-timing estimation\nfor one of the jembe pieces in the datasets: “Suku”. The\nhistograms show the positions within the cycle where the\n12 pulses occurred (phase). The dashed lines indicate\nwhere the event would occur if they were isochronous:\nfrom this the existence of the micro-timing can be seen\nclearly by the positions of the second and third pulses in\neach beat. By examining the positions of the histograms,\nwe can see that the length of each pulse follows a short-\nmedium-long pattern (SML), which is consistent across\neach beat. Also shown are the probability density functions\n(PDF) of the maximum-likelihood estimated normal dis-\ntributions. The plots/data for each beats showed the same\npattern which also corresponds to other jembe pieces and\nmatches results previously reported by Polak [8].\nFigure 5 shows the results for the waltz dataset. The\ncalculations use the ﬁrst beat as the deﬁnition for the start\nof the cycle, so every Beat 1 has a displacement of 0.\nThe early onset of the second beat can be clearly seen in\nthe plot ( µ=−0.0743 ,σ= 0.0795 ). A one-sample t-\ntest conﬁrms the micro-timing as signiﬁcant ( t=−16.5,\np= 0.000). Beat 3 shows no signiﬁcant deviation from a−0.50.0 0.5\nDisplacement05101520DensityBeat 1\n−0.20.0\nDisplacement0246DensityBeat 2\n−0.50−0.250.00\nDisplacement0246DensityBeat 3\nFigure 5 : A histogram plot of the displacement of the sec-\nond and third beats for the waltz dataset. Dashed lines\nshow the metrical grid. Black curves show the PDF of the\nMLE-derived probability distributions.\n3-part isochronous division of the cycle, so the overall pat-\ntern identiﬁed is the short-long-medium (SLM) discussed\nelsewhere [7].\n6.2 Jembe Tempo Estimation\nThe results of the jembe tempo estimation showed the in-\ncrease in tempo throughout the piece that is characteristic\nof Malian jembe music. The more dramatic speedup at\nthe end is also reﬂected in this data – this is why we ﬁt\ntwo different curves to the data. For example, in “Suku”,\nthe tempo starts at around 135 bpm at the beginning of\nthe piece and ends at around 175 bpm. The tempo results\nmatch those found by Jacoby et al. [36], and each jembe\npiece showed the same trend.\n7. CONCLUSION\nIn this project, we have investigated and implemented\nprobabilistic style-speciﬁc micro-timing in a musical live\ncoding language. To do this, we extended the Sonic Pi\nlanguage with implementations of both musical metre and\nmicro-timing, and we performed data analysis on record-\nings of music from two case study styles to generate music\nwith realistic micro-timing.\nIn further work (not reported here but available on re-\nquest), we conducted a user study to assess how “realis-\ntic” our synthesised micro-timing sounded for each of the\ncase study styles. Signiﬁcant results were obtained for the\nViennese waltz, however participants struggled more with\nthe jembe, likely due to their unfamiliarity with the style.\nFuture work could conduct a new user study with expert\nparticipants, as in Neuhoff [37].\nNaturally, other future work could focus on additional\nstyles with well-documented micro-timing, such as jazz\nswing rhythms [38], candombe drum ensembles from\nUruguay [11, 39], and Brazilian samba music [11, 40].\nLikewise, larger datasets for the styles reported here would\nenable more accurate distributions – two notable datasets\nof Viennese waltz recordings have been released even since\nthe work reported here: Weigl et al. [41] and Yang [7].\nAs for software functionality, we imagine extensions in-\ncluding new variable gridline positions in DAWs, and ad-\nditional controls within Sonic Pi to dynamically adjust the\n“strength” of the micro-timing .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1038. ACKNOWLEDGEMENTS\nThanks to Rainer Polak for his invaluable advice and feed-\nback on this project, for his years of research on jembe\nmusic, and for releasing the data that made that case study\npossible. Thanks indeed to all who provided feedback on\nthis project. This research was conducted in the context of\nauthor MG’s time as an Afﬁliated Lecturer at the Depart-\nment of Computer Science and Technology, University of\nCambridge. We thank Alan Blackwell and others for their\nrole in organising and supporting this.\n9. REFERENCES\n[1] J. London, “Rhythm. Grove Music Online,” 2001.\n[Online]. Available: https://www.oxfordmusiconline.\ncom/grovemusic/view/10.1093/gmo/9781561592630.\n001.0001/omo-9781561592630-e-0000045963\n[2] ——, Hearing in time: Psychological aspects of musi-\ncal meter , 2nd ed. Oxford University Press, 2012.\n[3] R. Cohn, “Meter,” in The Oxford Handbook of Critical\nConcepts in Music Theory . Oxford University Press,\n01 2020. [Online]. Available: https://doi.org/10.1093/\noxfordhb/9780190454746.013.9\n[4] C. E. Seashore, “The psychology of music. ix. a\nperformance score with phrasing score for the violin,”\nMusic Educators Journal , vol. 24, no. 1, pp. 28–29,\n1937. [Online]. Available: http://www.jstor.org/stable/\n3385490\n[5] E. F. Clarke, “Levels of structure in the organization\nof musical time,” Contemporary Music Review , vol. 2,\nno. 1, pp. 211–238, 1987.\n[6] I. Bengtsson and A. Gabrielsson, “Rhythm research in\nUppsala,” Music, Room and Acoustics , 1977.\n[7] J. Yang, “Viennese style in viennese waltzes: An em-\npirical study of timing in the recordings of the blue\ndanube,” Musicologica Austriaca: Journal for Aus-\ntrian Music Studies , no. 2022, 2022.\n[8] R. Polak, “Rhythmic feel as meter: Non-isochronous\nbeat subdivision in jembe music from Mali,” Music\nTheory Online , vol. 16, no. 4, 2010.\n[9] J. London, R. Polak, and N. Jacoby, “Rhythm his-\ntograms and musical meter: A corpus study of Malian\npercussion music,” Psychonomic bulletin & review ,\nvol. 24, no. 2, pp. 474–480, 2017.\n[10] N. Jacoby, R. Polak, and J. London, “Extreme pre-\ncision in rhythmic interaction is enabled by role-\noptimized sensorimotor coupling: analysis and mod-\nelling of West African drum ensemble music,” Philo-\nsophical Transactions of the Royal Society B: Biologi-\ncal Sciences , vol. 376, no. 1835, p. 20200331, 2021.[11] M. Fuentes, L. S. Maia, M. Rocamora, L. W. Bis-\ncainho, H. C. Crayencour, S. Essid, and J. P. Bello,\n“Tracking beats and microtiming in Afro-Latin Amer-\nican music using conditional random ﬁelds and deep\nlearning,” in Proceedings of the 20th Conference of the\nInternational Society for Music Information Retrieval ,\n2019, pp. 251–258.\n[12] M. E. P. Davies, M. Fuentes, J. Fonseca, L. Aly,\nM. Jerónimo, and F. B. Baraldi, “Moving in time:\nComputational analysis of microtiming in maracatu de\nbaque solto,” in Proceedings of the 21st International\nSociety for Music Information Retrieval Conference ,\n2020, pp. 795–802.\n[13] J. Bilmes, “Timing is of the essence: Perceptual\nand computational techniques for representing, learn-\ning, and reproducing expressive timing in percussive\nrhythm,” Ph.D. dissertation, Massachusetts Institute of\nTechnology, 1993.\n[14] S. Flossmann, M. Grachten, and G. Widmer, “Expres-\nsive performance rendering: Introducing performance\ncontext,” Proceedings of the 6th Sound and Music\nComputing Conference (SMC) , pp. 155–160, 2009.\n[15] ——, Expressive Performance Rendering with Proba-\nbilistic Models . Springer London, 2013, pp. 75–98.\n[16] A. F. Blackwell, E. Cocker, G. Cox, A. McLean, and\nT. Magnusson, Live coding: a user’s manual . MIT\nPress, 2022.\n[17] A. McLean and G. Wiggins, “Tidal–pattern language\nfor the live coding of music,” in Proceedings of the\n7th sound and music computing conference , 2010, pp.\n331–334.\n[18] S. Aaron and A. F. Blackwell, “From Sonic Pi to\nOvertone: Creative musical experiences with domain-\nspeciﬁc and functional languages,” in Proceedings of\nthe ﬁrst ACM SIGPLAN workshop on Functional art,\nmusic, modeling & design , 2013, pp. 35–46.\n[19] J. McCartney, “Rethinking the computer music\nlanguage: Super collider,” Computer Music Journal ,\nvol. 26, no. 4, pp. 61–68, 2002. [Online]. Available:\nhttp://www.jstor.org/stable/3681770\n[20] J. Forth, “Cognitively-motivated geometric methods of\npattern discovery and models of similarity in music,”\nPh.D. dissertation, Goldsmiths, University of London,\n2012.\n[21] C. Ariza and M. S. Cuthbert, “Modeling beats, ac-\ncents, beams, and time signatures hierarchically with\nmusic21 meter objects,” in Proceedings of the 2010 In-\nternational Computer Music Conference, ICMC 2010 .\nMichigan Publishing, 2010. [Online]. Available:\nhttp://hdl.handle.net/2027/spo.bbp2372.2010.043Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n104[22] H. C. Longuet-Higgins and C. S. Lee, “The rhyth-\nmic interpretation of monophonic music,” Music\nPerception: An Interdisciplinary Journal , vol. 1,\nno. 4, pp. 424–441, 1984. [Online]. Available:\nhttp://www.jstor.org/stable/40285271\n[23] G. Sioros, M. E. P. Davies, and C. Guedes, “A\ngenerative model for the characterization of musical\nrhythms,” Journal of New Music Research , vol. 47,\nno. 2, pp. 114–128, 2018.\n[24] M. Gotham, “The metre metrics: Characterising\n(dis)similarity among metrical structures,” Ph.D. dis-\nsertation, University of Cambridge, 2015.\n[25] J. Neal, “Songwriter’s signature, artist’s imprint: The\nmetric structure of a country song,” in Country Mu-\nsic Annual 2000 , C. K. Wolfe and J. E. Akenson, Eds.\nLexington, KY: University Press of Kentucky, 2000,\npp. 112–140.\n[26] G. E. P. Box and M. E. Muller, “A note on the genera-\ntion of random normal deviates,” The Annals of Math-\nematical Statistics , vol. 29, no. 2, pp. 610–611, 1958.\n[27] S. Aaron, “Sonic Pi — reliable randomisation for\nperformances,” in 2016 IEEE Symposium on Visual\nLanguages and Human-Centric Computing (VL/HCC) ,\n2016, pp. 242–243.\n[28] M. Clayton, K. Jakubowski, T. Eerola, P. E. Keller,\nA. Camurri, G. V olpe, and P. Alborno, “Interpersonal\nentrainment in music performance: theory, method,\nand model,” Music Perception: An Interdisciplinary\nJournal , vol. 38, no. 2, pp. 136–194, 2020.\n[29] R. Polak, S. Tarsitani, and M. Clayton, “IEMP Malian\njembe,” 7 2020.\n[30] M. Clayton, S. Tarsitani, R. Jankowsky, L. Jure,\nL. Leante, R. Polak, A. Poole, M. Rocamora, P. Al-\nborno, A. Camurri et al. , “The interpersonal entrain-\nment in music performance data collection,” Empirical\nMusicology Review , vol. 16, no. 1, pp. 65–84, 2021.\n[31] S. Dixon, “Automatic extraction of tempo and beat\nfrom expressive performances,” Journal of New Music\nResearch , vol. 30, no. 1, pp. 39–58, 2001.\n[32] J. J. Moré, “The Levenberg-Marquardt algorithm:\nimplementation and theory,” in Numerical analysis .\nSpringer, 1977, pp. 105–116. [Online]. Available:\nhttps://www.osti.gov/biblio/7256021\n[33] I. Bengtsson, “Empirische rhythmusforschung in Up-\npsala,” Hamburger Jahrbuch für Musikwissenschaft ,\nvol. 1, pp. 195–219, 1974.\n[34] M. Müller and F. Zalkow, “libfmp: A Python pack-\nage for fundamentals of music processing,” Journal of\nOpen Source Software , vol. 6, no. 63, p. 3326, 2021.[35] M. Müller, Fundamentals of music processing: Us-\ning Python and Jupyter notebooks , 2nd ed. Springer,\n2021.\n[36] N. Jacoby, R. Polak, and J. London, “Supplementary\nmaterial from extreme precision in rhythmic interac-\ntion is enabled by role-optimized sensorimotor cou-\npling: analysis and modelling of West African drum\nensemble music,” 7 2021.\n[37] H. Neuhoff, R. Polak, and T. Fischinger, “Perception\nand evaluation of timing patterns in drum ensemble\nmusic from Mali,” Music Perception: An Interdisci-\nplinary Journal , vol. 34, no. 4, pp. 438–451, 2017.\n[38] C. Dittmar, M. Pﬂeiderer, S. Balke, and M. Müller,\n“A swingogram representation for tracking micro-\nrhythmic variation in jazz performances,” Journal of\nNew Music Research , vol. 47, no. 2, pp. 97–113, 2018.\n[39] L. Jure and M. Rocamora, “Microtiming in the rhyth-\nmic structure of Candombe drumming patterns,” in 4th\nInt. Conf. on Analytical Approaches to World Music\n(AAWM) , 6 2016.\n[40] L. Naveda, F. Gouyon, C. Guedes, and M. Leman,\n“Microtiming patterns and interactions with musical\nproperties in samba music,” Journal of New Music Re-\nsearch , vol. 40, no. 3, pp. 225–238, 2011.\n[41] D. M. Weigl, C. VanderHart, M. Pescoller, D. Ramm-\nler, M. Grassl, F. Trümpi, and W. Goebl, “The vienna\nphilharmonic orchestra’s new year’s concerts: Build-\ning a fair data corpus for musicology,” in Proceed-\nings of the 9th International Conference on Digital Li-\nbraries for Musicology . Association for Computing\nMachinery, 2022, pp. 36–40.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n105"
    },
    {
        "title": "Roman Numeral Analysis With Graph Neural Networks: Onset-Wise Predictions From Note-Wise Features.",
        "author": [
            "Emmanouil Karystinaios",
            "Gerhard Widmer"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265357",
        "url": "https://doi.org/10.5281/zenodo.10265357",
        "ee": "https://zenodo.org/records/10265357/files/000070.pdf",
        "abstract": "Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. \nThis paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score. \nThe proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm. \nOur results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets. \nIn addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/manoskary/chordgnn",
        "zenodo_id": 10265357,
        "dblp_key": "conf/ismir/KarystinaiosW23",
        "keywords": [
            "ChordGNN",
            "Graph Neural Networks",
            "tonal music",
            "symbolic music",
            "automatic Roman Numeral analysis",
            "note-wise representation",
            "edge contraction algorithm",
            "higher accuracy",
            "reference datasets",
            "post-processing"
        ],
        "content": "ROMAN NUMERAL ANALYSIS WITH GRAPH NEURAL NETWORKS:\nONSET-WISE PREDICTIONS FROM NOTE-WISE FEATURES\nEmmanouil Karystinaios1Gerhard Widmer1,2\n1Institute of Computational Perception, Johannes Kepler University Linz, Austria\n2LIT AI Lab, Linz Institute of Technology, Austria\nfirstname.lastname@jku.at\nABSTRACT\nRoman Numeral analysis is the important task of identify-\ning chords and their functional context in pieces of tonal\nmusic. This paper presents a new approach to automatic\nRoman Numeral analysis in symbolic music. While exist-\ning techniques rely on an intermediate lossy representation\nof the score, we propose a new method based on Graph\nNeural Networks (GNNs) that enable the direct description\nand processing of each individual note in the score. The\nproposed architecture can leverage notewise features and\ninterdependencies between notes but yield onset-wise repre-\nsentation by virtue of our novel edge contraction algorithm.\nOur results demonstrate that ChordGNN outperforms ex-\nisting state-of-the-art models, achieving higher accuracy in\nRoman Numeral analysis on the reference datasets. In addi-\ntion, we investigate variants of our model using proposed\ntechniques such as NADE, and post-processing of the chord\npredictions. The full source code for this work is available\nathttps://github.com/manoskary/chordgnn\n1. INTRODUCTION\nAutomatic Chord Recognition is one of the core problems\nin Music Information Retrieval. The task consists of iden-\ntifying the harmonies or chords present in a musical piece.\nVarious methods have been proposed to address this task\nusing either an audio or symbolic representation of the mu-\nsic [1]. In the symbolic domain, most approaches focus\non the related and arguably more complex problem of Au-\ntomatic Roman Numeral Analysis, which is a functional\nharmony analysis problem that has its roots in musicologi-\ncal research of Western classical music.\nRoman Numeral Analysis is a notational system used in\nmusic theory to analyze chord progressions and identify the\nrelationship between chords in a given key. In this system,\neach chord in a piece of music is assigned a Roman numeral\nbased on its position within the key’s scale. For example, in\nthe key of C major, the I chord is C major, the IV chord is\nF major, and the V chord is G major. Roman Numerals are\n© E. Karystinaios and G. Widmer. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribution:\nE. Karystinaios and G. Widmer, “Roman Numeral Analysis with Graph\nNeural Networks: Onset-wise Predictions from Note-wise Features”, in\nProc. of the 24th Int. Society for Music Information Retrieval Conf., Milan,\nItaly, 2023.an important tool for understanding and analyzing the har-\nmonic structure of music, and they are a valuable resource\nfor musicians, composers, and arrangers alike.\nIn Music Information Retrieval, a lot of work has been\ndone to automate Roman Numeral analysis. However, cur-\nrent approaches still face signiﬁcant challenges. Some of\nthese are related to the large chord symbol vocabulary. A\ncommon way to address this problem is to divide a Roman\nNumeral into several components (e.g., key, degree, inver-\nsion) and transform the analysis into a multitask learning\nscenario. However, multitask approaches themselves face\nchallenges with interdependencies among tasks. Lastly, Ro-\nman Numeral analysis faces a score representation problem\nrelated to existing models such as CNNs whose inputs must\nbe in ﬁxed-sized chunks. Recent state-of-the-art approaches\nfollow an audio-inspired strategy, dividing a musical score\ninto ﬁxed-length time frames (\"windows\") which are then\nprocessed by a Convolutional Recurrent Neural Network\n(CRNN). However, such a representation is unnatural for\nscores and has the added practical disadvantage of being\ntime-limited (for example regarding notes extending be-\nyond the current window) and, due to the ﬁxed-length (in\nterms of score time) constraint, capturing varying amounts\nof musically relevant context.\nIn this paper, we propose a new method for automatic\nRoman Numeral analysis based on Graph Neural Networks\nthat can leverage note-wise information to address the score\nrepresentation issue. Our model, ChordGNN , builds on\ntop of existing multitask approaches but introduces several\nnovel aspects, including a graph convolutional architecture\nwith an edge contraction pooling layer that combines convo-\nlution at the note level but yields the learned representation\nat the onset level.\nOur proposed method, ChordGNN , is evaluated on a\nlarge dataset of Western classical music, and the experimen-\ntal results demonstrate that it outperforms existing state-\nof-the-art methods, in terms of the commonly used Chord\nSymbol Recall measure. To address the interdependencies\namong tasks we investigate the effect of post-processing\nand other proposed techniques such as NADE and gradient\nnormalization. Finally, we look at a qualitative musical\nexample and compare our model’s predictions with other\nstate-of-the-art models.597Figure 1 . A Roman Numeral analysis for two bars for four-\npart harmony in Cmajor. Capital letters stand for major\nquality and lowercase for minor quality. The third chord has\na dominant seven as its primary degree and the dominant of\nCmajor as its secondary degree. The V6\n5indicates a major\nwith a seven quality in second inversion. The bass (lowest\nchord note) of that chord is Fsharp, the root is D, and the\nlocal key is Cmajor.\n2. RELATED WORK\nThere is a big body of literature covering the topic of Au-\ntomatic Chord Recognition applied in the audio domain;\nhowever, in our work, we focus on the problem of auto-\nmatic Roman Numeral Analysis in the symbolic domain.\nIt consists of labeling the chords and harmonic progres-\nsions in a piece of music using Roman Numerals, where\neach numeral represents a chord built on a particular scale\ndegree. Numerous approaches have tried to automate Ro-\nman Numeral analysis or infer harmonic relations between\nchords. Notable work includes statistical models such as\nMelisma [2], HMM-based models [3], and grammar-based\napproaches [4].\nIn recent years, research has shifted towards a deep learn-\ning and data-driven approach. Due to the large vocabulary\nof possible Roman Numerals, the problem has been divided\ninto several component subtasks, thus resulting in a multi-\ntask learning setting [5]. As a multitask problem, a Roman\nNumeral is characterized by the following components: the\nprimary and secondary degree (as illustrated in Figure 1),\nthe local key at the time point of prediction, the root of the\nchord, the inversion of the chord, and the quality (such as\nmajor, minor, 7, etc.). Although the root can be derived\nfrom the other components, it was pointed out by [6] that\nredundancy is assisting Roman Numeral analysis systems\nto learn. An example of Roman Numerals and their com-\nponents can be viewed in Figure 1. Recent state-of-the-art\napproaches decompose the numeral prediction task to the\nsimultaneous prediction of those 6 components [5–9].\nMost deep learning approaches to Roman Numeral anal-\nysis are inspired by work in audio classiﬁcation, cutting\na score into ﬁxed-size chunks (in terms of some constant\nscore time unit; e.g., a 32nd note) and using these as input\nto deep models. Using this quantized time frame repre-\nsentation, [6] introduced a CRNN architecture to predict\nRoman Numerals. Other work has continued to build on the\nlatter by introducing more tasks to improve performance\nsuch as the AugmentedNet model [7], or introducing intra-\ndependent layers to inform in an orderly fashion the predic-\ntion of one task with the previously predicted task, such as\nthe model introduced by [8]. Other architectures, such as\nFigure 2 . Different representations of the score excerpt\nshown in the middle. Top: quantized time frame representa-\ntion, bottom: graph representation.\nthe CSM-T model, have demonstrated good results by intro-\nducing modular networks which treat a score as a sequence\nof notes ordered ﬁrst by onset and then by pitch [9].\nShould a musicologist perform music analysis on a piece\nof music, they would consider the individual notes exist-\ning in the score. Thus, a time frame representation would\ncome across as unnatural for symbolic music and in partic-\nular for such an analysis task. In this paper, we present a\nmethod that no longer treats the score as a series of quan-\ntized frames but rather as a partially ordered set of notes\nconnected by the relations between them, i.e., a graph. A\nvisual comparison of the two representations is shown in\nFigure 2. Recently, modeling scores as graphs has also been\ndemonstrated to be beneﬁcial for problems such as expres-\nsive performance generation [10], cadence detection [11],\nvoice separation [12], or boundary detection [13].\nAutomatic Roman Numeral analysis, as a multitask prob-\nlem, is mostly tackled with hard parameter-sharing models.\nThese models share part of the model across all tasks as an\nencoder, and then the common embeddings are branched\nto a classiﬁcation model per task [6 –8]. However, some\napproaches separate tasks from this paradigm to a more\nmodular or soft parameter sharing approach [9].\nIn the ﬁeld of multitask learning, a lot of research has\nbeen done on the problem of conﬂicting gradients during\nbackpropagation in hard parameter-sharing models. Issues\nwith multi-objective optimization have been early addressed\nby Zhang et al. [14] and recent solutions have been pro-\nposed for the multitask setting in the form of dynamic task\nprioritization [15], gradient normalization [16], rotation\nmatrices [17], or even game-theoretic approaches [18]. In\nour work, we experimentally evaluate some of these tech-\nniques in the multitask setting to investigate whether Roman\nNumeral analysis subtasks conﬂict with each other (see Sec-\ntion 5.2).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n598Figure 3 . The proposed Architecture Chord-GNN\n3. METHODOLOGY\n3.1 Roman Numeral Analysis\nWe already discussed, in Section 2, how Roman Numeral\nanalysis can be viewed as a multi-task problem. In this\nsection, we describe in detail the additional tasks introduced\nby [7] that we also use for training and prediction. First,\nlet us assume that the prediction can be broken down into\nspeciﬁc time points, and each time point is attributed to a\nunique onset in the score.\nThe Roman Numeral prediction can be viewed as a si-\nmultaneous prediction of the local key, degree (primary\nand secondary), quality, inversion, and root. Each one of\nthese tasks is a categorical, multiclass classiﬁcation prob-\nlem. However, [7] indicated that only three tasks would be\nsufﬁcient for 98% of the Roman Numeral annotations in\nour dataset (detailed in Section 4.1). These three tasks com-\nprise the prediction of a restricted vocabulary of common\nRoman Numeral symbols in combination with the local key\nand the inversion. We refer to Roman Numeral prediction\ninvolving the 5 tasks as conventional RN , and the combined\nprediction of key, inversion, and restricted RN vocabulary\nalternative RN , asRNalt, in accordance with [7].\nSeveral other tasks have been introduced that have been\nshown to improve the performance of related models [7].\nThese include the Harmonic Rhythm, which is used to infer\nthe duration of a Roman Numeral at a given time point; the\nTonicization task, a multiclass classiﬁcation task that refers\nto a tonicized key implied by the Roman Numeral label and\nis complementary to the local key; the Pitch Class Sets task,\nwhich includes a vocabulary of different pitch class sets,\nand the Bass task, which aims to predict the lowest note in\nthe Roman Numeral label.\n3.2 Graph Representation of Scores\nOur approach to automatic Roman Numeral analysis no\nlonger treats the score as a sequence of quantized time\nframes but rather as a graph, which permits us to specify\nnote-wise information such as pitch spelling, duration, and\nmetrical position. We use graph convolution to model inter-\ndependencies between notes. We model our score generally\nfollowing Karystinaios and Widmer [11], but we opt for a\nheterogeneous graph convolution approach, i.e., including\ndifferent edge relations/types. Furthermore, we develop anedge contraction pooling layer that learns onset-wise rep-\nresentations from the note-wise embeddings and therefore\nyields a sequence.\nAfter the edge contraction, we follow [6 –8] by adding\nto the graph convolution a sequence model for the hard-\nsharing part of our model, and simple shallow multi-layer\nperceptron heads for each task. In essence, we replace the\nCNN encoder that works on quantized frames of the score\nin previous approaches, with a graph convolutional encoder\nfollowed by an edge contraction layer. Our proposed archi-\ntecture is shown in Figure 3.\nThe input to the GNN encoder is an attributed graph G=\n(V,E,X)whereVandEdenote its node and edge sets and\nXrepresents the node feature matrix, which contains the\nfeatures of the notes in the score. For our model, we used\npitch spelling, note duration, and metrical position features.\nGiven a musical piece, the graph-building process cre-\nates a set of edges E, with different relation types R. A\nlabeled edge (u,r,v)of typerbetween two notes u,vbe-\nlongs toEif the following conditions are met:\n• notes starting at the same time:\non(u) =on(v)− →r=onset\n•note starting while the other is sounding: on(u)>\non(v)∧on(u)≤on(v)+dur(v)− →r=during\n• note starting when the other ends:\non(u)+dur(u) =on(v)− →r=follow\n•note starting after a time frame when no note is sound-\ning:on(u)+dur(u)< on(v)∧∄v′∈V, on(v′)<\non(v)∧on(v′)> on(u)+dur(u)− →r=silence\n3.3 Model\nIn this section, we introduce and describe ChordGNN , a\nGraph Convolutional and Recurrent Neural Network. The\nstructure of the network is visually outlined in Figure 3.\nChordGNN uses heterogeneous graphSAGE [19] convolu-\ntional blocks deﬁned as:\nh(l+1)\nNr(v)= mean/parenleftbig\n{hl\nu,∀u∈ Nr(v)}/parenrightbig\nh(l+1)\nvr=σ/parenleftBig\nW·concat(hl\nv,hl+1\nNr(v))/parenrightBig\nh(l+1)\nv=1\n|R|/summationdisplay\nr∈Rh(l+1)\nvr(1)Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n599whereh(0)\nv=xvandxuis the input features for node\nu,N(u)are the neighbors of node u, andσis a ReLU\nactivation function. We name the output representations of\nall nodes after graphSAGE convolution H={h(L)\nu|u∈\nV}whereLis the total number of convolutional layers.\nGiven the hidden representation Hof all nodes, and\nonset edges EOn={(u,v)|on(u) =on(v)}, the on-\nset edge contraction pooling is described by the following\nequations: ﬁrst, we update the hidden representation with\na learned weight, H′=HW(cpool). Subsequently we need\nto unify the representations for every node u, such that\n∀v∈ N On(v), h(cp)\nu=h(cp)\nv:\nh(cp)\nu=hu+/summationdisplay\nv∈N On(v)hv (2)\nwhere,huandhvbelong to H′. Subsequently, we ﬁlter the\nvertices:\nV′={v∈V| ∀u∈V,(v,u)∈EOn=⇒u /∈V′}(3)\nTherefore, H(cp)={h(cp)\nu| ∀u∈V′}are the rep-\nresentations obtained. Sorting the representations by the\nonset on which they are attributed we obtain a sequence\nS= [h(cp)\nu1,h(cp)\nu2,...h(cp)\nuk]such that on(u1)< on(u2)<\n···< on(uk).\nThe sequence Sis then passed through an MLP layer\nand 2 GRU layers. This concludes the hard-sharing part of\nour model. Thereafter, an MLP head is attached per task,\nas shown in Figure 3.\nFor training, we use the dynamically weighted loss in-\ntroduced by [20]. The total loss Ltotof our network is\ncalculated as a weighted sum of the individual losses for\nevery task, where the weights are learned during training:\nLtot=/summationdisplay\nt∈TLt∗1\n2γ2\nt+log(1+ γ2\nt) (4)\nwhereTis the set of tasks; Ltis the cross-entropy loss\nrelating to task t; theγtare learned scalars that give the\nweight for each task t; and thelogexpression is a regular-\nization term [20].\nFigure 4 . Post-processing of Roman Numeral predictions.3.3.1 Post-processing\nWe enhance our model with a post-processing phase after\nthe model has been trained. The post-processing phase com-\nbines the logits of all tasks’ predictions by concatenating\nthem and, then, feeds them to a single-layer bidirectional\nLTSM block. Then, again the embeddings of the sequential\nblock are distributed to 11 one-layer MLPs, one for each\ntask. The post-processing block is sketched in Figure 4.\n4. EXPERIMENTS AND CORPORA\nIn the experiments, we compare our model, ChordGNN ,\nwith other recent models for automatic Roman Numeral\nanalysis. We run experiments with our model in the ex-\nact same way as described in the paper [7], including the\nspeciﬁc data splits, so that our results are directly compa-\nrable to the ﬁgures reported there. A detailed comparison\nof the results will be given in Table 1. Furthermore, we\ndevelop variants of our model using proposed techniques\nsuch as NADE [8], and post-processing of the chord pre-\ndictions. We report a conﬁguration study of our model on\nthe use of gradient normalization techniques and NADE\nthat should improve results on Multi-Task learning scenar-\nios and avoid common Multi-Task Learning problems such\nas conﬂicting gradients. Lastly, we compare our model\nwith the updated version v1.9.1 of the state-of-the-art model\nAugmented-Net [21] and datasets.\n4.1 Datasets\nFor training and evaluation, we combined six data sources\ninto a single \"Full\" Dataset of Roman Numeral annota-\ntions in accordance with [7]: the Annotated Beethoven Cor-\npus (ABC) [22]; the annotated Beethoven Piano Sonatas\n(BPS) dataset [5]; the Haydn String Quartets dataset\n(HaydnSun) [23]; the TA VERN dataset [24]; a part of\nthe When-in-Rome (WiR) dataset [25, 26]; and the Well-\nTempered-Clavier (WTC) dataset [25] which is also part of\nthe WiR dataset.\nTraining and test splits for the full dataset were also\nprovided by [7]. It is worth noting that the BPS subset splits\nwere already predeﬁned in [5]. In total, approximately 300\npieces were used for training, and 56 pieces were used\nfor testing, proportionally taken from all the different data\nsources. We draw a distinction for the BPS test set, which\nincludes 32 Sonata ﬁrst movements and for which we ran\nan additional experiment. The full test set also includes the\n7 Beethoven piano sonatas.\nIn addition to the above datasets, we include data aug-\nmentations identical to the ones described in [7]: textur-\nization and transposition. The texturization is based on a\ndataset augmentation technique introduced by [27]. The\ntransposition augmentation boils down to transposing a\nscore to all the keys that lie within a range of key signatures\nthat have up to 7 ﬂats or sharps. It should be noted that the\naugmentations are only applied in the training split.\nFor our last experiment (to be reported on in Section 5.3\nbelow), we add additional data that were recently introduced\nby [21]. The additional data include the annotated MozartProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n600Model Key Degree Quality Inversion Root RN RN (Onset) RN altBPSMicchi (2020) 82.9 68.3 76.6 72.0 - 42.8 - -\nCSM-T (2021) 69.4 - - - 75.4 45.9 - -\nAugNet (2021) 85.0 73.4 79.0 73.4 84.4 45.4 - 49.3\nChordGNN (Ours) 79.9 71.1 74.8 75.7 82.3 46.2 46.6 48.6\nChordGNN+Post (Ours) 82.0 71.5 74.1 76.5 82.5 49.1 49.4 50.4FullAugNet (2021) 82.9 67.0 79.7 78.8 83.0 46.4 - 51.5\nChordGNN (Ours) 80.9 70.1 78.4 78.8 84.8 48.9 48.4 50.4\nChordGNN+Post (Ours) 81.3 71.4 78.4 80.3 84.9 51.8 51.2 52.9\nTable 1 . Model comparison on two different test sets, the Beethoven Piano Sonatas (BPS), and the full test set. RN stands\nfor Roman Numeral, RNaltfor the alternative Roman Numeral computations discussed in Section 3.1. RN(Onset)refers\nto onset-wise prediction accuracy, all other scores use the CSR score (see Section 5). Note that model CSM-T reports Mode\ninstead of Quality .\nPiano Sonatas (MPS) dataset [28] for which we also applied\nthe aforementioned augmentations.\n4.2 Conﬁguration\nFor all our experiments, we train our network with the\nAdamW optimizer. We ﬁx our architecture with a hidden\nsize of256, a learning rate of 0.0015 , a weight decay of\n0.005, and a dropout of 0.5which is applied to each learning\nblock of our architecture.\n5. RESULTS\nAs an evaluation metric, we use Chord Symbol Recall\n(CSR) [29] where for each piece, the proportion of time\nis collected during which the estimated label matches the\nground truth label. We apply the CSR at the 32nd note\ngranularity level, in accordance with [6, 7, 9].\n5.1 Quantitative Results\nIn the ﬁrst experiment, which compares our ChordGNN to\nexisting state-of-the-art approaches, we evaluate the full\ndataset, but also the annotated Beethoven Piano Sonatas\n(BPS) [5] subset, which many previous approaches had also\nused. The results are shown in Table 1. We present the CSR\nscores (where they are applicable) for Local Key, Degree,\nQuality, Inversion, Root, conventional Roman Numeral, and\nAlternative Roman Numeral (see Section 3). Furthermore,\nwe include the onset-wise accuracy score for our models’\nconventional Roman Numeral predictions.\nOn the BPS subset, we compare our model ChordGNN\nwith the Micchi (2020) model [6], the CSM-T (2021)\nmodel [9] and the AugmentedNet 2021 model [7]. Our\nresults on Roman Numeral prediction surpass all previous\napproaches. Note that the AugmentedNet model exhibits\nhigher prediction scores on the individual Key, Degree,\nQuality, and Root tasks, which are used jointly for the\nprediction of the Roman numeral. These results indicate\nthat our model obtains more meaningfully interrelated pre-\ndictions, with respect to the Roman numeral prediction,\nresulting in a higher accuracy score.\nMoreover, we compare ChordGNN toAugmentedNet on\nthe full test dataset. Our model surpasses AugmentedNetVariant RN RN alt\nChordGNN (Baseline) 46.1±0.003 47 .8±0.007\nChordGNN + WLoss 48.9±0.00150.4±0.010\nChordGNN + Rotograd 45.5±0.003 47 .1±0.005\nChordGNN + R-GradN 45.2±0.006 46 .7±0.005\nChordGNN + NADE 48.2±0.005 49 .9±0.005\nTable 2 . Conﬁguration Study: Chord Symbol Recall on\nRoman Numeral analysis on the full test set. RN stands for\nRoman Numeral, RNaltrefers to the alternative Roman Nu-\nmeral computations discussed in section 3.1. WLoss stands\nfor the dynamically weighted loss described in Section 3,\nand R-GradN stands for Rotograd with Gradient Normal-\nization. Every experiment is repeated 5 times with the same\nChordGNN model as Table 1 without post-processing.\nwith and without post-processing in all ﬁelds apart from\nlocal key prediction and quality. Our model obtains up\nto11.6%improvement in conventional Roman Numeral\nprediction.\nIn both experiments, post-processing has been shown\nto improve both RN andRNalt. However, ChordGNN\nwithout post-processing already surpasses the other models.\n5.2 Conﬁguration Study\nFor a systematic study of multitask training, we investi-\ngated the effects of extension modules, gradient normal-\nization techniques, and learnable weight loss. In detail,\nwe test 5 conﬁgurations using as baseline the ChordGNN\nmodel (without post-processing) with standard CE loss and\nno weighing. Furthermore, we test our proposed architec-\nture using the dynamically weighted loss described in Sec-\ntion 3.3 (same as the model in Table 1), Rotograd [17] and\nGradNorm [16] for Gradient Normalization, and NADE [8].\nThe models are run on the Full data set described above\nand averaged over ﬁve runs with random initialization. The\nresults, summarized in Table 2, suggest that using the dy-\nnamically weighted loss yields better results compared to\nother methods such as the Baseline or Gradient Normaliza-\ntion techniques. Furthermore, the dynamically weighted\nloss is comparable to NADE but also more robust on Con-\nventional Roman Numeral prediction on our datasets.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n601Figure 5 . A comparison between the human annotation, AugmentedNet, and ChordGNN on a passage of Haydn’s string\nquartet op.20 No.3 movement 4. The red (wrong) markings on Human Analysis and AugNet (2022) are from [21]\n5.3 Latest developments\nOur last experiment focuses on speciﬁc developments that\nhave very recently been published in Nápoles López’s Ph.D.\nthesis [21]. In the thesis, three additional tasks, related to\npredicting the components of a canonical representation of\nthe current chord, as implied by the Roman Numeral, were\nproposed and the dataset was extended with the Annotated\nMozart Piano Sonatas (MPS) corpus [28], as mentioned in\nSection 4.1 above.\nTo test the relevance of these updates, we trained an\nadapted version of our model, now with 11+3=14 individ-\nual tasks and including the Mozart data. It turns out that\nthe updated model improves signiﬁcantly in performance,\nachieving a 53.5CSR score on conventional Roman Nu-\nmeral (compare this to row \"ChordGNN (Ours)\" in Table\n1). Furthermore, post-processing can improve the results by\nup to two additional percentage points.1\n5.4 A Musical Example\nIn Figure 5, we look at a comparison between the human an-\nnotations, AugmentedNet andChord-GNN predictions (The\nmusical excerpt is taken from Nápoles López’s thesis [21],\nand the predictions relate to the new models trained as de-\nscribed in the previous section.). Marked in red are false\npredictions, and marked in yellow are correct predictions\nof the model with wrong ground-truth annotations. Both\nmodels’ predictions are very similar to the human analysis.\nHowever, our model correctly predicts the initial pickup\nmeasure annotation. In measure 2, the ground truth anno-\ntation marks a tonic in ﬁrst inversion; however, the viola\nat that point is lower than the cello and therefore the chord\nis actually in root position. Both models obtain a correct\nprediction at that point. Subsequently, our model predicts\na harmonic rhythm of eighth notes, which disagrees with\nthe annotator’s half-note marking. Analyzing the underly-\ning harmony in that passage, we can justify our model’s\nchoices.\n1Unfortunately, we cannot directly compare these numbers to [21], as\ntheir results are not reported in comparable terms.The human annotation suggests that the entire second\nhalf of the 2nd measure represents a viiochord. However,\nit should not be in the ﬁrst inversion, as the cello plays\nan F# as the lowest note (which is the root of viio). The\nAugNet analysis faces the same issue, in contrast with the\npredictions of ChordGNN. However, there are two conﬂict-\ning interpretations of the segment. First, the viioon the\nthird beat is seen as a passing chord between the surround-\ning tonic chords, leading to a dominant chord in the next\nmeasure. Alternatively, the viiocould already be part of\na prolonged dominant harmony (with passing chords on\nthe offbeats) leading to the V7. The ChordGNN solution\naccommodates both interpretations as it doesn’t attempt to\ngroup chords at a higher level, treating each eighth note as\nan individual chord rather than a passing event. The other\ntwo solutions prefer the second option.\n6. CONCLUSION\nIn this paper, we presented ChordGNN , a model for auto-\nmatic Roman Numeral analysis in symbolic music, based on\na note-level, graph-based score representation. We showed\nthatChordGNN improves on other state-of-the-art models,\nand that post-processing can further improve the accuracy\nof the predictions. A conﬁguration study suggests that gra-\ndient normalization techniques or techniques for carrying\nprediction information across tasks are not particularly ben-\neﬁcial or necessary for such a model.\nFollow-up work will focus on strengthening the robust-\nness of our models by pre-training with self-supervised\nmethods on large corpora. We believe that such pre-training\ncan be beneﬁcial for learning helpful intrinsic musical in-\nformation. Such a step is crucial since more data improves\npredictions but Roman Numeral annotations are hard to\nﬁnd or produce. Moreover, we aim to enrich the number of\ntasks for joint prediction by including higher-level analyti-\ncal targets such as cadence detection and phrase boundary\ndetection. Finally, we aim to extend our method to the audio\ndomain.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6027. ACKNOWLEDGEMENTS\nWe gratefully acknowledge the musical analysis of the viio\npassage in Fig. 5 (Section 5.4) that was offered by an\nanonymous reviewer, and which we took the liberty of\nadopting for our text. This work is supported by the Eu-\nropean Research Council (ERC) under the EU’s Horizon\n2020 research & innovation programme, grant agreement\nNo. 101019375 (“Whither Music?”), and the Federal State\nof Upper Austria (LIT AI Lab).\n8. REFERENCES\n[1]J. Pauwels, K. O’Hanlon, E. Gómez, M. Sandler et al. ,\n“20 years of Automatic Chord Recognition from Audio,”\ninProceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR) , 2019.\n[2]D. Temperley, The cognition of basic musical structures .\nMIT press, 2004.\n[3]C. Raphael and J. Stoddard, “Functional Harmonic\nAnalysis Using Probabilistic Models,” Computer Music\nJournal , vol. 28, no. 3, pp. 45–52, 2004.\n[4]J. P. Magalhaes and W. B. de Haas, “Functional Mod-\nelling of Musical Harmony: an experience report,” ACM\nSIGPLAN Notices , vol. 46, no. 9, pp. 156–162, 2011.\n[5]T.-P. Chen, L. Su et al. , “Functional Harmony Recogni-\ntion of Symbolic Music Data with Multi-task Recurrent\nNeural Networks.” in Proceedings of the International\nSociety for Music Information Retrieval Conference (IS-\nMIR) , 2018.\n[6]G. Micchi, M. Gotham, and M. Giraud, “Not all roads\nlead to Rome: Pitch representation and model archi-\ntecture for automatic harmonic analysis,” Transactions\nof the International Society for Music Information Re-\ntrieval (TISMIR) , vol. 3, no. 1, pp. 42–54, 2020.\n[7]N. Nápoles López, M. Gotham, and I. Fujinaga, “Aug-\nmentedNet: A Roman Numeral Analysis Network with\nSynthetic Training Examples and Additional Tonal\nTasks.” in Proceedings of the International Society for\nMusic Information Retrieval Conference (ISMIR) , 2021.\n[8]G. Micchi, K. Kosta, G. Medeot, and P. Chanquion,\n“A deep learning method for enforcing coherence in\nAutomatic Chord Recognition.” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2021.\n[9]A. P. McLeod and M. A. Rohrmeier, “A modular system\nfor the harmonic analysis of musical scores using a large\nvocabulary,” in Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2021.\n[10] D. Jeong, T. Kwon, Y . Kim, and J. Nam, “Graph Neural\nNetwork for Music Score Data and Modeling Expres-\nsive Piano Performance,” in Proceedings of Interna-\ntional Conference on Machine Learning (ICML) , 2019.[11] E. Karystinaios and G. Widmer, “Cadence Detection\nin Symbolic Classical Music using Graph Neural Net-\nworks,” Proceedings of the International Society for\nMusic Information Retrieval Conference (ISMIR) , 2022.\n[12] E. Karystinaios, F. Foscarin, and G. Widmer, “Musi-\ncal V oice Separation as Link Prediction: Modeling a\nMusical Perception Task as a Multi-Trajectory Track-\ning Problem,” in Proceedings of the International Joint\nConference on Artiﬁcial Intelligence (IJCAI) , 2023.\n[13] C. Hernandez-Olivan, S. R. Llamas, and J. R. Beltran,\n“Symbolic Music Structure Analysis with Graph Repre-\nsentations and Changepoint Detection Methods,” 2023.\n[14] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial\nLandmark Detection by Deep Multi-task Learning,” in\nProceedings of the European Conference on Computer\nVision (ECCV) , 2014.\n[15] M. Guo, A. Haque, D.-A. Huang, S. Yeung, and L. Fei-\nFei, “Dynamic Task Prioritization for Multitask Learn-\ning,” in Proceedings of the European Conference on\nComputer Vision (ECCV) , 2018.\n[16] Z. Chen, V . Badrinarayanan, C.-Y . Lee, and A. Rabi-\nnovich, “GradNorm: Gradient Normalization for Adap-\ntive Loss Balancing in Deep Multitask Networks,” in\nProceedings of the International Conference on Ma-\nchine Learning (ICML) , 2018.\n[17] A. Javaloy and I. Valera, “RotoGrad: Gradient Homog-\nenization in Multitask Learning,” in Proceedings of the\nInternational Conference on Learning Representations\n(ICLR) , 2022.\n[18] A. Navon, A. Shamsian, I. Achituve, H. Maron,\nK. Kawaguchi, G. Chechik, and E. Fetaya, “Multi-task\nLearning as a Bargaining Game,” Proceedings of the\nInternational Conference on Machine Learning (ICML) ,\n2022.\n[19] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive rep-\nresentation learning on large graphs,” Advances in neu-\nral information processing systems , vol. 30, 2017.\n[20] L. Liebel and M. Körner, “Auxiliary tasks in multi-task\nlearning,” arXiv preprint arXiv:1805.06334 , 2018.\n[21] N. Nápoles López, “Automatic roman numeral analysis\nin symbolic music representations,” Ph.D. dissertation,\nSchulich School of Music McGill University, December\n2022.\n[22] M. Neuwirth, D. Harasim, F. C. Moss, and\nM. Rohrmeier, “The Annotated Beethoven Corpus\n(ABC): A dataset of harmonic analyses of all Beethoven\nstring quartets,” Frontiers in Digital Humanities , vol. 5,\np. 16, 2018.\n[23] N. Nápoles López, “Automatic Harmonic Analysis of\nClassical String Quartets from Symbolic Score,” Ph.D.\ndissertation, Master’s thesis, Universitat Pompeu Fabra,\n2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n603[24] J. Devaney, C. Arthur, N. Condit-Schultz, and K. Nisula,\n“Theme and Variation Encodings with Roman Numerals\n(TA VERN): A new data set for symbolic music analysis,”\ninProceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR) , 2015.\n[25] M. Gotham, D. Tymoczko, and M. S. Cuthbert, “The Ro-\nmanText Format: A Flexible and Standard Method for\nRepresenting Roman Numeral Analyses.” in Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , 2019.\n[26] M. R. H. Gotham and P. Jonas, “The Openscore Lieder\nCorpus,” in Proceedings of the Music Encoding Confer-\nence (MEC) , 2021.\n[27] N. Nápoles López and I. Fujinaga, “Harmonic Reduc-\ntions as a Strategy for Creative Data Augmentation,” in\nLate-Breaking Demo at International Society for Music\nInformation Retrieval Conference (ISMIR) , 2020.\n[28] J. Hentschel, M. Neuwirth, and M. Rohrmeier, “The An-\nnotated Mozart Sonatas: Score, Harmony, and Cadence,”\nTransactions of the International Society for Music In-\nformation Retrieval (TISMIR) , vol. 4, no. ARTICLE, pp.\n67–80, 2021.\n[29] C. Harte, “Towards automatic extraction of harmony\ninformation from music signals,” Ph.D. dissertation,\nQueen Mary University of London, 2010.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n604"
    },
    {
        "title": "Sequence-to-Sequence Network Training Methods for Automatic Guitar Transcription With Tokenized Outputs.",
        "author": [
            "Sehun Kim",
            "Kazuya Takeda",
            "Tomoki Toda"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265341",
        "url": "https://doi.org/10.5281/zenodo.10265341",
        "ee": "https://zenodo.org/records/10265341/files/000062.pdf",
        "abstract": "We propose multiple methods for effectively training a sequence-to-sequence automatic guitar transcription model which uses tokenized music representation as an output. Our proposed method mainly consists of 1) a hybrid CTC-Attention model for sequence-to-sequence automatic guitar transcription that uses tokenized music representation, and 2) two data augmentation methods for training the model. Our proposed model is a generic encoder-decoder Transformer model but adopts multi-task learning with CTC from the encoder to speed up learning alignments between the output tokens and acoustic features. Our proposed data augmentation methods scale up the amount of training data by 1) creating bar overlap when splitting an excerpt to be used for network input, and 2) by utilizing MIDI-only data to synthetically create audio-MIDI pair data. We confirmed that 1) the proposed data augmentation methods were highly effective for training generic Transformer models that generate tokenized outputs, 2) our proposed hybrid CTC-Attention model outperforms conventional methods that transcribe guitar performance with tokens, and 3) the addition of multi-task learning with CTC in our proposed model is especially effective when there is an insufficient amount of training data.",
        "zenodo_id": 10265341,
        "dblp_key": "conf/ismir/KimTT23",
        "keywords": [
            "sequence-to-sequence automatic guitar transcription",
            "tokenized music representation",
            "hybrid CTC-Attention model",
            "data augmentation methods",
            "multi-task learning",
            "CTC from the encoder",
            "bar overlap",
            "synthetic audio-MIDI pair data",
            "training generic Transformer models",
            "conventional methods"
        ],
        "content": "SEQUENCE-TO-SEQUENCE NETWORK TRAINING METHODS FOR\nAUTOMATIC GUITAR TRANSCRIPTION WITH TOKENIZED OUTPUTS\nSehun Kim\nNagoya University\nkim.sehun@g.sp.m.is.nagoya-u.ac.jpKazuya Takeda\nNagoya University\nkazuya.takeda@nagoya-u.jpTomoki Toda\nNagoya University\ntomoki@icts.nagoya-u.ac.jp\nABSTRACT\nWe propose multiple methods for effectively training\na sequence-to-sequence automatic guitar transcription\nmodel that uses tokenized music representation as an out-\nput. Our proposed method mainly consists of 1) a hybrid\nCTC-Attention model for sequence-to-sequence automatic\nguitar transcription that uses tokenized music representa-\ntion, and 2) two data augmentation methods for training the\nmodel. Our proposed model is a generic encoder-decoder\nTransformer model but adopts multi-task learning with\nCTC from the encoder to speed up learning alignments be-\ntween the output tokens and acoustic features. Our pro-\nposed data augmentation methods scale up the amount of\ntraining data by 1) creating bar overlap when splitting an\nexcerpt to be used for network input, and 2) by utilizing\nMIDI-only data to synthetically create audio-MIDI pair\ndata. We conﬁrmed that 1) the proposed data augmentation\nmethods were highly effective for training generic Trans-\nformer models that generate tokenized outputs, 2) our pro-\nposed hybrid CTC-Attention model outperforms conven-\ntional methods that transcribe guitar performance with to-\nkens, and 3) the addition of multi-task learning with CTC\nin our proposed model is especially effective when there is\nan insufﬁcient amount of training data.\n1. INTRODUCTION\nAutomatic guitar transcription is a challenging task that\nhas gained signiﬁcant attention in the ﬁeld of music infor-\nmation retrieval due to its potential applications in music\nanalysis, performance evaluation, and transcription of mu-\nsic compositions. Despite recent advancements in the ﬁeld,\nthere are still several challenges that need to be addressed.\nOne of the major challenges in automatic guitar transcrip-\ntion is the difﬁculty in extracting relevant features from the\naudio signal. The variations in timbre, pitch, and playing\nstyle make it difﬁcult to distinguish individual notes accu-\nrately [1, 2].\nMultiple methods exist for representing musical nota-\ntion suitable for employment in a DNN framework. Two\n© S. Kim, K. Takeda, and T. Toda. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: S. Kim, K. Takeda, and T. Toda, “Sequence-to-Sequence\nNetwork Training Methods for Automatic Guitar Transcription with To-\nkenized Outputs”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.\nFigure 1 . Examples of a pianoroll (upper) and tokenized\nmusic representation (lower).\nof the popular methods are pianoroll and tokenized musical\nrepresentation. Figure 1 shows a visualization of the differ-\nences between pianoroll and tokenized music representa-\ntions. Note that each can be converted from one to another.\nPianoroll is a visual representation of music that uses a\ngrid-like structure to display the timing, pitch, and dura-\ntion of notes in a piece. Tokenized music representation is\na type of symbolic music representation that breaks down\na music signal into small, discrete tokens, which can be\nanalyzed and processed as a sequence of tokens to extract\nrelevant musical features such as pitch, duration, and tim-\ning. Recent studies have shown that using tokenized music\nrepresentation over pianoroll can better learn the tempo-\nral dependency between different musical events [3]. Us-\ning tokenized music representation along with sequence-\nto-sequence models is particularly effective and have the\npotential to improve the performance of automatic music\ntranscription model [1,4]. However, these models also face\nseveral challenges such as the lack of training data [1].\nThe ﬁeld of automatic speech recognition (ASR) has\nprovided inspiration for improving automatic music tran-\nscription models, as both face similar challenges, includ-\ning the need to extract relevant features and handle com-\nplex temporal and frequency relationships [5]. In the ﬁeld\nof ASR, various techniques and models, such as connec-\ntionist temporal classiﬁcation (CTC) [6], Transformer [7]\nmodels, data augmentation, transfer learning, and multi-\ntask learning, have shown promising results [8, 9] and can\npotentially aid the performance of an automatic guitar tran-\nscription system.\nCTC and attention are two popular techniques used in\nsequence-to-sequence models for various tasks. CTC is524an efﬁcient method for training models with an unknown\nalignment between input and output sequences. It can han-\ndle variable-length input and output sequences. However,\nit does not explicitly model dependencies between input\nand output sequences [10]. Attention, on the other hand,\nallows the model to selectively focus on different parts of\nthe input sequence, improving the accuracy of the model\non tasks that require complex dependencies. However, at-\ntention mechanism is too ﬂexible in the sense that it allows\nextremely non-sequential alignments, making it relatively\ndifﬁcult to train [8].\nIn an attempt to improve the performance of an ASR\nsystem, researchers have also explored hybrid models that\ncombine CTC and attention mechanisms [8]. The authors\nreported that the addition of CTC solves the misalignment\nissues and improves robustness and achieves fast conver-\ngence.\nAlthough models such as Conformer-Transformer have\nbeen successful in ASR tasks [11], these were not used\nin guitar transcription mainly due to a lack of training data\navailable. To resolve this issue, we propose data augmenta-\ntion techniques and a hybrid CTC-Attention model suited\nfor a guitar transcription system1that utilizes tokenized\nmusic representation and show the effectiveness of the pro-\nposed methods. Our contributions are summarized as fol-\nlows.\n• We propose two data augmentation methods for\ntraining a sequence-to-sequence model that utilizes\ntokenized music representation.\n• We propose a hybrid CTC-Attention model for auto-\nmatic guitar transcription.\n• We conduct experimental evaluations to conﬁrm the\neffectiveness of our proposed methods and prove\nthat both the data augmentation techniques and the\nproposed model enhance guitar transcription perfor-\nmance.\n2. RELATED WORK\n2.1 Automatic guitar transcription\nThere have been many successful automatic guitar tran-\nscription systems [1, 12–15]. Some of them are based\non audio signal processing to estimate the tablature score\nfrom a guitar sound signal [12]. Also, approaches that em-\nploy probabilistic models have been proposed in some au-\ntomatic guitar transcription tasks. In [14, 15], a two-step\nmethod was employed, where the ﬁrst step involves de-\ntermining the pitch of each played note, and the second\nstep involves computing the optimal ﬁnger positioning by\ncombining the estimated pitch with physical limitations of\nfeasible ﬁngerings. Since this approach processes informa-\ntion in a sequential manner, information cannot ﬂow from\ndownstream components to upstream ones, making it dif-\nﬁcult to be jointly optimized [16].\n1Source code available :\nhttps://github.com/KimSehun725/seq2seqGuitarTranscriptionMost of the recent state-of-the-art systems were mainly\nbased on end-to-end deep neural network (DNN) models\nsince end-to-end DNN models have the advantage of the\nability to jointly optimize the whole model, showing better\nresults compared to multi-step approaches [13]. Wiggins\net al. proposed a convolutional neural network (CNN)-\nbased model architecture [13] that estimates the frame-\nwise ﬁngering position of a guitar performance. In our\nprevious work [17], a self-attention mechanism was in-\ntroduced along with CNN to better capture long-term re-\nlations and estimate the ﬁngering position in both frame-\nlevel and note-level. We proved the effectiveness of the\nself-attention mechanism used in the guitar transcription\nmodel. However, since the proposed systems in [13] and\n[17] do not detect onset, the output can not be interpreted\ninto reproducible forms such as music score or MIDI.\n2.2 Automatic music transcription using tokens\nRecently, the use of generic encoder-decoder Transformer\narchitecture has shown its potential in automatic music\ntranscription tasks. Howthorne et al. proposed a generic\nTransformer architecture for an automatic piano transcrip-\ntion [4]. The proposed method takes mel-spectrogram of\naudio and autoregressively generates a token sequence.\nThe tokenization method used in this work is similar to\nhow MIDI ﬁle stores its note sequences. The vocabulary\nconsists ofnote ,velocity , andtime tokens, with the\naddition of an end-of-sentence ( EOS) token for ending the\nsequence. In this tokenization method, the timing of each\nnote is represented with absolute time location within the\nsegment, quantized into 10 ms bins. This kind of tokeniza-\ntion method which represents the time of a note in location\nas opposed to the time shift from the previous note was\nreported to work better [18].\nChen et al. proposed a multi-objective generic Trans-\nformer model that not only predicts token sequences but\nalso frame-level onsets, offsets, and pitch activation [1].\nIn the original paper of [1], the authors report that al-\nthough the proposed model is a generic Transformer model\nthat generates a sequence of tokens, introducing multi-task\nlearning with frame-level labels, i.e., the pianoroll repre-\nsentation, lowers the performance of the token-wise pre-\ndiction from the decoder, but improves frame-level estima-\ntion performance compared to frame-level guitar transcrip-\ntion model proposed in [19]. The authors also mentioned\nthat the lower performance of the model without multi-task\nlearning (that only predicts token sequence) could be at-\ntributed to the insufﬁcient size of the training set to learn a\ndependable language model for the decoder.\n3. PROPOSED METHOD\n3.1 Hybrid CTC-Attention model for tokenized guitar\ntranscription\n3.1.1 Tokenization\nAs for the tokenization method, we use a slightly modiﬁed\nversion of revamped MIDI-derived events (REMI) [20].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n525Figure 2 . Overview of our proposed model architecture.\nSince our main interest is to accurately transcribe a per-\nformance with pitch and timing information, we excluded\nvelocity tokens and chord tokens from the original\nREMI. Excluding these tokens makes the total sequence\nshorter, making it easier to pass the restriction of CTC (to-\nken sequence length must be shorter than input sequence\nlength), and easier to train. The tokenization method we\nused in our proposed approach includes the following vo-\ncabulary:\nBlank [1 category] Used to represent the blank token\nwhen using CTC. This token gets dropped when de-\ncoding the ﬁnal prediction when applying CTC.\nPosition [16 categories] Indicates the location in a bar\nquantized into 16th note. This token is placed to\nindicate the start position of a note.\nPitch [45 categories] Each class represents the pitch rang-\ning fromE1toC5. This token is placed after the\nPosition token to indicate the pitch of a note.\nDuration [16 categories] Represents the duration ranging\nfrom the 16th note to a bar in 16th note increments.\nThis token is placed after the Pitch token to indi-\ncate the duration of a note.\nBar[1 category] Token used to denote the start of a bar.\nSOS, EOS [2 categories] Used to represent the start and\nend of a token sequence. These tokens are used\nfor training and inferencing with the Transformer\ndecoder.\n3.1.2 Encoder\nFigure 2 shows the overview of the proposed model ar-\nchitecture. We will refer to the left side of the ﬁgure as\nencoder and the right side as decoder from here on out.The structure of the encoder of our proposed model is\nlargely inspired by our previous automatic guitar transcrip-\ntion model proposed in [17], with some modiﬁcations for\ngenerating a token sequence and conditioning with BPM\ninformation. The encoder structure can be divided into\nthree main parts: a convolution stack, a Conformer en-\ncoder, and an output layer for generating a token sequence\nto which CTC can be applied later.\nThe convolution stack has 2D convolution, max pool-\ning, dropout layers, and a linear layer. Input features\ngo through two convolution blocks with 2D convolution,\nbatch normalization, and an activation function. Latent\nfeatures are then subsampled by max pooling and re-\nﬁned by another convolution block and max pooling layer.\nLastly, a linear layer is added to reduce dimension. Three\ndropout layers prevent overﬁtting after max pooling and\nthe ﬁnal linear layer.\nThe Conformer encoder closely follows the Conformer\nblock architecture proposed in [21]. The Conformer en-\ncoder mainly consists of self-attention modules, convolu-\ntion modules, and feed-forward modules. For the input\nto the Conformer encoder, ﬁrst, we concatenate the output\nfrom the convolution stack and the given BPM informa-\ntion of the input segment. Then, the concatenated feature\ngoes through a linear transformation layer, which is omit-\nted from Figure 2 for simplicity. We concatenate BPM in-\nformation to the output of the convolution stack because\nthe problem formulation of our method is estimating a se-\nquence of tokens based on both acoustic features and BPM\ninformation.\nFinally, the output layer is a simple linear transforma-\ntion layer with softmax function at the end for generat-\ning CTC token outputs. Unlike the model that predicts\nframe-level activation probability (pianoroll) from the en-\ncoder [1], the encoder of our proposed model generates\nthe probability of token sequence, which we can directly\ncalculate the loss between the output and the ground truth\ntoken sequence by calculating CTC loss [6].\nDuring inference, we ﬁrst apply argmax to the en-\ncoder outputs, then repeating tokens get merged. Then,\ntheblank token gets removed to obtain the ﬁnal output.\n3.1.3 Decoder\nThe structure of the decoder in our proposed model is\nroughly the same as the ones used in ASR tasks or other\nsymbolic music transcription tasks such as in [1, 4]. The\ndecoder consists of multiple Transformer decoder blocks\nstacked in serial. The Transformer decoder block consists\nof a masked self-attention module, a cross-attention mod-\nule, and a feed-forward module.\nThe decoder is trained and validated with a teacher forc-\ning scheme where the token is predicted one step ahead\nwithout self-attention looking ahead by masking the self-\nattention with a diagonal mask in a non-autoregressive\nmanner. During inference, we only give a start-of-sentence\n(SOS) token at ﬁrst, and autoregressively generate the fol-\nlowing tokens by selecting the most probable tokens at\neach timestep. The generation stops when the decoder gen-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n526erates an end-of-sentence ( EOS) token.\n3.1.4 Multi-task learning\nOur proposed method is a multi-objective model with both\nthe CTC output from the encoder and the output from the\ndecoder. Therefore, we deﬁne a custom loss function for\nbackpropagation.\nFirst, we deﬁne the CTC loss LCTC as\nLCTC=−/summationdisplay\ny∈Blogp(y|x), (1)\nwhereBis the set of all possible output sequences includ-\ning blank symbols, xis the input sequence, and p(y|x)is\nthe probability of generating the output sequence yfrom\nthe encoder given the input sequence x. The CTC loss is\ncalculated by summing the negative log probabilities of all\npossible output sequences ythat can be generated from\nthe ground truth sequence. The loss encourages the model\nto learn to generate the correct output sequence while ac-\ncounting for possible alignment errors between the input\nand output sequences.\nNext, we deﬁne the cross-entropy loss as\nLCE=−1\nNN/summationdisplay\ni=1C/summationdisplay\nj=1yijlog(ˆyij), (2)\nwhereNis the number of samples, Cis the number of\nclasses,yijis a binary indicator of whether sample ibe-\nlongs to class j, andˆyijis the predicted probability from\nthe decoder of sample ibelonging to class j.\nThe total loss function of our system Ltotal can be ex-\npressed as\nLtotal=αLCTC+LCE, (3)\nwhereαis the hyperparameter for controlling the weight\nofLCTC.\n3.2 Data augmentation\nWhen using a tokenization method whose time resolution\nis in units of musical lengths, training a model to gener-\nate a sequence of tokens requires a large amount of data to\nproperly model the language structure of the tokenization\nmethod and the concept of musical length. However, un-\nlike musical instruments such as the piano, which have a\nsigniﬁcant amount of publicly available training data, the\nguitar lacks sufﬁcient data for training. The goal of the pro-\nposed data augmentation methods is to scale up the amount\nof data used in the training process.\n3.2.1 Bar overlap\nIn the ﬁeld of automatic music transcription, splitting a\nmusical excerpt into multiple segments is common, espe-\ncially with the models that use the attention mechanism\nsince the attention mechanism has a space complexity of\nO(n2)with respect to sequence length n. There have been\nmany attempts to train a network by cutting musical pieces\ninto exact lengths in seconds. However, there have been\nonly a few attempts to handle music pieces by cutting theminto the same musical length, e.g., 4 bars [17]. When cut-\nting a musical piece into segments whose length is in units\nof bars, the most naive way of cutting it would be to cut\nwithout overlap so that the timing of the start of a segment\nis the end of the previous one. This results in obtaining\nlexcerpt/lsegment segments, assuming that lexcerpt is devidable\nbylsegment , wherelexcerpt denotes the bar length of a musi-\ncal excerpt, and lsegment denotes the bar length of a segment.\nThis method was done in our previous work [17].\nWe propose a method that creates more segments when\ncutting musical excerpts, by overlapping segments, i.e.\nsliding a window with a hop length shorter than the win-\ndow length. This results in obtaining lexcerpt/loverlap−\n(lsegment−1)whereloverlap denotes the bar length of the\noverlap.\n3.2.2 Synthetic audio-MIDI pair\nWith the goal of training a model to properly learn how\nto generate the token sequence by reliably training the\nlanguage model for the decoder with a large amount of\ndata, we propose a method that can synthetically create an\naudio-MIDI pair dataset from MIDI-only data. We gener-\nate synthetic audio data by utilizing an oscillator such as a\nsinusoidal oscillator or a square wave oscillator. This re-\nsults in obtaining an audio-MIDI pair with its audio being\nperfectly aligned with the matching MIDI, yet with unnat-\nural timbre. With the synthetically generated audio-MIDI\npair dataset, we pretrain the model before training with\nreal-world data. This results in the decoder being trained as\na reliable language model for tokenization method, and the\nmodel having preliminary knowledge regarding extracting\npitch and timing information.\nIt is possible to use the method to produce an endless\namount of data theoretically, by applying it to either a\npublicly available MIDI dataset or automatically generated\nMIDI from an automatic symbolic music generation model\nsuch as [20,22,23] since the method generates audio-MIDI\npair data solely from MIDI.\n3.3 Implementation details\nRegarding the network settings for the encoder and de-\ncoder of our proposed model, we set the number of atten-\ntion heads and the number of sequential Conformer blocks\nto 8 and 6 respectively. For the Transformer decoder, the\nnumber of attention heads and the number of sequential\nblocks are set to 4 and 4 respectively. The dimension of\nboth the Conformer encoder and the Transformer decoder\nis set to 128. We implement the Conformer encoder us-\ning the ESPNet2 framework [11]. We use the leaky ReLU\nactivation function [24] throughout the encoder, except for\nthe activation functions in the Conformer encoder and the\nﬁnal output layer.\nFor the implementation of tokenization, we use Midi-\nTok [25], but slightly modify the original implementation\nas mentioned in Section 3.1.1. As for the learning rate, we\nuse a cyclic learning rate scheme [26]. The base learning\nrate is set to 1e-5, the max learning rate is set to 0.001, the\nnumber of training iterations in the increasing half of a cy-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n527Encoder output Decoder output\nMethod F1 TER F1 TER\nNo data augmentation 0.363±0.159 0.469 ±0.185 0.526±0.154 0.713 ±0.219\nBar overlap (BO) 0.555±0.125 0.388 ±0.090 0.630±0.196 0.497 ±0.176\nPretrain (PT) 0.512±0.043 0.365 ±0.029 0.699±0.017 0.441 ±0.011\nProposed (BO+PT) 0.666±0.047 0.307±0.025 0.804±0.015 0.336±0.021\nTable 1 . Estimation metrics for evaluating the proposed data augmentation methods. For all metrics, we report the mean\nand standard deviation over the entire dataset. All the experiments were done with the proposed model.\nEncoder output Decoder output\nModel F1 TER F1 TER\nBaseline [1] 0.767±0.026 - 0.603±0.026 0.589 ±0.017\nAttention only - - 0.784±0.014 0.345 ±0.021\nProposed 0.666±0.047 0.307 ±0.025 0.804±0.015 0.336±0.021\nTable 2 . Estimation metrics for our proposed model, compared with a baseline model and a model without multi-task\nlearning with CTC. For all metrics, we report the mean and standard deviation over the entire dataset. All the experiments\nwere done by applying both of the proposed data augmentation methods. Note that the baseline model does not have TER\nfrom the encoder output because the encoder output is pianoroll-like frame-level annotation, not tokens.\nDecoder output\nModel F1 TER\nAttention only + BO 0.114±0.046 1.520 ±0.378\nProposed + BO 0.630±0.196 0.497±0.176\nTable 3 . Estimation metrics for simulating the situation\nwhere only a small amount of training data is available by\nnot pretraining with synthetic audio-MIDI pair data. We\nonly show the results of the output from the decoder for\nsimplicity.\ncle is set to 4 epochs, and the same for the decreasing half.\nWe made the peak learning rate decreases by a rate of 0.9\nafter each cycle. For the optimizer, we employ Rectiﬁed\nAdam (RAdam) [27]. Finally, we set the weight αwhich\nis used in the loss function, to 0.2.\n4. EXPERIMENTAL EV ALUATION\n4.1 Experimental conditions\nAs for the MIDI data used in the pretraining phase, we\nused data from Classical Guitar MIDI Archives [28]. We\nﬁltered out the data that did not have the properties that we\nwant such as having a time signature other than 4/4 beat\nor changing tempo over time. As a result, we obtained a\ntotal of 1232 minutes of data. We used square wave oscil-\nlator to make the synthetic audio, and split the dataset for\ntraining/validation/testing with a ratio of 0.9:0.05:0.05.\nFor the data used in the ﬁnetuning phase, we used the\nGuitarSet [29]. GuitarSet is a dataset for guitar transcrip-\ntion research containing 360 audio recordings, totaling ap-\nproximately 3 hours. Since the dataset was recorded by\nsix players, we left the recordings of one player for testing\nand used the rest of the data for training and validation. We\nrotated the test player to evaluate the methods with a six-\nfold cross-validation method. For both the pretraining dataand ﬁnetuning data, we cut the tracks into segments with 4\nbars, with 1 bar hop length.\nRegarding the input of the network, ﬁrst, we resampled\nthe audio to 22050 Hz, then we converted the audio to a\nconstant-Q transform (CQT) [30] with a hop length of 256\npoints, 24 bins per octave spanning over 8 octaves, result-\ning in a total of 192 frequency bins.\nWe evaluated the proposed methods in three points: 1)\nthe effectiveness of data augmentation methods, 2) the\nperformance of the model, 3) the effectiveness of intro-\nducing CTC in the proposed model when there is only a\nsmall amount of training data available. For the experi-\nment measuring the effectiveness of the data augmentation\ntechniques, we compared the metrics of 1) using GuitarSet\nonly and not using bar overlap, 2) only applying bar over-\nlap, 3) pretraining the model with synthetic audio-MIDI\npair dataset without bar overlapping, and 4) using both bar\noverlapping and pretraining.\nFor the model comparison, we compared our proposed\nmodel with 1) the model proposed in [1] which refer to as\nbaseline hereafter, 2) our proposed model without the use\nof CTC (Ltotal=LCE) which we refer to as attention only\nmodel from here on out, and 3) our proposed model.\nRegarding the network settings of the baseline model\n[1], we followed the settings described in the original pa-\nper, except we input a 4-bar-long acoustic feature to the\nnetwork and use the same tokenization method as our pro-\nposed method.\n4.2 Evaluation metrics\nSince the output of our proposed model is a sequence of to-\nkens that can be converted into pianoroll, we used different\nevaluation metrics for pianoroll domain and token domain.\nIn the pianoroll domain, we used precision, recall, and\nF1 score. If the output is a sequence of tokens, we decode\nthe token sequence to pianoroll to calculate precision, re-\ncall, and F1 score. In the token domain, we used token er-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n528Figure 3 . Comparison of the speed in learning align-\nments between acoustic features (horizontal axis) and to-\nkens (vertical axis). The training was done using only the\nGuitarSet.\nror rate (TER), which is calculated similarly to word error\nrate (WER) which is a widely used metric in the research\nﬁeld of ASR and natural language processing (NLP). The\nonly difference is that every element is a token index in-\nstead of a word in WER. The TER can be calculated as\nTER =S+D+I\nS+D+C, (4)\nwhereSis the number of substitutions, Dis the number\nof deletions, Iis the number of insertions, and Cis the\nnumber of correct tokens.\nThe reason why we introduce TER as an evaluation\nmetric is that we want to compensate for the mismatch\nbetween the metrics used in pianoroll domain and human\nperception. For instance, if we only evaluate the system\nin the pianoroll domain, the outputs with the notes that are\nshifted for a consistent amount of time will have very low\nscores. However in the same case, if we use TER as an\nevaluation metric, only the position tokens will lower\nthe metric, not penalizing for the shifted notes as much.\n4.3 Results\nThe result of the experiment comparing the effectiveness of\nthe data augmentation methods is shown in Table 1. The\nresult shows that both data augmentation methods are ef-\nfective in raising the estimation performance of the pro-\nposed model. By comparing the effectiveness of bar over-\nlapping and pretraining, pretraining shows slightly better\nresults in the output from the decoder but worse in the out-\nput of the encoder.\nThe result of the experiment comparing the perfor-\nmance of the models mentioned in Section 4.1 is shown in\nTable 2. The result shows that our proposed model outper-\nformed the baseline model and the proposed model without\nutilizing CTC (attention only) in both F1 score and TER.\nUpon comparing the estimation results of the attention-\nonly model with the proposed model, it is evident that the\nlatter produced superior results, thereby indicating that the\nFigure 4 . A sample of transcription result from our pro-\nposed model. TP, FP, and FN denote true positive, false\npositive, and false negative respectively.\ninclusion of CTC considerably improves transcription per-\nformance.\nThe result of the experiment simulating a situation with\na small amount of training data available is shown in Ta-\nble 3. We simulated this situation by only using the Gui-\ntarSet for training. The result shows that the model with\nattention only performed very poorly when there is only\na small amount of data. However, our proposed model\nwhich utilizes multitask learning with CTC outputs from\nthe encoder performs better compared to attention only\nmodel. This indicates that employing multi-task learn-\ning with CTC is highly effective when there is an insuf-\nﬁcient amount of data. Figure 3 shows the attention align-\nments between acoustic features and tokens. We observed\nthat despite being trained for 64 epochs, the attention only\nmodel failed to acquire a reasonable alignment, whereas\nthe suggested model achieved to acquire the desired align-\nments early on in the training process. The performance\ndifference between the attention only model and the pro-\nposed model is likely to be attributed to the difference in\nthe difﬁculty of learning the correct alignments.\nWhile we did not include the outcomes of using solely\nsynthetic audio-MIDI pair data for both training and test-\ning in any of the tables, it is worth stating that during the\npretraining phase of the experiment utilizing the proposed\nmodel and data augmentation methods, the output of the\ndecoder with the test data yielded an F1 score of 0.959 and\nTER of 0.029. This indicates that the amount of data used\nin the pretraining was sufﬁcient enough to train a reliable\nlanguage model for the decoder.\n5. CONCLUSION\nIn this paper, we proposed two data augmentation methods\nfor training sequence-to-sequence networks that used tok-\nenized music representation as output, and a hybrid CTC-\nAttention model for automatic guitar transcription. We\nconﬁrmed that 1) both of the data augmentation methods\nare highly effective in training the sequence-to-sequence\nmodels when there is an insufﬁcient amount of data, 2) our\nproposed hybrid CTC-Attention model outperforms con-\nventional methods that transcribe guitar performance with\ntokens, and 3) the addition of multi-task learning with CTC\nin our proposed model is especially effective when there is\nan insufﬁcient amount of training data.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n5296. ACKNOWLEDGMENT\nThis work was partly supported by JST CREST Grant\nNumber JPMJCR19A3 and JSPS KAKENHI Grant Num-\nber 21H04892, Japan.\n7. REFERENCES\n[1] Y .-H. Chen, W.-Y . Hsiao, T.-K. Hsieh, J.-S. R. Jang,\nand Y .-H. Yang, “Towards automatic transcription of\npolyphonic electric guitar music: A new dataset and\na multi-loss transformer model,” in 2022 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2022, pp. 786–790.\n[2] X. Fiss and A. Kwasinski, “Automatic real-time elec-\ntric guitar audio transcription,” in 2011 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2011, pp. 373–376.\n[3] C. Zhang, Y . Ren, K. Zhang, and S. Yan, “Sd-\nmuse: Stochastic differential music editing and gen-\neration via hybrid representation,” arXiv preprint\narXiv:2211.00222 , 2022.\n[4] C. Hawthorne, I. Simon, R. Swavely, E. Manilow, and\nJ. Engel, “Sequence-to-sequence piano transcription\nwith transformers,” in International Society for Music\nInformation Retrieval Conference (ISMIR) , 2021.\n[5] S. Sigtia, E. Benetos, and S. Dixon, “An end-to-end\nneural network for polyphonic piano music transcrip-\ntion,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , vol. 24, no. 5, pp. 927–939,\n2016.\n[6] A. Graves, S. Fernández, F. Gomez, and J. Schmidhu-\nber, “Connectionist temporal classiﬁcation: Labelling\nunsegmented sequence data with recurrent neural ’net-\nworks,” in Proceedings of the 23rd International Con-\nference on Machine Learning (ICML) , 01 2006, pp.\n369–376.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in Neural Infor-\nmation Processing Systems (NIPS) , vol. 30, pp. 5998–\n6008, 2017.\n[8] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and\nT. Hayashi, “Hybrid ctc/attention architecture for end-\nto-end speech recognition,” IEEE Journal of Selected\nTopics in Signal Processing , vol. 11, no. 8, pp. 1240–\n1253, 2017.\n[9] H. Kheddar, Y . Himeur, S. Al-Maadeed, A. Amira,\nand F. Bensaali, “Deep transfer learning for automatic\nspeech recognition: Towards better generalization,”\narXiv preprint arXiv:2304.14535 , 2023.\n[10] A. Graves, “Sequence transduction with recurrent neu-\nral networks,” arXiv preprint arXiv:1211.3711 , 2012.[11] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y . Higuchi,\nH. Inaguma, N. Kamo, C. Li, D. Garcia-Romero,\nJ. Shi, J. Shi, S. Watanabe, K. Wei, W. Zhang, and\nY . Zhang, “Recent developments on espnet toolkit\nboosted by conformer,” in 2021 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2021, pp. 5874–5878.\n[12] C. Kehling, J. Abeßer, C. Dittmar, and G. Schuller,\n“Automatic tablature transcription of electric guitar\nrecordings by estimation of score and instrument-\nrelated parameters,” in Proc. 17th International Con-\nference on Digital Audio Effects (DAFx-14) , 2014.\n[13] A. Wiggins and Y . E. Kim, “Guitar tablature estimation\nwith a convolutional neural network,” in Proc. 20th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2019, pp. 284–291.\n[14] K. Yazawa, K. Itoyama, and H. G. Okuno, “Automatic\ntranscription of guitar tablature from audio signals in\naccordance with player’s proﬁciency,” in 2014 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , 2014, pp. 3122–3126.\n[15] G. Hori, H. Kameoka, and S. Sagayama, “Input-output\nHMM applied to automatic arrangement for guitars,”\nJournal of Information Processing , vol. 21, no. 2, pp.\n264–271, 2013.\n[16] G. W. Lee and H. K. Kim, “Two-step joint optimization\nwith auxiliary loss function for noise-robust speech\nrecognition,” Sensors , vol. 22, no. 14, 2022.\n[17] S. Kim, T. Hayashi, and T. Toda, “Note-level auto-\nmatic guitar transcription using attention mechanism,”\nin2022 30th European Signal Processing Conference\n(EUSIPCO) , 2022, pp. 229–233.\n[18] S. Oore, I. Simon, S. Dieleman, D. Eck, and K. Si-\nmonyan, “This time with feeling: Learning expressive\nmusical performance,” Neural Computing and Appli-\ncations , vol. 32, pp. 955–967, 2020.\n[19] C. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Si-\nmon, C. Raffel, J. Engel, S. Oore, and D. Eck, “Onsets\nand Frames: Dual-Objective Piano Transcription,” in\nProceedings of the 19th International Society for Mu-\nsic Information Retrieval Conference (ISMIR) . Paris,\nFrance: ISMIR, Sep. 2018, pp. 50–57.\n[20] Y .-S. Huang and Y .-H. Yang, “Pop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions,” in Proceedings of the 28th ACM\nInternational Conference on Multimedia , 2020, pp.\n1180–1188.\n[21] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang,\nJ. Yu, W. Han, S. Wang, Z. Zhang, Y . Wu, and R. Pang,\n“Conformer: Convolution-augmented Transformer for\nspeech recognition,” in Proc. Interspeech , 2020, pp.\n5036–5040.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n530[22] G. Mittal, J. Engel, C. Hawthorne, and I. Simon, “Sym-\nbolic music generation with diffusion models,” in Pro-\nceedings of the 22nd International Society for Music\nInformation Retrieval Conference (ISMIR) , 2021.\n[23] A. Muhamed, L. Li, X. Shi, S. Yaddanapudi, W. Chi,\nD. Jackson, R. Suresh, Z. C. Lipton, and A. J. Smola,\n“Symbolic music generation with transformer-gans,”\nin35th AAAI Conference on Artiﬁcial Intelligence,\nAAAI , 2021.\n[24] B. Xu, N. Wang, T. Chen, and M. Li, “Empirical evalu-\nation of rectiﬁed activations in convolutional network,”\narXiv preprint arXiv:1505.00853 , 2015.\n[25] N. Fradet, J.-P. Briot, F. Chhel, A. El Fal-\nlah Seghrouchni, and N. Gutowski, “MidiTok: A\npython package for MIDI ﬁle tokenization,” in Ex-\ntended Abstracts for the Late-Breaking Demo Session\nof the 22nd International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , 2021.\n[26] L. N. Smith, “Cyclical learning rates for training neural\nnetworks,” in 2017 IEEE Winter Conference on Appli-\ncations of Computer Vision (WACV) , 2017, pp. 464–\n472.\n[27] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and\nJ. Han, “On the variance of the adaptive learning rate\nand beyond,” in International Conference on Learning\nRepresentations (ICLR) , 2020.\n[28] “Classical guitar midi archives,” accessed on\nApril 1, 2023. [Online]. Available: https:\n//www.classicalguitarmidi.com/\n[29] Q. Xi, R. Bittner, J. Pauwels, X. Ye, and J. P. Bello,\n“Guitarset: A dataset for guitar transcription,” in Proc.\n19th International Society for Music Information Re-\ntrieval Conference (ISMIR) , Paris, France, Sep. 2018,\npp. 453–460.\n[30] J. Youngberg and S. Boll, “Constant-Q signal analysis\nand synthesis,” in 1978 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\nvol. 3, 1978, pp. 375–378.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n531"
    },
    {
        "title": "A Computational Evaluation Framework for Singable Lyric Translation.",
        "author": [
            "Haven Kim",
            "Kento Watanabe",
            "Masataka Goto",
            "Juhan Nam"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265405",
        "url": "https://doi.org/10.5281/zenodo.10265405",
        "ee": "https://zenodo.org/records/10265405/files/000092.pdf",
        "abstract": "Lyric translation plays a pivotal role in amplifying the global resonance of music, bridging cultural divides, and fostering universal connections. Translating lyrics, unlike conventional translation tasks, requires a delicate balance between singability and semantics. In this paper, we present a computational framework for the quantitative evaluation of singable lyric translation, which seamlessly integrates musical, linguistic, and cultural dimensions of lyrics. Our comprehensive framework consists of four metrics that measure syllable count distance, phoneme repetition similarity, musical structure distance, and semantic similarity. To substantiate the efficacy of our framework, we collected a singable lyrics dataset, which precisely aligns English, Japanese, and Korean lyrics on a line-by-line and section-by-section basis, and conducted a comparative analysis between singable and non-singable lyrics. Our multidisciplinary approach provides insights into the key components that underlie the art of lyric translation and establishes a solid groundwork for the future of computational lyric translation assessment.",
        "zenodo_id": 10265405,
        "dblp_key": "conf/ismir/KimWGN23",
        "keywords": [
            "Lyric translation",
            "global resonance",
            "cultural divides",
            "universal connections",
            "computational framework",
            "quantitative evaluation",
            "singable lyric translation",
            "syllable count distance",
            "phoneme repetition similarity",
            "musical structure distance"
        ],
        "content": "A COMPUTATIONAL EV ALUATION FRAMEWORK\nFOR SINGABLE LYRIC TRANSLATION\nHaven Kim1Kento Watanabe2Masataka Goto2Juhan Nam1\n1Graduate School of Culture Technology, KAIST, South Korea\n2National Institute of Advanced Industrial Science and Technology (AIST), Japan\nkhaven@kaist.ac.kr, kento.watanabe@aist.go.jp, m.goto@aist.go.jp, juhan.nam@kaist.ac.kr\nABSTRACT\nLyric translation plays a pivotal role in amplifying the\nglobal resonance of music, bridging cultural divides, and\nfostering universal connections. Translating lyrics, unlike\nconventional translation tasks, requires a delicate balance\nbetween singability and semantics. In this paper, we present\na computational framework for the quantitative evaluation\nof singable lyric translation, which seamlessly integrates\nmusical, linguistic, and cultural dimensions of lyrics. Our\ncomprehensive framework consists of four metrics that mea-\nsure syllable count distance, phoneme repetition similarity,\nmusical structure distance, and semantic similarity. To\nsubstantiate the efﬁcacy of our framework, we collected\na singable lyrics dataset, which precisely aligns English,\nJapanese, and Korean lyrics on a line-by-line and section-\nby-section basis, and conducted a comparative analysis\nbetween singable and non-singable lyrics. Our multidisci-\nplinary approach provides insights into the key components\nthat underlie the art of lyric translation and establishes a\nsolid groundwork for the future of computational lyric trans-\nlation assessment.\n1. INTRODUCTION\nTranslating lyrics is a prevailing method of enhancing the\nglobal appeal and allure of music across a multitude of\ngenres, such as theatre music, animation music, pop music,\netc [1]. Furthermore, recent advancements in media technol-\nogy have facilitated the exchange of intercultural products\nand globalized fandom culture, resulting in an increase in\nthe popularity of user-translated lyrics across diverse social\nmedia platforms [2].\nDespite its popularity, lyric translation is acknowledged\nas a challenging ﬁeld, requiring an interdisciplinary ap-\nproach [2]. As early as 1915, it was suggested that an ideal\nlyric translator should possess expertise in both linguis-\ntics and music, highlighting the need for a comprehensive\nunderstanding of the principles and techniques used in trans-\nlation studies, coupled with a background in musicology [3].\n© H. Kim, K. Watanabe, M. Goto, and J. Nam. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: H. Kim, K. Watanabe, M. Goto, and J. Nam, “A\nComputational Evaluation Framework for Singable Lyric Translation”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.Moreover, it is crucial to understand the cultural context\nof each language, such as different strategies employed for\nforming rhymes [4]. Because of these challenges, the sys-\ntematic analysis and evaluation of lyric translation remain\nan under-researched topic of study. Thus far, only a few\nhave proposed guidelines for scoring the quality of trans-\nlated lyrics [4, 5]. While these principled approaches have\nproven successful, they lack automation. Consequently,\ndespite the growing interest in the development of neural\nlyric translation, its evaluation has predominantly relied\non human evaluation, making the evaluation process time-\nconsuming, unreliable, and subjective [6–8].\nOur study aims to computationally analyze and evaluate\nlyric translation based on a comprehensive understanding of\nlyrics that accounts for their musical, linguistic, and cultural\nelements. Unlike prior research that only proposed rhyme-\nscoring guidelines applicable to English [4], our framework\nis extendable to Japanese and Korean. Though our frame-\nwork may be limited in its application to speciﬁc languages,\nwe strive to provide valuable insights into establishing dis-\ntinct evaluation rules for phoneme repetition in diverse\nlanguages. Our comprehensive framework employs a multi-\nfaceted evaluation approach that examines lyric translation\nfrom four distinct perspectives: syllable counts, phoneme\nrepetition, musical structure, and semantics. In the remain-\nder of this paper, we explicate the rationale behind our\nselection of these perspectives by delving into the unique\ncharacteristics of lyric translation that differentiate it from\ngeneral language translation tasks. In addition, we intro-\nduce the singable lyrics dataset we collected, which features\nline-by-line and section-by-section alignment of English,\nJapanese, and Korean lyrics. Moving forward, we propose\nrobust evaluation metrics for lyric translation and analyze\nthe results of our experiments based on the perspectives\nmentioned above. Finally, we conclude our paper by reﬂect-\ning on the profound insights gleaned from our experiment\nand highlighting possible directions for future research.\n2. BACKGROUND\nPrevious research indicates that linguistic analysis meth-\nods designed for standard text may not achieve desired\noutcomes when used to examine lyrics [9]. Although auto-\nmated evaluation metrics, such as n-gram-based [10 –12] or\nneural approaches [13], have proven valuable and effective\nin assessing conventional machine translation tasks, they774fall short in evaluating lyric translation. This is due to the\nunique characteristics of lyrics that render the translation\nprocess subject to many constraints and less direct [14].\nOne of the most signiﬁcant constraints is the syllable\ncount. This is because the original and translated lyrics must\nmatch the same melody lines, while it is a common practice\nto tweak the melody to accommodate minor changes in syl-\nlable count [4, 15]. In fact, conveying the same message in\ndifferent languages requires vastly different syllable counts.\nFor example, “Happy New Year” in English consists of\n4 syllables, whereas 15 and 9 are required for Japanese\n(あけましておめでとうございます ) and Korean (ᄉ ᅢ\nᄒ ᅢᄇ ᅩ ᆨᄆ ᅡ ᆭᄋ ᅵᄇ ᅡ ᆮᄋ ᅳᄉ ᅦᄋ ᅭ ), respectively. For the numerical\ncomparison, we examined PAWS-X, a dataset that contains\n23,659 English sentences paired with human-translated sen-\ntences in various languages [16]. The average number of\nsyllables per sentence in the dataset is 50.89 for Japanese,\nwhereas 36.18 per English and 40.40 per Korean. With\nthese statistics, it can be deduced that Japanese necessi-\ntates approximately 41% more syllables than English and\n26% more syllables than Korean to express an equivalent\nmessage. This limitation forces translators to often mod-\nify the meaning of original lyrics by adding, omitting, or\neven tweaking the message. However, translated lyrics still\naim to capture the theme, mood, and spirit of the original\nlyrics [17]. Therefore, while original and translated lyrics\nneed not be semantically identical, they still need to be\nsemantically relevant [4, 18].\nIt is also crucial to preserve the frequency of phoneme\nrepetition (e.g., rhyme) in translated lyrics, particularly\nwhen the music demands it [17]. For instance, some\nsections, such as choruses, require a substantial degree\nof phoneme repetition, while others do not. Moreover,\ndue to the inherent connection between lyrics and music,\nlyrics must be arranged in a way that complements the mu-\nsic [19]. As a result, musically similar sections should main-\ntain resembling linguistic features, including the choice of\nphonemes and the frequency of phoneme repetition [20].\n3. DATASET\nAlthough some websites provide user-translated multilin-\ngual lyrics, we found that most of them lack singabil-\nity, as these translations were focused on delivering the\nmeaning of the original lyrics rather than making them\nperformable. While there are a few singable translations\navailable, they are often not aligned on a line-by-line nor\nsection-by-section basis due to the subjective nature of the\nlyric structure that there is no universal agreement on what\nto call a line and what to call a section. The absence of\nalignment makes it difﬁcult to compare the original lyrics\nwith their translated versions. To address this issue, we\ncollected a set of singable lyrics, sourced from either of-\nﬁcial lyrics of commercial songs or user-translated ones\nfound on YouTube, meticulously aligned on a line-by-line\nbasis in English, Japanese, and Korean. This approach en-\nsures that lyrics on the same line share the same melodies.\nMoreover, the dataset divides the lyrics into sections, allow-\ning for section-by-section analysis. Alongside the lyrics, itSection\n#Line\n#English (EN) Japanese (JP) Korean (KR)\n11 Twinkle, twinkle, little star きらきらひかる 반ᄍ ᅡ ᆨ반ᄍ ᅡ ᆨᄌ ᅡ ᆨᄋ ᅳ ᆫ별\n2 How I wonder what you are おそらのほしよ ᄋ ᅡᄅ ᅳ ᆷᄃ ᅡ ᆸᄀ ᅦᄇ ᅵᄎ ᅵᄂ ᅦ\n3 Up above the world so high まばたきしては ᄉ ᅥᄍ ᅩ ᆨᄒ ᅡᄂ ᅳ ᆯᄋ ᅦᄉ ᅥᄃ ᅩ\n4 Like a diamond in the sky みんなをみてる ᄃ ᅩ ᆼᄍ ᅩ ᆨᄒ ᅡᄂ ᅳ ᆯᄋ ᅦᄉ ᅥᄃ ᅩ\n5 Twinkle, twinkle, little star きらきらひかる 반ᄍ ᅡ ᆨ반ᄍ ᅡ ᆨᄌ ᅡ ᆨᄋ ᅳ ᆫ별\n6 How I wonder what you are おそらのほしよ ᄋ ᅡᄅ ᅳ ᆷᄃ ᅡ ᆸᄀ ᅦᄇ ᅵᄎ ᅵᄂ ᅦ\n27 Twinkle, twinkle, little star きらきらひかる 반ᄍ ᅡ ᆨ반ᄍ ᅡ ᆨᄌ ᅡ ᆨᄋ ᅳ ᆫ별\n8 How I wonder what you are おそらのほしよ ᄋ ᅡᄅ ᅳ ᆷᄃ ᅡ ᆸᄀ ᅦᄇ ᅵᄎ ᅵᄂ ᅦ\n9 When the blazing sun is gone みんなのうたが ᄉ ᅥᄍ ᅩ ᆨᄒ ᅡᄂ ᅳ ᆯᄋ ᅦᄉ ᅥᄃ ᅩ\n10 When he nothing shines upon とどくといいな ᄃ ᅩ ᆼᄍ ᅩ ᆨᄒ ᅡᄂ ᅳ ᆯᄋ ᅦᄉ ᅥᄃ ᅩ\n11 Then you show your little light きらきらひかる 반ᄍ ᅡ ᆨ반ᄍ ᅡ ᆨᄌ ᅡ ᆨᄋ ᅳ ᆫ별\n12 Twinkle, twinkle, all the night おそらのほしよ ᄋ ᅡᄅ ᅳ ᆷᄃ ᅡ ᆸᄀ ᅦᄇ ᅵᄎ ᅵᄂ ᅦ\nTable 1 . Sample data illustrating the original English lyrics\nof “Twinkle, Twinkle, Little Star” and their corresponding\nsingable translations in Japanese and Korean, aligned on a\nline-by-line and section-by-section basis.\nprovides essential metadata such as genre, artist, original\nlanguage, and the ofﬁcial status of lyrics. The dataset con-\nsists of 162 songs, each having lyrics in the three languages.\nIt covers a diverse range of genres, including 109 K-pop, 23\nanimation music (e.g., Disney), 13 J-pop, 10 theatre music,\nand more. Table 1 shows sample data.\n4. EV ALUATING SINGABILITY\nOur primary goal is to develop an evaluation framework\nthat automatically assesses the quality of translated lyrics.\nOne of the most important factors determining the quality is\nsingability , deﬁned as not only the ability of being sung, but\nalso the suitability (easiness) of being sung [18]. To ensure\nsuch singability, we aim to provide metrics from three dis-\ntinct perspectives by making sure that they i) maintain the\nsong’s melodic integrity, ii) preserve the degree of phoneme\nrepetition, and iii) consider the underlying musical struc-\nture.\nTo substantiate the reliability of our evaluation metrics,\nwe conducted a comparative analysis of singable lyrics ver-\nsus non-singable lyrics based on each proposed evaluation\nmetric. In all our comparative analyses, we utilized our\ndataset for singable lyrics, where ofﬁcial lyrics served as\nboth source and target lyrics, and unofﬁcial functioned as\nonly target lyrics. For non-singable lyrics, we obtained\npairs of original singable (source) and human-translated\nnon-singable (target) lyrics, aligned line-wise and section-\nwise, for 3,642 songs from https://lyricstranslate.com/.\nSource Target Singable Non-singable\nEnglishJapanese 0.17 (80 songs) 0.74 (1401 songs)\nKorean 0.11 (80 songs) 0.48 (620 songs)\nJapaneseEnglish 0.16 (162 songs) 0.39 (589 songs)\nKorean 0.11 (162 songs) 0.31 (73 songs)\nKoreanEnglish 0.09 (161 songs) 0.20 (702 songs)\nJapanese 0.12 (161 songs) 0.52 (257 songs)\nTable 2 . The average line syllable count distance ( Dissyl)\nbetween source and target languages for singable and non-\nsingable lyrics.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n775Section English Japanese (English translation) Korean (English translation) pho\nE(A1),\nJ(A1),\nK(A1)Do you wanna build a snowman?\nCome on, let’s go and play!\nI never see you anymore\nCome out the door\nIt’s like you’ve gone away雪だるま作ろう (Let’s build a snowman)\nドアを開けて (Please open the door)\n一緒に遊ぼう (Let’s play together)\nどうして (Why)\n出てこないの ? (don’t you come out?)ᄀ ᅡ ᇀᄋ ᅵ눈ᄉ ᅡᄅ ᅡ ᆷ만ᄃ ᅳ ᆯᄅ ᅢ? (Do you wanna build a snowman?)\nᄌ ᅦ발ᄌ ᅩ ᆷᄂ ᅡ와봐(Please come out)\n언ᄂ ᅵ를만ᄂ ᅡ ᆯᄉ ᅮ없ᄋ ᅥ(I can’t meet you)\nᄀ ᅡ ᇀᄋ ᅵ놀ᄌ ᅡ(Let’s play together)\nᄂ ᅡ혼ᄌ ᅡ심심ᄒ ᅢ(I’m lonely alone)0.85,\n0.73,\n0.77\nE(B1),\nJ(B1),\nK(B1)We used to be best buddies\nAnd now we’re not\nI wish you would tell me why!前は仲良く(We were close before)\nしてたのに (We used to be)\nなぜ会えないの (Why can’t we meet each other?)ᄀ ᅳ렇ᄀ ᅦᄎ ᅵᆫ했는ᄃ ᅦ(We were close before)\nᄋ ᅵ젠ᄋ ᅡᄂ ᅣ (and we’re not)\nᄀ ᅳᄋ ᅵᄋ ᅲ를ᄋ ᅡ ᆯᄀ ᅩᄑ ᅡ (I want to know the reason why)0.92,\n0.80,\n0.91\nE(A2),\nJ(A2),\nK(A2)Do you wanna build a snowman?\nOr ride our bike around the halls?\nI think some company is overdue\nI’ve started talking to the pictures on the walls!雪だるま作ろう (Let’s build a snowman)\n自転車に乗ろう (Let’s ride a bike)\nずっと一人でいると (When I’m alone all the time)\n壁の絵とおしゃべりしちゃう\n(I’m almost talking to the pictures on the walls)ᄀ ᅡ ᇀᄋ ᅵ눈ᄉ ᅡᄅ ᅡ ᆷ만ᄃ ᅳ ᆯᄅ ᅢ? (Do you wanna build a snowman?)\nᄋ ᅡᄂ ᅵ면ᄌ ᅡ전ᄀ ᅥ탈ᄅ ᅢ? (or do you wanna ride a bike?)\nᄋ ᅵᄌ ᅦ는ᄂ ᅡᄃ ᅩᄌ ᅵᄎ ᅧᄀ ᅡᄂ ᅡ봐(Seems I’m getting tired)\nᄇ ᅧ ᆨᄋ ᅦᄃ ᅡᄆ ᅡ ᆯ을ᄒ ᅡᄆ ᅧ놀ᄀ ᅩ있ᄌ ᅡ ᆭᄋ ᅡ\n(because I’ve started talking to the walls)0.79,\n0.73,\n0.82\nE(B2),\nJ(B2),\nK(B2)It gets a little lonely\nAll these empty rooms\nJust watching the hours tick byさびしい部屋で(In a lonely room)\n柱時計(the wall clock)\n見てたりするの (I look at or something)ᄉ ᅡ시 ᆯᄋ ᅳ ᆫᄌ ᅩ그 ᆷ외ᄅ ᅩᄋ ᅯ(In fact, I’m a little lonely)\n텅ᄇ ᅵᆫ방ᄋ ᅦ선(In empty rooms)\nᄉ ᅵᄀ ᅨᄉ ᅩᄅ ᅵ만ᄃ ᅳ ᆯᄅ ᅧ(All I can hear is the clock’s ticking)0.90,\n0.88,\n0.96\nTable 3 . Lyric excerpt from “Do You Want to Build a Snowman” from the animation “Frozen,” singable in all languages.\nSectionsA1andA2form a musically similar pair, while B1andB2are also musically similar to each other. Each section is\ndenoted as E(A1),...,E(B2)in English, J(A1),...,J(B2)in Japanese, and K(A1),...,K(B2)in Korean.\n4.1 Line Syllable Count Distance\nIt is crucial to preserve the syllable counts between the\noriginal and translated lyrics for each line as similar as\npossible in order to maintain the integrity of a song’s\nmelody [21]. Therefore, it is unsurprising that our eval-\nuation framework incorporates a metric to assess the dif-\nferences in syllable counts. Let the line syllable counts for\na pair of lyrics that consist of nlines,X={x1,...,xn}\nand˜X={˜x1,...,˜xn}be denoted as {syl(x1),...,syl(xn)}\nand{syl( ˜x1),...,syl( ˜xn)}where each element refers to\nthe syllable count of each line. For instance, if the ﬁrst\nline of the English lyrics Xis “Silent night holy night”\nand the corresponding line in the Korean lyrics ˜Xis\n“Goyohanbam-georukhanbam ( ᄀ ᅩᄋ ᅭ한ᄇ ᅡ ᆷᄀ ᅥᄅ ᅮ ᆨ한ᄇ ᅡ ᆷ)”, the\nvalue ofsyl(x1)is 6 andsyl( ˜x1)is 8. We deﬁne the line\nsyllable count distance between a pair of lyrics Xand\n˜X(Dissyl(X,˜X)) in order to evaluate the disparities in\nsyllable counts, as follows.\nDissyl(X,˜X) =1\n2n/summationtextn\ni=1(|syl(xi)−syl( ˜xi)|\nsyl(xi)+|syl(xi)−syl( ˜xi)|\nsyl( ˜xi))\n(1)\nWe compare the line syllable count distance of singable\nand non-singable lyrics. As shown in Table 2, non-singable\nlyrics display a considerably greater Dissyl(X,˜X)com-\npared to singable lyrics due to the varying syllable count\nrequirements across languages.\n4.2 Phoneme Repetition Similarity\nRhyme, deﬁned as the repetition of a vowel sound and any\nsubsequent sounds [22], has historically been a fundamen-\ntal element in the realm of poetry, including in Western\nlanguages like English. However, the concept of rhyme has\nnot been as prevalent in Japanese or Korean poetry [23].\nIn fact, traditional Korean poetry did not incorporate this\nconcept [24]. Despite the increasing tendency to adopt the\nconcept of rhyme in Japanese and Korean lyrics due to\nintercultural exchanges, we observed that lyrics in these\nlanguages often rely more on repeating grammatical ele-\nments. For example, in section A1of Table 3, the Japanese\npair “tsukurou (作ろう , Let’s build)” and “asobou ( 遊ぼう ,\nLet’s play)” generates a sense of rhyme because both endwith the same conjugation “ou” meaning “let’s”. Similarly,\nin Section A2, the Korean pair “mandeullae ( 만ᄃ ᅳ ᆯᄅ ᅢ, Do\nyou wanna build)” and “tallae ( 탈ᄅ ᅢ, Do you wanna ride)”\ncreates a sense of repetition because both end with “llae”\nmeaning “Do you wanna”. Another example is the repeti-\ntion of particles at the end of sentences, such as “yo ( よ)”\nand “no (の)” in Japanese and “yo ( ᄋ ᅭ)” and “da (ᄃ ᅡ)” in\nKorean, which convey cultural nuances related to formality.\nWe therefore propose that English, Japanese, and Korean\nshare common ground in adopting phoneme repetition for\npoetic expression. However, as such repetition is not neces-\nsarily called rhyme in Japanese and Korean, we will refrain\nfrom using the term “rhyme” and instead employ the term\n“phoneme repetition.”\nWe noticed that each section’s degree of phoneme repe-\ntition remains consistent across different languages when\nthe lyrics are singable. For example, in Table 3, the ﬁrst\nsection of the original English lyrics ( E(A1)) displays a\nstrong degree of phoneme repetition, with three rhyming\npairs: “come-come”, “play-away”, and “anymore-door” (In\nthis paper, we denote a section as an uppercase with a\nnumber and a line as a lower case with a number). Sim-\nilarly, both the Japanese and Korean translations ( J(A1),\nK(A1)) also exhibit a substantial degree of phoneme repe-\ntition, featuring three pairs of repeated phonemes in each:\n“doa (ドア )”-“dou (どう )”, “tsukurou (作ろう )”-“asobou\n(遊ぼう )”, “akete (開けて )”-“shite (して )” in Japanese,\nand “gachi (ᄀ ᅡ ᇀᄋ ᅵ)”-“gachi (ᄀ ᅡ ᇀᄋ ᅵ)”, “mandeul (만ᄃ ᅳ ᆯ)”-\n“eonnireul (언ᄂ ᅵ를)”, “mandeullae (만ᄃ ᅳ ᆯᄅ ᅢ)”-“simsimhae\n(심심ᄒ ᅢ)” in Korean. However, we realized that it is not\nfair to directly compare the number of phoneme repetitions\nwhen attempting to quantify the degree of phoneme repe-\ntition as each language has a different number of vowels\nand consonants: English has 15 vowels and 24 consonants,\nwhereas Japanese has 5 and 15 and Korean has 21 and\n19. Hence, in an attempt to minimize the differences in\nthe number of phonemes, we treated acoustically similar\nvowels as the same vowel in English, such as ‘IH’-‘IY’,\n‘UH’-‘UW’, or ‘EH’-‘AE’(e.g., ’mass’ and ’mess’) [25]\nbecause they can still form slant rhymes [26]. Conversely,\nwe considered ‘A’-‘YA’, ‘O’-‘YO’, and ‘U’-‘YU’ as sep-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n776Source Target Singable Non-singable\nEnglishJapanese 0.69 0.56\nKorean 0.97 0.72\nJapaneseEnglish 0.79 0.61\nKorean 0.80 0.48\nKoreanEnglish 0.97 0.78\nJapanese 0.80 0.50\nTable 4 . The average phoneme repetition similarity\n(Simpho) of singable lyrics and non-singable lyrics.\narate vowels in Japanese, as they are unlikely to function\nas the same grammatical components. In Korean, we re-\ngarded the perceptually similar vowels (e.g., ‘AE’-‘E’or\n‘OE’-‘OI’-‘OAE’) as the same vowels [27, 28].\nTo quantitatively represent the degree of phoneme rep-\netition, we utilized the concept of distinct-2 , the ratio of\nthe number of distinct bi-grams to the total number of bi-\ngrams [29]. While the original concept formed bi-grams\nusing two consecutive words, we used two consecutive\nphonemes to assess the degree of repetition because lower\ndistinct-2 values indicate higher repetition and vice versa.\nThephoneme distinct-2 (pho) of a section Xi, is deﬁned as\nfollows:\npho(Xi) =unique bi-gram # in Xi\ntotal bi-gram # in Xi. (2)\nFor example, consider a section with a single line “twinkle\ntwinkle little star”, denoted as X1. First, we decomposed\nthe section into phonemes and added the ‘ <eos> ’to each\nline: ‘T’, ‘W’, ‘IH’, ‘NG’, ‘K’, ‘AH’, . . . , ‘S’, ‘T’, ‘AA’,\n‘R’, and ‘<eos> ’. Next, we grouped each component into\nbi-grams: ‘TW’, ‘WIH’, ‘IHNG’, ‘NGK’, ‘KAH’, ‘AHL’,\n. . . , ‘ST’, ‘TAA’, ‘AAR’, ‘R <eos> ’. Finally, we calcu-\nlated the phoneme distinct-2 of the section ( pho(X1)) by\ndividing the number of unique bi-grams by the total num-\nber of bi-grams (17/23 = 0.74). To measure the similarity\nbetween two sections in terms of the degree of phoneme\nrepetition, we introduce the phoneme repetition similar-\nity(Simpho). Given two sets of lyrics with msections,\nX={X1,...,Xm}and˜X={˜X1,...,˜Xm}, the phoneme\nrepetition similarity between Xand˜Xis deﬁned as the\nSpearman correlation between {pho(X1),...,pho(Xm)}\nand{pho(˜X1),...,pho(˜Xm)}, as shown below.\nSimpho(X,˜X) =corr({pho(X1),...,pho(Xm)},{pho(˜X1),...,pho(˜Xm)})\n(3)\nWe present the statistical results for the average phoneme\nrepetition similarity of singable and non-singable lyrics in\nTable 4. The table clearly exhibits a higher correlation\nbetween the original lyrics and singable translated lyrics in\nterms of the phoneme distinct-2 than non-singable lyrics.\nThis result suggests that singable lyric translation takes\ninto account the degree of phoneme repetition within each\nsection to convey a sense of repetition for that section.\n4.3 Musical Structure Distance\nUpon examining our section-divided singable lyrics data,\nwe identiﬁed two tendencies in lyrics when musical sections\nFigure 1 . Musical self-dissimilarity matrices for English,\nJapanese, and Korean versions of the K-pop song “Icy” by\nITZY . Dissimilarity between the i-th and the j-th section\nwas computed using diss(Xi,Xj).\nare repeated (e.g., the repetition of the chorus). First, we\nobserved that musically similar sections tend to share the\nsame phonemes and, as expected, the same phrases. For\ninstance, in Table 3, musically similar sections share the\nsame vowels (e.g., “why” in E(B1)and “by” in E(B2)) or\nidentical phrases (e.g., “Do you wanna build a snowman” in\nE(A1)andE(A2)) in order to create a sense of consistency.\nAs a result, when calculating the phoneme distinct-2 (pho)\nfor two concatenated sections, musically similar sections\nare likely to have smaller values than musically different\nsections. For example, pho(E(A1+ +A2))is 0.70 (‘+ +’ de-\nnotes the concatenation of text), whereas pho(E(A1+ +B1))\nis 0.82, where A1is musically similar to A2but not to B1.\nHowever, a low value of phodoes not always imply musical\nsimilarity, as a meager phovalue in one section could result\nin a lowphoof two concatenated sections despite the musi-\ncal dissimilarity (e.g., “lalalalalalalalalalalalalala” + +“do\nyou wanna build a snowman?”). From this case, we derived\nour second observation that a signiﬁcant difference in pho\nfor each section could imply musical differences. Accord-\ningly, we also realized that musically similar sections tend\nto have a similar degree of pho. For instance, in Table 3,\nbothA1andA2, a set of musically similar sections, exhibit\nrelatively low pho, indicating a strong degree of phoneme\nrepetition (rhyme), with similar values to each other across\nall languages. Likewise, both B1andB2, another pair of\nmusically similar sections, demonstrate a higher pho, with\nsimilar values to each, in all languages.\nTherefore, to quantify the musical similarity between\nsections, we examined whether they have 1) a tendency to\nshare the same phoneme by obtaining pho(Xi+ +Xj), and\n2) similar phovalues by calculating |pho(Xi)−pho(Xj)|.\nGiven that higher values represent dissimilarity in both\ncases, we deﬁne the musical dissimilarity between two\nsections, diss(Xi,Xj), as the sum of these two values, as\nfollows.\ndiss(Xi,Xj) =pho(Xi+ +Xj)+|pho(Xi)−pho(Xj)|(4)\nAs shown in Figure 1, self-dissimilarity matrices employ-\ning our deﬁnition of musical dissimilarity look highly sim-\nilar across English, Japanese, and Korean, where all are\nsingable, visually representing musical structure.\nFinally, we quantitatively evaluated the distance between\nmatrices. We refer to this distance between matrices as the\nmusical structure distance, as it represents the structural ele-\nment of lyrics. In summary, the musical structure distanceProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n777Source Target SingableNon-singable\n(Human)Non-singable\n(Machine)\nEnglishJapanese 0.14 0.26 0.30\nKorean 0.10 0.15 0.15\nJapaneseEnglish 0.13 0.20 0.18\nKorean 0.11 0.13 0.14\nKoreanEnglish 0.10 0.10 0.10\nJapanese 0.11 0.10 0.17\nTable 5 . The average musical structure distance ( Dismus)\nof singable lyrics and non-singable lyrics.\nbetween lyrics in different languages Xand˜X, each con-\nsisting of msections,Dismus(X,˜X), is deﬁned as follows:\nDismus(X,˜X) =1\nm2/radicalBig/summationtextm\ni,j=1(diss(Xi,Xj)−diss(˜Xi,˜Xj))2\n(5)\nIn Table 5, we provide a summary of the average musical\nstructure distance for singable lyrics, human-translated non-\nsingable lyrics, and machine-translated non-singable lyrics\ngenerated by automatically translating ofﬁcial singable\nlyrics from 80 English, 162 Japanese, and 161 Korean\nsongs using Google Translator. Our ﬁndings show that\nsingable lyrics exhibit the lowest Dismus values, while\nmachine-translated non-singable lyrics display the high-\nest, suggesting that machine-translated ones lack structural\ncoherence the most. As human-translated non-singable\nlyrics maintain structural coherence in aspects such as word\nchoice and nuances, they demonstrate lower distances than\nmachine-translated counterparts.\n5. EV ALUATING SEMANTICS\nSemantic relatedness to the original lyrics is by no means\nless fundamental than syllable counts, phoneme repetition,\nand structural factors [18]. We therefore introduce a fourth\nmetric, semantic similarity, to ensure the semantic relevance\nof translated lyrics to the original.\n5.1 Semantic Similarity\nTo numerically assess the semantic textual similarity ( sts)\nbetween a pair of lyrics, we ﬁrst obtained the contextual\nembeddings of each text from lyrics using a pre-trained\nSentence BERT model1[31] and then calculated the cosine\nsimilarity between the embeddings. As this model was\ntrained for English, the Japanese and Korean lyrics were\nautomatically translated using Google Translator before\nobtaining the embeddings.\nWe started by examining hierarchical semantic similarity\nusing cross-scape plots [32], as shown in Figure 2. Given\na pair of lyrics X={x1,...,x n}and˜X={˜x1,...,˜xn}\nwithnlines each, the ﬁrst (leftmost) block of the lowest\nline represents the semantic textual similarity between x1\nand˜x1(denoted as sts(x1,˜x1)) while the last (rightmost)\nblock signiﬁes sts(xn,˜xn). The ﬁrst (leftmost) block of the\nsecond-lowest line denotes sts(x1+ +x2,˜x1+ + ˜x2), and the\nsecond block corresponds to sts(x2+ +x3,˜x2+ + ˜x3). Lastly,\n1We usedall-MiniLM-L6-v2 [30].\nFigure 2 . Semantic similarity cross-scape plot for the J-pop\nsong “A Thousand Winds” between English and Japanese\n(Left), English and Korean ( Middle ), and Japanese and\nKorean ( Right ). Any value less than 0 was regarded as 0.\nLine\n#EnglishJapanese\n(English translation)sts\n1please do not stand at\nmy grave and weep.私のお墓の前で\n(in front of my grave)0.56\n2 I am not there, I do not sleep泣かないでください\n(please stop crying)0.27\n1,\n2please do not stand at\nmy grave and weep. I am\nnot there, I do not sleep私のお墓の前で\n泣かないでください\n(in front of my grave.\nplease stop crying.)0.76\nTable 6 . Semantic textual similarity ( sts) between English\nand Japanese versions of “A Thousand Winds”.\nthe highest block (line) represents the similarity between\nthe entire lyrics, sts(x1+ +···+ +xn,˜x1+ +···+ + ˜xn).\nIn each plot of Figure 2, there are semantic disparities\nat lower levels, but similarities increase at higher (broader)\nlevels. We have two explanations for this. First, the number\nof musical notes within a single lyric line may be adequate\nto deliver a speciﬁc message in one language but insufﬁcient\nin another language. Therefore, it is common for a mes-\nsage conveyed in one line in one language to span two lines\nin another language. As an example, we provide Table 6,\nwhich presents the semantic textual similarity ( sts) between\nJapanese and English lyrics of the J-pop song “A Thousand\nWinds (千の風になって )”. As demonstrated in the table,\nthe similarity between Japanese and English at a broader\nlevel (sts(x1+ +x2,˜x1+ + ˜x2)) can be higher than at the line\nlevel (sts(x1,˜x1),sts(x2,˜x2)) because Japanese generally\nrequires more syllables than English and it often takes two\nlines in Japanese to express a single-line message in En-\nglish. Second, the semantic similarities at broader levels\ncan be higher because of grammatical/linguistic differences.\nEach language has its own natural word order patterns. For\nexample, in the phrase “I’m going to travel to ﬁnd the gold,”\nit is natural in English to mention “I’m going to travel” be-\nfore “to ﬁnd the gold.” However, in Japanese and Korean,\nexpressing “to ﬁnd the gold ( 金を探しに ,그 ᆷ을찾ᄋ ᅳᄅ ᅥ )”\nbefore “I’m going to travel ( 旅に出る,ᄄ ᅥ난ᄃ ᅡ)” is a more\ntypical and natural construction. Table 7 shows that these\ndifferences between languages make line-level semantic as-\nsessments insufﬁcient. Since lines 1, 2, and 3 in the English\nversion correspond to lines 3, 1, and 2 respectively in the\nJapanese version, these pairs exhibit low semantic similari-\nties at the line level ( sts(x1,˜x1),sts(x2,˜x2),sts(x3,˜x3)),\nwhile demonstrating higher similarity when considered as a\nwhole (sts(x1+ +x2+ +x3,˜x1+ + ˜x2+ +x3)).\nConsidering these factors, it becomes evident thatProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n778Line\n#EnglishJapanese\n(English translation)sts\n1Dare to try and reach out\nfor hеaven望むように生きるなら (If you want\nto become what you’re meant to be)0.22\n2You must become\nwhat you’rеmeant to be星からの金を求め\n(to ﬁnd the gold from stars)0.27\n3And bring the gold of\nheaven to the world一人旅に出るのよ\n(Dare to embark on a solo journey)0.13\n1-3Dare to try and reach out\nfor hеaven You must become\nwhat you’rеmeant to be\nAnd bring the gold of\nheaven to the world望むように生きるなら星からの\n金を求め一人旅に出るのよ\n(Dare to embark on a solo journey\nif you want to become what you’re\nmeant to be to ﬁnd the gold from stars)0.53\nTable 7 . Semantic textual similarity ( sts) between English\nand Japanese versions of “Gold von den Sternen”.\nFigure 3 . The line-wise ( Left) and section-wise ( Right )\nsemantic similarity matrices between Japanese and Korean\nversions of “Wie wird man seinen Schatten los?”\nsingable lyric translations do not prioritize line-wise seman-\ntic similarity. Rather, we observed that singable translations\naim to preserve semantic connections at the section level\nsince the organization of the lyric storyline follows a section-\nwise approach. To illustrate this, we present Figure 3, which\ndisplays both line-wise and section-wise semantic similarity\nmatrices for the Japanese and Korean versions of “How do\nyou get rid of your shadow? (Wie wird man seinen Schatten\nlos?)” from the German musical “Mozart!”. As shown in\nthe Figure, the section-wise matrix represents the semantic\nrelatedness more clearly than the line-wise matrix.\nTherefore, we propose assessing section-wise semantic\nrelatedness for evaluating singable lyric translation. To\nachieve this, we deﬁne the semantic similarity between a\npair of lyrics X={X1,...,Xm}and˜X={˜X1,...,˜Xm},\nconsisting of msections and n=n(X1)+···+n(Xm)\nlines, where n(Xi)denotes the number of lines in the i-th\nsection, as follows:\nSimsem(X,˜X) =/summationtextm\ni=1(n(Xi)\nnsts(Xi,˜Xi)). (6)\nTable 8 compares singable and non-singable lyrics in terms\nof line-wise semantic similarities (1\nn/summationtextn\ni=1sts(xi,˜xi))\nand section-wise similarities, using our proposed metric\n(Simsem). The table reveals that non-singable transla-\ntions exhibit high semantic similarity for both line-wise\nand section-wise measures, with similar values for each. In\ncontrast, singable translations show low line-wise similarity,\nas expected, since they do not prioritize line-wise semantic\nsimilarity. However, when evaluated using section-wise\nsimilarity, they display a level of similarity comparable to\nthat between “Machine learning is so easy” and “Deep learn-\ning is so straightforward”, which is 0.623 when measured\nwith the same pre-trained model [30].Source TargetSingable Non-singable\nline section line section\nEnglishJapanese 0.40 0.54 0.64 0.74\nKorean 0.42 0.55 0.70 0.76\nJapaneseEnglish 0.47 0.59 0.66 0.72\nKorean 0.52 0.61 0.77 0.79\nKoreanEnglish 0.53 0.63 0.78 0.81\nJapanese 0.52 0.61 0.73 0.75\nTable 8 . The average line-wise semantic similarity and\nsection-wise semantic similarity ( Simsem) of singable and\nnon-singable lyrics.\n6. DISCUSSIONS AND CONCLUSIONS\nIn this paper, we introduced a computational evaluation\nframework for singable lyric translation, grounded in the\nmusical, linguistic, and cultural understanding of lyrics,\ncomprised of four evaluation metrics, line syllable count\ndistance ( Dissyl), phoneme repetition similarity ( Simpho),\nmusical structure distance ( Dismus), and semantic similar-\nity (Simsem). These metrics are designed to ensure that the\ntranslated lyrics maintain the integrity of melodies, degree\nof phoneme repetition, structural coherence, and seman-\ntics of the original lyrics. Our framework is automated,\nguaranteeing objectivity and efﬁciency in terms of time and\ncost. We showed the efﬁcacy of our evaluation metrics by\noffering comparative statistics between singable and non-\nsingable lyrics. In addition, our analysis revealed that the\ndegree of phoneme repetition in the original lyrics is fre-\nquently mirrored in the translated lyrics, musically similar\nsections tend to share the same phonemes and display com-\nparable degrees of phoneme repetition, and section-wise\nanalysis is better suited for evaluating semantic similarity\nfor lyric translation than line-wise analysis.\nNonetheless, there remains room for improvement. Al-\nthough we have assembled a singable lyrics dataset, aligned\nacross English, Japanese, and Korean, our dataset has some\nlimitations; it lacks musical information and its volume is\nlimited. As a result, we have not been able to incorporate\nmusical notes into our experiment or conduct comparative\nstudies across various genres. We recognize that an ideal\nlyric translation evaluation system should take into account\nthe relationship between musical notes and phonemes, as\nwell as adapt to different genres. Moreover, although we\nhave endeavored to incorporate cultural understandings of\npoetry in different languages, we acknowledge the need for\ndeeper cultural considerations. For example, we noticed\nthat cultural similarities might have an impact on the extent\nof semantic similarities. This is demonstrated in an English\ntranslation of “MIC Drop”, a K-pop song by BTS originally\nwritten in Korean, made by YouTuber Iris Phuong. The\ntranslated singable lyrics do not include a translation of the\nterm “hyodo (ᄒ ᅭᄃ ᅩ , taking care of parents)” as there is no di-\nrect equivalent in English, while the Japanese version of the\nsong effortlessly conveys the concept as “koukou ( 孝行)”.\nIn the future, we aim to expand our dataset to contribute\nmore to lyric translation studies and to further explore the\nimpact of genre and cultural inﬂuences on lyric translation.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n7797. ACKNOWLEDGMENTS\nThis work was supported in part by JST CREST Grant\nNumber JPMJCR20D4.\n8. REFERENCES\n[1]M. Mateo, “Music and translation,” Handbook of trans-\nlation studies , vol. 3, pp. 115–121, 2012.\n[2]¸ S. Susam-Sarajeva, “Translation and music: Changing\nperspectives, frameworks and signiﬁcance,” The Trans-\nlator , vol. 14, no. 2, pp. 187–200, 2008.\n[3]S. Spaeth, “Translating to music,” The Musical Quar-\nterly, vol. 1, no. 2, pp. 291–298, 1915.\n[4]P. Low, “Translating songs that rhyme,” Perspectives:\nStudies in translatology , vol. 16, no. 1-2, pp. 1–20,\n2008.\n[5]——, “The pentathlon approach to translating songs,”\ninSong and signiﬁcance . Brill, 2005, pp. 185–212.\n[6]F. Guo, C. Zhang, Z. Zhang, Q. He, K. Zhang, J. Xie,\nand J. Boyd-Graber, “Automatic song translation for\ntonal languages,” in Findings of the Association for\nComputational Linguistics (ACL) . Dublin, Ireland:\nAssociation for Computational Linguistics, May\n2022, pp. 729–743. [Online]. Available: https:\n//aclanthology.org/2022.ﬁndings-acl.60\n[7]L. Ou, X. Ma, M.-Y . Kan, and Y . Wang, “Songs across\nborders: Singable and controllable neural lyric transla-\ntion,” arXiv preprint arXiv:2305.16816 , 2023.\n[8]C. Li, K. Fan, J. Bu, B. Chen, Z. Huang, and Z. Yu,\n“Translate the beauty in songs: Jointly learning to\nalign melody and translate lyrics,” arXiv preprint\narXiv:2303.15705 , 2023.\n[9]K. Watanabe and M. Goto, “Lyrics information process-\ning: Analysis, generation, and applications,” in Pro-\nceedings of the 1st Workshop on NLP for Music and\nAudio (NLP4MusA) , 2020, pp. 6–12.\n[10] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu,\n“BLEU: A method for automatic evaluation of machine\ntranslation,” in Proceedings of the 40th annual meeting\nof the Association for Computational Linguistics (ACL) ,\n2002, pp. 311–318.\n[11] C.-Y . Lin, “ROUGE: A package for automatic evalu-\nation of summaries,” in Text summarization branches\nout, 2004, pp. 74–81.\n[12] S. Banerjee and A. Lavie, “METEOR: An automatic\nmetric for MT evaluation with improved correlation\nwith human judgments,” in Proceedings of the ACL\nWorkshop on Intrinsic and Extrinsic Evaluation Mea-\nsures for Machine Translation and/or Summarization ,\n2005.[13] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and\nY . Artzi, “BERTScore: Evaluating text generation with\nbert,” in International Conference on Learning Repre-\nsentations (ICLR) , 2019.\n[14] I. Marc, “Travelling songs: On popular music transfer\nand translation,” IASPM Journal , vol. 5, no. 2, pp. 3–21,\n2015.\n[15] E. C. Hui-tung, “Translation of songs,” An Encyclopedia\nof Practical Translation and Interpreting , p. 351, 2019.\n[16] Y . Yang, Y . Zhang, C. Tar, and J. Baldridge, “PAWS-\nX: A cross-lingual adversarial dataset for paraphrase\nidentiﬁcation,” in Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP) , 2019, pp.\n3687–3692.\n[17] H. S. Drinker, “On translating vocal texts,” The Musical\nQuarterly , vol. 36, no. 2, pp. 225–240, 1950.\n[18] J. Franzon, “Three dimensions of singability. An ap-\nproach to subtitled and sung translations,” Text and\nTune. On the Association of Music and Lyrics in Sung\nVerse. Bern: Peter Lang , pp. 333–346, 2015.\n[19] J. P. G. Mahedero, A. MartÍnez, P. Cano, M. Koppen-\nberger, and F. Gouyon, “Natural language processing of\nlyrics,” in Proceedings of the 13th Annual ACM Interna-\ntional Conference on Multimedia , ser. MULTIMEDIA\n’05. Association for Computing Machinery, 2005, pp.\n475–478.\n[20] K. Watanabe and M. Goto, “A chorus-section detection\nmethod for lyrics text.” in Proceedings of the 21th In-\nternational Conference on Music Information Retrieval\n(ISMIR) , 2020, pp. 351–359.\n[21] R. Apter and M. Herman, “Translating art songs for\nperformance: Rachmaninoff’s six choral songs,” Trans-\nlation Review , vol. 84, no. 1, pp. 27–42, 2012.\n[22] A. Bain, “English composition and rhetoric,(2nd ameri-\ncan ed.),” New York: D. Appleton and Company , 1867.\n[23] N. Manabe, “Globalization and Japanese creativity:\nAdaptations of Japanese language to rap,” Ethnomu-\nsicology , vol. 50, no. 1, pp. 1–36, 2006.\n[24] Y . B. Yoon and B. L. Derwing, “A language without\na rhyme: Syllable structure experiments in Korean,”\nCanadian Journal of Linguistics/Revue canadienne de\nlinguistique , vol. 46, no. 3-4, pp. 187–237, 2001.\n[25] A. K. Syrdal and H. S. Gopal, “A perceptual model of\nvowel recognition based on the auditory representation\nof American English vowels,” The Journal of the Acous-\ntical Society of America , vol. 79, no. 4, pp. 1086–1100,\n1986.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n780[26] K. Hanson, “Formal variation in the rhymes of Robert\nPinsky’s the inferno of dante,” Language and Literature ,\nvol. 12, no. 4, pp. 309–337, 2003.\n[27] C. Ito, Y . Kang, and M. Kenstowicz, “The adaptation of\nJapanese loanwords into Korean,” MIT Working Papers\nin Linguistics , vol. 52, no. 2006, pp. 65–104, 2006.\n[28] S.-E. Chang, “Enhancement effects of clear speech and\nword-initial position in Korean glides.” The Journal of\nthe Acoustical Society of America , 2017.\n[29] J. Li, M. Galley, C. Brockett, J. Gao, and W. B. Dolan,\n“A diversity-promoting objective function for neural con-\nversation models,” in Proceedings of the 2016 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics (NACCL) : Human Lan-\nguage Technologies , 2016, pp. 110–119.\n[30] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and\nM. Zhou, “MiniLM: Deep self-attention distillation for\ntask-agnostic compression of pre-trained transformers,”\nAdvances in Neural Information Processing Systems\n(NeurIPS) , vol. 33, pp. 5776–5788, 2020.\n[31] N. Reimers and I. Gurevych, “Sentence-BERT: Sen-\ntence embeddings using Siamese BERT-networks,” in\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , 2019, pp. 3982–3992.\n[32] S. Park, T. Kwon, J. Lee, J. Kim, and J. Nam, “A\ncross-scape plot representation for visualizing symbolic\nmelodic similarity,” in Proceedings of the 20th Inter-\nnational Conference on Music Information Retrieval\n(ISMIR) , 2019, pp. 423–430.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n781"
    },
    {
        "title": "Self-Refining of Pseudo Labels for Music Source Separation With Noisy Labeled Data.",
        "author": [
            "Junghyun Koo",
            "Yunkee Chae",
            "Chang-Bin Jeon",
            "Kyogu Lee"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265389",
        "url": "https://doi.org/10.5281/zenodo.10265389",
        "ee": "https://zenodo.org/records/10265389/files/000085.pdf",
        "abstract": "Music source separation (MSS) faces challenges due to limited availability and potential noise in correctly labeled individual instrument tracks. In this paper, we propose an automated approach for refining mislabeled instrument tracks in a partially noisy-labeled dataset. The proposed self-refining technique with noisy-labeled dataset results in only a 1% accuracy degradation for multi-label instrument recognition compared to a classifier trained with a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data for training MSS models and shows that utilizing the refined dataset for MSS leads to comparable results to a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on self-refined datasets even outperformed those trained on datasets refined with a classifier trained on clean labels.",
        "zenodo_id": 10265389,
        "dblp_key": "conf/ismir/KooCJL23",
        "keywords": [
            "Music source separation",
            "instrument tracks",
            "noisy-labeled dataset",
            "self-refining technique",
            "accuracy degradation",
            "multi-label instrument recognition",
            "partially noisy-labeled dataset",
            "MSS models",
            "clean-labeled dataset",
            "MSS results"
        ],
        "content": "SELF-REFINING OF PSEUDO LABELS\nFOR MUSIC SOURCE SEPARATION WITH NOISY LABELED DATA\n*Junghyun Koo1*Yunkee Chae2Chang-Bin Jeon1Kyogu Lee1,2,3\n1Department of Intelligence and Information,2Interdisciplinary Program in Artiﬁcial Intelligence,\n3Artiﬁcial Intelligence Institute, Seoul National University\n{dg22302, yunkimo95, vinyne, kglee}@snu.ac.kr\nABSTRACT\nMusic source separation (MSS) faces challenges due to\nthe limited availability of correctly-labeled individual in-\nstrument tracks. With the push to acquire larger datasets\nto improve MSS performance, the inevitability of encoun-\ntering mislabeled individual instrument tracks becomes a\nsigniﬁcant challenge to address. This paper introduces an\nautomated technique for reﬁning the labels in a partially\nmislabeled dataset. Our proposed self-reﬁning technique,\nemployed with a noisy-labeled dataset, results in only a 1%\naccuracy degradation in multi-label instrument recognition\ncompared to a classiﬁer trained on a clean-labeled dataset.\nThe study demonstrates the importance of reﬁning noisy-\nlabeled data in MSS model training and shows that utiliz-\ning the reﬁned dataset leads to comparable results derived\nfrom a clean-labeled dataset. Notably, upon only access\nto a noisy dataset, MSS models trained on a self-reﬁned\ndataset even outperform those trained on a dataset reﬁned\nwith a classiﬁer trained on clean labels.\n1. INTRODUCTION\nMusic source separation (MSS) is a critical task in the\nﬁeld of music information retrieval (MIR), with applica-\ntions ranging from remixing [1–3] to transcription [4–6]\nand music education [7, 8]. To train high-performing MSS\nmodels, it is essential to have clean single-stem music\nrecordings for guidance, which serve as the ground truth\nfor model training. However, obtaining clean, large-scale\ndatasets of single instrument tracks remains a challenging\ntask.\nWith the increasing availability of music data on the in-\nternet, platforms such as YouTube provide a vast pool of\npotential single-instrument tracks. Although these sources\noffer an opportunity for performance gains through larger\ntraining datasets, collecting single instrument tracks from\nsuch platforms inevitably leads to encountering tracks with\n*Equal contribution\n© J. Koo, Y . Chae, C.-B. Jeon, and K. Lee. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: J. Koo, Y . Chae, C.-B. Jeon, and K. Lee, “Self-reﬁning of\nPseudo Labels for Music Source Separation with Noisy Labeled Data”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.\nFigure 1 . Overview of self-reﬁning procedure on a noisy-\nlabeled dataset for music source separation.\nincorrect labels. For example, a query aimed at obtain-\ning drum recordings might yield results that contain other\ntypes of instruments or noise, causing discrepancies be-\ntween the expected and actual content of the collected\nrecordings.\nLabel noise in datasets can arise from various factors,\nsuch as bleeding between instrument tracks, mislabeling\ndue to human error, or the ambiguous timbre of instru-\nments that resemble other instrument categories [9]. These\nfactors make it challenging to assign a single deﬁnitive in-\nstrument label to a given recording. Such label noise is\ndetrimental to the performance of MSS models, and there\nis a pressing need for an approach that can effectively train\nMSS models using partially corrupted datasets.\nIn response to this challenge, we propose an automated\napproach for reﬁning mislabeled instrument tracks in a par-\ntially noisy-labeled dataset. Our self-reﬁning technique,\nwhich leverages noisy-labeled data, results in only a 1%\naccuracy degradation for multi-label instrument recogni-\ntion compared to a classiﬁer trained with a clean-labeled\ndataset. The study highlights the importance of reﬁning\nnoisy-labeled data for training MSS models and demon-\nstrates that utilizing the reﬁned dataset for MSS yields re-\nsults comparable to those obtained using a clean-labeled\ndataset. Notably, when only a noisy dataset is available,\nMSS models trained on self-reﬁned datasets even outper-\nform those trained on datasets reﬁned with a classiﬁer\ntrained on clean labels. This paper presents a comprehen-\nsive analysis of our proposed method and its impact on the\nperformance of MSS models.716Figure 2 . Overall training procedure of the Instrument Classiﬁer Ψ. The classiﬁer is trained to perform instrument recogni-\ntion with mixtures that are synthesized by randomly selecting each stem from the noisy labeled dataset. After this training\nprocedure, we reﬁne the original noisy dataset and then use this new dataset to train the ﬁnal Ψ.\n2. RELATED WORKS\nSelf-training of machine learning models has been stud-\nied in various literatures, where a teacher model is ﬁrst\ntrained with clean labeled data and is used as a label predic-\ntor of unlabeled data, then a student model is trained with\nclean and pseudo-labeled data [10, 11]. Recently, Xie et\nal. proposed a noisy student method for self-training [12],\nwhich uses an iterative training of teacher-student mod-\nels and noise injection methods for training student mod-\nels. Thanks to their usefulness, these self-training methods\nhave been used in diverse MIR tasks, such as singing voice\ndetection [13, 14] and vocal melody extraction [15].\nInstrument recognition or classiﬁcation has been re-\nsearched in various literatures, both in single-instrument\n[16–19] or multi-source settings [20–27]. Although such\nresearch has been focused on single or predominant-label\nprediction, Zhong, et al. [28] recently proposed the hierar-\nchical approach for multi-label music instrument classiﬁ-\ncation.\nOur self-reﬁning method for training of instrument clas-\nsiﬁer shares similar attributes with noisy student train-\ning [12] and the previous multi-label instrument classiﬁ-\ncation [28] but differs from some perspectives. i)We train\nall our models only with partially noisy-labeled data, with-\nout access to clean-labeled data. ii)We train the classi-\nﬁers for direct prediction of labels used in standard music\nsource separation, e.g., vocals, bass, drums, and others, in-\nstead of the hierarchical approach. iii)We train multi-label\nclassiﬁers with mixtures of randomly selected instruments,\nwhich are based on the characteristic of musical audio. If\nthere exist two different instruments in one audio signal,\nthat can be classiﬁed into two instruments. This random\nmixing of different instrumental tracks has been used in\nmusic source separation as well [29]. Note that the mixup\nmethod [30], which is also a mixing method of two differ-\nent images, also shares a similar attribute with our method\nbut is used for regularization of training single-label clas-\nsiﬁers, not like our multi-label classiﬁers.3. METHODOLOGY\nGiven a real-world scenario where the available multi-\ntrack dataset for MSS is partially incorrect with its instru-\nment labels, a possible naive approach is ﬁrst to rectify\nmislabeled tracks and then train an MSS model using stems\nwith the revised labels. In this section, we introduce an\neffective training technique that ﬁrst performs instrument\nrecognition by only utilizing data with noisy labels and\nthen leverages the reﬁned dataset inferred with the trained\nmulti-label instrument classiﬁer to train the MSS model.\nWith this two-stage approach, we explore the impact of the\nreﬁned noisy datasets on the performance of MSS models.\n3.1 Multi-label Instrument Recognition\nFigure 2 summarizes the proposed training procedure of\nthe Instrument Classiﬁer Ψ. Similar yet different from\nself-training, our approach learns directly from noisy la-\nbeled data and re-labels the training data to train the ﬁnal\nΨusing this reﬁned dataset. We call this training proce-\ndure self-reﬁning , and this is possible by random mixing ,\na method to synthesize a mixture of multiple instruments\nwith pseudo labels. The random mixing technique takes\nadvantage of the acoustic music domain in that mixing\nsources of different instrument tracks still leads to natural\noutput mixture, whereas naively combining different im-\nages in the image domain is likely to produce unrealistic\nresults. We further discuss about the beneﬁts and the de-\ntailed process of random mixing at 3.1.1.\nThe network architecture of Ψis that of the ConvNeXt\nmodel [31], where it has shown great performance on a\nmulti-instrument retrieval in [27]. The input of the net-\nwork is a stereo-channeled magnitude linear spectrogram.\nFollowed by a sigmoid layer, the model outputs four labels\nindicating the presence of each stem. The objective func-\ntion for instrument recognition LΨis a mean absolute loss\nbetween the estimated and synthesized pseudo labels. Pre-\nliminary experiments showed no signiﬁcant difference in\nperformance when employing mean absolute loss as com-\npared to binary cross-entropy loss. This is likely due toProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n717Figure 3 . Music source separation training. Similar to the training procedure of the instrument classiﬁer, we randomly mix\neach stem from the reﬁned dataset to synthesize a mixture and use it as a network input. When a multi-labeled segment is\nselected for synthesis, the corresponding estimated stems are summed for loss computation.\nthe random mixing sampling that ensures similar occur-\nrences of positive and negative labels of each instrument\nclass during the training procedure.\n3.1.1 Random Mixing\nRandomly mixing stems with label noise not only creates\nvarious combinations of multi-labeled mixtures for train-\ning the instrument classiﬁer but also brings the chance to\ngenerate a correct pseudo label from mislabeled stems. For\ninstance, if we randomly select one correctly labeled drum\ntrack and a track that contains both sources of drums and\nvocals but is mislabeled as vocals, the mixing process syn-\nthesizes a correctly labeled mixture. Thanks to these for-\ntunate chances, the random mixing technique assists the\ninstrument classiﬁer’s accuracy in reﬁning the label noise\ndataset by utilizing mislabeled stems.\nTo synthesize a random mixture and its pseudo label,\neach stem is ﬁrst selected with a chance rate from the noisy\ndataset. The audio effects manipulation is then applied to\neach chosen track by simulating the music mixing process\nfor data augmentation [3]. The order of applying audio\neffects with random parameters is 1. dynamic range com-\npression , 2.algorithmic reverberation , 3.stereo imaging ,\nand 4. loudness manipulation . Labels corresponding to\nrandomly selected stems are used as a multi-label objec-\ntive for the instrument classiﬁer.\n3.2 Music Source Separation\nIn this section, we describe the training procedure of MSS\nmodel employing a multi-labeled reﬁned dataset curated\nby the classiﬁer trained in Section 3.1. The majority\nof MSS research has focused on estimating each of the\nfour instrument groups ( vocals ,bass,drums , and other )\n[32–35]. However, our reﬁned dataset contains sources la-\nbeled with multiple stems, which are unsuitable for use\nas distinct target instruments. To utilize multi-labeled\nsources, we propose an appropriate MSS training frame-\nwork tailored to our reﬁned dataset.\nFirst, we determine whether to include the multi-stem\nsource for each input mixture sample by considering the\nprobability p. If we decide not to include the multi-labeled\nsource, we can train the MSS model in a conventional man-\nner, computing the losses for each stem. Otherwise, we se-\nlect a multi-labeled source from the reﬁned dataset. Sub-sequently, we choose the remaining stems that do not cor-\nrespond to the selected multi-labeled source from a pool\nof single-labeled sources and combine them to simulate\na mixture. For example, when selecting a multi-labeled\nsource bass+drums , we opt for single sources labeled as\nvocals andothers to synthesize the mixture. After conduct-\ning inference with the MSS model, we add the estimated\nstems corresponding to the multi-stem source of the input\nmixture and assess the loss between them. Figure 3 illus-\ntrates our training procedure when a multi-labeled source\nis selected. We compute the losses for each stem, treating\nthe multi-labeled source as an individual stem, and subse-\nquently sum these losses to derive the ﬁnal loss value.\n4. EXPERIMENTS\n4.1 Dataset\nWe use the label noise dataset provided by the Music\nDemixing Challenge 2023 (MDX2023) [36], which con-\nsists of 203 songs, licensed by Moises.AI1. Similar to\nMUSDB18 [37], the provided dataset contains mixtures of\nmusic recordings segregated into four different instrumen-\ntal stems: vocals ,bass,drums , and other . Each stem and\nits corresponding label are intentionally altered to produce\na corrupted dataset to simulate mislabeling such as bleed-\ning or human mistakes. That is, for instance, drums.wav\nmay contain drum sounds and singing voices simultane-\nously, which is likely to be caused by bleeding. For an-\nother example, a kick-drum sound might be mislabeled as\nbass.wav when the pitch of the kick drum is melodic\nenough to trick a human labeler. Due to the nature of the\nMDX2023 challenge, the dataset does not contain the ac-\ntual ground truth labels. Hence, we use all 203 songs of\nthe MDX2023 dataset only as training data.\nTo validate our system trained with noisy labeled data,\nwe employed the MUSDB18 [37] as the clean dataset for\ncomparison and evaluation. MUSDB18 comprises 150\nsongs, with 100 songs for the training and 50 songs for\nthe test set. We adopt the test subset for evaluating all sys-\ntems, while the training subset is used to train the upper\nbound system for observation.\nData preprocessing. To prevent models from mislabels\ncaused by silence, we remove all silent sections through-\n1https://moises.ai/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n718Label Type Training DataAccuracy / F1 Score\nPrecision / Recall\nvocals bass drums other avg\nSingle-Labelclean97.8% / 0.947\n0.91 / 0.9894.4% / 0.891\n0.84 / 0.9495.1% / 0.914\n0.85 / 0.9893.2% / 0.880\n0.90 / 0.8595.1% / 0.906\n0.87 / 0.93\nnoisy93.6% / 0.860\n0.76 / 0.9790.0% / 0.821\n0.73 / 0.9393.7% / 0.893\n0.81 / 0.9892.6% / 0.865\n0.92 / 0.8192.5% / 0.860\n0.80 / 0.92\nreﬁned96.1% / 0.911\n0.84 / 0.9889.6% / 0.818\n0.71 / 0.9693.1% / 0.884\n0.79 / 0.9892.3% / 0.862\n0.90 / 0.8292.8% / 0.866\n0.80 / 0.93\nMulti-Labelclean92.4% / 0.929\n0.92 / 0.9389.6% / 0.905\n0.89 / 0.9290.5% / 0.913\n0.87 / 0.9588.1% / 0.878\n0.90 / 0.8590.2% / 0.907\n0.90 / 0.91\nnoisy87.9% / 0.895\n0.83 / 0.9687.5% / 0.888\n0.86 / 0.9387.7% / 0.891\n0.82 / 0.9687.3% / 0.872\n0.88 / 0.8787.6% / 0.887\n0.85 / 0.93\nreﬁned91.9% / 0.928\n0.88 / 0.9787.8% / 0.894\n0.84 / 0.9589.6% / 0.906\n0.85 / 0.9687.4% / 0.874\n0.88 / 0.8789.2% / 0.901\n0.86 / 0.94\nTable 1 . Instrument recognition performance on single and multi-label instrument classiﬁers trained with different datasets.\nThe training data of clean ,noisy , and reﬁned each represents the training subset of MUSDB18, MDX2023, and MDX2023\nreﬁned with the instrument classiﬁer trained with MDX2023 Ψnoisy, respectively.\nout both datasets. The preprocessing procedure for silence\nremoval is as follows:\n1. For each song, detect silent areas that are below 30\ndB relative to the maximum peak amplitude.\n2. Remove all detected areas then merge them into one\nsingle long audio track.\n3. Repeat 1. (with the threshold of 60 dB) and 2. based\non the merged audio track, in case of stems that are\nalmost silent.\nAfter trimming silent regions, the total durations for each\nstem in the respective order of vocals ,bass,drums , and\nother are [7.2, 7.8, 9.2, 10.3] hours for the MDX2023\ndataset, and [2.2, 2.7, 2.9, 3.3] hours for the test subset of\nMUSDB18. Note that for evaluating MSS performance,\nwe instead follow the original convention of processing\nentire songs from the test subset without any silence re-\nmoval. We use the original audio speciﬁcations of both\ndatasets where all audio tracks are stereo-channeled and\nhave a sampling rate of 44.1 KHz.\n4.2 Experimental Setups\nFor multi-label instrument recognition, the network archi-\ntecture of Ψis ConvNeXt’s tiny version [31], which con-\nsists of 27.8M parameters. We feed the network with\nstereo-channeled mixtures of instruments that are of 2.97\nseconds, which are transformed into a time-frequency do-\nmain linear magnitude spectrogram with an FFT size of\n2048 and a hop size of 512. We train all Ψfor 100 epochs.\nDuring inference, Ψperforms classiﬁcation by processing\nthe entire input audio in windows of a size equivalent to\nthe network input size, with a hop size of one-fourth of\nthis window size. The output labels from these windows\nare then averaged to yield the ﬁnal decision, based on a\nthreshold value of 0.9. We utilized this inference proce-\ndure to reﬁne the noisy dataset, which was then used to\ntrain our MSS models. Our ﬁnal version of the instrumentclassiﬁer trained on the reﬁned dataset Ψreﬁned only uses\nstems inferred as a single-labeled for better performance\nbased on our preliminary experiments.\nWe employed two MSS models, Hybrid Demucs (De-\nmucs v3) [38] and CrossNet-Open-Unmix (X-UMX) [39],\nto evaluate their performance when trained on the pro-\ncessed datasets. Multi-labeled sources were selected with\na probability of 0.4, and input loudness normalization (-\n14 LUFS) was applied for both training and inference in\naccordance with [40]. pyloudnorm [41] was used for\nloudness calculation [42].\nFor Demucs, the input duration was set to 3 seconds,\nand optimization was performed using Adam optimizer\n[43] and L1 loss on the time domain. The model was\ntrained for 21,000 iterations with a batch size of 160.\nFor X-UMX, the input duration was set to 6 seconds,\nand optimization was performed using AdamW optimizer\n[44] and mean squared error loss on the time-frequency do-\nmain. For the sake of simplicity, we omit the multi-domain\nand combination loss proposed in [39]. The model was\ntrained for 56,400 iterations with a batch size of 32. For\nthe+ ﬁnetune w/ multi-labeled model in Table 3, we ﬁrst\ntrain the model with only single-labeled data for 20,680\niterations, then ﬁnetune it with multi-labeled data for an-\nother 35,720 iterations.\n5. RESULTS\n5.1 Instrument Recognition\nTable 1 presents the instrument recognition performance of\nthe multi-instrument classiﬁer on single-labeled and multi-\nlabeled data. As ground-truth labels are not available for\nthe MDX2023 dataset, we validate the classiﬁcation per-\nformance according to single and multi-labeled data with\nthe MUSDB18 test set for evaluation. For the multi-label\nevaluation, we synthesized 3,941 mixtures from the test\nset with the random mixing technique described in 3.1.1.\nWe observe the performance of Ψtrained with MUSDB18\n(clean ), MDX2023 ( noisy ), and MDX2023 once reﬁnedProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n719NetworkTraining\nDataSDR [dB]\nvocals bass drums other avg\nDemucs\n[38]clean 5.92 6.16 5.58 4.43 5.52\nnoisy 3.37 1.92 0.70 0.86 1.71\nw/Ψclean 5.31 5.12 1.32 2.16 3.48\nw/Ψnoisy 4.15 4.58 1.62 2.85 3.30\nw/Ψreﬁned 5.36 5.04 3.09 3.13 4.16\nX-UMX\n[39]clean 5.76 4.44 5.47 3.65 4.83\nnoisy 3.39 1.78 1.52 0.96 1.91\nw/Ψclean 4.50 3.22 3.66 2.73 3.53\nw/Ψnoisy 4.72 4.11 3.22 2.89 3.74\nw/Ψreﬁned 4.99 3.93 5.00 3.18 4.28\nTable 2 . Source separation performance of Demucs v3\n[38] and CrossNet-Open-Unmix [39] trained on different\ntraining datasets. Sub-items below noisy dataset indicate\ndata reﬁned with the respective instrument classiﬁers, de-\nnoted asΨ•.\nwithΨnoisy(reﬁned ). The evaluation metrics used are ac-\ncuracy, F1 score, precision, and recall for each instrument\nclass and the overall averaged result.\nFor single-labeled data, the classiﬁer achieves the high-\nest average performance on the clean dataset, with an ac-\ncuracy of 95.1% and an F1 score of 0.906. As clean\ndataset does not contain any noisy labels, the obtained\nresults can be considered an upper bound for the perfor-\nmances of the classiﬁers. The Ψtrained on reﬁned dataset\nresults in slightly lower performance, with an accuracy\nof 92.8% and F1 score of 0.866, while the noisy dataset\nshows an accuracy of 92.5% and F1 score of 0.860. Al-\nthough the accuracy, F1 score, and precision are higher for\nthenoisy dataset in the bass, drums , and other stems, the\nperformance metrics for vocals and recall values across all\nstems exhibit superior results when trained with the reﬁned\ndataset.\nFor instrument recognition on multi-labeled data, Ψ\ntrained on clean dataset yields an average accuracy of\n90.2% and F1 score of 0.907. The noisy dataset results in\nan accuracy of 87.6% and an F1 score of 0.887. The reﬁned\ndataset achieves superior performance, with an accuracy of\n89.2% and an F1 score of 0.901, which is comparable to\nthe results obtained from the clean dataset. Contrary to the\nevaluation with single-labeled data, the reﬁned dataset gen-\nerally demonstrates superior performance across all met-\nrics in comparison to the noisy dataset. Notably, the re-\ncall values are observed to be even higher than those of the\nclean dataset. An in-depth analysis of the multi-instrument\nclassiﬁer results, alongside the performance outcomes of\nthe MSS models, is discussed in Section 5.2.\n5.2 Source Separation\nThe results of MSS models trained on different training\ndatasets are presented in Table 2. In our evaluation, we\nused Signal-to-Distortion Ratio (SDR) [45], which is cal-\nculated using the museval toolkit [46]. For all MSS ex-\nperiments, we report the SDR median of frames and the\nmedian of tracks. The Demucs and X-UMX models areMethodSDR [dB]\nvocals bass drums other avg\nproposed 4.99 3.93 5.00 3.18 4.28\nthreshold = 0.5 5.06 4.13 4.77 3.06 4.25\nadaptive thresholds 4.70 3.72 3.70 2.62 3.68\ntrain only w/ single-labeled 4.90 3.73 4.54 3.18 4.09\n+ ﬁnetune w/ multi-labeled 4.33 4.33 4.19 3.14 4.00\nself-reﬁning ×5 4.65 3.87 5.07 2.89 4.12\nTable 3 . Ablation studies on MSS performances with\nCrossNet-Open-Unmix.\ntrained on clean ,noisy , and data processed with multi-\ninstrument classiﬁers, denoted by Ψ•. In this context, Ψ•\nrepresents the classiﬁer trained on each respective dataset,\nas described in Section 5.1.\nThe baseline for this experiment is established using\nMSS models trained on the noisy dataset. It is noteworthy\nthat all the results presented in the table exceed the base-\nline performance. For the dataset processed with the multi-\ninstrument classiﬁer Ψreﬁned , average SDR improvements\nof 2.45 and 2.31 are observed for Demucs and X-UMX\nmodels, respectively, in comparison to the noisy dataset.\nSpeciﬁcally, in Ψreﬁned case, both Demucs and X-UMX\nmodels demonstrate substantial improvements in SDR val-\nues across all stems compared to those of Ψnoisy, with the\nexception of bass in the X-UMX model.\n5.2.1 Analysis in relation to instrument recognition\nIn Table 2, it is noteworthy that the performance of Ψreﬁned\nexceeds the performance of Ψclean, even though Ψclean is\ntrained with a noise-free labeled dataset. This implies the\nclassiﬁcation performance of Ψcleanis inferior to the classi-\nﬁcation performance of Ψreﬁned . This discrepancy could be\nattributed to differences in the data distribution between\nthe MUSDB18 and MDX2023 datasets. Moreover, the\nnumber of training samples varies, with 100 samples in\nthe MUSDB18 dataset and 203 samples in the MDX2023\ndataset. When reﬁning a partially noisy dataset, employ-\ning the same partially noisy dataset can yield advantageous\noutcomes than using the smaller clean dataset. This obser-\nvation might be aligned with the ﬁndings in [12], which\nreport an improvement in performance when a larger quan-\ntity of unlabeled data is present.\nAn additional factor to consider is the distinctive nature\nof the MSS model training framework in our approach.\nMSS models utilize the output of the classiﬁer as input.\nThe performance of the MSS model can be affected dif-\nferently depending on the type of error in the classiﬁer’s\noutput. For example, assume that the MSS model receives\na sample misclassiﬁed as a vocal stem when no vocals are\nactually present (i.e. a false-positive sample for vocals). In\nthis case, the MSS model simply needs to predict silence\nfor the vocals stem and produce it as output, resulting in\nno signiﬁcant confusion. Conversely, consider a scenario\nin which the MSS model receives a sample misclassiﬁed as\na non-vocal stem (e.g. drums + bass), despite the presence\nof vocals, resulting in a false-negative sample for vocals. In\nsuch a case, the model will attempt to allocate the vocalsProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n720Figure 4 . Precision and recall curves of the proposed classiﬁer across different thresholds (x-axis) on each instrument. The\ncurves are generated using the MUSDB18 test set ( clean ).\npresent in the input data to the drum and bass stems. Fur-\nthermore, our model differs from traditional MSS training\nmethods as it also accepts multi-stem data as input. In this\ncontext, the vocals are present as the correct answer for\nmultiple mislabeled non-vocal stems, which confuses the\nmodel. This not only negatively affects the performance of\nthe mislabeled stems but also the vocal stem itself.\nAs a consequence of the unique characteristics of our\ntraining process, false-negative samples have a more sig-\nniﬁcant impact on MSS compared to false-positive sam-\nples, highlighting the increased signiﬁcance of the recall\nmetric. Considering this perspective, the results presented\nin Table 1 imply the possibility of the sub-optimal perfor-\nmance of MSS trained on outputs of Ψclean, where the re-\ncall values are lower for all stems compared to Ψreﬁned .\n5.2.2 Ablation studies\nAs shown in Table 3, we evaluate the performance of X-\nUMX under various conditions to better understand the\nsigniﬁcance of distinct aspects of our proposed method.\nThreshold. We conduct experiments to examine the im-\npact of threshold determination for the classiﬁer during the\ntraining of MSS models using a classiﬁed dataset. The\nevaluation is performed on the MUSDB18 test set. We\nobserve that reducing the threshold to 0.5 only exhibits an\nSDR of 0.03 degradation compared to the original thresh-\nold value of 0.9. This outcome can be attributed to the fact\nthat only 8% of Ψreﬁned outputs fall within the range of [0.1,\n0.9] upon inference on the MUSDB18 test set. In Figure 4,\nwe present the precision and recall curves for each thresh-\nold on individual instruments. It is evident from the curves\nthat the variations within that range for both precision and\nrecall are not substantial. Consequently, the choice be-\ntween thresholds of 0.9 or 0.5 does not yield any notice-\nable disparity. Furthermore, we conduct an experiment in-\nvolving adaptive thresholds for each instrument, where the\nthreshold for each instrument was set to maximize the F1\nscore of the classiﬁcation performance. However, we ob-\nserve a signiﬁcant degradation in performance across all\ninstruments when employing adaptive thresholds. Maxi-\nmizing the F1 score necessitates a trade-off between re-\ncall and precision, often leading to a decline in recall toenhance precision. Consequently, the performance of the\nMSS model experience degradation, aligning with the dis-\ncussion presented in Section 5.2.1.\nTraining with multi-labeled data. When training solely\nwith the data estimated as single-labeled, the performance\nis not as good as that of the proposed method. Incorporat-\ning both single- and multi-labeled data for ﬁne-tuning after\nthe initial training on single-labeled data leads to a slightly\ndiminished performance, despite utilizing both types of la-\nbeled data during the training process.\nIterative self-reﬁning. Finally, we examine the inﬂu-\nence of the iterative self-reﬁning technique on MSS per-\nformance. The results indicate that an MSS model trained\nwith a noisy-labeled dataset reﬁned ﬁve times through our\nmethod does not yield superior performance compared to\nthe proposed model, trained on a dataset reﬁned twice, and\nthe performance difference is insigniﬁcant. This observa-\ntion suggests that excessive reﬁnement iterations do not\nnecessarily lead to improved performance and that reﬁn-\ning the dataset twice may be sufﬁcient for optimal results.\n6. CONCLUSION\nIn conclusion, this paper presented a self-reﬁning approach\nto address the challenges of noisy-labeled data in train-\ning music source separation (MSS) models. Our proposed\nmethod reﬁnes mislabeled instrument tracks in partially\nnoisy-labeled datasets, resulting in only a 1% accuracy\ndegradation for multi-label instrument recognition com-\npared to a classiﬁer trained on a clean-labeled dataset. This\nstudy highlights the importance of reﬁning noisy-labeled\ndata for training MSS models effectively and demonstrates\nthat utilizing the reﬁned dataset for MSS yields results\ncomparable to those obtained using a clean-labeled dataset.\nConsidering the real-world scenario of accessibility only\nto a noisy dataset, MSS models trained on self-reﬁned\ndatasets outperformed those trained on datasets reﬁned\nwith a classiﬁer trained on clean labels. The self-reﬁning\napproach we introduced offers a promising direction for\nfuture research in the ﬁeld of music information retrieval\nand has the potential to be extended to other applications\nrequiring robust training on noisy-labeled datasets.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n7217. ACKNOWLEDGEMENTS\nThis work was partially supported by Culture, Sports and\nTourism R&D Program through the Korea Creative Con-\ntent Agency grant funded by the Ministry of Culture,\nSports and Tourism in 2022 [No.R2022020066, 90%], and\nInstitute of Information & communications Technology\nPlanning & Evaluation (IITP) grant funded by the Korea\ngovernment(MSIT) [NO.2021-0-01343, Artiﬁcial Intelli-\ngence Graduate School Program (Seoul National Univer-\nsity), 10%].\n8. REFERENCES\n[1] J. F. Woodruff, B. Pardo, and R. B. Dannenberg,\n“Remixing stereo music with score-informed source\nseparation.” in ISMIR , 2006, pp. 314–319.\n[2] J. Pons, J. Janer, T. Rode, and W. Nogueira, “Remixing\nmusic using source separation algorithms to improve\nthe musical experience of cochlear implant users,” The\nJournal of the Acoustical Society of America , vol. 140,\nno. 6, pp. 4338–4349, 2016.\n[3] J. Koo, M. A. Martínez-Ramírez, W.-H. Liao, S. Uh-\nlich, K. Lee, and Y . Mitsufuji, “Music mixing style\ntransfer: A contrastive learning approach to disentan-\ngle audio effects,” in ICASSP 2023-2023 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2023, pp. 1–5.\n[4] M. D. Plumbley, S. A. Abdallah, J. P. Bello, M. E.\nDavies, G. Monti, and M. B. Sandler, “Automatic mu-\nsic transcription and audio source separation,” Cyber-\nnetics &Systems , vol. 33, no. 6, pp. 603–627, 2002.\n[5] L. Lin, Q. Kong, J. Jiang, and G. Xia, “A uniﬁed\nmodel for zero-shot music source separation, transcrip-\ntion and synthesis,” in The 22nd International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2021.\n[6] K. W. Cheuk, K. Choi, Q. Kong, B. Li, M. Won, J.-C.\nWang, and Y .-N. H. D. Herremans, “Jointist: Simulta-\nneous improvement of multi-instrument transcription\nand music source separation via joint training,” arXiv\npreprint arXiv:2302.00286 , 2023.\n[7] C. Dittmar, E. Cano, J. Abeßer, and S. Grollmisch,\n“Music information retrieval meets music education,”\ninDagstuhl Follow-Ups , vol. 3. Schloss Dagstuhl-\nLeibniz-Zentrum fuer Informatik, 2012.\n[8] E. Cano, G. Schuller, and C. Dittmar, “Pitch-informed\nsolo and accompaniment separation towards its use in\nmusic education applications,” EURASIP Journal on\nAdvances in Signal Processing , vol. 2014, pp. 1–19,\n2014.\n[9] C. T. Hoopen, “Issues in timbre and perception,” Con-\ntemporary Music Review , vol. 10, no. 2, pp. 61–71,\n1994.[10] H. Scudder, “Probability of error of some adaptive\npattern-recognition machines,” IEEE Transactions on\nInformation Theory , vol. 11, no. 3, pp. 363–371, 1965.\n[11] D. Yarowsky, “Unsupervised word sense disambigua-\ntion rivaling supervised methods,” in 33rd annual\nmeeting of the association for computational linguis-\ntics, 1995, pp. 189–196.\n[12] Q. Xie, M.-T. Luong, E. Hovy, and Q. V . Le, “Self-\ntraining with noisy student improves imagenet classi-\nﬁcation,” in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , 2020, pp.\n10 687–10 698.\n[13] J. Schlüter, “Learning to pinpoint singing voice from\nweakly labeled examples.” in ISMIR , 2016, pp. 44–50.\n[14] G. Meseguer-Brocal, A. Cohen-Hadria, and G. Peeters,\n“DALI: A large dataset of synchronized audio, lyrics\nand notes, automatically created using teacher-student\nmachine learning paradigm,” in The 19th International\nSociety for Music Information Retrieval Conference\n(ISMIR) , 2018.\n[15] S. Keum, J.-H. Lin, L. Su, and J. Nam, “Semi-\nsupervised learning using teacher-student models for\nvocal melody extraction,” in The 21th International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2020.\n[16] E. Benetos, M. Kotti, and C. Kotropoulos, “Musi-\ncal instrument classiﬁcation using non-negative matrix\nfactorization algorithms and subset feature selection,”\nin2006 IEEE International Conference on Acoustics\nSpeech and Signal Processing Proceedings , vol. 5.\nIEEE, 2006, pp. V–V .\n[17] A. Eronen and A. Klapuri, “Musical instrument recog-\nnition using cepstral coefﬁcients and temporal fea-\ntures,” in 2000 IEEE International Conference on\nAcoustics, Speech, and Signal Processing. Proceedings\n(Cat. No. 00CH37100) , vol. 2. IEEE, 2000, pp. II753–\nII756.\n[18] V . Lostanlen and C.-E. Cella, “Deep convolutional net-\nworks on the pitch spiral for musical instrument recog-\nnition,” in The 17th International Society for Music In-\nformation Retrieval Conference (ISMIR) , 2016.\n[19] S. Essid, G. Richard, and B. David, “Hierarchical clas-\nsiﬁcation of musical instruments on solo recordings,”\nin2006 IEEE International Conference on Acoustics\nSpeech and Signal Processing Proceedings , vol. 5.\nIEEE, 2006, pp. V–V .\n[20] Y . Han, J. Kim, and K. Lee, “Deep convolutional neu-\nral networks for predominant instrument recognition in\npolyphonic music,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , vol. 25, no. 1, pp.\n208–221, 2016.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n722[21] Y .-N. Hung and Y .-H. Yang, “Frame-level instrument\nrecognition by timbre and pitch,” in The 19th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , 2018.\n[22] S. Gururani, C. Summers, and A. Lerch, “Instru-\nment activity detection in polyphonic music using deep\nneural networks.” in The 19th International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2018, pp. 569–576.\n[23] S. Gururani, M. Sharma, and A. Lerch, “An attention\nmechanism for musical instrument recognition,” in The\n20th International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2019.\n[24] Y .-N. Hung, Y .-A. Chen, and Y .-H. Yang, “Multi-\ntask learning for frame-level instrument recognition,”\ninICASSP 2019-2019 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2019, pp. 381–385.\n[25] H. Flores Garcia, A. Aguilar, E. Manilow, and\nB. Pardo, “Leveraging hierarchical structures for few-\nshot musical instrument recognition,” in The 22nd In-\nternational Society for Music Information Retrieval\nConference (ISMIR) , 2021.\n[26] L. C. Reghunath and R. Rajan, “Transformer-based en-\nsemble method for multiple predominant instruments\nrecognition in polyphonic music,” EURASIP Journal\non Audio, Speech, and Music Processing , vol. 2022,\nno. 1, p. 11, 2022.\n[27] K. Kim, M. Park, H. Joung, Y . Chae, Y . Hong, S. Go,\nand K. Lee, “Show me the instruments: Musical in-\nstrument retrieval from mixture audio,” in ICASSP\n2023-2023 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) . IEEE,\n2023, pp. 1–5.\n[28] Z. Zhong, M. Hirano, K. Shimada, K. Tateishi, S. Taka-\nhashi, and Y . Mitsufuji, “An attention-based approach\nto hierarchical multi-label music instrument classiﬁca-\ntion,” in ICASSP 2023-2023 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2023, pp. 1–5.\n[29] S. Uhlich, M. Porcu, F. Giron, M. Enenkl, T. Kemp,\nN. Takahashi, and Y . Mitsufuji, “Improving music\nsource separation based on deep neural networks\nthrough data augmentation and network blending,”\nin2017 IEEE international conference on acoustics,\nspeech and signal processing (ICASSP) . IEEE, 2017,\npp. 261–265.\n[30] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz,\n“mixup: Beyond empirical risk minimization,” in In-\nternational Conference on Learning Representations ,\n2018.[31] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell,\nand S. Xie, “A convnet for the 2020s,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2022, pp. 11 976–11 986.\n[32] F.-R. Stöter, S. Uhlich, A. Liutkus, and Y . Mitsufuji,\n“Open-unmix-a reference implementation for music\nsource separation,” Journal of Open Source Software ,\nvol. 4, no. 41, p. 1667, 2019.\n[33] M. Kim, W. Choi, J. Chung, D. Lee, and S. Jung,\n“Kuielab-mdx-net: A two-stream neural network for\nmusic demixing,” arXiv preprint arXiv:2111.12203 ,\n2021.\n[34] S. Rouard, F. Massa, and A. Défossez, “Hybrid trans-\nformers for music source separation,” arXiv preprint\narXiv:2211.08553 , 2022.\n[35] Y . Luo and J. Yu, “Music source separation with band-\nsplit rnn,” arXiv preprint arXiv:2209.15174 , 2022.\n[36] Y . Mitsufuji, G. Fabbro, S. Uhlich, F.-R. Stöter, A. Dé-\nfossez, M. Kim, W. Choi, C.-Y . Yu, and K.-W. Cheuk,\n“Music demixing challenge 2021,” Frontiers in Signal\nProcessing , vol. 1, p. 18, 2022.\n[37] Z. Raﬁi, A. Liutkus, F.-R. Stöter, S. I. Mimilakis,\nand R. Bittner, “Musdb18-hq - an uncompressed\nversion of musdb18,” Aug. 2019. [Online]. Available:\nhttps://doi.org/10.5281/zenodo.3338373\n[38] A. Défossez, “Hybrid spectrogram and waveform\nsource separation,” arXiv preprint arXiv:2111.03600 ,\n2021.\n[39] R. Sawata, S. Uhlich, S. Takahashi, and Y . Mitsufuji,\n“All for one and one for all: Improving music sepa-\nration by bridging networks,” in ICASSP 2021-2021\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2021, pp.\n51–55.\n[40] C.-B. Jeon and K. Lee, “Towards robust music source\nseparation on loud commercial music,” in Proc. of the\n23rd Int. Society for Music Information Retrieval Con-\nference , 2022.\n[41] C. J. Steinmetz and J. D. Reiss, “pyloudnorm: A simple\nyet ﬂexible loudness meter in python,” in 150th AES\nConvention , 2021.\n[42] R. ITU-R, “Itu-r bs. 1770-2, algorithms to mea-\nsure audio programme loudness and true-peak au-\ndio level,” International Telecommunications Union,\nGeneva , 2011.\n[43] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” arXiv preprint arXiv:1412.6980 ,\n2014.\n[44] I. Loshchilov and F. Hutter, “Decoupled weight decay\nregularization,” in International Conference on Learn-\ning Representations , 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n723[45] E. Vincent, R. Gribonval, and C. Févotte, “Perfor-\nmance measurement in blind audio source separation,”\nIEEE transactions on audio, speech, and language pro-\ncessing , vol. 14, no. 4, pp. 1462–1469, 2006.\n[46] F. Stöter, A. Liutkus, and N. Ito, “The 2018 signal sep-\naration evaluation campaign,” in International Confer-\nence on Latent Variable Analysis and Signal Separa-\ntion. Springer, 2018, pp. 293–305.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n724"
    },
    {
        "title": "Algorithmic Harmonization of Tonal Melodies Using Weighted Pitch Context Vectors.",
        "author": [
            "Peter van Kranenburg",
            "Eoin Kearns"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265307",
        "url": "https://doi.org/10.5281/zenodo.10265307",
        "ee": "https://zenodo.org/records/10265307/files/000046.pdf",
        "abstract": "Most melodies from the Western common practice period have a harmonic background, i.e., a succession of chords that fit the melody. In this paper we provide a novel approach to infer this harmonic background from the score notation of a melody. We first construct a pitch context vector for each note in the melody. This vector summarises the pitches that are in the preceding and following contexts of the note. Next, we use these pitch context vectors to generate a list of candidate chords for each note. The candidate chords fit the pitch context of a given note each with a computed strength. Finally, we find an optimal path through the chord candidates, employing a score function for the fitness of a given candidate chord. The algorithm chooses one chord for each note, optimizing the total score. A set of heuristics is incorporated in the score function. The system is heavily parameterised, extremely flexible, and does not need training. This creates a framework to experiment with harmonization of melodies. The output is evaluated by an expert survey, which yields convincing and positive results.",
        "zenodo_id": 10265307,
        "dblp_key": "conf/ismir/KranenburgK23",
        "keywords": [
            "melodies",
            "Western common practice period",
            "harmonic background",
            "score notation",
            "pitch context vector",
            "candidate chords",
            "score function",
            "heuristics",
            "expert survey",
            "convincing and positive results"
        ],
        "content": "ALGORITHMIC HARMONIZATION OF TONAL MELODIES USING\nWEIGHTED PITCH CONTEXT VECTORS\nPeter van Kranenburg\nMeertens Institute, Utrecht University\npeter.van.kranenburg@meertens.knaw.nlEoin Kearns\nMeertens Institute\neoin.kearns@meertens.knaw.nl\nABSTRACT\nMost melodies from the Western common practice period\nhave a harmonic background, i.e., a succession of chords\nthat ﬁt the melody. In this paper we provide a novel ap-\nproach to infer this harmonic background from the score\nnotation of a melody. We ﬁrst construct a pitch context\nvector for each note in the melody. This vector summarises\nthe pitches that are in the preceding and following contexts\nof the note. Next, we use these pitch context vectors to gen-\nerate a list of candidate chords for each note. The candidate\nchords ﬁt the pitch context of a given note each with a com-\nputed strength. Finally, we ﬁnd an optimal path through\nthe chord candidates, employing a score function for the\nﬁtness of a given candidate chord. The algorithm chooses\none chord for each note, optimizing the total score. A set\nof heuristics is incorporated in the score function. The sys-\ntem is heavily parameterised, extremely ﬂexible, and does\nnot need training. This creates a framework to experiment\nwith harmonization of melodies. The output is evaluated\nby an expert survey, which yields convincing and positive\nresults.\n1. INTRODUCTION\nOne of the essential aspects of Western folk music is that\nit is in oral circulation among practitioners regardless of\nformal music training. As such, the transmitted music is\nexpected to conform to melodic patterns which belong to\nWestern music traditions. This is most tangible in the per-\nception of rules of tonality, including the perception of sta-\nble scale tones, modes, and key centres [1]. These factors\ndictate the implied harmonic movement within the melody.\nDetecting this implied harmony is an integral part of the\naccompaniment of folk music. With this knowledge, it is\npossible to create musically meaningful harmonic progres-\nsions, using symbolic chord representations to accompany\na melody.\nIn this paper, our aim is to explicitly design a model\nof how to generate a sequence of accompanying chords\nfor a given melody, such that e.g., a guitarist could play\n© P. van Kranenburg and E. Kearns. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). Attri-\nbution: P. van Kranenburg and E. Kearns, “Algorithmic Harmonization\nof Tonal Melodies using Weighted Pitch Context Vectors”, in Proc. of\nthe 24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.along. Most of the recent work on this task involves ma-\nchine learning in which a model is trained on a set of ex-\namples. In contrast, an essential aspect of our approach is\nto explicitly incorporate musical expert knowledge into the\nmodel. Our model is heavily parameterized. This has the\nadvantage of allowing the user to have full control over the\nprocess. A disadvantage could be that the resulting model\nlacks the ﬂexibility to handle various situations, which of-\nten is a reason to train a neural network instead of hand-\ncrafting a model. Our results show, however, that our cur-\nrent model is capable of generating convincing chord se-\nquences for a given melody.\nThe model can be employed in a wide range of appli-\ncations. It allows a musician to quickly obtain a suitable\naccompaniment for a given melody. This can be accom-\nplished by using the default parameter settings that are es-\ntablished in this paper, but the model also allows to tune\nthe parameters to get a certain desired effect. In Section 5\nof this paper we provide an example in which the number\nof generated chords greatly varies, while each generated\nchord sequence is acceptable to accompany the melody.\nThus, the generated harmony can be adjusted to various\nlevels of mastering an instrument.\nFrom a music theory perspective, our model can be\nconsidered an experimental framework to explore general\nprinciples of harmonization. In this approach, the model is\nused to better understand these principles. It is extremely\ninstructive to add a heuristic to the model, or to adjust a\nparameter, and to examine the cases in which this leads to\nstrong chord sequences, but even more so to examine the\ncases that are not acceptable. These are conditions under\nwhich the general rule apparently fails. In the current pa-\nper, we do not elaborate on this use of our model, but it\nis an important affordance that we do not want to be left\nunmentioned.\nWe also can imagine the system being used in an artis-\ntic way, rather than to just generate an accompaniment for\npractical use. In the current implementation, we incorpo-\nrate well-established principles of harmonization, but it is\nvery well possible to include other heuristics that generate\nchord sequences that, although not adhering to the general\nprinciples of Western tonality, could be considered an artis-\ntic contribution, or an inspiration for a new composition.\nFinally, we mention the possible educational use of the\nmodel. By exploring the generated chord sequences, stu-\ndents can get ideas to improve or enrich their own compo-\nsitions or improvisation.3912. RELATED WORK\nMultiple approaches have been taken for the task of au-\ntomatic harmonization [2]. Early applications of formal\ngrammars and rule-based algorithms for automatic harmo-\nnization [3, 4] mainly sought to compose chorales in the\nstyle of Bach. This was achieved by harmonizing the so-\nprano melody line using a set of rules to ascertain harmonic\nchoices. These rules and heuristics are informed by obser-\nvation of chorales, and enhanced by rules found in trea-\ntises. The resulting systems output successful harmoniza-\ntions of existing melodies, as well as new compositions.\nContext free grammars have found use for this task.\nKoops [5] adopts this approach in his HarmTrace and\nFHarm models to derive the harmonic function of a chord\nin its tonal context according to a set of predeﬁned rules.\nTemperley [6] proposed a rule-based algorithm to har-\nmonize a melody by dividing the piece temporally into seg-\nments (chord spans). All possible roots are then assigned\na score according to a set of four rules. The model prefers\nroot relations which best conform to the circle of ﬁfths.\nThe model also predominantly chooses chord spans which\nbegin on the metrical downbeat, and identiﬁes and prefers\nornamental dissonances which can be resolved in the sub-\nsequent chord span. While approach is related to Temper-\nley’s algorithm, it is more ﬂexible as it does not hard-code\none musical model, but instead allows basically any kind of\nmusical preference by redeﬁning the chord transition scor-\ning function. Our approach is also more practical since it\nnot only generates a sequence of root notes, but also the\nchord qualities.\nMost of the more recent approaches are based on some\nform of machine learning, sometimes explicitly stating\nthe aim to include a “minimal use of music knowledge”\n[7]. These approaches include Statistical Grammar learn-\ning [7], Hidden Markov Models [8–10], and neural net-\nworks [11, 12]. [13] presents a hybrid approach based on\nMarkov chains, combining a music theoretic framework\nwith learning from data. Our approach is distinct in that\nit does not require learning at all, and thus allows for full\ncontrol over the process of generating the sequences.\n3. DATA\nThe algorithm is evaluated using MTC-FS-INST-2.0,\nwhich forms part of the Meertens Tune Collections [14].\nThe data set consists of c. eighteen thousand melodies,\nboth vocal and instrumental, collected from Dutch sources.\nThe melodies have a variety of time signatures and\nmodes. We use the pre-computed features as distributed\nin MTCFeatures.1Since our model relies on notated me-\nter, we only use the melodies with a meter.\n3.1 Music Representation\nWe represent the melodies as sequences of feature val-\nues, one value per note. In this paper, we use three fea-\ntures as provided by MTCFeatures, namely pitch40 ,\n1https://zenodo.org/record/3551003/noteheads.s2/clefs.G/accidentals.sharp68\n-1/3 beatinsong0.25 beatstrength8 pitch40/flags.u3/noteheads.s2\n01.025/noteheads.s2\n2/30.2537/noteheads.s2\n5/60.12537/noteheads.s2\n10.537/noteheads.s2\n4/30.2537/noteheads.s2\n5/30.2537/noteheads.s2\n21.031/noteheads.s2\n8/30.258/flags.d3/noteheads.s2\n30.58/noteheads.s2\n11/30.252/flags.d3/noteheads.s2\n41.037/dots.dot/rests.2/noteheads.s2\n17/30.2531/flags.u3\n/clefs.G/accidentals.sharp4\n/noteheads.s2\n61.025/noteheads.s2\n20/30.2537/noteheads.s2\n41/60.12537/noteheads.s2\n70.537/noteheads.s2\n22/30.2537/noteheads.s2\n23/30.2537/noteheads.s2\n81.031/noteheads.s2\n26/30.258/flags.d3/noteheads.s2\n90.58/noteheads.s2\n29/30.252/flags.d3/noteheads.s2\n101.037/dots.dot/rests.2/noteheads.s2\n35/30.258/flags.d3\n/clefs.G/accidentals.sharp7/noteheads.s2\n121.08/noteheads.s2\n38/30.258/noteheads.s2\n77/60.1258/noteheads.s2\n130.58/noteheads.s2\n40/30.252/noteheads.s2\n41/30.2537/noteheads.s2\n141.02/noteheads.s2\n44/30.2514/flags.d3/noteheads.s2\n150.514/noteheads.s2\n47/30.252/flags.d3/noteheads.s2\n161.037/noteheads.s2\n50/30.2537/flags.d3/noteheads.s2\n170.58/dots.dot/noteheads.s2\n35/20.1252/noteheads.s2\n53/30.2537\n/clefs.G/accidentals.sharp10\n/noteheads.s2\n181.031/noteheads.s2\n56/30.2531/flags.u3\n/noteheads.s2\n190.531/noteheads.s2\n59/30.252/flags.d3/noteheads.s2\n201.037/noteheads.s2\n62/30.2537/flags.d3/noteheads.s2\n210.52/noteheads.s2\n65/30.252/flags.d3/noteheads.s2\n221.08/dots.dot/noteheads.s2\n230.514/dots.dot/noteheads.s2\n241.08/noteheads.s2\n74/30.258/flags.d3/noteheads.s2\n250.52/noteheads.s2\n76/30.2537/noteheads.s2\n77/30.2531/noteheads.s2\n261.025/dots.dot/rests.2\nFigure 1 . Example melody with the values for pitch40 ,\nbeatstrength , andbeatinsong per note.\nbeatstrength , andbeatinsong , which gives on-\nset times in units of the beat. The base-40 representation\nof pitch preserves the pitch spelling [15]. It includes 40\nvalues per octave representing 40 possible pitches starting\nwith C♭♭and ending with B×. We map all pitch values into\none octave. We use the encoding as designed by Hewlett\nwith one adaptation: we give the ﬁrst pitch (C ♭♭) index 0\ninstead of index 1, which has a practical advantage when\ndoing the implementation in Python.\nWe use the beatstrength as computed by the music21\nmeter model [16, 17]. Music21 is a Python library for pro-\ncessing symbolic musical scores. We heavily use this li-\nbrary. In the meter model of music21, a beatstrength is\ncomputed for each note, which indicates the metric weight\nat the moment of onset of the note. The main accent in the\nmeasure gets value 1.0, secondary accents get value 0.5,\nlower metric positions get 0.25, 0.125, etc. Figure 1 shows\nan example.\n4. METHOD\nOur approach to generate a sequence of chords for a given\nmelody consists of three stages: First, we construct for\neach note a vector summarising the pitch context of that\nnote. Second, we generate for each note a list of poten-\ntial chords from the pitch context vector. Each chord gets\na score indicating the extent to which it ﬁts the pitch con-\ntext. Finally, for each note, we choose one of the candi-\ndate chords, based on its score, and on a chord transition\nscore, such that the sum of all transition scores across the\nsequence of chords is maximized.\nThe evaluation also consists of several steps. First, we\ntune the various parameters on a randomly chosen set of\nmelodies. Next, we use the best parameter setting to gen-\nerate chord sequences for an independent, disjoint set of\nmelodies. We then provide six music experts with the re-\nsults and to provide us with a rating of each harmonization\non a ﬁve-level rating scale. Finally, we use statistics to ex-\nplore and summarise the responses.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n392In the following of this section, we will explain each of\nthese steps in detail.\n4.1 Weighted Pitch Context Vectors\nFor a given note, which we indicate as the focus note , we\nconsider both a preceding and a following context. These\nconsist of the sequences of notes that are preceding, and\nrespectively following the focus note. For both the pre-\nceding and the following context we construct a weighted\npitch context vector . Each of these vectors has 40 elements\ncorresponding to the 40 pitches in base-40 representation.\nThe value of each of the elements represents the “amount”\nof the corresponding pitch that is present in the context of\nthe focus note. The full context vector is a 80-dimensional\nvector which is the concatenation of the preceding and fol-\nlowing context vectors.\n4.1.1 Length of Contexts\nChoosing the length of the contexts is not straightforward.\nIt is, in fact, an important parameter in our model. The\nmusic21 meter model provides metric information for each\nnote, notably concerning the beat and the beatstrength of\na note (as explained in Section 3.1). This allows us to ex-\npress the length of the context as a number of beats. This\nseems a good approach since the beat is a perceptually\nmeaningful unit. Alternatives would be a ﬁxed number of\nnotes or a certain amount of score time. We did not explore\nthese for the current study.\nWe experimented with different values for the context\nlength, as well as with a variable context length based on\nthe beatstrengths of the surrounding notes. We found that\nthe latter approach, with variable length, yielded the best\nresults in terms of acceptable chord sequences. In our re-\nsulting implementation the context length is computed as\nfollows. We start with the focus note. For the preceding\ncontext, we consider the notes before the focus note in re-\nversed order, starting with the note directly before the fo-\ncus note, and we keep adding the notes to the context until\n(and including) we reach a note with beatstrength 1.0. For\nthe following context, the same procedure is followed, ex-\ncept that the ﬁrst encountered note with beatstrength of 1.0\nis not included in the context. In our implementation, there\nis also a parameter whether to include the focus note itself\ninto the context or not. Since the current aim is to generate\na chord for the focus note, we always add the focus note to\nthe contexts.\nThe consequence of this procedure is that for a note on\nthe main accent of the measure (i.e., the ﬁrst note), the pre-\nceding context is the entire previous measure, and the fol-\nlowing context is the remainder of the measure of the focus\nnote. In contrast, for notes that are not on the main ac-\ncent, the preceding context includes all the previous notes\nin the same measure, while the following context includes\nthe remaining notes in the measure. To a certain extent,\nthis accounts for harmonic progression at different metric\nlevels.4.1.2 Weighting of Context Notes\nThe contribution of each context note to the value of\nthe corresponding pitch in the pitch context vector is de-\ntermined by two components: the metric weight (beat-\nstrength) of the context note, and the distance to the focus\nnote.\nIntuitively, the duration of a context note has an impact\non its importance in the context of the corresponding focus\nnote. Therefore, we do not simply take the beatstrength\nof the moment of onset of the context note as weighting\nfactor. Instead, we compute a metric grid, which is a suc-\ncession of evenly spaced moments in score time. The basic\nunit of the grid, i.e., the distance between two subsequent\npositions in the grid, is the greatest common divisor of all\nnote durations. Therefore, each note of the melody starts\nat a position in the grid, and the “span” of the note mostly\nincludes several grid positions. The metric weighting fac-\ntor of a context note is the sum of metric weights (beat-\nstrengths) of all positions of the metric grid that are in the\n“span” of the note. Thus, the duration of the note, as well\nas the metric importance of the note are incorporated in the\nweighting. This approach also accounts for syncopation.\nDuring the span of a syncopated note, a grid-position with\nhigher metric weight than the metric weight at the start of\nthe note occurs. This is included in the sum.\nAlso intuitively, the further a note is away from the fo-\ncus note, the lower the importance in the context of the fo-\ncus note. In our model, we use a linearly decreasing win-\ndowing function. The metric weighting factor of a given\ncontext note is multiplied by the value of this window func-\ntion at the position of the onset of the context note. The\nvalue of the window function during the span of the focus\nnote is 1.0, and is linearly decreasing towards the end of\nthe context. The value at the end of the context is a pa-\nrameter in our model. We set this to a value slightly higher\nthan 0.0 in order to have some inﬂuence from the notes that\nare at the outer boundaries of the contexts.\n4.2 Generating Candidate Chords\nOnce we have computed a pitch context vector consisting\nof a preceding and following pitch context for each of the\nnotes in a melody, we use these vectors to generate a set of\ncandidate chords for each of the notes in a melody.\nIn our current implementation, we consider four types\nof chords: diminished triad, minor triad, major triad, and\ndominant seventh chord, and we consider three types of\ncontext: preceding context, following context, and full\ncontext. The full context just is a superposition, i.e., an\nelement-wise sum, of the preceding and following con-\ntexts. Discerning these three types of contexts is a crucial\nelement in our model. It allows the method to determine\nthe position of a chord change. If the set of chords that\nis implied by the preceding context is sufﬁciently differ-\nent from the set of chords that is implied by the following\ncontext, a chord change is likely, while the presence of a\nchord that sufﬁciently ﬁts the full context likely results in\na continuation.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n393/noteheads.s2\ndel/noteheads.s2\nbrug/noteheads.s2\nhet/noteheads.s2\nvon/noteheads.s2/noteheads.s2\ngie liep/noteheads.s2\nhet/noteheads.s2\nmeis/timesig.C44/noteheads.s2 /clefs.G/accidentals.sharp\nToen o daar/noteheads.s2/noteheads.s2/noteheads.s2\nver/noteheads.s2\nje\nPreceding context Following contextfocusC♭♭ C♭ C C ♯ C♯♯ G♭ G A B B ♯ B♯♯\n⇢ ⇢ ⇢\n⇢ ⇢ ⇢\nC♭♭ C♭ C C ♯ C♯♯ D E E ♯ B B ♯ B♯♯wpc pre\nwpc post\nFigure 2 . Example of a Pitch Context Vector. The vector consists of two parts, wpcpreandwpcpost, which represent\nrespectively the preceding and the following context. The full Pitch Context Vector is a concatenation of the two parts.\nEach element in the vector gets a value representing the ‘amount’ of the corresponding pitch that is present in the context.\nThus, considering 40 possible root notes, we have 160\n(40*4) possible chords for each of the preceding, follow-\ning, and full contexts.\n4.2.1 Candidate Chord Score and Strength\nFor each focus note, we construct a 120*4 matrix, con-\ntaining a score for each possible chord in each possible\ncontext.\nThe score of a chord with respect to a context vector is\ndetermined by two factors: ﬁrst, the extent to which the\nchord pitches match the pitches in the context vector, and,\nsecond, whether the root note of the chord is present in\nthe local scale. We will explain these two factors in the\nfollowing.\nFor each chord quality (diminished, minor, major, and\ndominant), a chord mask is deﬁned. This is a 40-\ndimensional binary vector with ones at the positions of the\ncorresponding chord tones. E.g., for a major chord on C ♭♭\nthe positions 0, 12, and 23, corresponding with C ♭♭, E♭♭and\nG♭♭are assigned value 1, while other positions get value 0.\nTo compute the score of this chord for a given context, we\nmultiply the mask element-wise with the pitch context vec-\ntor, and we sum the resulting values. The resulting value\nrepresents the overlap between the context and the chord.\nTo compute the scores for all possible root notes, we\nsubsequently rotate the mask over all possible 40 shifts,\nand compute for each shift the sum of products. We do\nthis for the preceding context vector, the following context\nvector, and the full context vector. For the repertoire we\nhave, we do not perform all 40 shifts, we only take into\naccount natural root notes, root notes with one ﬂat, and\nroot notes with one sharp.\nNext to these scores, we also compute a strength value\nfor each of the possible chords. The strength takes a value\nbetween 0 and 1, and is computed as the ratio of the sum of\nthe pitch context values for the chord tones (as determined\nby the mask) and the sum of all pitch context values. E.g, if\na pitch context vector has some weight for C, E, G, and A, a\nC major chord would get a high score, but a strength lower\nthan 1.0, because there is also weight for the A, which is\nnot a chord tone.\nTo obtain a single score for each chord candidate, wesimply multiply the score with the corresponding strength.\nThis implies a penalty for non-chord tones within the pitch\ncontext.\nWe normalize the score matrix for a given focus note\nby dividing all scores by the highest score. Thus, the best\nﬁtting candidate always has a score of 1.0.\n4.2.2 Local Scale\nA second factor that determines the possible candidate\nchords is the local scale. As with the chord mask, we de-\nﬁne a scale as a 40-dimensional binary vector. The ele-\nments with value 1 are the scale tones. For each note in the\nmelody we derive a local scale vector. This records the al-\nterations of the stemtones that are ‘in use’ at that position in\nthe melody. For each stemtone ∈{A,B,C,D,E,F,G },\nwe look for the occurrence closest to the focus note, ac-\ncepting all possible alterations, and we record the alter-\nation in the scale vector. This accounts for modulations.\nE.g., if in a melody in D major a G ♯occurs, which is even-\ntually cancelled back to a G, the notes that are closer to\nthe G♯have a 1 at position 26 in the local scale vector (the\nbase40 representation of G ♯) while the notes closer to the\nG have a 1 at position 25.\nOne problem is posed if a stemtone is missing alto-\ngether in a melody. For example, the melody in Figure 1\nlacks the note F. The key signature suggests a F ♯, but that\nis not available to our algorithm. In these cases, we add\nthe tone with the most likely alteration to the scale vector.\nFor sharps, we ﬁnd this by following the circle of ﬁfths\nupwards from the missing tone and check the alteration of\nthe next tone. For example, if a C ♯is present in the local\nscale, we infer that the scale should have a F ♯, and not a F\nnatural. For ﬂats, we do the same, but we inspect the circle\nof ﬁfths in reversed order. For edge cases, we include both\nthe natural and altered tone in the scale. E.g., if stemtone G\nis missing throughout the melody, and the scale does have\na C♯and a D natural, we include both the G natural and the\nG♯as possible scale tones in the local scale vector.\nWe use the local scale for a given focus note to elimi-\nnate those chord candidates that have a root which is not in\nthe scale, by setting its score to 0.0. E.g., a C ♯diminished\nchord ﬁts a context vector with weight for pitches E andProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n394G, but we eliminate this candidate for a melody in C ma-\njor because C ♯is not in the scale (except when sometime\nduring the melody the C is temporarily raised).\n4.3 The Chord Transition Score\nThe result of the procedure as described in the previous\nsection is a sequence of matrices, one for each melody\nnote, containing a score for each possible chord for that\nmelody note. The next challenge is to choose one chord\nfor each melody note out of these 120*4 possibilities. For\nthat, we employ a chord transition scoring function (T RS),\nwhich computes a score for a given succession of chords,\nc1andc2for two subsequent melody notes, n1andn2.\nThis transition scoring function can be considered a\nmodel of what would be a good chord transition. We im-\nplement this function as a series of heuristics, each penal-\nizing the score if an aspect of the transition is undesired.\nWe discern two kinds of penalty which could be described\nas a “total ban” and a “discouragement” respectively. For\na total ban, we assign a very low score (-10 in our imple-\nmentation), which forbids the transition in almost all cases.\nFor a discouragement, we multiply the score by multiplier\n∈[0,1]. The lower the multiplier, the higher the penalty for\nthe undesired aspect of the chord transition. In our model\nwe include the following heuristics.\n• The initial transition score is the candidate score of c2\nfor noten2, as computed in the previous step.\n• If the root note of c2differs from the root of c1, multiply\nwith 0.8. This stimulates continuation of a chord.\n• If the root notes of both chords are the same, but the\nchord qualities differ, multiply with 0.1. Except for a\nchange from major to dominant.\n• For a root movement other than a prime, a fourth, or a\nﬁfth, multiply by 0.75. Root movements of fourths and\nﬁfths generally account for good harmonic progression.\n• Ifc1is a dominant chord and the root of c2is not a\nfourth higher, multiply by 0.1. We strongly want a V-I\nrelation after a dominant chord.\n• If the root of c2is a fourth up, and c1is not major or\ndominant, multiply by 0.8.\n• Ifc1is diminished, and the root of c2is not a semitone\nup, multiply by 0.1. We strongly want a VII-I relation\nafter a diminished chord.\n• If the beatstrength of n2is below a threshold, do not\nallow a chord change (score -10), except for a transi-\ntion from major to dominant with the same root. The\nthreshold is determined by the meter. For 2/4, 2/8, and\n2/2 meter we take 0.25, for all other meters 0.5.\n• Do not allow a chord change (score -10) if n2is not a\nchord tone in c2, and if the beatstrength of n2is 0.5 or\nhigher. The seventh of a dominant chord is not consid-\nered a chord tone. On strong metric positions, we want\nchord tones in the melody.\n• As an exception to the previous rule, do always allow a\nchord change to c2if the next note after n2is a chord\ntone ofc2, and has a lower beatstrength than n2. This\nallows for appoggiaturas.\n• Do not allow (score -10) a chord that starts at a lowbeatstrength (<1.0) to continue past a note with higher\nbeatstrength. Except for a chord that starts on an up-\nbeat. This prevents chord syncopation.\n• If the ﬁnal root change is not a fourth up, or a ﬁfth down,\nmultiply with 0.1.\n• If the ﬁnal root change is a ﬁfth up (a plagal cadence),\nmultiply with 0.8.\n• Only allow the root or the third of c2as melody note if\nn2is the ﬁnal note of the melody. If this is not the case\nassign score -10. If the ﬁnal note is the third, multiply\nwith 0.75.\n4.4 Finding the Optimal Sequence\nWe designed an algorithm that optimizes the score for a se-\nquence of chord transitions. It takes the sequence of chord\nscore matrices as input and uses the chord transition scor-\ning function. Algorithm 1 shows the pseudo code of our\nalgorithm. We ﬁll a matrix, Score, which contains for each\nnote, and for each possible chord, the total score of the\nchord sequence up until that note and that chord. In paral-\nlel, we ﬁll a traceback matrix, Trace, which for each note,\nand for each chord, points to the chord of the previous note\nwhich is the previous chord in the sequence (i.e., max-\nimises the total score of the chord sequence). After both\nthe Score and Trace matrices are ﬁlled, we ﬁnd the chord\nsequence by ﬁnding the chord with the maximal score for\nthe ﬁnal note, and following the trace back according to\nthe pointers in the Trace matrix.\nAlgorithm 1 Algorithm to ﬁnd the optimal sequence of\nchord transitions, in which lis the length of the melody in\nnumber of notes, Cand is the sequence of matrices with\nscores for the chord candidates, and T RS is the Chord\nTransition Scoring Function as deﬁned in Section 4.3.\nRequire: Cand : ARRAY[ l][120][4] of ﬂoat\nfunction HARM SCORE (Cand)\ndeclare Score : ARRAY[ l][120][4] of ﬂoat\ndeclare Trace : ARRAY[ l][120][4][2] of int\nScore[0]←Cand[0]\nfornin{1,2,...,l−1}do\nixs1←indices of cells in Cand[ n−1] > 0\nixs2←indices of cells in Cand[ n] >0\nfor(p2,c2)in ixs2 do\ndeclare S : ARRAY[120][4] of ﬂoat\nfor(p1,c1)in ixs1 do\ntrs←TRS(Cand,p1,c1,p2,c2)\nS[p1][c1]←Score[n−1][p1][c1]+trs\nend for\n(pm,cm)←argmax (S)\nScore[n][p2][c2]←max(S)\nTrace[n][p2][c2]←(pm,cm)\nend for\nend for\nreturn Score, Trace\nend functionProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n395/noteheads.s2/clefs.G/accidentals.flat /timesig.C44\nFFF/noteheads.s2\nBb/noteheads.s2\nF/noteheads.s2/noteheads.s2\nCC/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nC/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nFF/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nC7GmBb/noteheads.s2/noteheads.s2\nF/noteheads.s1\nC/noteheads.s2\nFDmF/noteheads.s2/noteheads.s2\nC/noteheads.s1\nF\n/clefs.G/accidentals.flat7\n/noteheads.s2\nCC/noteheads.s2/noteheads.s2/noteheads.s2\nF/noteheads.s2\nC/noteheads.s2/noteheads.s2\nFF/noteheads.s2/noteheads.s1/noteheads.s2\nGmBb/noteheads.s2/noteheads.s2\nF/noteheads.s1\nC/noteheads.s2\nDmF/noteheads.s2/noteheads.s2\nC/noteheads.s1\nF/noteheads.s2\nCC/noteheads.s2/noteheads.s2/noteheads.s2\nF/noteheads.s2\nC/noteheads.s2/noteheads.s2\nFF/noteheads.s2/noteheads.s1\n...\nFigure 3 . Example of harmonic progressions at various\nlevels of abstraction.\n4.5 Evaluation\nTo evaluate our algorithm, we ﬁrst tuned the various pa-\nrameters ourselves by inspecting the parameter space and\nchose settings which seemed to yield good results. The\nvalues as reported in Section 4.3 are the result of this pro-\ncess.\nNext, we randomly chose another unrelated set of 50\nmelodies and computed chord sequences for these using\nthe parameter values from the previous step. We then asked\nsix music experts to evaluate each harmonization. All eval-\nuators are practicing musicians on a professional level, and\nhave extensive experience in musical analysis. They were\ngiven a ﬁve level scale and a set of directions in order to\nrate the harmonizations:\n1. Bad. Numerous basic mistakes.\n2. Somethings are good but contains a number of in-\ncorrect chord choices.\n3. Largely okay, small number of incorrect chord\nchoices.\n4. Acceptable harmonization.\n5. Excellent harmonization. No improvements to be\nmade.\nEvaluators were also given a set of directions on how to\nrate the harmonizations. They were asked to judge to what\nextent the chords ﬁt the melody, with an emphasis on the\ncorrectness of chords with regards to the local context, as\nopposed to creativity. They were not to take voice leading\ninto consideration for the chord correctness, as the bass\nline is not modelled in this version of the algorithm.\nWe then use these ratings to compute inter-rater agree-\nment and explore the extremes.\n5. RESULTS\n5.1 Parameter Exploration\nExploring the parameter space of our model is an interest-\ning endeavor which appears meaningful in itself. It allows\na better understanding of general textbook rules for harmo-\nnizing melodies. By implementing and manipulating these\nprinciples in our scoring function we can observe the im-\npact of the rigorous application of these principles.\nAs an example, there are various ways to inﬂuence the\nchange rate of chords. Figure 3 shows three sequences of\nchords at different levels of abstraction. For the middle\nsequence we used the default parameters as established in\n1 2 3 4 5020406080100FrequencyFigure 4 . Distribution of ratings.\nthe previous sections. The other sequences have been ob-\ntained by changing the following parameters with respect\nto the defaults. The top sequence is generated by tolerating\nchord changes at every metric level, and by not penalizing\nroot changes. For the bottom sequence we set the context\nlengths to the length of the entire melody, i.e., all preceding\nnotes are in the preceding context and all following notes\nare in the following context of a given note. These three\nsequences show which harmonies are implied by the same\nmelody at different time scales. This could be employed\nin a hierarchic strategy of harmonization, by e.g. ﬁrst gen-\nerating a sequence on a high level to ﬁnd modulations and\nextended harmonic sections, and subsequently using that\nhigh level sequence as a background for the selection of\nmore ﬁne-grained chord sequences.\n5.2 Expert Ratings\nFigure 4 shows the distribution of the ratings of the ex-\nperts. The average over all ratings is 3.52 and the standard\ndeviation is 1.05. It can be observed that only a minority\nof the harmonizations got a rating lower than 3. Only one\nharmonization (no. 48) has a highest rating of 2 across the\nraters, and only six have a highest rating of 3. All 45 others\ngot a 4 or 5 as highest rating. 22 sequences got a 1 or 2 as\nlowest rating, and 28 sequences 3 or higher. It appears that\nour algorithm produces an acceptable output, but there are\nstill some issues to address. Some problems we observed\nare related to tonality, e.g., starting and ending in a differ-\nent key (mostly the parallel), or including a leading tone\nat inappropriate places. Also, a low harmonic movement\nmight be unsatisfactory.\n6. CONCLUDING REMARKS\nWe presented a successful approach to generate a sequence\nof chords to accompany a folk-like melody by leveraging\nmusical expert knowledge and a dynamic programming al-\ngorithm to ﬁnd an optimal trace through the chord space.2\nThere are many directions to further build on the current\nmodel. We plan to address the observed shortcomings in a\nnext version. Our framework can be used to explore theory\non harmonization or to model implied harmony. It also\ncan serve as tool in educational settings, and of course to\ngenerate a accompaniment for a performance.\n2The full code of our implementation as well as the test set, the ex-\npert ratings, and a demo are available at: https://github.com/\npvankranenburg/ismir2023 .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3967. ACKNOWLEDGEMENTS\nThis work has been enabled by the H2020 Project Poli-\nfonia: a digital harmoniser for musical heritage knowl-\nedge funded by the European Commission Grant number\n101004746.\n8. REFERENCES\n[1] J. Bharucha, “Anchoring effects in music: The reso-\nlution of dissonance,” Cognitive Psychology , vol. 16,\nno. 4, pp. 485–518, 1984.\n[2] D. Makris, I. Karydis, and S. Sioutas, “Automatic\nmelodic harmonization: An overview, challenges and\nfuture directions,” in Trends in Music Information\nSeeking, Behavior, and Retrieval for Creativity . IGI\nGlobal, 06 2016, pp. 146–165.\n[3] M. Baroni and C. Jacoboni, “Computer generation of\nmelodies: Further proposals,” Computers and the Hu-\nmanities , pp. 1–18, 1983.\n[4] K. Ebcio ˘glu, “An expert system for harmonizing four-\npart chorales,” Computer Music Journal , vol. 12, no. 3,\npp. 43–51, 1988.\n[5] H. V . Koops, J. P. Magalhães, and W. B. de Haas, “A\nfunctional approach to automatic melody harmonisa-\ntion,” in Proceedings of the First ACM SIGPLAN Work-\nshop on Functional Art, Music, Modeling amp; De-\nsign. New York, NY , USA: Association for Comput-\ning Machinery, 2013, pp. 47–58.\n[6] D. Temperley, “An algorithm for harmonic analy-\nsis,” Music Perception: An Interdisciplinary Journal ,\nvol. 15, no. 1, pp. 31–68, 1997.\n[7] D. Ponsford, G. Wiggins, and C. Mellish, “Statistical\nlearning of harmonic movement,” Journal of New Mu-\nsic Research , vol. 28, no. 2, pp. 150–177, 1999.\n[8] J.-F. Paiement, D. Eck, and S. Bengio, “Probabilistic\nmelodic harmonization,” in Canadian Conference on\nAI, 2006.\n[9] I. Simon, D. Morris, and S. Basu, “Mysong: automatic\naccompaniment generation for vocal melodies,” in CHI\n’08: Proceedings of the SIGCHI Conference on Hu-\nman Factors in Computing Systems , 2008, pp. 735–\n734.\n[10] S. A. Raczy ´nski, S. Fukayama, and E. Vincent,\n“Melody harmonization with interpolated probabilis-\ntic models,” Journal of New Music Research , vol. 42,\nno. 3, pp. 223–235, 2013.\n[11] H. Lim, S. Ryu, and K. Lee, “Chord generation from\nsymbolic melody using blstm networks,” in 18th Inter-\nnational Society for Music Information Retrieval Con-\nference , 2017, pp. 621–627.[12] Y .-C. Yeh, W.-Y . Hsiao, S. Fukayama, T. Kita-\nhara, B. Genchel, H.-M. Liu, H.-W. Dong, Y . Chen,\nT. Leong, and Y .-H. Yang, “Automatic melody harmo-\nnization with triad chords: A comparative study,” Jour-\nnal of New Music Research , vol. 50, no. 1, pp. 37–51,\n2021.\n[13] C.-H. Chuan and E. Chew, “Generating and evaluating\nmusical harmonizations that emulate style,” Computer\nMusic Journal , vol. 35, no. 4, pp. 64–82, 2011.\n[14] P. Van Kranenburg and M. De Bruin, “The meertens\ntune collections: Mtc-fs-inst 2.0,” Meertens Institute,\nAmsterdam, Meertens Online Reports 2019-1, 2019.\n[15] W. B. Hewlett, “A base-40 number-line representation\nof musical pitch,” Musikometrika , vol. 4, pp. 1–14,\n1992.\n[16] M. S. Cuthbert and C. Ariza, “Music21: A toolkit for\ncomputer-aided musicology and symbolic music data,”\ninProceedings of the 11th International Conference on\nMusic Information Retrieval (ISMIR 2010) , 2010, pp.\n637–642.\n[17] C. Ariza and M. S. Cuthbert, “Modeling beats,\naccents, beams, and time signatures hierarchically\nwith music21 meter objects,” in Proceedings of\nthe International Computer Music Conference , New\nYork, 2010, pp. 216–223. [Online]. Available: http:\n//mit.edu/music21/papers/2010MeterObjects.pdfProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n397"
    },
    {
        "title": "The FAV Corpus: An Audio Dataset of Favorite Pieces and Excerpts, With Formal Analyses and Music Theory Descriptors.",
        "author": [
            "Ethan Lustig",
            "David Temperley"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265293",
        "url": "https://doi.org/10.5281/zenodo.10265293",
        "ee": "https://zenodo.org/records/10265293/files/000039.pdf",
        "abstract": "We introduce a novel audio corpus, the FAV Corpus, of over 400 favorite musical excerpts and pieces, formal analyses, and free-response comments. In a survey, 140 American university students (mostly music majors) were asked to provide three of their favorite 15-second musical excerpts, from any genre or time period. For each selection, respondents were asked: \"Why do you love the excerpt? Try to be as specific and detailed as possible (music theory terms are encouraged but not required).\" Classical selections were dominated by a very small number of composers, while the pop and jazz artists were diverse. A thematic coding of the respondents' comments found that the most common themes were melody (34.2% of comments), harmony (27.2%), and sonic factors: texture (27.6%), instrumentation (24.3%), and timbre (12.5%). (Rhythm (19.5%) and meter (4.6%) were less present in the comments.) The comments cite simplicity three times more than complexity, and energy gain 14 times more than energy decrease, suggesting that people's favorite excerpts involve simple moments of energy gain or \"build-up\". The complete FAV Corpus is publicly available online at EthanLustig.com/FavCorpus. We will discuss future possibilities for the corpus, including potential directions in the spaces of machine learning and music recommendation.",
        "zenodo_id": 10265293,
        "dblp_key": "conf/ismir/LustigT23",
        "keywords": [
            "FAV Corpus",
            "over 400 favorite musical excerpts",
            "formal analyses",
            "free-response comments",
            "140 American university students",
            "music theory terms",
            "Classical selections",
            "pop and jazz artists",
            "melody",
            "harmony"
        ],
        "content": "THE FAV CORPUS: AN AUDIO DATASET OF FAVORITE \nPIECES AND EXCERPTS, WITH FORMAL ANALYSES AND \nMUSIC THEORY DESCRIPTORS \nEthan Lustig David Temperley \nEthan@EthanLustig.com  Eastman School of Music, University \nof Rochester \nABSTRACT \nWe introduce a novel audio corpus, the FAV Corpus, of \nover 400 favorite musical excerpts and pieces, form al anal-\nyses, and free-response comments. In a survey, 140 Amer-\nican university students (mostly music majors) were  asked \nto provide three of their favorite 15-second musica l ex-\ncerpts, from any genre or time period. For each sel ection, \nrespondents were asked: “Why do you love the excerp t? \nTry to be as specific and detailed as possible (mus ic theory \nterms are encouraged but not required).” Classical selec-\ntions were dominated by a very small number of comp os-\ners, while the pop and jazz artists were diverse. A  thematic \ncoding of the respondents’ comments found that the most \ncommon themes were melody (34.2% of comments), har-\nmony (27.2%), and sonic factors: texture (27.6%), i nstru-\nmentation (24.3%), and timbre (12.5%). (Rhythm (19. 5%) \nand meter (4.6%) were less present in the comments. ) The \ncomments cite simplicity three times more than comp lex-\nity, and energy gain 14 times more than energy decr ease, \nsuggesting that people's favorite excerpts involve simple \nmoments of energy gain or \"build-up\". The complete FAV \nCorpus is publicly available online at \nEthanLustig.com/FavCorpus. We will discuss future p os-\nsibilities for the corpus, including potential dire ctions in \nthe spaces of machine learning and music recommenda -\ntion.  \n1. INTRODUCTION \nWhy do we like the music we like? Perusal of the ra nge of \noptions in a record store or streaming platform, or  of live \nmusic offerings in a large city, shows the enormous  diver-\nsity of musical taste among individuals. Research h as \nprobed some of the factors involved in this variabi lity, \nsuch as gender, age, personality, social identity, cultural \nbackground, and musical training [1-6]. Still, ther e seems \nto be general agreement that particular pieces of m usic are \nespecially “good.” Certain hymns, Christmas carols,  folk \nsongs, and classical pieces remain favorites across  decades \nand centuries; certain popular songs cause world-wi de and \nlasting explosions of enthusiasm. It seems, also, t hat spe-\ncific sections or moments within these pieces are e spe-\ncially pleasurable, giving rise to what are sometim es called peak experiences  [7-8]. Our own personal reflections cer-\ntainly confirm this, and anecdotally, there seems t o be at \nleast some agreement as to what the “best” moments of a \npiece are. But what makes a certain part of a piece  espe-\ncially enjoyable?  \nMusic psychology has begun to address this issue, \nthough in tentative and exploratory ways. Most of t his re-\nsearch has focused on the physiological manifestati ons of \npeak experiences, such as chills, which have been s hown \nto correlate with pleasure. A pioneering study by S loboda \n[9] asked participants to identify passages causing  strong \nphysiological effects—what he called “thrills” (p. 110, af-\nter [10])—and to describe the nature of those respo nses. \nMore recent studies follow Sloboda’s model in havin g par-\nticipants identify pieces that cause physiological re-\nsponses, especially chills, and then probing the po ssible \ncauses and correlates of these responses: neurologi cal cor-\nrelates [11, 12], musical elicitors [13, 14], and s elf-re-\nported perceptual correlates such as the perceived sadness \nor happiness of the music [15]. With regard to musi cal elic-\nitors of chills, studies have found many factors in cluding \nsequences, appoggiaturas, new or unexpected harmoni es, \ncrescendi, climaxes, sudden dynamic or textural cha nges, \nand entrances of instruments [9, 13-15]. Also deser ving \nmention is a large project by Gabrielsson and Wik [ 16] fo-\ncusing on the effects (physical, emotional, and cog nitive) \nof “strong experiences” of music (p. 158). Musical elici-\ntors are mentioned only briefly and in very general  terms: \n“instruments, rhythm, melody, harmony, musical form , \nperformance qualities etc.” (p. 198; see also [17],  p. 568). \nIn this study we offer a novel approach to the stud y of \npeak experiences and the musical factors that elici t them. \nIn contrast to the exploratory research cited above , our \nstudy takes a systematic, survey-based approach. Ou r con-\nception of peak experience is close to Maslow’s [7] —a pri-\nmarily internal albeit not physiological, intensely  positive \nexperience—and falls within the fairly broad range of \nways that the term is used [8]. \nOur project differs from most research on peak expe ri-\nences by focusing on passages of music directly rep orted \nto be strongly liked, rather than those causing chi lls and \nother physiological responses. While chills have ge nerally \nbeen shown to coincide with pleasurable experiences  [12, \n13], they may not always do so; conversely, one can  cer-\ntainly get great enjoyment from a musical passage w ithout \nexperiencing chills. \nIn a survey, 140 respondents identified favorite mu sical \nexcerpts. Respondents also provided free-response c om-\nments explaining their choices, and we provide a co ntent  © E. Lustig and D. Temperley. Licensed under a Cre ative \nCommons Attribution 4.0 International License (CC B Y 4.0). Attribu-\ntion:  E. Lustig and D. Temperley, “The FAV Corpus: An au dio dataset \nof favorite pieces and excerpts, with formal analys es and music theory \ndescriptors”, in Proc. of the 24th Int. Society for Music Information Re-\ntrieval Conf. , Milan, Italy, 2023. \n335  \n \nanalysis of these comments. We also present a publi cly \navailable corpus, the FAV Corpus, which includes au dio \nfiles of excerpts and complete pieces, formal analy ses of a \nsubset of the pieces, and the respondents’ free-res ponse \ncomments. \n2. METHOD \n2.1 Participants \nIn 2017, 140 students at the University of Rocheste r (New \nYork) were given a survey regarding their favorite musical \nexcerpts. Approximately 85% of the respondents to t he \nsurvey were students at the Eastman School of Music  (a \ndivision of the university) and were therefore musi c ma-\njors. The remaining 15% were students in an introdu ctory \nmusic psychology course; while students in this cou rse \nwere mostly not music majors, the course required b asic \nknowledge of music theory as a prerequisite. While stu-\ndents with music-theory training may not be represe ntative \nof the broader population, we deliberately chose th em for \ntheir ability to articulate the musical reasons for  their pref-\nerences, with regard to matters such as harmony, rh ythm, \nand form. On average, the respondents had 11.1 year s of \nmusic training on a musical instrument (including v oice) \n(SD = 4.2). There were 73 females, 63 males, and four who \npreferred not to say. The average age of the respon dents \nwas 19.7 years old, with a range of 17 to 29 years old (SD \n= 1.9). Respondents received extra credit points in  their \ncourses for participation. The survey received ethi cal ap-\nproval by the Institutional Review Board of the Uni versity \nof Rochester. \n2.2 Data Collection \nThe survey asked each respondent to identify “three  of \nyour favorite excerpts of music... in any style and  from any \ntime period.” For each excerpt, they were instructe d to pro-\nvide a URL (web address) to a recording of the \npiece/song/movement on YouTube or Spotify. We used \nthe phrase “piece/song/movement” to avoid stylistic  bias, \nbut hereafter we will refer only to “pieces.” Respo ndents \nwere then asked to “identify the 15-second excerpt that’s \nyour favorite” by providing start- and end-points f or the \nexcerpt in relation to the recording. The choice of  15 sec-\nonds was fairly arbitrary. We chose it, in part, be cause it \nroughly corresponds to the length of some of our fa vorite \nmusical passages. \nFollowing each selection, respondents were prompted  \nto write a response to the following question: “Why  do you \nlove the excerpt? Try to be as specific and detaile d as pos-\nsible (music theory terms are encouraged but not re -\nquired).” We take the term love to indicate a high degree \nof liking or preference, similar in meaning to enjoy  or \ngreatly like . Our mention of “music theory terms” was \naimed at encouraging respondents to identify the mu sical \nfeatures giving rise to their preferences. There is  a possible \ndownside to this wording; by drawing attention to o ur own \n \n      1 Due to reasons such as invalid web links, responde nt errors, etc., the \nactual corpus contains 399 excerpt audio files and 402 piece audio files. \n(See EthanLustig.com/FavCorpus for details.) music-theoretical background, it may have steered r e-\nspondents toward pieces or excerpts that they thoug ht were \ntheoretically “respectable” in some way. However, t he \nhuge stylistic variety of the chosen excerpts (desc ribed be-\nlow), including many from very recent popular music , sug-\ngests to us that this was not a concern for many re spond-\nents. Additionally, respondents were asked to choos e be-\ntween either “I enjoy this excerpt much more than t he other \nparts of the piece” or “I enjoy this excerpt about as much \nas the other parts of the piece.” \n2.3 Creating the Corpus \nRecordings of the complete pieces provided by the r e-\nspondents were extracted from the YouTube/Spotify \nURLs and saved as WAV audio files; audio files were  also \nmade of each preferred 15-second excerpt. In some c ases, \nthe beginning of the internet recording did not cor respond \nto the true beginning of the piece. To adjust for t his, any \ntime before the beginning of the piece was subtract ed from \nthe timepoints of the preferred excerpt, so that th e adjusted \ntimepoints indicated the excerpt’s location in rela tion to \nthe piece. In about 8% of cases, the chosen excerpt  was not \nexactly 15 seconds long, but usually just a few sec onds \nshorter or longer. In such cases, the excerpt was c onverted \nto a 15-second excerpt with the same midpoint as th e cho-\nsen excerpt. (For example, 0:00-0:25 would be conve rted \nto 0:05-0:20.) For more detail about this process, see [18].  \nThe corpus, which we call  the FAV Corpus, is publicly \navailable at EthanLustig.com/FavCorpus. The corpus con-\ntains 420 items (three excerpts from each of the 14 0 re-\nspondents). A spreadsheet indicates, for each item,  (a) the \nrespondent’s number, which had been assigned arbitr arily, \n(b) the excerpt number for that respondent (1, 2, o r 3), (c) \nthe artist and title of the piece, (d) the style an d historical \nera or year (explained below), (e) the duration of the piece, \n(f) the timepoints of the preferred excerpt, (g) wh ether the \nrespondent indicated that they enjoyed the excerpt “much \nmore than” [A] or “about as much as” [B] the rest o f the \npiece, and (h) the respondent’s comment about why t hey \nliked the excerpt. In what follows, we indicate exc erpts by \nrespondent and excerpt number; for example, Respond ent \n1’s three excerpts are 1_1, 1_2, and 1_3. We also p rovide \nsound files for both the complete pieces and the pr eferred \n15-second excerpts.1 \n3. RESULTS \n3.1 Stylistic Content of the Corpus \nThe distribution of styles and artists in the corpu s was ex-\namined. While this is not the main focus of the cur rent \nstudy, it provides a window into the musical tastes  and pas-\nsions of students at an American music school in 20 17 (re-\ncall that roughly 85% of respondents were music stu dents). \nEach excerpt was categorized as classical (49.5%), pop \n(41.8%), or jazz (8.7%). For most excerpts, classif ication \nwas clear; there were a few borderline cases, such as jazz-\nrock fusion pieces. The most popular artists in the  survey Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n336  \n \nare listed in Table 1, with the number of excerpts for each. \nFollowing convention, for classical works, we ident ify the \ncomposer as the artist; for jazz and pop, we identi fy the \nperformer(s) as the artist. Table 1 alone might giv e the im-\npression that respondents strongly favored classica l music, \nbut the style statistics just cited show otherwise;  the pre-\nponderance of classical composers in Table 1 indica tes, ra-\nther, that classical selections were dominated by a  small \nhandful of artists, while pop and jazz selections w ere much \nmore widely dispersed. \n \nComposer # Excerpts \nBach \nBrahms \nBeethoven \nTchaikovsky  \nRachmaninov \nMahler \nKendrick Lamar \nDebussy \nSibelius \nHandel 17 \n14 \n12 \n10 \n9 \n6 \n6 \n6 \n5 \n5 \n \nTable 1.  Artists (composers/performers) most represented \nin survey. \n \nAmong the classical excerpts, 12.1% were from the B a-\nroque period (1600-1749), 8.5% Classical (1750-1819 ), \n31.7% Romantic (1820-1899), and 47.7% 20th/21st-cen -\ntury (1900-present). (Each composer was assigned to  a sin-\ngle period, based on their years of greatest activi ty.) Again, \nthe large number of 20th/21st-century selections is  not re-\nflected in Table 1 since they are distributed over a much \nlarger number of composers. We also observed that m any \nof these 20th/21st-century composers were toward th e \nconservative end of the stylistic spectrum; the mos t popu-\nlar was Rachmaninoff, with nine excerpts. For the p op and \njazz selections, we identified the year of release of each \nrecording. The pop selections strongly favored rece nt mu-\nsic: 69.0% were from 2010–2017 (more than half of t hese \nfrom 2016–2017 alone), and 17.9% from the 2000s. Ja zz \nselections had a weaker bias toward recent music, w ith \n31.4% of selections from 2010 through 2017.  \n3.2 Formal Analysis \nOne of us (David Temperley) did a formal analysis o f a \nsubset of pieces in the corpus. He did not know whi ch ex-\ncerpts were preferred when doing the analysis. The subset \nconsisted of pieces in which respondents had said t hat they \nliked their preferred excerpt “much more than” the rest of \nthe piece; this yielded a set of 127 pieces (about 30% of \nthe survey responses).2 The recordings of the pieces were \ndivided into sections to the nearest second, and th e sections \nwere given formal labels, as the genre warranted (f or in-\nstance, P = primary theme for a sonata-form piece; V \n(verse) and CH (chorus) for pop songs). It was assu med \nthat each section continued until the beginning of the next \n \n      2 Altogether there were 137 eligible pieces, 10 prove d impossible to \nanalyze into formal sections, because there was no large-scale repetition \nand no clear moments of change demarcating reasonab ly-sized sections. \nSome of these were contemporary pieces; others were  Baroque pieces, section, so that each piece was exhaustively partit ioned \ninto sections. As an arbitrary constraint to simpli fy the \nanalysis, no section was allowed to be less than 15  seconds \nlong. Two main criteria were used for determining t he lo-\ncation of formal sections: change and repetition. A  signif-\nicant change in any musical parameter, such as harm ony, \nmelody, instrumentation, texture, meter, or rhythmi c pat-\ntern, was considered to make a good candidate for a  section \nbreak. Repetition could also define sections: for i nstance, \nthe return of the opening theme in a sonata-form mo ve-\nment might define a new section beginning even in t he ab-\nsence of obvious local changes. Repetition of the s ame la-\nbel signified exact or slightly modified repetition ; for ex-\nample, V would be used for two verses of a pop song , with \ndifferent lyrics and perhaps some changes in instru menta-\ntion, but mostly similar melody and harmony. For mo re \nsubstantially modified repetitions, numbers were us ed \n(e.g., V1 and V2 for two verses that had significan tly dif-\nferent melody or harmony). See [18] for more detail  about \nthe annotation system.  \nWe analyzed the preferred excerpts in relation to t heir \nlocation within the piece. First, we wondered if pe ople tend \nto choose excerpts that are near section boundaries . For \neach preferred excerpt, in the set of 127 excerpts for which \nformal analyses were available, we found the tempor al dis-\ntance between the midpoint of that excerpt and the closest \nformal section boundary; we then performed the same  pro-\ncess for random 15-second excerpts from the same pi eces, \nrepeating the process 10 times to mitigate the effe ct of ex-\ntreme values. (One piece had a 7-minute section tha t \nseemed to create outliers in the data; this piece w as re-\nmoved from the analysis.) Midpoints of preferred ex cerpts \nhave an average (absolute) temporal distance from t he \nnearest section boundary of 11.41 seconds, while fo r mid-\npoints of random excerpts, the distance is 13.67 se conds—\na modest but significant difference ( t(168.73) = -2.46, p < \n0.01). Thus, preferred excerpts show a slight tende ncy to \nbe located near formal boundaries. A total of 49.2%  of the \npreferred excerpts actually contain a section bound ary; \namong the random excerpts, only 37.6% do. We then r e-\nanalyzed the same distances as signed values, to se e \nwhether preferred excerpts tend to be near the begi nning \nor end of a formal section. For preferred excerpts,  the mean \nsigned difference between the midpoint and the near est \nboundary is 2.90 (i.e., on average, the midpoint oc curs 2.90 \nseconds after the boundary), significantly greater than zero \n(one-sample t-test, t(125) = 2.22, p < 0.05). This indicates \na slight tendency to choose excerpts near the begin ning of \na section rather than near the end, or, perhaps, ov erlapping \nmore with the beginning of a section than with the end of \nthe previous one.  \nFinally, we examined the location of each excerpt i n re-\nlation to the piece as a whole. For this analysis, we used all \n399 excerpts in the corpus. Each excerpt received a  value \nfor its proportional position in the piece, where 0  would be \nat the very beginning, and 1 would be at the very e nd. The \nfor example imitative textures with a rapid or seam less alternation be-\ntween subject entries and episodes. \n Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n337  \n \nmean value was 0.46; clearly there was not a strong  bias \ntoward choosing excerpts early or late in the piece . \n3.3 Content Analysis of Comments \nAs mentioned earlier, respondents were asked to com ment \non their reasons for liking each excerpt in their o wn words. \nResponses varied from a few words to several senten ces. \nWhile a few responses were flippant or minimal, a g reat \nmany respondents showed enthusiasm for the task and  \ntook considerable effort in explaining their choice s. We \ndid a content analysis of the respondents’ comments . One \nof us (David Temperley) coded all 420 comments, ide nti-\nfying 17 themes that seemed to appear repeatedly in  the \ncomments. We then provided a list of the 17 themes and \ntheir definitions (Table 2) to an independent coder  (a mu-\nsic theory Ph.D. student at the Eastman School of M usic) \nand asked him to assign themes to the comments usin g that \nlist. Each comment could be tagged with any number of \nthemes (including zero). In choosing themes, both c oders \naimed to represent the respondents’ actual reasons for lik-\ning the excerpts, as opposed to aspects mentioned s imply \nto aid reference, although this distinction was not  always \neasy to make. For example, a comment like “I love t he vi-\nolin melody” was encoded as MEL (melody) rather tha n \nINS (instrumentation). \n \nBIO Autobiographical connection: references to the  \n      respondent’s past experience with the piece o r excerpt, \n      OR incidents in their life that it reminds th em of for \n      any reason. \nCOM (+/-) Complexity (or its opposite, simplicity).  \nDYN (+/-) Dynamics. \nEN (+/-) Energy. Energy level in music is thought t o be \n      conveyed by such as dynamics, register, rhyth mic  \n      activity, and textural thickness; an increase  in any of \n      these dimensions could create a rise in energ y.  \n      However, when the change is described in thes e more \n      specific terms (e.g. dynamics) it can be code d in that \n      way; EN should be reserved for more general  \n      descriptions of energy change or level, e.g. “buildup” \n      or “climax”. \nHAR Harmony: includes harmonic progression, functio n, \n      or chord quality; also tonality (e.g. modulat ion), mode \n      (major/minor), and dissonance/consonance. \nINS Instrumentation: choices of instrument or instr ument \n      combinations; also includes general uses of a n  \n      instrument (e.g. “I like the clarinet in a hi gh register”), \n      or special timbral effects prescribed by comp oser, e.g. \n      extended techniques; also synthesized parts i n popul \n      music textures. (Compare to TIM). \nINT Interpretation (e.g. expressive timing; also ge ner \n      statements about beauty/expressiveness of a  \n      performance or quality of performer). \nLYR Lyrics. \nMEL Melody: the main melody in this particular part  of \n      the piece. Also includes improvised solos, e. g. in jazz. \nMET Meter (incl. tempo). \nPHY Mentions of a physical or physiological respons e to \n      the music. \nRET Return of earlier thematic material. \nRH Rhythm. Includes references to general rhythmic feel,       e.g. “groove”. \nSUR Explicit mentions of surprise or denial of  \n      expectation. \nTEX Texture: a catch-all category including aspects  of \n      pitch-rhythmic patterns other than melody, su ch as  \n      details of accompaniment or bass lines, chord  voicings, \n      or polyphonic patterns. \nTIM Timbre: when credited to performer (e.g. a sing er’s \n      tone), or synthesized/electronic sounds that are not a \n      consistent part of the texture. (Compare to I NS). \nVIR Virtuosity (or just proficiency, i.e. playing a  very  \n      difficult bit accurately; also intonation). \n \nTable 2.  Themes and definitions used in content analysis. \n \nAgreement between the two coders regarding the as-\nsignment of each theme was measured using Cohen’s \nkappa, where 1.0 would indicate that the two coders  as-\nsigned the theme to exactly the same comments. Agre e-\nment levels varied between 0.37 and 0.84, depending  on \nthe theme, and thus were mostly in the range of moderate  \nor substantial  according to Landis and Koch’s [19] rubric. \nIn what follows we discuss the results of this cont ent anal-\nysis. We also analyzed word frequencies in the comm ents, \ngrouping together similar words such as “simple,” “ sim-\npler,” and “simplicity”. We include some results of  that \nanalysis in the following discussion. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 1.  The percentage of respondents’ comments iden-\ntified with themes in the content analysis. For exp lanation \nof abbreviations, see Table 2.  \n \nFigure 1 shows the percentage of occurrences of eac h \ntheme in the comments. The counts of each theme wer e \naveraged between the two coders. The frequent menti ons \nof melody (MEL, occurring in 34.2% of comments) and  \nharmony (HAR, 27.2%) indicate the importance of the  \npitch domain in respondents’ preferences. Rhythmic fac-\ntors—rhythm (RH, 19.5%) and meter (MET, 4.6%)—were \nless important, though it should be remembered that  mel-\nody has a rhythmic aspect as well. Notably, the wor d \n“groove” occurred 20 times—confirming the widely he ld \nview that this is a significant factor in musical e njoyment \n[20, 21]. What might be called sonic factors were a lso \nmentioned frequently: texture (TEX, 27.6%), instrum enta-\ntion (INS, 24.3%), and timbre (TIM, 12.5%). There w ere \ncomparatively few mentions of performance aspects: in-\nterpretation (INT, 6.0%) and virtuosity (VIR, 4.3%) . Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n338  \n \nGiven that a large majority of respondents were maj oring \nin classical music performance, we had expected the se fac-\ntors to weigh more heavily. Lyrics (LYR) were menti oned \nin 12.5% of comments, and autobiographical factors (BIO, \nconnections with the respondents’ life experience) in just \n3.6%. The PHY theme, physiological responses (such as \nchills), was mentioned in just 3.1% of comments. It  is pos-\nsible that the survey instructions—which encouraged  the \nuse of music-theory terms—steered respondents’ atte ntion \ntowards musical features and away from autobiograph ical \nand physiological factors. \nThree of the themes—complexity (COM), energy (EN), \nand dynamics (DYN)—were parametric: They could be \nsubscripted as “+” (indicating an increase or relat ively \nhigh level) or “–” (a decrease or relatively low le vel), \nthough this was optional. While there were 6.5 inst ances \nof COM+ in the comments, there were 22 instances of  \nCOM– (again, theme counts reported here and below a re \naveraged across the two coders’ analyses). This res ult sug-\ngests that respondents favored moments of relativel y low \nor decreasing complexity. Our analysis of word freq uen-\ncies also supports this view: “simple” (and related  words) \noccurs 32 times in the comments, while “complex” (a nd its \nvariants) only occurs 11 times. Related to this, th e words \n“tension/tense” and “resolution/resolve” were used about \nequally often (19 and 18 times, respectively). Howe ver, \nseven of the comments mentioning tension refer spec ifi-\ncally to the resolution of the tension (sometimes u sing \nother words like “relax,” “release,” or “relief”); in the re-\nmaining cases, the tension seems to be valued in it self.    \nThe energy (EN) theme shows an even stronger para-\nmetric tilt than complexity: 35.5 of its mentions a re EN+, \nwhile only 2.5 are EN–. Energy is often treated as more or \nless synonymous with the arousal/activation dimensi on in \nRussell’s [22] two-dimensional model of emotion, an d this \nin turn has been associated with musical parameters  such \nas loudness, pitch register, and rhythmic activity [23].3 \nNote from Table 2 that this theme reflects general refer-\nences to energy, as opposed to mentions of energy-i nvok-\ning musical dimensions such as dynamics, rhythm, or  tex-\nture. The dynamics (DYN) theme also showed a parame t-\nric tilt, marked “+” eight times and “–” only three  times. \nAnalysis of word frequencies shows further evidence  of a \npreference for increasing energy. For example, the word \n“build” and related words such as “build-up” occurs  47 \ntimes. It is not obvious what the opposite of “buil d” would \nbe, indicating a general decrease  in energy level; one \nthinks of such words such as “decrease,” “decline,”  “fade,” \n“wane,” “subside,” and “dwindle.” None of these wor ds \noccurred even once, except “fade,” which occurred j ust \nthree times.4 Several other frequent word categories indi-\ncate an increase or peak in energy, such as “climax /climac-\n \n      3 In experiments on music and emotion, manipulations  in the temporal \ndimension usually involve changing the speed of a m elody, and are there-\nfore described (correctly) as variations in tempo ( for a survey, see [23]). \nWithin a piece, however, the tempo (i.e., the speed  of the main beat) \nrarely changes, except for small fluctuations; temp oral variation is more \nlikely to involve changes in rhythmic values (e.g.,  from a quarter-note \ntexture to a 16th-note texture). In both of these c ases, though, the varia-\ntions involve a change in the temporal density of e vents; if an increase in \ntempo conveys an increase in energy, it seems likel y that an increase in \nrhythmic activity over a fixed tempo would also do so. tic” (used 28 times), “power(ful)” (27 times), and “cre-\nscendo” (9 times; “diminuendo” is never used and “d ecre-\nscendo” just once).  \nIn this connection, a result from our analyses of f ormal \nstructures, described earlier, is relevant. In pop songs, \nwhich nearly always contain both choruses and verse s, re-\nspondents’ preferred excerpts were more often in ch oruses \n(13 times) than verses (7 times). (Recall that our analysis \nof formal structures included only about 30% of the  survey \nresponses.) Respondents’ comments also mentioned ch o-\nruses (44 times) much more often than verses (18 ti mes).5 \nIt has been observed that choruses tend to be highe r than \nverses in the “energetic” dimensions mentioned abov e, \nsuch as pitch register and textural thickness [24, pp. 39-\n40]. Thus, several patterns in our data point to an  increase \nin energy as an important elicitor of musical pleas ure.  \nPerusal of the comments suggests other possible the mes \nas well. For instance, many comments contain terms or \nphrases that could be described as emotional. In th e first \n20 comments, we see “aggressive\" (1_2), “raw emotio n” \n(2_3), “intensity” (2_3), “exciting” (4_2, 6_2), “[ the \nsinger] let[s] emotions loose,” (5_2) “dramatic” (6 _1), and \n“triumphant” (6_2). In many cases, such terms are u sed to \ndescribe a specific aspect of the music that could also be \nencoded in some other way: for example, “a triumpha nt \ntheme” (MEL); “the buildup is very exciting” (EN). An-\nother issue is the distinction between induced and per-\nceived emotion [25]. Sometimes the distinction is c lear—\n“It is insanely happy” is perceived emotion, “[It] always \nmakes me so happy” is induced emotion—but not alway s: \nif a passage is described as “exciting” or “relaxin g,” is that \ninduced or perceived emotion? If induced emotion is  in-\ncluded in the “emotion” theme, one could potentiall y in-\nclude a large number of comments implying a positiv e \nemotional reaction: for example, “I love the cellis t’s inter-\npretation.” Indeed, one might say that such a react ion is \nimplicit in all of the comments, given the nature o f the task.  \n4. DISCUSSION \nIn our study, 140 college students, mostly music st udents, \nidentified three of their favorite 15-second passag es of mu-\nsic. One result emerging from our analysis of the s urvey \ncomments was a preference for passages that increas e in \nenergy—often described by respondents as “builds” o r \n“build-ups,” or as sections that “build.” As noted earlier, \nenergy in music is generally associated with parame ters \nsuch as loudness, pitch register, and rhythmic acti vity. It \nalso seems intuitive to us, although this does not seem to \nhave been widely studied, that textural thickness i s also as-\nsociated with energy, perhaps partly because a thic kening \nof texture implies greater loudness, whether or not  the \nloudness actually increases. Our finding that incre ases in \n      4 Some of these words, such as “build,” “decrease,” and “fade,” could \nbe either nouns or verbs; we counted both, includin g all verb forms. The \nword “drop” is also of interest; it occurs 12 times , as noun or verb, but \nonly five of those uses could be taken to refer to energy level. Sometimes \nthe term is used to refer to the re-entrance of the  kick drum in a pop or \nEDM song. \n      5 One might wonder if choruses are more frequent tha n verses in our \ncorpus, and therefore take up more time. Actually t hey do not: choruses \ntake up a total of 1769 seconds, in the portion of the corpus that was for-\nmally analyzed; verses take up a total of 1977 seco nds. Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n339  \n \nenergy are often pleasurable accords well with othe r work \non peak musical experiences that has linked them to  cre-\nscendi and increases in texture [13-15, 26]. It als o appears \nthat there is a strong preference for passages perc eived as \nhaving relatively low or decreasing complexity and ten-\nsion, compared to passages perceived to be high in com-\nplexity or tension. This is in line with Meyer's [2 7] obser-\nvation that in music, \"The greater the buildup of s uspense, \nof tension, the greater the emotional release upon resolu-\ntion\" (p. 28) and Huron's [28] idea of \"contrastive  valence\" \n(p. 39). \nEarlier studies have found that a wide range of fac tors \naffect peak experiences [9, 13, 14], and this is ap parent \nfrom the free-response comments in our survey. The single \nmost common theme in the comments was melody. While  \nit is hardly news that people like a good melody, t his result \ndraws our attention to the huge importance of this factor; \nthe question of what makes a melody good is one tha t mu-\nsic theory and music psychology are still a long wa y from \nanswering. Our corpus might provide a useful starti ng \npoint for an exploration of this topic. Other frequ ent \nthemes in respondents’ comments—such as harmony, in -\nstrumentation, rhythm, and texture—also point to fa ctors \nthat greatly influence listeners’ preferences; how they do \nso is, at present, largely mysterious.  \n4.1 Future Directions \nIn terms of future directions, the first avenue of exploration \ncould be expanding the existing dataset. The survey  could \nbe re-run online, and globally, with many more part ici-\npants, increasing sample size and statistical power , as well \nas diversifying the participant set. Instead of thr ee songs \nand excerpts, many more songs and excerpts could be  re-\nquested from each participant, allowing for better trend \nanalysis within  participants, to potentially identify differ-\nent listener-types. The usefulness of this kind of data for \nthe music-recommendation space, and associated indu stry \napplications, is clear [29].  \nAs the corpus grows in size, the potential for usin g ma-\nchine learning and related methods (which tend to e xcel \nwith larger datasets) to analyze the data becomes m ore vi-\nable. An acoustical signal-analysis-based approach,  using \nthe many tools available in the field of music info rmation \nretrieval, for instance, could be applied to the co rpus, to \ndetermine which audio features (e.g. spectral flux,  disso-\nnance, loudness, etc.) are determinative of the fav orite ex-\ncerpts as compared to random controls from the same  \npieces. This acoustical approach could be effective ly com-\nbined with a symbolic, music-theoretic approach. \nIn fact, even without venturing outside of the symb olic \nspace, there is immense potential for further codin g and \nanalysis of corpus features such as scale-degree di stribu-\ntions, metric position, harmonic root patterns, and  so forth, \nakin to the statistical work applied to the Rolling  Stone \nCorpus [30, 31]. This computational approach to the  cor-\npus could be supplemented by a more humanistic, ana lyti-\ncal approach in which more speculative and traditio nal \nanalysis is conducted to attempt to understand why these \nparticular excerpts are so powerful. For instance, given the \noverwhelming emphasis on pitch (melody and harmony)  in participants’ comments, it would be interesting to deter-\nmine the melodic, harmonic, rhythmic, and contrapun tal \nstructures characteristic of the excerpts in the co rpus; and \nwhat distinguishes a favorite excerpt from a non-fa vorite \nexcerpt in the same piece.  \nAnother possible direction for future research coul d be \nto measure the energy and complexity trajectories o f the \npieces in our corpus. While energy can be measured using \nlow-level spectral features such as root-mean-squar e \n(RMS) acoustic energy, some efforts have been made to \ncreate more sophisticated predictors of perceived m usical \nenergy using combinations of features [32, 33]. Suc h algo-\nrithms could be applied to our corpus. Meanwhile, m eas-\nuring complexity (especially in an automatic way) p resents \nmore of a challenge. Complexity—in its information- the-\noretic sense—is inherently subjective, since it dep ends on \nthe listener’s expectations, which in turn can vary  widely \ndepending on their musical experiences. Furthermore , \ncomplexity presumably depends heavily on patterns o f \npitch and rhythm, which cannot yet be reliably extr acted \nfrom polyphonic audio [34]. For classical pieces, M IDI en-\ncodings could be used, but for popular songs, trans crip-\ntions would need to be created. Once these problems  were \nsolved, it might be possible to create measures of complex-\nity using probabilistic models (such as Markov mode ls); \nindeed, there have been interesting efforts in this  direction, \nthough they relate only to melody [35, 36]. \nAnother intriguing area is the correlation of perso nality, \npersonal values, and socio-economic data with music  taste \n[1-6]. An expanded iteration of the survey could pe rhaps \ninclude a personality inventory and collect socio-e conomic \ndata, building a more holistic and accurate model o f music \ntaste.  \nWe hope that the current study has taken a small st ep \ntoward advancing our understanding of peak musical ex-\nperiences, and that our publicly available corpus w ill be \nuseful to other researchers in this area, as we con tinue to \nanswer the question: why do we like the music that we \nlike? \n5. REFERENCES \n[1] D. J. Hargreaves, C. Comber, and A. Colley, \"Effect s \nof age, gender, and training on musical preferences  \nof British secondary school students,\" Journal of Re-\nsearch in Music Education , vol. 43, no. 3, pp. 242-\n250, 1995. \n[2] P. J. Rentfrow and S. D. Gosling, \"The do re mi's o f \neveryday life: The structure and personality corre-\nlates of music preferences,\" Journal of Personality \nand Social Psychology , vol. 84, no. 6, pp. 1236–\n1256, 2003. \n[3] S. Manolios, A. Hanjalic, and C. C. S. Liem, \"The \ninfluence of personal values on music taste: Toward s \nvalue-based music recommendations,\" Proceedings \nof the 13th ACM Conference on Recommender Sys-\ntems, pp. 501-505, 2019. \n[4] T. Schäfer and C. Mehlhorn, \"Can personality traits  \npredict musical style preferences? A meta-analysis, \" Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n340  \n \nPersonality and Individual Differences , vol. 116, pp. \n265-273, 2017. \n[5] A. Mohan and E. Thomas, \"Effect of background mu-\nsic and the cultural preference to music on adoles-\ncents’ task performance,\" International Journal of \nAdolescence and Youth , vol. 25, no. 1, pp. 562–573, \n2020. \n[6] B. I. L. M. Mendis et al., \"Exploration of music pref-\nerences among the socioeconomic stereotypes: A \ncross-sectional study,\" Journal of Advanced Re-\nsearch in Social Sciences , vol. 4, no. 4, pp. 1–18, \n2021.  \n[7] A. H. Maslow, Religions, Values, and Peak-experi-\nences . Columbus, OH: Ohio State University Press, \n1964. \n[8] J. Whaley, J. Sloboda, and A. Gabrielsson,  \"Peak ex-\nperiences in music,\" in The Oxford Handbook of Mu-\nsic Psychology, S. Hallam, I. Cross, and M. Thaut, \nEds. Oxford, UK: Oxford University Press, 2009, pp.  \n452–61. \n[9] J. Sloboda, \"Music structure and emotional response : \nSome empirical findings,\" Psychology of Music , vol. \n19, no. 2, pp. 110-120, 1991. \n[10] A. Goldstein, \"Thrills in response to music and oth er \nstimuli,\" Physiological Psychology, vol. 8, pp. 126-\n129, 1980. \n[11] A. J. Blood and R. J. Zatorre, \"Intensely pleasurab le \nresponses to music correlate with activity in brain  re-\ngions implicated in reward and emotion,\" Proceed-\nings of the National Academy of Sciences , vol. 98, no. \n20, pp. 11818–11823, 2001. \n[12] V. N. Salimpoor, M. Benovoy, K. Larcher, A. \nDagher, and R. J. Zatorre, \"Anatomically distinct d o-\npamine release during anticipation and experience o f \npeak emotion to music,\" Nature Neuroscience , vol. \n14, no. 2, pp. 257–262, 2011. \n[13] O. Grewe, F. Nagel, R. Kopiez, and E. Altenmüller, \n\"Listening to music as a re-creative process: Physi o-\nlogical, psychological, and psychoacoustical corre-\nlates of chills and strong emotions,\" Music Percep-\ntion, vol. 24, no. 3, pp. 297–314, 2007. \n[14] S. Bannister, \"A survey into the experience of musi -\ncally induced chills,\" Psychology of Music , vol. 48, \nno. 2, pp. 297–314, 2020. \n[15] J. Panksepp, \"The emotional sources of “chills” in-\nduced by music,\" Music Perception , vol. 13, no. 2, \npp. 171–207, 1995. \n[16] A. Gabrielsson and S. L. Wik, \"Strong experiences \nrelated to music: A descriptive system,\" Musicae Sci-\nentiae , vol. 7, no. 2, pp. 157–217, 2003. \n[17] A. Gabrielsson, \"Strong experiences with music,\" in  \nHandbook of Music and Emotion , P. Juslin and J. Slo-\nboda, Eds. Oxford, UK: Oxford University Press, \n2010, pp. 547-574. [18] E. Lustig, \"The effect of perceived complexity and \nformal location on musical preference,\" Ph.D. disse r-\ntation, Dept. Music Theory, University of Rochester , \nRochester, NY, 2021. \n[19] J. R. Landis and G. G. Koch, \"The measurement of \nobserver agreement for categorical data,\" Biometrics , \nvol. 33, no. 1, pp. 159–174, 1977. \n[20] P. Janata, S. T. Tomic, and J. M. Haberman, \"Sen-\nsorimotor coupling in music and the psychology of \nthe groove,\" Journal of Experimental Psychology: \nGeneral , vol. 141, no. 1, pp. 54-75, 2012. \n[21] M. Witek, E. Clarke, M. Wallentin, M. Kringelbach, \nand P. Vuust, \"Syncopation, body-movement and \npleasure in groove music,\" PLoS ONE , vol. 9, no. 4, \n2014. \n[22] J. A. Russell and L. F. Barrett, \"Core affect, prot otyp-\nical emotional episodes, and other things called em o-\ntion: Dissecting the elephant,\" Journal of Personality \nand Social Psychology , vol. 76, no. 5, pp. 805–819, \n1999. \n[23] A. Gabrielsson and E. Lindström, \"The role of struc -\nture in the musical expression of emotions,\" in Hand-\nbook of Music and Emotion , P. Juslin and J. Sloboda, \nEds. Oxford, UK: Oxford University Press, 2010, pp.  \n367–400. \n[24] T. de Clercq, \"Sections and successions in successf ul \nsongs: A prototype approach to form in rock music,\"  \nPh.D. dissertation, Dept. Music Theory, University \nof Rochester, Rochester, NY, 2012. \n[25] P. Evans and E. Schubert, \"Relationships between \nexpressed and felt emotions in music,\" Musicae Sci-\nentiae , vol. 12, no. 1, pp. 75–99, 2008. \n[26] B. K. Hurley, P. A. Martens, and P. Janata, \"Sponta -\nneous sensorimotor coupling with multipart music,\" \nJournal of Experimental Psychology: Human Per-\nception and Performance , vol. 40, no. 4, pp. 1679–\n1696, 2014. \n[27] L. Meyer, Emotion and Meaning in Music. Chicago, \nIL: University of Chicago Press, 1956. \n[28] D. Huron, Sweet Anticipation: Music and the Psy-\nchology of Expectation. Cambridge, MA: MIT Press, \n2006. \n[29] A. Agostinelli et al., \"MusicLM: Generating music \nfrom text,\" arXiv preprint arXiv:2301.11325 , 2023. \n[30] T. de Clercq and D. Temperley, \"A corpus analysis \nof rock harmony,\" Popular Music , vol. 30, pp. 47-70, \n2011.  \n[31] I. Tan, E. Lustig, and D. Temperley, \"Anticipatory \nsyncopation in rock: A corpus study,\" Music Percep-\ntion, vol. 36, no. 4, pp. 353-370, 2019. \n[32] A. Zils and F. Pachet, \"Extracting automatically th e \nperceived intensity of music titles,\" in Proceedings of \nthe 6th COST-G6 Conference on Digital Audio Ef-\nfects (DAFX03) , 2003. Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n341  \n \n[33] P. Wood and S. Semwal, \"An algorithmic approach \nto music retrieval by emotion based on feature data ,\" \nin Proceedings of 2016 Future Technologies Confer-\nence (FTC) , 2016, pp. 140-144. \n[34] E. Benetos, S. Dixon, Z. Duan, and S. Ewert, \"Auto-\nmatic music transcription: An overview,\" IEEE Sig-\nnal Processing Magazine , vol. 36, no. 1, pp. 20-30, \n2018. \n[35] T. Eerola, \"Expectancy-violation and information-\ntheoretic models of melodic complexity ,” Empirical \nMusicology Review , vol. 11, no. 1, 2016. \n[36] B. P. Gold, M. T. Pearce, E. Mas-Herrero, A. Dagher , \nand R. J. Zatorre, \"Predictability and uncertainty in \nthe pleasure of music: A reward for learning?,\" Jour-\nnal of Neuroscience , vol. 39, no. 47, pp. 9397–9409, \n2019. Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n342"
    },
    {
        "title": "On the Effectiveness of Speech Self-Supervised Learning for Music.",
        "author": [
            "Yinghao Ma",
            "Ruibin Yuan",
            "Yizhi Li",
            "Ge Zhang",
            "Chenghua Lin",
            "Xingran Chen",
            "Anton Ragni",
            "Hanzhi Yin",
            "Emmanouil Benetos",
            "Norbert Gyenge",
            "Ruibo Liu",
            "Gus Xia",
            "Roger B. Dannenberg",
            "Yike Guo",
            "Jie Fu 0001"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265321",
        "url": "https://doi.org/10.5281/zenodo.10265321",
        "ee": "https://zenodo.org/records/10265321/files/000054.pdf",
        "abstract": "Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent models such as wav2vec2.0 have shown promise. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train 12 SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information. Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms.",
        "zenodo_id": 10265321,
        "dblp_key": "conf/ismir/MaYLZLCRYBGLXDG23",
        "keywords": [
            "self-supervised learning",
            "speech and natural language processing",
            "music information retrieval",
            "wav2vec2.0",
            "speech SSL models",
            "polyphonic information",
            "music adaption",
            "SSL models",
            "MIR tasks",
            "empirical suggestions"
        ],
        "content": "ON THE EFFECTIVENESS OF SPEECH SELF-SUPERVISED LEARNING\nFOR MUSIC\nYinghao Ma,1*Ruibin Yuan,2,3*Yizhi Li,4*Ge Zhang,2,5*Xingran Chen6Hanzhi Yin3Chenghua Lin4†\nEmmanouil Benetos1†Anton Ragni4Norbert Gyenge4Ruibo Liu7Gus Xia8Roger Dannenberg3Yike Guo9Jie Fu2†\nMultimodal Art Projection Research Community1Queen Mary University of London2Beijing Academy of Artiﬁcial Intelligence\n3Carnegie Mellon University4University of Shefﬁeld5University of Waterloo6University of Michigan Ann Arbor\n7Dartmouth College8New York University Shanghai9Hong Kong University of Science and Technology\n{yinghao.ma, emmanouil.benetos}@qmul.ac.uk, ruibiny@andrew.cmu.edu\n{yizhi.li, c.lin}@sheffield.ac.uk, gezhang@umich.edu, fujie@baai.ac.cn\nABSTRACT\nSelf-supervised learning (SSL) has shown promising results\nin various speech and natural language processing applica-\ntions. However, its efﬁcacy in music information retrieval\n(MIR) still remains largely unexplored. While previous\nSSL models pre-trained on music recordings may have\nbeen mostly closed-sourced, recent speech models such as\nwav2vec2.0 have shown promise in music modelling. Nev-\nertheless, research exploring the effectiveness of applying\nspeech SSL models to music recordings has been limited.\nWe explore the music adaption of SSL with two distinctive\nspeech-related models, data2vec1.0 and Hubert, and refer\nto them as music2vec and musicHuBERT, respectively. We\ntrain12SSL models with 95M parameters under various\npre-training conﬁgurations and systematically evaluate the\nMIR task performances with 13 different MIR tasks. Our\nﬁndings suggest that training with music data can generally\nimprove performance on MIR tasks, even when models\nare trained using paradigms designed for speech. However,\nwe identify the limitations of such existing speech-oriented\ndesigns, especially in modelling polyphonic information.\nBased on the experimental results, empirical suggestions\nare also given for designing future musical SSL strategies\nand paradigms.\n1. INTRODUCTION\nDeep learning (DL) techniques have shown promising re-\nsults in a wide range of auditory tasks, including speech and\nmusic information retrieval (MIR). However, the quantity\nand quality of labelled data is a bottleneck for developing\nalgorithms with better generalisation in complex real-world\nsettings for machine listening. To address this issue, self-\nsupervised learning (SSL) such as BERT [1] has emerged\n∗The authors contributed equally to this work.\n†Corresponding authors.\n© Y . Ma, R. Yuan, Y . Li, G. Zhang et al.. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Y . Ma, R. Yuan, Y . Li, G. Zhang et al., “On the Effectiveness\nof Speech Self-Supervised Learning for Music”, in Proc. of the 24th Int.\nSociety for Music Information Retrieval Conf., Milan, Italy, 2023.as a solution to leverage diverse and representative unla-\nbelled data to train a deep feature extractor with better\ngeneralisation. By combining this pre-trained SSL encoder\nwith a naive classiﬁer, typically a multi-layer perceptron\n(MLP) or long short-term memory (LSTM) with limited\nhidden layers, the model can achieve strong or state-of-\nthe-art (SOTA) performance in various downstream tasks\nincluding NLP [1 –3], computer vision [4], and audio [5, 6],\nwhere well-labelled datasets are limited. For music, larger\ndatasets can be more expensive due to copyright and anno-\ntation costs, making SSL essential for developing effective\nMIR systems. Investigating versatile SSL approaches in\nMIR can further improve the performance on many MIR\ntasks, beneﬁtting the music industry, music education, and\nheritage preservation. Although SSL has signiﬁcantly im-\nproved the performance of models in tasks such as speech\nrecognition, sentiment analysis, and language modelling,\nits effectiveness in MIR remains largely unexplored.\nThere has been much work on SSL for audio represen-\ntation learning, including speech, sound events or music.\nBut most results are difﬁcult to evaluate or ﬁne-tune due\nto limited access to training data, pre-trained parameters or\ntraining codes. PANN [7] is trained on noisy/weak-label\nclassiﬁcation and does not provide promising results in\nmusic tasks such as pitch classiﬁcation and instrument clas-\nsiﬁcation [8]. Besides, it can hardly be re-trained on music\ndatasets given that the MIR community does not have a\nweekly labelled large music dataset. MusiCoder [9], Music\nPASE [10], and MAP-Music2Vec [11] use strategies mainly\nbased on masked prediction, where training models predict\nthe audio waveform manually-designed feature or learnable\ndeep feature of input removed randomly from the ground\ntruth. Such models trained on music are not open-sourced\nexcept MAP-Music2Vec, which provides pre-trained param-\neters on hugging-face1. Jukebox [12] uses similar strate-\ngies for pop-song recording generation and demonstrates\ngood potential for multiple MIR tasks [6]. But the training\ncode for it is unavailable and is hard to ﬁne-tune given its\n6 billion parameters. MAP-MERT v0 [13] mimicks Hu-\nBERT [14], which regards the clustering results of audio as\na pseudo label or pseudo spectrum to be reconstructed rather\n1https://huggingface.co/m-a-p/music2vec-v1457than a cluster assignment. But it does not provide training\ncodes for further model evaluation. Furthermore, there are\nsome music SSL models based on instance discrimination.\nIn this family of approaches, each instance is considered its\nclass, and models are trained to distinguish among different\ninstances. CLMR [15] is trained with a limited number\nof parameters and shows limited capacity [6]. PEMR [16]\ndoes not show promising results besides tagging and is not\nopen-source for further evaluation.\nAlthough not designed for MIR tasks, some speech\nSSL models provide promising results on music tasks,\nand their training codes are available for ﬁne-tuning or re-\ntraining on musical audio. Mockingjay [17], and PASE [18]\nuse masked waveform / audio-feature prediction for pre-\ntraining. COLR [19] uses EfﬁcientNet with a limited num-\nber of parameters and is designed for general audio, though\nit has a promising result on instrument classiﬁcation. SF-\nNFNet-F0 [20] also uses an architecture based on convolu-\ntion neural networks, a SlowFast Normalizer-Free ResNet,\nfor audio pre-training. Furthermore, apart from provid-\ning good results on automatic speech recognition (ASR),\nWav2Vec2.0 [5], HUBERT and data2vec [21] also provide\nmuch better results on pitch estimation and instrumental\nclassiﬁcation than PANN, though they are still far from\nperfect [8]. All of the speech SSL models are helpful for\nmusic SSL model development.\nPrevious work on re-training speech SSL systems with\nmusic recordings is limited to the size of training datasets or\nmodel structure. Ragano et al. [22] re-trained wav2vec2.0\non music audio and improved performance on pitch esti-\nmation and instrument classiﬁcation signiﬁcantly. But the\ntraining set is less than 100 hours which may be less rep-\nresentative, and the downstream tasks are limited and not\nuniversal. MusiCoder and Music PASE can be regarded as\nre-training speech SSL models on music recordings, but the\nmodel performance is not promising. Besides, these models\nare evaluated with a limited number of downstream tasks,\nmaking the learned embedding less persuasive. SF-NFNet-\nF0 is trained on music recordings and provides better results\non multiple music tagging tasks [23]. But its model archi-\ntecture is based on CNN, without much room for further\nscale-up and longer sequence modelling.\nThe missing science in the previous studies is as follows.\nAll of the existing models trained on music are either with\na limited number of parameters and capacity for MIR tasks\nother than tagging or not open-source for further evalua-\ntion. Some of the systems developed on speech or general\naudio recordings demonstrate promising but not satisfying\nresults on MIR tasks. Besides, previous investigations on\nthe efﬁcacy of applying speech-related SSL models to mu-\nsic recordings are limited by the size of the training set, not\nenough universality on the downstream tasks, or paying less\nattention to powerful transformer structures.\nOur key contributions are four-fold: (1) exploring two\nspeech-related SSL models based on transformer structures,\ndata2vec and HuBERT, and comparing the results with\nthose models pre-trained in speech recordings; (2) carrying\nout ablation studies for pre-training, thus providing moreintuition for further music SSL system design; and (3) sys-\ntematically comparing the performance on 13 downstream\ntasks, which facilitates comprehensive model evaluation on\na wide range of MIR tasks.\n2. METHOD\nIn order to keep the pre-training and representation evalua-\ntion protocols comparable, we focus on adapting from the\nspeech self-supervised learning frameworks that support\ndirect audio input and end-to-end pre-training. Given our\nintent of exploring the inﬂuence of the pre-training design\nitself, we choose two SSL frameworks mainly distinguished\nby their self-supervised learning targets while sharing very\nsimilar training settings, including model architecture, train-\ning datasets, and evaluation protocols. In this section, we\nbrieﬂy describe the two selected SSL models – data2vec-\n1.0 [21] and HuBERT [14] – in the uniﬁed auto-encoding\nframework (cf. Fig. 1) and discuss the similarities and\ndifferences under music audio pre-training.\n2.1 Music2Vec: Continuous Target Prediction\nWe adapt the pre-training paradigm from the speech version\nof the multi-modal framework data2vec-1.0 [21], where\nthe prediction targets during pre-training are continuous\nrepresentations. We refer to this continuous prediction\nmodel adapted with music recordings as Music2Vec.\nModiﬁed from the design of bootstrap your own latent\n(BYOL) [24], Music2Vec aims to predict continuous latent\nrepresentations from the teacher model for the masked input\naudios, which is illustrated in Fig. 1a. The teacher model\nand student model share the same architecture, and the\nparameters of the teacher model are updated according to\nthe exponential moving average of the student [21]. The\nstudent model takes the partially masked input and is asked\nto predict the average pooling of top- Klayer outputs from\nthe Transformer [25] in the teacher model. In contrast,\nthe teacher model takes the unmasked input and provides\ncontextual prediction targets in the pre-training.\nFollowing the data2vec [5] setting, we train the Mu-\nsic2Vec of 95M parameters with a comparable 1k hours\nof music recordings. Since pre-trained speech models can\nbarely beneﬁt music representation learning [22], we in-\nstead train the base model from scratch to verify its effec-\ntiveness in modelling music audio recordings.\n2.2 MusicHuBERT: Discrete Target Prediction\nAnother efﬁcient speech SSL model, HuBERT [14], is cho-\nsen as the representative of discrete target prediction design.\nWe referred to the music adaption version as MusicHu-\nBERT. It takes masked music audios as input (Similar to\nMusic2Vec) and predicts pre-processed discrete labels cor-\nresponding to the masked area, as shown in Fig. 1b. The\ndiscrete targets are pseudo labels provided by K-means\nthat are trained on the MFCC features of the training au-\ndios. The number of clusters Kof the K-means model is\na hyperparameter, and all the centroids are assigned with\nrandomly initialised embeddings and learned during theProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n458Auto-encoding\nModels\nstudent\nteacher\nG C B\n7A\nm\n7D E\nmG C B\n7A\nm\n7D E\nmG C B\n7A\nm\n7D E\nm\nMasked Audio Original AudioContinuous\nRepresentations\nContextual\nRepresentations\nPredict Predict\nPooling(a) Music2Vec\nAuto-encoding\nModelContextual\nRepresentations\nG C B\n7A\nm\n7D E\nmG C B\n7A\nm\n7D E\nm\nMasked AudioDiscrete\nK-means LabelsPredict (b) MusicHuBERT\nFigure 1 : Pre-training Paradigms of Selected Models. Both of the models are fed with masked audio inputs and predict\ngiven targets without supervised information.\nMusicHuBERT pre-training. MusicHuBERT can also be\ntrained for an extra niterations, where K-means clustering\nis learned from model outputs’ previous iteration. We fol-\nlow the original HuBERT [14] setting to train a model with\n95M parameters of the same size as Music2Vec.\n2.3 similarities & Differences of SSL frameworks\nThis subsection will examine the similarities and differences\nbetween the SSL frameworks mentioned above.\nBoth Music2Vec and MusicHuBERT are annotation-free\nand utilise SSL techniques; their most common character-\nistic is the training task of “reconstructing” information\nfrom masked inputs, making them auto-encoding models.\nDuring the denoising process, these models learn the se-\nmantics contained in the audio. Furthermore, they share\nsimilar model architecture designs, which are inherited from\nwav2vec-2.0 [5], wherein the audio is initially encoded by a\nmulti-layer 1-D CNN feature extractor that maps a 16 kHz\nwaveform to 50 Hz representations. The encoded tokens\nare then fed into a 12-layer transformer block with a hidden\ndimension of H= 768 .\nRegarding the differences in the designs, the most no-\ntable one is that Music2Vec is required to predict continuous\nlatent variables, whereas MusicHuBERT predicts discrete\npseudo-labels. The time cost of SSL target preparation bot-\ntleneck varies according to their mechanism. In Music2Vec,\nthe pre-training consumes twice the model forward time\nsince the target representations from the teacher model are\ninferred on-the-ﬂy. In contrast, MusicHuBERT trains the\nK-means model and infers all the pseudo-labels before train-\ning, which requires high parallel processing ability when\nthe dataset is scaled-up.\n3. DATASET & EV ALUATION\n3.1 Training\nWe use a private dataset with 1000 hours of music audio\nrecordings for pre-training; each sample is a 30s-long ex-cerpt from pop-song or instrumental music. The size of the\npre-training dataset is roughly the same as the pre-training\nfor HuBERT-base and data2vec-audio-base models.\n3.2 Evaluation\nWe evaluate the models on 13 downstream tasks, including\ntimbre classiﬁcation tasks such as genre and instrumen-\ntal classiﬁcation, singing, playing technique classiﬁcation,\nsinger classiﬁcation, and music tagging; emotion-related\ntasks like music mood classiﬁcation and regression; and\nnote-related tasks such as pitch estimation, key detection;\nand sequential tasks like beat tracking.\nMusic Tagging is a multi-label classiﬁcation task. We\nused MagnaTagATune (MTT) [26] and MTG-Jamendo [27]\nfor this task, tag categories of which include genre, in-\nstrumentation, mood, and tempo (e.g. fast) etc. For both\ndatasets, we limit the tag vocabulary to the 50 most com-\nmon tags. We use all clips in MTT and MTG-Jamendo\nfor evaluation. Since many of the audio recordings among\n5.5k MTG-Jamendo excerpts are longer than the 30s, we\naveraged the multiple embeddings computed with a 30s\nsliding window as the overall embedding. The metrics are\nthe macro-average of ROC-AUCs and the average precision\n(AP) / PR-AUC among all top-50 tags.\nKey detection . We use a commonly-used subset of\nGiantsteps-MTG-keys [28] as the training and validation\nset following the data splitting [6], and Giantsteps (GS) [29]\nas the test set. The metric is a reﬁned accuracy that gives\npartial credit to reasonable errors [30].\nGenre classiﬁcation . We report the multi-class classi-\nﬁcation accuracy of the GTZAN [31] dataset, along with\nROC and AP on MTG-Genre for multi-label. We used the\nstandard \"fail-ﬁltered\" split [32] for GTZAN.\nEmotion score regression. The Emomusic dataset [33]\ncontains 744 music clips of 45 seconds, each reported on a\n2-D valence-arousal plane after listening. We use the same\ndataset split as [6]. The evaluation metric is the determina-\ntion coefﬁcient ( r2) between the model regression resultsProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n459and human annotations of arousal (EmoA) and valence\n(EmoV) [33]. We split the 45-second clip into a 5-second\nsliding window for inference and averaged the prediction.\nInstrument classiﬁcation . We use the Nsynth [34] and\nMTG-instrument datasets. The former is a multi-class task\non 306k audio samples in 11 instruments with accuracy\nas an indicator. The latter is a subset of MTG-Jamendo,\ncontaining 25k audio tracks and 41 instrument tags; each\ntrack can contain multiple instruments and is evaluated on\nROC and AP.\nPitch classiﬁcation . Given these audios are short mono-\nphonic audio, this task is multi-class to determine which\nof the 128 pitch categories, and the accuracy is used as an\nevaluation metric.\nVocal technique detection . We use the V ocalSet dataset\n[35], which is the only publicly available dataset for the\nstudy of singing techniques. The dataset contains the vocals\nof 17 different singing techniques in various contexts for\na total of 10.1 hours. As the audio clips are divided into 3\nseconds, the task only requires a judgment on the type of\ntechnique and not on the start and end of the technique. We\nused the same 10 different singing techniques as in [36] as\na subset and used the same 15 singers as the training and\nvalidation sets and 5 singers as the test set. Since there is\nno accepted division between training and validation sets,\nwe selected 9 singers as the training set and 6 singers as the\nvalidation set. All the 3-second segments originate from the\nsame recording are allocated to the same part of the split\n(e.g. all in the testing set).\nSinger identiﬁcation is to identify the vocal performer\nfrom a given recording. We randomly divided the V ocalSet\ndataset, which contains 20 different professional singers\n(9 female and 11 male), into a training set, validation set\nand testing set based on a ratio of 12:8:5, all containing the\nsame 20 singers.\nBeat tracking . We use an ofﬂine approach to the binary\nclassiﬁcation, i.e. the model can use the following infor-\nmation from each frame to help with inference. The model\nneeds to output frame-by-frame predictions at a certain fre-\nquency and post-process them using a dynamic Bayesian\nnetwork (DBN) [37], the same methods with supervised\nSOTA. The DBN is implemented using madmom [38]. The\ndataset we use is GTZAN Rhythm [39]. We also label the\ntwo adjacent frames of each label as beat, a common way\nof smoothing in beat tracking. The model is evaluated using\nthe f_measure implemented in mir_eval [30], and the\nprediction is considered correct if the difference between\nthe predicted event and the ground truth does not exceed\n20ms. In this task, some models were trained on other\ndatasets, and the full GTZAN set was used as the test set.\nFor all cases, however, we use GTZAN-train as the training\nset and GTZAN-test as the test set.\nEmotion Tagging. We use MTG-MoodTheme, another\nsubset of MTG-Jamendo [27] that contains 18.5k audio\ntracks and 59 tags. Unlike Emomusic, this is a multi-label\ntask, with ROC and AP as metrics.4. EXPERIMENTAL RESULTS\nWe use the fairseq framework2from Meta to train Mu-\nsicHuBERT and Music2Vec models. All the MusicHu-\nBERT and Music2Vec models are trained for 400k steps\nwith 8×NVIDIA A100-40GB GPUs. Training with 8\nGPUs takes around 2−3days. The experimental results\nare chieﬂy as follows.\nOur ﬁndings suggest these SSL models pre-trained on\nspeech can be helpful for MIR tasks, but pre-trained on\nmusic is generally more helpful, besides some exceptions.\nIn section 4.2, we identify the strengths along with weak-\nnesses of training strategies, revealing areas for further\nimprovement. In section 4.3, we discuss the effect of hyper-\nparameters in pretext tasks.\n4.1 Pre-trained on Speech and Music\nTable 1 demonstrates the performance of HuBERT3and\ndata2vec4SSL models that were pre-trained on speech\nrecordings and music recordings separately. Here, we only\nconsider the SOTA performance trained with the same\ndataset train/valid/test split. All of the models are used\nas parameter-frozen feature extractors. The weighted sum\nof one output of the CNN tokeniser as well as the 12 outputs\nof all the transformer layers, are combined with an MLP as\nthe back end. The MLP has only one single 512-dimension\nhidden layer. The learning rate of the probing is set to 1e-3.\nFor the HuBERT model, the results pre-trained on speech\nrecordings are comparable with SOTA on tasks like music\ntagging, beat tracking, pitch estimation and singing tech-\nnique classiﬁcation etc., and are surpassed by the results\npre-trained on music audio on most of the downstream tasks\nbesides pitch estimation on Nsynth and key detection on\nGS. For pitch detection, the data samples in Nsynth are a\nsingle note played by one single monophonic instrument,\nwhich is similar to speech data. So it is reasonable that Hu-\nBERT pre-trained on speech data is capable of modelling\na single pitch. Although HuBERT surpasses the vanilla\nMusicHuBERT on GS and Nsynth-pitch, it is surpassed\nby the results of MusicHuBERT with an ablation study on\npre-training hyperparameters (shown in Table 2).\nFor data2vec, the data2vec-audio results are also com-\nparable with SOTA on many tasks and have a large gap on\nothers, and overall surpassed by Music2Vec or its ablation\nstudy shown in Table 3 on most of the tasks as well. But the\ndata2vec results of beat tracking on GTZAN-Rhythm and\nsinger identiﬁcation on V ocalset surpassed all Music2Vec.\nV ocalset includes singing of different phonemes with dif-\nferent singing techniques by different singers. The speech\nSSL system is capable of modelling diverse phonemes in\nASR and various timbres of speakers but has less focus\non timbre in speaking techniques you may ﬁnd in opera.\nOn the contrary, the music SSL models may focus more\non phonemes (lyrics) and singing timbre (techniques) but\ninclude less focus on the singer itself. For beat tracking, we\nobserve that the performance is reduced signiﬁcantly when\n2https://github.com/facebookresearch/fairseq\n3https://huggingface.co/facebook/HuBERT-base-ls960\n4https://huggingface.co/facebook/data2vec-audio-baseProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n460Table 1 : Experimental performance of the SSL baseline systems on all downstream tasks\nDownstream MTTGS keyGTZAN EMO Nsynth Nsynth V ocalSet V ocalSet GTzAN MTG MTG MTG MTG\ndataset Genre Instr pitch tech singer Rhythm Instrument MoodTheme Genre Top50\nMetrics ROC AP Reﬁned Acc Acc EmoVEmoA Acc Acc Acc Acc F1 (beat) ROC AP ROC AP ROC AP ROC AP\nHuBERT89.8 36.4 15.0 64.8 31.0 57.5 68.2 79.4 61.0 58.8 83.5 73.2 17.0 74.0 11.6 85.0 16.3 81.8 26.5base\nMusicHuBERT90.2 37.7 14.7 70.0 42.1 66.5 69.3 77.4 65.9 75.3 88.6 75.5 17.8 76.0 13.9 86.5 18.0 82.4 28.1base\ndata2vec88.4 33.6 15.5 60.7 23.0 49.6 69.3 77.7 64.9 74.6 36.4 73.1 16.9 73.3 11.0 83.5 14.5 80.6 24.8audio base\nMusic2vec89.1 35.1 19.0 59.7 38.5 61.9 69.4 88.9 68.3 69.5 33.5 73.1 16.3 74.3 12.2 85.2 16.5 81.4 26.2vanilla\nSOTA 92.0 [40] 41.4 [6] 74.3 [28] 82.1 [41] 61.7 72.1 [6]78.2 [20] 89.2 [23] 65.6 [36] 80.3 [42] 80.6 [43] 78.8 20.2 [44] 78.6 16.1 [23] 87.7 20.3 [44] 84.3 32.1 [23]\nthe number of transformer layers increases from 0 to 12.\nThis shows that the data2vec structure may not be useful\nfor learning temporal information.\n4.2 Pre-trained with Different Paradigms\nFrom Table 1, we can tell that MusicHuBERT is more\npromising than Music2vec given that it provides better re-\nsults in most of the downstream tasks, especially genre\nclassiﬁcation on GTZAN, emotion regression on EMO and\nbeat tracking on GTZAN. But it is worse on single-pitch\nestimation on Nsynth, along with key detection on GS.\nThese phenomena suggest pre-training with the HuBERT\nparadigm is strongly correlated with the MFCC feature in-\nformation used for k-means. Therefore, the quantisation\nresults lack multi-pitch information, including harmony or\nchord modelling, that is essential to key detection. The\nfollowing research can use the chroma feature to replace\nMFCCs5. On the contrary, the mask prediction for the deep\nfeature in the data2vec pre-training paradigm is clearly bet-\nter but still has much room for improvement compared to\nthe SOTA. Although the deep feature still lacks sufﬁcient\nharmonic information for key detection, it already contains\nenough information for single-pitch estimation, and the\nMFCCs may focus more on the timbre of instruments in-\nstead of the fundamental frequency. Apparently, Music2Vec\ncan learn pitch information more freely. Besides, data2vec\nis generally a bit worse for tagging than Music2vec, and\nboth are signiﬁcantly worse on beat tracking compared to\nHuBERT and MusicHuBERT.\n4.3 Ablation Studies on Pretraining Hyperparameters\nHere, we carry out an ablation study of hyperparameter\nsearch under both pre-training paradigms. Given the time\nlimitation, we did not extract features on MTG datasets and\nonly calculated the results in another 9 downstream tasks.\n4.3.1 Ablation Study on MusicHuBERT\nWe use the number of clusters k =500 and k=2000. For the\ncase k=500, we increase the dimension of MFCC features\nfrom 13, which is commonly used in the speech community,\nto 20, which is widely used in sound event detection. Thus,\nthe dimension of MFCCs combined with their delta features\n5For more information on this, please refer to our following paper\nMERT: Acoustic Music Understanding Model with Large-Scale Self-\nsupervised Training at https://arxiv.org/abs/2306.00107and delta-delta features have 39 and 60 dimensions respec-\ntively. For the case of k=2000, we use the 768-dimension\ndeep feature learned from the ﬁrst iteration experiment to\ncarry out the second iteration k-means.\nFrom Table 2, we can see that MusicHuBERT with\nk=2000 is better than the k=500 case for most of the tasks.\nGiven HuBERT is good for speech when k=100 or k=500,\nwhich is roughly the number of human phonemes, this im-\nplies music tokens or notes are much richer than speech and\ntherefore need a larger number for quantisation.\nThe results on k-means for deep features are better than\nthe vanilla MusicHuBERT besides genre classiﬁcation on\nGTZAN, singer identiﬁcation on vocalset, and singing tech-\nniques classiﬁcation on vocalset. This implies the MFCCs\nfeatures are good for modelling the human voice, regardless\nof speech or singing. The results of GTZAN may be due to\nthe randomness as the dataset is very small.\nBesides, increasing the dimension of MFCCs provides\nno signiﬁcant difference among most of the tasks other\nthan tasks on Nsynth and GS. Increased dimensionality for\nMFCC features can provide more detailed information on\nimpulse response for a sound event. Thus, monophonic in-\nstrumental notes can be better modelled with 60-dimension\nMFCC features. Furthermore, the emotion regression also\nprovides different results, but the average of the two metrics\nis nearly the same, providing no signiﬁcant improvement.\n4.3.2 Ablation Study on Music2Vec\nWe use audio ﬁles with 30s length, mask span length 10,\nmask probability 65%, target top- 8transformer layer the\nteacher model as a deep feature, and training step 400K as\nthe vanilla setting. We conduct parameter searching and\ncorrelation analysis for Music2Vec pretraining, including\nmasking strategy, training steps, the learning target layers,\nand recording length; the results are shown in Table 3.\nWe revise the masking strategy by changing the mask\nspan length andmask token probability in the data2vec-\naudio-base setting. Mask token probability is the probability\nfor each token to be chosen as the start of the span to be\nmasked, the length of which can also be adapted for differ-\nent data modalities. The results in Table 3 show that the\nother span value and other mask token probability provide\na bit worse results on nearly all the tasks. This suggests that\nthe data2vec hyperparameters for speech pre-training are\ngenerally helpful for music pre-training.\nGiven the fact that early transformer layer representa-\ntions generally perform well on key detection and beatProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n461Table 2 : Ablation study on MusicHuBERT hyperparameters (k is the number of MFCC clusters)\nDownstream MTTGS keyGTZAN EMO Nsynth Nsynth V ocalSet V ocalSet GTZAN Average\ndataset Genre Instr pitch tech singer Rhythm Score\nMetrics ROC AP Reﬁned Acc Acc EmoVEmoA Acc Acc Acc Acc F1 (beat) score\nHuBERT 89.8 36.4 15.0 64.8 31.0 57.5 68.2 79.4 61.0 58.8 83.5 59.8\nk=2000 MFCC dim=39 90.2 37.7 14.7 70.0 42.1 66.5 69.3 77.4 65.9 75.3 88.6 64.4\nk=2000 iter2 90.4 37.5 13.8 68.3 43.3 67.4 70.0 80.3 63.6 70.4 88.8 63.8\nk=500 MFCC dim=39 89.6 36.1 15.7 64.5 41.0 67.7 66.7 76.8 60.5 72.3 87.5 62.4\nk=500 MFCC dim=60 90.3 38.0 17.6 69.7 40.8 67.5 70.3 79.0 66.2 75.5 88.6 65.0\nTable 3 : Ablation study on Music2Vec hyperparameters (span is mask span, prob is mask probability, step is training steps,\ntarget=12 uses all 12 transformer layers, and crop5s uses 5s music excerpts)\nDownstream MTTGS keyGTZAN EMO Nsynth Nsynth V ocalSet V ocalSet GTZAN Average\ndataset Genre Instr pitch tech singer Rhythm Score\nMetrics ROC AP Reﬁned Acc Acc EmoVEmoA Acc Acc Acc Acc F1 (beat) score\ndata2vec 88.4 33.6 15.5 60.7 23.0 49.6 69.3 77.7 64.9 74.6 36.4 55.2\nvanilla 89.1 35.1 19.0 59.7 38.5 61.9 69.4 88.9 68.3 69.5 33.5 57.8\nspan=5 87.3 32.0 15.7 47.6 22.7 41.2 64.2 84.8 56.7 53.8 33.2 49.7\nspan=15 88.7 34.3 16.4 56.6 39.0 58.8 67.1 88.1 63.1 61.9 33.1 55.2\nprob=50 88.5 34.0 23.7 59.3 40.6 55.0 66.8 87.7 64.9 61.7 33.9 56.3\nprob=80 88.2 33.9 18.4 50.3 36.7 55.7 67.9 88.9 64.2 65.2 33.7 55.1\nstep=800k 87.7 32.7 20.3 54.5 34.9 47.3 66.9 87.5 65.6 65.1 33.4 55.0\ntarget=12 89.7 35.2 26.5 64.5 41.7 64.2 71.1 89.2 71.0 73.2 34.1 60.6\ncrop5s 90.0 36.6 18.5 76.6 53.4 71.6 68.3 88.9 71.3 72.4 33.9 61.8\ntracking, we modify the prediction target provided by the\nteacher model. We change the prediction target in Mu-\nsic2Vec from the original one, that is, the average of the\ntop-8 layer representations, to all the 12 layers. The results\nin Table 3 show that Music2Vec actually beneﬁts, not only\nfrom the potentially preserved key information shown by a\nsigniﬁcant increase on GS but all the other tasks as well.\nFurthermore, we use audio length cropping to shorten\nmusic excerpts since longer sequences are more difﬁcult to\nmodel. Note that the combined audio length in a batch on\na single GPU is not altered, and the hardware environment\nremains the same, making a single training batch contain\na larger number of music samples when clips are cropped.\nDue to the position embedding in the SSL systems, the\nmodel can get information more than 5 seconds after pre-\ntraining on only 5-second music recordings. But the key\ndetection provides worse results which may lead to the fact\nthat a local key within a 5-second song may not be identical\nto the global key in the whole music sentence.\n5. CONCLUSION & DISCUSSION\nIn this paper, we explore the music variants of two dis-\ntinctive speech-related transformer-based SSL models,\ndata2vec and HuBERT. Our ﬁndings suggest that pre-\ntraining with music recordings rather than speech can gen-\nerally improve performance on a wide range of MIR tasks,\neven when the models and training are designed for speech.\nThere are exceptions for data2vec, however, such as singer\nidentiﬁcation, the dataset of which is similar to the speech\ndataset used to pre-train. Thus, when resources are lim-\nited, our suggestion is to use speech pre-training models,\ngiven that they can provide helpful information about music\nalready. Speech data can be beneﬁcial if lacking a sufﬁ-cient vocal dataset with different singers, but one should\nuse mainly music data if possible.\nFurthermore, we can use the same speech training hy-\nperparameters for masked span and masked probability in\nmusic pre-training. But some other hyperparameters, such\nas the number for pseudo label clustering, might be the\nshortage of pretext strategies. We identiﬁed some limita-\ntions of existing speech SSL systems, especially in the case\nof harmonic information and diversity of music notes. One\nsuggestion is to emphasise key or harmonic in the pretext\ntask for music SSL models by using more than just MFCC\nfeatures. Also, the number of categories for quantisation in\nk-means should be much larger if necessary, given the num-\nber of pitch, chord, and timbre categories is much larger\nthan the number of human speech phones. This diversity in\nmusic might be a bottleneck for both speech SSL systems to\nlearn good music features. For one thing, the larger number\nof clusters for k-means in HuBERT is expensive to calculate,\nmaking it harder to scale up, preventing transformer-based\nmodels from reaching their potential for better performance\nand longer sequence modelling. In addition, it may not\nbe easy for data2vec to jointly learn deeper features. We\nmay need curriculum learning skills or manually-designed\nfeatures to increase training stability.\nAnother general suggestion for pre-training recognises\nthat batch size should be as diverse as possible. Given that\nthe memory of one single machine is limited, it is a good\nidea to shorten the length of audio to be modelled at ﬁrst,\nallowing for an increase in batch size, and then train another\nlanguage model for long sequence modelling.\nWe believe the ﬁndings in this paper to be of value in\nunderstanding the potential for SSL speech models applied\nto music, and we have offered some general insights about\nmusic modelling that resulted from this study.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4626. ACKNOWLEDGEMENTS\nYinghao Ma is a research student at the UKRI Centre\nfor Doctoral Training in Artiﬁcial Intelligence and Music,\nsupported by UK Research and Innovation [grant number\nEP/S022694/1]. Yizhi Li is fully funded by an industrial\nPhD studentship (Grant number: 171362) from the Uni-\nversity of Shefﬁeld, UK. This work is supported by the\nNational Key R&D Program of China (2020AAA0105200).\nWe acknowledge IT Services at The University of Shefﬁeld\nfor the provision of services for High-Performance Com-\nputing. We would also like to express great appreciation for\nthe suggestions from faculties Dr Chris Donahue, and Dr\nRoger Dannenberg, as well as the facility support from Mr.\nYulong Zhang in the preliminary stage.\n7. REFERENCES\n[1]J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language\nunderstanding,” in Proceedings of NAACL-HLT , 2019,\npp. 4171–4186.\n[2]S. Gururangan, A. Marasovi ´c, S. Swayamdipta, K. Lo,\nI. Beltagy, D. Downey, and N. A. Smith, “Don’t stop\npretraining: Adapt language models to domains and\ntasks,” arXiv preprint arXiv:2004.10964 , 2020.\n[3]J. Sarzynska-Wawer, A. Wawer, A. Pawlak, J. Szy-\nmanowska, I. Stefaniak, M. Jarkiewicz, and\nL. Okruszek, “Detecting formal thought disor-\nder by deep contextualized word representations,”\nPsychiatry Research , vol. 304, p. 114135, 2021.\n[4]A. Newell and J. Deng, “How useful is self-supervised\npretraining for visual tasks?” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2020, pp. 7345–7354.\n[5]A. Baevski, Y . Zhou, A. Mohamed, and M. Auli,\n“wav2vec 2.0: A framework for self-supervised learning\nof speech representations,” Advances in neural infor-\nmation processing systems , vol. 33, pp. 12 449–12 460,\n2020.\n[6]R. Castellon, C. Donahue, and P. Liang, “Codiﬁed\naudio language modeling learns useful representa-\ntions for music information retrieval,” arXiv preprint\narXiv:2107.05677 , 2021.\n[7]Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D.\nPlumbley, “Panns: Large-scale pretrained audio neural\nnetworks for audio pattern recognition,” IEEE/ACM\nTransactions on Audio, Speech, and Language Process-\ning, vol. 28, pp. 2880–2894, 2020.\n[8]J. Turian, J. Shier, H. R. Khan, B. Raj, B. W. Schuller,\nC. J. Steinmetz, C. Malloy, G. Tzanetakis, G. Velarde,\nK. McNally et al. , “Hear: Holistic evaluation of audio\nrepresentations,” in NeurIPS 2021 Competitions and\nDemonstrations Track . PMLR, 2022, pp. 125–145.[9]Y . Zhao and J. Guo, “Musicoder: A universal music-\nacoustic encoder based on transformer,” in International\nConference on Multimedia Modeling . Springer, 2021,\npp. 417–429.\n[10] H.-H. Wu, C.-C. Kao, Q. Tang, M. Sun, B. McFee, J. P.\nBello, and C. Wang, “Multi-task self-supervised pre-\ntraining for music classiﬁcation,” in ICASSP 2021-2021\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2021, pp.\n556–560.\n[11] Y . Li, R. Yuan, G. Zhang, Y . MA, C. Lin, X. Chen,\nA. Ragni, H. Yin, Z. Hu, H. He et al. , “Map-music2vec:\nA simple and effective baseline for self-supervised mu-\nsic audio representation learning,” in ISMIR late braking\ndemo , 2022.\n[12] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,\nand I. Sutskever, “Jukebox: A generative model for\nmusic,” arXiv preprint arXiv:2005.00341 , 2020.\n[13] Y . Li, R. Yuan, G. Zhang, Y . Ma, C. Lin, X. Chen,\nA. Ragni, H. Yin, Z. Hu, H. He et al. , “Large-scale\npretrained model for self-supervised music audio repre-\nsentation learning,” in Digital Music Research Network\n(DMRN) workshop , 2022.\n[14] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia,\nR. Salakhutdinov, and A. Mohamed, “Hubert: Self-\nsupervised speech representation learning by masked\nprediction of hidden units,” IEEE/ACM Transactions on\nAudio, Speech, and Language Processing , vol. 29, pp.\n3451–3460, 2021.\n[15] J. Spijkervet and J. A. Burgoyne, “Contrastive\nlearning of musical representations,” arXiv preprint\narXiv:2103.09410 , 2021.\n[16] D. Yao, Z. Zhao, S. Zhang, J. Zhu, Y . Zhu, R. Zhang,\nand X. He, “Contrastive learning with positive-negative\nframe mask for music representation,” in Proceedings of\nthe ACM Web Conference 2022 , 2022, pp. 2906–2915.\n[17] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y.\nLee, “Mockingjay: Unsupervised speech representation\nlearning with deep bidirectional transformer encoders,”\ninICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2020, pp. 6419–6423.\n[18] M. Ravanelli, J. Zhong, S. Pascual, P. Swietojanski,\nJ. Monteiro, J. Trmal, and Y . Bengio, “Multi-task self-\nsupervised learning for robust speech recognition,” in\nICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2020, pp. 6989–6993.\n[19] A. Saeed, D. Grangier, and N. Zeghidour, “Contrastive\nlearning of general-purpose audio representations,” in\nICASSP 2021-2021 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2021, pp. 3875–3879.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n463[20] L. Wang, P. Luc, Y . Wu, A. Recasens, L. Smaira,\nA. Brock, A. Jaegle, J.-B. Alayrac, S. Dieleman, J. Car-\nreira et al. , “Towards learning universal audio repre-\nsentations,” in ICASSP 2022-2022 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2022, pp. 4593–4597.\n[21] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and\nM. Auli, “Data2vec: A general framework for self-\nsupervised learning in speech, vision and language,”\ninInternational Conference on Machine Learning .\nPMLR, 2022, pp. 1298–1312.\n[22] A. Ragano, E. Benetos, and A. Hines, “Learning mu-\nsic representations with wav2vec 2.0,” arXiv preprint\narXiv:2210.15310 , 2022.\n[23] M. C. McCallum, F. Korzeniowski, S. Oramas,\nF. Gouyon, and A. F. Ehmann, “Supervised and un-\nsupervised learning of audio representations for music\nunderstanding,” in ISMIR , 2022.\n[24] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond,\nE. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo,\nM. Gheshlaghi Azar et al. , “Bootstrap your own latent-\na new approach to self-supervised learning,” Advances\nin neural information processing systems , vol. 33, pp.\n21 271–21 284, 2020.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in neural informa-\ntion processing systems , vol. 30, 2017.\n[26] E. Law, K. West, M. I. Mandel, M. Bay, and J. S.\nDownie, “Evaluation of algorithms using games: The\ncase of music tagging.” in ISMIR . Citeseer, 2009, pp.\n387–392.\n[27] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and\nX. Serra, “The mtg-jamendo dataset for automatic music\ntagging.” ICML, 2019.\n[28] F. Korzeniowski and G. Widmer, “End-to-end musical\nkey estimation using a convolutional neural network,”\nin2017 25th European Signal Processing Conference\n(EUSIPCO) . IEEE, 2017, pp. 966–970.\n[29] P. Knees, Á. Faraldo Pérez, H. Boyer, R. V ogl, S. Böck,\nF. Hörschläger, M. Le Goff et al. , “Two data sets for\ntempo estimation and key detection in electronic dance\nmusic annotated from user corrections,” in Proceedings\nof the 16th International Society for Music Informa-\ntion Retrieval Conference (ISMIR); 2015 Oct 26-30;\nMálaga, Spain.[Málaga]: International Society for Mu-\nsic Information Retrieval, 2015. p. 364-70. Interna-\ntional Society for Music Information Retrieval (ISMIR),\n2015.\n[30] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon, O. Ni-\neto, D. Liang, D. P. Ellis, and C. C. Raffel, “Mir_eval:\nA transparent implementation of common mir metrics.”\ninISMIR , 2014, pp. 367–372.[31] G. Tzanetakis and P. Cook, “Musical genre classiﬁca-\ntion of audio signals,” IEEE Transactions on speech and\naudio processing , vol. 10, no. 5, pp. 293–302, 2002.\n[32] C. Kereliuk, B. L. Sturm, and J. Larsen, “Deep learning\nand music adversaries,” IEEE Transactions on Multime-\ndia, vol. 17, no. 11, pp. 2059–2071, 2015.\n[33] M. Soleymani, M. N. Caro, E. M. Schmidt, C.-Y . Sha,\nand Y .-H. Yang, “1000 songs for emotional analysis of\nmusic,” in Proceedings of the 2nd ACM international\nworkshop on Crowdsourcing for multimedia , 2013, pp.\n1–6.\n[34] J. Engel, C. Resnick, A. Roberts, S. Dieleman,\nM. Norouzi, D. Eck, and K. Simonyan, “Neural au-\ndio synthesis of musical notes with wavenet autoen-\ncoders,” in International Conference on Machine Learn-\ning. PMLR, 2017, pp. 1068–1077.\n[35] J. Wilkins, P. Seetharaman, A. Wahl, and B. Pardo,\n“V ocalset: A singing voice dataset.” in ISMIR , 2018, pp.\n468–474.\n[36] Y . Yamamoto, J. Nam, and H. Terasawa, “De-\nformable cnn and imbalance-aware feature learning\nfor singing technique classiﬁcation,” arXiv preprint\narXiv:2206.12230 , 2022.\n[37] S. Böck, F. Krebs, and G. Widmer, “Joint beat and\ndownbeat tracking with recurrent neural networks.” in\nISMIR . New York City, 2016, pp. 255–261.\n[38] S. Böck, F. Korzeniowski, J. Schlüter, F. Krebs, and\nG. Widmer, “madmom: a new Python Audio and Music\nSignal Processing Library,” in Proceedings of the 24th\nACM International Conference on Multimedia , Amster-\ndam, The Netherlands, 10 2016, pp. 1174–1178.\n[39] U. Marchand and G. Peeters, “Swing ratio estimation,”\ninDigital Audio Effects 2015 (Dafx15) , 2015.\n[40] Q. Huang, A. Jansen, L. Zhang, D. P. Ellis, R. A.\nSaurous, and J. Anderson, “Large-scale weakly-\nsupervised content embeddings for music recommen-\ndation and tagging,” in ICASSP 2020-2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2020, pp. 8364–8368.\n[41] J. Lee, J. Park, K. L. Kim, and J. Nam, “Samplecnn:\nEnd-to-end deep convolutional neural networks using\nvery small ﬁlters for music classiﬁcation,” Applied Sci-\nences , vol. 8, no. 1, p. 150, 2018.\n[42] M. Modrzejewski, P. Szachewicz, and P. Rokita, “Trans-\nfer learning with deep neural embeddings for music\nclassiﬁcation tasks,” in Artiﬁcial Intelligence and Soft\nComputing: 21st International Conference, ICAISC\n2022, Zakopane, Poland, June 19–23, 2022, Proceed-\nings, Part I . Springer, 2023, pp. 72–81.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n464[43] M. Heydari, F. Cwitkowitz, and Z. Duan, “Beatnet:\nCrnn and particle ﬁltering for online joint beat downbeat\nand meter tracking,” arXiv preprint arXiv:2108.03576 ,\n2021.\n[44] P. Alonso-Jiménez, X. Serra, and D. Bogdanov, “Music\nrepresentation learning based on editorial metadata from\ndiscogs,” in ISMIR , 2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n465"
    },
    {
        "title": "Composer&apos;s Assistant: An Interactive Transformer for Multi-Track MIDI Infilling.",
        "author": [
            "Martin E. Malandro"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265291",
        "url": "https://doi.org/10.5281/zenodo.10265291",
        "ee": "https://zenodo.org/records/10265291/files/000038.pdf",
        "abstract": "We introduce Composer's Assistant, a system for interactive human-computer composition in the REAPER digital audio workstation. We consider the task of multi-track MIDI infilling when arbitrary track-measures have been deleted from a contiguous slice of measures from a MIDI file, and we train a T5-like model to accomplish this task. Composer's Assistant consists of this model together with scripts that enable interaction with the model in REAPER. We conduct objective and subjective tests of our model. We release our complete system, consisting of source code, pretrained models, and REAPER scripts. Our models were trained only on permissively-licensed MIDI files.",
        "zenodo_id": 10265291,
        "dblp_key": "conf/ismir/Malandro23",
        "keywords": [
            "interactive human-computer composition",
            "REAPER digital audio workstation",
            "multi-track MIDI infilling",
            "T5-like model",
            "scripts for interaction",
            "objective and subjective tests",
            "permissively-licensed MIDI files",
            "system release",
            "source code",
            "pretrained models"
        ],
        "content": "COMPOSER’S ASSISTANT: AN INTERACTIVE TRANSFORMER FOR\nMULTI-TRACK MIDI INFILLING\nMartin E. Malandro\nSam Houston State University\nmalandro@shsu.edu\nABSTRACT\nWe introduce Composer’s Assistant, a system for interac-\ntive human-computer composition in the REAPER digi-\ntal audio workstation. We consider the task of multi-track\nMIDI inﬁlling when arbitrary track-measures have been\ndeleted from a contiguous slice of measures from a MIDI\nﬁle, and we train a T5-like model to accomplish this task.\nComposer’s Assistant consists of this model together with\nscripts that enable interaction with the model in REAPER.\nWe conduct objective and subjective tests of our model.\nWe release our complete system, consisting of source code,\npretrained models, and REAPER scripts. Our models were\ntrained only on permissively-licensed MIDI ﬁles.\n1. INTRODUCTION\nMany generative models for music exist. For instance,\nMuseNet [1] and SymphonyNet [2] can generate or con-\ntinue a piece of music, and Music Transformer [3] can con-\ntinue a piano performance or harmonize a piano melody.\nWhen we tried using these tools as compositional aides,\nhowever, we quickly ran into limitations. For instance,\nwhile Music Transformer is capable of harmonizing a\ngiven melody, it does not offer the ability to keep part of\nthe harmonization and regenerate the other part. MuseNet\nand SymphonyNet can generate a continuation of a user’s\nprompt, but do not allow the user to regenerate individ-\nual instruments or measures within the continuation while\nkeeping the rest of the continuation intact.\nDeepBach [4] can perform inﬁlling on Bach-like\nchorales in a window speciﬁed by the user. Motivated\nby the idea of extending the DeepBach user experience to\nmore styles, arbitrary collections of instruments, and arbi-\ntrary inﬁlling target locations, we train a transformer [5, 6]\nmodel on the task of multi-track MIDI inﬁlling. Our model\nallows composers to generate new notes for arbitrary sub-\nsets of track-measures in their compositions, conditioned\non any contiguous slice of measures containing the sub-\nset. (By a track-measure , we simply mean a measure\nwithin a track—see Figure 1.) We also build a novel sys-\ntem for interacting with our model in the REAPER digital\n© M. E. Malandro. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0). Attribution: M. E.\nMalandro, “Composer’s Assistant: An Interactive Transformer for Multi-\nTrack MIDI Inﬁlling”, in Proc. of the 24th Int. Society for Music Infor-\nmation Retrieval Conf., Milan, Italy, 2023.\nFigure 1 . A prompt in REAPER, followed by a model out-\nput. Vertical lines separate measures. Users place empty\nMIDI items in REAPER to tell the model in which mea-\nsures to write notes, and track names to tell the model what\ninstrument is on each track. A track-measure in the prompt\nis boxed. Our model writes at least one note into every\ntrack-measure in every empty MIDI item in the prompt.\naudio workstation (DAW).1Our system is cross-platform\nand easy to install. When a user runs one of our REAPER\nscripts, a model prompt is built directly from the slice\nof measures selected in the user’s REAPER project, our\nmodel evaluates the prompt, and the model output is writ-\nten back into the user’s project—see Figure 1. All of this\nhappens within a few seconds, allowing the user to listen\nto the output, modify it to create a new prompt, generate\nan output from that, etc. This allows our model to be used\nin an interactive manner, where model outputs are reﬁned\nby the user over the course of several prompts.\nWe note that our inﬁlling objective includes continuing\na piece of music, simply by including empty measures at\nthe end of the piece in the prompt. Additionally, our model\nhas the ability to write variations: One can randomly mask\n1/nof the track-measures in a measure selection and ask\nthe model to ﬁll in those parts, then feed the result back\ninto the model with another 1/nmasked, and so on, until\n1Our system and video demo are available at https://github.\ncom/m-malandro/composers-assistant-REAPER .327all track-measures have been masked and ﬁlled.\nToward the end of this project we discovered MMM\n[7,8], which consists of two separate GPT-2–like [9] mod-\nels trained on the tasks of measure inﬁlling and track in-\nﬁlling. The authors include code to use these models to\ninﬁll arbitrary subsets of track-measures, as we do. MMM\ncomes in 4-bar and 8-bar variants, which are limited to in-\nputs with 12 and 6 tracks, respectively, and its web demo\nis limited to inputs having a 4/4 time signature.\nThe primary contributions of this work are as follows.\nFirst, in Section 3 we introduce a novel data ﬁltering and\npreprocessing approach, applicable to any MIDI dataset\nused for training models. Our approach helps rectify cer-\ntain issues we have encountered when using other mod-\nels. Second, we train and release a new model, capable\nof inﬁlling arbitrary track-measures in an arbitrary slice of\nmeasures in a MIDI ﬁle, with no effective restriction (aside\nfrom a soft input token limit of 1650) on tempos, number\nof measures, or number of instrument tracks. Tracks may\nbe polyphonic or monophonic in any combination. The\nonly time signature restriction is that all measures must be\neight quarter notes or fewer. Our model is more ﬂexible\nthan MMM and compares favorably to MMM in both ob-\njective and subjective tests—see Sections 6–7. Addition-\nally, our model was trained only on permissively-licensed\nMIDI ﬁles, so its outputs should be usable by composers\nwith minimal risk—see Section 5. Finally, we release our\ncomplete system, including training code and scripts that\nenable rapid interaction with our model in REAPER. Our\nmodel is the ﬁrst DAW-integrated model capable of inﬁll-\ning parts for all 128 pitched MIDI instruments (including\nrepeated instruments) and drums, in any combination.\n2. RELATED WORK\nAs mentioned in Section 1, MMM [7, 8] performs multi-\ntrack inﬁlling for all MIDI instruments (subject to bar and\ntrack limits), and DeepBach [4] performs multi-track in-\nﬁlling for Bach-like chorales. Coconet [10] also performs\nmulti-track inﬁlling for Bach-like chorales. MusIAC [11]\nincorporates user controls and performs track-based and\nmeasure-based inﬁlling, although its inputs and outputs\nare limited to a maximum of three tracks, 16 measures,\nand four common time signatures. MusicV AE [12] can in-\nterpolate between two given clips of music, which can be\nviewed as a type of inﬁlling. To our knowledge, all other\nexisting music inﬁlling systems are limited to monophonic\ninﬁlling [13–17] or single-instrument inﬁlling [18, 19].\nGenerating or continuing a piece of music can be seen\nas a special case of inﬁlling. Models which can generate\nor continue a piece of music include [1–3, 20, 21].\nPrevious DAW-integrated generative music systems in-\nclude [4,18,22]. NONOTO [23] is a model-agnostic inter-\nface that can be linked with a model to perform interactive\nmeasure-based inﬁlling. This interface could potentially\nbe altered to allow for the expanded type of inﬁlling our\nmodel is capable of. However, we opt to build an inter-\nface between our model and REAPER directly, essentially\nusing REAPER as the GUI for our model.3. DATA FILTERING AND PREPROCESSING\nIn this section we describe our ﬁltering and preprocess-\ning approach, any portion of which can be applied to any\ndataset of MIDI ﬁles. First, we remove from the dataset\nany ﬁle whose notes seem to have no relation to the un-\nderlying grid (Section 3.1). Next, we dedupe ﬁles from\nthe dataset using note onset chromagrams (Section 3.2).\nFinally, we preprocess all remaining ﬁles to standardize\nproperties like track order (Section 3.3). This ﬁnal prepro-\ncessing step includes a method for detecting and remov-\ning shifted duplicate and near-duplicate tracks within ﬁles\n(Section 3.4).\n3.1 Cosine Similarity for On-Grid Note Detection\nEvery MIDI ﬁle has a measure and grid structure deﬁned\nby tempo and time signature events. However, MIDI ﬁle\nauthors are free to ignore this structure, and frequently do\nwhen recording free-ﬂowing performances. Other mod-\nels we have used occasionally write a note in the wrong\nplace—e.g., a 32nd note away from where it clearly should\nbe—and a small experiment we ran suggests that training\non MIDI ﬁles that don’t quantize well to the grid used by\nthe model is a major cause of this. To address this, we re-\nmove from our dataset any MIDI ﬁle whose note onsets ap-\npear to have no relation to the underlying grid. This is not\nas simple as checking whether all (or most) note onsets oc-\ncur on the grid, as many MIDI ﬁle authors who use the grid\ninclude “humanization” of note timings, where many note\nonsets that occur slightly off the grid nevertheless quantize\ncorrectly to the grid. For instance, in a MIDI ﬁle with a res-\nolution of 960 ticks per quarter note, a humanized quarter-\nnote performance might have notes occurring in a 40-tick\nwindow centered at every 960th tick.\nTo perform this ﬁltering, given a MIDI ﬁle M, we quan-\ntize the note onsets in Mto a resolution of 12 ticks per\nquarter note, and we form a length-12 vector vMwhose\nith entry ( i∈ {0,...,11}) is the number of note onsets\ninMoccurring iticks after a grid quarter note. The idea\nis that if the note onsets in Mhave nothing to do with the\ngrid, then vMwill point in a similar direction to the uni-\nform vector v1= (1,...,1)∈R12. We therefore compute\nthe cosine of the angle θMbetweenvMandv1:\ncos(θM) =⟨vM,v1⟩\n||vM||·||v1||,\nand we declare a threshold Tsuch that when cos(θM)> T\nwe remove the ﬁle Mfrom our dataset. Hand exploration\nindicated that T= 0.8was a reasonable threshold, which\nwe chose for this project. We note that a straight fully-on-\ngrid 8th-note pattern Mhascos(θM)≈0.41and a straight\nfully-on-grid 16th-note pattern Mhascos(θM)≈0.58.\n3.2 Deduping Using Note Onset Chromagrams\nWe dedupe our dataset to avoid data imbalance during\ntraining and to prevent overlap between our training and\ntest sets. Given a MIDI ﬁle M, we compute a size-12 set\nof note onset chromagrams using the following procedure.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n328First, we remove all drum tracks from M. Then, using a\n12-tick-per-quarter-note grid, we quantize the note onsets\ninMto the nearest 16th note or 8th note triplet. Then,\nwe remove all empty measures at the beginning and end of\nM, and we replace each set of contiguous empty measures\nwithinMwith one empty measure. Then, for each tick in\nMand for each pitch class, we record whether Mhas at\nleast one note onset of that pitch class at that tick. This\ninformation comprises one note onset chromagram for M.\nThe other 11 come from repeating this procedure for each\npossible transposition of M. We dedupe the dataset by\nkeeping only one ﬁle with a given set of note onset chro-\nmagrams. Quantization helps us catch ﬁles that differ only\ntrivially in grid resolution and/or note onset times, while\ntransposition helps us catch ﬁles that differ only in key.\n3.3 Preprocessing of Individual MIDI Files\nAfter deduping, we preprocess each MIDI ﬁle in our\ndataset in the following way.\nFirst, we arrange the information in the MIDI ﬁle so that\neach track holds notes for one instrument. We order tracks\naccording to their MIDI instrument number (0–127), tak-\ning drums as instrument 128. We also consolidate all drum\ntracks to a single track, and we apply a drum simpliﬁcation\nmap (consolidating, e.g., three different bass drum pitches\nto a single pitch).\nNext, we apply pedal information in the ﬁle (if present)\nto extend note lengths, and then delete all continuous con-\ntroller (cc) data. We do not model cc data in this project.\nWith the exception of drums, we allow multiple tracks\nto use the same instrument. However, when this happens,\nif there is more than one track having a given instrument,\nwe remove all but one of those tracks that are equal to, a\nshift of, or close to a shift of another track with the same\ninstrument, using the procedure in Section 3.4.\nWe impose the restriction that all measures must be\neight quarter notes or fewer. If any time signature in the\nﬁle declares longer measures, we alter the time signatures\nto shorten the measures.\nFinally, using a 24-tick-per-quarter-note grid, we quan-\ntize the events in the ﬁle to the nearest 32nd note or 16th\nnote triplet. This is ultimately the level of quantization we\nuse to train our model. (Earlier experiments involved quan-\ntizing to 16th notes or 16th notes + 8th note triplets, which\nwe found insufﬁcient for expressive generation.)\n3.4 Removing Shifted Duplicate and Near-Duplicate\nTracks\nA MIDI ﬁle may contain duplicate tracks. Such tracks\ncontain no useful information for modeling, so we remove\nthem. Shifted duplicate tracks are frequently used by MIDI\nﬁle authors to encode delay effects (as the MIDI spec of-\nfers no way to encode the use of a delay directly). Choos-\ning to use a delay is a mixing decision, not a compositional\ndecision, and we want our model to focus on making com-\npositional decisions, so we remove shifted duplicate tracks\nas well. We have also seen tracks that are duplicates orshifted duplicates of other tracks within a ﬁle, plus or mi-\nnus a few notes and/or humanization. We remove such\nnear-duplicate tracks as well.\nGiven a note nin a track T, let st(n)and end(n)in-\ndicate the start and end times of the note n, respectively,\nand let pitch (n)∈ {0,...,127}indicate the MIDI pitch of\nn. We record, for p∈ {0,...,127}, the union of closed\nintervals\nIT(p) =∪n∈T:pitch(n)=p{[st(n),end(n)]} ⊆R,\nand we deﬁne |IT|=/summationtext127\np=0|IT(p)|,where|IT(p)|is the\nsum of the lengths of the disjoint intervals in IT(p).\nGiven tracks T1andT2, we deﬁne the overlap measure\nO(T1,T2)∈[0,1]⊆Rto be\nO(T1,T2) =/summationtext127\np=0|IT1(p)∩IT2(p)|\nmax(|IT1|,|IT2|).\nThe idea is that O(T1,T2)measures the percentage of the\nnote intervals in the larger of the two tracks accounted for\nby the note intervals in the smaller of the two.\nWe use a threshold of 0.9 for asserting near-overlap be-\ntween two tracks. As we go through the tracks in a MIDI\nﬁle in order, a later track Tis thrown out if there exists an\nearlier track T0using the same instrument such that some\nshiftTsofTof no more than a half note has the property\nthatO(T0,Ts)≥0.9. In our experience with our trained\nmodel, we have found this preprocessing step sufﬁcient to\nprevent the model from outputting duplicates or shifted du-\nplicates of tracks in its inputs.\n4. OUR LANGUAGE\nAfter applying the procedure from Section 3 to a collec-\ntion of MIDI ﬁles, we process the ﬁles into an event-based\nlanguage for modeling. Our language is similar to the\nstandard event-based MIDI language used for piano per-\nformance modeling in [3]. However, we use explicit mea-\nsure tokens to denote the start of each measure. Also, we\ndo not model velocity of individual notes directly. Instead,\nwe assign a dynamics level to each measure based on the\naverage velocity of the notes in the measure. We use eight\ndynamics levels, with thresholds learned from data.\nThe tokens used by our language are as follows:\n• M:x,x∈ {0,...,7}. Declares a measure of dynam-\nics levelx.\n• B:x,x∈ {0,...,7}. Declares the tempo (BPM)\nlevel at the start of a measure. We use eight tempo\nlevels, with thresholds learned from data.\n• L:x,x∈ {1,...,192}. Declares that a measure has\nlength equal to xticks.\n• I:x,x∈ {0,...,128}. Changes the current instru-\nment to MIDI instrument x(128 = drums).\n• R:x,x∈ {1,...,63}. Declares that the current in-\nstrument is the same MIDI instrument as another in-\nstrument in the ﬁle/project, but on a different track.\nHigherxvalues indicate lower average pitch.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n329Figure 2 . We tokenize this measure as M:5 B:6 L:96 I:0\nw:48 d:24 N:67 I:0 R:1 d:48 N:36 N:43 N:48 I:73 w:12\nd:12 N:84 w:12 N:81 w:12 N:79. Note that piano and ﬂute\nare MIDI instruments 0 and 73.\n• N:x,x∈ {0,...,127}. Note of pitch x. Used by\ninstruments 0–127.\n• D:x,x∈ {0,...,127}. Drum hit of drum pitch x.\nUsed by instrument 128.\n• d:x,x∈ {0,...,192}. Sets the duration of each\nnote declared from this point forward to xticks.\n• w:x,x∈ {1,...,191}. Advances the current inser-\ntion point for new notes in the measure by xticks.\n•<extra_id_x> ,x∈ {0,...,255}. Mask tokens.\n•<mono> ,<poly> . Instructs the model to write a\nmonophonic or polyphonic part for a masked part.\nFor our purposes a monophonic part is one where no\ntwo notes in the part have the same onset tick.\nTokens are assembled in a standardized manner to rep-\nresent measures. Each measure begins with M: x, B:x, and\nL:xtokens. I: commands are included for a measure only\nwhen that instrument is present in the measure. We do not\nintermingle instrument note instructions as we write each\nmeasure from left to right (as MuseNet [1] did), as that\nwould make it difﬁcult to mask individual instrument parts\nwithin measures. Rather, we write the full part for one in-\nstrument within the measure before writing the full part for\nthe next instrument within the measure. Figure 2 contains\nan example of a tokenized measure.\nTo form songs, we simply concatenate measures.\n5. MODEL, DATA, AND TRAINING PROCEDURE\nWe use recent recommendations from the language mod-\neling community to design and train our model. Based\non the recommendations in [24–26], we choose a T5\n(full, relative-attention) encoder-decoder architecture [6].\nWe opt for a full attention model because such models\nwere found to outperform memory-efﬁcient models in [24]\nwhen the full input sequence ﬁts in memory, as we expect\nto be the case in most real-world applications of our model.\nAlso, we adopt the DeepNarrow strategy of [27], opting\nfor a model dimension of 384, 10 encoder layers, and 10\ndecoder layers. For training, we use the pytorch [28]Hugging Face [29] implementation of T5. For inference,\nwe use nucleus sampling [30] with a threshold of p= 0.95.\nTo train a model that is essentially free of copyright\nworry, we collect MIDI ﬁles from the internet marked\nas being in the public domain, freely available to use\nwithout attribution, or available under a CC BY license.\nWe exclude ﬁles marked as having share-alike or non-\ncommercial licenses, since we want composers to be able\nto use model outputs however they wish. We also collect\nprivate donations and ﬁles from the internet for which we\nsecure direct permission from the MIDI ﬁle authors to use\nfor training. This results in a dataset of approximately 40K\nMIDI ﬁles after ﬁltering. Most of our training ﬁles are in\nWestern classical, folk, and hymnal styles, although some\nmodern styles are also present.\nWe follow the standard approach to the training of large\nlanguage models of splitting our training procedure into\npretraining and ﬁnetuning phases. A similar approach was\nalso used in [31]. For pretraining, we use the T5 corrupted-\nspan sequence-to-sequence objective [6]. We begin by pre-\ntraining on the 192K training ﬁles in the CocoChorales\n[32] dataset and their piano reductions for three epochs.\nThe CocoChorales are only used for this initial pretraining\nto teach the model the basics of music theory and our lan-\nguage. We then move on to our dataset of 40K MIDI ﬁles.\nAfter tokenization and corruption, we greedily chunk each\nsong into inputs of 512 or fewer (short) or 1650 or fewer\n(long) tokens. Additionally, each song in our dataset is\ntransposed a random amount between -5 and +6 semitones\n(inclusive) for each epoch. Following the recommenda-\ntions in [24], we train our model on short examples for 20\nepochs and then long examples for 11 epochs. We release\nthe resulting pretrained model, which others may ﬁnd use-\nful for ﬁnetuning on downstream tasks.\nFor ﬁnetuning, we continue to leverage the corrupted-\nspan sequence-to-sequence objective to ﬁnetune our model\non the task of multi-track MIDI inﬁlling. We create train-\ning examples from songs in our training dataset by taking\nslices of measures from the songs and masking subsets of\ntrack-measures from these slices. During ﬁnetuning ev-\nery N:, D:, d:, and w: token for a given track-measure\nis masked, and corresponds to a single mask token. With\nprobability 0.75, we add a <poly> or<mono> token cor-\nresponding to the nature of the masked tokens for each\nmask. (We choose not to include these tokens in every\ntraining example since users will not always include these\ninstructions in their prompts.) Finetuning examples are\nlimited to inputs with a maximum of 1650 tokens and out-\nputs with a maximum of 1650 tokens.\nWe generate our ﬁnetuning masks by selecting from\nmask patterns that we consider to be musically relevant\nand/or useful for training. To help train our model for use\non small numbers of measures, we also occasionally (15%\nof the time) truncate examples to a random smaller number\nof measures than the number allowed by our token limits.\nAs with pretraining, each example is transposed randomly.\nWe ﬁnetune our model for 51 epochs, and we release the\nresulting ﬁnetuned model.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n330Task Our Model Our Model -MP MMM-8 MMM-4\nNoteF1results. Higher is better.\n8-bar random inﬁll 0.5414±(0.1887)a0.5315±(0.1904)b0.4153±(0.1819)c0.4025±(0.1652)d\n16-bar random inﬁll∗0.5771±(0.1661)a0.5705±(0.1669)b0.4133±(0.1534)c0.4059±(0.1399)d\n8-bar track inﬁll 0.179±(0.1902)a0.1634±(0.18)b0.1063±(0.1573)d0.1427±(0.1646)c\n16-bar track inﬁll 0.1773±(0.1752)a0.1609±(0.165)b0.1107±(0.1383)d0.1467±(0.1489)c\n8-bar last-bar ﬁll 0.5019±(0.2719)a0.5063±(0.2751)a0.4329±(0.2445)b0.3756±(0.2289)c\n16-bar last-bar ﬁll∗0.5415±(0.2853)a0.539±(0.2823)a0.4338±(0.2468)b0.3818±(0.2293)c\nPitch class histogram entropy difference results. Lower is better.\n8-bar random inﬁll 0.2845±(0.1627)a0.2948±(0.1597)b0.3045±(0.1561)c0.3049±(0.1497)c\n16-bar random inﬁll 0.2691±(0.1325)a0.2797±(0.1326)b0.3093±(0.124)c0.3063±(0.1138)c\n8-bar track inﬁll 0.3933±(0.3032)c0.42±(0.3134)d0.2864±(0.2966)a0.3021±(0.2517)b\n16-bar track inﬁll 0.3842±(0.2654)c0.3995±(0.2763)c0.284±(0.2348)a0.3036±(0.2072)b\n8-bar last-bar ﬁll 0.3018±(0.2661)a0.3072±(0.2692)a0.3213±(0.2602)b0.3439±(0.2777)c\n16-bar last-bar ﬁll∗0.2851±(0.2652)a0.2925±(0.2672)a0.3209±(0.2619)b0.3454±(0.2741)c\nGroove similarity results. Higher is better.\n8-bar random inﬁll 0.9534±(0.0298)a0.9519±(0.0306)b0.9333±(0.0369)c0.9314±(0.0364)d\n16-bar random inﬁll∗0.956±(0.027)a0.9552±(0.0275)b0.9323±(0.0337)c0.9317±(0.032)c\n8-bar track inﬁll 0.9115±(0.0592)a0.9069±(0.0617)b0.8921±(0.0695)d0.8987±(0.0626)c\n16-bar track inﬁll 0.9113±(0.0547)a0.9082±(0.0553)b0.8946±(0.0561)d0.9011±(0.0536)c\n8-bar last-bar ﬁll 0.9517±(0.0414)a0.9524±(0.0411)a0.9381±(0.045)b0.9334±(0.0457)c\n16-bar last-bar ﬁll∗0.9544±(0.0481)a0.9542±(0.0424)a0.938±(0.051)b0.9339±(0.0475)c\nTable 1 . Objective inﬁlling summary statistics. All cells are of the form mean ±(std dev)s, wheresis a letter. Different\nletters within a row indicate signiﬁcant location differences ( p <0.01) in the samples for that row according to a Wilcoxon\nsigned rank test with Holm-Bonferroni correction. Asterisks ( ∗) indicate a signiﬁcant performance difference ( p <0.01)\nbetween a 16-bar task and the 8-bar task in the previous row for our model according to a Wilcoxon rank sum test.\n6. OBJECTIVE EV ALUATION OF OUR MODEL\nTo form our test set, we select a set of 2500 MIDI ﬁles from\nthe Lakh MIDI dataset [33, 34] that is disjoint (according\nto the procedure in Section 3.2) from our training set, all in\n4/4 time and having at least 16 measures. Given a MIDI ﬁle\nin our test set, for each of the three mask patterns below,\nwe select an 8- and a 16-measure slice of the ﬁle and mask\nthe selected slice with that mask pattern. We thus generate\nsix test examples from each test ﬁle, corresponding to the\nsix different tasks on which we evaluate models. Given a\nslice of measures, our mask patterns for testing are:\n1. Random mask: Each track-measure in the slice is\nmasked with probability 0.5.\n2. Track mask: Up to half of the tracks tare selected\nat random from the slice, and every measure of each\nsuch track tis masked.\n3. Last-bar mask: Given the last measure mof the\nslice, measure mof every track is masked. This\npattern is used to measure the ability of models to\ncontinue songs.\nThe ground truth for each example consists of the masked\nnotes in the example. In our test data, 99% and 75% of our\n8-measure and 16-measure prompts (respectively) encode\nto 1650 or fewer tokens. When input prompts are longer\nthan 1650 tokens, we chunk the prompts prior to evaluating\nthem with our model.To compare our model to MMM [7, 8], we modify the\nMMM Colab worksheet to run our examples through the\nMMM models in batches. We recreate our test exam-\nples, quantizing them from their underlying MIDI ﬁles\nto MMM’s 12-tick-per-quarter-note resolution, and then\nmodify them to accommodate the restrictions of the MMM\nmodels: Since the 4-bar and 8-bar MMM models are lim-\nited to inputs containing a maximum of 12 and 6 tracks,\nrespectively, we chunk each test example into 4-bar and 8-\nbar chunks, and then we split each chunk into sub-chunks\nconsisting of up to 12 and 6 tracks. The MMM models\nhave a strict input + output token limit of 2048, so when\nsub-chunking, we only add enough tracks to a sub-chunk\nto ensure that the input + ground truth has no more than\n2048 tokens. This biases the comparison in favor of the\nMMM models somewhat, as this requires us to look at the\nlength of the ground truth as part of the input chunking\nprocedure. Also, our test set is contained in MMM’s train-\ning set, but there is no reasonable way to avoid this as the\nMMM models were trained on the full Lakh MIDI dataset.\n(We wanted a diverse and well-randomized test set, and the\nLakh MIDI dataset is the only publicly-available dataset\nwe are aware of that ﬁts this bill.)\nWe evaluate models with standard metrics: Note F1\n[35], average pitch class histogram entropy difference\n[19, 36], and average groove similarity [19, 36]. Note F1\nmeasures how closely the generated notes match, on a\nnote-for-note basis, the notes in the ground truth. (For our\npurposes, a generated note matches a note nin the groundProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n331Real Music Our Model MMM\n1st place 66 32 27\nAvg rank 1.664 2.032 2.304\np-values Our Model MMM\nMMM 0.0239 -\nReal Music 0.0034 2.3·10−5\nTable 2 . Subjective results from our listening test.\ntruth if and only if its onset tick, measure, pitch, and track\nmatch exactly those of n.) The other metrics measure how\nwell certain higher-level statistics of the generated notes\nmatch those of real music. For pitch class histogram en-\ntropy calculations, drums are ignored. Each metric is com-\nputed on a per-example basis, and then for each model,\ntask, and metric, the 2500 results are averaged to give the\nresults in Table 1. For fairness of groove similarity com-\nparison, we use a denominator of 48 for all models. (This\nis reasonable, as our models and the MMM models both\neffectively have 48 possible note onset positions per 4/4\nmeasure.) For our model, “-MP” indicates that the exam-\nples were evaluated without <mono> or<poly> tokens\npresent.\nFor each row of Table 1, we perform a Wilcoxon signed\nrank test [37] with Holm-Bonferroni correction [38]. We\nﬁnd signiﬁcant differences between our model and the\nMMM models in all 18 rows, with our model outperform-\ning the MMM models in 16 out of 18 rows. The MMM\nmodels outperform our model only for pitch class his-\ntogram entropy difference for full-track inﬁlling.\nAdditionally, we ﬁnd a signiﬁcant difference in our\nmodel’s performance when <mono> and<poly> tokens\nare included in prompts in 11 out of 18 rows. All signif-\nicant differences favor including these tokens, suggesting\nthat the development of additional user controls (as in [11])\nwould be a useful line of future work.\nFinally, a Wilcoxon rank sum test [37] reveals signiﬁ-\ncant differences ( p <0.01) in 8-bar versus 16-bar results\nfor our model in ﬁve out of nine comparisons. All sig-\nniﬁcant differences favor the 16-bar results, emphasizing\nthe importance of training on longer measure slices. How-\never, we never observe a signiﬁcant difference in 8-bar ver-\nsus 16-bar results for track inﬁlling, suggesting that larger\ncontext windows generally provide no additional useful in-\nformation for completing this particular task.\nAdditional experiments not reported here indicate that\nscaling our training approach (training larger models on\nmore data) is a feasible path for improving model perfor-\nmance on the metrics presented here.\n7. SUBJECTIVE EV ALUATION OF OUR MODEL\nWhile the results in Section 6 are encouraging, the ground\ntruth may not reﬂect the only reasonable way to ﬁll in miss-\ning notes. To help address this, we conducted a small lis-\ntening test with 25 participants. We prepared nine exam-\nples mostly involving melodic generation. Each exampleconsisted of three 8-measure clips, one of which was real\nmulti-track music. The other two clips were created by\nremoving some tracks from the real music and using our\nmodel and MMM to ﬁll those tracks. Participants were\nshown ﬁve of the nine examples at random, and for each\nexample were asked to rank the three clips in order of pref-\nerence. Results are given in Table 2.\nA Wilcoxon signed rank test with Holm-Bonferroni cor-\nrection reveals signiﬁcant differences in rankings between\nall three types of music, with p-values given in Table 2. In\nthis test we see a clear preference for real music, and a sig-\nniﬁcant (p <0.05) preference for music generated by our\nmodel over music generated by MMM. One expert partic-\nipant commented that melodies generated by the models\nwere generally more directionless than those in real music,\noften failing to drive towards a cadence or “payoff.” We\nagree with this assessment, and this is a shortcoming of\nour model that we hope to address in future work.\n8. LIMITATIONS AND RISKS\nOur model writes music that is reﬂective of its training set.\nMost of our training ﬁles are in Western classical, folk, and\nhymnal styles. While we included in our training set only\nﬁles marked as being permissively licensed, it is possible\nthat some ﬁles were mismarked. It is also theoretically\npossible for our model to output copyrighted music, even\nif such music was not present in the training set.\nThe most common request we have heard from com-\nposers to whom we have shown our system is personaliza-\ntion. Generally speaking, they do not want systems that\nwrite full songs, and they do not want systems that write\n“generic” music. Rather, they want systems that can sug-\ngest ideas in their style. Some small experiments indicate\nthat our ﬁnetuned model can be personalized by individu-\nals (by continuing to ﬁnetune the model on their own MIDI\nﬁles) to write in their styles. Low-rank adaptation [39] of\nour model may also be possible. Personalization is an av-\nenue we would like to explore formally in future work. For\nnow, our code supports training by users, and our model di-\nmensions were chosen carefully to enable this on consumer\nhardware. A video card with 6 GB of RAM is sufﬁcient to\ntrain our released model on examples with input and out-\nput lengths of 1024, and 12 GB of RAM is sufﬁcient to\ntrain on examples with input and output lengths of 1650.\nWhile this can beneﬁt composers who wish to use our sys-\ntem, there is also the risk that our models may be trained\nby users to impersonate the styles of others.\n9. CONCLUSION\nWe have introduced Composer’s Assistant, a system for\ninteractive human-computer composition in the REAPER\ndigital audio workstation. Composer’s Assistant performs\nmulti-track MIDI inﬁlling and comes with an easy-to-use\ninterface. We have released our source code, a pretrained\nmodel, a ﬁnetuned model, and scripts for interacting with\nour ﬁnetuned model in REAPER. Our models were trained\nonly on permissively-licensed MIDI ﬁles.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n33210. ACKNOWLEDGMENT\nWe thank the many contributors to our MIDI training set\nfor this project. Contributor acknowledgments can be\nviewed at our website.2We also thank the IT department\nat Sam Houston State University for building and main-\ntaining the computational server on which we trained our\nmodel.\n11. REFERENCES\n[1] C. Payne, “MuseNet,” openai.com/blog/musenet,\n2019.\n[2] J. Liu, Y . Dong, Z. Cheng, X. Zhang, X. Li, F. Yu, and\nM. Sun, “Symphony Generation with Permutation In-\nvariant Language Model,” in Proc. 23rd Int. Society for\nMusic Information Retrieval Conf. , Bengaluru, India,\n2022, pp. 551–558.\n[3] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon,\nC. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman,\nM. Dinculescu, and D. Eck, “Music Transformer,” in\nInt. Conf. Learning Representations , 2019.\n[4] G. Hadjeres, F. Pachet, and F. Nielsen, “DeepBach:\na Steerable Model for Bach Chorales Generation,” in\nProc. 34th Int. Conf. Machine Learning , ser. Proceed-\nings of Machine Learning Research, D. Precup and\nY . W. Teh, Eds., vol. 70. PMLR, 06–11 Aug 2017,\npp. 1362–1371.\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is All you Need,” in Advances in Neu-\nral Information Processing Systems , I. Guyon, U. V .\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, Eds., vol. 30. Curran As-\nsociates, Inc., 2017.\n[6] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring\nthe Limits of Transfer Learning with a Uniﬁed Text-to-\nText Transformer,” Journal of Machine Learning Re-\nsearch , vol. 21, no. 140, pp. 1–67, 2020.\n[7] J. Ens and P. Pasquier, “MMM : Exploring Conditional\nMulti-Track Music Generation with the Transformer,”\narXiv preprint arXiv: 2008.06048 , 2020.\n[8] ——, “Flexible Generation with the Multi-Track Mu-\nsic Machine,” in Extended Abstracts for the Late-\nBreaking Demo Session of the 21st Int. Society for\nMusic Information Retrieval Conf. , Montréal, Canada,\n2020.\n[9] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever, “Language Models are Unsupervised\nMultitask Learners,” 2019. [Online]. Available: https:\n//cdn.openai.com/better-language-models/language_\nmodels_are_unsupervised_multitask_learners.pdf\n2https://github.com/m-malandro/\ncomposers-assistant-REAPER .[10] C.-Z. A. Huang, T. Cooijmans, A. Roberts,\nA. Courville, and D. Eck, “Counterpoint by Con-\nvolution,” in Proc. 18th Int. Society for Music\nInformation Retrieval Conf. , Suzhou, China, 2017, pp.\n211–218.\n[11] R. Guo, I. Simpson, C. Kiefer, T. Magnusson, and\nD. Herremans, “MusIAC: An Extensible Genera-\ntive Framework for Music Inﬁlling Applications with\nMulti-level Control,” in Artiﬁcial Intelligence in Music,\nSound, Art and Design. EvoMUSART 2022. Lecture\nNotes in Computer Science , T. Martins, N. Rodríguez-\nFernández, and S. M. Rebelo, Eds., vol. 13221.\nSpringer, Cham, 2022, pp. 341–356.\n[12] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and\nD. Eck, “A Hierarchical Latent Vector Model for\nLearning Long-Term Structure in Music,” in Proc. 35th\nInt. Conf. Machine Learning , ser. Proceedings of Ma-\nchine Learning Research, J. Dy and A. Krause, Eds.,\nvol. 80. PMLR, 10–15 Jul 2018, pp. 4364–4373.\n[13] G. Mittal, J. Engel, C. Hawthorne, and I. Simon, “Sym-\nbolic Music Generation with Diffusion Models,” in\nProc. 22nd Int. Society for Music Information Retrieval\nConf. , online, 2021, pp. 468–475.\n[14] S. Wei, G. Xia, Y . Zhang, L. Lin, and W. Gao, “Mu-\nsic Phrase Inpainting Using Long-Term Representation\nand Contrastive Loss,” in IEEE Int. Conf. Acoustics,\nSpeech and Signal Processing , 2022, pp. 186–190.\n[15] A. Pati, A. Lerch, and G. Hadjeres, “Learning to Tra-\nverse Latent Spaces for Musical Score Inpainting,” in\nProc. 20th Int. Society for Music Information Retrieval\nConf. , Delft, The Netherlands, 2019, pp. 343–351.\n[16] K. Chen, C. Wang, T. Berg-Kirkpatrick, and S. Dub-\nnov, “Music SketchNet: Controllable Music Gen-\neration via Factorized Representations of Pitch and\nRhythm,” in Proc. 21st Int. Society for Music Infor-\nmation Retrieval Conf. , Montréal, Canada, 2020, pp.\n77–84.\n[17] C. Benetatos and Z. Duan, “Draw and Listen! A\nSketch-Based System for Music Inpainting,” Trans.\nInt. Society for Music Information Retrieval , vol. 5,\nno. 1, pp. 141–155, 2022.\n[18] G. Hadjeres and L. Crestel, “The Piano Inpainting Ap-\nplication,” arXiv preprint arXiv: 2107.05944 , 2021.\n[19] C.-J. Chang, C.-Y . Lee, and Y .-H. Yang, “Variable-\nLength Music Score Inﬁlling via XLNet and Musically\nSpecialized Positional Encoding,” in Proc. 22nd Int.\nSociety for Music Information Retrieval Conf. , online,\n2021, pp. 97–104.\n[20] Y .-S. Huang and Y .-H. Yang, “Pop Music Transformer:\nBeat-Based Modeling and Generation of Expressive\nPop Piano Compositions,” in Proc. 28th ACM\nInt. Conf. Multimedia , ser. MM ’20. New York,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n333NY , USA: Association for Computing Machinery,\n2020, p. 1180–1188. [Online]. Available: https:\n//doi.org/10.1145/3394171.3413671\n[21] W.-Y . Hsiao, J.-Y . Liu, Y .-C. Yeh, and Y .-H. Yang,\n“Compound Word Transformer: Learning to Com-\npose Full-Song Music over Dynamic Directed Hyper-\ngraphs,” in 35th AAAI Conf. Artiﬁcial Intelligence ,\n2021, pp. 178–186.\n[22] A. Roberts, C. Kayacik, C. Hawthorne, D. Eck, J. En-\ngel, M. Dinculescu, and S. Nørly, “Magenta Studio:\nAugmenting Creativity with Deep Learning in Ableton\nLive,” in Proc. Int. Workshop on Musical Metacreation\n(MUME) , 2019.\n[23] T. Bazin and G. Hadjeres, “NONOTO: A Model-\nagnostic Web Interface for Interactive Music Compo-\nsition by Inpainting,” in Proc. 10th Int. Conf. Compu-\ntational Creativity , 2019.\n[24] J. Phang, Y . Zhao, and P. J. Liu, “Investigating Efﬁ-\nciently Extending Transformers for Long Input Sum-\nmarization,” arXiv preprint arXiv: 2208.04347 , 2022.\n[25] N. Shazeer, “GLU Variants Improve Transformer,”\narXiv preprint arXiv: 2002.05202 , 2020.\n[26] Y . Tay, M. Dehghani, S. Abnar, H. W. Chung, W. Fe-\ndus, J. Rao, S. Narang, V . Q. Tran, D. Yogatama, and\nD. Metzler, “Scaling Laws vs Model Architectures:\nHow does Inductive Bias Inﬂuence Scaling?” arXiv\npreprint arXiv: 2207.10551 , 2022.\n[27] Y . Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar,\nH. W. Chung, S. Narang, D. Yogatama, A. Vaswani,\nand D. Metzler, “Scale Efﬁciently: Insights from Pre-\ntraining and Finetuning Transformers,” in Int. Conf.\nLearning Representations , 2022.\n[28] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad-\nbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito,\nM. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, “PyTorch: An Im-\nperative Style, High-Performance Deep Learning Li-\nbrary,” in Advances in Neural Information Processing\nSystems . Curran Associates, Inc., 2019, vol. 32, pp.\n8024–8035.\n[29] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jer-\nnite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. Rush, “Transformers: State-of-the-\nArt Natural Language Processing,” in Proc. 2020 Conf.\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations . Association for Computa-\ntional Linguistics, Oct. 2020, pp. 38–45.\n[30] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi,\n“The Curious Case of Neural Text Degeneration,” in\nInt. Conf. Learning Representations , 2020.[31] C. Donahue, H. H. Mao, Y . E. Li, G. W. Cot-\ntrell, and J. McAuley, “LakhNES: Improving Multi-\nInstrumental Music Generation with Cross-domain\nPre-training,” in Proc. 20th Int. Society for Music Infor-\nmation Retrieval Conf. , Delft, The Netherlands, 2019.\n[32] Y . Wu, J. Gardner, E. Manilow, I. Simon,\nC. Hawthorne, and J. Engel, “The Chamber En-\nsemble Generator: Limitless High-Quality MIR\nData via Generative Modeling,” arXiv preprint\narXiv:2209.14458 , 2022.\n[33] C. Raffel, “The Lakh MIDI Dataset v0.1,” https://\ncolinraffel.com/projects/lmd/.\n[34] ——, “Learning-Based Methods for Comparing Se-\nquences, with Applications to Audio-to-MIDI Align-\nment and Matching,” Ph.D. dissertation, 2016.\n[35] J. Gardner, I. Simon, E. Manilow, C. Hawthorne,\nand J. Engel, “MT3: Multi-Task Multitrack Music\nTranscription,” in Int. Conf. Learning Representations ,\n2022.\n[36] S.-L. Wu and Y .-H. Yang, “The Jazz Transformer on\nthe Front Line: Exploring the Shortcomings of AI-\nComposed Music Through Quantitative Measures,” in\nProc. 21st Int. Society for Music Information Retrieval\nConf. , Montréal, Canada, 2020, pp. 142–149.\n[37] F. Wilcoxon, “Individual Comparisons by Ranking\nMethods,” Biometrics Bulletin , vol. 1, no. 6, pp. 80–\n83, 1945.\n[38] S. Holm, “A Simple Sequentially Rejective Multiple\nTest Procedure,” Scandinavian Journal of Statistics ,\nvol. 6, no. 2, pp. 65–70, 1979.\n[39] E. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang,\nL. Wang, and W. Chen, “LoRA: Low-Rank Adapta-\ntion of Large Language Models,” in Int. Conf. Learning\nRepresentations , 2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n334"
    },
    {
        "title": "Gender-Coded Sound: Analysing the Gendering of Music in Toy Commercials via Multi-Task Learning.",
        "author": [
            "Luca Marinelli",
            "György Fazekas",
            "Charalampos Saitis"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265249",
        "url": "https://doi.org/10.5281/zenodo.10265249",
        "ee": "https://zenodo.org/records/10265249/files/000018.pdf",
        "abstract": "Music can convey ideological stances, and gender is just one of them. Evidence from musicology and psychology research shows that gender-loaded messages can be reliably encoded and decoded via musical sounds. However, much of this evidence comes from examining music in isolation, while studies of the gendering of music within multimodal communicative events are sparse. In this paper, we outline a method to automatically analyse how music in TV advertising aimed at children may be deliberately used to reinforce traditional gender roles. Our dataset of 606 commercials included music-focused mid-level perceptual features, multimodal aesthetic emotions, and content analytical items. Despite its limited size, and because of the extreme gender polarisation inherent in toy advertisements, we obtained noteworthy results by leveraging multi-task transfer learning on our densely annotated dataset. The models were trained to categorise commercials based on their intended target audience, specifically distinguishing between masculine, feminine, and mixed audiences. Additionally, to provide explainability for the classification in gender targets, the models were jointly trained to perform regressions on emotion ratings across six scales, and on mid-level musical perceptual attributes across twelve scales. Standing in the context of MIR, computational social studies and critical analysis, this study may benefit not only music scholars but also advertisers, policymakers, and broadcasters.",
        "zenodo_id": 10265249,
        "dblp_key": "conf/ismir/MarinelliFS23",
        "keywords": [
            "Music",
            "ideological stances",
            "gender",
            "multimodal communicative events",
            "TV advertising",
            "children",
            "traditional gender roles",
            "gendered messages",
            "perceptual features",
            "emotion ratings"
        ],
        "content": "GENDER-CODED SOUND: ANALYSING THE GENDERING OF MUSIC IN\nTOY COMMERCIALS VIA MULTI-TASK LEARNING\nLuca Marinelli György Fazekas Charalampos Saitis\nC4DM, Queen Mary University of London, UK\n{l.marinelli, c.saitis, george.fazekas}@qmul.ac.uk\nABSTRACT\nMusic can convey ideological stances, and gender is just\none of them. Evidence from musicology and psychol-\nogy research shows that gender-loaded messages can be\nreliably encoded and decoded via musical sounds. How-\never, much of this evidence comes from examining mu-\nsic in isolation, while studies of the gendering of music\nwithin multimodal communicative events are sparse. In\nthis paper, we outline a method to automatically analyse\nhow music in TV advertising aimed at children may be de-\nliberately used to reinforce traditional gender roles. Our\ndataset of 606 commercials included music-focused mid-\nlevel perceptual features, multimodal aesthetic emotions,\nand content analytical items. Despite its limited size, and\nbecause of the extreme gender polarisation inherent in toy\nadvertisements, we obtained noteworthy results by lever-\naging multi-task transfer learning on our densely annotated\ndataset. The models were trained to categorise commer-\ncials based on their intended target audience, speciﬁcally\ndistinguishing between masculine, feminine, and mixed\naudiences. Additionally, to provide explainability for the\nclassiﬁcation in gender targets, the models were jointly\ntrained to perform regressions on emotion ratings across\nsix scales, and on mid-level musical perceptual attributes\nacross twelve scales. Standing in the context of MIR, com-\nputational social studies and critical analysis, this study\nmay beneﬁt not only music scholars but also advertisers,\npolicymakers, and broadcasters.\n1. INTRODUCTION\nThe purpose of this study is to analyse gender-coding in\na context where music is secondary to other modalities\nand serves a clear purpose, such as in advertisement. Our\naim is to investigate how music may be employed to re-\ninforce traditional gender roles in toy commercials, and\nwe propose an automatic method for analyzing this phe-\nnomenon.1Our overarching research objective is to pro-\n1https://github.com/marinelliluca/gender_coded_sound_ismir2023\n© L. Marinelli, C. Saitis, and G. Fazekas. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: L. Marinelli, C. Saitis, and G. Fazekas, “Gender-coded\nsound: Analysing the Gendering of Music in Toy Commercials via Multi-\ntask Learning”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.vide a basis for a theory of message production. Speciﬁ-\ncally, a theory of the effects that message producers, their\ndecision-making, or their unconscious gender biases have\non the selection and composition of sound and music in\ntoy adverts. For this goal, we propose an integrative ap-\nproach combining content analytical (CA) variables, mu-\nsic perceptual ratings, and multimodal affective ratings to\nannotate toy commercials, and using multi-task learning\n(Fig. 1) to analyse the gendering of their soundtracks.\n1.1 Gendered music styles as cognitive schemas\nEmpirical studies have demonstrated that gender and sex\nimpact the perception and processing of music [1–3].\nHowever, the idea that sex determines ﬁxed differences in\nbrain structure has been questioned due to potential mis-\ninterpretations, overestimations, and publication bias [4].\nGender schemas, instead, are learned cognitive networks\nof associations that guide an individual’s behavior by as-\nsimilating or rejecting gender-appropriate ideas and activi-\nties [5,6]. Schemas guide an individual’s perception, infor-\nmation processing, and memory retention, as they prevent\ninformation overload by organising one’s perceptual expe-\nrience into a coherent and intelligible whole [6, 7].\nPopular music genres have been themselves theorised as\ncognitive schemas containing extramusical concepts that\ncan be primed when a subject is exposed to the genre’s\nmusic [8, 9]. Such schemas are formed through repeated\nexposure to the multimodal discourse encompassing mu-\nsic, which is to some extent globalised, but that also varies\nfrom culture to culture as a result of glocalisation.2pro-\ncesses [9] Schema theory has also been used in literary\nreading and analysis to explain organised \"bundles of in-\nformation and features\" [10, p. 106-7] such as literary gen-\nres (e.g., science ﬁction, fantasy, horror).\nWhile gendered language patterns in text and lyrics\n[11] may be relatively more straightforward to interpret,\ngender-coding in sound and music ensues from the histor-\nical sedimentation, in musical practice, of multimodal as-\nsociations between gendered meanings in language, visual\nimages, and musical structures [12]. For example, instru-\nments have been consistently associated with masculinity\nor femininity, even when their sound is presented in iso-\nlation and not visually linked to the actual object [13, 14].\nSergeant and Himonides [15, 16] investigated whether in\nWestern art music individual sounds or their organization\n2A lexical blend of globalization and localism.166Figure 1 . Brief overview of the experimental pipeline.\nwithin a composition could infer the sex or gender of the\nperformer or composer. Even though they found no cor-\nrelation between the gender of the composers or perform-\ners and the gendering of music, raters agreed on the gen-\ndering of music, which was associated with features such\nas tempo, minor/major key, and tonal weight or density.\nTagg [17] studied the reception of gendered meanings in\nTV theme tunes and also found high agreement among\nparticipants. Several musical dimensions, such as average\ntempo, rhythmic and dynamic regularity, and presence of\nactive bass lines, may contribute to conveying gendered\nmeanings. In a subsequent investigation, Tagg and Clar-\nida [18] found that musical pieces linked to female char-\nacters were more prone to be classiﬁed as quiet and calm.\nWang and Horvát [19] computationally extracted twelve\ndescriptors of musical parameters and perceptual features\nfor over 200k songs by more than 8k globally distributed\nartists across a multitude of popular music genres. They\nfound statistically signiﬁcant differences for eleven out of\ntwelve musical parameters with regard to the gender of the\ncomposers, suggesting the existence of measurable, supra-\ngenre, gendered music styles in the global music industry.\nSome of these studies appear to contradict each other,3\nwhile at the same time sharing the same fallacy, in that\nfeminine and masculine patterns in the performance and\ncomposition of music should be considered on a par\nwith distinct gendered styles in spoken language, such as\nLakoff’s ‘women’s talk’ [20]. As such, these differences\nshould not be understood in terms of a causal relationship\nbetween the gender of the artists and gendered musical pat-\nterns. Individuals may have a tendency to use forms of\nexpression that they deem appropriate with regard to their\nidentity, but given the performative nature of gender we\ncannot possibly generalise this behaviour (i.e., even strong\ncorrelation is not causation), as this would end up reinforc-\ning gender stereotypes and their power relations.\nGender schemas therefore mediate our perception of\nmusic, and this relationship appears to be bidirectional.\nAt the same time, music-primed schemas can alter our\nperception of other people’s ethnicity, rural/urban back-\nground, age, expertise [8], and even gender [21, 22]. We\nthus posit that not only gender roles and stereotypes can\n3[19] found signiﬁcant correlations between the gender of the com-\nposers and characteristics of their music, while [16] did not.be understood in terms of schemas, but also that mascu-\nline and feminine music styles can be viewed as music-\nprimed gender schemas , which to some extent overlap with\nthe former. We also presume that different music-primed\nschemas might exist for other intersectional factors, such\nas class.\n1.2 Gendered toy marketing\nGender polarisation in TV advertising aimed at children\nhas been consistently found in a large body of studies\nspanning over 40 years [23–25]. Differences in commer-\ncials targeted at girls, boys, and mixed audience have been\nfound in terms of: sound (voices, background music and\nsound effects), language, transitions and camera work, set-\nting, interactions and activities, and colours.\nSpeciﬁcally in terms of sound and music, Welch et al.\n[23] noted that in general the sex of the voice-over matched\nthe target audience of the commercials, but that male nar-\nrating voices also occurred more often in mixed audience\ncommercials, and subsequent research conﬁrmed the same\ntrend [25]. They also found that commercials targeted at\nboys had more noise, louder music, and more sound ef-\nfects. Another study [24] conversely found that music\nused in girls’ advertisements is generally softer and more\nlikely to have a sung narration style. Whereas, Johnson\nand Young [26] identiﬁed what they called \"gender exag-\ngeration:\" male voice-overs tend to be exceedingly deep,\ngrowl-like or aggressive, whereas female voice-overs are\noften very high-pitched and singsong.\nBy interpreting music as an inherently multimodal dis-\ncourse, a critical analysis of gender markers in children’s\nTV adverts can help to investigate the relation between mu-\nsic and hegemonic discourses on gender; and to promote\nfurther research towards a commercial and contemporary\nmusical semiotics of gender. Analysing music in gendered\nadvertising aimed at children allows a privileged glance\ninto the birthplace of music-primed gender schemas.\n1.3 Automatic discourse processing\nDiscourse analysis is an umbrella term that refers to ap-\nproaches developed across diverse academic disciplines.\nThis includes disciplines that ﬁrst developed models for\nunderstanding discourse, such as linguistics, social semi-\notics and conversation analysis. But it also refers to otherProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n167All Feminine Masculine Mixed\nN= 606 N= 163 N= 149 N= 200\nType χ2(6,N= 512) = 89 .02,p=.000\n5.6% Sung 9.8% None 2.5%\n18.8% Spoken and sung 36.8% 6.0% 17.0%\n67.7% Spoken 52.8% 81.2% 75.5%\n7.9% No voices 0.6% 12.8% 5.0%\nAge χ2(6,N= 512) = 39 .51,p=.000\n79.5% Adults 76.7% 79.2% 83.0%\n5.9% Children and adults 8.0% 6.0% 6.5%\n6.6% Children 14.7% 2.0% 5.5%\n7.9% No voices 0.6% 12.8% 5.0%\nGender χ2(6,N= 512) = 332 .1,p=.000\n39.8% Feminine 95.7% 2.0% 29.5%\n46.9% Masculine 1.8% 83.9% 54.5%\n5.4% Feminine and masculine 1.8% 1.2% 11.0%\n7.9% No voices 0.6% 12.8% 5.0%\nGender exaggeration χ2(6,N= 512) = 243 .6,p=.000\n16.5% Exagg. feminine 44.8% None 8.0%\n15.5% Exagg. masculine None 40.3% 7.0%\n60.1% All normal sounding 54.6% 47.0% 80.0%\n7.9% No voices 0.6% 12.8% 5.0%\nTable 1 . Contingency tables of voice-related content analytical variables with χ2tests of independence. The column \"All\"\nincludes commercials without actors or presenter (94).\napproaches that apply and extend these models of under-\nstanding to their particular academic ﬁeld, such as cog-\nnitive psychology, literary criticism and artiﬁcial intelli-\ngence [27]. Research on discourse processing, an endeav-\nour of natural language processing (NLP), is already at a\nstage where machine learning approaches are able, for ex-\nample, to automatically detect social attitudes and political\nstances in online news or social media [28, 29].\nBeyond textual discourse and NLP, denotative mean-\nings in images and videos can be easily captured by ma-\nchine learning techniques [30,31]. However, works that try\nto address connotative meanings or the rhetoric of multi-\nmedia content are still in their infancy and such approaches\nare often not even framed as pertaining to discourse or\nsemiotic analysis. Dinkov et al. [32] predicted the politi-\ncal ideological bias (left, centre, right) of media outlets us-\ning text, metadata, and audio (via speech processing tech-\nniques) from YouTube channels, but not visual content. Ye\net al. [33] predicted the messages that image and video ad-\nvertisements convey by explicitly modeling symbolic as-\nsociations (e.g., gun for “danger”) and combining cues\nfrom multiple modalities, including the loudness in video\nsoundtracks. Notably, none of these studies leveraged ap-\nproaches and tools from music information retrieval.\n1.4 Multi-task learning in MIR\nIn multi-task learning we train a single model to perform\nmultiple related tasks simultaneously, leveraging shared\ninformation among tasks, which results in several beneﬁts.\nBöck et al. [34] simultaneously modelled tempo estima-tion and beat tracking of musical audio, showing state-of-\nthe-art performance for both tasks. Wu et al. [35] com-\nbined multi-task and self-supervised learning, resulting in\nimproved performance. Chowdhury et al. [36] proposed a\nVGG-style deep neural network to predict emotional char-\nacteristics of music based on mid-level perceptual features\n(e.g., melodiousness and tonal stability) and found that the\nloss in performance was negligible when compared to pre-\ndicting emotions directly. Further improvements were ob-\ntained by training jointly on the mid-level and emotion an-\nnotations, with the small loss in performance justiﬁed by\nthe gain in explainability of the predictions. Our study ex-\npands upon this foundation by incorporating emotions and\nperceptual features, while also adding more granular struc-\nture to facilitate a comprehensive understanding of the gen-\ndering of music in multimodal contexts.\n2. DATASET\nOur hierarchical data collection framework comprised CA\nvariables at the lower level, music-focused ratings from ex-\nperts at the middle level, and multimodal affective ratings\nat the highest level of subjectivity. Mid-level perceptual\nfeatures, which describe relevant and instantly identiﬁable\nmusical characteristics, exhibit high consistency across lis-\nteners and can be predicted from the acoustic signal. These\nfeatures also correlate with music’s affective dimensions\n[37]. The emotion ratings were collected from adults rather\nthan children because adults are better equipped to cap-\nture the commercials’ intended emotional impact. Fur-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n168thermore, research indicates that children exhibit adult-like\nemotion recognition capabilities by age 11 [38].\n2.1 Sampling method\nIn March 2022, we collected a sample of 5614 videos from\nthe ofﬁcial YouTube channel of Smyths Toys Superstores,\na major UK toy retailer. To ensure comparability with pre-\nvious studies [39,40], we selected only high-quality videos\nintended for television and excluded those without audio,\nformatted for mobile phones, or with substantial on-screen\ntext. Additionally, we excluded advertisements featuring\ntoddlers and pre-schoolers as these are actually targeted at\nparents. To minimise duplicates, we removed videos with\nthe same title from our sample.\nGiven that we are interested in understanding the gen-\ndering of sound and music in the toy industry at large, we\nneeded to enforce some balance across gender targets. We\nthus performed a preliminary classiﬁcation of 1778 com-\nmercials based on their intended target audience (feminine,\nmasculine or mixed audience) using simple heuristics re-\ngarding the gender of the majority of presenters featuring\nin the commercial, the colour coding of the video and ulti-\nmately the category of the product. This resulted in 780\n’feminine’, 509 ’masculine’, and 489 ‘mixed audience’\ncommercials. A ﬁnal sample of 606 commercials, span-\nning over 10 years from 2012 to 2022, was obtained by\nrandomly sampling from each category 202 videos.\n2.2 Content analysis (manual annotation)\nThe gender orientation (also target audience) of the\ncommercials was determined by the gender of the ac-\ntors/presenters. Following [26], in order to account for\ntokenism, whenever a presenter of the other gender was\nincluded in the background or for just a few seconds, these\nwere considered token gender representations and not ex-\nplicit market orientations. All ﬁctional characters, even\nwhen realistic (e.g. from a video game), were not con-\nsidered as actors/presenters and the corresponding com-\nmercials were coded as having no actors. Whenever com-\nmercials featured exclusively character ‘dismemberment’\n(e.g., showing only hands without a face or head) [41]\nthese were also coded as having no actors.\nFour distinct items describing the sound of the voices\nin the commercial were collected using a coding schema\nbased on Verna’s research [42]. But unlike the original\nwork, we coded for all voices in the commercial, both\ndiegetic and non-diegetic. The reason for this choice is\nthat there is no way to reliably distinguish between diegetic\nand non-diegetic sounds purely based on the audio signal.\nCommercials were coded in terms of type of voices (\"Spo-\nken\", \"Sung\", \"Both spoken and sung\", \"No voices\"), then\nin terms of voices age (\"Adults\" which included young\nadults, \"Children\", \"Children and adults\", \"No voices\"),\ngender exaggeration of the voices (\"All normal sound-\ning\", \"Exaggeratedly masc.\", \"Exaggeratedly fem.\", \"No\nvoices\"), and ﬁnally in terms of voice gender (\"Feminine\",\n\"Masculine\", \"Feminine and masculine\", \"No voices\"). In\norder to determine the reliability of each variable, 15% ofthe commercials was double-coded by two coders indepen-\ndently. For all variables we obtained Krippendorff’s alpha\nlevels above .80 (with ‘gender orientation‘ and ‘gender of\nthe voices’ exceeding .90), and therefore met the standards\nof reliability required for this type of analysis [43]. Out\nof 606 commercials analyzed, 163 were targeted at a femi-\nnine audience, 149 at a masculine audience, 200 at a mixed\naudience and 94 featured no actors or presenters. Contin-\ngency tables of the voice variables are shown in Table 1.\n2.3 Music-focused and emotion ratings\nParticipants in our study were paid between £7 and £8 per\nhour (depending on their completion time) on Proliﬁc.co.\nIn order to minimise the effects of careless responding, a\nlow-effort metric was computed by summing the length of\nall long strings for each participant, and those that scored\nabove two standard deviations from the average value were\nscreened out during data collection, as it was performed\nin batches of 50 participants. For 600 of the videos, we\ncollected between ﬁve and six ratings on each music and\nemotion scale. At an initial stage, the remaining 6 videos\nwere used as controls (i.e., were rated by all participants),\nbut we do not leverage them as such in the current study.\nMusically trained participants (at least three years of\nexperience with an instrument) rated the soundtracks\nof the commercials on 15 music-focused bipolar scales\n[44, 45]: Electric/Acoustic, Distorted/Clear, Loud/Soft,\nMany/Few instruments, Heavy/Light, High/Low pitch,\nPunchy/Smooth, Wide/Narrow pitch variation, Harmo-\nnious/Disharmonious, Clear melody/No melody, Com-\nplex/Simple rhythm, Repetitive/Non-repetitive, Dense/\nSparse, Fast/Slow tempo, and Strong/Weak beat. We col-\nlected a total of 4560 ratings from 152 participants from\nthe UK (75 M, 77 F, aged 40 ± 14). Given that our fo-\ncus is on music, but soundtracks consist of speech, music\nand sound effects, our question was formulated as follows:\n\"The following are a series of perceptual attributes of mu-\nsic. You are asked to evaluate the music in the background\nin terms of the adjectives on each side of the scale.\"\nTo annotate the perceived affect of videos, we drew\nfrom the aesthetic emotions scale [46, AESTHEMOS],\nwhich was devised from an extensive review of emotion\nmeasures from different domains such as music, litera-\nture, ﬁlm, painting, advertisements, design, and architec-\nture, and is thus ideal, in its ﬂexibility, for our use with\nmultimodal stimuli. Given that our focus is on music and\nsound, in a preliminary study we limited our choice to a\nsubset of 10 AESTHEMOS items that intersect with the\n13 music emotions listed by Cowen et al. [47]. Of these,\nwe kept only seven scales which showed signiﬁcant dis-\ncriminant capabilities: Happy or Delightful, Amusing or\nFunny, Beauty or Liking, Calm or Relaxing, Energising\nor Invigorating, Angry or Aggressive, and Triumphant or\nAwe-inspiring. We used a single unipolar item for each\nsubscale, instead of two. We collected a total of 4530 rat-\nings from 151 participants from the UK (76 M, 75 F, aged\n39 ± 13). Given that our aim is to analyse the intended\nemotional proﬁle, our question was formulated as follows:Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n169Target F1 Secon. F1 Avg. R2emo Avg. remo Avg. R2mid Avg. rmid\nEmbeddings V oice\nmfcc no .79 ± .08 .66 ± .07 .02 ± .17 .36 ± .11 .14 ± .16 .48 ± .09\nmfcc yes .78 ± .10 .65 ± .07 .06 ± .16 .38 ± .11 .13 ± .15 .43 ± .10\nmsd no .87 ± .05 .66 ± .06 .25 ± .11 .54 ± .08 .35 ± .14 .62 ± .09\nmsd yes .95 ± .04 .79 ± .05 .26 ± .15 .56 ± .09 .30 ± .12 .58 ± .09\nopenl3_env no .91 ± .05 .72 ± .06 .34 ± .10 .61 ± .08 .41 ± .10 .66 ± .07\nopenl3_env yes .95 ± .04 .77 ± .05 .34 ± .13 .62 ± .08 .35 ± .12 .62 ± .08\nopenl3_music no .87 ± .09 .71 ± .06 .31 ± .16 .56 ± .19 .39 ± .16 .64 ± .15\nopenl3_music yes .91 ± .11 .76 ± .10 .29 ± .17 .56 ± .19 .31 ± .14 .59 ± .13\nTable 2 . Mean and standard deviation from 5x repeated 5-fold cross-validation. ’Target’ refers to the gender orientation of\nads (binary); secondary tasks involve voice-related content analytical variables. ’No’ represents models trained on voice-\nseparated accompaniments, while ’Yes’ indicates models trained on entire soundtracks.\n\"Toys commercials are targeted at an audience mainly con-\nsisting of children and aim at evoking the following emo-\ntions. Pay attention to both sound and images and rate each\nintended emotion accordingly.\"\n2.4 Between-targets ANOV A\nWe ﬁrst performed between-targets (i.e., gender targets of\nthe commercials) one-way analyses of variance for each of\nthe music-focused and emotion scales. When ANOV A as-\nsumptions were violated, we performed a Kruskal-Wallis\nH-test instead. Highly signiﬁcant polarisation ( p < .001)\nemerged for twelve of the mid-level music perceptual\nscales, with stark contrasts observed between feminine\nand masculine-targeted commercials, and commercials tar-\ngeted at mixed audiences generally registering in-between\nvalues. Masculine adverts were more Electric than Acous-\ntic, more distorted, disharmonious and with a less clear\nmelodic contour than feminine ones. They also were\nmore dense in terms of instrumentation, more Punchy, with\nstronger beats, and therefore were generally louder and\nheavier. Also in terms of rhythmic complexity, they were\nmore complex than feminine-targeted commercials. Thus\na clear picture emerges, as the soundtracks in boys’ adverts\nare signiﬁcantly more abrasive than those in girls’ ads.\nSimilarly, stark contrasts ( p < .001) were observed be-\ntween feminine and masculine-targeted commercials for\nall affective scales, with commercials targeted at mixed au-\ndiences often registering in-between values. Commercials\ntargeted at boys were the least \"Happy or delightful\", the\nleast \"Amusing or funny\", \"Calm or relaxing\", and reg-\nistered the lowest values on the scale \"Beauty or liking\".\nThey instead were the most \"Energising or invigorating\",\n\"Angry or aggressive\", and \"Triumphant or awe-inspiring\".\nApart from the scale \"Amusing or funny\", which scored\nthe highest values within mixed audiences commercials,\nadverts targeted at girls displayed an opposite behaviour\nto those for boys. For example, they were the most\n\"Calm or relaxing\" and the least \"Angry or aggressive\".\nAs previously highlighted with the music-focused scales,\nmasculine-targeted commercials appear again to be signif-\nicantly more abrasive than the feminine ones.\nWe report a more in-depth analysis in an upcomingpublication. In this paper, we exclude \"Amusing or\nfunny\" from further analyses due to poor correlation with\nthe mid-level features. We also exclude the three non-\nsigniﬁcant mid-level scales: Wide/Narrow pitch variation,\nRepetitive/Non-repetitive, and Fast tempo/Slow tempo.\n3. MACHINE LEARNING PIPELINE\nOur machine learning framework is a multi-task learning\nmodel implemented in PyTorch (Fig. 1). It was trained to\nsimultaneously learn mid-level features regression, emo-\ntion regression, and all the CA variables (classes). These\ntasks share an initial hidden layer with 128 units and then\nbranch out into separate sub-tasks. Each sub-task has its\nown hidden layer with 128 units and an output layer with\ndimensions corresponding to the speciﬁc task.\nTo avoid the jingle of the retailer in the last 5 sec-\nonds of most soundtracks, we trimmed them accordingly.\nThen with Spleeter [48] we separated voices and accom-\npaniments. Features were extracted in non-overlapping\nchunks across the trimmed soundtrack and then averaged\nacross the chunks. We computed 20-band MFCCs using\nlibrosa [49], along with their delta and delta-deltas,\nyielding 60-dimensional embeddings. A reimplementa-\ntion of a state-of-the-art model trained on the million\nsong dataset (MSD) [50] provided 256-dimensional em-\nbeddings. OpenL3 features were computed using the pro-\nvidedconda package [51], generating 512-dimensional\nembeddings for both environmental and music models.\nThe proposed model employs an equally weighted,\ncombined loss function, incorporating the mean squared\nerror for the mid-level features and emotion regression\ntasks, and cross-entropy loss for the classiﬁcation tasks.\nThe model was trained jointly on all tasks. We also used\na model checkpoint and early stopping with a patience\nof 30 epochs (maximum of 200). Repeated 5-fold cross-\nvalidation was performed (10% test, 10% validation, for\n5 repetitions, i.e. 25 \"folds\", as the random seed was not\nset) and utilised the AdamW optimizer instead of Adam for\nregularization. Further optimising the network to surpass\nthe already remarkable results, as well as conducting abla-\ntion studies to evaluate the various components and design\nchoices, is beyond the scope of our investigation.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n170Target F1 Secon. F1 Avg. R2emo Avg. remo Avg. R2mid Avg. rmid\nEmbeddings V oice\nmfcc no .52 ± .05 .67 ± .06 .04 ± .16 .37 ± .11 .14 ± .14 .48 ± .09\nmfcc yes .48 ± .04 .67 ± .05 .05 ± .15 .38 ± .11 .15 ± .14 .46 ± .09\nmsd no .62 ± .05 .67 ± .06 .23 ± .15 .54 ± .10 .36 ± .12 .64 ± .07\nmsd yes .67 ± .06 .80 ± .05 .29 ± .12 .57 ± .08 .33 ± .10 .60 ± .07\nopenl3_env no .59 ± .06 .72 ± .06 .30 ± .12 .59 ± .08 .42 ± .10 .66 ± .07\nopenl3_env yes .66 ± .07 .77 ± .06 .34 ± .11 .61 ± .07 .35 ± .10 .62 ± .07\nopenl3_music no .64 ± .07 .73 ± .06 .32 ± .12 .60 ± .08 .43 ± .11 .68 ± .07\nopenl3_music yes .67 ± .04 .78 ± .05 .35 ± .12 .61 ± .08 .37 ± .10 .63 ± .07\nTable 3 . Same as Table 2, but results refer to ternary ‘Target’ classiﬁcation.\n4. RESULTS\nTables 2 and 3 reveal once again stark differences between\nthe soundtracks of commercials designed for feminine and\nmasculine audiences (the value \"no\" corresponds to mod-\nels trained on the voice-separated accompaniments). In\nfact, the binary classiﬁcation task on the soundtracks in-\ncluding voice achieves an impressively high Target F1\nscore of .95 ± .04 using the MSD and OpenL3 env em-\nbeddings. It is also worth noting that even without voice,\nthe soundtracks still contain enough information to clas-\nsify the commercials with a high degree of accuracy, with\nthe OpenL3 env embedding achieving a Target F1 score of\n.91 ± .11. In a way, the dataset is so gendered that it can be\nconsidered a toy dataset in all senses.\nUpon closer examination of the R2andremotions met-\nrics, we observe that they are relatively low across all ex-\nperiments compared to mid-level metrics. This contrasts\nwith previous research [36] where mid-level correlations\nwere lower than those of emotions, as in our case the R2\nmid-level and rmid-level metrics are generally higher,\nwith the OpenL3 embeddings performing the best.\nWhen comparing Tables 2 and 3, the high performance\nof the MSD and both OpenL3 embeddings on the binary\ntask, suggests that there are no signiﬁcant differences in\nthe soundtracks of mixed-audience commercials compared\nto those targeted at feminine or masculine audiences. This\nconﬁrms the results from the analysis of variance and high-\nlights the ability of these embeddings to perform simi-\nlarly across different target audiences. Overall, we found\nthat the OpenL3 embeddings performed better than others\nacross all tasks, indicating superior generalizability, as al-\nready shown in previous research especially in the context\nof limited training examples [52]. However, the relatively\nlowR2andrfor emotions suggest that there is still room\nfor improvement, possibly through multimodal fusion.\nIt is noteworthy that human voice plays a critical role\nin conveying higher-level connotations, as performance on\nthe classiﬁcation tasks andespecially the emotion regres-\nsions generally improves when voices are present. Addi-\ntionally, improvement in mid-level regressions on the ac-\ncompaniments of the soundtracks (no voice) indicates that\nparticipants in the data collection were able to focus on the\nbackground of the soundtracks, as they were asked to.\nAlthough the MFCCs are the worst performing, theirdiscriminative power on the target task and the decent per-\nformance on the mid-level features regression highlight the\nunderlying \"simplicity\" of the task, in terms of the strong\ncollinearity due to the degree of gender-polarization inher-\nent in the dataset.\n5. CONCLUSION\nBy examining the performance of different musical em-\nbeddings in classifying commercials targeted at different\naudiences, and by providing explainable inference of the\ntarget of the commercials, in terms of affective and of mu-\nsic perceptual features, this study sheds light on the role of\nmusic in gendered marketing strategies. Such approach has\nsigniﬁcant implications for advertisers, policymakers, and\nbroadcasters, who recently faced a public backlash against\nthe gendered marketing of toys and other products.4Fur-\nthermore, the study highlights the importance of consid-\nering the role of music when regulating marketing strate-\ngies and developing more inclusive and diverse advertising\ncampaigns. Our results suggest that gendered music styles\nin toy commercials emerge as a result of deliberate mar-\nketing strategies, as such styles reﬂect gender stereotypes\nthat are \"ludicrously old-fashioned and offensively out of\ntouch\" [53] and still prevalent in the industry.\nBy bringing together music analysis, machine learning,\nand critical analysis, this study illustrates the potential of\ninterdisciplinary approaches, contributing to the emerging\nﬁeld of computational social studies. It highlights the im-\nportance of considering the role of music, among other\nmodalities, in shaping societal norms and values and the\nneed for greater awareness and accountability in the use of\nsuch affordances in marketing and other industries.\nFuture research can build on these ﬁndings by further\ninvestigating the relationship between gendered music and\nadvertising strategies in different industries and contexts,\nexploring the impact of gendered music on consumer be-\nhavior and societal perceptions of gender, and developing\nnew methodologies for creating more inclusive and diverse\nmarketing campaigns. The results also emphasise the po-\ntential for the development of multimodal approaches to\nenhance the models’ performance on these tasks.\n4https://www.bbc.co.uk/news/world-us-canada-46613032Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1716. ACKNOWLEDGMENTS\nThis work was supported by UK Research and Innova-\ntion [grant number EP/ S022694/1], and was partially con-\nducted during the ﬁrst author’s Enrichment Scheme place-\nment at the Alan Turing Insitute. The authors would like\nto express their gratitude to Professor Petra Lucht for her\ninvaluable guidance at early stages of this study.\n7. REFERENCES\n[1] J. J. Kellaris and S. P. Mantel, “The inﬂuence of mood\nand gender on consumers’ time perceptions,” ACR\nNorth American Advances , 1994.\n[2] G. T. Toney and J. B. Weaver, “Effects of gender and\ngender role self-perceptions on affective reactions to\nrock music videos,” Sex Roles , 1994.\n[3] J. Meyers-Levy and R. Zhu, “Gender differences in\nthe meanings consumers infer from music and other\naesthetic stimuli,” Journal of Consumer Psychology ,\n2010.\n[4] G. Rippon, “Do women and men have different\nbrains?” New Scientist , 2019.\n[5] S. L. Bem, “Gender schema theory: A cognitive ac-\ncount of sex typing.” Psychological review , 1981.\n[6] E. Leung, “Gender schemas,” in Encyclopedia of Per-\nsonality and Individual Differences . Springer, 2020.\n[7] M. G. Boltz, “Musical soundtracks as a schematic in-\nﬂuence on the cognitive processing of ﬁlmed events,”\nMusic Perception , 2001.\n[8] M. Shevy, “Music genre as cognitive schema: Extra-\nmusical associations with country and hip-hop music,”\nPsychology of music , 2008.\n[9] S. Kristen and M. Shevy, “A comparison of german\nand american listeners’ extra musical associations with\npopular music genres,” Psychology of Music , 2013.\n[10] P. Stockwell, Cognitive poetics: An introduction ,\n2nd ed. Routledge, 2019.\n[11] L. Betti, C. Abrate, and A. Kaltenbrunner, “Large scale\nanalysis of gender bias and sexism in song lyrics,”\narXiv preprint arXiv:2208.02052 , 2022.\n[12] N. Dibben, “Gender identity and music,” in Musical\nidentities . New York: Oxford University Press, 2002.\n[13] C. A. Elliot and M. Yoder-White, “Masculine/feminine\nassociations for instrumental timbres among children\nseven, eight, and nine years of age,” Contributions to\nMusic Education , 1997.\n[14] L. M. Stronsick, S. E. Tuft, S. Incera, and C. T. McLen-\nnan, “Masculine harps and feminine horns: Timbre and\npitch level inﬂuence gender ratings of musical instru-\nments,” Psychology of Music , 2018.[15] D. C. Sergeant and E. Himonides, “Gender and the per-\nformance of music,” Frontiers in psychology , 2014.\n[16] ——, “Gender and music composition: A study of mu-\nsic, and the gendering of meanings,” Frontiers in psy-\nchology , 2016.\n[17] P. Tagg, “An anthropology of stereotypes in tv music?”\nSwedish Musicological Journal , vol. 71, 1989.\n[18] P. Tagg and B. Clarida, “Title tune gender and ideol-\nogy,” in Ten little title tunes: towards a musicology of\nthe mass media . Huddersﬁeld: The Mass Media Mu-\nsic Scholars’ Press, NY ., 2003.\n[19] Y . Wang and E.-Á. Horvát, “Gender differences in\nthe global music industry: Evidence from musicbrainz\nand the echo nest,” in Proceedings of the International\nAAAI Conference on Web and Social Media , 2019.\n[20] R. Lakoff, “Language and woman’s place,” Language\nin society , 1973.\n[21] H.-B. Brosius and H. M. Kepplinger, “Der einﬂuß von\nmusik auf die wahrnehmung und interpretation einer\nsymbolisierten ﬁlmhandlung,” Rundfunk und Fernse-\nhen, 1991.\n[22] A.-K. Herget, “On music’s potential to convey mean-\ning in ﬁlm: A systematic review of empirical evi-\ndence,” Psychology of Music , 2021.\n[23] R. L. Welch et al. , “Subtle sex-role cues in children’s\ncommercials.” Journal of Communication , 1979.\n[24] J. Lewin-Jones and B. Mitra, “Gender roles in televi-\nsion commercials and primary school children in the\nuk,”Journal of children and media , 2009.\n[25] B. Mitra and J. Lewin-Jones, “Colin won’t drink out\nof a pink cup,” in The handbook of gender, sex, and\nmedia . Wiley Online Library, 2012.\n[26] F. Johnson and K. Young, “Gendered voices in chil-\ndren’s television advertising,” Critical Studies in Me-\ndia Communication , 2002.\n[27] D. Schiffrin, D. Tannen, and H. E. Hamilton, “Intro-\nduction to the ﬁrst edition,” The handbook of discourse\nanalysis , 2015.\n[28] Y . Feng, H. Chen, and L. He, “Consumer responses to\nfemvertising: a data-mining case of dove’s “campaign\nfor real beauty” on youtube,” Journal of Advertising ,\n2019.\n[29] G. Wiedemann, “Text mining for discourse analysis:\nAn exemplary study of the debate on minimum wages\nin germany,” Quantifying approaches to discourse for\nsocial scientists , 2019.\n[30] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi,\nI. Laptev, and J. Sivic, “Howto100m: Learning a text-\nvideo embedding by watching hundred million nar-\nrated video clips,” in Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision , 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n172[31] S. Islam, A. Dash, A. Seum, A. H. Raj, T. Hossain, and\nF. M. Shah, “Exploring video captioning techniques: A\ncomprehensive survey on deep learning methods,” SN\nComputer Science , 2021.\n[32] Y . Dinkov, A. Ali, I. Koychev, and P. Nakov, “Pre-\ndicting the leading political ideology of youtube chan-\nnels using acoustic, textual, and metadata information,”\narXiv preprint arXiv:1910.08948 , 2019.\n[33] K. Ye, N. H. Nazari, J. Hahn, Z. Hussain, M. Zhang,\nand A. Kovashka, “Interpreting the rhetoric of visual\nadvertisements,” IEEE transactions on pattern analy-\nsis and machine intelligence , 2019.\n[34] S. Böck, M. E. P. Davies, and P. Knees, “Multi-task\nlearning of tempo and beat: Learning one to improve\nthe other,” in International Society for Music Informa-\ntion Retrieval Conference , 2019.\n[35] H.-H. Wu, C.-C. Kao, Q. Tang, M. Sun, B. McFee,\nJ. P. Bello, and C. Wang, “Multi-task self-supervised\npre-training for music classiﬁcation,” in ICASSP 2021-\n2021 . IEEE, 2021.\n[36] S. Chowdhury, A. Vall, V . Haunschmid, and G. Wid-\nmer, “Towards explainable music emotion recognition:\nThe route via mid-level features,” in Proceedings of the\n20th ISMIR Conference, Delft, The Netherlands , 2019.\n[37] A. Aljanaki and M. Soleymani, “A data-driven ap-\nproach to mid-level perceptual musical feature mod-\neling,” in Proceedings of the 19th ISMIR Conference,\nParis, France , 2018.\n[38] P. G. Hunter, E. G. Schellenberg, and S. M. Stalinski,\n“Liking and identifying emotionally expressive music:\nAge and gender differences,” Journal of Experimental\nChild Psychology , 2011.\n[39] M. S. Larson, “Interactions, activities and gender in\nchildren’s television commercials: A content analysis,”\nJournal of Broadcasting & Electronic Media , 2001.\n[40] S. G. Kahlenberg and M. M. Hein, “Progression on\nnickelodeon? gender-role stereotypes in toy commer-\ncials,” Sex roles , 2010.\n[41] E. Goffman, Gender advertisements . New York:\nHarper Colophon Books, 1976.\n[42] M. E. Verna, “The female image in children’s tv com-\nmercials,” Journal of Broadcasting & Electronic Me-\ndia, vol. 19, no. 3, 1975.\n[43] K. A. Neuendorf, “Content analysis—a methodologi-\ncal primer for gender research,” Sex roles , 2011.\n[44] V . Alluri and P. Toiviainen, “Exploring perceptual and\nacoustical correlates of polyphonic timbre,” Music Per-\nception , 2010.\n[45] K. L. Whiteford, K. B. Schloss, N. E. Helwig, and\nS. E. Palmer, “Color, music, and emotion: Bach to the\nblues,” i-Perception , 2018.[46] I. Schindler, G. Hosoya, W. Menninghaus, U. Beer-\nmann, V . Wagner, M. Eid, and K. R. Scherer, “Mea-\nsuring aesthetic emotions: A review of the literature\nand a new assessment tool,” PloS one , 2017.\n[47] A. S. Cowen, X. Fang, D. Sauter, and D. Keltner,\n“What music makes us feel: At least 13 dimensions\norganize subjective experiences associated with music\nacross different cultures,” Proceedings of the National\nAcademy of Sciences , 2020.\n[48] R. Hennequin, A. Khlif, F. V oituret, and M. Moussal-\nlam, “Spleeter: a fast and efﬁcient music source sepa-\nration tool with pre-trained models,” Journal of Open\nSource Software , 2020.\n[49] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,\nE. Battenberg, and O. Nieto, “librosa: Audio and music\nsignal analysis in python,” in Proceedings of the 14th\npython in science conference , 2015.\n[50] M. Won, S. Chun, and X. Serra, “Toward interpretable\nmusic tagging with self-attention,” arXiv preprint\narXiv:1906.04972 , 2019.\n[51] J. Cramer, H.-H. Wu, J. Salamon, and J. P. Bello,\n“Look, listen, and learn more: Design choices for deep\naudio embeddings,” in ICASSP 2019-2019 . IEEE,\n2019.\n[52] S. Grollmisch, E. Cano, C. Kehling, and M. Taenzer,\n“Analyzing the potential of pre-trained embeddings for\naudio classiﬁcation tasks,” in 2020 28th European Sig-\nnal Processing Conference (EUSIPCO) . IEEE, 2021.\n[53] C. Fine and E. Rush, ““Why does all the girls have to\nbuy pink stuff?” The ethics and science of the gendered\ntoy marketing debate,” Journal of Business Ethics ,\n2018.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n173"
    },
    {
        "title": "Real-Time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar.",
        "author": [
            "Andrea Martelloni",
            "Andrew P. McPherson",
            "Mathieu Barthet"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265236",
        "url": "https://doi.org/10.5281/zenodo.10265236",
        "ee": "https://zenodo.org/records/10265236/files/000013.pdf",
        "abstract": "Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar body percussion. We formulate several design objectives for RT-MIR systems for augmented instrument performance: (i) causal constraint, (ii) perceptually negligible action-to-sound latency, (iii) control intimacy support, (iv) synthesis control support. We present and evaluate real-time guitar body percussion recognition and embedding learning techniques based on convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs). We introduce a taxonomy of guitar body percussion based on hand part and location. We follow a cross-dataset evaluation approach by collecting three datasets labelled according to the taxonomy. The embedding quality of the models is assessed using KL-Divergence across distributions corresponding to different taxonomic classes. Results indicate that the networks are strong classifiers especially in a simplified 2-class recognition task, and the VAEs yield improved class separation compared to CNNs as evidenced by increased KL-Divergence across distributions. We argue that the VAE embedding quality could support control intimacy and rich interaction when the latent space's parameters are used to control an external synthesis engine. Further design challenges around generalisation to different datasets have been identified.",
        "zenodo_id": 10265236,
        "dblp_key": "conf/ismir/MartelloniMB23",
        "keywords": [
            "real-time music information retrieval",
            "augmenting traditional acoustic instruments",
            "percussive fingerstyle",
            "acoustic guitar playing",
            "guitar body percussion",
            "convolutional neural networks",
            "variational autoencoders",
            "taxonomy of guitar body percussion",
            "cross-dataset evaluation",
            "latent space parameters"
        ],
        "content": "REAL-TIME PERCUSSIVE TECHNIQUE RECOGNITION AND\nEMBEDDING LEARNING FOR THE ACOUSTIC GUITAR\nAndrea Martelloni\nQueen Mary University\nof London\na.martelloni@qmul.ac.ukAndrew P McPherson\nImperial College\nandrew.mcpherson@imperial.ac.ukMathieu Barthet\nQueen Mary University\nof London\nm.barthet@qmul.ac.uk\nABSTRACT\nReal-time music information retrieval (RT-MIR) has\nmuch potential to augment the capabilities of traditional\nacoustic instruments. We develop RT-MIR techniques\naimed at augmenting percussive ﬁngerstyle, which blends\nacoustic guitar playing with guitar body percussion. We\nformulate several design objectives for RT-MIR systems\nfor augmented instrument performance: (i) causal con-\nstraint, (ii) perceptually negligible action-to-sound latency,\n(iii) control intimacy support, (iv) synthesis control sup-\nport. We present and evaluate real-time guitar body per-\ncussion recognition and embedding learning techniques\nbased on convolutional neural networks (CNNs) and CNNs\njointly trained with variational autoencoders (V AEs). We\nintroduce a taxonomy of guitar body percussion based on\nhand part and location. We follow a cross-dataset evalua-\ntion approach by collecting three datasets labelled accord-\ning to the taxonomy. The embedding quality of the models\nis assessed using KL-Divergence across distributions cor-\nresponding to different taxonomic classes. Results indi-\ncate that the networks are strong classiﬁers especially in a\nsimpliﬁed 2-class recognition task, and the V AEs yield im-\nproved class separation compared to CNNs as evidenced\nby increased KL-Divergence across distributions. We ar-\ngue that the V AE embedding quality could support control\nintimacy and rich interaction when the latent space’s pa-\nrameters are used to control an external synthesis engine.\nFurther design challenges around generalisation to differ-\nent datasets have been identiﬁed.\n1. INTRODUCTION\nThere is increasing interest in deep neural networks for\nprocessing audio in real time with sufﬁciently low latency\nto be used in musical performance. There is also a drive to\nprovide small self-contained platforms that could perform\ninference at the edge , that is, on a device that can be ﬁtted\nin a musical interface or a musical instrument [1–3]. Many\nof the tasks in Music Information Retrieval, such as onset\n© A. Martelloni, A. P. McPherson, M. Barthet. Licensed\nunder a Creative Commons Attribution 4.0 International License (CC BY\n4.0). Attribution: A. Martelloni, A. P. McPherson, M. Barthet, “Real-\nTime Percussive Technique Recognition and Embedding Learning for the\nAcoustic Guitar”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.detection [4], playing technique classiﬁcation [5], timbre\ntransfer [6], re-synthesis of musical information [7] and\ngenerative composition [8], ﬁnd an application in the de-\nsign of Digital Musical Instruments (DMI) and augmented\ninstruments, as long as the solutions conform to real-time\nrequirements. For Real-Time MIR (RT-MIR), two physical\nconstraints that limit the application of Deep Neural Net-\nwork (DNN) models are causality , implying the inability to\nlook into the future, and low action-to-sound latency [9].\nAcceptable action-to-sound latency in music performance\nwas found to be 10 ms [10] for percussion instruments,\nand the latency’s jitter (the variation) was also found to be\na factor in the quality of the interaction [11]. Although\nthere are ways to work around higher latencies, for exam-\nple by synthesising generic attacks before a speciﬁc sound\nis generated [12], the ideal approach would be to develop\na system fulﬁlling the latency constraints in the ﬁrst place.\nIn this work, we investigate RT-MIR for the process-\ning and mapping of guitar body hit sounds to augment the\ntimbral palette of the instrument in percussive ﬁngerstyle.\nPercussive ﬁngerstyle is an extended guitar technique that\nuses layered arrangements, alternate tunings and hits on\nthe guitar’s body to create the impression of a “one-man\nband” [13]. Our method relies on deep learning to develop\nrecognition and embedding learning of guitar body percus-\nsion. Our model addresses the task of generating represen-\ntations of body hits according to performers’ percussive\ngestures, separating them by hand part and location. One\npossible application is to map such a description as pa-\nrameters for a synthesis engine, such as real-time physics-\nbased synthesis. We adapted an Automatic Drum Tran-\nscription (ADT) model based on a Convolutional Neural\nNetwork (CNN) to ﬁt the practical constraints of an aug-\nmented instrument for percussion. Our longer-term aim is\nto design a network that not only works as a classiﬁer, but\nalso describes guitar body hits with a set of features unique\nfor each sample, to support control intimacy [14] and try\nto achieve the same level of nuance afforded by acoustic\ninstruments. To this end, we propose a variation of our\nmodel that jointly trains a classiﬁer and a Variational Au-\ntoencoder (V AE) [15].\n2. BACKGROUND\nPercussion DMIs. In opposition to the direct control\noffered by acoustic percussion, digital percussion instru-121ments have historically afforded indirect control of discrete\nevents [16], with hit dynamics often being the only expres-\nsive parameter over individual hits. Jathal [17] provides\na detailed description of commercial digital percussion in-\nstruments, emphasising the fact that they force the player\nto adapt their technique to the tool, usually a set of but-\ntons or a zone-based sample trigger. The author also ad-\nvocates for the design of interfaces that interpret the tech-\nnique that performers of a particular acoustic instrument\nhave already mastered: traditional techniques will be the\nﬁrst sensorimotor reference that expert players will use for\nthe exploration of DMIs [18], and they have been used in\nthe past as the basis for controllers to navigate synthesiser\nspaces [19].\nHit classiﬁcation. One approach is to take data from\naudio transducers or other sensors for on-the-ﬂy event de-\ntection and classiﬁcation, and the use of that data to trig-\nger the generation of a sound associated to that category.\nExamples are Turchet et al. ’s Smart Cajón [20], Jathal’s\nHandSolo [17] and Zamborlin et al. ’s Mogees [21]. This\napproach is well supported by music tools and software\nfor machine learning in music such as bonk˜ for Max/PD\n[22], timbreID’s barkSpec˜ and the Wekinator [23].\nThis has also been applied to the acoustic guitar through\nthe work of Lähdeoja [24] and Stefani et al. [3, 25], the\nlatter applying fully-connected DNN layers for multi-class\nclassiﬁcation of guitar techniques, including percussive\nones. No direct attempts have been made to use machine\nlearning to achieve a description beyond classiﬁcation, es-\npecially one that would support Moore’s control intimacy\n[14].\nAutomatic Drum Transcription in MIR. A task re-\nlated to guitar percussion classiﬁcation in MIR is Auto-\nmatic Drum Transcription, the audio-based detection and\ninference of score notations for percussive parts. Current\nliterature does not only address the Western drum kit, but\nalso other percussion instruments such as the tabla [26].\nA recent review [27] reports that, in the current state of\nthe art in ADT, solutions either use non-negative matrix\nfactorisation (NMF) or look into the relationships between\nhits and tackle the problem with a language model or a re-\ncurrent neural network. The most relevant system for our\napplication is based on a CNN that jointly performs event\ndetection and classiﬁcation for ADT using a sliding buffer\nof 150 ms [28] and was trained on the MIREX17 drums\ndataset [29]. Mattur Ananthanarayana (MA) et al. ﬁne-\ntuned the model on a dataset of tabla strokes, noting a re-\nsemblance between Kick Drum, Snare Drum and Hi-Hat\nsounds and the tabla strokes themselves [30].\nReal-time DNNs for music performance. Our pur-\npose is not the direct re-synthesis of guitar body hits, but\nrather the control of the parameters of a synthesis engine.\nHowever, we are inspired by the introduction of neural\nnetworks as tools for music performance, through solu-\ntions such as Neural Audio Synthesis and Neural or Dif-\nferentiable Digital Signal Processing (DDSP). Bottlenecks\nand latent spaces have been used with V AEs [31, 32] and\nautoencoder-like structures [33] for re-synthesis of soundsTool Latency\nbonk˜ (Puckette [22]) 6 ms\nStefani et al. [3] 20 ms\nRA VE (Caillon et al. [34]) DAW-deﬁned\nMogees (Zamborlin [21]) 23 ms\nHandSolo (Jathal [17]) 17 ms\nTabla stroke classiﬁer (MA et al. [30]) 150 ms\nTable 1 : Reported buffer values for detection and inference\nfor some of the works and studies cited in this section.\nand timbre transfer, with successful real-time implementa-\ntions such as RA VE [34] and DDX7 [7]. NN-based so-\nlutions have also been applied to model linear [35] and\nnon-linear audio systems, such as guitar ampliﬁers [36]\nand stomp-box overdrives [37, 38]. Solutions exist to load\nan arbitrary neural DSP network into a plugin to be run in\na DAW, such as the Neutone VST host by Qosmo1and\nIRCAM’snn˜2Max/PD external.\nLatency. The impact of latency and jitter in music\nperformance systems was investigated, for example, by\nMcPherson et al. [11]. Table 1 reports the measurements\npublished by the authors cited so far on the latency of their\ntools, speciﬁcally the duration of analysis and inference\nrather than audio input-to-output latency, which is system-\ndependent more than algorithm-dependent. Most tools that\nare meant for real-time use achieve latencies in the region\nof 20 ms, which exceeds Wessel and Wright’s 10 ms ceil-\ning for musical instruments [10].\nChallenges in rich representation. Gesture classiﬁ-\ncation toolkits like bonk˜ have been deployed in many\nmusic-making interfaces, including guitars [24], but they\nwere shown to make percussive guitar performers uneasy\nowing to the chance of misclassiﬁcation for ambiguous or\nunexpected inputs [39]. Standard classiﬁers also do not\nrepresent subtle variability within gestural categories, lead-\ning to a small gestural bottleneck [18]. Related studies in\nHuman-Computer Interaction (HCI) also promote the de-\nsign of DMIs sensitive to the micro-scale of musical ac-\ntions, the scale of differences across gestures of the same\ncategory [40]; authors have suggested that rich and con-\ntrollable behaviour could be as important as high classiﬁ-\ncation accuracy for creative applications [41]. Dimension-\nality reduction of input representations through V AEs, as\nperformed for example by RA VE, could help investigate\nthese rich dimensions.\n3. METHODOLOGY\n3.1 From taxonomy to datasets\nThis work builds upon our two prior studies on the inves-\ntigation of the technique of percussive ﬁngerstyle [13] and\nthe design of a prototype augmented guitar to optimally\ncapture those techniques [39]. Those observations ﬁrstly\n1https://neutone.space/plugin/\n2https://github.com/acids-ircam/nn_tildeProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n122Input Features CNN Type Bottleneck\nTablaCNN 80-band Mel 2-layer 2D.\nKernel size:\n1x7, 1x3128 dimensions,\nreduced through\nPCA\nPercCNN 512-bin FFT dec-\nimated to 64 bins3-layer 1D.\nKernel size:\n6, 5, 52 dimensions\nPercVAE 512-bin FFT dec-\nimated to 64 bins3-layer 1D.\nKernel size:\n6, 5, 52 dimensions ( µ\n+σ)\nTable 2 : Differences across network architectures used.\nHand Part Location In networks\n2-class Kick - K (heel),\nNon-Kick - NK\n(all others)None TablaCNN,\nPercCNN,\nPercVAE\n4-class Heel - H, Thumb\n- T, Fingers - F,\nNails - NNone TablaCNN,\nPercCNN,\nPercVAE\n4-class +\n5-class\n(Hierar-\nchical)Heel - H, Thumb\n- T, Fingers - F,\nNails - NSoundhole, Up-\nper Bout, Lower\nBout, Upper\nSide, Lower SideTablaCNN,\nPercCNN\nTable 3 : Output layers mapped to guitar body percussion\ntaxonomy.\nled to the creation of a taxonomy of guitar body percus-\nsion, inspired by the work by Goddard on the taxonomy of\nbass playing techniques [42]:\n/braceleftBigg\nhit\nscrapethe guitar with\n\nheel\nthumb\nﬁngers\nnailsat the\n\nsoundhole\nupper bout\nlower bout\nupper side\nlower side\nThis was used to create the labelled dataset GPercRep\nby producing 50 examples (one hit per second) of each\ncombination of taxonomy attributes, excluding those that\nare ergonomically impossible, e.g. reaching the lower\nsides of the body with the heel of the hand. This leads to an\nimbalanced but ecologically valid dataset [43]. Each com-\nbination was repeated at four dynamics levels ( p,mp,mf,\nf). All recordings were made by the ﬁrst author on the gui-\ntar built for [39], which has a six-channel output made out\nof one magnetic pickup and ﬁve piezo sensors on each of\nthe locations (soundhole, etc..., see taxonomy above). The\nguitar had 12-53 gauge strings in standard tuning, muted\nwith the left hand, and the hits were played with the bare\nright hand. After excluding scrapes from the analysis, as\nthey require a time-based gesture follower, the dataset has\n3,157 examples extracted from 52 minutes and 37 seconds\nof audio at 44.1 kHz. The dataset is currently not public.\n3.2 Network architectures\nThe baseline model for our experiments is an adaptation of\nthe tabla transcription model proposed in MA et al. [30].\nThis network processes three stacked spectrograms with\ndifferent time/frequency resolutions on a window of 150Input: 6 channels x 64 bins\nLinear: 64 to Nemb\nLinear: Nemb to Ncl (sigmoid)Linear: Nemb to 8 (ReLU)Linear: 8 to Nloc (sigmoid)\nLinear: 64 to Nemb (σ)Recon: 6 channels x 64 bins\nConv1d: 32 x 6 (BN + Leaky ReLU)\nMaxPool1d: 2Conv1d: 64 x 5 (BN + Leaky ReLU)MaxPool1d: 2\nConv1d: 64 x 5 (BN + Leaky ReLU)\nLinear: 1792 to 64 (Leaky ReLU)\nDropout: 0.2Conv1dTranspose: 32 x 6\nMaxUnpool1d: 2Conv1dTranspose: 64 x 5MaxUnpool1d: 2\nConv1dTranspose: 64 x 5\nLinear: 64 to 1792\nV AE Reparametrization\nFigure 1 : Architecture of PercCNN . The extra layers for\nlocation classiﬁcation are on the right-hand side, the de-\ncoder of PercVAE on the left. Nemb= 2,Nloc= 5,\nNcl= 2or4.\nms. Each frame of the spectrogram has an 80-bin Mel rep-\nresentation of a window.\nTo adapt this network to real-time requirements we con-\nstrained the input window to be 512 samples, or 11.6\nms. Our adaptation ( TablaCNN ) receives one window\nof six single Mel-frequency spectra, one for each pickup\nof the prototype. A further modiﬁcation ( PercCNN ) pro-\ncesses down-sampled FFT features through three one-\ndimensional convolutional layers and a bottleneck layer of\ntwo dimensions before the output (Figure 1). To perform\ndimensionality reduction jointly with classiﬁcation, we im-\nplemented reparametrisation from the bottleneck layer and\na decoder mirroring the encoding CNN ( PercVAE ).\n3.3 Output classes\nThe labels according to the guitar body taxonomy were\nsimpliﬁed to: (i) a 2-class scenario with “kicks” (heel hits,\nin reference to kick drum sounds that heel hits are sup-\nposed to imitate) and “non-kicks”; a 4-class output imple-\nmenting all hand parts; (ii) 4-class hand part plus another\n5-class output trained on hit location on the body (hier-\narchical output). The hierarchical output was not imple-\nmented on the V AE. This gave us a total of eight network\nconﬁgurations. Tables 2 and 3 illustrate differences be-\ntween network architectures, and the mappings between\nthe taxonomy in Section 3.1 and the output layers.\nLoss functions used were Binary Cross-Entropy for 2-\nway classiﬁcation, Cross-Entropy for 4-way classiﬁcation,\nand a sum of two equally weighted Cross-Entropies for hi-\nerarchical classiﬁcation (hand part and location). The V AE\nused the following loss function, where γ= 0.001 and\nβ= 3 after hyperparameter search, and BCE replaced by\nCross-Entropy in the four-class model:\nLVAE=BCE+γ(MSERecon+βKLD)\n3.4 Training, data augmentation, cross-validation\nAll networks were trained on the GPercRep dataset with\nhold-out cross-validation: a stratiﬁed 20% of the shufﬂed\ndataset was reserved for testing, whereas the remainder ofProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n123GPercRep GPercHeel GPercPat\nK NK F N W/Avg Recall K NK W/Avg\nTablaCNN - 2-class 97.46 99.42 99.05 85.69 44.44 85.07 74.56\nPercCNN - 2-class 98.33 99.61 99.37 91.68 0.00 85.14 63.10\nPercV AE- 2-class 97.87 99.51 99.20 85.02 0.00 85.14 63.10\nH T F N W/Avg\nTablaCNN - 4-class 97.48 91.06 89.81 94.46 92.92 91.35 35.71 87.32 73.97\nPercCNN - 4-class 94.61 78.95 80.86 93.26 86.92 69.05 74.29 93.33 88.40\nPercV AE - 4-class 97.44 90.30 87.44 93.16 91.63 81.86 0.00 85.14 63.10\nTablaCNN - Hierarchical 96.61 92.24 89.59 93.99 92.77 89.18 23.08 86.11 69.80\nPercCNN - Hierarchical 95.73 82.05 87.60 94.18 90.12 69.55 0.00 85.14 63.10\nTable 4 : F-Measure (as percent) for each network on the three test datasets (hold-out of GPercRep ,GPercHeel andGPerc-\nPat).H= heel, K= kick, T= thumb, NK= non-kick, F= ﬁngers, N= nails, W/Avg = weighted average.\nthe examples was used for training (80%) and validation\n(20%). All networks were trained with an Adam optimiser\nfor 100 epochs with a batch size of 128, saving the model\nwith the highest accuracy on the test set. We trained with\nthe following data augmentation functions: high-pass at 80\nHz, high-pass at 160 Hz, tanh()waveshaping distortion\nwith gain of 5, phase inversion and random changes in gain\nbetween the six channels of each example. Those functions\nare meant to represent the different input impedances and\ndifferent gains of other audio preampliﬁers.\nFurther to the GPercRep dataset, generalisation was\nchecked by performing cross-dataset evaluation [44]. We\nrecorded a snippet of real-world guitar percussion patterns\n(GPercPat ); musically coherent patterns, still with no tonal\nsounds, were played, rather than hits repeated every sec-\nond. Acquisition was done with a different audio inter-\nface, which led to a different combination of gains and fre-\nquency responses across channels in the input audio. The\ndataset was annotated only with a “kick\" and “non-kick”\nlabel, leading to 85 hits in one minute of audio.\nTesting on the GPercPat dataset highlighted a bias in\nour networks against heel hits or “kicks”. An explana-\ntion was thought to be the lack of balance across classes\nin the dataset, however the issue was not mitigated by bal-\nancing the dataset, ensuring the same number of examples\nfor each category. To gather further information about this\nphenomenon, we created a third dataset consisting exclu-\nsively of 601 heel hits, acquired and labelled with the same\ntaxonomy as GPercRep : this will be called GPercHeel .\n3.5 Evaluation metrics\nThe classiﬁcation performance of the networks was eval-\nuated with Precision, Recall and F-measure for each cate-\ngory, as a 2-class or 4-class problem (see Table 3).\nWe also wanted to quantitatively investigate the qual-\nity of the network’s embeddings. Thus, we made sub-\nsets of the data in GPercRep according to each label the\nnetwork was trained on (kick VS non-kick or hand part),\nand for each category, we drew distributions for each of\nthe other parts in GPercRep ’s taxonomy: for example, we\ndivided non-kicks according to their location or their dy-\nnamics. Then we calculated the KL-divergences betweenthe probability distributions of each sub-category. The hy-\npothesis behind this method is that, if the embeddings do\nnot carry any meaningful information beyond the classes\nthat the network was trained on, the distributions will over-\nlap and their KL-divergences will be small and noisy. If,\non the other hand, different hit properties lead to different\npositions in the embeddings, KL-divergences will be dif-\nferent across sub-categories and the sub-categories will be\narranged following a certain order of similarity.\nReconstruction metrics for the V AE were not evaluated\nbeyond their inclusion in the loss function. Future work\ncould focus on the correlation between better reconstruc-\ntion and better separation of each feature.\n4. EV ALUATION\n4.1 Classiﬁcation\nTable 4 contains the F-Measure for the predictions of\neach network, with the three test datasets. In the case of\nGPercHeel , only the Recall is reported; the Precision is al-\nways 1, as all hits are heel hits and there cannot be false\npositives (non-heel hits classiﬁed as heel hits).\n2-Class discrimination. All networks are able to pre-\ncisely discriminate between kicks and non-kicks with an F-\nmeasure above 99%. The GPercHeel dataset shows much\nreduced but still effective classiﬁcation, especially with\nPercCNN . However, the test on GPercPat exposes a gen-\neralisation problem: despite performing data augmenta-\ntion during training, all networks show a bias toward non-\nkicks. PercCNN andPercVAE return only non-kicks in the\ndataset, despite the two classes being visually separable\nwhen data points are extracted and plotted from their em-\nbeddings (not pictured). This result may suggest that the\nnetworks still overﬁt to the extent that they are very sensi-\ntive to the way that the data is acquired.\n4-class discrimination. Uniformly across the tests, the\nnetworks yield an F-measure around 90% for GPercRep .\nThe introduction of the classiﬁcation by location (in the\ntwo Hierarchical networks) does not affect the score of\nthe hand-part classiﬁer. GPercHeel yields a similar Recall\nscore, although higher in the case of TablaCNN networks.\nInterestingly, the weakest model in GPercRep , the 4-classProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n124(a)PercCNN\n/uni00000015/uni00000013 /uni00000013 /uni00000015/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013\n/uni00000053\n/uni00000050/uni00000053\n/uni00000050/uni00000049\n/uni00000049/uni00000029/uni0000004c/uni00000051/uni0000004a/uni00000048/uni00000055/uni00000003/uni00000045/uni0000005c/uni00000003/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000053 /uni00000050/uni00000053 /uni00000050/uni00000049 /uni00000049/uni00000053 /uni00000050/uni00000053 /uni00000050/uni00000049 /uni00000049 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni0000002e/uni0000002f/uni00000010/uni00000027/uni0000004c/uni00000059/uni00000048/uni00000055/uni0000004a/uni00000048/uni00000051/uni00000046/uni00000048(b)TablaCNN\n/uni00000015 /uni00000013 /uni00000015 /uni00000017/uni00000015/uni00000013/uni00000015/uni00000017/uni00000053\n/uni00000050/uni00000053\n/uni00000050/uni00000049\n/uni00000049/uni00000029/uni0000004c/uni00000051/uni0000004a/uni00000048/uni00000055/uni00000003/uni00000045/uni0000005c/uni00000003/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000053 /uni00000050/uni00000053 /uni00000050/uni00000049 /uni00000049/uni00000053 /uni00000050/uni00000053 /uni00000050/uni00000049 /uni00000049 /uni00000013/uni00000011/uni00000013 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000018/uni00000011/uni0000001c /uni00000015/uni00000013/uni00000011/uni00000014\n/uni00000014/uni00000019/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000014/uni00000017/uni00000011/uni00000015 /uni00000014/uni00000017/uni00000011/uni0000001a\n/uni00000014/uni0000001a/uni00000011/uni00000015 /uni00000014/uni00000016/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013 /uni00000014/uni00000013/uni00000011/uni00000015\n/uni00000015/uni00000013/uni00000011/uni00000014 /uni00000014/uni00000017/uni00000011/uni00000013 /uni00000014/uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni0000002e/uni0000002f/uni00000010/uni00000027/uni0000004c/uni00000059/uni00000048/uni00000055/uni0000004a/uni00000048/uni00000051/uni00000046/uni00000048(c)PercVAE\n/uni00000014 /uni00000013 /uni00000014/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013\n/uni00000053\n/uni00000050/uni00000053\n/uni00000050/uni00000049\n/uni00000049/uni00000029/uni0000004c/uni00000051/uni0000004a/uni00000048/uni00000055/uni00000003/uni00000045/uni0000005c/uni00000003/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000053 /uni00000050/uni00000053 /uni00000050/uni00000049 /uni00000049/uni00000053 /uni00000050/uni00000053 /uni00000050/uni00000049 /uni00000049 /uni00000013/uni00000011/uni00000013 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni0000001b/uni00000011/uni0000001a /uni00000017/uni00000016/uni00000011/uni00000015\n/uni00000015/uni00000019/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000013 /uni00000016/uni0000001a/uni00000011/uni00000015 /uni00000016/uni0000001b/uni00000011/uni00000014\n/uni00000016/uni0000001b/uni00000011/uni00000015 /uni00000015/uni0000001a/uni00000011/uni0000001c /uni00000013/uni00000011/uni00000013 /uni00000016/uni00000014/uni00000011/uni00000013\n/uni00000016/uni00000014/uni00000011/uni0000001a /uni00000015/uni0000001a/uni00000011/uni0000001c /uni00000016/uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni0000002e/uni0000002f/uni00000010/uni00000027/uni0000004c/uni00000059/uni00000048/uni00000055/uni0000004a/uni00000048/uni00000051/uni00000046/uni00000048\nFigure 2 : Embeddings from GPercRep : example with ﬁnger hits labelled by dynamics, with matrix of KL divergence\nacross the distributions of each dynamic level.\nPercCNN , ends up being the best model in GPercPat , al-\nthough an F-measure of 74% for kick hits may still not\nbe satisfactory in musical performance. PercVAE shows\nbetter performance than PercCNN , although it still fails to\ngeneralise to GPercPat and defaults to ﬂagging all events\nas non-kicks.\nThe F-Measures in our results are higher on average\nthan the ones found in MA et al. ’s work on tabla hit\ntranscription [30]. At the same time, our results fall be-\nlow the 95% accuracy achieved by Stefani [45] with an\n8-class discriminator on guitar techniques, and the 97%\nby Jathal [17] on the three-way discriminator for tabletop\ndrumming. These results, however, are not directly com-\nparable as the ﬁgures refer to different datasets.\n4.2 Computation times and latency\nAvg Std Dev\nPercCNN 0.496 0.332\nTablaCNN 0.422 0.496\nPercCNN in Max 12.675 1.132\nSystem ( PercCNN )22.310 0.670\nSystem (no NN) 9.922 0.020\nTable 5 : Computation times in µs of both networks mea-\nsured through TorchScript in a C++ wrapper, then end-to-\nend within Max and with an external analogue excitation.\nOur models all require a ﬁxed 11.6 ms input buffer\nto populate the input window after an event is detected,\nfor example through a time-based attack detector [46].TorchScript was used to wrap the two-class ( PercCNN and\nTablaCNN ) into a C++ test routine and a Max/MSP ex-\nternal for a synthetic soak test and real-world latency tests\non a laptop with an Intel i7-8665U CPU running Windows\n(Table 5).\nPercCNN andTablaCNN have comparable latencies in\nthe synthetic test. They both execute in less than half a mil-\nlisecond on average when called 10,000 times. The real-\nworld latency measured manually within Max/MSP (over\n30 examples) reports a value that is consistent with the 11.6\nms window plus the synthetic timing reported above, with\nthe attack detection not introducing much further latency\nor jitter. The low-power laptop used requires an audio\nbuffer size of 256 samples to comfortably run PercCNN\nin real time alongside a suitable synthesis engine: the total\nsystem latency jumps to 22 ms when probing with a Bela3\nboard attached to the laptop’s sound card (averaged over\n500 examples). Input and output buffers can be greatly re-\nduced on ad-hoc hardware or software.\n4.3 Embeddings\nAs introduced in Section 3.5, the distribution of subclasses\nof the taxonomy within each class was explored in the em-\nbeddings of each network. In addition to a visual and qual-\nitative inspection of the distribution through scatter plots,\nKL-Divergence is used here as a similarity metric to mea-\nsure the distance between distributions. In the following\nanalysis, we will take ﬁnger hits divided according to dy-\nnamics as an example, but our observations are valid for\nall other hand parts, and versus hit location (e.g. heel hits\n3https://bela.ioProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n125divided by body location).\nClassiﬁer embedding. When PercCNN is only trained\nas a classiﬁer, the four dynamic points overlap in the\nembeddings (Figure 2a). KL-Divergences range between\n2·10−5and 0.2, so this range of numbers will be used as\na baseline for the interpretation of further results. PercC-\nNNHierarch , trained to discriminate according to hand part\nand location, shows very precise segmentation of hit lo-\ncations but no meaningful segmentation by dynamics (not\npictured4).\nEmbedding with PCA. TablaCNN ’s embeddings are\nnot a bottleneck within the network itself, but they are cal-\nculated through PCA on the 128-dimensional dense layer.\nPrincipal Component Analysis is shown to disentangle\nsome of the other features in the dataset, as the dynam-\nics subclasses are distributed along a right-to-left gradient\n(Figure 2b). The KL-Divergence across those distributions\nreaches a maximum of 20.1.\nEmbedding/V AE latent space. PercVAE shows a sim-\nilar but more pronounced subdivision in the 2-dimensional\nlatent space. The right-to-left gradient is visible but the\nKL-Divergence is much greater at a maximum of 43.2.\nThe KL-Divergence values steadily increase from ptof,\nmore evidently than in the embeddings extracted via PCA.\nThis was noticeable also when hits were segmented by\nlocation (not pictured): for example, Lower Side had a\nKL-Divergence of 30.8 versus Upper Side, 31.7 vs Lower\nBout, 38.7 vs Upper Bout, and 40.7 vs Soundhole.\n5. DISCUSSION\nThe evaluation shows that all our models act as very accu-\nrate 2-class classiﬁers. Even though classiﬁcation accuracy\nis not as high as in other situations (with different datasets),\nthe simplicity of our models and the 11.6 ms input buffer\nmakes them faster than those systems, and well suited for\nimplementation on an edge device.\nChallenges. The main issue arising from our evalu-\nation is the poor generalisation to our GPercPat dataset.\nStill, we have anecdotal evidence that these networks do\nnot behave like poor classiﬁers in the real-world context\nof musical performance with our augmented guitar proto-\ntype, the HITar5. The 2-class PercCNN was coupled with\na time-domain hit detector and made to run in real time;\nits continuous output probability was mapped to a linear\ninterpolation of parameters on the modal synthesis engine\nMetaSynth by CNRS-AMU PRISM [47]; the signal chain\nwas connected to a different guitar (same make and model)\nto the one the network was trained with; the network is\nable to reliably adapt synthesis parameters even when used\nby players other than the main author. There is scope to\nexpand the training and the evaluation by involving more\nguitars, more players and different data augmentation tech-\nniques. However, the augmented guitar that we built allows\n4All pictures of embeddings available at https://github.com/\niamtheband/martelloni_et_al_ismir2023\n5Performance of the HITar at the Guthman Musical Instru-\nment Competition 2023: https://www.youtube.com/live/\nNPtHGYH0JV0?t=1150\nHITar’s Linktree: https://linktr.ee/hit4rus to pursue a further type of behavioural evaluation with\nguitar players. In particular, musicians performing in real\ntime may adapt their gestures until they reliably produce a\ndesired set of outcomes, something not possible with pre-\nrecorded data. A study on the performance of guitar play-\ners with different network conﬁgurations running on the\naugmented guitar prototype will help investigate the degree\nto which the musicians can adapt to the expectation of the\nnetwork; such a study would continue our work in [39].\nSupport for rich interaction. We observed that Per-\ncVAE is able to encode differences in hit dynamics and\nlocation within the embeddings without being trained to\ndiscriminate between them; rather than separating them\nwith decision boundaries like PercCNNHierarch , each sub-\ncategory overlaps neighbouring subcategories, providing a\nsmooth transition that could map well to continuous quan-\ntities such as dynamics or location on a surface. The use\nof a bottleneck layer is also a more efﬁcient solution than\nPCA, as performing PCA would require extra matrix com-\nputation that was not captured in the timings measured at\nSection 4.2. The parameters of a synthesis engine such as\nMetaSynth could be controlled not just by the categorical\noutput of the discriminator, but also by the latent represen-\ntation of the V AE, either directly or through a transform. A\nmapping function could be designed between the embed-\ndings and synthesis parameters, or the embedding vectors\ncould be exposed directly to synthesisers as MIDI Poly-\nphonic Expression (MPE) [48] controls.\n6. CONCLUSIONS\nWe presented three adaptations of Automatic Drum Tran-\nscription for guitar body percussion classiﬁcation and em-\nbedding learning, to support real-time music performance\nand the augmentation of an acoustic guitar through Deep\nNeural Networks. We chose and simpliﬁed a model for\nADT that was shown to be effective in the detection of\ntabla strokes; a variant was also proposed which supports\nhigh-level continuous feature representation through the\nuse of embeddings jointly trained as a Variational Au-\ntoencoder’s latent space. All network conﬁgurations were\ntrained on a dataset of percussive ﬁngerstyle hits acquired\nad hoc , and they were tested on a hold-out portion of\nthat dataset plus two other datasets of similar material.\nThe networks performed very well on a simpliﬁed 2-class\ndiscrimination, and comparably to the state of the art on\nthe full 4-class stroke classiﬁcation with smaller latency.\nHowever, they generalise poorly on a dataset that was\nrecorded with different computer equipment. The embed-\ndings were analysed both qualitatively and quantitatively\nthrough KL-Divergence between subclasses in the taxon-\nomy; they show that the network encodes some informa-\ntion beyond the categories with which it was trained. We\nargue that this information can be used to support richness\nin musical interaction with digital and augmented instru-\nments based on DNN analysis.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1267. ACKNOWLEDGEMENTS\nThis work was supported by UK Research and Innova-\ntion’s CDT in AI & Music [grant number EP/S022694/1]\nand by PRISM Laboratory (CNRS, Aix-Marseille Univer-\nsity).\n8. REFERENCES\n[1] L. Turchet, C. Fischione, G. Essl, D. Keller, and\nM. Barthet, “Internet of musical things: Vision and\nchallenges,” IEEE access : practical innovations, open\nsolutions , vol. 6, pp. 61 994–62 017, 2018.\n[2] T. Pelinski, V . Shepardson, S. Symons, F. S. Caspe,\nA. L. Benito Temprano, J. Armitage, C. Kiefer,\nR. Fiebrink, T. Magnusson, and A. McPherson, “Em-\nbedded AI for NIME: Challenges and Opportunities,”\ninNIME , Jun. 2022.\n[3] D. Stefani, S. Peroni, and L. Turchet, “A comparison\nof deep learning inference engines for embedded real-\ntime audio classiﬁcation,” in Int. Conf. on Digital Au-\ndio Effects , vol. 3, 2022, pp. 256–283.\n[4] S. Böck, A. Arzt, F. Krebs, and M. Schedl, “Online\nreal-time onset detection with recurrent neural net-\nworks,” in Proceedings of the 15th International Con-\nference on Digital Audio Effects (DAFx-12), York, UK .\nsn, 2012, pp. 17–21.\n[5] C.-Y . Wang, P.-C. Chang, J.-J. Ding, T.-C. Tai, A. San-\ntoso, Y .-T. Liu, and J.-C. Wang, “Spectral–temporal\nreceptive ﬁeld-based descriptors and hierarchical cas-\ncade deep belief network for guitar playing tech-\nnique classiﬁcation,” IEEE Transactions on Cybernet-\nics, vol. 52, no. 5, pp. 3684–3695, 2020.\n[6] D. K. Jain, A. Kumar, L. Cai, S. Singhal, and V . Ku-\nmar, “ATT: Attention-based timbre transfer,” in 2020\nInternational Joint Conference on Neural Networks\n(IJCNN) . IEEE, 2020, pp. 1–6.\n[7] F. Caspe, A. McPherson, and M. Sandler, “DDX7:\nDifferentiable FM Synthesis of Musical Instrument\nSounds,” in ISMIR , 2022.\n[8] F. T. Liang, M. Gotham, M. Johnson, and J. Shotton,\n“Automatic stylistic composition of bach chorales with\ndeep LSTM.” in ISMIR , 2017, pp. 449–456.\n[9] D. Stefani and L. Turchet, “On the challenges of\nembedded real-time music information retrieval,” in\nDAFx , 2022.\n[10] D. Wessel and M. Wright, “Problems and Prospects\nfor Intimate Musical Control of Computers,” Computer\nMusic Journal , vol. 26, no. 3, p. 13, 2002.\n[11] A. McPherson, R. Jack, and G. Moro, “Action-sound\nlatency: Are our tools fast enough?” in NIME , 2016,\npp. 20–25.[12] D. Stowell and M. D. Plumbley, “Delayed Decision-\nmaking in Real-time Beatbox Percussion Classiﬁca-\ntion,” Journal of New Music Research , vol. 39, no. 3,\npp. 203–213, Sep. 2010.\n[13] A. Martelloni, A. McPherson, and M. Barthet, “Per-\ncussive ﬁngerstyle guitar through the lens of NIME:\nAn interview study,” in NIME , Jul. 2020, pp. 440–445.\n[14] F. R. Moore, “The Dysfunctions of MIDI,” Computer\nMusic Journal , vol. 12, no. 1, p. 19, 1988.\n[15] D. P. Kingma and M. Welling, “Auto-Encoding Varia-\ntional Bayes,” arXiv:1312.6114 [cs, stat] , May 2014.\n[16] M. M. Wanderley, “Gestural control of music,” in In-\nternational Workshop Human Supervision and Control\nin Engineering and Music , 2001, pp. 632–644.\n[17] K. Jathal, “Real-Time Timbre Classiﬁcation for Table-\ntop Hand Drumming,” Computer Music Journal ,\nvol. 41, no. 2, pp. 38–51, Jun. 2017.\n[18] R. H. Jack, T. Stockman, and A. McPherson, “Rich\ngesture, reduced control: The inﬂuence of constrained\nmappings on performance technique,” in Proceed-\nings of the 4th International Conference on Movement\nComputing - MOCO ’17 , 2017, pp. 1–8.\n[19] P. A. Tremblay and D. Schwarz, “Surﬁng the Waves:\nLive Audio Mosaicing of an Electric Bass Performance\nas a Corpus Browsing Interface,” in NIME , 2010.\n[20] L. Turchet, A. McPherson, and M. Barthet, “Real-Time\nHit Classiﬁcation in a Smart Cajón,” Frontiers in ICT ,\nvol. 5, Jul. 2018.\n[21] B. Zamborlin, “Studies on customisation-driven digi-\ntal music instruments,” Ph.D. dissertation, Goldsmiths,\nUniversity of London.\n[22] M. S. Puckette, T. Apel, and D. D. Zicarelli, “Real-time\naudio analysis tools for Pd and MSP,” in ICMC , 1998,\np. 5.\n[23] R. Fiebrink and P. R. Cook, “The Wekinator: A system\nfor real-time, interactive machine learning in music,”\ninISMIR , 2010.\n[24] O. Lahdeoja, “Augmenting Chordophones with Hybrid\nPercussive Sound Possibilities,” in NIME , 2009, p. 4.\n[25] D. Stefani and L. Turchet, “Demo of the TimbreID-\nVST Plugin for Embedded Real-Time Classiﬁcation of\nIndividual Musical Instruments Timbres,” in FRUCT ,\n2020, p. 3.\n[26] K. Narang and P. Rao, “Acoustic Features for Deter-\nmining Goodness of Tabla Strokes,” in ISMIR , 2017.\n[27] C.-W. Wu, C. Dittmar, C. Southall, R. V ogl, G. Wid-\nmer, J. Hockman, M. Muller, and A. Lerch, “A Review\nof Automatic Drum Transcription,” IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing ,\nvol. 26, no. 9, pp. 1457–1483, Sep. 2018.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n127[28] C. Jacques and A. Roebel, “Automatic drum transcrip-\ntion with convolutional neural networks,” in DAFx ,\n2018, p. 8.\n[29] R. V ogl and P. Knees, “Mirex submission for drum\ntranscription 2018,” in MIREX Extended Abstracts,\n19th International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2018.\n[30] R. MA, A. Bhattacharjee, and P. Rao, “Four-way clas-\nsiﬁcation of tabla strokes with models adapted from\nAutomatic Drum Transcription,” in ISMIR , 2021.\n[31] P. Esling, A. Chemla–Romeu-Santos, and A. Bitton,\n“Generative timbre spaces: Regularizing vari-\national auto-encoders with perceptual metrics,”\narXiv:1805.08501 [cs, eess] , Oct. 2018.\n[32] A. Bitton, P. Esling, and A. Chemla-Romeu-Santos,\n“Modulated Variational auto-Encoders for many-to-\nmany musical timbre transfer,” arXiv:1810.00222 [cs,\neess] , Sep. 2018.\n[33] J. Engel, L. Hantrakul, C. Gu, and A. Roberts,\n“DDSP: Differentiable Digital Signal Processing,”\narXiv:2001.04643 [cs, eess, stat] , Jan. 2020.\n[34] A. Caillon and P. Esling, “RA VE: A variational autoen-\ncoder for fast and high-quality neural audio synthesis,”\narXiv:2111.05011 [cs, eess] , Nov. 2021.\n[35] J. T. Colonel, C. J. Steinmetz, M. Michelen, and\nJ. D. Reiss, “Direct design of biquad ﬁlter cascades\nwith deep learning by sampling random polynomials,”\narXiv:2110.03691 [cs, eess] , Oct. 2021.\n[36] F. Eichas, S. Möller, and U. Zölzer, “Block-oriented\nGray Box Modeling of Guitar Ampliﬁers,” in DAFx ,\n2017, p. 8.\n[37] J. T. Colonel, M. Comunità, and J. Reiss, “Reverse En-\ngineering Memoryless Distortion Effects with Differ-\nentiable Waveshapers,” in AES Convention , Oct. 2022,\np. 10.\n[38] E.-P. Damskagg, L. Juvela, and V . Valimaki, “Real-\nTime Modeling of Audio Distortion Circuits with Deep\nLearning,” in SMC , 2019.\n[39] A. Martelloni, A. McPherson, and M. Barthet, “Gui-\ntar augmentation for Percussive Fingerstyle: Combin-\ning self-reﬂexive practice and user-centred design,” in\nNIME , Jun. 2021.\n[40] J. Armitage, T. Magnusson, and A. McPherson,\n“Studying subtle and detailed Digital Liutherie: Mo-\ntivational contexts and technical needs,” in NIME (Ac-\ncepted) , 2023.\n[41] G. Vigliensoni, P. Perry, and R. Fiebrink, “A Small-\nData Mindset for Generative AI Creative Work,” in\nCHI, 2022.[42] C. Goddard, “Virtuosity in computationally creative\nmusical performance for bass guitar,” Ph.D. disserta-\ntion, Queen Mary University of London, 2021.\n[43] D. D. Ramyachitra and P. Manikandan, “Imbalanced\nDataset Classiﬁcation and Solutions: A Review,” Inter-\nnational Journal of Computing and Business Research ,\nvol. 5, no. 4, 2014.\n[44] A. Livshin and X. Rodet, “The importance of cross\ndatabase evaluation in musical instrument sound clas-\nsiﬁcation: A critical approach.” in ISMIR , Jan. 2003.\n[45] D. Stefani, S. Peroni, and L. Turchet, “A Compari-\nson of Deep Learning Inference Engines for Embedded\nReal-time Audio classiﬁcation,” p. 9, 2022.\n[46] Luca Turchet, “Hard Real-Time Onset Detection Of\nPercussive Sounds,” in Proceedings of the 21st Inter-\nnational Conference on Digital Audio Effects (DAFx-\n18), 2018, p. 9.\n[47] S. Conan, E. Thoret, M. Aramaki, O. Derrien,\nC. Gondre, R. Kronland-Martinet, and S. Ystad, “Nav-\nigating in a space of synthesized interaction-sounds:\nRubbing, scratching and rolling sounds,” p. 9, 2013.\n[48] T. Romo, “MIDI: A Standard for Music in the Ever\nChanging Digital Age,” Capstone Project, California\nState University, 2018.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n128"
    },
    {
        "title": "On the Performance of Optical Music Recognition in the Absence of Specific Training Data.",
        "author": [
            "Juan C. Martinez-Sevilla",
            "Adrian Rosello",
            "David Rizo",
            "Jorge Calvo-Zaragoza"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265289",
        "url": "https://doi.org/10.5281/zenodo.10265289",
        "ee": "https://zenodo.org/records/10265289/files/000037.pdf",
        "abstract": "Optical Music Recognition (OMR) has become a popular technology to retrieve information present in musical scores in conjunction with the increasing improvement of Deep Learning techniques, which represent the state-of-the-art in the field. However, its effectiveness is limited to cases where the target collection is similar in musical context and graphical appearance to the available training examples. To address this limitation, researchers have resorted to labeling examples for specific neural models, which is time-consuming and raises questions about usability. In this study, we propose a holistic and comprehensive study for dealing with new music collections in OMR, including extensive experiments to identify key aspects to have in mind that lead to better performance ratios. We resort to collections written in Mensural notation as specific use case, comprising 5 different corpora of training domains and up to 15 test collections. Our experiments report many interesting insights that will be important to create a manual of best practices when dealing with new collections in OMR systems.",
        "zenodo_id": 10265289,
        "dblp_key": "conf/ismir/Martinez-Sevilla23",
        "keywords": [
            "Optical Music Recognition",
            "Deep Learning techniques",
            "Musical scores",
            "Training examples",
            "New music collections",
            "Comprehensive study",
            "Extensive experiments",
            "Mensural notation",
            "Key aspects",
            "Performance ratios"
        ],
        "content": "ON THE PERFORMANCE OF OPTICAL MUSIC RECOGNITION IN THE\nABSENCE OF SPECIFIC TRAINING DATA\nJuan C. Martinez-Sevilla1Adrian Rosello1\nDavid Rizo1,2Jorge Calvo-Zaragoza1\n1U. I. for Computing Research, University of Alicante, Spain\n2Instituto Superior de Enseñanzas Artísticas de la Comunidad Valenciana, Spain\nadrian.rosello@ua.es, {jcmartinez, drizo, jcalvo}@dlsi.ua.es\nABSTRACT\nOptical Music Recognition (OMR) has become a popu-\nlar technology to retrieve information present in musical\nscores in conjunction with the increasing improvement of\nDeep Learning techniques, which represent the state-of-\nthe-art in the ﬁeld. However, its effectiveness is limited\nto cases where the target collection is similar in musical\ncontext and graphical appearance to the available train-\ning examples. To address this limitation, researchers have\nresorted to labeling examples for speciﬁc neural models,\nwhich is time-consuming and raises questions about us-\nability. In this study, we propose a holistic and comprehen-\nsive study for dealing with new music collections in OMR,\nincluding extensive experiments to identify key aspects to\nhave in mind that lead to better performance ratios. We\nresort to collections written in Mensural notation as a spe-\nciﬁc use case, comprising 5 different corpora of training\ndomains and up to 15 test collections. Our experiments\nreport many interesting insights that will be important to\ncreate a manual of best practices when dealing with new\ncollections in OMR systems.\n1. INTRODUCTION\nManual sheet music transcription is a tedious process,\nprone to errors, and generally requires professionals with\nprecise knowledge of the type of notation and/or music at\nissue. The alternative to this manual digitization of con-\ntent is to resort to cutting-edge technology based on artiﬁ-\ncial intelligence, which performs an automated reading of\ndocuments. This technology is known as Optical Music\nRecognition (OMR).\nOMR has been an active research area for decades [1],\nalthough the ﬁeld progressed slowly [2]. Recently, the\nuse of modern machine learning techniques, namely Deep\nLearning, has led to a paradigm shift that has partially un-\nlocked this situation [3, 4]. Indeed, it has been shown that\n© J. C. Martinez-Sevilla, A. Rosello, D. Rizo and J. Calvo-\nZaragoza. Licensed under a Creative Commons Attribution 4.0 Interna-\ntional License (CC BY 4.0). Attribution: J. C. Martinez-Sevilla, A.\nRosello, D. Rizo and J. Calvo-Zaragoza, “On the Performance of Optical\nMusic Recognition in the Absence of Speciﬁc Training Data”, in Proc. of\nthe 24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.current OMR technologies, despite the fact that they are\nnot yet fully mature, are usually a better alternative than\nperforming the entire transcription by hand [5].\nConcerning the machine learning methods, the related\nliterature reports that the models provide sufﬁcient preci-\nsion when the collections to be transcribed are from the\nsame graphic and content domain as the corpus used to\ntrain them. This, however, makes it difﬁcult to transfer\ntechnology to new collections, since it is not always possi-\nble, desirable, or efﬁcient to invest resources in annotating\na small portion of the target collection. Although it is naive\nto assume the availability of training sets from the same\ndomain as a given target collection, in the current data era\nwe can assume to have at least a series of labeled collec-\ntions, even with different graphic and musical character-\nistics. This, of course, can and should be used to improve\nthe efﬁciency of ﬁtting OMR models to new collections for\nwhich we do not have speciﬁc training sets.\nIn this paper, we report on a case study focused on Men-\nsural notation to answer questions about the transferability\nof OMR models to new music collections. To our best\nknowledge, this work constitutes the ﬁrst to analyze this\nissue in the ﬁeld. We consider Mensural notation as the\nstructuring experimental body because the OMR technol-\nogy can be considered mature for this notation. Also, we\nhave a signiﬁcant number of labeled and unlabeled collec-\ntions in this notation, which allows us to carry out an ex-\nhaustive study that is expected to lead to more generaliz-\nable conclusions. Speciﬁcally, we consider 5 labeled col-\nlections that will be used as training sets, along with their\npossible combinations, and up to 15 unlabeled collections\nas target.\nThe rest of the paper is structured as it follows: in Sec-\ntion 2, we provide some background to the topic; in Sec-\ntion 3, we present our methodology to analyze the question\nat issue; the experimental setup is described in Section 4,\nwhile the results and analysis are given in Section 5; ﬁ-\nnally, we conclude the paper in Section 6, while pointing\nout some interesting avenues for future work.\n2. BACKGROUND\nRecent advances in artiﬁcial intelligence, with extensive\nuse of Deep Learning (DL) technologies, resulted in about\nsuccessful approaches to OMR. Speciﬁcally, a holistic ap-319proach, also known as end-to-end formulation, which has\nbeen dominating the state of the art in other applications\nsuch as text or speech recognition [6, 7], is currently con-\nsidered the reference model in OMR. The related literature\nincludes many successful solutions of this type [8–10]. In\nthis work, we resort to this approach as representative of\nthe state of the art based on DL.\nHowever, as introduced above, there is still no computa-\ntional approach for creating a universal OMR system; i.e,\none that is capable of dealing with any kind of collection.\nThe underlying issue is an overly unsolved challenge in\nartiﬁcial intelligence [11]: DL works well if the problem\nis statistically regular and there is abundant training data\nto adequately and representatively learn such regularity.\nThis is, unfortunately, quite difﬁcult to expect when deal-\ning with ancient documents. Instead of trying to solve the\nunderlying problem of machine learning, we take a more\npractical path to provide a series of best practices to tackle\nthe situation of target collections in the absence of speciﬁc\ntraining data successfully.\nIt is important to highlight that, in the OMR literature,\nthere are very few works dedicated to studying the prac-\ntical aspects of the technology. Pugin and Crawford [12]\nestimated through a quantitative evaluation the suitability\nof using the Aruspix machine-learning-based OMR sys-\ntem on a real collection. Furthermore, Alfaro-Contreras et\nal. [5] analyzed the beneﬁts of using OMR in cases where\nthe accuracy of the system was not perfect. Our work fur-\nther contributes to this barely explored line of practical as-\npects for the application of OMR to real-world scenarios\nfrom the perspective of the available training data.\n3. METHODOLOGY\nThe focus of the work is essentially experimental. We\nwant to be able to answer speciﬁc questions about how\nto approach the generation of generalizable OMR mod-\nels. Our objective is to reduce the uncertainty when facing\nthe recognition of collections for which there is no speciﬁc\ntraining set.\nTo answer these questions, we will consider as a starting\npoint the availability of Ntraining sets that, even depict-\ning the same musical notation (Mensural notation), differ\nin graphic characteristics. This will allow drawing more\ninteresting conclusions about the synergy of using a het-\nerogeneous set of training collections. To cover all possi-\nbilities, we create models from all possible combinations\nof these sets ( 2N−1possibilities). Each of these possibil-\nities will be directly evaluated on Mtest sets (not seen in\nany training case), also showing heterogeneous character-\nistics.\nAs previously mentioned, we will consider a deep end-\nto-end model as representative of the state of the art in\nOMR. Below we explain in more detail how this model\nworks.3.1 Learning framework\nFor the task, a Convolutional Recurrent Neural Network\n(CRNN) scheme is proposed for the end-to-end optical\nmusic transcription pipeline. The CRNN architecture con-\nsists of a block of convolutional layers that learns the rele-\nvant features from the input image (single staff), followed\nby a group of recurrent stages that model the temporal de-\npendencies of the feature-learning block. Finally, a fully-\nconnected network with a softmax activation is used to re-\ntrieve the posteriogram, which is decoded to obtain the pre-\ndicted musical symbols.1\nThe Connectionist Temporal Classiﬁcation (CTC) [13]\ntraining procedure is used to train the CRNN model using\nunsegmented sequential data. The training set Tconsists\nof pairs of single musical staff images xiand their corre-\nsponding symbol sequence ziin a symbol vocabulary Σ,\nwith 261 units corresponding to the number of different\nsymbols among the training sets. To use CTC as an end-\nto-end sequence labeling framework, an additional \"blank\"\nsymbol is included in the vocabulary Σ′.\nFormally, let T ⊂ X × Σ∗be a set of data where an\nimagexi∈ X of a single staff is related to symbol se-\nquencezi=/parenleftbig\nzi1,zi2,...,z i|zi|/parenrightbig\n∈Σ∗, whereΣrepre-\nsents the symbol vocabulary used for encoding the music\nscore. Note that the use of CTC to model the transcrip-\ntion task as an end-to-end sequence labeling framework\nrequires the inclusion of an additional “ blank ” symbol in\ntheΣvocabulary, i.e., Σ′= Σ∪{blank}.\nAt prediction, for a given musical staff image input\nxi∈ X, the model outputs a posteriogram pi∈R|Σ′|×K,\nwhereKrepresents the number of frames given by the\nrecurrent stage. Finally, the predicted sequence ˆziis ob-\ntained resorting to a greedy policy that retrieves the most\nprobable symbol per frame in pi, later a subsequent map-\nping function merges consecutive repeated symbols and re-\nmoves blank labels.\n4. EXPERIMENTAL SETUP\nIn this section, we present our choices for the experimental\ndesign. First, we describe the considered evaluation met-\nric. Then, we give more implementation details of the deep\nlearning model. Finally, we present and describe the col-\nlections selected as train and target sets.\n4.1 Evaluation\nTo evaluate the performance of the OMR model, we resort\nto the Symbol Error Rate (SER). This is computed as the\naverage number of elementary editing operations (inser-\ntions, deletions, or substitutions) required to convert pre-\ndictionˆziinto reference zi, normalized by the length of\nthe latter.\nIn general, we are interested in computing the amount\nof effort it would take for a person to correct the remaining\nerrors in the system. Since computing this human effort\n1Understanding musical symbol as the conjunction of\nglyph:position ,i.e.,note_half:L2 (a glyphnote_half\npresent in the second staff line).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n320does not scale well in practice (it consumes huge amounts\nof resources), we believe that this metric is suitable to mea-\nsure the transcription correctness. In addition, it is a metric\nthat has been commonly applied in previous works on this\nsubject (cf. Section 2).\n4.2 Neural model conﬁguration\nThe CRNN topology is based on the one used in the re-\nsearch [14], where the authors adopt a 4 convolutional\nlayer block with batch normalization, Leaky ReLu activa-\ntion, and max-pooling down-sampling. The feature maps\nextracted from the convolutional block are fed into two\nBidirectional Long Short-Time Memory layers with 256\nhidden units each and a dropout value of d= 50% fol-\nlowed by a fully-connected network with |Σ′|units.\nThe models were trained with a batch size of 16\nelements—note that in experiments where multiple train-\ning sets were used all the generated batches in the train-\ning process were balanced so the net didn’t adjust to a\ncertain corpus. The ADAM optimizer [15] was consid-\nered and a ﬁxed learning rate of 10−3. We iterate for 300\nepochs, keeping the weights that minimize the SER met-\nric in the validation partition with an early stopping policy\nof 30 epochs. Finally, all experiments were run using the\nPython language (v. 3.8.13) with the PyTorch framework\n(v. 1.13.0) on a single NVIDIA GeForce RTX 4090 card\nwith 24GB of GPU memory.\n4.3 Datasets\nA set of 20 different white Mensural notation works has\nbeen collected for this work, consisting of pairs of staff\nimages and their transcription into sequences of musical\nsymbols. The pieces have been selected looking for diverse\ncases concerning printers or copyists, layouts, authors, the\nperiod in history, and extension.2\n4.3.1 Training Datasets\nFor training, 4 different datasets were chosen from real col-\nlections, trying to cover as much variability as possible.\nWhen facing a new transcription project, it is usual that\nno training collection is similar or big enough for build-\ning a model to obtain reliable results from the automatic\nrecognition process. In this scenario, the creation of syn-\nthetic training data from scratch is a valid alternative that\nwill be evaluated in the work with the P RIMENS dataset.\nTherefore, we will add this synthetic collection to the set\nof training sets, resulting in 5 different collections. These\ntraining collections are described below.\n•CAPITAN . The Capitan dataset contains 100 handwrit-\nten pages of ca. 17th-century manuscripts in late white\nMensural notation extracted from the work with signa-\nture B59.850 in the Catedral del Pilar in Zaragoza [16].\n•SEILS . The SEILS dataset contains 151 printed pages\nof the “Il Lauro Secco” collection corresponding to an\n2The whole set, along with a comprehensive description of\nthe contents, can be found at https://grfia.dlsi.ua.es/\npolifonia/ismir2023.html .anthology of 16th-century Italian madrigals in white\nMensural notation [17].\n•GUATEMALA . The Guatemala dataset presents 383\nhandwritten pages from a polyphonic choir book, part\nof a larger collection held at the “Archivo Histórico Ar-\nquidiocesano de Guatemala” [18].\n•MOTTECTA . This dataset corresponds to the work\n“Mottecta (Mottecta Francisci Guerreri, que partim\nquaternis partim quinis alia senis alia octonis concinun-\ntur vocibus, liber secundus dataset)”, authored by Fran-\ncisco Guerrero in the 16th-century and edited by Gi-\nacomo Vincenti in the 17th-century. This 297-printed\nmensural pages corpus has been obtained from the col-\nlection of mensural books of the Biblioteca Digital His-\npánica.3\n•PRIMENS. The Printed Images of Mensural Staves\n(PrIMenS) dataset is a synthetic corpus that tries to\nresemble low-quality real scans of printed mensural\nsources. It has been built from works composed by Agri-\ncola, Frye, and Ockeghem available in the Josquin Re-\nsearch Project4. Given polyphonic scores encoded in\n**kern [19] format, each voice is separated into a single\nﬁle. In order to increase the variability, the original clefs\nare modiﬁed according to the instrument annotation in\nthe voice. To obtain single staves, the whole work has\nbeen divided into a random number of measures from\n3 to 18, and the resulting ﬁles have been converted into\n**mens [20] format. The corresponding agnostic encod-\ning has been generated following the method described\nin [17]. The images have been obtained using the digi-\ntal engraver Verovio [21] by applying random values to\nall the options in the allowed ranges. Finally, those im-\nages have been distorted to simulate real printed image\nscans by using a random sequence of graphical ﬁlters\nwith the GraphicsMagick Image Processing. Addition-\nally, this real-image simulation process has been com-\nplemented by composing randomly damaged old paper\ntextures with distorted images.\nTo better understand the differences that might appear\namong these corpora, we provide a staff example from\neach corpus in Fig. 1.\n4.3.2 Target Datasets\nFor the task of testing the suitability of each model, 15\ndatasets have been chosen. These corpora have been care-\nfully and speciﬁcally labeled for this work, and are sum-\nmarized in Table 1 and Fig. 2.\nThe printed sets have been extracted from the publicly\navailable collection of Mensural books in the Biblioteca\nDigital Hispánica.5The handwritten collections are ob-\ntained from archive of Catedral del Pilar in Zaragoza [16].\n3bdh.bne.es/bnesearch/detalle/bdh0000008932\n4https://josquin.stanford.edu/ (accessed September\n1st, 2022).\n5https://www.bne.es/es/catalogos/\nbiblioteca-digital-hispanica (accessed March 7th, 2023)Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n321(a) C APITAN\n(b) SEILS\n(c) G UATEMALA\n(d) M OTTECTA\n(e) P RIMENS\nFigure 1 : Samples of staves of the different training\ndatasets employed.\nName (ID) Number of staves Printer\nAmorosa (Amo) 224 H. of G. Scoto\nChansons (Cha) 173 A. Le Roy, R. Ballard\nDolci (Dol) 170 H. of G. Scoto\nLamentationes (Lam) 528 G. G. Carlino\nMadrigali (Mad) 201 G. Scotto\nMagniﬁcat (Mag) 1361 Antonio Gardano\nMissarum (Mis) 489 H. of G. Scoto\nMusicaNova (Mus) 874 Antonio Gardano\nOrlande (Orl) 259 A. Le Roy, R. Ballard\nResponsoria (Res) 666 G. G. Carlino\nSacrarum (Sac) 460 Antonio Gardano\nVillanelle (Vil) 59 G. G. Carlino\nB3.28 (B3) 60 Handwritten\nB50.747 (B50) 80 Handwritten\nB53.781 (B53) 32 Handwritten\nTable 1 : Features of the different target collections consid-\nered in this work.\n5. RESULTS\nGiven the number of training corpora (5), the test datasets\n(15), and the number of experiments (31), we are able to\nreport up to 465 different SER results. This enables us to\nproperly summarize the experimentation, extracting mean-\ningful learnings that will be used to state the best practices\nto deal with training data on new projects. The analysis of\nthe results follows. The extended raw results of each exper-\niment are attached to this document in the supplementary\nmaterial.\n5.1 Importance of size and variability\nIn order to understand which is the best training set selec-\ntion strategy when facing a new unseen collection, all the\npossible combinations of the datasets available for training\nhave been evaluated against the different target sets.\nFigure 2 : Image examples from the selected corpora as\ntest partition. The images follow a left-right-top-bottom\norder concerning the list order from Table 1.\nThe more training sets we include in the combination\nthe greater the number of staves of that combined training\nset will be. To evaluate which factor is more important, ei-\nther the variability, given by the number of different train-\ning sets included in each combination, or the size as the\ntotal number of staves to train, we have plotted in Fig. 3\nthe summary statistics of the SER obtained by each trained\nmodel over all the target collections.\nIn general, the best behavior has been obtained when\nmerging all the available training corpora. This ﬁrst out-\ncome may seem obvious, but due to the variability of the\ntraining datasets and some of the test works, it was not il-\nlogical to expect otherwise. From this result, the fact to\nbe explained is why it performs the best, either due to the\nsize of the training set in terms of the number of staves or\nthe generality the model encompasses due to the training\ncorpora of different natures included.\nThe plot shows that, although adding more training cor-\npus does not worsen the results, it is not a determining fac-\ntor. In general, good results are generally obtained with\ncombinations of at least 3 training sets. However, a com-\nbination of just two corpora ( i.e.CS) yields a good per-\nformance both in mean and dispersion that denotes its ro-\nbustness. These two corpora are complementary from the\ngraphical point of view and seem to be representative of\nboth printed sources (SEILS) and handwritten manuscripts\n(Capitan). When applying 3-corpora training set combi-\nnations, the results are equivalent: CGS experiment com-\npared with the GMP, wherein the combination of the ﬁrst\ntwo handwritten corpora and one printed appear compared\nto the collection of one handwritten and two printed train-\ning sets. From these evidences, it can be deduced that the\nvariability of training sets is relevant for better overall per-\nformance.\nIf we focus on the size of the training collection, i.e.,\nthe total number of staves used for training, the plot shows\nthat it is not as important as the variability for the ﬁnal\nperformance. For example, experiment CMS, having less\nthan 4 000 staves, brings better results than experiment GP\nwith over 8 000 samples for training.\nTo conﬁrm the size is not all that matters, Fig. 4 illus-\ntrates the results reported by calculating the number of ex-\nperiments where the SER is minimized in any of the target\ndatasets, taking into account the number of datasets usedProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3220%10%20%30%40%50%60%70%80%90%100%\n 0  2000  4000  6000  8000  10000  12000SER\nAccumulated number of staves in train ing corporaC\nS\nPM\nG\nCG\nGPGMGS\nCPCMCS\nMPPSMS\nCGPCGMCGS\nGMPGPSGMS\nCMPCPSCMS\nMPS\nCGMPCGPSCGMS\nGMPSCMPS\nCGMPS\n#1#2#3#4#5Figure 3 : The boxplot shows different statistical SER ﬁgures (min, Q1, mean, Q3, max) over the test corpora using a\ndifferent combination of training corpora. The colors shown in the right bar represent the number of training corpora used\nin each experiment. The labels are the initials of the corpora included in each training set: C: Capitan, S: SEILS, M:\nMottecta, G: Guatemala, P: PrIMenS.\n0%25%50%75%100%% of no. of collections with lowest errors\nNumber of corpora0.0%6.2%31.2% 31.2% 31.2%\n#1#2#3#4#5\nFigure 4 : Percentage of experiments that minimize the\nSER value for any of the available test corpora.\nto train. It can be noticed that trained experiments with\nsizes 3, 4, and 5 report a value of 31.2%. Aside from the\nvalue itself, what this aspect exposes is that the size of your\ndataset at a given point is no longer a critical factor for the\ntranscription quality.\n5.2 The complexity of a corpus\nThe average SER values for all experiments on each target\ndataset are plotted in Fig. 5. The main noticeable aspect is\nthe difference between Q1 and Q3 (the colored box ends)\nin the diverse corpora. This substantial contrast in disper-\nsion is what we named “The complexity of a corpus”. The\nplot shows that, as expected, the performance depends on\nthe precise selection of the combination of training corpora\nto use. The maximum SER values are obtained when the\ntraining data is built from just one dataset.\nIn general, the worst results in the graph are obtained\nfor handwritten target works (those named with the preﬁx“B”) because, intrinsically, they are more difﬁcult to deal\nwith and need a higher variability in the number of training\ncorpora of handwritten works.\n0%20%40%60%80%100%SER\nT est CorporaAmo\nChaDol Lam\nMad\nMagMis\nMusOrlRes SacVilB3B50 B53\nFigure 5 : The boxplot shows different statistical SER ﬁg-\nures over all experiments made in each one of the testing\ncorpora.\n5.3 The importance of leveraging the availability of\ntraining corpora\nFigure 6 shows the results of the experiments that use each\nspeciﬁc training corpus compared to the experiments that\ndo not use it. The image presents the casuistry when hav-\ning to choose either adding new samples from a different\ndataset or continue increasing the size of existing labeled\nsamples. As the image reveals, every dataset available for\nthe train, no matter the type—printed or handwritten, real\nor synthetic—should be included. It is worth mentioning,\nthat the relevance of adding a new corpus is more notice-\nable than others. For example, referring to the Capitan cor-\npus, if we compare the experiments CMP – MP, CPS – PS,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3230%5%10%15%20%25%30%\nC - Capitan S - SEILS P - PrIMenS M - Mottecta G - GuatemalaSER Analogous Di ﬀerence\nCorpusCGP\nCGM\nCGS\nCMP\nCPS\nCMS\nCGMP\nCGPS\nCGMS\nCMPS\nCGMPSGP\nGM\nGS\nMP\nPS\nMS\nGMP\nGPS\nGMS\nMPS\nGMPS\nCGS\nGPS\nGMS\nCPS\nCMS\nMPS\nCGPS\nCGMS\nGMPS\nCMPS\nCGMPSCG\nGP\nGM\nCP\nCM\nMP\nCGP\nCGM\nGMP\nCMP\nCGMP\nCGP\nGMP\nGPS\nCMP\nCPS\nMPS\nCGMP\nCGPS\nGMPS\nCMPS\nCGMPSCG\nGM\nGS\nCM\nCS\nMS\nCGM\nCGS\nGMS\nCMS\nCGMS\nCGM\nGMP\nGMS\nCMP\nCMS\nMPS\nCGMP\nCGMS\nGMPS\nCMPS\nCGMPSCG\nGP\nGS\nCP\nCS\nPS\nCGP\nCGS\nGPS\nCPS\nCGPS\nCGP\nCGM\nCGS\nGMP\nGPS\nGMS\nCGMP\nCGPS\nCGMS\nGMPS\nCGMPSCP\nCM\nCS\nMP\nPS\nMS\nCMP\nCPS\nCMS\nMPS\nCMPSFigure 6 : Comparison between all experiments containing (green) and not containing (blue) each training set.\nand CMS – MS, we can observe this phenomenon: because\nof the variability that Capitan adds to the training set, the\nimprovement is noticeable. Therefore, a new corpus seems\nto generally improve the model performance, as outlined in\nFig. 5.\nBut not only adding a different corpus helps to improve,\nas the key is to be aware of what is missing in terms of\ngraphical variability in the available training data to build\na more robust model. An interesting piece of evidence in\nthe plot that shows how to proceed when this happens is\nto notice that even a synthetic corpus helps in improving\nthe overall results when it complements the available orig-\ninal training data. Note the reduction in SER when adding\nPrIMenS, that synthetically simulates printed sources, to\ncomplement two other handwritten datasets (Capitan and\nGuatemala).\n5.4 Lessons learned\nIn order to summarize and establish a set of best practices\nto improve the generalization performance of OMR sys-\ntems in the absence of speciﬁc training data, we will intro-\nduce some questions and answers related to the knowledge\nacquired from the experimental outcomes.\n•Which is the best choice to transcribe a new collec-\ntion? In general, one must use all the available training\ncorpora even if some of them are quite different from the\ntarget collection.\n•Is it better to have fewer collections with a high num-\nber of samples or more collections with fewer sam-\nples each? It is preferable to have more variability even\nat the cost of a smaller sample set.\n•How important is it to be aware of the collection to\ntranscribe for selecting the right corpora to train the\nmodel? It is indeed relevant, and depending on the difﬁ-\nculty (for example, whether or not it is handwritten) the\ndifferences in performance can be very varied.•Does the introduction of a synthetic corpus improve\nthe performance? Yes, the introduction of a reliable\nsynthetic collection adds size and variability to the train-\ning data, enabling better performance rates.\nWe consider that these answers can be used as general\nrules of thumbs , although of course in certain cases they\nmay not hold.\n6. CONCLUSIONS\nOMR promises to make written music collections more ac-\ncessible and browsable by automatically recognizing the\nsymbolic content from their images. However, modern\ntechnologies are based on machine learning with deep neu-\nral networks, which typically causes unpredictable perfor-\nmance when processing a collection for which no speciﬁc\ntraining data is available. In this work, we have studied\nthis issue using a large number of training and test col-\nlections depicting Mensural notation. This extensive study\nhas been developed considering a state-of-the-art model as\nrepresentative of the ability to transfer knowledge between\ncollections with dissimilar characteristics.\nOur experiments allowed us to analyze various phenom-\nena related to the synergies created between different train-\ning collections, the importance of choosing a good recog-\nnition trained model to alleviate the uncertainty about per-\nformance in a new collection, as well as a series of gen-\neral good practices on how to proceed for training general\nOMR models.\nAs future work, we want to keep on in this line of inves-\ntigating practical aspects of OMR systems that have a di-\nrect impact on particular use cases. For example, we want\nto extend the case study to the scenario of transfer learning\nand ﬁne-tuning, where a (limited) amount of training data\nfrom a new collection can be assumed. Also, it is interest-\ning to analyze the nature of the errors made by the different\nOMR models, as well as to have a more precise estimate\nof the impact of the different errors on the amount of effort\nrequired during the post-editing correction process.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3247. ACKNOWLEDGMENT\nThis paper is part of the I+D+i TED2021-130776A-\nI00 (PolifonIA) project, funded by MCIN/AEI\n/10.13039/501100011033 and European Union NextGen-\nerationEU/PRTR.\n8. REFERENCES\n[1] D. Bainbridge and T. Bell, “The challenge of optical\nmusic recognition,” Computers and the Humanities ,\nvol. 35, pp. 95–121, 2001.\n[2] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. Marcal,\nC. Guedes, and J. S. Cardoso, “Optical music recog-\nnition: state-of-the-art and open issues,” International\nJournal of Multimedia Information Retrieval , vol. 1,\npp. 173–190, 2012.\n[3] A. Pacha, K.-Y . Choi, B. Coüasnon, Y . Ricquebourg,\nR. Zanibbi, and H. Eidenberger, “Handwritten music\nobject detection: Open issues and baseline results,” in\n2018 13th IAPR International Workshop on Document\nAnalysis Systems (DAS) . IEEE, 2018, pp. 163–168.\n[4] J. Calvo-Zaragoza, J. Hajic Jr, and A. Pacha, “Under-\nstanding optical music recognition,” ACM Computing\nSurveys (CSUR) , vol. 53, no. 4, pp. 1–35, 2020.\n[5] M. Alfaro-Contreras, D. Rizo, J. M. Inesta, and\nJ. Calvo-Zaragoza, “OMR-assisted transcription: a\ncase study with early prints,” in Proceedings of the\n22nd International Society for Music Information Re-\ntrieval Conference . Online: ISMIR, Nov. 2021, pp.\n35–41.\n[6] A. Chowdhury and L. Vig, “An efﬁcient end-to-end\nneural model for handwritten text recognition,” in\nBritish Machine Vision Conference 2018, BMVC 2018,\nNewcastle, UK, September 3-6, 2018 . BMV A Press,\n2018, p. 202.\n[7] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar,\nP. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao,\nE. Gonina, N. Jaitly, B. Li, J. Chorowski, and\nM. Bacchiani, “State-of-the-art speech recognition\nwith sequence-to-sequence models,” in 2018 IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , 2018, pp. 4774–4778.\n[8] J. Calvo-Zaragoza, A. H. Toselli, and E. Vidal,\n“Handwritten music recognition for mensural notation\nwith convolutional recurrent neural networks,” Pattern\nRecognition Letters , vol. 128, pp. 115–121, 2019.\n[9] P. Torras, A. Baró, L. Kang, and A. Fornés, “On the\nintegration of language models into sequence to se-\nquence architectures for handwritten music recogni-\ntion,” in Proceedings of the 22nd International Soci-\nety for Music Information Retrieval Conference, ISMIR\n2021, Online, November 7-12 , 2021.[10] M. Alfaro-Contreras, A. Ríos-Vila, J. J. Valero-Mas,\nJ. M. Iñesta, and J. Calvo-Zaragoza, “Decoupling mu-\nsic notation to improve end-to-end optical music recog-\nnition,” Pattern Recognition Letters , vol. 158, pp. 157–\n163, 2022.\n[11] Y . Bengio, Y . Lecun, and G. Hinton, “Deep learning\nfor AI,” Communications of the ACM , vol. 64, no. 7,\npp. 58–65, 2021.\n[12] L. Pugin and T. Crawford, “Evaluating OMR on the\nearly music online collection,” in Proceedings of the\n14th International Society for Music Information Re-\ntrieval Conference, ISMIR 2013, Curitiba, Brazil,\nNovember 4-8, 2013 , A. de Souza Britto Jr., F. Gouyon,\nand S. Dixon, Eds., 2013, pp. 439–444.\n[13] A. Graves, S. Fernández, F. J. Gomez, and J. Schmid-\nhuber, “Connectionist temporal classiﬁcation: la-\nbelling unsegmented sequence data with recurrent neu-\nral networks,” in Proceedings of the Twenty-Third In-\nternational Conference on Machine Learning, (ICML\n2006), Pittsburgh, Pennsylvania, USA, June 25-29,\n2006 , 2006, pp. 369–376.\n[14] J. Calvo-Zaragoza, A. Toselli, and E. Vidal, “Hand-\nwritten music recognition for mensural notation with\nconvolutional recurrent neural networks,” Pattern\nRecognition Letters , vol. 128, 08 2019.\n[15] D. P. Kingma and J. Ba, “Adam: A Method for\nStochastic Optimization,” in 3rd Int. Conf. on Learning\nRepresentations , Y . Bengio and Y . LeCun, Eds., San\nDiego, USA, 2015.\n[16] J. Calvo-Zaragoza, D. Rizo, and J. M. I. Quereda, “Two\n(note) heads are better than one: Pen-based multimodal\ninteraction with music scores,” in Proceedings of the\n17th International Society for Music Information Re-\ntrieval Conference, ISMIR 2016, New York City, United\nStates, August 7-11, 2016 , M. I. Mandel, J. Devaney,\nD. Turnbull, and G. Tzanetakis, Eds., 2016, pp. 509–\n514.\n[17] E. Parada-Cabaleiro, A. Batliner, and B. Schuller, “A\ndiplomatic edition of il lauro secco: Ground truth for\nomr of white mensural notation,” 10 2019.\n[18] M. E. Thomae, J. E. Cumming, and I. Fujinaga, “Digi-\ntization of choirbooks in guatemala,” in Proceedings of\nthe 9th International Conference on Digital Libraries\nfor Musicology , ser. DLfM ’22. New York, NY , USA:\nAssociation for Computing Machinery, 2022, p. 19–26.\n[19] D. Huron, “Humdrum and Kern: Selective Feature En-\ncoding BT - Beyond MIDI: The handbook of musi-\ncal codes,” in Beyond MIDI: The handbook of musical\ncodes . Cambridge, MA, USA: MIT Press, jan 1997,\npp. 375–401.\n[20] D. Rizo, N. Pascual-León, and C. S. Sapp, “White\nmensural manual encoding: from humdrum to mei,”Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n325Cuadernos de Investigación Musical , no. 6, pp. 373–\n393, 2018.\n[21] L. Pugin, R. Zitellini, and P. Roland, “Verovio: A li-\nbrary for engraving MEI music notation into SVG,”\ninProceedings of the 15th International Society for\nMusic Information Retrieval Conference, ISMIR 2014,\nTaipei, Taiwan, October 27-31, 2014 , H. Wang,\nY . Yang, and J. H. Lee, Eds., 2014, pp. 107–112.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n326"
    },
    {
        "title": "Polar Manhattan Displacement: Measuring Tonal Distances Between Chords Based on Intervallic Content.",
        "author": [
            "Jeff Miller",
            "Johan Pauwels",
            "Mark Sandler 0001"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265427",
        "url": "https://doi.org/10.5281/zenodo.10265427",
        "ee": "https://zenodo.org/records/10265427/files/000103.pdf",
        "abstract": "Large-scale studies of musical harmony are often hampered by lack of suitably labelled data. It would be highly advantageous if an algorithm were able to autonomously describe chords, scales, etc. in a consistent and musically informative way. In this paper, we revisit tonal interval vectors (TIVs), which reveal certain insights as to the interval and tonal nature of pitch class sets. We then describe the qualities and criteria required to comprehensively and consistently measure displacements between TIVs. Next, we present the Polar Manhattan Displacement (PMD), a compound magnitude and phase measure for describing the displacements between pitch class sets in a tonally-informed manner. We end by providing examples of how PMD can be used in automated harmonic sequence analysis over a complex chord vocabulary.",
        "zenodo_id": 10265427,
        "dblp_key": "conf/ismir/MillerP023",
        "keywords": [
            "Large-scale studies",
            "suitably labelled data",
            "algorithm",
            "autonomously describe chords",
            "consistent and musically informative way",
            "tonal interval vectors",
            "insights",
            "TIVs",
            "qualities and criteria",
            "Polar Manhattan Displacement"
        ],
        "content": "POLAR MANHATTAN DISPLACEMENT: MEASURING TONAL\nDISTANCES BETWEEN CHORDS BASED ON INTERV ALLIC CONTENT\nJeff Miller\nj.k.miller@qmul.ac.ukJohan Pauwels\nCentre for Digital Music\nQueen Mary University of London\nj.pauwels@qmul.ac.ukMark Sandler\nmark.sandler@qmul.ac.uk\nABSTRACT\nLarge-scale studies of musical harmony are often ham-\npered by lack of suitably labelled data. It would be highly\nadvantageous if an algorithm were able to autonomously\ndescribe chords, scales, etc. in a consistent and musically\ninformative way. In this paper, we revisit tonal interval\nvectors (TIVs), which reveal certain insights as to the in-\nterval and tonal nature of pitch class sets. We then describe\nthe qualities and criteria required to comprehensively and\nconsistently measure displacements between TIVs. Next,\nwe present the Polar Manhattan Displacement (PMD), a\ncompound magnitude and phase measure for describing\nthe displacements between pitch class sets in a tonally-\ninformed manner. We end by providing examples of how\nPMD can be used in automated harmonic sequence analy-\nsis over a complex chord vocabulary.\n1. INTRODUCTION\nAttempts to autonomously label and analyse harmonic se-\nquences in music constitute some of the longest-standing\nchallenges in music information research (MIR) [1]. Var-\nious strategies have been applied to chord sequence iden-\ntiﬁcation, including information theory [2], graph theory\n[3–5], and predictive methods such as Hidden Markov\nModels [6]. Much attention has been given to developing\ngeometric models of musical distance [7–11].\nFirst proposed by Lewin in 1959 [12] and later, in 2007,\n[13], the discrete Fourier transform can be applied to col-\nlections of pitch classes to produce Tonal Interval Vec-\ntors (TIVs), which can be used to describe the tonal quali-\nties of chords and pitch class proﬁles (PCPs) by revealing\ntheir constituent intervals and tonal structures [14]. Yust\net al. [15] have employed the Fourier transform as a form\nof cluster analysis on large groups of weighted PCP sets,\nwhile Tymoczko and Yust [16] have explored the relation-\nship between voice-leading and Fourier analysis. Other\nprevious work has focused primarily on the magnitude\n© J. Miller, J. Pauwels, and M. Sandler. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: J. Miller, J. Pauwels, and M. Sandler, “Polar Manhattan\nDisplacement: measuring tonal distances between chords based on inter-\nvallic content”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.components of TIVs [17, 18], which offer a useful but in-\ncomplete picture of intervallic content and tonal quality,\nas transposition is ignored and certain chord types such as\nmajor and minor cannot be disambiguated [19]. Further-\nmore, many existing methods of measuring distance be-\ntween TIVs are problematic as they are restricted to pairs\nof chords and are inconsistent when groups of three or\nmore chords are considered. Additionally, many musical\ndistances falter because they do not capture the directional\nnature of musical harmonic tension or are adversely af-\nfected by enharmonic spellings and conﬂicting chord vo-\ncabularies.\nWe present the Polar Manhattan Displacement (PMD),\na method of describing component-wise directional dis-\ntance (i.e., displacement) between TIVs which utilises both\nmagnitude and phase information. We demonstrate PMD\nwithin the context of the 12-tone equally tempered sym-\nbolic domain.\nPMD addresses all 4,095 possible pitch class combi-\nnations and thus can be applied to any chord vocabulary.\nPMD offers a consistent displacement measure between\nchord types (e.g., major7, diminished7, etc.) regardless\nof transposition or the complexity of the chord vocabulary\nemployed. PMD also measures the intervallic displace-\nment between chords, regardless of chord type. In both\ncases, displacement measurements are unaffected by trans-\nposition of an entire sequence, allowing PMD to identify\nrelative harmonic movements regardless of local key struc-\nture.\nTo demonstrate the utility of PMD, we employ a ro-\nbust chord vocabulary of 13 chord types including triads,\n7th chord types, and suspended chords, as well as chro-\nmatic and whole tone scales. We measure displacements\namongst these chord types and transpositions, and close by\npresenting example applications of PMD to automated har-\nmonic analysis and discuss potential applications to other\nareas of music informatics.\n2. BACKGROUND\n2.1 Pitch Class Proﬁles & common musical terms\nA pitch class proﬁle (PCP) is a vector of 12 binary val-\nues, each representing the categorical presence of its cor-\nresponding pitch class in the relevant musical context. This\ncontext is set within the time domain; unless otherwise\nspeciﬁed, we shall be considering notes which occur si-868multaneously. Commonly occurring collections of simul-\ntaneous pitch events may be referred to as ‘chords’. A suc-\ncession of notes occurring sequentially in ascending or de-\nscending pitch order is commonly referred to as a ‘scale’.\nWhen the time window is increased further and some sta-\ntistical weighting or ﬁltering is considered, the dominant\nmembers of the pitch class set may imply a ‘key’.\nThroughout this paper, when referring to the col-\nlection of possible PCPs, we exclude the empty PCP\n[0,0,0,0,0,0,0,0,0,0,0,0] which represents an absence of all\npitch classes, resulting in 212−1 = 4,095possible PCPs.\nTo improve clarity, the terms ’chord’ and ’scale’ shall be\nconsidered interchangeable with ’PCP’ unless otherwise\nnoted. The term ’chord type’ refers to the quality of a chord\n(e.g., major, minor, etc.) regardless of the chord transposi-\ntion. A ’chord’, however, is a speciﬁc combination of root\nand chord type, such as AminorFmaj7.\n2.2 Chord vocabulary\nFor transcription and harmonic analysis purposes, it is use-\nful to focus on the subset of PCPs which correspond to cer-\ntain chord types. Within the domain of all 4,095 PCPs, the\nchoice of chord vocabulary can be a fairly arbitrary deci-\nsion. Often, smaller vocabularies of simple chords are cho-\nsen to simplify experiments and boost performance scores.\nThere is a risk that an over-simpliﬁed chord vocabulary can\nreduce the usefulness of an analytic system, so it is advan-\ntageous that a suitably complex chord vocabulary is em-\nployed.\nFor our examples, we restrict ourselves to a vocabulary\nof 13 chord types (including, by extension, two scales),\nbut the PMD can be applied to any pitch class proﬁle. Our\nvocabulary included triads: major, minor, diminished, aug-\nmented, suspended4 ; tetrads: major7, minor7, dominant7,\ndiminished7, half-diminished7, minor/major7 , and scales:\nchromatic, wholetone . Note that some PCPs can be de-\nscribed using different chord types depending on context,\nthus some chord types, such as maj6, min6 and sus2, are\nsynonymous with other chords already listed in our vocab-\nulary. Regardless of the labels assigned to such synony-\nmous chords, the source PCPs remain the same. For ex-\nample, PCP [1,0,0,0,1,0,0,1,0,1,0,0] could be described as\neitherCmaj6orAmin7. Labelling of chords is highly de-\npendent on context and annotator subjectivity. As PMD\noperates on the basis of underlying PCPs, it is unaffected\nby such discrepancies in annotation.\n2.3 DFTs and Tonal Interval Vectors\nBy applying a discrete Fourier transform (DFT) to a pitch\nclass proﬁle, we can decompose the PCP into a series of\nconstituent intervallic components. Each of these compo-\nnents will describe the degree to which a particular interval\nis present within the PCP. Only components F1–F6are\nrequired to provide a complete representation, since com-\nponentsF7−F11are redundant. Additionally, F0reveals\nthe cardinality of the pitch class set; its value is useful for\nnormalisation and allows us to compare any pair of PCPs\nregardless of the number of pitches present in either.The resulting 6-dimensional complex vector is referred\nto as a tonal interval vector (TIV). Scaling may be applied\nto the various components – often for normalisation pur-\nposes, but also in an attempt to more accurately depict\nperceived cognitive distances between the various interval\ntypes within a musical context. [18]\nIt is worth noting that when generating TIVs, the DFT\nis applied to the symbolic pitch class vectors and not to\naudio data. The purpose of the DFT and resulting TIV is\nto discover the manner in which the pitch classes divide an\noctave into various musical intervals, and to describe the\nstrength and evenness of each intervallic division.\n2.4 Mapping PCPs to TIV space\nEach of the 4,095 possible non-empty PCPs produces a\nunique tonal interval vector. The TIV space therefore is\nan injection of PCP space: each TIV can be mapped un-\nambiguously to a corresponding pitch class proﬁle. Fur-\nthermore, each 6D complex TIV can be represented as a\n12-dimensional real-valued vector by converting the com-\nplex values of the TIV into magnitude and phase values.\nThe magnitude and phase values of the recast TIV vec-\ntor can be represented as a set of 6 tuples (m,p), wherem\nis an unbounded positive real value, and pis a real value\np∈Rsuch that −π≤p≤π. We will refer to a mag-\nnitude and phase tuple (m,p)as a MagPhase tuple. The\nset of 6 tuples describing Fourier components F1-F6of a\nTIV will be referred to as a MagPhase vector.\nIt has been shown [14,17,18] that converting the Fourier\ncoefﬁcients from complex values to real magnitude and\nphase values reveals direct correlations to chord type,\ntransposition, and interval ordinality.\n2.5 Descriptive properties of Magnitude and Phase\nEach TIV Fourier component Fncan be associated with a\ntonal interval and its complementary inverse interval, e.g.\nF1is associated with the presence of both minor 2nd and\nmajor 7th intervals, etc. [14] The magnitude value of a TIV\ncomponent reﬂects how strongly the associated interval oc-\ncurs in the source PCP. For example, F3is associated with\nthe presence of major 3rds. Augmented triads (which are\ncomposed of nothing but major 3rds) have a maximal F3\nmagnitude, whereas diminished 7th chords, which contain\nno major 3rds, have an F3magnitude of 0. Most chords are\ncomposed of several interval types and thus have non-zero\nmagnitudes for all 6 components.\nChords containing the same collection of intervals will\nshare a common magnitude proﬁle. This allows some de-\ngree of chord classiﬁcation based on magnitude values.\nHowever, magnitude proﬁles do not convey the ordinal\nplacement of the intervals within the source PCP, mean-\ning that chord types with identical sets of intervals cannot\nbe disambiguated. For example, major and minor triads\nboth contain one of each of the following atomic intervals:\n{m3,M3,P4}and thus will have identical magnitude pro-\nﬁles. For the same reason, magnitude proﬁles alone do\nnot encode information about the transposition (i.e., the\nroot) of a source PCP, making it impossible to differentiateProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n869Cdom7fromFdom7orGdom7, for example. By incorporat-\ning phase information, both of these shortcomings can be\naddressed.\n3. MOTIV ATION\nEach component F1-F6of a TIV describes the strength\nand position of a particular intervallic quality. By mea-\nsuring the differences between TIV dimensions separately,\nthe intervallic content of TIVs could be exposed and com-\npared. It is reasonable to consider how distances between\nthem might be useful in describing the relationship be-\ntween their respective PCPs, as well as enabling compu-\ntational modelling of musical chords and chord sequences,\nand the automated study of large corpora.\nDistance measures are by deﬁnition non-directional.\nHowever, harmonic transitions are often asymmetrical in\npractice, e.g., there is a difference harmonically between\naV7→Itransition and a I→V7transition. By mea-\nsuring displacement between TIVs, both the distance and\ndirection between them are exposed, which increases the\ndescriptive power of the measure.\nTo extend the difference measure beyond comparisons\nof isolated pairs of chords, it would be highly advanta-\ngeous for the measurements to exhibit collinearity under\nthe addition operation. For example, if two chords of the\nsame type a semitone apart have a particular displacement\nvalue, it stands to reason that two chords a whole tone apart\nshould have a displacement double that value. Crucially,\nthis would allow multiple displacements to be summed,\nmaking it possible to build sequences and represent vari-\nous progressions between two chords consistently.\nFinally, if differences in chord types and intervallic dis-\ntances between chords were decoupled, displacement be-\ntween chords could be visualised on a grid whose orthog-\nonal axes represented each quality. The additive collinear-\nity discussed above would ensure that displacement val-\nues could be summed consistently, regardless of which dis-\nplacements were measured or in what order.\nThis would allow chord sequences to be represented\nas traversals across a tonal grid. An example of such a\ngrid is displayed in Figure 1. Chord transitions such as\nV7→Iwould have the same displacement regardless of\nthe transposition of the pair. For example, G7→Cmaj\nandA7→Dmaj would have an identical displacement,\ndespite being in different keys. Such transpositional invari-\nance would enable an algorithm to catalogue and identify\ncommonly occurring functional sequences.\n4. POLAR MANHATTAN DISPLACEMENT\nTo satisfy the criteria outlined in section 3, we propose the\nPolar Manhattan Displacement (PMD). PMD measures the\ncomponent-wise displacement between two TIVs by cal-\nculating the differences in the magnitude and phase of each\ncomponent. As each component Fkdescribes some aspect\nof the tonal nature of the source chord, this allows us to\nmeasure the displacements between chords in an interval-\nlically informed way.\nFigure 1 . Decoupled chord type and intervallic displace-\nments for the chord transition G7→Cmaj. There are\nthree routes from G7→Cmaj; The PMDs of each arrow\nare summed. All routes result in the same ﬁnal displace-\nment value. Note that the direction of the progression is\nsigniﬁcant.\nPMD thus borrows from L-1 Manhattan distance but ex-\ntends it to measure the directional displacements between\ncorresponding magnitudes and phases in each dimension\nof the polar representation of the TIV plane. The differ-\nences are signed, meaning that PMD reﬂects displacement\nrather than formal distance. This is advantageous within a\nmusical context because tonal harmony is directional; for\nexample the transition (Cmaj→G7)has a markedly dif-\nferent tonal impact than (G7→Cmaj).\n4.1 Magnitude processing\nThe processing of magnitude values is straightforward as\nit involves only scaling and subtraction. After a TIV is ex-\ntracted from a PCP, the magnitudes of F1toF6are divided\nby the value of F0. This normalises all magnitudes to the\nsame scale and allows comparison of TIVs having different\nnumbers of pitches in their source PCPs. Further scaling\nof each component magnitude may be applied to improve\nthe perceptual basis of the space [18] [20]. However, per-\nceptual scaling has not been applied in our study. Follow-\ning the application of normalisation and scaling, magni-\ntude values are simply subtracted such that the difference\nbetween the magnitudes of TIVs UandVis\nDispmag(U→V) = (Vmag−Umag) (1)\n4.2 Phase processing\nPhase values are simply subtracted in a similar fashion to\nthat presented in section 4.1 such that the angular displace-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n870Chord type M1 M2 M3 M4 M5 M6\nMaj7 X\nMin7 X X\nDim7 X X X X X\nAug X X X X\nSus4 X\nChrom X X X X X X\nWholeTone X X X X X\nTable 1 . Some common chord types with zero-magnitude\ncomponents. ’X’ indicates a zero-magnitude vector.\nment between TIVs UandVis\nDispphase(U→V) = (Vphase−Uphase) (2)\nThe cyclic nature of phase information necessitates ad-\nditional processing. Angular phase values θneed to be\ncyclically wrapped into the interval −π < θ≤πafter all\nadditive and subtractive operations on phase values.\n4.3 Deﬁnition of PMD\nHaving deﬁned Dispmag in Eq. (1) and Dispphase in Eq.\n(2), we can now present the deﬁnition of the Polar Manhat-\ntan Displacement. PMD is created by concatenating the 6D\nmagnitude and phase displacement vectors Dispmag and\nDispphase into one 12D vector.\nGiven chords ( Pj,Qk) and their corresponding TIVs\n(U,V),\nPMD(Pj,Qk) =/bracketleftbiggDispmag(U→V)\nDispphase(U→V)/bracketrightbigg\n(3)\n4.4 Zero-magnitude handling\nSome PCPs have various TIV components with zero-\nvalued magnitudes. This indicates that the correspond-\ning interval type is absent from the source PCP. While\nthis is of no consequence when calculating magnitude\ndifference (for the magnitude is simply 0), the associ-\nated phase of these components is effectively undeﬁned,\ncomplicating the calculation of differences between phase\nvalues. In our vocabulary, the following chord types\ncontain one or more zero-magnitude component vectors:\n{maj7,min7,dim7,aug,sus 4,chrom,wltn }. Table 1\ndetails these zero-magnitude chords and their affected\ncomponents.\nWe employ the convention that these phase values are\nconsidered to be 0. This preserves the additive proper-\nties of PMD and allows multiple displacement values to\nbe added together consistently.\n4.5 Investigating displacements of type and interval\nAs indicated in section 3, it would be highly advantageous\n(and musically interesting) to ﬁnd a way to distinguish the\ndegree of displacement due to changes in chord types ver-\nsus shifts of interval. In this section, we examine to what\nextent such a decoupling is possible.For convenience, we present the terms Disptype (rep-\nresenting the displacement between chord types) and\nDispintv (representing the intervallic displacement be-\ntween any two chords of the same type). We deﬁne each\nwith a functional representation, then provide an example\nof the relevant function in use. We employ the following\nconvention to represent a movement from one chord to an-\nother:\n(Pj→Qk) (4)\nwherejandkdenote chord types from our vocabulary and\nPandQdenote two arbitrary chord roots. For example,\nthe following represents a movement from G7toCmaj:\n(G7→Cmaj) (5)\nwherej=dom7,k=maj ,P=G, andQ=C. Note\nthat the direction of the chord transition is signiﬁcant.\n4.5.1 Type-based displacement\nA type-based displacement Disptypecan be derived by cal-\nculating the PMD of two chords (Pj,Qk)having the same\nroot (i.e., P=Q) but different types ( j̸=k). Formally,\nDisptype(j,k) = PMD( Pj,Pk) (6)\nwherej̸=k.\nSigniﬁcantly, these displacement values are consistent\nfor all pairs of chord types (j,k)regardless of the values\nof root (P). As an example, the PMD values corresponding\nto the displacement from a chromatic scale (j)to each of\nthe chords in our vocabulary ( k) are detailed in table 2.\n4.5.2 Intervallic displacement\nLikewise, the intervallic displacement Dispintv between\nany two chords ( Pj,Qk)should ideally be consistent re-\ngardless of the chord types involved. In a similar fashion\nto the calculation of Disptype, we calculate Dispintv by\ncalculating the PMD of two chords (Pj,Qk)having differ-\nent roots (i.e., P̸=Q) but identical types ( j=k).\nDispintv(P,Q,j) = PMD( Pj,Qj) (7)\nwhereP̸=Q. Note that Dispintvis a vector of real num-\nber values and is not expressed in semitones.\nCrucially, the Dispintv is largely – but not entirely\n– independent of chord type j. For all chord types j\nwith uniquely non-zero TIV magnitude components, the\nDispintvin function of the root interval shift ( P→Q) ex-\npressed in semitones is shown in table 3. Chord types jthat\ncontain magnitudes of zero (as shown in table 1) have the\nsame PMD as in table 3 for their non-zero components, but\nboth magnitude and phase components of the PMD corre-\nsponding to the TIV components with value zero are also\nzero. By combining tables 1 and 3, the Dispintv for all\nchord types in our vocabulary can be determined.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n871Comp Chrom Dim7 Wltn Aug Min7 Dom7 Maj7 Dim HDim7 Major MinMaj7 Minor Susp4\nM1 0.00 0.00 0.00 0.00 0.18 0.13 0.13 0.33 0.13 0.17 0.25 0.17 0.24\nM2 0.00 0.00 0.00 0.00 0.00 0.25 0.43 0.33 0.25 0.33 0.25 0.33 0.67\nM3 0.00 0.00 0.00 1.00 0.50 0.35 0.71 0.33 0.35 0.75 0.79 0.75 0.33\nM4 0.00 1.00 0.00 0.00 0.50 0.66 0.25 1.00 0.66 0.58 0.25 0.58 0.00\nM5 0.00 0.00 0.00 0.00 0.68 0.48 0.48 0.33 0.48 0.64 0.25 0.64 0.91\nM6 0.00 0.00 1.00 1.00 0.00 0.50 0.00 0.33 0.50 0.33 0.50 0.33 0.33\nP1 0.00 0.00 0.00 0.00 0.52 1.31 0.26 -1.57 -0.26 -2.36 0.00 -1.31 3.14\nP2 0.00 0.00 0.00 0.00 0.00 1.05 0.53 0.00 1.05 0.00 0.00 -1.05 0.00\nP3 0.00 0.00 0.00 0.00 1.57 0.79 0.79 1.57 2.36 0.46 1.25 1.11 0.00\nP4 0.00 0.00 0.00 0.00 -1.05 -1.76 -2.09 0.00 -0.33 -1.57 0.00 -0.52 0.00\nP5 0.00 0.00 0.00 0.00 -0.52 0.26 1.31 -1.57 -1.31 0.79 0.00 -0.26 0.00\nP6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.14 3.14 3.14\nTable 2 . PM Displacements for each chord type from our vocabulary as end chord, starting from the chromatic scale and\nwith the same root. Each column is a PMD vector representing the displacement from the chromatic scale chord type. To\nreverse the direction, invert the signs of the values.\nComponent +1 +2 +3 +4 +5 +6 +7 +8 +9 +10 +11 +12\nM1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nM2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nM3 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nM4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nM5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nM6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nP1 -0.52 -1.05 -1.57 -2.09 -2.62 3.14 2.62 2.09 1.57 1.05 0.52 0.00\nP2 -1.05 -2.09 3.14 2.09 1.05 0.00 -1.05 -2.09 3.14 2.09 1.05 0.00\nP3 -1.57 3.14 1.57 0.00 -1.57 3.14 1.57 0.00 -1.57 3.14 1.57 0.00\nP4 -2.09 2.09 0.00 -2.09 2.09 0.00 -2.09 2.09 0.00 -2.09 2.09 0.00\nP5 -2.62 1.05 -1.57 2.09 -0.52 3.14 0.52 -2.09 1.57 -1.05 2.62 0.00\nP6 3.14 0.00 3.14 0.00 3.14 0.00 3.14 0.00 3.14 0.00 3.14 0.00\nTable 3 . PM Displacements of each ascending transposition interval in semitones for chord types that have no zero-valued\nTIV magnitudes. Each column is a PMD vector. Note the symmetry of the column values; intervallic distances are constant,\nwhile the sign indicates the direction of transposition. To reverse the direction, (i.e., to transpose down) invert the signs of\nthe values. Also, note that only the phase elements are affected by transposition.\nComponent Tritone Substitution V7−I\n(G7,Db7) (A7,Eb7)(G7,Cmaj) (B7,Emaj)\nM1 0 0 0.043 0.043\nM2 0 0 0.083 0.083\nM3 0 0 0.392 0.392\nM4 0 0 -0.084 -0.084\nM5 0 0 0.161 0.161\nM6 0 0 -0.167 -0.167\nP1 3.142 3.142 0 0\nP2 0 0 0 0\nP3 3.142 3.142 -1.893 -1.893\nP4 0 0 2.285 2.285\nP5 3.142 3.142 0 0\nP6 0 0 3.142 3.142\nTable 4 . PMD comparisons of a) two tritone substitutions and b) two V7−Isequences. Notice that within each pair the\nPMD values are identical. This indicates that any pair of chords separated by this displacement will constitute a tritone\nsubstitution pair or V7−Isequence, respectively, regardless of the transposition.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8725. PMD EXAMPLES\n5.1 Example 1:Tritone substitution\nIt is well known within musical harmonic practice that cer-\ntain chords may be substituted for one another to provide\nalternative or extended versions of an existing or expected\nharmony. A common example of this is the tritone substi-\ntution, wherein a dominant 7th chord can be replaced with\na different dominant 7th whose root is a tritone (i.e., an\naugmented 4th or diminished 5th) away from the original\nroot. The pitches acting as the 3rd and 7th of the original\nchord are retained, but their functions are swapped. The re-\nmaining 2 pitches of the ﬁrst chord are replaced with other\npitches. The overall function is of a new chord that retains\nthe essential character of the original chord, but provides\naddition harmonic tension.\nWhile issues of perceptual similarity are beyond the\nscope of the current study, PMD can provide an ob-\njective means of numerically describing such relation-\nships. For example, consider the Polar Manhattan Dis-\nplacements between each of these two tritone substitution\npairs:(G7,Db7)and(A7,E♭7)as detailed in Table 4. No-\ntice that the PMD values are identical. Any pair of chords\nseparated by this displacement will constitute a tritone sub-\nstitution pair.\n5.2 Example 2: V7 - I detection\nThere are a number of MIR tasks involving chord estima-\ntion, transcription, and automated harmonic analysis that\ncould beneﬁt from the ability to autonomously identify cer-\ntain chord progressions, particularly those which are har-\nmonically signiﬁcant. Traditionally, these tasks are ham-\npered by lack of labelled data, inconsistent chord vocab-\nularies, inter-annotator disagreement, etc. As PMD oper-\nates on unlabelled symbolic data, it could contribute to ad-\ndressing such shortcomings and improving performance in\nautomated transcription, labelling, and harmonic analysis.\nTable 4 describes the displacement between two different\n(V7→I)progressions and conﬁrms that PMD can iden-\ntify and encode the (V7→I)progression consistently,\nregardless of key or transpositional context.\n6. CONCLUSIONS & FURTHER WORK\nWe began with a background review of pitch class proﬁles\nand some basic terms of musical harmonic structures. We\nthen discussed tonal interval vectors: how discrete Fourier\ntransforms can be applied to PCPs to create TIVs, how\nTIVs can be represented as vectors containing magnitude\nand phase values, and how those values describe some as-\npects of the intervallic construction of a chord. We pro-\nposed that it could be useful to measure displacements be-\ntween these objects, and then described the properties nec-\nessary for a robust and self-consistent measure of displace-\nment.\nWe then presented the Polar Manhattan Displacement,\nits fundamental components, and the processing required\nto calculate the measurement of magnitude and phase dif-ference values. There was a brief description of our chord\nvocabulary and the need for suitable chord vocabularies,\nand a brief discussion of how to maintain transpositional\ninvariance when dealing with non-existent magnitude vec-\ntors.\nHaving discussed the criteria for a suitable displace-\nment measure, and detailed the functional components of\nour proposed measurement algorithm, we demonstrated\nhow these components could be aggregated to create the\nPolar Manhattan Displacement measure. We then provided\ntwo examples of potential use cases of PMD, one involv-\ning the autonomous identiﬁcation of tritone substitutions,\nand the other, V7−Iprogressions.\nFuture technical work will involve evaluating the ro-\nbustness of PMD when employed on audio data and at\nvarious scales of temporal granularity. It would be inter-\nesting to investigate extension of PMD to process non-\nbinary PCPs, such as weighted PCPs and harmonic pitch\nclass proﬁles. As the additive properties of PMD allow\ndisplacements to be summed, we would also like to extend\nthe application of PMD to chord sequence modelling and\nanalysis. Finally, we would like to deploy PMD as part\nof a large corpus study to investigate chord similarity and\nharmonic practice.\n7. ACKNOWLEDGEMENTS\nThis research was partially supported by EPSRC grant\nEP/R512072/1 and the British Broadcasting Corporation\nthrough an Industrial CASE studentship in collaboration\nwith the BBC Audio Research Partnership.\n8. REFERENCES\n[1] T. Fujishima, “Realtime Chord Recognition of Musi-\ncal Sound: A System Using Common Lisp Music,” pp.\n464–467, 1999.\n[2] M. T. Pearce, “The Construction and Evaluation of Sta-\ntistical Models of Melodic Structure in Music Percep-\ntion and Composition,” Ph.D. dissertation, City Uni-\nversity, London, 2006.\n[3] R. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan,\nD. Chklovskii, and U. Alon, “Network Motifs: Sim-\nple Building Blocks of Complex Networks,” Science ,\nvol. 298, pp. 824 – 827, 2002.\n[4] R. Milo, S. Itzkovitz, N. Kashtan, R. Levitt, S. Shen-\nOrr, I. Ayzenshtat, M. Sheffer, and U. Alon, “Super-\nfamilies of Evolved and Designed Networks,” Science ,\nvol. 303, no. March, pp. 1538 – 1542, 2004.\n[5] S. Itzkovitz, R. Milo, N. Kashtan, R. Levitt, A. La-\nhav, and U. R. I. Alon, “Recurring Harmonic Walks\nand Network Motifs in Western Music,” Advances in\nComplex Systems , vol. 9, no. 1 & 2, pp. 121–132, 2006.\n[6] C. Harte, M. Sandler, and M. Gasser, “Detecting har-\nmonic change in musical audio,” Proceedings of theProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n873ACM International Multimedia Conference and Exhi-\nbition , pp. 21–26, 2006.\n[7] E. Chew, “Towards a mathematical model of\ntonality,” Ph.D. dissertation, 1999. [Online].\nAvailable: http://www-rcf.usc.edu/{~}echew/papers/\nDissertation2000/ec-dissertation.pdf\n[8] ——, “Out of the Grid and Into the Spiral: Geomet-\nric Interpretations of and Comparisons with the Spiral-\nArray Model,” Computing in Musicology , vol. 15, pp.\n51–72, 2007.\n[9] C. Harte, “Towards Automatic Extraction of Harmony\nInformation from Music Signals,” Ph.D. dissertation,\nQueen Mary University London, 2010.\n[10] D. Tymoczko, “The geometry of musical chords,” Sci-\nence, vol. 313, no. 5783, pp. 72–74, 2006.\n[11] ——, A Geometry of Music: Harmony and Counter-\npoint in the Extended Common Practice . New York,\nNew York, USA: Oxford University Press, 2011.\n[12] D. Lewin, “Re : Intervallic Relations between Two\nCollections of Notes,” Journal of Music Theory , vol. 3,\nno. 2, pp. 298–301, 1959.\n[13] ——, Generalized Musical Intervals and Transforma-\ntions . Oxford University Press, 2007.\n[14] J. D. Harding, “Applications of the Discrete Fourier\nTransform to Music Analysis,” Ph.D. dissertation,\nFlorida State University, 2021.\n[15] J. Yust, J. Lee, and E. Pinsky, “A Clustering-Based\nApproach to Automatic Harmonic Analysis: An Ex-\nploratory Study of Harmony and Form in Mozart’s Pi-\nano Sonatas,” Transactions of the International Society\nfor Music Information Retrieval , vol. 5, no. 1, pp. 113–\n128, 2022.\n[16] D. Tymoczko and J. Yust, “Fourier Phase and Pitch-\nClass Sum,” Lecture Notes in Computer Science (in-\ncluding subseries Lecture Notes in Artiﬁcial Intelli-\ngence and Lecture Notes in Bioinformatics) , vol. 11502\nLNAI, pp. 46–58, 2019.\n[17] G. Bernardes, D. Cocharro, C. Guedes, and M. E. P.\nDavies, “Conchord: An Application for Generating\nMusical Harmony by Navigating in a Perceptually Mo-\ntivated Tonal Interval Space,” International Symposium\non Computer Music Multidisciplinary Research , pp. 1–\n16, 2015.\n[18] G. Bernardes, D. Cocharro, M. Caetano, C. Guedes,\nand M. E. Davies, “A multi-level tonal interval space\nfor modelling pitch relatedness and musical conso-\nnance,” Journal of New Music Research , vol. 45, no. 4,\npp. 281–294, 2016.[19] A. Ramires, G. Bernardes, M. E. P. Davies, and\nX. Serra, “TIV .lib: An open-source library for the\ntonal description of musical audio,” in Proc. of the\n23rd International Conference on Digital Audio Effects\n(DAFx-20) , Vienna, 2020, pp. 8–13.\n[20] I. Sha’ath, “Estimation of Key in Digital Music\nRecordings,” Master’s thesis, Birkbeck College, Uni-\nversity of London, 2011.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n874"
    },
    {
        "title": "Polyffusion: A Diffusion Model for Polyphonic Score Generation With Internal and External Controls.",
        "author": [
            "Lejun Min",
            "Junyan Jiang",
            "Gus Xia",
            "Jingwei Zhao"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265265",
        "url": "https://doi.org/10.5281/zenodo.10265265",
        "ee": "https://zenodo.org/records/10265265/files/000026.pdf",
        "abstract": "We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.",
        "zenodo_id": 10265265,
        "dblp_key": "conf/ismir/MinJXZ23",
        "keywords": [
            "Polyffusion",
            "diffusion model",
            "polyphonic music scores",
            "image-like piano roll representations",
            "controllable music generation",
            "internal control",
            "external control",
            "cross-attention mechanism",
            "melody generation",
            "accompaniment generation"
        ],
        "content": "POLYFFUSION: A DIFFUSION MODEL FOR POLYPHONIC SCORE\nGENERATION WITH INTERNAL AND EXTERNAL CONTROLS\nLejun Min1,2,4Junyan Jiang1,2Gus Xia1,2Jingwei Zhao3\n1Music X Lab, Computer Science Department, NYU Shanghai\n2MBZUAI3Institute of Data Science, NUS\n4Zhiyuan College, Shanghai Jiao Tong University\naik2mlj@gmail.com, {jj2731, gxia}@nyu.edu, jzhao@u.nus.edu\nABSTRACT\nWe propose Polyffusion, a diffusion model that generates\npolyphonic music scores by regarding music as image-\nlike piano roll representations. The model is capable of\ncontrollable music generation with two paradigms: inter-\nnalcontrol and external control. Internal control refers to\nthe process in which users pre-deﬁne a part of the music\nand then let the model inﬁll the rest, similar to the task\nof masked music generation (or music inpainting). Ex-\nternal control conditions the model with external yet re-\nlated information, such as chord, texture, or other features,\nvia the cross-attention mechanism. We show that by us-\ning internal and external controls, Polyffusion uniﬁes a\nwide range of music creation tasks, including melody gen-\neration given accompaniment, accompaniment generation\ngiven melody, arbitrary music segment inpainting, and mu-\nsic arrangement given chords or textures. Experimental re-\nsults show that our model signiﬁcantly outperforms exist-\ning Transformer and sampling-based baselines, and using\npre-trained disentangled representations as external condi-\ntions yields more effective controls.1\n1. INTRODUCTION\nDiffusion models [1, 2], as a new class of generative mod-\nels, have been successful in generating high-quality sam-\nples of image data and beyond. They achieve state-of-the-\nart sample quality on a number of image generation bench-\nmarks [3, 4], and also show strong results for the genera-\ntion of various media such as audio [5,6], video [7–9], and\ntext [10, 11].\nSymbolic music generation, a task very different from\naudio generation, has highly discrete outputs and is of-\nten described in terms of constraint optimization problems\n[12, 13]. Despite the improvement of deep music genera-\n1Demo page: https://polyffusion.github.io/ . Code\nrepository: https://github.com/aik2mlj/polyffusion\n© L. Min, J. Jiang, G. Xia, and J. Zhao. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: L. Min, J. Jiang, G. Xia, and J. Zhao, “Polyffusion: A\nDiffusion Model for Polyphonic Score Generation with Internal and Ex-\nternal Controls”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.\nFigure 1 : The forward and reverse process of the proposed dif-\nfusion model trained on piano roll representations. The red dot at\nthe front of each note denotes its onset; the green bar following\nit denotes its sustain. Notice that the image axes are swapped for\nproper visualization.\ntive modeling [14,15], symbolic music generation still suf-\nfers from the lack of controllability and consistency at dif-\nferent time scales [16]. In our study, we experiment with\nthe idea of using diffusion models to approach controllable\nsymbolic music generation.\nInspired by the high-quality and controllable image\ngeneration that diffusion models have achieved in com-\nputer vision, we devise an image-like piano roll format as\nthe input, and used a UNet-based diffusion model to step-\nwise denoise a randomly sampled piano roll, as illustrated\nin Figure 1. We show in our experiments and demos that\nour design provides excellent generation results.\nBesides unconditional generation, the model also ac-\ncepts two categories of controls, namely internal control\nand external control:\n•Internal Control (Inpainting): By masking out part of\nthe given piano roll, we can specify the remaining area to\nbe generated, thus implicitly conditioning the generation\nto ﬁt in the masked part. We regard this strategy as a\ngeneralized music inpainting method.\n•External Control (Conditional Generation): By\nadopting the cross-attention mechanism of Latent Dif-\nfusion [17], we can explicitly control the music genera-\ntion on given external conditions such as chords and tex-\ntures. They are ﬁrst encoded into latent representations\nusing pre-trained, disentangled variational autoencoders\n(V AEs), and then fed into the backbone UNet of the dif-\nfusion model to condition the denoising process. We\nshow that the generated music complies with the given\nconditions well. We also add classiﬁer-free guidance to\ncontrol the variance of the generation.231These controls of diffusion models enable us to unify a\nwide spectrum of creative music tasks that previously re-\nquire separate modeling and training. In this paper, we\nshowcase the following scenarios:\n•Melody generation given accompaniment by genera-\ntion with the accompaniment part being masked out.\n•Accompaniment generation given melody by genera-\ntion with the melody part being masked out.\n•Arbitrary music segment inpainting by generation\nwith any time segments being masked out.\n•General music arrangement given chords or textures\nby conditioning on external chord or texture signals.\n2. RELATED WORK\nWe review three realms of related work: 1) music inpaint-\ning, which is related to our internal control method, 2)\nconditioned music generation with external signals, which\nis related to our external control method, and 3) recent\nprogress on diffusion-based modeling in the music domain.\n2.1 Music Inpainting\nMusic inpainting is a controlled music generation task\nthat regulates the generation with pre-deﬁned musical con-\ntexts. We see various studies on polyphonic music in-\npainting. For example, DeepBach [18] develops a context-\naware recurrent neural network (RNN) capable of inpaint-\ning missing notes for chorales in the style of Bach. Co-\nconet [19] uses blocked Gibbs sampling to repeatedly\nrewrite a masked music score. Chang et al. [20] achieve\nvariable-length music score inpainting. Music Sketch-\nNet [21] and MusIAC [22] introduce various controls to the\ninpainting task under V AE-based and Transformer-based\nframework respectively. Comparatively, diffusion models\nnaturally possess the inpainting ability via masked genera-\ntion [23], and there is no need to train or ﬁne-tune a task-\nspeciﬁc model for inpainting.\nThough the current inpainting tasks mostly apply masks\nover a continuous period of time, the inpainted area, in\ntheory, can be any note in the score (any area of a piano\nroll). In this study, we show that our image-like repre-\nsentation enables both part-wise and time-wise inpainting.\nThe former refers to inpainting melody or accompaniment\npart given the other part, while the latter refers to inﬁlling\nnotes falling in arbitrary time segments.\n2.2 Music Generation Conditioned on External\nSignals\nExternal control signals are also one of the mainstream\nmethods to control the music generation process. Com-\nmon scenarios include generating music given chords [18,\n24–26], lyrics [27], and other relevant features such as note\ndensity and voicing numbers [28].\nOur study focuses on polyphonic score generation con-\ntrolled by external chords and textures. In particular, the\nFigure 2 : The model structure with an additional condition mod-\nule for external control. Each UNet unit ϵθapplies one denoising\nstep during the reverse process. External condition signals are\nencoded by pre-trained encoders and fed into the cross-attention\nlayers, which are represented by the yellow squares in the UNet\nunit.\n“control by texture” task has great practical value in both\nmusic arrangement and composition style transfer [29],\nwhile very few existing models could realize this function.\n2.3 Diffusion Models for Music Generation\nRecently, we have seen several attempts to introduce dif-\nfusion models to symbolic music tasks. Mittal et al. [30]\ngenerate monophonic music by training a diffusion model\non the latent representations learned by MusicV AE [31].\nCheuk et al. [32] brings diffusion models to the music tran-\nscription task by adapting the piano roll format into the\nDiffWave [5] structure. It is relevant to our study as the\nmodel can also output piano rolls. However, the model fo-\ncuses on transcription instead of generation by relying on\na ground-truth spectrogram as its control. In general, for\nsymbolic music generation, conditioning diffusion models\non external controls is still an area to be explored.\n3. METHODS\n3.1 Data Representation\nOur image-like piano roll representation is a 2-channel bi-\nnary tensorx∈R2×T×P. The generation task targets 8-\nbar (32-beat) long music segments, with 1/4 beat as the\ntime step, resulting in T= 128 time steps per sample. We\nuse a MIDI pitch range 0...127, resulting in P= 128 pitch\nbins. Each entry x(c,t,p)represents whether there is a\nnote onset (for c= 0) or sustain (for c= 1) at time step t\nand MIDI pitch p.\n3.2 Diffusion Model\nDiffusion models [1, 2] are latent-variable models com-\nprised of a forward (diffusion) process which gradually\ndisrupts the structure of data x0and a reverse (denoising)\nprocess that learns to recover the original data x0from the\nnoisy input. In our study, x0denotes the clean piano roll.\nThe forward process iteratively adds Gaussian noise in N\ndiffusion steps:\nq(xt|xt−1) =N(xt;/radicalbig\n1−βtxt−1,βtI) (1)\nq(x1:N|x0) =N/productdisplay\nt=1q(xt|xt−1) (2)Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n232whereβ1,β2,...,β Nare a series of variance scheduling\nparameters. The reverse process requires the model to pa-\nrameterize a Markov chain that iteratively reconstructs the\npiano roll x0from a corrupted input xN∼ N(0,I).\npθ(xt−1|xt) =N(xt−1;µθ(xt,t),σθ(xt,t)) (3)\npθ(x0:N) =p(xN)N/productdisplay\nt=1pθ(xt−1|xt) (4)\nDuring training, we optimize the model parameters ϵθ\nby minimizing the following target:\nL(θ) =Ex0,ϵ,t/bracketleftBig/vextenddouble/vextenddoubleϵ−ϵθ/parenleftbig√¯αtx0+√\n1−¯αtϵ,t/parenrightbig/vextenddouble/vextenddouble2/bracketrightBig\n(5)\nwheretis uniformly sampled from [1,N]andϵ∼ N(0,I),\nαt:= 1−βt,¯αt:=/producttextt\ns=1αs. As shown in Figure 2, our\nunconditional model structure is based on [2], an image-\noriented diffusion model using a 2-D UNet as its backbone\nϵθ.\n3.3 Internal Control (Inpainting)\nInternal control refers to the use of the music notes them-\nselves to regulate and inﬂuence the generation process, and\nwe regard music inpainting as a means of internal control.\nSpeciﬁcally, we denote the given piano roll sample as s\nand the mask as m. At each step tduring inference sam-\npling, the ﬁxed area of the image is diffused with the for-\nward process q(st|s) =N(st;√¯αts,(1−¯αt)I)and put\ntogether with the denoising sample st−1. Algorithm 1 [23]\nshows the detailed implementation of this inpainting pro-\ncess.\nAlgorithm 1 Inpainting Process\nInput : inpainting mask m, original sample s,xN∼\nN(0,I)\n1:fort=N,...,1do\n2:ϵ1,ϵ2∼ N(0,I)ift >1, elseϵ1=ϵ2= 0\n3:y=√¯αts+√1−¯αtϵ1ift >1, elses\n4:xt−1=µθ(xt,t)+σθ(xt,t)ϵ2\n5:xt−1=xt−1⊙(1−m)+y⊙m\n6:end for\n7:returnx0\n3.4 External Control (Conditional Generation)\nExternal control means using external signals to condition\nthe generation process. We aim to incorporate a general\nstrategy that does not place strong assumptions on the for-\nmatof input control signals. To this end, we use the cross-\nattention mechanism [33] for conditional generation intro-\nduced by Latent Diffusion [17] since it is insensitive to the\ndimension of the condition signals. We also adopted the\nstrategy used by Rombach et al. [17], which augments the\nbackbone UNet structure with cross-attention layers that\nmap condition signals into the UNet intermediate latent\nrepresentations.Formally, to preprocess the external musical signal c,\nwe introduce a corresponding encoder τthat projects cto a\nlatent representation τ(c). The encoder τis pre-trained and\nﬁxed during diffusion model training. The cross-attention\nlayers then map τ(c)to the intermediate layers of the UNet\n(as shown in Figure 2). The conditional training objective\nis\nLcond(θ) :=Ex0,c,ϵ,t/bracketleftBig/vextenddouble/vextenddoubleϵ−ϵθ/parenleftbig√¯αtx0+√\n1−¯αtϵ,t,τ(c)/parenrightbig/vextenddouble/vextenddouble2/bracketrightBig\n(6)\nWe use classiﬁer-free guidance (CFG) [34] to enable\nboth conditioned and unconditioned generation by control-\nling the intensity of the condition signals during sampling.\nWe refer readers to [34] and [35] for details on CFG.\n4. CONTROLLABLE MUSIC GENERATION\nIn this section, we present four general musical applica-\ntions our model empowers with internal and external con-\ntrols: 1) melody generation given accompaniment, 2) ac-\ncompaniment generation given melody, 3) arbitrary mu-\nsic segment inpainting, and 4) music arrangement given\nchords or textures. For each application, we provide non-\ncherry-picked generated samples as a case study. We also\nrefer readers to our demo page for more examples.\n4.1 Melody Generation Given Accompaniment\nThis task is achieved by internal control — to pre-deﬁne\nthe accompaniment part and let the model inﬁll the up-\nper melody. Figure 3(a) shows an example of pop song\nmelody generation given the accompaniment. We see that\nthe melody is consistent with the underlying chords of the\ngiven accompaniment, and maintains an overall consistent\nrhythmic pattern, except for a 16th-note jump at the begin-\nning of the 3rd bar.\n4.2 Accompaniment Generation Given Melody\nSimilarly, given a lead melody, we can inpaint its corre-\nsponding lower accompaniment. Figure 3(b) shows an ex-\nample, in which we see that the generated chord sequence\nsuits the key (E minor) of the melody well, realized by a\nconsistent arpeggio texture. The generated counter-melody\nalso ﬁlls in the gaps between melody onsets well.\n4.3 Arbitrary Music Segment Inpainting\nThe common scenario of music inpainting, also called mu-\nsic inﬁlling [20], is to generate a music segment that ﬁlls\nin the gap between given past and future contexts. For our\nmodel, this task can be fulﬁlled by masking out the full\npitch range of selected bars for inpainting.\nFigure 3(c) shows an example of the inpainting process\nof the 3rd, 4th, 5th, and 7th bars, given the rest as ﬁxed\ncontexts. In the example, the model is capable of generat-\ning a full cadence connecting the 7th and the 8th bar, and\nalso a nice applied chord in the non-diatonic progression\nGm-Adim-BZm connecting the 5th and the 6th bar.\nWe also extend the problem setup and let the diffusion\nmodel generate long-term music by iteratively inpaintingProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n233                   \n     \n \n\n     \n     \n\n\n   \n      \n   \n \n(a) An example of melody generation given accompaniment.\n                                \n           \n  \n\n\n          \n\n\n     \n  \n\n     \n  \n(b) An example of accompaniment generation given melody.\n\n        \n              \n \n     \n\n  \n \n  \n \n  \n \n   \n\n   \n \n\n\n\n   \n \n\n\n(c) An example of arbitrary segment inpainting.\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n   \n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n \n \n\n \n\n\n\n\n  \n\n \n\n\n\n\n  \n\n  \n\n\n \n\n  \n \n \n\n\n  \n   \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n  \n\n\n \n \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n  \n \n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n \n\n\n   \n\n\n\n\n\n\n\n \n\n \n\n\n\n  \n  \n  \n \n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n1\n9\n17\n(d) An example of iterative inpainting for long-term music generation.\n      \n  \n  \n  \n\n \n     \n          \n         \n\n \n \n        \n \n\n\n  \n\n\n\n\n     BC D E D F F E GHC GHC E BC E D D F F\n\n \n(e) An example of chord-conditioned generation. Chords (annotated above) are used as external condition signals.\n       \n      \n        \n    \n\n  \n          \n      \n     \n\n\n          \n\n\n\n\n  \n \n\n\n    \n   \n(f) An example of texture-conditioned melody generation. The texture of a given melody (the staff above) is used as external condition signals.\nFigure 3 : Generated samples in various tasks of controllable music generation. The generated parts are marked in blue. These examples\nhave corresponding hearable demos on the demo page.\nthe future given the past. Figure 3(d) shows an example of\na 24-bar generation based on a 4-bar prompt. The model\ngenerates 4 bars during each inference and ﬁnishes the pro-\ncess with ﬁve iterations. We see that the generated music\ncontains a smooth chord progress, with a key modulation\ntowards the end. The long-term textural structure is coher-\nent, however lacking a consistent music theme.\n4.4 Music Arrangement Given Chords or Textures\nInspired by the chord -texture disentanglement work [13,\n29], we choose these two factors as the external condition\nsignals for polyphonic generation. In our context, chords\nrefer to the harmonic information, and textures refer to the\nrhythmic information. The latent chords and textures are\nencoded using pre-trained V AEs and cross-attended with\nthe backbone UNet.\nBeat-wise chords are ﬁrst extracted by rule-based meth-ods [36, 37], in which we adopted a 36-D chord represen-\ntation consisting of a 12-D one-hot root encoding, a 12-D\none-hot bass encoding and 12-D multi-hot chroma encod-\ning. We then use a chord V AE [13] to extract a 512-D\nrepresentation for each 8-bar chord sequence. For texture\nconditioning, we encode each 2-bar segment with the pre-\ntrained texture encoder in [13] and then concatenate four\nencoded 256-D representations into a 1024-D vector as an\n8-bar texture representation.\nFigure 3(e) demonstrates an example of polyphonic mu-\nsic generation conditioned on chords. In the example, the\naccompaniment and the melody are mostly chord notes,\nwith a certain degree of non-chord passing and neighbor-\ning tones that increase the interestingness of the song.\nTo show the complex combinations of conditions that\nthe model can handle, we showcase a “texture-speciﬁed\nmelody generation” for a given accompaniment segment\nas an example of the combination of internal and externalProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n234controls. As shown in Figure 3(f), We generate the melody\npart of a given accompaniment segment conditioned on\nthe encoded texture representations of a given melody line.\nThe result preserves a similar rhythmic pattern and ﬁts the\ntonality of the new accompaniment.\n5. EXPERIMENTS\n5.1 Dataset and Training\nWe train our model using the POP909 dataset [38], a pop\nsong dataset containing around 1K MIDI ﬁles. We only\nkeep the pieces with 2/4 and 4/4 meters and cut them into\n8-bar music segments with 1-bar hopping size, which re-\nsults in 64K samples in total. The dataset is randomly split\ninto the training set (90%) and validation set (10%) on a\nsong level. The training samples are randomly transposed\nto all 12 keys for data augmentation.\nThe classiﬁer-free guidance technique stated in Sec-\ntion 3.4 combines unconditional and conditional train-\ning. We adopt the implementation of DDPM and cross-\nattention layers in [39]. With 1K total diffusion steps, the\nmodel converges around 50 epochs (200K steps) on Adam\nOptimizer [40] with a constant learning rate 5e-5.\nTo turn the generated 2-channel piano roll representa-\ntions into MIDI ﬁles, we round them to {0, 1} and neglect\nnotes without an onset. In practice, the generation process\nof 160 8-bar samples report zero invalid notes.\n5.2 Evaluation\nTo validate the generation quality and control effectiveness\nof our model, we conducted both objective and subjec-\ntive evaluations on 5 tasks: (1) unconditional generation,\n(2) accompaniment generation, (3) segment inpainting, (4)\nchord-conditioned generation, and (5) texture-conditioned\ngeneration. Tasks 2-3 focus on the evaluation of internal\ncontrols, and tasks 4-5 focus on external controls. Table 1\nsummarizes the evaluation method for each task.\n5.2.1 Evaluation Metrics\nObjective metrics : To objectively measure the music\nquality for all 5 tasks, we use the averaging overlapped\narea of pitch distribution ( DP) and duration distribution\n(DD) from [41], which measure the distribution similarity\nof pitch and duration between the generated samples and\nground truth. Additionally, we introduce chord distance\n(CD) [41] and onset distance (OD) to evaluate the efﬁcacy\nof external control. These metrics measure the ℓ2distance\nof chord (for task 4) and onset distribution (for task 5) be-\ntween the generated samples and the chord/texture condi-\ntion.\nSubjective metrics : Subjective metrics include creativ-\nity(C),naturalness (N), and musicality (M), which provide\na perceptive evaluation complementing the objective musi-\ncal quality metrics. To demonstrate the efﬁcacy of internal\ncontrol, we pick accompaniment generation as an exam-\nple and add a ﬁtness (F) metric to evaluate how well the\ngenerated parts ﬁt in with the given melody.5.2.2 Baseline models\nWe use two types of models as our baselines:\nTransformer models : As suggested in the polyphonic\nrepresentation disentanglement study [13], applying a\nTransformer on disentangled latent codes yields better re-\nsults than raw token predictions. Following [13], we train\na Transformer to predict the chord and texture representa-\ntions from melody representations. For unconditional gen-\neration (task 1), we sample the latent spaces of the ﬁrst\n2-bar melody and then predict its accompaniment and the\nfollowing content. For accompaniment generation (task 2)\nand external conditioning (tasks 4-5), the melody (task 2),\nchord (task 4), or texture (task 5) latent representation is\ndirectly encoded as the condition for the Transformer. We\nadopt the XLNet-based model proposed in [20] for the mu-\nsic segment inpainting task (task 3).\nSampling-based models : We adopt the V AE-based dis-\nentanglement model in [13] and generate music segments\nby sampling the latent spaces. For unconditional genera-\ntion (task 1), we sample from the chord and texture latent\nspaces of the ﬁrst and the last 2 bars, then linearly inter-\npolate the middle latent codes to form a coherent 8-bar\nsegment. For inpainting (task 3), we also use linear in-\nterpolation on latent codes to inﬁll the missing bars. For\nexternal conditioning (tasks 4-5), the chord (task 4) or tex-\nture (task 5) latent component is directly encoded from the\ngiven condition.\n5.3 Comparative Results\nWe calculate the average of each objective metric on\n160 generated samples for each task. As shown in Ta-\nble 2, Polyffusion and its variations achieve the high-\nest objective scores in tasks 1-4. For controllability, our\nmodel yields competitive results on segment inpainting and\nchord-conditioned generation. For the texture-conditioned\ngeneration task, our model does not perform as well as the\nbaseline but is capable of preserving the general musical\ntexture, since the baseline model is explicitly trained on\ntexture reconstruction targets, while the texture condition\nof our model only serves as a hint for the generation.\nWe also show the effectiveness of classiﬁer-free guid-\nance in Table 2. With a guidance scale of 5, the model\n(Polyffusion-S5) shows improved controllability on both\nchord conditioning and texture conditioning. Notably,\na large guidance scale for chord conditions negatively\nimpacts the DDmetric. We speculate that this is be-\ncause notes regular in length provide clearer chord context,\nwhich can be noticed in the guidance demos.\nFor subjective evaluation, we invite participants to rate\nthe generation quality via a double-blind online survey.\nOur survey consists of 4 groups of samples of uncondi-\ntional generation and accompaniment generation, respec-\ntively. Each group contains a ground-truth piece, gener-\nated samples by Polyffusion and all baselines with random\norders. 36 participants completed our survey. Each par-\nticipant rated 4 random groups on average based on a 5-\npoint scale. The evaluation results are shown in Figure 4\nand 5. The height of each bar represents the mean rating,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n235(1) Uncond. Gen. (2) Acc. Gen. (3) Seg. Inp. (4) Chord Cond. (5) Texture Cond.\nObjective Metrics DP,DD DP,DD DP,DDDP,DD, CD DP,DD, OD\nSubjective Metrics C, N, M C, N, M, F N/A N/A N/A\nGenerative Length 8 bars 8 bars 4 bars 8 bars 8 bars\nTransformer Baselines Wang Wang Chang Wang Wang\nSampling Baselines Wang N/A Wang Wang Wang\nTable 1 : Speciﬁcations of the evaluation tasks and the baseline models. C, N, M, F in subjective metrics mean creativity, naturalness,\nmusicality, and ﬁtness respectively. Wang refers to the Transformer models (for Transformer baselines) and V AE-based models (for\nsampling baselines) in [13]; Chang refers to the XLNet-based model in [20].\nUncond. Gen. Acc. Gen. Seg. Inp. Chord Cond. Texture Cond.\nDP↑ DD↑DP↑ DD↑DP↑ DD↑DP↑ DD↑CD↓DP↑ DD↑OD↓\nPolyffusion 0.89 0.93 0.89 0.96 0.90 0.93 0.90 0.96 0.75 0.88 0.98 1.85\nPolyffusion-S5 0.89 0.93 0.89 0.96 0.90 0.93 0.92 0.81 0.51 0.87 0.97 1.75\nPolyffusion-A 0.89 0.93 0.89 0.96 0.90 0.93 0.90 0.94 0.79 0.95 0.98 4.37\nTransformer 0.78 0.84 0.88 0.89 0.90 0.83 0.87 0.88 0.56 0.84 0.93 0.13\nSampling 0.86 0.90 N/A N/A 0.89 0.91 0.86 0.90 0.70 0.91 0.93 0.20\nTable 2 : The objective evaluation and ablation study results. The statistics of generation, accompaniment generation and segment\ninpainting are identical for three Polyffusion models (hence gray-out for the latter two models) since they share the same internal control\nmethod.\n/uni00000026/uni00000055/uni00000048/uni00000044/uni00000057/uni0000004c/uni00000059/uni0000004c/uni00000057/uni0000005c /uni00000031/uni00000044/uni00000057/uni00000058/uni00000055/uni00000044/uni0000004f/uni00000051/uni00000048/uni00000056/uni00000056 /uni00000030/uni00000058/uni00000056/uni0000004c/uni00000046/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000014/uni00000011/uni00000013/uni00000015/uni00000011/uni00000013/uni00000016/uni00000011/uni00000013/uni00000017/uni00000011/uni00000013/uni0000002a/uni00000037\n/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000049/uni00000011/uni00000037/uni00000055/uni00000049/uni00000011\n/uni00000036/uni00000044/uni00000050/uni00000053/uni00000011\nFigure 4 : Subjective evaluation for unconditional generation.\nand the error bars are MSEs computed by within-subject\nANOV A [42]. We report a signiﬁcantly better performance\n(p-value<0.05) of Polyffusion than baseline models in\nnaturalness and musicality for both tasks and in ﬁtness\nfor accompaniment generation. Interestingly, Polyffusion\neven outperforms the ground truth on the creativity metric.\n5.4 Ablation Study\nWe perform an ablation test on the use of V AE encoders\nfor condition signals. For both chord conditioning and\ntexture conditioning, we remove the corresponding pre-\ntrained encoders. The ablated model of chord condition-\ning uses concatenated 36-D chord vectors as the condition\nsignals. The ablated model of texture conditioning uses a\nmodiﬁed piano roll representation [13]. Both models are\ntrained with the same settings as the proposed model. Ta-\nble 2 shows that the ablated models (Polyffusion-A) per-\nform worse than the proposed models on the controllabil-\nity metrics (CD & OD), showing the advantage of using\ndisentangled latent representations as condition signals for\ndiffusion models.\n/uni00000026/uni00000055/uni00000048/uni00000044/uni00000057/uni0000004c/uni00000059/uni0000004c/uni00000057/uni0000005c /uni00000031/uni00000044/uni00000057/uni00000058/uni00000055/uni00000044/uni0000004f/uni00000051/uni00000048/uni00000056/uni00000056 /uni00000030/uni00000058/uni00000056/uni0000004c/uni00000046/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005c /uni00000029/uni0000004c/uni00000057/uni00000051/uni00000048/uni00000056/uni00000056/uni00000014/uni00000011/uni00000013/uni00000015/uni00000011/uni00000013/uni00000016/uni00000011/uni00000013/uni00000017/uni00000011/uni00000013/uni0000002a/uni00000037\n/uni00000033/uni00000052/uni0000004f/uni0000005c/uni00000049/uni00000011/uni00000037/uni00000055/uni00000049/uni00000011Figure 5 : Subjective evaluation for accompaniment generation.\n6. CONCLUSION AND FUTURE WORK\nIn this paper, we propose a diffusion model for polyphonic\nsymbolic music generation. We show that an image-like\npiano roll representation is effective for modeling the mu-\nsical context for a high-quality score generation. We spec-\nify two methods for controllable generation: internal con-\ntrol via masked generation, and external control via condi-\ntioning using cross-attention. Experiments show that our\nmethod achieves higher quality and controllability com-\npared to the Transformer and sampling-based baselines on\nboth internal and external control tasks.\nWe regard the diffusion framework as a prospective di-\nrection for future work on controllable music generation,\nsince it achieves ﬁne-grained controls over high-quality\ngeneration and enables a wide spectrum of arrangement\napplications. Currently, our generation is limited to quan-\ntized music scores without performance features. We\nplan to extend this methodology to expressive performance\nmodeling. Several new controls can also be introduced to\nfacilitate human-AI co-creation of symbolic music, e.g.,\nhierarchical structure controls (e.g., music segment labels)\nand multimodal controls (e.g., text descriptions).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2367. REFERENCES\n[1] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan,\nand S. Ganguli, “Deep unsupervised learning us-\ning nonequilibrium thermodynamics,” in International\nConference on Machine Learning . PMLR, 2015, pp.\n2256–2265.\n[2] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion\nprobabilistic models,” Advances in Neural Information\nProcessing Systems , vol. 33, pp. 6840–6851, 2020.\n[3] P. Dhariwal and A. Nichol, “Diffusion models beat\ngans on image synthesis,” Advances in Neural Infor-\nmation Processing Systems , vol. 34, pp. 8780–8794,\n2021.\n[4] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi,\nand T. Salimans, “Cascaded diffusion models for\nhigh ﬁdelity image generation.” J. Mach. Learn. Res. ,\nvol. 23, pp. 47–1, 2022.\n[5] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catan-\nzaro, “Diffwave: A versatile diffusion model for audio\nsynthesis,” arXiv preprint arXiv:2009.09761 , 2020.\n[6] N. Chen, Y . Zhang, H. Zen, R. J. Weiss, M. Norouzi,\nand W. Chan, “Wavegrad: Estimating gradi-\nents for waveform generation,” arXiv preprint\narXiv:2009.00713 , 2020.\n[7] R. Yang, P. Srivastava, and S. Mandt, “Diffusion proba-\nbilistic modeling for video generation,” arXiv preprint\narXiv:2203.09481 , 2022.\n[8] J. Ho, T. Salimans, A. Gritsenko, W. Chan,\nM. Norouzi, and D. J. Fleet, “Video diffusion models,”\narXiv preprint arXiv:2204.03458 , 2022.\n[9] W. Harvey, S. Naderiparizi, V . Masrani, C. Weilbach,\nand F. Wood, “Flexible diffusion modeling of long\nvideos,” arXiv preprint arXiv:2205.11495 , 2022.\n[10] X. L. Li, J. Thickstun, I. Gulrajani, P. Liang, and T. B.\nHashimoto, “Diffusion-lm improves controllable text\ngeneration,” arXiv preprint arXiv:2205.14217 , 2022.\n[11] S. Gong, M. Li, J. Feng, Z. Wu, and L. Kong, “Dif-\nfuseq: Sequence to sequence text generation with\ndiffusion models,” arXiv preprint arXiv:2210.08933 ,\n2022.\n[12] F. Pachet and P. Roy, “Musical harmonization with\nconstraints: A survey,” Constraints , vol. 6, no. 1, pp.\n7–19, 2001.\n[13] Z. Wang, D. Wang, Y . Zhang, and G. Xia, “Learning in-\nterpretable representation for controllable polyphonic\nmusic generation,” arXiv preprint arXiv:2008.07122 ,\n2020.\n[14] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,\nand I. Sutskever, “Jukebox: A generative model for\nmusic,” arXiv preprint arXiv:2005.00341 , 2020.[15] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel,\nM. Verzetti, A. Caillon, Q. Huang, A. Jansen,\nA. Roberts, M. Tagliasacchi et al. , “Musiclm:\nGenerating music from text,” arXiv preprint\narXiv:2301.11325 , 2023.\n[16] J.-P. Briot and F. Pachet, “Deep learning for music gen-\neration: challenges and directions,” Neural Computing\nand Applications , vol. 32, no. 4, pp. 981–993, 2020.\n[17] R. Rombach, A. Blattmann, D. Lorenz, P. Esser,\nand B. Ommer, “High-resolution image synthesis\nwith latent diffusion models,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , 2022, pp. 10 684–10 695.\n[18] G. Hadjeres, F. Pachet, and F. Nielsen, “Deepbach: a\nsteerable model for bach chorales generation,” in Inter-\nnational Conference on Machine Learning . PMLR,\n2017, pp. 1362–1371.\n[19] C.-Z. A. Huang, T. Cooijmans, A. Roberts,\nA. Courville, and D. Eck, “Counterpoint by con-\nvolution,” arXiv preprint arXiv:1903.07227 , 2019.\n[20] C.-J. Chang, C.-Y . Lee, and Y .-H. Yang, “Variable-\nlength music score inﬁlling via xlnet and musi-\ncally specialized positional encoding,” arXiv preprint\narXiv:2108.05064 , 2021.\n[21] K. Chen, C.-i. Wang, T. Berg-Kirkpatrick, and S. Dub-\nnov, “Music sketchnet: Controllable music generation\nvia factorized representations of pitch and rhythm,”\narXiv preprint arXiv:2008.01291 , 2020.\n[22] R. Guo, I. Simpson, C. Kiefer, T. Magnusson, and\nD. Herremans, “Musiac: An extensible generative\nframework for music inﬁlling applications with multi-\nlevel control,” in International Conference on Compu-\ntational Intelligence in Music, Sound, Art and Design\n(Part of EvoStar) . Springer, 2022, pp. 341–356.\n[23] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Tim-\nofte, and L. Van Gool, “Repaint: Inpainting using de-\nnoising diffusion probabilistic models,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2022, pp. 11 461–11 471.\n[24] I. Simon, D. Morris, and S. Basu, “Mysong: automatic\naccompaniment generation for vocal melodies,” in Pro-\nceedings of the SIGCHI conference on human factors\nin computing systems , 2008, pp. 725–734.\n[25] Y .-S. Huang and Y .-H. Yang, “Pop music transformer:\nGenerating music with rhythm and harmony,” arXiv\npreprint arXiv:2002.00212 , 2020.\n[26] C. Donahue, H. H. Mao, Y . E. Li, G. W. Cot-\ntrell, and J. McAuley, “Lakhnes: Improving multi-\ninstrumental music generation with cross-domain pre-\ntraining,” arXiv preprint arXiv:1907.04868 , 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n237[27] Z. Ju, P. Lu, X. Tan, R. Wang, C. Zhang, S. Wu,\nK. Zhang, X. Li, T. Qin, and T.-Y . Liu, “Telemelody:\nLyric-to-melody generation with a template-based\ntwo-stage method,” arXiv preprint arXiv:2109.09617 ,\n2021.\n[28] J. Zhao and G. Xia, “Accomontage: Accompaniment\narrangement via phrase selection and style transfer,”\narXiv preprint arXiv:2108.11213 , 2021.\n[29] S. Dai, Z. Zhang, and G. G. Xia, “Music style transfer:\nA position paper,” arXiv preprint arXiv:1803.06841 ,\n2018.\n[30] G. Mittal, J. Engel, C. Hawthorne, and I. Simon, “Sym-\nbolic music generation with diffusion models,” arXiv\npreprint arXiv:2103.16091 , 2021.\n[31] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and\nD. Eck, “A hierarchical latent vector model for learning\nlong-term structure in music,” in International confer-\nence on machine learning . PMLR, 2018, pp. 4364–\n4373.\n[32] K. W. Cheuk, R. Sawata, T. Uesaka, N. Murata,\nN. Takahashi, S. Takahashi, D. Herremans, and Y . Mit-\nsufuji, “Diffroll: Diffusion-based generative music\ntranscription with unsupervised pretraining capability,”\narXiv preprint arXiv:2210.05148 , 2022.\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in neural infor-\nmation processing systems , vol. 30, 2017.\n[34] J. Ho and T. Salimans, “Classiﬁer-free diffusion guid-\nance,” arXiv preprint arXiv:2207.12598 , 2022.\n[35] S. Dieleman, “Guidance: a cheat code for diffusion\nmodels,” 2022. [Online]. Available: https://benanne.\ngithub.io/2022/05/26/guidance.html\n[36] B. Pardo and W. P. Birmingham, “Algorithms for\nchordal analysis,” Computer Music Journal , vol. 26,\nno. 2, pp. 27–49, 2002.\n[37] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, D. P. Ellis, and C. C. Raffel,\n“Mir_eval: A transparent implementation of common\nmir metrics.” in Proceedings of the 15th International\nConference on Music Information Retrieval , 2014.\n[38] Z. Wang, K. Chen, J. Jiang, Y . Zhang, M. Xu, S. Dai,\nX. Gu, and G. Xia, “Pop909: A pop-song dataset\nfor music arrangement generation,” arXiv preprint\narXiv:2008.07142 , 2020.\n[39] N. W. Varuna Jayasiri, “labml.ai annotated paper\nimplementations,” 2020. [Online]. Available: https:\n//nn.labml.ai/\n[40] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” arXiv preprint arXiv:1412.6980 ,\n2014.[41] Y . Ren, J. He, X. Tan, T. Qin, Z. Zhao, and T.-Y . Liu,\n“Popmag: Pop music accompaniment generation,” in\nProceedings of the 28th ACM international conference\non multimedia , 2020, pp. 1198–1206.\n[42] H. Scheffe, The analysis of variance . John Wiley &\nSons, 1999, vol. 72.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n238"
    },
    {
        "title": "Data Collection in Music Generation Training Sets: A Critical Analysis.",
        "author": [
            "Fabio Morreale",
            "Megha Sharma",
            "I-Chieh Wei"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265217",
        "url": "https://doi.org/10.5281/zenodo.10265217",
        "ee": "https://zenodo.org/records/10265217/files/000003.pdf",
        "abstract": "The practices of data collection in training sets for Automatic Music Generation (AMG) tasks are opaque and overlooked. In this paper, we aimed to identify these practices and surface the values they embed. We systematically identified all datasets used to train AMG models presented at the last ten editions of ISMIR. For each dataset, we checked how it was populated and the extent to which musicians wittingly contributed to its creation.\\ Almost half of the datasets (42.6%) were indiscriminately populated by accumulating music data available online without seeking any sort of permission. We discuss the ideologies that underlie this practice and propose a number of suggestions AMG dataset creators might follow. Overall, this paper contributes to the emerging  self-critical corpus of work of the ISMIR community, reflecting on the ethical considerations and the social responsibility of our work.",
        "zenodo_id": 10265217,
        "dblp_key": "conf/ismir/MorrealeSW23",
        "keywords": [
            "data collection",
            "Automatic Music Generation",
            "training sets",
            "opaque",
            "overlooked",
            "values embedded",
            "datasets",
            "populated",
            "musicians",
            "contributed"
        ],
        "content": "DATA COLLECTION IN MUSIC GENERATION TRAINING SETS:\nA CRITICAL ANALYSIS\nFabio Morreale\nUniversity of Auckland\nf.morreale@auckland.ac.nzMegha Sharma\nUniversity of Tokyo\nmeghas@g.ecc.u-tokyo.ac.jpI-Chieh Wei\nUniversity of Auckland\niwei022@aucklanduni.ac.nz\nABSTRACT\nThe practices of data collection in training sets for Au-\ntomatic Music Generation (AMG) tasks are opaque and\noverlooked. In this paper, we aimed to identify these prac-\ntices and surface the values they embed. We systemati-\ncally identiﬁed all datasets used to train AMG models pre-\nsented at the last ten editions of ISMIR. For each dataset,\nwe checked how it was populated and the extent to which\nmusicians wittingly contributed to its creation. Almost half\nof the datasets (42.6%) were indiscriminately populated by\naccumulating music data available online without seeking\nany sort of permission. We discuss the ideologies that un-\nderlie this practice and propose a number of suggestions\nAMG dataset creators might follow. Overall, this paper\ncontributes to the emerging self-critical corpus of work of\nthe ISMIR community, reﬂecting on the ethical considera-\ntions and the social responsibility of our work.\n1. INTRODUCTION\nThe quest to generate music with AI (Automatic Music\nGeneration, AMG) is undergoing crucial but overlooked\nontological, artistic, and political transformations. Orig-\ninally conﬁned to academic labs and employed in niche\nmusic genres, this quest is gaining traction mostly among\ncommercial companies1aiming at automatically gener-\nating music in all genres. These transformations are en-\nabled by a combination of socio-technical novelties, in-\ncluding i) the growing inﬂux of money in the ﬁeld [2, 3];\nii) advanced in Deep-Learning (DL) techniques, such as\nTransformers [4]; and iii) the increase of cheap computa-\ntional power. While, from a purely musical perspective,\nthe quality of the music created with AI is undoubtedly\nrising, from a socio-political perspective, a new gold rush\nresulting from efforts to outperform competitors and make\nthe best AMG model is following the typical blueprint of\ncapitalist innovation [5–7]: corners are being cut; critical\n1A list from Water & Music [1] includes, as of July 2023, companies\nlike Microsoft, Facebook, Google, Spotify, Deezer, and ByteDance.\n© F. Morreale, M. Sharma, and I. Wei. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: F. Morreale, M. Sharma, and I. Wei, “Data Collection in\nMusic Generation Training Sets: A Critical Analysis”, in Proc. of the\n24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.questions have not been asked; short-term gains are priori-\ntised; permission is not being sought.\nThese ethically questionable practices are causing in-\ncreased concerns. A group of artists2recently released\na manifesto that identiﬁes one of the most urgent ethical\nissues arising from AI-generated art: the exploitation of\nartists’ work in training AI generation systems. Similarly,\nHolly Herndon, a musician famous for popularising AI-\ngenerated music, recently criticised OpenAI for not asking\nliving performers’ permission to use their music in their AI\nmodel, JukeBox [8]. Most systems that generate artistic\ncontent using Machine Learning (ML) indeed often indis-\ncriminately populate their training datasets by accumulat-\ning original material that is available online [9–12].\nWithin the ISMIR community, occasional ﬁery calls re-\nquested the community to reﬂect on the ethical implica-\ntions [13–15] of, and demanded accountability [2] for the\nwork we produce. However, no speciﬁc work investigated\nthe potentially exploitative nature of the datasets we use,\nand no ethical consideration has been given to how data\nhas been generated. We argue that such investigation is\nlong overdue, especially as the publication of AMG mod-\nels proceeds undisturbed - and actually, as we will show in\nthe paper, is steadily increasing.\nTo ﬁll this gap, we aimed to assess the extent to which\ntraining sets used in ISMIR papers that propose new AMG\nmodels are affected by this issue. We ﬁrst identiﬁed all\npapers presented at the last ten editions of the conference,\nfrom 2013 to 2022, that introduced a new music genera-\ntion model or a pertinent dataset. Then we identiﬁed all\ndataset(s) that have been used in these papers. Finally, we\nsurveyed information for each dataset, including how data\nwas populated and the extent to which musicians wittingly\ncontributed to its creation.\nThe contribution of this paper is threefold. First, we\nprovide descriptive statistics about the datasets that are\nmostly used at ISMIR in AMG applications and how they\nare populated. Second, we report the ideologies that are\nembedded in them and outline a lack of adequate engage-\nment with musicians and carelessness on ethical matters.\nThird, we offer suggestions for dataset creators interested\nin following responsible practices in their work.\nThe rest of the paper is structured as follows. We ﬁrst\nreview literature in Critical Data set Studies and report dis-\ncussions on ethical issues within MIR. We then describe\n2The European Guild for Artiﬁcial Intelligence Regulation, https:\n//www.egair.eu/37the research process, report the results, identify the values\nthat are inscribed in the datasets, and offer suggestions for\ndataset creators. We conclude the paper with a summary\nof the study and directions for future work.\n2. BACKGROUND\nDeep-learning (DL), which nowadays is the most com-\nmonly adopted method to generate music automatically\n[16–19], signiﬁcantly relies on the quality and volume of\nvast training data. Despite this reliance, dataset develop-\nment remains an underappreciated element in DL practice.\n2.1 Critical Data Set Studies\nA growing literature on critical data set studies [20] aims\nat identifying the ethical issues and hegemonic power\nstructures of datasets, in particular when used to train ML\nmodels [10,12,21]. One of the most urgent issues concerns\nthe exploitation of user labour in AI systems: datasets are\npopulated with data generated by “unwitting labourers” [9]\nand scraped from the Internet “without context and without\nconsent” [11]. The question around consent is particularly\nconvoluted: consent may have been given unwittingly, for\na speciﬁc use only, and “some people may never have been\ngiven the chance to offer their consent at all” [20]. This\nconcern is not limited to the ivory tower of academia. Mu-\nsicians are gaining awareness of this issue, and an increas-\ning number of complaints arise from the unfair or uncon-\nsented use of original material in AI-generated art.3\nMost critical work on dataset creation addressed Com-\nputer Vision (CV) sets [11, 24, 25] like ImageNet and MS-\nCeleb, which contain tens of millions of digital images up-\nloaded by platform users. [25] identiﬁed the values em-\nbedded into these datasets and their formation: evaluating\nmodel work is prioritised to the detriment of careful data\nwork . Another case study that received attention is that of\nreCAPTCHA [26–28]. Disguised as a human authentica-\ntion tool , reCAPTCHA can be seen as a capture-machine\nthat exploits unpaid individuals’ perceptual abilities and\nmicro-labour to train AI datasets [27, 29].\nThe very way in which most datasets are created\nembeds speciﬁc neoliberal values, like extractivism and\nderegulation, as exempliﬁed by OpenAI’s argument that\n“IP should be free to use for AI, with training constituting\nfair use“ [30, p. 54]. The all-you-can-scrap ideology dis-\nmisses individuals’ contributions to dataset creation, which\ncan be met by their creators with a laissez-faire attitude\n[31] that overlooks the ethical implication and liability of\nscraping the whole Internet [21,25]. In fact, when concerns\nare voiced, they are speciﬁcally aligned with libertarian\nvalues and related to how data privacy and data ownership\nare barriers to collecting data [25].\nThe practices and routines of data accumulation are not\nsecret. The opposite is true. Among dataset creators, they\nhave become widely accepted, unquestioned, and unchal-\nlenged following a process of dataset naturalisation : “the\n3Notable cases include GettyImages suing Stable Diffusion’s creators\n[22] and audiobook narrators complaining against Apple for using their\nvoices to train AI [23].contingencies of dataset creation are eroded in a manner\nthat ultimately renders the constitutive elements of their\nformation invisible” [24]. Notably, the values and ideolo-\ngies are not only inscribed in how technology is used but\nalso in how it is taught. The lack of interest in how datasets\nare constructed can indeed be found in the lack of guidance\nin typical ML textbooks or syllabi [24, 32].\nWhile many dataset creators do not consciously attempt\nto hide their data accumulation practices, they do not try\nto fully disclose them either. Dataset naturalisation is in-\ndeed exacerbated by ill documentary practices: as reported\nby [33], ML communities pay little attention to document-\ning data creation and use. [24] proposes that the lack of in-\nformation on dataset creation (e.g. how datasets have been\ncreated, and whether and how much annotators have been\npaid) is structural - thus ideological - rather than acciden-\ntal. Every decision and every step in dataset development\nthat is left unaccounted and unarticulated from documen-\ntary practices has a political meaning as these steps and\ndecisions are related as \"not important\" [25]. We will re-\nturn to this point in the discussions.\n2.2 Critical turn in MIR and AIM\nSeveral technology communities are undergoing a critical\nturn [34–37] that challenges existing knowledge produc-\ntion methods and political positions as well as ethical and\npolitical thoughts within a ﬁeld. This turn is ethico-onto-\nepistemological [38, 39] insofar as it questions what kinds\nof work, knowledge, and social commitment is pursued\nwithin and by the community.\nWhile most criticisms of MIR research come from out-\nside the community [3, 40–42], recent academic produc-\ntion within MIR [2, 3, 14, 43–45] and the development of\na workshop series on Human-Centric MIR [46] testify that\nwe might be close to a Critical MIR - i.e. MIR scholarship\ndevoted to critically analysing the work produced in the\nﬁeld. However, the sort of work that is (not) published at\nISMIR (less than 0.5% of the ISMIR submissions engage\nwith any sort of ethical discussions [2]) indicates that the\nresponse of the ﬁeld on ethical issues is still inadequate.\nWith respect to AMG, the ethical issues that have been\nidentiﬁed include copyright issues [15, 47], a narrow and\nWestern-centered understanding to music [43, 45], the risk\nofmusician redundancy [2, 14] or the crisis of prolifera-\ntion [44, 48], diversity issues [49], colonialist and extrac-\ntive practices [2], and assumptions and bias that are embed-\nded in the AI systems [13–15]. To the best of our knowl-\nedge, the potentially exploitative nature of AMG datasets\nremains uncharted territory.\n3. METHODOLOGY\n3.1 Researcher Positionality and Motivation\nPositionality statements are common in critical studies and\nserve as a foundation for critical work to understand the\nresearch context and the authors’ interpretation of the re-\nsults. Since the outset of the research process, we haveProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n38strived to maintain objectivity and reﬂexivity by acknowl-\nedging our unique positions and backgrounds. All three\nauthors are actively involved in MIR. The ﬁrst author is\nformally trained in computer science and is expert in criti-\ncal theory and technology studies; the second author has a\nbackground in computer science; and the third author has\na background in electronic engineering and is specialised\nin machine learning algorithms.\nThe motivation for undertaking this study is twofold.\nFirst, we aimed to support the growth of ISMIR com-\nmunity by contributing to the corpus of self-reﬂective\nwork, identifying ideologies that might be latent but, once\nsurfaced, can be considered problematic by community\nmembers. Second, the development of the suggestions for\ndataset creation derived from the personal experience of\none of the authors, who was involved in dataset creation\nfor AMGs and acknowledged the importance of commu-\nnity guidance on the best ethical practices to adhere to.\n3.2 Analysis of ISMIR publications\nWe conducted a systematic review of the last ten editions\nof ISMIR (2013-2022). A total of 1078 publications were\nsourced from the conference proceedings. Two of the au-\nthors manually ﬁltered the papers adopting two inclusion\ncriteria. First, we included all papers presenting a new mu-\nsic generation model. We included all models that generate\nnew compositions or performances, including in-painting,\nstyle transfer, and improvisation. Second, we included all\npapers that introduced a new dataset that could potentially\nbe utilised as training material for AMG models but re-\njected works that did not contain symbolic or raw audio\nmusic ﬁles. For example, we did not include the NSynth\nDataset [50], which contains sampled notes from different\ninstruments, but we included MedleyDB [51], which con-\ntains annotated multitrack audio.\nThe analysis proceeded in two phases. First, for each\npaper, we identiﬁed whether authors employed existing\ndatasets (i.e. datasets released or introduced before the\npublication of the ISMIR paper) or created new ones (i.e.\ndatasets created or introduced as part of the original re-\nsearch reported in the paper). We also examined the pres-\nence of any discussions of ethics and permission for using\ndata entries training data for AMG models.\nIn the second phase, we examined the datasets identi-\nﬁed in the ﬁrst phase. For papers that used an existing\ndataset, we retrieved dataset information from the origi-\nnal paper (whether or not it was published at ISMIR) in\nwhich it was introduced. When we could not ﬁnd sufﬁ-\ncient information in the paper, we checked dataset release\nlinks, which were found either in the original paper or by\na web search of the dataset name. The information we col-\nlected included i) data format (symbolic or audio), ii) how\ndatasets were populated; iii) whether data contained orig-\ninal performances, compositions, or arrangements; iv) the\ndata type; v) the extent to which musicians were involved\nin the dataset creation and whether they were aware of the\nintended purposes for the dataset; and vi) whether ethical\nconcerns were discussed.\nFigure 1 : Distribution of selected papers and datasets over\nthe years (only papers after 2003 were considered).\nDataset Name Format Occurrence\nPOP909 Symbolic 11\nNottingham Symbolic 9\nLakh MIDI Symbolic 5\nHTPD3 Symbolic 3\nYamaha e-Competition Symbolic 3\nLakh Pianoroll Symbolic 3\nMusicNet Both 3\nURMP Both 3\nAILABS17k Both 3\nBach Music21 Symbolic 3\nBach Chorales Symbolic 3\nRWC Both 3\nTable 1 : The most popular datasets and their occurrences.\nEach dataset comprises data in symbolic form, but three of\nthem also include audio ﬁles.\nFrom a methodological point of view, most of these in-\nvestigations involved checking the aspect under scrutiny\n(e.g. whether ethics was discussed) from the dataset\nsources. The task of identifying how datasets were pop-\nulated was not as straightforward. In order to streamline\nthe analysis and facilitate the report of the ﬁndings, we\naimed to cluster datasets into categories that reﬂected dif-\nferent ways of populating datasets. Two of the authors per-\nformed this categorisation following a deductive approach.\nAs they analysed more datasets, they introduced new cat-\negories and deleted or merged existing ones. The analy-\nsis spreadsheet is available at https://github.com/\nSma1033/amgdatasetethics .\n4. FINDINGS\nA total of 121 papers survived the ﬁltering. Fig. 1 shows\nthe signiﬁcant rise of interest in AMG in recent years.\nThree fourth of the articles (82) introduced a new model,\nwhich was either introduced on its own or with a new\ndataset (Fig. 2a). From this list of papers, we identiﬁed\n115 datasets (Fig. 2b). When only considering the 82 pa-\npers that introduced a new model, most (62 papers, 75.6%)\nISMIR researchers use, at least in part, existing datasets to\ntrain their AMG models. Tab. 1 shows the 12 most fre-\nquently used datasets in our survey, along with data for-\nmat and their occurrence in our survey. The remaining 104\ndatasets were only used in one or two papers.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n39(a)\n (b)\nFigure 2 :(a) What is introduced: new model (73 pa-\npers), new dataset (39), both (9). (b) Dataset Originality:\nnew datasets (58), existing datasets (53), both (12).\n4.1 Dataset Creation\nWe clustered datasets into nine categories to reﬂect the dif-\nferent ways in which entries were collected (Tab. 2). The\ncategories are non-orthogonal: datasets could be associ-\nated with more than one category. In most cases, datasets\nwere populated without creators incurring any costs. Only\nﬁve datasets used paid online sources or compensated the\ninvolved musicians. With the exception of those belong-\ning to the ‘Involved musicians’ and ‘Synthesised music’\ncategories, all datasets were populated with existing music\ndata. New music data accounted for 16.5% of all datasets.\nWe found evidence of poor documentary practices for 18\ndatasets (15.7%): in 10 cases, there was no information on\nhow data was collected; in 8 cases, we could not ﬁnd any\ndocumentation reporting how datasets were created.\n4.2 Musicians’ involvement\nOnly 17 datasets (14.8%) involved musicians in any ca-\npacity (category ‘Involved musicians’). In 11 cases, musi-\ncians performed or arranged existing compositions. Three\nof these datasets (ASF-4, HP-10, and AIST) were en-\ntirely created from novel compositions. The remaining\nthree datasets from this category did not contain compo-\nsitions or performances created speciﬁcally for the dataset,\nor at least, it was not explicitly mentioned. The Irish Tra-\nditional Dance Music dataset [61] used the recordings of\none of the authors’ own performances. For the remaining\ntwo datasets [51, 62], the creators mentioned that profes-\nsional musicians had created those recordings. However,\nit is unclear whether these recordings are speciﬁcally cre-\nated for the dataset. The other two datasets that included\nnew music belonged to the ‘Synthesised music’ category\nand algorithmically generated monophonic melodies [59]\nor polyphonic MIDI sequences [63].\n4.3 Musicians’ permission and awareness\nWe checked whether explicit permission was sought from\nmusicians to use their creations to train an AMG model.\nOnly three datasets creators reported having asked such\npermission. The authors of ASF-4 and HP-10 datasets\n[64] explicitly mentioned that the musicians involved were\nmade aware of the purpose of the dataset for AMG. Jazz\nplayers participating in the creation of the FILOSAXdataset [54] signed a document that provided explanations\nabout the goals of the dataset. However, it is not clear\nwhether these goals included AMG: whereas the authors\nmention \"music generation\" in the Abstract, AMG was not\nincluded in the list of potential applications of the dataset.\nIn the Mozart Piano Music Dataset [65], pianists gave per-\nmission to use their performances for the intended use of\nthe dataset (music analysis), but they were probably not\naware and did not consent to have their performances used\nto train the AMG model introduced in [66].\nTwo cases were particularly problematic. The MAST\nDataset [67], which was introduced for automatic rhythm\nassessment, was sourced from student entrance exams\nwithout seeking consent from the students. Another pop-\nular dataset, the Yamaha e-Competition dataset4features\nMIDI ﬁles of piano performances obtained from the en-\ntries of the piano competition. Although Yamaha claims\nownership over all data generated during the event, com-\npetitors are unlikely aware their performances are used to\ntrain AMG models, as seen in [68]. The lack of permission\nsought from the musicians clashes with the several com-\nments offered by dataset creators that often acknowledged\nthe valuable contributions made by these musicians, which\nallows the dataset to existing in the ﬁrst place.\n4.4 Discussions on Ethical Issues\nOur analysis revealed a lack of engagement with ethical\nissues, corroborating ﬁndings from [25] in their analysis\nof CV datasets. Only four datasets included any ethical\nconsiderations, and only two of them contained an explicit\nethics statement. The authors of [69], which presented a\nnew GuitarPro dataset, listed several questions, some of\nwhich are particularly relevant to this paper: “ How to ac-\nknowledge, reward and remunerate artists whose music\nhas been used to train models? ” and “ What if an artist\ndoes not want to be part of a dataset? ”. While their spon-\ntaneous engagement with these issues is commendable, it\nis not clear to which extent the authors used these questions\nin the development of their dataset.\nIn [70], the authors raised concerns about the impact\nof AMG for “human musicians of the future”. They also\nstated “care have ( sic) to be given regarding the fair use of\nexisting musical material for model training” but did not\nfurther explain what sort of care and what constitutes un-\nfair use. [57] included an analysis concerning plagiarism\nissues and observed that their model demonstrated a poten-\ntial tendency for plagiarism. This issue was also recently\nhighlighted in [47], similar to the level exhibited by a hu-\nman musician.\n5. DISCUSSIONS\nBy leveraging our ﬁndings, this section ﬁrst reports and\ndiscusses the values embedded in the datasets used at IS-\nMIR for AMG models. Then, we move to offer practical\nsuggestions to AMG dataset creators.\n4https://www.piano-e-competition.com/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n40Category Description Occurrence\nScraped online Existing music data collected online from websites [52] or databases [53] 49\nExisting datasets Existing music data collected from existing datasets [16] 26\nInvolved musicians New music data was created by involving musicians in some capacity [54] 17\nPrivate data Existing music data was collected from private databases [55] 5\nBook collection Existing music data collected from printed books [56] 5\nOnline store Existing music data collected from an online commercial website [57] 4\nCD collection Existing music data collected from published CD recordings [58] 3\nSynthesised music New music synthesised using rule-based heuristics or other methods [59] 2\nNot mentioned No explicit information about how data was obtained [60] 18\nTable 2 : Categorisation of how data was collected in the datasets. For each category, we included exemplary references.\n5.1 The Values Embedded in Our Datasets\nOur analysis extends what [24, 25, 33] have suggested for\nother ML applications areas: data work and data collection\npractices are de-prioritised and de-valued in AMG datasets\nused at ISMIR. Most datasets ( ∼60%) had been populated\neither by scraping songs from the Internet or by accumulat-\ning data from existing ones. Considering original music as\naterra nullius that is free for the taking means addressing\ndataset creation with expediency. This approach follows\nthe hegemonic narrative that compares data tooil. This\ncomparison is highly ideological [71–73] as it disguises the\norigin (and ends) of data [11], and de-penalises and jus-\ntiﬁes extractive practices using neo-colonial rhetoric that\ndata is something waiting to be discovered [71, 73].\nThis narrative underestimates or blatantly neglects the\nhuman labour necessary for its development - which in-\ncludes writing, performing, transcribing, and recording\nmusic. The majority of datasets were created by amassing\nmusical compositions initially intended for purposes other\nthan AMG. In these cases, the original labour that was put\ninto the creative acts of composition or performance is sim-\nply neglected. Relatively few datasets included original\nmaterial, and in only two cases, speciﬁc permission was\nasked to use musicians’ work to train datasets for AMG\npurposes. This discussion point resonates with objections\nto the unfair and exploitative practices of capturing individ-\nuals’ labour and humanness [9] when creating data for dig-\nital platforms [6, 74–76] and training AI systems [10–12].\nAs proposed by [77], human labour is structurally obfus-\ncated in ML applications to the beneﬁt of proﬁt and inno-\nvation. Similarly, [78] proposes that hiding the labour in\nthis context is crucial to attracting capital investments.\nOur direct knowledge and lived experience of MIR of-\nfers us a vantage point that we can employ in our reﬂexive\ninquiry. We propose that dataset creators might have pri-\noritised safety over criticality and followed common, albeit\nquestionable, procedures simply because these are the pro-\ncedures that are typically employed in AMG research. This\ncomment is not intended to absolve dataset creators from\nthe responsibilities that come with their work. Rather, it\nis an invitation to self-assess one’s alignment with the ex-\nploitative ideologies we surfaced in this section. Yet, we\nunequivocally found a lack of data work - including a lim-\nited interest in creating one’s own data, exploitation of thelabour of unwitting musicians (e.g. in the e-piano compe-\ntition) and students [67] in dataset curation, and poor doc-\numentary practices regarding the source of data [60, 79].\nWe argue that this lack is ideological. What we leave unac-\ncounted for or unspoken in dataset creation and documen-\ntation signs what we consider important or irrelevant [25].\nOur ﬁndings indicate that the rights and demands of\nmusicians are not prioritised by dataset creators and that\nthe degree to which new models and datasets advance or\ncurb a fair model for musicians is largely ignored. This\ncomment resonates with a note from [80], who explained\nthat streaming services overlook “the rights of musicians\nor users because their decisions are made based on wholly\nother problems”. It is thus essential that ISMIR researchers\nand practitioners reﬂect on the problems they drive their\ndecisions on and the agendas they implicitly or explic-\nitly follow. Answering questions like \"what is the agenda\nwe are following and who beneﬁts from it?\" [81] requires\ncommunity discussions that are difﬁcult, uncomfortable,\nand controversial but nevertheless necessary. Avoiding en-\ngaging with these questions is not a political absence but\nrather a political tacit acceptance of the status quo [36, 37]\nas datasets do not exist in a political void [20, 82].\n5.2 Suggestions\nIn this section, we offer suggestions to the broader com-\nmunity and to individual authors interested in creating\nnew datasets or using existing ones to train AMG mod-\nels. We developed these suggestions by integrating re-\nsults from our analysis with ﬁndings from other academic\ncontributions, including ethical CV datasets recommenda-\ntions [25]. These suggestions are not intended to be metic-\nulously followed as a recipe book. Rather, we devised\nthem as probes, navigation tools, or structured conversa-\ntions whose development should continue in a participa-\ntory way with the rest of the community.\nDevelop one’s own dataset . While exploiting musi-\ncians’ labour in AI dataset creation is a questionable prac-\ntice [9], expecting dataset creators to seek and obtain con-\nsent from all humans involved in AMG datasets is unreal-\nistic [30]. Thus, we recommend creating, whenever pos-\nsible, one’s own dataset and hire musicians for as many\ntasks as possible (i.e. composing, performing, arranging\nsongs). A small but important amount of datasets in ourProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n41investigation followed this practice. We acknowledge that\nthis suggestion might lead to equity issues. If it were to\nbe enforced, only big companies and top university labs\nwould have the economic means to develop such datasets.\nHowever, rather than dismissing this issue as unsolvable\nand continuing business as usual, we propose that the com-\nmunity interrogates itself and ﬁnds strategies to tackle it.\nAs an alternative, efforts might be made to develop mod-\nels that are trainable on small or procedurally-generated\ndatasets following recent successful examples like [30,83].\nReceive consent from musicians and remunerate\nthem . Dataset creators should inform musicians about the\nspeciﬁc goals of the dataset. It is possible that musicians\nwould willingly consent to train a dataset for several MIR\ntasks but not for training AMG. When possible, dataset\ncreators should consider paying musicians for their labour\nand disclose the amount [24], as found in [54]. Given the\nequity problem discussed above, when paying musicians\nis not feasible, that should be reported [25], and musicians\nshould be at least acknowledged. When AMG systems are\nintegrated into commercial products, a technical infrastruc-\nture might be implemented to distribute royalties to dataset\ncontributors. This suggestion shares Holly Herndon’s vi-\nsion for a novel IP framework “compensates me for my\nlikeness when (and only when) money is made from it” [8].\nDocument the process of dataset development . Our\nanalysis revealed a general lack of care not only in doing\nbut also in documenting data work. For instance, POP909\ndataset’s creators did not mention the source or selection\nprocess of the “909 popular songs” used to generate pi-\nano arrangements [84] and the Lakh dataset’s creators sim-\nply mentioned that they extracted songs from “publicly-\navailable sources on the internet”5website. Careless doc-\numentary practices, which we believe were mostly invol-\nuntary and caused by an undervaluing of this process in\nthe ﬁeld [24, 25], implicitly reveal that how a dataset is\ndeveloped andwhose labour goes in it is not important.\nWe suggest the community develop protocols, guidelines,\nor templates offering fair practice suggestions for dataset\ncreators to follow.\nReport the intended use of the dataset . Our ﬁnd-\nings indicate that it is a common practice among AMG\ndataset creators to reuse existing datasets. We suggest that\ndataset creators should report the original intended use of\ntheir dataset and list the potential ‘allowed’ applications,\nfollowing the example of [54]. This practice would pre-\nvent, or at least dissuade, future dataset creators from us-\ning that data for purposes other than the ones envisioned by\nthe creators and that musicians agreed on. This suggestion\nis grounded on the observation that technologies are often\ninterpreted, used, and appropriated in ways that their cre-\nators cannot foresee or control (what [85] terms designer’s\nfallacy ). As new applications of datasets are discovered,\nmeasures should be taken to ensure that permission from\ninvolved musicians is obtained to use their work for uses\nother than the ones they agreed on.\nWhen borrowing data, maintain the purpose of the\n5https://colinraffel.com/projects/lmd/original datasets . Connected to the above suggestion, cre-\nators should maintain their original purpose when borrow-\ning entries for new datasets and avoid misappropriation.\nThis is particularly important when dealing with culturally\nrelevant and sensitive music. This is, for instance, the case\nof the dataset on the Australian Aboriginal language used\nby [86]. The author reported: “These datasets were public\ndomain and encouraged for use by the creator as a way to\nshare the sound of the language. Even so, it is not clear\nthat the creators of the dataset from the late nineties could\npredict this (AI generation) ‘future use’ case” [30].\nVolunteer ethical considerations . Our analysis re-\nvealed that almost the entirety of the papers did not engage\nin any form of ethical considerations. Authors can show\ncommitment to advancing more just practices in dataset\ncreation by reﬂecting on potential ethical limitations in\ntheir datasets. Preferably, they should also include docu-\nments approved by an Ethics board, if applicable, that were\ngiven and signed by the participating musicians.\n6. CONCLUSIONS AND FUTURE WORK\nWe identiﬁed the dominant approaches to dataset creation\nwithin ISMIR and analysed them with critical lenses to un-\nderstand their ideological substrate. Most authors seem to\nhandle dataset creation with neoliberal attitudes and ex-\npediency. However, a small - yet signiﬁcant - number of\ndataset creators showed that other attitudes and values are\nat play within ISMIR when creating datasets for AMG.\nOur analysis did not explain the motivations for dataset\ncreators to engage, or not engage, with ethical issues in\ntheir work, and this investigation is left for future work. Fi-\nnally, we aim to extend the analysis to papers other than the\nones published at ISMIR and to conduct an ethnographic\nstudy with AMG dataset creators to give voice to their\nperspectives on the topic. To conclude, ISMIR has been\nplaying a signiﬁcant role in the growth of ML models for\nAMGs but the lack of an ethical infrastructure may facili-\ntate an exploitative industry. It is our responsibility as the\nmain academic hub of AMG to recognise the need to en-\ngage in discussions around the matters raised in the article\nand to establish ISMIR as the home of this debate.\n7. ETHICAL STATEMENT\nIn this study, we only used secondary data (desk research)\nthat is publicly available online. We reﬂect that analysing\nexisting information does not incur ethical issues.\n8. CONFLICTS OF INTEREST\nWe have no relevant ﬁnancial or non-ﬁnancial interests to\ndisclose and no ﬁnancial or proprietary interests in any-\nthing discussed in this paper.\n9. DATA A V AILABILITY\nThe spreadsheet with our analysis is available at https:\n//github.com/Sma1033/amgdatasetethics .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4210. REFERENCES\n[1] D. Edwards and D. McGlynn, “Creative AI for artists:\nTrack 80+ tools,” https://www.waterandmusic.com/\ndata/creative-ai-for-artists/, [Accessed: 12-Apr-2023].\n[2] F. Morreale, “Where does the buck stop? Ethical and\npolitical issues with AI in music creation.” in Transac-\ntions of the International Society for Music Information\nRetrieval , 2021, pp. 105–114.\n[3] E. Drott, “Copyright, compensation, and commons in\nthe music AI industry,” in Creative Industries Journal ,\n2021, pp. 190–207.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” in Advances in neural in-\nformation processing systems , 2017.\n[5] D. Harvey, A brief history of neoliberalism . Oxford\nUniversity Press, USA, 2007.\n[6] S. Zuboff, “The age of surveillance capitalism: The\nﬁght for a human future at the new frontier of power,”\nPublicAffairs , 2018.\n[7] E. Morozov, To save everything, click here: The folly\nof technological solutionism . Public Affairs, 2013.\n[8] M. Clancy, “The Artist: Interview with Holly Hern-\ndon,” in Artiﬁcial Intelligence and Music Ecosystem ,\n2023, pp. 44–51.\n[9] F. Morreale, E. Bahmanteymouri, B. Burmester,\nA. Chen, and M. Thorp, “The unwitting labourer: ex-\ntracting humanness in ai training,” AI & SOCIETY , pp.\n1–11, 2023.\n[10] P. Tubaro, “Learners in the loop: Hidden human skills\nin machine intelligence,” in Sociologia Del Lavoro ,\n2022, pp. 110–129.\n[11] K. Crawford, “The atlas of AI: Power, politics, and the\nplanetary costs of Artiﬁcial Intelligence,” Yale Univer-\nsity Press , 2021.\n[12] N. Dyer-Witheford, A. M. Kjøsen, and J. Steinhoff,\n“Inhuman power: Artiﬁcial Intelligence and the future\nof capitalism,” Pluto Press , 2019.\n[13] A. Holzapfel, B. Sturm, and M. Coeckelbergh, “Ethical\ndimensions of music information retrieval technology,”\ninTransactions of the International Society for Music\nInformation Retrieval , 2018, pp. 44–55.\n[14] G. Born, J. Morris, F. Diaz, and A. Anderson, “Artiﬁ-\ncial Intelligence, music recommendation, and the cura-\ntion of culture,” Schwartz Reisman Institute for Tech-\nnology and Society White Paper , 2021.\n[15] B. L. Sturm, M. Iglesias, O. Ben-Tal, M. Miron, and\nE. Gómez, “Artiﬁcial Intelligence and Music: Open\nquestions of copyright law and engineering praxis,” in\nArts, 2019, p. 115.[16] H.-W. Dong, W.-Y . Hsiao, L.-C. Yang, and Y .-H. Yang,\n“Musegan: Multi-track sequential Generative Adver-\nsarial Networks for symbolic music generation and ac-\ncompaniment,” in Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , 2018.\n[17] W. Chen, J. Keast, J. Moody, C. Moriarty, F. Villalo-\nbos, V . Winter, X. Zhang, X. Lyu, E. Freeman, J. Wang,\nS. Cai, and K. M. Kinnaird, “Data usage in MIR: His-\ntory & future recommendations,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference , 2019, pp. 25–30.\n[18] Y .-S. Huang and Y .-H. Yang, “Pop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions,” in Proceedings of the ACM Inter-\nnational Conference on Multimedia , 2020.\n[19] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel,\nM. Verzetti, A. Caillon, Q. Huang, A. Jansen,\nA. Roberts, M. Tagliasacchi et al. , “MusicLM: Gen-\nerating music from text,” arXiv:2301.11325 , 2023.\n[20] N. B. Thylstrup, “The ethics and politics of data\nsets in the age of machine learning: deleting traces\nand encountering remains,” Media, Culture & Society ,\nvol. 44, no. 4, pp. 655–671, 2022.\n[21] R. Van Noorden, “The ethical questions that haunt\nfacial-recognition research,” Nature , vol. 587, no.\n7834, pp. 354–359, 2020.\n[22] J. Vincent, “Getty images is suing the creators of\nAI art tool Stable Diffusion for scraping its con-\ntent,” https://www.theverge.com/2023/1/17/23558516/\nai-art-copyright-stable-diffusion-getty-images-lawsuit,\n[Accessed: 11-Apr-2023].\n[23] S. Agarwalt, “Audiobook narrators fear apple used\ntheir voices to train AI,” https://www.wired.com/story/\napple-spotify-audiobook-narrators-ai-contract/, [Ac-\ncessed: 11-Apr-2023].\n[24] E. Denton, A. Hanna, R. Amironesei, A. Smart, and\nH. Nicole, “On the genealogy of machine learning\ndatasets: A critical history of imagenet,” in Big Data\n& Society , 2021.\n[25] M. K. Scheuerman, A. Hanna, and E. Denton, “Do\ndatasets have politics? Disciplinary values in com-\nputer vision dataset development,” in Proceedings of\nthe ACM on Human-Computer Interaction , 2021, pp.\n1–37.\n[26] V . Avanesi and J. Teurlings, “I’m not a robot, or am\ni?: Micro-labor and the immanent subsumption of the\nsocial in the human computation of recaptchas,” in In-\nternational Journal of Communication , 2022, p. 19.\n[27] B. T. Pettis, “reCAPTCHA challenges and the produc-\ntion of the ideal web user,” in Convergence , 2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n43[28] R. Mühlhoff, “Human-aided Artiﬁcial Intelligence: Or,\nhow to run large computations in human brains? To-\nward a media sociology of machine learning,” in New\nmedia & Society , 2020, pp. 1868–1884.\n[29] J. O’Malley, “Captcha if you can: How you’ve\nbeen training AI for years without realising it,”\nhttps://www.techradar.com/news/captcha-if-you-\ncan-how-youve-been-training-ai-for-years-without-\nrealising-it, [Accessed: 12-Apr-2023].\n[30] R. Savery and G. Weinberg, “Robotics: Fast and curi-\nous: A CNN for ethical deep learning musical gener-\nation,” in Artiﬁcial Intelligence and Music Ecosystem ,\n2022, pp. 52–67.\n[31] E. S. Jo and T. Gebru, “Lessons from archives: Strate-\ngies for collecting sociocultural data in machine learn-\ning,” in Proceedings of the conference on fairness, ac-\ncountability, and transparency , 2020, pp. 306–316.\n[32] I. Goodfellow, Y . Bengio, and A. Courville, “Deep\nlearning,” MIT press , 2016.\n[33] T. Gebru, J. Morgenstern, B. Vecchione, J. W.\nVaughan, H. Wallach, H. D. III, and K. Crawford,\n“Datasheets for datasets,” Commun. ACM , vol. 64,\nno. 12, p. 86–92, nov 2021. [Online]. Available:\nhttps://doi.org/10.1145/3458723\n[34] N. E. Gold, R. Masu, C. Chevalier, and F. Morreale,\n“Share your values! Community-driven embedding of\nethics in research,” in CHI Conference on Human Fac-\ntors in Computing Systems Extended Abstracts , 2022,\npp. 1–7.\n[35] S. Benford, C. Greenhalgh, B. Anderson, R. Jacobs,\nM. Golembewski, M. Jirotka, B. C. Stahl, J. Timmer-\nmans, G. Giannachi, M. Adams et al. , “The ethical im-\nplications of HCI’s turn to the cultural,” in ACM Trans-\nactions on Computer-Human Interaction , 2015, pp. 1–\n37.\n[36] F. Morreale, A. Bin, A. McPherson, P. Stapleton, and\nM. Wanderley, “A NIME of the times: Developing an\noutward-looking political agenda for this community,”\ninNew Interfaces for Musical Expression , 2020.\n[37] O. Keyes, J. Hoy, and M. Drouhard, “Human-computer\ninsurrection: Notes on an anarchist HCI,” in Proceed-\nings of the CHI conference on human factors in com-\nputing systems , 2019, pp. 1–13.\n[38] K. Barad, “Meeting the universe halfway: Quantum\nphysics and the entanglement of matter and meaning,”\nduke university Press , 2007.\n[39] C. Frauenberger, “Entanglement HCI the next wave?”\ninACM Transactions on Computer-Human Interac-\ntion, 2019, pp. 1–27.\n[40] J. W. Morris, “Curation by code: Infomediaries and the\ndata mining of taste,” in European journal of cultural\nstudies , 2015, pp. 446–463.[41] N. Seaver, “Computing taste: Algorithms and the mak-\ners of music recommendation,” University of Chicago\nPress , 2022.\n[42] J. Sterne and E. Razlogova, “Machine learning in con-\ntext, or learning from LANDR: Artiﬁcial Intelligence\nand the platformization of music mastering,” in Social\nMedia + Society , 2019.\n[43] G. Born, “Diversifying mir: Knowledge and real-\nworld challenges, and new interdisciplinary futures,”\ninTransactions of the International Society for Music\nInformation Retrieval , 2020.\n[44] M. Clancy, “Reﬂections on the ﬁnancial and ethical\nimplications of music generated by Artiﬁcial Intel-\nligence,” Ph.D. dissertation, Trinity College Dublin.\nSchool of Creative Arts. Discipline of Music, 2021.\n[45] R. Huang, B. L. Sturm, and A. Holzapfel, “De-\ncentering the west: East asian philosophies and the\nethics of applying Artiﬁcial Intelligence to music.” in\nProceedings of the International Society for Music In-\nformation Retrieval Conference , 2021, pp. 301–309.\n[46] L. Porcaro, C. Castillo, and E. Gómez Gutiérrez, “Mu-\nsic recommendation diversity: a tentative framework\nand preliminary results,” in Workshop on Designing\nHuman-Centric Music Information Research Systems. ,\n2019.\n[47] Z. Yin, F. Reuben, S. Stepney, and T. Collins, “Deep\nlearning’s shallow gains: a comparative evaluation of\nalgorithms for automatic music generation,” in Ma-\nchine Learning , 2023, pp. 1–38.\n[48] J. Attali, “Noise: The political economy of music,”\nManchester University Press , 1985.\n[49] L. Porcaro, C. Castillo, and E. Gómez Gutiérrez, “Di-\nversity by design in music recommender systems,” in\nTransactions of the International Society for Music In-\nformation Retrieval , 2021.\n[50] J. Engel, C. Resnick, A. Roberts, S. Dieleman, D. Eck,\nK. Simonyan, and M. Norouzi, “Neural audio synthesis\nof musical notes with wavenet autoencoders,” in Pro-\nceedings of the International Conference on Machine\nLearning , 2017.\n[51] R. M. Bittner, J. Salamon, M. Tierney, M. Mauch,\nC. Cannam, and J. P. Bello, “Medleydb: A multitrack\ndataset for annotation-intensive mir research.” in Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference , 2014, pp. 155–160.\n[52] M. Defferrard, K. Benzi, P. Vandergheynst, and\nX. Bresson, “FMA: A dataset for music analysis,” in\nProceedings of the International Society for Music In-\nformation Retrieval Conference , 2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n44[53] M. Pesek, P. Godec, M. Poredos, G. Strle, J. Guna,\nE. Stojmenova, M. Pogacnik, and M. Marolt, “Intro-\nducing a dataset of emotional and color responses to\nmusic.” in Proceedings of the International Society\nfor Music Information Retrieval Conference , 2014, pp.\n355–360.\n[54] D. Foster, S. Dixon et al. , “Filosax: A dataset of anno-\ntated jazz saxophone recordings,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference , 2021.\n[55] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-\nZ. A. Huang, S. Dieleman, E. Elsen, J. Engel, and\nD. Eck, “Enabling factorized piano music modeling\nand generation with the MAESTRO dataset,” in In-\nternational Conference on Learning Representations ,\n2019.\n[56] E. Parada-Cabaleiro, A. Batliner, A. Baird, and B. W.\nSchuller, “The seils dataset: Symbolically encoded\nscores in modern-early notation for computational mu-\nsicology.” in Proceedings of the International Society\nfor Music Information Retrieval Conference , 2017, p.\n575–581.\n[57] S. H. Hakimi, N. Bhonker, and R. El-Yaniv, “Bebop-\nnet: Deep neural models for personalized jazz impro-\nvisations.” in Proceedings of the International Society\nfor Music Information Retrieval Conference , 2020, pp.\n828–836.\n[58] V . Eremenko, E. Demirel, B. Bozkurt, and X. Serra,\n“Audio-aligned jazz harmony dataset for automatic\nchord transcription and corpus-based research.” in Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference , 2018, pp. 483–490.\n[59] A. Pati, S. Gururani, and A. Lerch, “dmelodies: A mu-\nsic dataset for disentanglement learning,” in Proceed-\nings of the International Society for Music Information\nRetrieval Conference , 2020.\n[60] L. N. Ferreira and J. Whitehead, “Learning to generate\nmusic with sentiment,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence, 2019.\n[61] P. Beauguitte, B. Duggan, and J. D. Kelleher, “A cor-\npus of annotated irish traditional dance music record-\nings: Design and benchmark evaluations.” in Proceed-\nings of the International Society for Music Information\nRetrieval Conference , 2016, pp. 53–59.\n[62] L. Crestel, P. Esling, L. Heng, and S. McAdams, “A\ndatabase linking piano and orchestral midi scores with\napplication to automatic projective orchestration,” in\nProceedings of the International Society for Music In-\nformation Retrieval Conference , 2017.\n[63] A. Ycart, E. Benetos et al. , “A study on lstm net-\nworks for polyphonic music sequence modelling,” inProceedings of the International Society for Music In-\nformation Retrieval Conference , 2017.\n[64] L. Angioloni, V . Borghuis, L. Brusci, and P. Frasconi,\n“Conlon: A pseudo-song generator based on a new pi-\nanoroll, wasserstein autoencoders, and optimal inter-\npolations.” in Proceedings of the International Society\nfor Music Information Retrieval Conference , 2020, pp.\n876–883.\n[65] G. Widmer, “Discovering simple rules in complex data:\nA meta-learning algorithm and some surprising mu-\nsical discoveries,” in Artiﬁcial Intelligence , 2003, pp.\n129–148.\n[66] S. Lattner, M. Grachten, and G. Widmer, “A predictive\nmodel for music based on learned interval representa-\ntions,” in Proceedings of the International Society for\nMusic Information Retrieval Conference , 2018.\n[67] F. Falcao, B. Bozkurt, X. Serra, N. Andrade, and\nO. Baysal, “A dataset of rhythmic pattern reproduc-\ntions and baseline automatic assessment system,” in\nProceedings of the International Society for Music In-\nformation Retrieval Conference , 2019.\n[68] H. H. Tan and D. Herremans, “Music fadernets: Con-\ntrollable music generation based on high-level features\nvia low-level feature modelling,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference , 2020.\n[69] P. Sarmento, A. Kumar, C. Carr, Z. Zukowski, M. Bar-\nthet, and Y .-H. Yang, “DadaGP: A dataset of tokenized\nguitarpro songs for sequence models,” in Proceedings\nof the International Society for Music Information Re-\ntrieval Conference , 2021.\n[70] W.-Y . Hsiao, J.-Y . Liu, Y .-C. Yeh, and Y .-H. Yang,\n“Compound word transformer: Learning to compose\nfull-song music over dynamic directed hypergraphs,”\ninProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence , vol. 35, no. 1, 2021, pp. 178–186.\n[71] L. Gitelman, “Raw data is an oxymoron,” MIT press ,\n2013.\n[72] J. Sadowski, “When data is capital: Dataﬁcation, accu-\nmulation, and extraction,” Big data & society , vol. 6,\nno. 1, p. 2053951718820549, 2019.\n[73] L. Stark and A. L. Hoffmann, “Data is the new what?\npopular metaphors & professional ethics in emerging\ndata culture,” Journal of Cultural Analytics , 2019.\n[74] H. R. Ekbia and B. A. Nardi, “Heteromation, and other\nstories of computing and capitalism,” MIT Press , 2017.\n[75] B. Brown, “Will work for free: The biopolitics of\nunwaged digital labour,” in tripleC: Communication,\nCapitalism & Critique. Open Access Journal for a\nGlobal Sustainable Information Society , 2014, pp.\n694–712.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n45[76] M. Pasquinelli, “Google’s pagerank algorithm: A dia-\ngram of cognitive capitalism and the rentier of the com-\nmon intellect,” in Deep search: The politics of search\nbeyond Google , 2009, pp. 152–162.\n[77] J. Sadowski, “Planetary potemkin AI: The humans hid-\nden inside mechanical minds,” in Digital Work in the\nPlanetary Market , p. 229.\n[78] L. Irani, “Difference and dependence among digital\nworkers: The case of amazon mechanical turk,” in\nSouth Atlantic Quarterly , 2015, pp. 225–234.\n[79] C. Ó Nuanáin, H. Boyer, S. Jordà Puig et al. , “An eval-\nuation framework and case study for rhythmic concate-\nnative synthesis,” in Devaney J, Mandel MI, Turnbull\nD, Tzanetakis G, editors. ISMIR 2016. Proceedings\nof the 17th International Society for Music Informa-\ntion Retrieval Conference; 2016 Aug 7-11; New York\nCity (NY).[Canada]: ISMIR; 2016. p. 67-72. Inter-\nnational Society for Music Information Retrieval (IS-\nMIR), 2016.\n[80] J. W. Morris, “Selling digital music, formatting cul-\nture,” University of California Press , 2015.\n[81] C. Cath, “Governing artiﬁcial intelligence: ethical,\nlegal and technical opportunities and challenges,” in\nPhilosophical Transactions of the Royal Society A:\nMathematical, Physical and Engineering Sciences ,\n2018.\n[82] J.-P. Deranty and T. Corbin, “Artiﬁcial intelligence and\nwork: a critical review of recent research from the so-\ncial sciences,” AI & SOCIETY , pp. 1–17, 2022.\n[83] T. Moore and J. Brazeau, “Serge modular archive in-\nstrument (smai): Bridging skeuomorphic machine\nlearning enabled interfaces,” in New Interfaces for Mu-\nsical Expression , 2023.\n[84] Z. Wang, K. Chen, J. Jiang, Y . Zhang, M. Xu, S. Dai,\nX. Gu, and G. Xia, “Pop909: A pop-song dataset\nfor music arrangement generation,” arXiv preprint\narXiv:2008.07142 , 2020.\n[85] D. Ihde, “The Designer Fallacy and Technological\nImagination,” in Deﬁning Technological Literacy: To-\nwards an Epistemological Framework , 2006, pp. 121–\n131.\n[86] R. Savery, R. Rose, and G. Weinberg, “Establishing\nhuman-robot trust through music-driven robotic emo-\ntion prosody and gesture,” in International Confer-\nence on Robot and Human Interactive Communication ,\n2019, pp. 1–7.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n46"
    },
    {
        "title": "Sounds Out of Pläce? Score-Independent Detection of Conspicuous Mistakes in Piano Performances.",
        "author": [
            "Alia Morsi",
            "Kana Tatsumi",
            "Akira Maezawa",
            "Takuya Fujishima",
            "Xavier Serra"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265297",
        "url": "https://doi.org/10.5281/zenodo.10265297",
        "ee": "https://zenodo.org/records/10265297/files/000041.pdf",
        "abstract": "In piano performance, some mistakes stand out to listeners, whereas others may go unnoticed. Former research concluded that the salience of mistakes depended on factors including their contextual appropriateness and a listener's degree of familiarity to what is being performed. A conspicuous error is considered to be an area where there is something obviously wrong with the performance, which a listener can detect regardless of their degree of knowledge of what is being performed. Analogously, this paper attempts to build a score-independent conspicuous error detector for standard piano repertoire of beginner to intermediate students. We gather three qualitatively different piano playing MIDI data: (1) 103 sight-reading sessions for beginning and intermediate adult pianists with formal music training, (2) 245 performances by presumably late-beginner to early-advanced pianists on a digital piano, and (3) 50 etude performances by an advanced pianist. The data was annotated at the regions considered to contain conspicuous mistakes. Then, we use a Temporal Convolutional Network to detect the sites of such mistakes from the piano roll. We investigate the use of two pre-training methods to overcome data scarcity: (1) synthetic data with procedurally-generated mistakes, and (2) training a part of the model as a piano roll auto-encoder. Experimental evaluation shows that the TCN performs at an F-measure of 0.78 without pretraining for sight-reading data, but the proposed pretraining steps improve the F-measure on performance and etude data, approaching the agreement between human raters on conspicuous error labels. Importantly, we report on the lessons learned from this pilot study, and what should be addressed to continue this research direction.",
        "zenodo_id": 10265297,
        "dblp_key": "conf/ismir/MorsiTMFS23",
        "keywords": [
            "mistakes",
            "listeners",
            "contextual appropriateness",
            "degree of familiarity",
            "conspicuous error",
            "piano roll",
            "Temporal Convolutional Network",
            "pre-training methods",
            "data scarcity",
            "F-measure"
        ],
        "content": "SOUNDS OUT OF PLÄCE? SCORE-INDEPENDENT DETECTION OF\nCONSPICUOUS MISTAKES IN PIANO PERFORMANCES\nAlia Morsi1Kana Tatsumi2Akira Maezawa3Takuya Fujishima3Xavier Serra1\n1Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n2Nagoya Institute of Technology, Nagoya, Japan\n3Yamaha Corporation, Hamamatsu, Japan\nABSTRACT\nIn piano performance, some mistakes stand out to listeners,\nwhereas others may go unnoticed. Former research con-\ncluded that the salience of mistakes depended on factors\nincluding their contextual appropriateness and a listener’s\ndegree of familiarity to what is being performed. A con-\nspicuous error is considered to be an area where there is\nsomething obviously wrong with the performance, which a\nlistener can detect regardless of their degree of knowledge\nof what is being performed. Analogously, this paper at-\ntempts to build a score-independent conspicuous error de-\ntector for standard piano repertoire of beginner to inter-\nmediate students. We gather three qualitatively different\npiano playing MIDI data: (1) 103 sight-reading sessions\nfor beginning and intermediate adult pianists with formal\nmusic training, (2) 245 performances by presumably late-\nbeginner to early-advanced pianists on a digital piano, and\n(3) 50 etude performances by an advanced pianist. The\ndata was annotated at the regions considered to contain\nconspicuous mistakes. Then, we use a Temporal Convo-\nlutional Network to detect the sites of such mistakes from\nthe piano roll. We investigate the use of two pre-training\nmethods to overcome data scarcity: (1) synthetic data with\nprocedurally-generated mistakes, and (2) training a part of\nthe model as a piano roll auto-encoder. Experimental eval-\nuation shows that the TCN performs at an F-measure of\n0.78 without pretraining for sight-reading data, but the pro-\nposed pretraining steps improve the F-measure on perfor-\nmance and etude data, approaching the agreement between\nhuman raters on conspicuous error labels. Importantly, we\nreport on the lessons learned from this pilot study, and what\nshould be addressed to continue this research direction.\n1. INTRODUCTION\nA commonly held notion in automatic music performance\nanalysis (MPA) research is that deviations of music perfor-\nmances from their underlying music score can be regarded\nas performance mistakes. But previous music pedagogy\n© A. Morsi, K. Tatsumi, A. Maezawa, T. Fujishima, and X.\nSerra. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: A. Morsi, K. Tatsumi, A. Maezawa,\nT. Fujishima, and X. Serra, “Sounds out of pläce? Score-independent\ndetection of conspicuous mistakes in Piano Performances”, in Proc. of\nthe 24rd Int. Society for Music Information Retrieval Conf., Milano, Italy,\n2023.research suggests that some of such deviations are more\napparent to a listener than others [1, 2]. For example, a\nchord that is voiced differently from that written in the\nscore might be overlooked, but missing a note in a char-\nacteristic motif or playing a note that clashes with the un-\nderlying harmony would stand out. Repp [1] referred to\nerrors of the former category as perceptually inconspicu-\nous. Accordingly, we consider a conspicuous error to be\n\"a performance error that can be detected by the majority\nof listeners with a formal music training, regardless of their\ndegree of knowledge about the underlying music score of\na performed piece.\"\nThis paper explores the potential of building score-\nindependent models that detect regions of conspicuous er-\nrors in MIDI piano performances of piano solo pieces\nbased on Western music theory, as shown conceptually in\nFigure 1. Based on the intuition that a listener is capable\nof detecting obvious mistakes in piano performances by\nlistening to the surrounding context, we use a non-causal\nvariant of the Temporal Convolutional Network (TCN) [3]\nWe gather datasets for our task, since despite the plethora\nof work in automatic MPA that has spanned both the\nscore-dependent (or reference-dependent) [4–7] and score-\nindependent paradigms [8–13], there is no data available to\nsupport our desired goal.\nMore speciﬁcally we: (1) gather three datasets of con-\nspicuous errors in various performance situations, report-\ning on the dataset creation process and annotation proce-\ndure, (2) study the properties of the annotated data through\n(i) observing the annotated data for sources of inconsisten-\ncies, (ii) analyzing the relationship between inconspicuous\nand conspicuous errors and (ii) analyzing the ambiguity of\nthe task through listening experiments, (3) present a model\nbased on TCN to identify conspicuous errors from piano\nMIDI performance and discuss its effectiveness through\nexperimental evaluation, and (4) present and evaluate two\npre-training strategies, depending on the nature of the un-\nlabeled data that can be acquired. A subset of the gathered\ndata and listening examples can be found on the compan-\nion page1\n2. RELATED WORK\nWe distinguish between locally and globally-based auto-\nmatic MPA. In local approaches (such as the majority\n1https://bit.ly/3UCCiea352Inconspicuous:\nHarmonically consistent note modificationsConspicuous:\nHarmonically conflicting note insertion\nMusically natural note modificationsInconspicuous:Abrupt pauses or repetitionsConspicuous:\nRhythmically unnatural timingConspicuous:\ntiments\nFigure 1 : Illustration of our problem deﬁnition. Some errors stand out more than others in performance. Our goal is to\nidentify segments containing conspicuous errors to the listeners, without the need for music score data.\nof score-dependent performance assessment), the analysis\nis conducted at a note (or equivalent) level. Global ap-\nproaches learn from data mapping large performance snip-\npets (often entire performances) to overall evaluations.\nLocal approaches include score-based performance\nmistake identiﬁcation, which tends to cover note-level (or\nequivalent) errors such as pitch [1, 2, 4, 7] and rhythm mis-\ntakes [2]. Pitch mistakes are essentially categorized as\npitch intrusions (extra note) and pitch omissions (miss-\ning note), and occasionally pitch substitutions (wrong\nnote in-place of a correct one), although the latter can be\ntreated the joint occurrence of the former two [1]. Align-\nment/score comparison-based approaches for detecting de-\nviations are locally-based by deﬁnition. Piano assessment\nexamples of such include [4,7,14], which cover pitch mis-\ntakes. Not all local approaches are score-dependent, such\nas those which capture note-level aspects relating to the ar-\nticulation or sound quality. Examples are [15] and [12], for\npiano (3-point scale for quality of legato or staccato) and\ntrumpet (7-point scale) respectively.\nGlobal approaches to performance assessment have\nusually been score-free, with the exception of [5] which\nutilizes the score as input. Usually, such approaches\nare based on regression models mapping features to\nperformance-wide ratings [9, 11, 16, 17], or end-to-end\napproaches which learn correspondences between whole\nor parts of performances to performance wide ratings\n[5, 10, 13]. Such ratings can be discrete or continuous and\ncan span several performance dimensions. Although the\nconnection has not been explicitly made, we speculate that\nmost likely they would excel in capturing conspicuous per-\nformance mistakes that manifest as consistent errors/error\npatterns across a performance.\nAccordingly, we frame our approach as a score-\nindependent locally based one since our goal is to return\nbinary labels for each time point in a piano MIDI roll re-\nﬂecting the presence or absence of an obvious performance\nmistake. Therefore, we need similarly annotated data for\npiano MIDI performances to train our models. Despite\nscore deviations not necessarily indicating conspicuous er-\nrors, our desired output is closest to that of score-based\nperformance mistake identiﬁcation systems because their\noutput can be interpreted as a binary sequence indicating\nthe presence or absence of a score deviation albeit with-out perceptual relevance. However, their methods are not\napplicable for our problem formulation.\n3. DATA\nWe obtain 3 sources of non-commercial, piano MIDI per-\nformance data for different playing situations:\nSight-Reading Data (SR) : 103 sight-reading perfor-\nmances comprising mostly of piano reductions of popular\nclassical pieces, arranged for beginner to intermediate\ndifﬁculty. They are played by seven beginning to interme-\ndiate adult pianists with formal music training.\nPerformance Data (PF) : 245 performances of approx-\nimately 3 minutes each, collected from a digital piano\nrecording app. Not all performed pieces are known, but\nmost of them are pop and classical, that are either read\nfrom a score, or semi-improvised. While user attributes\nare unknown, the performance data suggests that the skill\nlevels range between late-beginner and early-advanced.\nBurgmüller Data (BM) : 50 performances from\nBurgmüller’s 25 Etudes, Op. 100 recorded twice on\na digital piano. They are played by an advanced pianist\nwho had previously played the etudes. The pianist\npracticed each etude brieﬂy before recording two takes.\nThe total time for the SR,PF, and BMare 379, 723, and\n60 minutes respectively, of which 128, 176, and 3 minutes\nwere annotated as conspicuous errors. Non-overlapping\nsplits of SRandPFare used for training, validation, and\ntesting, whereas BM is kept exclusively for testing. The\nannotation procedure is described in 3.1. SRandPFsub-\nsets cannot be shared, but short excerpts of them, and the\nfullBMset can be found in the companion page.\n3.1 Annotation Procedure\nWe had 2 annotators: Annotator 1 , who has experience\nas a classical piano teacher, and Annotator 2 , has train-\ning in music production and is also an intermediate-level\npianist. We asked Annotator 1 to label the SRandBM\ndata, and asked Annotator 2 to label the PFdata, and to\nindicate (yes/no) whether they know the piece being per-\nformed. For the SRandPFsubsets, annotators were given\ninstructions to annotate obvious performance mistakes that\ncan be recognized even without checking the score, and it\nwas left open to them to decide what that entails. The an-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n353notation was done with Cubase2, and they were asked to\nadd an annotation at MIDI note 0 covering the span of the\ntime window which they judge as pertaining to an error.\nDespite the potential label ambiguity due to the openness\nof the instructions, we wanted to observe the judgments of\ndifferent people in this pilot study so that we can improve\nthe data annotation protocol for future experiments.\nThe BM subset was treated differently because it has\nbeen played off of known music score data. First, the per-\nformances were automatically annotated with sites of score\ndeviations using a score alignment system. Then, the anno-\ntator manually reviewed the labels by listening to the per-\nformance while looking at the corresponding sheet music,\nand added missing deviations from the score or removed\nthose which do not reﬂect errors. The annotator simultane-\nously manually labeled each error as conspicuous or not.\n3.2 Annotation Examples and Pitfalls\nSome types of errors were labeled more consistently than\nothers. The more common error modes, as shown in Fig-\nure 2, include insertions and deletions of notes that do not\nﬁt in musical context, abrupt pauses, and unstable rhythm\ncoming from hesitations during playing. Annotators have\nshown reasonable consistency in terms of label location\nand span when mistakes are relatively short after which\nthe player recovers into their playing ﬂow, such as those\nof Figure 2. However, more compound deviations were la-\nbelled ambiguously. For example, sometimes after an error\na player would ’sneak-in’ some practice before resuming\nthe ﬂow of the piece. In such examples, if the short phrase\nbeing practiced sounds out of context, but in itself is co-\nherent, an open question is where the label should be, and\nwhether it should be one continuous label or an intermit-\ntent one.\nMoreover, we also observe the presence of non-\nannotated conspicuous mistakes in the data, but there is\nan inherent ambiguity in how one would assess a \"bad but\nacceptable\" and \"erroneous\" performance\". In a discus-\nsion with Annotator 1 after the annotations, they indicated\nthat their mental model for deciding whether a segment\nshould be labelled was dependent on every performance.\nIf a region contrasts with their expectation of the music\ngiven how that performer is playing, then it was annotated.\nThis opens the possibility that annotators have calibrated\nwhat should count as a mistake based on individual per-\nformance. Silence regions are one of the main sources of\nambiguity, since silences between correct portions are non-\nannotated regardless of their length, but silences within or\nsurrounding mistake portions often receive a mistake label.\n3.3 Analysis of the dataset\n3.3.1 Conspicuous to total label ratio in BM\nAlthough the ratio of annotated regions to total perfor-\nmance time is very small in the BM data, its annotation\napproach of allows us to investigate the relationship be-\ntween the set of errors obtained by comparing with a score\n2https://www.steinberg.net/cubase/\nFigure 2 : Examples of musical attributes that seemed to be\nconsistently annotated as conspicuous errors (in red). (a)\nmissed note that breaks a pattern, (b) harmonically unnat-\nural note insertions, (c) repetition, (d) abrupt pauses.\n(presumably all errors) to conspicuous errors. We found\nthat 59% of all identiﬁed errors were perceived as conspic-\nuous. Note that this is a very subset-speciﬁc result, because\nit depends on the ratio between subtle and obvious errors\nin the performances themselves as much as the qualities of\nthe performer and the annotation.\n3.3.2 Listening test of conspicuous errors\nThrough a listening test of some performance portions la-\nbeled as conspicuous errors and unlabeled areas for PF,\nwe assess how different subjects agree with the annotations\nand among themselves. We chose PFbecause we expect\nit to contain a nice balance between famous and unknown\npieces for each subject.\nConditions : We recruited 31 subjects, not necessarily\ntrained musicians. 84% of the subjects had experience\nplaying a musical instrument, and 97% of the subjects had\nexperienced either reading or notating music scores. Each\nsubject is asked to ﬁrst listen with headphones to a snip-\npet from the PFdataset, ranging from 4 to 12 seconds.\nThe snippet is either (1) a randomly chosen conspicuous\nmistake segment, with 2 seconds of padding on either end,\nor (2) a segment that contains no error label, whose dura-\ntion is the average duration of the conspicuous error seg-\nments within the piece, plus two seconds of padding. The\nsubjects were allowed to skip questions and no constraints\nwere given on the number of times the snippet may be lis-\ntened to. The subject is then asked to choose if they hear\nan obvious mistake or not, along with the subject’s knowl-\nedge of the piece. This procedure was repeated 15 times.\nThen, we scale the counts obtained when presenting non-\nconspicuous snippets, to provide a sensible assessment of\nthe dataset itself. That is, the ratio of snippets contain-\ning the inconspicuous error to the conspicuous ones, ρ0,\nshould match the ratio between the total duration of the\ninconspicuous error labels to that of the conspicuous la-\nbels in the dataset, ρ1. Thus, we scale the count of the re-\nsponses obtained when presenting the inconspicuous error\nbyρ1/ρ0.\nResults and discussion : A total of 462 responses were\nobtained (30-31 responses per snippet). The precision, re-\ncall, and the F-measure of how correctly the subjects iden-\ntiﬁed the mistakes were 0.37, 0.50, and 0.43, respectively.\nThe result suggests that the notion of conspicuous error is\nnot so clear-cut when only presenting a short snippet sur-\nrounding an error, without providing a longer musical con-\ntext. We also found that famous pieces tend to get more\nconsistent responses. To check this, we computed for eachProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n35460%\nPiano-rollTCN backbone\nOnset piano-rollClassi\n er headConspicuous\nerror probability\nFigure 3 : Our method reads a piano roll and outputs the\nprobability of the center of a segment being a conspicuous\nerror. It is comprised of a TCN backbone and a 1d convo-\nlution classiﬁer head.\nsnippet (1) the probability that a song is unknown and (2)\nthe entropy of the probability that a subject would identify\nthat snippet to contain an error. The correlation between\n(1) and (2) was 0.63, indicating a moderate correlation be-\ntween how well the piece is known among the subjects and\nhow consistent are the labels.\n4. METHODOLOGY\nGiven a sequence of piano note events, the goal is to infer\na time sequence of binary labels that indicates the presence\nof conspicuous errors at a given time.\n4.1 Model\nOur model is a TCN-based network that receives a piano\nrollXas input and emits a binary label of conspicuous\nerroreat each time frame of the piano roll. As shown in\nFigure 3, it is comprised of a feature extraction backbone\nfollowed by a classiﬁcation head. We choose to assign a\nlabel at frame-level instead of note-level, since not only\nthe note itself but its absence can indicate errors.\n4.1.1 Piano Roll Input\nTwo piano rolls are extracted for a given sequence\nof piano note events, one for the note onset and an-\nother for the sustained portion according to the key de-\npression. Speciﬁcally, suppose a set of IMIDI note\nevents (start time, end time, pitch, velocity) given as\n{(si,ei,pi,vi)}I\ni, and a sampling rate of Rare given.\nThen, a 256-dimensional piano roll X∈R256×Tis com-\nputed, such that X(pi,round(Rsi)) =vi, andX(128 +\npi,round(Rs)) =vifors∈[si,ei]. Partitura [18] is used\nfor the computation, and Ris set to 16 Hz.\nNotice that the sustain pedal information is ignored in\nthe computation of the piano roll. This is necessary to pre-\nvent the piano roll of the sustained portion from smear-\ning since a beginning pianist has a tendency to keep the\npedal depressed which causes and excessive elongation of\nthe computed note durations.\n4.1.2 Conspicuous mistake detector\nWe model the mistake detector as a simple TCN compris-\ning of a feature extraction backbone followed by a classi-\nﬁcation head, based on preliminary experiments exploring\nmodel architectures and inspired by the approach in [13].Feature extraction backbone : Given the piano roll X,\nthe feature extraction backbone computes a feature φ∈\nRD×T. We setD= 256 in this paper. This is realized as\na 5-layer noncausal TCN with dilation of [1,2,4,8,16], and\nfor all layers, has an output channel size of 256, kernel size\nof 3, uses ELU nonlinearity and has a residual connection,\nsimilar in spirit to [3].\nClassiﬁcation head : Given the feature φ, a network com-\nprising of three layers of 1x1 convolution with output chan-\nnel sizes [256,64,1] with residual connections and ELU\nnonlinearity followed by a sigmoid function is used to ar-\nrive at the conspicuous error posterior probability e.\n4.2 Training strategies\nThe model is trained using RAdam with a learning rate of\n10−3, as to minimize the cross-entropy between the con-\nspicuous error probability eand the posterior distribution\ncomputed from the ground-truth label. We augment the\ndata by randomly transposing the entire MIDI ﬁle in the\ntraining data. Furthermore, when computing the cross-\nentropy loss, we smooth the ground-truth label to account\nfor annotation inconsistencies in the start and end times\nof the conspicuous error segment. Furthermore, since it\nis difﬁcult to obtain annotations of conspicuous errors, we\npre-train the model as well, using the following two strate-\ngies.\n4.2.1 Pretraining the feature extractor as an autoencoder\nThe feature extractor can be trained in an unsupervised\nmanner, by training it as an autoencoder for a much larger\ncollection of piano performances in the wild. Speciﬁcally,\nwe train an auto-encoder using the feature extraction TCN\nintroduced earlier as the encoder and a TCN with trans-\nposed 1d convolutions instead of a 1d convolution as the\ndecoder. This way, the space of φis pre-trained as to model\nthe space of piano performances within a given receptive\nﬁeld of a TCN. This method could be useful if a large\ndataset of performances of unknown performance qualities\nare obtainable.\n4.2.2 Pretraining the model with synthetic mistake labels\nThe model can also be pre-trained on performance data\nonto which mistakes are simulated and corresponding mis-\ntake labels are inserted to match the expected format of\ndata in Section 3.1. Speciﬁcally, we apply systematic ad-\njustments to a set of mistake-free performances and modify\nthe note events, in a manner inspired by performance mis-\ntakes made by beginning adult pianists [19]. For each note\nevent, with probability pcwe modify the note in one of the\nfollowing ways:\n1. With probability poomit a note with a probability po\n2. With probability prreplace a note, to the same\nnote transposed nsemitones, to simulate hitting the\nwrong key.\n3. With probability piinsert a note that is transposed\nbynsemitones.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n355Method Precision Recall F-measure\nBaseline 0.79 0.80 0.78\nSYNTH 0.65 0.76 0.69\nSYNTH(FT) 0.61 0.69 0.62\nAE 0.55 0.59 0.55\nAE+SYNTH 0.44 0.65 0.51\n(a)SRData\nMethod Precision Recall F-measure\nBaseline 0.28 0.46 0.33\nSYNTH 0.27 0.54 0.34\nSYNTH(FT) 0.30 0.61 0.38\nAE 0.28 0.52 0.34\nAE+SYNTH 0.27 0.63 0.36\n(b)PFData\nMethod Precision Recall F-measure\nBaseline 0.26 0.36 0.26\nSYNTH 0.26 0.69 0.35\nSYNTH(FT) 0.26 0.49 0.32\nAE 0.27 0.46 0.31\nAE+SYNTH 0.28 0.52 0.35\n(c)BMData\nTable 1 : Results for different training strategies\n4. With probability pppause the performance by a\nsmall amount distributed uniformly between 0.3 and\n0.8 seconds. With probability ppr, repeat the last\nplayed note.\n5. With probability pspause the performance by a large\namount distributed uniformly between 2 and 4 sec-\nonds. Repeat the last played note.\nIn this paper, we set pc= 5% ,po= 10% ,pi= 39% ,\npr= 39% ,ps= 2% , andpp= 10% . Furthermore,\nfor note replacement and insertion, nis chosen so that\nn= 1,2are chosen with probabilities of 22% and n= 4,6\nby 2%. For a set of mistake-free performances, we ob-\ntained 260 hours of mostly jazz and classical MIDI piano\nperformances. The quality and repertoire are comparable\nto those available from Yamaha PianoSoft3.\nThis method is useful if many performances that are\nknown to be relatively error-free are obtainable Further-\nmore, this idea may possibly be used for data augmenta-\ntion, at the risk of increasing false positives, since not all\nsynthetic errors sound conspicuous, as also hinted by [1,2].\n4.3 Experiment: Model Evaluation\nWe evaluate our model using different training strategies.\n4.3.1 Experimental conditions\nOur model has been trained with the following strategies:\n1. Baseline - The model is trained on SRandPFdata.\n2. SYNTH - Same as Baseline, in addition to the inclu-\nsion of a subset of the synthetic data introduced in\nSection 4.2.2 during training and validation.\n3. SYNTH(FT) - The model is pretrained on the syn-\nthetic data, then ﬁne-tuned using SRandPF. This\n3https://shop.usa.yamaha.com/simulates a situation where a new annotated dataset\nbecomes available after traing a model solely trained\non a synthetic data.\n4. AE - Train TCN autoencoder introduced in Sec-\ntion 4.2.1 as a pretraining step for the backbone\nTCN, using approximately 100,000 MIDI perfor-\nmances played by various users. The set of perfor-\nmances does not contain SR PF orBM, although it\nis obtained from the same source as PF. The model\nis ﬁne-tuned on SRandPF.\n5. AE+SYNTH - Use the pretrained autoencoder back-\nbone and ﬁne-tune using SR,PFand the synthetic\ndata.\nThe trained models have been validated on SRandPF, and\ntested on a test split of SR,PF, and the entire BM.\nAs the metric, we have evaluated the transcription\nprecision/recall/F1-measure using mir_eval [20], treat-\ning the estimated and the ground-truth annotations as note\nevents occurring at a predeﬁned pitch. When computing\nthe transcription metrics, the note onset and offset toler-\nances have been set to 2 seconds. Furthermore, based\non the validation set, the ends of the estimated segments\nhave been padded by 0.2 seconds and overlapping seg-\nments have been merged.\n4.3.2 Results and discussion\nThe results are shown in Table 1. For PFandBMdatasets,\nthe augmentation strategies offer some improvements. The\ntwo strategies proposed, i.e., the use of synthetic data and\nautoencoder, also result in improvements. In general, both\nstrategies tend to improve the recall rate, suggesting that\nthey provide similar qualitative improvements, and either\none can be used depending on the data available.\nDespite the augmentation strategies, the F-measures for\nPFandBM data suggest future room for improvement,\neven taking into account the ambiguity of conspicuous\nerrors. The PFand BM data are difﬁcult to infer, as\nseen by the differences in the F-measure between the SR\ndataset and the two. As another example, the validation\nF-measure of the models on the synthetic dataset is about\n0.60. This suggests that the model is moderately capable of\npin-pointing the ground-truth labels if they are easy to clas-\nsify, or generated stochastically but systematically. At the\nsame time, however, the model has room for improvement,\nas the best-performing F-measure of 0.38 on the PFdataset\nfalls somewhat short of the oracle F-measure of 0.43, as\ndiscussed in Section 3.3.\nThe method performs well for the SRdata, perhaps\nbecause most of the mistakes are quite conspicuous in\na sight-reading situation, especially compared to PFand\nBM, both of which contain mostly beginner-intermediate\nperformances with occasional mistakes. The performance\ntends to drop as more pretraining steps are added, presum-\nably because the pretraining data mostly contain data of the\nsame type as the PFset, increasing the disparity between\nthe training data and the test data. In sight-reading situa-\ntions, the results suggest it is sufﬁcient simply to train on aProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n356(a) True positives. The black band indicates the detected conspic-\nuous error with different training strategies. The model presum-\nably responds to (a) repetition, (b) silences, (c) slight hesitations\nin playing, (d) note insertions, and (e) lack of synchrony voices.\n(b) False positives. The model presumably confuses (a) the re-\npeated motives as an error, (b) rhythm with rest as abrupt pauses,\n(c) an audible but weak note with a note deletion, and (d) a long\nchord after a fast passage with hesitations.\nFigure 4 : Examples of typical operation and failure modes.\ndataset that solely contains data from the same set, instead\nof pretraining or augmenting the dataset with typical ama-\nteur performances containing some conspicuous errors.\n4.3.3 Qualitative insights of the estimates\nFigure 4 shows some examples of true positives that are\nconsistent across different strategies and consistent false\npositives. The proposed method tends to capture repeti-\ntion, pauses, hesitations, and note insertions that occur in\nnarrow pitch intervals as mistakes. At the same time, how-\never, the very same properties arising from musical expres-\nsion or composition are detected as false positives, such\nas repeated motifs, ornaments, and grand pauses. Even\nthough such musical aspects are superﬁcially performed\nsimilarly to the aforementioned mistakes, humans are ca-\npable of differentiating between genuine performance mis-\ntakes and those within musical contexts. This suggests that\nthe model has room to improve by modeling the underly-\ning composition better. The readers are invited to check\nthe companion page for examples.\n5. LIMITATIONS AND IMPROVEMENTS\nOur work opens door to many open problems that need to\nbe solved, some more fundamental than others.\nProblem deﬁnition and annotation protocol : More\nwork is needed to deﬁne the concept of conspicuous er-\nrors, and how the task should be evaluated from a music\ntechnology perspective. Accordingly, a more comprehen-\nsive protocol for data collection should be developed. Al-\nthough we had kept the annotation instructions open to also\ndevelop an understanding of annotator behavior, it became\nevident that our data collection approach does not guaran-\ntee that the labels we have are for solely conspicuous er-\nrors. In [1], conspicuous errors were identiﬁed in a music\nperformance by ﬁnding the subset of agreed-upon mistake\nlabels between multiple listening subjects.\nTo deﬁne manifestations of conspicuous errors, a mid-\npoint should be found between a rule-based approach and\none learned from empirical labels. The outcome should be\na set of error descriptions, some of which happen at partic-\nular time instants and some over longer windows, whether\ncontinuous windows or a longer span of intermittent la-\nbels. However, since the conspicuousness of errors is in-spired by a perceptual idea, we think these errors should be\ndeﬁned through an empirical process albeit better deﬁned\nthan the one in this study to avoid the same pitfalls.\nSynthetic mistakes : Synthetic data is important for\nimproving performance, but current synthesized mistakes\nsound unnatural. A simple example was a case of induced\npitch insertions, where it seemed impossible that someone\ncan perform with such conﬁdence and tempo despite the\nextent of out-of-context pitch insertions. We observe that\nbeginners make mistakes and employ recovery strategies in\na manner that is more complex than the presented method,\nso a better understanding of beginning pianists’ behavior\nis necessary to create more natural-sounding mistakes.\nListener, player, expression, and style : Conspicuous\nerrors are dependent on the listener’s knowledge of the\npiece and the proﬁciency of the performer. Furthermore,\nconspicuous error and expression are two sides of the same\ncoin. For example, hitting an adjacent key can either come\nacross as an expressive ornament or a conspicuous error.\nThis suggests that conspicuous error detection should in-\nherently be conditioned on the style, the level of the lis-\ntener, and the player’s proﬁciency.\nConnecting with pedagogy and edu-tainment : The\nimpact of music education software which provides anal-\nysis solely founded on rigid note-level rhythmic and pitch\ncorrectness has been challenged [21] on the basis that users\nmight end up too focused on playing too correctly (almost\nrobotically) to attain the highest scores. There are many\npedagogical considerations for designing useful automatic\nassessments [22].\n6. CONCLUSION\nThis paper presented a study on detecting conspicuous per-\nformance mistakes for a piano solo performance of begin-\nning to intermediate players. We (1) clariﬁed the idea of a\nconspicuous error in line with previous research, (2) gath-\nered locally annotated piano MIDI performance data, and\n(3) discussed sources of inconsistencies in our data through\nanalysis of the annotation procedure and subjective tests.\nAlthough some of our models show an acceptable perfor-\nmance on the test split of the SRdata subset, we ﬁnd that\nthe our pre-training suggestions do not provide remarkable\nperformance improvements.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3577. REFERENCES\n[1] B. H. Repp, “The art of inaccuracy: Why pianists’ er-\nrors are difﬁcult to hear,” Music Perception: An Inter-\ndisciplinary Journal , vol. 14, p. 161–183, 1996.\n[2] B. Gingras, C. Palmer, P. N. Schubert, and\nS. McAdams, “Inﬂuence of melodic emphasis, texture,\nsalience, and performer individuality on performance\nerrors,” Psychology of Music , vol. 44, p. 847–863,\n2016.\n[3] M. E. P. Davies and S. Böck, “Temporal convolutional\nnetworks for musical audio beat tracking,” in Proc.\nEuropean Signal Processing Conference (EUSIPCO) ,\nSeptember 2019.\n[4] E. Nakamura, K. Yoshi, and H. Katayose, “Perfor-\nmance error detection and post-processing for fast and\naccurate symbolic music alignment,” in Proc. Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , October 2017, pp. 347–353.\n[5] J. Huang, Y . N. Hung, K. A. Pati, S. Gururani, and\nA. Lerch, “Score-informed networks for music per-\nformance assessment,” in Proc. International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2020.\n[6] H. Zhang and Y . a. Jiang, “Learn by referencing: To-\nwards deep metric learning for singing assessment,” in\nProc. International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2021.\n[7] E. Benetos, A. Klapuri, and S. Dixon, “Score-informed\ntranscription for automatic piano tutoring,” in Proc.\nEuropean Signal Processing Conference (EUSIPCO) ,\n2012, pp. 2153–2157.\n[8] T. Nakano, M. Goto, and Y . Hiraga, “An automatic\nsinging skill evaluation method for unknown melodies\nusing pitch interval accuracy and vibrato features,” in\nProc. International Conference on Spoken Language\nProcessing (ICSLP) , September 2006, p. 1706–1709.\n[9] C. W. Wu, S. Gururani, C. Laguna, A. Pati, A. Vid-\nwans, and A. Lerch, “Towards the objective assessment\nof music performances,” in Proc. International Con-\nference on Music Perception and Cognition (ICMPC) ,\nJuly 2016.\n[10] K. A. Pati, S. Gururani, and A. Lerch, “Assessment\nof student music performances using deep neural\nnetworks,” Journal of Applied Sciences , vol. 8, no. 4,\n2018. [Online]. Available: https://www.mdpi.com/\n2076-3417/8/4/507\n[11] J. Abeßer, J. Hasselhorn, S. Grollmisch, C. Dittmar,\nand A. Lehmann, “Automatic competency assessment\nof rhythm performances of ninth-grade and tenth-grade\npupils,” in Joint Proc. International Computer Music\nConference (ICMC), and Sound and Music Computing\nConference (SMC) , September 2014, pp. 1252–1256.[12] T. Knight, F. Uphamm, and I. Fujinaga, “The potential\nfor automatic assessment of trumpet tone quality,” in\nProc. International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2011.\n[13] P. Seshadri and A. Lerch, “Improving music perfor-\nmance assessment with contrastive learning,” in Proc.\nInternational Society for Music Information Retrieval\nConference (ISMIR) , November 2021.\n[14] T. Fukuda, Y . Ikemiya, K. Itoyama, and K. Yoshii, “A\nscore-informed piano tutoring system with mistake de-\ntection and score simpliﬁcation.” Proc. Sound and\nMusic Computing Conference (SMC), Jul 2015.\n[15] V . Phanichraksaphong and W.-H. Tsai, “Automatic\nevaluation of piano performances for steam education,”\nApplied Sciences , vol. 11, no. 24, 2021. [Online].\nAvailable: https://www.mdpi.com/2076-3417/11/24/\n11783\n[16] J. Abeßer, J. Hasselhorn, S. Grollmisch, C. Dittmar,\nand A. Lehmann, “Automatic quality assessment of\nvocal and instrumental performances of ninth-grade\nand tenth-grade pupils,” in Proc. International Sympo-\nsium on Computer Music Multidisciplinary Research\n(CMMR) , 2013.\n[17] J. Pan, M. Li, Z. Song, X. Li, X. Liu, H. Yi, and\nM. Zhu, “An Audio Based Piano Performance Eval-\nuation Method Using Deep Neural Network Based\nAcoustic Modeling,” in Proc. Interspeech 2017 , 2017,\npp. 3088–3092.\n[18] C. E. Cancino-Chacón, S. D. Peter, E. Karystinaios,\nF. Foscarin, M. Grachten, and G. Widmer, “Partitura:\nA Python Package for Symbolic Music Processing,” in\nProc. Music Encoding Conference (MEC2022) , 2022.\n[19] Y . Morijiri, S. Obata, A. Maezawa, and T. Fujishima,\n“Understanding the challenges for adult beginners at\npiano practice from an analysis of errors,” in Proc.\nAsia-Paciﬁc Symposium for Music Education Research\n(APSMER2021) , 2021.\n[20] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P. W. Ellis, “MIR_EV AL:\nA Transparent Implementation of Common MIR\nMetrics,” in Proc. International Society for Music\nInformation Retrieval Conference (ISMIR) , 2014, pp.\n367–372. [Online]. Available: http://dblp.uni-trier.de/\ndb/conf/ismir/ismir2014.html#RaffelMHSNLE14\n[21] A. Acquilino and G. Scavone, “Current state and future\ndirections of technologies for music instrument peda-\ngogy,” Frontiers in Psychology , vol. 13:835609, 2022.\n[22] V . Eremenko, A. Morsi, J. Narang, and X. Serra,\n“Performance assessment technologies for the sup-\nport of musical instrument learning,” in Proc. Interna-\ntional Conference on Computer Supported Education\n(CSEDU) , May 2020, pp. 629–640.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n358"
    },
    {
        "title": "Exploring the Correspondence of Melodic Contour With Gesture in Raga Alap Singing.",
        "author": [
            "Shreyas Nadkarni",
            "Sujoy Roychowdhury",
            "Preeti Rao",
            "Martin Clayton"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265213",
        "url": "https://doi.org/10.5281/zenodo.10265213",
        "ee": "https://zenodo.org/records/10265213/files/000001.pdf",
        "abstract": "Musicology research suggests a correspondence between manual gesture and melodic contour in raga performance. Computational tools such as pose estimation from video and time series pattern matching potentially facilitate larger-scale studies of gesture and audio correspondence. We present a dataset of audiovisual recordings of Hindustani vocal music comprising 9 ragas sung by 11 expert performers. With the automatic segmentation of the audiovisual time series based on analyses of the extracted F0 contour, we study whether melodic similarity implies gesture similarity. Our results indicate that specific representations of gesture kinematics can predict high-level melodic features such as held notes and raga-characteristic motifs significantly better than chance.",
        "zenodo_id": 10265213,
        "dblp_key": "conf/ismir/NadkarniRRC23",
        "keywords": [
            "Musicology",
            "correspondence",
            "manual gesture",
            "melodic contour",
            "raga performance",
            "computational tools",
            "pose estimation",
            "time series pattern matching",
            "Hindustani vocal music",
            "audiovisual recordings"
        ],
        "content": "EXPLORING THE CORRESPONDENCE OF MELODIC CONTOUR WITH\nGESTURE IN RAGA ALAP SINGING\nShreyas Nadkarni1Sujoy Roychowdhury1Preeti Rao1Martin Clayton2\n1Department of Electrical Engineering, Indian Institute of Technology Bombay, India\n2Department of Music, Durham University, United Kingdom\nprao@ee.iitb.ac.in, martin.clayton@durham.ac.uk\nABSTRACT\nMusicology research suggests a correspondence between\nmanual gesture and melodic contour in raga performance.\nComputational tools such as pose estimation from video\nand time series pattern matching potentially facilitate\nlarger-scale studies of gesture and audio correspondence.\nWe present a dataset of audiovisual recordings of Hindus-\ntani vocal music comprising 9 ragas sung by 11 expert per-\nformers. With the automatic segmentation of the audiovi-\nsual time series based on analyses of the extracted F0 con-\ntour, we study whether melodic similarity implies gesture\nsimilarity. Our results indicate that speciﬁc representations\nof gesture kinematics can predict high-level melodic fea-\ntures such as held notes and raga-characteristic motifs sig-\nniﬁcantly better than chance.\n1. INTRODUCTION\nManual gesturing by singers is an integral part of vocal\nmusic performances in the Indian classical traditions. Pre-\nvious work has demonstrated that singers’ gestures have\nseveral different referents and functions: for example, they\nmay relate to the rhythmic structure of the music (marking\na steady beat or tala cycle) or play a role in signalling to co-\nperformers or audience members, as well as appearing to\naccompany or illustrate aspects of the melody being sung.\nIn the latter case, hand movements sometimes appear to\ncorrespond to pitch height (i.e. ascending pitch co-occurs\nwith one or both hands rising and/or moving to one side);\nat other times they relate to other aspects of melody, such\nas the tension felt while sustaining certain notes, or the im-\nage or abstract design visualised by the performer [1–5].\nLittle computational work has been carried out on\ngesture-to-audio correspondence in Hindustani vocal mu-\nsic. Paschalidou [6] carried out research on a motion cap-\nture dataset of solo alap recordings in the dhrupad genre,\nlooking at a range of movement and audio features in re-\nlation to the concept of ‘effort’: although she found cor-\n© S. Nadkarni, S. Roychowdhury, P. Rao and M. Clayton.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: S. Nadkarni, S. Roychowdhury, P.\nRao and M. Clayton, “Exploring the correspondence of melodic contour\nwith gesture in raga alap singing”, in Proc. of the 24th Int. Society for\nMusic Information Retrieval Conf., Milan, Italy, 2023.Dataset Singers Ragas Pakad Alap Dur(min)\nStudy in [7] 3 (1M,2F) 9 37 55 193\nCurrent Work 11(5M,6F) 9 109 199 664\nTable 1 : A summary of the newly augmented audiovisual\ndataset compared with that of closest previous work [7].\nrespondences, generalising across performers proved chal-\nlenging.\nClayton et al [7] explored the use of movement data to\nclassify 12-sec excerpts drawn from a corpus comprising\n3 singers performing 9 common Hindustani ragas in the\nkhyal genre. The use of solo alap meant the gestures can-\nnot refer to either metric structure or interaction with co-\nperformers, and thus relate predominantly to the melody\nof the ragas being presented. An inception block preceded\nby independently trained convolution layers for each of\naudio and gesture time series classiﬁcation provided the\nbest performance in the context of singer-dependent raga\nclassiﬁcation, especially reducing the confusion between\nmelodically similar ragas with respect to the otherwise\nhigh-performing audio-only classiﬁcation. While the work\ndemonstrated the complementarity of gesture and melodic\nproﬁles in relation to raga identity, we are more interested\nin the present work in understanding which characteris-\ntics of gesture correlate with speciﬁc melodic character-\nistics. Further, given that the dataset of [8] was limited to\n3 singers and therefore not suited to cross-singer studies,\nwe present here a considerably enlarged corpus with 8 ad-\nditional performers, collected following a similar method-\nology, as summarised in Table 1.\nIn the related Karnatak tradition, Pearson’s research has\nlooked at the role of gesture in vocal teaching [9]. The re-\nlation between the acoustics and the kinematics was stud-\nied in recent work by Pearson & Pouw using the track-\ning of left and right wrist positions [10]. They manually\nsegmented the gesture tracks and studied the correspon-\ndence of various kinematic extrema with the temporally\naligned changes in the acoustics (fundamental frequency,\nor F0, and amplitude envelope). A correspondence was\nestablished between the magnitudes of local peaks in ac-\nceleration and changes in F0, in line with previous work in\nco-speech gesturing [11].\nIn this work, we study the newly expanded corpus of21Figure 1 : The overall testing and evaluation framework for the raga phrase-based segmentation. The audio and visual\ncomponents of a candidate A V segment (i-th segment from an alap) are separately compared with the respective audio\nand visual components of a reference phrase segment (from a pakad) to see whether they are together consistent in their\nestimation of similarity with the reference phrase. We note that the gesture T.S. (time series) is multidimensional while the\naudio T.S. is a unidimensional sequence of F0 samples.\nsolo alap recordings. Since the same set of 9 ragas is per-\nformed by all the singers (11 in this case), we can explore\ncommonalities in the gestures used by different singers for\nparticular raga-speciﬁc melodic movements. That is, in\ncontrast to the body of previous work, we use musically\nmotivated units, implied by the raga melodic structure, to\ngroup the representations of melody and gesture. The aim\nof the study is to investigate correspondences between the\nsingers’ movements (captured in the time series for x- and\ny-coordinates of their wrist positions) and the melodies\nthey sing (represented as F0 contours).\nFigure 1 depicts our overall framework. The melodic\nphrase segments are obtained for each alap audio via a\nsubsequence search using a reference audio template (such\nas a manually labeled phrase segment). The audio seg-\nment start and end times are then used to identify the cor-\nresponding time-synchronised video segment. The audio\nand video segments are individually processed to compute\naudio-based similarity and video-based similarity with re-\nspect to the corresponding components of the A V (audio-\nvisual) reference template. We now seek to quantify the\nextent to which video-based similarity predicts audio simi-\nlarity. We simplify the evaluation task to comparing, across\nthe two modalities, the following binary labels: L (i.e.\nclose to, or Like, the reference) or U (Unlike the refer-\nence).\nIn the next section, we provide the details of our dataset.\nThis is followed by a discussion of the audiovisual segmen-\ntation methods. The experiments and results are presented\nin the ﬁnal two sections of the paper.\n2. DATASET AND PREPROCESSING\nWe consider our dataset of vocal alap performances by 11\nprofessional musicians performing 2 alaps each of 9 ragas.\nEach alap is about 3 minutes long. The singers also con-\ntributed shorter ‘pakad’ recordings, rendering some of thekey phrases of each raga in a brief format of a few sec-\nonds. The total duration of this newly expanded dataset\n(summarised in Table 1) is about 11 hours. Each piece\nwas recorded using three video cameras and separate mi-\ncrophone; only the central camera is used in the current\nanalyses. While each alap is labeled only by singer and\nraga, we carry out further manual annotation of the pakad\naudio ﬁles for selected raga phrases as used in this study.\nThat is, all the pakads of a given raga across the 11 singers\nare searched for instances of the desired phrase (e.g. gmD\nin raga Bageshree). This task, carried out by a musician, is\nfacilitated by the fact that the pakad is almost always sung\nwith solfege (unlike the alap).\nOur audio and video processing pipelines closely follow\nthose of [7]. An initial stage of audio suppression of the\nbackground drone is obtained via source separation [12].\nThe suppression, while not complete, is adequate for the\nreliable estimation voicing and pitch at 10 ms intervals us-\ning monophonic pitch detection based on short-time au-\ntocorrelation analysis [13]. Brief unvoiced regions (less\nthan 400 ms) arising from short breath pauses and con-\nsonant utterances are ﬁlled in via cubic spline interpola-\ntion to obtain the continuous pitch contours associated with\nmelodic movements that are bounded by silence (>400 ms)\non both ends. These are termed ‘Silence-Delimited Seg-\nments’ (SDS). The pitch contour is tonic-normalised using\nan automatically detected (and manually veriﬁed) tonic to\nobtain the F0 (cents) contour [14].\nIn order to extract the movement data, the central\nvideo view of each piece is processed using the Open-\nPose pose estimation algorithm, which generates x- and\ny-coordinates for 11 upper body joints [15]. We select the\nright and left wrist coordinates. Any missing data are in-\nterpolated and each of the time series is low-pass ﬁltered to\nremove jitter. The position time-series, originally sampled\nat 25 fps is interpolated to 100 samples/sec to synchroniseProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n22it with the sampled F0 contour. Other important low-level\nhuman motion descriptors include velocity (rate of change\nof the 2d position) and acceleration (rate of change of the\nvelocity) [16]. We derive velocity and acceleration proﬁles\nfrom the 2d position time-series of each joint by computing\nderivatives. A robust estimate of the derivative is obtained\nvia a differencing kernel such as a biphasic ﬁlter with its\ncontrollable smoothing parameters [17, 18]. We ﬁnd that a\n101-point ﬁlter achieves a lowpass ﬁltering of about 2 Hz,\ngiving a sufﬁciently smooth and physiologically plausible\nmovement acceleration proﬁle [19]. We eventually obtain\nthe 8-dim gesture time series of position (x and y), veloc-\nity and acceleration for each of the left and right wrists for\neach of the singer-alap and pakad recordings. In this, we\ninclude the synchronized F0 contour to get the complete\naudiovisual time series for an alap, now in the form of a\nsequence of SDS. A detailed review of the data collection\nand processing appears in the supplementary material.\n3. SEGMENTATION METHODS\nThe ﬁrst stage of segmentation of the synchronised\nA V time series comprises the silence-delimited segments\n(SDS) obtained in the previous section. We discard seg-\nments of duration less than 500 ms as too limited for our\nfurther analyses. The retained SDS, numbering approxi-\nmately 30 per alap, have a mean duration of 5.2 s with less\nthan 1% (of the total count of 6012) exceeding 20 s. As\ndiscussed next, we apply melodic segmentation principles\nto each SDS to obtain stable note and raga-characteristic\nmelodic movements or phrases that can help us explore the\nlinks between speciﬁc musical expressions and the corre-\nsponding gestures.\nIn a top-down approach, the alap can be segmented into\nits phrases. A raga phrase, although notated simply by its\nsolfege sequence, has a melodic-rhythmic realisation com-\nprising speciﬁc intonations and durations of its constituent\nsvaras, together with the transitions to/from neighbouring\nsvaras [20]. On the other hand, in a bottom-up approach,\nthe melodic contour can be viewed as comprising the fol-\nlowing broad categories of segments: stable notes, and the\ntransitions between the notes which can include distinc-\ntive melodic ornaments such as glides (meend) and os-\ncillatory movements (andolan) apart from steep changes\nof pitch or pauses [21]. Figure 2 presents an example of\nan SDS that comprises a variety of stable and transitional\nsub-segments. It is therefore of interest to examine audio-\nvisual correspondences in the context of the distinct types\nof melodic movements. The two different audio-based seg-\nmentation procedures are detailed next.\n3.1 Stable note segmentation\nTo identify occurrences of stable or sustained tones,\nthe continuous F0 contour corresponding to an SDS is\nsearched for instances in which the same raga note (svara)\nis sustained for > 250 ms. That is, a stable note is de-\nﬁned as a region where the F0 lies within a 25 cent interval\nof the mean intonation of the raga note. This is based on\nFigure 2 : A sample SDS with identiﬁed steady notes\n(shaded regions of blue F0 contour) and pitch salience dis-\ntribution (on the left) computed from the entire alap audio\nwith detected svara locations highlighted.\npast work that associated the similar duration and intona-\ntion parameters with a listener’s percept of a held note [22].\nFurther, given that a svara may not be realised on the eq-\nuitempered grid but rather with a raga-speciﬁc intonation,\nwe use a ﬁnely binned pitch salience distribution computed\nacross the alap to establish the svara locations [22]. Stable\nnote regions corresponding to the same svara that are sepa-\nrated by less than 100 ms are next merged. The boundaries\nof the so detected stable notes are shown in the example of\nFigure 2. Across our alap dataset, stable notes were found\nto range from 0.25 s to 9.9 s with a mean of 0.73 s.\nIn a similar vein, we considered the segmentation of an-\nother characteristic melodic movement, the glide (or slide).\nThis has been attempted previously via the quality of a\nlinear ﬁt to the F0 contour for Indian popular vocal mu-\nsic [23]. However we found that the variety and complex-\nity of glide movements in raga music make it challenging\nto develop a universal glide detection algorithm. We there-\nfore resort to template-based phrase detection for the pur-\npose, as explained next.\n3.2 Phrase-based segmentation\nAs depicted in Table 2, the raga motifs selected for our\nexploration include a distinctive upward slide of an aug-\nmented fourth in Shree, a falling slide of a fourth in Nand,\nand a three-note ascending phrase in Bageshree. The cho-\nsen phrases are highly characteristic of the corresponding\nraga and occur in the raga alap with relatively unchanged\nmelodic shape, prompting the question about whether their\ngesture executions also bear some measurable similarity.\nThe corresponding pakad phrases serve as templates for\nthe segmentation of the alaps for the chosen raga across\nthe 11 singers. We obtain a number of templates of the\ngiven phrase from across the 11 singers’ pakads. The set\nof templates represents the diversity in the realization of\nthe phrase across and within singers. This is manually re-\nduced to a set of 6 templates per phrase while retaining the\ndiversity. Figure 3 shows a few examples for each of the\nphrases chosen for the current study. We observe that the\nsimple notation used to represent the up or down slide (/,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n23Raga Svara (Notes) Phrase\nBageshree S R g m P D n gmD\nShree S r G M P d N r/P\nNand S R G m M P D N P\\R\nTable 2 : The ragas and phrases used in this study. The\nsvaras S r R g G m M P d D n N correspond to the 12\nnotes of the Western chromatic scale with S representing\nthe tonic. The symbols / and \\ denote the upward and\ndownward slide respectively [24], [25].\n\\) belies the complexity of contour shapes deﬁned by raga\ngrammar. Also clear are the essential shape features that\npoint to the need for dynamic time warping (DTW) based\ncomparisons [26]. Next, the following steps (also visu-\nalised in Figure 4) lead to the desired segmentation of the\nalap audio ﬁles for each selected raga phrase.\n1. The six phrase templates from the pakads are warped\nto the same target length (that of the 3rd template in in-\ncreasing length order in the set) using a penalty parameter\nthat discourages large deviations from the diagonal path.\nThis helps to ensure that the subsequence DTW matching\ncosts can be meaningfully compared across the templates.\n2. As shown in the middle panel of Figure 4, con-\nstrained DTW based subsequence search is executed on\neach SDS with each of the 6 warped audio templates\n(WAT) to obtain for each WAT the lowest cost match that\nsatisﬁes a duration criterion ( >0.5s) in order to avoid cases\nof pathological warping [27]. Such matches are accepted\nas valid and stored with the cost, temporal boundaries and\nWAT index. In case no valid match is returned (in the top\n20 retrieved responses) for a particular template, that SDS-\ntemplate is not considered further. This step leaves us with\nbetween 1 to 6 best matched segments per SDS along with\nthe associated DTW costs.\n3. Next, we pick the single lowest cost for each SDS\nand use this value to cluster the entire set of SDS, across 22\nalaps of the raga, into 2 clusters by ﬁtting a kernel density\nestimate (KDE) to the distribution of costs as shown in Fig-\nure 5 [28]. The cost value coinciding with the lowest point\nin the valley between the peaks is used as a cost thresh-\nold to label each SDS as one of the two classes:‘Like’ (i.e.\nsimilar to the raga motif) and ‘Unlike’ (different from the\nraga motif). These are the labels we would like to predict\nfrom the corresponding gesture time series segments in the\ncontext of our investigation of audiovisual correspondence.\n4. In order to increase the number of examples for the\ngesture-based prediction task, we club all the different tem-\nplate matches obtained in Step 2 for the same SDS un-\nder the same label. This was justiﬁed by our observation\nthat the SDS labeled Like (L) in Step 3 typically exhibited\nsimilar low cost matches across all templates of the same\nphrase. The SDS marked Unlike (U), on the other hand,\nexhibited a relative wide spread in cost values above the\nthreshold, similar to that depicted in Figure 5.\nFinally, with the audio segments computed in this sec-\ntion, we extract the corresponding temporally synchro-\nFigure 3 : Sample templates for each of the three phrases:\ngmD (purple), r/P (blue) and P\\R (violet).\nFigure 4 : The pipeline for phrase-based segmentation us-\ning alap and pakad audio data. For warping the pakad\nphrase templates, a window size of 100 and a penalty of\n200 was chosen, while for the subsequence alignment, K =\n20 and penalty = 0.1 were chosen [27].\nnised gesture time series for each SDS and WAT pair. In\nthe next section, we report our experiments on testing vari-\nous kinematic features for the prediction of the correspond-\ning audio-derived labels in our two distinct tasks.\n4. EXPERIMENTS\nPast work on gesture kinematics in the context of speech\nand singing co-gesturing has considered velocity and ac-\nceleration parameters rather than the raw position time se-\nries with these parameters relating more directly to human\neffort or force [10, 29–31]. We therefore include the (x,y)\nposition of each wrist as well as the corresponding ve-\nlocity and acceleration proﬁles across the segment as in-\nput features for our two classiﬁcation tasks. Summarising\nthe previous section, the gesture time series is segmented\nbased on the previously obtained audio segment bound-\naries giving us a time-aligned multidimensional time seriesProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n24Figure 5 : Distribution of the DTW subsequence cost\nacross the SDS of all singer alaps for the best matched\naudio phrase template for P\\R of raga Nand. The dashed\nvertical line shows the threshold derived from the KDE ﬁt\n(dashed contour), using which the SDS are labeled as Like\nand Unlike with reference to the template phrase.\nfor (i) each stable-note and non-stable segment across all\nthe alaps in the dataset, and (ii) each pakad phrase gesture\ntemplate and its audio-matched gesture segment from the\nSDS. We consider supervised classiﬁcation for each task\nwith the different features as discussed next.\n4.1 Stable-note prediction\nStable note segments were labeled as such based on the\nF0 variation across the segment as discussed in Section\n3.1. We would like to investigate whether there is any con-\nsistency in the gesture kinematics corresponding to stable\nnote regions. We implement a binary classiﬁer trained and\ntested on the dataset of labeled stable notes and the (com-\nplementary) non-stable regions where the training and test\ndata are both drawn from across singers and ragas. Al-\nthough 250 ms regions of stable pitch qualiﬁed as stable\nnotes, we restricted the examples of both categories used\nin this experiment to those with a minimum duration of 0.5\ns in order to ensure that the training dataset was relatively\nbalanced. When the segment duration was constrained to\nthe range 0.5 s to 5 s, the stable notes constituted 39% of\nthe total examples, with across-singer variability as cap-\ntured in row 2 of Table 3.\nPostulating that gesture kinematics are relatively more\nsubdued during the stable note events, we investigate a\nsimple set of explicit features using a Support Vector Ma-\nchine (SVM) classiﬁer. We compute the statistical aggre-\ngates of each of the velocity and acceleration in the form\nof the mean and variance across the duration of the cor-\nresponding time series segment. We thus have 4 features\nper wrist (i.e. 8 features in all) for the binary classiﬁca-\ntion of segments into stable and non-stable pitch events.\nWe carry out 10-fold cross-validation on the dataset and\nreport the F1 score for the detection of stable notes with\nthe SVM hyperparameters tuned to maximise the average\nperformance across the folds. This exercise is carried out\non the entire dataset as well as on singer-speciﬁc datasets,\nwhere the corresponding counts of examples are provided\nin Table 3.4.2 Raga phrase detection\nOur goal is to determine whether the L and U labels (that\nwere assigned based on audio proximity) can be predicted\nby gesture alone at better than chance (i.e. based only on\nthe priors) and, if so, which kinematic features are most\nuseful in this task. Our measure of similarity is the DTW\ndistance computed between the template and test (i.e.the\nalap SDS subsequence) time series. In the context of our\nalap gesture time series, already segmented based on the\naudio phrase matching, we now compute DTW distance\nbetween the multidimensional reference and candidate un-\nder test.\nMultidimensional time series present us with some dis-\ntinct options for the distance computation. Two obvious\napproaches are DTW Iand DTW Ddepending on whether\nthe individual time series are each warped independently or\nwhether they are all forced into a single warping path [32].\nThe use of DTW Dappears meaningful for the incorpora-\ntion of the velocity and acceleration contours derived from\nthe corresponding position time series of the wrists. How-\never, it is interesting also to test with independent DTW\ncosts across the separate time series (to get an 8-dim fea-\nture vector of costs) to see if this helps reduce the effect of\nthe less informative features, if any. We term this DTW IND.\nFurther, decoupling the left and right wrists to obtained two\ndifferently warped sets of time series (DTW LR) is also per-\nfectly meaningful in the current task.\nWith DTW cost(s) as the input features, we create 5\ntrain-test splits with the uniform distribution of singers\nacross the splits. Thus every example appears once in the\ntest set. We train a logistic regression classiﬁer with L2\nregularizer and use 3-fold cross-validation within the train\nset to learn the best set of parameters.\n5. RESULTS AND DISCUSSION\n5.1 Stable-note detection\nTable 3 presents classiﬁer performance in terms of the F1\nscore for the retrieval of stable notes. We restrict ourselves\nto the set of labeled segments of duration between 0.5 s\nand 5 s, with 20897 examples in all. With 38.9% of these\ncorresponding to stable notes, we ﬁnd that the obtained\nF1 score is 65.7% when considering the overall dataset\nacross singers and ragas. Given the known high singer-\ndependence of gesturing, we also evaluate singer-speciﬁc\nclassiﬁcation with the same kinematic features, now re-\nstricted to training and validation (10-fold CV as before)\non the smaller dataset of each singer’s alaps across ragas.\nAs anticipated, we note a large variation in the F1 scores\nacross singers in Table 3 but with all values considerably\nabove chance (which equals the corresponding % Stable\nentry in row 2). While some of the variation could be at-\ntributed to the differences in distributions of labels across\nthe singers’ datasets, we observe variation even across\nsingers with similar distribution characteristics (such as AP\nand SM, for instance).\nAs for the singers with F1 scores well below the across-\nsingers stable note detection F1 score (such as the case ofProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n25Singer All AG AK AP CC MG MP NM RV SCh SM SS\nCount 20897 1242 1987 2382 2274 1822 2111 1769 1563 2069 2083 1595\n% Stable 38.9 53.6 36.7 44.1 34.0 51.5 47.3 32.8 34.7 22.5 43.6 30.1\nF1 Score (%) 65.7 81.1 63.6 69.5 68.2 72.5 71.6 65.8 65.2 60.5 75.1 49.2\nTable 3 : Overall and singer-speciﬁc performances for stable note detection from segmented gesture time series across the\nset of instances in the duration range [0.5, 5] s. Count indicates the number of instances in each singer (or overall) dataset.\nThe F1 scores in the ﬁnal row may be compared with the values in the row 2 that correspond to the chance-level F1 score.\nPhrase Like Unlike Chance Accuracy DTW D(1) DTW I(1) DTW Ind(8) DTW LR(2)\ngmD 944 827 50.2 52.2 48.6 51.8 52.4\nr/P 1035 1268 50.5 55.3 47.1 56.1 55.1\nP\\R 817 1340 53.0 65.0 45.7 65.2 65.1\nTable 4 : Classiﬁcation accuracy (%) for Like and Unlike phrase detection with gesture time series and different DTW\ndistance measures. Feature dimensionality (i.e. DTW path costs) appears in parantheses. Bold font indicates that the model\nperformance is signiﬁcantly better (p<0.005) than chance, with the chance accuracy (%) also mentioned in the table.\nSCh and SS), we note the relatively low proportions of sta-\nble notes in their data. Such behaviour can arise, for ex-\nample, when the singer makes a choice to focus more on\nmelodic movements in their alap rather than long periods\nof held notes. With a relatively low representation of their\nstable note examples in the training data, it is probable that\nidiosyncratic aspects of their stable note gestures, if any,\nwere not learned by the classiﬁer. We did not ﬁnd much\nof raga dependence in stable note detection performance.\nWe also did an analysis of tonic versus other stable notes\nto ﬁnd that the tonic notes (fewer in number overall) were\nharder to detect; this observation needs more data for a bet-\nter understanding.\n5.2 Raga-phrase detection\nTable 4 displays the Like/Unlike classiﬁcation of raga\nphrases across the alaps of all singers. We see a roughly\nequal proportion of L and U examples and therefore chance\nbaseline accuracies close to 50%. Both P\\R and r/P exhibit\ngesture classiﬁcation accuracies that are statistically better\nthan chance for all versions of DTW distance except the\nDTW Iwhich is the simple summing of independent path\ncosts across the 8 different series. In the case of the gmD\nphrase, we see a relatively small increase over chance with\nthe only signiﬁcant difference provided by the DTW LRthat\ncombines left and right wrist paths, each computed in-\ndependently of the other. A singer-based breakdown of\nthe overall accuracy showed relatively uniform behaviour\nacross singers for all the phrases except for one outlier (out\nof the 11) for each of P\\R and r/P phrases.\nWe would also like to comment on the equal proportion\nof L and U examples in our data for this task. Although\nthere is a far larger number of U instances (that is alap seg-\nments that probably do not contain the phrase of interest\nand therefore expected to return a high cost in the DTW\nsubsequence search of the audio), we found that many of\nSuppl. material: https://dap-lab.github.io/audioGestureCorrespondence/these actually led to invalid paths from pathological warp-\ning and thus were unusable candidates for this study.\n6. CONCLUSION\nThis work proposed a new approach to examining melodic\nsimilarity captured in co-singing gestures by analysing au-\ndiovisual recordings. With a new dataset of 11 singers,\nraga-characteristic phrases were proposed as a proxy for\nsimilar melodic movements within and across singers. As\nin previous work, wrist movements that accompanied the\nsolo alap singing were represented as kinematics time se-\nries. In the absence of ground-truth phrase labels for the\nalap data, we developed a pipeline for achieving the A V\nsegmentation for the chosen phrases via DTW-based au-\ndio template matching using a small set of hand-labeled\nsegments. We also considered the classiﬁcation task for\nmore generic A V segments deﬁned in a bottom-up man-\nner such as stable-note regions. Overall, our experimental\nresults indicate that there is signiﬁcant kinematic informa-\ntion linked to the selected melodic events, and conﬁrm the\nimportance of computed velocity and acceleration proﬁles\nin the gesture representation.\nA useful contribution of this work is the musicological\nquestions it encourages. Apart from the aspects already\nmentioned in the discussion of the results, we note that the\nuse of multiple phrase templates can facilitate larger exper-\nimental validation of hypotheses, such as that of Rahaim\n[5], that gestures could function to draw attention to what\nis different between two semantically close melodic pat-\nterns. Finally, several enhancements to the presented meth-\nods are possible including better-motivated movement fea-\ntures, more keypoints (elbow and hand joints) and using all\n3 camera views to include depth movement.\nThe authors S. Nadkarni and S. Roychowdhury contributed equally to this\nwork.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n267. REFERENCES\n[1] M. Clayton, “Time, gesture and attention in a khy ¯al\nperformance,” Asian Music , vol. 38, no. 2, pp. 71–96,\n2007.\n[2] L. Leante, “The lotus and the king: Imagery, gesture\nand meaning in a hindustani r ¯ag,”Ethnomusicology Fo-\nrum, vol. 18, no. 2, pp. 185–206, 2009.\n[3] Leante, “Gesture and imagery in music performance:\nPerspectives from north indian classical music,” in The\nRoutledge Companion to Music and Visual Culture .\nRoutledge, 2013, pp. 145–152.\n[4] L. Leante, “The cuckoo’s song : imagery and move-\nment in monsoon ragas,” in Monsoon feelings : a his-\ntory of emotions in the rain , I. Rajamani, M. Pernau,\nand K. R. B. Schoﬁeld, Eds. New Delhi: Niyogi\nBooks, 2018.\n[5] M. Rahaim, Musicking Bodies: Gesture and Voice in\nHindustani Music . Wesleyan University Press, 2012.\n[6] S. Paschalidou, “Effort inference and prediction by\nacoustic and movement descriptors in interactions with\nimaginary objects during dhrupad vocal improvisa-\ntion,” Wearable Technologies , vol. 3, p. e14, 2022.\n[7] M. Clayton, P. Rao, N. Shikarpur, S. Roychowdhury,\nand J. Li, “Raga classiﬁcation from vocal performances\nusing multimodal analysis,” in Proceedings of the 23rd\nInternational Society for Music Information Retrieval\nConference, ISMIR, Bengaluru, India, pp. 283-290. ,\n2022.\n[8] M. Clayton, J. Li, A. R. Clarke, M. Weinzierl,\nL. Leante, and S. Tarsitani, “Hindustani raga and singer\nclassiﬁcation using pose estimation,” 2021. [Online].\nAvailable: https://doi.org/10.17605/OSF.IO/T5BWA\n[9] L. Pearson, “Gesture and the sonic event in karnatak\nmusic,” Empirical Musicology Review , vol. 8, no. 1,\npp. 2–14, 2013.\n[10] L. Pearson and W. Pouw, “Gesture–vocal coupling\nin karnatak music performance: A neuro–bodily dis-\ntributed aesthetic entanglement,” Annals of the New\nYork Academy of Sciences , vol. 1515, no. 1, pp. 219–\n236, 2022.\n[11] W. Pouw et al. , “A kinematic-acoustic analysis of\ngesture-speech coupling in persons with aphasia,”\n2021.\n[12] R. Hennequin, A. Khlif, F. V oituret, and M. Moussal-\nlam, “Spleeter: a fast and efﬁcient music source sepa-\nration tool with pre-trained models,” Journal of Open\nSource Software , vol. 5, p. 2154, 2020.\n[13] Y . Jadoul, B. Thompson, and B. de Boer, “Introducing\nparselmouth: A python interface to praat,” Journal of\nPhonetics , vol. 71, pp. 1–15, 2018.[14] D. Bogdanov, N. Wack, E. Gómez, S. Gulati, P. Her-\nrera, O. Mayor et al. , “Essentia: an audio analysis li-\nbrary for music information retrieval,” in Proc. of the\n14th Int. Soc. for Music Information Retrieval Confer-\nence, Curitiba, Brazil, 2013.\n[15] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and\nY . Sheikh, “Openpose: Realtime multi-person 2d pose\nestimation using part afﬁnity ﬁelds,” IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence ,\nvol. 43, no. 1, pp. 172–186, 2021.\n[16] C. Larboulette and S. Gibet, “A review of computable\nexpressive descriptors of human motion,” in Proceed-\nings of the 2nd International Workshop on Movement\nand Computing , 2015, pp. 21–28.\n[17] D. J. Hermes, “V owel-onset detection,” The Journal of\nthe Acoustical Society of America , vol. 87, no. 2, pp.\n866–873, 1990.\n[18] P. Rao, T. P. Vinutha, and M. A. Rohit, “Structural seg-\nmentation of alap in dhrupad vocal concerts,” Transac-\ntions of the International Society for Music Information\nRetrieval , vol. 3, no. 1, 2020.\n[19] W. Pouw, J. de Wit, S. Bögels, M. Rasenberg,\nB. Milivojevic, and A. Ozyurek, “Semantically related\ngestures move alike: Towards a distributional seman-\ntics of gesture kinematics,” in Digital Human Mod-\neling and Applications in Health, Safety, Ergonomics\nand Risk Management. Human Body, Motion and Be-\nhavior: 12th International Conference, DHM 2021,\nHeld as Part of the 23rd HCI International Conference,\nHCII 2021, Virtual Event, July 24–29, 2021, Proceed-\nings, Part I . Springer, 2021, pp. 269–287.\n[20] K. Ganguli and P. Rao, “A study of variability in raga\nmotifs in performance contexts,” Journal of New Music\nResearch , vol. 50, pp. 1–15, 02 2021.\n[21] W. Van der Meer, Hindustani music in the 20th century .\nSpringer Science & Business Media, 2012.\n[22] K. K. Ganguli and P. Rao, “On the distributional repre-\nsentation of ragas: Experiments with allied raga pairs,”\nTransactions of the International Society for Music In-\nformation Retrieval , vol. 1, no. 1, 2018.\n[23] C. Gupta and P. Rao, “An objective assessment tool\nfor ornamentation in singing,” in Proceedings of the\nInternational Symposium of Frontiers of Research on\nSpeech and Music and Computer Music Modelling and\nRetrieval , 2011.\n[24] “Music in motion, the automatic transcription system\nfor indian music,” https://autrimncpa.wordpress.com/,\nnote = Last Accessed: 2023-04-14.\n[25] S. Kulkarni, Shyamrao Gharana . Prism Books Pvt.\nLtd, 2017, vol. 1.\n[26] M. Müller, Fundamentals of Music Processing .\nSpringer, 2015.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n27[27] T. V . C. . P. R. Wannes Meert, Kilian Hendrickx,\n“Dtaidistance (version v2),” last Accessed: 2023-04-\n14. [Online]. Available: http://doi.org/10.5281/zenodo.\n5901139\n[28] S.-T. Chiu, “Bandwidth selection for kernel density\nestimation,” The Annals of Statistics , pp. 1883–1905,\n1991.\n[29] R. C. Madeo, C. A. Lima, and S. M. Peres, “Gesture\nunit segmentation using support vector machines: seg-\nmenting gestures from rest positions,” in Proceedings\nof the 28th Annual ACM Symposium on Applied Com-\nputing , 2013, pp. 46–52.\n[30] W. Pouw and J. A. Dixon, “Quantifying gesture-speech\nsynchrony,” in the 6th gesture and speech in interaction\nconference . Universitaetsbibliothek Paderborn, 2019,\npp. 75–80.\n[31] Y . Ferstl, M. Neff, and R. McDonnell, “Express-\ngesture: Expressive gesture generation from speech\nthrough database matching,” Computer Animation and\nVirtual Worlds , vol. 32, no. 3-4, p. e2016, 2021.\n[32] M. Shokoohi-Yekta, B. Hu, H. Jin, J. Wang, and\nE. Keogh, “Generalizing dtw to the multi-dimensional\ncase requires an adaptive approach,” Data mining and\nknowledge discovery , vol. 31, pp. 1–31, 2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n28"
    },
    {
        "title": "Music Source Separation With MLP Mixing of Time, Frequency, and Channel.",
        "author": [
            "Tomoyasu Nakano",
            "Masataka Goto"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265417",
        "url": "https://doi.org/10.5281/zenodo.10265417",
        "ee": "https://zenodo.org/records/10265417/files/000100.pdf",
        "abstract": "This paper proposes a new music source separation (MSS) model based on an architecture with MLP-Mixer that leverages multilayer perceptrons (MLPs). Most of the recent MSS techniques are based on architectures with CNNs, RNNs, and attention-based transformers that take waveforms or complex spectrograms or both as inputs. For the growth of the research field, we believe it is important to study not only the current established methodologies but also diverse perspectives. Therefore, since the MLP-Mixer-based architecture has been reported to perform as well as or better than architectures with CNNs and transformers in the computer vision field despite the MLP's simple computation, we report a way to effectively apply such an architecture to MSS as a reusable insight. In this paper we propose a model called TFC-MLP, which is a variant of the MLP-Mixer architecture that preserves time-frequency positional relationships and mixes time, frequency, and channel dimensions separately, using complex spectrograms as input. The TFC-MLP was evaluated with source-to-distortion ratio (SDR) using the MUSDB18-HQ dataset. Experimental results showed that the proposed model can achieve competitive SDRs when compared with state-of-the-art MSS models.",
        "zenodo_id": 10265417,
        "dblp_key": "conf/ismir/NakanoG23",
        "keywords": [
            "music source separation",
            "MLP-Mixer",
            "multilayer perceptrons",
            "waveforms",
            "complex spectrograms",
            "attention-based transformers",
            "computer vision field",
            "reusable insight",
            "TFC-MLP",
            "MUSDB18-HQ dataset"
        ],
        "content": "MUSIC SOURCE SEPARATION WITH MLP MIXING\nOF TIME, FREQUENCY, AND CHANNEL\nTomoyasu Nakano Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\n{t.nakano, m.goto}@aist.go.jp\nABSTRACT\nThis paper proposes a new music source separation (MSS)\nmodel based on an architecture with MLP-Mixer that lever-\nages multilayer perceptrons (MLPs). Most of the recent\nMSS techniques are based on architectures with CNNs,\nRNNs, and attention-based transformers that take wave-\nforms or complex spectrograms or both as inputs. For\nthe growth of the research ﬁeld, we believe it is impor-\ntant to study not only the current established methodolo-\ngies but also diverse perspectives. Therefore, since the\nMLP-Mixer-based architecture has been reported to per-\nform as well as or better than architectures with CNNs\nand transformers in the computer vision ﬁeld despite the\nMLP’s simple computation, we report a way to effec-\ntively apply such an architecture to MSS as a reusable in-\nsight. In this paper we propose a model called TFC-MLP,\nwhich is a variant of the MLP-Mixer architecture that pre-\nserves time-frequency positional relationships and mixes\ntime, frequency, and channel dimensions separately, us-\ning complex spectrograms as input. The TFC-MLP was\nevaluated with source-to-distortion ratio (SDR) using the\nMUSDB18-HQ dataset. Experimental results showed that\nthe proposed model can achieve competitive SDRs when\ncompared with state-of-the-art MSS models.\n1. INTRODUCTION\nMusic source separation (MSS) is the task of obtaining\nindividual source signals — such as vocals, drums, and\nbass — from real music acoustic signals. This is an es-\nsential technique for various applications, including music\ninformation retrieval and music listening interfaces, where\nthe characteristics of individual sound sources are ana-\nlyzed and utilized. MSS has actually been used to add\neffects to individual source (instrument) sounds for mu-\nsic appreciation [1] and adjust their volume [1–4], to im-\nprove the cochlear implant user’s musical experience by\nadjusting the volume of preferred instruments [5], to syn-\nthesize singing voices [6], to acquire feature expressions of\nsinging voices [7], to identify singers [8], to achieve audio-\nto-lyrics alignment [9,10], to create music mashups [11], to\n© T. Nakano and M. Goto. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nT. Nakano and M. Goto, “Music source separation with MLP mixing of\ntime, frequency, and channel”, in Proc. of the 24th Int. Society for Music\nInformation Retrieval Conf., Milan, Italy, 2023.separate sources for music education [12], and to estimate\ncompatibility between vocals and accompaniment [13].\nCurrently, the mainstream approaches for MSS use\ndeep neural networks [14, 15], and their performance is\nimproving year by year. For their performance compar-\nison to measure the improvement, MUSDB18 [16] had\nbeen used as the common standard data set for the four\ntarget sound sources (V ocals, Drums, Bass, and Other).\nThen MUSDB18-HQ [17], an extended frequency band-\nwidth version of MUSDB18, was released and has been\nused for recent evaluations.\nAs for the current state-of-the-art MSS models, the top-\nranked models [18, 19] in the Music Demixing (MDX)\nChallenge 2021 [20] and the two models presented in\ntwo arXiv papers in 2022 [21, 22] have an average SDR\n(source-to-distortion ratio) over 7 dB for the four sources\nwhen using MUSDB18-HQ as training, validation, and test\ndata. These models are explained in the next section.\nSuch deep MSS models can be classiﬁed in terms of the\ntype of input and output used for separation and the type\nof architecture. The input and output of the models are se-\nlected from waveforms, amplitude spectrograms, complex\nspectrograms, phase spectrograms, etc. The architecture\nof the models is mainly selected from ResNet, DenseNet,\nU-Net, and Transformer and is used with layers of Con-\nvolutional Neural Networks (CNNs) or Recurrent Neural\nNetworks (RNNs). Simpler architectures based on multi-\nlayer perceptrons (MLPs), however, have not been used in\nstate-of-the-art MSS models.\nMeanwhile, in the ﬁeld of computer vision, high-\nperformance architectures based on MLPs have recently\nbeen proposed and reported to perform as well as or bet-\nter than architectures using CNNs or transformers [23,24].\nIn addition, those simpler MLP-centric architectures have\nalso been applied in audio-related research, with cases of\nsinging voice synthesis [25] and speech enhancement [26].\nHowever, such MLP-centric architectures have not been\napplied in the MSS domain, though fully connected layers\nand MLPs have been used for linear transformation such\nas embedding and expansion. Since we believe that new\nperspectives, in addition to the development of established\nmethodologies, are important for the advancement of the\nresearch ﬁeld, this paper investigates how MLP-based ar-\nchitectures can be effectively leveraged for MSS.\nAs such a modern MLP-centric high-performance ar-\nchitecture, Tolstikhin et al. proposed the MLP-Mixer [23]\nthat applies MLPs to a Tp×Cpmatrix to estimate the840Table 1 . SDRs in MUSDB18-HQ for the state-of-the-art models and our proposed TFC-MLP. The “Avg.” column means\nthe average of the SDR results for the four sources. The “Per-source” column means that the per-source adjustment/tuning\nhas been implemented. The “Extra” column indicates the number of songs added as extra training data, and †means that\nonly mixed sounds were added. Models with “*” are evaluated in MUSDB18 [16], and SDRs for models with “**” are\nrecalculated in MUSDB18-HQ using the pretrained model. A bold font indicates the maximum value at each source.\nModel Test SDR in dB\nName Per-source Extra Avg. V ocals Drums Bass Other\nKUIELab-MDX-Net (w/o Demucs) [18]* ✓ 7.28 8.91 7.07 7.33 5.81\nTFC-MLP (ours) 7.30 8.91 7.18 6.96 6.14\nKUIELab-MDX-Net [18]** ✓ 7.48 8.97 7.20 7.83 5.90\nHybrid Transformer Demucs [22] 7.52 7.93 7.94 8.48 5.72\nHybrid Demucs [19] 7.64 8.35 8.12 8.43 5.65\nBand-Split RNN [21] ✓ 8.24 10.01 9.01 7.22 6.70\nTFC-MLP (ours) 120 7.78 9.68 7.75 7.23 6.46\nHybrid Demucs [19] 800 8.34 8.75 9.31 9.13 6.18\nHybrid Transformer Demucs [22] 150 8.49 8.56 9.51 9.76 6.13\nHybrid Transformer Demucs [22] 800 8.80 8.93 10.05 9.78 6.42\nBand-Split RNN [21] ✓ 1750†8.97 10.47 10.15 8.16 7.08\nHybrid Transformer Demucs [22] ✓ 800 9.00 9.20 10.08 10.39 6.32\nSparse HT Demucs [22] ✓ 800 9.27 9.37 10.83 10.47 6.41\nclass of an image. The matrix is obtained by dividing the\nimage into patches and embedding each patch into a Tp-\ndimensional vector, which is called a token, and the num-\nber of tokens is the channel dimension Cp. Here, a token-\nmixing MLP , a full connection within an individual token,\nand a channel-mixing MLP , a full connection between to-\nkens ( i.e.,channel direction), are applied alternately. Given\nsufﬁcient training data, MLP-Mixer was shown to perform\nas well as or better than CNNs or transformers.\nBy extending this MLP-Mixer, for the image recon-\nstruction task, Mansour et al. proposed the Image-to-Image\nMixer [24] that performs better when trained with fewer\nimages than the original MLP-Mixer. The Image-to-Image\nMixer transforms images as a 3D tensor ( W×H×C)\ninstead of the 2D matrix ( Tp×Cp) and preserves the rel-\native positions of patches to induce a bias towards natu-\nral images. In other words, the token-mixing MLP and\nthe channel-mixing MLP are split into three processes\nofwidth-mixing MLP ,height-mixing MLP andchannel-\nmixing MLP . This also keeps the total number of trainable\nparameters low, since the size of each dimension of the 2D\nmatrix ( i.e.,TpandCp) obtained by transformation from\nthe 3D tensor is relatively large.\nIn investigating such MLP-based architectures in the\nMSS domain, we decided to use a complex spectrogram,\nwhich is a reasonable representation for MSS, as the input.\nSince the size of its complex time-frequency representation\nis larger than the size of typical images in the computer\nvision domain, we apply the memory-efﬁcient Image-to-\nImage Mixer to MSS and report its experimental results.\n2. RELATED WORK\nAs described in Section 1, the state-of-the-art models [18,\n19, 21, 22] in the MSS study show that the average SDR\nscore for the four source separations exceeds 7 dB in theevaluation using MUSDB18-HQ. The SDRs for each of the\nfour sources based on these models are shown in Table 1.\nKUIELab-MDX-Net was proposed by Kim et al. [18].\nIt is an architecture that combines an extended version\nof TFC-TDF-U-Net [27], which separates in the time-\nfrequency domain ( i.e., complex spectrogram), and De-\nmucs [28], which separates in the time domain ( i.e.,wave-\nform). Each source signal iseparated by those sub-\nnetworks is mixed using source-dependent weights wi.\nSpeciﬁcally, wiwas set to 0.5, 0.5, 0.7, and 0.9 for bass,\ndrums, other, and vocals in MDX Challenge 2021 [20]1.\nIn KUIELab-MDX-Net, to improve performance, a mech-\nanism called Mixer was used to remix the music acoustic\nsignal with the separated signal by using the 1x1 convo-\nlution, and different FFT frame sizes were used for each\nsound source by applying a frequency cut-off trick [20].\nTFC-TDF-U-Net used in the KUIELab-MDX-Net is\na variant of U-Net architecture that combines the time-\nfrequency convolutions (TFC) block with the time-\ndistributed fully-connected networks (TDF) block. Here,\nTFC is a dense block of 2D CNNs, and TDF is a block that\nextracts nonlocal features along the frequency axis, such as\ncorrelations between harmonics, by fully connecting the\nentire frequency range of a single frame of the spectro-\ngram. TDF was inspired by the Frequency Transformation\nBlock (FTB) proposed by Yin et al. [29] and was intro-\nduced speciﬁcally to help separate singing voices [27]. As\nthe other component of KUIELab-MDX-Net, Demucs [28]\nis a U-Net encoder/decoder structure with waveforms as\ninput and BiLSTM applied to the innermost layer between\nthe encoder and decoder to provide long-range context.\nHybrid Demucs was proposed by Défossez et al. [19].\nIt is a bi-U-Net encoder-decoder model that combines 1D\nconvolution in the time domain and along the frequency\n1https://github.com/kuielab/mdx-net/blob/\nLeaderboard_A/README_SUBMISSION.mdProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n841axis in the complex time-frequency domain. BiLSTM and\nlocal attention were used in the innermost layer, and resid-\nual branches, group normalization [30] and GELU were\nintroduced. In addition, better generalization and stability\nwere achieved by penalizing the largest singular value in\neach layer [31], and overall performance was improved by\nbagging multiple models.\nBand-Split RNN was proposed by Luo et al. [21]. It is\na state-of-the-art spectrogram-based model that uses com-\nplex spectrograms as input and output. By splitting the\ncomplex spectrogram input into subbands speciﬁcally de-\nsigned for each sound source, the intrinsic properties and\npatterns of each source signal are utilized. For a 3D ten-\nsor representing the time dimension, frequency dimension,\nand subband dimension, similar to the Dual-path RNN\n[32], a sequence-level RNN is ﬁrst applied across the time\ndimension, then a band-level RNN is applied across the\nband dimension. In fact, for V ocals, results showed that the\nmodiﬁed utterance-level SDR can be improved by more\nthan 2 dB by setting the bandwidth appropriately. In ad-\ndition, semi-supervised ﬁne tuning was performed by us-\ning pseudo-targets separated from the mixtures using a pre-\ntraining model in order to make effective use of songs with\nonly mixtures (1750 songs). The Band-Split RNN cur-\nrently achieves a state-of-the-art SDR of 8.24 dB in the\nevaluation using only MUSDB18-HQ as training data.\nHybrid Transformer Demucs was proposed by Rouard\net al. [22]. The innermost layer, called the bottleneck,\nof the Hybrid Demucs [19] is replaced by a cross-domain\ntransformer encoder ( i.e.,time domain and time-frequency\ndomain) that uses self-attention within each domain and\ncross-attention across domains. Compared to the Hy-\nbrid Demucs, the performance is lower when trained with\nMUSDB18-HQ only, but the SDR was 0.46 dB better\nwhen 800 additional training songs were used. Sparse HT\nDemucs [22], which extended the receptive ﬁeld using a\nsparse attention kernel, achieved a state-of-the-art result of\n9.27 dB SDR by ﬁne-tuning for each source.\nAs described above, the MLP-centric architecture has\nnever been applied in state-of-the-art MSS models where\nthe average SDR exceeds 7 dB on MUSDB18-HQ.\n3. TFC-MLP\nThis paper proposes Time-Frequency-Channel-MLP (TFC-\nMLP) , which is a model that leverages the Image-to-Image\nMixer architecture [24] to separate music sources using a\ncomplex spectrogram as input. This is realized by replac-\ning the height, width, and color (RGB) in the image with\nthe time, frequency, and channel in the complex spectro-\ngram, respectively. In other words, TFC-MLP has a struc-\nture that alternates mixing in the time, frequency, and chan-\nnel dimensions. In this way, we expect to be able to take\ninto account the nonlocal structure. Especially with respect\nto the frequency dimension, we expect to extract nonlocal\nrelationships along the frequency axis, such as harmonic\nstructures, by connecting the entire frequency range of the\nspectrogram, as in the FTB [29] and the TDF [27].\nThe process of the TFC-MLP model is shown in Fig-ure 1. The complex spectrogram XSTFT∈CF0×T0×2of\na 2-channel (stereo) mixture signal is ﬁrst converted to a\n4-channel 3D tensor XCaC∈RF0×T0×4with the real and\nimaginary parts represented as channels, a.k.a. complex-\nas-channels (CaC) [27]. Here T0is a ﬁxed length. Then, as\nwith the MLP-Mixer and Image-to-Image Mixer as well as\nthe Vision Transformer (ViT) [33], the patch embedding is\nﬁrst performed using Clinear weights of size PT×PF×4\n(Figure 2). This reduces the frequency dimension F0to\nF0/PFand the time dimension T0toT0/PT, which re-\nduces the matrix size and memory consumption in the fol-\nlowing frequency-mixing MLP and time-mixing MLP. To\ncompensate for the information loss due to decreasing the\nresolution in the time-frequency plane and the loss of con-\ntinuity in the time-frequency direction, we increase the di-\nmension in the channel direction.\nThe embedded tensor then passes through NMLP-\nMixer layers. As shown in Figure 3, each MLP-Mixer\nlayer mixes the tensor with the frequency dimension, then\nthe time dimension, and ﬁnally the channel dimension.\nSuch mixing is passed through an MLP consisting of a lin-\near layer, a GELU nonlinearity, and another linear layer,\nand output without changing the tensor size. The dimen-\nsion of the tensor at the input/output layer of the MLP is\nkept constant, and the dimension at the hidden layer is ad-\njusted by multiplying it by a factor fdepending on the\ninput dimension. Skip connections and channel layer nor-\nmalization have also been added to help with the optimiza-\ntion. Here, following the implementation of the Image-to-\nImage Mixer [24], skip connections are placed before and\nafter the two mixing steps of frequency and time (“Type\nA”). In addition to that, a version with skip connection and\nlayer normalization added before time-mixing MLP was\nalso implemented (“Type B”). After mixing the time, fre-\nquency, and channel dimensions, patch expansion is used\nto restore the number of time and frequency dimensions for\ninverse transformation into waveforms and also to match\nthe channel dimension to the number of separated sources.\n4. EV ALUATION\nThe proposed TFC-MLP was evaluated using the\nMUSDB18-HQ dataset [17]. 86 songs were used as train-\ning data, 14 songs as validation data, and 50 songs as test\ndata. The music acoustic signals were stereo with a sam-\npling frequency of 44.1 kHz, and four sound sources –\n“V ocals,” “Drums,” “Bass,” and “Other” – were used for\nseparation. Separation performance was evaluated by cal-\nculating SDR using the museval Python package2. As in\nmost previous studies ( [19, 21], etc.), the SDR of each\nsource was calculated by taking the median values over all\n1-second segments of each song to obtain the SDR of the\ntrack and then taking the median of all tracks.\n4.1 Experimental setting\nThe proposed model was optimized using Adam [34] for\nthe L1 loss between its separated signals and the ground-\n2https://github.com/sigsep/sigsep-mus-evalProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8424 ch\nStereo signal\nCaC spectrogramTFC-MLP\nSeparated source signalsDrums\nBass\nOtherV ocals\nPatch\nembeddingPatch\nexpansionN x (Mixer layer)\nT  framesF\nbins0\n0\nFigure 1 . Overview of the TFC-MLP model.\nT  framesTFCF\nbins4 ch\n4 ch\nC linear weightsPatch embedding\nP\nPTF0\n0\nInput Output...\nFigure 2 . Patch embedding. After dividing the CaC tensor\nof sizeF0×T0×4into patches of size PF×PT×4, each\npatch was linearly transformed into 1×1×Cto obtain a\n3D tensor of size F0/PF×T0/PT×C(i.e.,F×T×C).\nThis can be implemented as a nonoverlapping CNN.\ntruth source signals. The Adam parameters were set as\nno weight decay, learning rate 0.0003, β1= 0.9, and\nβ2= 0.999. The waveform for calculating the CaC spec-\ntrogram to be input to the model was normalized so that\nthe mean amplitude of the music acoustic signal was 0 and\nthe standard deviation was 1. The training was distributed\nacross multiple GPUs, with a batch size of 4 on each GPU.\nIn training, we used data augmentation techniques [19], in-\ncluding pitch shift and tempo stretch, randomly swapping\nchannels, random sign ﬂip, random scaling of amplitude,\nand remixing of stems within one batch.\n4.2 Base hyperparameters\nDue to the many hyperparameters required for the building\nof the proposed TFC-MLP model, hyperparameters related\nto the short-time Fourier transform (STFT) were ﬁrst deter-\nmined through preliminary experiments. To obtain XCaC,\nthe STFT frame size ( i.e.,FFT size) was chosen as 4096,\nwhich had the highest SDR when compared among 512,\n1024, 2048, and 4096. As Kim et al. [18] also stated,\nthe performance tended to increase with larger FFT size.\nTherefore the frequency dimension F0is 2048. Related to\nthe FFT size, the STFT hop size was investigated among\n128, 256, 512, and 1024, and 1024 was selected. Also re-\nlated to the FFT size and STFT hop size, the number of\ntime frames T0in the complex spectrogram was selected\nas 128 from 32, 64, 128, 256, and 512.\nThe number of time frames T0of 128 based on an FFT\nsize of 4096 and an STFT hop size of 1024 is input to the\nmodel, thus the waveform size required for this is equal\ntoTw= 1024×128−1(about 3 seconds). Therefore,\nit is necessary to train the model while cutting out an ap-Mixer layer (Type B)\nMixing MLPs\nFrequency-mixing MLP\nTime-mixing MLP\nChannel-mixing MLP\nInput dim. Hidden dim. Output dim.skip connections skip connections skip connectionsskip connections skip connections\nLayer\nnormFreq.\nmixingTime\nmixingLayer\nnormChannel\nmixing\nLayer\nnormLayer\nnormLayer\nnormFreq.\nmixingTime\nmixingChannel\nmixingMixer layer (Type A)\nF F f \u0001F\nT T f \u0001T\nC C f \u0001CFully\nconnectedFully\nconnectedGELU\nFigure 3 . The Mixer layer contains one frequency-mixing\nMLP, one time-mixing MLP, and one channel-mixing\nMLP. Before each MLP, a transposition is performed to\napply the MLP to the frequency dimension F, the time di-\nmensionT, and the channel dimension C. Transposition\nis also performed before every layer normalization to nor-\nmalize along the channel dimension C.\nproximately 3-second segment of the waveform, and the\nshift size of 16384 (about 0.37 seconds) was used. Luo\net al. [21] found that the shorter the shift, the better the\nmodiﬁed utterance-level SDR, and they used 0.5 seconds.\nThe above 16384 is short enough compared to 0.5 seconds\nand can be considered sufﬁcient in this study.\nAs hyperparameters regarding the model, the dimen-\nsionsCof the patch embedding were investigated by us.\nSpeciﬁcally, 128 and 256 were investigated and 256 was\nselected. As hyperparameters related to the Mixer layer, 8\nand16were investigated as the number of layers Nand16\nwas selected, and the parameter ffor adjusting the num-\nber of dimensions of the hidden layer of each MLP was\nselected as 4 from 1, 2, and 4. Dropout in the MLP [25]\nand skip connections before and after the Mixer layer were\nalso investigated, but they have not yielded better results.\n4.3 Experiment\nUsing the set of hyperparameters determined in Section 4.2\nas a basis, we report the SDR scores obtained under the fol-\nlowing various conditions for the other hyperparameters.\n• Investigation of the results of complex spectrogram\nloss [21].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n843Table 2 . SDRs for different hyperparameters of the proposed model. The “Seed” column indicates random seeds. In the\n“Loss” column, “W” indicates that only waveform loss was used, and “W+S” indicates that complex spectral loss was\nadded. The “Epoch” column indicates the number of epochs with the smallest validation loss and the speciﬁed number\nof epochs, where “*” means that validation loss was not considered. Note that “ †” means the cases where the number of\nepochs with the smallest validation loss did not change when training beyond 200 epochs ( i.e.,the same model was used\nfor the evaluation). A bold font indicates the maximum value at each source, both without and with extra training songs.\nTFC-MLP Test SDR in dB\nType Seed (PF,PT)Loss Epoch Extra Avg. V ocals Drums Bass Other\nA seed 1 (4,4) W 134†/ 200 7.30 8.91 7.18 6.96 6.14\nA seed 2 (4,4) W 166†/ 200 7.26 8.84 7.07 6.89 6.22\nA seed 3 (4,4) W 109 / 200 6.97 8.32 6.98 6.57 5.99\nA seed 3 (4,4) W 283 / 300 6.93 8.49 6.80 6.55 5.87\nB seed 1 (4,4) W 190 / 200 7.17 8.92 6.95 6.83 5.96\nB seed 2 (4,4) W 191 / 200 7.02 8.59 6.78 6.68 6.01\nB seed 3 (4,4) W 157 / 200 6.95 8.56 6.58 6.73 5.91\nA seed 1 (2,2) W 189 / 200 7.13 8.58 7.02 6.99 5.91\nA seed 1 (1,1) W 175 / 200 6.38 7.40 6.33 6.49 5.30\nA seed 1 (4,4) W+S 197 / 200 6.83 8.76 6.60 6.14 5.83\nA seed 1 (4,4) W+S 253 / 300 6.72 8.39 6.69 6.02 5.79\nA seed 1 (4,4) W 142 / 200 120 7.71 9.42 7.66 7.37 6.39\nA seed 1 (4,4) W 200* / 200 120 7.78 9.68 7.75 7.23 6.46\nTable 3 . The number of model parameters and the average\nReal Time Factor (RTF) value. The Hybrid Transformer\nDemucs is denoted as “HT Demucs”. Column “GPU”\nshows the RTF with a single GPU, and column “CPU”\nshows the RTF under the condition without a GPU.\nModel GPU CPU Params.\nHybrid Demucs 0.14 1.87 83.9 M\nHT Demucs 0.17 2.55 26.9 M\nTFC-MLP 0.51 12.28 43.2 M\n• Investigation of patch size ( PF,PT) between ( 4,4),\n(2,2), and (1,1), using ( 4,4) as the basis, halving\nthe FFT size and time dimension Tfor (2,2), and\nhalving it further for ( 1,1). For (1,1), the STFT hop\nsize was set to 512 since the FFT size is 1024.\n• The number of epochs for training was set to 200\nor 300, and models with the smallest validation loss\nwithin each epoch-condition were evaluated.\n• The results of adding 120 full-length songs (sung in\nEnglish) to the training data were evaluated.\n• For the basic parameter condition, we trained three\ntimes with different random seeds.\nIn the test phase, the model with the smallest validation\nloss was used for evaluation in each training condition. The\nsignal separated by the proposed model was divided into\nsegments of ﬁxed length Twwith shift width Tw/4, which\nwere weighted overlap-added to obtain the ﬁnal signal. In\naddition, the shift trick [28] was performed 10 times.\n4.4 Results and discussions\nIn addition to the evaluation of SDRs using the\nMUSDB18-HQ test set, the number of parameters and theReal Time Factor (RTF) will also be discussed, as ﬁle size\nand the time required for separation may be important in\nsome situations depending on how the MSS model is used.\n4.4.1 SDRs\nThe experimental results are shown in Table 2. The high-\nest SDR score, up to 7.30 dB, was obtained for the Type\nA model using the patch size of ( 4,4) and waveform L1\nloss in addition to the base hyperparameters. This was not\nsigniﬁcantly higher than the state-of-the-art values shown\nin Table 1, but close performance was achieved.\nFocusing on the results for each source with respect to\nour TFC-MLP, as shown in Table 1, the SDRs for V ocals\nand Other were 8.91 dB and 6.14 dB, respectively, which\nwere higher than the 8.35 dB and 5.65 dB SDRs for the\nHybrid Demucs and the 7.93 dB and 5.72 dB SDRs for\nthe Hybrid Transformer Demucs. This model’s SDR score\nfor Other also exceeded the one obtained using KUIELab-\nMDX-Net, 5.90. Here, in comparison to the KUIELab-\nMDX-Net results without waveform information ( i.e.,ex-\ncluding processing by Demucs), it is possible that similar\nor better performance was obtained for Drums and V ocals,\nalthough an exact comparison cannot be made because the\ntest data is different ( i.e.,MUSDB18 was used). In com-\nparison to the Band-Split RNN results, TFC-MLP could\nnot yield a higher SDR for all sound sources. However,\nthe current TFC-MLP does not include a source-speciﬁc\nframework, so addressing this issue is a future challenge.\nAs for the patch size, the FFT size and other condi-\ntions were different due to memory capacity. Therefore, al-\nthough exact comparisons are not possible, the best results\nwere obtained for ( 4,4) in the current results. However, ad-\nditional study is needed for ( 2,2), since it gave results sim-\nilar to those given by ( 4,4). As for complex spectrogramProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n844loss, its SDR was slightly lower than that of all conditions\nusing only waveform loss. Not only comparisons using the\nreal and imaginary parts of the complex numbers, but also\nlosses based on amplitude and phase could be considered.\nWe also showed that the performance of the models was\nfurther improved by using additional training data. As\nshown in Table 1, compared to the current world’s best\nmodel Sparse HT Demucs with 800 songs added as the\ntraining data, we obtained competitive results with an SDR\nof 9.68 dB for the V ocals and 6.46 dB for the Others.\n4.4.2 RTF and the number of parameters\nAs comparison, models were trained for each of the Hy-\nbrid Demucs and Hybrid Transformer Demucs. Based\non the published source codes, a model parameter setting\n“80a68df8”3was used for Hybrid Demucs, and the default\nparameter setting was used for Hybrid Transformer De-\nmucs. The same implementation for audio synthesis was\nused for all TFC-MLP, Hybrid Demucs, and Hybrid Trans-\nformer Demucs. We used the 50 songs in the MUSDB18-\nHQ test set to obtain the average of their RTFs.\nTable 3 shows the results. The RTF values of TFC-MLP\nwere slower than the other two models. TFC-MLP had an\nRTF lower than 1.0 (faster than real time) when a GPU\nwas used, but the computation time was long without GPU.\nThis could be due to the large size of the time-frequency\nspectrogram. As for the number of model parameters,\nTFC-MLP had more parameters than Hybrid Transformer\nDemucs, but fewer parameters than Hybrid Demucs.\n4.4.3 Comparison with the state-of-the-art models\nThe proposed TFC-MLP model has some similarities to\nthe state-of-the-art MSS models, which potentially have\nled to the competitive performance achieved.\n• The frequency-mixing MLP is similar to the full\nconnection of frequency dimensions in TDF [27]\nand the band-level RNN applied across band dimen-\nsions in Band-Split RNN [21].\n• The time-mixing MLP is similar to the sequence-\nlevel RNNs applied across time dimensions in Band-\nSplit RNN [21].\n• Increasing the number of the channel dimensions\nthrough patch embedding and increasing the number\nof hidden layer dimensions in MLP are techniques\nthat are usually used regardless of MSS. For MSS,\nthe improvement is potentially related to the increase\nin channel dimensionality in the encoder part, such\nas Hybrid Transformer Demucs [22].\n• The extra training data improved performance in the\nstate-of-the-art models, and we conﬁrmed the per-\nformance improvement with extra training data in\nTFC-MLP as well.\nOn the other hand, the following are included in the\nexisting state-of-the-art MSS models but not currently in-\ncluded in our TFC-MLP. They have the potential to im-\nprove performance when applied in the future.\n3https://github.com/facebookresearch/demucs/\nblob/main/docs/training.md• As presented by Défossez et al. [19] and Kim\net al. [18], a hybrid approach that also considers\nwaveforms could improve performance.\n• As Kim et al. [18], Luo et al. [21], and Rouard\net al. [22] have shown, the introduction of source-\nspeciﬁc techniques, such as band splitting, could im-\nprove separation performance.\n• Deep learning techniques such as model selection\nmethods ( e.g., exponential moving average), train-\ning stabilization ( e.g., singular value decomposition\nand sparsiﬁcation), and the introduction of a learning\nrate scheduler could further improve performance.\nFinally, to the best of our knowledge, there are no stud-\nies that mix the channel dimension with the time and fre-\nquency dimensions as in TFC-MLP. Such a mixer layer\nused in the TFC-MLP architecture has the advantage of re-\nducing the overall memory usage compared to the original\nMLP-Mixer, just as the Image-to-Image Mixer reduced the\nmemory usage. This reusable insight of mixing the chan-\nnel dimension separately could be useful for other studies\nthat have dealt with the time and frequency dimensions so\nfar, but could be extended to the channel dimension.\n4.4.4 Future directions\nAs future work, we plan to improve the performance of\nTFC-MLP by incorporating the ideas discussed above and\nfurther exploring more optimal hyperparameters. For ex-\nample, increasing the FFT window size by utilizing fre-\nquency cut-off trick [20] is expected to improve the perfor-\nmance. Automatic optimization of hyperparameters could\nalso be incorporated.\nFuture work will also include the visualization of the\ninside of TFC-MLP for analysis. We could visualize the\nlinear weights used in the patch embedding by converting\nthem back to complex numbers and then calculating their\namplitudes. The visualized results could allow us to ana-\nlyze what local patterns the model is focusing on. How-\never, when we tried it, it was difﬁcult to understand the\nbehavior of the mixer layer due to the mixing of real and\nimaginary parts during the patch embedding. We are there-\nfore interested in using the amplitude and phase (group de-\nlay) instead of the real and imaginary parts so that we can\nanalyze the model in a more comprehensive way.\n5. CONCLUSION\nThis paper has described a new MSS architecture called\nTFC-MLP that uses complex spectrograms as input. Our\ncontributions are summarized as follows:\n(1) We proposed a simpler MLP-centric MSS architecture\nthat achieves competitive performance compared to\nstate-of-the-art models.\n(2) We reported on some hyperparameter searches that\nwill be useful for other researchers exploring this\ntype of architecture.\n(3) We discussed the similarities and differences between\nthe state-of-the-art models and TFC-MLP, and sug-\ngested directions for future research.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8456. ACKNOWLEDGMENTS\nThis work was supported in part by JST CREST Grant\nNumber JPMJCR20D4 and JSPS KAKENHI Grant Num-\nber JP21H04917, Japan.\n7. REFERENCES\n[1] J. Woodruff, B. Pardo, and R. Dannenberg, “Remixing\nstereo music with score-informed source separation,”\ninProc. the 7th International Conference on Music In-\nformation Retrieval (ISMIR 2006) , 2006, pp. 314–319.\n[2] O. Gillet and G. Richard, “Extraction and remixing of\ndrum tracks from polyphonic music signals,” in Proc.\nthe 2005 IEEE Workshop on Applications of Signal\nProcessing to Audio and Acoustics (WASPAA 2005) ,\n2005, pp. 315–318.\n[3] K. Yoshii, M. Goto, and H. G. Okuno, “INTER:D: A\ndrum sound equalizer for controlling volume and tim-\nbre of drums,” in Proc. the 2nd European Workshop\non the Integration of Knowledge, Semantic and Digi-\ntal Media Technologies (EWIMT 2005) , 2005, pp. 205–\n212.\n[4] K. Itoyama, M. Goto, K. Komatani, T. Ogata, and H. G.\nOkuno, “Instrument equalizer for query-by-example\nretrieval: Improving sound source separation based on\nintegrated harmonic and inharmonic models,” in Proc.\nthe 9th International Conference of Music Information\nRetrieval (ISMIR 2008) , 2008, pp. 133–138.\n[5] J. Pons, J. Janer, T. Rode, and W. Nogueira, “Remixing\nmusic using source separation algorithms to improve\nthe musical experience of cochlear implant users,” The\nJournal of the Acoustical Society of America , vol. 140,\nno. 6, pp. 4338–4349, 2016.\n[6] Y . Ren, X. Tan, T. Qin, J. Luan, Z. Zhao, and T.-Y . Liu,\n“DeepSinger: Singing voice synthesis with data mined\nfrom the web,” in Proc. the 2020 ACM SIGKDD In-\nternational Conference on Knowledge Discovery and\nData Mining (KDD 2020) , 2020, pp. 1979–1989.\n[7] H. Yakura, K. Watanabe, and M. Goto, “Self-\nsupervised contrastive learning for singing voices,”\nIEEE/ACM Trans. on Audio Speech and Language Pro-\ncessing , vol. 30, pp. 1614–1623, 2022.\n[8] B. Sharma, R. K. Das, and H. Li, “On the importance\nof audio-source separation for singer identiﬁcation in\npolyphonic music,” in Proc. the 20th Annual Confer-\nence of the International Speech Communication As-\nsociation (Interspeech 2019) , 2019, pp. 2020–2024.\n[9] H. Fujihara, M. Goto, J. Ogata, and H. G. Okuno,\n“LyricSynchronizer: Automatic synchronization sys-\ntem between musical audio signals and lyrics,” IEEE\nJournal of Selected Topics in Signal Processing , vol. 5,\nno. 6, pp. 1252–1261, 2011.[10] L. Ou, X. Gu, and Y . Wang, “Transfer learning of\nwav2vec 2.0 for automatic lyric transcription,” in Proc.\nthe 23rd International Society for Music Information\nRetrieval Conference (ISMIR 2022) , 2022, pp. 187–\n195.\n[11] J. Huang, J.-C. Wang, J. B. L. Smith, X. Song, and\nY . Wang, “Modeling the compatibility of stem tracks\nto generate music mashups,” in Proc. the Thirty-Fifth\nAAAI Conference on Artiﬁcial Intelligence (AAAI-21) ,\n2021, pp. 187–195.\n[12] E. Cano, G. Schuller, and C. Dittmar, “Pitch-informed\nsolo and accompaniment separation towards its use in\nmusic education applications,” EURASIP Journal on\nAdvances in Signal Processing , vol. 2014, no. 23, pp.\n1–19, 2014.\n[13] T. Nakatsuka, K. Watanabe, Y . Koyama, M. Hamasaki,\nM. Goto, and S. Morishima, “V ocal-accompaniment\ncompatibility estimation using self-supervised and\njoint-embedding techniques,” IEEE Access , vol. 9, pp.\n101 994–102 003, 2021.\n[14] Z. Raﬁi, A. Liutkus, F.-R. Stöter, S. I. Mimilakis,\nD. FitzGerald, and B. Pardo, “An overview of lead\nand accompaniment separation in music,” IEEE/ACM\nTrans. on Audio Speech and Language Processing ,\nvol. 26, no. 8, pp. 1307–1335, 2018.\n[15] C. Gupta, H. Li, and M. Goto, “Deep learning ap-\nproaches in topics of singing information processing,”\nIEEE/ACM Trans. on Audio Speech and Language Pro-\ncessing , vol. 30, pp. 2422–2451, 2022.\n[16] Z. Raﬁi, A. Liutkus, F.-R. Stöter, S. I. Mimilakis, and\nR. Bittner, “The MUSDB18 corpus for music separa-\ntion,” https://doi.org/10.5281/zenodo.1117372.\n[17] ——, “MUSDB18-HQ - an uncompressed version of\nMUSDB18,” https://doi.org/10.5281/zenodo.3338373.\n[18] M. Kim, W. Choi, J. Chung, D. Lee, and S. Jung,\n“KUIELab-MDX-Net: A two-stream neural network\nfor music demixing,” in Proc. Music Demixing Work-\nshop 2021 (MDX 2021) , 2021, pp. 1–7.\n[19] A. Défossez, “Hybrid spectrogram and waveform\nsource separation,” in Proc. Music Demixing Workshop\n2021 (MDX 2021) , 2021, pp. 1–11.\n[20] Y . Mitsufuji, G. Fabbro, S. Uhlich, F.-R. Stöter, A. Dé-\nfossez, M. Kim, W. Choi, C.-Y . Yu, and K.-W. Cheuk,\n“Music demixing challenge 2021,” Front. Sig. Proc. ,\n2022.\n[21] Y . Luo and J. Yu, “Music source separation with band-\nsplit rnn,” CoRR, arXiv:2209.15174 , pp. 1–10, 2022.\n[22] S. Rouard, F. Massa, and A. Défossez, “Hybrid\ntransformers for music source separation,” CoRR,\narXiv:2211.08553 , pp. 1–5, 2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n846[23] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer,\nX. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Key-\nsers, J. Uszkoreit, M. Lucic, and A. Dosovitskiy,\n“MLP-Mixer: An all-MLP architecture for vision,” in\nProc. the 35th Conference on Neural Information Pro-\ncessing Systems (NeurIPS 2021) , 2021, pp. 24 261–\n24 272.\n[24] Y . Mansour, K. Lin, and R. Heckel, “Image-to-\nImage MLP-Mixer for image reconstruction,” CoRR,\narXiv:2202.02018 , pp. 1–15, 2022.\n[25] J. Tae, H. Kim, and Y . Lee, “MLP Singer: Towards\nrapid parallel korean singing voice synthesis,” in Proc.\nthe 2021 IEEE International Workshop on Machine\nLearning for Signal Processing (MLSP 2021) , 2021,\npp. 1–6.\n[26] H. Song, M. Kim, and J. W. Shin, “Speech enhance-\nment using mlp-based architecture with convolutional\ntoken mixing module and squeeze-and-excitation net-\nwork,” IEEE Access , vol. 10, pp. 119 283–119 289,\n2022.\n[27] W. Choi, M. Kim, J. Chung, D. Lee, and S. Jung, “In-\nvestigating U-Nets with various intermediate blocks\nfor spectrogram-based singing voice separation,” in\nProc. the 21st International Society for Music Infor-\nmation Retrieval Conference (ISMIR 2020) , 2020, pp.\n192–198.\n[28] A. Défossez, N. Usunier, L. Bottou, and F. R. Bach,\n“Music source separation in the waveform domain,”\nCoRR, arXiv:1911.13254 , pp. 1–16, 2021.\n[29] D. Yin, C. Luo, Z. Xiong, and W. Zeng, “PHASEN: A\nphase-and-harmonics-aware speech enhancement net-\nwork,” in Proc. the The Thirty-Fourth AAAI Confer-\nence on Artiﬁcial Intelligence (AAAI-20) , 2020, pp.\n9458–9465.\n[30] Y . Wu and K. He, “Group normalization,” in Proc. the\n15th European Conference on Computer Vision (ECCV\n2018) , 2018, pp. 3–19.\n[31] Y . Yoshida and T. Miyato, “Spectral norm regulariza-\ntion for improving the generalizability of deep learn-\ning,” CoRR, arXiv:1705.10941 , pp. 1–12, 2017.\n[32] Y . Luo, Z. Chen, and T. Yoshioka, “Dual-path RNN:\nefﬁcient long sequence modeling for time-domain\nsingle-channel speech separation,” in Proc. the 2020\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP 2020) , 2020, pp. 46—\n-50.\n[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” in Proc. the\nNinth International Conference on Learning Represen-\ntations (ICLR 2021) , 2021.[34] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” in Proc. the 3rd International Con-\nference on Learning Representations (ICLR 2015) ,\n2015, pp. 1–15.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n847"
    },
    {
        "title": "Human-AI Music Creation: Understanding the Perceptions and Experiences of Music Creators for Ethical and Productive Collaboration.",
        "author": [
            "Michele Newman",
            "Lidia Morris",
            "Jin Ha Lee 0001"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265227",
        "url": "https://doi.org/10.5281/zenodo.10265227",
        "ee": "https://zenodo.org/records/10265227/files/000008.pdf",
        "abstract": "Recently, there has been a surge in Artificial Intelligence (AI) tools that allow creators to develop melodies, harmonies, lyrics, and mixes with the touch of a button. The reception of and discussion on the use of these tools - and more broadly, any AI-based art creation tools - tend to be polarizing, with opinions ranging from enthusiasm about their potential to fear about how these tools will impact the livelihood and creativity of human creators. However, a more desirable future path is most likely somewhere in between these two polar opposites where productive and ethical human-AI collaboration could happen through the use of these tools. To explore this possibility, we first need to improve our understanding of how music creators perceive and utilize these types of tools in their creative process. We conducted case studies of a range of music creators to better understand their perception and usage of AI-based music creation tools. Through a thematic analysis of these cases, we identify the opportunities and challenges related to the use of AI for music creation from the perspective of the musicians and discuss the design implications for AI music tools.",
        "zenodo_id": 10265227,
        "dblp_key": "conf/ismir/NewmanM023",
        "keywords": [
            "Artificial Intelligence (AI) tools",
            "creators develop melodies",
            "polarizing reception",
            "human-AI collaboration",
            "music creation tools",
            "music creators perception",
            "challenges related to AI",
            "design implications",
            "productive and ethical",
            "productive future path"
        ],
        "content": "HUMAN-AI MUSIC CREATION: UNDERSTANDING\nTHE PERCEPTIONS AND EXPERIENCES OF MUSIC\nCREATORS FOR ETHICAL AND PRODUCTIVE COLLABORATION\nMichele Newman\nUniversity of Washington\nmmn13@uw.eduLidia Morris\nUniversity of Washington\nljmorris@uw.eduJin Ha Lee\nUniversity of Washington\njinhalee@uw.edu\nABSTRACT\nRecently, there has been a surge in Artiﬁcial Intelligence\n(AI) tools that allow creators to develop melodies, har-\nmonies, lyrics, and mixes with the touch of a button. The\nreception of and discussion on the use of these tools - and\nmore broadly, any AI-based art creation tools - tend to be\npolarizing, with opinions ranging from enthusiasm about\ntheir potential to fear about how these tools will impact the\nlivelihood and creativity of human creators. However, a\nmore desirable future path is most likely somewhere in be-\ntween these two polar opposites where productive and eth-\nical human-AI collaboration could happen through the use\nof these tools. To explore this possibility, we ﬁrst need to\nimprove our understanding of how music creators perceive\nand utilize these types of tools in their creative process.\nWe conducted case studies of a range of music creators to\nbetter understand their perception and usage of AI-based\nmusic creation tools. Through a thematic analysis of these\ncases, we identify the opportunities and challenges related\nto the use of AI for music creation from the perspective of\nthe musicians and discuss the design implications for AI\nmusic tools.\n1. INTRODUCTION\nIn the past few years, there has been an increase in the\ncreation of AI tools that support various musical activi-\nties. These activities are varied, including music recom-\nmendation/organization [1], sound synthesis [2], compo-\nsition [3–5], and mixing [6, 7]. Current discourse on the\nuse of AI-based tools in music production often presents\ntwo polarized perspectives: one that sees AI as an oppor-\ntunity for innovation and progress [8, 9], while the other\nviews it as a threat to the artistic creativity and livelihood\nof human creators [10–12]. However, a more nuanced and\ndesirable approach entails a productive and ethical collab-\noration between humans and AI in the creative process,\nallowing both human creators and AI tools to create some-\nthing neither could easily do alone.\n© M. Newman, L. Morris, and J.H. Lee. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: M. Newman, L. Morris, and J.H. Lee, “Human-AI Mu-\nsic Creation: Understanding the Perceptions and Experiences of Music\nCreators for Ethical and Productive Collaboration”, in Proc. of the 24th\nInt. Society for Music Information Retrieval Conf., Milan, Italy, 2023.Discussion around the perception of AI and music cre-\nativity tends to be focused on evaluation of the product of\nthe AI [13–15], legal issues [14, 16], or human-computer\ninteraction [17], and not on the implications and connec-\ntions these factors have on the creative thinking of mu-\nsicians, though there is growing interest in this domain\n[18, 19]. Additionally, while there has been discussion\nwithin the MIR community around the ethical implica-\ntions of AI in music creation [18, 20], the experience of\nusing AI to perform songwriting tasks [17], and the per-\nspectives of expert users in creative music information re-\ntrieval [21, 22], there is still a need to further understand\nhow creative MIR tasks are impacted by AI tools based on\ncreative context. Even within the ISMIR community, in the\nlast decade, there were fewer than 20 publications that dis-\ncussed AI music creation tools, and less than half of them\nconsidered the creator’s perspective before developing the\ntool.\nMusicians engage in creativity in many different ways\nthrough generating products such as compositions, anal-\nyses, and performances [23]. Our paper aims to address\nthe impact of AI tools on the perception and work of one\nsuch domain: composition. Within composition we ex-\nplore the impact of AI on what Peter R. Webster [23, p.\n22] describes as \"Creative Thinking,\" or \"the mental pro-\ncesses associated with creative production.\" We will refer\nto those who engage in this act of creative thinking in com-\nposition as creators , their environment/creation goals as\ncreative context , and the act of creative thinking as the cre-\native process . This paper addresses three research ques-\ntions: (1) In what way do creators perceive and envision\nthe use of AI tools during their compositional process?, (2)\nHow does their creative context inﬂuence their use of AI?\nand (3) What design implications can we derive to inform\nthe creation of AI tools for music creators?\nOur paper extends knowledge about how AI impacts the\ncreative process of musical creators and adds to the discus-\nsion of expert users of creative MIR and human-AI collab-\noration in music creation acts. [19,21,22,24,25] To address\nthese questions we conducted six case studies across a se-\nlection of creative contexts and our results are collected\nin a model that emphasizes the ﬂuidity of roles that AI can\nplay across creative thinking in composition and represents\nthe start of future work aimed at building a dynamic model\nof human-ai musical creativity.802. RELATED WORK\nThe idea of using computational means in composition is\nnot a recent development. Over the course of music his-\ntory, creators have considered various ways to develop al-\ngorithmic procedures to help with their process [26]. Since\nthe early days of the computer, programmers and music\ncreatives alike have utilized their skills to create programs\nthat allowed them to continuing this tradition; creating\ncomputer-aided compositions (CAC) and computer-aided\nalgorithmic compositions (CAAC). As a whole, CAC tools\nrequire user intervention - correction to misnoted parts, ad-\njustment of autotuned voices, and creators choosing which\nelectronic instruments to employ and when [27]. CAAC\ntools, unlike CAC tools, are intended to be used to help\n\"make music with minimal human intervention\" [28]. Pop-\nular examples of CAAC tools include programs such as\nOpusmodus [29] - a library for real-time computer-aided\ncomposition in Max [30] - and more. These tools are ex-\ntremely distinct in their purpose and use, helping to al-\ngorithmically aid creators, with rhythmic trees, polymet-\nric notation, and data visualizations of algorithmically-\ngenerated material. When deﬁning algorithmic composi-\ntion, Pearce et al. [31] state:\nMany who write programs for music compo-\nsition are motivated by artistic goals: these\nprograms are used to generate novel musical\nstructures, compositional techniques and even\ngenres of music...The composer may use an\nexisting computer program or she may write\na program herself: since identical motivations\nare involved, we count both of these as algo-\nrithmic composition. [31, p. 5]\nIn both cases, the question arises: What is creativity and\nwhat does it mean for computers to be part of the creative\nprocess? This question has been discussed in many ways\nin multiple ﬁelds, as scholars from the humanities [32,33],\nthe sciences [34–36] , HCI [37, 38] and MIR [21, 22] have\nspeculated for years over how the use of computer systems\nchanges the creation process.\nThrough their exploration of using AI to co-songwrite,\nMicchi et al. [17] list two potential ways in which AI tools\ncould assist creators: through (1) automation and (2) AI as\nsuggestion. They note that while AI as automation is more\nakin to the tasks given to AI outside of the artistic ﬁeld, the\nidea of AI as suggesting solutions to compositional tasks,\nacting as a partner in the process, is unique to the use of AI\nwithin creative pursuits. As Tipei et al. [39] discuss in their\npaper where student composers utilized DISSCO (Digital\nInstrument for Sound Synthesis and Composition), com-\npositions were still considered by users to be collaborative,\nas participants were able to add, modify, or reject contri-\nbutions made by the software and other users. Researchers\ncompared this interaction to the process of collective im-\nprovisation, with the software playing a key role as a col-\nlaborator and manager in this compositional process - \"[the\ncomputer/software]...becomes part of the process not only\nby performing a vast number of operations very quickly,AI as Collaborator Democratization\nMeaning of Creativity Bias in AI\nCreative Control Corporatization of Art\nInﬂuence Knowledge of AI\nMechanism Creating Opportunities\nOld vs. New AI Sharing of Tools\nTypes of AI Contributions Current State of AI\nTable 1 . Final Codebook for Interviews\nbut also as a consequential contributor to the creative ef-\nfort\" [39]. More speciﬁcally, this implies that AI simul-\ntaneously acts as a collaborator in the process and as a\ntool, allowing the creator to explore different possibilities\nof how AI can be applied within their workﬂow.\n3. STUDY DESIGN AND METHODS\nWe employed a exploratory, multi-subject case study\nmethod [40] to examine how creators perceive the use\nof AI tools within their compositional process. Using\nmultiple-subject case studies allows us to better explore\nthe phenomenon of AI within the compositional process\nacross a variety of contexts in order to build a stronger ba-\nsis of understanding and is useful for formulating concepts\nfor theory construction [40, 41].\nOur case selection strategy was focused on representing\ndiverse cases within the varied creative contexts of both\nwestern art music and western popular/commercial music\ntraditions [42]. Our cases included a classical/jazz music\ncomposer, a ﬁlm and video game music composer, an in-\nteractive media composer, an electroacoustic composer, a\nsound artist, and a DJ. Due to the scope of the study and re-\nsources, we did not include case studies of programmers,\nlisteners, or creators outside the western context, though\nthese communities will be explored in further studies.\nThere were a total of six creators, one for each case, all\nof which were over eighteen years of age and recruited via\nemail. Of the recruited participants, all had heard of AI\ntools and ﬁve worked actively with AI tools within their\nprocess. While all creators were actively working within\nthe music ﬁeld professionally, the ﬁlm music creator and\nthe intermedia creator were the only ones not afﬁliated\nwith an academic institution as a student, though both had\nbeen trained within western academic music schools. All\nparticipants had been composing over ﬁve years at the time\nof the interview.\nFor each case, we conducted in-depth, semi-structured\ninterviews between 60 and 90 minuets. Interview ques-\ntions for this study were generated via a review of the\nexisting literature on the use of AI in music composi-\ntion and production, where we identiﬁed relevant themes\nand topics (e.g., deﬁnitions of AI, AI creativity, typi-\ncal tools in music creation). Topics ranged from par-\nticipants’ experiences with AI-based tools in music pro-\nduction, their perceptions of the advantages and disad-\nvantages of using AI, and their views on the ethical im-\nplications of AI in music creation. Both descriptions of\nthe case contexts and interview questions can be found atProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n81the url:https://github.com/michelenewman/\nISMIR23_supplemental_material .\nAll interviews were conducted online over Zoom and\nfully transcribed and edited for clarity. We created the\ncodebook using a mix of the inductive and deductive ap-\nproach [43]. Initially, two of the authors created the ﬁrst\ndraft of the codebook using thematic analysis of the tran-\nscribed interviews. The codebook was iteratively reﬁned\nby adjusting and aligning the themes that emerged from\nthe interview data with those from existing literature. Us-\ning the ﬁnal codebook, we ﬁrst coded the interviews sep-\narately on the qualitative coding software ATLAS.ti, then\ncame together to discuss any discrepancy with a goal of\nreaching an agreement and assigning ﬁnal codes following\nthe consensus model [44].\n4. RESULTS\nDuring analysis, 12 categories emerged which were\ngrouped into two main sets: AI as Collaborator and De-\nmocratization of Music Creation. The themes were inﬂu-\nenced by the current or lack of use of AI by the creators\nand the reasoning behind their choices. Themes that arose\nsuch as tool sharing are common practice among commu-\nnities of creators, especially on the internet [45, 46], but\nwithin the this study, refers speciﬁcally the sharing of AI\nand ML tools.\nCoding the interview transcripts led to the insight that\ncreators had speciﬁc creative tasks with which they would\nor would not feel comfortable utilizing AI tools, as well\nas parts of the process in which they would consider the\nuse of AI. The most common code within our analysis was\n\"Types of AI Contribution\" in which the creators reﬂected\non how they would personally use AI within their own pro-\ncess. This included tasks such as creating repositories (P4)\nand mastering songs (P6). The least common code was\n\"Sharing of Tools.\" As a whole, all participants had some\nknowledge of what AI was, and all but the jazz/classical\ncreator had utilized it in some capacity within their work-\nﬂow. Three of the creators used also used non-musical AI\n(such as text-based AI) in their process.\nBased on our analysis, we present the Human-AI Cre-\native Collaboration Model (Figure 1) to represent the use\nof AI tools throughout the compositional process of mu-\nsic creators situated within the western tradition of com-\nposition who may employ computer assisted tools. The\nmodel is comprised of three parts: Factors on Inﬂuences,\nAI Roles, and Creation as Process.\n4.1 Factors on Inﬂuences\nThe far-most left section of our model represents the vari-\nous contextual factors that impact creators’ perception on\nwhere AI should fall within their creative process. These\nfactors are broken into three parts within our model: per-\nsonal context, social context, and creative goal. While all\ncreation contexts are different, these are the three most\ncommon aspects that arose from our analysis.\nFigure 1 . Human-AI Creative Collaboration Model\n4.1.1 Personal Context\nThePersonal Context is deﬁned by the individual creator’s\nfamiliarity with their own creative process and their music\nliteracy. When reﬂecting on whether or not participants\nthought that an AI tool would be helpful to them, they\nconsidered where it would fall into their process and how\nmuch control they would be able to maintain over the ﬁ-\nnal product. Because our participants had been composing\nfor over ﬁve years, they already had a strong idea of how\ntheir process worked and a familiarity with their personal\ncontext of musical creation. Creators commented on their\ndesire to have AI tools that are ﬂexible enough to work\nwithin their current creative process, are interoperable with\nexisting compositional software, and have clear and con-\ncise interfaces to help with facilitating their adoption and\nuse.\nFor example, the ﬁlm music creator and DJ who work\nin more commercial settings, with tighter schedules, men-\ntioned utilizing AI tools that were already integrated within\nsoftware they used such as Ozone [7] and Logic’s Drum-\nmer [47]. If integration is not possible, they suggested\nAI should be designed in a way that it does not interfere\nwith the use of a primary creation tool. The electroacous-\ntic creator, interactive music creator, and sound artist pre-\nferred to use older forms of AI due to developer trans-\nparency. The participants differentiated older models from\nthe newer models, suggesting that newer models were hid-\nden behind a corporate \"black box\" in order to work. These\ncreators preferred supervised learning algorithms to unsu-\npervised learning algorithms, so that they could change the\nopen source code and exert more creative control (P5). As\nthey had more experience with AI, they were more open to\nlearning and working with AI tools. All participants also\ncommented that current AI tools were not able to support\ntheir process in the ways that they wanted due to the lack\nof control and low-ﬁdelity outputs.\n4.1.2 Social Context\nCreators also consider Social Context ; this includes their\ncurrent community of practice where they often converse\nand share their art with (other creators, their audience) and\ntheir educational and musical upbringing. Many aspects\nof the act of creativity are tied to the sociocultural aspects\nof making music [48]. Those whose communities were\nmost open to using AI tools, often in more experimental\ncreative contexts such as in academic experimental com-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n82munities, were much more willing to engage with the idea\nof AI in different parts of their process. The jazz/classical\ncomposer was adamant that they were very skeptical of AI\nin part because the community around them was also very\nskeptical, especially with a strong tradition of composition\nwithin the western art context.\nSimilarly, they decided on what was an appropriate use\nof AI by comparing the impact the tool had on others. Par-\nticipants raised the issue of bias in AI and using the cre-\native works of others. All creators noted that most large-\nscale models rely on Eurocentric training data that may not\nalign with their individual artistic expressions or require-\nments, feeling that the AI would \"ﬂatten\" their work with\nit has \"biases and this kind of Eurocentric Westernization\nof aesthetics\" (P5). P4 noted, \"There’s been a big problem\nin the past couple of weeks with people coming out talk-\ning about how that’s not right, to be able to use someone’s\nlikeness and their voice however you want.\" This sentiment\nhighlights the worry that there was oversight in the ways\nin which these models are being created and distributed,\nleading to potential harms in taking the intellectual prop-\nerty of others and using it to quickly make ﬁnancial gains,\na sentiment echoed by others in the music industry [15].\nFurthermore, participants expressed worry about the im-\npact of such rapidly generated artworks on not only their\npersonal work, but also on the general public’s perception\nof art as a whole.\nParticipants also talked about how AI tools opened up\nthe potential for more types of creators to get involved in\nthe music creation process. Our DJ participant discussed\ntheir use of the AI tool Ozone, which he uses to digitally\nmaster his songs. While the tool costs 50 USD to purchase,\nthe participant could use it to process all of his songs and\nhave them match the audio speciﬁcations needed to up-\nload to streaming services such as Spotify within seconds,\nwhereas if it was sent to a mastering engineer, each song\nwould cost him hundreds of dollars to master. In the same\nvein, multiple participants mentioned that one of the poten-\ntial abilities of AI music tools would be the ways it could\npotentially allow entry-level musicians to bypass some of\nthe extensive music education they would need before be-\ning able to create music, including the cost and time invest-\nment of said education. \"I think it can really accelerate the\nlearning process, the process of studying music and experi-\nencing all this music that we’ve documented...I think it can\nkind of build each person’s personal vocabulary of what\nmusic is.\" (P1). These participants qualiﬁed their state-\nments by clarifying that this would not mean users should\nbypass the whole process of learning the art of composi-\ntion. Rather, the ability to create music without network-\ning and funding as a necessity in the creative process is\na kind of \"freedom\" one participant noted, one which be-\ngun with the advent of the personal computer and has only\ncontinued to expand as the process is simpliﬁed (P2).\n4.1.3 Creative Goal\nLastly, Goal refers to creators’ speciﬁc reason for compos-\ning (i.e. for a ﬁlm, for a commission, a performance). Thegoal can put pressure on creation time, inﬂuence the social\npractices and expectations, and change the personal work-\nﬂow of the creator.\n4.2 Potential AI Roles: Inﬂuences and Mechanisms\nAt the center of the model, we present the different ways\nthat AI could potentially be used in the creation process.\nParticipants expressed an overall positive view towards the\npotential of AI tools as collaborators. Participants also per-\nsoniﬁed the AI in their process stating it was similar to\nhaving a \"second person\" check over their work or a way\nto bounce off other ideas with the AI tools. While the list\nis not exhaustive, these represent the most common tasks\nthat creators in our study talked about. These roles were\nprimarily impacted by the concept of \"control\" of a cre-\native output across the process. All participants agreed\nthat computational creativity cannot supplant human cre-\nativity. While participants recognized that AI can \"create\nsomething\" and output a product that mirrors human cre-\nativity, such as P4 stating that they were \"...sure AI could\ncreate something like a poem, for example, that would be\nreally hard for me to tell if it was from a human or from an\nAI,\" they highlighted that AI lacks the deliberate decision-\nmaking of human creators, continuing they would have a\nhard time \"emotionally connect[ing] with it.\"\nIn the former case, participants discussed engaging in\na process of \"play\" with the AI, which allowed them to\nexplore a variety of prompts and generate a collection of\npotential options that they could later modify or combine\nto achieve their artistic goals. P5 noted: \"So sometimes\nwhen I’m stuck, I like to grab some of the models I pre-\ntrained and just ask it something.\" The creators used the\ntools to explore, both as a way to spark new ideas and as\na way to generate a large repository of content to remix\nin their own way. Within this process though, the creators\nemphasized the AI does not make the ﬁnal choice. The\nﬁnal decision was always made by the creator to maintain\ntheir artistic agency.\nFor all of the participants, within the context of their\nown compositional process, intention and choice was as\nimportant as the creative product. One participant stated,\n\"You can have [AI] generate some sort of electronic mu-\nsic code for you and that sort of just skips for me a whole\nimportant step in the process, because in my process of\ncreating live electronic music, there’s sort of an interplay\nbetween my coding and my writing. I think a lot will be\nlost if you just take out an entire part of that process\" (P3).\nAnother participant remarked, \"For me, creativity also in-\nvolves the decision-making in a big way. And then to de-\ntermine where to end things. It doesn’t seem that my expe-\nrience with AI so far affords these possibilities\" (P6).\nElaborating on this idea of creative control, P6 noted: \"I\nfeel that it doesn’t sound like me, or especially with music\ncompositions and working with some of these AI that will\ngive you a MIDI ﬁle, you know?\", implying some kind of\nloss in the creator’s personality in automatically generated\nmusic pieces. P1 stated that \"AI seems to be something\nthat’s designed to do some of that channeling of an idea forProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n83you. It seems like AI is kind of trying to be designed to do\nthe human part of the process.\" Personality Theory related\nto intellectual property, put forward by thinkers like Em-\nmanual Kant and George Hegel [49], suggests that a per-\nson’s personality is incorporated into their creative work\nduring the labor process, and is therefore an essential part\nof their work. When an AI takes that labor away from the\ncreator, creators felt that the work now no longer has their\npersonality, and thus is no longer their creation.\n4.2.1 Inﬂuences to Creative Process\nThe top section Inﬂuences to Creative Process lists tasks\nthat directly inﬂuence the ﬁnal artistic product, allowing\nfor integrity of expression by the creator. On the left side\nareacceptable inﬂuences . These tasks involve aspects that\nhelp prompt ideas or create inspiration. Tasks that fall\nwithin the acceptable inﬂuences do not need to be as inte-\ngrated with the programs creators already use, though they\nshould integrate with the overall creative process - espe-\ncially in the ideation phase, where many creators felt AI\ninﬂuences ﬁt best. These tools should allow for continu-\nous reiteration, with understandable in-tool design signi-\nﬁers that indicate the ways they can edit, change, and ma-\nnipulate the AI’s data before and after each iteration. Once\nthis ideation phase is over, there should be a clear way to\nexport their ideas into a new software or system, again al-\nlowing for the interoperability that is vital for music cre-\nator’s process; this could be done in a number of ways such\nas using MIDI ﬁles, WA V ﬁles, or MusicXML.\nParticipants also discussed ways that AI tools could go\nbeyond what humans are traditionally capable of, and in\nthat way become a partner in the expansion of their com-\npositional capabilities. One potential function as noted by\na participant was the ability to use AI as a music analy-\nsis tool, helping users pinpoint things they were not aware\nof or even able to perceive with human hearing, such as\n\"the sound ﬁeld...expanding from the front to the back\"\n(P6). Another participant described how their current use\nof AI as part of their process has changed how they see the\nworld around them, gaining a new understanding around\nwhat could be used or turned into data which allows them\nto create patterns and connections within their music (P5).\nAI tools also helped many creators ﬁnd relationships be-\ntween sounds, found materials, words, and pictures. One\nparticipant explained it as a \"feedback loop\" (P6).\nThe right side displays aspects of the creative process\nwhere creators are not comfortable engaging in Human-AI\ncollaboration. They felt using AI with these tasks nega-\ntively affect the creative process by taking away an essen-\ntial component of their creations. This includes losing the\nability to control their intent and choices, not being able to\nspecify performance parameters/low-ﬁdelity outputs, and\ninterfering with their process and creative personality.\n4.2.2 Mechanisms of Creation\nThe lower section is titled Mechanisms of Creation . It in-\ncludes types of Human-AI collaborations where our par-\nticipants had little issue if AI took over the process com-pletely, often searching for and utilizing AI that could com-\nplete these tasks. In general, mechanisms are tasks that oc-\ncur within the creative process that do not require direct\ndecision-making by the music creators, including house-\nkeeping tasks such as ﬁle naming, information retrieval\ntasks such as looking for electronic instruments, and repli-\ncating sounds. Many participants noted they would use AI\nto complete tasks to help speed up their process or com-\nplete tasks they did not want to do. These tasks often had\nto do with analyzing data in some way. For example, P3\nnoted \"I have an idea that I want to do, and I just use the\nAI to make that idea happen faster.\"\n4.3 Creation as Process\nLastly, on the far right side is a spectrum representing what\nwe call \"Creation as Process,\" emphasizing the role of iter-\nation and thinking that happens during the process of writ-\ning music [23, 32]. For all of the participants, within the\ncontext of their own creative process, intention and choice\nwas as important as the creative product. One participant\nstated, \"You can have [AI] generate some sort of electronic\nmusic code for you and that sort of just skips for me a\nwhole important step in the process, because in my process\nof creating live electronic music, there’s sort of an inter-\nplay between my coding and my writing. I think a lot will\nbe lost if you just take out an entire part of that process\"\n(P3). Another participant remarked, \"For me, creativity\nalso involves the decision-making in a big way. And then\nto determine where to end things. It doesn’t seem that my\nexperience with AI so far affords these possibilities\" (P6).\nThe spectrum represents the level of intellectual en-\ngagement needed in each task, ranging from highly inten-\ntional choices to mechanical and repetitive tasks. Within\nthe process of creating, the given AI tasks may move to\nhigher or lower levels along the spectrum, sometimes inﬂu-\nencing the process more and other times receding to lower\nlevels of impact. The creative process is ﬂuid, meaning\nthat both the factors androles of the AI can change over\nthe course of creation.\n5. DISCUSSION\nAlthough our focus on only six case-studies of music cre-\nators in speciﬁc creative contexts presents a limitation to\nour study, we believe that our focus allowed us to explore\npossible applications of AI tools to creators’ needs, and\nallowed us to form initial insights into the perception of\nthe use of AI tools in the creative process. Musical cre-\nativity is not a monolith and it is our belief that in order\nto understand how to design speciﬁc AI systems that sup-\nport creative musical tasks, we need to know how creative\nthinking is conceived by those engaging in these types of\nmusical activities. Our main contribution in this paper is\nto begin to situate certain AI tasks as potential helpful or\npotentially harmful to the creative process of those who\ncreate.\nIn this study, we argue that the discussion around the\nthreat that AI poses to both the jobs of creators and artis-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n84tic integrity is of importance to creators; emphasizing that\nco-creation of music in the context of music composition\nis dependent not only on the larger creative context, but on\nthe process of creative thinking as well. Our work suggest\nthat Andersen and Knees [21, 127] notion of the impor-\ntance of an \"individual user[s’] models of music percep-\ntion as well as a solid understanding of usage context\" is\nnot only needed for exploring dissimilarity in search, but\nalso for understanding AI systems in the other forms cre-\native endeavors. Knees et al.’s [50] consideration of the\nuse of \"strangeness\" for artists recommendations is use-\nful in AI systems as so far the AI is helping to generate\nnew ideas for creators; though strangeness is one aspect\nof many needs that a creative engages with in AI music\nsystems. Other tasks such as analysis and editing are also\nelements of creative MIR tasks that may be helpful to fa-\ncilitating the creative process, though often can occur at\ndifferent points or simultaneously with the task of idea\ngeneration. We argue that there is a need to understand\nhow speciﬁc creative processes view and interact with AI\nat all stages. While there are some aspects of the use of\nAI that many of our cases agreed on, such as allow AI to\ntake over tasks that have little to no control over the ﬁnal\nproduct of creative thinking which is echoed in other liter-\nature [50, 51], our study also indicated that the role of AI\nis also dependent upon personal, social, and creative goal\nrelated factors that are constantly in ﬂux. The Human-AI\nCollaboration Model demonstrates our belief that the role\nthat AI plays on creative thinking is highly ﬂexible within\nmusic creation and that without a clear understanding of\nhow creators are thinking, AI systems can hurt musical cre-\native practices of musicians.\nOliver Bown has warned against the possible negative\naffect that AI tools can have if it disrupts cultural applica-\ntions and creation of music. [33]. If music AI systems are\ndesigned to limit creator control, intent, or process, they\ncould potentially lead to Schröter’s notion of the \"(possi-\nble) automatization of artistic work\" [52]. Full control over\nthe ﬁnal artistic product and an understanding of the cre-\nator’s emphasis on their process are the most important as-\npects to developing tools that can support, instead of harm,\nhuman creativity - as noted by Knees, it is important that\nthe user is given agency in the process of \"co-creation\"\nwith high-level control of the generative process [19].\nWhile it is true that the concept of \"Explainable AI\"\n[53–55] can help to educate those who worry about the\nrole AI plays in future creative endeavors, it is not a full\nsolution to the lack of user trust or changing user hesi-\ntancy in tool adoption. Recent fears over data misuse by\ngenerative AI, backed up by online discussions and even\nlegal investigations into data scraping [56] and intellectual\nproperty [57] have made creators fear utilizing AI tools,\nwith creators fearing that AI is trained on data that does\nnot meet their personal artistic goals or actively hurts other\nartists. Increasing common knowledge about the functions\nof AI tools would create more trust in these systems and\nencourage users to integrate them more into their creative\nprocess [58], but there is also a need to design in systemsin such a way that creators feel they can ethically use these\nsystems in their own work. This means ethically sourcing\nmaterial and allowing for the control of elements within AI\nsystems.\nDesigners of AI tools for creators should consider what\nrole they expect for their tool to play within speciﬁc cre-\nation processes and make choices that support this. The\nspeciﬁc inputs, outputs, and needs of an AI system will\nchange over the creative process. Because AI tasks can\nmove up and down in importance, that means that it is\nhighly possible to have a mismatch of the execution and\nevaluation of AI systems that may lead to less cohesion be-\ntween the creator and the AI as they will continually need\nto reevaluate how these systems ﬁt in their workﬂow.\n6. CONCLUSION AND FUTURE WORK\nOur ﬁndings support that creators have concerns surround-\ning the transparency of and lack control within AI tools,\nbut that there is still much to do in relation to understand-\ning the exact needs of creative users. In order to develop\nuseful AI tools, designers must consider the speciﬁc cre-\nation context, existing processes of creators, control of cre-\nator, and the ﬂuidity of creating. Our Human-AI Creative\nCollaboration Model is designed to help developers and\nresearchers who create AI systems to consider the variety\nof factors and inﬂuences that exist on creative process and\nhow they might intersect with a creators experience. We\nhope that this work encourages developers and other MIR\nresearchers to continue to consider advancing Human-AI\ncollaborations that align with music creators’ needs. There\nare a variety of tasks that AI can perform, and consider-\ning if tasks are impacting creative thinking in a different\nphases of creation will allow for a more ethical and pro-\nductive experience for music creators.\nWhile we interviewed different creation contexts within\nour case studies, there is still a need for future work to\nconsider how differences in cultural background, musical\ntraining, and experience with AI factor into Human-AI cre-\native thinking. Composing music can happen in a variety\nof other contexts not explored in this study, including as\npart of music education and cultural situations. Yet, com-\nposition is only one form of creative thinking within music\nand future work might will continue work to identify the\ndifferences that arise when using AI systems within differ-\nent forms of creation such as music analysis and perfor-\nmance. Creators may be utilizing all these forms of think-\ning across the creative process in non-linear ways. There\nis still much to learn about the impact that AI will have\non music as an art; if designed and deployed ethically, AI\noffers the opportunity to enhance human creation and pro-\nvide new avenues for creating and learning about music.\nBut, in order for AI to support musicians in any form of\ncreative thinking, we need to ensure we are designing AI\ntools with creators in mind.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n857. REFERENCES\n[1] “Cyanite ai-based music tagging system,” https://\ncyanite.ai/, accessed: 2023-03-30.\n[2] “VOCALOID ai engine,” https://www.vocaloid.com/\nen/news/news_001/, accessed: 2023-03-30.\n[3] A. Roberts, J. Engel, Y . Mann, J. Gillick, C. Kay-\nacik, S. Nørly, M. Dinculescu, C. Radebaugh,\nC. Hawthorne, and D. Eck, “Magenta Studio: Aug-\nmenting Creativity with Deep Learning in Ableton\nLive,” in Proceedings of the 6th International Work-\nshop on Musical Metacreation . Charlotte, United\nStates: MUME, Jun. 2019, p. 7. [Online]. Available:\nhttps://doi.org/10.5281/zenodo.4285266\n[4] “AIV A artiﬁcial intelligence composer,” https://www.\naiva.ai/, accessed: 2023-03-30.\n[5] “Sony Flow Composer,” https://www.\nﬂow-machines.com/history/projects/\nﬂowcomposer-composing-with-ai/, accessed: 2023-\n03-30.\n[6] “LANDR mastering software,” https://www.landr.\ncom/online-audio-mastering/, accessed: 2023-03-30.\n[7] “Ozone izotope mastering software,” https://www.\nizotope.com/en/products/ozone.html, accessed: 2023-\n03-30.\n[8] J. Hong, “Bias in perception of art produced by ar-\ntiﬁcial intelligence,” in International Conference on\nHuman-Computer Interaction: Interaction in Context ,\n2018.\n[9] C. Moruzzi, “Should human artists fear ai? : A report\non the perception of creative ai,” in xCoAx 2020 :\nProceedings of the Eighth Conference on Computation,\nCommunication, Aesthetics & X , M. Verdicchio,\nM. Carvalhais, L. Ribas, and A. Rangel, Eds. Porto:\nUniversidade do Porto, 2020, pp. 170–185. [Online].\nAvailable: https://2020.xcoax.org/xCoAx2020.pdf\n[10] M. Mazzone and A. Elgammal, “Art, creativity, and the\npotential of artiﬁcial intelligence,” Arts, vol. 8, no. 1,\np. 26, 2019.\n[11] J. Hong, Q. Peng, and D. Williams, “Are you ready for\nartiﬁcial mozart and skrillex? an experiment testing\nexpectancy violation theory and ai music,” New Media\n& Society , vol. 23, no. 7, pp. 1920–1935, 2021.\n[12] H. Zuli ´c, “How ai can change/improve/inﬂuence mu-\nsic composition, performance and education: Three\ncase studies,” INSAM Journal of Contemporary Music ,\nvol. 1, no. 2, pp. 100–114, 2019.\n[13] D. Zlatkov, J. Ens, and P. Pasquier, “Searching for\nhuman bias against ai-composed music,” in Artiﬁcial\nIntelligence in Music, Sound, Art and Design: 12th\nInternational Conference, EvoMUSART 2023, Heldas Part of EvoStar 2023, Brno, Czech Republic,\nApril 12–14, 2023, Proceedings . Berlin, Heidelberg:\nSpringer-Verlag, 2023, p. 308–323. [Online]. Avail-\nable: https://doi.org/10.1007/978-3-031-29956-8_20\n[14] F. Tigre, F. Moura, and C. Maw, “Artiﬁcial intelligence\nbecame beethoven: how do listeners and music profes-\nsionals perceive artiﬁcially composed music?” Journal\nof Consumer Marketing , vol. 38, no. 2, pp. 137–146,\n2021.\n[15] K. Lee, G. Hitt, E. Terada, and J. Lee, “Ethics of\nsinging voice synthesis: Perceptions of users and de-\nvelopers,” in Proc. of the 23rd Int. Society for Music\nInformation Retrieval Conf. , Bengaluru, India, 2022,\npp. 733–740.\n[16] E. Drott, “Copyright, compensation, and commons in\nthe music ai industry,” Creative Industries Journal ,\nvol. 14, no. 2, pp. 190–207, 2021.\n[17] G. Micchi, L. Bigo, M. Giraud, R. Groultand, and\nF. Levé, “I keep counting: An experiment in human/ai\nco-creative songwriting,” Transactions of the Interna-\ntional Society for Music Information Retrieval , vol. 4,\nno. 1, p. 263–275, 2021.\n[18] M. Rohrmeier, “On creativity, music’s ai completeness,\nand four challenges for artiﬁcial musical creativity,”\nTransactions of the International Society for Music In-\nformation Retrieval , vol. 5, no. 1, p. 50–66, 2022.\n[19] P. Knees, M. Schedl, and M. Goto, “Intelligent user in-\nterfaces for music discovery,” Transactions of the In-\nternational Society for Music Information Retrieval ,\nvol. 3, no. 1, p. 165–179, 2020.\n[20] F. Morreale, “Where does the buck stop? ethical and\npolitical issues with ai in music creation,” Transactions\nof the International Society for Music Information Re-\ntrieval , vol. 4, no. 1, p. 105–113, 2021.\n[21] P. K. Kristina Andersen, “Conversations with expert\nusers in music retrieval and research challenges for cre-\native mir,” in 17th International Society for Music In-\nformation Retrieval Conference , 2016, pp. 122–128.\n[22] C.-E. Cella, “Music information retrieval and contem-\nporary classical music: A successful failure.” Transac-\ntions of the International Society for Music Information\nRetrieval , vol. 3, no. 1, pp. 126–136, 2020.\n[23] P. R. Webster, “Creativity as creative thinking,” Music\nEducators Journal , vol. 76, no. 9, pp. 22–28, 1990.\n[24] A.-M. Gioti, “From artiﬁcial to extended intelligence\nin music composition,” Organised Sound , vol. 25,\nno. 1, p. 25–32, 2020.\n[25] P. Pasquier, A. Eigenfeldt, O. Bown, and S. Dubnov,\n“An introduction to musical metacreation,” Comput.\nEntertain. , vol. 14, no. 2, jan 2017. [Online].\nAvailable: https://doi.org/10.1145/2930672Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n86[26] G. Nierhaus, “Historical development of algorithmic\nprocedures,” in Algorithmic Composition: Paradigms\nof Automated Music Generation . Vienna, Austria:\nSpringer Vienna, 2008, pp. 7–66.\n[27] D. Bouche, J. Nika, A. Chechile, and J. Bresson,\n“Computer-aided composition of musical processes,”\nJournal of New Music Research , vol. 46, no. 1, pp. 3–\n14, 2017.\n[28] A. Alpern, “Techniques for algorithmic composition of\nmusic,” Hampshire College , 1995.\n[29] Janusz Podrazik, “Opusmodus.” [Online]. Available:\nhttps://opusmodus.com/\n[30] Cycling 74, “Max.” [Online]. Available: https:\n//cycling74.com/products/max\n[31] M. Pearce, D. Meredith, and G. Wiggins, “Mo-\ntivations and methodologies for automation of the\ncompositional process,” Musicae Scientiae , vol. 6,\nno. 2, pp. 119–147, 2002. [Online]. Available:\nhttps://doi.org/10.1177/102986490200600203\n[32] D. Cope, Computer models of musical creativity . MIT\nPress, 2005.\n[33] O. Bown, “Sociocultural and design perspectives on ai-\nbased music production: Why do we make music and\nwhat changes if ai makes it for us?” in Handbook of\nArtiﬁcial Intelligence for Music , E. R. Miranda, Ed.\nCham: Springer, 2021.\n[34] F. Carnovalini and A. Rodà, “Computational creativ-\nity and music generation systems: An introduction to\nthe state of the art,” Frontiers in Artiﬁcial Intelligence ,\nvol. 3, no. 14, pp. 111–222, April 2020.\n[35] C. Lamb, D. G. Brown, and C. L. A. Clarke, “Eval-\nuating computational creativity: An interdisciplinary\ntutorial,” ACM Computing Surveys , vol. 51, no. 2, feb\n2018.\n[36] F. Nake, “Creativity in algorithmic art,” in Proceedings\nof the Seventh ACM Conference on Creativity and Cog-\nnition , Berkeley, California, USA, 2009, pp. 97–106.\n[37] M. Lee, P. Liang, and Q. Yang, “Coauthor: Designing\na human-ai collaborative writing dataset for exploring\nlanguage model capabilities,” in Proceedings of the\n2022 CHI Conference on Human Factors in Computing\nSystems , ser. CHI ’22. New York, NY , USA:\nAssociation for Computing Machinery, 2022. [Online].\nAvailable: https://doi.org/10.1145/3491102.3502030\n[38] G. A. Wiggins, “A preliminary framework for\ndescription, analysis and comparison of creative\nsystems,” Know.-Based Syst. , vol. 19, no. 7, p.\n449–458, nov 2006. [Online]. Available: https:\n//doi.org/10.1016/j.knosys.2006.04.009[39] S. Tipei, A. Craig, and P. Rodriguez, “Using high-\nperformance computers to enable collaborative and in-\nteractive composition with dissco.” Multimodal Tech-\nnologies and Interaction , vol. 5, no. 24, 2006.\n[40] R. K. Yin, Base Study Research and Applications : De-\nsign and Methods . Sage Publications, 2017.\n[41] B. E. White, S. J. Gandhi, A. Gorod, V . Ireland, and\nB. Sauser, “On the importance and value of case stud-\nies,” in 2013 IEEE International Systems Conference\n(SysCon) , 2013, pp. 114–122.\n[42] J. Seawright and J. Gerring, “Case selection techniques\nin case study research: A menu of qualitative and quan-\ntitative options,” Political Research Quarterly , vol. 61,\nno. 2, p. 294–308, 2008.\n[43] J. Corbin and A. Strauss, Basics of Qualitative Re-\nsearch: Techniques and Procedures for Developing\nGrounded Theory . Sage Publications, 2014.\n[44] C. E. Hill, S. Knox, B. Thompson, E. Williams,\nS. Hess, and N. Ladany, “Consensual qualitative re-\nsearch: An update,” Journal of Counseling Psychol-\nogy, vol. 52, no. 2, p. 196–205, 2005.\n[45] J.-P. Fourmentraux, “Internet artworks, artists and\ncomputer programmers: Sharing the creative process,”\nLeonardo , vol. 39, no. 1, p. 44–50, 2021.\n[46] Y . Kjus, “The use of copyright in digital times: A study\nof how artists exercise their rights in norway,” Popular\nMusic and Society , vol. 44, no. 3, pp. 241–257, 2021.\n[47] “Logic Pro virtual drummer,” https://support.apple.\ncom/guide/logicpro/drummer-lgcpa4324884/mac, ac-\ncessed: 2023-03-30.\n[48] V . P. Gl ˘aveanu, “Creativity as a sociocultural act,”\nThe Journal of Creative Behavior , vol. 49, no. 3,\npp. 165–180, 2015. [Online]. Available: https:\n//onlinelibrary.wiley.com/doi/abs/10.1002/jocb.94\n[49] J. Hughes, “The philosophy of intellectual property,”\nGeorgetown Law Journal , vol. 77, p. 287, 1988.\n[50] P. Knees, K. Andersen, and M. Tkal ˇciˇc, ““I’d\nlike it to do the opposite”: Music-Making Between\nRecommendation and Obstruction,” in Proceedings of\nthe 2nd International Workshop on Decision Making\nand Recommender Systems , vol. 1533. Charlotte,\nUnited States: MUME, 2015. [Online]. Available:\nCEUR-WS.org.\n[51] D. Buschek, L. Mecke, F. Lehmann, and H. Dang,\n“Nine potential pitfalls when designing human-ai co-\ncreative systems,” CoRR , vol. abs/2104.00358, 2021.\n[Online]. Available: https://arxiv.org/abs/2104.00358\n[52] J. Schröter, “Artiﬁcial intelligence and the democrati-\nzation of art,” in The Democratization of Artiﬁcial In-\ntelligence: Net Politics in the Era of Learning Algo-\nrithms , A. Sudmann, Ed. Transcript publishing, 2019,\npp. 297–311.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n87[53] C. Zednik, “Solving the black box problem: A norma-\ntive framework for explainable artiﬁcial intelligence,”\nPhilosophy & Technology volume , vol. 34, p. 265–288,\n2021.\n[54] A. Holzinger, “From machine learning to explainable\nai,” in 2018 World Symposium on Digital Intelligence\nfor Systems and Machines (DISA) , 2018, pp. 55–66.\n[55] W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen,\nand K.-R. Müller, Explainable AI: Interpreting, ex-\nplaining and Visualizing Deep Learning . Springer,\n2019.\n[56] C. Fiesler, N. Beard, and B. C. Keegan, “No robots,\nspiders, or scrapers: Legal and ethical regulation of\ndata collection methods in social media terms of ser-\nvice,” Proceedings of the International AAAI Confer-\nence on Web and Social Media , vol. 14, p. 187–196,\n2020.\n[57] T. Zipper, “Mind Over Matter: Addressing Chal-\nlenges of Computer-Generated Works Under\nCopyright Law,” Wake Forest Journal of Busi-\nness and Intellectual Property Law , aug 7 2022,\nhttps://jbipl.pubpub.org/pub/zn744tze.\n[58] E. Ruane, A. Birhane, and A. Ventresque, “Conversa-\ntional ai: Social and ethical considerations,” in Irish\nConference on Artiﬁcial Intelligence and Cognitive\nScience , 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n88"
    },
    {
        "title": "From West to East: Who Can Understand the Music of the Others Better?",
        "author": [
            "Charilaos Papaioannou",
            "Emmanouil Benetos",
            "Alexandros Potamianos"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265287",
        "url": "https://doi.org/10.5281/zenodo.10265287",
        "ee": "https://zenodo.org/records/10265287/files/000036.pdf",
        "abstract": "Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset. Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture. The implementation and the trained models are both provided in a public repository.",
        "zenodo_id": 10265287,
        "dblp_key": "conf/ismir/PapaioannouBP23",
        "keywords": [
            "MIR",
            "benchmark deep learning models",
            "downstream tasks",
            "Western pop/rock music",
            "different music cultures",
            "transfer learning",
            "audio embedding models",
            "auto-tagging",
            "source dataset",
            "public repository"
        ],
        "content": "FROM WEST TO EAST: WHO CAN UNDERSTAND THE MUSIC OF THE\nOTHERS BETTER?\nCharilaos Papaioannou1,2Emmanouil Benetos2Alexandros Potamianos1\n1School of ECE, National Technical University of Athens, Greece\n2Centre for Digital Music, Queen Mary University of London, UK\ncpapaioan@mail.ntua.gr\nABSTRACT\nRecent developments in MIR have led to several bench-\nmark deep learning models whose embeddings can be used\nfor a variety of downstream tasks. At the same time, the\nvast majority of these models have been trained on Western\npop/rock music and related styles. This leads to research\nquestions on whether these models can be used to learn\nrepresentations for different music cultures and styles, or\nwhether we can build similar music audio embedding mod-\nels trained on data from different cultures or styles. To that\nend, we leverage transfer learning methods to derive in-\nsights about the similarities between the different music\ncultures to which the data belongs to. We use two Western\nmusic datasets, two traditional/folk datasets coming from\neastern Mediterranean cultures, and two datasets belonging\nto Indian art music. Three deep audio embedding mod-\nels are trained and transferred across domains, including\ntwo CNN-based and a Transformer-based architecture, to\nperform auto-tagging for each target domain dataset. Ex-\nperimental results show that competitive performance is\nachieved in all domains via transfer learning, while the best\nsource dataset varies for each music culture. The imple-\nmentation and the trained models are both provided in a\npublic repository.\n1. INTRODUCTION\nAs the time passes by, more and more pre-trained models\nare being made available in the MIR ﬁeld. These models\ncan be used in a variety of tasks by providing informative\ndeep audio embeddings for music pieces. In correspon-\ndence with publicly available datasets, the vast majority\nof these models are trained on the so called “Western”1\nmusical tradition [1]. While studying world, folk, or tradi-\ntional music, that fact arises two research questions; on the\none hand what is the potential of these models when they\n1we use the term “Western” to denote music styles which mostly orig-\ninate from Western cultures, including pop, rock, and Western classical.\n© C. Papaioannou, E. Benetos, and A. Potamianos. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: C. Papaioannou, E. Benetos, and A. Potami-\nanos, “From West to East: Who can understand the music of the others\nbetter?”, in Proc. of the 24th Int. Society for Music Information Retrieval\nConf., Milan, Italy, 2023.are being used in the realm of a different culture and on the\nother hand how capable can a model be when trained on\na speciﬁc music tradition on providing meaningful audio\nembeddings.\nThere are several experimental setups one can employ\nin order to derive answers to the above questions. By tak-\ning into account the importance of the auto-tagging task in\nthe MIR ﬁeld [2], it becomes clear that transferring knowl-\nedge between domain-speciﬁc models to perform this task\nmay lead us to valuable insights. Automatic content-based\ntagging aims to predict the tags of a music piece given its\naudio signal. The audio signal includes the acoustic char-\nacteristics and some of them are responsible for the oc-\ncurrence of a tag in a piece, forming a multiple instance\nproblem [3].\nA variety of models have been proposed to cope with\nthe automatic tagging of music pieces. They can be di-\nvided, according to the input data they process, into the\nones that utilize time-frequency representations and the\nothers that accept the raw audio signal. In the ﬁrst cate-\ngory, CNN-based models which are adopted by the com-\nputer vision ﬁeld can be found, such as VGG-ish [4] as\nwell as speciﬁcally developed architectures for music, like\nMusicnn [5]. A Transformer-based architecture was re-\ncently proposed in [6] called Audio Spectrogram Trans-\nformer (AST). Regarding the models that process audio,\nthe TCNN [7] and the Wave-U-Net [8] architectures are\nbeing commonly used. For the purposes of our study, it\nis essential to use models of the same category with re-\nspect to the input they accept and, thus, we selected the\nones that process time-frequency representations because\nof their popularity in the MIR ﬁeld.\nWhile using deep neural networks, transfer learning of\na trained model can lead to a signiﬁcant performance im-\nprovement on the target domain, compared to one that\nstarts from a random state in the parameters space [9]. Typ-\nically, the weights of the target domain model are initial-\nized with the ones of a pre-trained model and then ﬁne-\ntuning is applied. During this step, one has to determine\nwhich of the layers will be trainable and which ones will\nbe kept frozen [10]. In general, it is not clear which part\nof the network should be allowed to be trained in the tar-\nget task and, thus, experimentation with different setups is\nnecessary. Standard methods include the ﬁne-tuning of the\nwhole network, as suggested in [11], as well as only the\nlast few layers or a part of the network, as in [12]. We311experiment with both setups to derive valuable insights on\nknowledge transfer across domains.\nEven though under-represented in general, datasets\nfrom speciﬁc music cultures are evident in the MIR ﬁeld\nand a set of the aforementioned methods have been used to\nperform several tasks. In [13] a classiﬁcation of Indian art\nmusic was conducted using deep learning models while au-\ntomatic makam recognition in Turkish music was carried\nout in [14, 15]. With respect to Western music, there are\nseveral research works performing auto-tagging via deep\nlearning models, as in [16] and [17].\nIn this paper, we incorporate a mosaic of different cul-\ntures by including six datasets from Western to Mediter-\nranean and Indian music. Three music audio embedding\nmodels, two that mainly consist of convolutional layers\nand a Transformer-based architecture, are utilized on both\nsingle-domain and transfer learning experimental setups\nfor music tagging. Results indicate that any model, de-\nspite the music culture that it is trained on, has the po-\ntential to adapt to another and achieve competitive results.\nWhen comparing the contributions of cross-domain knowl-\nedge transfers, we notice that they vary for each music\nculture and we suggest which one is the best candidate to\noutperform the single-domain approach. To the authors’\nknowledge, this is the ﬁrst study which attempts to explore\nwhether existing music audio embedding models can be\nused to transfer or learn representations for non-Western\ncultures. For reproducibility, we share the implementation\nin a public repository2.\n2. DATASETS\nThe selection of the datasets is a prominent theme in the\ncurrent study and it is constrained by the available corpora\nthat reﬂect different music cultures. By basing our intu-\nition on the location of each culture, we pursue to include\nthree distinct geographic regions each one represented by\ntwo corpora.\nEven though spread in several continents, we consider\nthe “West” as a single entity and utilize the MagnaTa-\ngATune [18] and FMA-medium [19] datasets that mainly\nbelong to this culture. The second region is the eastern\nMediterranean represented by the traditions of Greece and\nTurkey in our study with Lyra [20] and Turkish-makam\n[21] datasets. The Indian subcontinent is also incorporated\nwith Hindustani and Carnatic corpora [22], corresponding\nto the music traditions of the Northern and Southern areas\nof India respectively.\n2.1 MagnaTagATune\nMagnaTagATune [18] is a publicly available dataset that is\ncommonly used for the auto-tagging problem in the MIR\nﬁeld. It consists of more than 25,000 audio recordings,\nsumming to 210 hours of audio content at total. Each\naudio recording is annotated with a subset of the unique\n188 tags. Typically, only the top 50 most popular tags are\nused, which include annotations about genre, instruments\n2https://github.com/pxaris/ccmland mood. In Table 1, the most frequent tags for Mag-\nnaTagATune are presented along with the ones of the other\ndatasets.\n2.2 FMA-medium\nThe Free Music Archive [19] is an open and easily acces-\nsible dataset that is used for evaluating several tasks. It\ncontains over 100,000 tracks which are arranged in a hier-\narchical taxonomy of 161 genres. In order to keep the du-\nrations of the datasets balanced whenever possible, and to\ninclude genres belonging to Western music styles, we use\nFMA-medium that consist of 25,000 tracks of 30 seconds\neach. That means that its total duration is 208 hours, al-\nmost equal to the one of MagnaTagATune. With regards to\nthe metadata, we include the top-20 hierarchically related\ngenres of the music pieces.\n2.3 Lyra\nLyra [20] is a dataset for Greek traditional and folk music\nthat comprises 1570 pieces and metadata information with\nregards to instrumentation, geography and genre. Its to-\ntal duration is 80 hours which makes it the only dataset\nwith duration less than 200 hours in our study. We in-\ncorporate the top-30 tags retrieved from columns “genre”,\n“place” and “instruments” to form our multi-label classiﬁ-\ncation setup.\n2.4 Turkish-makam\nThe Turkish makam corpus [21, 23] includes thousands of\naudio recordings covering more than 2,000 works from\nhundreds of artists. It is part of CompMusic Corpora3[24]\nwhich comprises data collections that have been created\nwith the aim of studying particular music traditions. Us-\ning Dunya [25] and the related software tool4, we were\nable to get access to 5297 audio recordings, summing in\n359 hours, along with their metadata. In order to keep\nthe dataset sizes similar, we set a maximum audio dura-\ntion equal to 150 seconds which reduced the total length to\n215 hours. For the tags, the top-30 most popular with re-\ngards to “makam”, “usul” and “instruments” information\nhave been included.\n2.5 Hindustani\nThe Hindustani corpus [22] is also part of CompMusic\nCorpora. It includes 1204 audio recordings, with a total\nduration of 343 hours, covering a plethora of artists and\nmetadata categories. By setting the maximum audio du-\nration to 780 seconds, the size of the dataset has been de-\ncreased to 206 hours for the needs of our study. Further-\nmore, information about “raga”, “tala”, “instruments” and\n“form” has been used to form the labels of each piece. The\ntop-20 most frequent tags have been incorporated to our\nstudy as the target of the classiﬁcation models.\n3https://compmusic.upf.edu/corpora\n4https://github.com/MTG/pycompmusicProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n312MagnaTagATune FMA-medium Lyra Turkish-makam Hindustani Carnatic\nguitar 18.76% Rock 28.41% V oice 76.21% V oice 63.33% V oice 83.90% V oice 82.35%\nclassical 16.52% Electronic 25.26% Traditional 76.05% Kanun 31.09% Tabla 53.03% Violin 78.45%\nslow 13.71% Punk 13.28% Violin 57.34% Tanbur 27.93% Khayal 41.33% Mridangam 75.65%\ntechno 11.42% Experimental 9.00% Percussion 53.71% Ney 27.56% Harmonium 39.25% Kriti 70.87%\nstrings 10.55% Hip-Hop 8.80% Laouto 51.69% orchestra 26.38% Teentaal 35.35% adi 51.88%\ndrums 10.05% Folk 6.08% Guitar 37.34% Oud 24.36% Tambura 27.88% Ghatam 30.32%\nelectronic 9.74% Garage 5.67% Klarino 31.05% kemence 22.79% Ektaal 21.58% Khanjira 17.65%\nrock 9.17% Instrumental 5.40% Nisiotiko 26.85% Cello 17.83% Pakhavaj 7.88% rupaka 11.98%\nfast 8.92% Indie-Rock 5.17% place-None 25.16% Violin 17.62% Sarangi 7.30% mishra chapu 7.27%\npiano 7.95% Pop 4.74% Bass 24.76% Hicaz 10.63% Dhrupad 7.05% Tana Varnam 5.21%\nTable 1 . Relative frequencies of the top 10 most popular tags in each dataset.\n2.6 Carnatic\nThe Carnatic corpus [22] comprises 2612 audio record-\nings, summing in more than 500 hours of content. As\nwith the previous datasets, by setting a maximum dura-\ntion cut equal to 330 seconds, the total duration has been\ndecreased to 218 hours. Identical to Hindustani, the top-20\nmost popular annotations regarding “raga”, “tala”, “instru-\nments” and “form” have been included for the metadata.\n3. METHOD\nIn this section, the models which are used for the purposes\nof this study are ﬁrst presented. We, then, describe how\ntransfer learning is utilized to infer similarities between the\nmusic cultures by employing knowledge from the domain\nadaptation ﬁeld.\n3.1 Models\n3.1.1 VGG-ish\nAll of our models use the mel-spectrogram as their in-\nput, a commonly used feature for MIR tasks such as au-\ntomatic tagging [26]. This selection enables the utiliza-\ntion of CNN-based architectures which have been success-\nfully used in computer vision tasks. The Visual Geometry\nGroup (VGG) network [27] and its variants consist of a\nstack of convolutional layers followed by fully connected\nlayers.\nWe use a VGG-ish architecture, similar to the one im-\nplemented by the authors in [28], that is a 7-layer CNN,\nwith3×3convolution ﬁlters and 2×2max-pooling,\nfollowed by two fully-connected layers. It accepts mel-\nspectrograms that correspond to short chunks of audio as\nits input, with duration equal to 3.69 seconds.\n3.1.2 Musicnn\nMusicnn [17] is a music inspired model that uses convo-\nlutional layers at its core. Its ﬁrst convolutional layer con-\nsists of vertical and horizontal ﬁlters in order to capture\ntimbral and temporal features respectively. These features\nare, then, concatenated and fed to 1D convolutional lay-\ners followed by a pair of dense layers that summarize them\nand predict the relevant tags. Similar to VGG-ish, it usesmel spectrograms from short audio chunks at its input with\nduration 3 seconds.\n3.1.3 Audio Spectrogram Transformer\nAs its name indicates, Audio Spectrogram Transformer\n(AST) is a purely attention-based model for audio classi-\nﬁcation [6]. Based on the Transformer architecture [29],\nAST splits the input mel-spectrogram to 16×16patches in\nboth time and frequency dimensions that are, in turn, ﬂat-\ntened to 1D embeddings of size 768 using a linear projec-\ntion layer. A trainable positional embedding is also added\nto each patch embedding so that the model will capture\nthe spatial structure of the input 2D spectrogram. The re-\nsulting sequence is fed to the Transformer, where only the\nencoder is utilized since AST is designed for classiﬁcation\ntasks. The output of the encoder is followed by a linear\nlayer that predicts the labels. As the authors that intro-\nduced the architecture suggest, we set a speciﬁc cut to the\ninput length of the AST model that is equal to 8 seconds in\nall our experiments.\n3.2 Transfer Learning\nThe purpose of transfer learning is to improve the per-\nformance of the models on target domains by transferring\nknowledge from different but related source domains [30].\nIn the ﬁeld of MIR, both transferring feature representa-\ntions to the target domain from a pre-trained model on a\nsource task [31] as well as learning shared latent represen-\ntations across domains [32] have been proposed in the past.\nYet, these methods have not been applied to non-Western\nmusic datasets neither by adapting an existing model to\nthem nor by studying to what end these cultures can be\nvaluable source domains for widely developed models, two\naspects which are both studied in this work.\nAccording to the categorization conducted by the au-\nthors in [33], these methods belong to parameter sharing\ncategory of the model-based transfer learning techniques.\nIn the deep learning realm, it is, thus, common to use a\ntrained network for a source task, share its parameters and\nin turn ﬁne-tune some or all layers to produce a target net-\nwork. While following this method, one expects it to lead\nto better results when the participating domains are similarProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n313Model VGG-ish Musicnn AST\nMetric /\nDatasetROC-AUC PR-AUC ROC-AUC PR-AUC ROC-AUC PR-AUC\nMagnaTagATune 0.9123 0.4582 0.9019 0.4333 0.9172 0.4654\nFMA-medium 0.8889 0.4949 0.8766 0.4473 0.8886 0.5024\nLyra 0.8097 0.4806 0.7391 0.4042 0.8476 0.5333\nTurkish-makam 0.8696 0.5639 0.8505 0.5299 0.8643 0.5669\nHindustani 0.8477 0.6082 0.8471 0.6016 0.8307 0.5786\nCarnatic 0.7392 0.4278 0.7496 0.4182 0.7706 0.4394\nTable 2 . ROC-AUC and PR-AUC scores of the models on single domain auto-tagging tasks.\nto each other. Indeed, by studying the prior work on do-\nmain adaptation, one will ﬁnd that the main strategy con-\nsists of minimizing the difference between the source and\ntarget feature distributions, when transferring representa-\ntions from a labeled dataset to a target domain where la-\nbeled data is sparse or non-existent [34, 35].\nBy adapting the above rationale to our study, where the\nparticipating domains are all rich in labeled data, we expect\nthat when applying transfer learning by parameter sharing,\nthe more the similarity between the participating domains\nthe better the performance of the target domain on its su-\npervised learning task.\nIn order to study to what end this hypothesis stands in\ncomputational musicology with deep neural networks, we\nutilize the previously presented models which are widely\nused in the MIR ﬁeld and consist of different cores, namely\nconvolutional layers (VGG-ish and Musicnn) and a Trans-\nformer module (AST). Having the models trained on each\nsingle dataset, we apply all the cross-domain knowledge\ntransfers for each architecture by ﬁne-tuning only the out-\nput layer as well as the whole network. We then aggre-\ngate the results across the models seeking to derive in-\nsights with regards to the similarities between the domains\nas well as specifying which source is the best candidate for\neach target dataset.\n4. EXPERIMENTS\nAs already mentioned, we use mel spectrograms as the\ninput of all our models. In order to convert the audio\nrecordings of the datasets to this representation, we use Li-\nbrosa [36] to re-sample them to 16 kHz sample rate. Then,\n512-point FFT with a 50% overlap is applied, the maxi-\nmum frequency is set to 8 kHz and number of Mel bands\nto 128. Our intention, in this study, is not the optimization\nof the performance of the single-domain tasks but rather\nstudying the knowledge transfer across the domains. So,\nwe keep our training setup as close as possible to the liter-\nature, at each single domain task, in order to have a sanity\ncheck for the implementation.\nFor VGG-ish and Musicnn models, we use a mixture\nof scheduled Adam [37] and stochastic gradient descent\n(SGD) for the optimization method, identical to what the\nauthors at [28] have used. The batch size is set to 16 and\nthe learning rate to 1e−4for both models while the max-\nimum number of epochs are 200 for VGG-ish and 50 forMusicnn. With regards to the AST model, we follow the\nsetup proposed in [6], namely batch size 12, Adam opti-\nmizer, learning rate scheduling that begins from 1e−5\nand is decreased by a factor of 0.85 every epoch after the\n5th one as well as pre-trained on Imagenet Transformer\nweights.\nAll our models accept a ﬁxed size audio chunk at their\ninput but need to predict song-level tags. During the evalu-\nation phase, we aggregate the tag scores across all chunks\nby averaging them to acquire the label scores for the whole\naudio. We use the area under receiver operating character-\nistic curve (ROC-AUC), a widely used evaluation metric\non multi-label classiﬁcation problems and the area under\nprecision-recall curve (PR-AUC), a suitable metric for un-\nbalanced datasets [38].\nDuring transfer learning, we initialize all parameters of\nthe target model, except for the output layer, from each\nsource dataset and (i) allow only the output layer to be\ntrained and (ii) train the whole network. In both settings,\nwe use the same hyper-parameters and evaluation proce-\ndure with the single-domain setups across all datasets for\neach model architecture.\n5. RESULTS\nThe performance of the three models on all single-domain\ntasks can be seen in Table 2. The performance of the Mu-\nsicnn and VGG-ish models on MagnaTagATune is similar\nto the reported metrics in [28], which indicates the validity\nof our implementation. In general, the AST model shows\nthe best performance followed by VGG-ish and then Mu-\nsicnn. This result should not be taken into account solidly,\nbecause no hyper-parameter tuning has been taken place\nfor each domain and in order to keep the duration of the\ntraining to less than 24 hours for each task, the number of\nepochs for Musicnn was signiﬁcantly less than VGG-ish.\nOn the other hand, one should consider that the AST [6]\nand VGG-ish [28] models may, indeed, perform better for\nlimited time resources.\nIn Table 3, one can see the ROC-AUC scores in all\nsingle-domain and cross-domain setups. The rows are the\nsource datasets while the columns are the target datasets.\nA sub-table is constructed for each model architecture and\nfor a transfer from domain AtoB, the result of the ﬁne-\ntuning of only the output layer (‘output’) as well as all\nthe layers (‘all’) are reported. The single-domain setup isProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n314Target domain MagnaTagATune FMA-medium Lyra Turkish-makam Hindustani Carnatic\ntrainable layer(s) /\nSource domainoutput all output all output all output all output all output all\nVGG-ish\nMagnaTagATune - 91.23 88.11 92.39 74.69 85.40 76.79 86.84 76.09 85.04 67.19 74.71\nFMA-medium 85.82 91.29 - 88.89 68.56 84.04 75.40 87.78 75.77 84.39 67.03 74.56\nLyra 84.34 90.93 82.84 92.10 - 80.97 76.98 87.21 77.41 84.24 67.30 73.52\nTurkish-makam 85.19 90.90 84.41 91.74 70.93 82.38 - 86.96 77.54 85.32 67.16 73.50\nHindustani 84.24 91.02 83.83 91.91 66.27 79.71 77.25 87.63 - 84.77 66.72 74.63\nCarnatic 84.18 91.00 82.62 91.73 61.59 76.72 77.07 87.40 78.19 84.81 - 73.92\nMusicnn\nMagnaTagATune - 90.19 87.34 91.03 71.79 78.74 74.72 85.96 75.87 84.18 66.12 75.57\nFMA-medium 85.52 90.35 - 87.66 65.94 77.59 75.51 85.13 73.16 85.49 66.38 75.77\nLyra 81.38 90.03 82.23 90.80 - 73.91 74.11 85.20 78.10 83.29 65.09 75.51\nTurkish-makam 84.35 90.11 83.79 90.81 61.87 79.83 - 85.05 75.67 83.75 67.49 74.09\nHindustani 82.38 89.86 83.42 90.85 64.48 78.95 74.60 85.58 - 84.71 65.25 76.95\nCarnatic 83.02 90.05 82.78 90.74 61.83 77.92 75.09 85.43 75.34 84.19 - 74.96\nAST\nMagnaTagATune - 91.72 89.25 91.99 75.68 83.77 76.28 87.20 74.67 86.57 66.03 75.43\nFMA-medium 88.63 91.62 - 88.86 65.72 82.17 76.37 87.43 74.51 85.76 67.33 75.98\nLyra 87.49 91.44 87.44 92.43 - 84.76 77.08 86.80 72.24 83.73 68.47 76.59\nTurkish-makam 87.33 91.40 86.31 91.95 72.70 77.95 - 86.43 70.13 83.56 67.10 75.23\nHindustani 87.40 91.35 87.11 92.26 71.74 84.60 75.70 86.90 - 83.07 67.75 75.85\nCarnatic 87.42 91.45 86.83 91.75 63.33 81.44 76.87 87.14 74.11 82.91 - 77.06\nTable 3 . ROC-AUC scores (%) when applying transfer learning using the models VGG-ish, Musicnn and Audio Spectro-\ngram Transformer. Rows are the source domains and columns the target domains. After initializing the network with the\nparameters of the trained (at the source dataset) model, ﬁne-tuning on the output layer as well as on the whole network\nis applied. The diagonal values (under the “all” columns) correspond to the respective single-domain models (no transfer\nlearning) where the experimentation with only the output layer trainable has no meaning.\nwhen source and target is the same dataset and, thus, only\ntraining of the whole network has meaning. The table is\nbetter parsed column-wise, e.g., by inspecting the results\nof VGG-ish model on MagnaTagATune when transferring\nknowledge from the other domains at the upper-left pair of\ncolumns in the table.\nIn order to aggregate all the cross-domain knowledge\ntransfers, we follow the subsequent procedure: for each\ntarget task that consists of a speciﬁc model, target dataset\nand ﬁne-tuning method, min-max normalization is applied\nto theN−1transfer learning results, where Nis the num-\nber of all datasets. The previous step leads to the con-\nstruction of M×Fmatrices, Mthe number of the models\nandFthe number of ﬁne-tuning methods, where rows are\nthe source domains, columns the target domains and di-\nagonal elements are empty. Each cell has a value in the\nrange[0,1], as a result of the normalization step, while the\nvalue1corresponds to the knowledge transfer that led to\nthe best performance in the target domain. By calculating\nthe element-wise mean of the produced M×Fmatrices,\nwe reach to the result that can be seen in Figure 2.\n6. DISCUSSION\nThe results indicate that knowledge transfer both from\nWestern to non-Western cultures and the opposite can bebeneﬁcial when deep learning models are used to perform\nautomatic music tagging. Indeed, by inspecting Table 3,\nthe general take-home message one should acquire is that\nregardless of the model architecture, all datasets have the\npotential to contribute as a source to a target domain by\nproviding their deep audio embeddings. To investigate how\nvaluable knowledge transfers from widely used datasets\nto non-Western music cultures can be, we focus on the\nlast four datasets, i.e., the last eight columns of the ta-\nble, and parse the two ﬁrst rows, corresponding to Mag-\nnaTagATune and FMA datasets, at each model architec-\nture. For instance, we notice that for Lyra, when Musicnn\nis used and ﬁne-tuning only of the output layer is applied,\nthe model coming from MagnaTagATune has the greater\nROC-AUC score, namely 71.79%. Additionally, the AST\nmodel trained on the FMA-medium dataset, outperforms\nthe others when totally ﬁne-tuned to the Turkish-makam\ndataset, scoring 87.43%.\nIn order to study the inverse transfer direction, we cen-\nter our interest to the ﬁrst four columns of the entire table.\nEven though MagnaTagATune and FMA are almost always\nthe best source for each other, the deep audio embeddings\nprovided by the other datasets achieve competitive perfor-\nmance. For example, when MagnaTagATune is the target\ndomain and ﬁne-tuning is restricted to the output layer of\nthe network, we observe that transferring from Turkish-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n315Figure 1 . Average, over the three models, ROC-AUC scores of all cross-domain transfers when ﬁne-tuning of the output\nlayer is applied. The highest bar at each group corresponds to the respective single-domain model.\nFigure 2 . Cross-cultural music transfer learning results.\nRows correspond to the source datasets and columns to the\ntarget datasets. The value of each cell (knowledge trans-\nfer) is normalized and averaged across all models and ﬁne-\ntuning methods.\nmakam leads to a performance that is comparable to the\nbest source (FMA-medium) for all models.\nBy considering all cross-domain knowledge transfers,\none can specify the best candidate to provide a trained\nmodel, with a speciﬁc architecture, for each target dataset.\nWe, thus, notice that the model that is transferred from\nHindustani outperforms the others at the Carnatic dataset,\nwhen ﬁne-tuning on the whole Musicnn architecture is ap-\nplied. A holistic picture of the cross-cultural music transfer\nlearning is depicted in Figures 1 and 2.\nIn Fig. 1 the scores of all cross-domain transfers when\nﬁne-tuning the output layer, can be seen, averaged across\nthe three models. The uniformity of the performances\nof different sources at each target dataset can be exam-\nined. We, thus, recognize that the most unbalanced perfor-\nmances are spotted on the Lyra target domain, a result that\nis probably related to the smaller size of this dataset com-\npared to the others. By exploring Fig. 2 in a column-wise\nfashion, we observe that for MagnaTagATune as the tar-get domain, FMA-medium is the best source with a value\nequal to1. This means that in all transfer learning setups,\nthis source performed better than the others in this domain.\nBoth ﬁgures show that MagnaTagATune and FMA-\nmedium perform consistently well across the domains,\nsomething that possibly indicates their appropriateness for\nthe auto-tagging task. However, as we move to the East-\nern cultures, we notice that their contribution is somehow\ndecreased and other domains tend to contribute similarly\nor even more in those targets. The values at Fig. 2 should\nnot be considered solidly as similarity metrics between the\ndomains because other factors may also affect the results\nwe notice. It is, although, a ﬁrst step towards studying dif-\nferent music cultures using deep learning methods.\n7. CONCLUSIONS\nIn this paper, the transferrability of music cultures by uti-\nlizing deep audio embedding models is studied. To that\nend, six datasets and three models were employed while\nexperimentation with two ﬁne-tuning methods took place.\nThe automatic tagging of music pieces served as the su-\npervised learning task where all cross-domain knowledge\ntransfers were applied and evaluated.\nThe results show that state-of-the-art models can bene-\nﬁt from knowledge transfer not only from Western to non-\nWestern cultures but also the opposite too. By aggregating\nthe scores across all models and ﬁne-tuning methods, the\nsuitability of each source domain for a target task was cal-\nculated and, thus, which domain can be the best candidate\nto transfer knowledge from for each dataset was proposed.\nBased on the literature, we suggest that this result can be\ninterpreted to a degree as a similarity metric between the\nmusic cultures.\nWe identify that the current study has limitations. In\nthe future, the semantic similarities between the labels of\nthe involved domains will be examined. More datasets and\nmodels, like those that process raw audio signals, will be\nconsidered as well as semi-supervised and unsupervised\nlearning techniques. Other tasks may be employed such\nas mode estimation, assuming that key in Western cultures\nfunctions in a similar way with makam or raga in other\ncultures. All datasets can also be utilized to learn music\nembeddings in order to unveil cross-cultural links between\nacoustic features and tags.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3168. ACKNOWLEDGEMENTS\nThe authors would like to thank Sertan ¸ Sentürk, Alastair\nPorter and the Universitat Pompeu Fabra for their willing-\nness to provide us with the data without which this study\nwould not have been possible. We would like to also thank\nCharalampos Saitis and the reviewers for their valuable and\nconstructive comments that helped us improve our work.\n9. REFERENCES\n[1] E. Gómez, P. Herrera, and F. Gómez-Martin, “Com-\nputational Ethnomusicology: perspectives and chal-\nlenges,” Journal of New Music Research , vol. 42, no. 2,\npp. 111–112, June 2013.\n[2] K. Choi, “Deep Neural Networks for Music Tagging,”\nPh.D. dissertation, Queen Mary University of London,\nSeptember 2018.\n[3] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Pérez,\n“Solving the multiple instance problem with axis-\nparallel rectangles,” Artiﬁcial Intelligence , vol. 89,\nno. 1, pp. 31–71, January 1997.\n[4] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gem-\nmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt,\nR. A. Saurous, B. Seybold, M. Slaney, R. J. Weiss, and\nK. Wilson, “CNN architectures for large-scale audio\nclassiﬁcation,” in 2017 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\nMarch 2017, pp. 131–135.\n[5] J. Pons and X. Serra, “musicnn: Pre-trained convolu-\ntional neural networks for music audio tagging,” arXiv\npreprint arXiv:1909.06654 , September 2019.\n[6] Y . Gong, Y .-A. Chung, and J. Glass, “AST:\nAudio Spectrogram Transformer,” arXiv preprint\narXiv:2104.01778 , July 2021.\n[7] A. Pandey and D. Wang, “TCNN: Temporal Convolu-\ntional Neural Network for Real-time Speech Enhance-\nment in the Time Domain,” in ICASSP 2019 - 2019\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , May 2019, pp. 6875–\n6879.\n[8] D. Stoller, S. Ewert, and S. Dixon, “Wave-U-Net:\nA Multi-Scale Neural Network for End-to-End Audio\nSource Separation,” arXiv preprint arXiv:1806.03185 ,\nJune 2018.\n[9] Z. Yang, R. Salakhutdinov, and W. W. Cohen,\n“Transfer Learning for Sequence Tagging with\nHierarchical Recurrent Networks,” arXiv preprint\narXiv:1703.06345 , March 2017.\n[10] J. Yosinski, J. Clune, Y . Bengio, and H. Lipson, “How\ntransferable are features in deep neural networks?”\narXiv preprint arXiv:1411.1792 , November 2014.[11] R. Girshick, J. Donahue, T. Darrell, and J. Ma-\nlik, “Rich feature hierarchies for accurate object de-\ntection and semantic segmentation,” arXiv preprint\narXiv:1311.2524 , October 2014.\n[12] M. Long, Y . Cao, J. Wang, and M. I. Jordan, “Learn-\ning Transferable Features with Deep Adaptation Net-\nworks,” arXiv preprint arXiv:1502.02791 , May 2015.\n[13] A. K. Sharma, G. Aggarwal, S. Bhardwaj,\nP. Chakrabarti, T. Chakrabarti, J. H. Abawajy,\nS. Bhattacharyya, R. Mishra, A. Das, and H. Mahdin,\n“Classiﬁcation of Indian Classical Music With Time-\nSeries Matching Deep Learning Approach,” IEEE\nAccess , pp. 102 041–102 052, 2021.\n[14] E. Demirel, B. Bozkurt, and X. Serra, “Automatic\nmakam recognition using chroma features,” in Pro-\nceedings of the 8th International Workshop on Folk\nMusic Analysis; Thessaloniki, Greece, p. 19-24 , 2018.\n[15] K. K. Ganguli, S. ¸ Sentürk, and C. Guedes, “Cri-\ntiquing task-versus goal-oriented approaches: A case\nfor makam recognition,” in Proceedings of the 23rd Int.\nSociety for Music Information Retrieval Conf., Ben-\ngaluru, India , December 2022.\n[16] K. Choi, G. Fazekas, and M. Sandler, “Automatic tag-\nging using deep convolutional neural networks,” arXiv\npreprint arXiv:1606.00298 , June 2016.\n[17] J. Pons, O. Nieto, M. Prockup, E. Schmidt, A. Ehmann,\nand X. Serra, “End-to-end learning for music audio\ntagging at scale,” arXiv preprint arXiv:1711.02520 ,\nJune 2018.\n[18] E. Law, K. West, M. Mandel, M. Bay, and J. S. Downie,\n“Evaluation of algorithms using games: The case of\nmusic tagging,” in 10th International Society for Music\nInformation Retrieval Conference, ISMIR 2009 , 2009,\npp. 387–392.\n[19] M. Defferrard, K. Benzi, P. Vandergheynst, and\nX. Bresson, “FMA: A Dataset For Music Analysis,”\narXiv preprint arXiv:1612.01840 , September 2017.\n[20] C. Papaioannou, I. Valiantzas, T. Giannakopoulos,\nM. Kaliakatsos-Papakostas, and A. Potamianos, “A\nDataset for Greek Traditional and Folk Music: Lyra,”\ninProceedings of the 23rd Int. Society for Music Infor-\nmation Retrieval Conf., Bengaluru, India , December\n2022.\n[21] B. Uyar, H. S. Atli, S. ¸ Sentürk, B. Bozkurt, and\nX. Serra, “A corpus for computational research of turk-\nish makam music,” in Proceedings of the 1st Interna-\ntional Workshop on Digital Libraries for Musicology ,\n2014, pp. 1–7.\n[22] A. Srinivasamurthy, G. K. Koduri, S. Gulati, V . Ishwar,\nand X. Serra, “Corpora for music information research\nin indian art music,” in Proceedings of the 2014 In-\nternational Computer Music Conference, ICMC/SMC;\n2014 Sept 14-20; Athens, Greece , 2014.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n317[23] S. ¸ Sentürk, “Computational analysis of audio record-\nings and music scores for the description and discovery\nof ottoman-turkish makam music,” Ph.D. dissertation,\nUniversitat Pompeu Fabra, 2016.\n[24] X. Serra, “Creating research corpora for the compu-\ntational study of music: the case of the compmu-\nsic project,” in Audio engineering society conference:\n53rd international conference: Semantic audio , 2014.\n[25] A. Porter, M. Sordo, and X. Serra, “Dunya: A sys-\ntem for browsing audio music collections exploiting\ncultural context,” in Proceedings of the 14th Int. So-\nciety for Music Information Retrieval Conf., Curitiba,\nBrazil , 2013.\n[26] S. Dieleman and B. Schrauwen, “Multiscale ap-\nproaches to music audio feature learning,” in 14th In-\nternational Society for Music Information Retrieval\nConference (ISMIR-2013) , 2013, pp. 116–121.\n[27] K. Simonyan and A. Zisserman, “Very Deep Convolu-\ntional Networks for Large-Scale Image Recognition,”\narXiv preprint arXiv:1409.1556 , April 2015.\n[28] M. Won, A. Ferraro, D. Bogdanov, and X. Serra, “Eval-\nuation of CNN-based Automatic Music Tagging Mod-\nels,” arXiv preprint arXiv:2006.00751 , June 2020.\n[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Kaiser, and I. Polosukhin, “At-\ntention is All you Need,” in Advances in Neural Infor-\nmation Processing Systems , 2017.\n[30] S. J. Pan and Q. Yang, “A Survey on Transfer Learn-\ning,” IEEE Transactions on Knowledge and Data En-\ngineering , no. 10, pp. 1345–1359, October 2010.\n[31] A. van den Oord, S. Dieleman, and B. Schrauwen,\n“Transfer learning by supervised pre-training for\naudio-based music classiﬁcation,” in Conference of the\nInternational Society for Music Information Retrieval,\nProceedings , 2014.\n[32] P. Hamel, M. E. P. Davies, K. Yoshii, and M. Goto,\n“Transfer Learning In MIR: Sharing Learned Latent\nRepresentations For Music Audio Classiﬁcation And\nSimilarity,” in 14th International Conference on Music\nInformation Retrieval (ISMIR ’13) , 2013.\n[33] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y . Zhu, H. Zhu,\nH. Xiong, and Q. He, “A Comprehensive Survey on\nTransfer Learning,” arXiv preprint arXiv:1911.02685 ,\nJune 2020.\n[34] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko,\n“Simultaneous Deep Transfer Across Domains and\nTasks,” arXiv preprint arXiv:1510.02192 , October\n2015.\n[35] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapt-\ning Visual Category Models to New Domains,” in\nComputer Vision – ECCV 2010 . Springer, 2010, pp.\n213–226.[36] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,\nE. Battenberg, and O. Nieto, “librosa: Audio and music\nsignal analysis in python,” in Proceedings of the 14th\npython in science conference , vol. 8, 2015, pp. 18–25.\n[37] D. P. Kingma and J. Ba, “Adam: A Method\nfor Stochastic Optimization,” arXiv preprint\narXiv:1412.6980 , January 2017.\n[38] J. Davis and M. Goadrich, “The relationship between\nprecision-recall and roc curves,” in Proceedings of the\n23rd international conference on Machine learning ,\n2006, pp. 233–240.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n318"
    },
    {
        "title": "Self-Similarity-Based and Novelty-Based Loss for Music Structure Analysis.",
        "author": [
            "Geoffroy Peeters"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265397",
        "url": "https://doi.org/10.5281/zenodo.10265397",
        "ee": "https://zenodo.org/records/10265397/files/000089.pdf",
        "abstract": "Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity. \nIn this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels. \nFor this we jointly optimize \n- a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and \n- a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss. \nWe also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA. \nFinally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI.",
        "zenodo_id": 10265397,
        "dblp_key": "conf/ismir/Peeters23",
        "keywords": [
            "Music Structure Analysis",
            "Music boundary detection",
            "Self-Similarity-Matrix",
            "novelty score",
            "joint optimization",
            "relative feature learning",
            "Self-Attention",
            "SSM-loss",
            "novelty-loss",
            "standard RWC-Pop"
        ],
        "content": "SELF-SIMILARITY-BASED AND NOVELTY-BASED LOSS FOR MUSIC\nSTRUCTURE ANALYSIS\nGeoffroy Peeters\nLTCI, Télécom-Paris, Institut Polytechnique de Paris, France\nABSTRACT\nMusic Structure Analysis (MSA) is the task aiming at\nidentifying musical segments that compose a music track\nand possibly label them based on their similarity. In this\npaper we propose a supervised approach for the task of\nmusic boundary detection. In our approach we simulta-\nneously learn features and convolution kernels. For this\nwe jointly optimize - a loss based on the Self-Similarity-\nMatrix (SSM) obtained with the learned features, denoted\nby SSM-loss, and - a loss based on the novelty score ob-\ntained applying the learned kernels to the estimated SSM,\ndenoted by novelty-loss. We also demonstrate that relative\nfeature learning, through self-attention, is beneﬁcial for the\ntask of MSA. Finally, we compare the performances of our\napproach to previously proposed approaches on the stan-\ndard RWC-Pop, and various subsets of SALAMI.\n1 Introduction\nMusic Structure Analysis (MSA) is the task aiming at\nidentifying musical segments that compose a music track\n(a.k.a. segment boundary estimation) and possibly label\nthem based on their similarity (a.k.a. segment labeling).\nWe deal here with MSA from audio. MSA is one of the\noldest task in Music Information Retrieval1but still one\nof the most challenging. This is due to the difﬁculty to\nexactly deﬁne what music structure is and hence be able\nto create annotated datasets to measure progress or train\nsystems. People agree that the structure can be considered\nfrom multiple viewpoints2[2] [3], is hierarchical [4] and\nis partly subjective [5]. Probably because of this complex-\nity, the number of contributions in MSA has remained low\ndespite its large number of applications: audio summariza-\ntion [6], interactive browsing [7–9], musical analysis [10],\ntools for researcher (to help chord recognition [11], source\nseparation [12] or downbeat estimation [13]).\nTo solve the two MSA tasks (boundary detection and\nsegment labeling), three assumptions [14] are commonly\nused: (1) novelty (we assume that segments are deﬁned\n1Foote’s paper [1] on SSM was published in 1999.\n2musical role, acoustic similarity, instrument role, perceptual tests\nc/circlecopyrtG. Peeters. Licensed under a Creative Commons Attri-\nbution 4.0 International License (CC BY 4.0). Attribution: G. Peeters,\n“Self-Similarity-Based and Novelty-based loss for music structure anal-\nysis”, in Proc. of the 24th Int. Society for Music Information Retrieval\nConf., Milan, Italy, 2023.by large —novel— changes of the musical content over\ntime), (2) homogeneity (the musical content is homoge-\nneous within a given segment) and (3) repetition (the musi-\ncal content —homogeneous or not— can be repeated over\ntime). This has been extended by [15] to a fourth regu-\nlarity assumption (the segment’s durations are regular over\ntime). Combining those allows to construct MSA systems.\n1.1 Related works\nOver time, a large palette of approaches has been proposed\nfor MSA. We only review the ones related to our work and\nrefer the reader to Nieto et al. [16] for a good overview. We\nconsider three periods according to the nature of the audio\nfeatures –hand-crafted (HC) or learned by deep learning\n(DL) –, and the nature of the detection system which uses\nthe audio features – HC or trained by DL –.\nFirst period: HC detection system applied to HC au-\ndio features. In these systems HC audio features (such\nas MFCC or Chroma) were given as input to HC detec-\ntion system (such as the checkerboard kernel, novelty-\nscore [17]), unsupervised training (such as HMM [6],\nNMF [18]), supervised (such as OLDA [19]) or pattern\nmatching algorithms (such as DTW [20] or variants [21]) .\nSecond period: DL detection system applied to HC\naudio features. Over time, more and larger annotated\ndatasets for MSA have been developed; which concomi-\ntantly with the development of DL has allowed to re-\nformulate the MSA task in terms of supervised learning.\nThe detection system developed here mainly target the task\nof boundary detection. For example, [22] [23] [24] pro-\npose to train in a supervised way a Convolutional Net-\nworks (ConvNet) ˆy=fθ(X)to estimate if the center\nof a patch of HC audio features Xis a boundary ( y=1).\nVarious HC audio features (or combinations of) are used\nhere: Log-Mel-Spectrogram, Pich-Class-Proﬁle through\nSSM expressed in (time,time) or (time,lag).\nThird period: HC detection system applied to DL\naudio features. To deal with the endless debate about the\nchoice of HC audio features, McCallum et al. [25] pro-\npose to learn them. For this, they train an encoder fθ\nby minimizing a Triplet Loss (TL) [26] between patches\nof beat-synchronous Constant-Q-Transform (CQT). For\nthe TL, they propose a Self-Supervised-Learning (SSL)\nparadigm3to deﬁne the anchor Apatch, positive Ppatch\nand negative Npatch. Using the homogeneity assumption,\nneighboring times are supposed to be more similar to each\n3which does not require any annotated segments and labels749other (therefore used to deﬁne AandP) than to distant\nones (used to deﬁne N). For training they use a very large\nunlabeled dataset of 28345 songs. This method however\ndoes not consider the repetition assumption4.\nWang et al. [27] revised McCallum approach in a su-\npervised setting. In this, the patches P(resp.N) are now\nexplicitly chosen so as to have the same (resp. different)\nannotated segment label than the patches A. This super-\nvised method now consider both the homogeneity and rep-\netition assumption. In another work [28], they propose\na spectral-temporal Transformer-based model (SpecTNT)\ntrained with a connectionist temporal localization (CTL)\nloss to jointly estimate music segments ad their labels.\nMcCallum approach has also been extended by Buisson\net al. [29] to take beneﬁt from the hierarchy of structure in\nmusic. They show that the obtained deep embeddings can\nimprove segmentation at various levels of granularity.\nRather than learning features for MSA, Salamon et\nal. [30] proposed to re-use pretrained ones. Those are ob-\ntained using encoders previously trained on different tasks\n(Few-Shot Learning sound event and music auto-tagging).\nThose are then used as input to a Laplacian Structural De-\ncompositon algorithm for MSA.\n1.2 Proposal and paper organization\nFollowing the previous taxonomy, our proposal would be-\nlong to the category “DL detection system applied to DL\naudio features” . Unlike previous feature learning ap-\nproaches (that rely on a Triplet Loss paradigm), we utilize\na more straightforward paradigm (illustrated in Figure 1)\nwhich is a succession of two steps, each with its own ob-\njective. The two objectives are jointly optimized.\nIn the ﬁrst step , we learn the parameters θof an en-\ncoderfθsuch that when applied to the sequence of inputs\n{Xi}i∈{1...T}that represent a given track (where Tis the\nlength of temporal sequence), the encoded features allows\nthe estimation of a SSM, ˆSθ\nij, which attempts to reproduce\na ground-truth SSM, Sij. For training fθwe use an ap-\nproach similar to the SSM-Net approach proposed in [31],\ni.e. deﬁning a loss which directly compare the obtained\nSSMˆSθ\nijto a ground-truth SSM Sij.\nIn the second step , we learn a set of kernels Kθsuch\nthat when convolved over the main diagonal of the esti-\nmated SSM ˆSθ\nijit allows the estimation of a novelty score\nˆ nθ\ni, which attempts to reproduce a ground-truth novelty\nscore,ni. This novelty score is usually obtained using a\nﬁxed checkerboard kernel [32]. The resulting function is\nnamed novelty score since high values in it indicate times\nwhere the content change (it is homogeneous before and\nafter). It has been shown that better kernels can be used\n(for example using multi-scale kernels [33]) and that it is\npossible to train such kernels Kθconsidered as the kernels\nof a ConvNet (for example [22] and [23] in the case of a\n(time,lag) SSM or [24] in the case of a (time,time) SSM,\nwhich is our case). This is the approach we follow here.\n4Ncould potentially be in a segment which is a repetition of the seg-\nment containing AAnother proposal we make in this paper, is to consider\nthe learning of relative features, i.e. features which are\nrelative to the given track.\nPaper organization. We provide an overview of\nour system in part 2, describe the inputs to our system\n(part 2.1), detail the two losses (parts 2.2 and 2.3), moti-\nvate relative feature learning (part 2.4), detail the architec-\nture of our encoder fθ(part 2.5) and the training process\n(part 2.6). In part 3, we provide a large-scale evaluation\nof our proposal. It should be noted that although we only\nevaluate our method for the task of segment boundary de-\ntection, it can also be used for segment labeling given the\nclearness of the obtained SSM.\n2 Proposal\n2.1 Input data {Xi}\nThe inputs {Xi}to our system are simple patches5\nof Log-Mel-Spectrogram. We didn’t consider beat-\nsynchronous features as in [25] given the non-reliability\nof beat estimation outside popular music. Using\nlibrosa [34], we ﬁrst computed Mel-spectrogram with\n80 mel-bands, using a 92ms window length and 23ms hop\nsize. Those are then converted to log-amplitude using\nlog(1+100 ·mel). We then aggregate them (mean op-\nerator) over time to lead to a 0.1s hop size. The ﬁnal {Xi}\nare then patches of 40 successive frames (corresponding to\n4s.) with a hop size of 5 frames (corresponding to 0.5s.).\n2.2 SSM-loss\nGiven a sequence of inputs {Xi}i∈{1...T}, we apply the\nsame encoder fθindividually to each Xito obtain the cor-\nresponding sequence of embeddings {eθ\ni}i∈{1...T}. Those\nare then L2-normalized. We can then easily construct an\nestimated SSM, ˆSθ\nij, using a distance/similarity/divergence\ngbetween all pairs of projections:\nˆSθ\nij=g(eθ\ni=fθ(Xi),eθ\nj=fθ(Xj)),∀i,j (1)\nWe use here a “scaled” cosine-similarity for gwhich, be-\ncause the embeddings are L2-normalized, reduces to\nˆSθ\nij= 1−1\n4/bardbleθ\ni−eθ\nj/bardbl2\n2∈[0,1] (2)\nIt is then possible to compare ˆSθ\nijto a ground-truth bi-\nnary SSM, Sij, derived from annotations. We consider\nthis as a multi-class (a set of T2binary classiﬁcations)\nproblem and hence minimize the sum of Binary-Cross-\nEntropy (BCE) losses. However, given the unbalancing\nbetween the two classes in Sij(which contains much more\n0 than 1), we used a weighting factor λcomputed as the\npercentage of positive values in Sij. The lower λis, the\nmore we put emphasis on positive ( Sij=1) examples:\nLθ\nSSM=−1\nT2T/summationdisplay\ni,j=1(1−λ)/bracketleftBig\nSijlog(ˆSθ\nij)/bracketrightBig\n+λ/bracketleftBig\n(1−Sij)log(1−ˆSθ\nij)/bracketrightBig\n(3)\nSince the computation of the SSM ˆSθ\nijis differentiable\nw.r.t. to the embeddings {eθ\ni}, we can compute∂Lθ\nSSM\n∂θ:\n5We utilized patches as input (rather than frames) because we believe\nthat homogeneity exists at the pattern level rather than the frame level.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n750N*\nTransformer \nEncoder\nBlockInput\n(f=80, t=40)\n128128Estimated SSM\nGround-truth SSM\nÉ ÉFlatten\ntanhz }| {\n<latexit sha1_base64=\"MwHhWfgT/PaYVvsige0dOpWFu3g=\">AAADhHic1VHLSsNAFL1ptNb6qo+dm2ApuJCS+EBF0IILBTcVmlpoS0mm0xqaF5OJUEN/yV9w4z+IS3f6F96ZpqAW0a0zJDlz7jknc2fs0HUirusvSkadmc3O5ebzC4tLyyuF1bV6FMSMUJMEbsAathVR1/GpyR3u0kbIqOXZLr2xB+eifnNHWeQEfo0PQ9r2rL7v9BxicaQ6hcdWgGWbWYQmrZP/NUfJKN8pFPWyLoc2DYwUFM+e7l8vHjaSalB4hhZ0IQACMXhAwQeO2AULIpxNMECHELk2JMgxRI6sUxhBHr0xqigqLGQH+O7jqpmyPq5FZiTdBP/i4sPQqUEJPQHqGGLxN03WY5ks2J+yE5kp9jbEr51mechyuEX2N99E+Vef6IVDD45kDw72FEpGdEfSlFieiti59qkrjgkhcgJ3sc4QE+mcnLMmPZHsXZytJetvUilYsSapNoZ3sUu8YOP7dU6D+m7Z2C8fXxvFSgXGIwebsAXbeJ+HUIFLqIIJRCkpV0pNMdWsuqPuqQdjaUZJPevwZainH3HWy4s=</latexit>\nXi+1\n<latexit sha1_base64=\"AM/IqBnx7lwBmN1dHmkEnwA+uv0=\">AAAC0XicjVHLSsNAFD2Nr1pfVVciSLAIglASEdRdwY3gpqJ9QK2SpKMOTZMwmQilFNStP+BWt36H3yD+gf6Fd6Yp+EB0QpIz555zZu6MG/k8lpb1mjFGRsfGJ7KTuanpmdm5/PxCNQ4T4bGKF/qhqLtOzHwesIrk0mf1SDCn4/qs5rb3VL12xUTMw+BYdiPW7DgXAT/nniOJOj1xQ7/Vq/fPenzD7p/lC1bR0sP8CewUFEorN08H4nmpHOZfcIIWQnhI0AFDAEnYh4OYngZsWIiIa6JHnCDEdZ2hjxx5E1IxUjjEtul7QbNGygY0V5mxdnu0ik+vIKeJNfKEpBOE1Wqmric6WbG/Zfd0ptpbl/5umtUhVuKS2L98Q+V/faoXiXPs6B449RRpRnXnpSmJPhW1c/NTV5ISIuIUblFdEPa0c3jOpvbEund1to6uv2mlYtXcS7UJ3tUu6YLt79f5E1Q3i/ZWcffQLpRKGIwslrGKdbrPbZSwjzIqlC1wjwc8GkdG17g2bgdSI5N6FvFlGHcfKSmYHA==</latexit>\nXi\n<latexit sha1_base64=\"zIdm/9tasL2sVTmeIoNS3Xwbtc8=\">AAACzXicjVHLSsNAFD2Nr1pf1S7dBIvgQkoigroruHFnBfvAtpQkndahaRKSiVBqXbjxB9zqZyn+Qf0FV96ZpqAW0QlJzpx7zpm5M3bg8kgYxltKm5tfWFxKL2dWVtfWN7KbW5XIj0OHlR3f9cOabUXM5R4rCy5cVgtCZvVtl1Xt3qmsV29YGHHfuxSDgDX7VtfjHe5Ygqirhu277WFt1OKtbN4oGGros8BMQL6Ye/m4H4/2S372FQ204cNBjD4YPAjCLixE9NRhwkBAXBND4kJCXNUZRsiQNyYVI4VFbI++XZrVE9ajucyMlNuhVVx6Q3Lq2CWPT7qQsFxNV/VYJUv2t+yhypR7G9DfTrL6xApcE/uXb6r8r0/2ItDBseqBU0+BYmR3TpISq1ORO9e/dCUoISBO4jbVQ8KOck7PWVeeSPUuz9ZS9bFSSlbOnUQb413uki7Y/Hmds6ByUDAPCycXZr5YxGSksY0d7NF9HqGIM5RQpmwPj3jCs3auxdqtdjeRaqnEk8O3oT18ArIGl7E=</latexit>\nXi+2\n<latexit sha1_base64=\"LniKmHP8u0e9/bBn7c+QVdYxkrs=\">AAAC0XicjVHLSsNAFD2N73fVlQgSLIIglEQEdVdwI7ipaG3BVknSaR2aF5OJUEpB3foDbnXrd/gN4h/oX3hnjKAW0QlJzpx7zpm5M27s80Ra1kvOGBoeGR0bn5icmp6ZncvPL5wkUSo8VvEiPxI110mYz0NWkVz6rBYL5gSuz6puZ0/Vq5dMJDwKj2U3Zo3AaYe8xT1HEnVWdyO/2av1z3t8Y7N/ni9YRUsPcxDYGSiUVq4fD8TTUjnKP6OOJiJ4SBGAIYQk7MNBQs8pbFiIiWugR5wgxHWdoY9J8qakYqRwiO3Qt02z04wNaa4yE+32aBWfXkFOE2vkiUgnCKvVTF1PdbJif8vu6Uy1ty793SwrIFbigti/fJ/K//pULxIt7OgeOPUUa0Z152UpqT4VtXPzS1eSEmLiFG5SXRD2tPPznE3tSXTv6mwdXX/VSsWquZdpU7ypXdIF2z+vcxCcbBbtreLuoV0olfAxxrGMVazTfW6jhH2UUaFsgTvc48E4MrrGlXHzITVymWcR34Zx+w4ripgd</latexit>\nL2-NormEncoder fθ\n<latexit sha1_base64=\"Yw6mo3KNEAzBm/8APzO8CWZez4A=\">AAACzXicjVHLSsNAFD2Nr1pfVVfiwmARXJVEBHUXcOPOCvaBbS1JOm2DeZFMhFLrtj/gVj/KjfgHuvYHvDNNQS2iE5KcOfecM3NnrNB1Yq5prxllZnZufiG7mFtaXlldy69vVOIgiWxWtgM3iGqWGTPX8VmZO9xltTBipme5rGrdnIp69ZZFsRP4l7wfsqZndn2n49gmJ+qqcz1o8B7j5rCVL2hFTQ51GugpKBhbo1bwvPNRCvIvaKCNADYSeGDwwQm7MBHTU4cODSFxTQyIiwg5ss4wRI68CakYKUxib+jbpVk9ZX2ai8xYum1axaU3IqeKPfIEpIsIi9VUWU9ksmB/yx7ITLG3Pv2tNMsjlqNH7F++ifK/PtELRwfHsgeHegolI7qz05REnorYufqlK04JIXECt6keEbalc3LOqvTEsndxtqasv0mlYMXcTrUJ3sUu6YL1n9c5DSoHRf2weHKhFwwD45HFNnaxT/d5BANnKKFM2T4e8Ign5VxJlDvlfixVMqlnE9+GMvoEgaaXJg==</latexit>\nBack-propagation\nSi,j\n<latexit sha1_base64=\"hkGN9YeRvaCIXoMR0I61lLN3s2M=\">AAAC0XicjVHNSsNAGBzj/3/VoyDBIniQktRi9Vbw4rFS2wpWJUlXXU2zYbMRSimIHn0Br/pS4hvoI3jz220KehDdkGR2vpnZ/Xb9OOSJcpy3EWt0bHxicmp6ZnZufmExt7TcSEQqA1YPRCjkse8lLOQRqyuuQnYcS+Z1/JA1/Zt9XW/eMplwER2pbsxOO95lxC944Cmizlq+CNu9Wv+8x7eu++e5vFPY290plnZsp+A4ZbfoalAsl7ZLtkuMHvnK+vPD51pbVEXuFS20IRAgRQcMERThEB4Sek7gwkFM3Cl6xElC3NQZ+pghb0oqRgqP2Bv6XtLsJGMjmuvMxLgDWiWkV5LTxgZ5BOkkYb2abeqpSdbsb9k9k6n31qW/n2V1iFW4IvYv31D5X5/uReECu6YHTj3FhtHdBVlKak5F79z+1pWihJg4jdtUl4QD4xyes208ieldn61n6u9GqVk9DzJtig+9S7rg4S3av4NGseCWCnuHbr5SwWBMYRXr2KT7LKOCA1RRp2yJJzzjxapZXevOuh9IrZHMs4Ifw3r8AuVrmN4=</latexit>\neθ\ni=fθ(Xi)\n<latexit sha1_base64=\"FLsHCTimZn2+t7PJ22uUWeNi05c=\">AAAC73icjVHLSsNAFD3GV31XXboJFqG6KIko6kIouHFZwdZCqyVJp3UwTWIyKZTSD3Dnzp249Qfc6l+IX6D+givvTFPxgeiEJOeee8+ZuXPtwOWRMIynIW14ZHRsPDUxOTU9MzuXnl8oRX4cOqzo+K4flm0rYi73WFFw4bJyEDKrZbvsyD7bk/mjNgsj7nuHohOw45bV9HiDO5YgqpZeq9q+W++y3km3Kk6ZsHo1vtv4CLL9dJnY1Vo6Y+QMtfSfwExAJq+/XbQ3n/cKfvoRVdThw0GMFhg8CMIuLET0VGDCQEDcMbrEhYS4yjP0MEnamKoYVVjEntG3SVElYT2KpWek1A7t4tIbklLHCml8qgsJy910lY+Vs2R/8+4qT3m2Dv3txKtFrMApsX/pBpX/1cleBBrYVj1w6ilQjOzOSVxidSvy5PqnrgQ5BMRJXKd8SNhRysE960oTqd7l3Voq/6IqJStjJ6mN8SpPSQM2v4/zJyit58yN3M6Bmcnn0V8pLGEZWZrnFvLYRwFF8r7EHe7xoJ1rV9q1dtMv1YYSzSK+LO32HUiQpUo=</latexit>\neθ\ni+1\n<latexit sha1_base64=\"Ir42NKSTWU+1tdA5GkOrQSN0114=\">AAAC3HicjVG7SsRAFD3G9ztqYWETXBRRWBIR1G7Bxk4FVxd2dUlmRx02L5KJICGdndj6A7b6HX6CWNvoX3hnjOAD0QlJzpx7zp1753qxL1Jp2089Rm9f/8Dg0PDI6Nj4xKQ5NX2QRlnCeJ1FfpQ0PDflvgh5XQrp80accDfwfH7odbdU/PCcJ6mIwn15EfOjwD0NxYlgriSqbc62vMjv5Lw4zlvyjEu3aOdixSnaZsWu2npZP4FTgkptced5eYA97EbmI1roIAJDhgAcISRhHy5SeppwYCMm7gg5cQkhoeMcBUbIm5GKk8IltkvfU9o1SzakvcqZajejU3x6E3JaWCBPRLqEsDrN0vFMZ1bsb7lznVPVdkF/r8wVECtxRuxfvg/lf32qF4kTbOgeBPUUa0Z1x8osmb4VVbn1qStJGWLiFO5QPCHMtPPjni3tSXXv6m5dHX/RSsWqPSu1GV5VlTRg5/s4f4KD1aqzVt3ccyq1Gt7XEOYwjyWa5zpq2MYu6rr+W9zh3jg2Lo0r4/pdavSUnhl8WcbNG/6enEk=</latexit>\neθ\ni+2\n<latexit sha1_base64=\"Y9EmHpPYWBWOgW3uTVXW34qPRa8=\">AAAC3HicjVHLSsNAFD2NWmt9VV24cBMsiiiURAR1V3DjzgpWBaslmY51aF4kE0FCd+7ErT/gVr/DTxDXbvQvvDNG8IHohCRnzj3nzr1z3cgTibSsp4IxMDhUHC6NlEfHxicmK1PT+0mYxow3WeiF8aHrJNwTAW9KIT1+GMXc8V2PH7i9LRU/OOdxIsJgT15E/Nh3uoE4FcyRRLUrsy039DoZ759kLXnGpdNvZ2Jltd+uVK2apZf5E9g5qNYXd56Xi+yhEVYe0UIHIRhS+OAIIAl7cJDQcwQbFiLijpERFxMSOs7RR5m8Kak4KRxie/Tt0u4oZwPaq5yJdjM6xaM3JqeJBfKEpIsJq9NMHU91ZsX+ljvTOVVtF/R381w+sRJnxP7l+1D+16d6kTjFhu5BUE+RZlR3LM+S6ltRlZufupKUISJO4Q7FY8JMOz/u2dSeRPeu7tbR8RetVKzas1yb4lVVSQO2v4/zJ9hfrdlrtc1du1qv432VMId5LNE811HHNhpo6vpvcYd748S4NK6M63epUcg9M/iyjJs3AQ6cSg==</latexit>\n∂Lθ\nSSM\n∂θ\n<latexit sha1_base64=\"CpOzK7qWP2DmHMZV+NhfHiyaA5M=\">AAADAHicjVFPT9RAHH1UVMR/KyZeuDRuNJ427bJhIVw28eIBEgwukFAk02GWnTD9k+nUhDS9+E24cTNe/QKEcIGT8RvIxS/gxd8M3QQPRKdp++b93nszv5k4V7IwQfBjyrszfffe/ZkHsw8fPX7ytPVsbrPISs3FkGcq09sxK4SSqRgaaZTYzrVgSazEVnz41ta3PgldyCz9YI5ysZuwg1SOJGeGqL3WSjTSjFdRzrSRTPlRwsyYM1Wt1h+ryIyFYfVetbGxVtc3RA3faged5aXFbm/RDzpB0A+7oQXdfm+h54fE2NEevOZnv85/v1jPWt8RYR8ZOEokEEhhCCswFPTsIESAnLhdVMRpQtLVBWrMkrcklSAFI/aQvgc022nYlOY2s3BuTqsoejU5fbwiT0Y6Tdiu5rt66ZIte1t25TLt3o7oHzdZCbEGY2L/5Zso/9dnezEYYcn1IKmn3DG2O96klO5U7M79G10ZSsiJs3if6powd87JOfvOU7je7dkyV//plJa1c95oS1zZXdIFT27Rvx1sdjthr7P8PmwPBrgeM5jHS7yh++xjgHdYx5Cyj3GKC1x6n70T74v39VrqTTWe5/hreN/+AL0prdc=</latexit>\nLθ\nSSM(ˆSθ\nij,Sij)\n<latexit sha1_base64=\"7kJXm6mlbPuh1MG4KHCWYKwTFJI=\">AAADDHicjVHLThRBFD208hAFBl266TgxgYRMuscJA7tJWOhCEswwQELDpLqmYEqqH+muJiGd/gT4ExMX7IxbfsCFMVHX+hfeKnoSXBCtTnefe+49p+rWDVMlc+1536acBw+nZ2bnHs0/frKwuNRYfrqXJ0XGxYAnKskOQpYLJWMx0FIrcZBmgkWhEvvh2ZbJ75+LLJdJvKsvUnEUsdNYnkjONFHDxusgYnrMmSrfVsdloMdCs2pY9vvb1UowZroMwkSNyn51NyvfV2vuJGHD1WGj6bU2N9bbnXXXa3le12/7BrS7nVcd1yfGrGZv9ePVz93ry52k8RUBRkjAUSCCQAxNWIEhp+cQPjykxB2hJC4jJG1eoMI8aQuqElTBiD2j7ylFhzUbU2w8c6vmtIuiNyOli5ekSaguI2x2c22+sM6Gvc+7tJ7mbBf0D2uviFiNMbH/0k0q/1dnetE4wYbtQVJPqWVMd7x2KeytmJO7d7rS5JASZ/CI8hlhbpWTe3atJre9m7tlNv/LVhrWxLyuLfDbnJIGPJmiez/Ya7f8Tmvznd/s9XC75vAcL7BC8+yihzfYwYC8P+ALvuOHc+VcO5+cz7elzlSteYa/lnPzB1Fcsrs=</latexit>\nni\n<latexit sha1_base64=\"vrCzYyKXi63rYtKtOQ6ws8R/W1k=\">AAACz3icjVHNSsNAGJzGv1r/qh69BIvgQUpWxLa3ghePFawW2iJJuq1L0yQkG6WUinjzBbzqUym+QX0FT367TUEPohuSzM43M7vfrhN6IpaW9Z4x5uYXFpeyy7mV1bX1jfzm1kUcJJHL627gBVHDsWPuCZ/XpZAeb4QRtweOxy+d/omqX97wKBaBfy6HIW8P7J4vusK1JVGtlhN4nZE/vhqJ8VW+YBUty2KMmQqw0rFFoFIpH7KyyVSJRqG6/fr5MBkf1IL8G1roIICLBANw+JCEPdiI6WmCwUJIXBsj4iJCQtc5xsiRNyEVJ4VNbJ++PZo1U9anucqMtdulVTx6I3Ka2CNPQLqIsFrN1PVEJyv2t+yRzlR7G9LfSbMGxEpcE/uXb6b8r0/1ItFFWfcgqKdQM6o7N01J9KmonZvfupKUEBKncIfqEWFXO2fnbGpPrHtXZ2vr+kQrFavmbqpN8KF2SRc8u0Xzd3BxWGRHxcoZK1SrmI4sdrCLfbrPEqo4RQ11yg7xhGe8GGfGrXFn3E+lRib1bOPHMB6/AAOrmQk=</latexit>\nˆ n θ\ni\n<latexit sha1_base64=\"eT3b+Pbsd1E2jUgIHgtQEQ8nKN0=\">AAAC4HicjVG7TsMwFD2EV3kXGFkiKiSmKkY82q0SC2ORKCBRqJzUUIs0iRIHCUUZ2GBCrPwAK3wN4guAX2Di2qQSDAgcJTk+95xjX9uNfJkox3kZsoZHRsfGSxOTU9Mzs3Pl+YX9JExjT7S80A/jQ5cnwpeBaCmpfHEYxYL3XV8cuOfbun5wIeJEhsGeuozEcZ+fBfJUelwR1Skvtd3Q72btHldZkOedTOYnWVv1hOJ5p1xxqo7jMMZsDdjWpkOgXq+tsZrNdIlGpWF/3FxsvG43w/Iz2ugihIcUfQgEUIR9cCT0HIHBQUTcMTLiYkLS1AVyTJI3JZUgBSf2nL5nNDsq2IDmOjMxbo9W8emNyWljhTwh6WLCejXb1FOTrNnfsjOTqfd2SX+3yOoTq9Aj9i/fQPlfn+5F4RQ104OkniLD6O68IiU1p6J3bn/rSlFCRJzGXarHhD3jHJyzbTyJ6V2fLTf1N6PUrJ57hTbFu94lXfDgFu3fwf5ala1X67us0mjga5SwhGWs0n1uoYEdNNGi7Cs84BFPlmtdW7fW3ZfUGio8i/gxrPtPRnGfqw==</latexit>\nLθ\nnov(ˆnθ\ni,ni)\n<latexit sha1_base64=\"zClwDNU0uOXhX2/32xs9UggwlyA=\">AAADCnicjVHLahRBFD1pXzG+Rl26KRyECDJ0BTUzu4EguHARwUkC6ThU11QyRaq7mu7qgdD0Bwj+iTt34tYfcCXqB6i/4MpblR6Ii6DVdPe5595zqm7dtDC6cnH8dSW6cPHS5SurV9euXb9x81bv9p2dytalVBNpjS33UlEpo3M1cdoZtVeUSmSpUbvp8ZbP7y5UWWmbv3InhTrIxFGuD7UUjqhp71mSCTeXwjQv2tdN4ubKiXba5HbRridz4ZoktWbW5O3ZrG4fsSXvo4fTXj8exHHMOWce8M2nMYHRaLjBh4z7FK3+mP1+s3jyY2vb9r4gwQwWEjUyKORwhA0EKnr2wRGjIO4ADXElIR3yCi3WSFtTlaIKQewxfY8o2u/YnGLvWQW1pF0MvSUpGR6QxlJdSdjvxkK+Ds6ePc+7CZ7+bCf0TzuvjFiHObH/0i0r/1fne3E4xDD0oKmnIjC+O9m51OFW/MnZma4cORTEeTyjfElYBuXynlnQVKF3f7ci5H+GSs/6WHa1NX75U9KAl1Nk54OdjQF/PBi95P3xGKdrFfdwH+s0z02M8RzbmJD3O3zGN3yP3kbvow/Rx9PSaKXT3MVfK/r0B1kksfs=</latexit>\n∂Lθ\nnov\n∂θ\n<latexit sha1_base64=\"jjMrnpBA60LPQo6JolyShLNEiqE=\">AAADAHicjVE/TxRBHH0sKoj/DkxsbDZcNFaXHWLgLjSX0FhYQMIBCYtkdpjjJsz+yewsCdls4zexszO0fgFjbLQyfANs/AI2/mbYS6AgOpvdffN+772Z30xSaFXaKLqYCWbv3L03N39/4cHDR4+fdBaXdsq8MkKORK5zs5fwUmqVyZFVVsu9wkieJlruJicbrr57Kk2p8mzbnhXyIOXHmRorwS1Rh531eGy4qOOCG6u4DuOU24ngun7bvKtjO5GWN4d1lp82zTVRy3e6US+KIsZY6ABbW40IDAb9FdYPmSvR6A5fiq+/v/15tpl3fiLGEXIIVEghkcES1uAo6dkHQ4SCuAPUxBlCytclGiyQtyKVJAUn9oS+xzTbb9mM5i6z9G5Bq2h6DTlDvCBPTjpD2K0W+nrlkx17W3btM93ezuiftFkpsRYTYv/lmyr/1+d6sRij73tQ1FPhGdedaFMqfypu5+G1riwlFMQ5fER1Q1h45/ScQ+8pfe/ubLmvX3qlY91ctNoKv9wu6YKntxjeDnZWeux1b7DFusMhrsY8nmMZr+g+1zDEG2xiRNkf8AXf8SN4H3wMPgXnV9JgpvU8xY0RfP4Li0GuKw==</latexit>\nGround-truth NoveltyEstimated Novelty ˆSθ\ni,j\n<latexit sha1_base64=\"JU04i+o/E0kFT5z5tYyroevVZCc=\">AAAC4nicjVFBS9xAGH2mrbVa7VaPpRBchAqyJNtlV28BLz1adHVhs10m2Vl33GwSkokgIafeBAXptX+gt9L6X4oXz+2/6DezWWgPi05I8uZ9772Zb8aLA5FKy7pbMJ48fbb4fOnF8srL1bVXldfrx2mUJT5v+1EQJR2PpTwQIW9LIQPeiRPOJl7AT7zxvqqfnPMkFVF4JC9i3puw01AMhc8kUf3KW3fEZO56UTDID4viU+7KEZes6Odi56zoV6pWbW+3WW80TatmWS27bitQbzXeN0ybGDWqzvb36/uj26uDqPILLgaI4CPDBBwhJOEADCk9XdiwEBPXQ05cQkjoOkeBZfJmpOKkYMSO6XtKs27JhjRXmal2+7RKQG9CThNb5IlIlxBWq5m6nulkxc7LznWm2tsF/b0ya0KsxIjYh3wz5WN9qheJIXZ1D4J6ijWjuvPLlEyfitq5+U9XkhJi4hQeUD0h7Gvn7JxN7Ul17+psma7/1krFqrlfajP8UbukC57dojkfHNdrdqO299GuOg6mYwlvsIl3dJ8tOPiAA7Qp+zO+4Qd+GgPj0rgxvkylxkLp2cB/w/j6F7+SoKg=</latexit>\n5*Convolutional Block\nkt\nPReLU\nMax Pool\nConv-2D \nnc*(kf,kt)\nstride=1, sameMax-Pool \n(pf,pt)kf\n~<latexit sha1_base64=\"JqMi9HCjhiKSAtpIi5X9FrDWOH8=\">AAACznicjVHLSsNAFD3GV33Xx85NsAiuSiKCurLgQpcVrAq1SDKd1qF5MZkUtIhbf8Ct/oFf4T+IS3f6F96ZpqAW0QlJzpx7zp259/pJIFLlOK8j1ujY+MRkYWp6ZnZufqG4uHSSxplkvMbiIJZnvpfyQES8poQK+FkiuRf6AT/1O/s6ftrlMhVxdKyuEt4IvXYkWoJ5iqj6OROSBbzppeqiWHLKjln2MHBzUNp7vn47eFrpVePiC87RRAyGDCE4IijCATyk9NThwkFCXAM94iQhYeIcN5gmb0YqTgqP2A5927Sr52xEe50zNW5GpwT0SnLaWCdPTDpJWJ9mm3hmMmv2t9w9k1Pf7Yr+fp4rJFbhkti/fAPlf326FoUWdkwNgmpKDKOrY3mWzHRF39z+UpWiDAlxGjcpLgkz4xz02Tae1NSue+uZ+LtRalbvWa7N8KFvSQN2f45zGJxslt2t8u6RW6pU0F8FrGINGzTPbVRwiCpqpuP3eMCjVbW61o1125daI7lnGd+WdfcJE9KXzA==</latexit>\nKθ\n<latexit sha1_base64=\"sSyqxZ4aQ/J8Z/uOkXqku+TGW8E=\">AAACzXicjVHLSsNAFD2Nr1pf9bFzEyyCq5KIoK4suFBwYQX7wLZKkk5raF4kE6HWuvUH3Oon+BX+g7h0p3/hnWkKahGdkOTMueecmTtjBo4dcU17TSlj4xOTU+npzMzs3PxCdnGpHPlxaLGS5Tt+WDWNiDm2x0rc5g6rBiEzXNNhFbOzL+qVKxZGtu+d8m7AGq7R9uyWbRmcqLOj816dXzJu9C+yOS2vyaGOAj0Bub3n67eDp5Ve0c++oI4mfFiI4YLBAyfswEBETw06NATENdAjLiRkyzpDHxnyxqRipDCI7dC3TbNawno0F5mRdFu0ikNvSE4V6+TxSRcSFqupsh7LZMH+lt2TmWJvXfqbSZZLLMclsX/5hsr/+kQvHC3syB5s6imQjOjOSlJieSpi5+qXrjglBMQJ3KR6SNiSzuE5q9ITyd7F2Rqy/i6VghVzK9HG+BC7pAvWf17nKChv5vWt/O6JnisUMBhprGING3Sf2yjgEEWUKNvDPR7wqBwrsXKj3A6kSirxLOPbUO4+AfTal1s=</latexit>\nÉ ÉBack-propagation\nFigure 1 . Proposed architecture and training paradigm minimizing a SSM loss Lθ\nSSM and a novety loss Lθ\nnov.\n@Lθ\nSSM\n@θ=T/summationdisplay\ni;j=1@Lθ\nSSM\n@^Sθ\nij/parenleftBigg\n@^Sθ\nij\n@eθ\ni@eθ\ni\n@θ+@^Sθ\nij\n@eθ\nj@eθ\nj\n@θz\n(4)\nWe can then use standard gradient-descent algorithms to\noptimizeθwhich will jointly optimize fθfor all the Xi.\nOptimizing directly ^Sθ\nijhas relationship with Metric\nLearning / Contrastive Learning in which the A,P,Nare\nchosen based on their similarity (such as in Wang et al.\n[27]). In comparison, we consider here simultaneously all\npossible pairs of time as A,P,N. This is actually in line\nwith the fact that we aim at learning features relative to a\ntrack (see part 2.4) and we therefore need to consider si-\nmultaneously the interaction between all projections feθ\nig.\n2.3 Novelty-loss\nWe propose to learn the kernels Kθsuch that when con-\nvolved with the estimated SSM ^Sθ\nij(see eq.(2)) along its\nmain diagonal the resulting estimated novelty score ^ nθ\niap-\nproximate a ground-truth novelty score ni. This kernel\nconvolution can be simply implemented as an extra con-\nvolution layer (without bias) on top of the estimated SSM\n^Sθ\nijwith a sigmoid output activation. We then deﬁne the\nnovelty-loss as\nLθ\nnov=1\nTT/summationdisplay\ni=1BCE(^ nθ\ni;ni) (5)\n2.4 Relative feature learning\nIn previous works dealing with feature learning for MSA\nit is assumed that, once trained, the network fθalways\nprojects a given segment Xiin the same way whatever its\nsurrounding context.\nWe advocate here that for the task of MSA the projec-\ntion ofXishould depend on its context. The motivation\nfor doing so is that the features that highlight the tempo-\nral structure of a music track usually depend on the track\nitself. For example, if the instrumentation or the timbre re-mains constant over the track, the structure may arise from\nvariation of the harmonic content; in other cases, it will be\nthe opposite. Therefore, feature learning for MSA should\nbe made relative-to-a-track.\nTo let each feature Xi“know” about surrounding times\nfeaturesfX1:::Xi−1;Xi+1:::XTgwe introduce layers\nof Self-Attention (SA) [35] in our encoder6.\n2.5 Network architecture fθ\nThe architecture of the encoder fθis given in Figure 1. It is\nmade of a succession of 5 consecutive convolution blocks\nfollowed by Nblocks of Transformer-Encoder.\nEach convolution block is made of a 2D convolution\nfollowed by a PReLU [36] activation and a 2D max-\npooling. The kernel size (kf;kt), the number of channels\nncand pooling size (pf;pt)) of each layer are the follow-\ning: layer-1: (kf;kt)=(5,5)nc=32(pf;pt)=(2,2), layer-\n2: (5,5) 32 (2,2), layer-3: (5,5) 64 (2,2), layer-4: (5,5) 64\n(2,2), layer-5: (5,2) 128 (5,2). The output of the last convo-\nlutional blocks has dimension (1,1) with nc=128 channels\nand is ﬂattened to a 128-dim vector.\nEach input Xiis independently projected using the\nconvolutional blocks. These outputs are then considered\nas a temporal sequence which is fed to Nblocks of\nTransformer Encoder (each made up of a SA layer with\n8 heads, skip-connection, a normalization layer and two\nfully-connected layers with an internal dimension of 128).\nThe outputs are then passed to a tanh and L2-normalized.\nThey form a sequence of embeddings feθ\nigi2 f1:::Tgwith\neθ\ni2R128which are used to compute ^Sθ\nij.\nThe size of the kernels Kθis ﬁxed to (41,41) which\nroughly corresponds to 20s. The kernels Kθare either\ninitialized randomly or initialized with checkerboard ker-\nnels similar to the ones of [32]. In this case, checkerboard\nkernels have the same size (41,41) but are damped with\nGaussian function with different σ(randomly chosen in the\nrange[3s;5s]). We used 3 different kernels Kθwhich are\n6Note that the use of the SSM-loss alone does not allows fθto encode\nrelative features; this is the task of the SA.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n751then combined using (1x1) convolution. The diagonal of\nthe resulting feature-map then goes to a sigmoid activation\nand is considered as the estimated novelty ^ nθ\ni.\nOur architecture remains lightweight with a number of\nparameters ranging from 268K to 567K depending on the\nnumber of Transformer Encoder blocks (from N=0 to 3).\n2.6 Training.\nWe train our network by minimizing jointly the two losses\ndeﬁned by eq. (3) and eq. (5):\nLθ=αLθ\nSSM+(1−α)Lθ\nnov (6)\nWe used the ADAM optimizer with a learning rate of\n0.001, used early-stopping monitoring Lθon the validation\ndata with a patience of 50 and a maximum of 500 epochs.\nConsidering that we need the whole sequence of embed-\ndingsfeθ\nigof a track to compute ^Sθ\nijand get the gradients\n@Lθ\n@θ, the mini-batch-size mis here deﬁned as the number\nof tracks. We used a value of m=10 tracks.\n2.6.1 Generating ground-truth for training\nGround-truth SSM Sij.The ground-truth SSM, Sij, is\nconstructed using annotated segments (start and end time)\nand their associated labels. We rely on the homogene-\nity assumption, i.e. we suppose that all times tithat fall\nwithin a segment are identical since they share the same la-\nbel. If we denote by seg (ti)the segment tibelongs to and\nby label(seg(ti))its label, we assign the value Sij=1if\nlabel(seg(ti)) = label(seg(tj))and0otherwise. Note that\nwe could relax this identity constraint to allow represent-\ning similarity between labels (for example using an edit\ndistance between labels). This is for example important\nfor RWC-POP dataset, where labels denotes some proxim-\nities (verse A andverse B ) but are here considered as\ndifferent. Also, it could be important to consider the case\nof non-homogeneity of the repetitions and create a ground-\ntruthSijmade of “sub-diagonals” rather than “blocks”.\nGround-truth novelty score ni.The ground-truth\nnovelty score, ni, is also constructed using the anno-\ntated segments (start and end time). We set nito 1\nwhen segment changes at time i, 0 otherwise. As pro-\nposed by [37] we smooth over time the boundary annota-\ntions by applying a low-pass ﬁlter with a triangular-shape\nf0:25;0:5;1;0:5;0:25g.\n3 Evaluation\nWe assess here the performance of our proposal using var-\nious test sets, compare it to previously published results,\nconduct an ablation study, and illustrate its results.\n3.1 Datasets\nFor training we used a subset of 693 tracks from the Har-\nmonix dataset [38]7and the 298 tracks of the Isophonics\ndataset [39]. For testing we used\n7Given the non-accessibility of Harmonix audio, those have been\ndownloaded from YouTube and re-annotation has been necessary be-\ncause of non-synchronicity of the original annotations.Datasets T S L S L\nHarmonix 693 13 17.15\nIsophonics 298 11 15.98\nRWC-Pop-AIST 100 17 14.31\nUpper Lower\nSA-Pop (An1) 276 12 15.49 30 5.73\nSA-Pop (An2) 175 12 14.64 31 5.67\nSA-IA (An1) 444 14 18.32 50 4.43\nSA-IA (An2) 244 12.5 18.67 37 7.00\nSA-Two (An1) 882 11 18.25 30 6.89\nSA-Two (An2) 882 11 17.76 31 6.39\nTable 1 . Description of the datasets used in our evalua-\ntion: number of tracks T, median value of the number of\nsegments per track S, median value of segment duration L\nin seconds (note that [29] indicate Lin number of beats).\n•RWC-Pop-AIST the 100 tracks of the RWC-Pop [40]\nwith AIST annotations [41] and the following three\nsubsets of the SALAMI [3] dataset:\n•SA-Pop is the subset of SALAMI tracks with CLASS\nequal to Popular,\n•SA-IA is the subset of SALAMI tracks with SOURCE\nequal to IA (Internet Archive),\n•SA-Two is the subset of SALAMI tracks with at least\ntwo annotations.\nFor each SALAMI subset we considered the two an-\nnotations (An1, An2) and the two levels of ﬂat anno-\ntations (Upper, Lower); those correspond to the ﬁles\ntextfile{1,2}_{upper,lowercase}.txt .\nIn Table 1 we describe these datasets. According to the\nvalues of Lour training-sets better match the Upper anno-\ntations than the Lower ones of SALAMI. Also, the size of\nour kernels Kθ(roughly 20s., see part 2.5) assumes homo-\ngeneous segments of roughly 10s. and are therefore closest\nto theLof Upper annotations.\n3.2 Segment detection from novelty score\nTo get the estimated segment boundaries from the esti-\nmated novelty score ^ nθ\niwe used a simple peak-to-mean\nratio algorithm similar to [25]. Using the same notations\nas [25] eq. (5), we compute the mean with a window of du-\nration2T=20s, and then detect local peaks with a threshold\nτ=1.35 and a minimum inter-distance of 7s.\n3.3 Performance metrics\nWe evaluate the performance of segment boundary\ndetection using the common Hit-Rate metrics us-\ning precision-windows of 3s and 0.5s. We only\ndisplay here the Hit-Rate F-measures denoted by\nHR3F andHR0.5F . We used mir_eval [43] with\nmir_eval.segment.detection ignoring track start\nand end annotations ( Trim=True ). We point out that\nwithout “trimming” (the start and end time) we would gain\n+3% on average (from .713 to .743 for RWC-Pop).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n752RWC-Pop-AIST SA-Pop SA-IA SA-Two\nHR.5F HR3F HR.5F HR3F HR.5F HR3F HR.5F HR3F Annotation\nGrill [23, 42] GS1 .506 .715 - - - - .541 .623 Up./An-*\nMcCallum [25] Unsynch. - - - - - .497 - -\nBeat-synch. - - - - - .535 - -\nSalamon [30] DEF0.5,0.5/∗µH,γH - - - - - - .337 .563 Up./An-*\nWang [27] scluster/D/eu/mul .438 .653 .447 .623 - - .356 .553 Up./An-*\nBuisson [29] HE0/HE1 - .681 - - - - - .597 / .595 Up./An-1/2\n- - - - - .611 / .600 Low./An-1/2\nOurs (best conf.) .399 .713 .298 / .295 .631/ .624 .250 / .261 .520 / .511 .231 / .237 .521 / .530 Up./An-1/2\n.296 / .318 .570 / .610 .302 / .336 .547 / .612 .287 / .287 .589 / .589 Low./An-1/2\nAblation study N\nN=3/α=0.5/K:train-Init:chck .713 .532 .472 .448 Up./An-1\nN=2/α=0.5/K:train-Init:chck .701 .535 .474 .449 Up./An-1\nN=1/α=0.5/K:train-Init:chck .677 .631 .520 .521 Up./An1\nN=0/α=0.5/K:train-Init:chck .696 .535 .459 .443 Up./An-1\nAblation study α\nN=3/α=1/K:train-Init:chck .154 .121 .102 .111 Up./An-1\nN=3/α=0/K:train-Init:chck .007 .120 .026 .095 Up./An-1\nAblation study Kθ\nN=3/α=0.5/K:train-Init:randn .713 .543 .470 .457 Up./An-1\nN=1/α=0.5/K:train-Init:randn .709 .547 .470 .457 Up./An-1\nN=3/α=0.5/K:ﬁx-Init:chck .330 .250 .199 .196 Up./An-1\nTable 2 . Results of segment boundary detection using various test-sets and conﬁgurations\n3.4 Comparison with previous works\nIn the following we will compare our results with the ones\npreviously published by Grill and Schlüter in [23,42], Mc-\nCallum et al. in [25], Salamon et al. in [30], Wang et al.\nin [27] and Buisson et al. in [29]. We ﬁrst check if their\ntest-sets match ours.\nFor SA-Pop, Wang [27] used “a subset with 445 an-\nnotated songs (from 274 unique songs) in the “popular”\ngenre” . This roughly matches our SA-Pop (An1)+(An2)\nwhich provides 276+175=451 annotations. They used the\nUpper-case annotations (personal communication).\nFor SA-IA, McCallum [25] used “the internet archive\nportion of the SALAMI dataset (SALAMI-IA) consisting of\n375 hand annotated recordings” . This is much lower than\nour SA-IA (An1)+(An2) which provides 444+244=688 an-\nnotations. Moreover, it is not clear whether they used the\nUpper, Lower or Functional annotations.\nFinally, for SA-Two, Salamon [30] Table 3 used the\nUpper-case annotations of tracks with at least 2 annota-\ntions (884 tracks); Wang et al. [27] “we treat each an-\nnotation of a song separately, yielding 2243 annotated\nsongs in total” and Buisson et al. [29] used the Upper and\nLower-case annotations of tracks with at least 2 annota-\ntions (884 tracks). This roughly corresponds to our SA-\nPop (An1)+(An2) which has 882 tracks.\n3.5 Results and discussions\nResults are given in Table 2. The upper part shows pre-\nviously published results, although not all systems were\nevaluated on all test sets. The middle part shows the re-\nsults achieved with the best conﬁguration of our system.\nForRWC-Pop-AIST , we obtained a HR3F= .7138\nwhich is comparable to those of Grill and Schlüter (.715).\nHowever, for HR.5F our results are below (.399 < .506).\nThis can be explained by the fact that the hop-size of our\n8The Precision and Recall at 3seconds are P3F=.735, R3F=0.715featuresfXigwas chosen large (0.5s) and does not allow\nto have a precise boundary estimation. We have chosen a\nlarge hop size to reduce the size of ^Sθ\nij(hence the compu-\ntation time and memory footprints); it also allows to keep\nthe size of the Kθmanageable. Because of this, all our\nresults with HR.5F are actually low. Therefore, we only\ndiscuss the results for HR3F in the following.\nForSA-Pop , we obtained a HR3F of .631/.6249for\nthe two Upper annotations (Up./An-1/2) which is slightly\nabove those of Wang et al. (.623). For the two lower an-\nnotations (Low./An-1/2) we get a HR3F of .570/.61010.\nWang et al. does not provide these results.\nForSA-IA , we obtained a HR3F of .520/.51111for the\ntwo Upper annotations and .547/.61212for the two Lower\nannotations. This has to be compared to the .497 (unsyn-\nchronized) and .535 (beat-synchronized) obtained by Mc-\nCallum et al., but as explained, it is not clear whether they\nused Upper, Lower or Functional annotations.\nForSA-Two , we obtained a HR3F of .521/.53013for\nthe two Upper annotations. This is slightly lower than the\nresults of Wang et al. (.553), Salamon et al. (.563), Buis-\nson et al. (.597) and largely below the ones of Grill and\nSchlüter (.623). For the Lower annotations, we obtained a\nHR3F of .589/.58914which is slightly below the ones of\nBuisson et al. (.611). It should be noted however that in\nour work we didn’t used any data from SALAMI, neither\nfor training or validation (such as early stopping).\nFor SA-IA and SA-two, our results are higher for the\nLower annotations than the Upper ones. This is surprising\nsince according to Table 1 the characteristics ( Lvalue) of\nour training sets are closer to the Upper case. Also (see\nfootnotes 8–14), our algorithm tends to over-segment when\n9P3F=.581, R3F=0.760/ P3F=.566, R3F=0.771 →over-segmentation\n10P3F=.860, R3F=0.468/ P3F=.877, R3F=0.497 →under-segment.\n11P3F=.435, R3F=0.718/ P3F=.411, R3F=0.751 →over-segment.\n12P3F=.811, R3F=0.451/ P3F=.756, R3F=0.546 →under-segment.\n13P3F=.433, R3F=0.749/ P3F=.442, R3F=0.754 →over-segment.\n14P3F=.768, R3F=0.523/ P3F=.768, R3F=0.523 →under-segment.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n753considering the Upper annotation and under-segment when\nconsidering the Lower ones. Our kernel size is actually\nbetween the Lvalues of the Upper and Lower annotations.\n3.6 Ablation study\nIn the lower part of Table 2 we perform an ablation study\nof our system. For the SA-{Pop,IA,Two} test-sets, we only\nperform the study using the Upper/An1 annotations\nWe ﬁrst check the optimal number N2 f0;1;2;3gof\nlayers of Transformer Encoder. We see that for all test-sets\nthe use of Transformer Encoder ( N >0) is beneﬁcial. For\nRWC-Pop-AIST, the optimal number is N=3 while for all\nthree SA-{Pop,IA,Two} test-sets it is always N=1.\nWe then check whether jointly optimizing the two\nlossesLθ\nSSM andLθ\nnovof eq. (6) is necessary. We consid-\nered three cases: α=1 (only optimizing Lθ\nSSM ),α=0.5 (op-\ntimizing both), α=0 (only optimizing Lθ\nnov). For all test-\nsets, we see that optimizing jointly the two losses is highly\nbeneﬁcial: for example, for RWC-Pop-AIST, HR3F=.713\nwithα=0.5, .154 with α=1 and .007 for α=0.\nFinally, we check various conﬁgurations of the ker-\nnelsKθ.Kθis either [K:train-Init:chck]: trained start-\ning from checkerboard kernels initialisation, [K:train-\nInit:randn]: trained starting from random initialisations,\n[K:ﬁx-Init:chck]: ﬁxed (not trained) to checkerboard ker-\nnels (we still train the 1x1 convolution). We see that for all\ntest-sets it is beneﬁcial to train Kθ(the worst results are ob-\ntained with [K:ﬁx-Init:chck]). For RWC-Pop-AIST, the re-\nsults are the same whether kernels are initialized randomly\nor with checkerboard kernels. For SA-{Pop,IA,Two} the\ncheckerboard kernels initialization is beneﬁcial.\n3.7 Examples\nFigure 2 illustrates the three kernels Kθlearned using\nthe [N=3/ α=0.5/K:train-Init:chck] conﬁguration. As one\ncan see, while the middle one looks close to the classical\ncheckerboard kernel of Foote [32] (but with an emphasis\non the diagonal), the ﬁrst seems to highlight the transition\nfrom a non-homogeneous to an homogeneous part; while\nthe third seems a re-scaled version of the second (homo-\ngeneity at a larger scale). Figure 3 illustrates the ^Sθ\nijand\n^ nθ\niobtained by our system on track-01 from RWC-Pop-\nAIST (chosen as the ﬁrst item of our test-set). We com-\npare the results when trained in the [N=3 / α=0.5 / K:train-\nInit:chck] conﬁguration and with the untrained system us-\ning [K:ﬁx-Init:chck] for the kernels. For comparison we\nindicate the ground-truth Sijandni. In this ﬁgure, the\nbeneﬁts of training both Lθ\nSSM andLθ\nnovappears clearly.\nReproducibility. The code and the\ndatasets used in this work are available at:\nhttps://github.com/geoffroypeeters/ssmnet_ISMIR2023\n4 Conclusion\nIn this work, we proposed a simple approach for deep\nlearning-based Music Structure Analysis algorithm: we\n0 20 400\n10\n20\n30\n40\n0.2 0.0 0.2\n0 20 400\n10\n20\n30\n40\n0.00 0.05\n0 20 400\n10\n20\n30\n40\n0.05 0.00\nFigure 2 . The three kernels Kθlearned using the [N=3 /\nα=0.5 / K:train-Init:chck] conﬁguration.\nFigure 3 . [Top]^Sθ\nijand^ nθ\niobtained with untrained system\nusing [K:ﬁx-Init:chck] for the kernels, [Middle] same with\n[N=3 /α=0.5 / K:train-Init:chck], [Bottom] ground-truth\nSijandni.\nlearn an encoder fθsuch that the resulting learned fea-\ntures allow to best approximate a ground-truth SSM; we\njointly learn segmentation kernels that when applied to the\nestimated SSM we best approximate a ground-truth nov-\nelty score. We also propose to learn relative features, i.e.\nfeatures relative to a track, by introducing Self-Attention\nlayers in our encoder. According to HR3F, our results are\neither better than previous state-of-the-art (SA-Pop, SA-IA\nunsynchronous), similar (RWC-Pop-AIST) or worst (SA-\nTwo). Our approach has the advantage to be lightweight\n(around 500K parameters) and based on criteria which are\nsemantically linked to the task of MSA. Future works\nwill concentrate on making our approach applicable to\nﬁner temporal resolutions, therefore allowing improving\nour performances at HR.5F.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n7545 References\n[1] J. Foote, “Visualizing music and audio using self-\nsimilarity,” in Proc. of ACM Multimedia , Orlando,\nFlorida, USA, 1999, pp. 77–80.\n[2] G. Peeters and E. Deruty, “Is music structure anno-\ntation multi-dimensional ? a proposal for robust lo-\ncal music annotation,” in Proc. of LSAS (International\nWorkshop on Learning the Semantics of Audio Sig-\nnals) , Graz, Austria, 2009.\n[3] J. B. L. Smith, J. Burgoyne, I. Fujinaga, D. Roure,\nand J. S. Downie, “Design and creation of a large-\nscale database of structural annotations,” in Proc. of IS-\nMIR (International Society for Music Information Re-\ntrieval) , Miami, Florida, USA, 2011.\n[4] B. McFee, O. Nieto, M. M. Farbood, and J. P. Bello,\n“Evaluating hierarchical structure in music annota-\ntions,” Frontiers in Psychology , vol. 8, p. 1337, 2017.\n[5] M. Bruderer, M. McKinney, and A. Kohlrausch,\n“Structural boundary perception in popular music,” in\nProc. of ISMIR (International Society for Music Infor-\nmation Retrieval) , Victoria, Canada, 2006.\n[6] G. Peeters, A. Laburthe, and X. Rodet, “Toward au-\ntomatic music audio summary generation from signal\nanalysis,” in Proc. of ISMIR (International Society for\nMusic Information Retrieval) , Paris, France, 2002, pp.\n94–100.\n[7] G. Peeters, D. Fenech, and X. Rodet, “MCIpa: A music\ncontent information player and annotator for discover-\ning music,” in Proc. of ISMIR (International Society for\nMusic Information Retrieval) , Philadelphia, PA, USA,\n2008.\n[8] G. Peeters, F. Cornu, D. Tardieu, C. Charbuillet, J. J.\nBurred, M. Ramona, M. Vian, V . Botherel, J.-B. Rault,\nand J.-P. Cabanal, “A multimedia search and navigation\nprototype, including music and video-clips,” in Proc.\nof ISMIR (International Society for Music Information\nRetrieval) , Porto, Portugal, October 2012.\n[9] M. Goto, K. Yoshii, H. Fujihara, M. Mauch, and\nT. Nakano, “Songle: A web service for active mu-\nsic listening improved by user contributions.” in Proc.\nof ISMIR (International Society for Music Information\nRetrieval) , Miami, Florida, USA, 2011.\n[10] M. Mueller and N. Jiang, “A scape plot representation\nfor visualizing repetitive structures of music record-\nings,” in Proc. of ISMIR (International Society for Mu-\nsic Information Retrieval) , Porto, Portugal, 2012.\n[11] M. Mauch, K. Noland, and S. Dixon, “Using musical\nstructure to enhance automatic chord transcription,” in\nProc. of ISMIR (International Society for Music Infor-\nmation Retrieval) , Kobe, Japan, 2009.[12] Z. Raﬁi and B. Pardo, “Repeating pattern extraction\ntechnique (repet): A simple method for music/voice\nseparation,” Audio, Speech and Language Processing,\nIEEE Transactions on , vol. 21, no. 1, pp. 73–84, Jan-\nuary 2013.\n[13] M. Fuentes, B. McFee, H. C. Crayencour, S. Essid,\nand J. P. Bello, “A music structure informed downbeat\ntracking system using skip-chain conditional random\nﬁelds and deep learning,” in Proc. of IEEE ICASSP\n(International Conference on Acoustics, Speech, and\nSignal Processing) , Brighton, UK, 2019, pp. 481–485.\n[14] J. Paulus, M. Müller, and A. Klapuri, “Audio-based\nmusic structure analysis,” in Proc. of ISMIR (Inter-\nnational Society for Music Information Retrieval) ,\nUtrecht, The Netherlands, 2010.\n[15] G. Sargent, F. Bimbot, and E. Vincent, “A regularity-\nconstrained viterbi algorithm and its application to\nthe structural segmentation of songs,” in Proc. of IS-\nMIR (International Society for Music Information Re-\ntrieval) , Miami, Florida, USA, 2011.\n[16] O. Nieto, G. J. Mysore, C.-i. Wang, J. B. Smith,\nJ. Schlüter, T. Grill, and B. McFee, “Audio-based mu-\nsic structure analysis: Current trends, open challenges,\nand applications,” Transactions of the International So-\nciety for Music Information Retrieval , vol. 3, no. 1,\n2020.\n[17] J. Foote, “Automatic audio segmentation using a mea-\nsure of audio novelty,” in Proc. of IEEE ICME (Inter-\nnational Conference on Multimedia and Expo) , New\nYork City, NY , USA, 2000.\n[18] F. Kaiser and T. Sikora, “Music structure discovery\nin popular music using non-negative matrix,” in Proc.\nof ISMIR (International Society for Music Information\nRetrieval) , Utrecht, The Netherlands, 2010.\n[19] B. McFee and D. P. W. Ellis, “Learning to segment\nsongs with ordinal linear discriminant analysis,” in\nProc. of IEEE ICASSP (International Conference on\nAcoustics, Speech, and Signal Processing) , Florence,\nItaly, 2014.\n[20] M. Müller, N. Jiang, and P. Grosche, “A robust ﬁt-\nness measure for capturing repetitions in music record-\nings with applications to audio thumbnailing,” Audio,\nSpeech and Language Processing, IEEE Transactions\non, vol. 21, no. 3, pp. 531–543, 2013.\n[21] J. Serrà, M. Müller, P. Grosche, and J. L. Arcos, “Un-\nsupervised Music Structure Annotation by Time Se-\nries Structure Features and Segment Similarity,” IEEE\nTransactions on Multimedia , vol. 16, no. 5, pp. 1229–\n1240, Aug. 2014.\n[22] K. Ullrich, J. Schlüter, and T. Grill, “Boundary Detec-\ntion in Music Structure Analysis using ConvolutionalProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n755Neural Networks,” in Proc. of ISMIR (International So-\nciety for Music Information Retrieval) , Taipei, Taiwan,\n2014.\n[23] T. Grill and J. Schlüter, “Music Boundary Detec-\ntion Using Neural Networks on Combined Features\nand Two-Level Annotations,” in Proc. of ISMIR (In-\nternational Society for Music Information Retrieval) ,\nMalaga, Spain, 2015.\n[24] A. Cohen-Hadria and G. Peeters, “Music Structure\nBoundaries Estimation Using Multiple Self-Similarity\nMatrices as Input Depth of Convolutional Neural Net-\nworks,” in AES International Conference on Semantic\nAudio , Erlangen, Germany, June, 22–24, 2017.\n[25] M. C. McCallum, “Unsupervised Learning of Deep\nFeatures for Music Segmentation,” in Proc. of\nIEEE ICASSP (International Conference on Acoustics,\nSpeech, and Signal Processing) , Brighton, UK, May\n2019.\n[26] F. Schroff, D. Kalenichenko, and J. Philbin, “FaceNet:\nA uniﬁed embedding for face recognition and cluster-\ning,” in 2015 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , Jun. 2015, pp. 815–\n823, iSSN: 1063-6919.\n[27] J.-C. Wang, J. B. L. Smith, W.-T. Lu, and X. Song, “Su-\npervised metric learning for music structure features,”\ninProc. of ISMIR (International Society for Music In-\nformation Retrieval) , Online, November, 8–12 2021.\n[28] J.-C. Wang, Y .-N. Hung, and J. B. L. Smith, “To catch a\nchorus, verse, intro, or anything else: Analyzing a song\nwith structural functions.” in Proc. of IEEE ICASSP\n(International Conference on Acoustics, Speech, and\nSignal Processing) , Virtual and Singapore, May 2022.\n[29] M. Buisson, B. McFee, S. Essid, and H.-C. Crayen-\ncour, “Learning multi-level representations for hierar-\nchical music structure analysis,” in Proc. of ISMIR (In-\nternational Society for Music Information Retrieval) ,\n2022.\n[30] J. Salamon, O. Nieto, and N. J. Bryan, “Deep embed-\ndings and section fusion improve music segmentation,”\ninProc. of ISMIR (International Society for Music In-\nformation Retrieval) , Online, November, 8–12 2021.\n[31] G. Peeters and F. Angulo, “Ssm-net: Feature learn-\ning for music structure analysis using a self-similarity-\nmatrix based loss,” in Late-Breaking/Demo Session of\nISMIR (International Society for Music Information\nRetrieval) , 2022.\n[32] J. Foote, “Automatic audio segmentation using a mea-\nsure of audio novelty,” in Proc. of IEEE ICME (Inter-\nnational Conference on Multimedia and Expo) , New\nYork City, NY , USA, 2000, pp. 452–455.[33] F. Kaiser and G. Peeters, “Multiple hypotheses at mul-\ntiple scales for audio novelty computation within mu-\nsic,” in Proc. of IEEE ICASSP (International Confer-\nence on Acoustics, Speech, and Signal Processing) ,\nVancouver, British Columbia, Canada, May 2013.\n[34] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,\nE. Battenberg, and O. Nieto, “librosa: Audio and music\nsignal analysis in python,” in Proceedings of the 14th\npython in science conference , vol. 8, 2015.\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin,\n“Attention is all you need,” in Advances in Neural In-\nformation Processing Systems , vol. 30, 2017.\n[36] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochre-\niter, “Self-normalizing neural networks,” in Proceed-\nings of the 31st International Conference on Neural\nInformation Processing Systems , ser. NIPS’17. Red\nHook, NY , USA: Curran Associates Inc., 2017, pp.\n972–981.\n[37] J. Schlüter and S. Böck, “Improved musical onset de-\ntection with Convolutional Neural Networks,” in 2014\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , May 2014, pp. 6979–\n6983, iSSN: 2379-190X.\n[38] O. Nieto, M. McCallum, M. E. P. Davies, A. Robert-\nson, A. Stark, and E. Egozy, “The Harmonix Set:\nBeats, Downbeats, and Functional Segment Annota-\ntions of Western Popular Music,” in Proc. of ISMIR (In-\nternational Society for Music Information Retrieval) ,\nDelft, The Netherlands, 2019.\n[39] M. Mauch, C. Cannam, M. Davies, S. Dixon, C. Harte,\nS. Klozali, D. Tidhar, and M. Sandler, “Omras2 meta-\ndata project 2009,” in Late-Breaking/Demo Session of\nISMIR (International Society for Music Information\nRetrieval) , Kobe, Japan, 2009.\n[40] M. Goto, “Development of the RWC Music Database,”\nProc. of ICA (18th International Congress on Acous-\ntics), 2004.\n[41] ——, “Aist annotation for the rwc music database,”\ninProc. of ISMIR (International Society for Music In-\nformation Retrieval) , Victoria, BC, Canada, 2006, pp.\npp.359–360.\n[42] T. Grill and J. Schlüter, “Structural segmentation with\nconvolutional neural networks MIREX submission,”\n2015.\n[43] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P. W. Ellis, “mir_eval: A\ntransparent implementation of common mir metrics,”\ninProc. of ISMIR (International Society for Music In-\nformation Retrieval) , Taipei, Taiwan, 2014.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n756"
    },
    {
        "title": "Efficient Notation Assembly in Optical Music Recognition.",
        "author": [
            "Carlos Peñarrubia",
            "Carlos Garrido-Munoz",
            "Jose J. Valero-Mas",
            "Jorge Calvo-Zaragoza"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265253",
        "url": "https://doi.org/10.5281/zenodo.10265253",
        "ee": "https://zenodo.org/records/10265253/files/000020.pdf",
        "abstract": "Optical Music Recognition (OMR) is the field of research that studies how to computationally read music notation from written documents. Thanks to recent advances in computer vision and deep learning, there are successful approaches that can locate the music-notation elements from a given music score image. Once detected, these elements must be related to each other to reconstruct the musical notation itself, in the so-called notation assembly stage. However, despite its relevance in the eventual success of the OMR, this stage has been barely addressed in the literature. This work presents a set of neural approaches to perform this assembly stage. Taking into account the number of possible syntactic relationships in a music score, we give special importance to the efficiency of the process in order to obtain useful models in practice. Our experiments, using the MUSCIMA++ handwritten sheet music dataset, show that the considered approaches are capable of outperforming the existing state of the art in terms of efficiency with limited (or no) performance degradation. We believe that the conclusions of this work provide novel insights into the notation assembly step, while indicating clues on how to approach the previous stages of the OMR and improve the overall performance.",
        "zenodo_id": 10265253,
        "dblp_key": "conf/ismir/PenarrubiaGVC23",
        "keywords": [
            "Optical Music Recognition",
            "computational reading music notation",
            "recent advances in computer vision and deep learning",
            "successful approaches",
            "music-notation elements detection",
            "notation assembly stage",
            "neural approaches",
            "syntactic relationships",
            "handwritten sheet music dataset",
            "efficient models"
        ],
        "content": "EFFICIENT NOTATION ASSEMBLY IN OPTICAL MUSIC RECOGNITION\nCarlos Penarrubia1Carlos Garrido-Munoz1\nJose J. Valero-Mas2Jorge Calvo-Zaragoza1\n1U. I. for Computing Research, University of Alicante, Spain\n{carlos.penarrubia, carlos.garrido, jorge.calvo}@ua.es\n2Music Technology Group, Universitat Pompeu Fabra, Spain\njosejavier.valero@upf.edu\nABSTRACT\nOptical Music Recognition (OMR) is the ﬁeld of research\nthat studies how to computationally read music notation\nfrom written documents. Thanks to recent advances in\ncomputer vision and deep learning, there are successful ap-\nproaches that can locate the music-notation elements from\na given music score image. Once detected, these elements\nmust be related to each other to reconstruct the musical\nnotation itself, in the so-called notation assembly stage.\nHowever, despite its relevance in the eventual success of\nthe OMR, this stage has been barely addressed in the liter-\nature. This work presents a set of neural approaches to per-\nform this assembly stage. Taking into account the number\nof possible syntactic relationships in a music score, we give\nspecial importance to the efﬁciency of the process in order\nto obtain useful models in practice. Our experiments, using\nthe MUSCIMA++ handwritten sheet music dataset, show\nthat the considered approaches are capable of outperform-\ning the existing state of the art in terms of efﬁciency with\nlimited (or no) performance degradation. We believe that\nthe conclusions of this work provide novel insights into\nthe notation assembly step, while indicating clues on how\nto approach the previous stages of the OMR and improve\nthe overall performance.\n1. INTRODUCTION\nOptical Music Recognition (OMR) is the ﬁeld of research\nthat enables the automatic reading of music notation from\nscanned documents [1]. OMR has become increasingly\nimportant due to its potential for a better preservation of\nmusic archives, while also facilitating new data to the\nwealth of Music Information Retrieval algorithms that rely\non symbolic formats [2, 3].\nAs in many other ﬁelds, deep learning brought about\na drastic change in the performance of the proposed ap-\nproaches for OMR [4]. As we will mention in the next sec-\ntion, tasks that used to be a difﬁcult barrier are now feasible\n© C. Penarrubia, C. Garrido-Munoz, J.J. Valero-Mas, and\nJ. Calvo-Zaragoza. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: C. Penarrubia, C.\nGarrido-Munoz, J.J. Valero-Mas, and J. Calvo-Zaragoza, “Efﬁcient No-\ntation Assembly in Optical Music Recognition”, in Proc. of the 24th Int.\nSociety for Music Information Retrieval Conf., Milan, Italy, 2023.and successful models are known, e.g., staff detection [5]\nor the identiﬁcation of musical symbols in the image [6].\nHowever, although these tasks are the ﬁrst obstacles of an\nOMR system, they are not enough to complete the pro-\ncess. Once the graphic elements have been identiﬁed, it is\nnecessary to reconstruct the musical notation itself by in-\nferring the syntactic relationships that exist between such\nelements, namely notation assembly .\nTo account for all existing relations, the retrieval is usu-\nally performed in a pairwise fashion among all the identi-\nﬁed graphic units. On this note, given the (typically large)\ndensity of elements within music score images, the task\nexhibits a high computational complexity that complicates\nits integration in an end-user application. Therefore, in ad-\ndition to accuracy, one must carefully take into account the\nefﬁciency of this type of schemes.\nThis work addresses the efﬁcient estimation of all the\nsyntactic relations among the elements of a music score us-\ning neural network. More precisely, we propose and assess\ntwo approaches to address this task in an efﬁcient manner:\none that is based on classifying each pair of elements em-\nploying a series of numerical features, while the other uses\nasymmetric kernels [7], which can be computed with high\nparallelization and provide results very fast. In our exper-\niments, using the well-known MUSCIMA++ corpus, we\nwill compare the trade-off between effectiveness and efﬁ-\nciency that these methods provide and discuss the experi-\nmental outcomes. In addition, assuming that the previous\nstages of the process may contain errors, we also assess\nthe robustness of the assembly proposals by intentionally\ndegrading the estimations of these precedent phases. This\nanalysis is expected to provide useful insights for the ade-\nquate design of notation assembly methods in OMR.\nThe remainder of the paper is as follows: in Section\n2, we provide some background on the ﬁeld of OMR; in\nSection 3, we present the problem and the proposed ap-\nproaches; in Section 4, the complete experimental setup is\ndescribed; in Section 5, results are reported and discussed;\nand, ﬁnally, the main conclusions of the work are summa-\nrized in Section 6.\n2. RELATED WORK\nTraditionally, OMR has been considered a multi-stage pro-\ncess [8]. The legacy pipeline distinguishes four stages:182(i)image preprocessing , including tasks such as binariza-\ntion [9], distortion correction, or stave separation [10]; (ii)\nmusic symbol detection , including steps such as staff-line\nremoval [11], connected-component search, and classiﬁ-\ncation [12]; (iii) notation assembly , where the independent\ncomponents are related to each other to reconstruct the mu-\nsical notation [13]; and (iv) encoding , in which the recog-\nnized notation is exported to a speciﬁc language that can be\nstored and further processed by computational means [14].\nWith the rise of deep learning, many of these steps\nhave been reformulated as machine learning problems to\nbe solved by neural networks [15, 16]. Also, many stages\nhave been merged, giving rise to models that are capable\nof locating and categorizing the musical elements of the\ngiven image in a single step. This task has been the sub-\nject of extensive recent research [6, 17–19]. Alternatively,\nthe so-called holistic orend-to-end approaches that seek\nto perform the entire pipeline in a single step have also\nbeen proposed, often with some prior pre-processing such\nas staff segmentation [14, 20, 21].\nAlthough end-to-end approaches seem promising, so\nfar they have only been successfully implemented for\nmonodic music collections, where there is a clear left-to-\nright reading order. This is useful in many of the histori-\ncal music heritage, such as plainchant or mensural music,\nwhere the different voices (if any) typically appear on dif-\nferent pages or sections, and staves are therefore monodic\nin the graphical sense. However, to deal with the common\nwestern modern notation, multi-stage OMR approaches\nseem to be the only ones capable of dealing with such com-\nplexity [4].\nHowever, despite the aforementioned recent advances\nin the detection of music symbols with deep learning, there\nare hardly any proposals that complete the notation assem-\nbly stage employing machine learning techniques. To our\nknowledge, the only existing work that focuses on the re-\ntrieval of relationships using learning techniques is that of\nPacha et al. [22]. In such work, for each pair of nodes,\na single image is built with different channels: one that\ndepicts the area of the image that contains both nodes, an-\nother that depicts the same region but only shows the ﬁrst\nnode, and a last one that depicts also the region of inter-\nest but only with the second node. A Convolutional Neural\nNetwork (CNN) is then trained to recognize whether or not\nthere is a relationship between the nodes involved in this\nthree-channel image. Despite the reported good results,\nthe approach is tremendously inefﬁcient, since it requires\nthe independent construction and classiﬁcation of an image\nfor each pair of nodes. As we will see below, this scheme\nentails a huge computational complexity that makes it in-\nfeasible to use in practice.\nIn this paper, we especially focus on providing a so-\nlution to the notation assembly stage with a level of efﬁ-\nciency that enables its use in a real system, while keeping\ngood accuracy ﬁgures.3. METHODOLOGY\nThis paper follows the formulation proposed in previous\nworks [19, 22, 23], where it is assumed that the computa-\ntional reading of a music score, in the context of OMR, can\nbe described by retrieving a graph structure from the im-\nage. In this graph, the atomic notation elements (referred\nto as “primitives”) represent nodes, while edges denote the\nrelationships between them. Here, we are particularly in-\nterested in the retrieval of the edges, once the nodes have\nbeen detected somehow (for instance, with the existing ap-\nproaches mentioned in the previous section). Note that,\ninstead of relying on case-speciﬁc heuristics, we frame the\ntask within a learning-based formulation due to its inherent\ncapability of modeling any relationship among the primi-\ntives as far as there exists a set of annotated reference data.\nTherefore, the formulation is general and can be used as\nlong as there is a training set consistent with the envisioned\nmodel for the music-notation graph.\n3.1 Formulation\nA graph is a mathematical structure that models pairwise\nrelationships between elements—referred to as nodes or\nvertices—through its edges. Here, we aim to retrieve the\nedges (relationships) between each pair of nodes in music\nscores, where each node represents a music primitive—\ne.g., a notehead, a stem, or an accidental.1The formal\ndeﬁnition of the problem is as follows.\nWe assume that for a given music score sthere ex-\nists a graph gsthat represents its symbolic music notation.\nThe graph is deﬁned as a pair (V,E), whereVdenotes\nthe set of nodes and Edenotes the set of edges. Two\nnodesvi,vj∈Vare connected if there exists an edge\nei,j= (vi,vj)∈E.\nIn the context of OMR, information about the set of\nsymbols Vcorresponds to the music symbol detection\nstage of the OMR pipeline. Although they are still far from\nperfect, there are approaches in the literature that address\nthis stage (cf. Sect. 2). Therefore, we here assume that\nthere exists a function that maps sonto setV. Typically,\neach symbol vi∈Vis further represented as a set of fea-\ntures with, at least, the following information: primitive\nclass and coordinates within the image score. The problem\nwe address from now on is how to get the set EgivenV,\nwhich corresponds to the notation assembly stage of the\nOMR pipeline.\nThe problem can be considered as a binary classiﬁ-\ncation task in which a model predicts the class between\neach pair of nodes vi,vjpresent in the score. In this re-\ngard,ei,jis labeled as a 1if there is a relationship be-\ntweenviandvj, and0otherwise. The prediction of\nthe relationship—henceforth, ˆei,j—can be represented as\na function ϕ(vi,vj)that takes the two nodes’ features as\ninput and computes the probability of connection, i.e.,\nP(ei,j= 1). Figure 1 depicts a general outline of the\nmethodology adopted in this work.\n1Hereafter, we use the terms “node”, “symbol”, and “primitive” inter-\nchangeably: a graphical element placed in the music score with certain\nattributes.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n183Figure 1 : General schema of the methodology for retriev-\ning the edges of the music notation graph.\n3.2 Approaches\nFrom the formulation given above, it is important to em-\nphasize that the complexity of predicting each possible\nedge belongs to O(|V|2). Therefore, the approaches to ϕ\nmust take into account the computational cost to make the\ntask feasible in practice. This is a driving criterion for our\napproaches below since, with a sufﬁcient number of prim-\nitives in a score, no system depicting the aforementioned\ncomplexity would be practical in a real-world scenario.\nWe here propose two shallow neural architectures that\ntake a pair of nodes and predict the class of the relation-\nship. These two neural architectures are: (i) a Multilayer\nPerceptron (MLP) architecture that takes the input of each\nnode’s features concatenated; and (ii) an asymmetric ker-\nnel model.\n3.2.1 MLP architecture.\nIn this method, the features—attributes—of the nodes are\nﬁrst concatenated, forming a single feature vector that con-\ntains the entire information from the pair of nodes. Then,\nthis vector is passed through a series of layers of an MLP.\nThe ﬁnal layer implements a function σthat models the\nprobability of the two input nodes being connected:\nˆei,j=σ(ϕMLP([vi,vj]))\n3.2.2 Asymmetric kernels.\nIn this second scheme, our proposed neural architecture\nlearns an asymmetric kernel (AsymK) function [7]. This\nfunction is deﬁned by k(v1,v2) = (⟨φk1(v1),φk2(v2)⟩),\nwhere⟨·,·⟩is the dot product of two N-dimensional points\nin two Hilbert spaces—features spaces. In this work, we\nuse this asymmetric kernel as a similarity function between\nthe two mapped features to distinct Hilbert spaces as:\nˆei,j=σ(⟨φk1(vi),φk2(vj)⟩)\nIn this approach, φk1(v1),φk2(v2)are kernels imple-\nmented as dense neural layers that map the initial node\nfeatures onto two different (asymmetric) spaces that suit\nthe task at hand. After computing the similarity score, a σ\nfunction is applied to obtain probabilities between 0and1.\nNote that, since the embeddings are calculated only\nonce per node, the scheme is remarkably efﬁciency. Foreach possible relationship, it is only necessary to compute\nthe dot product between node embeddings and apply the σ\nfunction. That is why the complexity is much lower than\nthe previous approach.\n3.2.3 Loss function.\nIn both neural architectures proposed, the objective is to\nminimize the binary cross-entropy (BCE) loss function\nLBCE=/summationdisplay\nei,j∈Eei,jlog(ˆei,j)+(1−ei,j) log(1−ˆei,j)\n(1)\nwhereˆei,jcorresponds to the probability predicted by the\nmodel and ei,jis the ground-truth data for the edge ( 1for\na positive relationship, 0otherwise).\n4. EXPERIMENTS\nIn this section, we describe the experimental setup for eval-\nuating the neural architectures proposed. More precisely,\nthe rest of the section presents the corpus considered for\nthe experiments, the contemplated ﬁgures of evaluation,\nthe implementation details of the two neural proposals, and\nthe feature descriptions used.\n4.1 Data\nThe experiments were carried out using the MUSCIMA++\ndataset [23]. This corpus provides 140handwritten mu-\nsic scores with manual annotations of the different musical\nsymbols—primitives deﬁned by the symbol bounding box\nand the corresponding class label—and existing relation-\nships among them. The dataset provides the direction of\nthe edges; in our work, however, an undirected edge is as-\nsumed between two nodes that are connected regardless of\nthe speciﬁc direction (undirected graph). Figure 2 depicts\nan example from this corpus.\nFigure 2 : Example of a music score extracted from the\nMUSCIMA++ dataset.\nConcerning the data partitioning, we follow a 5-fold\ncross-validation scheme. At each iteration, 60% of the\ndataset is used for training, 20% is used for validation, and\n20% is used as test.\nFinally, it must be highlighted that each music sheet de-\npicts an average value of 734 primitives, which constitutes\na large number of relations to be modeled. Due to this,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n184and as aforementioned, efﬁciency must be considered in\nthe design of practical notation assembly strategies.\n4.2 Figures of merit\nWe consider a two-fold assessment of the proposed ap-\nproaches, i.e., we evaluate their recognition capabilities as\nwell as efﬁciency rates. These criteria are now detailed.\nIn terms of recognition performance, as in previous\nworks considering the same evaluation corpus [22,23], we\nresort to the F-measure (F 1) metric. Note that, instead of\nproviding the average scores for the two classes, the ﬁg-\nures reported exclusively refer to the positive relationships\nwith the aim of measuring the quality of the retrieval.\nConcerning the efﬁciency assessment, we measure the\ncomputation time in the prediction phase of the methods.\nSince this metric depends on the computational capabili-\nties of the device used, all methods are run over the same\nmachine to avoid any possible bias.2Moreover, each ex-\nperiment is repeated 10 times, being the average process-\ning time the one reported as the efﬁciency score.\n4.3 Neural architectures\nRegarding the MLP architectures, we consider two imple-\nmentations with a varying number of layers and weights\nto balance the trade-off between efﬁciency and representa-\ntional power:\n• MLP 64,512: A three-layered fully-connected net-\nwork comprising two hidden layers with 64 and 512,\nrespectively, with Rectiﬁer Linear Unit (ReLU) acti-\nvations and a single output unit to compute the score\nof the binary classiﬁcation.\n• MLP 32: A two-layered fully-connected network\ncomprising a 32-unit hidden layer and ReLu activa-\ntion and a single unit as output.\nConcerning the AsymK, φk1,φk2are implemented as\ntwo different 4-layered MLP comprising 512, 1024, 512,\nand 256 units, respectively, with ReLU activation. The\nidea is to generate two 256-dimensional embeddings—two\npoints in different Hilbert spaces—to then compute the\nsimilarity through the dot product.\nIn all cases, the last operation is implemented as a\nsigmoid activation function to understand the output as a\nprobability of a positive relationship. This probability is\neventually thresholded considering a value of 0.5to con-\nvert it in an actual decision.\nRegarding optimization, all models were trained for 200\nepochs using the Adam optimizer [24] with a learning rate\nof10−3.\n4.4 Feature description\nAs aforementioned, the music-object detection stage of an\nOMR process retrieves, at least, the position (coordinates)\nof the detected object in the image sheet together with its\n2The experiment was run over 8 cores of i7-7700K CPU at 4.20GHz\nwith 16 GB of RAM memory, with no explicit parallelization or GPU\nspeed-up.estimated class label. For our relationship prediction ap-\nproaches, we consider that each vertex ( vi∈V) is repre-\nsented only by these features, being the inclusion of addi-\ntional information left as future work.\nDelving on the features considered, the spatial (posi-\ntion) information is directly encoded using four normalized\nvalues that denote the top-left and right-bottom corners of\nthe bounding box. Conversely, the class information is pro-\ncessed by a 16-dimension learnable embedding layer to ob-\ntain an adequate representation for the task. Therefore, ev-\nery single node is ﬁnally represented as a 20-dimensional\nfeature vector.\n5. RESULTS\nHaving introduced the different neural proposals as well as\nthe experimental procedures, this section presents and dis-\ncusses the results obtained. To establish a reference in the\neffectiveness that can be obtained for this task, we include\nthe results of Pacha et al. [22], measured under the same\nexperimental conditions as the rest of the methods in the\nwork.3\nThe rest of the section separately studies and analyzes\nthe two individual aspects considered, i.e., performance ef-\nﬁciency and the ability to retrieve syntactic relationships\nbetween primitives.\n5.1 Performance efﬁciency\nFocusing ﬁrst on the temporal aspect of the strategies,\nTable 1 shows the per-page average execution time of\nthe contemplated notation assembly strategies. Note that,\nsince this evaluation disregards the correctness of the es-\ntimation but simply assesses its temporal cost, all experi-\nments are performed considering the ground-truth annota-\ntions.\nTable 1 : Efﬁciency results in terms of the per-page abso-\nlute execution time (in milliseconds) on the MUSCIMA++\ncorpus for the different notation assembly methods as-\nsessed. Each value corresponds to the average execution\ntime obtained with 10 different iterations over all test sam-\nples.\nAsymK MLP 32 MLP64,512 CNN [22]\nExecution\ntime (ms)<0.5 55 176 >1.5·106\nAs can be observed, the existing CNN method [22]\nproves to be the least efﬁcient among the considered strate-\ngies due to the large execution time it exhibits (roughly, 25\nminutes per page). Such a point directly disables its possi-\nble integration in any practical system that comprises user\ninteraction.\n3All experiments have been run considering the Python language\n(version 3.8), being the PyTorch (version 1.12.1) and PyTorch-lightning\nframeworks (1.9.1) particularly contemplated for reproducing the archi-\ntecture proposed in Pacha et al. [22].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n185Oppositely, the different neural proposals presented in\nthe work remarkably outperform these low-efﬁciency ﬁg-\nures, achieving execution times in the order of a few mil-\nliseconds per page. More in detail, the AsymK stands as\nthe most efﬁcient strategy of the proposed ones as it re-\nports ﬁgures several orders of magnitude faster than the\nMLP32and MLP 64,512. Note that this is because AsymK\nexploits the parallelization of the dot product operation and\nthe independent node processing while the two other pro-\nposals require more computation because of the classiﬁca-\ntion framework they are based on.\nIt must be pointed out that the presented neural archi-\ntectures depict execution times several orders of magnitude\nfaster than the reference state-of-the-art method. In this re-\ngard, while the CNN strategy may be further optimized,\nthe difference with the AsymK case—the most efﬁcient\nstrategy—must be considered as insurmountable.\n5.2 Recognition capability\nLet us now move to compare the estimation goodness of\nthe different notation assembly strategies. As aforemen-\ntioned, these methods take as input the result from a given\nframework that detects the primitives in the music score,\ni.e., an object detection strategy.\nTaking this into consideration, we will not consider any\nexisting object detection approach as starting point, since\nother additional issues should be taken into account which\nare outside the scope of this work— e.g., which object de-\ntection strategy to use, what conﬁdence level to actually\nretrieve an object, or how to evaluate cases where a node is\nmissing or has been predicted with the wrong label. Note\nthat, since these questions are not related to the retrieval\nof the relationships themselves, any decision in this regard\nmight bias the analysis of the models for the targeted nota-\ntion assembly stage.\nNevertheless, to cover a greater number of possibil-\nities in an agnostic way to the considered music-object\ndetection step, we will simulate inaccuracies in the lo-\ncation process of the nodes. Speciﬁcally, we will con-\nsider a set of ranges for the Intersection over Union\n(IoU) metric between the original objects—the ground-\ntruth annotations—and those generated for this experi-\nment.4For that, assuming that the MUSCIMA++ corpus\ndepict an IoU = 1(ground-truth annotation), we will pro-\ngressively perturb the location of the objects— i.e., altering\nthe coordinates of the bounding boxes—so that the over-\nall IoU metric degrades to the range IoU ∈[0.85,0.95].\nThis range will decrease in steps of 0.1 (i.e., [0.75–0.85],\n[0.65–0.75], ..., [0.05–0.15]) to simulate scenarios depict-\ning more limited symbol detection methods. In this way,\nour study focuses on general advantages and limitations of\nthe notation assembly models, which can be then consid-\nered for developing more adequate pipelines for the previ-\nous stages. Figure 3 shows examples of how node loca-\ntions are perturbed at some of the IoU levels considering\n4The IoU estimates the degree of overlap between two sets (in this\ncase, areas of two music-notation objects) as the ratio of their intersection\nand their union.the proposed strategy.\nConsidering this experimental set-up, Figure 4 shows\nthe recognition rates in terms of F 1achieved by the differ-\nent neural schemes contemplated with respect to increasing\nIoU conditions (x-axis).\nFor the ideal scenario of perfect object-detection re-\ntrieval (IoU = 1), the reference CNN method [22] reports\nthe highest recognition rate among all the schemes, with a\nvalue of F 1= 93.0%. However, the MLP 64,512proposal\nshows slightly lower ﬁgures to the reference strategy—\nF1= 91.9%—thereby proving itself as a competitive al-\nternative to the CNN-based method in terms of accuracy.\nIn relation to the AsymK and MLP 32proposals, these two\nstrategies depict the least competitive results among the\nones studied. However, since the MLP 32case shows a\nmore competitive performance than the AsymK method,\nthe former may be deemed as an intermediate case among\nthe best-performing strategies—CNN and MLP 64,512—\nand the AsymK approach.\nAs the music-object detection becomes more realistic\n(IoU<1), the neural models (except for CNN, which\nwill be discussed below) do not degrade ostensibly but ex-\nhibit certain robustness up to reasonable IoU cases (above\n0.5).5Digging deeper into the curves, the most rele-\nvant phenomenon is that, although at the higher ranges the\nCNN approach maintains the best accuracy, it decays much\nfaster than the MLPs. Speciﬁcally, from IoU ∈[0.75–\n0.85], the MLP 64,512outperforms it in terms of F 1, while\nmaintaining the clear advantage in efﬁciency reported in\nthe previous section. Furthermore, the rather shallow\nMLP32approach also outperforms the CNN from IoU ∈\n[0.55–0.65], which are still likely values for music-object\ndetection. These results reﬂect the adequacy of the efﬁcient\napproaches proposed in this work, which are not only efﬁ-\ncient enough to be used in practice but also keep greater\nrobustness against very common distortions in previous\nstages to that of notation assembly in the OMR pipeline.\nIn contrast, the AsymK shows a similar trend to the other\nefﬁcient approaches, so from the perspective of this exper-\niment it maintains the same advantages and disadvantages\nalready discussed above (even higher efﬁciency but very\npoor retrieval).\nAs a last point, it must be noted that the results reported\nin this work may be considered as a turning point for the\ndevelopment of novel approaches to music-object detec-\ntion in a complete OMR workﬂow. For example, using\nthe efﬁcient approaches of this work, one can prioritize re-\ntrieving most of the objects at the cost of slightly losing\nsome location accuracy. An object that is not detected is\nimpossible to relate correctly, but if it is detected, even if\ninaccurately it might connect successfully with other nodes\n(see Fig. 4).\nAs a ﬁnal note, the whole experiments clearly prove that\nthere is no single strategy capable of optimizing both con-\ntemplated criteria at the same time: high recognition rates\nimply large execution times ( e.g., the CNN method [22],\n5While it is true that the models obtain very poor results for the lowest\nranges, this is not relevant in practice because 0.5 is the minimum IoU\nthreshold for most object detectors to consider a correct retrieval.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n186(a) Simulated symbol detection performance of IoU ∈[0.45−0.55].\n(b) Simulated symbol detection performance of IoU ∈[0.85−0.95].\nFigure 3 : Examples obtained with our proposal under different simulated symbol-detection scenarios based on the overall\nIoU. The green and blue bounding boxes respectively denote the ground truth and the modiﬁed ones.\n 40 50 60 70 80 90 100\n 0.2  0.4  0.6  0.8  1F1 (%)\nIOU overlapCNN (Pacha et al., 2019)\nMLP (32)MLP (64-512)\nAsymK\nFigure 4 : Results in terms of the F 1metric for the com-\npared note assembly strategies for different object detec-\ntion performance rates based on the IoU score.\nwhich results impractical in real-world applications) while\nfaster strategies show more limited recognition rates (for\ninstance, the AsymK case). In this regard, the proposed\nMLP-based architectures seem to provide an adequate bal-\nance between the two evaluation criteria, being particularly\nrelevant to the MLP 64,512one as it shows a remarkable\ntemporal efﬁciency with a slightly worse performance than\nthe highest attainable recognition results by the CNN case.\n6. CONCLUSION\nOptical Music Recognition (OMR) represents the re-\nsearch ﬁeld that studies how to computationally read mu-\nsic notation from written documents. Generally, these\nstrategies comprise an initial phase in which the music-\nnotation elements from a given image are located—symbol\ndetection—followed by a notation assembly stage that esti-\nmates the relations among these elements to reconstruct the\nmusical notation itself. However, while there exist a large\nnumber of approaches that address the former process, the\nlatter one has been scarcely addressed in the related litera-ture.\nThis work frames in this particular assembly stage.\nConsidering the high number of possible relationships in\na music score, this work proposes two neural architectures\nto address this task in an efﬁcient manner: (i) a strategy\nbased on a Multilayer Perceptron (MLP) scheme; and (ii)\na model based on asymmetric kernels. The results obtained\nwith the MUSCIMA++ benchmark corpus [23] show that\nthe MLP-based approach achieves recognition rates com-\nparable to those of the reference strategy by Pacha et\nal. [22] with considerably less computational cost. More-\nover, the asymmetric kernel approach, while proven to be\nextremely fast, exhibits a noticeable loss of accuracy with\nrespect to the highest attainable one. In addition, these re-\nsults also prove MLP-based schemes as remarkably robust\nwhen facing adverse symbol detection scenarios compared\nto the state-of-the-art method.\nSeveral avenues of future research are opened: on the\none hand, it would be important to estimate the relevance\nof each error produced since it has not been yet studied\nwhat errors—missing positive relationships or predicting\nnon-existing relationships—and what type of elements in-\nvolved cause the most impact on the eventual OMR sys-\ntem. On the other hand, this work has considered the given\nlabeling of the MUSCIMA++; however, it has not been ex-\nplored in depth whether this annotation scheme is actually\nadequate for these learning algorithms. More consistent\nor easy-to-learn annotations may be possible, as long as\nthe goal of correctly encoding music notation is still met.\nBesides, just as the music-object detection step has been\nintegrated into a single process, it would be beneﬁcial to\ntrain end-to-end models that take into account both object\ndetection and notation assembly. In this way, the model\ncould leverage contextual and semantic information, pro-\nvided by the notation assembly stage, when detecting ob-\njects that would be otherwise difﬁcult or impossible. Fi-\nnally, it would be also relevant to carry out user studies to\nassess the usefulness of these efﬁcient approaches in real-\nworld OMR scenarios.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1877. ACKNOWLEDGMENT\nWork produced with the support of a 2021 Leonardo Grant\nfor Researchers and Cultural Creators, BBV A Foundation.\nThe Foundation takes no responsibility for the opinions,\nstatements and contents of this project, which are entirely\nthe responsibility of its authors.\n8. REFERENCES\n[1] D. Bainbridge and T. Bell, “The challenge of optical\nmusic recognition,” Computers and the Humanities ,\nvol. 35, pp. 95–121, 2001.\n[2] C. Müller, Between Digital Transformation in Li-\nbraries and the Digital Humanities: New Perspectives\non Librarianship . De Gruyter, 2020, pp. 379–384.\n[3] E. R. Miranda, Handbook of artiﬁcial intelligence for\nmusic . Springer, 2021.\n[4] J. Calvo-Zaragoza, J. Hajic Jr., and A. Pacha, “Under-\nstanding Optical Music Recognition,” ACM Comput.\nSurv. , vol. 53, no. 4, pp. 77:1–77:35, 2020.\n[5] F. J. Castellanos, C. Garrido-Munoz, A. Ríos-Vila, and\nJ. Calvo-Zaragoza, “Region-based layout analysis of\nmusic score images,” Expert Systems with Applica-\ntions , vol. 209, p. 118211, 2022.\n[6] A. Pacha, J. Haji ˇc, and J. Calvo-Zaragoza, “A baseline\nfor general music object detection with deep learning,”\nApplied Sciences , vol. 8, no. 9, p. 1488, 2018.\n[7] W. Wu, J. Xu, H. Li, and S. Oyama, “Asymmetric ker-\nnel learning,” Technical Report, Microsoft Research ,\n2010.\n[8] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. Marcal,\nC. Guedes, and J. S. Cardoso, “Optical music recog-\nnition: state-of-the-art and open issues,” International\nJournal of Multimedia Information Retrieval , vol. 1,\nno. 3, pp. 173–190, 2012.\n[9] J. A. Burgoyne, L. Pugin, G. Eustace, and I. Fuji-\nnaga, “A comparative survey of image binarisation al-\ngorithms for optical recognition on degraded musi-\ncal sources,” in Proceedings of the 8th International\nConference on Music Information Retrieval , 2007, pp.\n509–512.\n[10] M. Kletz and A. Pacha, “Detecting Staves and Mea-\nsures in Music Scores with Deep Learning,” in Pro-\nceedings of the 3rd International Workshop on Reading\nMusic Systems , Alicante, Spain, 2021, pp. 8–12.\n[11] J. dos Santos Cardoso, A. Capela, A. Rebelo,\nC. Guedes, and J. P. da Costa, “Staff Detection with\nStable Paths,” IEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 31, no. 6, pp. 1134–1139, 2009.[12] A. Rebelo, G. A. Capela, and J. S. Cardoso, “Optical\nrecognition of music symbols - A comparative study,”\nInt. J. Document Anal. Recognit. , vol. 13, no. 1, pp.\n19–31, 2010.\n[13] F. Rossant and I. Bloch, “Robust and Adaptive OMR\nSystem Including Fuzzy Modeling, Fusion of Musical\nRules, and Possible Error Detection,” EURASIP Jour-\nnal on Advances in Signal Processing , vol. 2007, no. 1,\np. 081541, 2006.\n[14] A. Ríos-Vila, J. Calvo-Zaragoza, and D. Rizo, “Evalu-\nating simultaneous recognition and encoding for opti-\ncal music recognition,” in Proceedings of the 7th Inter-\nnational Conference on Digital Libraries for Musicol-\nogy. ACM, 2020, pp. 10–17.\n[15] A. Pacha and H. Eidenberger, “Towards a univer-\nsal music symbol classiﬁer,” in Proceedings of the\n12th International Workshop on Graphics Recognition .\nIEEE, 2017, pp. 35–36.\n[16] F. J. Castellanos, A. Gallego, and J. Calvo-Zaragoza,\n“Automatic scale estimation for music score images,”\nExpert Syst. Appl. , vol. 158, p. 113590, 2020.\n[17] A. Pacha, K. Choi, B. Coüasnon, Y . Ricquebourg,\nR. Zanibbi, and H. Eidenberger, “Handwritten music\nobject detection: Open issues and baseline results,” in\nProceedings of the 13th IAPR International Workshop\non Document Analysis Systems . IEEE Computer So-\nciety, 2018, pp. 163–168.\n[18] L. Tuggener, Y . P. Satyawan, A. Pacha, J. Schmidhu-\nber, and T. Stadelmann, “The DeepScoresV2 Dataset\nand Benchmark for Music Object Detection,” in Pro-\nceedings of the 25th International Conference on Pat-\ntern Recognition . IEEE, 2020, pp. 9188–9195.\n[19] C. Garrido-Munoz, A. Ríos-Vila, and J. Calvo-\nZaragoza, “Retrieval of music-notation primitives via\nimage-to-sequence approaches,” in Proceedings of\nthe 10th Iberian Conference on Pattern Recognition ,\nser. Lecture Notes in Computer Science, vol. 13256.\nSpringer, 2022, pp. 482–492.\n[20] J. Calvo-Zaragoza, A. H. Toselli, and E. Vidal, “Hy-\nbrid hidden markov models and artiﬁcial neural net-\nworks for handwritten music recognition in mensural\nnotation,” Pattern Anal. Appl. , vol. 22, no. 4, pp. 1573–\n1584, 2019.\n[21] C. Garrido-Munoz, A. Ríos-Vila, and J. Calvo-\nZaragoza, “A holistic approach for image-to-graph: ap-\nplication to optical music recognition,” Int. J. Doc-\nument Anal. Recognit. , vol. 25, no. 4, pp. 293–303,\n2022.\n[22] A. Pacha, J. Calvo-Zaragoza, and J. Hajic Jr., “Learn-\ning notation graph construction for full-pipeline optical\nmusic recognition,” in Proceedings of the 20th Interna-\ntional Society for Music Information Retrieval Confer-\nence, 2019, pp. 75–82.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n188[23] J. Hajic Jr. and P. Pecina, “The MUSCIMA++ dataset\nfor handwritten optical music recognition,” in Proceed-\nings of the 14th IAPR International Conference on\nDocument Analysis and Recognition . IEEE, 2017, pp.\n39–46.\n[24] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” in 3rd International Conference on\nLearning Representations , 2015.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n189"
    },
    {
        "title": "MoisesDB: A Dataset for Source Separation Beyond 4-Stems.",
        "author": [
            "Igor Pereira",
            "Felipe Araújo",
            "Filip Korzeniowski",
            "Richard Vogl"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265363",
        "url": "https://doi.org/10.5281/zenodo.10265363",
        "ee": "https://zenodo.org/records/10265363/files/000073.pdf",
        "abstract": "In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres. \nFor each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems. \nThis will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data. \nTo facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB.\nAlongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results.",
        "zenodo_id": 10265363,
        "dblp_key": "conf/ismir/PereiraAKV23",
        "keywords": [
            "MoisesDB dataset",
            "240 tracks",
            "45 artists",
            "12 musical genres",
            "individual audio sources",
            "two-level hierarchical taxonomy",
            "stem organization",
            "easy-to-use Python library",
            "baseline results",
            "open-source separation models"
        ],
        "content": "MoisesDB: A DATASET FOR SOURCE SEPARATION BEYOND 4-STEMS\nIgor Pereira Felipe Araújo Filip Korzeniowski Richard Vogl\nMoises Systems Inc., Salt Lake City, USA.\nigor@moises.ai\nABSTRACT\nIn this paper, we introduce the MoisesDB dataset for\nmusical source separation. It consists of 240 tracks from\n45 artists, covering twelve musical genres. For each song,\nwe provide its individual audio sources, organized in a\ntwo-level hierarchical taxonomy of stems. This will facili-\ntate building and evaluating ﬁne-grained source separation\nsystems that go beyond the limitation of using four stems\n(drums, bass, other, and vocals) due to lack of data. To\nfacilitate the adoption of this dataset, we publish an easy-\nto-use Python library to download, process and use Moi-\nsesDB. Alongside a thorough documentation and analysis\nof the dataset contents, this work provides baseline results\nfor open-source separation models for varying separation\ngranularities (four, ﬁve, and six stems), and discuss their\nresults.\n1. INTRODUCTION\nSource separation is the task of splitting an audio signal\ninto separate signals for each signal source. For music, the\nsignal sources are the instruments that appear in the track,\ne.g.: guitar, bass, piano, drums, and vocals.\nMusic source separation is a relevant task within mu-\nsic information retrieval. While it can be used as a pre-\nprocessing step for other tasks (e.g. voice separation for\nf0 tracking), source separation enables diverse applications\non arbitrary music tracks that would need manual creation\nof stems otherwise. For example, in the context of music\neducation, the creation of play-along tracks for students,\nfacilitating by-ear transcription of relevant instruments, or\nautomatic creation of karaoke backing tracks. Such appli-\ncations are relevant for industry, as demonstrated by initia-\ntives like the demixing challenges1.\nState-of-the-art source separation systems are usually\nbuilt using neural-network-based machine learning sys-\ntems, trained in a supervised way [1–3]. In order to train\nthese systems, a large amount of training data is required.\nFor supervised approaches, the training data is represented\n1https://www.aicrowd.com/challenges/\nmusic-demixing-challenge-ismir-2021/sound-demixing-challenge-2023\n© I. Pereira, F. Araújo, F. Korzeniowski, and R. V ogl. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: I. Pereira, F. Araújo, F. Korzeniowski, and\nR. V ogl, “MoisesDB: A Dataset for Source Separation beyond 4-Stems”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.Dataset Year No. of Tracks Stems / Multitracks\nMedleyDB [4] 2014 122 Multitracks\nMedyleyDB-V2 [5] 2016 196 Multitracks\nDSD100 [6] 2015 100 4 Stems\nMUSDB18 [7] 2017 150 4 Stems\nMUSDB18-HQ [8] 2019 150 4 Stems\nMoisesDB 2023 240 Multitracks\nTable 1 . Overview of publicly released datasets for music\nsource separation. The datasets are grouped according to\nthe set of tracks they contain. For example, DSD100 is a\nsubset of MUSDB18. Additionally, 46 songs from Med-\nleyDB are also used in MUSDB18.\nby pairs of i. a mixed audio track and ii. a set of so-\ncalled stems that, when combined, recreate the audio track.\nStems are audio signals containing only one (or a group\nof related) sources, i.e. instruments. A pair of one mixed\ntrack and its corresponding stems constitutes one training\nexample.\nBesides the large amount of manual work involved in\nany large-scale dataset creation, this kind of data is es-\npecially hard to come by for several reasons. Whenever\ndealing with music audio data, legal issues may arise by\ncollecting and sharing a dataset. The copy and distribu-\ntion rights for most music are held by music publishers\nand record labels and are enforced rigorously. Obtaining\nthe audio recordings for the individual instruments (stems)\nalong with the ﬁnal mix may expose recording, mixing,\nand mastering techniques of the recording studios, respon-\nsible for producing a track, which is why recording stu-\ndios may oppose the publishing of stems in order to keep\ntheir trade secrets. Finally, processing, exporting, and or-\nganizing stems from recording projects (often from a digi-\ntal audio workstation) is a considerable task. Usually, these\nrecording projects are created without considering the re-\nquirement of exporting instrument stems. All these fac-\ntors hinder the creation and release of multitrack and stem\ndatasets.\nWhile there exist source separation datasets aimed at a\nspeciﬁc task, like vocal separation [9, 10], these are only\nof limited relevance for the more general task of splitting\naudio tracks up into stems. A majority of the existing\nstem datasets [6, 8] use a limited taxonomy of four stems,\nnamely: vocals, drums, bass, and other. While this has\nbecome a de-facto standard for works on source separa-\ntion [1–3] due to the availability of data and comparability\nof results, this is a strong limitation of the resulting mod-6190 10 20 30 40 50 60Firefly\nFrank Sermon\nBen James\nThe Unfortunates\nHorizon\nSunny Side Up\nViolet Sun\n Ritual Slaughter\nJames Graydon\nCalculus\nFigure 1 . Artist distribution of MoisesDB.\n0 20 40 60 80 100Rock\nSinger/Songwriter\nPop\nRap\nElectronic\nMusical Theatre\nBlues\nCountry\nReggae\nWorld Folk\nBossa Nova\nJazz\nFigure 2 . Genre distribution of MoisesDB.\nels. For many practical applications, separation of other,\nwidely used instruments may be relevant: e.g. guitars,\nkeys, strings, etc.\nDatasets featuring individually recorded tracks (multi-\ntrack, e.g. [5]), as well as other collections of multitrack\nrecordings, like Open Multitrack Testbed [11], do exist.\nHowever, these are not prepared to be used for source sep-\naration, out of the box, and may come with license restric-\ntions. Looking at recent source separation publications,\nwe see that non-public data usually represents the bulk of\ntraining data (e.g. Bean dataset [12] in [1]; 800 tracks of\nundisclosed source in [3]). This hints that by only using\npublicly available data, it is not possible to train compet-\nitive source separation models. Thus, there is a need for\nmore free data featuring a more detailed taxonomy, in or-\nder to be able to successfully train and test robust source\nseparation models with the capability to separated more\nstems.\nTo improve the current situation, we introduce Moi-\nsesDB, a multitrack dataset featuring track annotations and\na taxonomy to group individual tracks into stems. This\ndataset is offered free of charge for non-commercial re-\nsearch use only. It consists of 240 music tracks from differ-\nent artists and genres with a total duration of over 14 hours.\nAlong with the dataset, we provide baseline performance\nvalues for state-of-the-art source separation systems.\nThe remainder of this work is structured as follows:\nSection 2 covers related work and contrasts it with the\ndataset presented here. Section 3 discusses the details\nof MoisesDB. Section 4 introduces baseline performance\nevaluation statistics using freely available source separa-\ntion models. Finally, Section 5 provides concluding re-\nmarks.Stem Track\nBass Bass Guitar, Bass Synthesizer, Contrabass\nBowed Strings Cello, Cello Section, Other Strings, String Sec-\ntion, Viola Section, Viola Solo\nDrums Cymbals, Drum Machine, Full Acoustic\nDrumkit, Hi-Hat, Kick Drum, Overheads,\nSnare Drum, Toms\nGuitar Acoustic Guitar, Clean Electric Guitar, Dis-\ntorted Electric Guitar\nOther Fx\nOther Keys Organ, Electric Organ, Other Sounds, Synth\nLead, Synth Pad\nOther Plucked Banjo/Mandolin/Ukulele/Harp\nPercussion A-Tonal Percussion, Pitched Percussion\nPiano Electric Piano, Grand Piano\nV ocals Background V ocals, Lead Female Singer, Lead\nMale Singer, Other\nWind Brass, Flutes, Other Wind, Reeds\nTable 2 . MoisesDB stem-track taxonomy used to organize\nindividual tracks into stems.\n2 3 4 5 6 7 8 9 10\nNumber of Stems0102030405060\nFigure 3 . Number of stems per track in MoisesDB.\n2. RELATED WORK\nIn the past, several multitrack and stem datasets have been\npublished by the community (see Tab. 1). This section will\ndiscuss their properties and set the context for the dataset\npresented in this work. Since the main focus of this work\nis source separation into as many stems as possible, single\nstem focused datasets (e.g. voice separation datasets [9,\n10]) will be mainly ignored.\nIn 2014, Bittner et al. released the MedleyDB dataset\n[4], which comprises 122 songs in multitrack format. It\nwas extended by 74 songs (totalling 196 songs) in 2016,\nand published as MedleyDB 2.0 [5]. The dataset provides\naudio ﬁles in a hierarchical structure, where the ﬁnal mix\nis split into multiple stems, each containing numerous raw\naudio sources (multitracks). Besides the multitrack data,\nthe MedleyDB dataset provides an extensive list of meta-\ndata, such as artist, track name, origin, genre, and pro-\nducer, amongst others. Additionally it provides multiple\nannotations, such as instrument activation, melody, and\npitch.\nThe annotations in MedleyDB make it useful for many\nMIR tasks, including the source separation of diverse in-\nstruments. However, the shortcoming of MedleyDB forProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6200 50 100 150 200 250Vocals\nDrums\nBass\nGuitar\nOther Keys\nPiano\nPercussion\nBowed Strings\nOther\nWind\nOther Plucked\nFigure 4 . Distribution of stems in MoisesDB.\nfrommoisesdb.dataset importMoisesDB\ndb = MoisesDB (data_path= './moises-db-data' )\nn_songs = len(db)\ntrack = db [0]\n# mix multitracks to stems\nstems = track.stems\n# stems = {\n# 'vocals': np.ndarray (stem audio data),\n# 'bass': np.ndarray (stem audio data),\n# ...}\nmixture = track.audio # mixture: np.ndarray\ntrack.save_stems ('./stems/track_0' )# save mixed stems\nListing 1: Usage of the MoisesDB Python package.\nmusic source separation is the way it organizes tracks into\nstems. While it provides instrument information for each\nof them, and functional annotations for stems (such as\n“melody” or “bass”), stems are not meaningfully labelled,\nonly numbered. As a result, stem 01 of one song may\nbe the drum kit, while stem 01 of a another mix is the\nbassoon. Furthermore, instruments—and thus tracks—are\ngrouped according to how they physically produce their\nsound, rather than their role in the mix of a song. For ex-\nample, the “drum machine” falls into the same category\nas “electric piano”, namely “electric →electronic”. These\nshortcomings make it cumbersome to use for music source\nseparation out of the box and signiﬁcant work has to be\ndone in order to use it for this task.\nIn 2016, Liutkus et al. released the DSD100 [6] dataset\nas part of the 2016 signal separation evaluation campaign\nto develop and benchmark source separation models. It\ncontains 100 songs and uses the the four-stems taxonomy\n(vocals, drums, bass, and other). Later, in 2017, Raﬁi et\nal. extended DSD100 to 150 songs by adding 46 pieces\nfrom MedleyDB, and including four previously unreleased\nrecordings from commercial providers. This dataset be-\ncame known as the MUSDB18 [7] dataset, and was used\nfor the the 2018 signal separation evaluation campaign.\nIn 2019, Z. Raﬁi et al. released an uncompressed ver-\nsion of the MUSDB18 dataset, MUSDB18-HQ [8]. As its\npredecessor DSD100, this dataset provides four stems—\nvocals, drums, bass, and other—as well as linear mixes.\nMUSDB18 is widely used to train and benchmark source\nseparation models, but the limited number of stems pre-\nvents researchers from building more granular source sep-\naration systems.25 20 15 10 5\nLoudness [LUFS]4681012141618Dynamic RangeHarmonixSet\nMoisesDB0110Count\n0 110\nCount0 20Count\nFigure 5 . Loudness and Dynamic Range distribution of\ntracks in MoisesDB. For a comparison with commercially\nmixed and mastered songs, we sampled 240 tracks from\nthe HarmonixSet [13].\nIn summary, data for training granular source separa-\ntion systems is scarce: the 150 tracks from MUSDB18\nare ready to use, but offer only four stems to separate;\nthe 140 remaining tracks from MedleyDB (46 of the orig-\ninally 196 are already part of MUSDB18) are not orga-\nnized in a way that easily supports source separation re-\nsearch. This issue is also reﬂected in the fact that state-\nof-the-art source separation models often use larger, non-\npublic datasets for training [1, 3], or have to resort to syn-\nthetic training data (e.g. [14, 15]). Other works ﬁnd that\nMUSDB18’s \"source groupings remain overly coarse for\nmany real-world remixing applications.\" [16]. To address\nthese issues and to foster more research in music source\nseparation, we created the MoisesDB dataset.\nMoisesDB comprises the largest publicly available set\nof multitrack audio recordings—240 previously unreleased\nsongs—organized in a taxonomy that reﬂects the needs\nof source separation systems (as detailed in Sec. 3.1).\nThe large number of songs, the diverse types of stems\nand tracks, and their organization in a source-separation-\nfocused taxonomy will allow researchers to build their own\nstems according to their own requirements, and thus de-\nvelop more granular source separation systems.\n3. DATASET\nMoisesDB consists of 240 songs by 47 artists that span\ntwelve high-level genres. Both artists and genres follow\na power-law-like distribution, where the majority of songs\nbelong to few genres and are performed by few artists—see\nFig. 1 and 2. The total duration of the dataset is 14 hours,\n24 minutes and 46 seconds, where the average recording is\n3:36 seconds, with a standard deviation of 66 seconds.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n621Bass GuitarSnare DrumKick Drum\nLead Male SingerT oms\nBackground VocalsAcoustic GuitarHi-Hat\nDistorted Electric GuitarOverheads\nClean Electric GuitarGrand PianoCymbals\nA-T onal PercussionLead Female SingerSynth Pad\nOrgan, Electric OrganSynth LeadFx\nBass SynthesizerDrum MachineString SectionElectric Piano\nFull Acoustic DrumkitOther Sounds\nPitched PercussionBrassReeds\nContrabass\nBanjo, Mandolin, Ukulele, HarpOther StringsOther WindFlutesOtherCello\nCello SectionViola Solo\nViola Section0255075100125150175200Stem\nVocals\nDrums\nBass\nGuitar\nOther Keys\nPiano\nPercussion\nBowed Strings\nOther\nWind\nOther Plucked\nFigure 6 . Distribution of tracks in MoisesDB.\n3.1 Stem Taxonomy\nModern song recordings consist of multiple recorded\ntracks , which can be grouped and down-mixed into a\nsmaller number of stems . For example, the “drums” stem\nmight comprise tracks for the snare drum, the bass drum,\nhi-hat, cymbals, and so on. MoisesDB provides all individ-\nual tracks for each song, grouped into stems by the taxon-\nomy shown in Table 2. This taxonomy reﬂects the record-\ning & mixing process, and thus facilitates its reversal—\nmusic source separation—by grouping the raw tracks into\nsemantically labeled stems. This also means that songs\nmay consist of different numbers of stems, as shown in\nFig. 3. MoisesDB thus facilitates many future research di-\nrections: source separation models for a larger number of\nstems, data augmentation through mixing stems on-the-ﬂy\nfrom their tracks, or separation of individual tracks from a\nstem, to name a few.\nGiven the genres of the songs in MoisesDB, certain\nstems are more common in the dataset than others: “vo-\ncals”, “drums”, and “bass” appear on virtually every song,\nwhile “wind” is rare. Similarly, certain tracks appear much\nmore frequently than others, both within stems (“bass gui-\ntar” vs. “contrabass”) and between stems (“snare drum”\nvs. “cello”). Figs. 4 and 6 show the distributions of stems\nand sources, respectively.\nWe anticipate that this imbalance will present a chal-\nlenge in training source separation models for underrep-\nresented stems, as it is likely that certain tracks, such as\n“other plucked” tracks, will still be difﬁcult to distinguish\nfrom “guitar” tracks if trained solely on MoisesDB. How-\never, the available data provides an opportunity for re-\nsearchers to better identify and characterize errors made\nby their models. For instance, instead of simply observing\nthat the separated “other” stem bleeds into “guitar,” Moi-\nsesDB enables researchers to pinpoint this issue to tracks\nwhere “other” includes plucked instruments.3.2 Recording and Mastering\nThe songs in MoisesDB are professionally recorded in\nstereo. The individual tracks are combined additively to\ncreate stems, which are then mixed together to produce the\nﬁnal version of the song. Due to technical limitations dur-\ning recording, minuscule bleeding from other stems may\nbe present for some of the tracks. No compression, equal-\nization, or other effects are used during the mixing pro-\ncess, and the songs are not subjected to mastering. As a\nresult, the song mixes have a lower loudness and a higher\ndynamic range than professionally mastered commercial\nsongs. This raises concerns about the distributional shift\nbetween un-mastered training data and commercial record-\nings. Indeed, Jeon and Lee [17] have found that training\nseparation models using mastered mixes can improve sep-\naration quality. However, providing un-mastered mixes is\ncommon in existing datasets such as MUSDB18, and mod-\nels such as HT-Demucs [3] generalize reasonably well to\nmastered recordings, even if trained on un-mastered data.\nFigure 5 shows the loudness and dynamic range dis-\ntributions for the dataset, where loudness is measured in\nLUFS (Loudness Units relative to Full Scale) [18], and\nDynamic Range is computed based on the deﬁnitions of\nthe “Pleasurize Music Foundation” as implemented in the\n“DR14 T.meter” software2.\n3.3 Python Library\nWith MoisesDB comes a Python library that facilitates\nworking with the dataset by parsing metadata and auto-\nmatically building stems and mixes. Listing 1 shows an\nexample usage of the library. The code shown there initial-\nizes the library, retrieves the number of tracks, creates the\nstems and the full mix, and saves the individual stems to a\ndirectory. For a detailed and up-to-date documentation, we\nrefer the reader to the GitHub repository3.\n2https://github.com/simon-r/dr14_t.meter\n3https://github.com/moises-ai/moises-dbProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n622-505101520vocals bass drums other\n-505101520SDR [dB]vocals bass drums other piano\nHT-DEMUCSSPLEETERIBMIRMMWF-505101520vocals\nHT-DEMUCSSPLEETERIBMIRMMWFbass\nHT-DEMUCSSPLEETERIBMIRMMWFdrums\nHT-DEMUCSSPLEETERIBMIRMMWFother\nHT-DEMUCSSPLEETERIBMIRMMWFpiano\nHT-DEMUCSSPLEETERIBMIRMMWFguitar4 stems\n5 stems\n6 stems\nFigure 7 . SDR values of each group of sources, for IBM, IRM, MWF, Demucs, and Spleeter source separation methods.\n4 stems (N = 235)\nHT-Demucs Spleeter IBM IRM MWF\nMean±Std Mdn Mean ±Std Mdn Mean ±Std Mdn Mean ±Std Mdn Mean ±Std Mdn\nvocals 10.05 ±2.48 9.62 7.61 ±2.45 7.27 9.02 ±2.13 8.67 10.72±2.03 10.37 10.72±2.11 10.27\nbass 11.64±3.35 11.99 6.46 ±2.26 6.57 6.46 ±2.08 6.31 8.43 ±2.03 8.20 8.68 ±2.07 8.38\ndrums 10.94±2.30 10.91 6.65 ±1.72 6.64 7.33 ±1.77 7.30 8.98 ±1.68 8.92 9.01 ±1.67 8.83\nother 7.00 ±2.76 7.30 4.45 ±2.26 4.69 5.77 ±1.72 5.61 7.74 ±1.65 7.57 7.90±1.65 7.79\noverall 9.91±3.27 9.69 6.29 ±2.47 6.24 7.14 ±2.28 6.99 8.97 ±2.16 8.81 9.08 ±2.15 8.87\n5 stems (N = 104)\nvocals 6.99 ±1.97 6.74 8.29 ±1.66 8.08 9.94 ±1.59 9.75 10.01±1.71 9.68\nbass 6.26 ±2.27 6.28 6.13 ±2.15 5.86 8.02 ±2.07 7.82 8.32±2.08 8.03\ndrums 6.89 ±1.88 6.97 7.67 ±1.94 7.87 9.29 ±1.84 9.34 9.32±1.84 9.36\nother 1.97 ±1.76 2.09 4.04 ±1.47 4.13 6.00 ±1.44 6.01 6.10±1.48 6.19\npiano 1.17 ±1.86 0.75 3.04 ±2.37 2.55 4.99 ±2.32 4.60 5.30±2.46 4.79\noverall 4.66±3.20 5.02 5.12 ±2.81 4.87 7.65 ±2.66 7.60 7.81±2.66 7.83\n6 stems (N = 88)\nvocals 9.55 ±1.87 9.39 8.09 ±1.51 7.98 9.73 ±1.46 9.61 9.81±1.49 9.61\nbass 11.93±2.87 12.13 6.04 ±1.98 5.83 7.92 ±1.93 7.73 8.24 ±1.96 8.03\ndrums 11.02±2.44 11.28 7.58 ±1.96 7.79 9.19 ±1.86 9.21 9.23 ±1.85 9.25\nother 0.28 ±1.84 0.39 2.85 ±1.76 2.74 4.67 ±1.76 4.57 4.72±1.82 4.55\npiano 1.60 ±1.68 1.64 2.78 ±1.61 2.49 4.71 ±1.61 4.47 4.97±1.74 4.70\nguitar 3.07 ±1.81 3.16 3.35 ±1.54 3.44 5.28 ±1.54 5.36 5.41±1.65 5.46\noverall 6.24±5.17 6.05 5.12 ±2.81 4.87 6.91 ±2.70 6.69 7.06±2.73 6.89\nTable 3 . Mean, standard deviation (Std), and median (Mdn) of the SDR in dB for each Model/Method and stem type. The\nvarying number of available tracks is denoted by N. Overall indicates performance over all tracks regardless of stem group.\nBest results are marked in bold.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6234. BENCHMARKING\nIn order to establish reference values for each track of\nthe MoisesDB dataset, we computed the Source to Dis-\ntortion Ratio (SDR) [19] scores for Ideal Binary Mask\n(IBM) [20], Ideal Ratio Mask (IRM) [21], and Multichan-\nnel Wiener Filter (MWF) [22] oracle separation methods.\nAdditionally, we assessed SDR scores for two popular pub-\nlic available and open-source architectures: Hybrid Trans-\nformer Demucs (HT-DEMUCS) [3] and Spleeter [1]. The\nSDR scores were calculated for three different groups of\nsources: four, ﬁve, and six stems. Given the architecture of\nthe open-source models, results for Spleeter are available\nfor four and ﬁve stems, and for HT-DEMUCS for four and\nsix stems.\nThe SDR measure [19] represents how much of the en-\nergy in a true source signal is preserved in an estimated\nsource signal after applying a separation algorithm. The\nequation can be deﬁned as\nSDR = 10log10/summationtext\nn|s(n)|2+ϵ/summationtext\nn|s(n)−ˆs(n)|2+ϵ, (1)\nwheres(n)represents the true source signal at time n,ˆs(n)\nrepresents the estimated source signal at time n, and the\nresult is given in decibels (dB).\nTable 3 shows the SDR values in dB for each group\nof stems (4, 5, and 6) evaluated in this benchmark. For\na better comparison, we chose the stems available in the\nopen-source models: vocals, bass, drums, other, guitar, and\npiano. We also pick tracks containing at least all the stems\nchosen for each group, which explains the distinct number\nof tracks in Table 3. Songs with more individual tracks\nthan the ones speciﬁed for each group were merged into\nthe “other” stem using a linear sum strategy.\nFigure 7 depicts boxplots representing the distribution\nof the SDR metric for both oracle and separation methods,\ncalculated for each group of tracks comprising 4, 5, and\n6 stems. The groups of stems evaluated were vocal, bass,\ndrums, other, piano, and guitar. Detailed results for every\ntrack and each stem are provided in the GitHub3reposi-\ntory.\nThe ﬁrst fact that calls our attention can be seen in\nFigure 7, where the SDR results of IRM and MWF ora-\ncle methods did not show a signiﬁcant difference for all\ngroups of stems. The striking fact is the performance of\nHT-DEMUCS architecture, which outperforms the oracle\nmethods for bass and drums stems, for the groups of 4\nand 6 stems tracks, as we can see in Figures 7 A and\nC, respectively. Those results contrast with the slightly\nworse performance of HT-DEMUCS for other, piano, and\nguitar stems, compared with oracle methods, as seen in\nFigure 7 C.5. CONCLUSION\nIn this work, we introduced MoisesDB, a multitrack\ndataset with a hierarchical taxonomy aimed at more-\nthan-four-stems source separation. We set the context\nby analysing the current landscape of source separation\ndatasets and presented a comparison with other relevant\ndatasets along with a detailed analysis of MoisesDB.\nSpeciﬁcally, we discussed the organizational taxonomy fo-\ncused on source separation, the distribution over track du-\nration, the distribution over genres, and the number of\nsongs for each stem and source available in the dataset.\nMoreover, we include performance results for two pub-\nlicly available source separation methods: HT-Demucs,\nwhich has the best overall SDR score evaluated on the\nMUSDB18 test set, and Spleeter, which was one of the ﬁrst\nsource separation models released and adopted by the gen-\neral public. We also added results for a few masking-based\noracle methods: IBM, IRM, and MWF, which indicate the\ntheoretical performance limits for mask-based source sep-\naration models. Additionally, we provide an easy-to-use\nPython library to access the data which allows fast integra-\ntion with machine learning libraries.\nOverall, this paper represents a detailed report on the\nMoisesDB dataset, which will hopefully prove to be a great\nresource for the source separation community in the future.\nThis work aims at facilitating the development of better\nand extended source separation models as well as provid-\ning opportunities to be applied for other use cases, such as\nautomatic mixing and generative accompaniment systems,\namong others.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6246. REFERENCES\n[1] R. Hennequin, A. Khlif, F. V oituret, and M. Moussal-\nlam, “Spleeter: A fast and efﬁcient music source sep-\naration tool with pre-trained models,” Journal of Open\nSource Software , vol. 5, no. 50, p. 2154, Jun. 2020.\n[2] Q. Kong, Y . Cao, H. Liu, K. Choi, and Y . Wang,\n“Decoupling magnitude and phase estimation\nwith deep ResUNet for music source separation,”\narXiv:2109.05418 [cs, eess] , Sep. 2021.\n[3] S. Rouard, F. Massa, and A. Défossez, “Hy-\nbrid transformers for music source separation,”\narXiv:2211.08553 [cs, eess] , Nov. 2022.\n[4] R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Can-\nnam, and J. Bello, “MedleyDB: A multitrack dataset\nfor annotation-intensive MIR research,” in Proceed-\nings of the 15th International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , Taipei, Taiwan,\nOct. 2014, pp. 155–160.\n[5] R. Bittner, J. Wilkins, H. Yip, and J. P. Bello, “Med-\nleyDB 2.0: New data and a system for sustainable data\ncollection,” in Late Breaking Demo of the 16th Inter-\nnational Society for Music Information Retrieval Con-\nference (ISMIR) , New York, USA, Aug. 2016.\n[6] A. Liutkus, F.-R. Stöter, Z. Raﬁi, D. Kitamura,\nB. Rivet, N. Ito, N. Ono, and J. Fontecave, “The 2016\nsignal separation evaluation campaign,” in Proceed-\nings of the 13th International Conference on Latent\nVariable Analysis and Signal Separation (LVA/ICA) ,\nGrenoble, France, Feb. 2017, pp. 323–332.\n[7] Z. Raﬁi, A. Liutkus, F.-R. Stöter, S. I. Mimilakis,\nand R. Bittner, “The MUSDB18 corpus for music\nseparation,” Dec. 2017. [Online]. Available: https:\n//doi.org/10.5281/zenodo.1117372\n[8] ——, “MUSDB18-HQ - an uncompressed version\nof MUSDB18,” Dec. 2019. [Online]. Available:\nhttps://doi.org/10.5281/zenodo.3338373\n[9] T.-S. Chan, T.-C. Yeh, Z.-C. Fan, H.-W. Chen, L. Su,\nY .-H. Yang, and R. Jang, “V ocal activity informed\nsinging voice separation with the ikala dataset,” in Pro-\nceedings of the 40th IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\nSouth Brisbane, QLD, Australia, Apr. 2015, pp. 718–\n722.\n[10] C.-L. Hsu and J.-S. R. Jang, “On the improvement of\nsinging voice separation for monaural recordings us-\ning the mir-1k dataset,” IEEE Transactions on Audio,\nSpeech, and Language Processing , vol. 18, no. 2, pp.\n310–319, 2010.\n[11] B. De Man, M. Mora-Mcginity, G. Fazekas, and J. D.\nReiss, “The open multitrack testbed,” in Proceedings of\nthe 137th Audio Engineering Society Convention . Los\nAngeles, CA, USA: Audio Engineering Society, Oct.\n2014.[12] L. Prétet, R. Hennequin, J. Royo-Letelier, and\nA. Vaglio, “Singing voice separation: A study on train-\ning data,” in Proceedings of the IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) . Brighton, UK: IEEE, May 2019, pp.\n506–510.\n[13] O. Nieto, M. McCallum, M. E. P. Davies, A. Robert-\nson, A. Stark, and E. Egozy, “The HARMONIX set:\nBeats, downbeats, and functional segment annotations\nof western popular music,” in Proceedings of the 20th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pp. 565–572.\n[14] Y . Özer and M. Müller, “Source separation of piano\nconcertos with test-time adaptation,” in Proceedings of\nthe 23rd International Society for Music Information\nRetrieval Conference (ISMIR) , Bengaluru, India, Dec.\n2022, pp. 493–500.\n[15] E. Manilow, G. Wichern, P. Seetharaman, and\nJ. Le Roux, “Cutting music source separation some\nslakh: A dataset to study the impact of training data\nquality and quantity,” in IEEE Workshop on Appli-\ncations of Signal Processing to Audio and Acoustics\n(WASPAA) . IEEE, Oct. 2019, pp. 45–49.\n[16] E. Manilow, G. Wichern, and J. Le Roux, “Hierarchi-\ncal musical instrument separation.” in Proceedings of\nthe 21st International Society for Music Information\nRetrieval Conference (ISMIR) , Virtual Conference, Oct\n2020, pp. 376–383.\n[17] C.-B. Jeon and K. Lee, “Towards robust music source\nseparation on loud commercial music,” in Proceedings\nof the 23rd International Society for Music Information\nRetrieval Conference (ISMIR) , Bengaluru, India, Dec.\n2022, pp. 575–582.\n[18] ITU-R, “Algorithms to measure audio programme\nloudness and true-peak audio level,” International\nTelecommunication Union, Recommendation\nBS.1770-4, Oct. 2015.\n[19] E. Vincent, R. Gribonval, and C. Fevotte, “Perfor-\nmance measurement in blind audio source separation,”\nIEEE Transactions on Audio, Speech, and Language\nProcessing , vol. 14, no. 4, pp. 1462–1469, Jul. 2006.\n[20] E. Vincent, H. Sawada, P. Boﬁll, S. Makino, and J. P.\nRosca, “First stereo audio source separation evaluation\ncampaign: Data, algorithms and results,” in Proceed-\nings of the 7th International Conference on Indepen-\ndent Component Analysis and Signal Separation (ICA) ,\nLondon, UK, Sep. 2007, pp. 552–559.\n[21] A. Liutkus and R. Badeau, “Generalized Wiener ﬁlter-\ning with fractional power spectrograms,” in Proceed-\nings of the IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , South\nBrisbane, Australia, Apr. 2015, pp. 266–270.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n625[22] N. Q. K. Duong, E. Vincent, and R. Gribonval, “Under-\ndetermined reverberant audio source separation using a\nfull-rank spatial covariance model,” IEEE Transactions\non Audio, Speech, and Language Processing , vol. 18,\nno. 7, pp. 1830–1840, Sep. 2010.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n626"
    },
    {
        "title": "Online Symbolic Music Alignment With Offline Reinforcement Learning.",
        "author": [
            "Silvan David Peter"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265367",
        "url": "https://doi.org/10.5281/zenodo.10265367",
        "ee": "https://zenodo.org/records/10265367/files/000075.pdf",
        "abstract": "Symbolic Music Alignment is the process of matching\nperformed MIDI notes to corresponding score notes. In\nthis paper, we introduce a reinforcement learning (RL)-\nbased online symbolic music alignment technique. The\nRL agent — an attention-based neural network — itera-\ntively estimates the current score position from local score\nand performance contexts. For this symbolic alignment\ntask, environment states can be sampled exhaustively and\nthe reward is dense, rendering a formulation as a simpli-\nfied offline RL problem straightforward. We evaluate the\ntrained agent in three ways. First, in its capacity to identify\ncorrect score positions for sampled test contexts; second,\nas the core technique of a complete algorithm for symbolic\nonline note-wise alignment; and finally, as a real-time sym-\nbolic score follower. We further investigate the pitch-based\nscore and performance representations used as the agent's\ninputs. To this end, we develop a second model, a two-\nstep Dynamic Time Warping (DTW)-based offline align-\nment algorithm leveraging the same input representation.\nThe proposed model outperforms a state-of-the-art refer-\nence model of offline symbolic music alignment.",
        "zenodo_id": 10265367,
        "dblp_key": "conf/ismir/Peter23",
        "keywords": [
            "reinforcement learning",
            "symbolic music alignment",
            "performed MIDI notes",
            "score notes",
            "online symbolic alignment",
            "reinforcement learning agent",
            "attention-based neural network",
            "score position estimation",
            "dense reward",
            "offline RL problem formulation"
        ],
        "content": "ONLINE SYMBOLIC MUSIC ALIGNMENT WITH OFFLINE\nREINFORCEMENT LEARNING\nSilvan David Peter\nInstitute of Computational Perception, Johannes Kepler University Linz, Austria\nsilvan.peter@jku.at\nABSTRACT\nSymbolic Music Alignment is the process of matching\nperformed MIDI notes to corresponding score notes. In\nthis paper, we introduce a reinforcement learning (RL)-\nbased online symbolic music alignment technique. The\nRL agent — an attention-based neural network — itera-\ntively estimates the current score position from local score\nand performance contexts. For this symbolic alignment\ntask, environment states can be sampled exhaustively and\nthe reward is dense, rendering a formulation as a simpli-\nﬁed ofﬂine RL problem straightforward. We evaluate the\ntrained agent in three ways. First, in its capacity to identify\ncorrect score positions for sampled test contexts; second,\nas the core technique of a complete algorithm for symbolic\nonline note-wise alignment; and ﬁnally, as a real-time sym-\nbolic score follower. We further investigate the pitch-based\nscore and performance representations used as the agent’s\ninputs. To this end, we develop a second model, a two-\nstep Dynamic Time Warping (DTW)-based ofﬂine align-\nment algorithm leveraging the same input representation.\nThe proposed model outperforms a state-of-the-art refer-\nence model of ofﬂine symbolic music alignment.\n1. INTRODUCTION\nMusic alignment refers to matching at least two different\nversions of the same musical material. In this paper, we\naddress symbolic music alignment, for our purposes de-\nﬁned as models that match individual notes of a perfor-\nmance recorded as MIDI ﬁle to individual notes of a score\nencoded as MusicXML ﬁle.\nAlignment procedures can be separated into online or\nofﬂine classes. If the alignment procedure is carried out\nwith access to the full versions of the musical material, we\nrefer to it as ofﬂine alignment. Conversely, if one version\nis only known up to the point currently to be matched, we\nrefer to it as online.\nWe introduce a reinforcement learning (RL)-based on-\nlinesymbolic music alignment technique. It aligns sym-\nbolically encoded music or, more speciﬁcally, MIDI per-\nformances to their corresponding MusicXML scores by\n© S. Peter. Licensed under a Creative Commons Attribu-\ntion 4.0 International License (CC BY 4.0). Attribution: S. Peter, “On-\nline Symbolic Music Alignment with Ofﬂine Reinforcement Learning”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.matching individual notes of each version. The RL agent –\na small attention-based neural network – is trained to itera-\ntively predict the current score position from limited score\nand past performance contexts. The current performance\nnote and estimated score position are then processed to\ncompute a symbolic note-wise matching.\nRL terminology introduces another online versus ofﬂine\ndifferentiation. RL is termed online if the agent learns from\ndata created by the agent’s interaction with its environment\nduring training. In our case, we use ofﬂine RL, that is,\nthe agent is trained using a dataset of exhaustively sam-\npled environment states and associated rewards, effectively\nturning agent training into a supervised learning problem.\nOnce trained in an ofﬂine fashion, the agent can be used in\nonline alignment.\nThe agent processes a purely pitch-based representa-\ntionand timing information is only incorporated in a post-\nprocessing step. Before addressing the online problem,\nwe investigate the same separation of pitch and time pro-\ncessing in an ofﬂine setting: we develop a two-step (ﬁrst\npitch, then time) Dynamic Time Warping (DTW) ofﬂine\nmodel and evaluate it against the state of the art in note-\nwise alignment in Section 3. The subsequent Section 4 ad-\ndresses the RL-based online model reusing the input setup.\nThe rest of this paper is thus structured as follows: Sec-\ntion 2 introduces related work. Section 3 discusses ofﬂine\nsymbolic music alignment. We develop as well as evalu-\nate an ofﬂine symbolic music alignment algorithm based\non two different applications of DTW, ﬁrst on pitch infor-\nmation, then on onset times. Starting from these results,\nsection 4 introduces a formulation of online alignment as\nreinforcement learning problem. In particular, we train an\nagent’s value function in an ofﬂine setting. In section 5\nwe evaluate the trained agent in three ways: as a stan-\ndalone score onset identiﬁcation model, as an online sym-\nbolic alignment model (where the aim is the production\nof correct note-wise alignments), and in a score following\nscenario (where the aim is the precise temporal tracking\nthe current score position). Finally, Section 6 concludes\nthe paper with a critical appraisal of our models as well as\nrecommendations for future research.\n2. RELATED WORK\nSymbolic music alignment has been a popular research\narea for many years. We begin our review of related work\nwith online symbolic music alignment, then we progress634to ofﬂine symbolic music alignment, general music align-\nment, and ﬁnally applications of reinforcement learning.\nMost often, online models have been presented in the\ncontext of score following, where the principal aim is to\nidentify the current score position. Dannenberg [1] and\nVercoe [2] pioneered this area of research in the mid 1980s.\nRecent works commonly use Dynamic Bayesian Networks\nto track the performance [3–6]. Recently, A. Anonymous\ncompared both Hidden Markov Models (HMM) and On-\nLine Time Warping (OLTW) techniques. We use their\nOLTW model as comparison baseline for our online align-\nment technique. This model processes inputs represented\nas piano rolls, as is common for OLTW and DTW applica-\ntions to symbolic music alignment in general.\nThe ofﬂine setting has seen more recent work [3, 5, 7–\n11]. Symbolic music alignment methods often perform\nvery well, with error rates rarely exceeding 10%. Con-\nsequently, much recent work focused on the rare, indeter-\nminate, or asynchronous events that make the errors difﬁ-\ncult to identify and ﬁx. Ornaments are one source of such\nevents [9]. Another is left-right hand asynchrony in piano\nperformance as discussed in Nakamura et al. [3]. Arbitrary\nskips and repeats further present a very difﬁcult challenge\nfor most algorithms, especially when runtime considera-\ntions are important [10]. This series of articles by Naka-\nmura et al. [3, 9, 10] culminated in one of the most widely\nused automatic score-performance alignment tools and the\ncurrent state of the art (SOTA) [11]. We use this model as\nreference for the evaluation of our ofﬂine algorithm.\nAlthough beyond the scope of this article, no introduc-\ntion of music alignment is complete without the mention\nof the large body of work concerning alignment of non-\nsymbolic music formats, in particular audio. Wang [12],\nArzt [13], and chapter three in Müller [14] present intro-\nductory discussions of audio alignment. As in our ofﬂine\napproach, applications of (non-standard) DTW are central\nto audio alignment [15–18]. Audio score following is com-\nmonly computed using On-Line Time Warping and vari-\nants of Hidden Markov Models [19–23].\nTo the best of our knowledge, Dorfer et al. [24] (later\nexpanded upon by Henkel et al. [25]) are the only prior ap-\nplication of RL in a music alignment task, namely online\naudio to sheet music image alignment. For a general intro-\nduction to RL, we refer the reader to Sutton and Barto [26],\nfor a discussion of the merits and disadvantages of ofﬂine\nRL to Levine et al. [27].\n3. OFFLINE SYMBOLIC MUSIC ALIGNMENT\nIn this section, we introduce an ofﬂine symbolic music\nalignment based on two different DTW steps as well as\nan intermediate cleanup step. We close the section with an\nevaluation of our model against a state-of-the-art reference.\nSymbolic music alignment produces note alignments ,\ni.e., it matches individual notes of a performance recorded\nas MIDI ﬁle to individual notes of a score encoded as Mu-\nsicXML ﬁle. Three types of note alignments exist: a match\nis tuple of a performance note and a score note, a deletion\nis a score note not played, and an insertion is a performed\nFigure 1 . First half measure of Chopin Op. 9 No. 2\n(bottom score), encoded as pitch set sequence (left) and\nwarped to its performance, encoded as sequence of pitches\nas played (top). The matrix shows the corresponding pair-\nwise distance (shaded is distance of 1, see equation 1), red\nlines indicate equivalent optimal warping paths.\nnote not notated.\nOur proposed ofﬂine algorithm consists of the following\nsteps: First, the performance and score are aligned using\nDTW on a purely pitch-based representation (Section 3.1).\nThen, remaining gaps are ﬁlled by complete sequences of\na single pitch. Finally, individual notes are aligned using\nan application of DTW on their onset times (Section 3.2).\n3.1 Pitch Sequence Warping\nIn this approach, we align sequences of performance notes,\nencoded as integer pitches pt∈I:={1...88}, with se-\nquences of score onset notes, encoded as sets of integer\npitchesst∈ P(I)\\ {∅}, withP(I)denoting the power\nset of the set I. Since these sequence elements are of dif-\nferent types — integers and sets of integers — no standard\nlocal distance metric can be used. Instead we opt for a non-\nsymmetric inclusion metric, with some abuse of the term.\nm(pt,st) =/braceleftbigg0ifpt∈st\n1else(1)\nHaving deﬁned the metric in Equation 1, two standard\nDTW paths are computed, one forward and one backward,\ni.e., using inverted sequences. Figure 1 shows the encoding\nas well as examplary DTW paths computed from cumula-\ntive pairwise distances. While the optimal DTW distance is\nunique, the DTW paths are not necessarily so. In our case,\nsuch ambiguity is often introduced by repeated pitches in\nneighboring score onsets, see e.g., the two adjacent (left,\nstacked vertically) notes of pitch D4/62 in Figure 1. To\npinpoint non-robust path segments, we use the backward\nDTW path. Wherever the forward and backward paths dis-\nagree, they effectively bracket ambiguous parts from both\nsides, and we exclude all bracketed notes from the path.\nThese excluded segments are then processed using a\nsimple heuristic: The notes in bracketed segments are sep-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n635arated by pitch. If two pitch-wise sequences with match-\ning number of notes (in the performance and the score) are\nfound, they are aligned and the result is added to the path.\nIf no matching sequence is found, the path is linearly inter-\npolated. We ﬁnally compute a mapping from score time to\nperformance time from this merged and cleaned path.\n3.2 Onset Sequence Warping\nThe next goal is to derive note-wise alignments from the\napproximate score to performance mapping computed in\nSection 3.1. To this end, the performance and score are\nsplit into pitch-wise sequences for each pitch occurring in\nthe union of score and performance pitches. The approx-\nimate score-time-to-performance-time mapping computed\nin the previous step is used to project all score onsets (in\nbeats) to performance time points (in seconds). The last\nstep aligns the performance onset sequence with the score\nonset sequence, now mapped to the same space.\nThis alignment is computed by a DTW path between the\nonset sequences, this time using a simple L1metric, and\nfor each non-unique alignment, keeping the tuple with the\nlowest distance. A threshold of maximal distance is further\nbuilt-in (and set to 5 seconds) to avoid spurious alignment\nof unrelated deletions and insertions.\n3.3 Ofﬂine Model Evaluation\nTo test the full ofﬂine model (ﬁrst pitch DTW 3.1,\nthen onset DTW 3.2), we compute alignments on four\ndatasets of high-quality note-wise alignment piano mu-\nsic. The datasets are the Vienna 4x22 Dataset [28],\nfour excerpts performed by 22 performers each; the\nZeilinger Dataset [29], nine piano sonatas by Ludwig van\nBeethoven performed by Clemens Zeilinger; the Magaloff\nDataset [30], the near complete solo piano works by Fred-\neric Chopin performed by Nikita Magaloff; and the Batik\nDataset [31], twelve piano sonatas by Wolfgang Amadeus\nMozart performed by Roland Batik. We compare our\nmodel against the reference by Nakamura et al. [11], post-\nprocessed to produce the same output format we employ.\nTo compare produced alignments to ground truth ones,\nwe have to deﬁne a metric. Recall that note alignments\nconsist of three types: matches (tuples of performance and\nscore notes), deletions (unplayed score notes), and inser-\ntions (unnotated performed notes). We use an F-score met-\nric for matches: A predicted match is counted as a true pos-\nitive (TP) only if the same notes are matched in the ground\ntruth alignment. A false positive (FP) is a predicted note\nlabel that isn’t in the ground truth, a false negative (FN) is\na ground truth note label that isn’t predicted. The F-score\nis deﬁned as the harmonic mean of precision (TP / (TP +\nFP)) and recall (TP / (TP + FN)).\nTable 3.3 shows dataset-wise and globally averaged\nF-scores for matches. Our proposed model outperforms\nthe reference on each dataset. A two-sided sign test on\nperformance-wise rankings shows signiﬁcantly ( α= 0.01)\nhigher performance for our proposed model on all datasets\nexcept the Vienna 4x22 Dataset. On the Vienna 4x22\ndataset, the models reach the same F-score of 1.0 for 38Dataset DTW Ofﬂine Nakamura\nMagaloff 98.4±0.9 % 97.8±1.4 %\nZeilinger 99.3±0.9 % 98.8±1.2 %\nBatik 99.4±0.7 % 98.5±2.1 %\nVienna 4x22 99.8±0.4 % 99.5±0.5 %\nCombined 99.0±1.0 % 98.5±1.5 %\nTable 1 . Dataset-wise averaged F-scores and standard de-\nviations of each model.\nperformances and our proposed model has higher F-scores\nfor the remaining 50 performances.\n4. ONLINE ALIGNMENT AGENT\nHaving established the effectiveness of the separation into\npitch-based and time-based input representations in the of-\nﬂine setting, we now introduce a formulation of RL-based\nonline alignment. We continue with the model and training\nsetup used to approximate the agent’s value function.\nReinforcement learning is formalized as Markov Deci-\nsion Process (MDP). An MDP consists of the following\ncomponents: a state space S, an action space A, an index\nsetT, a reward function R, transition probabilities P, and\na discount factor γ.\nAn agent is placed in an environment and perceives\nthis environment and itself as being in a possible state\nSt∈ S(t∈ T). The agent now takes an action At∈\nAand receives a reward Rt+1as well as a new state\nSt+1∈ S. Repeating this process iteratively yields a se-\nquence of states, actions, and rewards, called an episode:\nSt,At,Rt+1,St+1,At+1,Rt+2,St+2,At+2,...It is now\nthe agent’s task to infer actions from states that maximize\nlong-term reward. Before we look at our formulation of\nthis optimization problem, we discuss the online align-\nment’s state and action space in more detail.\n4.1 Alignment as Reinforcement Learning\nThe state information Stcomprises both the current score\ncontext as well as the most recent past performance.\nSpeciﬁcally, the score context is represented as a window\nof the pitch set sequence introduced in Section 3.1. The\nwindow centers the last predicted score onset position and\nspans seven score onsets to the past as well as eight score\nonsets to the future for a total windowed sequence of 16\npitch sets. The performance context is only derived from\npast performance notes to enable real-time application. It\nconsists of the eight most recent notes in the performance\npitch sequence. Whenever less score or performance con-\ntext is available, e.g., at the very beginning or end of a\npiece, the windows are shortened accordingly.\nAt each state Stthe agent aims to match the most recent\nperformance note to its most likely score onset. There are\n16 actions at St; select one score onset as matching onset\nposition. Having decided on a next score onset, the agent\nreceives a reward Rt+1which is set to one if the score on-\nset is correctly aligned, zero otherwise. The environment\ntransition probabilities Pdetermine a new state St+1: theProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n636performance context of the new state is determined by an\nactual performance, i.e., the agent is presented a new state\nbased on the estimated next position in the score and a new\nincoming performance note.\n4.2 Simpliﬁed Deep Q-learning\nThe agent’s behavior is captured by its policy π(A|S), the\ndistribution of actions taken by the agent in state S. Al-\nthough it is possible to optimize the policy directly, we in-\nstead adapt a value-function-based formulation, or more\nspeciﬁcally, deep Q-learning [32, 33]. Q-learning aims to\noptimize a state-action value function Q:S × A −→ R,\nan estimate of the expected cumulative discounted future\nreward, also called return, given a state and an action.\nIn deep Q-learning the value function Q(S,A,θ)contains\ntrainable parameters θwhich are ﬁtted to the experienced\nreward distribution. A typical optimization loss llooks like\nthis:\nl= (Rt+1+γmax\nAt+1Q(St+1,At+1,θ)−Q(St,At,θ))2\n(2)\nwhere the discount factor γ∈[0,1]determines the\nweighting of future rewards. For the alignment case we\ncan make several simpliﬁcations. We opt for a completely\nmyopic agent, i.e., γ= 0. The argument for this is that the\noptimal action to take for each incoming performance note\nis determined by the correct score onset which can in turn\nbe speciﬁed by the immediate reward. >For a discussion of\nthe implications of this modeling choice see section 6. Set-\ntingγ= 0removes the value function at subsequent states\nfrom the loss. Using that R={0,1}, we make a second\nreformulation and replace this squared error loss by a bi-\nnary classiﬁcation: For each state-action tuple (S,A)the\nagent predicts the probabilities of the reward being 1 or 0,\noptimized with a cross-entropy loss. Note that this formu-\nlation still optimizes a state-action value function Qand\nnot a policy π(A|S), i.e., the probabilities of rewards are\ncomputed for each possible action (that is, per score onset)\nand do not sum to one over all actions. There are several\nways of deriving a policy from a value function; two are\ndiscussed in section 4.5.\n4.3 Value Function Model\nTo approximate Q(S,A,θ), we use an attention-based\nTransformer Neural Network. The input of the network\nconsists of a sequence of tokens encoding the performance,\na delimiter token, the score, and an ending token. We en-\ncode 88 pitches of the piano keyboard, adding extra tokens\nfor \"no_pitch\", \"delimiter\", and \"end\" in a 64-dimensional\nembedding space. The performance pitches are straight-\nforward to embed, however, the score onset pitch sets re-\nquire more processing. For our data, more than 99% of\nscore onsets can be represented with pitch sets with no\nmore than seven different pitches, we thus limit our pitch\nsets to this length (with a subset of seven taken randomly\nat onsets with more pitches). Pitch sets with fewer than\nseven pitches are ﬁlled up with a pitch corresponding to the\nFigure 2 . Setup of the value function model: states are en-\ncoded as contiguous token sequence of past performance\n(red) and current score (blue) contexts. Pitch set embed-\ndings are summed over individual pitch embeddings. The\nmodel is set up as token classiﬁer as each score onset in\nthe context corresponds to a possible action (= \"select this\nonset as next score onset\") and is classiﬁed according to its\nexpected reward class. The vector on the right shows the\nreward probability for each action (pink).\n\"no_pitch\" token. To create score onset embedding with\nno more than 64 dimensions independent of the number\nof pitches at any onset, the pitch set tokens are summed\nup. Figure 2 shows the setup of the value function model.\nThe model is set up with eight heads, and six layers, layer\nnormalization, and a single feedforward head for binary\nclassiﬁcation, making for a total of 157250 parameters.\n4.4 Training\nThe training is set up as token classiﬁcation problem; i.e.,\nfor each token in the sequence, the probability of receiv-\ning a reward is estimated. The aligned piano datasets from\nSection 3.3 are used again. For our ofﬂine RL setting, a\ndataset of states is created before training. We extract lo-\ncal score and performance contexts from the aligned data,\nshifting the performance window such that true next score\nonsets fall from leftmost (current position minus seven) to\nrightmost (current position plus eight) to cover the possible\nstates exhaustively. During training, batches of states are\nsampled randomly, not in sequence. To aid generalization,\nwe further augment the data by random pitch shifting of all\nnotes in the state within +/- one octave. We use an ADAM\noptimizer with a learning rate with warm-up followed by\nsquare root decay. The batch size is set to 8192, the models\nare trained for 50 epochs.\n4.5 Online Models\nUsing a trained value function model, we derive two\ncomplete models. First, a simple score follower model\n(\"Greedy Agent Model\") that outputs only greedily esti-\nmated score onsets for incoming performance notes. Sec-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n637Figure 3 . Schematic overview of the Online Alignment\nModel with a monophonic piece and 8 onset context. Score\n(blue, top) and performance (red) contexts are inputs to\nthe Value Function Model which outputs value estimates\n(pink, bottom). The top three onsets (1,2,3) are passed to\na tempo extrapolator, along with existing alignments (yel-\nlow). The tempo extrapolator predicts three onsets ( ˆ1,ˆ2)\nfor the the candidate onsets. The one with lowest distance\n(∆1) to the newest performance note (*) is aligned (pink).\nond, a note-level alignment model (\"Online Alignment\nModel\") that produces both note alignments as well as an\nestimate of the next score onset for the score following set-\nting. The Greedy Agent Model consists of an agent fol-\nlowing a greedy policy based on the trained value function\nmodel, i.e., an agent picking the action Awith maximal\nestimated reward Q(S,A).\nFor the Online Alignment Model a few additional steps\nare taken. See Figure 3 for an overview of the alignment\nloop. In this model, an action is selected from the top three\nvalue estimates for a given score and performance context.\nTo pick one of these three actions, onset time informa-\ntion is incorporated. A simple local tempo estimator ap-\nproximates an expected performed onset time for each of\nthe three possible score onsets using linear extrapolation\nof beat periods computed from previously aligned notes.\nThis process takes on the role of the second onset-wise\nDTW step in the ofﬂine model (see Section 3.2): to match\nnotes that are closest together according to an approximate\nscore-to-performance mapping.\nThere are two further heuristics worth mentioning. If\nthe current performed pitch is not available at any of the\nthree highest ranking score positions, the performed note\nis counted as an insertion, and the current score position is\nunchanged. Furthermore, we decrease the number of calls\nmade to the agent in a real-time setting by directly aligning\npitches that are trivially missing at the current score onset.\n5. ONLINE EV ALUATION\nIn the following, we evaluate the agent and the proposed\nonline alignment model. In section 5.1, we address a\ngreedy agent’s capacity to identify correct score positions\nfor sampled test contexts. In section 5.2, the Online Align-ment Model is evaluated with respect to correct note-wise\nalignment. Finally, we use both the Greedy Agent Model\nas well as the Online Alignment Model as real-time sym-\nbolic score followers in an experiment in section 5.3.\n5.1 Agent Evaluation\nTop0 Top1 Top2\n94.5±0.8 % 96.6±0.5 % 97.6±0.4 %\nTable 2 . Average topK score onset hit rate and standard\ndeviation across the ﬁve test folds.\nFor direct value function evaluation, we assume a\ngreedy policy for each testing state S. That is, the\nagent picks the action Awith the highest estimated value\nQ(S,A). We evaluate this action (= chosen score onset)\nvia the distance from the ground truth score onset. Specif-\nically, we compute three metrics: the number of states\nwhere this action corresponds exactly to the true score\nonset (\"Top0\"), the number of times this action picks a\nscore onset in the neighborhood of ±one score onset of\nthe true location (\"Top1\"), and the number of times this\naction picks a score onset in the neighborhood of ±two\nscore onsets of the true location (\"Top2\"), each normal-\nized by the total number of test states. We use ﬁve-fold\ncross-validation on the same combined datasets used in\nsection 3.3, and report mean and standard deviation val-\nues across testing folds. The fold splitting is carried out\npiece-wise with roughly the same number of score onsets\nin each fold.\nTable 2 shows the results. Greedy action selects the cor-\nrect score onset with more than 94 % probability on unseen\npieces. Furthermore, for more than half of the remaining\nerrant actions, the greedy action is not further than two on-\nsets from the correct one.\n5.2 Online Note-wise Alignment\nPiece OAM DTW Ofﬂine Nakamura\nB. Op. 53 3rd. m. 99.0 % 99.4 % 98.2 %\nC. Op. 9 No. 1 97.6 % 98.4 % 98.8 %\nC. Op. 9 No. 2 97.4 % 99.1 % 97.6 %\nC. Op. 10 No. 11 90.3 % 96.3 % 94.3 %\nC. Op. 60 95.1 % 97.9 % 94.7 %\nTable 3 . Piece-wise F-scores of each model. OAM = On-\nline Alignment Model, DTW Ofﬂine = model of section\n3.3, Nakamura = reference SOTA model [11].\nTo evaluate the Online Alignment Model’s perfor-\nmance, we perform alignments for ﬁve selected perfor-\nmances: Nocturnes Op. 9 No. 1 and 2, Etude Op. 10\nNo. 11, Nocturne Op. 15 No. 2, the Barcarole Op. 60 by\nF. Chopin, and the third movement of the Sonata Op. 53\n(Waldstein) by L. v. Beethoven. The value function model\nused in this section was trained on all data except these ﬁve\npieces for 100 epochs, the rest of the training and model\nsetting remains the same. The same metrics of section 3.3Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n638Model Async ≤25ms≤50ms≤100ms\nOLTW 60.6ms 38.0 % 63.3 % 86.7 %\nGAM 36.0ms 89.0 % 91.4 % 94.6 %\nOAM 15.7ms 91.4 % 93.8 % 96.6 %\nTable 4 . Asynchrony of the models in score follower set-\nting. Column \"Async\" presents the median asynchrony.\nColumns 3, 4, 5 present the percentage of onset estimates\nwith lower asynchrony than 25ms, 50ms and 100ms, re-\nspectively.\napply, namely the F-score of retrieved matched note tuples.\nFor comparison we also add piece-wise F-scores of our\nproposed ofﬂine model as well as the model by Nakamura\net al. Table 3 shows the F-scores of note alignments. As\nexpected, the proposed online alignment performs worse\nthan ofﬂine methods, albeit with small difference. Notably,\nall models show the lowest performance on Chopin’s Op.\n10 No. 11.\n5.3 Score Following\nIn the score following setting, the core metric is the accu-\nrate prediction of the current position. We thus compute\nasynchrony values in milliseconds which give the absolute\ntime between any onset in a performance and the onset in\nthe same performance that corresponds to the estimated\nscore onset. The data used for this experiment consists\nof the same ﬁve pieces used in the previous section 5.2\nwith the same value function model training. Three mod-\nels are compared in this setting: The Online Alignment\nModel (OAM) previously evaluated in terms of note align-\nment F-scores in Table 3, the Greedy Agent Model (GAM),\nand an On-Line Time Warping (OLTW) Model. This lat-\nter OLTW model performed best in a recent music score\nfollowing comparison by Cancino-Chacón et al. [34] and\nis added as a reference. However, this model does not pre-\ndict note alignments, hence a comparison in terms of note\nalignment F-score as in Section 5.2 is not possible.\nBoth the GAM and the OAM outperform the reference\nmodel in all metrics in Table 4. Most of the lower per-\nformance of the GAM is due to the fact that for Chopin’s\nOp. 10 No. 11, this agent loses track of the performance\nclose to the end when a full measure is deleted. All subse-\nquent performance notes are estimated very wrongly. The\nonline alignment model on the other hand follows all test\nperformances robustly until the end.\n6. DISCUSSION AND CONCLUSION\nIn this paper, we introduce two models, an ofﬂine align-\nment model based on dual DTW steps, and an online align-\nment model based on an RL agent trained in an ofﬂine fash-\nion. Both models perform competitively; with the ofﬂine\nmodel surpassing the relevant state of the art.\nIn the setup of the RL training we made some simpli-\nﬁcations that warrant further discussion. Speciﬁcally, we\nset the discount factor γto zero and train using a datasetof sampled states. In section 4.2, we claim that the opti-\nmal action for each step is determined by the correct score\nonset. While this is true for the states in the dataset and if\noptimality is deﬁned by accuracy in note-wise alignment, it\nmight not be for out-of-distribution states or if the focus of\nthe agent is shifted to robustness, i.e., following the entire\nperformance even at the cost of some misaligned notes.\nFor ofﬂine RL, a crucial issue is distributional drift [27];\ni.e., the fact that the agent learns from states that follow a\ndifferent distribution that the states it would encounter in\nan online setting. Even though we can sample the state\nspace exhaustively for the training set, out-of-distribution\nstates are expected in test sets consisting of previously un-\nseen pieces and performances. Furthermore, the relative\nfrequency of training samples does not necessarily corre-\nspond to the states an online agent is likely to see dur-\ning training, where all target locations have the same fre-\nquency. Speciﬁcally, for an agent that already learned to\npredict the score onset with some accuracy, the targets\nat the limits of the context are going to be less frequent\nthan the center ones. In other words, a non-myopic on-\nline agent is likely to behave more conservatively, avoiding\nlarge skips as they do not occur that frequently in actual\nperformances.\nOn the other hand, the ofﬂine RL formulation success-\nfully leverages prior knowledge about the task and — more\nimportantly — stabilizes the gradient, rendering the train-\ning of a complex value function approximator feasible. Fu-\nture work includes shifting this trade-off back towards on-\nline RL, for example with online RL training after initial\nofﬂine training.\nThe RL agent learns to align purely on pitch informa-\ntion. Including onset or even duration information is likely\nto increase the accuracy of following at the cost of requir-\ning a more expressive model which in turn affects infer-\nence speed — a hard bottleneck for real-time application.\nIn fact, running the value estimation for every incoming\nperformance note (such as the score follower \"GAM\" in\nTable 4) uses up to a minute of computation time for the\n7273 notes in the performance of Beethoven’s Op. 53 Mvt.\n3 (Roughly 10 msper note). A further increase is liable to\naffect real-time score following in fast passages.\nIn terms of post-processing steps, both our ofﬂine and\nonline models are comparatively crude, making little use\nof score information such as ornaments. As Nakamura et\nal. [11] correctly remark, their post-processing step is in\nprinciple able to improve upon any prior more error-prone\nalignment. Further research is needed to know whether the\nofﬂine model can be improved in this way.\nTo conclude, we developed and evaluated two models of\nsymbolic music alignment which both outperform relevant\nprior work. To the best of our knowledge, this RL-based\nonline alignment model is one of the ﬁrst applications of\nnot only trainable but effectively trained models to sym-\nbolic music alignment.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6397. REPRODUCIBILITY\nOur alignment models are available at: https:\n//github.com/sildater/parangonar\n8. ACKNOWLEDGEMENTS\nThis work is supported by the European Research Council\n(ERC) under the EU’s Horizon 2020 research & innovation\nprogramme, grant agreement No. 101019375 (“Whither\nMusic?”).\n9. REFERENCES\n[1] R. B. Dannenberg, “An On-Line Algorithm for Real-\nTime Accompaniment,” in Proceedings of the Interna-\ntional Computer Music Conference (ICMC) , vol. 84,\n1984, pp. 193–198.\n[2] B. Vercoe, “The Synthetic Performer in the Context of\nLive Performance,” in Proceedings of the International\nComputer Music Conference (ICMC) , 1984, pp. 199–\n200.\n[3] E. Nakamura, N. Ono, Y . Saito, and S. Sagayama,\n“Merged-Output Hidden Markov Model for Score Fol-\nlowing of MIDI Performance with Ornaments, Desyn-\nchronized V oices, Repeats and Skips,” in International\nConference on Mathematics and Computing , 2014.\n[4] C. Raphael and Y . Gu, “Orchestral Accompaniment for\na Reproducing Piano,” in International Conference on\nMathematics and Computing , 2009.\n[5] E. Nakamura, P. Cuvillier, A. Cont, N. Ono, and\nS. Sagayama, “Autoregressive hidden semi-markov\nmodel of symbolic music performance for score fol-\nlowing,” in 16th International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , 2015.\n[6] A. Maezawa, H. G. Okuno, T. Ogata, and M. Goto,\n“Polyphonic audio-to-score alignment based on\nbayesian latent harmonic allocation hidden markov\nmodel,” in 2011 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2011, pp. 185–188.\n[7] B. Gingras and S. McAdams, “Improved Score-\nPerformance Matching using Both Structural and Tem-\nporal Information from MIDI Recordings,” Journal of\nNew Music Research , vol. 40, no. 1, pp. 43–57, 2011.\n[8] C.-T. Chen, J.-S. R. Jang, and W. Liou, “Improved\nScore-Performance Alignment Algorithms on Poly-\nphonic Music,” in 2014 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2014, pp. 1365–1369.\n[9] E. Nakamura, N. Ono, S. Sagayama, and K. Watanabe,\n“A Stochastic Temporal Model of Polyphonic MIDI\nPerformance with Ornaments,” Journal of New Music\nResearch , vol. 44, no. 4, pp. 287–304, 2015.[10] E. Nakamura, T. Nakamura, Y . Saito, N. Ono, and\nS. Sagayama, “Outer-product hidden markov model\nand polyphonic midi score following,” Journal of New\nMusic Research , vol. 43, no. 2, pp. 183–201, 2014.\n[11] E. Nakamura, K. Yoshii, and H. Katayose, “Perfor-\nmance Error Detection and Post-Processing for Fast\nand Accurate Symbolic Music Alignment,” in Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , 2017, pp. 347–\n353.\n[12] S. Wang, “Computational Methods for the Alignment\nand Score-Informed Transcription of Piano Music,”\nPh.D. dissertation, Queen Mary University of London,\nLondon, UK, 2017.\n[13] A. Arzt, “Flexible and robust music tracking,” Ph.D.\ndissertation, Johannes Kepler University Linz, Linz,\nAustria, 2016.\n[14] M. Müller, Fundamentals of Music Processing – Au-\ndio, Analysis, Algorithms, Applications . Springer,\n2015.\n[15] T. Prätzlich, J. Driedger, and M. Müller, “Memory-\nrestricted multiscale dynamic time warping,” in 2016\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2016, pp. 569–573.\n[16] M. Müller, Y . Özer, M. Krause, T. Prätzlich, and\nJ. Driedger, “Sync toolbox: A python package for ef-\nﬁcient, robust, and accurate music synchronization,”\nJournal of Open Source Software , p. 3434, 2021.\n[17] C. J. Tralie and E. Dempsey, “Exact, parallelizable dy-\nnamic time warping alignment with linear memory,” in\n21st International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2020.\n[18] S. Ewert, M. Muller, and P. Grosche, “High resolution\naudio synchronization using chroma onset features,”\nin2009 IEEE International Conference on Acoustics,\nSpeech and Signal Processing . IEEE, 2009.\n[19] S. Dixon, “An On-Line Time Warping Algorithm\nfor Tracking Musical Performances,” in International\nJoint Conference on Artiﬁcial Intelligence , 2005, pp.\n1727–1728.\n[20] A. Cont, “ANTESCOFO: Anticipatory Synchroniza-\ntion and Control of Interactive Parameters in Computer\nMusic.” in Proceedings of the International Computer\nMusic Conference (ICMC) , 2008, pp. 33–40.\n[21] C. Raphael, “Music Plus One and Machine Learning,”\ninInternational Conference on International Confer-\nence on Machine Learning , 2010, pp. 21–28.\n[22] Z. Duan and B. Pardo, “A State Space Model for\nOnline Polyphonic Audio-Score Alignment,” in 2011\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2011, pp.\n197–200.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n640[23] A. Arzt and G. Widmer, “Real-Time Music Tracking\nUsing Multiple Performances as a Reference,” in Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , 2015.\n[24] M. Dorfer, F. Henkel, and G. Widmer, “Learning\nto Listen, Read, and Follow: Score Following as\na Reinforcement Learning Game,” in Proceedings of\nthe 19th International Society for Music Information\nRetrieval Conference . Paris, France: ISMIR, Sep.\n2018, pp. 784–791. [Online]. Available: https:\n//doi.org/10.5281/zenodo.1492535\n[25] F. Henkel, S. Balke, M. Dorfer, and G. Widmer, “Score\nfollowing as a multi-modal reinforcement learning\nproblem,” Transactions of the International Society for\nMusic Information Retrieval , vol. 2, no. 1, 2019.\n[26] R. S. Sutton and A. G. Barto, Reinforcement learning:\nAn introduction . MIT press, 2018.\n[27] S. Levine, A. Kumar, G. Tucker, and J. Fu,\n“Ofﬂine reinforcement learning: Tutorial, review,\nand perspectives on open problems,” arXiv preprint\narXiv:2005.01643 , 2020.\n[28] W. Goebl. (1999) The Vienna 4x22 Piano Corpus.\n[Online]. Available: http://repo.mdw.ac.at/projects/\nIWK/the_vienna_4x22_piano_corpus/index.html\n[29] C. E. Cancino-Chacón, T. Gadermaier, G. Widmer, and\nM. Grachten, “An Evaluation of Linear and Non-linear\nModels of Expressive Dynamics in Classical Piano and\nSymphonic Music,” Machine Learning , vol. 106, no. 6,\npp. 887–909, 2017.\n[30] S. Flossmann, W. Goebl, M. Grachten, B. Nieder-\nmayer, and G. Widmer, “The Magaloff Project: An In-\nterim Report,” Journal of New Music Research , vol. 39,\nno. 4, pp. 363–377, 2010.\n[31] G. Widmer and A. Tobudic, “Playing mozart by anal-\nogy: Learning multi-level timing and dynamics strate-\ngies,” Journal of New Music Research , vol. 32, no. 3,\npp. 259–268, 2003.\n[32] C. J. Watkins and P. Dayan, “Q-learning,” Machine\nlearning , vol. 8, pp. 279–292, 1992.\n[33] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-\nness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\nFidjeland, G. Ostrovski et al. , “Human-level control\nthrough deep reinforcement learning,” nature , vol. 518,\nno. 7540, pp. 529–533, 2015.\n[34] C. Cancino-Chacón, S. Peter, P. Hu, E. Karystinaios,\nF. Henkel, F. Foscarin, N. Varga, and G. Widmer, “The\naccompanion: Combining reactivity, robustness, and\nmusical expressivity in an automatic piano accompa-\nnist,” in International Joint Conference on Artiﬁcial In-\ntelligence , 2023.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n641"
    },
    {
        "title": "Similarity Evaluation of Violin Directivity Patterns for Musical Instrument Retrieval.",
        "author": [
            "Mirco Pezzoli",
            "Raffaele Malvermi",
            "Fabio Antonacci",
            "Augusto Sarti"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265243",
        "url": "https://doi.org/10.5281/zenodo.10265243",
        "ee": "https://zenodo.org/records/10265243/files/000015.pdf",
        "abstract": "The directivity of a musical instrument is a function that describes the spatial characteristics of its sound radiation. The majority of the available literature focuses on measuring directivity patterns, with analysis mainly limited to visual inspections. Recently, some similarity metrics for directivity patterns have been introduced, yet their application has not being fully addressed. In this work, we introduce the problem of musical instrument retrieval based on the directivity pattern features.\nWe aim to exploit the available similarity metrics for directivity patterns in order to determine distances between instruments. We apply the methodology to a data set of violin directivities, including historical and modern high-quality instruments. Results show that the methodology facilitates the comparison of musical instruments and the navigation of databases of directivity patterns.",
        "zenodo_id": 10265243,
        "dblp_key": "conf/ismir/PezzoliMAS23",
        "keywords": [
            "directivity",
            "musical instrument",
            "spatial characteristics",
            "sound radiation",
            "literature",
            "similarity metrics",
            "analysis",
            "musical instrument retrieval",
            "directivity pattern features",
            "distances between instruments"
        ],
        "content": "SIMILARITY EV ALUATION OF VIOLIN DIRECTIVITY PATTERNS FOR\nMUSICAL INSTRUMENT RETRIEV AL\nMirco Pezzoli Raffaele Malvermi Fabio Antonacci Augusto Sarti\nDipartimento di Elettronica, Informazione e Bioingegneria - Politecnico di Milano, Milan, Italy\nmirco.pezzoli@polimi.it, raffaele.malvermi@polimi.it\nABSTRACT\nThe directivity of a musical instrument is a function that\ndescribes the spatial characteristics of its sound radiation.\nThe majority of the available literature focuses on mea-\nsuring directivity patterns, with analysis mainly limited to\nvisual inspections. Recently, some similarity metrics for\ndirectivity patterns have been introduced, yet their appli-\ncation has not being fully addressed. In this work, we in-\ntroduce the problem of musical instrument retrieval based\non the directivity pattern features. We aim to exploit the\navailable similarity metrics for directivity patterns in order\nto determine distances between instruments. We apply the\nmethodology to a data set of violin directivities, including\nhistorical and modern high-quality instruments. Results\nshow that the methodology facilitates the comparison of\nmusical instruments and the navigation of databases of di-\nrectivity patterns.\n1. INTRODUCTION\nThe analysis of the directional sound radiation characteris-\ntics of musical instruments is a rather old topic in the liter-\nature with ﬁrst works by Olson [1] and Meyer [2–4] dating\nback to the seventies. In the past few decades, numerous\nstudies were proposed mainly focusing on accurate mea-\nsurements of the directivity patterns [5–8] or on qualitative\ncomparisons of the instrument characteristics [9–11].\nRecently, the interest in spatial audio technologies [12]\nfor virtual and augmented reality increased the attention\ntowards the modeling and analysis of directivity patterns.\nIn particular, the modeling of directional sound sources\nshowed to provide improved sound ﬁeld reconstruction for\nthe navigation of sound scenes [13, 14]. Therefore, differ-\nent solutions have been proposed to include the directiv-\nity of acoustic sources in simulation frameworks such as\nboundary and ﬁnite element methods [15], numerical sim-\nulation [16] and geometrical acoustics [17]. As a matter of\nfact, the directivity of sound sources impacts on the accu-\nracy of room acoustics simulation [17] and it was shown to\nbe relevant for auralization [18].\n© M. Pezzoli, R. Malvermi, F. Antonacci and A. Sarti. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: M. Pezzoli, R. Malvermi, F. Antonacci and\nA. Sarti, “Similarity evaluation of violin directivity patterns for musical\ninstrument retrieval ”, in Proc. of the 24th Int. Society for Music Infor-\nmation Retrieval Conf., Milan, Italy, 2023.In [19], the authors demonstrated that users are able\nto perceive differences between omnidirectional and direc-\ntional sound sources, however the evaluation is limited to a\nsingle-tone dependent directivity pattern. In the work [20],\nit was shown that ﬂuctuations occurring in the directivity\npatterns due to the movements of the musician inﬂuence\nthe perception of listeners both in anechoic and reverberant\nconditions. More recently, in [21], the difference between\nfrequency-dependent directivities and an average directiv-\nity pattern has been investigated proving the importance\nof modelling speciﬁc frequency-dependent directivities by\nmeans of listening tests.\nSeveral studies [22–26] focus on the analysis of voice\ndirectivity patterns. In particular, [23, 25] analyze the pat-\nterns associated to held or isolated vowels and consonants\nfrom speech and singing voice [24]. Interestingly, the re-\nsults on mouth and vocal tract conﬁgurations [26] showed\ntheir impact on the directivity pattern shape.\nAs far as the musical instruments are concerned, most\nof the works put the emphasis on accurate measurement\nprocedures. Typically, the directional sound pressure is\nacquired in anechoic environments and under controlled\nconditions [6, 7]. Alternatively, near-ﬁeld acoustic holog-\nraphy [27, 28] has been employed for the evaluation of the\ndirectional sound radiation using scanning microphone ar-\nrays [29]. More recently, a ﬂexible procedure for measur-\ning the directivity pattern of sound sources that works in\nlow-reverberant environments was introduced in [8].\nIn [9], the directivity patterns of forty one orchestral\ninstruments have been acquired and analyzed. The in-\nstruments were played by musicians, rather than mechani-\ncally excited, showing that the presence of the player body\nhas the effect of smoothing the patterns. Nevertheless, al-\nthough [9] draws an interesting analysis of the patterns, the\nevaluation is mainly limited to graphical inspection with-\nout a systematic comparison of the directivity patterns.\nAs a matter of fact, the quantitative and objective com-\nparison of directivity patterns is still an open challenge.\nIn the literature, some simple metrics have been proposed\n[30–33]. Although effective, the interpretation of the re-\nsults and the quantiﬁcation of the differences might not be\neasily interpreted. In general, most of the proposed metrics\nrely on the correlation between the directivity patterns, ei-\nther in the spherical harmonics domain [30] or in the spa-\ntial domain [32]. In [30], the authors employed the nor-\nmalized cross correlation ( NCC ) over the spherical har-\nmonics coefﬁcients of the directivity patterns. The anal-138ysis assessed similarities of partials at a given frequency\ngenerated by different played pitches. In [34], a rotation-\ninvariant version of the NCC has been proposed to com-\npare the directivity patterns of the data sets in [9,35], which\nhave been made available through [36]. The devised met-\nric has been used to ﬁnd similarities across the partials of\none instrument or between different instruments and a vi-\nsualization of the corpus through MultiDimensional Scal-\ning (MDS) [37] has been provided.\nMore recently, in [38], a novel set of metrics has been\nintroduced, which includes the Jaccard similarity index\n(JSI) and the centers of mass distance ( CMD ) in addition\ntoNCC . BothJSIandCMD are derived from the anal-\nysis of the so-called principal radiation regions , namely\nangular regions of the directivity pattern which exhibit the\nhighest sound energy radiation. In [38], the metrics are\nused for the characterization of directivity patterns of pres-\ntigious historical violins enabling the quantitative compari-\nson of the instruments. Nonetheless, the analysis is limited\nto a small set of 10 instruments and the conclusions drawn\nby the analysis of each metric, although relevant, are not\nreadily combined.\nIn this work, we aim to exploit available similarity met-\nrics for directivity patterns in a comprehensive and sys-\ntematic fashion. Considering the problem of musical in-\nstrument retrieval based on the directivity pattern features\n[34, 39], we introduce novel distances, namely the Jaccard\nsimilarity distance ( JSD) based on JSI, and the directivity\nindex distance ( DID ) derived from the so-called directivity\nindex (DI), which are combined with the CMD in a cumu-\nlative Directivity Pattern Distance (DPD) . The proposed\ndistances are blind with respect to the source type, be-\ncause they work directly on the directivity values. There-\nfore, ideally they can be applied on any kind of sound\nsources, including musical instruments of different fami-\nlies. The joint adoption of multiple distances allows us to\ntake into account different aspects of the directivity pat-\nterns without limiting the comparison to a single metric.\nMoreover, the introduced DPD provides a single-valued\nsolution that represents the distance between the directiv-\nity patterns combining the information provided by each\nconsidered metric.\nAlthough the proposed distances can be applied on dif-\nferent musical instruments, we tested them on a data set of\nviolin directivities. As a matter of fact, violins represent an\ninteresting case study due to the highly variability of direc-\ntivity patterns among the instruments [40, 41]. The corpus\ncontains a total of 18 instruments equally divided between\n10 historical and 8 modern high-quality violins. To the\nbest of our knowledge, this is the largest data set of vio-\nlin directivity patterns evaluated in the current literature.\nThe analysis allowed us to observe interesting similarities\namong the instruments, identifying relevant information in\nthe data set. In particular, modern instruments are rela-\ntively distant from the historical ones. Moreover, thanks to\nthe adoption of DPD , we could identify clusters of histori-\ncal instruments made by one violin maker and two modern\n“twin” violin. Similarly to [34], we exploit the MDS tech-\nD(ϕ,θ,ω)\n[dB]\n180∘90∘\n0∘−90∘−180∘90∘60∘30∘\n120∘\n150∘\nϕθ\nFigure 1 . Example of directivity pattern D(φ,θ,ω)of a\nGenelec 8030A at 1.4kHz , taken from [43]. The principal\nradiation region Pis delimited by a solid black line, while\nthe center of mass ris marked by a black cross. The refer-\nence system is reported from top and frontal views.\nnique for the visualization of the data set, which allows\nus to graphically assess the distances between the instru-\nments observing the clusters of similar violins within. The\nobtained results pave the way to the retrieval of musical\ninstruments according to their directional sound radiation\nand open novel perspectives for the exploitation of direc-\ntivity pattern databases.\n2. SIMILARITY METRICS FOR DIRECTIVITY\nPATTERNS\nLet us deﬁne the directivity pattern of an acoustic source as\nthe square-integrable function D(·)∈L2(S2)describing\nthe energy of the directional sound radiation. The directiv-\nity pattern is thus deﬁned over a unit sphere comprising all\nthe possible directions of emission. It follows that the di-\nrectivity pattern can be conveniently expressed through the\nwidely adopted spherical harmonics expansion [5,8,13] as\nD(φ,θ,ω) =N/summationdisplay\nn=0n/summationdisplay\nm=−nCm\nn(ω)Ym\nn(φ,θ), (1)\nwhereφ∈[0,2π]andθ∈[0,π]are the azimuth and\ninclination angles, respectively, Cm\nn(ω)are the spherical\nharmonics coefﬁcients associated with the source directiv-\nity pattern and Ym\nn(φ,θ)is the spherical harmonic of de-\ngreenand order m[42]. It is worth noting that the di-\nrectivity pattern (1) depends on the temporal frequency ω.\nMoreover, in (1), we assumed the directivity pattern to be\nband-limited being Nthe maximum expansion order. In\nFig. 1, an example of a loudspeaker directivity pattern is\nreported.\n2.1 Data model\n2.1.1 Binary directivity pattern\nIn [38], the principal radiation region of a directivity pat-\ntern is deﬁned as the set of adjacent directions Pthat cor-\nrespond to the maximum acoustic energy emission. In par-\nticular, given a threshold value τ, the principal radiation\nregion is deﬁned as\nP(ω) =/braceleftbig\n(¯φp,¯θp) :DdB(¯φp,¯θp,ω)≥τ/bracerightbig\n, (2)\nwhereτ=−3dB and\nDdB(φ,θ,ω) = 10log10/parenleftbiggD(φ,θ,ω)\nmax(D(φ,θ,ω))/parenrightbigg\n(3)\nrepresents the normalized directivity pattern in decibel\nscale with max the function extracting its maximum value.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n139In Fig. 1, the principal radiation region Pis delimited by a\nsolid black line.\nThe thresholding procedure in (2) allows one to deﬁne\nthe binary directivity pattern indicating the principal radi-\nation region as\n¯D(φ,θ,ω) =/braceleftbigg1 (φ,θ)∈ P(ω)\n0 otherwise. (4)\nThe adoption of the binary patterns is preferable rather\nthan considering only the direction of the maximum, i.e.\na single point, in the directivity pattern. As a matter of\nfact, the binary pattern indicates the regions of high energy\nemission, i.e. principal radiation, which can have arbitrary\nshape and extension accordingly to the overall directional\ncharacteristics of the directivity pattern.\n2.1.2 Centers of mass\nAlthough the binary pattern (4) provides a comprehensive\nrepresentation of the principal radiation regions, it is con-\nvenient to further identify a “preferred” direction of emis-\nsion for each region. Therefore, we deﬁne the center of\nmass for a principal radiation region Pas [38]\nr(ω) =1\nM/summationdisplay\np∈P(ω)mprp, (5)\nwhererp=/bracketleftbig\nsin¯θpcos¯φp,sin¯θpsin¯φp,cos¯θp/bracketrightbigTare the\npoints belonging to the set deﬁned in (2). In practice, the\ndirections of Pare weighted using the corresponding en-\nergy value in the normalized pattern, namely\nmp=D(¯φp,¯θp,ω)\nmax(D(·,ω)),withM=/summationdisplay\np∈P(ω)mp.(6)\nThe center of mass of the directivity pattern is marked in\nFig. 1 by a black cross.\n2.2 Distance Metrics\nIn order to compare the directivity patterns of acoustic\nsources within a data set, we rely on a set of metrics re-\ncently proposed in [38]. Differently from customarily di-\nrectivity pattern comparisons, where a single metric is con-\nsidered, the employment of multiple metrics allows us to\ntake into account different characteristics that are captured\nby each metric.\n2.2.1 Jaccard Similarity Distance ( JSD)\nAccording to [38], we deﬁne the Jaccard similarity index\n(JSI) between two binary directivity patterns as\nJSIk,j(ω) =|¯Dk(ω)∩¯Dj(ω)|\n|¯Dk(ω)∪¯Dj(ω)|, (7)\nwhere∩is the intersection operator and ∪is the union be-\ntween the binary patterns of the kth andjth sources. From\nthe deﬁnition in (7), it follows that JSIk,j(ω) = 1 when\ntwo binary patterns match exactly, while JSIk,j(ω) = 0\nwhen the corresponding principal radiation regions do notoverlap. In order to interpret the JSIin terms of a distance,\nwe introduce the JSD metric as\nJSDk,j(ω) = 1−JSIk,j(ω), (8)\nso that the JSD decreases up to 0 when two principal radi-\nation regions are matched and the maximum value of JSD\nis 1, indicating two completely disjoint regions.\n2.2.2 Center of Mass Distance ( CMD )\nTheCMD is deﬁned in order to compute the distance be-\ntween two centers of mass as [38]\nCMDk,j(ω) = arctan/parenleftbigg|rk(ω)×rj(ω)|\nrk(ω)·rj(ω)/parenrightbigg\n,(9)\nwhere×and·denote the vectorial cross and dot products,\nrespectively. As in [33], when multiple centers of mass are\npresent inside the directivity patterns, the vectors r(5) are\nselected in order to retain the lowest CMDk,j(ω)values.\n2.2.3 Directivity Index Distance ( DID )\nThe directivity index ( DI) is a well-known feature that de-\nscribes the directionality of a sound source [33]. In par-\nticular, the DImeasures how much energy is concentrated\naround the principal directions of a directivity pattern. In\nthis work, we consider the DIof the normalized directivity\npatterns deﬁned as\nDIk(ω) =1/integraltext2π\n0/integraltextπ\n0/hatwideDk(φ,θ,ω)dφdθ, (10)\nwhere/hatwideDkis the normalized directivity pattern of the kth\nsource in linear scale. The DIin (10) is computed with\nrespect to the maximum value of the directivity pattern,\nwhich in case of normalized patterns is equal to 1. It fol-\nlows that high DIvalues occur for directivity patterns with\nlarge principal radiation regions, and vice versa.\nIn order to compare two directivity patterns in terms of\ntheirDIvalues, we deﬁne the DID as\nDIDk,j(ω) =/radicalBig\n(DIj(ω)−DIk(ω))2, (11)\nwhereDIkandDIjare theDI(10) of the kth andjth\nsources, respectively.\n2.2.4 Directivity Pattern Distance ( DPD )\nIn order to conveniently compare two sound sources in\nterms of their directivity features, we introduce an over-\nall metric that combines the previously deﬁned JSD,CMD\nandDID into a scalar value. Hence, we deﬁne the so-called\ndirectivity pattern distance DPD metric as\nDPDk,j=JSDk,j+CMDk,j\nmax(CMDk,j)+DIDk,j\nmax(DIDk,j),(12)\nwhereJSDk,j,CMDk,j,DIDk,jdenote the mean of the\nthree distance metrics over the frequency axis. It must be\nnoted that the values of CMDk,jandDIDk,jin (12) are\nnormalized with respect to the maximum value encoun-\ntered in the data set under analysis, such that all the com-\nponents of the sum vary within the same dynamic range,\ni.e. between 0 and 1, and thus have the same relative im-\nportance in the deﬁnition of DPD .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1403. EV ALUATION\n3.1 Data set of violin directivity patterns\nThe proposed methodology is applied to a data set of vi-\nolin directivities. The data set includes the frequency-\ndependent directivity patterns of eighteen violins, includ-\ning ten historical violins made between the 16th and 17th\ncenturies and eight modern violins made during the last\ntwo centuries. For all the instruments, the owners provided\nconsent for the usage of the results in an anonymous fash-\nion. For this reason, and for the ease of reading, we will\ndenote all the historical violins with labels H1–H10, while\nwe will refer to the modern ones as M1–M8.\nConcerning the collection of modern violins, it is note-\nworthy that the instruments labeled with M1–M6 are ﬁne\nviolins selected among the candidates of the “Antonio\nStradivari International Triennial Competitions of Stringed\ninstrument making”. The competition, held in the city of\nCremona since 1976, embraces both Cremonese and inter-\nnational competitors. Moreover, violins M7 and M8 were\nmade by a Cremonese luthier and are known as “twin vi-\nolins”. The twin violins were built by employing the very\nsame block of tonewood and following the same geomet-\nrical model. As a matter of fact, previous research already\nshowed the high similarity in all the spatial characteristics\nof their sound.\nThe patterns were collected experimentally through the\nmeasurement procedure described in [8] and were evalu-\nated at varying frequency within the range [200,5000] Hz\nusing a 4th-order spherical harmonics expansion in (1).\nThe instruments are played by one professional violin-\nist who is free to move and play comfortably, while the\nsource position and orientation are estimated by the system\nenabling the measurement of the directivity as described\nin [8]. The data processing pipeline and the computation\nof the metrics is developed using the M ATLAB software.\n3.2 Analysis of the metrics\nTo assess the signiﬁcance of the proposed distance metrics,\nwe ﬁrst compare the frequency-averaged values of JSD,\nCMD andDID computed over the set of violin directivity\npatterns to those obtained for the same data with a com-\nmonly used similarity metric, namely NCC . TheNCC\nmetric provides a measure of the element-wise similarity\nbetween two patterns. In order to properly compare the\npreviously deﬁned distances with the baseline, the NCC\nbetween the patterns of the k-th andl-th violins is formal-\nized in terms of a distance as\nNCCk,l= 1−1\nSS/summationdisplay\ns=1/hatwideDk(φ,θ,ωs)/hatwideDl(φ,θ,ωs)\n∥/hatwideDk(φ,θ,ωs)∥∥/hatwideDl(φ,θ,ωs)∥,(13)\nwhereωsis thes-th frequency at which the directivity pat-\nterns are evaluated, with s= 1,...,S andSthe total num-\nber of frequency bins in the data set. In this way, NCCk,l\nis close to zero when two patterns are similar and reaches\na value equal to 2 when they are inversely correlated.\nFig. 2 shows a comparison between all the metrics under\nstudy. For any possible pair of metrics, a 2D scatter plot is\n(e)(c) (b) (a)\n(d) (f)Figure 2 . Comparison between proposed distance met-\nrics (JSD,CMD,DID ) and Normalized Cross Correlation\n(NCC ). For each combination of metrics, a 2D scatter plot\nof the corresponding frequency-averaged values is shown.\nZ-score normalization is applied to ensure the same dy-\nnamic range along the axes [44]. Linear regression is per-\nformed to analyze the correlation between the metrics. The\nregressed line and the R2value, measuring the degree of\ncorrelation, are highlighted in red.\nreported. The coordinates of the markers in the plot corre-\nspond to the two distances for all the possible pairs of vio-\nlins in the data set. Z-score normalization is applied to the\nresulting values to ensure the same dynamic range along\nthe axes [44]. The scatter plots in the ﬁrst row show the\ncomparison between NCC (13) and each of the proposed\nmetrics, while the scatter plots in the second row present\nthe comparison between JSD,CMD andDID only. By in-\nspecting the resulting distributions of points, it is possible\nto highlight correlations between the metrics.\nOn the one hand, it can be noticed that some pairs of\nmetrics exhibit a point distribution that concentrates along\na line. A linear trend, in fact, can be observed in Fig. 2a\nand 2b, showing the ( NCC,JSD) and (NCC,CMD ) point\ndistributions, respectively. Although less emphasized, a\nsimilar trend can be noticed in Fig. 2d and 2e, reporting the\ndistribution of ( JSD,CMD ) and (JSD,DID ), respectively.\nThe presence of linearity in these point distributions\ncan be interpreted as due to correlation, i.e. shared in-\nformation, between the metrics under analysis. This can\nbe particularly true for NCC andJSD, which both mea-\nsure the degree of pattern matching by deﬁnition. More in-\nterestingly, however, correlation can be observed between\nCMD andNCC . We can thus conclude that two violins\nwith similar principal directions of radiation tend to ex-\nhibit highly matching directivity patterns. Furthermore,\nJSD andCMD can be used instead of NCC to provide two\nsimilarity measures by looking at the pattern shape and at\nthe principal direction of radiation separately without los-\ning information. Indeed, Fig. 2d shows that JSD andCMD\nare less correlated than when considering NCC .\nOn the other hand, Fig. 2c and Fig. 2e do not exhibit\na linear distribution. We can interpret this evidence as the\nabsence of correlation between DID ,NCC andCMD . As\na matter of fact, DID measures the difference in the direc-\ntivity index of two patterns, which is related to the energy\ndistribution, and thus extracts an energy-related informa-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n141(a)\n (b)\n (c)Figure 3 . Evaluation of violin similarity based on JSD (a),CMD (b)andDID (c). The elements inside the matrices are\nobtained by averaging the frequency-dependent distance values. Pairs of similar violins are denoted with dark blue colors,\nwhile dissimilar violins are highlighted in yellow. Hierarchical clustering algorithms are employed to sort the elements\ninside the resulting matrices. The resulting dendrograms are reported above each distance matrix.\ntion that is not captured by the other metrics.\nA quantitative measure of the correlation between the\nmetrics under analysis can be evaluated by performing lin-\near regression on each point distribution. The regressed\nlines are denoted in red inside each 2D scatter plot. The\nregression accuracy is assessed in terms of the coefﬁcient\nof determination R2, which is related to the Pearson cor-\nrelation coefﬁcient in the case of simple linear regression.\nThe resulting R2values are reported as an inset inside each\nplot. According to [45], the values range between 0 and\n1, and moderate and strong correlation occurs for values\ngreater than 0.3 and 0.6, respectively.\nIt can be noticed that the pair (NCC,JSD) shows mod-\nerate to strong correlation, with R2= 0.57. Moderate cor-\nrelations can be observed for (NCC,CMD) ,(JSD,CMD)\nand(JSD,DID) , withR2= 0.42,R2= 0.33and\nR2= 0.38, respectively. Finally, no correlation occurs\nfor(NCC,DID) and(CMD,DID) , withR2= 0.09and\nR2= 0.03, respectively.\n3.3 Violins clustering based on similarity metrics\nIn order to group musical instruments that exhibit a sound\nemission with similar spatial characteristics, the proposed\ndistance metrics can be used together with classical clus-\ntering methods. In this case, hierarchical clustering meth-\nods are employed, based on the generation of dendrograms\n[46]. In particular, the proposed similarity metrics are used\nfor the iterative deﬁnition of the dendrogram. It is worth to\nunderline, that the adopted clustering algorithm does not\nrequire any training data and it is applied directly on the\ncomputed similarities. Fig. 3 shows the distance matrices\nassessing the pairwise similarity between all the violins in\nthe data set under study. The matrix elements in Fig. 3a,\n3b and 3c are obtained using the frequency-averaged JSD,CMD andDID values, respectively. Pairs of similar vi-\nolins are highlighted with dark blue colors, while dissim-\nilar violins are colored in yellow. The elements of each\ndistance matrix are sorted according to the leaf order of a\ndendrogram tree. The Ward’s method [47] is used to gener-\nate the tree branches, such that similar violins concentrate\ninside the matrix.\nBy inspecting the resulting distance matrices, it is note-\nworthy that the order of the elements in the matrix varies\ndepending on the speciﬁc distance considered. However,\nexpected groups of violins can be highlighted. In Fig. 3a,\nthe subsets of historical and modern violins are clearly dis-\ntinguished, being placed at the top-left and bottom-right\ncorners of the JSD matrix, respectively. In particular, the\ntwin violins (M7-M8) exhibit the minimum JSD value\nin the matrix and the remaining modern violins (M1-M6)\ncluster together. The same behavior occurs also in Fig. 3b\nand 3c, although at different locations inside the matrices.\nRegarding the historical violins, H1 appears to be very\ndifferent with respect to the rest of the data set. In particu-\nlar, high values are encountered for JSD andDID , which\nare related to the pattern shape and energy, respectively.\nConversely, the same violin is more similar to other histor-\nical violins concerning the principal directions of radiation.\nFig. 4 shows the results of violin clustering based on the\nproposed overall metric DPD . On the left, the dendrogram\ncomputed with the Ward’s method is shown, while on the\nright the resulting distance matrix is reported, with the ele-\nments sorted following the dendrogram hierarchy. Pairs of\nviolins characterized by DPD values close to either zero or\nthe maximum are colored in green or white, repsectively.\nTypically, clusters can be extracted from the hierarchy\nof the dendrogram tree by applying a thresholding with re-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n142Figure 4 . Violin clustering based on the proposed DPD metric. Small distance values correspond to pairs of similar violins\nand are highlighted in green, while pairs of dissimilar violins exhibit high distance values and are highlighted in white. The\nelements inside the matrix are sorted according to the dendrogram tree, shown on the left. Clustering is performed by\nthresholding the dendrogram tree. The threshold is denoted with a cyan line, while the resulting clusters are colored in red.\nspect to the tree height. We decide to subdivide the dendro-\ngram at a height equal to 1.9, i.e. the mean value between\nthe height of the lowest branch in the tree and the height\nof its root, denoted with a vertical dashed cyan line. As a\nresult, seven clusters are identiﬁed inside the data set: (i)\nthree consisting of a single violin (i.e. H1, H10 and H3),\n(ii) one cluster made of ﬁve historical violins (i.e. H4-H6-\nH7-H8-H9), (iii) one cluster made of two historical violins\nand the twin violins (i.e. H2-H5-M7-M8), (iv) one clus-\nter with four modern violins (i.e. M3-M4-M5-M6) and (v)\none cluster with two modern violins (i.e. M1-M2).\nThe obtained clusters are coherent with the similarities\nextracted from the single proposed metrics. In particular,\nthe distinction between historical and modern violins and\nthe high similarity between the twin violins (M7-M8) are\nemphasized by the DPD . Moreover, hierarchical cluster-\ning based on DPD is able to recognize a cluster with ﬁve\nhistorical violins i.e. H4-H9-H6-H8-H7. Remarkably, the\ninstruments belonging to this cluster have been made by\nthe same luthier.\n3.3.1 Visualization of the data collection through MDS\nGiven the similarity analysis of the violin directivity pat-\nterns based on the proposed DPD metric, the employment\nof MultiDimensional Scaling methods (MDS) allows one\nto easily visualize and navigate the collection of data [37].\nIn practice, MDS methods enable the mapping of the vio-\nlins into a multidimensional space so that the similarities\nbetween the musical instruments in the data set are pre-\nserved.\nFig. 5 shows a 3D representation of the data set based on\nMDS. In this case, the coordinate system results from the\nuse of Nonclassical MDS with the distance matrix shown\nin Fig. 4 as input. Each marker in the scatter plot corre-\nsponds to a violin, and the same marker color is used to\ndenote violins belonging to the same cluster.\n4. CONCLUSION\nIn this paper, we tackle the problem of directivity patterns\ncomparison by introducing a novel distance metric denoted\nFigure 5 . 3D representation of the violin data set based\non Multidimensional Scaling. Nonclassical MDS is ap-\nplied on the resulting DPD matrix to map the violins into\na three-dimensional space. Each marker in the scatter plot\ncorresponds to a violin. The same marker color is used for\nviolins belonging to the same cluster.\nasDPD , which is based on a combination of different sim-\nilarity metrics and features of the patterns. This approach\nallows one to compactly compare the similarity of directiv-\nity patterns exploiting the different information provided\nbyJSD,CMD andDID . The considered metrics are com-\npared within each other and with respect to the well-known\nNCC , highlighting that they provide mutually uncorrelated\ninformation.\nWe analyzed a data set of directivity patterns of 18 vi-\nolins divided between 10 historical and 8 modern instru-\nments. Through the use of DPD , we were able to iden-\ntify clusters of similar instruments among which a set of\nhistorical instruments made by the same maker and two\n“twin” violins. Finally, the MDS technique enabled the vi-\nsualization of the violin data collection starting from the\ncomputed distances.\nWe foresee the application of the proposed approach for\nthe retrieval of musical instruments based on directivity\npattern characteristics. This opens new perspectives for the\nnavigation of data sets of directivity patterns which can be\nused to provide a more realistic acoustic presence of musi-\ncal instruments within spatial audio applications.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1435. REFERENCES\n[1] H. F. Olson, Music, physics and engineering . Courier\nCorporation, 1967, vol. 1769.\n[2] J. Meyer, “Directivity of the bowed stringed instru-\nments and its effect on orchestral sound in concert\nhalls,” The Journal of the Acoustical Society of Amer-\nica, vol. 51, no. 6B, pp. 1994–2009, 1972.\n[3] ——, Acoustics and the performance of music: Man-\nual for acousticians, audio engineers, musicians, ar-\nchitects and musical instrument makers . Springer Sci-\nence & Business Media, 2009.\n[4] ——, “The inﬂuence of the directivity of musical in-\nstruments on the efﬁciency of reﬂecting or absorb-\ning areas in proximity to the orchestra,” Acta Acustica\nunited with Acustica , vol. 36, no. 3, pp. 147–161, 1976.\n[5] G. Weinreich and E. B. Arnold, “Method for measuring\nacoustic radiation ﬁelds,” The Journal of the Acoustical\nSociety of America , vol. 68, no. 2, pp. 404–411, 1980.\n[6] J. Pätynen, V . Pulkki, and T. Lokki, “Anechoic record-\ning system for symphony orchestra,” Acta Acustica\nunited with Acustica , vol. 94, no. 6, pp. 856–865, 2008.\n[7] J. Curtin, “Measuring violin sound radiation using an\nimpact hammer,” J. Violin Soc. Am. VSA Papers, XXII ,\nno. 1, pp. 186–209, 2009.\n[8] A. Canclini, F. Antonacci, S. Tubaro, and A. Sarti, “A\nmethodology for the robust estimation of the radiation\npattern of acoustic sources,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing , vol. 28,\npp. 211–224, 2020.\n[9] N. R. Shabtai, G. Behler, M. V orländer, and\nS. Weinzierl, “Generation and analysis of an acoustic\nradiation pattern database for forty-one musical instru-\nments,” The Journal of the Acoustical Society of Amer-\nica, vol. 141, no. 2, pp. 1246–1256, 2017.\n[10] F. Otondo and J. H. Rindel, “The inﬂuence of the direc-\ntivity of musical instruments in a room,” Acta acustica\nunited with Acustica , vol. 90, no. 6, pp. 1178–1184,\n2004.\n[11] J. Pätynen and T. Lokki, “Directivities of symphony or-\nchestra instruments,” Acta Acustica united with Acus-\ntica, vol. 96, no. 1, pp. 138–167, 2010.\n[12] J. G. Tylka and E. Y . Choueiri, “Fundamentals of a\nparametric method for virtual navigation within an ar-\nray of ambisonics microphones,” Journal of the Audio\nEngineering Society , vol. 68, no. 3, pp. 120–137, 2020.\n[13] M. Pezzoli, F. Borra, F. Antonacci, A. Sarti, and\nS. Tubaro, “Reconstruction of the virtual microphone\nsignal based on the distributed ray space transform,”\nin26th European Signal Processing Conference (EU-\nSIPCO) . IEEE, 2018, pp. 1537–1541.[14] M. Pezzoli, F. Borra, F. Antonacci, S. Tubaro, and\nA. Sarti, “A parametric approach to virtual miking for\nsources of arbitrary directivity,” IEEE Trans. on au-\ndio, speech, and language Process. , vol. 28, pp. 2333–\n2348, 2020.\n[15] R. Mehra, L. Antani, S. Kim, and D. Manocha,\n“Source and listener directivity for interactive wave-\nbased sound propagation,” IEEE transactions on visu-\nalization and computer graphics , vol. 20, no. 4, pp.\n495–503, 2014.\n[16] J. Ahrens and S. Bilbao, “Computation of spher-\nical harmonic representations of source directivity\nbased on the ﬁnite-distance signature,” IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing , vol. 29, pp. 83–92, 2021. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/9257177/\n[17] J. Klein and M. V orländer, “Simulative investigation of\nrequired spatial source resolution in directional room\nimpulse response measurements,” in EAA Spatial Au-\ndio Signal Processing Symposium , 2019, pp. 37–42.\n[18] B. N. Postma, H. Demontis, and B. F. Katz, “Subjec-\ntive evaluation of dynamic voice directivity for aural-\nizations,” Acta Acustica united with Acustica , vol. 103,\nno. 2, pp. 181–184, 2017.\n[19] L. M. Wang and M. C. Vigeant, “Evaluations of output\nfrom room acoustic computer modeling and auraliza-\ntion due to different sound source directionalities,” Ap-\nplied Acoustics , vol. 69, no. 12, pp. 1281–1293, 2008.\n[20] D. Ackermann, C. Böhm, F. Brinkmann, and\nS. Weinzierl, “The acoustical effect of musicians’\nmovements during musical performances,” Acta Acus-\ntica united with Acustica , vol. 105, no. 2, pp. 356–367,\n2019.\n[21] A. Corcuera Marruffo and V . Chatziioannou, “A pilot\nstudy on tone-dependent directivity patterns of musi-\ncal instruments,” in Audio Engineering Society Con-\nference: AES 2022 International Audio for Virtual and\nAugmented Reality Conference . Audio Engineering\nSociety, 2022.\n[22] C. Nouﬁ, D. Markovic, and P. Dodds, “Reconstruct-\ning the dynamic directivity of unconstrained speech,”\narXiv preprint arXiv:2209.04473 , 2022.\n[23] B. F. Katz, F. Prezat, and C. d’Alessandro, “Human\nvoice phoneme directivity pattern measurements,” in\n4th Joint Meeting of the Acoustical Society of America\nand the Acoustical Society of Japan , 2006, p. 3359.\n[24] B. Katz and C. d’Alessandro, “Directivity measure-\nments of the singing voice,” in International Congress\non Acoustics (ICA 2007) , 2007, p. 6p.\n[25] P. Kocon and B. B. Monson, “Horizontal directivity\npatterns differ between vowels extracted from running\nspeech,” The Journal of the Acoustical Society of Amer-\nica, vol. 144, no. 1, pp. EL7–EL12, 2018.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n144[26] S. Bellows and T. Leishman, “High-resolution analysis\nof the directivity factor and directivity index functions\nof human speech,” in Audio Engineering Society Con-\nvention 146 . Audio Engineering Society, 2019.\n[27] J. D. Maynard, E. G. Williams, and Y . Lee, “Nearﬁeld\nacoustic holography: I. theory of generalized holog-\nraphy and the development of NAH,” The Journal of\nthe Acoustical Society of America , vol. 78, no. 4, pp.\n1395–1413, 1985.\n[28] M. Olivieri, M. Pezzoli, F. Antonacci, and A. Sarti,\n“A physics-informed neural network approach for\nnearﬁeld acoustic holography,” Sensors , vol. 21,\nno. 23, 2021. [Online]. Available: https://www.mdpi.\ncom/1424-8220/21/23/7834\n[29] D. Fernandez Comesana, T. Takeuchi,\nS. Morales Cervera, and K. R. Holland, “Mea-\nsuring musical instruments directivity patterns with\nscanning techniques,” in 25th International Congress\non Sound and Vibration, ICSV 2019 , 2019.\n[30] F. Hohl and F. Zotter, “Similarity of musical instrument\nradiation-patterns in pitch and partial,” Fortschritte der\nAkustik, DAGA, Berlin , 2010.\n[31] P. Guillon, R. Nicol, and L. Simon, “Head-related\ntransfer functions reconstruction from sparse measure-\nments considering a priori knowledge from database\nanalysis: A pattern recognition approach,” in Audio\nEngineering Society Convention 125 . Audio Engi-\nneering Society, 2008.\n[32] S. Moreau, J. Daniel, and S. Bertet, “3d sound\nﬁeld recording with higher order ambisonics–objective\nmeasurements and validation of a 4th order spherical\nmicrophone,” in 120th Convention of the AES , 2006,\npp. 20–23.\n[33] C. Molloy, “Calculation of the directivity index for var-\nious types of radiators,” The Journal of the Acoustical\nSociety of America , vol. 20, no. 4, pp. 387–405, 1948.\n[34] T. Carpentier and A. Einbond, “Spherical correla-\ntion as a similarity measure for 3d radiation patterns\nof musical instruments,” in 16ème Congrès Français\nd’Acoustique . HAL Open Science, 2022.\n[35] S. Weinzierl, M. V orländer, G. Behler, F. Brinkmann,\nH. von Coler, E. Detzner, J. Krämer, A. Lindau,\nM. Pollow, F. Schulz et al. , “A database of anechoic mi-\ncrophone array measurements of musical instruments,”\n2017.\n[36] J. Ahrens, “Database of spherical harmonic representa-\ntions of sound source directivities,” https://doi.org/10.\n5281/zenodo.3707708, Mar 2020.\n[37] N. Saeed, H. Nam, M. I. U. Haq, and D. B. Muham-\nmad Saqib, “A survey on multidimensional scaling,”\nACM Computing Surveys (CSUR) , vol. 51, no. 3, pp.\n1–25, 2018.[38] M. Pezzoli, A. Canclini, F. Antonacci, and A. Sarti, “A\ncomparative analysis of the directional sound radiation\nof historical violins,” The Journal of the Acoustical So-\nciety of America , vol. 152, no. 1, pp. 354–367, 2022.\n[39] M. Olivieri, R. Malvermi, M. Pezzoli, M. Zanoni,\nS. Gonzalez, F. Antonacci, and A. Sarti, “Audio infor-\nmation retrieval and musical acoustics,” IEEE Instrum.\nMeas. Mag. , vol. 24, no. 7, pp. 10–20, 2021.\n[40] G. Weinreich, “Directional tone color,” The Journal of\nthe Acoustical Society of America , vol. 101, no. 4, pp.\n2338–2346, 1997.\n[41] J. Woodhouse, “The acoustics of the violin: a review,”\nReports on Progress in Physics , vol. 77, no. 11, p.\n115901, 2014.\n[42] A. Schmitz, T. Karolski, and L. Kobbelt, “Using spher-\nical harmonics for modeling antenna patterns,” in 2012\nIEEE Radio and Wireless Symposium . IEEE, 2012,\npp. 155–158.\n[43] J. G. Tylka, R. Sridhar, and E. Choueiri, “A database of\nloudspeaker polar radiation measurements,” in Audio\nEngineering Society Convention 139 . Audio Engi-\nneering Society, 2015.\n[44] A. Jain, K. Nandakumar, and A. Ross, “Score nor-\nmalization in multimodal biometric system,” Pattern\nRecognition , vol. 38, pp. 2270–2285, 12 2005.\n[45] J. Cohen, Statistical power analysis for the behavioral\nsciences . Routledge, 2013.\n[46] G. Gruvaeus and H. Wainer, “Two additions to hierar-\nchical cluster analysis,” British Journal of Mathemati-\ncal and Statistical Psychology , vol. 25, no. 2, pp. 200–\n206, 1972.\n[47] S. Sharma, N. Batra et al. , “Comparative study of sin-\ngle linkage, complete linkage, and ward method of ag-\nglomerative clustering,” in 2019 International Confer-\nence on Machine Learning, Big Data, Cloud and Par-\nallel Computing (COMITCon) . IEEE, 2019, pp. 568–\n573.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n145"
    },
    {
        "title": "Carnatic Singing Voice Separation Using Cold Diffusion on Training Data With Bleeding.",
        "author": [
            "Genís Plaja-Roglans",
            "Marius Miron",
            "Adithi Shankar",
            "Xavier Serra"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.13984075",
        "url": "https://doi.org/10.5281/zenodo.13984075",
        "abstract": "Pre-trained model for compIAM. Initialization identifier: separation:cold-diff-sep. Trained with the Saraga dataset using the strategy in:\nPlaja-Roglans, G., Miron, M., Shankar, A., Serra, X. (2023). 'Carnatic singing voice separation using cold diffusion on training data with bleeding', In: Proc. of the 24rd Int. Society for Music Information Retrieval Conference (ISMIR 2023), Milan, Italy.",
        "zenodo_id": 13984075,
        "dblp_key": "conf/ismir/Plaja-RoglansMS23",
        "keywords": [
            "pre-trained model",
            "compIAM",
            "initialization identifier",
            "separation:cold-diff-sep",
            "trained with Saraga dataset",
            "strategy in Plaja-Roglans et al.",
            "Carnatic singing voice separation",
            "cold diffusion on training data with bleeding",
            "ISMIR 2023",
            "Proc. of the 24th Int. Society for Music Information Retrieval Conference"
        ],
        "ee": "https://zenodo.org/records/13984075/files/cold-diff-sep.zip"
    },
    {
        "title": "Segmentation and Analysis of Taniavartanam in Carnatic Music Concerts.",
        "author": [
            "Gowriprasad R.",
            "Srikrishnan Sridharan",
            "R. Aravind 0001",
            "Hema A. Murthy"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265221",
        "url": "https://doi.org/10.5281/zenodo.10265221",
        "ee": "https://zenodo.org/records/10265221/files/000005.pdf",
        "abstract": "In Carnatic music concerts, taniavartanam is a solo percussion segment that showcases intricate and elaborate extempore rhythmic evolution through a series of homogeneous sections with shared rhythmic characteristics. While taniavartanam segments have been segmented from concerts earlier, no effort has been made to analyze these percussion segments. This paper attempts to further segment the taniavartanam portion into musically meaningful segments. A taniavartanam segment consists of an abhipraya, where artists show their prowess at extempore enunciation of percussion stroke segments, followed by an optional korapu, where each artist challenges the other, and concluding with mohra and korvai, each with its own nuances. This work helps obtain a comprehensive musical description of the taniavartanam in Carnatic concerts. However, analysis is complicated owing to a plethora of tala and nade. The segmentation of a taniavartanam section can be used for further analysis, such as stroke sequence recognition, and help find relations between different learning schools. The study uses 12 hours of taniavartanam segments consisting of four tala-s and five nade-s for analysis and achieves 0.85 F1-score in the segmentation task.",
        "zenodo_id": 10265221,
        "dblp_key": "conf/ismir/RS0M23",
        "keywords": [
            "intricate",
            "elaborate",
            "extempore",
            "rhythmic",
            "evolution",
            "homogeneous",
            "sections",
            "shared",
            "rhythmic",
            "characteristics"
        ],
        "content": "SEGMENTATION AND ANALYSIS OF TANI ¯AV ARTANAM IN CARNATIC\nMUSIC CONCERTS\nGowriprasad R1Srikrishnan S2R Aravind1Hema A Murthy1\n1Indian Institute of Technology Madras,2Carnatic Percussionist,1,2Chennai, India\nee19d702@smail.iitm.ac.in, aravind@ee.iitm.ac.in, hema@cse.iitm.ac.in\n2srikrishnansridharan@gmail.com\nABSTRACT\nIn Carnatic music concerts, tani ¯avartanam is a solo per-\ncussion segment that showcases intricate and elaborate ex-\ntempore rhythmic evolution through a series of homoge-\nneous sections with shared rhythmic characteristics. While\ntani¯avartanam segments have been segmented from con-\ncerts earlier, no effort has been made to analyze these per-\ncussion segments. This paper attempts to further segment\nthe tani ¯avartanam portion into musically meaningful seg-\nments. A tani ¯avartanam segment consists of an abhipr ¯aya,\nwhere artists show their prowess at extempore enunciation\nof percussion stroke segments, followed by an optional\nkorapu, where each artist challenges the other, and con-\ncluding with mohra and korvai, each with its own nuances.\nThis work helps obtain a comprehensive musical descrip-\ntion of the tani ¯avartanam in Carnatic concerts. However,\nanalysis is complicated owing to a plethora of t ¯ala and\nnad.e. The segmentation of a tani ¯avartanam section can be\nused for further analysis, such as stroke sequence recog-\nnition, and help ﬁnd relations between different learning\nschools. The study uses 12 hours of tani ¯avartanam seg-\nments consisting of four t ¯ala-s and ﬁve nad .e-s for analysis\nand achieves 0.85F1-score in the segmentation task.\n1. INTRODUCTION\nCarnatic music (CM) is a South Indian music tradition con-\nsidered an ancient form of Indian art music (IAM). A typ-\nical CM concert features a lead artist, typically a vocal-\nist, accompanied by a violinist and percussion instrument\nartists. The lead percussion instrument in this ensemble is\nusually the mridangam , while additional percussion instru-\nments like the ghatam ,khanjira , and morsing may also be\npresent. A CM concert includes a solo percussion perfor-\nmance known as tani¯avartanam , ortanifor short. Tani is\na structured sequence of rhythmic elaborations performed\nat a ﬁxed metric tempo and bound to a metric cycle ( t¯ala).\nThis study attempts to study the elaborations in tani, seg-\nment them using a culture-speciﬁc approach, and assigns\nsemantically meaningful labels.\n© Gowriprasad R, Srikrishnan Sridharan, R Aravind and\nHema A Murthy. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: Gowriprasad R, Srikr-\nishnan Sridharan, R Aravind and Hema A Murthy, “Segmentation and\nAnalysis of Tani ¯avartanam in Carnatic Music Concerts”, in Proc. of the\n24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.Audio recordings of concert performances available on-\nline often lack detailed metadata and annotations regard-\ning section boundaries and other information, particularly\nin the context of IAM. With the increasing availability of\nmusic collections and digital devices, there is growing in-\nterest in accessing music based on its characteristics. The\npaucity of editorial metadata has necessitated the develop-\nment of music information retrieval (MIR) techniques to\nextract music’s characteristic properties from audio record-\nings automatically. The paper is organized as follows. The\ntani¯avartanam structure is described, followed by the task\nobjectives, challenges, and dataset description. Domain-\nspeciﬁc feature engineering is done, and the task is ad-\ndressed for different cases. The experimental results are\nanalyzed and discussed with culture-speciﬁc explanations.\n1.1 Tani ¯avartanam Description\nThe tani is a highly structured and elaborate percussion\nperformance that is a prominent feature of CM, showcas-\ning the rhythmic skills and creativity of the percussionist.\nThe main percussion instrument is the mridangam, occa-\nsionally accompanied by ghatam (clay pot), khanjira, and\nmorsing (Jew’s Harp). Since tani is part of a main item,\nit is performed in the same t ¯ala, and metrical tempo as the\nmain item. The intricacies are based on the precise mathe-\nmatical calculations of the metric cycle.\nThe duration of the tani is divided among the mridan-\ngam and accompanying percussion to showcase individ-\nual artistry, e.g., if mridangam and ghatam are present, the\nstructural framework of the tani is typically as follows:\nThe mridangam always starts ﬁrst by playing sarvalaghu\n(SV) patterns (indicators of basic t ¯ala structure), and the\ncomplex patterns are introduced gradually. These elabora-\ntions are performed in a particular rhythm structure called\nnad.e(usually in chatura ´sraat ﬁrst) for a few rhythm cy-\ncles. These elaborations on a particular rhythmic theme\nare termed as abhipr ¯aya. The literal meaning is \"opinion\",\ni.e, the artists’ viewpoint of that particular rhythm struc-\nture. Ghatam follows and tries to keep the same theme\nbuilt by the mridangam in the ﬁrst cycle by playing in the\nsame nad .e [1]. In the second cycle, the mridangist usually\nmay change the nad .e (to ti´sra, for example) and elaborates.\nThe ghatam usually follows in the same nad .e or switches\nto a different nad .e (khand .a). These may or may not con-\ntinue for more than two cycles, usually owing to time con-\nstraints. Each abhipr ¯aya ends with a pattern called korvai ,\nwhich is repeated thrice to arrive at downbeat.56Figure 1 . Spectral Illustration of a few Carnatic Percussion\nStrokes. D: Damped Strokes, R: Resonant Strokes\nThese abhipr ¯aya-s are followed by the korapu , usu-\nally seen as a question-answer between mridangam and\nghatam. Here it starts with multiple cycles of rhythmic pat-\nterns by the mridangam followed by ghatam, where each\nartist challenges the other. The duration of the rhythm pat-\nterns in korapu keeps reducing progressively from full cy-\ncle, half cycle, quarter cycle until it ﬁnally reduces to a\nsingle beat. It can be translated as “rhythmic descent” or\n“step-by-step reduction”. The artist(s) then start playing\ntogether playing faster with crisp strokes ( farans ), build-\ning up the necessary momentum for playing the last parts\nof the tani called mohra and longer korvai [1]. Each of\nthese has a speciﬁc composition structure upon which the\nartist builds. This structure holds even if only the mridan-\ngam is present, except that the korapu part might be absent.\nSummarizing the sequence of sections in a tani segment\ncan be listed as sarvalaghu patterns abhipr ¯aya in a speciﬁc\nnad.e→change of nad .e→back to starting nad .e→korapu\n→farans →mohra →ﬁnal korvai [2].\n1.1.1 Aspects of timbre and spectral differences among\nthe Carnatic percussion\nIn Indian music tradition, accompanying instruments are\nrelatively tuned according to the main melodic instrument\nor voice. The percussion instruments are also catego-\nrized on the sonic aspect. Figure 1 illustrates the damped\n(D) strokes and resonant (R) strokes of Carnatic percus-\nsion instruments. Two-sided percussion, mridangam has\nboth the low-frequency and mid-frequency spectra cov-\nered. The ghatam occupies a little over the mid-frequency\nband, and morsing predominantly spreads over the high-\nmid frequency spectrum and has a larger resonance. Khan-\njira occupies a low-frequency spectrum a bit less than the\nleft side of the mridangam. This explains the aesthetic\nquality of the percussion instruments that have been tra-\nditionally in use for CM concerts. The tonal nature also\nenhances the entire concert when played harmoniously.\n1.2 Problem Objective and Challenges\nThis work addresses three primary tasks: (1) Diariza-\ntion of the audio into mridangam, khanjira, and ghat .am\nsections when multiple instruments are present, (2) Es-\ntimation of section boundaries using musical attributes,\nand (3) Classiﬁcation of segments into broad categories\nsuch as abhipr ¯aya, korapu, farans, mohra, and korvai. To\nachieve these goals, the paper applies techniques from\nwell-researched music genres while also considering the\nculture-speciﬁc characteristics of tani. To improve read-\nability and clarity, several terms are deﬁned in Table 1.\nIdentifying and understanding the segments in tani is\ndifﬁcult for most CM audiences, except for professionallySegmentAudio fragment between any two adjacent detected boundaries\nthat may or may not cover a complete section.\nSectionA primary portion of the tani ¯avartanam. A section can contain\nmultiple compositions and multiple segments.\nNad.eA modiﬁer to t ¯ala that decides the number of strokes per beat,\nThe subdivision structure within a beat in CM\nChatura ´sra, Ti ´sra, Khand .a are different kinds of nad .e-s\nAbhipraya (AB) A rhythmic elaboration in a particular nad .e during tani.\nKorapu (KP) A musical dialogue between the musicians during performance.\nFarans (FA)The ﬁrst part of the conclusion in tani where the\npercussionists play fast to gain momentum toward the end.\nMohra (MO)Popular rhythmic structure played after the farans hinting\nthe climax of tani ¯avartanam.\nKorvai (KO) Stroke patterns that are played three times, concluding the tani.\nTable 1 . Deﬁnitions of terms relevant to this paper\ntrained and practicing percussionists. However, this chal-\nlenge can be addressed if we have a reliable system that can\nclassify the primary segments in tani from audio record-\nings. Such a system would not only aid in appreciating the\nart form for a broader audience, but also serve as a valuable\nlearning tool for beginner-level percussion students.\nComing to the challenges, tani is very diverse and ex-\ntempore. The number of percussions may vary across the\nconcerts. The duration of the tani also varies, inﬂuencing\nthe number of possible segments. Additionally, the pres-\nence of the korapu section is contingent on the number of\npercussions, which is rare when only mridangam is played.\nEach rhythmic structure is presented at multiple speeds.\nThis is reﬂected in the boundary within a single abhipr ¯aya\ndue to sudden tempo changes. The rendition also has small\npauses, which may be part of the rhythmic elaboration\nor due to the artist’s presentation style. As a result, the\ntani segmentation task presents unique challenges to exist-\ning audio segmentation methods. Listening to the entire\naudio carefully to mark the segment boundaries is time-\nconsuming. This underscores the need to develop systems\nfor automatic segmentation and annotations.\n1.3 Dataset Description\nExperimenting with various shades of tani requires a di-\nverse collection of annotated audio data. As there is no\nproperly annotated dataset available for this task, we col-\nlected diverse recordings of tani and labeled them. All the\naudio data used in this work is a subset of the Charsur Car-\nnatic [3,4], Sangeethapriya [5] datasets along with two au-\ndios from [6]. The tani part from the main concerts is ex-\ntracted by marking the start and end points. Professional\nperformers listened and annotated the boundaries of pri-\nmary sections in the tani. By doing so, we collected around\n12 hours of annotated tani audio. The duration of each tani\nin the dataset ranges from 6 minutes to 29 minutes, with\n11 minutes of mean duration.\nThe dataset details are described in Table 2. The con-\nsidered audios comprises of tani played in four major t ¯ala-s\nof CM [7, 8], namely ¯adi, mi ´sra ch ¯apu, khand .a ch¯apu, and\nrupaka. The annotations consist of t ¯ala labels, boundary\ninstances, and labels of primary sections of tani. The mul-\ntiple percussion audios considered in this work have only\ntwo instruments along with additional labelings of the in-\nstrument name for their respective segments. The dataset is\nheterogeneous with artist variability (22 mridangam, >12\nghatam,>8 khanjira), tonic, and tempo variability.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n57No. of Abhipr ¯aya No. of Concerts Duration ~ (hrs:mins)\nMridangam 51 16 02:24\nMrid + Ghat 86 18 04:56\nMrid + Khanj 94 21 05:47\nTotal 231 55 12:08\nTable 2 . Dataset Description.\n1.4 Related Work\nSegmentation and metadata labeling of a music recording\nhave a fairly good research history both in Western [9–11]\nand IAM traditions [12, 13]. Various acoustic and tempo-\nral parameters were used for the segmentation task [9, 14].\nFoote et al. [15] proposed a self-distance matrix method to\ndetermine the boundary between contrasting musical char-\nacteristics. The changes in musical features in Pop and\nRock music were used to train the boosted decision stump\n[16]. Lately, [17] explored neural networks for structural\nsegmentation, spanning various genres [18].\nIn the context of IAM, different approaches were ex-\nplored for segmenting the main concert audios in the\nDhrupad [13, 19, 20], Hindustani [21, 22], and Carnatic\n[12, 23–25] music traditions. For instrumental concerts,\nVinutha et al. [22] considered the segmentation of sitar and\nsarod concerts using reliable tempo detection [26]. The\nanalysis of rhythm/percussion in IAM has primarily fo-\ncused on stroke onset detection [27, 28], stroke recogni-\ntion [6, 29–33], and sequence modeling [34, 35] percus-\nsion pattern identiﬁcation [36]. Ajay Srinivasamurthy [37]\nworked on tracking the \"downbeat,\" provided the t ¯ala is\nknown. Tani diarization was also attempted in [4]. Fur-\nther, mridangam artist identiﬁcation from tani audio was\nattempted [38]. Parallel to [38], tabla ghar ¯an¯a recognition\nfrom the tabla solo was addressed in [39, 40].\nNevertheless, no attempts have been reported on the\nstructural analysis of Indian solo percussion. This paper\nattempts to include additional meta-information to the tani\nportion of a concert, where the audio is segmented based\non musical attributes. This can help identify the t ¯ala and\nenable the association of the cycle of strokes with that of\nthe lyrics of the main composition in CM. The outcomes\ncan help in the concert summarization task and for further\nMIR studies in the ﬁeld of percussion, which is crucial as\nit can give insights into the rhythm of the main item of the\nconcert. Combined with works on meter tracking [7], per-\ncussion source separation [41], and stroke recognition [6],\nthis could lead to additional metadata that could be impor-\ntant to an ardent listener or performer.\n2. AUDIO FEATURE ENGINEERING\nThe raw concert audios have to be pre-processed for further\nanalysis. Since each concert is unique in the choice of met-\nric tempo, tonic, and compositional structure, the features\nused should be based on concert-speciﬁc characteristics.\nAt the same time, it should scale inter-concert. We address\nthe tasks by computing relevant features considering the\nculture-speciﬁc musicological perspectives. Initially, the\nraw audio is pre-processed by computing the Hilbert enve-\nlope of the linear prediction (LP) residual on the raw audio\n[27]. Then the onset detection function (ODF) is computed\nFigure 2 . Flow Diagram for Segmentation and Labeling\nusing the spectral ﬂux method [42]. It is shown to perform\non par with state-of-the-art machine learning-based onset\ndetection algorithms on percussion instruments [27]. The\ncomputed onset locations are considered for further rhythm\nanalysis. While we have used LP analysis, any onset de-\ntection technique could have been used.\n2.1 Rhythm and Tempo Features\nThe change in the rhythm structure or the tempo is a promi-\nnent indicator of the section transitions. In the case of per-\ncussion instruments, rhythm pattern refers to the aspects of\nstroke patterns. A rhythm representation can be obtained\nby considering the stroke ODF (sampled at 10 ms) over a\nsuitably long window and computing the auto-correlation\nfunction (ACF). The periodicity analysis using the ACF of\nthe ODF represents the audio in terms of rhythm called\nrhythmogram [43–45], where rhythm/tempo alone is em-\nphasized.\nThe ACF of the ODF is computed frame-wise with a\nframe length of 4 seconds and a frameshift of 0.5 seconds\nup to a lag of 1 second. The dimension of each frame of the\nrhythmogram is p= 100 , corresponding to a 1-second lag.\nThe window length must be large enough to contain sufﬁ-\ncient strokes for computing the ACF. Even while playing a\nslower tempo, we observe at least more than 8-10 strokes\n(sufﬁcient to calculate the periodicity) in a window length\nof 4-5 seconds. A uniform window size of 4s is chosen to\naccommodate variability in rhythm. The peaks along the\nlag axis of the rhythmogram depict the periodicity of the\nsurface rhythm, indicating surface tempo [22].\nThe tempo estimation using the product of ACF-DFT\n[46] is often prone to tempo octave errors due to uneven\nstroke distribution. We compute the number of strokes in\neach 4 seconds frame and divide by 4 to get the stroke den-\nsity at every frame instance. The feature is named aver-\nage stroke density (ASD), as the averaging is done over 4\nseconds frame. The ASD is robust to tempo octave errors\nand is representative of surface tempo [13]. The mean and\nstd. deviation of strokes per second, as obtained in the en-\ntire dataset, are 8.6and3.8, respectively. The variance of\nASD depicts the tempo diversity in the dataset. Figure 4(c)\nshows the evolution of ASD over time.\n2.2 Spectral Feature\nFrom Section 1.1.1, it is clear that each of the Carnatic per-\ncussion instruments has distinct spectral properties, and the\nspectral features can serve as potential features for instru-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n58Figure 3 . Self-Similarity Matrices on Different Features\nment classiﬁcation. In this work, we need to localize the\nsegments as coming from one of the percussion. To get the\ncomplete spectral aspects of a particular instrument, the\nspectrum must be computed over a window with almost all\nkinds of strokes. Thus we computed a narrow band spec-\ntrogram (NBS) with a window size of 4sand a hop size\nof0.5s. From Section 2.1, we know that the mean ASD\nis eight strokes per second. Thus in a four-second frame,\nwe can expect at least one resonant stroke. We can clearly\ndistinguish mridangam and ghatam segments from NBS in\nFigure 4(a).\n2.3 Spectral and Rhythm Posteriors\nThe high-dimensional NBS and ACF rhythmogram repre-\nsent the spectral and rhythmic-tempo homogeneity within\nthe segment and the changes between the adjacent seg-\nments. This allows us to use Gaussian mixture models\n(GMM) to model the instrument’s spectral and temporal\nhomogeneity. The NBS and ACF vectors are converted to\nspectral and rhythm posteriors (NBS-P, ACF-P), represent-\ning class conditional probabilities.\nWe use two mixtures GMM to represent NBS feature\nvectors with the intuition that each instrument property is\nmodeled by one mixture. Interestingly we ﬁnd that each\nmixture corresponds to a different timbre. We also tried a\nthird mixture to represent the portion where both the in-\nstruments play together (FA, MO, KO). This failed due\nto the volume dominance of mridangam and gave false\nalarms. The posterior feature computed on NBS is de-\npicted in Figure 4(d). The posteriors from the rhythmo-\ngram are computed with ﬁve mixture components, each\nrepresenting a particular speed. The GMM is ﬁt only on\nthe NBS and ACF vectors from a particular concert. The\nnumber of Gaussians is determined by the different speeds\nand nad .e-s expected in a concert.\n3. TANI SEGMENTATION AND LABELING\nSince tani may contain only mridangam or multiple in-\nstruments, we ﬁrst need to detect if a particular tani au-\ndio has multiple instrument or not. The abhipr ¯aya region\nsegmentation task is slightly different in both cases. Lo-\ncating the abhipr ¯aya boundaries is based on detecting a\nchange in the instrument itself (in case of multiple instru-\nment) and the local rhythmic structure of segments at the\nhighest timescale (in case of solo mridangam). Figure 2\nshows the overall steps involved in the task. Each of the\nsegmentation and labeling steps is described here.\n3.1 Multiple Instrument Detection\nFrom Sections 1.1.1, we know that different Carnatic per-\ncussion instruments differ in their sonic and timbral aspects\nFigure 4 . Eg: Multiple Instrument Tani: Segment labels\non top (a) NBS feature (b) ACF Rhythmogram with NF-\nACF+P overlay-ed (c) ASD evolution over time (d) Poste-\nriors computed on NBS (e) NF-NBS-P (red) obtained from\n(15s×15s)kernel, NF-NBS (blue) from (3s×3s)kernel\n(f) NF-NBS-P replaced with NF-ACF+P in last 2.5min in-\ndicating FA, MO, KO boundaries, and ground truth\nand occupy different frequency bins in the spectrum. We\nuse the NBS extracted in Section 2.2 from all the available\naudios. We built a Gaussian Mixture Model (GMM) on\nNBS with ﬁve mixtures, one for each class – mridangam,\nghatam, khanjira, mrid-ghat, mrid-khan. If the ratio of the\nnumber of frames from any two classes to the total num-\nber of frames in a concert is greater than 20%, then that\nconcert is classiﬁed as having multiple instruments. Oth-\nerwise, we verify if most frames are from mridangam (at\nleast 80%) and classify it as single instrument mridangam.\nWe performed GMM classiﬁcation on MFCC features as\nwell. Both methods gave 100% classiﬁcation accuracy in\ndetecting multiple percussion instruments in a recording.\n3.2 Novelty Function Computation\nThe aim is to get an NF whose peaks indicate the desired\nsegment boundaries. Given the ACF, ACF-P, NBS, and\nNBS-P feature vectors, the Self-Similarity Matrices (SSM)\nare computed on each of them using L2distance mea-\nsure [10]. The SSM obtained on the ACF, ACF-P, NBS,\nand NBS-P are displayed in Figure 3. The homogeneous\nsegments of length Lframes possibly appear as (L×L)\nblocks. The section change points with high contrast in\nSSM are captured by convolving a checker-board kernel\nalong the diagonal of SSM [15]. The 1D output obtained\nis called a novelty function (NF). The peaks of the NF in-\ndicate the segment boundary instances having high con-\ntrast in SSM. The obtained NFs are (1) the average of NF-\nACF, NF-ACF-P (Figure 4(b), Figure 5(a)), (2) NF-NBS,\nand NF-NBS-P (Figure 4(e).\nNFs are computed by convolving (15s×15s)ker-\nnel with SSM of different features. Peak picking is per-\nformed by maintaining a minimum distance between ad-\njacent peaks as 5s. We experimented with smaller ker-\nnel sizes such as (3s×3s), and(5s×5s), resulting in\nnoisy NFs. This decreased the precision due to a lot of\nfalse positives. Though much larger kernel sizes, such as\n(50s×50s), made the NFs smoother, they compromised inProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n59Figure 5 . Eg: Solo Mridangam Tani, (a) ACF Rhyth-\nmogram with NF-ACF+P overlay-ed along with detected\npeaks (b) First Dominant peak along the Lag axis FDL\n(yellow), and its 1stdiff. FDL-1D highlighting the dis-\ncontinuities (c) FDL computed on the Gaussian smoothed\nACF (yellow) and its 1stdiff. FDL-1D (white) (d) NF-\nFDL-ACF (black) is a point-wise product of NF-ACF+P\n(red) and FDL-1D (blue) (e) NF-FDL-ACF replaced with\nNF-ACF+P in last 2.5min indicating FA, MO, KO bound-\naries along with ground truth, and the segment labels below\nresolving the closer boundaries. All the features and NFs\nin this work are computed at the resolution of 0.5seconds.\n3.3 Case1: Multiple Instrument Tani\nIn the case of multiple instrument tani, each round of indi-\nvidual percussion elaboration is considered one abhipr ¯aya\n(one thematic development). Thus instrument change\npoint detection is necessary and sufﬁcient for getting the\nabhipr ¯aya boundaries. Since the instrument change points\nare visually evident from the NBS, we used NF-NBS and\nNF-NBS-P to get the boundaries. A NF obtained from a\nsmaller kernel enhances the rapid instrument change in the\nKP section, useful in localizing the KP section but creat-\ning false positives during segmentation. The ﬁrst portion\nof the KP section is fairly large. A larger kernel empha-\nsizes only the start instance of KP by suppressing the rapid\ninstrument change. Thus we used NF obtained from a\nlarger(15s×15s)kernel for the segmentation task and the\nsmaller(3s×3s)kernel NF for localizing the KP section.\nThe FA, MO, and KO are always played toward the end\nof the tani and the FA has a higher ASD. As we see in the\nFigure 4(e), NF-NBS and NF-NBS-P do not capture these\nchange points. Thus we replace the last two and a half min-\nutes of NF-NBS-P with the average of NF-ACF and NF-\nACF-P (NF-ACF+P). This gives the ﬁnal NF (’red’ curve\nin Figure 4(g)) in the case of multiple instrument tani. We\nempirically choose the last 2.5mins as the FA, MO, KO\nare always found in the last 2.5mins in the entire dataset.\n3.4 Case2: Solo Mridangam Tani\nComputing the AB boundaries on solo mridangam tani is\na tough task, as the AB change needs to be detected based\non the rhythm (nad .e) change. Nad .e change detection ispivotal in getting the AB boundaries, especially in the case\nof solo mridangam tani. Relying only on the raw rhyth-\nmogram features (NF-ACF+P) creates false alarms due to\nmultiple tempo changes and irregularities within a single\nAB segment. This necessitates the computation of a ro-\nbust function to tempo octave changes but also captures the\nnon-octave tempo changes that indicate the nad .e changes.\nWe initially set to track the ﬁrst peak along the lag axis\nof the rhythmogram over time, and the change in the peak\nlag apart from doubling and halving is expected to indi-\ncate the nad .e change. But this is also found to be noisy\n(’yellow’ curve in Figure 5(b)). Thus, we perform hori-\nzontal Gaussian smoothing on the rhythmogram to mask\nthe irregularities, then pick the ﬁrst dominant lag peak\n(FDL). This fetched a smoother curve (’yellow’ curve in\nFigure 5(c)) having discontinuities around the nad .e change\nwith less tempo octave errors. The peaks on the ﬁrst\ndifference of this curve (FDL-1D) gave fairly good nad .e\nchange estimates, along with a few false positives. We\ncan observe that the peaks of both NF-ACF+P and FDL-\n1D (Figure 5(d)-E1) coincide around the nad .e change in-\nstances but not elsewhere. Thus we perform \"AND\" oper-\nation by multiplying NF-ACF+P and FDL-1D to get a NF\nwhich is an indicator of nad .e change. We can observe that\nthe false positives are considerably reduced. Again we can\nsee that towards the last FA-MO-KO portion, this NF is\nnot indicating FA-MO-KO boundaries. Thus, we replace\nthe last two and a half minutes of NF-FDL-ACF with NF-\nACF+P, similar to Case1. This gives the ﬁnal NF in the\ncase of solo mridangam tani (’black’ curve in Figure 5(e)).\n3.5 Section Classiﬁcation and Labeling\nGiven the hypothesized segment boundaries, the task is to\nclassify each segment with appropriate labels. Each sec-\ntion, AB, KP, FA, MO, and KO, has unique structural, po-\nsitional, and duration characteristics common across the\nconcerts. We use the characteristic musical cues to classify\nand label the segments. For the multiple instrument tani, a\nNF obtained from a smaller kernel (3s×3s)gives multiple\npeaks in the KP portion. The hypothesized segment hav-\ning multiple peaks is labeled as KP [Figure 4(e)(E-2)]. The\nsegments before the KP are classiﬁed broadly as AB. We\ncompute the mean of ASD in each segment. As the ASD is\nhigh during FA-MO, the segment after KP having the high-\nest mean-ASD is labeled FA [Figure 4(c)(E-1)], followed\nby KO at last. Labeling of FA, MO, and KO is the same for\nsolo mridangam concerts as well. Korapu is not present if\nonly mridangam is present. All the segments before FA are\nbroadly labeled as AB for solo mridangam concerts. Thus\nthe algorithm with a set of rules based on the structure of\ntani and the domain knowledge performs classiﬁcation and\nlabeling. Implementation, annotations, and dataset details\nare shared for research purposes1.\n4. ANALYSIS OF RESULTS AND DISCUSSION\nThe tani structural segmentation task is approached as a\nboundary detection task, where the presence or absence of\n1https://bit.ly/3XIJfMaProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n60Case Section Precision Recall F1-Score\nMultiple\nPercussionAB 0.92 0.99 0.96\nKP-FA-MO-KO 0.82 0.89 0.86\nOverall 0.87 0.94 0.91±0.03\nSingle\nPercussionAB 0.7 0.82 0.74\nFA-MO-KO 0.82 0.86 0.83\nOverall 0.75 0.84 0.79±0.05\nTable 3 . Segmentation Results\na boundary is examined in uniformly spaced feature frames\nof 0.5 seconds. Unlike stroke onset detection, the task is\naddressed at a larger time scale and thus has a tolerance\nduration in \"seconds\" rather than milliseconds [27, 47]. A\ntrue-positive detection is one where the prediction bound-\nary falls within ±3seconds of the ground truth boundary,\nwhile a false-positive detection is one where it does not.\nPrecision, recall, and F1-scores are used for evaluation.\nEvaluation is performed on the entire dataset, as the pro-\nposed method is unsupervised, and no model training is\ndone.\nThe segmentation evaluation scores for each case and\nindividual sections are tabulated in Table 3. The recall is\ngood in all cases, indicating that the system successfully\ndetects the desired boundaries considerably. We can ob-\nserve that the precision is consistently less than recall, in-\ndicating false positives. The change in local rhythm struc-\nture, which may be both gradual and abrupt, causes peaks\nin the NFs. The gradual change in rhythm structure can be\nseen often in the AB section as it is extempore.\nIn Case 1, the AB boundaries are identical to the in-\nstrument switching instances, and the NF-NBS/NF-NBS-\nP captured it well with a 0.96F1-score. The KP-FA-MO-\nKO section performance is slightly lower, as the rapid in-\nstrument switching caused false positives. The end of the\nKP section is not always evident as the cycle duration re-\nduces to one beat. A small SV pattern may also exist after\nKP while moving towards FA, making boundary detection\nchallenging. Since MO is played along with or immedi-\nately follows the FA, the FA-MO boundary is often missed,\nreducing recall.\nIn Case 2, the AB boundaries are not straightforward.\nThe local variations, tempo doubling and halving cause\nfalse positives when the NF-ACF and NF-ACF-P are used.\nThese local variations also cause the ﬁrst dominant lag\non the ACF to be noisy. The horizontal averaging of the\nrhythmogram aided in noise-free ﬁrst dominant lag tracing\nand considerably reduced false positives, but still, the false\nalarms persisted. The nad .e changes are also very gradual\nin many cases, which are not evident with tempo-related\nACF analysis. For example, while transiting from 6 to 5\nstrokes per beat, the change is hardly noticeable when the\nmetric tempo is fast. A few of the AB boundaries are also\nmissed during smoothing. The performance on the FA-\nMO-KO is similar to Case:1, as the NF-ACF+P is used in\nthe last2.5mins for both cases. Case 2 has more variance\nin F1-Score than Case 1. The average F1-score for both\ncases combined is 0.83.\nWe also experimented with ±5s and±1s tolerance win-\ndows. The overall recall increased by 0.2with a marginal\nincrement in precision for the ±5s case. The ±1s case re-ported a drop of precision and recall by 0.4and0.3, respec-\ntively. This is evident as 1s corresponds to only two feature\nframes in this work, and many boundaries are missed.\nSection classiﬁcation performance is evaluated by con-\nsidering the ground truth markings. We quantify the per-\nformance of calculating the ratio of correctly classiﬁed\nframes to the total number of frames in a tani. The\nweighted average of correctly classiﬁed frames in the en-\ntire dataset considering the lengths of each tani is 92%.\nThat is, given 10m of segmented tani, around 9m-15s of\nthe frames are correctly labeled as AB, KP, FA, MO, KO.\n5. CONCLUSIONS\nThis work has addressed an unexplored problem, structural\nsegmentation, and labeling of tani audios. We motivate the\nproblem and present different facets and challenges in the\ntask. From the experiments performed, it is clear that in-\ndividual features alone are inadequate for segmentation. A\nculture-speciﬁc approach is clearly required, both in fea-\nture choice and modeling. Timbre is used when it is re-\nquired to detect if multiple instruments are present in the\ntani, and MFCC features were found to be adequate. On\nthe other hand, detecting AB sections required analysis of\nboth timbre and rhythmogram to detect boundaries. Iden-\ntifying AB sections when two percussion instruments are\npresent is quite easy. In contrast, determining AB sec-\ntions in a solo percussion instrument is difﬁcult as nad .e\nchanges/speed changes are difﬁcult to determine. The hope\nis that such a task will aid in including additional meta-data\nw.r.t a concert.\nThe major contributions of this work are as follows: (i)\ncurating a diverse dataset of tani recordings of around 12\nhours having section boundary information along with pri-\nmary section labels, (ii) evaluating the existing MIR tech-\nniques with culture-speciﬁc adaptation for a musicologi-\ncally important task, segmentation and labeling of tani,\n(iii) formulating average stroke density (ASD) feature (a\nrepresentative of surface tempo), which is robust to tempo\noctave errors, (iv) formulating the class-conditional prob-\nability features from the rhythmogram, and spectral fea-\ntures, and (v) exploring the combination of different NFs\nobtained from different features to achieve the task. Fi-\nnally, this work provides an example of adapting available\nMIR methods to genre-speciﬁc problems by performing\nappropriate feature engineering.\n6. ACKNOWLEDGMENTS\nThe authors are grateful to the percussion maestros V Sel-\nvaganesh, Patri Satish Kumar and Giridhar Udupa for their\nsupport and help. We are thankful to Ajay Srinivasamurthy\nfor his support and timely guidance. We thank Jom Kuri-\nakose for sharing the dataset audios.\n7. REFERENCES\n[1] U. Giridhar. (2020) Description of tani avartanam.\n[Online]. Available: https://www.ghatamudupa.com/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n61[2] E. N. Sunil, Resounding Mridangam: The Majestic\nSouth Indian Drum . Erickavu N Sunil, March\n2021. [Online]. Available: https://www.youtube.com/\nc/erickavunsunil\n[3] Charsur digital workstation. [Online].\nAvailable: https://musicbrainz.org/label/\n3e188240-9eb5-4842-b7b9-d6c2393211b7\n[4] N. Dawalatabad, J. Kuriakose, C. C. Sekhar, and H. A.\nMurthy, “Information bottleneck based percussion in-\nstrument diarization system for taniavartanam seg-\nments of carnatic music concerts.” in INTERSPEECH ,\n2018.\n[5] Sangeethapriya – indian ﬁne arts. [Online]. Available:\nhttps://www.sangeethapriya.org/\n[6] J. Kuriakose, J. C. Kumar, P. Sarala, H. A. Murthy,\nand U. K. Sivaraman, “Akshara transcription of mru-\ndangam strokes in carnatic music,” in Twenty First Na-\ntional Conference on Communications (NCC) 2015 .\n[7] A. Srinivasamurthy, A. Holzapfel, A. T. Cemgil, and\nX. Serra, “Particle ﬁlters for efﬁcient meter tracking\nwith dynamic bayesian networks,” in Proc. 16th Inter-\nnational Society for Music Information Retrieval (IS-\nMIR), Málaga, Spain. Canada , 2015.\n[8] A. Srinivasamurthy, G. K. Koduri, S. Gulati, V . Ishwar,\nand X. Serra, “Corpora for music information research\nin indian art music,” in Proc. International Com-\nputer Music Conference, ICMC/SMC; Athens, Greece. ,\n2014.\n[9] R. B. Dannenberg and M. Goto, “Music structure anal-\nysis from acoustic signals,” in Handbook of signal pro-\ncessing in acoustics . Springer, 2008, pp. 305–331.\n[10] J. Paulus, M. Müller, and A. Klapuri, “State of the art\nreport: Audio-based music structure analysis.” in Proc.\n11th International Society for Music Information Re-\ntrieval (ISMIR) . Utrecht, 2010, p. 625–636.\n[11] O. Nieto, “Discovering structure in music: Automatic\napproaches and perceptual evaluations,” Ph.D. disser-\ntation, New York University, 2015.\n[12] S. Padi and H. A. Murthy, “Segmentation of continuous\naudio recordings of carnatic music concerts into items\nfor archival,” S¯adhan ¯a, vol. 43, no. 10, pp. 1–20, 2018.\n[13] P. Rao, T. P. Vinutha, and M. A. Rohit, “Structural seg-\nmentation of alap in dhrupad vocal concerts,” Transac-\ntions of the International Society for Music Information\nRetrieval , vol. 3, no. 1, 2020.\n[14] P. Grosche, M. Müller, and F. Kurth, “Cyclic tem-\npogram—a mid-level tempo representation for mu-\nsicsignals,” in Proc. International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2010.[15] J. Foote, “Automatic audio segmentation using a mea-\nsure of audio novelty,” in Proc. International Confer-\nence on Multimedia and Expo. (ICME) . IEEE, 2000.\n[16] D. Turnbull, G. R. Lanckriet, E. Pampalk, and M. Goto,\n“A supervised approach for detecting boundaries in\nmusic using difference features and boosting.” in Proc.\n8th International Society for Music Information Re-\ntrieval (ISMIR) , 2007.\n[17] K. Ullrich, J. Schlüter, and T. Grill, “Boundary de-\ntection in music structure analysis using convolutional\nneural networks.” in ISMIR , 2014, pp. 417–422.\n[18] J. B. L. Smith, J. A. Burgoyne, I. Fujinaga,\nD. De Roure, and J. S. Downie, “Design and creation\nof a large-scale database of structural annotations.” in\nProc. 22nd International Society for Music Informa-\ntion Retrieval (ISMIR) , 2011.\n[19] M. A. Rohit and P. Rao, “Structure and automatic seg-\nmentation of dhrupad vocal bandish audio,” Unpub-\nlished technical report , 2020.\n[20] M. A. Rohit, T. P. Vinutha, and P. Rao, “Structural\nsegmentation of dhrupad vocal bandish audio based on\ntempo,” in Proc. International Society for Music Infor-\nmation Retrieval (ISMIR) , 2020.\n[21] P. Verma, T. P. Vinutha, P. Pandit, and P. Rao, “Struc-\ntural segmentation of hindustani concert audio with\nposterior features,” in Proc. International Conference\non Acoustics, Speech and Signal Processing (ICASSP),\nIEEE , 2015.\n[22] T. P. Vinutha, S. Sankagiri, K. K. Ganguli, and P. Rao,\n“Structural segmentation and visualization of sitar and\nsarod concert audio.” in Proc. 17th International Soci-\nety for Music Information Retrieval (ISMIR) , 2016.\n[23] K. S. PV , S. Sankaran, and H. Murthy, “Segmentation\nof carnatic music items using kl2, gmm and cfb energy\nfeature,” in Proc. Twenty Second National Conference\non Communication (NCC) . IEEE, 2016.\n[24] H. Ranjani and T. Sreenivas, “Hierarchical classiﬁ-\ncation of carnatic music forms,” in Proc. 14th Inter-\nnational Society for Music Information Retrieval (IS-\nMIR) , 2013.\n[25] B. Thoshkahna, M. Müller, V . Kulkarni, and N. Jiang,\n“Novel audio features for capturing tempo salience in\nmusic recordings,” in Proc. International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\n2015.\n[26] T. P. Vinutha, S. Sankagiri, and P. Rao, “Reliable\ntempo detection for structural segmentation in sarod\nconcerts,” in Proc. Twenty Second National Conference\non Communication (NCC) . IEEE, 2016.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n62[27] R. Gowriprasad and K. S. R. Murty, “Onset detection\nof tabla strokes using lp analysis,” in Proc. Interna-\ntional Conference on Signal Processing and Commu-\nnications (SPCOM) . IEEE, 2020.\n[28] P. A. M. Kumar, J. Sebastian, and H. A. Murthy,\n“Musical onset detection on carnatic percussion instru-\nments,” in Proc. Twenty First National Conference on\nCommunications (NCC) , 2015.\n[29] O. Gillet and Richard, “Automatic labelling of tabla\nsignals,” in Proc. 4th International Society for Music\nInformation Retrieval (ISMIR) , 2003.\n[30] P. Chordia, “Segmentation and recognition of tabla\nstrokes.” in Proc. 6th International Society for Music\nInformation Retrieval (ISMIR) , 2005.\n[31] K. Samudravijaya, S. Shah, and P. Pandya, “Computer\nrecognition of tabla bols,” Technical report, Tata Insti-\ntute of Fundamental Research, Tech. Rep., 2004.\n[32] M. A. Rohit, A. Bhattacharjee, and P. Rao, “Four-\nway classiﬁcation of tabla strokes with models adapted\nfrom automatic drum transcription,” in Proc. 22nd In-\nternational Society for Music Information Retrieval\n(ISMIR) , 2021.\n[33] A. Anantapadmanabhan, A. Bellur, and H. A. Murthy,\n“Modal analysis and transcription of strokes of the mri-\ndangam using non-negative matrix factorization,” in\nIEEE International Conference on Acoustics, Speech\nand Signal Processing, 2013 .\n[34] P. Chordia, A. Sastry, and S. ¸ Sentürk, “Predictive\ntabla modelling using variable-length markov and hid-\nden markov models,” Journal of New Music Research ,\nvol. 40, no. 2, pp. 105–118, 2011.\n[35] P. Chordia, A. Sastry, T. Mallikarjuna, and A. Albin,\n“Multiple viewpoints modeling of tabla sequences.” in\nProc. 11th International Society for Music Information\nRetrieval (ISMIR), 2010 .\n[36] S. Gupta, A. Srinivasamurthy, M. Kumar, H. A.\nMurthy, and X. Serra, “Discovery of syllabic percus-\nsion patterns in tabla solo recordings,” in Proc. 16th\nInternational Society for Music Information Retrieval\n(ISMIR); 2015 .\n[37] A. Srinivasamurthy, “A data-driven bayesian approach\nto automatic rhythm analysis of indian art music,”\nPh.D. dissertation, Universitat Pompeu Fabra, 2017.\n[38] K. Gogineni, J. Kuriakose, and H. A. Murthy, “Mridan-\ngam artist identiﬁcation from taniavartanam audio,” in\nProc. Twenty Fourth National Conference on Commu-\nnications (NCC) . IEEE, 2018.\n[39] R. Gowriprasad, V . Venkatesh, H. A. Murthy, R. Ar-\navind, and K. S. R. Murty, “Tabla Gharana Recognition\nfrom Audio Music recordings of Tabla Solo perfor-\nmances,” in Proc. 22nd International Society for Music\nInformation Retrieval Conference , 2021.[40] R. Gowriprasad, V . Venkatesh, and S. R. Murty K,\n“Tabla gharana recognition from tabla solo record-\nings,” in Proc. National Conference on Communica-\ntions (NCC) , 2022.\n[41] N. Dawalatabad, J. Sebastian, J. Kuriakose, C. C.\nSekhar, S. Narayanan, and H. A. Murthy, “Front-\nend diarization for percussion separation in taniavar-\ntanam of carnatic music concerts,” arXiv preprint\narXiv:2103.03215 , 2021.\n[42] S. Dixon, “Simple spectrum-based onset detection,”\nMIREX 2006 , p. 62, 2006.\n[43] K. Jensen, “Multiple scale music segmentation using\nrhythm, timbre, and harmony,” EURASIP Journal on\nAdvances in Signal Processing , vol. 2007, pp. 1–11,\n2006.\n[44] P. Grosche and M. Muller, “Extracting predominant lo-\ncal pulse information from music recordings,” IEEE\nTransactions on Audio, Speech, and Language Pro-\ncessing , vol. 19, no. 6, pp. 1688–1701, 2010.\n[45] K. K. Jensen, “Rhythm-based segmentation of popular\nchinese music,” in Proc. 6th International Society for\nMusic Information Retrieval (ISMIR) , 2005.\n[46] G. Peeters, “Template-based estimation of time-\nvarying tempo,” EURASIP Journal on Advances in Sig-\nnal Processing , vol. 2007, pp. 1–14, 2006.\n[47] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury,\nM. Davies, and M. B. Sandler, “A tutorial on onset de-\ntection in music signals,” IEEE Transactions on speech\nand audio processing , vol. 13, no. 5, pp. 1035–1047,\n2005.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n63"
    },
    {
        "title": "Correlation of EEG Responses Reflects Structural Similarity of Choruses in Popular Music.",
        "author": [
            "Neha Rajagopalan",
            "Blair Kaneshiro"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265273",
        "url": "https://doi.org/10.5281/zenodo.10265273",
        "ee": "https://zenodo.org/records/10265273/files/000030.pdf",
        "abstract": "Music structure analysis is a core topic in Music Information Retrieval and could be advanced through the inclusion of new data modalities. In this study we consider neural correlates of music structure processing using popular music - specifically choruses of Bollywood songs - and the {NMED-H} electroencephalographic (EEG) dataset. Motivated by recent findings that listeners' EEG responses correlate when hearing a shared music stimulus, we investigate whether responses correlate not only within single choruses but across pairs of chorus instances as well. We find statistically significant correlations within and across several chorus instances, suggesting that brain responses synchronize across structurally matched music segments even if they are not contextually or acoustically identical. Correlations were only occasionally higher within than across choruses. Our findings advance the state of the art of naturalistic music neuroscience, while also highlighting a novel approach for further studies of music structure analysis and audio understanding more broadly.",
        "zenodo_id": 10265273,
        "dblp_key": "conf/ismir/RajagopalanK23",
        "keywords": [
            "Music structure analysis",
            "Music Information Retrieval",
            "Neural correlates",
            "Choruses of Bollywood songs",
            "NMED-H EEG dataset",
            "Listeners EEG responses",
            "Shared music stimulus",
            "Brain responses synchronize",
            "Structurally matched music segments",
            "Music structure analysis state of the art"
        ],
        "content": "CORRELATION OF EEG RESPONSES REFLECTS STRUCTURAL\nSIMILARITY OF CHORUSES IN POPULAR MUSIC\nNeha Rajagopalan\nStanford University\nneharaj@stanford.eduBlair Kaneshiro\nStanford University\nblairbo@stanford.edu\nABSTRACT\nMusic structure analysis is a core topic in Music Infor-\nmation Retrieval and could be advanced through the in-\nclusion of new data modalities. In this study we consider\nneural correlates of music structure processing using pop-\nular music—speciﬁcally choruses of Bollywood songs—\nand the NMED-H electroencephalographic (EEG) dataset.\nMotivated by recent ﬁndings that listeners’ EEG responses\ncorrelate when hearing a shared music stimulus, we inves-\ntigate whether responses correlate not only within single\nchoruses but across pairs of chorus instances as well. We\nﬁnd statistically signiﬁcant correlations within and across\nseveral chorus instances, suggesting that brain responses\nsynchronize across structurally matched music segments\neven if they are not contextually or acoustically identi-\ncal. Correlations were only occasionally higher within\nthan across choruses. Our ﬁndings advance the state of the\nart of naturalistic music neuroscience, while also highlight-\ning a novel approach for further studies of music structure\nanalysis and audio understanding more broadly.\n1. INTRODUCTION\nMusic structure analysis (MSA)—the task of dividing and\nlabelling songs into perceptually salient segments [1]—is\na core topic of Music Information Retrieval (MIR) and has\nbeen approached through a variety of data types including\naudio representations, lyrics, and perceptual annotations.\nFor example, choruses of popular songs are often easily\nrecognizable by music listeners, and can be detected from\naudio due to both their placement throughout a song and\ntheir intrinsic features [2]. While much progress has been\nmade in this area, there may be new approaches and data\nmodalities that could advance it even further.\nMIR studies have come to involve brain data, partic-\nularly electroencephalography (EEG) [3]. EEG has been\nused to predict stimulus labels, decode musical attributes\nsuch as beat and tempo, and even reconstruct music. EEG\ninter-subject correlation (ISC), which captures neural syn-\nchronization of audience members experiencing a com-\n© N. Rajagopalan and B. Kaneshiro. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: N. Rajagopalan and B. Kaneshiro, “Correlation of EEG Re-\nsponses Reﬂects Structural Similarity of Choruses in Popular Music”, in\nProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.plex, real-world stimulus [4], has also advanced music neu-\nroscience research. We leverage and extend this approach\nto investigate MSA.\nWe focus on responses to four Bollywood songs writ-\nten in the popular form—speciﬁcally their choruses, due\nto their salience and tendency to repeat with a high de-\ngree of similarity. Importantly, while past EEG-ISC stud-\nies have considered responses among listeners experienc-\ning the same stimulus (e.g., one chorus instance), we ask\nfor the ﬁrst time whether neural responses also synchronize\nacross instances of structurally similar content (i.e., pairs\nof choruses). Moreover, by using a dataset containing two\nresponse trials from each participant, we can investigate\ncorrelations both across and within participants. In sum,\nwe address the following research questions:\nRQ 1 Does music structure similarity translate to measur-\nable similarity among responses? In other words, do brain\nresponses synchronize across structurally matched musi-\ncal segments, even when those segments are contextually\nunique (in their placement within the song) and also of-\nten acoustically unique from one another? Here we ex-\npect structural similarity to produce statistically signiﬁcant\nEEG correlations both within and across a song’s choruses.\nRQ 2 Even if responses are similar across chorus in-\nstances, are individual choruses still uniquely experi-\nenced? This question extends RQ1 to investigate whether\nEEG responses are more correlated within, versus across,\nchorus instances. We predict that within-chorus EEG cor-\nrelations will be higher than across-chorus correlations.\nRQ 3 Are a listener’s neural responses more similar to\nthemselves than to responses from other listeners? Under-\nstanding whether reliable measures of music structure sim-\nilarity can be obtained from single listeners can motivate\nthe design of future studies. We expect EEG correlation\nwith one’s own data will be higher than correlation with\nthe data of other listeners, due to individual differences in\nperception and EEG characteristics.\nWe report small but often signiﬁcant correlations\nthat align with previous published research. Moreover,\nwithin-chorus correlations do not systematically outper-\nform across-chorus correlations. While preliminary, our\nﬁndings suggest that this novel application of EEG corre-\nlation may capture structural similarity during music lis-\ntening, which may motivate future MSA studies.2642. RELATED WORK\n2.1 MSA and Chorus Analysis\nMSA involves recognizing and labelling non-overlapping\nmusical segments based on musical similarity [1]. Over\nthe years, MSA has come to involve speciﬁc features, sim-\nilarity representations, and algorithms [5]. One sub-topic\nof MSA is chorus identiﬁcation; here, choruses have often\nbeen identiﬁed based on repetition and contextual cues us-\ning measures of similarity [6] and Markov models [7], as\nwell as chroma features and image processing ﬁlters [8].\nSome systems have also used segment length and position-\ning to identify choruses [6, 8]. Independently of context,\nVan Balen et al. looked at intrinsic content features that\nmight distinguish choruses [2]. Their “Chorusness” vari-\nable, a probability measure of how likely a segment may\nbe labelled as a chorus by an independent annotator, high-\nlights audio features (e.g., higher loudness and roughness)\nthat qualify the particular salience of choruses.\nMSA remains a challenging task due, for example, to\nambiguities around deﬁning similarity as well as subjec-\ntivity and interpretation of annotations [1,9]. In their 2020\noverview article, Nieto et al. called for “richer human la-\nbels in upcoming MSA datasets” [1]; we propose that brain\ndata may ﬁt this call.\n2.2 MIR and EEG\nThe growing use of decoding and signal-based approaches\nand complex, naturalistic (real-world) stimuli in neuro-\nscience has increased that ﬁeld’s relevance to the more ap-\nplied ﬁeld of MIR. Kaneshiro & Dmochowski have sug-\ngested that MIR and neuroscience researchers might aug-\nment their gains through collaboration, highlighting EEG\nas a particularly relevant response type for MIR due to its\nhigh temporal resolution, non-invasiveness, whole-brain\ncoverage, and relative portability and low cost [3].\nEEG studies addressing MIR topics include using clas-\nsiﬁcation to predict which stimulus elicited an EEG re-\nsponse [10, 11] or which stream a listener attended to\nin a polyphonic stimulus [12–15]. Other tasks include\nEEG-based tempo detection/classiﬁcation [16–18], on-\nset detection [19], and music reconstruction [20]. EEG\nhas been mapped to time-varying music or audio fea-\ntures using Canonical Correlation Analysis (CCA) [21] or\ndeep-CCA [22]; by correlating EEG with semantic mu-\nsic vectors [23]; or using MEG—the magnetic analogue\nof EEG—and temporal response functions to decode sur-\nprisal [24]. In recent studies, Ofner and Stober exam-\nined EEG responses at automated segmentation bound-\naries [25], and Sangnark et al. performed music preference\nclassiﬁcation on EEG responses to choruses with and with-\nout lyrics [26]. However, we know of no study to date that\nhas assessed similarity among EEG responses to repeated\nstructural segments.\n2.3 Neural Correlation\nA particularly relevant approach for the current study in-\nvolves the correlation of neural responses to a shared stim-ulus, often termed inter-subject correlation (ISC). Has-\nson et al.’s 2004 seminal functional magnetic resonance\nimaging study showed that real-world stimuli (e.g., movie\nexcerpts) can synchronize neural responses across audi-\nence members, and that the timing and location of syn-\nchronized activity identiﬁes stimulus-evoked brain activ-\nity [27]. This data-driven approach, reducing the need for\ncontrolled stimuli and a priori event markers, facilitated\nthe use of complex stimuli in neuroscience. In 2012, Dmo-\nchowski et al. introduced an EEG implementation which\nﬁrst optimizes the data for ISC [4]. Often referred to as\n“Correlated Components Analysis (CorrCA)” [4] or “Re-\nliable Components Analysis (RCA)” [28], this optimiza-\ntion applies a relative eigenvalue decomposition to com-\npute multiple spatial ﬁlters in which across-trials variance\nrelative to within-trials variance (i.e., ISC) is maximized.\nRecent studies involving music have shown that EEG-\nISC is modulated by listener expertise [29], musical\ntempo [30], temporal stimulus manipulations [30, 31], and\nsalient musical events [31]. Auditory studies have reported\nsmall but signiﬁcant group-mean ISC ( 0.01< r <0.02)\nin RC1, the maximally reliable spatial component. Rep-\netition, explored through repeated listens of full excerpts,\nsometimes but not always results in lower ISC on repeated\nlistens [29, 30]. However, the topic of repeating structural\nelements within a song has not yet been addressed.\n2.4 Music-EEG Datasets\nThe acquisition and preparation of EEG data for analy-\nsis requires specialized expertise and sizeable investments\nin recording apparatus [3]. A key factor supporting MIR-\nEEG research is the growing number of open EEG datasets\nreleased with the intent for re-use by other researchers.\nDatasets vary in stimuli, stimulus manipulations, partic-\nipant samples, listening tasks, additional response types,\nand EEG platforms used. Shorter stimuli are used in\nthe MIIR dataset, comprising perceived and imagined re-\nsponses to 12 excerpts 6.9–16.0 seconds in length [32] and\nthe MAD-EEG dataset involving 78 solo, duet, or trio stim-\nuli, each around six seconds long [14]. Datasets involving\nslightly longer excerpts include the DEAP dataset, with 40\none-minute excerpts from music videos [33]; MUSIN-G,\nwith 12 excerpts, 100–132 seconds in length, from var-\nious genres [34]; and NMED-M, containing ﬁve-minute\nexcerpts of various versions of a minimalist work [31].\nFinally, a few datasets use complete musical works as\nstimuli: NMED-H includes four Bollywood songs [35],\nNMED-T uses 10 EDM-style songs [36], and NMED-E\nincludes a cello concerto movement [37].\n3. METHODS\n3.1 EEG Dataset and Stimuli\nAmong the available datasets, we chose to work with\nNMED-H (Naturalistic Music EEG Dataset—Hindi) [35]\nas it used full-length pop (Bollywood) songs with repeat-\ning choruses as stimuli. Speciﬁcally, we work with the four\n“Intact” songs of the dataset: “Ainvayi Ainvayi”, “DaaruProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n265Figure 1 . Analysis overview. The NMED-H dataset contains EEG responses recorded while 48 participants listened to\nfour full-length Bollywood songs. We used RCA to compute a spatial EEG component in which ISC was maximized, and\nused the stimulus audio and lyrics to identify chorus segmentation boundaries to further epoch the EEG. For each song,\ncorrelations were performed within and across choruses, as well as between choruses and segments epoched at random.\nDesi”, “Haule Haule”, and “Malang”. Each song is around\n4 min 30 sec in length and contains between 3 and 5 cho-\nruses as illustrated in color in Fig. 1. The stimuli were\nassumed to be new to the participants, who did not un-\nderstand the Hindi-dialect song lyrics. We used the pre-\nprocessed, 125-channel EEG data sampled at 125 Hz with\naverage reference; each song contained 24 trials from 12\nunique participants (48 participants total) as each partici-\npant had listened to their assigned song twice.\n3.2 EEG Analyses\nTo analyze the EEG, we followed an established proce-\ndure of spatial ﬁltering followed by correlation calcula-\ntions (Fig. 1). We used a publicly available RCA imple-\nmentation1to compute a single spatial ﬁlter across all four\nsongs. We computed RCA across entire song durations and\nnot just chorus segments, as our permutation testing pro-\ncedure involved segments sampled from throughout each\nsong (see § 3.3). We then analyzed the vectorized form of\nsingle EEG trials from only the maximally reliable com-\nponent RC1, as previous studies have shown that that com-\nponent explains most of the ISC in EEG responses to mu-\nsic [30, 31]. Thus, the response data for each song was a\ntime-by-trial matrix, with 24 trials from 12 participants for\neach song and a variable number of time samples per song.\nTo identify and segment song choruses, we ﬁrst iden-\ntiﬁed structural segment boundaries at the measure level\nusing lyrics.2Next, we used a publicly available beat-\ntracking algorithm [38] to identify audio sample indices\nof the boundaries and converted those time stamps to the\nsampling rate of the EEG to segment the EEG accordingly.\nCorrelations were performed on a per-song basis, in\ntwo broad categories. Within-chorus correlations involved\npairwise correlations among response trials from a sin-\ngle chorus instance, producing a symmetric matrix whose\ndiagonal (being 1) was excluded from further analysis.\nAcross-chorus correlations involved the cross-correlation\nof two matrices, each representing a different chorus in-\nstance. These correlations produced asymmetric matrices,\nsince no response vector was ever correlated with itself.\n1https://github.com/dmochow/rca\n2https://gaana.com/ ,https://www.jiosaavn.com/Each correlation also involved both intra -subject correla-\ntions (IaSC) of non-identical trials from the same partici-\npant and inter -subject correlations (ISC) of trials from dif-\nferent participants. As illustrated in Fig. 2, with 24 trials\nper song comprising two listens from each of 12 partic-\nipants, within-chorus correlations produced for each par-\nticipant one IaSC value (ﬁrst listen and second listen) and\n22 ISC values, excluding the diagonal. Across-chorus cor-\nrelations produced for each participant four IaSC values\n(reﬂecting two distinct chorus instances ×two distinct lis-\ntens) and 88 ISC values. For each calculation, we com-\nputed mean correlations at the participant as well as the\ngroup level.\n3.3 Statistical Analyses\nWe assessed statistical signiﬁcance over distributions of\nper-participant results ( N= 12 ). For RQ1 we used per-\nmutation testing: Each analysis was performed over 1000\npairs of segments of the same length as the true chorus\nsegments, but with one segment epoched from a random\nstart time in the song. The 1000 results served as the\nnull distribution against which we compared the true re-\nsult to compute the p-value. For RQ2 and RQ3 we used\nnonparametric Wilcoxon signed-rank tests to account for\nvariable standard deviations of the sampling distributions\ncaused by the discrepancy in the number of samples in\neach group (i.e., IaSC versus ISC; within- versus across-\nchorus). We performed one-sided tests in accordance with\nour expected results (RQ2 H 1:within > across ; RQ3\nH1:IaSC > ISC ). We corrected for multiple compar-\nisons using False Discovery Rate [39] on a per-song basis\nfor RQ1 and RQ3 and on a per-song, per-condition basis\nfor RQ2. We report statistically signiﬁcant results (‘***’,\n‘**’) and also indicate but do not summarize marginally\nsigniﬁcant results (‘*’) for this ﬁrst exploratory analysis.\n4. RESULTS\n4.1 Individual Correlations\nWe correlated vectors of spatially ﬁltered, single-trial EEG\non a per-song basis, both among responses to single cho-\nruses as well as across pairs of different choruses. The re-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n266Figure 2 . Illustration of IaSC and ISC matrix elements for\nParticipant 1 of 12; each participant heard their assigned\nstimulus twice. Top: Within-chorus correlation produces a\nsymmetric 24 ×24 matrix. The same IaSC correlation ap-\npears twice (purple), along with 22 unique ISC correlations\n(yellow). Bottom : Across-chorus correlation produces an\nasymmetric matrix with four unique IaSC correlations (2\nchoruses×2 listens) and 88 unique ISC correlations.\nsulting correlation matrices could then be partitioned into\ncorrelations from the same participant (IaSC) and differ-\nent participants (ISC). Results are visualized in Fig. 3 and\nprovided numerically in Tab. 1. After multiple compari-\nson correction, 10 of 15 within-chorus IaSC and 2 of 22\nacross-chorus IaSC were statistically signiﬁcant (‘**’ or\n‘***’). For ISC, 14 of 15 within-chorus calculations and\n12 of 22 across-chorus correlations were signiﬁcant. IaSC\ndistributions tended to have larger variance than ISC dis-\ntributions, both at the participant level for single analyses\n(Fig. 3) and across the group means (Tab. 1).\n4.2 Within- versus Across-Section Correlations\nWe assessed whether within-chorus correlations—\ninvolving identical musical content and context—were\nhigher than across-chorus correlations, which are struc-turally similar but not identical. Tab. 2 summarizes\nthe statistical signiﬁcance of each comparison. After\ncorrecting for multiple comparisons, within-chorus IaSC\nwas found to exceed across-chorus IaSC 7 times, while\nwithin-chorus ISC was higher than across-chorus ISC 4\ntimes. Signiﬁcant (and marginally signiﬁcant) results most\noften implicated the ﬁrst chorus of a song.\n4.3 Intra- versus Inter-Subject Correlation\nFor our last analysis, we assessed whether IaSC—being\ncomputed from the same listener’s data—would exceed\nISC. Contrary to our expectations, one-sided Wilcoxon\nsigned-rank tests revealed that after multiple comparison\ncorrection, IaSC did not exceed ISC for any within- or\nacross-chorus correlation.\n5. DISCUSSION\nMSA has leveraged various representations—e.g., audio,\nlyrics, human annotations—to model human perception\nof musical structure. In this study we have answered the\ncall for new forms of human response data to inform this\ntask [1] and explored perception of repeated structure seg-\nments using brain data. Speciﬁcally, we assessed whether\nEEG responses to repeating choruses of four Bollywood\nsongs were signiﬁcantly correlated.\nWe found that EEG responses within and across cho-\nruses of a song were often signiﬁcantly correlated, particu-\nlarly for ISC. While small, these ISCs are on par with those\nreported in previous auditory EEG studies [30,31,40]. Cor-\nrelating across choruses contrasts with past ISC research,\nwhich considered correlation only among responses to a\nsingle stimulus. That precedent may be due to those stud-\nies using predominantly narrative stimuli, such as movies\nor speeches, which generally do not include repeated seg-\nments. But for music, repetition is often integral to\nstructure, from brief melodic motifs to large-scale ele-\nments [41]. The present use of ISC to assess music struc-\nture similarity is also a departure from its previous ap-\nplication to index brain states of attention and “engage-\nment” in relation to attributes of stimuli (e.g., narrative ten-\nsion, temporal coherence) [4, 30, 31] or participants (e.g.,\ntrained versus untrained musicians) [29]. Future research\ncould consider data from spatial components beyond RC1\nand further explore relationships between EEG correlation,\nmusic structure, and repetition to index both content simi-\nlarity and listener engagement with repeated content.\nWe found that within-chorus correlation occasionally\nbut not consistently exceeded across-chorus correlation;\nfuture research is needed to elucidate the role of acous-\ntical or contextual differences across chorus instances in\nthis result. Notably, within-chorus correlation most often\nexceeded across-chorus correlation in a song’s ﬁrst cho-\nrus. Past studies have shown that EEG-ISC often drops\nupon repeated exposures to full stimuli [4, 29, 30], and\nmusic-discovery engagement has been shown to be highest\nfor ﬁrst choruses compared to subsequent instances [42].\nWhile this might lead one to expect higher ISC duringProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n267Chorus 1\nChorus 2\nChorus 3\nChorus 4Chorus 1 Chorus 2 Chorus 3 Chorus 4\nChorus 1\nChorus 2\nChorus 3\nChorus 4Chorus 1 Chorus 2 Chorus 3 Chorus 4 Chorus 5Chorus 5\nChorus 1\nChorus 2\nChorus 3Chorus 1 Chorus 2 Chorus 3\nChorus 1\nChorus 2\nChorus 3Chorus 1 Chorus 2 Chorus 3\nSong 2: Daaru DesiSong 3: Haule Haule Song 1: Ainvayi Ainvayi \nSong 4: Malang\nFigure 3 . EEG correlations within and across choruses of four Bollywood songs. Each plot shows a distribution of intra-\n(IaSC) and inter- (ISC) subject correlation values across the 12 participants assigned to that song. Statistical signiﬁcance\nof each correlation is denoted as p = 0 *** 0.01 ** 0.05 * 0.10 for FDR-corrected p-values.\nthe ﬁrst chorus, current results do not suggest that within-\nchorus correlation drops as a song progresses. However, it\nmay be that listeners have a unique perceptual experience\nof ﬁrst choruses relative to other choruses.\nOur expectation that IaSC would exceed ISC was not\nsupported by the data. The large variance of IaSC rel-\native to ISC, and greater number of signiﬁcant ISC re-\nsults despite lower group means, suggests that ISC ulti-\nmately provided a more stable estimate of neural correla-\ntion. Whether this is due to IaSC comprising fewer cor-\nrelations, or an advantage of correlating across a hetero-\ngeneous sample of listeners, can be further investigated to\ninform future study designs.\nThis study contributes a ﬁrst step toward using EEG\ndata for MSA. While we focused on establishing similar-ity of neural responses among pre-identiﬁed repeating seg-\nments, and not detection of repeated segments or segment\nboundaries, our ﬁndings lay a foundation for multiple av-\nenues of future work. For instance, a multimodal MSA\nframework could incorporate EEG measures of similarity\nalongside music content representations and human anno-\ntations. Other EEG-ISC analysis conﬁgurations may also\nprove useful for MSA: For instance, Dauer et al.’s ﬁnding\nthat ISC computed over short time windows peaked during\nsalient musical events including structural segment bound-\naries [31] is worth exploring further. Returning, too, to es-\ntablished connections between ISC and engagement, using\nISC to identify highly engaging portions of songs could in-\nform audio thumbnailing. Finally, while the present work\nleveraged an existing dataset, future studies could be de-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n268IaSC ISC\nC1 C2 C3 C4 C5 C1 C2 C3 C4 C5Song 1C1 0.069** 0.017***\nC2 0.020* 0.016 0.004 0.016**\nC3 0.021* 0.041** 0.083** 0.008* 0.017*** 0.017***\nC4 -0.006 0.019 0.005 0.028* 0.010** 0.014** 0.013*** 0.026***Song 2C1 0.082*** 0.019***\nC2 0.013 0.046** 0.019*** 0.001\nC3 0.004 -0.003 0.013 0.010 0.007 0.010**\nC4 0.018 0.014 0.044*** 0.053** 0.010 0.006 0.011** 0.020***\nC5 0.015 0.021 0.001 0.010 0.024 0.013* 0.006 0.001 0.016*** 0.023***Song 3C1 0.059** 0.020***\nC2 0.007 0.030** 0.003 0.005**\nC3 -0.003 0.000 0.026** 0.010** 0.008*** 0.007**Song 4C1 0.036** 0.017***\nC2 0.012 0.012 0.015*** 0.017***\nC3 0.017* 0.019* 0.045** 0.013*** 0.012*** 0.014***\nTable 1 . Intra- and inter-subject correlation coefﬁcients within and across choruses of four Bollywood songs. Statistical\nsigniﬁcance of correlations (FDR-corrected p-values) is denoted as p = 0 *** 0.01 ** 0.05 * 0.10.\nIaSC ISC\nC1 C2 C3 C4 C5 C1 C2 C3 C4 C5Song 1C1 - * * * - ** ** *\nC2 ns - ns ns ns - ns ns\nC3 ns ns - ** ns ns - ns\nC4 ns ns ns - * * ns -Song 2C1 - ** ** ** ** - ns ns ns ns\nC2 ns - ns ns ns ns - ns ns ns\nC3 ns ns - ns ns ns ns - ns ns\nC4 ns ns ns - ns ns ns ns - ns\nC5 ns ns ns ns - ns * * ns -Song 3C1 - *** *** - *** **\nC2 ns - * ns - ns\nC3 ns ns - ns ns -Song 4C1 - ns ns - ns ns\nC2 ns - ns ns - ns\nC3 ns ns - ns ns -\nTable 2 . Results of one-sided Wilcoxon signed-rank\ntests assessing whether within-chorus correlation exceeds\nacross-chorus correlation. Statistical signiﬁcance of corre-\nlations (FDR-corrected p-values) is denoted as p = 0 ***\n0.01 ** 0.05 * 0.10; ‘ns’ denotes non-signiﬁcance.\nsigned to address speciﬁc MSA questions with newly col-\nlected EEG data. In all, we do not propose that EEG\nshould or could replace existing data modalities for MSA,\nbut rather highlight potential insights from EEG that may\ncomplement other existing approaches and inputs.\n5.1 Limitations\nWe acknowledge limitations of this work. First, while we\nreport multiple signiﬁcant results, they do not imply gener-\nalizability: The correlations are small, and our ﬁndings—\nwhile promising—are not conclusive across all calcula-\ntions. Next, we chose NMED-H as a ready-to-use EEG\ndataset of responses to popular songs containing repeated\nchoruses. But the small stimulus set of four songs also hin-\nders generalization, and future conﬁrmatory studies shouldutilize a larger song set. We note that the original design of\nNMED-H speciﬁed that participants not be familiar with\nthe songs or the language of their lyrics [35]. This too may\nlimit generalizability, as more familiar or lyrically under-\nstandable songs may result in different EEG correlations.\nAnother main limitation is that while the song choruses\ncrucially elicited the EEG data, they were only treated as\nrepeating segments, and we did not consider nuances of\nplacement or content attributes of individual choruses. Yet\nsuch features are known to impact perceptual and neural\nresponses to choruses [26]. Thus, future research should\nconsider ﬁner-grained characterizations of music segments\ntreated as structurally similar. One concrete next step could\ninvolve cross-modal comparisons of music similarity—for\ninstance, whether similarity measures derived from audio,\nlyrics, or human annotations predict neural similarity.\nLastly, we trained RCA once over all available tri-\nals. Future work should incorporate cross-validation—\niteratively optimizing the RCA spatial ﬁlter on training\ndata and then applying it to holdout test trials—into the\nanalysis pipeline to avoid overﬁtting.\n6. CONCLUSION\nMSA is an MIR topic with rich applications in audio\nthumbnailing, motif-ﬁnding, music summarization, music\nrecommendation, and automatic music generation. Aim-\ning to expand the scope of data modalities that may in-\nform this task, we have contributed a ﬁrst look at structural\nrepetition using brain data. We used a publicly available\nEEG dataset and analyzed single-trial responses to cho-\nruses from four Bollywood pop songs by computing intra-\nand inter-subject correlations within and across choruses.\nWe ﬁnd that neural responses do often synchronize to a\nsigniﬁcant extent, which suggests that similarity among re-\npeated choruses may translate to neural similarity. These\nﬁndings motivate future studies of music similarity percep-\ntion and highlight EEG data as a promising input to multi-\nmodal MSA systems.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2697. ACKNOWLEDGMENTS\nThe authors thank Jacek Dmochowski for helpful advice\non the statistical analyses.\n8. REFERENCES\n[1] O. Nieto, G. J. Mysore, C.-i. Wang, J. B. Smith,\nJ. Schlüter, T. Grill, and B. McFee, “Audio-based mu-\nsic structure analysis: Current trends, open challenges,\nand applications,” Transactions of the International So-\nciety for Music Information Retrieval , vol. 3, no. 1,\n2020.\n[2] J. Van Balen, J. A. Burgoyne, F. Wiering, R. C.\nVeltkamp et al. , “An analysis of chorus features in pop-\nular song,” in Proceedings of the 14th Society of Music\nInformation Retrieval Conference , 2013.\n[3] B. Kaneshiro and J. P. Dmochowski, “Neuroimaging\nmethods for music information retrieval: Current ﬁnd-\nings and future prospects,” in Proceedings of the 16th\nInternational Society for Music Information Retrieval\nConference , 2015, pp. 538–544.\n[4] J. P. Dmochowski, P. Sajda, J. Dias, and L. Parra,\n“Correlated components of ongoing EEG point to emo-\ntionally laden attention—a possible marker of engage-\nment?” Frontiers in Human Neuroscience , vol. 6,\n2012.\n[5] R. B. Dannenberg and M. Goto, Music Structure\nAnalysis from Acoustic Signals . Springer, 2008, pp.\n305–331.\n[6] M. Goto, “A chorus-section detecting method for mu-\nsical audio signals,” in 2003 IEEE International Con-\nference on Acoustics, Speech, and Signal Processing,\n2003. Proceedings.(ICASSP’03). , vol. 5. IEEE, 2003,\npp. V–437.\n[7] J. Paulus and A. Klapuri, “Labelling the structural parts\nof a music piece with Markov models,” in Computer\nMusic Modeling and Retrieval. Genesis of Meaning\nin Sound and Music: 5th International Symposium,\nCMMR . Springer, 2009, pp. 166–176.\n[8] A. Eronen and F. Tampere, “Chorus detection with\ncombined use of MFCC and chroma features and im-\nage processing ﬁlters,” in Proc. of 10th International\nConference on Digital Audio Effects , 2007, pp. 229–\n236.\n[9] B. McFee, O. Nieto, M. M. Farbood, and J. P. Bello,\n“Evaluating hierarchical structure in music annota-\ntions,” Frontiers in psychology , vol. 8, p. 1337, 2017.\n[10] R. S. Schaefer, J. Farquhar, Y . Blokland, M. Sadakata,\nand P. Desain, “Name that tune: Decoding music from\nthe listening brain,” NeuroImage , vol. 56, no. 2, pp.\n843–849, 2011.[11] S. Stober, D. J. Cameron, and J. A. Grahn, “Classifying\nEEG recordings of rhythm perception.” in Proceedings\nof the 15th International Society for Music Information\nRetrieval Conference , 2014, pp. 649–654.\n[12] M. S. Treder, H. Purwins, D. Miklody, I. Sturm, and\nB. Blankertz, “Decoding auditory attention to instru-\nments in polyphonic music using single-trial EEG clas-\nsiﬁcation,” Journal of neural engineering , vol. 11,\nno. 2, p. 026009, 2014.\n[13] G. Cantisani, S. Essid, and G. Richard, “EEG-based\ndecoding of auditory attention to a target instrument in\npolyphonic music,” in 2019 IEEE Workshop on Appli-\ncations of Signal Processing to Audio and Acoustics\n(WASPAA) . IEEE, 2019, pp. 80–84.\n[14] G. Cantisani, G. Trégoat, S. Essid, and G. Richard,\n“MAD-EEG: An EEG dataset for decoding auditory\nattention to a target instrument in polyphonic music,”\ninSpeech, Music and Mind (SMM), Satellite Workshop\nof Interspeech 2019 , 2019.\n[15] G. Cantisani, S. Essid, and G. Richard, “Neuro-steered\nmusic source separation with EEG-based auditory at-\ntention decoding and contrastive-NMF,” in ICASSP\n2021-2021 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) . IEEE,\n2021, pp. 36–40.\n[16] S. Stober, T. Prätzlich, and M. Müller, “Brain beats:\nTempo extraction from EEG data,” in Proceedings of\nthe 17th International Society for Music Information\nRetrieval Conference , 2016, pp. 276–282.\n[17] M.-S. Kim, G. Y . Lee, and H.-G. Kim, “Multi-channel\nEEG classiﬁcation method according to music tempo\nstimuli using 3D convolutional bidirectional gated re-\ncurrent neural network,” The Journal of the Acoustical\nSociety of Korea , vol. 40, no. 3, pp. 228–233, 2021.\n[18] G. Y . Lee, M.-S. Kim, and H.-G. Kim, “Extraction\nand classiﬁcation of tempo stimuli from electroen-\ncephalography recordings using convolutional recur-\nrent attention model,” ETRI Journal , vol. 43, no. 6, pp.\n1081–1092, 2021.\n[19] A. Vinay, A. Lerch, and G. Leslie, “Mind the beat: De-\ntecting audio onsets from EEG recordings of music lis-\ntening,” in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , 2021,\npp. 231–235.\n[20] A. Ofner and S. Stober, “Shared generative representa-\ntion of auditory concepts and EEG to reconstruct per-\nceived and imagined music,” in Proceedings of the 19th\nInternational Society for Music Information Retrieval\nConference . ISMIR, 2018, pp. 392–399.\n[21] N. Gang, B. Kaneshiro, J. Berger, and J. P. Dmo-\nchowski, “Decoding neurally relevant musical features\nusing Canonical Correlation Analysis,” in ProceedingsProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n270of the 18th International Society for Music Information\nRetrieval Conference , 2017, pp. 131–138.\n[22] J. R. Katthi and S. Ganapathy, “Deep multiway Canon-\nical Correlation Analysis for multi-subject EEG nor-\nmalization,” in ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) . IEEE, 2021, pp. 1245–1249.\n[23] C. Foster, D. Dharmaretnam, H. Xu, A. Fyshe, and\nG. Tzanetakis, “Decoding music in the human brain\nusing EEG data,” in IEEE 20th International Workshop\non Multimedia Signal Processing (MMSP) , 2018, pp.\n1–6.\n[24] E. B. Abrams, E. Muñoz Vidal, C. Peloﬁ, and P. Ripol-\nlés, “Retrieving musical information from neural data:\nHow cognitive features enrich acoustic ones,” in Pro-\nceedings of the 23rd International Society for Music\nInformation Retrieval Conference , 2022.\n[25] A. Ofner and S. Stober, “Modeling perception with\nhierarchical prediction: Auditory segmentation with\ndeep predictive coding locates candidate evoked poten-\ntials in EEG,” in Proceedings of the 21st International\nSociety for Music Information Retrieval Conference ,\n2020, pp. 566–573.\n[26] S. Sangnark, P. Autthasan, P. Ponglertnapakorn,\nP. Chalekarn, T. Sudhawiyangkul, M. Trakulruangroj,\nS. Songsermsawad, R. Assabumrungrat, S. Amplod,\nK. Ounjai, and T. Wilaiprasitporn, “Revealing prefer-\nence in popular music through familiarity and brain\nresponse,” IEEE Sensors Journal , vol. 21, no. 13, pp.\n14 931–14 940, 2021.\n[27] U. Hasson, Y . Nir, I. Levy, G. Fuhrmann, and\nR. Malach, “Intersubject synchronization of cortical\nactivity during natural vision,” Science , vol. 303, no.\n5664, pp. 1634–1640, 2004.\n[28] J. P. Dmochowski, A. S. Greaves, and A. M. Norcia,\n“Maximally reliable spatial ﬁltering of steady state vi-\nsual evoked potentials,” NeuroImage , vol. 109, pp. 63–\n72, 2015.\n[29] J. Madsen, E. H. Margulis, R. Simchy-Gross, and L. C.\nParra, “Music synchronizes brainwaves across listeners\nwith strong effects of repetition, familiarity and train-\ning,” Scientiﬁc reports , vol. 9, no. 1, pp. 1–8, 2019.\n[30] B. Kaneshiro, D. T. Nguyen, A. M. Norcia, J. P. Dmo-\nchowski, and J. Berger, “Natural music evokes corre-\nlated EEG responses reﬂecting temporal structure and\nbeat,” NeuroImage , vol. 214, p. 116559, 2020.\n[31] T. Dauer, D. T. Nguyen, N. Gang, J. P. Dmochowski,\nJ. Berger, and B. Kaneshiro, “Inter-subject correlation\nwhile listening to minimalist music: A study of elec-\ntrophysiological and behavioral responses to Steve Re-\nich’s Piano Phase,” Frontiers in Neuroscience , vol. 15,\n2021.[32] S. Stober, A. Sternin, A. M. Owen, and J. A. Grahn,\n“Towards music imagery information retrieval: Intro-\nducing the OpenMIIR dataset of EEG recordings from\nmusic perception and imagination.” in Proceedings of\nthe 16th International Society for Music Information\nRetrieval Conference , 2015, pp. 763–769.\n[33] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yaz-\ndani, T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras,\n“DEAP: A database for emotion analysis using physi-\nological signals,” IEEE transactions on affective com-\nputing , vol. 3, no. 1, pp. 18–31, 2012.\n[34] K. P. Miyapuram, N. Ahmad, P. Pandey, and J. D. Lo-\nmas, “Electroencephalography (EEG) dataset during\nnaturalistic music listening comprising different genres\nwith familiarity and enjoyment ratings,” Data in Brief ,\nvol. 45, p. 108663, 2022.\n[35] B. Kaneshiro, D. T. Nguyen, J. P. Dmochowski,\nA. M. Norcia, and J. Berger, “Naturalistic music\nEEG dataset—Hindi (NMED-H),” in Stanford Digital\nRepository , 2016. [Online]. Available: http://purl.\nstanford.edu/sd922db3535\n[36] S. Losorelli, D. T. Nguyen, J. P. Dmochowski, and\nB. Kaneshiro, “NMED-T: A tempo-focused dataset of\ncortical and behavioral responses to naturalistic mu-\nsic,” in Proceedings of the 18th International Society\nfor Music Information Retrieval Conference , 2017, pp.\n339–346.\n[37] B. Kaneshiro, D. T. Nguyen, J. P. Dmochowski,\nA. M. Norcia, and J. Berger, “Naturalistic music\nEEG dataset—Elgar (NMED-E),” in Stanford Digital\nRepository , 2021. [Online]. Available: https://purl.\nstanford.edu/pp371jh5722\n[38] D. P. W. Ellis, “Beat tracking by dynamic program-\nming,” Journal of New Music Research , vol. 36, no. 1,\npp. 51–60, 2007.\n[39] Y . Benjamini and D. Yekutieli, “The control of the false\ndiscovery rate in multiple testing under dependency,”\nThe Annals of Statistics , vol. 29, no. 4, pp. 1165–1188,\n2001.\n[40] J. J. Ki, S. P. Kelly, and L. C. Parra, “Attention strongly\nmodulates reliability of neural responses to naturalis-\ntic narrative stimuli,” Journal of Neuroscience , vol. 36,\nno. 10, pp. 3092–3101, 2016.\n[41] E. H. Margulis, On repeat: How music plays the mind .\nOxford University Press, 2014.\n[42] B. Kaneshiro, F. Ruan, C. W. Baker, and J. Berger,\n“Characterizing listener engagement with popular\nsongs using large-scale music discovery data,” Fron-\ntiers in Psychology , vol. 8, p. 416, 2017.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n271"
    },
    {
        "title": "Predicting Performance Difficulty From Piano Sheet Music Images.",
        "author": [
            "Pedro Ramoneda",
            "Jose J. Valero-Mas",
            "Dasaem Jeong",
            "Xavier Serra"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265387",
        "url": "https://doi.org/10.5281/zenodo.10265387",
        "ee": "https://zenodo.org/records/10265387/files/000084.pdf",
        "abstract": "Estimating the performance difficulty of a musical score is crucial in music education for adequately designing the learning curriculum of the students. Although the music information retrieval community has recently shown interest in this task, existing approaches mainly use machine-readable scores, leaving the broader case of sheet music images unaddressed. Based on previous works involving sheet music images, we use a mid-level representation, bootleg score, describing notehead positions relative to staff lines coupled with a transformer model. This architecture is adapted to our task by introducing a different encoding scheme that reduces the encoded sequence length to one-eighth of the original size. In terms of evaluation, we consider five datasets---more than 7500 scores with up to 9 difficulty levels---, two being mainly compiled for this work. The results obtained when pretraining the scheme on the IMSLP corpus and fine-tuning it on the considered datasets prove the proposal's validity, achieving the best-performing model with a balanced accuracy of 40.3\\% and a mean square error of 1.3. Finally, we provide access to our code, data, and models for transparency and reproducibility.",
        "zenodo_id": 10265387,
        "dblp_key": "conf/ismir/RamonedaVJS23",
        "keywords": [
            "performance difficulty",
            "music education",
            "learning curriculum",
            "sheet music images",
            "music information retrieval",
            "notehead positions",
            "staff lines",
            "transformer model",
            "bootleg score",
            "encoding scheme"
        ],
        "content": "PREDICTING PERFORMANCE DIFFICULTY FROM\nPIANO SHEET MUSIC IMAGES\nPedro Ramoneda1Jose J. Valero-Mas1\nDasaem Jeong2Xavier Serra1\n1Music Technology Group, Universitat Pompeu Fabra, Barcelona\n{pedro.ramoneda, josejavier.valero, xavier.serra}@upf.edu\n2MALer Lab, Sogang University, Seoul\ndasaemj@sogang.ac.kr\nABSTRACT\nEstimating the performance difﬁculty of a musical score\nis crucial in music education for adequately designing the\nlearning curriculum of the students. Although the Music\nInformation Retrieval community has recently shown in-\nterest in this task, existing approaches mainly use machine-\nreadable scores, leaving the broader case of sheet music\nimages unaddressed. Based on previous works involv-\ning sheet music images, we use a mid-level representa-\ntion, bootleg score, describing notehead positions relative\nto staff lines coupled with a transformer model. This archi-\ntecture is adapted to our task by introducing an encoding\nscheme that reduces the encoded sequence length to one-\neighth of the original size. In terms of evaluation, we con-\nsider ﬁve datasets—more than 7500 scores with up to 9 dif-\nﬁculty levels—, two of them particularly compiled for this\nwork. The results obtained when pretraining the scheme\non the IMSLP corpus and ﬁne-tuning it on the considered\ndatasets prove the proposal’s validity, achieving the best-\nperforming model with a balanced accuracy of 40.34% and\na mean square error of 1.33. Finally, we provide access\nto our code, data, and models for transparency and repro-\nducibility.\n1. INTRODUCTION\nEstimating the difﬁculty of a piece is crucial for music ed-\nucation, as it enables the effective structuring of music col-\nlections to attend to the student’s needs. This has led to a\ngrowing research interest [1–4], as well as the development\nof automatic systems for exploring difﬁculties by major in-\ndustry players such as Muse Group [5,6] and Yousician [7].\nPrevious research on predicting piano difﬁculty has pri-\nmarily focused on symbolic machine-readable scores [1,\n2, 4, 8–10]. Early studies explored feature engineering de-\nscriptors [1,2] and the relationship between piano ﬁngering\n© P. Ramoneda, J. J. Valero-Mas, D. Jeong and X. Serra.\nLicensed under a Creative Commons Attribution 4.0 International Li-\ncense (CC BY 4.0). Attribution: P. Ramoneda, J. J. Valero-Mas, D.\nJeong and X. Serra, “Predicting performance difﬁculty from piano sheet\nmusic images”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.\nFigure 1 . We consider the bootleg score mid-\nrepresentation with a multi-task GPT-based recognition\nframework to predict the performance difﬁculty associated\nto a piano score directly from sheet images from multiple\nannotated collections with varied difﬁculty levels.\nand difﬁculty [8–10]. A recent study [4] used stacked re-\ncurrent neural networks and context attention for difﬁculty\nclassiﬁcation on machine-readable scores, employing em-\nbeddings from automatic piano ﬁngering, piano expressive\ngeneration [11], and score information. This study found\nthat modeling the score difﬁculty classiﬁcation task as an\nordinal regression problem [12] was advantageous, and us-\ning entire pieces for training, rather than fragments, was\nessential to avoid degraded performance.\nAlthough symbolic machine-readable scores offer more\ninterpretability [10], with all the music information com-\npletely accessible, their limited availability compared to\nsheet music images restricts the practical use of difﬁculty\nprediction tools for librarians, teachers, and students. Fo-\ncusing on sheet music image analysis expands the range\nof available music, has the potential to preserve the cul-\ntural heritage of symbolic-untranscribed scores, and ad-\ndresses the lack of diversity in Western classical piano cur-\nricula. By analyzing image-based sheet music, we aim708to create technology for highlighting historically under-\nrepresented communities like female composers [13, 14]\nand promoting diversity in piano education. This pro-\nmotion is crucial since the piano teaching repertoire has\nremained mostly unchanged for decades [15], containing\naround 3,300 pieces [16], while projects such as IMSLP\nhouse remarkably larger databases.\nOne of the main challenges in working with sheet music\nis attaining a symbolic music-based representation for di-\nrect analysis. Although Optical Music Recognition (OMR)\nliterature has considerably improved in creating such rep-\nresentations over the past 30 years, it remains an unsolved\ntask [17]. Bootleg score [18] is an alternative to symbolic\nscores obtained with OMR. This mid-level symbolic rep-\nresentation keeps the most relevant primitives of the mu-\nsic content in a music sheet, which has shown remarkable\nsuccess in several tasks [19–22], especially in classiﬁca-\ntion, such as piano composer classiﬁcation [19, 23, 24] or\ninstrument recognition [25].\nWe build on this literature, employing the GPT\nmodel [26] and bootleg score in our analysis. More pre-\ncisely, we consider the approach by Tsai et al. [18], in\nwhich a GPT model pretrained on the IMSLP piano col-\nlection is ﬁnetuned for speciﬁc recognition tasks. With\nadequate adaptations, we hypothesize that this framework\nmay also succeed in estimating performance difﬁculty on\nmusic sheet images.\nAs aforementioned, difﬁculty estimation beneﬁts from\nthe use of entire music pieces rather than excerpts to ob-\ntain adequate success rates. However, processing large\nsequence stands as a remarkable challenge in music pro-\ncessing, especially when addressing bootleg representa-\ntions due its considerable verbosity. While some recent\nmechanisms address this issue in general learning frame-\nworks ( e.g., Flash Attention [27]), we extend the original\nproposal by Tsai et al. [18] with a multi-hot optimization\ntarget for GPT pretraining, and replace the categorical en-\ncoding with causal convolutional or feedforward projec-\ntion layers to enhance performance and reduce costs.\nMoreover, addressing data scarcity is crucial for pro-\nmoting and establishing this task within the Music Infor-\nmation Retrieval community. As of now, the Mikrokosmos-\ndifﬁculty (MK) [10] and Can I Play It? (CIPI) [4] sym-\nbolic datasets stand for the only available annotated collec-\ntions, out of which music sheet images can be obtained by\nengraving mechanisms. To enhance data availability and\nencourage further research, we have collected additional\ndatasets from existing collections, namely Pianostreet-\ndifﬁculty (PS), Freescore-difﬁculty (FS), and black female\ncomposers collection Hidden V oices (HV). This results in\nmore than 7500 music pieces, spanning up to 9 difﬁculty\nlevels and each annotated with a difﬁculty classiﬁcation\nsystem. Although difﬁculty prediction contains a subjec-\ntive element, global trends may emerge when examining\nmultiple difﬁculty classiﬁcation systems simultaneously.\nTo our knowledge, no previous research has explored this\naspect. Consequently, we propose a multitask approach to\ntraining simultaneously on CIPI, PS, and FS datasets. Fi-nally, we also analyze the generalization of our proposed\nmethodologies with the MK and HV benchmark datasets.\nConsidering all above, our precise contributions are:\n(i) we adopt the previous bootleg-representation litera-\nture [23,24], pretraining a GPT model on IMSLP and ﬁne-\ntuning it for our task, adapting the encoding scheme ac-\ncordingly, as presented in Figure 1; (ii) we evaluate our\nproposal using a novel sheet music image collection of ﬁve\ndatasets with more than 7,500 pieces with difﬁculty levels\nranging up to 9; (iii) we propose a multi-task strategy for\ncombining multiple difﬁculty classiﬁcation systems from\nthe datasets; (iv) we conduct extensive experiments to as-\nsess the proposed methodologies, including a zero-shot\nscenario for testing generalization and comparisons with\nprevious proposals on the CIPI dataset; and (v) to promote\nthe task, code, and models1, and datasets2are publicly\navailable.\n2. MUSIC SHEET IMAGE DATASETS\nDue to the relative recentness of the ﬁeld, the lack of an-\nnotated corpora has severely constrained the performance\ndifﬁculty assessment. The earliest data assortments may\nbe found in the works by Sebastian et al. [1] and Chiu\net al. [2], which respectively collected 50 and 300 MIDI\nscores from different score repositories. However, these\ndatasets were never publicly released.\nTo our best knowledge, the Mikrokosmos difﬁculty\n(MK) set by Ramoneda et al. [10], which comprises 147\npiano pieces by Béla Bartók in a symbolic format graded\nby the actual composer, represents the ﬁrst publicly avail-\nable collection for the task at hand. More recently, the au-\nthors introduced the Can I Play It? (CIPI) dataset [4], a\ncollection of 652 piano works in different symbolic for-\nmats annotated after 9 different difﬁculty levels. Note that,\nwhile sheet music scores can be obtained by resorting to\nengraving mechanisms, the insights obtained may not ap-\nply to real-world scenarios.\nDataset Pieces Classes AIR Noteheads Composers\nMK [10] 147 147 .78 49.2k 1\nCIPI [4] 652 9 .33 1.1M 29\nPS 2816 9 .24 7.2M 92\nFS 4193 5 .37 5.8M 747\nHV 17 4 1 21.5k 10\nTable 1 . Description of existing collections for perfor-\nmance difﬁculty estimation based on the number of pieces,\nclasses, average imbalance ratio (AIR), noteheads, and\ncomposers. The dashed line differentiates the datasets\nbased on symbolic (above) and image (below) sheet mu-\nsic.\nTo address this limitation, we compiled a set of real\nsheet music images of piano works together with their per-\nformance difﬁculty annotations from different music edu-\ncation and score-sharing platforms on the Internet. More\n1https://github.com/PRamoneda/pdf-difficulty\n2https://zenodo.com/record/8126801Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n709precisely, we arranged three different collections attend-\ning to the source: (i) the Pianostreet-difﬁculty (PS) set\nretrieved from [28] that depicts 2,816 works with 9 dif-\nﬁculty levels annotated by the Pianostreet team; (ii) the\nFreescores-difﬁculty (FS) assortment from [29] that con-\ntains 4,193 pieces with 5 difﬁculty levels comprising a\nvariety of compositions and annotations by the users of\nthe platform; and (iii) the Hidden Voices (HV) collec-\ntion [30,31], a set of 17 pieces by black female composers\nannotated with 4-level difﬁculty labels by musicologists of\nthe Colorado Boulder Music Department.\nTable 1 summarizes the main characteristics of com-\nmented publicly-available collections. The average imbal-\nance ratio (AIR), measured as the mean of the individual\nratios between each difﬁculty class and the majority label\nin each collection, is also provided for reference purposes.\n3. METHODOLOGY\nBased on its success when addressing classiﬁcation tasks\nfrom sheet music images [23, 25], our proposal considers\nthe use of the so-called bootleg score representation cou-\npled with a GPT-based recognition model to estimate the\nperformance difﬁculty of a piece.\nIntroduced by [18], bootleg scores stand as a simple—\nyet effective—representation to encode the content of a\nsheet music image for certain recognition tasks. Formally,\na bootleg score is a binary matrix of length wandh= 62\nvertical positions— i.e.,X ∈ {0,1}w×62—that respec-\ntively denote the temporal and pitch dimensions. Note\nthat thewvalue represents the number of note heads de-\ntected by the bootleg extraction process. Our work resorts\nto this representation, being the use of alternative codiﬁca-\ntions posed as a future line to address.\nThe GPT recognition framework undergoes an unsu-\npervised pretraining step on the IMSLP piano collection,\nwhich was originally used by [18]. Eventually, considering\na set of labeled data T ⊂ X ×C whereC=/braceleftbig\nc1,...,c |C|/bracerightbig\ndenotes the possible difﬁculty levels, the model is ﬁne-\ntuned to retrieve the recognition function ˆf:X → C that\nrelates a bootleg representation to a particular difﬁculty\nlevel. Based on previous work addressing this task [4],\nwe consider an ordinal classiﬁcation framework [12] as the\ndifﬁculty grading scales naturally ﬁt this formulation.\nDespite being capable of addressing the task, the frame-\nwork was noticeably affected by two factors: (i) the ex-\ncessive length of the input sequences when pretraining the\nmodel; and (ii) the inconsistent deﬁnition of difﬁculty lev-\nels among corpora. Consequently, we introduce two mech-\nanisms speciﬁcally devised to address these limitations.\n3.1 Sequence length in pretraining\nOne of the main drawbacks related to bootleg representa-\ntions is their verbosity, as it depicts h= 62 elements per\nframe. To address this issue, Tsai et al. [23] proposed sub-\ndividing each column into groups of 8 elements and encod-\ning each according to a vocabulary of |σ|= 28elements.\nIn this regard, the initial bootleg score x∈ {0,1}w×62ismapped to a novel space deﬁned as Σw×8. This represen-\ntation is then ﬂattened to undergo a categorical embedding\nprocess that maps it to a feature-based space denoted as\nR8w×768, which is eventually used for pretraining the GPT\nmodel with 768-dim hidden states. Note that this process\nreduces the vocabulary size and remarkably increases the\nsequence length.\nTo address this issue, we propose substituting this to-\nkenization process with an embedding layer that directly\nmaps the bootleg score into a suitable representation,\navoiding the extension of the initial length of the se-\nquence. In this sense, the initial bootleg representation\nx∈ {0,1}w×62is mapped to a space deﬁned as Rw×768\nthat serves as input to the GPT model with a fraction of the\nlength of the encoding used by Tsai et al. [23]. Besides re-\nducing the length of the sequences to process, we hypoth-\nesize that such an embedding may beneﬁt the recognition\nmodel as a suitable representation is inferred for the task.\nIn this regard, our experiments will compare two types of\nembedding approaches—more precisely, a fully-connected\nlayer and a convolutional one, respectively denoted as FC\nand CNN—to quantitatively assess this claim.\nFigure 2 graphically describes the approach by Tsai et\nal. [23] and the presented proposal. In opposition to the ref-\nerence work, the proposal considers multi-hot encoding in-\nstead of discrete categorical index as the output of the GPT\nrecognition framework, by using binary cross-entropy loss\ninstead of negative log-likelihood loss.\nFigure 2 . Comparison between the proposal by Tsai et\nal. [23]—denoted as (a)—and the presented proposal—\nhighlighted as (b)—for a case of toy example with a du-\nration ofw= 4.\n3.2 Multi-task learning of multiple difﬁculty\nclassiﬁcation systems\nThe pretrained GPT model can be simply ﬁnetuned for a\nperformance difﬁculty classiﬁcation task by adding a pro-\njection layer and a learnable classiﬁcation token, as de-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n710picted in Figure 3. However, the actual deﬁnition of the\nperformance difﬁculty of a piece is a highly subjective\nproblem that may bias—and, hence, remarkably hinder—\nthe goodness of a recognition model. In this regard, we hy-\npothesize that using a multi-task approach that attends dif-\nferent deﬁnitions of difﬁculty— i.e., a labeled assortment\nof data from multiple annotators—may beneﬁt the gener-\nalization capabilities of the approach.\nIn this regard, we modify the reference architecture for\nthe downstream task to include an additional classiﬁca-\ntion layer for each training collection. While simple, such\na proposal is expected to improve the overall recognition\nperformance given the wider variety of data provided dur-\ning the training process. Figure 3 graphically describes this\nproposal.\nFinally, no pre-processing is done in relation to the label\ndistribution of the corpora to avoid inducing any type of\nbias. In this regard, the sampling protocol of the model has\nbeen forced to maintain its original distributions.\nFigure 3 . Graphical description of the downstream archi-\ntecture depicting the classiﬁcation heads for the multi-task\nproposals as well as the single-head case of the reference\nwork.\n4. EXPERIMENTAL SETUP\n4.1 Data collections and assessment metrics\nTo validate the proposal, we have considered the ﬁve\npublicly-available data collections presented in Section 2,\ni.e.,Mikrokosmos difﬁculty (MK) [10], Can I Play It?\n(CIPI) [4], Pianostreet-difﬁculty (PS) [28], Freescores-\ndifﬁculty (FS) [29], and Hidden Voices (HV) [30, 31].\nWhile MK and CIPI exclusively comprise symbolic scores,\nwe engraved them into music sheets and included them due\nto the commented scarcity of annotated data.We considered a 5-fold cross-validation scheme with a\ndata partitioning of 60% for the ﬁnetuning phase after the\npretraining stage with IMSLP together with two equal-size\nsplits of the remaining data for validation and testing. Note\nthat, since MK and HV are exclusively used for benchmark\npurposes, no partitioning is applied to them.\nIn terms of performance evaluation, we resort to\ntwo assessment criteria typically used in ordinal clas-\nsiﬁcation [32]: accuracy within n(Accn) and mean\nsquared error (MSE). To adequately described them, let\nS ⊂ X × Cdenote a set of test data and let Sc=\n{(xi,yi)∈ S:yi=c}with1≤i≤ |S| be the subset\nof elements in Swith class c.\nBased on this, Acc nis deﬁned as:\nAccn=1\n|C|/summationdisplay\n∀c∈C/vextendsingle/vextendsingle/vextendsingle/braceleftBig\ny∈ Sc:/vextendsingle/vextendsingle/vextendsingleˆf(x)−c/vextendsingle/vextendsingle/vextendsingle≤n/bracerightBig/vextendsingle/vextendsingle/vextendsingle\n|Sc|(1)\nwhereˆf(·)represents the trained recognition model and\nn∈N0denotes the tolerance or class-boundary relaxation\nthat allows for errors in adjacent labels. In our experiments\nwe consider the values of n= 0 (no tolerance) and n=\n1(smallest adjacency tolerance), respectively denoted as\nAcc0and Acc 1in the rest of the work.\nRegarding MSE, this ﬁgure of merit is deﬁned as:\nMSE=1\n|C|/summationdisplay\n∀c∈C/summationtext\n∀x∈Sc/parenleftBig\nˆf(x)−c/parenrightBig2\n|Sc|(2)\nFinally, note that all these metrics are macro-averaged\nto account for the unbalanced nature of the data collections\nused in the work.\n4.2 Training procedure\nAs commented, the recognition model undergoes an ini-\ntial pretraining stage considering the IMSLP corpus. Dur-\ning this stage, the model considers sequences of 256 to-\nkens, each with a binary cross-entropy as a loss function.\nTo speed up this process, the Flash Attention framework\nby [27] is also considered. For comparative purposes,\nall other parameters remain unaltered from the reference\nworks [23].\nAfter that, the model is ﬁnetuned on the downstream\ndifﬁculty estimation task, considering an Adam opti-\nmizer [33] with a learning rate of 10−5and early stopping\nbased on the Acc 0and MSE metrics on the validation set.\nMoreover, a balanced sampler is considered to tackle the\nissue of unbalanced data collections. Ordinal Loss [12]\nis applied to train the difﬁculty prediction as an ordinal\nclassiﬁcation problem, while no loss weighting considered\nin the multi-task framework. For regularization and stable\ntraining, gradient clipping is set to 10−4, with a batch size\nof 64 and L2 regularization. This optimization process is\ncarried out exclusively on the last layer of the model, re-\nsorting the remaining parts to the weights obtained during\nthe pretraining phase of the procedure.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n711Note that while these processes may be further studied\nto account for the optimal solution that retrieves the best-\nperforming results, such a study is out of the scope of the\nwork and is left as future work to address.\n5. EXPERIMENTS AND RESULTS\nThis section presents the results obtained with the intro-\nduced experimental scheme. To adequately provide in-\nsights about the task, the section provides a series of in-\ndividual experiments devoted to analyzing one aspect of\nthe proposal: Section 5.1 analyzes the inﬂuence of the en-\ncoding scheme; Section 5.2 evaluates the inﬂuence of the\nmultitask architecture; Section 5.3 delves on the ranking\ngeneralization in a zero-shot scenario; ﬁnally, Section 5.4\ncompares the attainable results when addressing the task\nfrom the symbolic versus the sheet-image domains.\n5.1 Encoding schemes experiment\nThis ﬁrst experiment compares the performance of the two\nencoding schemes presented in Section 3.1, i.e., GPTFC\nand GPT CNN. Table 2 presents the results obtained for the\nCIPI, FS, and PS collections for the three ﬁgures of merit\nconsidered.\nEncoding Acc 0(%) Acc 1(%) MSE\nCan I Play it?\nGPTFC 34.3(6.1) 78.1(4.6) 1.6(0.3)\nGPTCNN 36.2(8.2) 81.7(1.5) 1.4(0.1)\nPianoStreet\nGPTFC 30.9(3.8) 71.1(9.6) 2.1(0.4)\nGPTCNN 31.8(1.6) 78.8(1.8) 1.9(0.1)\nFreeScores\nGPTFC 46.6(1.9) 92.5(1.0) 0.8(0.1)\nGPTCNN 47.3(3.4) 92.4(0.6) 0.8(0.1)\nTable 2 . Results of comparing the encoding schemes\nGPTFCand GPT CNN. Bold values highlight the best results\nper collection and metric.\nAs it may be observed, the GPT CNNexperiment outper-\nformed the GPT FCexperiment in most evaluation metrics\nacross the three datasets. More precisely, the GPT CNNcon-\nsistently achieved the best performance in the Acc 0metric\nfor all data collections, showing an average improvement\nof1%concerning the GPT CNNcase. This trend remains for\nthe rest of the ﬁgures of merit except for the case in the\nFS assortment, in which the results of the FC-based model\noutperform those of the CNN case.\nNevertheless, attending to the high standard deviations,\nthe performance results of the two models show a remark-\nable overlap in performance, hence suggesting that both\nschemes are equally capable of performing the posed task\nof score difﬁculty analysis from sheet music images. In\nthis regard, further work should explore other encoding al-\nternatives to assess whether this performance stagnation is\ndue to the representation capabilities of the considered em-\nbedding layers or due to the recognition framework.5.2 Multi-task learning experiment\nIn this second study, we assess the capabilities of the multi-\ntask framework proposed in Section 3.2 trained simul-\ntaneously on the CIPI, PS, and FS datasets for the two\nGPTmulti\nFC and GPTmulti\nCNN encoding schemes. Table 3 pro-\nvides the results obtained.\nEncoding Acc 0(%) Acc 1(%) MSE\nGPTmulti\nFC\nCIPI 40.3(4.3) 82.0(1.4) 1.3(0.1)\nPS 35.9(3.1) 78.2(3.4) 1.9(0.2)\nFS 45.8(2.5) 92.0(1.4) 0.8(0.1)\nGPTmulti\nCNN\nCIPI 34.9(5.0) 81.4(1.3) 1.4(0.1)\nPS 35.9(2.8) 74.5(3.4) 2.7(0.2)\nFS 45.9(1.2) 92.4(2.1) 0.8(0.1)\nTable 3 . Results of multi-task learning experiment when\nevaluated on different test collections for the two encoding\nschemes. Bold values highlight the best results per collec-\ntion and metric.\nOverall, the GPTmulti\nFC method had higher results than\nthe GPTmulti\nCNN method on the CIPI and PS datasets, es-\npecially on Acc 0and Acc 1. For CIPI, GPTmulti\nFC sur-\npassed GPTmulti\nCNN with gains of 5.4% in Acc 0, 0.6% in\nAcc1, and 0.1 in MSE. For PS, GPTmulti\nFC slightly exceeded\nGPTmulti\nCNN with a 3.7% improvement in Acc 1and a 0.6-\npoint reduction in MSE, while Acc 0was nearly equal for\nboth methods, although GPTmulti\nCNN had a smaller standard\ndeviation. Both methods displayed similar performance on\nthe FS dataset, with less than a 1% difference across all\nmetrics. As a result, subsequent experiments will reference\nthe GPTmulti\nFC model.\nThe comparison between Tables 2 and 3 shows a trend\nchange with better results performed with the FC version\nof the models. The other major difference is the relative\nimprovement between the GPTmulti\nFC method and the best\nprevious model GPT CNNin the CIPI and slightly in the PS\ndataset. In contrast, the FS dataset results remain compa-\nrable. In CIPI, Acc 0is 11.3% higher in GPTmulti\nFC , and in\nPS, there is a relative improvement of 12.8%. For CIPI,\nAcc1sees a minor increase of 0.4%. MSE exhibits a small\nimprovement of 3.6% for CIPI and 0.5% for PS. Possible\nreasons include label quality differences—CIPI annotated\nby a musicology team, PS labels provided by the platform,\nand FS crowdsourced by users—or the impact of dataset\nsizes—CIPI being the smallest and FS the largest.\n5.3 Ranking generalization experiment\nIn this experiment, we assess the ranking capabilities of\nthe proposal in a zero-shot setting by utilizing the embed-\ndings of the projection layer of the model (check Figure 3).\nWe reduce the 768-dimensional embeddings to a single di-\nmension using Principal Component Analysis (PCA) and\nemploy the resulting values to rank the target pieces.\nTable 4 shows the results obtained resorting to theProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n712Kendall rank correlation coefﬁcient, τc, for all data col-\nlections discussed in the experiment, considering both the\nsingle-task and multi-task frameworks posed. Note that\nMK and HV are only used for benchmarking purposes.\nTrainEvaluation\nCIPI PS FS MK HV\nCIPI .67 (.01) .56 (.02) .56 (.01) .67 (.05) .50 (.05)\nPS .67 (.01) .58 (.02) .56 (.01) .68 (.01) .43 (.04)\nFS .64 (.04) .55 (.01) .56 (.02) .71 (.02) .56 (.07)\nMULTI .68 (.02) .59 (.02) .56 (.01) .63 (.02) .51 (.07)\nTable 4 . Zero-shot ranking results. Bold values denote the\nbest-performing result on each evaluation dataset.\nIn the three training datasets, the multi-task architec-\nture GPTmulti\nFC achieves the best performance with CIPI\n(τc= 0.68), PS (τc= 0.59), and FS ( τc= 0.56). Unex-\npectedly, the FS method outperforms others in the datasets\nof the MK ( τc= 0.61) and HV ( τc= 0.56). This outcome\nmay suggest that simultaneous training on all three datasets\ncould limit generalizability. Alternatively, the presence of\nlicense-free pieces composed after 1900 in the FS dataset,\nwhich users have uploaded, might explain the difference.\nThe HV dataset displays notably lower generalizabil-\nity, possibly due to the smaller number of pieces, result-\ning in higher standard deviations. Potential bias similar to\nMK could also arise from the predominance of pre-20th-\ncentury data in CIPI and PS. These factors might affect the\nzero-shot experiment’s performance. However, we must\nalso acknowledge that most composers used for training\nare white males, and the HV results are signiﬁcantly worse\nthan the rest of the datasets. Therefore, future research\nshould investigate and minimize the potential gender gap\nin difﬁculty prediction tasks.\n5.4 Comparison with previous approaches\nThis last experiment compares the goodness of the pro-\nposed methodology in sheet music scores against other\nimage-based approaches and with the symbolic-oriented\nmethods domain. Regarding sheet image methods, we\nconsider the reference method by Tsai et al. [23] based\non bootleg mid-representation, denoted as GPT EMB. Con-\ncerning the symbolic baseline, we reproduce the approach\nin [4] that proposes to describe the symbolic score in\nterms of piano ﬁngering information, expressive annota-\ntions, and pitch descriptors to feed a recurrent model based\non Gated Recurrent Units with attention layers (referred to\nas GRU+Att). Table 5 provides the results obtained. For\ncomparative purposes, we only consider the CIPI dataset as\nthe reference symbolic work accounted for that collection.\nExamining the experiments, the GPTmulti\nFC model may\nbe observed to outperform the other cases in the Acc 0ﬁg-\nure of merit. However, for the rest of the metrics, the refer-\nence symbolic case—denoted as GRU+Att—outperforms\nall image-oriented recognition models. Such a fact sug-\ngests that, while a bootleg score somehow suits this dif-ﬁculty estimation task, a performance gap between this\nrepresentation and pure symbolic notation needs to be ad-\ndressed.\nCase Acc 0(%) Acc 1(%) MSE\nSymbolic [4]\nGRU+Att 39.5(3.4) 87.3(2.2) 1.1(0.2)\nTsat et al. [23]\nGPTEMB 19.7(4.0) 58.1(7.2) 3.3(0.8)\nProposal\nGPTFC 34.3(6.1) 78.1(4.6) 1.6(0.3)\nGPTCNN 36.2(8.2) 81.7(1.5) 1.4(0.1)\nGPTmulti\nFC 40.3(4.3) 82.0(1.4) 1.3(0.1)\nTable 5 . Performance results for the symbolic [4] and Tsai\net al. [23] methods as well as the proposed approach for\nthe CIPI dataset. Bold values highlight the best result per\nﬁgure of merit.\nFinally, the GPT EMBmodel achieves the lowest perfor-\nmance of all alternatives, with remarkably lower accuracy\nrates than our proposal. Note that such a fact emphasizes\nthe relevance of our work as a more suitable approach for\nperforming difﬁculty estimation in sheet music images.\n6. CONCLUSIONS\nEstimating the performance difﬁculty of a music piece is\na crucial need in music education to structure the learn-\ning curriculum of the students adequately. This task has\nrecently gathered attention in the Music Information Re-\ntrieval ﬁeld, given the scarce existing research works de-\nvoted to symbolic machine-readable scores. However, due\nto the limited availability of this type of data, there is a\nneed to devise methods capable of addressing this task with\nimage-based sheet music.\nAttending to its success in related classiﬁcation tasks,\nthis work considers the use of a mid-level representation—\nnamely, bootleg score—that encodes the content of a\nsheet music image with a GPT-based recognition frame-\nwork for predicting the difﬁculty of the piece. Instead\nof directly applying this methodology, we propose using\nspeciﬁc embedding mechanisms and multi-task learning\nto reduce the task complexity and improve its recogni-\ntion capabilities. The results obtained with ﬁve different\ndata collections—three of them speciﬁcally compiled for\nthis work—prove the validity of the proposal as it yields\nrecognition rates comparable to those attained in symbolic\nmachine-readable scores.\nFurther work comprises assessing and proposing alter-\nnative representations to the bootleg scores ( e.g., solutions\nbased on Optical Music Recognition). Also, we consider\nthat using smaller training sequences using hierarchical\nattention models or weak labels for varying-length piece\nfragments may report beneﬁts in the process. Finally, the\npractical deployment of this proposal in real-world sce-\nnarios involving real users may report some additional in-\nsights about the validity of the proposal.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n7137. ACKNOWLEDGMENT\nWe want to thank T.J. Tsai and all his students, especially\nDaniel Yang, for having conducted the prior research on\nthe bootleg score and, above all, for sharing all their work\nin the interest of Open Science. We are also grateful to\nPedro D’Avila for bringing to our attention the work of\nAlejandro Cremaschi related to the Hidden V oices project.\nLastly, we thank Alejandro Cremaschi and the University\nof Colorado Boulder Libraries team, David M. Hays and\nJessica Quah, for providing us with the scores.\nThis work is funded by the Spanish Ministerio\nde Ciencia, Innovación y Universidades (MCIU)\nand the Agencia Estatal de Investigación (AEI)\nwithin the Musical AI Project – PID2019-111403GB-\nI00/AEI/10.13039/501100011033 and the Basic Science\nResearch Program through the National Research Foun-\ndation of Korea (NRF) funded by the Korea Government\n(MSIT) (NRF-2022R1F1A1074566).\n8. REFERENCES\n[1] V . Sébastien, H. Ralambondrainy, O. Sébastien, and\nN. Conruyt, “Score analyzer: Automatically determin-\ning scores difﬁculty level for instrumental e-learning,”\ninProceedings of 13th International Society for Music\nInformation Retrieval Conference, ISMIR , Porto, Por-\ntugal, 2012.\n[2] S.-C. Chiu and M.-S. Chen, “A study on difﬁculty\nlevel recognition of piano sheet music,” in Proceedings\nof the IEEE International Symposium on Multimedia .\nIEEE, 2012, pp. 17–23.\n[3] E. Nakamura and K. Yoshii, “Statistical piano re-\nduction controlling performance difﬁculty,” APSIPA\nTransactions on Signal and Information Processing ,\nvol. 7, 2018.\n[4] P. Ramoneda, D. Jeong, V . Eremenko, N. C. Tamer,\nM. Miron, and X. Serra, “Combining piano perfor-\nmance dimensions for score difﬁculty classiﬁcation,”\narXiv preprint arXiv:2306.08480 , 2023.\n[5] “Musescore have automatic difﬁculty categories from\nyear 2022,” https://musescore.com/, accessed on April\n11, 2023.\n[6] “Ultimate guitar have automatic difﬁculty categories\nfrom year 2022,” https://www.ultimate-guitar.com/,\naccessed on April 11, 2023.\n[7] “System for estimating user’s skill in playing a music\ninstrument and determining virtual exercises thereof,”\nPatent US9 767 705B1, 2017.\n[8] E. Nakamura, N. Ono, and S. Sagayama, “Merged-\noutput hmm for piano ﬁngering of both hands.” in Pro-\nceedings of the 13th International Society for Music\nInformation Retrieval Conference, ISMIR , Taipei, Tai-\nwan, 2014, pp. 531–536.[9] E. Nakamura and S. Sagayama, “Automatic piano re-\nduction from ensemble scores based on merged-output\nhidden markov model,” in Proceedings of the 41st In-\nternational Computer Music Conference, ICMC , Den-\nton, USA, 2015.\n[10] P. Ramoneda, N. C. Tamer, V . Eremenko, M. Miron,\nand X. Serra, “Score difﬁculty analysis for piano per-\nformance education,” in Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing, ICASSP , Singapore, Singapore, 2022, pp.\n201–205.\n[11] D. Jeong, T. Kwon, Y . Kim, K. Lee, and J. Nam, “Vir-\ntuosoNet: A hierarchical RNN-based system for mod-\neling expressive piano performance,” in Proceedings of\nthe 20th International Society for Music Information\nRetrieval Conference, ISMIR , 2019, pp. 908–915.\n[12] J. Cheng, Z. Wang, and G. Pollastri, “A neural network\napproach to ordinal regression,” in Proceedings of the\nIEEE International Joint Conference on Neural Net-\nworks, IJCNN . Hong Kong, China: IEEE, 2008, pp.\n1279–1284.\n[13] D. Bennett, S. Macarthur, C. Hope, T. Goh, and S. Hen-\nnekam, “Creating a career as a woman composer: Im-\nplications for music in higher education,” British Jour-\nnal of Music Education , vol. 35, no. 3, pp. 237–253,\n2018.\n[14] J. Halstead, The woman composer: Creativity and the\ngendered politics of musical composition . Routledge,\n2017.\n[15] R. Cutietta, “Content for music teacher education in\nthis century,” Arts Education Policy Review , vol. 108,\nno. 6, pp. 11–18, 2007.\n[16] J. Magrath, Pianists guide to standard teaching and\nperformance literature . Alfred Music, 1995.\n[17] J. Calvo-Zaragoza, J. H. Jr, and A. Pacha, “Under-\nstanding optical music recognition,” ACM Computing\nSurveys (CSUR) , vol. 53, no. 4, pp. 1–35, 2020.\n[18] D. Yang, T. Tanprasert, T. Jenrungrot, M. Shan, and\nT. Tsai, “MIDI passage retrieval using cell phone pic-\ntures of sheet music,” in Proceedings of the 20th Inter-\nnational Society for Music Information Retrieval Con-\nference, ISMIR , Delft, The Netherlands, 2019, pp. 916–\n923.\n[19] D. Yang, A. Goutam, K. Ji, and T. J. Tsai, “Large-scale\nmultimodal piano music identiﬁcation using market-\nplace ﬁngerprinting,” Algorithms , vol. 15, no. 5, p. 146,\n2022.\n[20] D. Yang, K. Ji, and T. Tsai, “Aligning unsynchronized\npart recordings to a full mix using iterative subtractive\nalignment,” in Proceedings of the 22nd International\nSociety for Music Information Retrieval Conference,\nISMIR , Online, 2021, pp. 810–817.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n714[21] K. Ji, D. Yang, and T. Tsai, “Piano sheet music identi-\nﬁcation using marketplace ﬁngerprinting,” in Proceed-\nings of the 22nd International Society for Music Infor-\nmation Retrieval Conference, ISMIR , Online, 2021, pp.\n326–333.\n[22] D. Yang and T. J. Tsai, “Piano sheet music identiﬁ-\ncation using dynamic n-gram ﬁngerprinting,” Transac-\ntions of the International Society for Music Information\nRetrieval , vol. 4, no. 1, pp. 42–51, 2021.\n[23] T. Tsai and K. Ji, “Composer style classiﬁcation of pi-\nano sheet music images using language model pretrain-\ning,” in Proceedings of the 21th International Society\nfor Music Information Retrieval Conference, ISMIR ,\nMontreal, Canada, 2020, pp. 176–183.\n[24] D. Yang and T. Tsai, “Composer classiﬁcation with\ncross-modal transfer learning and musically-informed\naugmentation,” in Proceedings of the 22nd Interna-\ntional Society for Music Information Retrieval Confer-\nence, ISMIR , Online, 2021, pp. 802–809.\n[25] K. Ji, D. Yang, and T. J. Tsai, “Instrument classiﬁca-\ntion of solo sheet music images,” in Proceedings of the\nIEEE International Conference on Acoustics, Speech\nand Signal Processing, ICASSP , Toronto, ON, Canada,\n2021, pp. 546–550.\n[26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nI. Sutskever et al. , “Language models are unsupervised\nmultitask learners,” OpenAI blog , vol. 1, no. 8, p. 9,\n2019.\n[27] T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. Ré,\n“FlashAttention: Fast and memory-efﬁcient exact at-\ntention with IO-awareness,” in Advances in Neural In-\nformation Processing Systems , 2022.\n[28] “Piano street,” https://www.pianostreet.com/, accessed\non April 11, 2023.\n[29] “Free-scores,” https://www.free-scores.com/, accessed\non April 11, 2023.\n[30] University of Colorado, “Hidden voices project,” https:\n//www.colorado.edu/project/hidden-voices/, accessed\non April 11, 2023.\n[31] H. Walker-Hill, Piano Music by Black Women Com-\nposers: A Catalog of Solo and Ensemble Works , ser.\nMusic Reference Collection. Greenwood Press, 1992.\n[32] L. Gaudette and N. Japkowicz, “Evaluation methods\nfor ordinal classiﬁcation,” in Proceedings of the 22nd\nCanadian Conference on Advances in Artiﬁcial Intelli-\ngence , Kelowna, Canada, 2009, pp. 207–210.\n[33] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” arXiv preprint arXiv:1412.6980 ,\n2014.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n715"
    },
    {
        "title": "Semi-Automated Music Catalog Curation Using Audio and Metadata.",
        "author": [
            "Brian Regan",
            "Desislava Hristova",
            "Mariano Beguerisse-Díaz"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265359",
        "url": "https://doi.org/10.5281/zenodo.10265359",
        "ee": "https://zenodo.org/records/10265359/files/000071.pdf",
        "abstract": "We present a system to assist Subject Matter Experts (SMEs) to curate large online music catalogs. The system detects releases that are incorrectly attributed to an artist discography (misattribution), when the discography of a single artist is incorrectly separated (duplication), and predicts suitable relocations of misattributed releases. We use historical discography corrections to train and evaluate our system's component models. These models combine vector representations of audio with metadata-based features, which outperform models based on audio or metadata alone. We conduct three experiments with SMEs in which our system detects misattribution in artist discographies with precision greater than 77%, duplication with precision greater than 71%, and by combining the approaches, predicts a correct relocation for misattributed releases with precision up to 45%.\nThese results demonstrate the potential of such proactive curation systems in saving valuable human time and effort by directing attention where it is most needed.",
        "zenodo_id": 10265359,
        "dblp_key": "conf/ismir/ReganHB23",
        "keywords": [
            "Subject Matter Experts (SMEs)",
            "large online music catalogs",
            "detect releases",
            "artist discographies",
            "incorrectly attributed",
            "discography separation",
            "predictions",
            "relocations",
            "historical corrections",
            "proactive curation"
        ],
        "content": "SEMI-AUTOMATED MUSIC CATALOG CURATION USING AUDIO AND\nMETADATA\nBrian Regan\nSpotify\nbrianr@spotify.comDesislava Hristova\nSpotify\ndesih@spotify.comMariano Beguerisse-Díaz\nSpotify\nmarianob@spotify.com\nABSTRACT\nWe present a system to assist Subject Matter Experts\n(SMEs) to curate large online music catalogs. The sys-\ntem detects releases that are incorrectly attributed to an\nartist discography (misattribution), when the discography\nof a single artist is incorrectly separated (duplication), and\npredicts suitable relocations of misattributed releases. We\nuse historical discography corrections to train and evaluate\nour system’s component models. These models combine\nvector representations of audio with metadata-based fea-\ntures, which outperform models based on audio or meta-\ndata alone. We conduct three experiments with SMEs in\nwhich our system detects misattribution in artist discogra-\nphies with precision greater than 77%, duplication with\nprecision greater than 71%, and by combining the ap-\nproaches, predicts a correct relocation for misattributed re-\nleases with precision up to 45%. These results demon-\nstrate the potential of such proactive curation systems in\nsaving valuable human time and effort by directing atten-\ntion where it is most needed.\n1. INTRODUCTION\nOnline music catalogs such as Spotify’s contain millions\nof releases, and new ones are added daily by providers\nranging from professionally-staffed music labels to DIY\nartists via aggregators. In such large catalogs, it is common\nthat multiple artists share the same or similar names, or\nthat content by one artist comes from different providers.\nFor example, there are 14 distinct metal bands with the\nname Burial1. When a new release by a Burial makes it to\nthe catalog, in the absence of a unique artist identiﬁer, we\nmust make a decision of where to place the content: Is it by\nthe Italian doom metal band, the English death metal band,\none of the other 12 bands named Burial , or an entirely new\none? In general, to which artist do we attribute a release\nwhen there are multiple artists with the same name?\nMusic streaming services have multiple systems to en-\nsure that releases are correctly placed on artist discogra-\n1https://www.metal-archives.com/bands/Burial\n© B. Regan, D. Hristova, and M. Beguerisse-Díaz. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: B. Regan, D. Hristova, and M. Beguerisse-\nDíaz, “Semi-Automated Music Catalog Curation Using Audio and Meta-\ndata”, in Proc. of the 24th Int. Society for Music Information Retrieval\nConf., Milan, Italy, 2023.phies. However, given the large volumes of content and the\ndiversity of sources, it is inevitable that on rare occasions\na release is incorrectly attributed (e.g. due to incomplete\nor incorrect metadata, extreme ambiguity, or human error).\nThese errors can manifest in two different ways: 1) Mis-\nattribution : when a release is incorrectly attributed to an\nartist, so that their discography now contains releases from\ntwo separate real-world artists; 2) Duplication : when a re-\nlease is not attributed to the correct existing discography\nbut to a new one, so that a single artist’s work is split across\nthe two discographies. These errors negatively impact the\nexperience of both artists and users on the platform.\nThe problem of Named Entity Disambiguation (NED)\nhas been extensively researched to attribute scientiﬁc pa-\npers to homonym authors using metadata such as the au-\nthor’s ﬁelds of research, academic afﬁliations, and co-\nauthors [1–3]. In Music Information Retrieval (MIR),\nNED is primarily tackled as artist identiﬁcation or multi-\nclass classiﬁcation with known artist classes. Approaches\nto this problem rely primarily on audio feature representa-\ntions [4–6]. These methods cannot be applied to catalogs\nwith a large or unknown number of artists, and do not take\nadvantage of all existing information.\nHere we present a semi-automated proactive curation\nsystem to detect and correct attribution errors across large\nmusic catalogs. The system consists of two machine\nlearning sub-systems: a system for detecting misattribu-\ntion by splitting discographies with releases from multiple\nreal-world artists into their constituent sub-discographies\n(Fig. 1a), and a deduplication system that takes pairs of\ndiscographies or sub-discographies and decides if they\nshould be combined (Fig. 1b). Both sub-systems rely on\nmetadata and the acoustic similarity between releases, us-\ning deep convolutional network embeddings of their mel-\nspectrograms [7]. We show that combining audio and\nmetadata features improves average precision in misattri-\nbution and duplicate detection by 10% and 6% respec-\ntively.\n“In the wild” experiments with music catalog cura-\ntion Subject Matter Experts (SMEs) show that our system\nachieves over 77% precision on misattribution detection,\nover 71% precision on duplicate detection, and 45% pre-\ncision on ﬁnding the correct relocation of misattributed re-\nleases. Together these results demonstrate the power of\nproactive catalog correction systems in assisting human-\nled curation efforts.6052. RELATED WORK\nRecent advances in audio feature representation using deep\nlearning [8] have applications to recommendations [7], au-\ndio classiﬁcation [4] and artist identiﬁcation [4, 6, 9, 10].\nThese works typically focus on the audio and do not in-\nclude additional information (the method in [9] uses genre\nin its negative sampling method, but the model takes only\naudio). Work in other Named Entity Disambiguation\n(NED) applications shows that combining learned feature\nrepresentations and manually crafted diverse features out-\nperforms using either in isolation [11, 12]. This suggests\nthat combining multiple data types (e.g. content and meta-\ndata) can improve the performance of music NED systems.\nDuplicate entity detection (also known as entity match-\ning or entity resolution) across or within databases typi-\ncally has a blocking step [13] optimised for recall to re-\nduce the set of pairwise comparisons, followed by an en-\ntity matching step optimised for precision. If labelled pairs\nof entities are available, supervised machine learning ap-\nproaches can be used for matching. These are typically\nbased on various string-based similarity features, such as\nentity name similarity [14].\nAlthough state-of-the-art NED research focuses on au-\ntomation [1], a human-in-the-loop (HITL) paradigm is\ncommonly used in practice. A HITL approach is useful\nfor resolving highly ambiguous cases and correcting au-\ntomated decisions. In [3] the authors describe a machine\nlearning approach that optimises human effort spent on la-\nbelling for author disambiguation. In the Microsoft Aca-\ndemic Graph [2], the author disambiguation system uses\ncrowdsourced data as supervision signals.\nCrowdsourced and authoritative sources such as Mu-\nsicBrainz [15], VIAF [16], Wikidata [17], or ISNI [18] are\nuseful for artist name disambiguation, but their beneﬁt is\nlimited for artists in the long tail or for brand new releases\nwithout unique artist identiﬁers.\n3. METHODOLOGY\nOur system operates on music releases (i.e. albums) de-\nnoted as a, and on artist credits in them. The set of re-\nleases credited to an artist forms the artist’s discography:\nA={ai,a2,...}. The objective of our system is two-\nfold:\nCorrect discographies Every release within a discogra-\nphy should credit the same real-world artist; i.e.\nthere is no misattribution in the discography.\nComplete discographies A real-world artist’s releases\nshould not be split across multiple discographies; i.e.\nthere should be no more than one discography per\nartist.\nFigure 1 illustrates our approach to achieve these goals;\nwe achieve correctness and completeness by relocating\nmisattributed releases and resolving (i.e. merging) du-\nplicate discographies. Note that there are cases where a\nsingle real person performs under distinct artist identities\n(e.g. Dan Snaith performs as Caribou and Daphni). These\na3\n a3 a4 a5a1 a2Discography\na1 a2Discography(a) Misattribution Detection\n(b) Deduplication / Relocationa4 a5Discography Discography\nDiscographya1 a2 a3Discography\na4 a5Discography\nFigure 1 :System Overview: (a) Misattribution detection\nis performed on each discography A. The misattributed\nreleasea3is split out from A1into sub-discography A∗\n1.\n(b) All (sub-)discographies are considered for deduplica-\ntion;A∗\n1is merged into A2, relocating the misattributed\nreleases into the correct discography.\ndiscographies should not be considered duplicates. In ad-\ndition, some releases can belong to multiple discographies\nif they credit multiple distinct artists (e.g. collaborations\nand remixes); however, a discography should always con-\ntain releases under a common artist.\n3.1 Misattribution Detection\nThe misattribution detection method, illustrated in Fig. 2,\nprocesses an artist’s attributed discography Ain two\nstages: First, we obtain a distance dist(ai,aj)between all\npairs of releases ai,aj∈ A using the combination of audio\nand metadata signals in Table 1. Second, we partition A\nusing this distance by constructing a Minimum Spanning\nTree (MST) [19] and imposing a threshold θdist. When\nwe cut the MST edges where dist(ai,aj)> θdist, the\nremaining connected components should contain releases\nfrom the same artist. These partitions are disjoint subsets:\nAi⊆ A,i= 1...m , for which all releases belong to the\nsame real-world artist. If the cardinality of the partition\nism >1, then there is at least one misattributed release\nin the discography (i.e. more than one artist’s content is\ndetected) and the discography should be split.\n3.1.1 Pairwise Model\nTo obtain the pairwise distance between releases in a\ndiscography, we train a Random Forest ensemble classi-\nﬁer [20]dist :A×A → (0,1], where high values indicate\nthat the releases are likely to be from different artists.\nData. The training data consists of ∼45K release pairs\nfrom∼28K artist discographies. This data, which we call\ntheRelocations dataset, contains historical corrections of\nartist misattributions. The genres of the releases in this data\nare representative of Spotify’s catalog. Each relocation is\na move of an incorrectly-placed release from an artist’s\ndiscography to the correct one. To construct the trainingProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n606a1 a2 a3a1 a2 a3\na1\na2\na30.05\n0.850.95\n0.850.05\n0.95a1a2\na30.050.85\na3 a1 a2\n(a) The artist discography containing \nmisattribution(b) Compute pairwise distances (c) Compute Minimum Spanning Tree (d) Threshold Mini mum Spanning Tree\nFigure 2 :Misattribution detection :(a):An artist discography A={a1,a2,a3}in which release a3is misattributed. (b):\nThe pairwise distance matrix Dcomputed using our model. (c):A Minimum Spanning Tree (MST) is computed from the\ndistances. (d):After applying a threshold θdistto the MST, the discography Ais split into two partitions, which correspond\nto the two distinct real world artists present in the discography.\ndata, consider a release ai\n1that was moved from discog-\nraphyAjtoAi. We pair ai\n1with a release aj\n1∈ Ajfrom\nthe discography where it was incorrectly located: (ai\n1, aj\n1),\nand give it the “mismatched” label. Then, we pair ai\n1with\na release from the correct discography: (ai\n1, ai\n2),ai\n2∈ Ai,\nand give it the “not mismatched” label.\nModel Features. We use a combination of metadata\nand audio-based features, summarised in Table 1. Audio\nfeatures include deep acoustic embeddings from a propri-\netary model trained in a fashion similar to [7], originally\ndeveloped for music recommendations, and speechiness -\na probability that a track contains spoken word as deter-\nmined by another proprietary model [21]. An advantage of\naudio features is that they are available for every release. In\ngeneral, we expect releases from the same artist to sound\nsimilar to each other. As mentioned in Sec. 2, previous\nworks report good performance using audio-based meth-\nods alone [4,9,10]. However, releases from different artists\ncan also sound similar (e.g. if they come from the same\ngenre), and releases from the same artist can be musically\ndifferent (e.g. an artist whose style evolved or spans many\ngenres).\nOn the other hand, metadata features such as music la-\nbels, composers or lyricists can have high precision (e.g.\nreleases from the same discography delivered by the same\nlabel are likely to be by the same artist), but in isolation\nmetadata matches can be sparse, or have mistakes. There-\nfore, we supplement audio similarity with metadata based\nfeatures to improve the performance of our classiﬁer.\n3.1.2 Grouping releases in a discography\nOur distance allows comparisons between individual pairs\nof releases to decide whether they belong to distinct artists.\nFor example, if dist(ai, aj)> θdistfor a given θdist∈\n(0,1], we could say that it is unlikely that the releases share\nan artist. However, this comparison ignores the context of\nthe whole discography A, and may fail when the sound of\nan artist has evolved in time, the artists changed collabo-\nrators or labels throughout their career. To mitigate these\n2The Dice score is the average of the Dice coefﬁcient [22] for n-gram\nvalues of 1,2,3 and 4.\n3Indicates whether the pair of releases have been identiﬁed by other\nsystems as duplicates\n4Number of pairs of artists with Dice score > 0.7Attribute Functions\nMusic Label∗Exact Match∗, Dice Score2\nMusic Licensor∗Exact Match\nMusic Source∗Exact Match\nRelease Name Exact Match, Dice Score\nRelease Group∗3Exact Match\nRelease Artists Overlap, Dice Overlap4\nRelease Track Names∗At Least 1 Exact Match, Min\nDice Score\nRelease Track Artists Max Overlap ,\nMax Dice Overlap\nRelease Track Language∗At Least One Exact Match\nRelease Type†∗Categorical\nRelease Is Remix†Categorical\nAt Least One Track Is\nRemix†∗Categorical\nTrack Audio Vectors∗Min/Max /Mean Cosine Sim-\nilarity\nTrack Speechiness†Min/Max/Mean\nTable 1 :Pairwise Model Inputs . The features above the\nline are metadata, and below are audio-based. Features\nwith∗were included in the model for the SME experi-\nment. Track level attributes are aggregated to release level\nwith the functions described. Attributes with†produce two\nfeatures, one for each release. Random permutations of\nunderlined feature values decreased test-set performance\n>95% of the time.\nissues, we consider each comparison in the context of all\nthe releases in A.\nWe construct the matrix D∈Rm×mwhereDij=\ndist(ai, aj), and use it to obtain a MST, which is a graph\nwith node set A, and edges with weight equal to the nodes’\npairwise distance (see Fig. 2c). The MST connects releases\nthat are “close” to each other, and provides a global sum-\nmary of how the releases are organised in a latent space,\nwhile capturing the continuity of the data arising from evo-\nlution in the style and career of an artist. We can attribute\ntwo dissimilar releases to the same artist if there is a path\nof short hops along the MST that connects them. Put an-\nother way, if we cut very long hops (i.e. long edges) in the\nMST, we get connected components in which we can only\ngo between nodes by a series of short hops. Our hypothesis\nis that these components (partitions of A) are releases thatProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n607are likely to be from the same artist. Speciﬁcally, we need\nto ﬁnd a threshold θdistand cut all edges in the MST that\nare larger. The remaining connected components preserve\ntransitive relations even when the distance is not transitive:\nifdist(ai, aj)is low and dist(aj, ak)is low,dist(ai, ak)\ncan still be high, but one can traverse from aitoakwith\nshort hops via aj. This approach preserves the diversity\nof releases over the careers of artists. If no edge is larger\nthanθdist, then the MST connects all releases with paths\nof short hops, and we assume that they are all correctly\nattributed to the same artist.\n3.2 Discography Deduplication\nThe goal of deduplication is to merge existing discogra-\nphies, or sub-discographies split out from misattribution\ndetection, that belong to the same artist (e.g. release a3in\nFig. 1). Deduplication consists of two steps: (1) generating\ncandidates for deduplication through a blocking strategy,\nand (2) a prediction step that determines whether the pair\nof discographies belong to the same real-world artist.\n3.2.1 Blocking\nTo reduce the comparisons between pairs of discogra-\nphies while maintaining high recall, we want to create\nsmall blocks of discographies that could belong to the same\nartist. One way is to simply take homonym artist discogra-\nphies as a block; however, errors which lead to misattri-\nbution and duplication in music catalogs are often associ-\nated with varied spellings or aliases of the same real-world\nartist. Therefore, we need a more robust blocking strategy.\nWe build an Elasticsearch [23] index of all artist names\nin the catalog which we use to match and rank dedupli-\ncation candidates. The matching strategy combines three\nconditions: (1) n-grams with n= 2,3,4; (2) fuzzy string\nmatching with edit distance ≤2; and (3) normalised\nstring matching without spaces and stop-words. If one or\nmore of these conditions match a seed discography artist\nname, Elasticsearch returns a list of all matching candi-\ndates ranked by their elastic score [24]. We evaluate this\nstrategy on a dataset of source and target artist name pairs\nfrom the Merges dataset (described below), and obtain a\nrecall@10 of 97%.\n3.2.2 Duplicate detection model\nWe train a Random Forest classiﬁer to compute the similar-\nitysim(Ai,Aj)∈(0,1]between pairs of artist discogra-\nphies within each block. A high similarity score means\nthat the two discographies are likely to come from the same\nreal-world artist and should be merged, while a low score\nindicates that they are from different artists and should re-\nmain separate.\nData. The training data consists of ∼224K discogra-\nphy pairs. This data, which we call the Merges dataset,\ncontains historical corrections of duplicate artist discogra-\nphies. We assign a positive label to each merged pair and\ngenerate up to 10 negative examples for each positive one\nusing the blocking strategy. During training we balance theAttribute Functions\nElasticsearch relevance score See [24]\nArtist name similarity 2-gram Dice coefﬁcient\nRelease Names Jaccard similarity\nRelease Track Names Jaccard similarity\nRelease Artists Overlap between artist names\nof collaborators on releases\nRelease Track Artists Overlap between artist names\nof collaborators on release\ntracks\nNumber of releases |Ai/uniontextAj|\nTrack Audio Vectors Mean Cosine Similarity\nTable 2 :Duplicate Discography Detection Model In-\nputs. Features above the line are metadata, and below are\naudio-based. Random permutations of underlined feature\nvalues decreased test-set performance >95% of the time.\ndata by applying a weight to each sample to be inversely\nproportional to its class frequency.\nModel Features. As in the misattribution model, we\ncombine engineered metadata features with acoustic em-\nbeddings (see Table 2). Duplicate entity detection sys-\ntems typically rely heavily on string similarity, but there\nare some challenges. For example, consider merging the\ndiscography referencing the artist Prince , with the one ref-\nerencing his alias Prince of Funk , while remaining dis-\ntinct from another artist called Princess . Relying solely on\nstring similarity would suggest that the discographies from\nPrince andPrincess are more likely to belong to the same\nartist than the ones from Prince andPrince of Funk . In\nthis scenario, including audio representations in the model\ncan improve performance in the absence of other distinc-\ntive features.\n4. EV ALUATION\nWe evaluate our system’s performance with a series of ex-\nperiments: First, we examine the ofﬂine performance of\neach sub-system under different feature ablations, includ-\ning audio and metadata signals alone, using the Reloca-\ntions andMerges datasets. Second, we conduct three ex-\nperiments with Subject Matter Experts (SMEs) showing\nthe performance “in the wild” of the misattribution and\ndeduplication models, and their uniﬁcation for the reloca-\ntion of misattributed releases, as described in Fig. 1.\n4.1 Audio and Metadata Feature Ablations\nWe test the hypothesis that metadata and learned audio rep-\nresentations model catalog correction tasks (i.e. misattri-\nbution and duplicate detection) better together than sepa-\nrately. Figure 3 shows the performance of the two models\nin three conﬁgurations: audio features only, metadata fea-\ntures only and combined. The features for each model and\nthe distinction between audio-based and metadata-based\nfeatures can be found in Tables 1 and 2. For each set of\nfeatures, we separately tuned the hyperparameters with 5-\nfold cross-validation.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n608(a) (b)\n(c) (d)\nFigure 3 :(a) - (b) : Precision-Recall curves in ofﬂine experiments with combinations of audio and metadata features for\nmisattribution detection (a)and deduplication (b). Average precision (AP) is reported in the legend for each set of features.\n(c) - (d) : Annotation experiment results for misattribution detection (c)and deduplication (d). Precision is calculated for\neach threshold bucket and reweighed by the distribution of predictions shown on the second y axis.\nFigure 3a shows that the pairwise misattribution model\nusing audio-based features alone has good performance,\nbut combining both audio and metadata produces the best\nperformance. The full model has an average precision (AP)\nincrease of 10.69% over the metadata-only model, and\n1.95% AP over the audio-based model. These improve-\nments come from a reduction in false positives (e.g. when\nthe sound is not similar, but metadata similarities exist be-\ntween two releases). For example, the test data contains\nthe releases SHOOT MY SHOT andHurts Like Hell (feat.\nOffset) from the American rapper Offset . The audio-only\nmodel predicts these releases come from different artists\n(their distance is 0.77). The full model gives the pair a dis-\ntance of 0.1 because “Kiari Kendrell Cephus” (which is\nOffset’s real name), appears in the credits of both releases\nas a writer and a composer/lyricist.\nFigure 3b shows the performance of the duplicate de-\ntection task under the different ablations. Using metadata\nfeatures alone outperforms audio features alone by 4% in\nAP. This is not surprising, as entity resolution tasks are\nusually heavily based on string similarity across aligned\nﬁelds. Here too we can achieve good performance with\nmetadata based features alone, but combining the features\nboosts AP by 6%. This boost is driven by cases where\nmetadata features are insufﬁcient. In the example of the\nPrince andPrince of Funk discographies, in the absence\nof shared collaborators or similarity on release titles we\nwould get a false negative. However, the acoustic similar-\nity between the two discographies is high, which allows us\nto correctly identify them as by the same real-world artist.4.2 Experiments with SMEs\nWe conducted three experiments with SMEs to understand\nthe performance of each task independently, and of the en-\ntire correction system (Fig. 1) in the context of its intended\nuse, for a range of decision thresholds. We use precision\nas our evaluation metric since we want to reduce human\neffort spent reviewing and correcting the catalog.\n4.2.1 Misattribution Detection\nWe ran the misattribution detection method from Sec. 3.1\nusing an early version of the pairwise model that was ready\nwhen the SMEs were available. The difference between\nthe full model and this early version is that the latter uses\nonly subset of the features of the full model (marked with\n* in Table 1). We selected a subset of artist discogra-\nphies from the Spotify catalog, biased toward more pop-\nular artists, that reviewers are able to cross-reference ex-\nternally. Then, we randomly sampled a pair of releases\nfrom each artist and calculated the value of the threshold\nθdistthat would split the pair into two different partitions\nof the discography. This value is the largest edge weight\nalong the path connecting the releases in the MST of the\nartist’s discography. In the example in Fig 2c, the thresh-\nold between releases a1anda3would be θdist= 0.85. We\nstratiﬁed our sample by these bucketed threshold values in\n10 equally sized bins between 0 and 1, with a maximum of\n100 pairs per bucket. The sampling produces ∼1K pairs,\neach of which was reviewed by a SME who classiﬁed it as\n“by the same artist” or“by different artists” . Figure 3c\nshows the precision for each value of θdist(blue line, left\ny-axis). For example, at a θdist> 0.7, we can achieve 77%Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n609precision. When θdistis small, many single-artist discogra-\nphies are split into more than one group. This lowers pre-\ncision but increases the fraction of artists that would have\ntheir discography partitioned into more than one group at\neach threshold (grey bars, right y-axis).\n4.2.2 Duplicate Detection\nTo evaluate the duplicate detection model from Sec. 3.2,\nwe generated a list of 140K seed artist discographies of\npopular artists from the catalog. Then, we generated 10\ncandidates for each seed artist using our blocking strategy\nto form artist-candidate pairs. For each pair we compute\nsim(ai,aj), and bucket the scores in the same way as for\nthe misattribution detection task above, sampling up to 100\nper bin. For this task, 3 SMEs reviewed each sample and\nanswered the question: Do the two discographies belong to\nthe same real-world artist? We aggregated the annotations\nper sample to reﬂect the majority vote (i.e. at least 2 out of\n3 of the annotators agree) and got 94% agreement. The re-\nmaining 6% of cases are ambiguous, and were excluded\nfrom the analysis. These cases are interesting and give\ninsight into edge cases for future iterations of the model.\nFor example, when the discographies were related but not\ntechnically by the same artist, e.g. the Thelonious Monk\nQuintet and the Thelonious Monk Quartet .\nAs in the misattribution task, as the threshold θsimin\nFig. 3d increases so does precision, but with fewer candi-\ndate pairs (shown as grey bars, right y-axis). At a θsim>\n0.7, we achieve 71% precision.\n4.2.3 Predicted Relocation\nDiscography pairs that have been reviewed and determined\nto be duplicates can be merged in the catalog in a straight-\nforward way. However, correcting misattributions is not\nso easy, and we still need to identify the correct discog-\nraphy in which they belong. Having validated both steps\nin our discography correction system, we can use the du-\nplicate detection method to predict the correct discography\n(if any) for misattributed releases. To do this, we iden-\ntify misattributions, using θdist> 0.7 based on the previous\nexperiments, and we treat the misattributed releases as a\nsub-discography. Then, we generate and score candidate\nduplicate discographies for these sub-discographies using\nthe deduplication model.\nWe evaluate performance on ∼1K release-discography\npairs. Since the model generates up to ten predictions per\nseed, we take the highest predicted placement as a candi-\ndate for annotation. We asked SMEs to review the release\nand its predicted relocation and answer the question: Does\nthe release belong with the discography?\nFigure 4 shows the precision as a function of the two\nsteps in the correction system θdistandθsim. The highest\nprecision is 45%, which is achieved when both the misattri-\nbution step and deduplication (relocation) step have a high\nθ(top right corner of Fig. 4, representing 17% of the sam-\nple). The relocation task is more difﬁcult and less precise\nbecause it inherits the uncertainty and performance of mis-\nattribution and duplicate detection. Additionally, we ex-\npect that a large number of misattributed releases might not0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nsim0.70.750.80.850.9dist\n0.15 0.20 0.25 0.30 0.35 0.40Precision\nFigure 4 : Precision of the combined system on the task of\npredicted relocation of misattributed releases for varying\nthresholds of the misattribution ( θdist) and duplicate de-\ntection (θsim) methods.\nbelong anywhere, and will become standalone discogra-\nphies. This means that even if the system considered this\nrelocation to be the best out of ten candidates, a reloca-\ntion might not be possible at all. Even in this scenario, the\nhuman effort to detect and correct misattributed content is\nsigniﬁcantly reduced.\n5. DISCUSSION\nWe present a system designed for SMEs to maintain the\ncorrectness and completeness of artist discographies in a\nlarge online catalog. We demonstrate that leveraging both\naudio and metadata-based signals for misattribution detec-\ntion and deduplication of discographies outperforms either\nin isolation. We validated each task separately, and the en-\ntire correction system across different thresholds, showing\nstrong performance in three experiments with SMEs.\nThe power of this system is that it can scan a large cata-\nlog efﬁciently and direct the attention of human reviewers\nto where errors are most likely to be found, as well as sug-\ngest corrections for cases of misattribution and deduplica-\ntion. This makes our system a key part of proactive catalog\ncuration strategies. It is possible that some curation steps\ncould be automated for high conﬁdence predictions; how-\never, due to the downstream impact of curation decisions\n(e.g. recommendations, search, user experience) the toler-\nance for incorrect relocations is low.\nThe current implementation of this system runs weekly,\nand the top-scoring candidates for misattibution, dedupli-\ncation and predicted relocations are ﬂagged for SMEs re-\nview. These reviews, in turn, become new labelled data on\nwhich the model can be re-trained and further improved.\nAlthough discography errors are rare, it is important to\nminimise them as much as possible. Systems such as this\nare one tool among many that streaming platforms can use\nto ensure their catalog is correct, and to safeguard the ex-\nperience of users and artists.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6106. ACKNOWLEDGMENTS\nWe thank Nicola Montecchio and Glenn McDonald for\ndiscussions around the original idea for this work, Dim-\nitrios Korkinof for valuable consultations throughout this\nproject, Dana Puleo for assistance running experiments\nwith SMEs, and Antonio Lima for comments on the\nmanuscript.\n7. REFERENCES\n[1] A. A. Ferreira, M. A. Gonçalves, and A. H. Laender,\n“A brief survey of automatic methods for author name\ndisambiguation,” Acm Sigmod Record , vol. 41, no. 2,\npp. 15–26, 2012.\n[2] K. Wang, Z. Shen, C. Huang, C.-H. Wu, Y . Dong, and\nA. Kanakia, “Microsoft academic graph: When experts\nare not enough,” Quantitative Science Studies , vol. 1,\nno. 1, pp. 396–413, 2020.\n[3] Y . Qian, Y . Hu, J. Cui, Q. Zheng, and Z. Nie, “Combin-\ning machine learning and human judgment in author\ndisambiguation,” in Proceedings of the 20th ACM in-\nternational conference on Information and knowledge\nmanagement , 2011, pp. 1241–1246.\n[4] H. Lee, P. Pham, Y . Largman, and A. Ng, “Unsu-\npervised feature learning for audio classiﬁcation using\nconvolutional deep belief networks,” Advances in neu-\nral information processing systems , vol. 22, 2009.\n[5] A. Brinkman, D. Shanahan, and C. Sapp, “Musical sty-\nlometry, machine learning and attribution studies: A\nsemi-supervised approach to the works of josquin,” in\nProceedings of the Biennial International Conference\non Music Perception and Cognition , 2016, pp. 91–97.\n[6] Z. Nasrullah and Y . Zhao, “Music artist classiﬁca-\ntion with convolutional recurrent neural networks,” in\nInternational Joint Conference on Neural Networks\n(IJCNN) . IEEE, 2019, pp. 1–8.\n[7] A. Van den Oord, S. Dieleman, and B. Schrauwen,\n“Deep content-based music recommendation,” Ad-\nvances in neural information processing systems ,\nvol. 26, 2013.\n[8] E. J. Humphrey, J. P. Bello, and Y . LeCun, “Moving\nbeyond feature design: Deep architectures and auto-\nmatic feature learning in music informatics.” in Inter-\nnational Society for Music Information Retrieval Con-\nference . Citeseer, 2012, pp. 403–408.\n[9] J. Royo-Letelier, R. Hennequin, V .-A. Tran, and\nM. Moussallam, “Disambiguating music artists at scale\nwith audio metric learning,” International Society for\nMusic Information Retrieval Conference , 2018.\n[10] J. Park, J. Lee, J. Park, J.-W. Ha, and J. Nam, “Repre-\nsentation learning of music using artist labels,” Inter-\nnational Society for Music Information Retrieval Con-\nference , 2018.[11] K. Kim, S. Rohatgi, and C. L. Giles, “Hybrid deep pair-\nwise classiﬁcation for author name disambiguation,”\ninProceedings of the 28th ACM International Con-\nference on Information and Knowledge Management ,\n2019, pp. 2369–2372.\n[12] Q. Zhou, W. Chen, W. Wang, J. Xu, and L. Zhao, “Mul-\ntiple features driven author name disambiguation,” in\n2021 IEEE International Conference on Web Services\n(ICWS) . IEEE, 2021, pp. 506–515.\n[13] G. Papadakis, D. Skoutas, E. Thanos, and T. Palpanas,\n“A survey of blocking and ﬁltering techniques for\nentity resolution,” arXiv preprint arXiv:1905.06167 ,\n2019.\n[14] P. V . Konda, Magellan: Toward building entity\nmatching management systems . The University of\nWisconsin-Madison, 2018.\n[15] “Musicbrainz,” musicbrainz.org, accessed: 2023-04-\n14.\n[16] “Virtual international authority ﬁle,” viaf.org, ac-\ncessed: 2023-07-03.\n[17] D. Vrande ˇci´c and M. Krötzsch, “Wikidata: a free\ncollaborative knowledgebase,” Communications of the\nACM , vol. 57, no. 10, pp. 78–85, 2014.\n[18] “International standard name identiﬁer,” isni.org, ac-\ncessed: 2023-07-03.\n[19] J. B. Kruskal, “On the shortest spanning subtree of a\ngraph and the traveling salesman problem,” Proceed-\nings of the American Mathematical society , vol. 7,\nno. 1, pp. 48–50, 1956.\n[20] L. Breiman, “Random forests,” Machine learning ,\nvol. 45, no. 1, pp. 5–32, 2001.\n[21] “Spotify for developers: Get track’s audio fea-\ntures,” developer.spotify.com/documentation/web-api/\nreference/get-audio-features, accessed: 2023-07-03.\n[22] G. Kondrak, “N-gram similarity and distance,” in In-\nternational symposium on string processing and infor-\nmation retrieval . Springer, 2005, pp. 115–126.\n[23] “Elasticsearch,” https://www.elastic.co/, accessed:\n2023-07-06.\n[24] “Lucene’s practical scoring function,” www.\nelastic.co/guide/en/elasticsearch/guide/current/\npractical-scoring-function.html, accessed: 2023-\n04-14.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n611"
    },
    {
        "title": "Music as Flow: A Formal Representation of Hierarchical Processes in Music.",
        "author": [
            "Zeng Ren",
            "Wulfram Gerstner",
            "Martin Rohrmeier"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265365",
        "url": "https://doi.org/10.5281/zenodo.10265365",
        "ee": "https://zenodo.org/records/10265365/files/000074.pdf",
        "abstract": "Modeling the temporal unfolding of musical events and its interpretation in terms of hierarchical relations is a common theme in music theory, cognition, and composition. To faithfully encode such relations, we need an elegant way to represent both the semantics of prolongation, where a single event is elaborated into multiple events, and process, where the connection from one event to another is elaborated into multiple connections. In existing works, trees are used to capture the former and graphs for the latter. Each such model has the potential to either encode relations between events (e.g., an event being a repetition of another), or relations between processes (e.g., two consecutive steps making up a larger skip), but not both together explicitly. To model meaningful relations between musical events and processes and combine the semantic expressiveness of trees and graphs, we propose a structured representation using  algebraic datatype (ADT) with dependent type. We demonstrate its applications towards encoding functional interpretations of harmonic progressions, and large scale organizations of key regions. This paper offers two contributions. First, we provide a novel unifying hierarchical framework for musical processes and events. Second, we provide a structured data type encoding such interpretations, which could facilitate computational approaches in music theory and generation.",
        "zenodo_id": 10265365,
        "dblp_key": "conf/ismir/RenGR23",
        "keywords": [
            "temporal unfolding",
            "musical events",
            "hierarchical relations",
            "semantics of prolongation",
            "process elaboration",
            "existing works",
            "trees",
            "graphs",
            "functional interpretations",
            "harmonic progressions"
        ],
        "content": "MUSIC AS FLOW: A FORMAL REPRESENTATION OF HIERARCHICAL\nPROCESSES IN MUSIC\nZeng Ren\nEPFL\nzeng.ren@epfl.chWulfram Gerstner\nEPFL\nwulfram.gerstner@epfl.chMartin Rohrmeier\nEPFL\nmartin.rohrmeier@epfl.ch\nABSTRACT\nModeling the temporal unfolding of musical events and its\ninterpretation in terms of hierarchical relations is a com-\nmon theme in music theory, cognition, and composition.\nTo faithfully encode such relations, we need an elegant\nway to represent both the semantics of prolongation, where\na single event is elaborated into multiple events, and pro-\ncess, where the connection from one event to another is\nelaborated into multiple connections. In existing works,\ntrees are used to capture the former and graphs for the lat-\nter. Each such model has the potential to either encode\nrelations between events (e.g., an event being a repetition\nof another), or relations between processes (e.g., two con-\nsecutive steps making up a larger skip), but not both to-\ngether explicitly. To model meaningful relations between\nmusical events and processes and combine the semantic\nexpressiveness of trees and graphs, we propose a struc-\ntured representation using algebraic datatype (ADT) with\ndependent type. We demonstrate its applications towards\nencoding functional interpretations of harmonic progres-\nsions, and large scale organizations of key regions. This\npaper offers two contributions. First, we provide a novel\nunifying hierarchical framework for musical processes and\nevents. Second, we provide a structured data type encoding\nsuch interpretations, which could facilitate computational\napproaches in music theory and generation.\n1. INTRODUCTION\nWhen understanding music as a temporal art, there are at\nleast two properties we need to model. The ﬁrst is that\nmusical events are ordered in a nontrivial way resembling\ngoal-directedness; the essence of a piece is lost if we “re-\ncompose” a piece by performing random temporal permu-\ntations. The second phenomenon is the temporal hierarchy,\nwhich is a central theme in the understanding of Western\ntonal music, where we hear multiple entities as the mani-\nfestation of a single musical entity. Regarding this hierar-\nchy, there are at least two kinds of such entities. The ﬁrst\nkind is a stationary process, such as key region, and har-\n© Z. Ren, W. Gerstner, and M. Rohrmeier. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Z. Ren, W. Gerstner, and M. Rohrmeier, “Music as ﬂow:\na formal representation of hierarchical processes in music”, in Proc. of\nthe 24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.mony; we can say a phrase that enforces a key contains\na tonic region (like some presentation in a non-modulating\nsentence) and we can also describe a time span as an arpeg-\ngiation of a harmony. The second kind is a transitory pro-\ncess, such as modulation region, passing, and neighboring\nmotion; a descending third progression contains two step-\nwise downward motions.\nThere are multiple attempts to represent the hierarchical\nstructure of such entities. For stationary entities, trees of\nmusical events have been used to model tonal harmony [1,\n2], extended tonal harmony [3], jazz harmony [4], rhythm\nand meter [2, 5]. One limitation of using trees of musical\nevents is that semantics such as passing tones could not be\nelegantly expressed because one is forced to select either\nthe left or the right parent event for the subordinate event\nwhereas we would like to express an intermediate event\nsubordinate to the melodic motion itself [2].\nFor the transitory process, trees on event transitions are\nalso sometimes used [6]. They could model the semantics\nfor a passing tone by describing how a melodic motion is\nsplit into two motions, one going to the passing note and\none leaving the passing note. However, as the fundamen-\ntal entities are transitions, it can not express the idea that\na single event being elaborated in the temporal dimension,\nsuch as unfoldings, complete neighbor chords/tones, repe-\ntitions, and rearticulations [7].\nThere are attempts using graphical notations to capture\nboth stationary and transitory processes [7–9]. There are\nalso models [10] that extend such hierarchical organiza-\ntions beyond the temporal dimension with inner structures\nof events resembling concurrent processes.\nOne could potentially encode the hierarchical organiza-\ntion between these two kinds of processes implicitly us-\ning networks and graphs, or more expressively using hy-\npergraphs where higher-order relations can be encoded as\nhyper-edge. One could formulate a rewrite grammar on\nsuch networks and hypergraphs to describe the elaboration\nof nodes and edges. However, we believe there should be a\nmore direct, elegant, and specialized solution (in a similar\nspirit as [11]) to not only implement but also characterize\nsuch generative principles of hierarchical processes.\nIn summary, there is a lack of formal representation as\nwell as a specially designed data structure that explicitly\ncaptures the intricate hierarchical organizations of both the\nstationary and transitory processes, a fundamental idea in\nreductive theories of tonal music. This paper offers two\ncontributions. First, we provide a novel unifying hierar-627chical framework for stationary and transitory processes.\nSecond, we provide a structured data type encoding such\ninterpretations, which could facilitate computational ap-\nproaches in music theory, musicology, and algorithmic\nmusic composition.\n2. THE HIERARCHICAL ORGANIZATIONS OF\nGENERAL PROCESSES\nTo demonstrate how these two kinds of processes could be\nhierarchically organized in the temporal dimension, per-\nhaps it is helpful to consider a scenario in everyday life:\n“On his way back to home, John went to the supermarket,\nwhere he got his favorite yogurt from the fridge. Although\nhe could take a bus directly to his house, he decided to\nget off one stop earlier by the lake to enjoy a short walk.”\nOne hierarchical organization of this particular scenario is\ndepicted in Fig.1. The overarching process is that John\nwent back home from someplace. This transitory process\n(represented by the arrow connecting “someplace” denoted\nbyXto “home”) contains three component processes: a\ntransitory process from “someplace” to “supermarket”, a\nstationary process at “supermarket,” and a transitory pro-\ncess from “supermarket” to “home.” The stationary pro-\ncess at “supermarket” further contains a stationary process\nat the “entrance” of the supermarket, a transitory process\nfrom “entrance” to the “exit,” and a stationary process at\nthe “exit”.\n2.1 The syntactic constraint of the hierarchical\norganization of stationary and transitory processes\nOne pattern that we observe is the mutual recursive rela-\ntionship between stationary and transitory processes. A\nstationary process can contain three components (station-\nary, transitory, stationary). Symmetrically, a transitory pro-\ncess can contain three components (transitory, stationary,\ntransitory).\nHowever, it is clear from the above example (Fig. 1)\nwe can not arbitrarily subdivide a stationary process at X\n(denoted by pX), or a transitory process from XtoY(de-\nnoted byXÑYÝ Ý Ý Ý Ñ ) into arbitrary triples of processes, even\nif they conform to the (stationary, transitory, stationary) or\n(transitory, stationary, transitory) patterns. We may allow a\ntransitory processAÑBÝ Ý Ý Ñ to be elaborated into three com-\nponents of the form\nAÑXÝ Ý Ý Ý Ñ pXXÑBÝ Ý Ý Ý Ñ\nBut we would not allow a decomposition like\nCÑDÝ Ý Ý Ý Ñ pXEÑFÝ Ý Ý Ñ\nbecause their states are not compatible.\nWe can summarize the constraints as the following: a\nstationary process pXmay contain ( pX,XÑXÝ Ý Ý Ý Ñ ,pX); like-\nwise, a transitory processXÑYÝ Ý Ý Ý Ñ may contain pXÑZÝ Ý Ý Ý Ñ\n,ˆZ,ZÑYÝ Ý Ý Ñq . The “entrance” and “exit” in the previous ex-\nample, although being technically different, are equivalent\nto “supermarket” from abstract level.3. LINEAR PROCESSES\nWe start with characterizing linear processes representing\na single hierarchical stream.\n3.1 An axiomatic system\nWe refer to a stationary process as Joint , and deﬁne it\nas a predicate Jxindexed by a state x:A. We refer to\na transitory process as Link and deﬁne it as a predicate\nLx,yindexed by two states x,yof the same type. Then we\npropose the following four axioms to characterize the hi-\nerarchical interactions between of stationary and transitory\nprocesses.\n@px:Aq Dpj:Jxq (1)\n@px,y:Aq Dpl:Lx,yq (2)\n@pj,j1:Jxq @pl:Lx,xq Dpj˚:Jxq (3)\n@pl:Lx,zq @pj:Jzq @pl1:Lz,yq Dpl˚:Lx,yq(4)\nAxiom 1 states that we may form a stationary process for a\ngiven state. Axiom 2 states that we may form a transitory\nprocess by for a pair of states of the same kind. Axiom\n3 states that we may form a stationary process at xfrom\nany triple of processes pj,l,j1q, wherejandj1are both\nof stationary processes at xandlis a loop starting and\nending at x. Axiom 4 states that we may form a transitory\nprocess from any triple of processes pl,j,l1q, wherelandl1\nare transitory and jis stationary, provided that their states\nare compatible.\n3.2 A syntax based on dependent type theory\nUsing two mutually inductive algebraic datatypes, Joint\nandLink , we formalize the notion of hierarchical process\nin Backus–Naur form (Eq. 5,6,7,8). For a stationary pro-\ncess, the base case (Eq. 5) of a Joint is aPoint , which\nmeans an atomic stationary process, whereas the inductive\ncase (Eq. 6) resembles a stationary process (on the current\nlevel) containing two stationary process and a transitory\nprocess. The base case of a Link is aUnit (Eq. 7), repre-\nsenting a indivisible change of state, whereas the inductive\ncase (Eq. 8) represents a composite motion that contains\ntwo changes and one stationary process. Eq. (5,6,7,8) cor-\nresponnds to Axiom. (1,3,2,4) respectively.\nJoint px:aq:“xPoint y px:aq (5)\n| xJoint yJointxLinkx,xJointx\n(6)\nLink px:aq,py:aq:“xUnit y px:aq py:aq (7)\n| xLink yLinkx,zJointzLinkz,y\n(8)\nDependent typing [12] allows us to deﬁne types de-\npending on value. This algebraic data structure with de-\npendent typing has an important application for a gener-\native system. One can deﬁne a function using polymor-\nphic recursion to sample a value of the given type. Do-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n628pXXÑhÝ Ý Ý Ñ {home\npXXÑsÝ Ý Ý Ñ {supermarketsÑhÝ Ý Ý Ñ {home\npXXÑs„enÝ Ý Ý Ý Ý Ý Ñ {entranceenÑexÝ Ý Ý Ý Ñ yexitex„sÑlÝ Ý Ý Ý Ý Ñ ylakelÑhÝ Ý Ý Ñ {home\npXXÑenÝ Ý Ý Ý Ñ {entranceenÑfÝ Ý Ý Ý Ñ {fridgefÑexÝ Ý Ý Ý Ñ yexitexÑlÝ Ý Ý Ñ ylakelÑhÝ Ý Ý Ñ {home\nFigure 1 : A hierarchical interpretation of John’s journey. Words on the transitions are abbreviated to save space. The\nsymbols„enmeans the \"entrance\" is functionally equivalent to \"supermarket\" in the interpretation of this journey\n.\ning so will guarantee the syntactic correctness of the out-\nput. For example, writing a harmonic transition between\nIandVmeans sampling a value of type LinkI,V; elab-\norating a melodic motion from ˆ8toˆ5becomes sampling\na value of type Linkˆ8,ˆ5. Note that between these two\nexamples, the type of the state is different; the ﬁrst is\nharmony(roman-numeral) while the second is scale degree.\nHowever, within each example, the types of the states are\nalways the same by construction (Axiom 2, 3, 4).\n3.3 A data structure in Haskell\nHaskell is a functional programming language with an ex-\npressive type system [13]. Although the dependent type\nis not yet built into the language, there exists an encoding\ninvolving Algebraic Datatype and Singletons [14] to sim-\nulate the behavior of dependent type. The implementation\nof the linear process is shown below1.\ndataJoint(x::a)\n=Point(Singx)\n|Joint(Jointx) (Linkx x) (Jointx)\ndataLink(x::a) (y::a)\n=Unit(Singx) (Singy)\n|forall(z::a). Link(Linkx z) (Jointz) (Linkz y)\n3.4 A graphical notation system\nTo visualize an interpretation of hierarchical processes, we\nuse two types of slurs to connect states in a sequence. For\naPoint , the base case of Joint , no visual representation\nis needed as the state it expresses is sufﬁcient. For a Unit ,\nthe base case of Link , a dashed slur is drawn connecting\nthe starting state to the ending state. Nontrivial stationary\nprocesses (the inductive cases) are represented as continu-\nous slurs whereas transitory processes are represented with\ndashed slur. For a non-trivial processes, the left/right an-\nchor point of the slur is the same as the left/right anchor\npoint of the slur of the process’s ﬁrst/last component.\nFive simple examples of such notation are provided in\nFig. 2. The easiest to understand is Ex. D, which is a\nstationary process pI, decomposed into a pIIÑIÝ Ý Ý Ñ pI. This\nexpresses not just repetition but also the loop from Ito it-\nself. This loop can be the parent for further elaborations\n1Specifying the singleton arguments can be sometimes redundant and\ncumbersome. Instead, one could use the \"implicit-passed\" singleton by\nreplacing the argument “Sing x\" by a constraint “SingI x\". In this way,\nthe “Point\" and “Unit\" constructor takes no explicit argument and the\nstate information is thus encoded as a phantom type that can be pattern\nmatched using type application such as “Point @IV\".such as passing and neighbor chords. Note that the contin-\nuous slur covers a point, a dashed slur, and a point. This\nvisual \"covering\" is intend to convey the hierarchical rela-\ntion of processes where parent processes contains its com-\nponent processes. Ex. C shows a more complex situa-\ntion; although the top two level is the same as Ex. D (a\nJoint containing two base case Joint and aLink ), there\nare richer structures within the Link in Ex. C. This dashed\nslur covers a dashed slur, a continuous slur, and a dashed\nslur, signaling a non-trivial transitory process that contains\na transitory processI VÝ Ý Ý Ý Ñ , a stationary process at V, and\na transitory processV IÝ Ý Ý Ý Ñ . This stationary process at Vis\nalso a non-trivial one, capturing the sense of prolongation\nwithin which passing notes can be generated connecting\nits chordal pitches, forming a harmonic the entity V7´6´5\n5´4´3.\nEx. B expresses an overarching stationary process at Icon-\ntaining a nontrivial initial stationary process, a nontrivial\ntransitory process and a trivial stationary process. The ini-\ntial stationary process resembles a neighbor-passing chord\nwith in the harmony of I. Note that there is a difference in\nsemantic in drawing the continuous slur over the ﬁrst two\nIs vs the last two Is. The former means that the overarch-\ning motion is stationary ﬁrst and then moving to the target\nIchord, whereas the latter means the motion moves ﬁrst\nand then performs a stationary rest. This kind of ﬁne dis-\ntinction could be used to further express embodied musi-\ncal concepts such as “momentum,” “potential energy,” and\n“forces” [15]. Ex. A presents two interpretations of the\nsame chord progression, where the top and bottom analy-\nsis reﬂects the melody and the bass respectively.\n4. APPLICATIONS IN MUSIC ANALYSIS\nWe will demonstrate the potential usage of the proposed\nmodel in reductive analysis in three different levels of com-\nplexity.\n4.1 The harmonic sequence of a hybrid theme type\nFor a harmonic sequence I´V´V´I´IV´V´I\nin an 8-bar phrase, we may interpret the ﬁrst four chords\nas a tonic prolongation and the rest as an incomplete ca-\ndential progression, as in an antecedent + cadential hybrid\ntheme type [16]. Such an interpretation is captured by the\nderivation process in Fig. 3.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n629Figure 2 : Examples of the graphic notation for Hierarchical processes where continuous slurs represent Joint and dashed\nslurs represent Link .\npI\npIIÑIÝ Ý Ý Ñ pI\npIIÑIÝ Ý Ý Ñ pIIÑVÝ Ý Ý Ñ pVVÑIÝ Ý Ý Ñ pI\npIIÑVÝ Ý Ý Ñ pVVÑIÝ Ý Ý Ñ pIIÑIVÝ Ý Ý Ý Ñ xIVIVÑVÝ Ý Ý Ý Ñ pVVÑIÝ Ý Ý Ñ pI\nFigure 3 : One derivation process for the harmonic se-\nquenceI´V´V´I´IV´V´I. The ﬁrst three steps\nare elaboration of types whereas the last step is the instan-\ntiations of the types (in the framework of formal grammar,\nthis corresponds to rules generating nonterminal and ter-\nminal symbols)\n4.2 The key-level modulation analysis of a simple\nternary form\nNow we present an analysis of a section of Haydn, Pi-\nano Sonata in D, H.37, iii, using the proposed model (Fig.\n4). The main focus of the analysis here is on key cen-\nter and functional harmony. This overall section resem-\nbles a stationary process at the home key region. It con-\ntains a stationary process expressing the home key region,\nfollowed by a transitory process from to a home key re-\ngion to another home key region. Within this transitory\nprocess, there is a stationary process at the dominant key\nregion, as well as the two transitory processes function-\ning as key transition. The ﬁrst connects the tonic chord\nin the home key ItIuto the tonic chord in the dominant\nkeyVtI6uas the pivot chord for modulation. The second\nconnects ItVutoItIuas to signal the return to the home\nkey. In a prolongational framework of pitch reduction, this\nlink at mm.12 is the interrupted motion in a typical inter-\nruption, implying the restart of the fundamental line [8].\nIn a functional harmony framework, this same link is the\ndominant to tonic preparation relation, also implying the\narrival of the tonic in the home key. Within the domi-\nnant region the ﬁrst stationary process corresponds to the\ncomplete cadential progression signaling the stabilization\nof the dominant key, whereas the following transitory pro-cessˆ5Ñ||ˆ1Ý Ý Ý Ý Ý Ý Ý Ý Ñ\nVtIuÑ||ItVucorresponds to ﬁrst attempted descent\n(interrupted) of the fundamental line accompanied by the\n“Ponte” modulation schema [17] that gradually change the\nunderlying key of a chord.\n4.3 A harmonic analysis of a Bach chorale\nA more elaborated example of such harmonic organiza-\ntion can be found in Bach Chorale No 9 BWV 248 (Fig.\n5). This analysis interprets the overarching structure of the\npiece as a stationary process in the home key tonic, con-\ntaining an establishment of the home key, detour around\nthe home key, and the re-stablization of the home key. This\ntransitory processˆ1Ñˆ8Ý Ý Ý Ý Ý Ý Ý Ñ\nItIuÑItIuenforces its unity with an as-\ncending linear progression of an octave, within which the\nmusic modulates to the relative minor ( vi) via its domi-\nnant minor key ( V{vi). The ﬁrst stationary process around\nthe home key is elaborated into a I´V´Iternary-like\nstructure on the key level.\n5. CONCURRENT PROCESSES\nBesides providing a formalization for processes in the tem-\nporal dimension, we also offer an extension to model the\nprocesses that are simultaneous, which enables us to model\npolyphonic texture.\n5.1 The Formalism\nTo model concurrent processes, we add two non-\ncommutative binary2operations to construct Joint and\nLink respectively:\np8q:JointxÑJointyÑJoint px,yq (9)\npòq:Linkx,yÑLinkx1,y1ÑLink px,x1q,py,y1q(10)\nThe ﬁrst constructor 8, called “ when ”, expresses the con-\ncurrency of two stationary processes. The second construc-\ntorò, called “ while ”, expresses the concurrency of two\ntransitory processes. Applying these operators using inﬁx\n2Although these operations are currently formulated as binary opera-\ntions, they can be naturally extended to n-nary versions where the input\nis a vector of Joint andLink respectively.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n630notations, we have Eq. 11, 12:\npx8py:“zpx,yq (11)\nxÑyÝ Ý Ý Ñòx1Ñy1\nÝ Ý Ý Ý Ñ:“px,x1qÑpy,y1qÝ Ý Ý Ý Ý Ý Ý Ý Ñ (12)\nIn addition we add an algebraic law (Eq. 13) on these two\noperations:\nppx8pyqpxÑxÝ Ý Ý ÑòyÑyÝ Ý Ý Ñqp px8pyq\n“ (13)\nppxxÑxÝ Ý Ý Ñ pxq8ppyyÑyÝ Ý Ý Ñ pyq\nTo model temporal displacement of concurrent pro-\ncesses (like the suspension in fourth species counterpoint)\nwe deﬁne a function ÒÒcalled “ leads ” that is derivable\nfrom the basic operations in terms of Eq. 9,10 :\npÒÒq:Linkx,yÑLinkx1,y1ÑLink px,x1q,py,y1q\nxÑyÝ Ý Ý ÑÒ Òx1Ñy1\nÝ Ý Ý Ý Ñ “px,x1qÑpy,x1qÝ Ý Ý Ý Ý Ý Ý Ý Ñ{py,x1qpy,x1qÑpy,y1qÝ Ý Ý Ý Ý Ý Ý Ý Ñ\n(14)\n“ pxÑyÝ Ý Ý Ñòx1Ñx1\nÝ Ý Ý Ý Ñqp py8px1qpyÑyÝ Ý Ý Ñòx1Ñy1\nÝ Ý Ý Ý Ñq\n(15)\nWith Eq. 14, we formalize the relation about two pro-\ncesses where that one process leads the other. For example,\nin a typical suspension, this allows us to capture the mean-\ning that the bass motion leads the melody motion, causing\na consonance-dissonance-consonance pattern.\n5.2 Concurrent processes in contrapuntal textures\nNow we demonstrate that using the proposed formalism,\nwe can model many complex hierarchical coordination of\nprocesses in both temporal (horizontal) and spatial (verti-\ncal) dimensions.\n5.2.1 First species counterpoint\nFor ﬁrst species counterpoint, processes are always verti-\ncally aligned in a one-to-one fashion.\n3´2´1\n1´7´1\ncan be modeled as:\nzp3,1q´`3Ñ2Ý Ý Ý Ñò1Ñ7Ý Ý Ý Ñ˘zp2,7q`2Ñ1Ý Ý Ý Ñò7Ñ1Ý Ý Ý Ñ˘¯\nzp1,1q\n5.2.2 Second species counterpoint\nFor second species counterpoint, we encounter concurrent\nprocesses where one is more elaborated than the other. A\nsegment of such texture\n5´4´3\n7´1can be modeled as\nzp5,7q`\np5Ñ4Ý Ý Ý Ñ p44Ñ3Ý Ý Ý Ñq ò p7Ñ1Ý Ý Ý Ñq˘zp3,1q\nNotice that the two-against-one coordination of the motion\nis reﬂected in the structure of the encoding and we do not\nneed to \"break\" the ˆ7into two copies of ˆ7to convert it into\nﬁrst species texture.\n5.2.3 Third species counterpoint\nThird species counterpoint is a more elaborated version of\nthe second species but the form of the representation is\nvery similar.\n5.2.4 Fourth species counterpoint\nFourth species counterpoint presents the opportunity of\nsuspension. We can generalize such textures as overlay-\ning transitory processes in an alternating manner, creating\ntemporal displacement. Consider this 7-6 suspension (“=”\nrepresents “tied-over”)\n3“3´2“2´1\n5´4“4´3“3\nIt can be modeled using Eq. 14 as the following:\nzp3,5q´`5Ñ4Ý Ý Ý ÑÒ Ò3Ñ2Ý Ý Ý Ñ˘zp2,4q`4Ñ3Ý Ý Ý ÑÒ Ò2Ñ1Ý Ý Ý Ñ˘¯\nzp1,3q\n6. DISCUSSION\nThe contribution of this paper is to offer a characteriza-\ntion and representation on the hierarchical organization of\nboth stationary and transitory musical processes as well as\nhow they can be concurrently structured. Linear processes\nare modeled using two mutually inductive types Joint and\nLink . Concurrent processes are modeled on top of linear\nprocesses by adding two binary operations for Joint and\nLink respectively. In addition, an algebraic law is imposed\non these two operators to express an isomorphism between\nthe horizontal view and the vertical view. We introduced a\ngraphical notation for linear processes and presented sev-\neral harmonic analysis using the notation to demonstrate\nthe music analytical application of the characterization of\nlinear processes. To demonstrate the analytical applica-\ntion of the concurrent processes, we presented their cor-\nresponding encoding for contrapuntal textures in species\ncounterpoint.\nThis general formalism can be ﬂexible to adapt to spe-\nciﬁc music theoretical constraints. One might encode spe-\ncialized elaboration rules by equipping the Link construc-\ntor with domain speciﬁc constraints on the type level and\nreuse the function itself. In Eq. 8, such constraint could be\na predicate on the type variables px,z,y3.\n3Upper neighbor elaboration, for example, corresponds to the predi-\ncatepx,z,y “ px“y,z “Òxq. Likewise, downward passing elabora-\ntion corresponds to the predicate px,z,y “ pz“Óx,y “Ózq.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n631Figure 4 : A prolongational analysis of Haydn, Piano Sonata in D, H.37, iii, mm. 1-20 using the notation of hierarchical process. Key regions are indicated by roman numerals\nfollowed by curly brackets.\nFigure 5 : An analysis of Bach Chorale No 9 BWV 248.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6327. ACKNOWLEDGEMENT\nThis project has received funding from the European Re-\nsearch Council (ERC) under the European Union’s Hori-\nzon 2020 research and innovation program under grant\nagreement No 760081 – PMSB. We thank Claude Latour\nfor supporting this research through the Latour Chair in\nDigital Musicology.\nWe thank the team of the Digital and Cognitive Musi-\ncology Lab (DCML), particularly Gabriele Cecchetti and\nXinyi Guan, for their helpful comments on earlier versions\nof this paper. Furthermore, we thank Yannis Rammos and\nChristoph Finkensiep for fruitful discussions on the mu-\nsic interpretations and implementation of the model in this\npaper.\n8. REFERENCES\n[1] M. Rohrmeier, “Towards a generative syntax of tonal\nharmony,” Journal of Mathematics and Music , vol. 5,\nno. 1, pp. 35–53, 2011.\n[2] F. Lerdahl and R. S. Jackendoff, A Generative Theory\nof Tonal Music, reissue, with a new preface . MIT\npress, 1996.\n[3] M. Rohrmeier and F. C. Moss, “A formal model of\nextended tonal harmony,” in Proceedings of the 22nd\nInternational Society for Music Information Retrieval\nConference , no. CONF, 2021, pp. 569–578.\n[4] M. Rohrmeier, “The syntax of jazz harmony: Diatonic\ntonality, phrase structure, and form,” Music Theory and\nAnalysis (MTA) , vol. 7, no. 1, pp. 1–63, 2020.\n[5] ——, “Towards a formalization of musical rhythm.” in\nISMIR , 2020, pp. 621–629.\n[6] É. Gilbert and D. Conklin, “A probabilistic context-free\ngrammar for melodic reduction,” in Proceedings of the\nInternational Workshop on Artiﬁcial Intelligence and\nMusic, 20th International Joint Conference on Artiﬁ-\ncial Intelligence , 2007, pp. 83–94.\n[7] P. Westergaard, An introduction to tonal theory . Nor-\nton New York, 1975.\n[8] H. Schenker, Free Composition: Volume III of new mu-\nsical theories and fantasies . Pendragon Press, 2001,\nvol. 1.\n[9] J. Yust, Organized time: rhythm, tonality, and form .\nOxford University Press, 2018.\n[10] C. Finkensiep, “The structure of free polyphony,” p.\n321, 2023. [Online]. Available: http://infoscience.epﬂ.\nch/record/300206\n[11] A. Mokhov, “Algebraic graphs with class (functional\npearl),” SIGPLAN Not. , vol. 52, no. 10, p. 2–13,\nsep 2017. [Online]. Available: https://doi.org/10.1145/\n3156695.3122956[12] M. Hofmann and M. Hofmann, “Syntax and semantics\nof dependent types,” Extensional Constructs in Inten-\nsional Type Theory , pp. 13–54, 1997.\n[13] S. Marlow et al. , “Haskell 2010 language report,”\nAvailable online http://www. haskell. org/(May 2011) ,\n2010.\n[14] R. A. Eisenberg and S. Weirich, “Dependently typed\nprogramming with singletons,” ACM SIGPLAN No-\ntices, vol. 47, no. 12, pp. 117–130, 2012.\n[15] S. Larson, Musical forces: Motion, metaphor, and\nmeaning in music . Indiana University Press, 2012.\n[16] W. E. Caplin, Analyzing classical form: An approach\nfor the classroom . Oxford University Press, USA,\n2013.\n[17] R. Gjerdingen, Music in the Galant Style . OUP USA,\n2007.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n633"
    },
    {
        "title": "FiloBass: A Dataset and Corpus Based Study of Jazz Basslines.",
        "author": [
            "Xavier Riley",
            "Simon Dixon"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10069709",
        "url": "https://doi.org/10.5281/zenodo.10069709",
        "abstract": "Dataset to accompany the paper \"FiloBass: A Dataset and Corpus Based Study of Jazz Basslines\" which was published at ISMIR 2023.",
        "zenodo_id": 10069709,
        "dblp_key": "conf/ismir/RileyD23",
        "keywords": [
            "FiloBass",
            "Dataset",
            "Corpus",
            "Study",
            "Jazz Basslines",
            "ISMIR 2023",
            "Paper",
            "Publication",
            "Abstract",
            "Key Aspects"
        ],
        "ee": "https://zenodo.org/records/10069709/files/FiloBass_v1.0.0.zip"
    },
    {
        "title": "PESTO: Pitch Estimation With Self-Supervised Transposition-Equivariant Objective.",
        "author": [
            "Alain Riou",
            "Stefan Lattner",
            "Gaëtan Hadjeres",
            "Geoffroy Peeters"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265343",
        "url": "https://doi.org/10.5281/zenodo.10265343",
        "ee": "https://zenodo.org/records/10265343/files/000063.pdf",
        "abstract": "In this paper, we address the problem of pitch estimation using self-supervised learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being trained only on a small unlabeled dataset.\n\nWe use a lightweight ( 30k parameters) Siamese neural network that takes as inputs two different pitch-shifted versions of the same audio represented by its constant-Q transform. To prevent the model from collapsing in an encoder-only setting, we propose a novel class-based transposition-equivariant objective which captures pitch information. Furthermore, we design the architecture of our network to be transposition-preserving by introducing learnable Toeplitz matrices.\n\nWe evaluate our model for the two tasks of singing voice and musical instrument pitch estimation and show that our model is able to generalize across tasks and datasets while being lightweight, hence remaining compatible with low-resource devices and suitable for real-time applications. In particular, our results surpass self-supervised baselines and narrow the performance gap between self-supervised and supervised methods for pitch estimation.",
        "zenodo_id": 10265343,
        "dblp_key": "conf/ismir/RiouLHP23",
        "keywords": [
            "self-supervised learning",
            "equivariance to pitch transposition",
            "monophonic audio",
            "pitch estimation",
            "Siamese neural network",
            "constant-Q transform",
            "class-based transposition-equivariant objective",
            "learnable Toeplitz matrices",
            "singing voice",
            "musical instrument pitch estimation"
        ],
        "content": "PESTO: PITCH ESTIMATION WITH SELF-SUPERVISED\nTRANSPOSITION-EQUIV ARIANT OBJECTIVE\nAlain Riou1,2Stefan Lattner2Gaëtan Hadjeres3Geoffroy Peeters1\n1LTCI, Télécom-Paris, Institut Polytechnique de Paris, France\n2Sony Computer Science Laboratories - Paris, France\n3Sony AI\nalain.riou@sony.com\nABSTRACT\nIn this paper, we address the problem of pitch estimation\nusing Self Supervised Learning (SSL). The SSL paradigm\nwe use is equivariance to pitch transposition, which en-\nables our model to accurately perform pitch estimation on\nmonophonic audio after being trained only on a small un-\nlabeled dataset. We use a lightweight ( <30k parameters)\nSiamese neural network that takes as inputs two differ-\nent pitch-shifted versions of the same audio represented\nby its Constant-Q Transform. To prevent the model from\ncollapsing in an encoder-only setting, we propose a novel\nclass-based transposition-equivariant objective which cap-\ntures pitch information. Furthermore, we design the archi-\ntecture of our network to be transposition-preserving by\nintroducing learnable Toeplitz matrices.\nWe evaluate our model for the two tasks of singing voice\nand musical instrument pitch estimation and show that our\nmodel is able to generalize across tasks and datasets while\nbeing lightweight, hence remaining compatible with low-\nresource devices and suitable for real-time applications. In\nparticular, our results surpass self-supervised baselines and\nnarrow the performance gap between self-supervised and\nsupervised methods for pitch estimation.\n1. INTRODUCTION\nPitch estimation is a fundamental task in audio analysis,\nwith numerous applications, e.g. in Music Information Re-\ntrieval (MIR) and speech processing. It involves estimat-\ning the fundamental frequency of a sound, which allows to\nestimate its perceived pitch. Over the years, various tech-\nniques have been developed for pitch estimation, ranging\nfrom classical methods (based on signal processing) [1–4]\nto machine learning approaches [5, 6].\nIn recent years, deep learning has emerged as a pow-\nerful tool for a wide range of applications, outperforming\nclassical methods in many domains. This is notably true in\n© A. Riou, S. Lattner, G. Hadjeres and G. Peeters. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: A. Riou, S. Lattner, G. Hadjeres and G.\nPeeters, “PESTO: Pitch Estimation with Self-supervised Transposition-\nequivariant Objective”, in Proc. of the 24th Int. Society for Music Infor-\nmation Retrieval Conf., Milan, Italy, 2023.MIR, where deep learning has led to signiﬁcant advances\nin tasks such as music transcription [7–9], genre classiﬁ-\ncation [10–12], and instrument recognition [13–15]. Pitch\nestimation has also beneﬁted greatly from deep learning\ntechniques [16, 17]. However, these deep learning mod-\nels often require a large amount of labelled data to be\ntrained, and can be computationally expensive, hindering\ntheir practical applications in devices with limited com-\nputing power and memory capabilities. Additionally, these\nmodels are often task-speciﬁc and may not generalize well\nto different datasets or tasks [18]. Therefore, there is a need\nfor a lightweight and generic model that does not require\nlabelled data to be trained. We address this here.\nWe take inspiration from the equivariant pitch estima-\ntion [19] and the equivariant tempo estimation [20] algo-\nrithms which we describe in part 2. As those, we use a SSL\nparadigm based on Siamese networks and equivariance to\npitch transpositions (comparing two versions of the same\nsound that have been transposed by a random but known\npitch shift). We introduce a new equivariance loss that en-\nforces the model to capture pitch information speciﬁcally.\nThis work has the following contributions :\n• we formulate pitch estimation as a multi-class prob-\nlem (part 3.1); while [19, 20] model pitch/tempo es-\ntimation as a regression problem,\n• we propose a novel class-based equivariance loss\n(part 3.1) which prevents collapse; while [19] neces-\nsitates a decoder,\n• the architecture of our model is lightweight and\ntransposition-equivariant by design. For this, we in-\ntroduce Toeplitz fully-connected layers (part 3.4).\nWe evaluate our method on several datasets and show that\nit outperforms self-supervised baselines on single pitch es-\ntimation (part 4.4.1). We demonstrate the robustness of our\nmethod to domain-shift and background music, highlight-\ning its potential for real-world applications (part 4.4.2).\nOur proposed method requires minimal computation re-\nsources and is thus accessible to a wide range of users for\nboth research and musical applications. In consideration\nof accessibility and reproducibility, we make our code and\npretrained models publicly available1.\n1https://github.com/SonyCSLParis/pesto5352. RELATED WORKS\n2.1 SSL to learn invariant representations.\nSiamese networks. Most common techniques for SSL\nrepresentation involve Siamese networks [21]. The under-\nlying idea is to generate two views of an input, feed them to\na neural network, and train the network by applying a cri-\nterion between the output embeddings. Various techniques\nhave been developed for generating views2.\nCollapse. However, a major issue with these methods\nis “collapse”, when all inputs are mapped to the same em-\nbedding. To address this, various techniques have been\nproposed. One of the most common is SimCLR [22] which\nalso uses negative samples to ensure that embeddings are\nfar apart through a contrastive loss. Additionally, several\nregularization techniques have been developed that mini-\nmize a loss over the whole batch. Barlow Twins [23] force\nthe cross-correlation between embeddings to be identity,\nwhile VICReg [24] add loss terms on the statistics of a\nbatch to ensure that dimensions of the embeddings have\nhigh enough variance while remaining independent of each\nother. On the other hand, [25] explicitly minimize a loss\nover the hypersphere to distribute embeddings uniformly.\nFurthermore, incorporating asymmetry between inputs has\nbeen shown to improve performance. [26, 27] uses a mo-\nmentum encoder, while [28] and [29] add a projection\nhead and a stop-gradient operator on top of the network,\nwith [28] also using a teacher network. Finally, [30] in-\ncorporates asymmetry to contrastive- and clustering-based\nrepresentation learning.\nApplication to audio. While originally proposed for\ncomputer vision, these methods have been successfully\nadapted to audio and music as well. For example, [31],\n[32], and [33] respectively adapted [22], [23], and [28] to\nthe audio domain. By training their large models on Au-\ndioSet [34], they aim at learning general audio represen-\ntations that are suited for many downstream tasks. More\nspeciﬁcally, [35] successfully adapts contrastive learning\nto the task of music tagging by proposing more musically-\nrelevant data augmentations.\n2.2 SSL to learn equivariant representations.\nThe purpose of the methods described above is to learn a\nmappingf:X → Y that is invariant to a set of transforms\nTX, i.e. so that for any input x∈ X and transform t∈ TX\nf(t(x))≈f(x) (1)\nHowever, recent approaches [36–38] try instead to learn\na mapping fthat is equivariant toTX, i.e. that satisﬁes\nf(t(x))≈t′(f(x)) (2)\nwheret′∈ TYwithTYa set of transforms that acts on the\noutput space Y. In other words, if the input is transformed,\nthe output should be transformed accordingly. Representa-\ntion collapse is hence prevented by design.\n2The most common technique involves randomly applying data aug-\nmentations to inputs to create pairs of inputs that share semantic content.Equivariant representation learning has mostly been ap-\nplied to computer vision and usually combines an invari-\nance and an equivariance criterion. E-SSL [36] trains two\nprojection heads on top of an encoder, one to return pro-\njections invariant to data augmentations while the other\npredicts the parameters of the applied data augmentations.\n[37] predicts separately a semantic representation and a ro-\ntation angle of a given input and optimizes the network\nwith a reconstruction loss applied to the decoded content\nrepresentation rotated by the predicted angle. Finally, SIE\n[38] creates a pair of inputs by augmenting an input and\nlearns equivariant representations by training a hypernet-\nwork conditioned on the parameters of the augmentation\nto predict one embedding of the pair from the other.\nApplication to audio. Finally, a few successful exam-\nples of equivariant learning for solving MIR tasks recently\nemerged [19,20]. In particular, [20] introduces a simple yet\neffective equivariance criterion for tempo estimation while\npreventing collapse without any decoder or regularization:\npairs are created by time-stretching an input with two dif-\nferent ratios, then the output embeddings are linearly pro-\njected onto scalars and the network is optimized to make\nthe ratio of the scalar projections match the time-stretching\nratio within a pair.\n2.3 Pitch estimation.\nMonophonic pitch estimation has been a subject of inter-\nest for over ﬁfty years [39]. The earlier methods typically\nobtain a pitch curve by processing a candidate-generating\nfunction such as cepstrum [39], autocorrelation function\n(ACF) [40], and average magnitude difference function\n(AMDF) [41]. Other functions, such as the normalized\ncross-correlation function (NCCF) [1, 2] and the cumula-\ntive mean normalized difference function [3,42], have also\nbeen proposed. On the other hand, [4] performs pitch es-\ntimation by predicting the pitch of the sawtooth waveform\nwhose spectrum best matches the one of the input signal.\nRecently, methods involving machine learning tech-\nniques have been proposed [5, 6]. In particular,\nCREPE [16] is a deep convolutional network trained on\na large corpus to predict pitch from raw audio waveforms.\nSPICE [19] is a self-supervised method that takes as inputs\nindividual Constant-Q Transform (CQT) frames of pitch-\nshifted inputs and learns the transposition between these\ninputs. It achieves quite decent results thanks to a decoder\nthat takes as input the predicted pitch and tries to recon-\nstruct the original CQT frame from it.\nFinally, some works [43, 44] aim at disentangling the\npitch and timbre of an input audio, thus predicting pitch\nas a side effect. In particular, DDSP-inv [45] is a DDSP-\nbased approach [46] that relies on inverse synthesis to infer\npitch in a self-supervised way.\n3. SELF-SUPERVISED PITCH ESTIMATION\n3.1 Transposition-equivariant objective\nWe focus on the problem of monophonic pitch estimation\nand model it as a classiﬁcation task. Our model is com-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n536Figure 1 . Example of k-transpositions. Visually, yand\ny′are just translated versions of each other. The sign of\nkand its absolute value respectively indicate the direction\nand the distance of the translation.\nposed of a neural network fθthat takes as input an audio\nsignalxand returns a vector y= (y0,...,y i,...,y d−1)∈\n[0,1]d, which represents the probability distribution of\neach pitch i.yirepresents the probability that iis the pitch\nofx. We propose here to train fθin a SSL way. For this,\nsimilarly to [22, 24, 26, 28, 29], we use data augmentations\nand Siamese networks.\nGivenx, we ﬁrst generate x(k)by pitch-shifting xby\naknown numberkof semitones. Then, both xandx(k)\nare fed to fθwhich is trained to minimize a loss function\nbetweeny=fθ(x)andy(k)=fθ(x(k)).\nDeﬁnition. For two vectors y,y′∈Rdand0≤k < d ,y′\nis ak-transposition of yif and only if for all 0≤i < d\n\n\ny′\ni+k=yiwhen0≤i < d−k\ny′\ni= 0 wheni < k\nyi= 0 wheni≥d−k−1(3)\nSimilarly, for −d < k≤0,y′is ak-transposition of yif\nand only if yis a−k-transposition of y′.\nThe concept of k-transposition is illustrated in Figure 1.\nNote also that for a vector y∈Rd, exists at most one\nvectory′∈Rdthat is a k-transposition of y. We can\ntherefore refer to y′asthek-transposition of this vector y.\nEquivariance loss. We then design our criterion based\non the following assumption: the probability of xto have\npitchiis equal to the probability of x(k)to have pitch i+k,\ni.e.yishould be equal to y(k)\ni+k3. In other words, if x(k)is a\npitch-shifted version of x, their respective pitch probability\ndistributions should be shifted accordingly, i.e. y(k)should\nbe thek-transposition of y.\nWe take inspiration from [20] to design our equivari-\nance loss. However, in our case, the output of our network\nfθis not a generic representation but a probability distri-\nbution. We therefore adapt our criterion by replacing the\nlearnable linear projection head from [20] by the following\ndeterministic linear form:\nφ:Rd→ R\ny/mapsto→(α,α2,...,αd)y(4)\nwhereαis a ﬁxed hyperparameter4.\n3For example, if k= 2 semitones, the probability of xto be C4 is\nexactly the probability of x(k)to be a D4, and the same holds for any\npitch independently of the actual pitch of x.\n4We found α= 21/36to work well in practice.Indeed, with this formulation, for any kify′is ak-\ntransposition of ythenφ(y′) =αkφ(y). Hence we deﬁne\nour loss as\nLequiv(y,y(k),k) =hτ/parenleftbiggφ(y(k))\nφ(y)−αk/parenrightbigg\n(5)\nwherehτis the Huber loss function [47], deﬁned by\nhτ(x) =/braceleftigg\nx2\n2if|x| ≤τ\nτ2\n2+τ(|x|−τ)otherwise(6)\nRegularization loss. Note that if y(k)is thek-\ntransposition of ythenLequiv(y,y(k),k)is minimal. How-\never, the converse is not always true. In order to ac-\ntually enforce pitch-shifted pairs of inputs to lead to k-\ntranspositions, we further add a regularization term which\nis simply the shifted cross-entropy (SCE) between yand\ny(k), i.e. the cross-entropy between the k-transposition of\nyandy(k):\nLSCE(y,y(k),k) =d−1/summationdisplay\ni=0yilog/parenleftig\ny(k)\ni+k/parenrightig\n(7)\nwith the out-of-bounds indices replaced by 0. The respec-\ntive contribution of LequivandLSCEis studied in part 4.4.3.\nInvariance loss. Lequiv andLSCEallow our model to\nlearn relative transpositions between different inputs and\nlearn to output probability distributions yandy(k)that sat-\nisfy the equivariance constraints. However, these distribu-\ntions may still depend on the timbre of the signal. This\nis because our model actually never observed at the same\ntime two different samples with the same pitch.\nTo circumvent this, we rely on a set Tof data augmen-\ntations that preserve pitch (such as gain or additive white\nnoise). We create augmented views ˜x=t(x)of our inputs\nxby applying random transforms t∼ T.\nSimilarly to [35], we then train our model to be invari-\nant to those transforms by minimizing the cross-entropy\nbetweeny=fθ(x)and˜y=fθ(˜x).\nLinv(y,˜y) =CrossEntropy (y,˜y) (8)\nCombining the losses. For a given input sample xand\na given set of augmentations T,\n• we ﬁrst compute x(k)by pitch-shifting xby a random\nnumber of bins k(the precise procedure is described\nin section 3.2);\n• we then generate two augmented views ˜x=t1(x)and\n˜x(k)=t2(x(k)), wheret1,t2∼ T;\n• we compute y=fθ(x),˜y=fθ(˜x)and˜y(k)=fθ(˜x(k)).\nOur ﬁnal objective loss is then:\nL(y,˜y,˜y(k),k) =λinvLinv(y,˜y)\n+λequivLequiv(˜y,˜y(k),k)\n+λSCELSCE(˜y,˜y(k),k)(9)\nWe illustrate this in Figure 2. To set the weights λ∗we\nuse the gradient-based method proposed by [48–50].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n537Pitch-Shift\n Augmentation Prediction Loss computationFigure 2 . Overview of the PESTO method. The input CQT frame (log-frequencies) is ﬁrst cropped to produce a pair of\npitch-shifted inputs (x,x(k)). Then we compute ˜xand˜x(k)by randomly applying pitch-preserving transforms to the pair.\nWe ﬁnally pass x,˜xand˜x(k)through the network fθand optimize the loss between the predicted probability distributions.\n3.2 Audio-frontend\nThe inputs xare the individual frames of the CQT. We\nhave chosen the CQT as input since its logarithmic fre-\nquency scale, in which bins of the CQT exactly correspond\nto a ﬁxed fraction bof pitch semitones, naturally leads to\npitch-shifting by translation. CQT is also a common choice\nmade for pitch estimation [17, 19, 51].\nTo compute the CQT, we use the implementation pro-\nvided in the nnAudio library [52] since it supports parallel\nGPU computation. We choose fmin= 27.5Hz, which is\nthe frequency of A0 the lowest key of the piano and select\na resolution of b= 3 bins per semitone. Our CQT has in\ntotalK= 99blog-frequency bins, which corresponds to\nthe maximal number of bins for a 16kHz signal.\n3.3 Simulating translations.\nTo avoid any boundary effects, we perform pitch-shift by\ncropping shifted slices of the original CQT input frame as\nin [19]5. From a computational point of view, it is indeed\nsigniﬁcantly faster than applying classical pitch shift algo-\nrithms based on phase vocoder and resampling.\n3.4 Transpostion-preserving architecture\nThe architecture of fθis illustrated in Figure 3. It is in-\nspired by [17]. Each input CQT frame is processed inde-\npendently: ﬁrst layer-normed [53] then preprocessed by\ntwo 1D-Conv (convolution in the log-frequency dimen-\nsion) with skip-connections [54], followed by four 1D-\nConv layers. As in [17], we apply a non-linear leaky-\nReLU (slope 0.3) [55] and dropout (rate 0.2) [56] between\neach convolutional layer. Importantly, the kernel size and\npadding of each of these layers are chosen so that the fre-\nquency resolution is never reduced. We found in practice\nthat it helps the model to distinguish close but different\n5Speciﬁcally, we sample an integer kuniformly from the range\n{−kmax,...,k max}, then generate two CQT outputs, denoted as x\nandx(k), wherexis obtained by cropping the input CQT at indices\n[kmax,K−kmax−1], andx(k)is obtained by cropping the input CQT\nat indices [kmax−k,K−kmax+k−1], withKthe total number of\nbins of the original CQT frame and kmax = 16 in practice (see Figure 2).\n40\n 40 30 30 103Toeplitz fcFigure 3 . Architecture of our network fθ. The number of\nchannels varies between the intermediate layers, however\nthe frequency resolution remains unchanged until the ﬁnal\nToeplitz fully-connected layer.\npitches. The output is then ﬂattened, fed to a ﬁnal fully-\nconnected layer and normalized by a softmax layer to be-\ncome a probability distribution of the desired shape.\nNote that all layers (convolutions6, elementwise non-\nlinearities, layer-norm and softmax), except the last ﬁnal\nfully-connected layer, preserve transpositions. To make the\nﬁnal fully-connected layer also transposition-equivariant,\nwe propose to use Toeplitz fully-connected layers . It\nsimply consists of a standard linear layer without bias but\nwhose weights matrix Ais a Toeplitz matrix, i.e. each of\nits diagonals is constant.\nA=\na0a−1a−2···a−n+2a−n+1\na1a0a−1......a−n+2\na2a1............\n..................\nam−1··· ··· ··· ··· am−n\n\n(10)\nContrary to arbitrary fully-connected layers, Toeplitz ma-\ntrices are transposition-preserving operations and only\nhavem+n−1parameters instead of mn. Furthermore,\nthey are mathematically equivalent to convolutions, mak-\ning them straightforward to implement.\n6Convolutions roughly preserve transpositions since the kernels are\napplied locally, meaning that if two transposed inputs are convolved by\nthe same kernel, then the output results will be almost transpositions of\neach other as wellProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n538Raw Pitch Accuracy\nModel # params Trained on MIR-1K MDB-stem-synth\nSPICE [19] 2.38M private data 90.6% 89.1%\nDDSP-inv [45] - MIR-1K /MDB-stem-synth 91.8% 88.5%\nPESTO (ours) 28.9k MIR-1K 96.1% 94.6%\nPESTO (ours) 28.9k MDB-stem-synth 93.5% 95.5%\nCREPE [16] 22.2M many (supervised) 97.8% 96.7%\nTable 1 . Evaluation results of PESTO compared to supervised and self-supervised baselines. CREPE has been trained in\na supervised way on a huge dataset containing in particular MIR-1K andMDB-stem-synth . It is grayed out as a reference.\nFor DDSP-inv, we report the results when training and evaluating on the same dataset.\n3.5 Absolute pitch inference from y\nOur encoder fθreturns a probability distribution over\n(quantized) pitches. From an input CQT frame x, we ﬁrst\ncompute the probability distribution fθ(x), then we infer\nthe absolute pitch ˆpby applying the afﬁne mapping:\nˆp(x) =1\nb(argmax fθ(x)+p0) (11)\nwhereb= 3 is the number of bins per semitones in the\nCQT and p0is a ﬁxed integer shift that only depends on\nfθ. As in [19], we set the integer shift p0by relying on a\nset of synthetic data7with known pitch.\n4. EXPERIMENTS\n4.1 Datasets\nTo evaluate the performance of our approach, we consider\nthe two following datasets:\n1.MIR-1K [57] contains 1000 tracks (about two hours)\nof people singing Chinese pop songs, with separate\nvocal and background music tracks provided.\n2.MDB-stem-synth [58] contains re-synthesized\nmonophonic music played by various instruments.\nThe pitch range of the MDB-stem-synth dataset is wider\nthan the one of MIR-1K . The two datasets have different\nsampling rates and granularity for the annotations.\nWe conduct separate model training and evaluation on\nboth datasets to measure overﬁtting and generalization per-\nformance. In fact, given that our model is lightweight and\ndoes not require labelled data, overﬁtting performance is\nparticularly relevant for real-world scenarios, as it is easy\nfor someone to train on their own dataset, e.g. their own\nvoice. However, we also examine generalization perfor-\nmance through cross-evaluation to ensure that the model\ntruly captures the underlying concept of pitch and does not\nmerely memorize the training data.\n4.2 Training details\nFrom an input CQT (see part 3.2), we ﬁrst compute the\npitch-shifted CQT (see part 3.3). Then two random data\naugmentations t1,t2∼ T are applied with a probability\nof 0.7. We used white noise with a random standard de-\nviation between 0.1 and 2, and gain with a random value\n7synthetic harmonic signals with random amplitudes and pitchpicked uniformly between -6 and 3 dB. The overall archi-\ntecture of fθ(see part 3.4) is implemented in PyTorch [59].\nFor training, we use a batch size of 256 and the Adam opti-\nmizer [60] with a learning rate of 10−4and default param-\neters. The model is trained for 50 epochs using a cosine an-\nnealing learning rate scheduler. Our architecture being ex-\ntremely lightweight, training requires only 545MB of GPU\nmemory and can be performed on a single GTX 1080Ti.\n4.3 Performance metrics\nWe measure the performances using the following metrics.\n1.Raw Pitch Accuracy (RPA): corresponds to the per-\ncentage of voiced frames whose pitch error8is less\nthan 0.5 semitone [61].\n2.Raw Chroma Accuracy (RCA): same as RPA but\nconsidering the mapping to Chroma (hence allowing\noctave errors) [61].\nRCA is only used in our ablation studies.\n4.4 Results and discussions\n4.4.1 Clean signals\nWe compare our results with three baselines: CREPE [16],\nSPICE [19] and DDSP-inv [45]. CREPE is fully-\nsupervised while SPICE and DDSP-inv are two SSL ap-\nproaches. To measure the inﬂuence of the training set, we\ntrain PESTO on the two datasets ( MIR-1K andMDB-stem-\nsynth ) and also evaluate on the two. This allows to test\nmodel generalization.\nWe indicate the results in Table 1. We see that PESTO\nsigniﬁcantly outperforms the two SSL baselines (SPICE\nand DDSP-inv) even in the cross-dataset scenario (93.5%\nand 94.6%). Moreover, it is competitive with CREPE (-\n1.7% and -1.2%) which has 750 times more parameters\nand is trained in a supervised way on the same datasets.\n4.4.2 Robustness to background music\nBackground noise and music can severely impact pitch es-\ntimation algorithms, making it imperative to develop ro-\nbust methods that can handle real-world scenarios where\nbackground noise is often unavoidable.\nWe therefore test the robustness of PESTO to back-\nground music. For this, we use the MIR-1K dataset,\nwhich contains separated vocals and background tracks\n8i.e. distance between the predicted pitch and the actual oneProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n539Raw Pitch Accuracy ( MIR-1K )\nModel clean 20 dB 10 dB 0 dB\nSPICE [19] 91.4% 91.2% 90.0% 81.6%\nPESTO\nβ= 0 94.8% 90.7% 79.2% 50.0%\nβ= 1 94.5% 94.2% 92.9% 83.1%\nβ∼ U(0,1) 94.7% 94.4% 92.9% 81.7%\nβ∼ N(0,1) 94.8% 94.5% 93.0% 82.6%\nβ∼ N(0,1\n2)94.8% 94.5% 92.9% 81.0%\nCREPE [16] 97.8% 97.3% 95.3% 84.8%\nTable 2 . Robustness of PESTO and other baselines to\nbackground music with various Signal-to-Noise ratios.\nAdding background music to training samples signiﬁcantly\nimproves the robustness of PESTO (see section 4.4.2).\nand allows testing various signal-to-noise (here vocal-to-\nbackground) ratios (SNRs).\nWe indicate the results in Table 2. As foreseen, the per-\nformance of PESTO when trained on clean vocals (row\nβ= 0) and applied to vocal-with-background consider-\nably drop: from 94.8% (clean) to 50.0% (SNR = 0 dB)9.\nTo improve the robustness to background music, we\nslightly modify our method to train our model on mixed\nsources. Instead of using gain and white noise as data aug-\nmentations, we create an augmented view of our original\nvocals signal xvocals by mixing it (in the complex-CQT do-\nmain) with its corresponding background track xbackground :\nx=xvocals+βxbackground (12)\nThen, thanks to Linv, the model is trained to ignore the\nbackground music for making its predictions.\nThe background level βis randomly sampled for each\nCQT frame. The inﬂuence of the distribution we sample\nβfrom is depicted in Table 2. This method signiﬁcantly\nlimits the drop in performances observed previously and\nalso makes PESTO outperform SPICE in noisy conditions.\n4.4.3 Ablation study\nTable 3 depicts the inﬂuence of our different design\nchoices. First, we observe that the equivariance loss Lequiv\nand the ﬁnal Toeplitz fully-connected layer (eq.(10)) are\nabsolutely essential for our model not to collapse. More-\nover, data augmentations seem to have a negligible inﬂu-\nence on out-of-domain RPA (-0.2%) but slightly help when\ntraining and evaluating on the same dataset (+1.2%).\nOn the other hand, it appears that both LinvandLSCEdo\nnot improve in-domain performances but help the model to\ngeneralize better. This is especially true for LSCE, whose\naddition enables to improve RPA from 86.9% to 94.6% on\nMDB-stem-synth .\nFinally, according to the drop of performances in RPA\nand RCA when removing Linv, it seems that the invariance\nloss prevents octave errors on the out-of-domain dataset.\n9It should be noted that the difference between the 96.1% of Table 1\nand the 94.8% of Table 2 is due to the fact that we do not apply any data\naugmentation (gain or additive white noise) when β= 0.MIR-1K MDB\nRPA RCA RPA RCA\nPESTO baseline 96.1% 96.4% 94.6% 95.0%\nLoss ablations\nw/oLequiv 5.8% 8.6% 1.3% 6.1%\nw/oLinv 96.1% 96.4% 92.5% 94.5%\nw/oLSCE 96.1% 96.5% 86.9% 93.8%\nMiscellaneous\nno augmentations 94.8% 95.4% 94.8% 95.2%\nnon-Toeplitz fc 5.7% 8.7% 1.2% 6.1%\nTable 3 . Respective contribution of various design choices\nof PESTO for a model trained on MIR-1K .\n5. CONCLUSION\nIn this paper, we presented a novel self-supervised learning\nmethod for pitch estimation that leverages equivariance to\nmusical transpositions. We propose a class-based equiv-\nariant objective that enables Siamese networks to capture\npitch information from pairs of transposed inputs accu-\nrately. We also introduce a Toeplitz fully-connected layer\nto the architecture of our model to facilitate the optimiza-\ntion of this objective. Our method is evaluated on two stan-\ndard benchmarks, and the results show that it outperforms\nself-supervised baselines and is robust to background mu-\nsic and domain shift.\nFrom a musical perspective, our lightweight model\nis well-suited for real-world scenarios, as it can run on\nresource-limited devices without sacriﬁcing performance.\nMoreover, its SSL training procedure makes it convenient\nto ﬁne-tune on a small unlabeled dataset, such as a spe-\nciﬁc voice or instrument. Additionally, the resolution of\nthe model is a sixth of a tone but could eventually be in-\ncreased by changing the resolution of the CQT. Moreover,\ndespite modelling pitch estimation as a classiﬁcation prob-\nlem, we make no assumption about scale or temperament.\nThese features make our method still a viable solution,\ne.g. for instruments that use quartertones and/or for which\nno annotated dataset exists. We therefore believe that\nit has many applications even beyond the limitations of\nWestern music.\nOverall, the idea of using equivariance to solve a clas-\nsiﬁcation problem is a novel and promising approach that\nenables the direct return of a probability distribution over\nthe classes with a single, potentially synthetic, labelled el-\nement. While our paper applies this approach to pitch es-\ntimation, there are other applications where this technique\ncould be useful, such as tempo estimation.\nMoreover, modelling a regression task as a classiﬁca-\ntion problem can offer greater interpretability as the output\nof the network is not a single scalar but a whole probability\ndistribution. Finally, it can generalize better to multi-label\nscenarios.\nOur proposed method hence demonstrates the potential\nof using equivariance to solve problems that are beyond the\nscope of our current work. In particular, it paves the way\ntowards self-supervised multi-pitch estimation.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n5406. ACKNOWLEDGEMENTS\nThis work has been funded by the ANRT CIFRE conven-\ntion n°2021/1537 and Sony France. This work was granted\naccess to the HPC/AI resources of IDRIS under the alloca-\ntion 2022-AD011013842 made by GENCI. We would like\nto thank the reviewers and meta-reviewer for their valuable\nand insightful comments.\n7. REFERENCES\n[1] D. Talkin, “A robust algorithm for pitch tracking\n(RAPT),” pp. 495–518, 1995.\n[2] P. Boersma, “Acurate short-term analysis of the\nfundamental frequency and the harmonics-to-noise\nratio of a sampled sound,” IFA Proceedings 17 , vol. 17,\npp. 97–110, 1993. [Online]. Available: http://www.\nfon.hum.uva.nl/paul/papers/Proceedings_1993.pdf\n[3] M. Mauch and S. Dixon, “PYIN: A fundamental fre-\nquency estimator using probabilistic threshold distri-\nbutions,” in 2014 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) ,\nvol. 1, no. 1, 2014, pp. 659–663.\n[4] A. Camacho and J. G. Harris, “A sawtooth waveform\ninspired pitch estimator for speech and music,” The\nJournal of the Acoustical Society of America , vol. 124,\nno. 3, pp. 1638–1652, 2008.\n[5] B. S. Lee and D. P. W. Ellis, “Noise robust pitch track-\ning by subband autocorrelation classiﬁcation,” in Proc.\nInterspeech 2012 , 2012, pp. 707–710.\n[6] K. Han and D. Wang, “Neural Network Based Pitch\nTracking in Very Noisy Speech,” IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing ,\nvol. 22, no. 12, pp. 2158–2168, 2014.\n[7] C. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Simon,\nC. Raffel, J. Engel, S. Oore, and D. Eck, “Onsets\nand frames: Dual-objective piano transcription,”\ninProceedings of the 19th International Society\nfor Music Information Retrieval Conference, ISMIR\n2018 . International Society for Music Information\nRetrieval, oct 2018, pp. 50–57. [Online]. Available:\nhttps://archives.ismir.net/ismir2018/paper/000019.pdf\n[8] J. W. Kim and J. P. Bello, “Adversarial learning\nfor improved onsets and frames music transcription,”\ninProceedings of the 20th International Society\nfor Music Information Retrieval Conference, ISMIR\n2019 . International Society for Music Information\nRetrieval, jun 2019, pp. 670–677. [Online]. Available:\nhttps://archives.ismir.net/ismir2019/paper/000081.pdf\n[9] Q. Kong, B. Li, X. Song, Y . Wan, and Y . Wang,\n“High-Resolution Piano Transcription with Pedals\nby Regressing Onset and Offset Times,” IEEE/ACM\nTransactions on Audio Speech and Language Pro-\ncessing , vol. 29, pp. 3707–3717, oct 2021. [Online].\nAvailable: https://arxiv.org/abs/2010.01815v3[10] G. Song, Z. Wang, F. Han, and S. Ding, “Transfer\nlearning for music genre classiﬁcation,” IFIP Advances\nin Information and Communication Technology , vol.\n510, pp. 183–190, 2017.\n[11] S. Oramas, F. Barbieri, O. Nieto Caballero, and\nX. Serra, “Multimodal deep learning for music genre\nclassiﬁcation,” Transactions of the International Soci-\nety for Music Information Retrieval. 2018; 1 (1): 4-21. ,\n2018.\n[12] N. Ndou, R. Ajoodha, and A. Jadhav, “Music Genre\nClassiﬁcation: A Review of Deep-Learning and Tradi-\ntional Machine-Learning Approaches,” in 2021 IEEE\nInternational IOT, Electronics and Mechatronics Con-\nference (IEMTRONICS) , 2021, pp. 1–6.\n[13] V . Lostanlen and C. E. Cella, “Deep convolutional net-\nworks on the pitch spiral for music instrument recogni-\ntion,” in Proceedings of the 17th International Society\nfor Music Information Retrieval Conference, ISMIR\n2016 . International Society for Music Information\nRetrieval, may 2016, pp. 612–618. [Online]. Available:\nhttps://archives.ismir.net/ismir2016/paper/000093.pdf\n[14] Y . Han, J. Kim, and K. Lee, “Deep Convolutional Neu-\nral Networks for Predominant Instrument Recognition\nin Polyphonic Music,” IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing , vol. 25, no. 1,\npp. 208–221, 2017.\n[15] A. Solanki and S. Pandey, “Music instrument recogni-\ntion using deep convolutional neural networks,” Inter-\nnational Journal of Information Technology , vol. 14,\nno. 3, pp. 1659–1668, 2022. [Online]. Available:\nhttps://doi.org/10.1007/s41870-019-00285-y\n[16] J. W. Kim, J. Salamon, P. Li, and J. P. Bello, “Crepe:\nA Convolutional Representation for Pitch Estimation,”\ninICASSP , IEEE International Conference on Acous-\ntics, Speech and Signal Processing - Proceedings ,\nvol. 2018-April, feb 2018, pp. 161–165. [Online].\nAvailable: http://arxiv.org/abs/1802.06182\n[17] C. Weiß and G. Peeters, “Deep-Learning Archi-\ntectures for Multi-Pitch Estimation: Towards Re-\nliable Evaluation,” feb 2022. [Online]. Available:\nhttp://arxiv.org/abs/2202.09198\n[18] R. Geirhos, J. H. Jacobsen, C. Michaelis, R. Zemel,\nW. Brendel, M. Bethge, and F. A. Wichmann,\n“Shortcut learning in deep neural networks,” Nature\nMachine Intelligence , vol. 2, no. 11, pp. 665–673, apr\n2020. [Online]. Available: https://www.nature.com/\narticles/s42256-020-00257-z\n[19] B. Gfeller, C. Frank, D. Roblek, M. Shariﬁ,\nM. Tagliasacchi, and M. Velimirovic, “SPICE:\nSelf-Supervised Pitch Estimation,” IEEE/ACM Trans-\nactions on Audio Speech and Language Processing ,\nvol. 28, pp. 1118–1128, oct 2020. [Online]. Avail-\nable: https://dl.acm.org/doi/pdf/10.1109/TASLP.2020.\n2982285Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n541[20] E. Quinton, “Equivariant Self-Supervision for Musical\nTempo Estimation,” Proceedings of the 23rd Interna-\ntional Society for Music Information Retrieval Con-\nference, ISMIR 2022 , sep 2022. [Online]. Available:\nhttps://archives.ismir.net/ismir2022/paper/000009.pdf\n[21] R. Hadsell, S. Chopra, and Y . LeCun, “Dimensional-\nity reduction by learning an invariant mapping,” Pro-\nceedings of the IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition , vol. 2, pp.\n1735–1742, 2006.\n[22] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton,\n“A simple framework for contrastive learning of visual\nrepresentations,” in 37th International Conference on\nMachine Learning, ICML 2020 , vol. PartF16814.\nInternational Machine Learning Society (IMLS), feb\n2020, pp. 1575–1585. [Online]. Available: http:\n//proceedings.mlr.press/v119/chen20j/chen20j.pdf\n[23] J. Zbontar, L. Jing, I. Misra, Y . LeCun, and S. Deny,\n“Barlow Twins: Self-Supervised Learning via Re-\ndundancy Reduction,” 38th International Conference\non Machine Learning, ICML 2021 , mar 2021. [On-\nline]. Available: http://proceedings.mlr.press/v139/\nzbontar21a/zbontar21a.pdf\n[24] A. Bardes, J. Ponce, and Y . LeCun, “VI-\nCReg: Variance-Invariance-Covariance Regulariza-\ntion for Self-Supervised Learning,” in The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29,\n2022 . OpenReview.net, 2022. [Online]. Available:\nhttps://openreview.net/forum?id=xm6YD62D1Ub\n[25] T. Wang and P. Isola, “Understanding contrastive rep-\nresentation learning through alignment and uniformity\non the hypersphere,” in 37th International Conference\non Machine Learning, ICML 2020 , vol. PartF16814.\nInternational Machine Learning Society (IMLS), may\n2020, pp. 9871–9881. [Online]. Available: https:\n//proceedings.mlr.press/v119/wang20k/wang20k.pdf\n[26] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick,\n“Momentum Contrast for Unsupervised Visual\nRepresentation Learning,” in Proceedings of the IEEE\nComputer Society Conference on Computer Vision\nand Pattern Recognition . IEEE Computer Society,\nnov 2020, pp. 9726–9735. [Online]. Available: https:\n//openaccess.thecvf.com/content_CVPR_2020/papers/\nHe_Momentum_Contrast_for_Unsupervised_Visual_\nRepresentation_Learning_CVPR_2020_paper.pdf\n[27] X. Chen, H. Fan, R. Girshick, and K. He, “Improved\nBaselines with Momentum Contrastive Learning,” mar\n2020. [Online]. Available: http://arxiv.org/abs/2003.\n04297\n[28] J. B. Grill, F. Strub, F. Altché, C. Tallec, P. H.\nRichemond, E. Buchatskaya, C. Doersch, B. A. Pires,\nZ. D. Guo, M. G. Azar, B. Piot, K. Kavukcuoglu,\nR. Munos, and M. Valko, “Bootstrap your ownlatent a new approach to self-supervised learn-\ning,” in Advances in Neural Information Processing\nSystems , vol. 2020-Decem. Neural information\nprocessing systems foundation, jun 2020. [On-\nline]. Available: https://papers.nips.cc/paper/2020/ﬁle/\nf3ada80d5c4ee70142b17b8192b2958e-Paper.pdf\n[29] X. Chen and K. He, “Exploring simple Siamese\nrepresentation learning,” in Proceedings of the IEEE\nComputer Society Conference on Computer Vision\nand Pattern Recognition . IEEE Computer Society,\nnov 2021, pp. 15 745–15 753. [Online]. Available:\nhttps://openaccess.thecvf.com/content/CVPR2021/\npapers/Chen_Exploring_Simple_Siamese_\nRepresentation_Learning_CVPR_2021_paper.pdf\n[30] Y . Dubois, T. Hashimoto, S. Ermon, and P. Liang, “Im-\nproving Self-Supervised Learning by Characterizing\nIdealized Representations,” sep 2022. [Online]. Avail-\nable: https://openreview.net/pdf?id=agQGDz6gPOo\n[31] A. Saeed, D. Grangier, and N. Zeghidour, “Contrastive\nlearning of general-purpose audio representations,” in\nICASSP , IEEE International Conference on Acoustics,\nSpeech and Signal Processing - Proceedings , vol.\n2021-June. Institute of Electrical and Electronics\nEngineers Inc., oct 2021, pp. 3875–3879. [Online].\nAvailable: https://arxiv.org/abs/2010.10915v1\n[32] J. Anton, H. Coppock, P. Shukla, and B. W.\nSchuller, “Audio Barlow Twins: Self-Supervised\nAudio Representation Learning,” sep 2022. [Online].\nAvailable: http://arxiv.org/abs/2209.14345\n[33] D. Niizumi, D. Takeuchi, Y . Ohishi, N. Harada,\nand K. Kashino, “BYOL for Audio: Exploring\nPre-Trained General-Purpose Audio Representations,”\nIEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , pp. 1–15, apr 2022. [Online].\nAvailable: https://dl.acm.org/doi/pdf/10.1109/TASLP.\n2022.3221007\n[34] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter,\n“Audio Set: An ontology and human-labeled dataset\nfor audio events,” ICASSP , IEEE International Con-\nference on Acoustics, Speech and Signal Processing -\nProceedings , pp. 776–780, 2017.\n[35] J. Spijkervet and J. A. Burgoyne, “Con-\ntrastive Learning of Musical Representations,”\nProceedings of the 22nd International Soci-\nety for Music Information Retrieval Conference,\nISMIR 2021 , mar 2021. [Online]. Available:\nhttps://archives.ismir.net/ismir2021/paper/000084.pdf\n[36] R. Dangovski, L. Jing, C. Loh, S. Han, A. Srivastava,\nB. Cheung, P. Agrawal, and M. Solja ˇci´c, “Equivariant\nContrastive Learning,” ICLR 2022 - 10th International\nConference on Learning Representations , oct 2022.\n[Online]. Available: https://openreview.net/pdf?id=\ngKLAAﬁytIProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n542[37] R. Winter, M. Bertolini, T. Le, F. Noé, and\nD.-A. Clevert, “Unsupervised Learning of Group\nInvariant and Equivariant Representations,” feb 2022.\n[Online]. Available: https://openreview.net/pdf?id=\n47lpv23LDPr\n[38] Q. Garrido, L. Najman, and Y . Lecun, “Self-\nsupervised learning of Split Invariant Equivariant\nrepresentations,” feb 2023. [Online]. Available: https:\n//openreview.net/pdf?id=2sIVxJ9Hp0\n[39] A. M. Noll, “Cepstrum pitch determination.” The Jour-\nnal of the Acoustical Society of America , vol. 41 2, pp.\n293–309, 1967.\n[40] J. J. Dubnowski, R. W. Schafer, and L. R. Rabiner,\n“Real-Time Digital Hardware Pitch Detector,” IEEE\nTransactions on Acoustics, Speech, and Signal Pro-\ncessing , vol. 24, no. 1, pp. 2–8, 1976.\n[41] M. J. Ross, H. L. Shaffer, A. Cohen, R. L. Freudberg,\nand H. Manley, “Average magnitude difference func-\ntion pitch extractor,” IEEE Transactions on Acoustics,\nSpeech, and Signal Processing , vol. 22, pp. 353–362,\n1974.\n[42] A. de Cheveigné and H. Kawahara, “YIN, a funda-\nmental frequency estimator for speech and music,” The\nJournal of the Acoustical Society of America , vol. 111,\nno. 4, pp. 1917–1930, 2002.\n[43] Y .-J. Luo, K. W. Cheuk, T. Nakano, M. Goto,\nand D. Herremans, “Unsupervised disentanglement\nof pitch and timbre for isolated musical instrument\nsounds,” in International Society for Music Informa-\ntion Retrieval Conference , 2020.\n[44] K. Tanaka, Y . Bando, K. Yoshii, and S. Morishima,\n“Unsupervised disentanglement of timbral, pitch, and\nvariation features from musical instrument sounds with\nrandom perturbation,” in 2022 Asia-Paciﬁc Signal and\nInformation Processing Association Annual Summit\nand Conference (APSIPA ASC) , 2022, pp. 709–716.\n[45] J. Engel, R. Swavely, A. Roberts, L. . Hanoi,\n. Hantrakul, and C. Hawthorne, “Self-Supervised Pitch\nDetection by Inverse Audio Synthesis,” Workshop\non Self-Supervision in Audio and Speech at the\n37th International Conference on Machine Learning\n(ICML 2020) , pp. 1–9, 2020. [Online]. Available:\nhttps://goo.gl/magenta/ddsp-inv\n[46] J. Engel, L. Hantrakul, C. Gu, and A. Roberts,\n“DDSP: Differentiable Digital Signal Processing,”\nThe Eighth International Conference on Learning\nRepresentations, ICLR 2020 , jan 2020. [Online].\nAvailable: https://openreview.net/pdf?id=B1x1ma4tDr\n[47] P. J. Huber, “Robust Estimation of a Location\nParameter,” The Annals of Mathematical Statistics ,\nvol. 35, no. 1, pp. 73–101, 1964. [Online]. Available:\nhttp://www.jstor.org/stable/2238020[48] Z. Chen, V . Badrinarayanan, C. Y . Lee, and\nA. Rabinovich, “GradNorm: Gradient normaliza-\ntion for adaptive loss balancing in deep mul-\ntitask networks,” in 35th International Confer-\nence on Machine Learning, ICML 2018 , vol. 2.\nInternational Machine Learning Society (IMLS),\nnov 2018, pp. 1240–1251. [Online]. Available:\nhttp://proceedings.mlr.press/v80/chen18a/chen18a.pdf\n[49] P. Esser, R. Rombach, and B. Ommer, “Taming\ntransformers for high-resolution image synthesis,”\ninProceedings of the IEEE Computer Society\nConference on Computer Vision and Pattern\nRecognition . IEEE Computer Society, dec 2021,\npp. 12 868–12 878. [Online]. Available: https:\n//openaccess.thecvf.com/content/CVPR2021/papers/\nEsser_Taming_Transformers_for_High-Resolution_\nImage_Synthesis_CVPR_2021_paper.pdf\n[50] J. MacGlashan, E. Archer, A. Devlic, T. Seno, C. Sher-\nstan, P. R. Wurman, and P. Stone, “Value Function\nDecomposition for Iterative Design of Reinforcement\nLearning Agents,” jun 2022. [Online]. Available:\nhttps://openreview.net/pdf?id=pNEisJqGuei\n[51] R. M. Bittner, J. J. Bosch, D. Rubinstein, G. Meseguer-\nBrocal, and S. Ewert, “A Lightweight Instrument-\nAgnostic Model for Polyphonic Note Transcription\nand Multipitch Estimation,” in ICASSP , IEEE In-\nternational Conference on Acoustics, Speech and\nSignal Processing - Proceedings , vol. 2022-May.\nInstitute of Electrical and Electronics Engineers\nInc., mar 2022, pp. 781–785. [Online]. Available:\nhttps://arxiv.org/abs/2203.09893v2\n[52] K. W. Cheuk, H. Anderson, K. Agres, and D. Herre-\nmans, “nnAudio: An on-the-Fly GPU Audio to Spec-\ntrogram Conversion Toolbox Using 1D Convolutional\nNeural Networks,” IEEE Access , vol. 8, pp. 161 981–\n162 003, 2020.\n[53] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer\nNormalization,” jul 2016. [Online]. Available: http:\n//arxiv.org/abs/1607.06450\n[54] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image recognition,” in Proceedings of\nthe IEEE Computer Society Conference on Computer\nVision and Pattern Recognition , vol. 2016-Decem,\ndec 2016, pp. 770–778. [Online]. Available:\nhttps://www.cv-foundation.org/openaccess/content_\ncvpr_2016/papers/He_Deep_Residual_Learning_\nCVPR_2016_paper.pdf\n[55] B. Xu, N. Wang, T. Chen, and M. Li, “Empirical\nEvaluation of Rectiﬁed Activations in Convolutional\nNetwork,” may 2015. [Online]. Available: http:\n//arxiv.org/abs/1505.00853\n[56] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,\nand R. Salakhutdinov, “Dropout: A simple way to\nprevent neural networks from overﬁtting,” Journal ofProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n543Machine Learning Research , vol. 15, pp. 1929–1958,\n2014.\n[57] C.-L. Hsu and J.-S. R. Jang, “On the Improvement\nof Singing V oice Separation for Monaural Recordings\nUsing the MIR-1K Dataset,” IEEE Transactions on Au-\ndio, Speech, and Language Processing , vol. 18, no. 2,\npp. 310–319, 2010.\n[58] J. Salamon, R. M. Bittner, J. Bonada, J. J. Bosch,\nE. Gómez, and J. P. Bello, “An analysis/synthesis\nframework for automatic f0 annotation of multitrack\ndatasets,” Proceedings of the 18th International Soci-\nety for Music Information Retrieval Conference, ISMIR\n2017 , pp. 71–78, 2017.\n[59] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,\nG. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Köpf, E. Yang,\nZ. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\nB. Steiner, L. Fang, J. Bai, and S. Chintala, “PyTorch:\nAn imperative style, high-performance deep learning\nlibrary,” in Advances in Neural Information Processing\nSystems , vol. 32. Neural information processing\nsystems foundation, dec 2019. [Online]. Available:\nhttps://pytorch.org/\n[60] D. P. Kingma and J. L. Ba, “Adam: A method\nfor stochastic optimization,” in 3rd International\nConference on Learning Representations, ICLR 2015\n- Conference Track Proceedings , dec 2015. [Online].\nAvailable: http://arxiv.org/abs/1412.6980\n[61] G. E. Poliner, D. P. Ellis, A. F. Ehmann, E. Gómez,\nS. Streich, and B. Ong, “Melody transcription from\nmusic audio: Approaches and evaluation,” IEEE\nTransactions on Audio, Speech and Language Process-\ning, vol. 15, no. 4, pp. 1247–1256, 2007.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n544"
    },
    {
        "title": "Crowd&apos;s Performance on Temporal Activity Detection of Musical Instruments in Polyphonic Music.",
        "author": [
            "Ioannis Petros Samiotis",
            "Christoph Lofi",
            "Alessandro Bozzon"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.1492479",
        "url": "https://doi.org/10.5281/zenodo.1492479",
        "ee": "https://zenodo.org/records/1492479/files/275_Paper.pdf",
        "abstract": "Although instrument recognition has been thoroughly research, recognition in polyphonic music still faces challenges. While most research in polyphonic instrument recognition focuses on predicting the predominant instruments in a given audio recording, instrument activity detection represents a generalized problem of detecting the presence or activity of instruments in a track on a fine-grained temporal scale. We present an approach for instrument activity detection in polyphonic music with temporal resolution ranging from one second to the track level. This system allows, for instance, to retrieve specific areas of interest such as guitar solos. Three classes of deep neural networks are trained to detect up to 18 instruments. The architectures investigated in this paper are: multi-layer perceptrons, convolutional neural networks, and convolutional-recurrent neural networks. An in-depth evaluation on publicly available multi-track datasets using methods such as AUC-ROC and Label Ranking Average Precision highlights different aspects of the model performance and indicates the importance of using multiple evaluation metrics. Furthermore, we propose a new visualization to discuss instrument confusion in a multi-label scenario.",
        "zenodo_id": 1492479,
        "dblp_key": "conf/ismir/SamiotisLB23",
        "keywords": [
            "instrument recognition",
            "polyphonic music",
            "instrument activity detection",
            "fine-grained temporal scale",
            "deep neural networks",
            "AUC-ROC",
            "Label Ranking Average Precision",
            "multi-track datasets",
            "instrument confusion",
            "visualization"
        ],
        "content": "INSTRUMENT ACTIVITY DETECTION IN POLYPHONIC MUSIC USING\nDEEP NEURAL NETWORKS\nSiddharth Gururani1Cameron Summers2Alexander Lerch1\n1Center for Music Technology, Georgia Institute of Technology , USA\n2Gracenote, Emeryville, USA\nfsiddgururani, alexander.lerch g@gatech.edu, cameron.summers@nielsen.com\nABSTRACT\nAlthough instrument recognition has been thoroughly\nresearch, recognition in polyphonic music still faces chal-\nlenges. While most research in polyphonic instrument\nrecognition focuses on predicting the predominant instru-\nments in a given audio recording, instrument activity detec-\ntion represents a generalized problem of detecting the pres-\nence or activity of instruments in a track on a ﬁne-grained\ntemporal scale. We present an approach for instrument activ-\nity detection in polyphonic music with temporal resolution\nranging from one second to the track level. This system\nallows, for instance, to retrieve speciﬁc areas of interest\nsuch as guitar solos. Three classes of deep neural networks\nare trained to detect up to 18 instruments. The architec-\ntures investigated in this paper are: multi-layer perceptrons,\nconvolutional neural networks, and convolutional-recurrent\nneural networks. An in-depth evaluation on publicly avail-\nable multi-track datasets using methods such as AUC-ROC\nand Label Ranking Average Precision highlights different\naspects of the model performance and indicates the impor-\ntance of using multiple evaluation metrics. Furthermore, we\npropose a new visualization to discuss instrument confusion\nin a multi-label scenario.\n1. INTRODUCTION\nMusic is an acoustic rendition of musical ideas. In most\ncases, one or more instruments are used for this acoustic\nrendition. As humans, we are easily able to identify the\ninstruments being played in a song after exposure to their\nsound. However, the same cannot be said for computer algo-\nrithms. The task of recognizing musical instruments in an\naudio signal has been an active area of research in the ﬁeld\nof Music Information Retrieval (MIR). While instrument\nrecognition in monophonic audio (only one instrument is\npresent in a signal) is reasonably successful [13], the task\nis much harder in a polyphonic setting. The challenges\nc\rSiddharth Gururani, Cameron Summers, Alexander\nLerch. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Siddharth Gururani, Cameron Sum-\nmers, Alexander Lerch. “Instrument Activity Detection in Polyphonic\nMusic using Deep Neural Networks”, 19th International Society for Music\nInformation Retrieval Conference, Paris, France, 2018.include, among others, the large variance in timbre and per-\nformance style within an instrument class combined with\nperceptual similarity of some instruments and the superpo-\nsition of multiple instruments in time and frequency. Last\nbut not least, the lack of data with relevant annotations for\ndata-driven approaches is also a problem.\nThe identiﬁcation of instruments and their activity in a\nsong is important for music browsing and discovery, such as\nsearching for songs with speciﬁc instruments or identifying\nthe position of lead vocals or a saxophone solo. Instrument\nrecognition can also inform other MIR tasks. For example,\nmusic recommendation systems can beneﬁt from modeling\na user’s afﬁnity towards certain instruments and music genre\nrecognition systems could improve with genre-dependent\ninstrument information. It can also be useful for tasks such\nas automatic music transcription, playing technique detec-\ntion, and source separation in polyphonic music, where\npre-conditioning a model on speciﬁc instruments present\ncould possibly boost its performance.\nAn Instrument Activity Detection (IAD) system takes\nan audio track as input and outputs continuous instrument\nactivity levels along the entire track. These activities may\nbe binary (on/off) or on a continuous scale as likelihood.\nIAD systems may have varying time-resolutions for the\ninstrument activity depending on the use case. For example,\na solo detection use case would have a ﬁner time-resolution\nthat an instrument tagging system which would work on\nthe track level. This paper proposes a deep neural network-\nbased IAD system trained using multi-track datasets. We\nalso address the problem of evaluation of an IAD system.\nThe following section reviews literature in instrument\nrecognition and other related tasks. Section 3 describes the\nproposed IAD system starting with pre-processing the data,\nthe model architectures and post-processing steps. Next,\nSection 4 describes the dataset used, the various experi-\nments, the evaluation metrics and the proposed method to\nvisualize confusion. We report the results for the experi-\nments in terms of the evalution metrics and discuss these\nresults in Section 5. Finally, in Section 6 we conclude\nthe paper enumerating a few possible future directions for\nresearch on IAD.5692. RELATED WORK\nThe task of ‘instrument recognition’ can be divided into two\ndistinct research problems based on the type of data being\nanalyzed: (i) instrument recognition in monophonic audio\nand (ii) instrument recognition in polyphonic audio. This\nsection presents an overview of past literature on instrument\nrecognition as well as related topics such as automatic music\ntagging and sound event detection (SED).\n2.1 Instrument Recognition in Monophonic Music\nIn monophonic music, instrument recognition may be per-\nformed on sounds at the note-level or on continuous audio\nsignals of solo instrument performances. An extensive\nreview of traditional feature extraction and classiﬁcation\napproaches for note-level instrument recognition has been\npublished by Herrera et al. [15]. For solo phrases, Es-\nsid et al. utilize MFCCs as features, Principal Component\nAnalysis (PCA) for dimensionality reduction, and Gaus-\nsian mixture models (GMM) for classifying solo phrases\nof 5 instruments [8]. Krishna and Sreenivas propose the\nso-called Line Spectral Features (LSF). LSFs are used with\na GMM and evaluated for instrument family classiﬁcation\nand 14-class instrument classiﬁcation [19].\nIn addition to extracting established pre-deﬁned features,\nlearned features have also been applied to this task. Yu et al.\nutilize sparse spectral codes and a support vector machine\n(SVM) for classifying single-source and multi-source (poly-\nphonic) audio [31]. Han et al. propose to use sparse coding\nfor learning features from mel-spectrograms extracted from\na dataset of single-note audio clips for 24 instruments. A\nSVM is trained to classify the instruments using the learned\nfeatures achieving a classiﬁcation accuracy of around 95%\nfor 24 instrument classes [13].\n2.2 Instrument Recognition in Polyphonic Music\nRecent work on instrument recognition has focused on poly-\nphonic musical signals. Polyphonic audio synthesized from\ndatasets of individual instrument sounds, such as the RWC\ndataset [10], as well as real-world audio recordings have\nbeen used for this task.\nKitahara et al. extract spectral and temporal features\nalong with PCA and Latent Discriminant Analysis (LDA)\nfor classiﬁcation in duo and trio music [17]. Heittola et al.\ncombine the results of Non-negative Matrix Factorization\n(NMF) with excitations of notes obtained from a multi-pitch\ntracking algorithm [18] to extract harmonic spectra from\na mixture signal. The separated spectra are represented by\nMFCCs and classiﬁed with a GMM [14].\nFuhrmann et al. extract a large set of features repre-\nsenting an audio clip and perform predominant instrument\ndetection in real-world audio signals using one SVM per\ninstrument [9]. The ‘predominant’ instrument is deﬁned\nas one with continuous presence in a snippet of audio and\nis easily audible for a human listener. Bosch et al. extend\nthe work by utilizing source separation to segregate the\npolyphonic audio into streams: ‘bass,’ ‘drums,’ ‘melody,’\nTraining \nSet\nTesting \nTracks Pre-processing \nPre-processing DNN Prediction DNN Training Trained \nModel \nInstrument \nActivity Temporal \nAggregation Figure 1 . Block Diagram for DNN-based IAD System\nand ‘other.’ The segregated audio is subsequently used for\nclassiﬁcation using the aforementioned system [2].\nHan et al. apply deep CNNs for the task of predominant\ninstrument recognition and report a signiﬁcant improvement\nof results over previous approaches [12]. The authors also\nprovide an in-depth discussion of the model parameters and\na qualitative analysis of the CNN models.\n2.3 Related Tasks\nIn music tagging, a track is labeled with a variety of labels\nthat describe it, such as genre, instruments, and mood. IAD\nmay be considered a sub-task in music tagging since the\ntags often include instrumentation. Choi et al. use CNNs\nand CRNNs for the task of automatic tagging [5, 6]. Liu\nand Yang further proposed a method to localize the events\nin music tagging [20] which may be compared to IAD.\nSound Event Detection (SED) aims at detecting envi-\nronmental sounds in a stream of audio. Some examples of\nsound events are gunshots, car horns, baby cries, dog barks,\netc. Cakir et al. explore this task with deep neural networks\non a dataset of environmental sounds [3, 4]. The main dif-\nference between SED and IAD is that in SED the sound\nevents are uncorrelated and thus easier to discriminate while\nmusical sources tend to have higher correlation in popular\nmusic. Music instrument sounds might also have a rich\nharmonic structure absent in most environmental sounds.\n3. METHOD\nA high-level block diagram for the presented IAD system\nis shown in Fig. 1. The individual processing steps are\ndescribed in detail below.\n3.1 Pre-processing\nAll tracks are downsampled to 22.05 kHz, downmixed to\nmono and normalized by the root mean square energy. Each\ntrack is the chunked to 1 s long snippets. Each snippet is\ntransformed into a mel-spectrogram, which is motivated\nby the non-linear frequency resolution of the human audi-\ntory system [22], and has been proven to be a useful input\nrepresentation for multiple MIR tasks such as automatic\ntagging [5], onset detection [25], and feature learning [29].\nThe mel-spectrograms are calculated using Librosa [21]\nwith 96mel bands from 0–11.025 kHz. The block size and\nhop size are 46.4 ms and 11.6 ms, respectively. Decibel\nscaling is applied to the Mel-Spectrogram energies. The\nresult is a matrix of dimension 96\u000286.570 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018CNN CRNN\nConv2D\nk= 3\u00023; d= 64\nMP(p= 2;2)\nConv2D\nk= 3\u00023; d= 128\nMP(p= 2;2)\nConv2D Conv2D\nk= 3\u00023; d= 256 k= 3\u00023; d= 256\nMP(p= 3;3) MP(p= 2;2)\nConv2D Conv2D\n(k= 3\u00023; d= 640) (k= 3\u00023; d= 256)\nMP(p= 3;3) MP(p= 2;2)\nFC(h= 128) GRU (h= 256)\nFC(h= 18)\nTable 1 . Model Architecture. (Conv2D: 2D Convolutional\nLayer, MP: 2D Max-Pooling, k: kernel size, d: ﬁlter depth)\n3.2 Model Architectures\nDeep Neural Networks (DNNs) have consistently outper-\nformed traditional MIR approaches in several tasks such\nas, music transcription [26, 30], onset detection [25], music\ntagging [5]. As this is also true for predominant instrument\nclassiﬁcation (compare Sect. 2), we choose to investigate\nDNNs for the task of IAD. Our architectural choices are\ninﬂuenced by the work of both Choi and Cakir [4–6].\nThe usability of DNNs stems from their ability to ap-\nproximate complex non-linear functions mapping an input\nfeature space to the outputs. This enables researchers to\nprovide raw or minimally processed data to a DNN so that\nit may learn features relevant for the task at hand.\nWe compare the three broad classes of DNNs: multi-\nlayer perceptrons, convolutional neural networks, and —\nsince convolutional networks are useful for acoustic mod-\neling [5]— a convolutional-recurrent network instead of a\ntraditional RNN. The beneﬁt of CRNN lies in the fact that it\nis able to learn both local and temporal features. Note that\nthe model hyperparameters have been chosen so that the\nnumber of parameters for the three models is comparable.\n3.2.1 Multi-Layer Perceptron\nThe input mel-spectrogram matrix is ﬂattened into a vector\nfor the MLP model. A fairly simple architecture is chosen:\n4hidden layers with 256hidden units in each layer and an\noutput layer of 18hidden units. Dropout [28] is used with a\nkeep probability of 0:5at each layer.\n3.2.2 Convolutional Neural Network\nThe CNN architecture is shown in Table 1 (left). Small\nsquare ﬁlters are chosen in order to facilitate hierarchical\nfeature learning from local patches that grow larger in size\nwith network depth. In order to preserve spatial dimensions,\nstride of 1andSame zero-padding scheme is used for all\nthe convolutional layers. Each Conv2D layer is followed by\nbatch-normalization [16] and the Exponential Linear Unit\n(ELU) [7] activation function. The ﬁnal convolution layer’s\noutput is ﬂattened before feeding it to a fully connected\nlayer. Finally, we connect to an output layer of 18 units\nwith a sigmoid activation function.Train Test\nInstrument Abbr. T # T #\ndrum set dru 300 720036 79 15957\nelectric bass bgtr 253 620592 62 13344\nmale singer ms 200 351384 62 10038\ndist. elec. gtr dgtr 171 396204 40 7522\nclean elec. gtr cgtr 119 225456 34 5875\nsynthesizer syn 118 295524 33 5712\nacoustic gtr agtr 91 230556 25 5241\npiano pf 89 187536 24 4063\nvocalists vox 84 154596 12 1895\nfemale singer fs 79 149232 23 3733\nstring section str 24 39444 10 1278\nelec. piano epf 24 52680 14 2075\nelect. organ eorg 22 39516 11 2117\ndouble bass db 21 40116 9 1786\ncello vc 13 22176 9 1623\nviolin vn 10 28452 15 2385\ntabla tab 9 41640 3 806\nﬂute ﬂ 7 9972 7 1171\nTable 2 . Dataset distribution: T denotes tracks and # de-\nnotes 1 s snippets\n3.2.3 Convolutional Recurrent Neural Network\nThe CRNN architecture is shown in Table 1 (right). CRNNs\nhave been applied to tasks such as music tagging [6] and\nsound event detection [4]. We hypothesize that it is a good\nchoice for IAD since we want the model to learn from the\nevolution of spectra over time. The same conﬁguration of\npadding and striding, batch-normalization, and non-linear\nactivation is used for the convolutional modules. Only the\ndepth and height of the ﬁnal Conv layer output is ﬂattened,\nthus preserving the temporal structure of the high-level\nConvNet features. Finally, the last GRU output is connected\nto the output layer consisting of 18 units with a sigmoid\nactivation function.\n3.2.4 Training Procedure\nBinary cross-entropy is used as the loss function for all\nmodels. Stochastic gradient descent with a learning rate\nof0:0001 and momentum of 0:9is used to optimize the\nloss function. The models are trained using batches of 32\ninstances for 20epochs, which is sufﬁcient for the training\nand validation loss to converge for each of the architectures.\n3.3 Temporal Aggregation\nSince the neural networks are trained using 1 s snippets of\naudio, a prediction is made for every 1 s in the test track. For\nexperiments and evaluation with varying time-resolution,\nwe max-pool the predictions and the ground truth over\nnon-overlapping segments according to the desired time-\nresolution. For example, in order to have a 5 s resolution,\nthe maximum across 5continuous predictions for every\ninstrument is chosen as the predicted score for the corre-\nsponding 5 s snippet in the track.\n4. EVALUATION\n4.1 Dataset\nThe dataset used in previous work on predominant instru-\nment detection [2, 9, 12], IRMAS, consists of a training setProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 571with 3 s audio snippets manually annotated with one of 11\npredominant (but non-percussive) instrument labels. These\nsnippets may contain other instruments. The testing set\ncontains audio snippets of variable length with 1or more\npredominant instruments. We believe that training using\npolyphonic audio labeled with a single instrument may not\nbe the ideal strategy for IAD. In this paper, we used multi-\ntrack audio to construct a dataset for IAD. The motivation\nbehind using multi-track datasets is that the annotations for\ninstrument activity can be generated automatically using\nstem energy as opposed to human annotations which may\ncontain more errors. In addition, each snippet may con-\ntain multiple instrument labels, providing the models richer\nground truth.\nTwo publicly available multi-track datasets are used for\ntraining and testing of the models. MedleyDB [1] and Mix-\ning Secrets [11] were combined for this task in order to\nincrease the number of tracks. In a pilot study involving\nonly MedleyDB, we observed a signiﬁcant improvement in\nmodel performance as the amount of training data increased.\nMedleyDB contains 330multi-tracks and Mixing Secrets\ncontains 258multi-tracks. The two datasets combined con-\ntain tracks with approximately 100different instruments.\nFor this paper we consider 18most frequently occurring in-\nstruments. The instruments considered are listed in Table 2 .\nNote that the tracks may contain other instruments that the\nIAD system is not trained to detect.\nEach multi-track in the dataset is associated with a mixed\ntrack. Instrument activation conﬁdence is annotated auto-\nmatically according to the process described in [1]. These\nannotations are computed with time-resolution of 0.0464 s.\nFor our IAD system, however, we deﬁned the minimum\ntime-resolution to be 1 s. The annotations are aggregated by\npicking the maximum value across the time-axis to obtain\none activation value per instrument per snippet. This allows\nfor instruments to have a large activation value in the snip-\npet even if they were active for a small period of time, as\nopposed to a value close to 0if the mean was chosen for\naggregation. Finally, the activations are binarized with a\nﬁxed threshold \u0012= 0:5.\nThe datasets contain tracks where the stems have cross-\ntalk or bleed. For these tracks, stem activations for a certain\ninstrument may contain activity from another instrument.\nTo prevent incorrect annotations, tracks with bleed are not\nconsidered for the IAD dataset, although we make excep-\ntions for rare instruments such as tabla. Additionally, tracks\nwithout a single instrument of interest are not considered.\nSubsequently, the dataset is split into a training and a\ntesting set. We generate a random artist-conditional split to\nprevent the album or artist effect in the testing phase. The\nsplit is chosen such that there is a reasonable number of\ntracks per instrument. Table 2 lists the distribution of the\ndata for the split. The training set consists of 361tracks\nand the testing set consists of 100tracks.1The training set\nis augmented using pitch-shifting: 6semitones lower to 5\nhigher than the original with 1semi-tone increments.\n1The track IDs for the dataset splits used are available at\nhttps://github.com/SiddGururani/ISMIR20184.2 Experimental Setup\nFirst, we preprocess both splits of data as described in\nSect. 3.1 resulting in a time-frequency input representation\nand ground truth pair for each 1 second snippet. Table 2\nlists the distribution of the different instrument classes in\nterms of 1 second snippets. Next, we train each of the DNN\narchitecture as described in Sect. 3.2. Since the models\nwere observed to converge to a solution in 20 epochs, we\ndo not perform any form of early-stopping. In addition,\nwe generate a validation set using a randomly sampled set\nof tracks from both the training and testing set due to lack\nof data. 50tracks from the training and testing splits are\npicked, resulting in a validation set of 100tracks. We use\nthis scheme since we want to validate on unseen data while\nnot using the entire test set. The validation set is used to\nevaluate the models at the end of each epoch. Finally, we\ntest the best performing model for each class of DNNs.\nWe test the models for various time-resolutions of activity\ndetection: 1 s, 5 s, 10 s and track-level aggregation.\n4.3 Evaluation Metrics\nEvaluation of IAD systems, when looked at in detail, poses\nsome challenges. Since each snippet has zero or more in-\nstruments, IAD is a multi-label classiﬁcation problem. The\nsigmoid activation leads to an output between 0and1, de-\nnoting the predicted activity of that instrument. However, as\npointed out by Han et al. [12], binarizing the outputs using a\nﬁxed threshold and evaluating the accuracy depends on the\nselected threshold. Additionally, the dataset is not balanced\nacross the instrument classes, hence stressing the need for\nmetrics robust against unbalanced class distribution.\nPrevious work on predominant instrument recognition\nuses metrics relevant for multi-class classiﬁcation systems\nsuch as precision, recall and f-measure [2,12]. Since IAD is\na multi-label classiﬁcation problem, we use Label Ranking\nAverage Precision (LRAP) and the Area Under Receiver\nOperating Characteristic curve (AUC-ROC).\n4.3.1 Label Ranking Average Precision\nLRAP was proposed in [24] to evaluate multi-label classiﬁ-\ncation systems. Intuitively, the LRAP measures the ability\nof a model to assign better ranks to true labels for an in-\nstance. For example, if all the true labels for an instance\nare ranked higher than other labels in consideration, the\nranking precision for this instance is 1. LRAP measures the\naverage ranking precision across all the instances. In our\nexperiments, we compute LRAP using 2approaches: (i) Mi-\ncro: LRAP computed using the concatenated outputs for\nall testing tracks. (ii) Macro: computed on the track level\nand averaged. This normalizes any effect of track length on\nthe model performance, which could skew the results, for\ninstance, if the model performs well for a particular long\nsong but poorly for shorter songs with fewer snippets.\n4.3.2 Area Under ROC Curve\nThe AUC-ROC or, in short, AUC is computed by ﬁrst plot-\nting the true positive rate and false positive rate on a plane\nfor various classiﬁcation thresholds, which results in a curve.572 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018MLP-micro\nMLP-macro\nCNN-micro\nCNN-macro\nCRNN-micro\nCRNN-macroFigure 2 . LRAP for various time-resolutions\nAUC is the area under this curve. It measures the probability\nthat the model assigns a higher score to a randomly selected\npositive instance than a negative instance. The AUC gives\na summary of the model performance without the need to\nadjust a threshold for binarization.\nSince AUC is usually applied to binary classiﬁcation,\nwe compute it per instrument class. Only the micro AUC\nis computed as not all tracks contain all instruments. We\nreport an average AUC by taking the mean of the micro\nAUC per class.\nThe reason for selecting AUC instead of precision, recall\nor f-measure is that most literature on instrument classiﬁca-\ntion tends to use a common ﬁxed threshold for all classes\nwhich bears the risk of being suboptimal. Han et al. suggest\nthe use of a different threshold per instrument class [12].\nUsing the AUC to summarize model performance alleviates\nthe problem of threshold selection while making it easier to\ndirectly compare model performance.\n4.4 Confusion Visualization\nIn multi-class classiﬁcation, every data sample has only one\npossible prediction and one ground truth label. A confusion\nmatrix visualizes the frequency of confusion between every\npair of predicted class label vs. ground truth class label.\nIn a multi-label classiﬁcation problem such at IAD, every\ninstance has multiple possible predictions and zero or more\nground truth labels. Hence, a traditional confusion matrix\ncannot be computed. However, as a confusion matrix is an\nintuitive way to gain insights into the model, we propose an\nalternative form of confusion visualization computed from\nthe binarized predictions and the ground truths.\nWe hypothesize that an instrument is wrongly detected\ndue to the activity of some instrument present in the audio.\nWe are particularly interested in looking at which instru-\nments were incorrectly missed (false negative) when an\ninstrument was wrongly detected (false positive). For a\nparticular false positive instrument, this is equivalent to\nlooking at the probability of observing false negatives for\nthe other instruments. This probability can be estimated\nusing a histogram of false negatives. Vertically stacking\nthese histograms for each instrument results in a matrix of\ndimensionC\u0002C(C=number of instrument classes). We\n1 s\n5 s\n10 s\nTrackFigure 3 . AUC per instrument for CRNN model\nconvert the histograms to probabilities by normalizing each\nrow of the matrix to a sum of 1.\nNote that unlike a traditional confusion matrix, this is\nnot a symmetric matrix. We only focus on one row at a\ntime in order to compare probabilities of observing false\nnegatives for a given false positive instrument.\n5. RESULTS AND DISCUSSION\nA comparison of model performance is summarized in\nFigure 2 andTable 3 . It can be observed that CNN and\nCRNN outperform MLP in both metrics. This is expected\nsince the convolutional layers allow the model to learn\nhierarchical acoustic features from the time-frequency rep-\nresentation more efﬁciently. However, the CRNN does not\noutperform the CNN, which may be attributed to the fact\nthat only 1 s second snippets are used. The temporal dimen-\nsion of the input is reduced to only 5time steps after the 4\nCNN layers. The beneﬁts of using recurrent layers are more\nnoticeable when longer sequences are involved as in work\nby Choi et al. where they use inputs of length 29 s [6]. In ad-\ndition, the receptive ﬁeld of the deeper layers of the CNN is\nlarge enough for learning temporal features. Another obser-\nvation is that output aggregation tends to improve models’\nlabel ranking performance and mean AUC.\nFigure 3 shows the AUC per instrument of the CRNN\nmodel for the chosen time-resolution aggregation. We ob-\nserve that using output aggregation in time leads to better\nperformance in almost all instrument classes. The model\nachieves high AUC not only for majority instruments in\nthe dataset but also for minority instruments such as ﬂute,\nviolin and cello, suggesting that it not simply predicting the\nmajority. We also observe that the model does not seem to\nperform well for vocals in general. While it does achieve\nhigh AUC for male singers, the AUC for female singers\nand vocalists is low. We investigate this further using the\nMLP CNN CRNN\n1 s 71.28 77.55 77.5\n5 s 70.85 78.35 78.76\n10 s 70.82 78.59 79.22\nTrack 71.1 80.92 80.1\nTable 3 . Mean AUC for various time-resolutionsProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 57300.050.10.15Figure 4 . Distribution of false negative instruments condi-\ntioned on a true positive of a particular instrument\nvisualization method described in Sect. 4.4.\nTo construct the confusion visualization as described in\nSect. 4.4, we pick the CRNN model and use the 1 s time-\nresolution outputs for the test set. Picking the threshold\nfor binarizing the predictions is not straightforward. A\nﬁxed threshold of 0:5, for example, led to 0detections for\nstring section and electronic organ. Therefore, we adjusted\nthe best thresholds for each class. These thresholds are\ndetermined by computing the class-wise f-measure at all\nscore thresholds and selecting the threshold giving the best\nf-measure in the validation set. Figure 4 shows the con-\nstructed visualization. A high value in a row implies that\nthe model may be confusing that particular pair of instru-\nments more often than others. The following observations\ncan be made from the ﬁgure:\n\u000f(bgtr,db): This confusion is possibly due to similar\nfrequency range of the electric bass and double bass.\n\u000f(dgtr,agtr), (dgtr,cgtr), (cgtr,dgtr), and ( agtr,dgtr):\nWhile confusion between acoustic and distorted gui-\ntar is unusual, the confusion between clean and dis-\ntorted guitar is possibly explained by the variety in\ntone for both the clean and distorted guitars. A light\ncrunch or low gain setting may possibly get misclas-\nsiﬁed. This could also explain the poor performance\nfor clean electric guitar.\n\u000f(dru,tab) and ( tab,dru): Both drum set and tabla are\npercussive instruments. In addition, one of the test\ntracks containing tabla has a ‘drum machine’ label\nwhich possibly causes drum false positives.\n\u000f(dgtr,syn) and ( syn,dgtr): This is an interesting case\nsince the variance in sound for both, the distorted\nguitar and synthesizers, is very large. Further investi-\ngation is needed to understand this case.\nNext, we investigate the poor model performance on\nvocal classes. Figure 4 shows confusion between male and\nfemale singers implying that the model might be incorrectly\nclassifying female singers as male. In order to investigate\nthis phenomenon, the three vocal classes were combined for\na follow-up experiment. We max-pool the predictions and1 s 5 s 10 s Track\nAverage AUC\n(ms, fs, vox)0.709 0.725 0.737 0.782\nAUC vocals 0.822 0.96 0.975 0.998\nTable 4 . AUC for different time resolutions comparing\npooled vocals against averaged AUC for vocal classes\nthe ground truth for these three classes, and recompute the\nAUC for this new ‘vocals’ class. Table 4 shows the average\nAUC of the three classes and the AUC of the combined ‘vo-\ncals’ class. The model performs signiﬁcantly better for the\ncombined class conﬁrming our hypothesis that it confuses\nthe vocal classes.\nAnother interesting ﬁnding is that the best threshold\nchosen per instrument for binarization ranges from 0:02\nto0:55with lower thresholds for minority instruments in\ngeneral. We observe a correlation coefﬁcient of 0:9between\nthe thresholds and the training data distribution suggesting\nthat the model has learned biases in the dataset. The impact\nof this ﬁnding requires further experiments.\n6. CONCLUSION\nWe presented a DNN-based IAD system trained using multi-\ntrack datasets to detect 18instruments. The CRNN and\nCNN outperform MLP architectures for the task and per-\nform well for detecting instruments common in popular mu-\nsic, such as drums, electric bass, acoustic guitars, distorted\nguitars and vocals. It also performs well for instruments in\nclassical music such as ﬂute, cello, violin even though they\nwere under-represented in the dataset. We also stress the\nneed for multiple metrics and visualizations for evaluation\nof systems such as IAD which is non-trivial to evaluate.\nAs future work, a few extensions and research directions\nare: (i) pre-training the network using monophonic stems\nfrom the multi-track datasets and subsequently training and\ntesting for IAD, (ii) designing the convolutional network\nfor the CRNN as proposed by Jordi et al. [23] instead of the\ncurrently used 3\u00023ﬁlters as is common in computer vision,\n(iii) converting the proposed monolithic model architecture\nfor IAD to a hierarchical architecture for instrument family\nclassiﬁcation ﬁrst and subsequently instrument classiﬁca-\ntion. While this paper treats the model as a black box and\nfocuses on evaluation and analysis of model outputs, it is\nworth studying the model to understand the internal repre-\nsentations by means of visualization tools such as t-SNE\nand saliency maps [27] as performed by Han et al. [12].\nBy drawing attention to challenges in IAD with this\npaper, we hope to encourage the MIR community to explore\nthis task. IAD is a rewarding avenue for research due to its\nreal-world use cases as well as the potential to augment and\nimprove performance in other tasks in MIR.\n7. ACKNOWLEDGMENTS\nThis work was funded by Gracenote. We thank them for\ntheir generous support. We would also like to thank Nvidia\nfor supporting us with a Titan Xp awarded as part of the\nGPU grant program.574 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 20188. REFERENCES\n[1]Rachel M Bittner, Justin Salamon, Mike Tierney,\nMatthias Mauch, Chris Cannam, and Juan Pablo\nBello. Medleydb: A multitrack dataset for annotation-\nintensive mir research. In Proc. of the International\nSociety for Music Information Retrieval Conference (IS-\nMIR) , pages 155–160, 2014.\n[2]Juan J Bosch, Jordi Janer, Ferdinand Fuhrmann, and\nPerfecto Herrera. A comparison of sound segregation\ntechniques for predominant instrument recognition in\nmusical audio signals. In Proc. of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 559–564, 2012.\n[3]Emre Cakir, Toni Heittola, Heikki Huttunen, and Tuo-\nmas Virtanen. Polyphonic sound event detection using\nmulti label deep neural networks. In Proc. International\nJoint Conference on Neural Networks (IJCNN) , pages\n1–7, 2015.\n[4]Emre Cakir, Giambattista Parascandolo, Toni Heittola,\nHeikki Huttunen, Tuomas Virtanen, Emre Cakir, Gi-\nambattista Parascandolo, Toni Heittola, Heikki Hut-\ntunen, and Tuomas Virtanen. Convolutional recurrent\nneural networks for polyphonic sound event detection.\nIEEE/ACM Transactions on Audio, Speech and Lan-\nguage Processing (TASLP) , 25(6):1291–1303, 2017.\n[5]Keunwoo Choi, Gy ¨orgy Fazekas, and Mark Sandler.\nAutomatic tagging using deep convolutional neural net-\nworks. In Proc. of the International Soceity of Music\nInformation Retrieval Conference (ISMIR) , pages 805–\n811, 2016.\n[6]Keunwoo Choi, Gy ¨orgy Fazekas, Mark Sandler, and\nKyunghyun Cho. Convolutional recurrent neural net-\nworks for music classiﬁcation. In Proc. of the Inter-\nnational Conference on Acoustics Speech and Signal\nProcessing (ICASSP) , pages 2392–2396, 2017.\n[7]Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and accurate deep network learning\nby exponential linear units (elus). International Confer-\nence on Learning Representations (ICLR) , 2015.\n[8]Slim Essid, Ga ¨el Richard, and Bertrand David. Musical\ninstrument recognition on solo performances. In Proc.\nof the 12th European Signal Processing Conference ,\npages 1289–1292, 2004.\n[9]Ferdinand Fuhrmann, Mart ´ın Haro, and Perfecto Her-\nrera. Scalability, generality and temporal aspects in au-\ntomatic recognition of predominant musical instruments\nin polyphonic music. In Proc. of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , pages 321–326, 2009.\n[10] Masataka Goto. Rwc music database: Music genre\ndatabase and musical instrument sound database. In\nProc. International Conference on Music Information\nRetrieval, 2003 , pages 229–230, 2003.[11] Siddharth Gururani and Alexander Lerch. Mixing se-\ncrets: A multitrack dataset for instrument detection in\npolyphonic music. In Late Breaking Demo (Extended\nAbstract), Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nSuzhou, 2017.\n[12] Yoonchang Han, Jaehun Kim, Kyogu Lee, Yoonchang\nHan, Jaehun Kim, and Kyogu Lee. Deep convolutional\nneural networks for predominant instrument recognition\nin polyphonic music. IEEE/ACM Transactions on Audio,\nSpeech and Language Processing (TASLP) , 25(1):208–\n221, 2017.\n[13] Yoonchang Han, Subin Lee, Juhan Nam, and Kyogu\nLee. Sparse feature learning for instrument identi-\nﬁcation: Effects of sampling and pooling methods.\nThe Journal of the Acoustical Society of America ,\n139(5):2290–2298, 2016.\n[14] Toni Heittola, Anssi Klapuri, and Tuomas Virtanen. Mu-\nsical instrument recognition in polyphonic audio using\nsource-ﬁlter model for sound separation. In Proc. of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 327–332, 2009.\n[15] Perfecto Herrera-Boyer, Geoffroy Peeters, and Shlomo\nDubnov. Automatic classiﬁcation of musical instrument\nsounds. Journal of New Music Research , 32(1):3–21,\n2003.\n[16] Sergey Ioffe and Christian Szegedy. Batch normaliza-\ntion: Accelerating deep network training by reducing in-\nternal covariate shift. In Proc. of the 32nd International\nConference on Machine Learning (ICML) , volume 37,\npages 448–456. PMLR, 2015.\n[17] Tetsuro Kitahara, Masataka Goto, Kazunori Komatani,\nTetsuya Ogata, and Hiroshi G Okuno. Instrument iden-\ntiﬁcation in polyphonic music: Feature weighting to\nminimize inﬂuence of sound overlaps. EURASIP Jour-\nnal on Applied Signal Processing , 2007(1):155–155,\n2007.\n[18] Anssi Klapuri. Multiple fundamental frequency estima-\ntion by summing harmonic amplitudes. In Proc. of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , pages 216–221, 2006.\n[19] AG Krishna and Thippur V Sreenivas. Music instrument\nrecognition: from isolated notes to solo phrases. In Proc.\nof the International Conference on Acoustics Speech\nand Signal Processing (ICASSP) , volume 4, pages iv–iv,\n2004.\n[20] Jen-Yu Liu and Yi-Hsuan Yang. Event localization in\nmusic auto-tagging. In Proc. of the 2016 ACM on Mul-\ntimedia Conference , pages 1048–1057. ACM, 2016.\n[21] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Nieto.\nlibrosa: Audio and music signal analysis in python. In\n14th python in science conference , pages 18–25, 2015.Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018 575[22] Brian CJ Moore. An Introduction to the Psychology of\nHearing . Brill, 2012.\n[23] Jordi Pons, Olga Slizovskaia, Rong Gong, Emilia\nG´omez, and Xavier Serra. Timbre analysis of music\naudio signals with convolutional neural networks. In\nProc. of the 25th European Signal Processing Confer-\nence, Kos island, Greece, 28/08/2017 2017. IEEE.\n[24] Robert E Schapire and Yoram Singer. Boostexter: A\nboosting-based system for text categorization. Machine\nlearning , 39(2-3):135–168, 2000.\n[25] Jan Schluter and Sebastian Bock. Improved musical\nonset detection with convolutional neural networks.\nInProc. of the International Conference on Acoustics\nSpeech and Signal Processing (ICASSP) , pages 6979–\n6983, 2014.\n[26] Siddharth Sigtia, Emmanouil Benetos, and Simon\nDixon. An end-to-end neural network for polyphonic\npiano music transcription. IEEE/ACM Transactions\non Audio, Speech and Language Processing (TASLP) ,\n24(5):927–939, 2016.\n[27] Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. Deep inside convolutional networks: Visualising\nimage classiﬁcation models and saliency maps. Interna-\ntional Conference on Learning Representations (ICLR) ,\n2013.\n[28] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. Dropout: A\nsimple way to prevent neural networks from overﬁtting.\nThe Journal of Machine Learning Research , 15(1):1929–\n1958, 2014.\n[29] Aaron Van den Oord, Sander Dieleman, and Benjamin\nSchrauwen. Deep content-based music recommenda-\ntion. In Advances in Neural Information Processing\nSystems (NIPS) , pages 2643–2651, 2013.\n[30] Richard V ogl, Matthias Dorfer, and Peter Knees. Drum\ntranscription from polyphonic music with recurrent neu-\nral networks. In Proc. of the International Conference\non Acoustics Speech and Signal Processing (ICASSP) ,\npages 201–205, 2017.\n[31] Li-Fan Yu, Li Su, and Yi-Hsuan Yang. Sparse cepstral\ncodes and power scale for instrument identiﬁcation.\nInProc. of the International Conference on Acoustics\nSpeech and Signal Processing (ICASSP) , pages 7460–\n7464, 2014.576 Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018"
    },
    {
        "title": "Mono-to-Stereo Through Parametric Stereo Generation.",
        "author": [
            "Joan Serrà",
            "Davide Scaini",
            "Santiago Pascual",
            "Daniel Arteaga",
            "Jordi Pons",
            "Jeroen Breebaart",
            "Giulio Cengarle"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265285",
        "url": "https://doi.org/10.5281/zenodo.10265285",
        "ee": "https://zenodo.org/records/10265285/files/000035.pdf",
        "abstract": "Generating a stereophonic presentation from a monophonic audio signal is a challenging open task, especially if the goal is to obtain a realistic spatial imaging with a specific panning of sound elements. In this work, we propose to convert mono to stereo by means of predicting parametric stereo (PS) parameters using both nearest neighbor and deep network approaches. In combination with PS, we also propose to model the task with generative approaches, allowing to synthesize multiple and equally-plausible stereo renditions from the same mono signal. To achieve this, we consider both autoregressive and masked token modelling approaches. We provide evidence that the proposed PS-based models outperform a competitive classical decorrelation baseline and that, within a PS prediction framework, modern generative models outshine equivalent non-generative counterparts. Overall, our work positions both PS and generative modelling as strong and appealing methodologies for mono-to-stereo upmixing. A discussion of the limitations of these approaches is also provided.",
        "zenodo_id": 10265285,
        "dblp_key": "conf/ismir/SerraSPAPBC23",
        "keywords": [
            "stereophonic presentation",
            "monophonic audio signal",
            "realistic spatial imaging",
            "parametric stereo (PS)",
            "generative approaches",
            "synthesize multiple renditions",
            "autoregressive and masked token modelling",
            "competitive classical decorrelation baseline",
            "modern generative models",
            "limitations of approaches"
        ],
        "content": "MONO-TO-STEREO THROUGH PARAMETRIC STEREO GENERATION\nJoan Serrà Davide Scaini Santiago Pascual Daniel Arteaga\nJordi Pons Jeroen Breebaart Giulio Cengarle\nDolby Laboratories\nfirstname.lastname@dolby.com\nABSTRACT\nGenerating a stereophonic presentation from a mono-\nphonic audio signal is a challenging open task, especially\nif the goal is to obtain a realistic spatial imaging with\na speciﬁc panning of sound elements. In this work, we\npropose to convert mono to stereo by means of predict-\ning parametric stereo (PS) parameters using both nearest\nneighbor and deep network approaches. In combination\nwith PS, we also propose to model the task with generative\napproaches, allowing to synthesize multiple and equally-\nplausible stereo renditions from the same mono signal. To\nachieve this, we consider both autoregressive and masked\ntoken modelling approaches. We provide evidence that the\nproposed PS-based models outperform a competitive clas-\nsical decorrelation baseline and that, within a PS prediction\nframework, modern generative models outshine equivalent\nnon-generative counterparts. Overall, our work positions\nboth PS and generative modelling as strong and appealing\nmethodologies for mono-to-stereo upmixing. A discussion\nof the limitations of these approaches is also provided.\n1. INTRODUCTION\nSingle-channel monophonic (mono) signals are found in\nmultiple situations, such as historical recordings or current\nones made with a single microphone (e.g., ﬁeld recordings,\namateur band rehearsals, etc.). Even recordings made with\ntwo or more microphones that are not spaced enough or\nthat do not have enough directivity may be better treated\nby downmixing to mono (e.g., mobile phone recordings).\nFurthermore, many processing algorithms, including mod-\nern deep neural network algorithms, cannot yet or are sim-\nply not designed to handle more than one channel. Unlike\nthese scenarios, the most common listening experiences,\neither though loudspeakers or headphones, involve two-\nchannel stereophonic (stereo) signals. Hence the useful-\nness of mono to stereo upmixing.\nClassical approaches to produce a pseudo-stereo ef-\nfect from a mono signal are based on decorrelation. Ini-\n© J. Serrà, D. Scaini, S. Pascual, D. Arteaga, J. Pons,\nJ. Breebaart, and G. Cengarle. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: J. Serrà,\nD. Scaini, S. Pascual, D. Arteaga, J. Pons, J. Breebaart, and G. Cengarle,\n“Mono-to-stereo through parametric stereo generation”, in Proc. of the\n24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.tial approaches used time delays and complementary ﬁl-\nters [1], although all-pass ﬁlters [2] are commonly used\nnowadays, together with multi-band processing to improve\nthe effect [3–5]. Instead of multi-band, estimation of fore-\nground/background time-frequency tiles can also be per-\nformed [6]. Decorrelation approaches, however, only pro-\nvide a mild stereo effect, with limited width, and cannot\nspatially separate individual elements in the mix. To over-\ncome the latter, researchers have considered source sepa-\nration approaches [7–9]. The main idea is that, if individ-\nual elements or tracks are available, those can be panned\nto any location, producing a more realistic spatial image.\nNevertheless, this approach presents several drawbacks:\nﬁrstly, even the best-performing source separation algo-\nrithms produce artifacts [10], which can be highly audi-\nble in the stereo render; secondly, current separation algo-\nrithms are very restrictive in the number and types of el-\nements they can separate [11], thus considerably limiting\ntheir application in real-world spatialization tasks; thirdly,\nafter elements or tracks are separated, it remains to be seen\nhow can they be automatically panned in a realistic manner\n(cf. [12]), which is the reason why separation-based ap-\nproaches usually involve user intervention in the panning\nstage [7–9].\nMusic is a paradigmatic example where, apart from\nstereo capture, artists and engineers massively exploit the\nstereo image to serve a creative artistic intent. Instrument\npanning is a fundamental part of music mixing, and achiev-\ning the right balance requires musical sensibility as well as\ntechnical knowledge [13]. However, apart from some style\nconventions, the stereo image of a music mix is a highly\nsubjective construct: given a set of input tracks, there are\nmany plausible stereo renditions from which selecting the\nﬁnal mix is practically only a matter of artistic choice.\nHence, we posit that this is a perfect ground for modern\ndeep generative models [14]. However, to our surprise, we\nonly found one work using deep neural networks for mono-\nto-stereo [15], with very limited generative capabilities.\nIn this work, we propose the use of machine learning\ntechniques and parametric stereo (PS) decoding [16,17] for\nconverting mono to stereo. PS is a coding technique that al-\nlows to transmit a stereo signal through a mono signal plus\nside information that, with enough bit rate, can be used to\nrecover an almost transparent version of the original stereo\ncontent. By leveraging machine learning techniques, we\ngenerate (or invent) plausible versions of PS parameters in\nsituations where side information is not available. These304parameters can then be used to decode an existing mono\nsignal into a plausible stereo one. We propose two variants\nof PS generation: one based on a classical nearest neigh-\nbor approach [18] and another one based on deep gener-\native modeling. For the latter, we consider both common\nautoregressive modeling [19] and more recent masked to-\nken modeling [20], and show that there can be noticeable\ndifferences between the two. We use subjective testing to\ncompare the proposed approaches and show that PS gen-\neration can produce results that are more appealing than\nconsidered competitive baselines. We also introduce two\nobjective evaluation metrics and discuss the limitations of\nboth PS and generative approaches for mono-to-stereo.\n2. PARAMETRIC STEREO\nPS exploits the perceptual cues that are more relevant to\nour spatial perception of sound, namely the fact that direc-\ntional sources produce interaural level and phase (or time\ndelay) differences, and the fact that diffuse sound ﬁelds\nmanifest as decorrelated signals at the two ears. These\ncues effectively describe how a mono signal is mapped to\nthe left and right stereo channels, and can be measured us-\ning three quantities or parameters [16, 17]: interchannel\nintensity differences (IID), interchannel time differences\n(or, equivalently, phase differences), and interchannel co-\nherence or correlation (IC). PS parameters are computed\nin frequency bands, to reﬂect the frequency-dependent na-\nture of the spatial properties of stereo content, and also\non a frame-by-frame basis, to reﬂect the time-varying na-\nture of frequency cues and spatial images. An important\nobservation is that PS is capable of capturing spatial at-\ntributes that are perceptually relevant and re-instate those\nwithout changing signal levels, tonality, or other artifacts\nthat may arise from methods that operate on audio sig-\nnals directly. In this work, for compactness and ease of\nimplementation, we choose to use the two-parameter ap-\nproach by Breebaart et al. [17], which models IID and IC\nwithout interchannel phase differences, accepting that this\ntwo-parameter approach is not providing the best possible\nquality of PS coding. We now overview this PS coding\nstrategy and introduce the main notation of the article.\n2.1 Encoding\nGiven two complex-valued spectrograms expressed as\ncomplex matrices Xand Y, where rows represent fre-\nquency bins and columns represent frames, we deﬁne the\nband-based cross-spectrogram function\nρ(X,Y) =B(X⊙Y∗),\nwhere⊙denotes elementwise multiplication,∗denotes el-\nementwise complex conjugate, and Bis a matrix with ones\nand zeros that is used to sum frequency bins according\nto a certain frequency band grouping (using matrix mul-\ntiplication). In this work, we use the same spectrogram\nsettings and banding as in [17]: frames of 4,096 samples\nfor 44.1 kHz signals, 75% overlap, a Hann window, and34 bands which are approximately distributed following\nequivalent rectangular bandwidths.\nGiven the two complex spectrograms LandRcorre-\nsponding to the left and right channels of a stereo signal,\nwe can compute the IID using\nPIID= 10log10(ρ(L,L)⊘ρ(R,R)),\nwhere⊘denotes elementwise division. The IC is similarly\nderived from the cross-spectrogram following\nPIC=Re{ρ(L,R)}⊘/radicalbig\nρ(L,L)⊙ρ(R,R),\nwhere Re {}extracts the real part of each complex value\nand the square root is applied elementwise. Notice that\nthe use of the real part instead of the absolute value allows\nto retain information on the relative phase of the two sig-\nnals that would otherwise be lost. We ﬁnally quantize PIID\nandPICby discretizing each matrix element. To do so,\nwe use the same non-uniform quantization steps as in [17]:\n31 steps for IID and 8 for IC. We denote the quantized ver-\nsions as QIIDandQIC.\nTo facilitate subsequent operation, and to prevent po-\ntential prediction mismatches between IID and IC, we join\nboth parameters and treat them as one. For PIIDandPIC,\nwe concatenate them in the frequency axis and form a sin-\ngle matrix P. For QIID\ni,jandQIC\ni,j, we fuse them elementwise\ninto individual integers using the amount of IC quantiza-\ntion steps. This way, Qi,j= 8·QIID\ni,j+QIC\ni,j(note that\nwe can recover back QIID\ni,jandQIC\ni,jusing the division and\nmodulo operators).\n2.2 Decoding\nTo decode the above PS encoding, we perform a mixing\nbetween the available mono signal and a decorrelated ver-\nsion of it. We decorrelate a mono signal Sby applying a\ncascade of 4 inﬁnite impulse response all-pass ﬁlters and\nobtain SD(this all-pass ﬁlter is an enhanced version of the\nbasic one proposed in [17] thanks to transient detection and\npreservation, which avoids time smearing). After that, we\ncan decode the estimated left and right channels ˆLandˆR\nby carefully mixing SandSD. We can do so with\nˆL=Ma⊙S+Mb⊙SD,\nˆR=Mc⊙S+Md⊙SD,\nusing mixing matrices M, which are computed from the\ncoded PS parameters PIIDandPIC. The exact calculation of\nmixing matrices Mis straightforward to obtain by adapting\nto matrix notation the formulation in [17], to which we\nrefer for further detail and explanation.\n3. PARAMETRIC STEREO GENERATION\nWe now explain the proposed approaches for PS genera-\ntion. All of them share the above encoding-decoding for-\nmulation, either using the quantized or unquantized ver-\nsions. During training, stereo signals are used to compute\ninput downmixes S= (L+R)/2and target PS parameters\nPorQ(hence the proposed approaches aim at producingProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n305ˆPorˆQ). Note that, in the case of the generative mod-\nels we consider, one has to additionally input contextual\nPS parameters in a teacher-forcing schema [21]. We also\nwant to note that, since they are quite common practice,\nit is not in the scope of the current work to provide a de-\ntailed explanation of existing generative models (instead,\nwe refer the interested reader to the cited references). In\nall proposed approaches, we tune model hyperparameters\nby qualitative manual inspection in a preliminary analysis\nstage. PS speciﬁcations are predeﬁned and correspond to\nthe ones mentioned in Sec. 2. Neural network approaches\nuse Pytorch’s [22] defaults and are trained with Adam for\n700 epochs using a batch size of 128 and a learning rate of\n10−4, with warmup cosine scheduling.\n3.1 Nearest neighbor\nThe ﬁrst approach proposes to impose the PS parameters\nof existing, similar stereo fragments to individual mono\nframes using a nearest neighbor (NN) algorithm [18]. We\ncall the approach PS-NN. The idea is to retrieve frame-\nbased PS parameters using mono frame sequences, and to\nuse the sequence of those retrieved parameters to decode\nthe mono input. At training time, we randomly select a\nsong, randomly extract an N= 20 frame spectrogram S\nand its corresponding parameters P, and compute a key-\nvalue vector pair (we here use the magnitude spectrogram).\nThe key vector is formed by framewise averaging the en-\nergy in each band,\nk=1\nNN/summationdisplay\nj=1B S:,j, (1)\nand the value vector corresponds to the PS parameters of\nthe last frame, v=P:,N, which allows for a fully-causal\nschema. We repeat the process half a million times and\nstore all pairs in a nearest neighbor structure. At test time,\nfor every frame of the input mono signal, we compute\nan average as in Eq. 1, query the nearest neighbor struc-\nture, retrieve the ˆvvector of the closest neighbor (using\nEuclidean distance), and assign it as the predicted PS pa-\nrameter for that frame. This way, we obtain a sequence of\nestimated PS parameters ˆP.\nIn preliminary analysis, we observed that PS-NN pro-\nduced a high-rate ‘wobbling’ effect between left and right\n(that is, panning was rapidly switching from one channel\nto the other) and presented some temporal inconsistencies\n(that is, sources were unrealistically moving with time,\neven within one- or two-second windows). To counteract\nthese effects, we implemented a two step post-processing\nbased on (i) switching the sign of ˆPIID\n:,jif the Euclidean dis-\ntance toˆPIID\n:,j−1was smaller, and (ii) applying an exponen-\ntial smoothing on the columns of ˆPwith a factor of 0.95.\nThis post-processing substantially reduced the aforemen-\ntioned undesirable effects.\n3.2 Autoregressive\nThe second approach proposes to model PS parameters\nwith a deep generative approach based on an autoregres-sive (AR) transformer [19]. We call the approach PS-AR.\nOur architecture is composed by 7 transformer encoder\nblocks of 512 channels, with 16 heads and a multilayer per-\nceptron (MLPs) expansion factor of 3. We use sinusoidal\npositional encoding at the input, and add a two-layer MLP\nwith an expansion factor of 2 at the output to project to\nthe ﬁnal number of classes (which is 31 ×8 tokens times\n34 bands per frame, see Sec. 2.1). The input is formed by\na projection of the mono spectrogram Sand the teacher-\nforcing information Qinto a 512-channel activation H,\nH=ϕ(S)+B/summationdisplay\ni=1ξi(Qi,:), (2)\nwhereϕis a two-layer MLP with an expansion factor of 2,\nB= 34 is the number of bands, and ξiis a learnable per-\nband token embedding (which includes the mask token,\nsee below). We train the model with weighted categorical\ncross-entropy, using the weight\nw= 1+λσ/parenleftBig/bracketleftbig\nPIID/bracketrightbig\n±ϵ/parenrightBig\n+σ(PIC), (3)\ncalculated independently for every element in the batch.\nIn Eq. 3, σ(X)corresponds to the elementwise standard\ndeviation of X,λ= 0.15compensates for different mag-\nnitudes,[ ]±ϵcorresponds to the clipping operation, and\nϵ= 20 is a threshold to take into account the little per-\nceptual relevance of IIDs larger than 20 dB [23]. In pre-\nliminary analysis, we observed that using wqualitatively\nimproved results, as it shall promote focus on wider stereo\nimages and more difﬁcult cases.\nPS-AR follows a PixelSNAIL recursive approach [24],\nstarting with the prediction of lower frequency bands, then\nhigher frequency bands, and moving into the next frame\nonce all bands are predicted. To efﬁciently exploit the past\ncontext, all input sequences have full-sequence teacher-\nforcing except for the upper frequency bands of the last\nframe, which are masked consecutively and uniformly at\nrandom during training [24]. At test time, we sample re-\ncursively, following the same masking strategy and using\na temperature hyperparameter τ= 0.9. In addition, we\nemploy classiﬁer-free guidance [25] with a hyperparam-\neterγ= 0.25. For that, we use the approach in [26],\nwhich modiﬁes the conditional logits Ucondwith uncondi-\ntional ones Uuncondsuch that\nU= (1+γ)Ucond−γUuncond. (4)\nTo have both a conditional and an unconditional model\nwithin the same architecture, following common practice,\nwe randomly replace ϕ(S)in Eq. 2 by a learnable dropout\ntoken 10% of the time.\n3.3 Masked token modeling\nThe third approach proposes to model PS parameters with\na deep generative approach based on masked token mod-\neling (MTM) [20]. We call the approach PS-MTM. The\narchitecture, loss, inputs, and outputs of the model are the\nsame as in PS-AR, including the cross-entropy weightsProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n306(Eq. 3) and classiﬁer-free guidance (Eq. 4). The only dif-\nference is the masking approach and the sampling proce-\ndure, which implies different hyperparameters for the test-\ning stage (we use τ= 4.5andγ= 0.75, but now the tem-\nperatureτhas a different meaning as explained below).\nMTM generates patch representations Qwith quantized\nelements Qi,jwhich are dubbed as tokens (in our case the\nmatrix Qhas dimensions B×N, withNbeing the num-\nber of considered audio frames; the maximum number of\ntokens in Qi,jis 31×8, as deﬁned in Sec. 2.1). During\ntraining, the teacher-forcing input Qis masked uniformly\nat random, and only the ground truth elements correspond-\ning to the masked positions are used to compute the cross-\nentropy loss at the output. The number of elements to mask\nis also selected at random following a cosine schedule [20]\n(this speciﬁcally includes the case where all patch elements\nare masked). During sampling, patch representations are\nformed with 50% overlap, using no masking for the ﬁrst\nhalf of the patch, similar to [26].\nMTM sampling is an iterative process that achieves or-\nders of magnitude speedups compared to autoregressive\nmodeling (in our case PS-MTM uses 20 steps for a 3 s hop,\nwhile PS-AR requires B= 34 steps for just a single au-\ndio frame of a few milliseconds). MTM iteratively sam-\nples a masked patch, performs predictions with classiﬁer-\nfree guidance [26], chooses the predictions with the high-\nest logit score for the next iteration (they will become un-\nmasked and ﬁxed), and reduces the percent of masked to-\nkens following the same scheduling as in training until no\nmasked elements remain [20]. Differently from training,\nthe masking used in sampling is not random, but based\non logit scores (lowest ones become masked), and noise\nis added to logit scores to promote diversity [20, 26]. In\nour case, we employ Gaussian noise with zero mean and\na standard deviation τ, which becomes our redeﬁned tem-\nperature parameter.\n4. EV ALUATION\nTo train and evaluate all approaches we use a collection\nof professionally-recorded stereo music tracks at 44.1 kHz.\nWe consider 419,954 tracks for training and 10 k for eval-\nuation, and randomly extract a 10 s chunk from each track.\nDuring training, we sample 6 s patches from those and per-\nform data augmentation using a random gain and also ran-\ndomly switching left and right channels.\n4.1 Baselines: regression and decorrelation\nIn addition to the original stereo and its mono downmix,\nwe consider two additional baselines to compare with the\nprevious approaches. The ﬁrst baseline corresponds to an\nablation of the deep generative approaches, and tries to an-\nswer the question of whether a generative component is\nneeded or convenient for the task. Thus, the baseline con-\nsists of a neural network with the exact same conﬁguration\nas PS-AR or PS-MTM, but substituting the generative part\nby standard regression with mean-squared error [18]. We\nterm this baseline PS-Reg, and note that it could be con-sidered an enhanced modern version of the approach of\nChun et al. [15], using PS.\nIt is interesting to mention that, in preliminary analysis,\nwe observed that PS-Reg accurately estimated IC values,\nbut consistently failed to predict IIDs. The predicted IIDs\nhad minimal deviation from zero, which can be attributed\nto the probability distribution function of IID values be-\ning centered around zero with equally plausible deviations\nto the right and to the left. This was an early indication\nthat the one-to-many mapping of IID prediction cannot be\ncorrectly handled by regression methods, and that the task\nwould be better served by a generative approach.\nThe second baseline we consider corresponds to a vari-\nant of classical decorrelation approaches. Here, the decor-\nrelation is implemented by means of an all-pass ﬁlter\nnetwork enhanced by (i) detection and preservation of\ntransients, and (ii) a frequency-dependent mix between\noriginal and decorrelated signals to achieve a frequency-\ndependent IC. We term this baseline Decorr, and we note\nthat it could be considered an improved modern version of\nthe approaches [1–6].\n4.2 Objective measures\nTo the best of our knowledge, there are no objective mea-\nsurements for plausible stereo renderings nor suitable PS\nprediction scores. Due to the highly creative/subjective na-\nture of the task, common error measurements may not be\nappropriate. Therefore, as a way of measuring progress,\nwe propose to use a couple of metrics inspired from the lit-\nerature on generative modeling (cf. [14]). The ﬁrst metric\nwe consider is the minimum error on a large sample basis,\nEmin. Given a large sample of generated PS parameters\n(K= 128 for a single audio excerpt), Eminchooses the\nminimum error with respect to the ground truth:\nEmin= min\nk\n/summationdisplay\ni,jδ/parenleftBig\nPi,j,ˆP(k)\ni,j/parenrightBig\n,\nwhereδis a suitable error function. The idea is that if we\nallow the model to generate many samples for every input,\nin the limit of very large Kone of them should come close\nto the ground truth. For PS parameters, we use absolute\nerrors, weight the IID to compensate magnitudes with IC,\nand take into account some perceptual relevance for IID as\nin Sec. 3.2 and Eq. 3:\nδ(x,y) =/braceleftBigg\nλ|[x]±ϵ−[y]±ϵ|for IID,\n|x−y| for IC.\nThe second metric we consider is the Fréchet distance\non the PS parameter space, DF. Given a pool of PS param-\netersPand aKtimes larger pool of generated parameters\nˆP, assuming Gaussian distributions, DFis computed as\nDF=/vextendsingle/vextendsingle/vextendsingleµ(P)−µ(ˆP)/vextendsingle/vextendsingle/vextendsingle2\n+\n+Tr/braceleftbigg\nσ(P)+σ(ˆP)−2/radicalBig\nσ(P)σ(ˆP)/bracerightbigg\n,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3070.00.20.40.60.81.0PreferenceAfrican2 Electro1 Jazz1 Latin3 Rap1 Rock1 Soul2 All items\nMono PS-Reg Decorr PS-AR PS-NN PS-MTM StereoFigure 1 . Preference results for the items included in the subjective test (Sec. 4.3). Markers indicate average values and\nvertical bars indicate the 95% conﬁdence interval associated to them.\nwhere Tr{}denotes the matrix trace and µandσcorre-\nspond to the mean vector and the covariance matrix over\nframes, respectively. The Fréchet distance has become a\nstandard measure in generative modeling where, instead\nof the PS parameters used here, activations of pre-trained\nclassiﬁcation networks are used. We will see that it is also\nable to provide some informative cues in our task (Sec. 5).\n4.3 Subjective evaluation\nGiven the creative/subjective nature of the task, the best\nway to measure performance is through subjective testing.\nIn this study, we ran a preference test with 24 listeners on\n7 song excerpts of 10 s from the test set. To select those ex-\ncerpts, we ranked the test excerpts based on w(Eq. 3) and\nrandomly selected them from the top quartile. When do-\ning so, we manually veriﬁed that the selected excerpts cov-\nered distinct musical genres and ensured that a PS-decoded\nversion did not exhibit signiﬁcant coding degradation (this\nway, we prime the listener to focus on the stereo image\ninstead of potential artifacts introduced by our implemen-\ntation of PS, Sec. 2).\nThe test consisted in providing a rating between 0 and\n100 to 7 approaches: the three proposed ones, the two\nbaselines, the mono downmix, and the original stereo sig-\nnal (professional mix, non-coded). Mono and stereo sig-\nnals provide us with intuitive bounds for the analysis of\npreference, and also serve us to discard non-expert lis-\nteners. Indeed, we found that the task is quite hard\nfor non-experts, who provided many inconsistent ratings\nwhen asked to evaluate an appropriate balance between\nthe width and the clarity of the mix. We used the most\nobvious of those inconsistencies to discard listeners from\nthe test, namely the fact that they rated mono (input) over\nstereo (professional mix) in one or more occasions. Half\nof the users (12) did not incur into such inconsistency\nand were considered reliable enough to derive conclusions\nfrom their ratings. To compensate for differences in sub-\njective scales, we normalized excerpt preference tuples\nbetween 0 and 1 (that is, we normalized the ratings for\nthe 7 approaches independently per audio excerpt and lis-\ntener). To measure statistical signiﬁcance, we used pair-wise Wilcoxon signed-rank tests and applied the Holm-\nBonferroni adjustment for multiple testing with p= 0.05.\nThe Wilcoxon signed-rank test is appropriate for our case\nas it is non-parametric and designed for matched samples.\n5. RESULTS\nIn Fig. 1 we depict the average listener preference for each\nitem and approach. Initially, we see that the pattern differs\ndepending on the test item. For some items, the proposed\napproaches are preferred over the baselines (e.g., Electro1,\nJazz1, and Latin3) while, for some other items, differences\nbetween approaches are less clear (e.g., Rock1 and Soul2).\nAll approaches seem to be preferred above the mono sig-\nnal, except for baseline approaches with Electro1. Notice-\nably, in some situations, preference for some of the pro-\nposed approaches even overlaps with the original stereo\n(e.g., Electro1, Latin3, and Soul2). The case of Soul2\nshows an example where considered approaches are al-\nmost as preferred as the original stereo, whereas the case\nof Jazz1 shows an example where considered approaches\nare still far from the professional mix.\nDespite the different preferences on individual excerpts,\nupon further inspection we see that a clear pattern emerges\nwhen considering all items: proposed approaches rank bet-\nter than mono and the considered baselines (Fig. 1, right).\nIn Table 1 we conﬁrm that, on average, PS-AR is preferred\nover the baseline approaches and that, in turn, PS-NN and\nPS-MTM are preferred over PS-AR. In Table 2, we report\nstatistically signiﬁcant differences beetween PS-NN/PS-\nMTM and the baseline approaches, but not between PS-AR\nand the baseline approaches (and neither between PS-AR\nand PS-NN/PS-MTM nor between PS-NN and PS-MTM).\nOverall, the results show that a generative approach to PS\nprediction can become a compelling system for mono-to-\nstereo. The performance of PS-NN is a nice surprise that\nwas not predicted by the objective metrics, which other-\nwise seem to correlate with listener preference (Table 1;\nperhaps PS-NN does not follow the trend because it is not\na generative approach).\nBesides quality, another aspect worth considering isProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n308Approach Emin↓DF↓ Preference ↑\nMono 0.104 20.89 0.090 ±0.042\nPS-Reg 0.069 8.11 0.451 ±0.066\nDecorr 0.093 8.32 0.457 ±0.064\nPS-AR 0.074 0.62 0.527 ±0.060\nPS-NN 0.089 3.08 0.582 ±0.057\nPS-MTM 0.068 0.59 0.608 ±0.050\nStereo 0.000 0.03 0.908 ±0.042\nTable 1 . Results for the objective ( Emin,DF) and subjec-\ntive (Preference ±95% conﬁdence interval) evaluations.\nPS-Reg Decorr PS-AR PS-NN PS-MTM Stereo\nMono ✓ ✓ ✓ ✓ ✓ ✓\nPS-Reg ✗ ✗ ✓ ✓ ✓\nDecorr ✗ ✓ ✓ ✓\nPS-AR ✗ ✗ ✓\nPS-NN ✗ ✓\nPS-MTM ✓\nTable 2 . Pairwise statistical signiﬁcance for the case of all\ntest items (12 subjects times 7 excerpts, see Sec. 4.3). The\nobtainedp-value threshold is 0.0053.\nspeed. In Table 3 we observe that PS-AR, as anticipated,\nis orders of magnitude slower than the other approaches, to\nthe point of making it impractical for real-world operation.\nDecorr, PS-Reg, and PS-NN are faster than real-time on\nCPU and PS-MTM is not. However, one should note that\nwith PS-MTM we can easily trade off sampling iterations\nat the expense of some quality reduction (see [20, 26]).\nPS-NN may dramatically improve speed if we consider\nthe use of fast nearest neighbor search algorithms or even\nhash tables, which make this approach very interesting for\nreal-world deployment (note we deliberately made PS-NN\ncomparable in size to the other approaches, see Table 3).\n6. DISCUSSION\nDespite the good results obtained above, the subjective test\nreveals that, for some of the considered excerpts, there is\nstill a gap between professional stereo mixes and the pro-\nposed approaches. We hypothesize that this gap is due to\n(i) limitations of the considered PS encoding, and (ii) the\ndifﬁculty of the task itself. Regarding (i), we suspect that\npart of the low subjective scores of PS-based approaches is\ndue to the audio distortions and tonal artifacts introduced\nby the PS decoding. Thus, we hypothesize that using a\ncommercial implementation of PS coding (or perhaps even\nlearning end-to-end the coding operation) could yield bet-\nter results. Besides, we think that the fact that PS is deﬁned\nin a banded domain poses a challenge to PS generation ap-\nproaches, namely that individual bands are panned but ap-\nproaches do not have an explicit notion of instrument or\n‘entity’. Indeed, we sometimes observe individual entities\nbeing panned into two different positions simultaneously\n(e.g., for the same instrument, we may get some frequen-\ncies panned to the left and some to the right, which is an\nuncommon stylistic decision). A potential solution to thisApproach Learnable RTF ↓\nparameters CPU GPU\nDecorr 0 0.25 n/a\nPS-Reg 30.1 M 0.32 0.21\nPS-NN 34.0 M†0.82 n/a\nPS-MTM 34.5 M 5.81 0.33\nPS-AR 34.5 M 255.87 8.38\nTable 3 . Number of learnable parameters and average real-\ntime factor (RTF). Superscript†indicates an estimation\nof 0.5 M key-value pairs with B= 34 bands (Sec. 3.1).\nRTFs are measured on a Xeon(R) 2.20 GHz CPU and on a\nGeForce GTX 1080-Ti GPU.\nproblem could be to add better (or more) inputs to the mod-\nels, together with more capacity, with the hope that they\nachieve a better understanding of what is a source before\npanning it. Along this line, it would be perhaps interesting\nto include some techniques used in the context of source\nseparation with neural network models [11]. Regarding\n(ii), another issue we sometimes observe is with the tem-\nporal consistency of panning decisions, with an instrument\nappearing predominantly in one channel but then moving\n(without much artistic criterion) to the other channel after\n10 or 20 s. Handling temporal consistency is a transversal\nproblem across all generative models, typically handled by\nbrute force (that is, more receptive ﬁeld and/or larger mod-\nels) or by some form of hierarchical or recurrent process-\ning. Nonetheless, it is still an open issue, especially in the\ncase of really long sequences like audio and music.\nIn addition to the limitations inherent to the technol-\nogy, there are also some shortcomings in the test method-\nology. The subjective tests were conducted using head-\nphones, whereas stereo images are typically created and\nmixed in a studio using professional loudspeaker monitor-\ning. This implies that when critically evaluating the pro-\nposed approaches on a professional setup, additional sub-\ntleties might be discernible. Another methodological chal-\nlenge was that often users had difﬁculty in evaluating mul-\ntiple test excerpts according to the stated evaluation crite-\nria. A potentially contributing factor to it was the absence\nof a standardized test methodology for multiple preference\ntesting without a reference.\n7. CONCLUSION\nIn this work we study methods to convert from mono to\nstereo. Our proposal entails (i) the use of PS for mono\nto stereo upmixing and (ii) the synthesis of PS parame-\nters with three machine learning methods. We also in-\ntroduce (iii) the use of modern generative approaches to\nthe task and propose two variants of them. We addition-\nally (iv) overview and adapt an existing PS methodology\nand (v) propose two tentative objective metrics to evaluate\nstereo renderings. The three proposed approaches outper-\nform the classical and the deep neural network baselines\nwe consider, and two of such approaches stand out with a\nstatistically signiﬁcant difference in the subjective test.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3098. ACKNOWLEDGMENTS\nWe thank all the participants of the listening test for their\ninput and Gautam Bhattacharya and Samuel Narváez for\npreliminary discussions on the topic.\n9. REFERENCES\n[1] M. R. Schroeder, “An artiﬁcial stereophonic effect ob-\ntained from a single audio signal,” Journal of the Audio\nEngineering Society , vol. 6, no. 2, p. 74–79, 1958.\n[2] B. B. Bauer, “Some techniques toward better stereo-\nphonic perspective,” IEEE Trans. on Audio , vol. 11, p.\n88–92, 1963.\n[3] R. Orban, “A rational technique for synthesizing\npseudo-stereo from monophonic sources,” Journal of\nthe Audio Engineering Society , vol. 18, no. 2, p.\n157–164, 1970.\n[4] C. Faller, “Pseudostereophony revisited,” in Proc. of\nthe Audio Engineering Society Conv. (AES) , 2005, p.\n118.\n[5] M. Fink, S. Kraft, and U. Zölzer, “Downmix-\ncompatible conversion from mono to stereo in time-\nand frequency-domain,” in Proc. of the Int. Conf. on\nDigital Audio Effects (DAFx) , 2015.\n[6] C. Uhle and P. Gampp, “Mono-to-stereo upmixing,” in\nProc. of the Audio Engineering Society Conv. (AES) ,\n2016, p. 140.\n[7] M. Lagrange, L. G. Martins, and G. Tzanetakis, “Semi-\nautomatic mono to stereo up-mixing using sound\nsource formation,” in Proc. of the Audio Engineering\nSociety Conv. (AES) , 2007, p. 122.\n[8] D. Fitzgerald, “Upmixing from mono - A source sep-\naration approach,” in Proc. of the Int. Conf. on Digital\nSignal Processing (DSP) , 2011.\n[9] A. Delgado Castro and J. Szymanski, “Semi-automatic\nmono-to-stereo upmixing via separation of note\nevents,” in Proc. of the AES Conf. on Immersive and\nInteractive Audio , 2019, p. 12.\n[10] J. Pons, S. Pascual, G. Cengarle, and J. Serrà, “Up-\nsampling artifacts in neural audio synthesis,” in Proc.\nof the IEEE Int. Conf. on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2021, p. 3005–3009.\n[11] E. Cano, D. FitzGerald, A. Liutkus, M. D. Plumbley,\nand F.-R. Stöter, “Musical source separation: An intro-\nduction,” IEEE Signal Processing Magazine , vol. 36,\nno. 1, pp. 31–40, 2018.\n[12] C. J. Steinmetz, J. Pons, S. Pascual, and J. Serrà, “Au-\ntomatic multitrack mixing with a differentiable mixing\nconsole of neural audio effects,” in Proc. of the IEEE\nInt. Conf. on Acoustics, Speech and Signal Processing\n(ICASSP) , 2021, p. 7175.[13] D. Gibson, The art of mixing: a visual guide to record-\ning, engineering, and production , 2nd ed. Fairview,\nUSA: ArtistPRO, 2005.\n[14] J. M. Tomczak, Deep generative modeling . New York,\nUSA: Springer Charm, 2022.\n[15] C. J. Chun, S. H. Jeong, S. Y . Park, and H. K. Kim,\n“Extension of monaural to stereophonic sound based\non deep neural networks,” in Proc. of the Audio Engi-\nneering Society Conv. (AES) , 2015, p. 139.\n[16] H. Purnhagen, “Low complexity parametric stereo cod-\ning in MPEG-4,” in Proc. of the Int. Conf. on Digital\nAudio Effects (DAFx) , 2004, p. 163–168.\n[17] J. Breebaart, S. van de Par, A. Kohlrausch, and\nE. Schuijers, “Parametric coding of stereo audio,”\nEURASIP Journal on Advances in Signal Processing ,\nvol. 2005, no. 9, p. 1305–1322, 2005.\n[18] T. Hastie and R. Tibshirani, The elements of statisti-\ncal learning: data mining, inference, and prediction ,\n2nd ed. New York, USA: Springer, 2009.\n[19] A. Radford, K. Narashiman, T. Salimans, and\nI. Sutskever, “Improving language understanding by\ngenerative pre-training,” Technical Report, OpenAI ,\n2018.\n[20] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T.\nFreeman, “MaskGIT: masked generative image trans-\nformer,” in Proc. of the IEEE Int. Conf. on Com-\nputer Vision and Pattern Recognition (CVPR) , 2022,\np. 11315–11325.\n[21] R. J. Williams and D. Zipser, “A learning algorithm for\ncontinually running fully recurrent neural networks,”\nNeural Computation , vol. 1, no. 2, p. 270–280, 1989.\n[22] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad-\nbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito,\nM. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, PyTorch: an imper-\native style, high-performance deep learning library .\nCurran Associates, Inc., 2019, vol. 32, p. 8024–8035.\n[23] B. Bartlett, Stereo microphone techniques . London,\nUK: Focal Press, 1991.\n[24] X. I. Chen, N. Mishra, M. Rohaninejad, and P. Abbeel,\n“PixelSNAIL: an improved autoregressive generative\nmodel,” in Proc. of the Int. Conf. on Machine learning\n(ICML) , 2018, p. 864–872.\n[25] J. Ho and T. Salimans, “Classiﬁer-free diffusion guid-\nance,” in Proc. of the NeurIPS Workshop on Deep Gen-\nerative Models and Downstream Applications , 2021.\n[26] H. Chang, H. Zhang, J. Barber, A. J. Maschinot,\nJ. Lezama, L. Jiang, M.-H. Yang, K. Murphy, W. T.\nFreeman, M. Rubinstein, Y . Li, and D. Krishnan,\n“Muse: text-to-image generation via masked genera-\ntive transformers,” ArXiv: 2301.00704 , 2023.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n310"
    },
    {
        "title": "Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction.",
        "author": [
            "Keren Shao",
            "Ke Chen 0021",
            "Taylor Berg-Kirkpatrick",
            "Shlomo Dubnov"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265373",
        "url": "https://doi.org/10.5281/zenodo.10265373",
        "ee": "https://zenodo.org/records/10265373/files/000078.pdf",
        "abstract": "In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.",
        "zenodo_id": 10265373,
        "dblp_key": "conf/ismir/Shao0BD23",
        "keywords": [
            "input feature modification",
            "training objective modification",
            "harmonics decay",
            "CFP representation",
            "discrete z-transform",
            "vocal and non-vocal segments",
            "differentiable loss function",
            "melody contour",
            "PianoNet",
            "piano transcription network"
        ],
        "content": "TOWARDS IMPROVING HARMONIC SENSITIVITY AND PREDICTION\nSTABILITY FOR SINGING MELODY EXTRACTION\nKeren Shao* Ke Chen* Taylor Berg-Kirkpatrick Shlomo Dubnov\nUniversity of California San Diego\n{k5shao, knutchen, tberg, sdubnov}@ucsd.edu\nABSTRACT\nIn deep learning research, many melody extraction mod-\nels rely on redesigning neural network architectures to im-\nprove performance. In this paper, we propose an input\nfeature modiﬁcation and a training objective modiﬁcation\nbased on two assumptions. First, harmonics in the spectro-\ngrams of audio data decay rapidly along the frequency axis.\nTo enhance the model’s sensitivity on the trailing harmon-\nics, we modify the Combined Frequency and Periodicity\n(CFP) representation using discrete z-transform. Second,\nthe vocal and non-vocal segments with extremely short du-\nration are uncommon. To ensure a more stable melody\ncontour, we design a differentiable loss function that pre-\nvents the model from predicting such segments. We apply\nthese modiﬁcations to several models, including MSNet,\nFTANet, and a newly introduced model, PianoNet, modi-\nﬁed from a piano transcription network. Our experimen-\ntal results demonstrate that the proposed modiﬁcations are\nempirically effective for singing melody extraction.\n1. INTRODUCTION\nSinging melody extraction is a challenging task that aims\nto detect and identify the fundamental frequency (F0) of\nsinging voice in polyphonic music recordings. This task\nis more complicated than the monophonic pitch detection\ntask due to the presence of various instrumental accompa-\nniments and background noises, making it more difﬁcult\nto accurately extract the singing melody. Singing melody\nextraction is not only crucial for music analysis by itself,\nbut also has many downstream applications, such as cover\nsong identiﬁcation [1], singing evaluation [2], and music\nrecommendation [3].\nDeep neural networks have been widely adopted in the\nsinging melody extraction task to produce promising per-\nformance in terms of extraction accuracy. Early models\n[4–6] simply leveraged deep neural networks (DNN) and\nconvolutional neural networks (CNN) [7]. In more recent\n*The ﬁrst two authors have equal contribution.\n© K. Shao, K. Chen, T. Berg-Kirkpatrick, S. Dubnov. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: K. Shao, K. Chen, T. Berg-Kirkpatrick, S.\nDubnov, “Towards Improving Harmonic Sensitivity and Prediction Sta-\nbility for Singing Melody Extraction”, in Proc. of the 24th Int. Society\nfor Music Information Retrieval Conf., Milan, Italy, 2023.models, musical and structural priors were incorporated to\nimprove performance. These include MSNet [8] with a\nvocal detection component at the encoder-decoder bottle-\nneck, joint detection model [9] setting up an auxiliary net-\nwork, and TONet [10] with tone-octave predictions. Ad-\nditionally, models can capture frequency relationships bet-\nter with multi-dilation [11], cross-attention networks [12],\ngraph-based neural networks [13], or harmonic constant-Q\ntransform (HCQT) [14].\nOne of our observations relates to the input representa-\ntions of the models, which play an important role in affect-\ning the extraction performance. Timbre, which is closely\nrelated to harmonics, is one of the key components that\nhelps models distinguish the vocal from other instruments.\nWhen the CFP representation [15] is chosen as the in-\nput representation, its second feature, the generalized cep-\nstrum, allows the model to learn the strength of harmonics\nof any given fundamental frequency in a localized man-\nner. However, in music, the harmonics of a single sound\nusually decays rapidly along the frequency axis (detail in\nsection 2.1), which can pose a challenge for the model to\ndistinguish sounds that only differ signiﬁcantly at the trail-\ning harmonics.\nThe transformation from the spectrum to the general-\nized cepstrum in CFP is a Fourier transform, and hence\nmostly captures the ﬁrst few peaks with large magnitudes.\nAs a result, this representation is not helpful in sensing the\ntrailing harmonics. This motivates us to apply a different\ntransformation function that produces a generalized cep-\nstrum with better harmonics sensitivity.\nAnother observation relates to the vocal detection com-\nponent. Extremely short vocal segments surrounded by\nnon-vocal regions, and vice versa, rarely occur since vo-\ncalists typically sing a melody for at least half a second or\nrest for at least a few hundred milliseconds. Threshold-\nbased removal [16], mean or median ﬁltering [17, 18] and\nViterbi-based smoothing [19, 20] are frequently used to\naddress the problem. When they are implemented along-\nside a network-based algorithm, however, the network re-\nmains unaware of our smoothing intention and conﬁgura-\ntion. To investigate whether such awareness can increase\nthe prediction performance, we derive a differentiable loss\ncomponent that speciﬁcally penalizes spurious short-term\npredictions of these kinds during training, thus potentially\nguiding the model to produce consistently stable predic-\ntions.\nIn this paper, we propose two techniques that attempt657modification + STFT\nFrequency (Hz)Magnitudemodification + z-transform\nk=0.0007\nk=0.0006\nk=0.0002\nk=0\nFigure 1 . Top: the transformation pipeline of the original\nCFP representation, and our proposed z-CFP representa-\ntion. Bottom: modiﬁed Spectrum ˜Swith different growing\nrateskapplied. Note that the original CFP corresponds to\nthe case of k= 0.\nto improve the two concerns mentioned above, namely the\nharmonic sensitivity and the prediction stability of singing\nmelody extraction models. Our contributions are as fol-\nlows:\n• We propose to use exponentially growing sinusoids\nalong the frequency axis to transform the spectrum\ninto the generalized cepstrum of the CFP represen-\ntation. This approach is equivalent to taking a z-\ntransform instead of Fourier transform, which in-\ncreases the harmonic sensitivity of the input.\n• We design a differentiable loss function as part of the\ntraining objective to teach the network to avoid pre-\ndicting unrealistically short sequences of vocal and\nnon-vocal at the voice detection bin.\n• We evaluate our techniques by applying them on\nseveral melody extraction models. Additionally, we\nadapt PianoNet [21], originally developed for piano\ntranscription, into the melody extraction task. Ex-\nperimental results demonstrate state-of-the-art per-\nformance of our improved models.\n2. METHODOLOGY\nIn this section, we introduce three main parts of our\nmethodology. First, we propose a modiﬁed CFP repre-\nsentation, z-CFP, to enhance the harmonic sensitivity of\nthe network input. Second, we introduce extraction mod-\nels used for evaluating our techniques, namely MSNet,\nFTANet, and PianoNet. Third, we propose a new loss func-\ntion as part of training objective to improve the prediction\nstability of models.\n2.1z-CFP Representation for Harmonic Sensitivity\nOur input representation of audio data is a modiﬁed ver-\nsion of the CFP representation. A CFP representationX∈R3×T×Fcontains three features, with Tthe length of\ntime frames and Fthe number of frequency bins. At each\ntime slice , it contains: (1) a power spectrum S∈R1×F;\n(2) a generalized cepstrum GC∈R1×F; and (3) a gener-\nalized cepstrum of spectrum GCoS∈R1×F,\nAs illustrated in the upper part of Figure 1, the standard\nCFP generation process begins by computing the frame-\nwise spectrum of an input audio waveform using short-\ntime Fourier transform (STFT). We then obtain the magni-\ntude of each spectrum, which serves as the ﬁrst feature of\nCFP, denoted as S. To derive the second feature, we com-\npute the generalized cepstrum using the following equa-\ntion:\nGC=|F−1(sγ(S))|=|F(sγ(S))| (1)\nwhereFandF−1denotes the Fourier transform and its in-\nverse,sγ:R→Ris an element-wise applied, logarithm-\nlike modiﬁcation function as described in [15], and the\nabsolute value sign represents an element-wise complex\nnorm operation. The second equality comes directly from\nthe fact that norm of a complex number equals to that of its\nconjugate.\nAs mentioned in the introduction, GC is not sensi-\ntive to the trailing harmonic dynamics, as it mostly cap-\ntures the ﬁrst few peaks with large magnitudes. Since the\nharmonics decay rapidly along with the frequency axis,\nwe shall revert the decay to better preserve such dynam-\nics. In other words, instead of applying complex sinu-\nsoids/summationtext\nnsγ(S[n])e−iwnas in Fourier transform ( nis the\nentry of frequency bins in S), we apply growing com-\nplex sinusoids/summationtext\nnsγ(S[n])e(k−iw)n, wherek∈Rand\nk >0. This is equivalent to taking a discrete z-transform/summationtext\nnsγ(S[n])z−n, wherez=eiw−k.\nIn the actual implementation, kis manually assigned\nand ﬁxed across different w. Therefore, as illustrated in\nFigure 1, we can separate the computation of kpart andw\npart as follows:\n˜S[n] =eknsγ(S[n])for∀n (2)\n˜GC=|F−1(˜S)|=|F(˜S)| (3)\nIn the lower part of Figure 1, we present ˜Sof an audio\nwaveform with different values of k. We can observe that\nthe harmonics of ˜Sat the tail gets ampliﬁed so that the sub-\nsequent Fourier transform can better capture their dynam-\nics. While we observe some ampliﬁcations of harmonics at\nfrequencies other than the fundamental frequencies, their\nmagnitudes are always smaller than those of nearby fun-\ndamental frequencies. Therefore, they pose no sufﬁcient\ndistraction for the extraction model, as long as the chosen\nkis not too large. In our experiments, we set k= 0.0006 .\nWe then generate the generalized cepstrum of spectrum\n˜GCoS from cepstrum ˜GCthe same way as in the original\nCFP. Finally, each time slice of our modiﬁed CFP repre-\nsentation ˜X∈R3×T×Fconsists of{S,˜GC,˜GCoS}with\nlog-scaled frequency axis. For the rest of the paper, we\ndenote itz-CFP.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n658Extraction Models\n2 × Conv2D (3,5,48)\n2 × Conv2D (3,5,96)\n2 × Conv2D (3,5,128)\nDense\nBi-GRU2 × Conv2D (3,5,128)\nDense + SoftmaxPianoNet\n3 × Conv2D (5,5,3 → 128)\nSF-ModuleFTA-Module\nSF-ModuleFTA-Module\nSoftmaxSoftmax3 × Conv2D (5,5,128 → 1)MSNet\nFTANet\ndownsampling-block (kernel, kernel, channel)\nupsampling-block (kernel, kernel, channel)\nFigure 2 . The model architecture. Note that we choose\nonly one of the three extraction models at a time.\n2.2 Model Architecture\nOur extraction models are referred from three state-of-\nthe-art (SoTA) models, MSNet [8], FTANet [12], and Pi-\nanoNet [21]. Different from MSNet and FTANet, Pi-\nanoNet is the SoTA model of piano transcription. Given its\nsuperior performance on piano transcription, we incorpo-\nrate a sub-network of PianoNet into singing melody extrac-\ntion, as we hypothesize that it may also yield good results\nfor melody extraction.\nMSNet contains a 3-layer encoder, a 3-layer decoder,\nand a bottleneck module. The channel size is shifted as\n3→32→64→128→64→32→1. The bottleneck\nmodule maps the encoder output to a 1-channel featuremap\nfor voice detection. All 2D-convolutional layers come with\n(5×5)kernel size. FTANet contains a 4-layer encoder, a\n3-layer decoder, and a 4-layer bottleneck module. Both en-\ncoder and decoder contain FTA-modules and SF-modules\nto process the audio latent features. The channel size is\nshifted from 3to128, then back to 1. More speciﬁcations\nof MSNet and FTANet can be found in their papers [8,12].\nThe PianoNet we use for this task is modiﬁed from\na sub-network of [21]. It starts with four convolutional\nblocks, each block containing two 2D-convolutional layers\nwith kernel sizes (3, 5) and (3, 3) respectively, a batch nor-\nmalization layer and a ReLU activation. Then it is followed\nby bidirectional-GRU and softmax layers, with dense lay-\ners as transitions. The layer bias is turned off for all layers\nbefore the Bi-GRU.\nFigure 2 illustrates a more detailed structure of the three\nextraction models. Following the pipeline, we ﬁrst process\n1.0\n0.00.5\nTime FrameV-D Prob\nSubsequence Processing\nreject such segments\naccept such segments\nFigure 3 . The illustration of how we perform the loss func-\ntionsLvandLnvon the subsequences of the voice detec-\ntion prediction. Each loss components Lare used to give\nlarge penalties (i.e., ✗) to certain types of subsequences.\nthe audio waveform into z-CFP representations. Then we\nfeed them into the extraction model, which produces out-\nput feature maps ˜Y∈RT×(F+1). The additional one fea-\nture along the frequency axis denotes the voice detection\nbin output. It is then compared against the ground truth\nlabelY∈RT×(F+1), through the loss function introduced\nin the following section.\n2.3 Loss Function for Prediction Stability\nWe add two differentiable training objectives, LvandLnv,\nto the conventional binary cross entropy loss LBCE to\nteach the extraction model to avoid unrealistically short\nvocal and non-vocal sequences at the vocal detection bin .\nSince the design for these two cases are symmetric, we ﬁrst\nintroduce the loss object Lv, for the vocal case.\nAs shown on the top of Figure 3, the predictions at the\nvocal detection bin is a time series {a1,a2,...,aT}. First,\nsince our training objectives are dealing with certain types\nof short burst segments of vocal and non-vocal, we extract\nall possible subsequences, with stride 1. For example, for\n3-length subsequences we have {a1:3,a2:4,...,aT−2:T},\nand similarly{a1:4,a2:5,...,aT−3:T}for subsequences of\nlength 4, etc.\nSecond, to simplify the problem a bit at the beginning,\nwe assume that the voice detection output is binary val-\nueda∈{0,1}. Formally, we do not want “sharp-burst\"\nsequences inside the following set:\nBv=Mv/uniondisplay\nm=3{a1...am|a1=am= 0,ai= 1for∀i̸= 1,m}(4)\nwhereMvis a hyperparameter threshold, above which the\nduration of vocal segments becomes reasonable. Figure 3\nillustrates examples of “sharp-burst\" sequences in Bv(and\nBnv) as red segments inside black-border boxes.\nSuppose m= 3 , all possible binary sequences are\n{000,001,010,011,100,101,110,111}and010∈Bv.\nTo make the model avoid predicting the short burst vocalProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n659segment, i.e., 010, we construct a polynomial objective that\ncan fulﬁll the goal by satisfying the following:\nL3\nv(a1a2a3) =/braceleftbigg\n1wherea1a2a3= 010\n0 otherwise(5)\nA decent choice will then be\nL3\nv(a1a2a3) = (1−a1)a2(1−a3) (6)\nwhich can be easily extended to sequences with longer\nlengthm.\nL4\nv(a1a2a3a4) = (1−a1)a2a3(1−a4)\n...\nLm\nv(a1...am) = (1−a1)(1−am)m−1/productdisplay\ni=2ai (7)\nHowever, there is a small caveat in this extension when\nwe move back from binary values to probability values\na∈[0,1]. For example, our loss component will be\nhaving trouble capturing sequences like {0.1,0.4,0.6,0.1}\nand{0.1,0.6,0.4,0.1}as bothL3\nvandL4\nvresult in rela-\ntively small values. However, we observe that polynomials\n(1−a1)(1−a2)a3(1−a4)and(1−a1)a2(1−a3)(1−a4)\nrespectively works better than our original L4\nv, but still in-\nsufﬁcient to work standalone.\nSince none of the polynomials above gives high values\nto sequences outside of Bvin4-length, a simple solution\nwould be to redeﬁne L4\nvto be the sum of all such polyno-\nmials:\nL4\nv= (1−a1)(1−a4)(a2a3+a2(1−a3)+(1−a2)a3)\n...\nLm\nv= (1−a1)(1−am)/summationdisplay\nc1...cm∈{0,1}m\nat least one ci̸=0m−1/productdisplay\ni=2aci\ni(1−ai)1−ci\n= (1−a1)(1−am)(1−m−1/productdisplay\ni=2(1−ai)) (8)\nThis redeﬁned loss Lvallows better recognition of the\nbad sequences mentioned above while not falsely ﬂagging\nsequences outside of Bv. Furthermore, when dealing with\nlonger sequences, for example {0.1,0.9,...,0.9,0.1}with\nincreasingly many 0.9s in the middle, the original Lv’s out-\nput quickly diminishes while the redeﬁned Lvdoes not.\nThis redeﬁned objective does come with a small side\neffect, as it over-counts the shorter bad sequences. For ex-\nample,(0.1,0.9,0.1,0.1)now gets a high loss value not\nonly inL3\nv, but also in L4\nv. However, we believe this side\neffect does not have signiﬁcant impact as it does not matter\nwhether neural network decides to stop producing shorter\nbad sequences or longer bad sequences ﬁrst.\nA further improvement is to pass the value of Lm\nvinto\nthe S-curve function:\nLm\nv←(Lm\nv)r\n(Lmv)r+(1−Lmv)r(9)\nwherer∈Randr >1. It will amplify those sequences\nthat receive loss values closer to 1 and suppress those se-\nquences with loss values closer to 0.Finally, for each m∈[3,Mv], we compute Lm\nvacross\nallm-length subsequences in the model’s output. The ag-\ngregated loss function Lvis then computed by concatenat-\ning all these Lm\nvarrays and taking the average.\nNow analogously, assuming non-vocal sequences be-\nyond length Mnvbecome reasonable, we can perform the\nsame analysis on the following set of sequences:\nBnv=Mnv/uniondisplay\nm=3{a1...am|a1=am= 1,ai= 0for∀i̸= 1,m}\n(10)\nand consequently obtain Lnv. Practically, Lm\nnvof any se-\nquencea1...amcan be computed as Lm\nvof the ﬂipped se-\nquenceb1...bm, wherebi= 1−aifor alli∈{1..m}. Our\nﬁnal loss function will then be:\nL=LBCE+Lv+Lnv (11)\n3. EXPERIMENTS\n3.1 Datasets and Experiment Setup\nFor the training data, we complied with the setting of\n[10, 12] and chose all 1000 Chinese pop songs from MIR-\n1K1and 35 vocal tracks from MedleyDB [22]. For the\ntesting data, we chose 12 tracks in ADC2004 and 9 tracks\nin MIREX052. We also selected 12 tracks from Med-\nleyDB that are disjoint from those already used for train-\ning.\nFor the signal processing part, we used 8000Hz sam-\npling rate to process audio tracks. We use a window size\nof768, a hop size of 80to compute the STFT of audio\ntracks. Note that the time resolution of our labels is 0.01\nseconds, and this hop size was chosen to match that. Then,\nwhen creating z-CFP representations, we set the time di-\nmension of the representation to be T= 128 , or1.28sec-\nonds, and the number of frequency bins F= 360 , or60\nbins per octave across 6octaves. The start and stop fre-\nquencies are 32.5Hz and2050Hz . Hence, the input shape\nbecomes X∈R3×128×360and the output/label shape be-\ncomesY∈R128×361.\nWithin the extra loss component, we set the duration\nthreshold of vocal segments Mv= 30 (0.3 seconds), the\nduration threshold of non-vocal segments Mnv= 7 (0.07\nseconds), and the S-curve exponent parameter r= 5.\nFor the training hyperparameters, we use a batch size\nof10, the Adam optimizer [23] with a ﬁxed learning rate\nof1×10−4. The maximum training epoch is 500. Dur-\ning the evaluation, we use the standard metrics of the\nsinging melody extraction task, namely, voice recall (VR),\nvoicing false alarm (VFA), raw pitch accuracy (RPA),\nraw chroma accuracy (RCA), and overall accuracy (OA)\nfrom themir_eval library [24]. Following the conven-\ntion of this task, overall accuracy (OA) is regarded as the\nmost important metric. All models are trained and tested\nin NVIDIA RTX 2080Ti GPUs and implemented in Py-\nTorch3.\n1http://mirlab.org/dataset/public/MIR-1K.zip\n2https://labrosa.ee.columbia.edu/projects/melody/\n3https://pytorch.org/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n660Dataset ADC 2004 MIREX 05 MEDLEY DB\nMetrics VR VFA ↓RPA RCA OA VR VFA ↓RPA RCA OA VR VFA ↓RPA RCA OA\nPianoNet 87.21 14.62 84.28 84.30 84.48 91.98 6.14 86.54 86.55 89.19 69.38 13.74 61.81 62.80 73.70\nPianoNet + z-CFP 88.25 7.58 84.87 84.93 86.27 93.44 6.21 86.78 86.79 89.33 68.76 11.91 62.22 63.10 74.80\nPianoNet + 3 point median 87.33 14.58 84.35 84.38 84.55 92.08 6.15 86.60 86.62 89.23 69.49 13.77 61.86 62.86 73.71\nPianoNet + 7 point median 87.58 14.53 84.46 84.48 84.65 92.47 6.14 86.78 86.8 89.35 69.71 13.83 61.92 62.91 73.71\nPianoNet + 15 point median 89.13 14.21 84.89 84.91 85.06 93.27 6.58 86.82 86.84 89.21 70.31 14.43 61.91 62.90 73.42\nPianoNet + {Lv,Lnv} 90.92 13.58 86.06 86.12 86.13 91.87 5.79 87.50 87.50 89.94 71.16 15.77 63.66 64.81 73.66\nPianoNet + z-CFP +{Lv,Lnv}90.50 7.99 85.76 85.82 86.92 92.84 6.39 87.57 87.59 89.76 68.88 12.29 62.05 62.91 74.53\nMSNet 89.78 23.12 80.83 81.60 80.10 84.85 11.44 77.76 78.09 81.68 53.49 9.41 46.90 48.24 68.15\nMSNet + z-CFP +{Lv,Lnv} 90.61 14.62 81.96 82.57 82.59 88.38 14.85 80.83 81.01 82.39 62.95 14.60 53.60 55.31 69.07\nFTANet 81.26 2.70 77.17 77.36 80.89 87.34 5.11 81.56 81.61 86.40 62.44 10.41 55.94 56.58 72.30\nFTANet + z-CFP +{Lv,Lnv}90.29 10.83 85.06 85.19 85.82 90.50 6.63 83.94 83.99 87.36 63.71 9.35 56.32 57.29 73.02\nTable 1 . Ablation studies on ADC2004, MIREX05 and MedleyDB testsets. Baselines use CFP as the input representation\nandLBCE as the loss function. {Lv,Lnv}denotes the use of our proposed loss function in section 2.3. Among median\nﬁlter sizes in the range [3,100]⊂Z, 3 point works best for MedleyDB, 7 point works best for MIREX 05, and 15 point\nworks best for ADC 2004. But they neither signiﬁcantly outperform our proposed loss component in any single dataset,\nnor uniformly outperform in all three datasets.\n3.2 Ablation Study\nWe choose three extraction models, namely MSNet [8],\nFTANet [12], and PianoNet [21], to evaluate our z-\ntransform and loss functions. We conducted ablation stud-\nies and presented the results in Table 1. We re-trained these\nmodels from scratch, and the results are largely consis-\ntent with the original reports of [8, 10, 12]. The option z-\ntransform denotes the use of z-CFP representations. Note\nthat{Lv,Lnv}in the table denote the use of loss functions\nto address short burst segments of vocal and non-vocal.\nDue to the page limitation, we present a detailed ablation\nstudy on PianoNet while ablating MSNet and FTANet in\nan all-or-nothing fashion.\nFrom Table 1 we can clearly observe decent perfor-\nmance of both z-CFP and{Lv,Lnv}when added to the\nPianoNet, MSNet, and FTANet. Among these results, the\naddition of loss functions {Lv,Lnv}increases the overall\naccuracy while improving the VR, RPA, and RCA. The\nmedian ﬁlter postprocessing [18] is used as a comparison.\nSince our loss component focuses on the vocal detection,\nwe took the pitches predicted by median ﬁlters only when\nthe original predictions are non-vocal. Further, to ensure\nfairness, we optimized the ﬁlter size against each single\ndataset within the range [3,100]⊂Zand listed the evalu-\nation results of those optimal ones. As we can see in Table\n1, none of these median ﬁlters outperforms our loss com-\nponent in a consistent manner, nor do they obtain consid-\nerable margins in any single dataset.\nThez-CFP also increases several metrics, especially ei-\nther VR or VFA, on each dataset. This indicates that by\npreserving more dynamics in the high frequency bins, the\nmodel can distinguish different sounds better and conse-\nquently improve the extraction performance. Also, note\nthat unlike TONet [10] and JDC [9], which achieved this\nthrough model design or music inductive bias, this tech-\nnique relies solely on the inherent characteristics of the\ndata.When we incorporate both techniques into the extrac-\ntion models, we observe a promising increase in each met-\nric compared to the original models. However, we notice\nthat some models with both techniques carried do not yield\nbetter performance than the models carrying only one of\nthe techniques. These models appear to be an averaging\nweighting or an ensemble of models improved with either\ntechnique, implying better generalization.\n3.3 Comprehensive Performance Comparison\nTable 2 presents the results as we compare our best model,\ni.e., PianoNet with z-transform and{Lv,Lnv}, with other\nSoTA models. Among these SoTAs, there are two models\nwith “*\", indicating that these are only partial comparisons.\nFor SpecTNT [25], since there is no ofﬁcial open-source\nimplementation, we report its results based on our own re-\nimplementation. For H-GNN [13], we directly copied its\nreported performance from the original paper.\nFrom Table 2, our improved PianoNet with z-transform\nand{Lv,Lnv}yield the best OA performance over all\ndatasets, the best RPA and RCA on ADC 2004 and MIREX\n05 datasets. We do note, despite the use of the extra loss\ncomponent, that our model’s VFA is not necessarily the\nsmallest. This is because the extra loss component only\ntargets a particular type of false positive, and is not meant\nto minimize the false positive rate in general. For exam-\nple, sometimes the network’s vocal to non-vocal transition\nhappens later than the reference labels. In this case, since\nthe vocal sequence itself lasts long enough, the extra loss\ncomponent will not mark this type of false positives. Ad-\ndressing this type of errors is potentially a future work.\nAnother thing we found is that the PianoNet, as one of\nSoTAs in the piano transcription task and ported by us to\nthe melody extraction task in this paper, has already yields\nvery high performance on MIREX 05 dataset. This indi-\ncates that there may exist more powerful network architec-\ntures for this task yet to be explored. Additionally, it isProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n661Dataset ADC 2004\nMetrics VR VFA ↓RPA RCA OA\nMCDNN [4] 65.0 10.5 61.6 63.1 66.4\nDSM [14] 89.2 51.3 75.4 77.6 69.8\nMSNet [8] 89.8 23.1 80.8 81.6 80.1\nFTANet [12] 81.3 2.7 77.2 77.4 80.9\nTONet [10] 91.8 17.1 82.6 82.9 82.6\nSpecTNT* [25] 85.4 8.2 83.5 83.6 85.0\nH-GNN* [13] 89.2 21.3 84.8 86.1 83.9\nOurs 90.5 8.0 85.7 85.8 86.9\nDataset MIREX 05\nMetrics VR VFA ↓RPA RCA OA\nMCDNN [4] 66.5 4.6 64.1 64.4 75.4\nDSM [14] 91.4 45.3 75.7 77.0 68.4\nMSNet [8] 84.8 11.4 77.8 78.1 81.7\nFTANet [12] 87.3 5.1 81.6 81.6 86.4\nTONet [10] 91.6 8.5 83.8 84.0 86.6\nSpecTNT* [25] 82.2 8.7 77.4 77.5 82.5\nH-GNN* [13] 93.2 21.7 85.2 86.4 81.3\nOurs 92.8 6.4 87.6 87.6 89.8\nDataset MEDLEY DB\nMetrics VR VFA ↓RPA RCA OA\nMCDNN [4] 37.4 5.3 34.2 35.3 62.3\nDSM [14] 86.6 44.3 70.2 72.4 64.8\nMSNet [8] 53.5 9.4 46.9 48.2 68.1\nFTANet [12] 62.4 10.4 55.9 56.6 72.3\nTONet [10] 64.2 12.5 56.6 58.0 71.6\nSpecTNT* [25] 62.7 18.8 54.7 56.4 63.9\nH-GNN* [13] 71.7 21.6 61.2 65.8 67.9\nOurs 68.9 12.3 62.1 62.9 74.5\nTable 2 . The comprehensive performance comparison\namong our improved models and current baselines.\nnoteworthy that our proposed PianoNet architecture has a\nsmall number of parameters (5.5 million), which is com-\nparable with MCDNN (5.6 million), FTANet (3.4 million)\nand far less than TONet (152 million). This demonstrates\nits potential in practical applications where computational\nresources are limited. Again, as demonstrated in Table\n1, our techniques could help models other than PianoNet\nachieve higher performance than their original versions.\n3.4 Loss Value and Extraction Visualization\nTo empirically verify if applying the polynomial loss func-\ntionsLvandLnvcould reduce the voice detection errors,\ni.e., short burst segments of vocal and non-vocal, we visu-\nalize two types of plots in Figure 4. The top three plots\ndemonstrate the loss values of L30\nvbetween the original\nextraction models and the improved models with Lvand\nLnv, across the entire MIREX05 dataset (i.e., we concate-\nnate all tracks in the dataset). We see that cases in which\nthe improved models’ prediction receive loss values close\nto 1 diminishes comparing to those of the original mod-\nels. This phenomenon implies that after applying Lvand\nLnv, the chance of models to predict short burst segments\nTime (second)\nTime (second)\nTime (second)Original MSNet Improved MSNet\nOriginal FTANet Improved FTANet\nOriginal PianoNet Improved PianoNet\nTime (second)Freq. (Hz)01000 1000\n0Original PianoNet Improved PianoNet Groudtruth Label\nFigure 4 . The effect of applying the loss LvandLnv.\nThe top three plots are values of L30\nvacross the entire\nMIREX05 dataset. The bottom two plots are one 5-sec\nMIREX05 predictions.\nsigniﬁcantly reduces.\nThe pair of plots in the last row compares the prediction\nperformance of PianoNets, trained without and with the ex-\ntra loss components, on a zoomed-in section of MIREX05.\nNote that the original PianoNet has a short burst non-vocal\nsegment in between the 10th second and 11th second. Fur-\nther, it has a considerable number of short burst vocal seg-\nments around the 12th second. Once trained with the extra\nloss components, these issues are resolved. Also note that\nboth the original version and the improved version make\na mistake in between the 13th and the 14th second. This\nis because the length of that non-vocal transition is greater\nthan our threshold Mnv, which ends up not triggering Lnv.\nAll these observations further verify the effectiveness of\nour proposed loss components.\n4. CONCLUSION\nIn this paper, we propose two techniques to respectively\nutilize the two assumptions we made for improving the\nperformance of singing melody extraction models. First,\ncomparing to Fourier transform, the use of z-transform in\ngenerating cepstrum allows the network to better recognize\nthe strength of harmonics of any fundamental frequencies.\nEmpirically, while the trailing harmonics of those frequen-\ncies that do not actually appear in the audio also get ele-\nvated, the beneﬁt of the technique is greater than its set-\nback. Second, our extra loss components make the net-\nwork less prone to predict vocal and non-vocal sequences\nare unreasonably short, while not affecting the network’s\noverall accuracy due to its differentiability. Along with\ndifferent extraction models, we achieve better performance\nwhen compared to their original version and other state-of-\nthe-art models. We regard these two techniques as decent\nimprovements on singing melody extraction models.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6625. ACKNOWLEDGMENTS\nWe would like to thank the Institute for Research and Co-\nordination in Acoustics and Music (IRCAM) and Project\nREACH: Raising Co-creativity in Cyber-Human Musi-\ncianship for supporting this project. This project has\nreceived funding from the European Research Council\n(ERC REACH) under the European Union’s Horizon 2020\nresearch and innovation programme (Grant Agreement\n#883313).\n6. REFERENCES\n[1] X. Du, K. Chen, Z. Wang, B. Zhu, and Z. Ma, “Byte-\ncover2: Towards dimensionality reduction of latent\nembedding for efﬁcient cover song identiﬁcation,” in\nProc. ICASSP , 2022, pp. 616–620.\n[2] N. Zhang, T. Jiang, F. Deng, and Y . Li, “Automatic\nsinging evaluation without reference melody using bi-\ndense neural network,” in Proc. ICASSP , 2019, pp.\n466–470.\n[3] K. Chen, B. Liang, X. Ma, and M. Gu, “Learning audio\nembeddings with user listening data for content-based\nmusic recommendation,” in Proc. ICASSP , 2021, pp.\n3015–3019.\n[4] S. Kum, C. Oh, and J. Nam, “Melody extraction on\nvocal segments using multi-column deep neural net-\nworks,” in Proc. ISMIR , 2016, pp. 819–825.\n[5] S. Li, “V ocal melody extraction using patch-based\ncnn,” in Proc. ICASSP , 2018, pp. 371–375.\n[6] J. W. Kim, J. Salamon, P. Li, and J. P. Bello, “Crepe:\nA convolutional representation for pitch estimation,” in\nProc. ICASSP . IEEE, 2018, pp. 161–165.\n[7] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner,\n“Gradient-based learning applied to document recog-\nnition,” Proc. IEEE , 1998.\n[8] T.-H. Hsieh, L. Su, and Y .-H. Yang, “A streamlined en-\ncoder/decoder architecture for melody extraction,” in\nProc. ICASSP , 2019, pp. 156–160.\n[9] S. Kum and J. Nam, “Joint detection and classiﬁcation\nof singing voice melody using convolutional recurrent\nneural networks,” Applied Sciences , 2019.\n[10] K. Chen, S. Yu, C. Wang, W. Li, T. Berg-Kirkpatrick,\nand S. Dubnov, “Tonet: Tone-octave network for\nsinging melody extraction from polyphonic music,” in\nProc. ICASSP , 2022, pp. 626–630.\n[11] P. Gao, C. You, and T. Chi, “A multi-dilation and multi-\nresolution fully convolutional network for singing\nmelody extraction,” in Proc. ICASSP , 2020, pp. 551–\n555.\n[12] S. Yu, X. Sun, Y . Yu, and W. Li, “Frequency-temporal\nattention network for singing melody extraction,” in\nProc. ICASSP , 2021, pp. 251–255.[13] S. Yu, X. Chen, and W. Li, “Hierarchical graph-based\nneural network for singing melody extraction,” in Proc.\nICASSP , 2022, pp. 626–630.\n[14] R. M. Bittner, B. McFee, J. Salamon, P. Li, and J. P.\nBello, “Deep salience representations for f0 estimation\nin polyphonic music.” in Proc. ISMIR , 2017, pp. 63–\n70.\n[15] L. Su and Y .-H. Yang, “Combining spectral and tem-\nporal representations for multipitch estimation of poly-\nphonic music,” IEEE Trans. Audio, Speech, Lang. Pro-\ncess. , vol. 23, no. 10, pp. 1600–1612, 2015.\n[16] R. M. Bittner, J. Salamon, J. J. Bosch, and J. P. Bello,\n“Pitch contours as a mid-level representation for music\ninformatics,” in Audio engineering society conference:\n2017 AES international conference on semantic audio ,\n2017.\n[17] J. Salamon and E. Gómez, “Melody extraction from\npolyphonic music signals using pitch contour charac-\nteristics,” IEEE Trans. Audio, Speech, Lang. Process. ,\nvol. 20, no. 6, pp. 1759–1770, 2012.\n[18] S. Rosenzweig, F. Scherbaum, and M. Müller, “De-\ntecting stable regions in frequency trajectories for tonal\nanalysis of traditional georgian vocal music.” in Proc.\nISMIR , 2019, pp. 352–359.\n[19] M. Mauch and S. Dixon, “pyin: A fundamental fre-\nquency estimator using probabilistic threshold distri-\nbutions,” in Proc. ICASSP . IEEE, 2014, pp. 659–663.\n[20] J. J. Bosch and E. Gómez Gutiérrez, “Melody extrac-\ntion based on a source-ﬁlter model using pitch contour\nselection,” in Proceedings SMC 2016. 13th Sound and\nMusic Computing Conference , 2016.\n[21] Q. Kong, B. Li, X. Song, Y . Wan, and Y . Wang, “High-\nresolution piano transcription with pedals by regress-\ning onset and offset times,” IEEE Trans. Audio, Speech,\nLang. Process. , vol. 29, pp. 3707–3717, 2021.\n[22] R. M. Bittner, J. Salamon, M. Tierney, M. Mauch,\nC. Cannam, and J. P. Bello, “Medleydb: A multitrack\ndataset for annotation-intensive mir research.” in Proc.\nISMIR , 2014, pp. 155–160.\n[23] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” Proc. ICLR , 2014.\n[24] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, L. Dawen, D. P. Ellis, and C. C. Raffel,\n“mir_eval: A transparent implementation of common\nmir metrics,” in Proc. ISMIR , 2014, pp. 367–372.\n[25] W. T. Lu, J. Wang, M. Won, K. Choi, and X. Song,\n“Spectnt: a time-frequency transformer for music au-\ndio,” in Proc. ISMIR , 2021, pp. 396–403.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n663"
    },
    {
        "title": "Optimizing Feature Extraction for Symbolic Music.",
        "author": [
            "Federico Simonetta",
            "Ana Llorens",
            "Martín Serrano",
            "Eduardo García-Portugués",
            "Álvaro Torrente"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265409",
        "url": "https://doi.org/10.5281/zenodo.10265409",
        "ee": "https://zenodo.org/records/10265409/files/000095.pdf",
        "abstract": "This paper presents a comprehensive investigation of existing feature extraction tools for symbolic music and contrasts their performance to determine the set of features that best characterizes the musical style of a given music score. In this regard, we propose a novel feature extraction tool, named musif, and evaluate its efficacy on various repertoires and file formats, including MIDI, MusicXML, and **kern. Musif approximates existing tools such as jSymbolic and music21 in terms of computational efficiency while attempting to enhance the usability for custom feature development. The proposed tool also enhances classification accuracy when combined with other sets of features. We demonstrate the contribution of each set of features and the computational resources they require. Our findings indicate that the optimal tool for feature extraction is a combination of the best features from each tool rather than those of a single one. To facilitate future research in music information retrieval, we release the source code of the tool and benchmarks.",
        "zenodo_id": 10265409,
        "dblp_key": "conf/ismir/SimonettaLSGT23",
        "keywords": [
            "musif",
            "kern",
            "symbolic music",
            "feature extraction",
            "music score",
            "classification accuracy",
            "custom feature development",
            "music information retrieval",
            "source code",
            "benchmarks"
        ],
        "content": "OPTIMIZING FEATURE EXTRACTION FOR SYMBOLIC MUSIC\nFederico Simonetta1Ana Llorens2Martín Serrano1\nEduardo García-Portugués3Álvaro Torrente1,2\n1ICCMU - Instituto Complutense de Ciencias Musicales, Madrid\n2Universidad Complutense de Madrid, Madrid\n3Universidad Carlos III, Madrid\ndidone@iccmu.es\nABSTRACT\nThis paper presents a comprehensive investigation of ex-\nisting feature extraction tools for symbolic music and con-\ntrasts their performance to determine the set of features that\nbest characterizes the musical style of a given music score.\nIn this regard, we propose a novel feature extraction tool,\nnamed musif, and evaluate its efﬁcacy on various reper-\ntoires and ﬁle formats, including MIDI, MusicXML, and\n**kern. Musif approximates existing tools such as jSym-\nbolic and music21 in terms of computational efﬁciency\nwhile attempting to enhance the usability for custom fea-\nture development. The proposed tool also enhances classiﬁ-\ncation accuracy when combined with other sets of features.\nWe demonstrate the contribution of each set of features and\nthe computational resources they require. Our ﬁndings in-\ndicate that the optimal tool for feature extraction is a combi-\nnation of the best features from each tool rather than those\nof a single one. To facilitate future research in music infor-\nmation retrieval, we release the source code of the tool and\nbenchmarks.\n1. INTRODUCTION\nFeature extraction is a pivotal task in contemporary ma-\nchine learning. Music features can be categorized into\ntwo main types: symbolic and audio. While audio fea-\ntures have been subject to extensive research, computa-\ntional techniques for symbolic music remain comparatively\nunderexplored.\nIn recent years, there has been an increasing interest\nin analyzing symbolic scores in music. This encompasses\nstudies on composer [1] and style recognition [2], affec-\ntive computing [3], music generation [4], analysis of perfor-\nmance [5], and interpretation [6]. The symbolic dimension\nof music concerns the conceptual representation of musi-\ncal data [7]. This level has been used in the ﬁeld of Mu-\nsic Information Retrieval (MIR), with particularly success-\n© F. Simonetta, A. Llorens, M. Serrano, E. García-\nPortugués, and Á. Torrente. Licensed under a Creative Commons At-\ntribution 4.0 International License (CC BY 4.0). Attribution: F. Si-\nmonetta, A. Llorens, M. Serrano, E. García-Portugués, and Á. Torrente,\n“Optimizing Feature Extraction for Symbolic Music”, in Proc. of the\n24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.ful outcomes when employed to support multimodal ap-\nproaches [8], which integrate both audio and symbolic lev-\nels through audio-to-score alignment techniques [9]. The\nsymbolic level is also crucial for musicologists, as mu-\nsic scores are the most common source for historical mu-\nsic studies. Musicologists rely on computational tools to\nextract and analyze musical scores on a large scale [10,\n11]. However, traditional manual annotations, such as har-\nmony [12] and cadence [13], are time-consuming and prone\nto errors. Therefore, computational tools are essential for\nefﬁcient and accurate musicological analysis. Presently,\ntwo primary tools are available for extracting features from\nsymbolic music: jSymbolic [14] and music21 [15]. Al-\nthough both tools are open-source and widely employed,\nno comprehensive comparison between them has been con-\nducted yet.\nIn this paper, we propose a novel set of features that is\nspeciﬁcally, although not exclusively, tailored for the anal-\nysis of 18th-century Italian opera. We have developed a\ntool for extracting these features, named musif, that is be-\ning used for the analysis of operatic music in the Didone\nproject1[16]. Here, we conduct a comparative study be-\ntween musif and other existing tools, thus providing valu-\nable insights into the strengths and weaknesses of each of\nthem. Additionally, we evaluate the efﬁciency of each tool\nand demonstrate that musif adds useful features to both\nmusic21 and jSymbolic. We observe that, in most cases,\na combination of features from multiple tools yields the\nmost powerful feature set. To validate our ﬁndings, we\ntest all three tools on various repertoires. We aim to com-\npare the feature sets on ﬁle formats with varying levels\nof representation abilities, such as MIDI, MusicXML, and\n**kern. While MIDI is widespread in computational stud-\nies, it is relatively simplistic for written music; MusicXML\nand **kern, instead, are less commonly utilized in MIR but\nprovide more accurate representations when dealing with\nmusic scores.\nThe main contributions of this paper are, therefore,\nthreefold. Firstly, we present a new set of features designed\nfor the study of an under-represented repertoire in music\ncomputing literature, i.e., 18th-century Italian opera. Sec-\nondly, we introduce musif, a new efﬁcient, extensible, and\nopen-source Python tool for feature extraction from sym-\nbolic music. Finally, we provide a benchmark of music21,\n1https://didone.eu802jSymbolic, and musif on a variety of repertoires and ﬁle\nformats.\nThe whole code used for this study, as well\nas the code used for the proposed tool, is avail-\nable athttps://github.com/DIDONEproject/\nmusic_symbolic_features/ .\n2. FEATURE EXTRACTION TOOLS\nIn this study, we compare three tools for feature extraction\nfrom symbolic music: jSymbolic [14], music21 [15], and\nmusif. Other tools such as Humdrum2may be used for\nfeature extraction, but they would require a larger effort for\nassembling different features from various toolkits and or-\nganizing them in a usable tabular format. We will describe\neach one in detail in the following subsections.\n2.1 jSymbolic\nThe jSymbolic tool was initially introduced in 2006 [17]\nand subsequently updated in 2018 [14]. It is an open-\nsource, Java-based software designed to extract features\nfrom both MIDI and MEI ﬁles. The latest iteration of jSym-\nbolic is capable of extracting 246 distinct features, some\nof which are multidimensional and account for a total of\n1022 values. However, the actual number of extracted fea-\ntures may vary depending on the user’s conﬁguration and\nthe musical composition itself. jSymbolic features relate\nto pitch statistics, melodic intervals, chords and vertical in-\ntervals, rhythm, instrumentation, texture, and dynamics. In\naddition to these features, jSymbolic is capable of comput-\ning certain characteristics that are not readily available in\nMIDI ﬁles. To achieve this, jSymbolic utilizes the MEI ﬁle\nformat to determine the number of slurs and grace notes in\na given piece. While MEI and other high-informative ﬁle\nformats offer additional features such as pitch names, har-\nmonic analysis, and written dynamic or agogic indications,\njSymbolic does not take these into consideration.\nThe jSymbolic software provides users with the ﬂexibil-\nity to customize conﬁgurations and features, facilitating the\nintegration of previously existing feature values into newer\nfeatures. Furthermore, users can extract windowed features\nby specifying window size and overlap in seconds. jSym-\nbolic does not provide pre-built methods for parallel pro-\ncessing of large corpora, thereby requiring the user to im-\nplement a suitable strategy. Lastly, jSymbolic provides out-\nput options in both CSV and Weka’s ARFF format.\nThe software is accessible as a self-contained program\nfeaturing a Graphical User Interface (GUI) and a Command\nLine Interface (CLI), as well as as a Java library.\n2.2 music21\nmusic21 is a Python toolkit designed for computational mu-\nsic analysis, which was ﬁrst introduced in 2010 [18]. One\nof its remarkable features is the capability to parse a wide\nrange of ﬁle formats, including MIDI, MusicXML, **kern,\nABC, and various others. The music information is rep-\nresented in an object-oriented hierarchical structure that is\n2https://github.com/humdrum-tools/\nhumdrum-toolsaimed at facilitating the development of novel tools.\nAfter its initial academic publication, music21 was fur-\nther developed with a set of features presented in 2011 [15].\nThe latest version of music21 includes 69 features intro-\nduced by jSymbolic, as well as 20 characteristics computed\nusing the information parsed from high-informative ﬁle for-\nmats. These characteristics are related to key, cadence, har-\nmony, and lyrics. Regardless of the input ﬁle format, mu-\nsic21 consistently outputs 633 features. However, the num-\nber of extracted features may vary since some features are\nzeroed out when they are not computable.\nmusic21 is a Python module that lacks a CLI or a GUI.\nIt does not have a conﬁguration format; rather, it offers a\nbroad range of methods for developing custom pipelines\nfor different types of music information processing. These\nmethods encompass the creation of new features and some\nautomated high-level inference of music characteristics,\nsuch as key [19], as well as tools for windowed analysis.\nOne disadvantage of music21 is that large music scores\nmay result in deeply nested Python objects with numerous\nnon-picklable attributes attached. This makes the program-\nming process challenging, particularly due to the difﬁculty\nof saving these objects to a ﬁle.\nIn this study, we have developed a CLI for utilizing mu-\nsic21 feature extraction tools in a manner comparable to\nmusif. This implementation facilitates parallel processing\nby distributing the extraction of features across numerous\nﬁles simultaneously.\n2.3 musif\nOur software is named musif [20]. It is implemented in\nPython and built upon the music21 library, and offers an\nApplication Programming Interface (API) with no default\nsettings of signiﬁcance and a CLI with default settings op-\ntimized for most common use cases.\nWe leverage music21’s internal representation, enabling\nus to extract features from any ﬁle format supported by mu-\nsic21. musif is highly customizable and allows users to add\ncustom features as required. After creating the internal rep-\nresentation of the musical score using music21, we extract\nmultiple features and store them in pandas dataframes.\nThis facilitates exporting results in various formats, mak-\ning musif easily integrable into diverse pipelines.\nOne limitation of music21 is its restricted ability to seri-\nalize complex and large music scores. This restriction also\naffects the possibility of parallel processing, as Python’s\nsingle-thread approach necessitates parallelization via pro-\ncesses, which in turn requires context copying and data se-\nrialization. Furthermore, parsing large XML ﬁles is one of\nthe slowest steps in the feature extraction process. To op-\ntimize this procedure, a more favorable strategy would be\nto store the parsed XML ﬁles’ logical structure on disk as\na cache. We have thus implemented a caching system ca-\npable of caching and serializing any music21 object. A re-\nstriction to note about the caching system is that the cached\nscores are read-only. However, this feature enables the writ-\ning of parsed scores onto disk and caching of the output\nfrom resource-intensive music21 functions into memory.\nmusif can extract harmony-related features by utiliz-\ning standardized harmonic analyses annotated in the Mus-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n803Table 1 . Computational efﬁciency of the three feature extraction tools. Each run was repeated twice and the second run\ntimes are indicated between parentheses.\nFile format Tool Avg CPU Time (s) Avg Real Time (s) Avg RAM (GB) Max RAM (GB) Tot. errored ﬁles Tot. ﬁles\nMIDImusif 66.30 (13.30) 5.62 ( 1.14) 9.10 (10.1) 14.2 (19.6) 1\n16734 music21 55.3 (55.2) 4.72 (4.71) 7.12 (7.12) 9.87 (9.94) 0\njSymbolic 2.20 (2.20) 1.98 (1.97) 7.97 (7.14) 16.1 (11.7) 14\nMusicXMLmusif 15.4 ( 6.63) 1.32 ( 0.57) 5.87 (5.12) 12 (10.1) 414712music21 10.8 (10.8) 0.91 (0.91) 4.30 (4.33) 5.68 (5.55) 0\n**kernmusif 26 (13.1) 2.26 ( 1.14) 5.20 (4.14) 5.60 (4.92) 0472music21 14.0 (14.1) 1.21 (1.21) 3.08 (3.04) 4.12 (4.18) 0\neScore ﬁle format [12, 13]. Besides, it encompasses a wide\nrange of features, including melodic intervals, harmony, dy-\nnamics, tempo, density and texture, lyrics, instrumentation,\nscoring, and key. Notably, dynamics and tempo are deter-\nmined by the composer’s text notation rather than by MIDI\nparameters. Furthermore, our implementation includes all\nfeatures provided by music21 with the exception of 14 fea-\ntures that utilized the caching system in writing mode. The\nnumber of extracted features depends on the complexity of\nthe score and is inﬂuenced by both the number of parts and\nmusif’s compatibility with the encoding.\nNaN values are used to represent non-computable fea-\ntures in a score. For example, when processing datasets\nwith varying instrumentations, some features may not be\navailable for all scores. These values can be replaced with a\ndefault value (e.g., 0) or removed from the corpus by delet-\ning either the score or the related feature. In the CLI, we\nhave implemented a heuristic to determine whether a score\nshould be removed from the extracted corpus if it contains\ntoo many NaNs. Speciﬁcally, we deﬁne ras the ratio be-\ntween the number of columns without NaN and the total\nnumber of rows in the output table. If r <0.1, we compute\nni, which is the number of NaNs in the ith row. We remove\nrows with nigreater than1\n0.99q0.99, whereq0.99is the99%\nquantile of {n1,n2,...}, indicating that 99% of rows are\nnot deleted. The factor1\n0.99can be better understood as di-\nviding the Q0.99by 99, thus obtaining an estimate of Q0.01,\nand multiplying it by 100, thus obtaining the expected value\nofQ1.00based on the ﬁrst 99% of the data. Put differently,\nit computes the maximum nithat we expect if the remain-\ning 1% of rows has a number of NaN “similar” to the pre-\nvious 99%. Larger values are thus considered outliers. This\nmethod was empirically tested on the corpuses used in this\nwork (see Section 3), revealing that only a few scores were\ngenerally removed while most lines of the output table were\nretained. In case a score is not deleted, the CLI removes\nfrom the tale the features that are NaN in that score.\nmusif also incorporates a post-processing module that\nfacilitates the removal, merging, or substitution of values\nin speciﬁc columns or groups of columns within the ex-\ntracted data. This functionality proves especially advanta-\ngeous when dealing with large tables generated by musif\nfrom a substantial set of scores, as it minimizes the compu-\ntational effort required for processing such tables.\nLike the other tools, we have implemented the capabil-\nity to extract features at a window level. However, unlike\njSymbolic, in our implementation, the window length is\nspeciﬁed in musically relevant units such as score measures\nrather than seconds. This provides more pertinent informa-tion for processing music scores.\nIn contrast to other tools, our solution provides an out-\nof-the-box capability for processing large corpora through\nparallel processing, resulting in a reduction of the required\ntime.\nThe design principles and the features included in musif\nwere presented in a previous publication [20]. The code and\ndocumentation of musif is available online3.\n3. BENCHMARKING METHODOLOGY\nTo assess the performance of musif in comparison to other\ntools, we devised a benchmarking methodology. Initially,\nwe identiﬁed several datasets that enable testing of diverse\nﬁle formats. Subsequently, we developed a standardized\nprotocol based on an AutoML pipeline [21]. We evaluated\nthe computational resources utilized by each tool during ex-\ntraction and their respective efﬁcacy in various classiﬁca-\ntion tasks.\n3.1 Datasets\nWe selected ﬁve datasets to evaluate the performance of the\ntools in analyzing both Standard MIDI Files (SMFs) and\nhighly informative music score formats. For MIDI analy-\nsis, we aimed to test both music scores and performances.\nAs for highly informative ﬁle formats for music scores, we\nchose MusicXML and **kern due to their popularity, avail-\nability of large datasets, various conversion tools, and com-\npatibility with common music score editing software such\nas Finale, Sibelius, and MuseScore. While MEI was con-\nsidered as an option, the limited availability of datasets in\nthis format led us to leave it for future studies.\nIn this study, we considered the following datasets:\n•ASAP [22]: This dataset contains music performances\nderived from the Maestro dataset [23] and is synchro-\nnized with a corresponding score obtained from the Mus-\neScore’s crowd-sourced online library. The dataset com-\nprises 222 music scores in MusicXML and MIDI for-\nmats, as well as 1068 music performances in MIDI for-\nmat. The authors have rectiﬁed any signiﬁcant notation\nerrors found in the music scores. We used this dataset for\ncomposer recognition based on music scores and music\nperformances.\n•EWLD [24]: It contains lead sheets obtained from Wik-\nifonia, a crowd-sourced archive. To reduce errors in mu-\nsic score transcription by inexperienced users, the au-\nthors applied algorithmic selection criteria to the dataset.\n3https://github.com/DIDONEproject/musif ,\nhttps://musif.didone.euProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n804Table 2 . Resulting task size for each dataset and feature set.\nFeatures\nmusif music21 Extension Dataset Classiﬁcation task Samples Classes\nmusif musif native music21 music21 nativejSymbolic\nASAP performances Composer 211 10 710 91 633 602 225\nASAP scores Composer 211 7 710 91 633 602 225\nEWLD Genre 2645 11 710 91 633 602 225\nJLR Attribution 109 3 732 113 633 602 226\nQuartets Composer 363 3 1593 974 633 602 225MIDI\nDidone Decade 1622 8 745 126 633 602 225\nASAP scores Composer 211 7 710 91 633 602\nEWLD Genre 3197 11 724 105 633 602\nJLR Attribution 109 3 739 120 633 602MusicXML\nDidone Decade 1636 8 971 352 633 602\n**kern Quartets Composer 363 3 734 115 633 602\nSpeciﬁcally, they retained only scores with simple nota-\ntion, without modulations and with a single melodic part.\nMoreover, all scores contained key signatures and chords\nthroughout. The dataset was augmented by incorporat-\ning genre and composer details, as well as the year of\nﬁrst performance, composer birth and death dates, pre-\ncise title, and additional metadata. This was achieved by\ncross-referencing the dataset with information sourced\nfromsecondhandsong.com anddiscogs.com .\nWe used this dataset for genre recognition.\n•Josquin-La Rue [25]: This dataset was created within\nthe context of the Josquin Research Project and includes\n59 Josquin duos and 49 duos by La Rue. The musical\nscores underwent a meticulous musicological transcrip-\ntion process. Moreover, the music scores were assigned\nto two labels based on the security of the attribution,\nthus resulting in four labels (Josquin secure, La Rue se-\ncure, Josquin not secure, La Rue not secure). The musi-\ncal scores are provided in various ﬁle formats including\nMIDI, MusicXML, **kern, Sibelius, and PDF. We used\nthis dataset for composer classiﬁcation in a real-world\nattribution problem.\n•Quartets [26]: We retrieved a selection of ﬁles from the\nkern.humdrum.org website, consisting of all avail-\nable string quartets in **kern format by Mozart, Haydn,\nand Beethoven. While the original sources of these mu-\nsical scores are not always declared, the encoding qual-\nity is generally considered to be at a musicological level.\nIn total, we obtained 363 ﬁles. We used this dataset for\ncomposer classiﬁcation.\n•Didone [16]: With the aim of ﬁlling an under-studied\nrepertoire, we curated, analyzed, and transcribed over\n1600 arias from 18th-century opera, written by dozens\nof composers. The music scores were transcribed into\nMusicXML format using Finale Music software and\nrevised by three musicologists independently. Har-\nmonic analyses were added by expert musicologists us-\ning MuseScore software in accordance with a prior stan-\ndard [12, 13] and were reviewed automatically using the\nms3 tool [27]. We also included various metadata in the\ndatabase such as year and place of premiere, composer,\nand high-level formal analysis. This database is an ongo-\ning project and will be made freely available in 2024. We\nutilized this dataset for classifying the period of compo-\nsition of each piece, each period being deﬁned in decades(i.e., 1720s, 1730s, 1740s, etc.).\n3.2 Experimental setup\nAfter selecting the datasets, a standardized protocol was de-\nveloped for benchmarking the three aforementioned tools.\nThe protocol is based on an AutoML pipeline [21] and com-\nprises the following steps:\n1.Conversion to MIDI : The datasets were selected and\nsubsequently converted into MIDI format, resulting in\ntwo or three ﬁle formats for each dataset: MIDI and\neither MusicXML or **kern. This step aims to evalu-\nate the impact of notational ﬁle formats, such as Mu-\nsicXML or **kern, on classiﬁcation tasks. Indeed, al-\nthough MIDI has limited capacity for representing nota-\ntional aspects of music, it remains uncertain the extent\nto which these aspects can determine the accuracy of\nmachine-learning algorithms for music symbolic analy-\nsis. MusicXML ﬁles were converted using MuseScore\n3, and **kern ﬁles were processed with the Humdrum\ntoolkit4.\n2.Feature extraction : Features were extracted from\nMIDI, MusicXML, and **kern ﬁles using the methods\ndetailed in Section 2 with default settings and without\nthe use of windows, resulting in one array of features for\neach ﬁle. The purpose of this step was to measure the\ncomputational cost of the tools. Therefore, all available\nﬁles in the datasets were used to obtain a larger number\nof samples and a more accurate estimation of the compu-\ntational cost, even if they were discarded in later steps.\nFor instance, MIDI scores were already provided in the\nASAP dataset; however, we additionally converted them\nfrom the MusicXML ﬁles. As a result, we extracted fea-\ntures from more ﬁles than necessary. We created a CLI\ntool in Python for music21 while we utilized the ofﬁcial\nCLI tools for jSymbolic and musif. Each ﬁle format was\nprocessed individually, resulting in CSV ﬁles for each\nformat. We calculated the average time and RAM us-\nage of each tool. Furthermore, CPU time was collected\nas a measure of the required time without parallel pro-\ncessing. Lastly, we documented the number of ﬁles for\nwhich each tool produced errors.\n3.AutoML : A state-of-the-art machine learning ap-\nproach was employed using the Python module\nauto-sklearn [21]. The method utilizes Bayesian\n4See footnote 2 .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n805Table 3 . Accuracies of AutoML using 10-fold cross-validation on the ﬁrst ten principal components. The best-performing\ntool is underlined. The best-performing combination is shown in bold.\nTools Combinations\nExtension Dataset Dummy guessing\nmusifmusif\nnativemusic21music21\nnativejSymbolicmusif native +\nmusic21 nativemusif native +\njSymbolicmusic21 native +\njSymbolicmusif native +\nmusic21 native +\njSymbolic\nASAP performances .100 .960 .715 .978 .976 .916 .972 .962 .980 .979\nASAP scores .146 .743 .644 .781 .751 .780 .791 .819 .819 .857\nEWLD .091 .201 .157 .212 .204 .257 .219 .245 .242 .259\nJLR .344 .700 .642 .779 .751 .722 .711 .751 .742 .741\nQuartets .340 .678 .668 .725 .711 .810 .768 .831 .791 .822MIDI\nDidone .125 .359 .362 .403 .380 .443 .414 .451 .479 .462\nASAP scores .171 .773 .669 .759 .745 .785\nEWLD .091 .216 .185 .215 .201 .231\nJLR .334 .793 .663 .768 .756 .793MusicXML\nDidone .126 .398 .399 .384 .374 .392\n**kern Quartets .340 .713 .711 .767 .763 .810\noptimization with surrogate models based on random\nforests and generates ensembles of models by explor-\ning a vast array of possible architectures. 10-fold cross-\nvalidation was used, and the balanced accuracy averaged\nacross the test folds was observed. The best-performing\nmodel’s result was used for comparison. To initiate the\nAutoML process, a list of valid ﬁles for each dataset was\ninitially deﬁned, discarding those processed in the pre-\nvious step but unsuitable for validating the classiﬁcation\ntask. Subsequently, ﬁles were selected for which all tools\nsucceeded in extraction, creating comparable datasets for\nvalidation. Finally, classes with a number of samples\nless than twice the number of cross-validation splits were\neliminated from each dataset. Consequently, the num-\nber of ﬁles and categories used in our study differs from\nthe numbers ofﬁcially provided by each dataset. The\nclassiﬁcation task performed depended on the dataset, as\nshown in Table 2.\nWe conducted two primary experiments: one utilizing\nall of the extracted features and another using only the ﬁrst\nten principal components. To achieve this, we standardized\nthe features and applied PCA to obtain the ten ﬁrst princi-\npal components. The rationale for the latter experiment is\nthat a larger feature space typically requires a longer Au-\ntoML optimization process and affects the performance of\nthe trained classiﬁers. As the tools extract varying num-\nbers of features, this experiment enables a principled com-\nparison of the usefulness of the non-redundant information\ngenerated by the different tools by homogenizing the num-\nber of variables in the AutoML process. In other words, it\nhelps decouple the AutoML optimization capabilities from\nthe number of features.\nDue to the overlap between the features extracted with\nmusif and those with music21 with jSymbolic, we also an-\nalyzed the concatenation of music21, jSymbolic, and our\nfeatures. We also observed the performance of musif and\nmusic21 when only the native features were used, i.e. when\nmusif was utilized without music21 features and when mu-\nsic21 was run without jSymbolic features. In the follow-\ning, we denote these feature sets as “native”. We run each\nfeature extraction and AutoML experiment on a Linux ma-\nchine with 32 GB of RAM and an i7-8700 CPU, ending\nthe AutoML procedure after 30 minutes. We also experi-\nmented with longer AutoML processes and more powerful\nmachines for the ﬁrst 5 columns of tables 4 and 3, but we\nnoticed no signiﬁcant change in accuracy.4. RESULTS\nTable 1 summarizes the comparative computational efﬁ-\nciency of the three tools. It is observed that jSymbolic out-\nperforms the other tools when no parallel processing is em-\nployed. This can be attributed to the superior performance\nof Java language, which facilitates faster I/O operations and\nparsing of byte-level structures such as MIDI ﬁles. musif’s\ncaching system signiﬁcantly reduces the time required for\nfeature extraction during multiple runs, such as those per-\nformed during the development and debugging of newly\nadded features. For MIDI ﬁles, the extraction process can\nbe accelerated by a factor of ﬁve. When comparing the time\nneeded for extraction, jSymbolic is still faster than musif.\nHowever, our caching system is advantageous when a cache\nis available. Regarding MusicXML and **kern ﬁles, musif\nand music21 use the same parser engine, making their time\nvalues more comparable. In this case, music21 is slightly\nfaster than musif but also attempts to extract a smaller num-\nber of features. Nevertheless, musif’s the caching system\nallows for a 50% reduction in extraction times. The mu-\nsic21 tool proves to be the optimal choice when taking into\naccount RAM utilization.\nTable 2 presents the dataset sizes used in our experi-\nments, which are obtained through the protocol detailed in\nSection 3.2. The sample sizes vary from 109 to 3197, while\nthe number of classes ranges from 3 to 11, depending on\nthe dataset. The music21 feature extraction process pro-\nduces a ﬁxed set of 602 native features, supplemented by an\nadditional 31 features re-implemented from the jSymbolic\nfeature set. In contrast, jSymbolic consistently extracts a\nset of 225 features with minor variations. musif extracts\na variable number of features depending on its ability to\nparse different music structures, ranging from 91 to 974 ex-\ntracted features. The remaining features extracted by musif\nare computed using the music21 feature extraction meth-\nods. It is worth noting that music21 always converts non-\ncomputable features to zero, whereas musif allows users to\nassign different values or perform other operations.\nTables 3 and 4 demonstrate the effectiveness of feature\nsets in representing signiﬁcant aspects of music analysis\nacross various repertoires. The results in Table 4 must be\ninterpreted with caution due to the longer AutoML process\nrequired by accurate models when using a higher number\nof features. Overall, music21 and jSymbolic are effective\ntools for extracting features from MIDI ﬁles, while musifProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n806Table 4 . Accuracies of AutoML using 10-fold cross-validation on all the extracted features. The best-performing tool is\nunderlined. The best-performing combination is shown in bold.\nTools Combinations\nExtension Dataset Dummy guessing\nmusif musif native music21 music21 native jSymbolicmusif native +\nmusic21 nativemusif native +\njSymbolicmusic21 native +\njSymbolicmusif native +\nmusic21 native +\njSymbolic\nASAP performances .100 .983 .839 .983 .984 .985 .983 .985 .990 .988\nASAP scores .146 .843 .626 .877 .887 .886 .911 .898 .912 .937\nEWLD .0912 .224 .180 .249 .227 .248 .236 .250 .249 .251\nJLR .344 .746 .697 .806 .761 .747 .789 .787 .751 .774\nQuartets .340 .828 .771 .843 .813 .901 .843 .896 .880 .904MIDI\nDidone .125 .480 .429 .525 .508 .586 .515 .572 .596 .557\nASAP scores .171 .830 .710 .880 .841 .847\nEWLD .091 .251 .200 .266 .253 .245\nJLR .334 .797 .704 .815 .806 .750MusicXML\nDidone .126 .510 .504 .527 .516 .535\n**kern Quartets .340 .822 .786 .830 .820 .842\nTable 5 . Accuracies of AutoML. Effect of harmonic features on the Didone dataset.\nExtension Harmonic features musif musif native musif native + music21 native musif native + jSymbolic musif native + music21 native + jSymbolic\nNo .359 .362 .414 .451 .462MIDIYes .380 .372 .398 .452 .465\nNo .398 .399 .392First 10 PCs\nMusicXMLYes .385 .406 .409\nNo .510 .504 .515 .596 .557MIDIYes .507 .437 .518 .575 .560\nNo .480 .429 .535All features\nMusicXMLYes .535 .521 .564\nshows promising results for MusicXML ﬁles, particularly\nwhen utilizing the ﬁrst ten principal components during val-\nidation. This difference in performance can be attributed to\nthe presence of highly correlated features in musif, a con-\nsequence of its granularity. We also evaluated combina-\ntions of feature sets and found that optimal performance\nis achieved by employing multiple tools. For MIDI ﬁles,\njSymbolic is fundamental in achieving model accuracy, but\nincorporating musif and music21 generally enhances per-\nformance. For MusicXML and **kern ﬁles, leveraging\nboth musif and music21 yields optimal results, especially\nwhen considering the ﬁrst ten principal components.\nWhen comparing the efﬁcacy of models trained on Mu-\nsicXML, **kern, and MIDI ﬁles, no discernible pattern\nemerges indicating the superiority of highly informative ﬁle\nformats over SMFs for representing music scores. In fact,\nthe only instances where the MusicXML ﬁles exhibit su-\nperior performance are in the Josquin-La Rue dataset and\ngenre recognition on the EWLD dataset when all features\nare utilized. However, for all the remaining tasks, MIDI\nﬁles demonstrate superior performance. This is likely due\nto the fact that jSymbolic can only extract features from\nMIDI ﬁles and is simultaneously the most important source\nof features for music score analysis. Consequently, in this\nstudy, the MusicXML and **kern datasets lack some rele-\nvant features that can be extracted only when converted to\nMIDI. Even when comparing only the proposed tool and\nmusic21’s performances, MusicXML and **kern ﬁles do\nnot show a clear advantage over MIDI ﬁles, particularly\nwhen considering the combination of both tools. It should\nbe noted that jSymbolic can extract features from MEI as\nwell, thus potentially allowing for better performances.\nThe effect of missing values on tool performance is a\nsigniﬁcant concern and may be a contributing factor to\nthe comparatively lower results for MusicXML and **kern\nﬁles. While music21 substitutes all missing values with 0,\nmusif utilizes a hybrid strategy that entails either removing\na row or column from the table (refer to Section 2). Themost effective method for handling missing values remains\nan open issue.\nWe assessed the impact of harmonic features on the Di-\ndone dataset using musif. Unfortunately, due to the time-\nconsuming nature of harmonic annotations, we were un-\nable to evaluate these features on the other datasets used\nin this study. We annotated our dataset of more than 1600\nopera arias using the standard established in previous works\n(see Section 2) and extracted melody- and accompaniment-\nrelated features with respect to the local key. The extrac-\ntion of harmonic features resulted in 22 additional features\nbeyond the 126 listed in Table 2 for MIDI ﬁles. For Mu-\nsicXML ﬁles, we extracted 265 additional features, raising\nthe total number of extracted features to 617. We observed\nan overall improvement in classiﬁcation accuracy when in-\ncorporating harmonic features, as demonstrated in Table 5.\nThe only instance where performance was degraded by the\ninclusion of harmonic features was for MIDI ﬁles when all\nthe available features were considered (without PCA). We\ninterpret this degradation as an indication that longer pro-\ncessing times are necessary for AutoML when additional,\npossibly highly correlated features are introduced.\n5. CONCLUSION\nThis paper presents a comprehensive analysis of tools for\nextracting features from symbolic music. A strict protocol\nwas deﬁned to compare the tools in terms of efﬁciency and\nefﬁcacy across various repertoires and ﬁle formats. The re-\nsults indicate that using multiple tools is the most effective\napproach, with the optimal tool choice depending on the ﬁle\nformat and repertoire.\nThe study emphasizes the importance of using ﬁle for-\nmats that are accessible by multiple tools. However, it re-\nmains open whether highly informative ﬁle formats such as\nMusicXML, **kern, or MEI are relevant for the automatic\nclassiﬁcation of symbolic scores. The available set of fea-\ntures indicates that, while these formats remain fundamen-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n807tal for certain types of musicological research, they do not\nseem to entail a signiﬁcant advantage for machine learning\ntasks.\nThe problem of NaN values in extracted features from\nmusic scores remains unresolved. Further research is re-\nquired to explore optimal approaches for replacing, remov-\ning, or inferring missing values in music applications.\nAdditionally, the new musif tool was proposed, which\ncan process various ﬁle formats using the music21 pars-\ning engine. The tool also includes a caching mechanism\nto speed up feature development. Moreover, motivated by\nthe experiments presented in this work, we included the\nwhole music21 and jSymbolic tools in the newer versions\nof musif, easing the extraction of the combined feature sets\nfrom large corpora.\n6. ACKNOWLEDGEMENTS\nThis publication is a result of the Didone Project, which\nhas received funding from the European Research Council\n(ERC) under the European Union’s Horizon 2020 research\nand innovation program, Grant agreement No. 788986.\nIt has also been conducted with funding from Spain’s\nMinistry of Science and Innovation (IJC2020-043969-\nI/AEI/10.13039/501100011033).\nPart of the computational experiments were run at the\nFinisTerrae III cluster of the Galician Supercomputing Cen-\nter (CESGA). The authors gratefully acknowledge the ac-\ncess to these resources.\n7. REFERENCES\n[1] K. C. Kempfert and S. W. K. Wong, “Where Does\nHaydn End and Mozart Begin? Composer Classiﬁ-\ncation of String Quartets,” Journal of New Music Re-\nsearch , vol. 49, no. 5, pp. 457–476, Oct. 2020.\n[2] W. Herlands, R. Der, Y . Greenberg, and S. Levin, “A\nMachine Learning Approach to Musically Meaning-\nful Homogeneous Style Classiﬁcation,” Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol. 28,\nno. 1, Jun. 2014.\n[3] J. Qiu, C. L. P. Chen, and T. Zhang, “A Novel Multi-\nTask Learning Method for Symbolic Music Emotion\nRecognition,” 2022.\n[4] M. Zeng, X. Tan, R. Wang, Z. Ju, T. Qin, and T.-\nY . Liu, “MusicBERT: Symbolic Music Understanding\nwith Large-Scale Pre-Training,” FINDINGS , 2021.\n[5] D. Jeong and J. Nam, “Note Intensity Estimation of Pi-\nano Recordings by Score-informed NMF,” 2017, p. 8.\n[6] F. Simonetta, S. Ntalampiras, and F. Avanzini,\n“Acoustics-Speciﬁc Piano Velocity Estimation,” in Pro-\nceedings of the IEEE MMSP 2022 , 2022.\n[7] H. Vinet, “The Representation Levels of Music Infor-\nmation,” in Computer Music Modeling and Retrieval .\nBerlin, Heidelberg: Springer Berlin Heidelberg, 2004,\npp. 193–209.[8] F. Simonetta, S. Ntalampiras, and F. Avanzini, “Mul-\ntimodal Music Information Processing and Retrieval:\nSurvey and Future Challenges,” in Proceedings of 2019\nInternational Workshop on Multilayer Music Represen-\ntation and Processing . Milan, Italy: IEEE Conference\nPublishing Services, 2019, pp. 10–18.\n[9] ——, “Audio-to-Score Alignment Using Deep Auto-\nmatic Music Transcription,” in Proceeddings of the\nIEEE MMSP 2021 , 2021.\n[10] A. Llorens and A. Torrente, “Constructing opera seria\nin the Iberian Courts: Metastasian Repertoire for Spain\nand Portugal,” Anuario Musical , vol. 76, pp. 73–110,\nJul. 2021.\n[11] F. Moss, W. Fernandes de Souza, and M. Rohrmeier,\n“Harmony and Form in Brazilian Choro: A Corpus-\nDriven Approach to Musical Style Analysis,” Journal\nof New Music Research , vol. 49, pp. 416–437, 2020.\n[12] M. Neuwirth, D. Harasim, F. C. Moss, and\nM. Rohrmeier, “The Annotated Beethoven Corpus\n(ABC): A Dataset of Harmonic Analyses of All\nBeethoven String Quartets,” Frontiers in Digital\nHumanities , vol. 5, 2018.\n[13] J. Hentschel, M. Neuwirth, and M. Rohrmeier, “The\nAnnotated Mozart Sonatas: Score, Harmony, and Ca-\ndence,” Transactions of the International Society for\nMusic Information Retrieval , vol. 4, no. 1, pp. 67–80,\nMay 2021.\n[14] C. McKay, J. Cumming, and I. Fujinaga, “jSymbolic\n2.2: Extracting Features from Symbolic Music for Use\nin Musicological and MIR Research,” in Proceedings\nof the 19th International Society for Music Information\nRetrieval Conference , 2018, pp. 348–354.\n[15] M. S. Cuthbert, C. Ariza, and L. Friedland, “Feature\nExtraction and Machine Learning on Symbolic Music\nUsing the Music21 Toolkit,” in Proceedings of the 12th\nInternational Society for Music Information Retrieval\nConference, ISMIR 2011, Miami, Florida, USA, Octo-\nber 24-28, 2011 . University of Miami, 2011, pp. 387–\n392.\n[16] A. Torrente and A. Llorens, “The Musicology Lab:\nTeamwork and the Musicological Toolbox,” in Music\nEncoding Conference Proceedings 2021, 19–22 July,\n2021 University of Alicante (Spain): Onsite &amp;\nOnline, 2022, ISBN 978-84-1302-173-7, págs. 9-20 .\nUniversidad de Alicante / Universitat d’Alacant, 2022,\npp. 9–20. [Online]. Available: https://dialnet.unirioja.\nes/servlet/articulo?codigo=8463477\n[17] C. McKay and I. Fujinaga, “jSymbolic: A Feature\nExtractor for MIDI Files,” in Proceedings of the 2006\nInternational Computer Music Conference, ICMC\n2006, New Orleans, Louisiana, USA, November 6-11,\n2006 . Michigan Publishing, 2006. [Online]. Available:\nhttps://hdl.handle.net/2027/spo.bbp2372.2006.063Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n808[18] M. S. Cuthbert and C. Ariza, “Music21: A Toolkit\nfor Computer-Aided Musicology and Symbolic Music\nData,” International Society for Music Information\nRetrieval Conference (ISMIR 2010) , pp. 637–642,\n2010. [Online]. Available: http://ismir2010.ismir.net/\nproceedings/ismir2010-108.pdf\n[19] C. L. Krumhansl, Cognitive Foundations of Musical\nPitch . Oxford University Press, 1990.\n[20] A. Llorens, F. Simonetta, M. Serrano, and Á. Torrente,\n“Musif: A Python Package for Symbolic Music Feature\nExtraction,” in Proceedings of SMC 2023 - Sound and\nMusic Computing Conference , Stockholm, 2023, June,\npp. 132–138.\n[21] M. Feurer, K. Eggensperger, S. Falkner, M. Lin-\ndauer, and F. Hutter, “Auto-Sklearn 2.0: Hands-free\nAutoML via Meta-Learning,” arXiv, Tech. Rep.\narXiv:2007.04074, Sep. 2021. [Online]. Available:\nhttp://arxiv.org/abs/2007.04074\n[22] F. Foscarin, A. McLeod, P. Rigaux, F. Jacquemard, and\nM. Sakai, “ASAP: A Dataset of Aligned Scores and\nPerformances for Piano Transcription,” in Proceedings\nof the 21st International Society for Music Information\nRetrieval , 2020, Proceedings.\n[23] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-\nZ. A. Huang, S. Dieleman, E. Elsen, J. H. Engel, and\nD. Eck, “Enabling Factorized Piano Music Modeling\nand Generation with the MAESTRO Dataset.” in In-\nternational Conference on Learning Representations ,\n2019.\n[24] F. Simonetta, F. Carnovalini, N. Orio, and A. Rodà,\n“Symbolic Music Similarity through a Graph-based\nRepresentation,” in Proceedings of the Audio Mostly\n2018 on Sound in Immersion and Emotion - AM’18 .\nACM Press, 2018.\n[25] J. Cumming, C. McKay, J. Stuchbery, and I. Fuji-\nnaga, “Methodologies for Creating Symbolic Corpora\nof Western Music Before 1600,” in Proceedings of the\nISMIR . Paris, France: ISMIR, Sep. 2018, pp. 491–498.\n[26] C. S. Sapp, “Online Database of Scores in the Hum-\ndrum File Format,” in Proceedings of the 6th Inter-\nnational Conference on Music Information Retrieval ,\n2005, p. 2.\n[27] J. Hentschel and M. Rohrmeier, “Creating and Evaluat-\ning an Annotated Corpus Using the Library Ms3,” 2020.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n809"
    },
    {
        "title": "Polyrhythmic Modelling of Non-Isochronous and Microtiming Patterns.",
        "author": [
            "George Sioros"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265245",
        "url": "https://doi.org/10.5281/zenodo.10265245",
        "ee": "https://zenodo.org/records/10265245/files/000016.pdf",
        "abstract": "Computational models and analyses of musical rhythms are predominantly based on the subdivision of durations down to a common isochronous pulse, which plays a fundamental structural role in the organization of their durational patterns. Meter, the most widespread example of such a temporal scheme, consists of several hierarchically organized pulses. Deviations from isochrony found in musical patterns are considered to form an expressive, micro level of organization that is distinct from the structural macro-organization of the basic pulse. However, polyrhythmic structures, such as those found in music from West Africa or the African diaspora, challenge both the hierarchical subdivision of durations and the structural isochrony of the above models. Here we present a model that integrates the macro- and micro-organization of rhythms by generating non-isochronous girds from isochronous pulses within a polyrhythmic structure. Observed micro-timing patterns may then be generated from structural non-isochronous grids, rather than being understood as expressive deviations from isochrony. We examine the basic mathematical properties of the model and show that meter can be generated as a special case. Finally, we demonstrate the model in the analysis of micro-timing patterns observed in Brazilian samba performances.",
        "zenodo_id": 10265245,
        "dblp_key": "conf/ismir/Sioros23",
        "keywords": [
            "Computational models",
            "musical rhythms",
            "isochronous pulse",
            "meter",
            "polyrhythmic structures",
            "micro-level organization",
            "expressive deviations",
            "non-isochronous girds",
            "structural isochrony",
            "basic mathematical properties"
        ],
        "content": "POLYRHYTHMIC MODELLING OF NON-ISOCHRONOUS \nAND MICROTIMING PATTERNS \n George Sioros  \n University of Plymouth \ngeorgios.sioros@plymouth.ac.uk  \nABSTRACT \nComputational models and analyses of musical rhythm s \nare predominantly based on the subdivision of durat ions \ndown to a common isochronous pulse, which plays a f un-\ndamental structural role in the organization of the ir dura-\ntional patterns. Meter, the most widespread example  of \nsuch a temporal scheme, consists of several hierarc hically \norganized pulses. Deviations from isochrony found i n mu-\nsical patterns are considered to form an expressive , micro \nlevel of organization that is distinct from the str uctural \nmacro-organization of the basic pulse. However, pol y-\nrhythmic structures, such as those found in music f rom \nWest Africa or the African diaspora, challenge both  the hi-\nerarchical subdivision of durations and the structu ral \nisochrony of the above models. Here we present a mo del \nthat integrates the macro- and micro-organization o f \nrhythms by generating non-isochronous grids from is och-\nronous pulses within a polyrhythmic structure. Obse rved \nmicro-timing patterns may then be generated from st ruc-\ntural non-isochronous grids, rather than being unde rstood \nas expressive deviations from isochrony. We examine  the \nbasic mathematical properties of the model and show  that \nmeter can be generated as a special case. Finally, we \ndemonstrate the model in the analysis of micro-timi ng pat-\nterns observed in Brazilian samba performances. \n1. INTRODUCTION \nIsochrony has been a fundamental element of computa -\ntional and cognitive models of musical rhythm, whic h are \noften inspired by the organization of rhythms found  in \nWestern classical music theory [1]. The advantages of \nisochrony as a structural foundation for the organi zation of \nmusic are numerous, from the potential to explain o ur abil-\nity to synchronize to music [2–5], to defining high er level \nqualities such as tempo [1, 6]. However, while peri odicity \nis common in music, strict isochrony is almost neve r ob-\nserved [7] and many studies document with empirical  data \nthe systematic durational patterns from music aroun d the \nworld, which do not fit into an isochronous structu re, in-\ncluding the Viennese waltz [8], Brazilian Samba [9– 11], \nMali Jembe music [12] and the Norwegian Telespringa r \n[13].  Generally, such patterns are understood on the basi s of \nan underlying structural basic isochronous pulse an d ex-\npressive deviations from it [14]. Although an isoch ronous \npulse may not be directly observed and measured in the \nmusic signal, a steady beat may be inferred by the listener, \nmost evidently when we bob our head or tap our feet  to the \nmusic. Numerous beat tracking algorithms exist in t he mu-\nsic information retrieval literature [15] and even cognitive \nmodels have been formalized that attempt to imitate  this \nbehavior [16, 17]. In most music-theoretical and co gnitive \nmodels of rhythm, the beats of the basic isochronou s pulse \nare grouped together, say every two or three, resul ting in \nslower pulses that coincide with all faster ones, f orming a \nhierarchical structure often referred to as meter [ 1, 18].  \nTypically, deviations from isochrony are modeled by  \nprocessing repeating rhythmic patterns to derive st atistical \nproperties, such as the mean deviation from an isoc hronous \npulse at each location of the repetition cycle (see  for ex-\nample [10]). Such statistical models only describe the pro-\ncessed recordings and cannot be generalized. They d o, \nhowever, provide evidence that deviations from isoc hrony \nmay be more than a mere expressive element of the p erfor-\nmance, and are rather structural components of musi c [12]. \nPolyrhythmic music challenges the principle of hier ar-\nchical organization of rhythms. Polyrhythms are org anized \non the basis of multiple isochronous pulses of diff erent pe-\nriods that do not coincide [19–21]. Polyrhythmic el ements \nare found in music around the world, with most repr esenta-\ntive examples coming from West Africa and the Afric an \ndiaspora [22, 23].  Even in groove-oriented music, where \na strong sense of pulse is felt, short patterns tha t suggest an \nalternative pulse are common [24]  and evidence ind icate \nthat they are central to experiencing groove [25]. Perhaps \nunsurprisingly, certain polyrhythmic music traditio ns also \nexhibit large and systematic deviations from isochr ony \n[26]. It has been proposed that the deviations from  \nisochrony and the polyrhythmic character of music f rom \nthe African diaspora are related, one enhancing the  other \n[9, p. 234, 23].  \nThis paper presents a music-theoretical model that con-\nstructs non-isochronous grids from different isochr onous \npulses within a polyrhythmic context. Essentially, it attrib-\nutes systematic non-isochronous patterns to underly ing \npolyrhythmic structures that may be considered fund amen-\ntal to the organization of the rhythmic patterns.  In section \n2, we present relevant music-theoretical concepts. Section \n3, introduces a mathematical formalization of our m odel \nalong with its key properties. In section 4, we emp loy the \nmodel to analyze Brazilian samba performance data t aken \nfrom existing literature.  © G. Sioros. Licensed under a Creative Commons Att ribu-\ntion 4.0 International License (CC BY 4.0). Attribution:  G. Sioros, “Pol-\nyrhythmic modelling of non-isochronous and microtim ing patterns” in \nProc. of the 24td Int. Society for Music Information Retrieval Conf. , Mi-\nlan, Italy, 2023. \n146  \n \n2. BACKGROUND \n2.1 Structural isochrony \nIn music theory, meter is formalized as a hierarchi cal \nstructure that groups pulses periodically [18].  In  typical \nmainstream Western music, the events are aligned ac ross \nthe voices in such a way that the salient moments c oincide \nto form a metrical hierarchy (Figure 1). The differ ent lev-\nels of the hierarchy correspond to various regulari ties \nfound in the rhythm. While each level takes the for m of a \nsteady pulse, the various levels are stacked so tha t slower \npulses coincide with all faster ones. Consequently,  the pe-\nriodicities in the articulated patterns in the musi c should \nalso align in a similar fashion, although each voic e may \nnot necessarily articulate a specific metrical leve l. The pe-\nriodic grouping of the beats has been formalized as  a prime \nfactorization problem [27–29]. Generative models of  me-\nter that can be implemented in computer algorithms have \nbeen developed as a set of transformations [30] and  an ab-\nstract context-free grammar [31]  \nAs a cognitive mechanism, meter is understood as os -\ncillations that represent the attentional energy of  the lis-\ntener [3, 4, 32] or a predictive schema [33] that e xpresses \nour expectations about the timing of musical events  [34]. \nThe limitations of our cognition impose temporal li mits to \nthe pulses of the metrical hierarchy [1, p. 29, 35] . At the \nlower limit, the shortest pulse has a period of ~10 0ms, and \nat the upper limit, the longest pulse has a period of ~1.5s. \nThe highest metrical salience is observed for pulse s with a \nmoderate period of ~700ms [6]. Tempo is then define d as \nthe frequency of the most salient isochronous pulse  of the \nmetrical hierarchy. \nThe durations of the sound events are classified by  the \nlistener into discrete categories [2, p. 382, 36–39 ] that are \ninfluenced by the sensation of a pulse or meter evo ked in \nthem, so that a certain duration may be interpreted  as a dif-ferent category when listened in a different metric al frame-\nwork [40]. A rhythmic pattern is essentially coded as a se-\nries of nominal durations that are a multiple of th e basic \nisochronous pulse of the metrical hierarchy. \nDespite the fundamental role that isochrony plays i n the \nconstruction of meter, non-isochronous grouping of beats \ncreate non-isochronous metrical levels and meters ( NI me-\nters) [1, Ch. 7] (see Figure 2 for an example). Suc h group-\nings are based on the principle of maximal evenness  [1, 41, \n42] which is also the basis for the Euclidean rhyth ms that \nare encountered in many traditional rhythms [43, 44 ]. Such \nnon-isochronous patterns are constructed by distrib uting a \nnumber of onsets k as evenly as possible over a number of \nbeats n of an isochronous pulse. A Euclidean rhythm can \nthen be denoted as E(k,n) [43]. \n2.2 Micro-timing \nWhile the metrical hierarchy determines durational cate-\ngories for musical events, continuously variable ti ming de-\ntermines an expressive level of organization of rhy thms \n[14]. Systematic deviations from the nominal durati ons are \ntypically measured in ms or as a percentage of the nominal \nbeat duration to allow for an easier comparison bet ween \nmusic segments with different tempi (Figure 3). The  phe-\nnomenology of micro-timing has been discussed withi n \nthe context of various music genres (see [9, 12, 45 , 46] for \nsome examples). Micro-timing modeling typically rel ies \non statistical analysis of the timing of musical ev ents over \nthe duration of a performance to identify systemati c, non-\nisochronous patterns [10, 47].  \n \nFigure 3 : Onsets (o) may not exactly align with the isoch-\nronous pulse and have expressive (micro) timing dev ia-\ntions denoted with arrows ( ®). \n2.3 Polyrhythms \nPolyrhythms have been described in music theory as a \nform of metric dissonance [19, 20]. While consonant  \npulses align to give rise to the hierarchical struc ture of me-\nter, dissonant pulses intertwine. Typically, polyrh ythms \nconsist of pulses with distinct beat durations that  are not \nsimple integer multiples of one another. The ratio between \nthe beat durations of the two pulses determines the  length \nof the repetition cycle of the entire polyrhythm. T he num-\nber of beats of each pulse within a cycle of the po lyrhythm \n!!and !\" are related to the beat durations of the two pulse s \n\"#!and \"#\": \n \"#!\n\"#\"$!\"\n!! (1) \n \n \nFigure 1 : The metrical structure (bottom) emerging from \nThe first two bars of “Conquest of Paradise” of com poser \nVangelis (top). Events are marked with (o). The thr ee pulse \nlevels of the metrical hierarchy ( | )  have period icities of \nsimple integer ratios 1:3:4 and are aligned with no  phase \ndifferences. \n \nFigure 2 : Example of a Non-Isochronous meter. The mid-\ndle metrical level is non-isochronous and can be co n-\nstructed as the Euclidean rhythm E(4,9). \n \n \nFigure 4 : The 2|3 polyrhythm. A basic isochronous pulse \nsubdivides both the 2-beat and the 3-beat pulses. \nProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n147  \n \nPolyrhythms can then be represented as !!%!\".  Figure 4 \ndepicts a polyrhythm in which 2 beats of one pulse have \nthe same duration as 3 beats of a second pulse. The  pulses \nthat constitute a polyrhythmic structure may have a  com-\nmon faster subdivision with a beat duration longer than the \nperceptual threshold of 100ms, resulting in a type of \ngrouping dissonance or polymeter [21].  \n2.4 Polyrhythms as flexible spaces \nIn principle, onsets are assumed to belong to one o f the \npulses of a polyrhythm, even if they are not perfec tly \naligned. However, it has been proposed that in musi c from \nthe African diaspora, the intervals between the pul ses of a \n16|12 polyrhythm define a flexible space [23]. Even ts oc-\ncurring between a beat from the 12-beat pulse and a  beat \nfrom the 16-beat pulse may have a ‘mixed’ character , be-\nlonging at the same time to both pulses. Here, we e xtend \nthis concept of ‘mixed’ character events to create non-\nisochronous grids that combine the character of bot h \npulses of a polyrhythm. In this way, we formalize ( micro) \ntiming deviations from isochrony as a structural el ement \nof musical rhythms rooted in polyrhythms.  \n3. NON-ISOCHRONOUS GRIDS \n3.1 Definition and construction \nNon-isochronous pulses are constructed from two iso chro-\nnous pulses of different periods. We will refer to the con-\nstructed non-isochronous pulses as grids (NI grids)  to bet-\nter distinguish them from isochronous pulses. The N I grid \nis formed by gradually changing the positions of th e beats \nof one isochronous pulse towards a proximate beat o f the \nother pulse. We call the first pulse the ‘formative ’ pulse \nand the later pulse the ‘target’ pulse of the NI gr id (Figure \n5). The new beat positions for the NI grid are dete rmined \nby: \n &#$ '#( )#* +#$, '#- (2) \nwhere i is the beat index of the formative pulse, '# is the \nformative’s beat time, #$ is the target’s proximate beat \ntime and )# is the ‘shift’ as a fraction of the distance be-\ntween the two beats taking values in the range [0, 1].  \nIn principle, each beat may be shifted independentl y to \nform an NI pattern with multiple durations. Here, w e ex-\namine the case of a uniform shift, where all format ive beats \nare shifted by the same parameter S towards the nearest \nbeat of the target pulse. Then, as S goes from 0 to 1, the \nformative pulse is being ‘morphed’ into the target pulse. \nWhile the relative shift S is uniform, the individual beat displacements in time units (e.g. ms) will still ha ve differ-\nent values as the distance between the beats of the  two \npulses is not the same for all beats. Furthermore, S can also \nexhibit dynamic variations. In this sense, it shoul d be un-\nderstood as analogous to tempo, which can serve as a uni-\nform parameter within a given time span and can als o ex-\nhibit variations over the duration of a piece.   \nNI grids constructed by a uniform shift consist of only \ntwo beat classes, which we refer to as Short and Lo ng for \nsimplicity. If \"' is the period of the formative pulse and \n\"# the period of the target pulse, then the two beat classes’ \ndurations are: \n \"& $ ./ , ) 0* \"' ( ) * \"# * 1  (3) \nwhere, k can take one of two values: \n 1%&'() $2\"'\n\"#3$ 45 1*'+, $6\"'\n\"#7$ 4 ( /  (4) \nwhere 8 9 and : ; denote the floor and ceiling functions. \nThe durations of the Short and Long beats are limit ed \nwithin a NI grid. For ) $ < , both the Short and Long beats \nhave a duration equal to the period of the formativ e pulse =\n\"', which is the upper limit for the Short beats and the \nlower limit for the Long beats. Conversely, for ) $ / , the \nShort beats reach their shortest duration and the L ong beats \ntheir longest duration which are =>?@ABAC=DEF@>GFAH=IJ=@KA=\nLECM@>I?= IJ= @KA= GAC>IL= IJ= @KA= @MCBA@= GEFHA= \"#. When \n\"' N \"# , i.e. !-O !., then 1%&'() $ < and  1*'+, $ / \nand therefore the Short beats can reach a duration of 0.=\nBy construction, the total number of beats of an NI  grid \nis the same as the number of beats of the formative  pulse.  \nThe number of Long beats of an NI grid can be calcu lated \nas the remainder of the division between the number  of \nbeats of the formative and target pulses: \n !*'+, $ !.=P=!- (5) \n3.2 Maximal evenness in NI grids \nOne of the key properties of the NI grids is that t he Short \nand Long beats are evenly distributed; a direct con se-\nquence of the underlying polyrhythmic pulses being isoch-\nronous and the shift S being uniform. Different alignments \nof the pulses result in different rotations of the Short-Long \nbeat pattern.  \nSo far, we have examined shifts of the formative be ats \ntowards the nearest target beats. The above equatio ns and \nthe even distribution of the Short-Long beat classe s also \napply to shifts towards the next or previous target  beat. Ar-\nbitrary combinations of the shifts, e.g. some towar ds the \nnearest beat and others towards the next beat, may also re-\nsult in an even distribution of the two beat classe s. How-\never, this is guaranteed only when all formative be ats fol-\nlow the same rule. In Figure 6, the example of a 6 beat \nformative pulse and an 8 beat target pulse is shown . It fol-\nlows from Eqn (5) that the number of Long beats in the \nresulting NI grid is 2, and the remaining 4 are Sho rt. The \ndifferent alignments of the formative and target pu lses in \nFigure 6 produce the same Short-Long beat pattern b ut in \ndifferent rotations.  \n  Given a polyrhythm =!-%!.=and its total duration, Eqns \n(3 - 5) can be used to calculate the durations and number \nof Long and Short beats in the corresponding NI gri d as a  \n \nFigure 5:  Construction of a non-isochronous grid by uni-\nformly shifting the beats of a formative pulse towa rds the \nnearest beat positions of a target pulse.  \nProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n148  \n \nfunction of the shift S. The NI grid can then be produced \ndirectly by the Euclidean algorithm as a maximally even \ndistribution of the two beat classes [1, 41, 43]. \nNI grids can be represented as polyrhythms with the  ad-\nditional parameter S: &Q.!-%!.5 )0. However, in contrast \nto polyrhythms, the order that the two pulses appea r in the \nNI grid definition is important. The formative and target \npulses are not equivalent as can easily be seen fro m Eqn \n(5), where the  modulo operation is not commutative , and \nthe fact that the number of beats of an NI grid is equal to \nthe number of beats of the formative pulse, but not  of the \ntarget pulse. \nAs a consequence of the even distribution of the Lo ng \nand Short beats, every Euclidean pattern can be con -\nstructed as a NI grid (see Figure 7 for an example taken \nfrom [43]). In fact, it follows directly from the c onstruction \nof Euclidean patterns that Euclidean patterns are e quiva-\nlent to NI grids with a shift S = 1: \n R.S5 T0$ &Q .S=%=T5 / 0 (6) \nSince the levels of a metrical hierarchy are evenly  dis-\ntributed, they can be constructed from NI grids. Fo r NI me-\nters, the process is similar to the construction of  Euclidean \npatterns. However, typically, meters include isochr onous \npulses at all levels of their hierarchy and, theref ore, these \nmeters can be constructed from degenerate NI grids for \nwhich the Short and Long beats have the same durati on.  \n4. APPLICATION TO SAMBA PERFORMANCES  \nMusic events may be assigned to the beats of an NI grid. \nSimilar to beat tracking, analyzing an observed dur ational pattern requires aligning event onsets with beats. As not all \nbeats in an NI grid are necessarily articulated, th ere may \nbe more than one NI grid that fits the rhythmic pat tern. In-\nformation about the polyrhythmic structures that ma y be \nexpected for the music at hand can help reduce the search \nspace and find meaningful solutions. Here, we demon -\nstrate the potential of the model in musical rhythm  analysis \nby constructing NI grids that fit the durational pa tterns \nfound in Brazilian samba.  \nVarious rhythmic patterns of samba recordings have \nbeen reported in the literature [9–11, 13]. They ar e consid-\nered a characteristic feature of the performances a nd an in-\ntegral part of the style, although the degree of de viation \nfrom isochrony as well as the specific non-isochron ous \npatterns measured vary between studies. In [10], th e same \nSamba rhythm was recorded at three different tempi.  From \nthe recordings, mean durations of the four events t hat make \nup the basic repetitive pattern were calculated. Th e results, \nwhich are summarized in Table 1, show that all thre e tempi \nfollow the same general Medium-Short-Medium-Long du -\nrational pattern. The relative durations however ar e differ-\nent at each tempo, with the fast and preferred temp i show-\ning the most similarity and the slow tempo being mo re dis-\ntinct and closer to isochrony with a characteristic  length-\nened last event. Additionally, the two Medium event s (1 \nand 3) were reported to be significantly different between \nthem, indicative of Medium-Short and Medium-Long du -\nration [10]. \nHere we hypothesize that the observed non-isochrono us \npatterns of  Table 1 are the result of an underlyin g poly-\nrhythmic structure and we attempt to reproduce them  as: \n1) NI grids with a 4-beat formative pulse and 3-bea t target \npulse (section 4.1), and 2) NI grids with a 5-beat formative \npulse and a binary target pulse (section 4.2). In s ection 4.3, \nwe use the NI grids to propose potential explanatio ns for \nthe differences in the observed durations between t he three \ntempi. \n4.1 The 4-beat formative pulse hypothesis \nOur first hypothesis is that the basic Samba patter n (Table \n1) emerges from an underlying 4|3 polyrhythm. From Eqn \n(5), it follows that the corresponding NI grid cons ists of 3 \n \nTempo - BPM Event number \n1 2 3 4 \nMeasured in ms  \nFast - 133 121 \n±7.1 69  \n±6.0 112 \n±5.1 153 \n±8.6 \nPreferred - 100 157 \n±8.7 110 \n±8.4 142 \n±5.8 196 \n±9.0 \nSlow - 69 212 \n±9.7 198 \n±12.4 206 \n±6.6 256 \n±9.5 \nMeasured in percent of the total duration \nFast - 133 27 15 25 34 \nPreferred - 100 26 18 24 33 \nSlow - 69 24 23 24 29 \nTable 1 : Mean durations and standard deviations of the \nfour events of the basic Samba pattern from [8, Tbl . 3].   \n \nFigure 7:  The Cuban cinquillo pattern is the Euclidean \nrhythm R.U5V0 and the NI grid &Q.U=%=V5 / 0. \n \n \nFigure 6 : Three different alignments of a formative pulse \nwith 6 beats and a target pulse of 8 beats produce the same \npattern of evenly distributed Short-Long (L/S) beat s, but \nin a different rotation. In A and B, all formative beats are \nshifted towards the nearest target beats. In C, the  formative \nbeats are all shifted forward, i.e. always towards the next \ntarget beat. \n \nProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n149  \n \nLong beats and a single Short beat. Since the NI gr id has \nthe same number of beats as the number of events in  the \nSamba pattern, all four beats coincide with an even t \n(Figure 8) and the shortest event duration (event 2 ) is \naligned to the Short beat of the NI grid. Then, S is chosen \nso that it minimizes the difference between the the oretical \nbeat durations and the mean event durations for the  three \ndifferent tempi. The results are summarized in Tabl e 2. \nThe three different Samba patterns correspond to th ree \ndifferent shifts S, so that events 1 and 2 of the Samba pat-\ntern are well aligned to the NI grid at all three t empi. How-\never, since the NI grids consist of only two beat c lasses, \nevents 1, 3 and 4 are matched to beats with the sam e dura-\ntion and therefore this model cannot capture the ch aracter-\nistic longer 4th event and the difference in the durations of \nthe two Medium events (1 and 3).  \n4.2 The 5-beat formative pulse hypothesis \nOur second hypothesis is that the observed Medium-S hort-\nMedium-Long pattern stems from the superposition of   5-beat and a binary pulse. Since only 4 of the 5 beat s of the \nNI grid coincide with events, the two beat classes may pro-\nduce three distinct event durations and in this way  repro-\nduce more accurately the event durations in the pat tern \n(Figure 9, top). As in the previous hypothesis, the  Short \nbeat is aligned with event 2. The longer fourth eve nt in this \nhypothesis spans two beats. \nIn the fast and preferred tempi, the length of even t 4 is \nroughly double of the 2nd event and therefore we hypothe-\nsize that it spans two Short beats. Consequently, N I grid \nconsists of 3 Short beats and 2 Long beats and ther efore it \nis derived from a 5|2 polyrhythm. In the slow tempo , the \ndifference between the duration of event 4 and the rest of \nthe events is smaller. Our hypothesis is that this longer \nevent spans two beats but not of equal durations. S ince the \nfirst 3 events have similar durations, we hypothesi ze that a \nNI grid based on a 5|4 polyrhythm can reproduce thi s pat-\ntern, with the 3 Long beats aligned to the first 3 events. \nFinally, as previously, S is chosen to minimize the differ-\nence between the theoretical and observed mean dura tions. \nThe results are summarized in Table 3. \nThe 5-beat formative pulse hypothesis reproduces mo re \naccurately the observed Samba pattern. The differen ces \nbetween the theoretical and observed durations are below \nthe respective standard deviations, except for even t 2 and \n3 at the preferred tempo. This is indicative of the  inability \nof the 5|2 model to capture the subtle difference b etween \nthe two Medium events.  \nIntroducing a subdivision to the Formative and Targ et \npulses can address the above shortcoming of the mod el. A  \n \nFigure 8 : Potential NI grids with a 4-beat formative pulse \nfor the Samba patterns of Table 1. A hypothetical a lign-\nment between the target (T) and formative (F) pulse s is \nshown. The NI grids are specified on the left. The four \nevents from Table 1 and the corresponding mean dura tions \nare indicated by grey circles and the integer numbe rs at the \ntop. \nTempo  \nNI grid Event number \n1 2 3 4 \nFast  \nNI(4|3, 0.39) 128.7 \n(7.7) 69 \n(0.0) 128.7 \n(16.7) 128.7 \n(24.33 ) \nPreferred \nNI(4|3, 0.27) 165.0 \n(8.0) 110.0 \n(0.0) 165.0 \n(23.0) 165.0 \n(31.0) \nSlow \nNI(4|3, 0.09) 224.7 \n(12.7) 198.0 \n(0.0) 224.7 \n(18.7) 224.7 \n(31.3) \nTable 2 : Theoretical durations in ms for the four samba \nevents based on the NI grids of Figure 8. In parent hesis, \nthe differences between the predicted durations and  the ob-\nserved mean durations for the respective events are  shown \nfor comparison with the standard deviations reporte d in \nTable 1. Difference greater than the respective sta ndard de-\nviations are shown in bold.  \n \n \n \nFigure 9 : Potential NI grids with a 5-beat formative pulse \n(specified on the left) for the patterns in Table 1  (indicated \nhere with grey circles) for the three different tem pi. Two \nalternative NI grids are shown for the preferred an d slow \ntempo duration patterns. \n \nProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n150  \n \n10|4 NI grid can reproduce the differences between the du-\nrations of event 1 and 3 (Figure 9 and Table 3,  al ternative \nNI grids). The period of the 10-beat Formative puls e at the \nthis tempo is 61ms, which is below the perceptual t hresh-\nold mentioned in section 2.1 for metrical subdivisi ons. \nNevertheless, it may still be a plausible hypothesi s consid-\nering the shortest event duration in the pattern at  hand is \n69ms. A similar hypothesis for the fast tempo would  result \nin a period of 46ms for the Formative pulse, which was \nconsidered too fast within this context and was omm ited. \nAt the slow tempo, the alignment of the 10-beat For mative \npulse is identical to the 5-beat one and thus offer s no ad-\nvantage.  \n4.3 Tempo dependence \nThe above hypotheses provide alternative explanatio ns to \nthose given in [10] for the tempo dependence of the  basic \nsamba pattern.  \nIn the 4-beat formative hypothesis, we reproduce th e \npatterns by gradually changing the shift S, from an almost \npurely binary pattern at slow tempo to a mixed char acter \npattern at faster tempi. As the tempo becomes faste r, the 4-\nbeat pulse approaches the lower threshold for a met rical \nsubdivision and events are pulled from their format ive po-\nsitions (period of 114ms) towards the ternary subdi vision \n(period of 151ms), which is still significantly abo ve the \nthreshold. \nIn the 5-beat formative hypothesis, the tempo depen d-\nence is explained by the introduction of a subdivis ion in \nthe target pulse at the slow tempo and change to th e poly-\nrhythmic structure from 5|2 to 5|4. At the preferre d tempo, \nthe small value of S indicates that events are mainly at-\ntracted to the faster 5-beat pulse (period of 121ms ) and to \na lesser extend to the slower binary subdivision (p eriod of \n303ms). As the tempo increases, S moves towards the  bi-\nnary pulse, possibly due to the 5-beat pulse crossi ng the \n100ms threshold (91ms period). At the slow tempo, t he bi-\nnary pulse is subdivided, resulting in pulses with moderate periods (174ms for the 5-beat pulse and 218ms for t he 4-\nbeat pulse) and a more mixed character pattern. \n5. CONCLUSION \nIn this paper, we formalize a novel model for non-i sochro-\nnous and micro-timing rhythmic patterns that depart s from \ntheoretical and cognitive models that emphasize the  hier-\narchical grouping of isochronous pulses prevalent i n West-\nern classical music theories. Instead, our model is  rooted \nin non-hierarchical polyrhythmic relationships, suc h as \nthose found in African polyphony. By incorporating poly-\nrhythmic structures consisting of two pulses, our a pproach \nintegrates both the structural/macro level and the expres-\nsive/micro level of musical rhythms, which are trad ition-\nally treated as separate. The resulting construct o f non-\nisochronous (NI) grids unifies these levels within a novel \nframework. Non-isochronous groupings such as the Eu -\nclidean rhythms are then a special case of the mode l. \nWhile, NI grids can model systematic timing deviati ons \nfrom isochrony, not all deviations from isochrony c an be \naccounted for by NI grids, and expressive timing ma y in-\ntroduce micro-timing deviations to NI grids. \nOur model is a music-theoretical one and is not int ended \nto represent cognitive processes directly. However,  some \nof its predictions may be relevant to music cogniti on. For \nexample, it has previously been argued that only tw o beat \nclasses, Long and Short, are perceptually relevant and that \nMedium duration events must be understood as expres sive \nvariants of these two beat classes [48]. A subseque nt study \nshowed that this is indeed the case in Mali Drum En semble \nmusic [49]. Our model makes similar predictions abo ut the \nexistence of only two beat classes, albeit for diff erent rea-\nsons and with different implications for the observ ed pat-\nterns. To assess the perceptual relevance of these predic-\ntions, further analysis of musical performances and  behav-\nioral experiments are needed. \nThe potential of non-isochronous grids in music ana ly-\nsis was demonstrated in the example of Brazilian Sa mba. \nWe developed two concrete hypotheses for the basic \nSamba pattern reported in the literature [10], whic h model \nthe most salient features of the measured event dur ations \nand offer alternative explanations and interpretati ons for \ntheir tempo dependence. Polyrhythmic interpretation s of \nthe non-isochronous patterns observed in Samba perf or-\nmances have been hypothesized before, for example i n [9]. \nHowever, such hypotheses are typically explored in a phe-\nnomenological and abstract, conceptual form. Our mo del \nprovides the basis for a formalized method of deter mining \nthe details of the polyrhythmic and micro-timing ch aracter \nof the observed durational patterns. \nA future study will further test our preliminary hy poth-\neses and compare them with other accounts of samba pat-\nterns. For example, we will investigate the possibi lity that \nsome of the variation in measured durations may be due to \na dynamic shift S that changes from one repetition of the \npattern to the next. In addition, we will explore t he devel-\nopment and evaluation of automated methods for disc ov-\nering NI grids that can account for the durational patterns \nobserved in music from various genres and music tra di-\ntions.  \nTempo  \nNI grid Event number \n1 2 3 4 \nFast  \nNI(5|2, 0.18) 115.7 \n(5.3) 74.5 \n(5.5) 115.7 \n(3.7) 149.0 \n(3.9) \nPreferred \nNI(5|2, 0.16) 150.8 \n(6.2) 101.2 \n(8.8) 150.8 \n(8.8) 202.3 \n(6.3) \nSlow \nNI(5|4, 0.71) 205.3 \n(6.7) 205.3 \n(7.3) 205.3 \n(0.7) 256.0 \n(0.0) \nAlternative NI grids \nPreferred \nNI(10|4, 0.55) 164.9 \n(7.9) 110.3 \n(0.3) 137.6 \n(4.4) 192.2 \n(3.8) \nSlow \nNI(10|4, 0.71) 205.3 \n(6.7) 205.3 \n(7.3) 205.3 \n(0.7) 256.0 \n(0.0) \nTable 3 : Theoretical durations in ms for the four samba \nevents based on the NI grids of Figure 9. In parent hesis, \nthe differences between the theoretical durations a nd the \nobserved mean durations for the respective events a re \nshown. Differences greater than the respective stan dard \ndeviations are shown in bold.  Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n151  \n \n6. REFERENCES \n[1] J. London, Hearing in Time: Psychological Aspects \nof Musical Meter . New York: Oxford University \nPress, 2012. doi: 10.1093/ac-\nprof:oso/9780199744374.001.0001. \n[2] H. Honing, ‘Structure and Interpretation of Rhy thm \nin Music’, in The Psychology of Music , D. Deutsch, \nEd., 3rd ed.Academic Press, Elsevier, 2012, pp. \n367–404. \n[3] E. W. Large and M. R. Jones, ‘The dynamics of a t-\ntending: How people track time-varying events.’, \nPsychological Review , vol. 106, no. 1, pp. 119–159, \n1999, doi: 10.1037//0033-295X.106.1.119. \n[4] M. R. Jones, ‘Musical time’, in The oxford hand-\nbook of music psychology , S. Hallam, I. Cross, and \nM. Thaut, Eds., New York: Oxford University \nPress, 2008, pp. 81–92. doi: 10.1093/ox-\nfordhb/9780199298457.013.0008. \n[5] E. W. Large et al. , ‘Dynamic models for musical \nrhythm perception and coordination’, Front. Com-\nput. Neurosci. , vol. 17, p. 1151895, May 2023, doi: \n10.3389/fncom.2023.1151895. \n[6] R. Parncutt, ‘A perceptual model of pulse salie nce \nand metrical accent in musical rhythms’, Music Per-\nception , vol. 11, no. 4, pp. 409–464, 1994. \n[7] N. Jacoby and J. H. McDermott, ‘Integer Ratio P ri-\nors on Musical Rhythm Revealed Cross-culturally \nby Iterated Reproduction’, Current Biology , vol. 27, \nno. 3, pp. 359–370, 2017, doi: \n10.1016/j.cub.2016.12.031. \n[8] A. Gabrielsson, ‘Interplay between Analysis and  \nSynthesis in Studies of Music Performance and Mu-\nsic Experience’, Music Perception , vol. 3, no. 1, pp. \n59–86, Oct. 1985, doi: 10.2307/40285322. \n[9] L. Naveda, F. Gouyon, C. Guedes, and M. Leman, \n‘Microtiming Patterns and Interactions with Musical  \nProperties in Samba Music’, Journal of New Music \nResearch , vol. 40, no. 3, pp. 225–238, Sep. 2011, \ndoi: 10.1080/09298215.2011.603833. \n[10] M. R. Haugen and A. Danielsen, ‘Effect of temp o on \nrelative note durations in a performed samba \ngroove’, Journal of New Music Research , vol. 49, \nno. 4, pp. 349–361, Aug. 2020, doi: \n10.1080/09298215.2020.1767655. \n[11] G. Guillot, ‘Multi-level Anisochrony in Afro-B ra-\nzilian music’, GMTH Proceedings , pp. 406–421, \n2022, doi: 10.31751/p.200. \n[12] R. Polak, ‘Rhythmic Feel as Meter : Non-Isochr o-\nnous Beat Subdivision in Jembe Music from Mali’, \nMusic Theory Online , vol. 16, no. 4, pp. 1–26, 2010. \n[13] M. R. Haugen, ‘Investigating Musical Meter as \nShape: Two Case Studies of Brazilian Samba and \nNorwegian Telespringar’, in Proceedings of the \n25th Anniversary Conference of the European Soci-\nety for the Cognitive Sciences of Music , E. Van \nDyck, Ed., Ghent, Belgium, 2017, pp. 67–74. \n[14] E. F. Clarke, ‘Levels of structure in the orga nization \nof musical time’, Contemporary Music Review , vol. \n2, no. 1, pp. 211–238, Jan. 1987, doi: \n10.1080/07494468708567059. [15] M. E. P. Davies and S. Bock, ‘Evaluating the E val-\nuation Measures for Beat Tracking’, presented at th e \nInternational Society for Music Information Re-\ntrieval Conference, 2014. \n[16] M. J. Velasco and E. W. Large, ‘Pulse Detectio n in \nSyncopated Rhythms Using Neural Oscillators’, in \nProceedings of the 12th International Society for \nMusic Information Retrieval Conference , A. Klapuri \nand C. Leider, Eds., Miami, Florida, USA: Univer-\nsity of Miami, 2011. \n[17] E. W. Large, J. A. Herrera, and M. J. Velasco,  ‘Neu-\nral Networks for Beat Perception in Musical \nRhythm’, Front. Syst. Neurosci. , vol. 9, no. Novem-\nber, Nov. 2015, doi: 10.3389/fnsys.2015.00159. \n[18] F. Lerdahl and R. Jackendoff, A Generative Theory \nof Tonal Music . Cambridge, Massachusetts: The \nMIT Press, 1983. \n[19] M. Yeston, The Stratification of Musical Rhythm . \nNew Haven, CT: Yale University Press, 1976. \n[20] H. Krebs, Metrical dissonance in the music of Rob-\nert Shumann . New York, Oxford: Oxford University \nPress, 1999. \n[21] V. K. Agawu, Representing African music: post-\ncolonial notes, queries, positions . New York: \nRoutledge, 2003. \n[22] S. Arom, African polyphony and polyrhythm . Cam-\nbridge University Press, 1991. doi: \n10.1017/CBO9780511518317. \n[23] C. D. Stover, ‘A theory of flexible rhythmic s paces \nfor diasporic African music’, PhD, University of \nWashington, 2009. doi: /10.7560/LAMR37202. \n[24] A. Danielsen, Presence and Pleasure: The Funk \nGrooves of James Brown and Parliament . Mid-\ndletown, CT: Wesleyan University Press, 2006. \n[25] G. Sioros, G. Madison, D. Cocharro, A. Daniels en, \nand F. Gouyon, ‘Syncopation and Groove in Poly-\nphonic Music: Patterns Matter’, Music Perception , \nvol. 39, no. 5, pp. 503–531, Jun. 2022, doi: \n10.1525/mp.2022.39.5.503. \n[26] R. Polak, J. London, and N. Jacoby, ‘Both isoc hro-\nnous and non-isochronous metrical subdivision af-\nford precise and stable ensemble entrainment: A \ncorpus study of malian jembe drumming’, Frontiers \nin Neuroscience , vol. 10, no. JUN, pp. 1–11, 2016, \ndoi: 10.3389/fnins.2016.00285. \n[27] G. Sioros and C. Guedes, ‘Syncopation as Trans for-\nmation’, in Sound, Music and Motion , M. Aramaki, \nO. Derrien, R. Kronland-Martinet, and S. Ystad, \nEds., in Lecture Notes in Computer Science, vol. \n8905. Springer International Publishing, 2014, pp. \n635–658. doi: 10.1007/978-3-319-12976-1. \n[28] C. Barlow and H. Lohner, ‘Two essays on theory ’, \nComputer Music Journal , vol. 11, no. 1, pp. 44–60, \n1987. \n[29] C. Barlow, ‘Corrections for Clarence Barlow’ s  Ar-\nticle: Two Essays on Theory’, Computer Music \nJournal , vol. 11, no. 4, p. 10, 1987. \n[30] G. Sioros, M. E. P. Davies, and C. Guedes, ‘A gen-\nerative model for the characterization of musical \nrhythms’, Journal of New Music Research , vol. 47, \nno. 2, pp. 114–128, Mar. 2018, doi: \n10.1080/09298215.2017.1409769. Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n152  \n \n[31] M. Rohrmeier, ‘Towards a formalization of musi cal \nrhythm’, in Proceedings of the 21st International \nSociety for Music Information Retrieval Confer-\nence, Montréal, Canada, 2020, pp. 621–629. \n[32] E. W. Large, ‘Neurodynamics of Music’, in Music \nPerception , 2010, pp. 201–231. doi: 10.1007/978-1-\n4419-6114-3. \n[33] S. Koelsch, P. Vuust, and K. Friston, ‘Predict ive \nProcesses and the Peculiar Case of Music’, Trends \nin Cognitive Sciences , vol. 23, no. 1, pp. 63–77, \n2019, doi: 10.1016/j.tics.2018.10.006. \n[34] D. Huron, Sweet anticipation: music and the psy-\nchology of expectation . Cambridge, Massachusetts / \nLondon, England: The MIT Press, 2006. \n[35] B. H. Repp, ‘Rate Limits of Sensorimotor Synch ro-\nnization’, Advances in Cognitive Psychology , vol. 2, \nno. 2, pp. 163–181, Jan. 2006, doi: 10.2478/v10053-\n008-0053-9. \n[36] E. F. Clarke, ‘Categorical Rhythm Perception: an \nEcological Perspective’, in Action and Perception in \nRhythm and Music , A. Gabrielsson, Ed., Stockholm: \nRoyal Swedish Academy of Music, 1987, pp. 19–\n33. \n[37] P. Desain and H. Honing, ‘The Quantization of Mu-\nsical Time: A Connectionist Approach’, Computer \nMusic Journal , vol. 13, no. 3, p. 56, Jan. 1989, doi: \n10.2307/3680012. \n[38] P. Desain and H. Honing, ‘The formation of rhy th-\nmic categories and metric priming’, Perception , vol. \n32, no. 3, pp. 341–365, 2003, doi: 10.1068/p3370. \n[39] M. L. A. Jongsma, P. Desain, and H. Honing, \n‘Rhythmic context influences the auditory evoked \npotentials of musicians and nonmusicians’, Biologi-\ncal Psychology , vol. 66, no. 2, pp. 129–152, Apr. \n2004, doi: 10.1016/j.biopsycho.2003.10.002. \n[40] E. W. Large, ‘Rhythm Categorization in Context ’, \nin Large, Edward W. ‘Rhythm categorization in con-\ntext.’ Proceedings of the International Conference \non Music Perception and Cognition , Keele, UK, \n2000. \n[41] J. Clough and J. Douthett, ‘Maximally Even Set s’, \nJournal of Music Theory , vol. 35, no. 1/2, pp. 93–\n173, 1991, doi: 10.2307/843811. \n[42] R. Cohn, ‘A Platonic Model of Funky Rhythms’, \nJournal of the Society for Music Theory , vol. 22, no. \n2, 2016, doi: 10.30535/mto.22.2.1. \n[43] G. T. Toussaint, ‘The Euclidean algorithm gene rates \ntraditional musical rhythms’, in Proceedings of \nBRIDGES: Mathematical Connections in Art, Music \nand Science , Banff, Alberta,, Canada, 2005, pp. 47–\n56. \n[44] G. T. Toussaint, The Geometry of Musical Rhythm: \nWhat Makes a ‘Good’ Rhythm Good?  Chapman and \nHall/CRC, 2013. \n[45] A. Danielsen, ‘The Sound of Crossover: Micro-\nrhythm and Sonic Pleasure in Michael Jackson’s \n“Don’t Stop 'Til You Get Enough”’, Popular Music \nand Society , vol. 35, no. 2, pp. 151–168, 2012, doi: \n10.1080/03007766.2011.616298. \n[46] D. Moelants, ‘The Performance of Notes Inégale s: \nThe Influence of Tempo, Musical Structure, and In-\ndividual Performance Style on Expressive Timing’, Music Perception , vol. 28, no. 5, pp. 449–460, Jun. \n2011, doi: 10.1525/mp.2011.28.5.449. \n[47] K. Hellmer and G. Madison, ‘Quantifying Micro-\ntiming Patterning and Variability in Drum Kit Re-\ncordings: A Method and Some Data’, Music Percep-\ntion: An Interdisciplinary Journal , vol. 33, no. 2, pp. \n147–162, Dec. 2015, doi: \n10.1525/mp.2015.33.2.147. \n[48] J. London, ‘Commentary’, Music Theory Online , \nvol. 16, no. 2, 2010. \n[49] H. Neuhoff, R. Polak, and T. Fischinger, ‘Perc eption \nand Evaluation of Timing Patterns in Drum Ensem-\nble Music from Mali’, Music Perception , vol. 34, \nno. 4, pp. 438–451, Apr. 2017, doi: \n10.1525/mp.2017.34.4.438. \n Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n153"
    },
    {
        "title": "A Review of Validity and Its Relationship to Music Information Research.",
        "author": [
            "Bob L. T. Sturm",
            "Arthur Flexer"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265219",
        "url": "https://doi.org/10.5281/zenodo.10265219",
        "ee": "https://zenodo.org/records/10265219/files/000004.pdf",
        "abstract": "Validity is the truth of an inference made from evidence and is a central concern in scientific work. Given the maturity of the domain of music information research (MIR), validity in our opinion should be discussed and considered much more than it has been so far. Puzzling MIR phenomena like adversarial attacks, horses, and performance glass ceilings become less mysterious through the lens of validity. In this paper, we review the subject of validity as presented in a key reference of causal inference: Shadish et al., Experimental and Quasi-experimental Designs for Generalised Causal Inference [1]. We discuss the four types of validity and threats to each one. We consider them in relationship to MIR experiments grounded with a practical demonstration using a typical MIR experiment.",
        "zenodo_id": 10265219,
        "dblp_key": "conf/ismir/SturmF23",
        "keywords": [
            "Validity",
            "truth of an inference",
            "scientific work",
            "MIR phenomena",
            "adversarial attacks",
            "horses",
            "performance glass ceilings",
            "causal inference",
            "four types of validity",
            "threats to each one"
        ],
        "content": "A REVIEW OF V ALIDITY AND ITS RELATIONSHIP TO\nMUSIC INFORMATION RESEARCH\nBob L. T. Sturm\nDivision of Speech, Music and Hearing\nKTH Stockholm, Sweden\nbobs@kth.seArthur Flexer\nInstitute of Computational Perception\nJohannes Kepler University Linz, Austria\narthur.flexer@jku.at\nABSTRACT\nValidity is the truth of an inference made from evidence and\nis a central concern in scientiﬁc work. Given the maturity of\nthe domain of music information research (MIR), validity\nin our opinion should be discussed and considered much\nmore than it has been so far. Puzzling MIR phenomena like\nadversarial attacks, horses, and performance glass ceilings\nbecome less mysterious through the lens of validity. In this\npaper, we review the subject of validity as presented in a key\nreference of causal inference: Shadish et al., Experimental\nand Quasi-experimental Designs for Generalised Causal\nInference [1]. We discuss the four types of validity and\nthreats to each one. We consider them in relationship to\nMIR experiments grounded with a practical demonstration\nusing a typical MIR experiment.\n1. INTRODUCTION\nThe multi-disciplinary ﬁeld of Music Information Research\n(MIR) is focused on making music and information about\nmusic accessible to a variety of users. This ranges from sys-\ntems for search and retrieval, to recommendation, and even\nto more creative applications like music generation. The\neffectiveness and reliability of MIR systems are of prime\nimportance to the MIR researcher, not to mention other\nstakeholders. The researcher thus performs experiments\nto compare approaches for modeling and retrieving music\ndata. A principal focus is on users, but the cost of perform-\ning experiments with users is high, and the replicability of\nsuch studies is difﬁcult. This has motivated the Cranﬁeld\nParadigm [2]: computer-based experiments where “test col-\nlections” serve as proxies for human users. While such an\napproach is inexpensive and replicable, its relevance and\nreliability for MIR, and information retrieval in general,\nhave been questioned [3, 4].\nUnder the Cranﬁeld Paradigm, state-of-the-art MIR sys-\ntems perform exceptionally well in reproducing the ground\ntruth of some datasets, e.g., inferring rhythm, genre or emo-\ntion from audio data. This leads to conclusions that the\n© Bob L. T. Sturm, Arthur Flexer. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribution:\nBob L. T. Sturm, Arthur Flexer, “A Review of Validity and its Relationship\nto Music Information Research”, in Proc. of the 24th Int. Society for\nMusic Information Retrieval Conf., Milan, Italy, 2023.systems are actually learning to perform the task believed\nnecessary to recover the ground truth from audio data. How-\never, slight and irrelevant transformations of the audio, e.g.,\n“adversarial attacks”, can suddenly render these systems\nineffectual [5 –9]. Such attacks can reveal what an MIR\nsystem is relying on for its success. In one case, a “genre\nrecognition” system relies on infrasonic signatures that are\nimperceptible and irrelevant for human listeners [8]. In an-\nother, a “rhythm recognition” system is recognising tempo\ninstead of rhythm, a confounding originating from the data\ncollection [6]. Systems relying on such “tricks” have been\ncalled “horses” [5]. A related topic in MIR is “glass ceil-\nings” [10, 11], i.e., that an observed barrier to improving\nsystem performance to perfect or human level is claimed as\ncoming from psychophysical and cultural factors of music\nmissing from features extracted from audio recordings [12].\nIn order to better understand the problems described\nabove it is necessary to consider what lies at the heart of any\nexperiment: the relationship between conclusions drawn\nfrom its results and their validity , or “truth value” [1]. Ide-\nally, an experiment will be carefully designed and imple-\nmented to answer a well-deﬁned hypothesis. Its compo-\nnents – units, treatments, design, observations, and settings\n– should be carefully operationalised (translated from theory\ninto practice) to maximize quality and minimize cost (e.g.,\nmoney and time). This is the purview of the discipline De-\nsign of Experiments : how can one get the strongest evidence\nfor the least cost?\nDespite a small chorus of calls to improve validity of\nconclusions in MIR, e.g., [4 –6, 13 –19], there has yet to be\npublished a systematic and critical engagement of what va-\nlidity means in the context of MIR, and how to consider it\nwhen designing, implementing and analyzing experiments.\nIn this paper, we focus on the four principal types of validity\nin Shadish et al. [1], an authoritative resource about validity\nin causal inference and experimental science. Other typolo-\ngies exist, e.g., [20], but we use that of Shadish et al. [1]\nbecause it is an established point of reference, and has al-\nready been mentioned in the context of MIR, e.g., in [4].\nWe review the four types of validity and present actionable\nquestions that can help MIR researchers to scrutinize the\nconclusions they draw from their experiments. We ground\nour general discussion of validity in this paper by a practical\ndemonstration,1which presents a typical MIR experiment\n1See supplementary material here: https://github.com/\nboblsturm/mirvaliditytutorial47Model Accuracy Precision Recall f1-score\nLDA 0.714 0.711 0.711 0.703\nQDA 0.719 0.715 0.723 0.717\n1NN 0.662 0.644 0.635 0.638\n3NN 0.681 0.673 0.651 0.656\n5NN 0.719 0.699 0.687 0.689\n7NN 0.695 0.669 0.656 0.659\n9NN 0.700 0.681 0.664 0.668\nunif0.12±0.020.13±0.030.12±0.020.12±0.02\nfreq0.13±0.020.13±0.030.13±0.020.13±0.02\nmaj 0.16 0.02 0.12 0.03\nTable 1 . Accuracy, and macro-averaged precision, recall\nand f1-score observed for several models in a testing parti-\ntion of BALLROOM [21]. The performance of two mod-\nels selecting labels randomly (with standard deviation) are\nshown in the rows labeled: unifsamples labels uniformly;\nfreq samples labels according to training data label fre-\nquency. The last row maj shows the performance of a\nmodel choosing the label most frequent in the training data.\nthat exempliﬁes a considerable amount of MIR research:\nmusic classiﬁcation using machine learning (ML) and a\nbenchmark dataset. We use the BALLROOM dataset [21],\nwhich has appeared in dozens of studies seeking to build\nMIR systems sensitive to rhythm [6]. We partition the\ndataset into training and testing sets, extract features and\ntrain ML models, then label test set recordings and count\ncoincident ground truth labels, and ﬁnally compute ﬁgures\nof merit for the different ML models. Table 1 presents the\nresults from which we wish to draw valid conclusions.\nA less abridged version of this paper [22] integrates the\nsupplementary material in more detail. We hope that these\nmaterials will help MIR researchers to design, implement\nand analyze experiments in MIR and draw valid conclu-\nsions, but also convince them that the language of validity\nis reason. Creative thinking is necessary when examining\nthe truth value of any conclusion drawn from an experiment.\n2. COMPONENTS OF EXPERIMENTS\nBefore discussing the validity of conclusions drawn from\nan experiment, we must identify its components: units,\ntreatments, design, observations, and settings. Treatments\nare the things applied to units in order to cause an effect\n(or not in the case of a control ),units are the things that are\ntreated, and observations are what is measured on a unit.\nThedesign speciﬁes which treatment is applied to which\nunit, and settings involve time, place, and condition. To\nmake this more concrete, consider a medical experiment in\nwhich the effect of a treatment on blood pressure is being\nstudied. A number of people are sampled from a population,\nsome of whom will receive the treatment while the others\nreceive a placebo (control). The design describes which\npeople get the treatment, and which do not. The observation\nis the blood pressure of a person after one month. The\nsetting can include particulars of the population (rural or\nurban), place of treatment (hospital or home), and so on.\nThe experimentalist contrasts blood pressure observations\nacross groups to conclude, e.g., the effect of the treatment\n(causes a decrease in blood pressure).Our typical MIR experiment measures the effectiveness\nof different ML models in predicting the labels of a test\nrecording dataset. There are two ways to see its compo-\nnents. We can see the treatments as the ten models and the\nunits as replicates of the entire testing dataset, or we can\nsee the entire testing dataset as the one treatment and the\nunits as the ten models. Since Table 1 reports ﬁgures of\nmerit (observations) of each model on the entire test dataset,\nthe latter interpretation motivates conclusions about the ef-\nfectiveness of particular models. In this case, the design is\nsimple: each unit (ML model) is given the same treatment\n(dataset). The setting involves the dataset partitioning, the\nextracted features, random seeds, software libraries, etc.\n3. STATISTICAL CONCLUSION V ALIDITY\nStatistical conclusion validity is “the validity of inferences\nabout covariation between two variables” [1]. This includes\nconcluding that a covariation exists, and perhaps its strength\nas well. This is the level at which one is concerned with\nstatistical signiﬁcance , i.e., that an observed covariation\nbetween treatment and effect is not likely to arise by chance.\nAs a concrete example, an experiment measuring the ef-\nfects of two different medicines on lowering blood pressure\nseeks to determine which of the medicines has the greatest\neffect, if at all. The statistical conclusion validity of a con-\nclusion resulting from this experiment relies on its power,\nbut can be threatened in other ways. Shadish et al. [1] (p.\n45) includes a table of nine different threats to statistical\nconclusion validity. Four threats relevant to computer-based\nexperiments are: violated assumptions about the statistics\nunderlying the observations (and the use of the wrong sta-\ntistical test, a type III error [23]); a sample size too small to\nreliably detect covariation (lack of power); the purposeful\nsearch for signiﬁcant results by trying multiple analyses and\ndata selections (“p-hacking” [24]); and increased variance\nin observations due to the heterogeneity of units.\nAre my results statistically signiﬁcant? Null hypothesis\nstatistical testing (NHST) quantiﬁes whether the observed\neffects of the treatments on the responses arise by mere\nchance, as well as the direction of effect and its size. This\nanswers the question: are the results statistically signiﬁ-\ncant? Fundamentals about statistical testing in MIR have\nalready been discussed [25], also for Artiﬁcial Intelligence\nin general [26], and for ML [27]. One must take care in\nselecting a statistical test to use; each one makes strong as-\nsumptions that could be violated. NHST is most straightfor-\nwardly applicable to completely randomized experimental\ndesigns [28], thereby reducing the possibility of structure in\nunits and treatments interfering with the responses (which\nresults in confounding). Most MIR experiments cannot\nuse complete randomisation because the target population\nfrom which samples come is unclear (what is a random\nsample of “sad” music, with the term “sad” being quite\nill-deﬁned?), and so the kinds of conclusions that can be\nmade with NHST in MIR are limited.2\n2Experimental designs that cannot be completely randomised are called\nquasi-experimental designs , another major topic of Shadish et al. [1].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n48Is the observed statistical signiﬁcance relevant for a\nuser? In MIR, even if one ﬁnds statistical signiﬁcance, this\nmay not generalise to a perceivable difference for actual\nusers interacting with the “improved” MIR system. As an\nexample from MIR, a crowd-sourced user evaluation [29]\ndemonstrates that there is an upper bound of user satisfac-\ntion with music recommendation systems of about 80%,\nsince this was the highest percentage of users agreeing that\ntwo systems “are equally good.” In addition, for the MIREX\ntask of Audio Music Similarity and Retrieval it has been\ndemonstrated [29] that statistically signiﬁcant differences\nbetween algorithms can be so small that they make no prac-\ntical difference for users.\nLet us now consider the typical MIR experiment and\nreason about what conclusions we can draw from it that\nhave statistical conclusion validity. Table 1 clearly shows\nthat each response of model to the dataset is greater than\nthe random approaches unif,freqandmaj. How likely is it\nthat any of the responses of models is due to chance, i.e.,\nthat any of the models is actually no better than one of the\nrandom approaches? Since we have the empirical distribu-\ntions for unifandfreq, we can estimate the probability of\neither of them resulting in, e.g., a macro-average recall at\nleast as large as 0.6:p < e−200.3Hence, a valid statisti-\ncal conclusion is that we observe a signiﬁcant covariation\nbetween the use of a machine learning model with these\nparticular features and the responses measured on a speciﬁc\npartition of BALLROOM.\nOne might consider statistical conclusions relating to the\ntype of ML, i.e., Gaussian modeling (LDA and QDA) vs.\nnearest neighbour modeling (KNN), or LDA vs. QDA. If\nwe conclude from Table 1 that Gaussian modeling performs\nbetter than nearest neighbour modeling with these features\non 70/30 partitions of BALLROOM, we would be wrong.\nThis is a “type I error”, which is concluding there to be\na signiﬁcant difference when in fact there is none. When\nwe perform this experiment 1000 times with random 70/30\npartitions we observe that the difference between the best\nresponse of a Gaussian model and the best response of\na nearest neighbour model is distributed Gaussian, and\nthat the probability of observing zero difference or less is\np >0.41for any of the ﬁgures of merit.\nThe most general statistical conclusion we can make\nfrom Table 1 is that the responses we observe from ML\nmodels are highly inconsistent with the responses of choos-\ning randomly. Each ML model knows something about\nBALLROOM linking the features computed from a music\nrecording with its ground truth label. Because we do not\nknow the amount of variation in any response due to par-\ntitioning, we cannot make any valid statistical conclusion\nabout which type of ML model is the best for this particular\ndataset. In order to go further, we must run the experiment\nmultiple times to obtain distributions of the contrasts. Even\nthen, however, we cannot say anything about the cause\nof signiﬁcant differences yet. This is where the notion of\ninternal validity becomes relevant.\n3See the supplementary material for an explanation.4. INTERNAL V ALIDITY\nInternal validity is “the validity of inferences about whether\nthe observed covariation between two variables is causal”\n[1]. While statistical conclusion validity is concerned only\nwith the strength of covariation between treatment and re-\nsponses, internal validity is focused on the cause of a par-\nticular response to the treatment. Shadish et al. [1] (p. 55)\nincludes a table of nine different threats to causal conclu-\nsions. Several of these involve confounding , which is the\nconfusion of the treatment with other factors arising from\npoor operationalisation in an experiment. As a concrete\nexample, consider an experiment measuring the effects of\ntwo different medicines on lowering blood pressure, but\nwhere one medicine is given to young patients and the other\nis given to elderly patients. This experimental design con-\nfounds the two medicines and patient age, and so the effects\ncaused by the two factors cannot be disambiguated. Any\nconclusion from this experiment about the effects of the\nmedicines lacks internal validity.\nDoes my data collection introduce confounds? One’s\nmethodology for collecting data might unintentionally in-\ntroduce structure. For instance, it has been discussed that\nBALLROOM was assembled by downloading excerpts of\nmusic CDs sold at a website selling music for ballroom\ndance competitions [6]. Ballroom dance competitions are\nregulated by organisations, e.g., World DanceSport Feder-\nation (WDSF),4to ensure uniformity of events for com-\npetitors around the world. These organisations set strict\nrequirements of tempo of each dance such that high skill is\nrequired of the dancers. Hence, the labels of BALLROOM\ncan reﬂect any of the following: 1) the rhythm of the music;\n2) the type of dance performed to the music; 3) the strict\ntempo requirements of the dance in the context of competi-\ntion. As a result, good performance in BALLROOM can be\ndue to rhythm detection and/or tempo estimation. Tempo\nand rhythm are related musical characteristics, but they are\nnot the same thing [30].\nDoes my data partitioning introduce confounds? Dataset\npartitioning can also introduce confounds, e.g., “bleeding\nground truth.” An example is to ﬁrst segment recordings\ninto short (e.g., 40ms) time frames and then partition these\nframes into training and testing sets, thus spreading highly\ncorrelated features across these sets. In the context of\naudio-based genre classiﬁcation, the presence of songs from\nthe same artists or albums in both training and test data\nhas been shown to artiﬁcially inﬂate performance [31, 32].\nAudio-based genre classiﬁcation using very direct represen-\ntations of spectral content has been shown [33] to degrade\nmore when employing artist/album ﬁlters than classiﬁca-\ntion based on more abstract kind of features like rhythmic\ncontent (ﬂuctuation patterns). This insight that problems\nof data partitioning can affect MIR systems in quite differ-\nent ways and hence change performance rankings has been\nconﬁrmed in another meta-study [34].\nReturning to our typical MIR experiment, of interest is\nwhat it is in our trained ML models causing their response\nto be inconsistent with random selection. Knowing how\n4https://www.worlddancesport.org/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n49Gaussian models used in LDA and QDA are built – mean\nand covariance parameters are estimated from training data\n– an internally valid conclusion is that these models work\nwell in BALLROOM because likelihood distributions esti-\nmated from the training data also ﬁt the testing data well.\nAnother internally valid conclusion is that the high perfor-\nmances of these ML models in BALLROOM are caused\nby the features together with the expressivity of the models\ncapturing information related to the labels in BALLROOM.\nWith reference to the aims of MIR research, we want\nto conclude something more speciﬁc, e.g., our ML mod-\nels have learned to recognize the rhythms in BALLROOM.\nThis is certainly one explanation consistent with our ob-\nservations, but is it the only one? The internal validity of\nthis conclusion relies on a key assumption: inferring the\nlabels of BALLROOM can only be the result of learning\nto discriminate between and identify its rhythms. In other\nwords, we must assume that there is no other way to infer\nlabels in BALLROOM than by perceiving rhythm.\nSince we know tempo is highly correlated with rhythm\nin BALLROOM, we thus perform an experiment to test\nthe sensitivity of our trained ML models to tempo: we\nalter all test recordings by some amount of pitch-preserving\ntime dilation, and then measure the responses of the models\nto these new treatments. We see that the responses of all\nML models decay to being not signiﬁcantly different from\nrandom selection with dilations in the range of ±15%. We\nsee this intervention clearly reveals the extent to which the\nML models we test rely on the tempi in the test data.\nThe experimental design of the typical MIR experiment\ndoes not account for the structure present in the dataset;\nwe do not control for other ways of inferring the labels\nof BALLROOM, which are guaranteed to exist by its very\nconstruction. From Table 1 and our experimental design, we\nthus cannot be any more speciﬁc in our causal inference than\nthis: the responses of our ML models are caused by their\nhaving learned something about BALLROOM. This then\ncalls into question how comparing predictions with ground\ntruth in BALLROOM relates to the ability we might actually\nwant to measure, that is the recognition of rhythm. This is\nwhere the notion of construct validity becomes relevant.\n5. CONSTRUCT V ALIDITY\nConstruct validity is “the validity of inferences about the\nhigher order constructs that represent sampling particu-\nlars” [1]. This involves the relationship between what is\nmeant to be inferred by the experimentalist from an experi-\nment and what is actually measured, i.e., the operationalisa-\ntionof the experimentalist’s intention. For instance, directly\nmeasuring the blood pressure of a person involves an inva-\nsive procedure inserting a measuring device in their veins.\nBlood pressure can be measured less invasively but indi-\nrectly by externally applying known pressure to a vein and\nlistening for when blood ﬂow ceases. Knowledge about the\nincompressibility of liquids in closed systems makes the\nmeasurement of pressure in the balloon a relevant measure\nof blood pressure. Shadish et al. [1] (p. 73) includes a table\nof fourteen different threats to construct validity, but severalof these are irrelevant to computer-based experiments. The\nmain threat is a questionable relationship between what\nis being measured and what is intended to be measured.\nSelecting a measure by convenience but not relevance, sam-\npling from convenient populations, and a lack of deﬁnition\nof what is intended to be measured, are threats to construct\nvalidity. Construct validity involves more than just how\nsomething is measured; it also involves what is measured\nand in what settings.\nHow is classiﬁcation accuracy, or any ﬁgure of merit, in\na labeled music dataset related to X?Two examples in MIR\nare the use of “genre” classiﬁcation accuracy as an indirect\nmeasure of music similarity [11], or user satisfaction (see,\ne.g., [14] for a discussion). The relationship between these\nis very tenuous, especially so considering that accuracy\nitself is an unreliable measure of whether or not a system has\nlearned anything relevant to music [5, 15]. A key reference\nin this respect is that of Pfungst [35] describing a series\nof experiments in trying to reliably measure the arithmetic\nacumen of a horse that was only able to tap out answers.\nCounting the number of correct answers tapped out by the\nhorse, no matter how many questions are asked, is irrelevant\nwithout considering how each question is posed (the setting).\nThe key to Pfungst discovering the cause of the horse’s\napparent arithmetic acumen involved changing the setting:\nthe questions remained the same, and accuracy of correct\nresponse was measured, but how the questions were posed\nwas changed in order to control for different factors of the\nexperiment. The same is true for MIR.\nWhat is the “use case” of the system to be tested? To\ncounter threats to construct validity the MIR experimental-\nist must operationalise as much as possible the use case of\nthe system to be built and tested. One attempt to do so for\nmusic description [36] emphasises the need to deﬁne suc-\ncess criteria. The experimentalist must determine how their\nmethod of measurement relates to the success criteria, e.g.,\nrelating accuracy in genre classiﬁcation to the satisfaction\nof a speciﬁc type of user.\nHow can we test the construct validity of a conclusion?\nOne possibility is to assess the outcomes of different ex-\nperiments which are supposed to measure the same higher\norder constructs. An example in MIR is to study corre-\nlations of different genre classiﬁers when given identical\ninputs [18]. Low correlations between classiﬁers point to\nproblems of construct validity. A related topic is that of\nadversarial examples, which casts doubt on the conclusion\nthat the high accuracy of an MIR system in some dataset\nreﬂects its “perception” of the music in the waveform. Ad-\nversarial examples have ﬁrst been described in image anal-\nysis [37], where imperceptible perturbations of input data\nsigniﬁcantly degraded classiﬁcation accuracy. For music\ngenre classiﬁcation systems, imperceptible audio ﬁltering\ntransformations of music signals have been used [5] to both\ndeﬂate and inﬂate classiﬁcation accuracy to be no better\nthan chance level or perfect 100% . Following these so-\ncalled untargeted attacks which try to change a prediction to\nan arbitrary target, targeted attacks aiming at changing pre-\ndictions to speciﬁc classes have been explored. A targetedProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n50attack on genre recognition has been reported [7], where\nmagnitude spectral frames computed from audio are treated\nas images and attacked using approaches from image object\nrecognition. For music instrument classiﬁcation a targeted\nattack allowing to add perturbations directly to audio wave-\nforms instead of spectrograms has also been presented [9].\nThe attacks were able to reduce the accuracy close to a ran-\ndom baseline and produce misclassiﬁcations to any desired\ninstrument. The authors also artiﬁcially boosted playcounts\nvia an attack on a real-world music recommender, thereby\ndemonstrating that such attacks can be a security issue in\nMIR. Follow-up work presented lines of defence against\nsuch malicious attacks [38].\nReturning to our typical MIR experiment, we are in-\nterested in making construct inferences around the latent\nability of rhythm recognition we are supposedly measuring\nin our ML models. For instance, one construct inference\nis that our features measure relevant aspects of rhythm in\nrecorded music. In some sense, by their deﬁnition from ba-\nsic signal processing components, our features come from\ntemporal aspects that are certainly relevant to rhythm. Our\nfeatures are also reliant on acoustic information, and in\nparticular there being high-contrast differences in onsets\ncaptured by spectral ﬂux – hence limiting their relationship\nto rhythms played by particular kinds of instruments with\nsharp attacks. However, we have seen above that the fea-\ntures are also indicative of tempo, and that tempo is another\npath an ML model can use to infer the rhythm label. Hence\nwe are left to question the relationship of our features to the\nconcept we are trying to operationalise, i.e., rhythm.\nHaving a system label any partition of the BALLROOM\ndataset provides no reliable measure of a system’s ability\nto recognise rhythm without changing the setting to control\nfor other factors. It is not as simple as choosing a differ-\nent feature, measure, cross-validation method, or using a\nparticular statistical test. One must change the experiment\nitself such that rhythm recognition is what is actually being\nmeasured. This means that BALLROOM can still be use-\nful to measuring the rhythm recognition of an ML model.\nIndeed, in the previous section we used it to disprove the\ncausal claim that the good performance of the ML systems\nof Table 1 is caused by their ability to recognize rhythm.\nMight performance in BALLROOM also be an indication\nof performance in other datasets focused on rhythm? This\nis where the notion of external validity becomes relevant.\n6. EXTERNAL V ALIDITY\nExternal validity is “the validity of inferences about the\nextent to which a causal relationship holds over variations\nin experimental units, settings, treatment variables and mea-\nsurement variables” [1]. More generally, external validity\nis the truth of a generalised causal inference drawn from an\nexperiment. An example is inferring that medicine found\nto lower blood pressure in patients living in Germany will\nalso lower blood pressure in people living in Mexico – a\nconclusion that can lack validity due to differences in diet,\nliving and working conditions, and so on. Another example\nis that increasing the dose of the medicine will cause bloodpressure to lower further in the studied population. If a\ncausal inference we draw from an experiment lacks internal\nvalidity, then generalising that inference to include varia-\ntions not tested will not have external validity. Shadish et\nal. [1] (p. 87) includes a table of ﬁve different threats to ex-\nternal validity, which are in addition to the threats to internal\nvalidity. The main threat is that variation of the components\nof the experiment might destroy the causal inference that\nholds in the experiment. For instance, a medication may\nwork for the type of illness tested, but that type of illness\nmay not be generalisable to other closely related illnesses.\nDoes my model generalize to out-of-sample data? The\nstandard approach in evaluating MIR classiﬁcation systems\nis to use separate train and test sets in cross-validation\nexperiments to obtain seemingly unbiased estimates of per-\nformance. However, if such MIR systems are exposed to\nindependent out-of-sample data often severe loss of per-\nformance is observed. One example are experiments on\ngenre recognition where accuracy results do not hold when\nevaluated across different collections that are not part of the\ntraining sets [39, 40]. The results do not generalize to sup-\nposedly identical genre labels in different collections, which\nreﬂects a lack of external validity. Genre labels like ‘Rock’\nwill be used differently by different annotators working on\nthese collections – which is also a threat to construct va-\nlidity. Another example are how different audio encodings\naffect subsequent computation of descriptors and classiﬁca-\ntion results [41], or how in general differences in software\nimplementations diminish replicability [42].\nDo different raters agree on a ground truth? Human\nperception of music is highly subjective resulting in pos-\nsible low inter-rater agreement. Therefore only a certain\namount of agreement can be expected if several human\nsubjects are asked to rate the same song pairs according to\ntheir perceived similarity, depending on a number of sub-\njective factors [14, 43] like personal taste, listening history,\nfamiliarity with the music, current mood, etc. Concern-\ning annotation of music, it has been shown [44] that the\nperformance of humans classifying songs into 19 genres\nranges from modest 26% to71%. Audio-based grounding\nof everyday musical terms shows the same problematic re-\nsults [45]. It has even been argued [12] that no such thing\nas an immovable ‘ground’ exists in the context of music,\nbecause music itself is subjective, highly context-dependent\nand dynamic.\nThe lack of inter-rater agreement presents a problem of\nexternal validity because inferences from the experiment do\nnot generalize from users or annotators in the experiment to\nthe intended target population of arbitrary users/annotators.\nIt is also a problem of reliability, since different groups of\nusers or annotators with their differing subjective opinions\nwill impede repeatability of experimental results. This lack\nof inter-rater agreement presents an upper bound for MIR\napproaches, since it is not meaningful to have computational\nmodels going beyond the level of human agreement. Such\nupper bounds have been reported [14,43,46] for the MIREX\ntasks of ‘Audio Music Similarity and Retrieval’ (AMS) and\n‘Music Structural Segmentation’ (MSS). For AMS the upperProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n51Accuracy Precision Recall f1-score\nLDA 0.659 0.647 0.643 0.643\nQDA 0.682 0.678 0.672 0.673\n1NN 0.622 0.616 0.602 0.604\n3NN 0.636 0.629 0.610 0.613\n5NN 0.644 0.643 0.617 0.619\n7NN 0.647 0.646 0.619 0.621\n9NN 0.645 0.643 0.615 0.618\nunif0.12±0.010.13±0.010.12±0.010.12±0.01\nfreq0.13±0.010.13±0.010.12±0.010.12±0.01\nmaj 0.13 0.02 0.12 0.03\nTable 2 . As in Table 1, models trained in BALLROOM and\ntested in all of X-BALLROOM [51].\nbound has already been reached in 2009, while for MSS\nthe upper bound is within reach for at least some genres of\nmusic. Comparable results exist concerning music structure\nanalysis [47] and chord estimation [48, 49].\nDo raters agree with themselves at different points in\ntime? Going beyond the question of whether different an-\nnotators agree on a ground truth one can also access what\nthe level of agreement within one person is when faced\nwith identical annotation tasks at different points in time.\nA high intra-rater agreement would help to overcome the\nproblem of upper bounds in MIR systems since it would\nmake personalization of models meaningful, i.e. to have\nseparate models for individual persons. However, at least\nfor the task of general music similarity it has been shown\nthat intra-rater agreement is only slightly higher than inter-\nrater agreement [19], with the absolute level also depending\non music material and mood of raters at test time. An ap-\nproach to personalize chord labels for individual annotators\nvia deep learning was more successful [50].\nReturning to the typical MIR experiment, we cannot\nvalidly conclude that any of our models is recognizing\nrhythm in general because we do not know if they are rec-\nognizing rhythm in BALLROOM. Our dilation intervention\nexperiment in Sec. 4 reveals that all of the models lose their\nsupposed ability to recognize rhythm in BALLROOM, so\nthere is no reason to infer they will recognize rhythm else-\nwhere. One causal conclusion we might make is that our\nmodels perform well in BALLROOM because they have\nlearned something about BALLROOM – a curated set of\nrecordings downloaded from a speciﬁc website in 2004.\nMight they have learned something about other recordings\nfrom that same website, but collected many years later?\nThe extended BALLROOM dataset (X-BALLROOM)\n[51] consists of 3,484 audio recordings in the same eight\ndance styles or music rhythms as BALLROOM, but down-\nloaded from the same website over a decade later. This\ngives us a chance to test our conclusion. The ﬁgures of\nmerit measured from our models trained in BALLROOM\nbut applied to all of X-BALLROOM are shown in Table 2.\nWe still see signiﬁcant covariation between response and\nthe use of ML with our features. By and large, whatever\nconcepts our ML models have learned about BALLROOM\ncarry over to X-BALLROOM – but we still do not know\nwhether or not those concepts have to do with rhythm.7. CONCLUSION\nThis paper provides a review of the notion of validity based\non the typology given in Shadish et al. [1]. It brings together\nthe few sources in MIR that mention validity, and several\nsources that do not but are related. This paper does not aim\nto prescribe how to design and perform experiments such\nthat valid conclusions can be drawn from them. Instead, it\naims to bring within the realm of MIR what validity means,\nwhy it is important, and how it can be threatened. One thing\nto reiterate is that one does not talk about the “validity of an\nexperiment”. An experiment does not possess “truth value”.\nValidity is a property of a conclusion made given evidence\ncollected from an experiment. The components of an exper-\niment – units, treatments, design, observations, and setting\n– have major consequences for the validity of conclusions\ndrawn from it, whether it is statistical conclusion validity,\ninternal validity, construct validity, or external validity.\nIn MIR the predominant experimental methodology is\nthe Cranﬁeld Paradigm: train a model on a partition of a\ndataset and count the number of correct answers on an-\nother partition. This kind of experiment is inexpensive,\nand provides numbers that can be compared in ways that\nconvince peer reviewers that progress has been accom-\nplished [52]. Despite various appeals [14, 53] and beseech-\nings [4, 5, 15, 16, 19, 29, 43], such an experimental approach\nis still standard in the ﬁeld and its serious ﬂaws are ignored.\nAny conclusion from this experiment that is more general\nthan “the system has learned something about the dataset”\nlacks internal, construct and external validity. This does not\nmean that all such inferences are false, just that they cannot\nfollow from the experiment as designed and implemented.\nReproducing the ground truth of a dataset represents a be-\nginning and must be followed by a search for the causes of\nthe observed behavior.\nShadish et al. [1] provides an established starting point\nfor MIR, but there exist other types of validity. For instance,\nLund [20] revises the typology of [1] to address ambigui-\nties between causes and treatments, to better deﬁne aspects\nof settings, and to establish a hierarchical ordering of ﬁve\ntypes of validity: statistical conclusion, causal, construct,\ngeneralization and theoretical. Other kinds of validity in-\nclude ecological, convergent, and criterion [13], but these\nstill deal with the kind of conclusion one is drawing from\nevidence collected in some way.\nAs a ﬁnal note, a frustration when encountering Shadish\net al. [1] as an engineer is that of its 623 pages there are\nonly ﬁve pages with at least one equation on them. Instead,\nShadish et al. [1] describe experiments and how each type\nof validity manifests in the conclusions drawn, with speciﬁc\nthreats to the reasoning of those conclusions. Experiments,\nnot to mention experimentalists, are such complex assem-\nblages that expressing them in formal ways that appear to\npermit the computation of numbers that relate to each type\nof validity would probably have very limited applicability,\nand then only be understood by a limited audience. The\nlanguage of validity is reason , and we hope this article\nwill inspire MIR researchers to think creatively about the\nphenomena they observe to discover their causes.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n528. ACKNOWLEDGMENTS\nWe thank J. Urbano, H. Maruri-Aguilar, and various anony-\nmous reviewers of previous versions of this paper. The\ncontribution of Sturm is supported by a project that has re-\nceived funding from the European Research Council (ERC)\nunder the European Union’s Horizon 2020 research and\ninnovation programme (Grant agreement No. 864189 MU-\nSAiC: Music at the Frontiers of Artiﬁcial Creativity and\nCriticism). The contribution of Flexer is supported by fund-\ning from the Austrian Science Fund (FWF, project numbers\nP 31988 and P 36653). For the purpose of open access, the\nauthors have applied a CC BY public copyright license to\nany author accepted manuscript version arising from this\nsubmission.\n9. REFERENCES\n[1]W. R. Shadish, T. D. Cook, and D. T. Campbell, Experi-\nmental and quasi-experimental designs for generalized\ncausal inference . Boston: Houghton Mifﬂin, 2002.\n[2]C. W. Cleverdon, “The signiﬁcance of the Cranﬁeld\ntests on index languages,” in Proc. Int. ACM SIGIR Conf.\nResearch and Development in Info. Retrieval , 1991, pp.\n3–12.\n[3]E. M. V oorhees, “The philosophy of information re-\ntrieval evaluation,” in Proc. Cross-Language Evaluation\nForum , 2001.\n[4]J. Urbano, M. Schedl, and X. Serra, “Evaluation in\nmusic information retrieval,” Journal of Intelligent In-\nformation Systems , vol. 41, no. 3, pp. 345–369, 2013.\n[5]B. L. Sturm, “A simple method to determine if a music\ninformation retrieval system is a ‘horse’,” IEEE Trans-\nactions on Multimedia , vol. 16, no. 6, pp. 1636–1644,\n2014.\n[6]——, “The “horse” inside: seeking causes behind the\nbehaviors of music content analysis systems,” Comput-\ners in Entertainment (CIE) , vol. 14, no. 2, pp. 1–32,\n2017.\n[7]C. Kereliuk, B. L. Sturm, and J. Larsen, “Deep learning\nand music adversaries,” IEEE Transactions on Multime-\ndia, vol. 17, no. 11, pp. 2059–2071, 2015.\n[8]F. Rodríguez-Algarra, B. L. Sturm, and H. Maruri-\nAguilar, “Analysing scattering-based music content\nanalysis systems: Where’s the music?” in Proc. Int.\nSymp. Music Information Retrieval , 2016, pp. 344–350.\n[9]K. Prinz, A. Flexer, and G. Widmer, “On end-to-end\nwhite-box adversarial attacks in music information re-\ntrieval,” Transactions of the International Society for\nMusic Information Retrieval , vol. 4, no. 1, pp. 93–104,\n2021.\n[10] J.-J. Aucouturier and F. Pachet, “Improving timbre sim-\nilarity: How high is the sky?” J. Neg. Results Speech\nAudio Sci. , vol. 1, no. 1, pp. 1–13, 2004.[11] T. Pohle, E. Pampalk, and G. Widmer, “Evaluation of\nfrequently used audio features for classiﬁcation of mu-\nsic into perceptual categories,” in Proc. Int. Workshop\nContent-Based Multimedia Indexing , 2008.\n[12] G. A. Wiggins, “Semantic gap?? Schemantic schmap!!\nMethodological considerations in the scientiﬁc study of\nmusic,” in Proc. Int. Symp. Multimedia . IEEE, 2009,\npp. 477–482.\n[13] J. Urbano, “Information retrieval meta-evaluation: Chal-\nlenges and opportunities in the music domain,” in Proc.\nInt. Symp. Music Information Retrieval , 2011, pp. 609–\n614.\n[14] M. Schedl, A. Flexer, and J. Urbano, “The neglected\nuser in music information retrieval research,” Journal\nof Intelligent Information Systems , vol. 41, no. 3, pp.\n523–539, 2013.\n[15] B. L. Sturm, “Classiﬁcation accuracy is not enough: On\nthe evaluation of music genre recognition systems,” J.\nIntell. Info. Systems , vol. 41, no. 3, pp. 371–406, 2013.\n[16] ——, “Revisiting priorities: Improving MIR evaluation\npractices,” in Proc. ISMIR , 2016.\n[17] J. Urbano and A. Flexer, “Statistical analysis of results\nin music information retrieval: why and how,” in\nTutorial at Int. Symp. Music Information Retrieval ,\n2018. [Online]. Available: http://ismir2018.ircam.fr/\npages/events-tutorial-17.html\n[18] C. C. Liem and C. Mostert, “Can’t trust the feeling?\nHow open data reveals unexpected behavior of high-\nlevel music descriptors,” in Proc. Int. Symp. Music In-\nformation Retrieval , 2020, pp. 240–247.\n[19] A. Flexer, T. Lallai, and K. Rašl, “On evaluation of inter-\nand intra-rater agreement in music recommendation,”\nTransactions of the International Society for Music In-\nformation Retrieval , vol. 4, no. 1, pp. 182–194, 2021.\n[20] T. Lund, “A revision of the Campbellian validity system,”\nScandinavian J. Educational Research , vol. 65, no. 3,\npp. 523–535, 2021.\n[21] S. Dixon, F. Gouyon, and G. Widmer, “Towards charac-\nterisation of music via rhythmic patterns,” in Proc. Int.\nSymp. Music Information Retrieval , 2004, pp. 509–517.\n[22] B. L. T. Sturm and A. Flexer, “Validity in mu-\nsic information research experiments,” arxiv , vol.\narXiv:2301.01578, 2023.\n[23] A. W. Kimball, “Errors of the third kind in statistical\nconsulting,” J. American Statistical Assoc. , vol. 52, no.\n278, pp. 133–142, June 1957.\n[24] M. L. Head, L. Holman, R. Lanfear, A. T. Kahn, and\nM. D. Jennions, “The extent and consequences of p-\nhacking in science,” PLOS Biology , vol. 13, no. 3, pp.\n1–15, 2015.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n53[25] A. Flexer, “Statistical evaluation of music information\nretrieval experiments,” Journal of New Music Research ,\nvol. 35, no. 2, pp. 113–120, 2006.\n[26] P. R. Cohen, Empirical methods for artiﬁcial intelli-\ngence . MIT press Cambridge, MA, 1995, vol. 139.\n[27] N. Japkowicz and M. Shah, Evaluating Learning Algo-\nrithms: A Classiﬁcation Perspective . New York, NY ,\nUSA: Cambridge University Press, 2011.\n[28] R. A. Bailey, Design of comparative experiments . Cam-\nbridge University Press, 2008.\n[29] J. Urbano, J. S. Downie, B. Mcfee, and M. Schedl,\n“How signiﬁcant is statistically signiﬁcant? the case of\naudio music similarity and retrieval.” in Proc. Int. Symp.\nMusic Information Retrieval , 2012, pp. 181–186.\n[30] W. A. Sethares, Rhythm and Transforms . Springer,\n2007.\n[31] E. Pampalk, A. Flexer, G. Widmer et al. , “Improve-\nments of audio-based music similarity and genre classiﬁ-\ncaton.” in Proc. Int. Symp. Music Information Retrieval ,\n2005, pp. 634–637.\n[32] A. Flexer and D. Schnitzer, “Effects of album and artist\nﬁlters in audio similarity computed for very large music\ndatabases,” Computer Music Journal , vol. 34, no. 3, pp.\n20–28, 2010.\n[33] A. Flexer, “A closer look on artist ﬁlters for musical\ngenre classiﬁcation,” in Proc. Int. Symp. Music Informa-\ntion Retrieval , 2007, pp. 341–344.\n[34] B. L. Sturm, “The state of the art ten years after a state of\nthe art: Future research in music information retrieval,”\nJournal of New Music Research , vol. 43, no. 2, pp. 147–\n172, 2014.\n[35] O. Pfungst, Clever Hans (The horse of Mr. Von Osten):\nA contribution to experimental animal and human psy-\nchology . New York: Henry Holt, 1911.\n[36] B. L. Sturm, R. Bardeli, T. Langlois, and V . Emiya,\n“Formalizing the problem of music description,” in Proc.\nInt. Symp. Music Information Retrieval , 2014, pp. 89–\n94.\n[37] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Er-\nhan, I. J. Goodfellow, and R. Fergus, “Intriguing prop-\nerties of neural networks,” in Proc. Int. Conf. Learning\nRepresentations , 2014.\n[38] K. Hoedt, A. Flexer, and G. Widmer, “Defending a Mu-\nsic Recommender Against Hubness-Based Adversarial\nAttacks,” in Proceedings of the 19th Sound and Music\nComputing Conference , 2022, pp. 385–390.\n[39] D. Bogdanov, A. Porter, H. Boyer, X. Serra et al. ,\n“Cross-collection evaluation for music classiﬁcation\ntasks,” in Proc. Int. Symp. Music Information Retrieval ,\n2016, pp. 379 – 385.[40] D. Bogdanov, A. Porter, H. Schreiber, J. Urbano, and\nS. Oramas, “The acousticbrainz genre dataset: Multi-\nsource, multi-level, multi-label, and large-scale,” in\nProc. Int. Symp. Music Information Retrieval , 2019.\n[41] J. Urbano, D. Bogdanov, H. Boyer, E. Gómez Gutiérrez,\nX. Serra et al. , “What is the effect of audio quality on\nthe robustness of MFCCs and chroma features?” in\nProc. Int. Symp. Music Information Retrieval , 2014, pp.\n573–578.\n[42] B. McFee, J. W. Kim, M. Cartwright, J. Salamon, R. M.\nBittner, and J. P. Bello, “Open-source practices for\nmusic signal processing research: Recommendations\nfor transparent, sustainable, and reproducible audio re-\nsearch,” IEEE Signal Processing Magazine , vol. 36,\nno. 1, pp. 128–137, 2018.\n[43] A. Flexer and T. Grill, “The problem of limited inter-\nrater agreement in modelling music similarity,” Journal\nof New Music Research , vol. 45, no. 3, pp. 239–251,\n2016.\n[44] K. Seyerlehner, G. Widmer, and P. Knees, “A compari-\nson of human, automatic and collaborative music genre\nclassiﬁcation and user centric evaluation of genre clas-\nsiﬁcation systems,” in Proc. Int. Workshop Adaptive\nMultimedia Retrieval , 2010, pp. 118–131.\n[45] J.-J. Aucouturier, “Sounds like teen spirit: Computa-\ntional insights into the grounding of everyday musical\nterms,” Language, evolution and the brain , pp. 35–64,\n2009.\n[46] M. C. Jones, J. S. Downie, and A. F. Ehmann, “Human\nsimilarity judgments: Implications for the design of for-\nmal evaluations.” in Proc. Int. Symp. Music Information\nRetrieval , 2007, pp. 539–542.\n[47] O. Nieto, M. M. Farbood, T. Jehan, and J. P. Bello,\n“Perceptual analysis of the f-measure for evaluating sec-\ntion boundaries in music,” in Proc. Int. Symp. Music\nInformation Retrieval , 2014, pp. 265–270.\n[48] Y . Ni, M. McVicar, R. Santos-Rodriguez, and T. De Bie,\n“Understanding effects of subjectivity in measuring\nchord estimation accuracy,” IEEE Transactions on Au-\ndio, Speech, and Language Processing , vol. 21, no. 12,\npp. 2607–2615, 2013.\n[49] H. V . Koops, W. B. De Haas, J. A. Burgoyne, J. Bransen,\nA. Kent-Muller, and A. V olk, “Annotator subjectivity\nin harmony annotations of popular music,” Journal of\nNew Music Research , vol. 48, no. 3, pp. 232–252, 2019.\n[50] H. V . Koops, W. B. de Haas, J. Bransen, and A. V olk,\n“Automatic chord label personalization through deep\nlearning of shared harmonic interval proﬁles,” Neural\nComputing and Applications , vol. 32, no. 4, pp. 929–\n939, 2020.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n54[51] U. Marchand and G. Peeters, “Scale and shift invari-\nant time/frequency representation using auditory statis-\ntics: Application to rhythm description,” in Proc. IEEE\nInt. Workshop Machine Learning for Signal Processing ,\n2016.\n[52] D. J. Hand, “Classiﬁer technology and the illusion of\nprogress,” Statistical Science , vol. 21, no. 1, pp. 1–15,\n2006.\n[53] G. Peeters, J. Urbano, and G. J. F. Jones, “Notes from\nthe ISMIR 2012 late-breaking session on evaluation in\nmusic information retrieval,” in Proc. Int. Symp. Music\nInformation Retrieval , 2012.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n55"
    },
    {
        "title": "High-Resolution Violin Transcription Using Weak Labels.",
        "author": [
            "Nazif Can Tamer",
            "Yigitcan Özer",
            "Meinard Müller",
            "Xavier Serra"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265263",
        "url": "https://doi.org/10.5281/zenodo.10265263",
        "ee": "https://zenodo.org/records/10265263/files/000025.pdf",
        "abstract": "A descriptive transcription of a violin performance requires detecting not only the notes but also the fine-grained pitch variations, such as vibrato. Most existing deep learning methods for music transcription do not capture these variations and often need frame-level annotations, which are scarce for the violin. In this paper, we propose a novel method for high-resolution violin transcription that can leverage piece-level weak labels for training. Our conformer-based model works on the raw audio waveform and transcribes violin notes and their corresponding pitch deviations with 5.8 ms frame resolution and 10-cent frequency resolution. We demonstrate that our method (1) outperforms generic systems in the proxy tasks of violin transcription and pitch estimation, and (2) can automatically generate new training labels by aligning its feature representations with unseen scores. We share our model along with 34 hours of score-aligned solo violin performance dataset, notably including the 24 Paganini Caprices.",
        "zenodo_id": 10265263,
        "dblp_key": "conf/ismir/TamerOMS23",
        "keywords": [
            "conformer-based",
            "high-resolution",
            "piece-level",
            "weak labels",
            "raw audio waveform",
            "5.8 ms frame resolution",
            "10-cent frequency resolution",
            "generic systems",
            "pitch estimation",
            "score-aligned solo violin performance dataset"
        ],
        "content": "HIGH-RESOLUTION VIOLIN TRANSCRIPTION USING WEAK LABELS\nNazif Can Tamer♭Yigitcan Özer♯Meinard Müller♯Xavier Serra♭\n♭Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n♯International Audio Laboratories Erlangen, Germany\nnazifcan.tamer@upf.edu, yigitcan.oezer@audiolabs-erlangen.de,\nmeinard.mueller@audiolabs-erlangen.de, xavier.serra@upf.edu\nABSTRACT\nA descriptive transcription of a violin performance re-\nquires detecting not only the notes but also the ﬁne-grained\npitch variations, such as vibrato. Most existing deep learn-\ning methods for music transcription do not capture these\nvariations and often need frame-level annotations, which\nare scarce for the violin. In this paper, we propose a\nnovel method for high-resolution violin transcription that\ncan leverage piece-level weak labels for training. Our\nconformer-based model works on the raw audio waveform\nand transcribes violin notes and their corresponding pitch\ndeviations with 5.8ms frame resolution and 10-cent fre-\nquency resolution. We demonstrate that our method (1)\noutperforms generic systems in the proxy tasks of violin\ntranscription and pitch estimation, and (2) can automati-\ncally generate new training labels by aligning its feature\nrepresentations with unseen scores. We share our model\nalong with 34 hours of score-aligned solo violin perfor-\nmance dataset, notably including the 24 Paganini Caprices.\n1. INTRODUCTION\nAutomatic music transcription (AMT) is a core task in Mu-\nsic Information Retrieval that aims to convert a musical\nperformance into some form of symbolic notation. While\ngeneral-purpose AMT systems have recently seen substan-\ntial progress with deep learning [1–5], instrument-speciﬁc\nsystems usually perform better, e.g., for piano [6–9], vo-\ncals [10, 11], guitar [12–14], and drums [15–17]. Despite\nthe prominence of the violin in Western classical music and\nother traditions, a specialized high-precision violin tran-\nscription system that applies the recent advances in deep\nlearning does not exist. In this paper, we aim to tran-\nscribe violin performances into a descriptive music nota-\ntion [18]. As opposed to a prescriptive transcription, whose\naim would be to produce an easily understandable score\nfrom which a musician can perform according to stylistic\nconventions of Western classical music writing, a descrip-\n© Nazif Can Tamer, Yigitcan Özer, Meinard Müller, Xavier\nSerra. Licensed under a Creative Commons Attribution 4.0 International\nLicense (CC BY 4.0). Attribution: Nazif Can Tamer, Yigitcan Özer,\nMeinard Müller, Xavier Serra, “High-Resolution Violin Transcription us-\ning Weak Labels”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.\nPostprocessingnotes\nConstrained Viterbi \nPitch EstimationMIDI transcription with pitch bends \nonsets\n(semitone)offsets\n(semitone)frames\n(semitone)f0s\n(10 cents)\n44.1 kHz waveform\nMulti-Stream ConformerFigure 1 : Our method transcribes violin recordings sam-\npled with 44.1kHz waveform into MIDI with a 5.8ms\ntime- and 10-cent frequency-resolution pitch bends.\ntive transcription has an analytical purpose, aiming at no-\ntating high-precision pitch modulations along the notes.\nMost typical AMT systems employ audio-to-MIDI tran-\nscription where each note event is represented with semi-\ntone resolution in the 12-tone equal temperament (12-\nTET). However, cognitive studies show that even the West-\nern classical violinists heavily deviate from the 12-TET\nin favor of Pythagorean tuning and just intonation [19,\n20]. Furthermore, the violin also plays a central role in\nmany other traditions that do not employ the Western 12-\nTET [21]. Considering playing styles such as the vibrato\nand glissando that involve pitch modulations, a higher\nfrequency resolution than the conventional 12-TET is re-\nquired for violin transcription. An important step towards\ntranscription outside the 12-TET was introduced by Bittner\net al. [1] with an instrument-agnostic AMT system, which\nemploys MIDI pitch bends to represent performances with\n33-cent frequency resolution. However, adapting their ap-\nproach to violin transcription remains to be a challenge\nsince33-cent frequency resolution is still too high com-\npared to a violinist’s intonation precision [20].\nA further main challenge in violin transcription is the\nlack of frame-level annotated training data. To cope\nwith the absence of frame-level annotations, Weiß and223Peeters [22] employ sequence-level targets and a variant\nof the Connectionist Temporal Classiﬁcation (CTC) loss\nfor multipitch estimation. However, this strategy is sensi-\ntive to the segment duration (stable until segment lengths\nof60seconds) and, therefore, still requires some form of\nweak alignment. While some works explore data aug-\nmentation for frame-level supervised models through ad-\nditional unlabeled [4] or pseudo-labeled [5] data, recent\nAMT methods are mostly trained using frame-level anno-\ntations [1, 3, 8, 9]. In some cases, obtaining such annota-\ntions is feasible through electronic music instruments, e.g.,\nDisklavier. For example, the MAESTRO [23] dataset, with\n200hours of virtuoso piano performances and respective\nnote labels captured with 3ms frame resolution, enabled\nsigniﬁcant improvements for piano transcription.\nIn case electronic music instruments are unavailable, a\ncommon approach for obtaining automatic frame-level an-\nnotations is employing audio-to-score alignment (ASA),\nwhich found application in score following [24, 25]. ASA\nitself is not a technology developed for creating training\ndatasets for AMT systems, and it has been reported that in-\naccurately aligned datasets may even worsen the result [2].\nThe intertwined nature of ASA and transcription can also\nbe viewed from another aspect. For example, Kwon et\nal. [26] showed that frame and onset features of an AMT\nsystem work as robust feature representations for ASA. To\nour knowledge, the only deep-learning-based transcription\nsystem that integrates ASA into AMT is the recent work by\nMaman and Bermano [2], which utilizes ASA with chroma\nrepresentations obtained from AMT frames.\nAs the main contribution of this paper, we propose a\nnovel AMT system speciﬁcally tailored for descriptive vi-\nolin transcription1regarding two crucial aspects: 1) We\nrepresent pitch deviations such as vibrato, glissando, or\nintonation choice by incorporating ﬁne-grained pitch rep-\nresentations into the transcription. While borrowing our\nnote postprocessing system and the MIDI pitch bend rep-\nresentations from Bittner et al. [1], we build a conformer-\nbased model that works on the raw audio waveform and\nfurther improves the pitch bend estimation through note-\nconstrained Viterbi pitch tracking. 2) We acquire frame-\nlevel annotations for violin transcription by considering si-\nmultaneous transcription and alignment in a joint frame-\nwork, similar to the work by Maman and Bermano [2].\nFollowing the ﬁndings from the music synchronization lit-\nerature, we also incorporate activation-function-based fea-\ntures in the alignment [27, 28].\nIn order to benchmark our descriptive violin transcrip-\ntion method, we consider the proxy tasks of transcription\nand pitch estimation and compare our model with general-\npurpose baselines. As a side contribution, we also release\na 34-hour dataset of solo violin recordings, with automat-\nically aligned MIDIs and note-constrained multi-f0 tracks\nobtained using our descriptive violin transcription system.\nThe remainder of this paper is organized as follows:\nin Section 2, we introduce our MUlti-Stream Conformer\n(MUSC) model for AMT that processes an audio wave-\n1https://github.com/MTG/violin-transcription/\n44.1 kHz waveformx4\nConformer \nBlocks\nFConsets frames offsets f0s\nConformer \nBlocks\nConformer \nBlocks\nConformer \nBlocks\n5.8 ms ratex16\nDuplex CNNs\nConformer Blocks\nFC\n FC\n FCFigure 2 : The Multi-Stream Conformer architecture con-\nverts raw audio sampled with 44.1kHz into four feature\nrepresentations with a frame rate of 5.8ms.\nform into four musical representations. In Section 3, we\ndescribe our strategy for learning without frame-level an-\nnotations. In Section 4, we introduce how we simultane-\nously annotate a novel violin transcription dataset while\ntraining our model. In Section 5, we compare our descrip-\ntive violin transcription model against general-purpose\ntranscription and pitch estimation baselines. Finally, we\nconclude in Section 6 with prospects on future work.\n2. MULTI-STREAM CONFORMER\nWe propose a MUlti-Stream Conformer (MUSC) that pro-\ncesses the raw audio waveform into four streams that es-\ntimate onset, offset, semitone-level pitch frames (denoted\nas frames as in the AMT literature), and high-resolution\nf0 frames as shown in Figure 2. The raw audio waveform\nsampled with 44.1kHz is converted into 256-dimensional\nfeatures with a hop size of 5.8ms through duplex CNNs.\nThen, these features pass through the Conformer blocks to\nestimate the four representations. The resulting represen-\ntations can be either used for MIDI transcription with pitch\nbends as in Figure 1, or for frame-level dataset annotation\nfor training (see Section 3).\n2.1 Duplex CNNs\nWe borrow the basic CNN structure from the ﬁrst two lay-\ners of the CREPE [29] pitch estimator, except for zero\npadding. We remove the zero padding in the convolutional\nlayers so that the duplex CNNs can access to the infor-\nmation at the borders of the window with varying recep-\ntive ﬁelds. With the raw audio in 44.1kHz as the input,\nthe duplex CNNs independently summarize the waveform\ninto 128-dimensional frames with a hop length of 5.8ms.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n224Figure 3 : A closer look at the Duplex CNNs.\nThe standard CNN (shown in red in Figure 3) analyzes the\nframe with the CREPE conﬁguration, resulting in a recep-\ntive ﬁeld of 26ms. The dilated CNN (depicted in yellow\nwithin Figure 3) incorporates double the number of dila-\ntions and strides per layer, ultimately leading to a receptive\nﬁeld of118ms. Thanks to the dilations and strides, the\nsampling rate for the dilated CNN is subsequently reduced\nto22.05kHz, and 11kHz. Thus, it effectively analyzes\na smoother version of the signal. The 128-dimensional\noutputs of the individual CNNs are then stacked into a\n256-dimensional representation and pass through a simple\nfully-connected layer before the main Conformer stream.\n2.2 Conformer Blocks\nDue to the direct analogy between music transcription and\nspeech recognition, we adopt the Conformer [30], a state-\nof-the-art automatic speech recognition (ASR) model, as\nthe base block of MUSC. We directly employ conformer\nblocks from the Conformer encoder (M version) as de-\nscribed by Gulati et al. [30], i.e., with four attention heads,\na depthwise convolution size of 32, and an encoder dimen-\nsion of 256. For the main stream, we repeat the conformer\nblocks 16 times as in Conformer (M). Then, we employ\nseparate conformer blocks for each of the onset, offset,\nframe, and f0 streams with four conformer blocks per rep-\nresentation. The total number of conformer blocks we uti-\nlize in the multi-stream conformer architecture is 32.\n2.3 Feature Representations\nOur method is based on transforming weak labels into\nframe-level features that are used both as training targets\nand alignment features. The feature representations en-\ncompass the violin pitch range from F♯3toE8, i.e.,58\nbins for the onsets, offsets, and note frames, which work\non semitone resolution, and 580 bins for the f0s, which\nwork on10-cent resolution. More precisely, we use a ﬁxed\nsequence duration of three seconds and convert the audio\nwaveform into 512×58dimensional onset, offset, and\n(note) frames, and 512×580dimensional f0 frames.\ntonset toffsetfMIDIfMID I + 100c\nfMIDI - 100cViterbi Constraint RegionFigure 4 : Constraint region for the Viterbi pitch tracking.\nWe train the model to predict strong onset, offset, and\nframe labels that are generated from iterative score align-\nments. We employ Gaussian label smoothing for onset,\noffset, and f0 features. For the onsets and offsets, we\nsmooth the feature representations with a standard devi-\nation of4ms. Following Kim et al. [29], we also blur the\nf0 features with a 12-cent standard deviation.\nNote that the high-precision f0 features are not included\nin the score, hence cannot be inferred from the alignment.\nFor the f0 features, we train the model to predict pseudo-\nlabels generated by the TAPE model [31] in the ﬁrst iter-\nation. Then, we use our model’s predictions as pseudo f0\nlabels. The polyphonic multipitch information are also en-\ncoded in the f0 representations. We employ constrained\nViterbi pitch estimation (see Section 2.5) for generating\npseudo-f0 labels for the polyphonic segments.\n2.4 Note postprocessing\nIn the original Conformer paper [30], which is designed\nfor ASR, the output of the encoder is proceeded by a de-\ncoder that uses an external language model to generate the\nword sequence. A natural adoption of this strategy to our\nscenario would require onsets, offsets, and frames to be\nfed into a language model that is specialized in the vio-\nlin repertoire. However, employing a decoder is not viable\nsince violin repertoire remains a low-resource language,\nand training decoders with such limited data is prone to\noverﬁtting. Instead, we experiment with postprocessing\ntechniques from open-source AMT libraries and adopt the\none2from Bittner et al. [1]. We leave improving the post-\nprocessing stage as an open question for further studies.\n2.5 Constrained Viterbi Pitch Estimation\nPrevious studies have shown that score information [32]\nand the continuity principle of pitch perception [33] can be\nused for reﬁning the f0 estimation. We apply continuity\nconstraints within note sections to detect the pitch bends\nwith higher accuracy. First, we deﬁne the constraint region\non the f0 matrix from the note onset, offset, and 200cents\naround the note frequency as shown in Figure 4. We calcu-\nlate the Viterbi path within the note boundaries by utiliz-\ning the constraint region as observation probabilities and\nf0 transition probability matrix S∈R21×21covering the\n2https://github.com/spotify/basic-pitch/blob/\nmain/basic_pitch/note_creation.pyProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n225Audio-Score\nAlignment\nConstrained Viterbi \nPitch Estimationframe-wise labels\nonsets\noffsets\nframes\nscore\nonsets offsets frames f0s\nMulti-Stream Conformer\n44.1 kHz waveformnotes\nFigure 5 : The proposed high-resolution violin transcrip-\ntion model only requires piece-level labels for learning as\nit can generate frame-wise labels using its own onset, off-\nset, and frame feature representations.\n200cents around the note frequency. For each consecutive\ntime instant, Sallows smooth transitions with a Gaussian\nstandard deviation of 25cents, i.e., 2.5f0 states:\nsij=exp/parenleftbigg\n−1\n2/parenleftBig\nj−i\n(25/10)/parenrightBig2/parenrightbigg\n(25/10)√\n2π,\nfori,j∈[1 : 21] , wheresijdenotes the state transition\nprobabilities in the 10-cent resolution f0 matrix.\nSince Viterbi algorithm has a complexity of O(n2), ap-\nplying the pitch tracking within the constrained region also\nimproves the runtime speed compared to Viterbi without\nnote constraints. Moreover, applying Viterbi within note\nconstraints allow detecting multiple f0s.\nAfter per-note Viterbi paths are calculated, the frame-\nwise pitch predictions are obtained through the regional\nweighted averaging method from Kim et al. [29] to deter\nthe f0 estimates through further interpolations.\n3. LEARNING FROM WEAK LABELS\nOur proposed method enables learning from weak labels,\nwhich involve pairs of violin recordings and their publicly-\navailable scores. The learning procedure consists of four\nphases. First, we create initial audio-score alignments us-\ning music synchronization techniques. Second, we use\nthe aligned audio-score pair for the ﬁrst round of train-\ning. Third, we recompute the alignment using the esti-\nmated features. Fourth and ﬁnally, we ﬁnetune our model\nusing the ﬁner features learned by the model.\nTo create the initial audio-score alignments, we use dy-\nnamic time warping (DTW), which is a well-known tech-nique for music synchronization [34–36]. Conventional\nmethods for music synchronization typically use DTW\nand chroma features as the input representation [32, 37],\nwhereas the integration of additional activation functions,\ne.g., onsets, beats, downbeats, has proven to enhance the\nsynchronization accuracy [27, 28]. Since we deal with vi-\nolin transcription in this paper, we follow the alignment\nmethod in [28], which deals with a similar scenario, i.e.,\naudio-to-audio synchronization of string quartets. Inspired\nby their combined synchronization approach, we ﬁrst in-\ncorporate beat, downbeat, and onset activation functions\nalongside chroma features to generate the initial audio-\nscore alignments. The inclusion of activation functions re-\nsults in a grid-like structure in the DTW cost matrix, which\nguides the alignment through activation cues that point to\nnote onsets or other musical events. At the same time,\nchroma features account for the harmonic and melodic in-\nformation.\nFollowing the setting in [28], we use a sample rate of\n22.05kHz and a feature rate of 50Hz to create the align-\nments. As this feature rate ( 20ms) is coarser than the\nmodel’s frame resolution ( 5.8ms), we apply linear inter-\npolation to create labels. Note that we cannot evaluate the\nsynchronization accuracy of the training data since we do\nnot have any annotations for these. Using these target la-\nbels obtained from the initial alignment, which can possi-\nbly be inaccurate, we train our model for one epoch in the\nﬁrst training phase.\nFollowing the ﬁrst training phase, we obtain the\nfour learned representations, onset, offset, semitone-level\nframes, and high-resolution f0 frames for each audio-score\npair. To acquire ﬁner and more accurate labels, we run a\nnovel synchronization stage. We recompute the alignment\nwith the reﬁned features, estimated semitone-level frame\nrepresentations, and the activation with the stacked onset\nand offset features (see Section 2.3). Note that the feature\nrate we use in the alignment is the same as the MUSC fea-\ntures (hop size of 5.8ms). Using the labels obtained from\nsynchronization, we ﬁnetune our model using early stop-\nping.\nOur iterative training strategy resembles the approach\nby Maman and Bermano [2]. Their approach starts with\ntraining the transcription model with synthetic data and\nthen creating the initial alignments with the features esti-\nmated by this model and involves three training iterations:\nﬁrst on synthetic data and two more iterations to ﬁnetune\nthe model on the target dataset. In contrast, we start from\na robust ASA and complete the training process in two it-\nerations.\n4. DATASET AND TRAINING\nIn this section, we describe our dataset that we use for the\ntraining and our training procedure. The weakly-labeled\ndataset consists of 120 scores and 34hours of solo vio-\nlin performances. We also provide automatic score align-\nments and frame-level pitch bends that are generated by\nour joint data curation and training process.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n226#s#p#rdur\nPaganini, Op. 1 24 10 235 13:00\nWohlfahrt, Op. 45 60 6 506 11:36\nKayser, Op. 20 36 8 280 09:48\nTotal 120 22 1021 34:24\nTable 1 : Dataset statistics. #s: number of scores, #p:\nnumber of distinct players, #r: number of recordings, dur:\ntotal recording duration in hh:mm.\n4.1 Dataset Statistics\nOur dataset comprises public scores of 96 etudes which are\nincluded in the Violin Etudes dataset [38], i.e., Wohlfahrt\nOp. 45, and Kayser Op. 20. We also extend these scores\nwith additional 24 etudes/caprices by Paganini Op. 1. In\ncontrast to the Violin Etudes dataset, which only includes\nmonophonic recordings, the recordings in our dataset in-\nclude a mix of monophonic and polyphonic etudes. We\ncollect multiple versions of these etudes from YouTube and\nautomatically match and align them using the method de-\nscribed in Section 3. For the Paganini Op. 1 score, we no-\nticed that performers do not always follow the repeat signs.\nTo ensure better alignments, we automatically expand each\nrepetition pattern individually and select the one that best\nmatches the recording based on the alignment distance. As\nthe most extreme case, we found four different repetition\npatterns for the Paganini Op. 1 No. 23, which we label as\nOp01-23, Op01-23-a, Op01-23-b, and Op01-23-c in the\ndataset, respectively.\nThe dataset we provide includes original YouTube links,\nannotated start and end timestamps, and aligned MIDI\nﬁles containing multi-pitch bends. These resources can be\nutilized to generate expressive performances featuring vi-\nbrato. Moreover, for each etude and caprice, we provide\nat least ﬁve performances, which can be utilized for audio-\nto-audio synchronization and comparative studies. Table 1\nsummarizes the dataset statistics.\n4.2 Training Details\nUsing Adam optimizer and a learning rate of 1e−3, we\ntrain the model to minimize the binary cross entropy (BCE)\nloss for the onset, offset, frame, and f0s:\nL=Lonset+Loﬀset+Lframe+Lf0\n10.\nIn addition to Gaussian label smoothing as described in\nSection 2.3, we weight positive onset and offsets with 9\nto balance the sparse matrices. Furthermore, we also ob-\nserve that weighting the Lf0by1/10helps in increasing\nthe stability of the training.\nSince our dataset includes several versions per piece,\nwe do not employ further data augmentations. We train\nthe model using a batch size of 16and a ﬁxed sequence\nduration of three seconds (512 frames). We employ (80−\n20)train–validation splits and consider each sample with\nthe etude no ≡3 (mod 5) for the validation set.\nAfter training for one epoch on the dataset obtained\nwith initial alignments and pseudo f0 labels, we realignthe dataset with the model’s onset, offset, and frame fea-\ntures and apply constrained Viterbi tracking for the f0 la-\nbels. Using the new labels estimated by the model, we train\nthe model further, applying early stopping.\n5. EXPERIMENTS\nWhile we aim at the task of descriptive violin transcription\nwith high-resolution pitch bends, there is no previous work\non which we can directly compare with. Therefore, we\ncompare our model with general-purpose baselines for the\nclosely-related proxy tasks of transcription and pitch esti-\nmation. We provide our experimental results on the violin\ntracks of two manually-annotated and corrected datasets,\ni.e., URMP [39] and Bach10 [40].\n5.1 Test Datasets\nThe URMP dataset [39] is a multimodal dataset that in-\ncludes44performances in various chamber ensemble set-\ntings. The dataset was annotated with the help of the\nTony melody transcription software [41], which utilizes the\npYIN [33] algorithm for the initial f0 estimates and applies\na hidden Markov model for note quantization. The note on-\nsets, offsets, and f0s are then manually corrected. For our\nevaluation, we use all the violin tracks from the URMP\ndataset. We note that one of our transcription baselines,\nthe MT3 [3] model, was trained using this dataset. Since\nwe employ our tests in the entirety of the violin tracks, the\ntests include the training samples of the MT3.\nOur second test dataset, Bach10 [40], comprises 10\nfour-part chorales played by a violin, clarinet, tenor sax-\nophone, and bassoon quartet. The ground-truth f0 annota-\ntions in the dataset were estimated ﬁrst using the YIN [42]\nalgorithm and then corrected manually. The dataset also\nincludes note annotations derived from the beat times that\nare manually-annotated by musicians. However, the man-\nual correction for offset times is not included in the dataset.\nFor our evaluation, we use all the violin tracks from the\nBach10 dataset. We note that the Bach10 dataset was in-\ncluded in the training of one of our baselines in pitch esti-\nmation, i.e., CREPE [29].\n5.2 Evaluation Metrics\nAs a proxy to descriptive violin transcription, we evalu-\nate our method’s transcription and pitch estimation perfor-\nmance separately using the common mir_eval metrics,\nand compare with general-purpose baselines. For the tran-\nscription, we provide our results with Precision P, Recall\nR, F1-score F1, and F1-score without offset F1nousing\nthe default thresholds. Namely, for P,R, andF1, a note\nis considered correct its pitch is within 50cents, the onset\nis within 50ms and the offset is within 20% of the note’s\nduration. We also include an additional measure, F1no,\nwhere a note is considered correct if the onset is within\n50ms without considering the offset. For the pitch estima-\ntion experiments, we used the Raw Pitch Accuracy (RPA)\nmetric with two thresholds: the standard RPA50 metric,\nwhich considers the estimate accurate if it is within 50Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n227URMP Bach10\nP R F1 F1 no P R F1 F1 no\nMUSC 86.5 83.1 84.6 93.0 65.0 64.8 64.8 77.0\nMT3 79.1 87.1 82.2 88.9 54.2 51.5 52.7 62.0\nBP 58.8 67.9 62.8 83.3 33.6 43.2 37.6 57.5\nTable 2 : Violin transcription results (%) comparing MUSC\nwith two general-purpose AMT methods. Tests are con-\nducted on all violin stems from the datasets. Bach10 repre-\nsents the fair evaluation in a dataset unseen to all models.\nURMP was involved in the training dataset of the MT3,\nwhereas it is unseen to both BP and MUSC.\nURMP Bach10\nP R F1 F1 no P R F1 F1 no\nIter1 84.6 82.5 83.6 92.9 63.1 63.5 63.2 75.3\nIter2 86.5 83.1 84.6 93.0 65.0 64.8 64.8 77.0\nTable 3 : Violin transcription results (%) before (Iter1) and\nafter (Iter2) ﬁne-tuning the proposed MUSC model with\nthe iterative alignment.\ncents, and the RPA10 metric, which has a more strict 10-\ncent threshold.\n5.3 Results\nWe compare MUSC with two recent general-purpose\nAMT baselines: Our ﬁrst baseline is the Basic Pitch [1]\n(BP), which is a lightweight model for instrument-agnostic\nAMT. The postprocessing method of BP is optimized for\nF1no, and MUSC also shares the same postprocessing\nscript with their default parameters. The second baseline\nwe consider for transcription is the MT3 [3], which is a\nmulti-instrument transcription model that predicts instru-\nment labels alongside transcription. Since we only test on\nviolin recordings, we combine their output without the in-\nstrument labels for fair evaluation.\nTable 2 summarizes the results for the transcription ex-\nperiments. At a ﬁrst glance, the proposed violin-speciﬁc\nmodel MUSC outperforms MT3 and BP on both datasets,\nindicating that it is a more effective method for violin tran-\nscription. Even though the training set of MT3 included the\ntest samples in the URMP dataset, MUSC yields the best\nF1-score value among the three AMT systems. Further-\nmore, the performance gap between MUSC and MT3 is\ngreater for the Bach10, which was not included the training\nset of any method. The results indicate that the all the mod-\nels yield rather poor scores on the Bach10 dataset when\nevaluated using the conventional P,R, andF1metrics.\nSince the offsets in the Bach10 dataset are not manually-\ncorrected, the F1noscores can be viewed as a better indi-\ncator of the transcription performance for this dataset.\nWe also compare our model’s transcription performance\nbefore and after ﬁne-tuning with alignments generated us-\ning its own feature representations. The Table 3 shows that\nsome of the improvements in our model’s transcription per-\nformance can be attributed to the iterative training strategy.\nFor the pitch estimation experiments, we compare\nMUSC with four well-known pitch estimators: the pre-URMP Bach10\nRPA50 RPA10 RPA50 RPA10\nMUSC 98.3 89.0 98.3 86.9\nvMUSC 98.6 89.4 98.4 87.0\nCREPE 96.4 87.2 98.6 88.1\nvCREPE 97.3 88.4 98.6 88.1\nYIN 95.3 88.4 97.1 81.7\npYIN 97.2 88.6 97.4 80.3\nSWIPE 97.2 89.3 97.7 84.3\nTable 4 : Violin Raw Pitch Accuracy (RPA, %) results.\nNote that the training set of CREPE involved the Bach10\ndataset. vMUSC and vCREPE contain an additional\nViterbi decoding stage.\ntrained CREPE model [29] from its ofﬁcial repository3,\npYIN [33], and YIN [42] from librosa4, and SWIPE [43]\nfrom the libf0 library5. We use the same F♯3(min) toE8\n(max) frequency range for a fair evaluation.\nTable 4 summarizes the pitch estimation results. First,\nall the pitch estimators achieve high accuracies on both\ndatasets. For the URMP dataset which is unseen to all\nthe models, vMUSC (MUSC with Viterbi decoding) out-\nperforms the common state-of-the-art pitch estimators in\nterms of RPA50 and RPA10. For the Bach10 dataset,\nwhich is included in the training samples of the pre-trained\nCREPE model, the CREPE expectedly yields the best RPA\nvalues. Note that even though our model was not trained\nwith these test samples from Bach10, MUSC remains to be\ncompetitive (e.g., 98.4% versus 98.6% RPA50 in Bach10).\n6. CONCLUSION\nIn this paper, we introduced MUSC, an AMT system tai-\nlored for violin transcription through high-precision pitch\nbend estimation, and the capability of learning from piece-\nwise weak labels. We showed that, by only utilizing 120\nscores, we were able to obtain state-of-the-art transcription\nand pitch estimation results for the violin. We also shared\nour descriptive violin transcription dataset to the MIR com-\nmunity. In the future, we will focus on improving the note\npostprocessing and alignment stages of the MUSC in or-\nder to specialize better for the string repertoire, and use\nit as a large-scale dataset curation tool for strings music,\nethnomusicology, and music education research. We be-\nlieve that the descriptive music transcription capabilities of\nthe MUSC will accelerate the research in music education,\nethnomusicology, and expressive performance generation.\n7. ACKNOWLEDGEMENTS\nThis research is funded by the project Musical AI\n- PID2019-111403GB-I00/AEI/10.13039/501100011033\nfunded by the Spanish Ministerio de Ciencia, Innovación y\nUniversidades (MCIU) and the Agencia Estatal de Inves-\ntigación (AEI), and by the German Research Foundation\n(DFG MU 2686/10-2).\n3https://github.com/marl/crepe\n4https://librosa.org/\n5https://github.com/groupmm/libf0Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2288. REFERENCES\n[1] R. M. Bittner, J. J. Bosch, D. Rubinstein, G. Meseguer-\nBrocal, and S. Ewert, “A lightweight instrument-\nagnostic model for polyphonic note transcription and\nmultipitch estimation,” in Proceedings of the IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) , Singapore, 2022.\n[2] B. Maman and A. H. Bermano, “Unaligned supervi-\nsion for automatic music transcription in the wild,” in\nProceedings of the International Conference on Ma-\nchine Learning (ICML) , July 2022, pp. 14 918–14 934.\n[3] J. Gardner, I. Simon, E. Manilow, C. Hawthorne,\nand J. H. Engel, “MT3: multi-task multitrack\nmusic transcription,” Computing Research Reposi-\ntory (CoRR) , vol. abs/2111.03017, 2021. [Online].\nAvailable: https://arxiv.org/abs/2111.03017\n[4] K. W. Cheuk, D. Herremans, and L. Su, “Reconvat: A\nsemi-supervised automatic music transcription frame-\nwork for low-resource real-world data,” in Proceed-\nings of the ACM Multimedia Conference , H. T. Shen,\nY . Zhuang, J. R. Smith, Y . Yang, P. Cesar, F. Metze,\nand B. Prabhakaran, Eds., Virtual Event, China, 2021,\npp. 3918–3926.\n[5] I. Simon, J. Gardner, C. Hawthorne, E. Manilow, and\nJ. Engel, “Scaling polyphonic transcription with mix-\ntures of monophonic transcriptions,” in Proc. Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Bengaluru, India, 2022, pp. 749–756.\n[6] S. Ewert and M. B. Sandler, “Piano transcription in the\nstudio using an extensible alternating directions frame-\nwork,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing , vol. 24, no. 11, pp. 1983–1997,\n2016.\n[7] R. Kelz and G. Widmer, “Towards interpretable poly-\nphonic transcription with invertible neural networks,”\ninProceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR) , Delft, The\nNetherlands, November 2019, pp. 376–383.\n[8] Q. Kong, B. Li, X. Song, Y . Wan, and Y . Wang, “High-\nresolution piano transcription with pedals by regress-\ning onset and offset times,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing , vol. 29,\npp. 3707–3717, 2021.\n[9] C. Hawthorne, I. Simon, R. Swavely, E. Manilow, and\nJ. H. Engel, “Sequence-to-sequence piano transcription\nwith transformers,” pp. 246–253, 2021.\n[10] R. Schramm and E. Benetos, “Automatic transcription\nof a cappella recordings from multiple singers,” in Pro-\nceedings of the AES International Conference on Se-\nmantic Audio , Erlangen, Germany, 2017, pp. 108–115.\n[11] R. Nishikimi, E. Nakamura, S. Fukayama, M. Goto,\nand K. Yoshii, “Automatic singing transcription basedon encoder-decoder recurrent neural networks with\na weakly-supervised attention mechanism,” in Pro-\nceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\nBrighton, UK, 2019, pp. 161–165.\n[12] T.-W. Su, Y .-P. Chen, L. Su, and Y .-H. Yang, “TENT:\nTechnique-embedded note tracking for real-world gui-\ntar solo recordings,” Transactions of the International\nSociety for Music Information Retrieval (TISMIR) ,\nvol. 2, no. 1, July 2019.\n[13] A. Wiggins and Y . E. Kim, “Guitar tablature estima-\ntion with a convolutional neural network.” in Proceed-\nings of the International Society for Music Informa-\ntion Retrieval Conference (ISMIR) , Delft, Netherlands,\nNovember 2019, pp. 284–291.\n[14] J. Abeßer and M. Müller, “Jazz bass transcription us-\ning a U-net architecture,” Electronics , vol. 10, no. 6, p.\n670, 2021.\n[15] M. A. Kaliakatsos-Papakostas, A. Floros, M. N. Vra-\nhatis, and N. Kanellopoulos, “Real-time drums tran-\nscription with characteristic bandpass ﬁltering,” in Pro-\nceedings of the Audio Mostly: A Conference on Inter-\naction with Sound , Corfu, Greece, September 2012, pp.\n152–159.\n[16] C. Southall, R. Stables, and J. Hockman, “Automatic\ndrum transcription using bi-directional recurrent neu-\nral networks,” in Proceedings of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , New York City, New York, USA, August 2016,\npp. 591–597.\n[17] K. Choi and K. Cho, “Deep unsupervised drum tran-\nscription,” in Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\nDelft, The Netherlands, 2019, pp. 183–191.\n[18] C. Seeger, “Prescriptive and descriptive music-\nwriting,” The Musical Quarterly , vol. 44, no. 2, pp.\n184–195, 1958.\n[19] P. C. Greene, Violin performance with reference to tem-\npered, natural, and Pythagorean intonation . Univer-\nsity of Iowa Press, 1937.\n[20] J. M. Geringer, “Eight artist-level violinists perform-\ning unaccompanied bach: Are there consistent tuning\npatterns?” String Research Journal , vol. 8, no. 1, pp.\n51–61, 2018.\n[21] G. N. Swift, The violin as cross cultural vehicle: Orna-\nmentation in South Indian violin and its inﬂuence on a\nstyle of Western violin improvisation . Wesleyan Uni-\nversity, 1989.\n[22] C. Weiß and G. Peeters, “Learning multi-pitch esti-\nmation from weakly aligned score-audio pairs using\na multi-label CTC loss,” in Proceedings of the IEEEProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n229Workshop on Applications of Signal Processing to Au-\ndio and Acoustics (WASPAA) , New Paltz, USA, 2021,\npp. 121–125.\n[23] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon,\nC. A. Huang, S. Dieleman, E. Elsen, J. H.\nEngel, and D. Eck, “Enabling factorized piano music\nmodeling and generation with the MAESTRO dataset,”\ninProceedings of the International Conference\non Learning Representations (ICLR) , New Orleans,\nLouisiana, USA, 2019. [Online]. Available: https:\n//openreview.net/forum?id=r1lYRjC9F7\n[24] D. Schwarz, N. Orio, and N. Schnell, “Robust poly-\nphonic midi score following with hidden Markov mod-\nels,” in International Computer Music Conference\n(ICMC) , Miami, Florida, USA, 2004.\n[25] M. Dorfer, A. Arzt, and G. Widmer, “Towards score\nfollowing in sheet music images,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , New York City, New York, USA,\n2016, pp. 789–795.\n[26] T. Kwon, D. Jeong, and J. Nam, “Audio-to-score align-\nment of piano music using RNN-based automatic mu-\nsic transcription,” in Proceedings of the Sound and\nMusic Computing Conference (SMC) , Espoo, Finland,\n2017, pp. 380–385.\n[27] S. Ewert, M. Müller, and P. Grosche, “High resolution\naudio synchronization using chroma onset features,”\ninProceedings of IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\nTaipei, Taiwan, 2009, pp. 1869–1872.\n[28] Y . Özer, M. Istvanek, V . Ariﬁ-Müller, and M. Müller,\n“Using activation functions for improving measure-\nlevel audio synchronization,” in Proc. International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , Bengaluru, India, 2022, pp. 749–756.\n[29] J. W. Kim, J. Salamon, P. Li, and J. P. Bello, “CREPE:\nA convolutional representation for pitch estimation,”\ninProceedings of the IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\nCalgary, Canada, 2018, pp. 161–165.\n[30] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang,\nJ. Yu, W. Han, S. Wang, Z. Zhang, Y . Wu et al. ,\n“Conformer: Convolution-augmented transformer for\nspeech recognition,” in Proceedings of the Annual\nConference of the International Speech Communica-\ntion Association (Interspeech) , 2020, pp. 5036–5040.\n[31] N. C. Tamer, Y . Özer, M. Müller, and X. Serra, “TAPE:\nAn end-to-end timbre-aware pitch estimator,” in Proc.\nInternational Conference on Acoustics, Speech, and\nSignal Processing (ICASSP) , 2023.\n[32] M. Müller, Fundamentals of Music Processing – Us-\ning Python and Jupyter Notebooks , 2nd ed. Springer\nVerlag, 2021.[33] M. Mauch and S. Dixon, “pYIN: A fundamental fre-\nquency estimator using probabilistic threshold distri-\nbutions,” in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , Flo-\nrence, Italy, 2014, pp. 659–663.\n[34] S. Salvador and P. Chan, “FastDTW: Toward accurate\ndynamic time warping in linear time and space,” in\nProceedings of the KDD Workshop on Mining Tempo-\nral and Sequential Data , 2004.\n[35] S. Dixon and G. Widmer, “MATCH: A music align-\nment tool chest,” in Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , London, UK, 2005, pp. 492–497.\n[36] M. Müller, Y . Özer, M. Krause, T. Prätzlich, and\nJ. Driedger, “Sync Toolbox: A Python package for ef-\nﬁcient, robust, and accurate music synchronization,”\nJournal of Open Source Software (JOSS) , vol. 6,\nno. 64, pp. 3434:1–4, 2021.\n[37] R. B. Dannenberg and N. Hu, “Polyphonic audio\nmatching for score following and intelligent audio ed-\nitors,” in Proceedings of the International Computer\nMusic Conference (ICMC) , San Francisco, USA, 2003,\npp. 27–34.\n[38] N. C. Tamer, P. Ramoneda, and X. Serra, “Violin\netudes: a comprehensive dataset for f0 estimation and\nperformance analysis,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Bengaluru, India, 2022, pp. 517–524.\n[39] B. Li, X. Liu, K. Dinesh, Z. Duan, and G. Sharma,\n“Creating a multitrack classical music performance\ndataset for multimodal music analysis: Challenges, in-\nsights, and applications,” IEEE Transactions on Multi-\nmedia , vol. 21, no. 2, pp. 522–535, 2019.\n[40] Z. Duan, B. Pardo, and C. Zhang, “Multiple fundamen-\ntal frequency estimation by modeling spectral peaks\nand non-peak regions,” IEEE Transactions on Audio,\nSpeech, and Language Processing , vol. 18, no. 8, pp.\n2121–2133, 2010.\n[41] M. Mauch, C. Cannam, R. Bittner, G. Fazekas, J. Sala-\nmon, J. Dai, J. Bello, and S. Dixon, “Computer-aided\nmelody note transcription using the Tony software:\nAccuracy and efﬁciency,” in Proceedings of the Inter-\nnational Conference on Technologies for Music Nota-\ntion and Representation , 2015.\n[42] A. de Cheveigné and H. Kawahara, “YIN, a fundamen-\ntal frequency estimator for speech and music.” Journal\nof the Acoustical Society of America (JASA) , vol. 111,\nno. 4, pp. 1917–1930, 2002.\n[43] A. Camacho and J. G. Harris, “A sawtooth waveform\ninspired pitch estimator for speech and music,” The\nJournal of the Acoustical Society of America , vol. 124,\nno. 3, pp. 1638–1652, 2008.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n230"
    },
    {
        "title": "Singer Identity Representation Learning Using Self-Supervised Techniques.",
        "author": [
            "Bernardo Torres",
            "Stefan Lattner",
            "Gaël Richard"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265323",
        "url": "https://doi.org/10.5281/zenodo.10265323",
        "ee": "https://zenodo.org/records/10265323/files/000053.pdf",
        "abstract": "Significant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis. We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations. We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization. Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz. We release our code and trained models to facilitate further research on singing voice and related areas.",
        "zenodo_id": 10265323,
        "dblp_key": "conf/ismir/TorresLR23",
        "keywords": [
            "voice identity representations",
            "speech data",
            "singing voices",
            "framework",
            "singer identity encoders",
            "isolated vocal tracks",
            "self-supervised learning",
            "data augmentations",
            "singer similarity",
            "singing voice synthesis"
        ],
        "content": "SINGER IDENTITY REPRESENTATION LEARNING USING\nSELF-SUPERVISED TECHNIQUES\nBernardo Torres∗1Stefan Lattner2Gaël Richard1\n1LTCI, Telecom Paris, Institut Polytechnique de Paris\n2Sony Computer Science Laboratories Paris\nbernardo.torres@telecom-paris.fr\nABSTRACT\nSigniﬁcant strides have been made in creating voice\nidentity representations using speech data. However, the\nsame level of progress has not been achieved for singing\nvoices. To bridge this gap, we suggest a framework for\ntraining singer identity encoders to extract representations\nsuitable for various singing-related tasks, such as singing\nvoice similarity and synthesis. We explore different self-\nsupervised learning techniques on a large collection of iso-\nlated vocal tracks and apply data augmentations during\ntraining to ensure that the representations are invariant to\npitch and content variations. We evaluate the quality of\nthe resulting representations on singer similarity and iden-\ntiﬁcation tasks across multiple datasets, with a particular\nemphasis on out-of-domain generalization. Our proposed\nframework produces high-quality embeddings that outper-\nform both speaker veriﬁcation and wav2vec 2.0 pre-trained\nbaselines on singing voice while operating at 44.1 kHz. We\nrelease our code and trained models to facilitate further re-\nsearch on singing voice and related areas.\n1. INTRODUCTION\nSinger representation learning is a complex task in Music\nInformation Retrieval (MIR) that involves extracting a rep-\nresentation of a singer’s voice, capturing their unique iden-\ntity or vocal timbre. This task is closely related to singer\nrecognition, which comprises two major tasks: singer\nidentiﬁcation and singer veriﬁcation. The ﬁrst aims to\ndetermine the singer of a given song from a ﬁxed set of\nsingers in the dataset, while the latter aims to determine\nif two audio excerpts come from the same singer or not.\nSinger representation learning has many potential appli-\ncations, including retrieval tasks (such as retrieving songs\nwith a similar singing voice), and providing singer embed-\ndings for conditioning Singing V oice Synthesis (SVS) [1]\nand Singing V oice Conversion (SVC) systems [2].\n*Work mostly conducted during an internship at Sony CSL Paris\n© B, Torres, S. Lattner, and G. Richard. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: B, Torres, S. Lattner, and G. Richard, “Singer Identity\nRepresentation Learning using Self-Supervised Techniques”, in Proc. of\nthe 24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.Singer recognition is related to speaker recognition, a\nwell-established domain with vast literature. Historically,\nit has received a much greater interest in particular due to\nthe need for authentication by voice in many telecommu-\nnications applications. Singing voice, however, is different\nfrom speech in several ways, typically containing a wider\nvariance of phoneme duration, utterances, and a wider\npitch range, which makes singer recognition more chal-\nlenging. Moreover, the lack of large labeled datasets fur-\nther restricts the development of data-driven approaches.\nIn this study, we investigate if speaker recognition mod-\nels trained on labeled speech data can be applied to singing\nvoice, and whether self-supervised learning (SSL) mod-\nels trained on singing voice data can achieve comparable\nperformance. We compare different self-supervised tech-\nniques, including SimCLR [3], Uniformity-Alignment [4],\nVICReg [5] and BYOL [6], trained on a large collection\nof isolated vocal tracks. We also explore high-frequency\nregions that are traditionally ignored in speech [7, 8] but\nmight be present in singing voice by working in 44.1 kHz\nsampling rate. Finally, we evaluate the generalization ca-\npabilities of our models on out-of-domain data.\nOur main contributions are as follows: 1. We perform\nsinger representation learning experiments using self-su-\npervised techniques, an area that few works have explored.\n2. We train encoders that operate at 44.1 kHz on a large\ndataset of singing voice recordings. 3. We conduct an ex-\ntensive evaluation of the obtained embeddings for singer\nidentiﬁcation and singer similarity tasks, comparing them\nwith publicly available pre-trained speech baselines. 4. We\nmeasure the out-of-domain generalization capabilities of\nour models on four public datasets.\n2. RELATED WORK\nSinger recognition has traditionally relied on acoustic fea-\ntures such as Mel-frequency cepstral coefﬁcients (MFCCs)\nor Line Spectral Frequencies (LSFs) to capture timbre\n[9–11]. Some approaches focus on singer identiﬁcation\non polyphonic music [12,13], while others separate vocals\nfrom background [14, 15]. In speaker veriﬁcation litera-\nture, time-invariant embeddings such as i-vector [16] or x-\nvector [17] have been extensively used, and the domain has\nshifted towards data-driven approaches using deep neu-\nral networks to encode acoustic features into a lower-\ndimensional representation that captures speaker charac-448teristics. Temporal aggregation is used to remove the time\ndimension, and these systems are usually optimized us-\ning speaker label infomation for classiﬁcation or metric\nlearning losses. Recent works have also explored SSL for\nspeaker veriﬁcation [18–21].\nSSL has been successful in many domains, particularly\nwith approaches such as SimCLR [3], MoCo [22], CPC\n[23], and BYOL [6]. In the audio domain, following the\nsuccess in Computer Vision and Natural Language Pro-\ncessing (NLP), successful SSL models for speech include\nWav2Vec 2.0 [24], HuBERT [25], and WavLM [26]. SSL\nhas also been successful in learning general-purpose au-\ndio representations, with examples like COLA [27], CLAR\n[28], and CLMR [29].\nWhile the idea of ﬁnding singer embeddings using con-\ntrastive approaches is not new [30], to the best of our\nknowledge, only one work has employed SSL for singer\nrepresentations [31]. In their work, contrastive learning is\nused to acquire feature embeddings of singing voices using\ndata augmentations that disturb a singer’s identity to make\nthe embeddings more attentive to timbre or technique. In\ncontrast, our work explores different SSL techniques, fo-\ncuses on out-of-domain testing, and evaluates on singer\nsimilarity as well as singer identiﬁcation.\n3. METHOD\n3.1 Goal\nOur objective is to obtain, from isolated vocal tracks,\nunique singer representations that capture the timbre of the\nsinger’s voice. These representations must satisfy three cri-\nteria: (I) clips from the same singer should have a higher\naverage similarity than clips from different singers; (II)\nthe representation should not be dependent on fundamental\nfrequency or linguistic content variations; and (III) the rep-\nresentations should generalize well to out-of-domain data.\n3.2 Overview\nThe ideal embedding space for singer representations\nshould cluster elements of the same singer while also en-\nsuring semantic consistency by placing similar voice tim-\nbres close to each other within the space [4]. In line with\nthe criteria outlined in Section 3.1, we conducted experi-\nments with various self-supervised techniques which force\nembeddings of similar input data to be close in the em-\nbedding space. We experimented with four frameworks:\nSimCLR [3], Uniformity-alignment [4], VICReg [5], and\nBYOL [6]. Although these frameworks share a common\ngoal, they differ in their approach (see Section 3.3). We\ntook great care in selecting appropriate data augmentations\nand used a diverse set of singing voice training data. In\nthe current section, we describe the general training frame-\nwork common to all our self-supervised experiments.\nData sampling : In our methodology, we use a COLA\n[27] approach to train our models by sampling audio seg-\nments on the ﬂy, from a randomly drawn audio clip com-\ning from a large database. We ﬁrst extract two segments\n(x,x′)∈RNcropped randomly from the audio clip,called the anchor and positive segment. We obtain aug-\nmented views of both audio segments of the positive pair\nvia a data augmentation module Aug (·)that operates in the\nwaveform domain, resulting in an augmented positive pair\n(x(1),x′(2)). We repeat this process Btimes for a batch\nsize ofB, obtaining a positive pair batch (x(1),x(2)), with\nno repetition of audio clips during a training epoch. The\nsuperscript′is further omitted for simplicity.\nModel : Our proposed model takes raw audio wave-\nforms sampled at 44.1 kHz as input. Firstly, we compute\nlog-compressed mel-spectrogram features m∈RF×Lon\nthe ﬂy using the nnAudio library1. Next, the encoder mod-\nuleg(·)maps the extracted mel-spectrograms to a latent\nrepresentation h′=g(m)∈RH×L. At this stage, adaptive\naverage pooling is used to aggregate embedding vectors h′\ninto time-invariant feature embeddings h∈RH. A pro-\njection layer p(·)mapshinto a lower dimensional latent\nspacez=p(h)∈RDusing a shallow neural network.\nWe denote the full model f(·)by stacking the acoustic\nfeature extraction, encoder, and projection modules. Dur-\ning training, we encode the training batch and obtain pro-\njectionsz=f(x). After training is completed, we discard\nthe projection layer and use only the feature embeddings h.\nThe similarity between a pair of embeddings is computed\nusing the cosine similarity.\nAlthough there are many specialized speaker veriﬁca-\ntion architectures in the speech domain [32, 33], we use\nthe EfﬁcientNet-B0 [34] architecture as the backbone for\nthe encoder module and a single SiLU non-linearity fol-\nlowed by a fully-connected layer for the projection layer.\nThe projections are ℓ2normalized.\n3.3 Self-supervised frameworks\nThe core concept of all used approaches is to leverage\nbig amounts of unlabeled data to build a good represen-\ntation space by aligning similar elements (and possibly\nseparating dissimilar ones). At training time, model f(·)\nacts in a Siamese setup by encoding both elements of\nthe augmented pair z(1)=f(x(1))andz(2)=f(x(2)).\nFor BYOL, we have a separate encoder f′with the same\narchitecture as fand we compute z(1)=f(x(1))and\nz(2)=f′(x(2)). For all setups, we compute a loss function\non the batch projections L(z(1),z(2)).\nContrastive Learning : We employ the contrastive loss\ncalled NT-Xent from SimCLR [3]. The loss maximizes the\nagreement between positive samples and pushes all other\nembeddings of the batch (the negative parts) away in the\nrepresentation space. It does so by maximizing the co-\nsine similarity (sim) between positive samples and mini-\nmizing the sum of similarities for all other pairs formed in\nthe batch:\nLcont(z) =−/summationdisplay\nilogexp(sim(z(1)\ni,z(2)\ni)/τ)\n/summationtext\nj̸=iexp(sim(z(1)\ni,z(2)\nj)/τ).(1)\nWe decouple the term exp (sim(z(1)\ni,z(2)\ni)/τ)from the\n1https://github.com/KinWaiCheuk/nnAudioProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n449denominator of the original NT-Xent [3], which has been\nshown to make the SSL task easier for smaller batch sizes\nand less sensitive to the hyperparameter τ[35].\nUniformity-Alignment : Proposed in [4], Uniformity-\nAlignment aims to align similar examples and distribute\nelements uniformly in an ℓ2normalized embedding space.\nInstead of using a contrastive loss, the authors propose op-\ntimizing directly for these two properties, resulting in two\nloss functions: alignment ( Lalign) and uniformity ( Lunif).\nLalign(z(1),z(2)) =1\nN/summationdisplay\ni/vextenddouble/vextenddouble/vextenddoublez(1)\ni−z(2)\ni/vextenddouble/vextenddouble/vextenddouble2\n, (2)\nLk\nunif(z(k)) = log1\nN/summationdisplay\ni,j/parenleftBig\nexp(−t∥z(k)\ni−z(k)\nj∥2)/parenrightBig\n,\n(3)\nwheret= 2andLunif=/summationtext\nk=1,2Lk\nunif/2.\nVICReg : VICReg [5] is an approach that attempts to\nmaximize the information content of the learned embed-\ndings. Three losses are proposed: the variance, invariance,\nand covariance losses. The invariance loss is the same as\nthe alignment loss (see Equation 2). The variance regu-\nlarization forces the standard deviation of a batch (in the\ndimension axis) to be close to the value µ, preventing col-\nlapse (when embedding dimensions become useless). Let\ndj(z)∈RBbe the vector composed of the values of a\nbatchzat dimension j. The variance regularization is:\nLvar(z) =1\nDD/summationdisplay\nj=1max(0,µ−S(dj(z),ϵ)),(4)\nwhere D is the number of dimensions of zi, andSis the\nregularized standard deviation S(x,ϵ) =/radicalbig\nVar(x)+ϵ.\nThe covariance regularization decorrelates the dimen-\nsions of the embedding, making them orthogonal:\nLcov(z) =1\nDz/summationdisplay\ni̸=j(C(z))2\ni,j, (5)\nwhereC(z) =1\nN−1/summationtextN\ni=1(zi−¯z)(zi−¯z)Tis the co-\nvariance matrix of z, and¯z=1\nN/summationtextN\ni=1zi.\nBYOL : Bootstrap Your Own Latent (BYOL) [6] em-\nploys two neural networks: the online and target networks.\nBoth networks share the same architecture. In addition,\nBYOL employs an additional predictor network qwhich\ncomputes predictions q(z). BYOL iteratively reﬁnes the\nrepresentation of the online network by minimizing the\nmean squared error (MSE) between its predictions and the\ntarget’s projections. If fandf′denote the online and tar-\nget networks, respectively, the loss function LBYOL on the\nprojections z(1)=f(x(1)),z(2)=f′(x(2))is:\nLBYOL(z(1),z(2)) =1\nN/summationdisplay\ni/vextenddouble/vextenddouble/vextenddoublez(1)\ni−q(z(2)\ni)/vextenddouble/vextenddouble/vextenddouble2\n.(6)\nThe target network f′is not trained using directly the\ngradients of LBYOL , but it is updated with an exponential\nmoving average of the weights of the online network.Corpus Language #Hours #Singers Type\nVCTK [36] English 44 110 Speech\nNUS-48E [37] English 1.91 12 Speech/Singing\nV ocalSet [38] English 10.1 20 Singing\nM4Singer [39] Chinese 29.77 20 Singing\nTable 1 : Out-of-domain datasets used for testing.\n4. EXPERIMENTS\n4.1 Data\nWe used a large private corpus of professionally recorded\nsinging voice data containing approximately 25,000 tracks,\ntotaling 940 hours of audio data. The dataset consists of\nisolated vocals of re-recordings of popular songs by 5,700\nartists and includes a variety of singing styles, voice types,\nlyrics, and audio effects. We note that the actual number of\nsingers is unknown, as the same artist might have been re-\nrecorded by multiple singers. Therefore, we do not believe\nthat this corpus is appropriate for supervised training. Ad-\nditionally, we added 6 hours of source-separated vocals to\nthe corpus. All samples were converted to mono 44.1kHz\ntracks with 16-bit encoding, and any silence lasting more\nthan 1.3 seconds was trimmed to 1.3 seconds. Segments\nwith less than 0.5% amplitude were considered silent, and\nsegments with more than 0.5% amplitude lasting less than\n0.2 seconds were silenced. The dataset was partitioned into\nthree distinct sets with ratios of 80% for training, 10% for\nvalidation, and 10% for testing, with no artist allocated to\nmore than one set. The length of a track is typically a few\nminutes.\nOut-of-domain evaluation : Four datasets are used to\ntest the out-of-domain generalization of the models. The\nsummary of all datasets is shown in Table 1.\n4.2 Experiment setup\nWe perform a series of experiments to determine the best\nSSL framework for singer representation learning:\n•CONT : We train a model on the decoupled version of\nthe contrastive loss L=Lcont[35].\n•CONT-VC : We train a model using Lcont(contrastive\nloss) with added variance and covariance regulariza-\ntionL=Lcont+µLvar+νLcov[40].\n•UNIF : We train a model using the uniformity- align-\nment loss L=Lalign+γLunif[4].\n•VICReg : We train a model using the VICReg loss\nL=λLalign+µLvar+νLcov[5] .\n•BYOL : We train a model on BYOL conﬁguration,\noptimizing the MSE L=LBYOL [6].\nThe contrastive loss has been shown to yield good re-\nsults in the literature [27, 31], but there is concern that\nit may break the semantic structure of the embeddings by\npushing similar singers away in the representation space\n[4]. In the CONT-VC approach, the addition of varianceProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n450and covariance losses from VICReg is tested as a regular-\nization method to mitigate this problem [21,40]. The UNIF\napproach attempts to optimize directly for uniformity of\nthe space, which has shown links with linear separabil-\nity [4] and potential for strong singer identiﬁcation results.\nWhileVICReg claims to be an information-theoretic ap-\nproach to general-purpose representation learning, it has\nnot yet been thoroughly tested in the audio domain. Fi-\nnally,BYOL is included in the study for comparison as it\nhas shown promising results in several audio downstream\ntasks, claiming state-of-the-art [41].\n4.3 Evaluation procedure\nThe models are ﬁrst trained until the validation loss stops\ndecreasing. Validation similarity metrics (Section 4.3.1)\nare tracked during training, and the best-performing model\nis selected. This model is evaluated on the test and on\nout-of-domain sets using cropped 4-second clips of singer\nrecordings (with no overlapping segments). The embed-\ndingshare evaluated in two tasks: singer/speech similarity\nand singer/speech identiﬁcation. For simplicity, we refer\nto singer similarity/identiﬁcation even when dealing with\nspeech data such as with VCTK/NUS-48E datasets (see\nSection 5 for details).\n4.3.1 Singer similarity\nWe evaluate singer similarity by measuring two metrics di-\nrectly on the singer feature embeddings h: the Equal Error\nRate (EER) and Mean Normalized Rank (MNR). The EER\nrelates to singer veriﬁcation. On the other hand, we relate\nthe MNR to singer retrieval by computing the similarities\nbetween a query excerpt and a set of candidates. No train-\ning is performed for the similarity evaluation.\nEER : The EER is a popular metric for veriﬁcation sys-\ntems. To compute the EER, the system is exposed to a\nset of trials consisting of true pairs (two segments coming\nfrom the same singer) and fake pairs (two segments coming\nfrom different singers), and a similarity metric is computed\nfor both cases (in our case the cosine similarity). False pos-\nitives (FP) and False Negatives (FN) can be computed by\napplying a threshold τon the similarity metric, and the De-\ntection Error Tradeoff (DET) is obtained by varying τas a\nfunction of FP and FN. The EER is the error rate at which\nFP = FN. We compute the EER following the implemen-\ntation available as part of the SupERB benchmark [42]2.\nWe sample 50,000 speaker pairs for computing the EER on\nthe test set and 20,000 speaker pairs for out-of-domain.\nMNR : Denoteq(1),q(2)two query audio samples, com-\ning from the same audio recording (and therefore the same\nsinger) drawn at random at each trial. Let Sbe a set of\nNaudio samples, drawn at random from a dataset, and\nq(2)∈S. The MNR is [40]:\nMNR=1\nKK/summationdisplay\nk=1R(q(1)\nk,Sk)\nN, (7)\n2https://github.com/s3prl/s3pModel #Params SR Dim. Backbone\nGE2E [43]31.4M 16 256 LSTM\nF-ResNet [33]41.4M 16 512 ResNet-34\nH/ASP [44]48.0M 16 512 ResNet-34\nWav2Vec-base [24]595M 16 12X768 Wav2Vec 2.0\nXLSR-53 [45]6300M 16 24X1024 Wav2Vec 2.0\nOurs 5.0M 44.1 1000 EfﬁcientNet-B0\nTable 2 : Number of network parameters, sampling rate in\nkHz (SR), the size of the feature embeddings (Dim), and\nthe architecture backbone for the baselines and our models.\nwhereR(q(1)\nk,Sk)is the integer position (rank) of q(2)in\nthe sorted list of cosine similarities between q(1)and the\nsamples in S. We perform K= 1000 trials forN= 512 .\nInput sample rate : To ensure a fair comparison with\nthe baselines, which operate on 16 kHz, the evaluation is\ndone in two scenarios: at 16 kHz and 44.1 kHz. In the\nformer, the 44.1 kHz inputs are downsampled to 16 kHz\nand upsampled back to 44.1 kHz before being fed to the\nmodels, removing energy above 8 kHz. In the latter, the\ntrained models have access to the full frequency range of\nthe input data.\n4.3.2 Singer identiﬁcation\nTo evaluate the linear separability of singer classes, we per-\nform singer classiﬁcation as a downstream task for singer\nidentiﬁcation on out-of-domain evaluations. We use 5-fold\ncross-validation to split the audio ﬁles of each singer into\ntrain, validation, and test subsets (4-fold for NUS-48E).\nA single feed-forward linear layer is trained with cross-\nentropy loss on the train subset to predict singer classes\nfrom embeddings extracted from frozen models. The best\nmodel is selected on the validation subset. Average metrics\non the test set over all folds are reported. We limit this task\nto out-of-domain evaluations since these datasets contain\nmultiple ﬁles per singer and the classes are balanced.\n4.4 Baselines\nIn our experiments, we use as baselines three speaker\nveriﬁcation networks: GE2E [43], Fast-ResNet34 [33]\n(hereafter referred to as F-ResNet ),H/ASP [44];\nand two large general purpose self-supervised models\nWav2Vec-base [24], andXLSR-53 [45]. These models\nhave been pre-trained on speech and either achieved state-\nof-the-art results or have been used for obtaining speaker\nrepresentations for speech/singing voice synthesis tasks\nwhile being publicly available. We provide an overview\nof the baseline models in Table 2.\nSince all baselines operate on 16kHz, we down-\nsample the test signals to 16kHz accordingly. For\nWav2Vec-base andXLSR-53 , we use adaptive aver-\nage pooling as the temporal aggregation method for the\nframe-wise feature embeddings, and we employ a learned,\nweighted sum of the ﬁrst three layers for the downstream\n3https://github.com/resemble-ai/Resemblyzer\n4https://github.com/clovaai/voxceleb_trainer\n5https://huggingface.co/facebook/wav2vec2-base\n6https://huggingface.co/facebook/wav2vec2-large-xlsr-53Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n451In-domain Out-of-domain\nTest dataset* VCTK NUS-48E M4Singer* Vocalset*\nModel EER MNR EER MNR EER MNR EER MNR EER MNR\n16 44.1 16 44.1 16 44.1 16 44.1 16 44.1 16 44.1 16 44.1 16 44.1 16 44.1 16 44.1\nGE2E†27.24 - 18.9 - 13.42 - 5.41 - 28.04 - 18.99 - 25.01 - 15.99 - 40.45 - 35.34 -\nF-ResNet†15.21 -7.76 - 1.01 - 0.08⋆- 15.36 - 6.63 - 14.21 - 5.98 - 40.64 - 33.82 -\nH/ASP†12.36 - 5.82 - 0.28⋆- 0.08⋆- 13.99 - 5.42 - 12.31 - 3.93 - 36.27 - 30.79 -\nWav2Vec-base 25.36 - 14.78 - 23.15 - 15.78 - 32.65 - 24.39 - 26.28 - 13.37 - 39.34 - 34.23 -\nXLSR-53 25.22 - 15.82 - 25.93 - 19.95 - 36.62 - 28.52 - 26.02 - 16.96 - 40.09 - 35.32 -\nVICReg 8.19 3.88 2.29 1.14 25.17 23.88 14.99 14.62 26.11 26.06 15.43 15.34 24.6 22.05 9.78 8.69 34.58 33.12 28.21 26.5\nUNIF 9.48 2.86 2.13 0.78 22.51 24.28 12.99 14.67 27.65 26.12 17.08 15.48 20.46 17.03 8.83 6.67 32.4 31.19 25.07 23.19\nCONT 6.39 2.16 1.33 0.48 20.04 22.87 9.34 11.56 23.67 24.51 12.86 12.45 14.28 12.67 5.52 4.51 32.16 30.61 23.64 22.6\nCONT-VC 7.39 2.74 1.61 0.52 19.92 21.79 10.35 11.12 24.99 25.4 15.06 13.91 15.97 12.68 6.94 4.81 31.03 29.74 22.65 21.87\nBYOL 5.88 3.82 1.5 0.68 17.44 19.97 7.8 9.73 26.01 23.9 15.62 12.21 15.65 12.28 5.86 3.77 31.59 29.76 23.93 21.25\nTable 3 : EER and MNR (%, lower is better) measured on frozen model embeddings. Datasets that contain only singing\nvoice are marked with *, and models which are not self-supervised are indicated with †. Results in bold are the best among\nall models, for both 44.1 kHz and 16 kHz input sample rates. Underlined results highlight the best on 16kHz input only.\nForWav2Vec-base andXLSR-53 , we use the embeddings of the ﬁrst layer and aggregate them using average pooling.\nclassiﬁer [42]. We empirically found that this approach\nboosts classiﬁcation performance compared to using a sin-\ngle layer. Speciﬁcally, the ﬁrst layers of these models are\nmore effective for speaker veriﬁcation [26] and are more\ncorrelated with speaker characteristics [46]. For singer\nsimilarity evaluations, we use only the ﬁrst layer, as there\nis no training involved to yield weights for a weighted sum.\n4.5 Training\nTo train our models, we used 4-second audio clips\nthat were normalized, augmented, and converted to log-\ncompressed mel-ﬁlterbanks with 80mel bins, a window\nlength of 2048 , and a hop size of 512. This results in\nan FFT frame of 46.4ms and sliding windows of 11.6ms\nfor 44.1 kHz audio. We initialized the EfﬁcientNet-B0\nbackbone with pre-trained weights on ImageNet [40] and\nused the ADAM optimizer with a learning rate of 1e-4 and\nweight decay of 1e-5, with a batch size of 120. For con-\ntrastive loss, we used a temperature parameter of τ= 0.2\n[4], and whenever we used covariance regularization, we\nsetν= 100 . For variance regularization, we set µ= 25 .\nAdditionally, for VICReg experiments, we used an invari-\nance loss factor of λ= 25 , andUNIF , we setγ= 1. For\nBYOL , we used a learning rate of 3e-5, a weight decay of\n1.5e-6 and an initial moving average value τof 0.99. We\nfound through empirical analysis that these hyperparame-\nters were effective for convergence and avoiding collapse.\nIn terms of data augmentation techniques, we applied\nGaussian noise, gain with a minimum attenuation of -6\ndB, and time masking with at most 1/8 of the clip be-\ning masked. We also used formant-preserving pitch shift-\ning with Praat [47, 48] as a method of data augmenta-\ntion, with the pitch shift ratio and pitch range ratio being\nsampled uniformly from U(1,3) and U(1,1.5), respectively,\nwith a random choice on whether to take the reciprocal\nof the sampled ratios or not [46]. All augmentations had\na probability of 0.5 of being applied. We avoided using\nnaive pitch-shifting techniques that transpose the formants,\nwhich can signiﬁcantly alter the singers’ timbre.Model VCTK NUS-48E M4Singer *Vocalset *\nGE2E†97.01 91.13 88.72 45.66\nF-ResNet†99.91 97.36 94.51 49.52\nH/ASP†99.93 98.32 97.87 74.65\nWav2Vec-base 98.70 96.16 96.52 79.19\nXLSR-53 99.66 97.02 98.62 86.05\nVICReg 52.52 78.98 87.34 49.69\nUNIF 74.43 93.05 93.55 67.52\nCONT 90.24 96.23 95.72 77.42\nCONT-VC 86.03 95.14 94.69 75.20\nBYOL 96.95 96.56 97.00 81.01\nTable 4 : Average linear classiﬁcation accuracy on out-of-\ndomain data (%) over K-fold cross-validation. Datasets\nthat contain only singing voice are marked with *. The\nbest scores are highlighted in bold and the best among the\ntrained models (bottom 5 rows) are underlined. Models\nwhich are not self-supervised are indicated with †.\n5. RESULTS AND DISCUSSION\nTable 3 presents the results of singer similarity evaluation\non both in-domain and out-of-domain test sets, reporting\nthe best Equal Error Rate (EER) and Mean Normalized\nRank (MNR) for trained models and baselines in all test\ndatasets. Table 4 shows the accuracies for downstream\nsinger identiﬁcation task on out-of-domain datasets. We\nalso share in supplementary material additional qualitative\nvisual evaluations of the embeddings7, and release code\nand models to encourage reproducibility and facilitate its\nuse in future projects8.\n5.1 Results on pre-trained models on speech\nThe results indicate that models pre-trained on speech in\na supervised manner (using speaker labels) exhibit good\ngeneralization to out-of-domain speech datasets. H/ASP\nachieves an impressive 0.28% EER on the VCTK, and all\nmodels score higher than 88% accuracy on VCTK, NUS-\n7https://sites.google.com/view/singer-representation-learning\n8https://github.com/SonyCSLParis/ssl-singer-identityProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n45248E, and M4Singer datasets. Their similarity performance\non singing voice datasets, however, is much worse than on\nspeech, but the best models still score below 10% EER on\nNUS-48E and 12.31% and 14.21% EER on M4Singer for\nH/ASP andF-ResNet , respectively.\nThis suggests that important features of the singing\nvoice can also be learned directly from speech. However,\nthe results show the pre-trained models perform worse on\nheavily processed data that includes uncommon effects and\nvocal techniques. This is evident, in particular, in the last\ncolumns of Tables 3 and 4 (V ocalSet), with all baselines\nscoring around 40% EER and from 20% to 40% worse ac-\ncuracy when compared to the other datasets.\n5.2 Results on Self-Supervised Models\nModels trained with contrastive loss ( CONT and\nCONT-VC) achieved the best EER and MNR on the\ntest set. These models were able to learn highly discrimi-\nnative features for the task of in-domain singer similarity.\nFor instance, the CONT model had the lowest overall EER\nand MNR (2.16% and 0.48% respectively) on the test set.\nIt can also be seen in Table 3 that in-domain test per-\nformance did not necessarily translate to good generaliza-\ntion to out-of-domain data. By adding variance and covari-\nance regularizations ( CONT-VC ), the model achieved bet-\nter generalization to out-of-domain data on some datasets\n(such as the V ocalSet, with approximately 1% EER dif-\nference). However, in the VICReg scenario, which has\nboth regularizations but lacks the contrastive part, the re-\nsults were worse. In fact, VICReg had the worst overall\nresults of all the tested self-supervised frameworks. UNIF ,\nwhile better than VICReg , also performed worse on aver-\nage when compared to the other approaches.\nCONT andBYOL achieved the best accuracy over all our\ntrained models on singer identiﬁcation (Table 4), achieving\nthe highest scores of 77.42% and 81.01%, respectively (the\nV ocalSet paper [38] reports 60-70% accuracy on a super-\nvised singer identiﬁcation task).\nBYOL achieved the best generalization on similarity,\nperforming best on out-of-domain data, even though its\nscores were worse on the in-domain test set. Interest-\ningly, of all explored self-supervised techniques, BYOL\nis the only one that does not explicitly force any kind of\nfeature distribution on the embedding space. In addition,\nBYOL was able to learn best how to leverage the informa-\ntion present at 16 kHz sample rate, with an EER of 5.88\non the test set. It also performed best on out-of-domain\nspeech data (VCTK). In general, our models struggled with\nspeech, performing generally better when they only had\naccess to a reduced frequency band. This suggests that\nin speech, high-frequency information the models rely on\nhinders their ability to generalize.\n44.1 vs 16 kHz : Using 44.1 kHz inputs consistently im-\nproved the similarity results on singing voice datasets (e.g.,\nM4Singer) for all models, highlighting the models’ ability\nto efﬁciently use high-frequency information. Moreover,\nmost models showed a marked decline in the in-domain\ndataset results when tested with 16 kHz inputs (the CONTmodel, for example, shows a drop from 2.16% EER to\n6.39% EER). While the 16 kHz inputs could be consid-\nered out-of-domain, this effect shows that high-frequency\ninformation is important for the trained models to achieve\nbetter performance.\nComparison to baselines : The trained models show\nbetter results than baselines on in-domain test sets and\nthe V ocalSet dataset for singer similarity tests, although\nthey fall behind F-ResNet andH/ASP on the mixed\nspeech/singing dataset NUS-48E and VCTK. Nonetheless,\non M4Singer, some self-supervised models outperformed\nthe supervised baselines, with BYOL showing the best per-\nformance (12.28% EER and 3.77% MNR), and CONT and\nCONT-VC also being superior to F-ResNet .\nThe trained models have substantially better singer\nsimilarity results compared to Wav2Vec-base and\nXLSR-53 . These results indicate the potential of training\nmodels on the proposed SSL tasks speciﬁcally on singing\nvoice data. Further improvements could be made by ﬁne-\ntuning the embeddings on veriﬁcation tasks, as has been\ndemonstrated in previous work on Wav2Vec 2.0 [49].\nMoreover, BYOL outperformed Wav2Vec-base for\nboth V ocalSet and M4Singer on classiﬁcation. Among all\nmodels,XLSR-53 achieved the best overall performance\nfor singer identiﬁcation of singing voice. However, is note-\nworthy that our models have signiﬁcantly fewer parameters\nthan the self-supervised Wav2Vec-base (19 times less)\nandXLSR-53 (63 times less).\n6. CONCLUSION\nIn conclusion, we have shown that self-supervised learn-\ning is an effective approach for learning representations\nof singers. The self-supervised models trained on a large\ncorpus of singing voice data demonstrated a performance\nthat either matched or surpassed publicly available super-\nvised speech models, without resorting to specialized ar-\nchitecture designs. Additionally, our models outperformed\ngeneral-purpose self-supervised counterparts even with a\nsigniﬁcantly reduced parameter count. When applied to\nsinger identiﬁcation, our models exhibited superior per-\nformance over Wav2vec-base on singing voice datasets\nbut fell somewhat short in comparison to the considerably\nmore expansive XLSR-53 .\nFurthermore, our results suggest that these models hold\npromise for singer identiﬁcation and similarity down-\nstream tasks. BYOL showed the most promise for gen-\neralizing to out-of-domain data, while the contrastive ap-\nproaches were more effective for in-domain data.\nHowever, we note that our models’ representations do\nnot yet fully capture a singer’s identity when confronted\nwith unique singing techniques, such as those found in the\nV ocalSet [38]. This underscores the need for further re-\nsearch on robust SSL frameworks capable of accommodat-\ning such variations. Our ﬁndings also suggest that employ-\ning a higher sampling frequency can be advantageous for\nsinging voice tasks, but optimal frequency for generalizing\nto both singing and speech tasks remains to be determined.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4537. ACKNOWLEDGMENTS\nThis work was partly funded by the European Union (ERC,\nHI-Audio, 101052978). Views and opinions expressed are\nhowever those of the author(s) only and do not necessar-\nily reﬂect those of the European Union or the European\nResearch Council. Neither the European Union nor the\ngranting authority can be held responsible for them.\nThe authors also thank Alain Riou for the BYOL imple-\nmentation.\n8. REFERENCES\n[1] S. Wang, J. Liu, Y . Ren, Z. Wang, C. Xu, and\nZ. Zhao, “MR-SVS: Singing voice synthesis with\nmulti-reference encoder,” CoRR , vol. abs/2201.03864,\n2022.\n[2] S. Nercessian, “End-to-End Zero-Shot V oice Conver-\nsion Using a DDSP V ocoder,” in IEEE Workshop on\nApplications of Signal Processing to Audio and Acous-\ntics, Oct. 2021, pp. 1–5.\n[3] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton,\n“A simple framework for contrastive learning of visual\nrepresentations,” in International Conference on Ma-\nchine Learning , 2020, pp. 1597–1607.\n[4] F. Wang and H. Liu, “Understanding the Behaviour\nof Contrastive Loss,” in 2021 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) .\nNashville, TN, USA: IEEE, Jun. 2021, pp. 2495–2504.\n[5] A. Bardes, J. Ponce, and Y . LeCun, “VICReg:\nVariance-invariance-covariance regularization for self-\nsupervised learning,” in ICLR , 2022.\n[6] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H.\nRichemond, E. Buchatskaya, C. Doersch, B. Á. Pires,\nZ. Guo, M. G. Azar, B. Piot, K. Kavukcuoglu,\nR. Munos, and M. Valko, “Bootstrap your own la-\ntent - A new approach to self-supervised learning,” in\nNeurIPS , 2020.\n[7] S. Ternström, “Hi-Fi voice: Observations on the distri-\nbution of energy in the singing voice spectrum above\n5 kHz,” in Acoustics’ 08, Paris, France, Jun 29-Jul 4,\n2008 , 2008, pp. 3171–3176.\n[8] B. B. Monson, E. J. Hunter, A. J. Lotto, and B. H.\nStory, “The perceptual signiﬁcance of high-frequency\nenergy in the human voice,” Frontiers in psychology ,\nvol. 5, p. 587, 2014.\n[9] Md. Sahidullah, S. Chakroborty, and G. Saha, “On the\nuse of perceptual Line Spectral pairs Frequencies and\nhigher-order residual moments for Speaker Identiﬁca-\ntion,” IJBM , vol. 2, no. 4, p. 358, 2010.\n[10] L. Regnier and G. Peeters, “Singer veriﬁcation: Singer\nmodel .vs. song model,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) . Kyoto, Japan: IEEE, 2012, pp. 437–440.[11] T. Nakano, K. Yoshii, and M. Goto, “V ocal timbre anal-\nysis using latent Dirichlet allocation and cross-gender\nvocal timbre similarity,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2014, pp. 5202–5206.\n[12] A. Mesaros, T. Virtanen, and A. Klapuri, “Singer iden-\ntiﬁcation in polyphonic music using vocal separation\nand pattern recognition methods.” in Proc. of the 8th\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2007, pp. 375–378.\n[13] M. Lagrange, A. Ozerov, and E. Vincent, “Robust\nsinger identiﬁcation in polyphonic music using melody\nenhancement and uncertainty-based learning,” in Proc.\nof the 13th International Society for Music Information\nRetrieval Conference (ISMIR) , 2012.\n[14] B. Sharma, R. K. Das, and H. Li, “On the Importance\nof Audio-Source Separation for Singer Identiﬁcation in\nPolyphonic Music,” in Interspeech 2019 . ISCA, Sep.\n2019, pp. 2020–2024.\n[15] T.-H. Hsieh, K.-H. Cheng, Z.-C. Fan, Y .-C. Yang, and\nY .-H. Yang, “Addressing the confounds of accompani-\nments in singer identiﬁcation,” in IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2020, pp. 1–5.\n[16] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and\nP. Ouellet, “Front-end factor analysis for speaker veriﬁ-\ncation,” IEEE Transactions on Audio, Speech, and Lan-\nguage Processing , vol. 19, no. 4, pp. 788–798, 2010.\n[17] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and\nS. Khudanpur, “X-Vectors: Robust DNN Embeddings\nfor Speaker Recognition,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, Apr. 2018, pp. 5329–5333.\n[18] J.-w. Jung, Y . J. Kim, H.-S. Heo, B.-J. Lee, Y . Kwon,\nand J. S. Chung, “Pushing the limits of raw waveform\nspeaker recognition,” in Interspeech . ISCA, 2022, pp.\n2228–2232.\n[19] M. Sang, H. Li, F. Liu, A. O. Arnold, and\nL. Wan, “Self-supervised speaker veriﬁcation with\nsimple siamese network and self-supervised regular-\nization,” in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) . IEEE,\n2022, pp. 6127–6131.\n[20] W. Xia, C. Zhang, C. Weng, M. Yu, and D. Yu, “Self-\nsupervised text-independent speaker veriﬁcation using\nprototypical momentum contrastive learning,” in IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) . IEEE, 2021, pp. 6723–\n6727.\n[21] T. Lepage and R. Dehak, “Label-efﬁcient self-\nsupervised speaker veriﬁcation with information max-\nimization and contrastive learning,” in Proc. Inter-\nspeech 2022 . ISCA, Sep. 2022, pp. 4018–4022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n454[22] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, “Mo-\nmentum contrast for unsupervised visual representa-\ntion learning,” in Proc. of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2020, pp.\n9729–9738.\n[23] A. van den Oord, Y . Li, and O. Vinyals, “Representa-\ntion learning with contrastive predictive coding,” arXiv\npreprint arXiv:1807.03748 , 2018.\n[24] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli,\n“Wav2vec 2.0: A framework for self-supervised learn-\ning of speech representations,” Advances in neural in-\nformation processing systems , vol. 33, pp. 12 449–\n12 460, 2020.\n[25] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia,\nR. Salakhutdinov, and A. Mohamed, “Hubert: Self-\nsupervised speech representation learning by masked\nprediction of hidden units,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing , vol. 29,\npp. 3451–3460, 2021.\n[26] S. Chen, C. Wang, Z. Chen, Y . Wu, S. Liu, Z. Chen,\nJ. Li, N. Kanda, T. Yoshioka, X. Xiao et al. , “Wavlm:\nLarge-scale self-supervised pre-training for full stack\nspeech processing,” IEEE Journal of Selected Topics\nin Signal Processing , vol. 16, no. 6, pp. 1505–1518,\n2022.\n[27] A. Saeed, D. Grangier, and N. Zeghidour, “Contrastive\nlearning of general-purpose audio representations,” in\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2021, pp.\n3875–3879.\n[28] H. Al-Tahan and Y . Mohsenzadeh, “Clar: Contrastive\nlearning of auditory representations,” in International\nConference on Artiﬁcial Intelligence and Statistics ,\n2021, pp. 2530–2538.\n[29] J. Spijkervet and J. A. Burgoyne, “Contrastive learn-\ning of musical representations,” in Proc. of the 22nd\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2021, pp. 673–681.\n[30] C.-i. Wang and G. Tzanetakis, “Singing Style Investi-\ngation by Residual Siamese Convolutional Neural Net-\nworks,” in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , Apr.\n2018, pp. 116–120.\n[31] H. Yakura, K. Watanabe, and M. Goto, “Self-\nSupervised Contrastive Learning for Singing V oices,”\nIEEE/ACM Trans. Audio Speech Lang. Process. ,\nvol. 30, pp. 1614–1623, 2022.\n[32] B. Desplanques, J. Thienpondt, and K. Demuynck,\n“ECAPA-TDNN: Emphasized channel attention, prop-\nagation and aggregation in TDNN based speaker veri-\nﬁcation,” in Interspeech . ISCA, 2020, pp. 3830–3834.[33] J. S. Chung, J. Huh, S. Mun, M. Lee, H. S. Heo,\nS. Choe, C. Ham, S. Jung, B.-J. Lee, and I. Han, “In\ndefence of metric learning for speaker recognition,” in\nInterspeech 2020 , Oct. 2020, pp. 2977–2981.\n[34] M. Tan and Q. Le, “Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks,” in Inter-\nnational Conference on Machine Learning , 2019, pp.\n6105–6114.\n[35] C.-H. Yeh, C.-Y . Hong, Y .-C. Hsu, T.-L. Liu, Y . Chen,\nand Y . LeCun, “Decoupled contrastive learning,” in\nECCV (26) , ser. Lecture Notes in Computer Science,\nvol. 13686. Springer, 2022, pp. 668–684.\n[36] J. Yamagishi, C. Veaux, and K. MacDonald, “CSTR\nVCTK Corpus: English Multi-speaker Corpus for\nCSTR V oice Cloning Toolkit (version 0.92),” 2019.\n[37] Z. Duan, H. Fang, B. Li, K. C. Sim, and Y . Wang,\n“The NUS sung and spoken lyrics corpus: A quanti-\ntative comparison of singing and speech,” in APSIPA .\nIEEE, 2013, pp. 1–9.\n[38] J. Wilkins, P. Seetharaman, A. Wahl, and B. Pardo,\n“V ocalSet: A Singing V oice Dataset.” in Proc. of the\n19th International Society for Music Information Re-\ntrieval Conference (ISMIR) , 2018, pp. 468–474.\n[39] L. Zhang, R. Li, S. Wang, L. Deng, J. Liu, Y . Ren,\nJ. He, R. Huang, J. Zhu, X. Chen et al. , “M4Singer:\nA multi-style, multi-singer and musical score provided\nmandarin singing corpus,” Advances in Neural Infor-\nmation Processing Systems , vol. 35, pp. 6914–6926,\n2022.\n[40] S. Lattner, “SampleMatch: Drum sample retrieval by\nmusical context,” in Proc. of the 23rd International So-\nciety for Music Information Retrieval Conference (IS-\nMIR) , 2022, pp. 781–788.\n[41] D. Niizumi, D. Takeuchi, Y . Ohishi, N. Harada, and\nK. Kashino, “BYOL for audio: Exploring pre-trained\ngeneral-purpose audio representations,” IEEE ACM\nTrans. Audio Speech Lang. Process. , vol. 31, pp. 137–\n151, 2023.\n[42] S.-W. Yang, P.-H. Chi, Y .-S. Chuang, C.-I. J. Lai,\nK. Lakhotia, Y . Y . Lin, A. T. Liu, J. Shi, X. Chang,\nG.-T. Lin, T.-H. Huang, W.-C. Tseng, K.-t. Lee, D.-\nR. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe,\nA. Mohamed, and H.-y. Lee, “SUPERB: Speech pro-\ncessing universal PERformance benchmark,” in Inter-\nspeech . ISCA, 2021, pp. 1194–1198.\n[43] L. Wan, Q. Wang, A. Papir, and I. L. Moreno, “Gen-\neralized end-to-end loss for speaker veriﬁcation,” in\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2018, pp.\n4879–4883.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n455[44] Y . Kwon, H.-S. Heo, B.-J. Lee, and J. S. Chung, “The\nins and outs of speaker recognition: Lessons from\nV oxSRC 2020,” in IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2021, pp. 5809–5813.\n[45] A. Conneau, A. Baevski, R. Collobert, A. Mohamed,\nand M. Auli, “Unsupervised cross-lingual representa-\ntion learning for speech recognition,” in Interspeech .\nISCA, 2021, pp. 2426–2430.\n[46] H.-S. Choi, J. Lee, W. Kim, J. Lee, H. Heo, and\nK. Lee, “Neural analysis and synthesis: Reconstruct-\ning speech from self-supervised representations,” Ad-\nvances in Neural Information Processing Systems ,\nvol. 34, pp. 16 251–16 265, 2021.\n[47] P. Boersma and D. Weenink, “Praat: Doing phonetics\nby computer (Version 5.1.13),” 2009.\n[48] Y . Jadoul, B. Thompson, and B. de Boer, “Introducing\nParselmouth: A Python interface to Praat,” Journal of\nPhonetics , vol. 71, pp. 1–15, Nov. 2018.\n[49] Z. Fan, M. Li, S. Zhou, and B. Xu, “Exploring wav2vec\n2.0 on Speaker Veriﬁcation and Language Identiﬁca-\ntion,” in Interspeech 2021 . ISCA, Aug. 2021, pp.\n1509–1513.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n456"
    },
    {
        "title": "A Dataset and Baselines for Measuring and Predicting the Music Piece Memorability.",
        "author": [
            "Li-Yang Tseng",
            "Tzu-Ling Lin",
            "Hong-Han Shuai",
            "Jen-Wei Huang",
            "Wen-Whei Chang"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265251",
        "url": "https://doi.org/10.5281/zenodo.10265251",
        "ee": "https://zenodo.org/records/10265251/files/000019.pdf",
        "abstract": "Nowadays, humans are constantly exposed to music, whether through voluntary streaming services or incidental encounters during commercial breaks. Despite the abundance of music, certain pieces remain more memorable and often gain greater popularity. Inspired by this phenomenon, we focus on measuring and predicting music memorability. To achieve this, we collect a new music piece dataset with reliable memorability labels using a novel interactive experimental procedure. We then train baselines to predict and analyze music memorability, leveraging both interpretable features and audio mel-spectrograms as inputs. To the best of our knowledge, we are the first to explore music memorability using data-driven deep learning-based methods. Through a series of experiments and ablation studies, we demonstrate that while there is room for improvement, predicting music memorability with limited data is possible. Certain intrinsic elements, such as higher valence, arousal, and faster tempo, contribute to memorable music. As prediction techniques continue to evolve, real-life applications like music recommendation systems and music style transfer will undoubtedly benefit from this new area of research.",
        "zenodo_id": 10265251,
        "dblp_key": "conf/ismir/TsengLSHC23",
        "keywords": [
            "memorability",
            "music",
            "memorable",
            "popularity",
            "interactive experimental procedure",
            "data-driven deep learning",
            "predicting music memorability",
            "intrinsic elements",
            "higher valence",
            "arousal"
        ],
        "content": "A DATASET AND BASELINES FOR MEASURING AND PREDICTING THE\nMUSIC PIECE MEMORABILITY\nLi-Yang Tseng Tzu-Ling Lin Hong-Han Shuai\nJen-Wei Huang Wen-Whei Chang\nNational Yang Ming Chiao Tung University, Hsinchu, Taiwan\n{liyangtseng.ee10, tzulinglin.11 ,hhshuai, admsd.ee10, wwchang}@nycu.edu.tw\nABSTRACT\nNowadays, humans are constantly exposed to music,\nwhether through voluntary streaming services or incidental\nencounters during commercial breaks. Despite the abun-\ndance of music, certain pieces remain more memorable\nand often gain greater popularity. Inspired by this phe-\nnomenon, we focus on measuring and predicting music\nmemorability. To achieve this, we collect a new mu-\nsic piece dataset with reliable memorability labels us-\ning a novel interactive experimental procedure. We then\ntrain baselines to predict and analyze music memorabil-\nity, leveraging both interpretable features and audio mel-\nspectrograms as inputs. To the best of our knowledge,\nwe are the ﬁrst to explore music memorability using data-\ndriven deep learning-based methods. Through a series\nof experiments and ablation studies, we demonstrate that\nwhile there is room for improvement, predicting music\nmemorability with limited data is possible. Certain intrin-\nsic elements, such as higher valence, arousal, and faster\ntempo, contribute to memorable music. As prediction tech-\nniques continue to evolve, real-life applications like music\nrecommendation systems and music style transfer will un-\ndoubtedly beneﬁt from this new area of research.\n1. INTRODUCTION\nMusic memorability is essential and has a wide range of\ncommercial applications. For instance, content creators\nand marketing teams can use unique visual aids or au-\ndio components to captivate target audiences and distin-\nguish themselves from other information sources [1, 2].\nSound logos, such as Netﬂix’s iconic “ta-dum,” are de-\nsigned to engage listeners and promote brand recognition.\nIn the realm of cognition literature, numerous studies have\nsought to understand the factors that contribute to music\nmemorability [3–6]. For instance, [5, 6] bridged the gap\nbetween cognitive science and MIR by examining whether\nimplicit or explicit memory for a single tune is impacted by\n© L.-Y . Tseng, T.-L. Lin, H.-H. Shuai, J.-W. Huang, and\nW.-W. Chang. Licensed under a Creative Commons Attribution 4.0 In-\nternational License (CC BY 4.0). Attribution: L.-Y . Tseng, T.-L. Lin,\nH.-H. Shuai, J.-W. Huang, and W.-W. Chang, “A dataset and Baselines\nfor Measuring and Predicting the Music Piece Memorability”, in Proc. of\nthe 24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.the type of encoding task and variations in timbre, tempo\nand structure.\nHowever, music memorability remains a relatively un-\nexplored area, particularly from a data-driven standpoint.\nResearch related to music memorability includes the study\nof involuntary musical imagery (INMI) [7, 8], also known\nas “earworms,” which refers to the phenomenon where\nfragments of music become mentally lodged on repeat. For\ninstance, Jakubowski et al. proposed a model that can de-\ntermine whether a piece of music may induce the INMI ef-\nfect by using statistical analysis and a random forest model\n[8]. However, the mechanism of INMI differs from music\nmemorability since the former is a passive process while\nthe latter can be active, e.g., everyone remembers how to\nsing “Happy Birthday,” but the song may not qualify as\nan earworm. Another line of prior studies [9–12] inves-\ntigating the intrinsic memorability of multimedia content\nhave predominantly focused on computer vision, with their\nﬁndings suggesting that data-driven approaches can effec-\ntively determine memorability levels. Motivated by these\nstudies, we break new ground in exploring music memora-\nbility from a data-driven perspective by compiling a novel\ndataset and employing machine learning techniques.\nSpeciﬁcally, to expand the scope of memorability detec-\ntion and recognition in music information retrieval (MIR),\nwe establish a new research domain called music memora-\nbility regression (MMR), which aims to predict a memo-\nrability score for a given music piece. We create an ex-\nperimental procedure as shown in Figure 1 to collect a\nnew dataset, the YouTube Music Memorability (YTMM)\ndataset, where memorability scores are determined by the\npercentage of participants who can recall the music piece\nafter a certain period. This dataset provides reliable and\nconsistent music memorability scores across all partici-\npants, paving the way for further research in the ﬁeld.\nWe also propose several baseline approaches for predict-\ning music memorability, including feature engineering us-\ning hand-crafted music-related features and transfer learn-\ning techniques. These baselines not only demonstrate the\npotential of machine learning in addressing music memo-\nrability but also serve as a foundation for future work.\nDespite the promise of machine learning in tackling\nmusic memorability by predicting memorability scores, its\n“black box” characteristics hinder the interpretation of ma-\nchine decisions in MIR tasks. A straightforward approach\nwould be to compute correlations without relying on black-174Figure 1 . The music memory game, which allows data annotators to label music memorability scores reliably. The\nexperiment is divided into three stages, each with a 3-minute long break in between. Each 18-minute stage is composed of\nmultiple 5-second music pieces and short breaks.\nbox prediction models to glean insights about the relation-\nship between memorability and musical features. How-\never, given the complexity of analyzing music memorabil-\nity, using a single feature results in an extremely low cor-\nrelation with memorability, leading to inconclusive ﬁnd-\nings. One alternative would be to explore all possible fea-\nture combinations when calculating correlations, but the\nsheer number of combinations, e.g., 220−1for just 20\nfeatures, renders this approach impractical. A/B testing\ncould be used to determine which type of music is more\nmemorable, but it suffers from similar drawbacks, such as\nbeing time-consuming and unable to account for all vari-\nables that may impact the experiment’s outcome. To make\nmachine learning models reveal their “black box” char-\nacteristics, researchers are increasingly adopting explain-\nable artiﬁcial intelligence (XAI) [13] for deeper insights.\nBuilding on previous interpretability analyses in audio pro-\ncessing [14, 15], we utilize Shapley Additive Explanations\n(SHAP) [16], a game-theoretic approach that clariﬁes the\noutput of machine learning models, to identify the key\ncomponents of memorable music.\nOur main contributions are as follows: ﬁrst, we present\nthe new YTMM dataset with objective annotations of\nmemorability scores, which will be publicly available for\nfuture research; second, we propose several deep learn-\ning baseline models for MMR; and ﬁnally, we explore\nthe potential characteristics of memorable music pieces\nwhile providing interpretability for these deep learning-\nbased methods.\n2. RELATED WORK\nIn addition to the cognition literature on music memora-\nbility [5, 6], there are several related yet distinct terms,\nsuch as Involuntary Musical Imagery (INMI) or \"ear-\nworms\"—fragments of music that involuntarily come to\nmind [7]. Studies have examined earworms through inter-\nviews, environmental and psychological conditions lead-\ning to INMI, and the impact of melodic features and song\npopularity on spontaneous musical imagination [8]. Cru-\ncial differences between INMI and music memorability in-\nclude: 1) INMI involves uncontrollable mental repetition,\nwhile memorability requires conscious recall; and 2) the\nstimuli in [8] are highly familiar to participants, whereas\nour study selects audios unfamiliar to most annotators tomitigate the inﬂuence of individual listening histories on\nmemorability. Another related concept is hook catchi-\nness [17–20], which refers to the most easily recalled frag-\nment of a musical piece. However, our focus lies in pre-\ndicting the memorability of different music pieces rather\nthan assessing the impact of various segments within the\nsame tune on catchiness prediction and recognition. Fur-\nthermore, we ensure our stimuli consist solely of pure in-\nstrumental music clips to prevent any textual information\nfrom lyrics inﬂuencing music memorability.\nMoreover, while deep learning has achieved signiﬁcant\nsuccess in supervised MIR tasks, it often demands large-\nscale annotated data. However, collecting useful anno-\ntations for MIR tasks can be costly, as it typically re-\nquires expertise and domain knowledge [21]. To tackle this\nchallenge, various data augmentation and training strate-\ngies have been proposed [21–24]. For instance, McFee\net al. [21] apply transformations such as pitch shifting,\ntime-stretching, and adding background noise to the orig-\ninal waveform. Cubuk et al. [22] mask both time and\nfrequency content to expand the input space in automatic\nspeech recognition (ASR) and MIR tasks. To enhance\nlearning robustness with limited data, Wu et al. [23] ex-\ntract general music representations using a multi-task pre-\ntrained encoder, inspired by speech processing research\n[25, 26]. Similarly, Castellon et al. [24] employ trans-\nfer learning from existing music generation architectures.\nHowever, not all the aforementioned methods are simul-\ntaneously open-source, computationally inexpensive, and\ninterpretable. Therefore, in this paper, we focus on apply-\ning signal processing approaches like masking, with fur-\nther details provided in Section 4.\n3. DATASET CONSTRUCTION FOR MUSIC\nMEMORABILITY\nIn this section, we discuss the details of our dataset collec-\ntion process and how music memorability is measured.\n3.1 Audio Collection\nTo construct a dataset with objective music memorabil-\nity scores, we ﬁrst ensure that the audio samples are un-\nbiased. We randomly select music by querying music-\nrelated videos using the YouTube API with random query\nkeys, avoiding any speciﬁc music genre preference. Addi-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n175Task Type # of Audios # of Repetition (min) # of Repetition (max) # of Repetition (avg) # of Repetition (std)\nFiller 65 - - - -\nVigilance 21 5 10 6.5 1.08\nShort-Term Target 88 10 49 25.23 10.81\nMedium-Term Target 41 61 131 110.33 16.32\nLong-Term Target 20 155 276 222.05 36.64\nTable 1 . Details of different audio tasks in the music memory game.\nFigure 2 . Distributions of the audio published location and\nthe distributions of the audio views in the ﬁnal dataset.\ntionally, we manually ﬁlter the music to conﬁrm that the\nqueried videos contain pure music content, excluding in-\nstrument tutorials or gadget unboxings. Next, we conduct a\npilot study to verify that the selected audios are unfamiliar\nto most of the annotators in our target user group. Consid-\nering the annotators’ nationalities might not be as varied as\nthe music collection’s, and language can be a memorable\nyet non-music-related element, we only use the intro part\nof each song. This approach helps eliminate other potential\nvariables affecting music memorability. Also, the volume\nacross all audio clips is normalized to minimize any mem-\norable attributes unrelated to the music itself. Loudness\nnormalization ensures the music is remembered based on\nits inherent qualities rather than its loudness.\nWe use only a segment of each audio for two reasons: i)\nto better eliminate confounding factors, such as vocal tim-\nbre, and ii) to shorten the period of annotations and prevent\nfatigue. We achieve this by truncating audios into struc-\nturally meaningful segments and applying proper time-\nstretching to alter the duration of an audio signal to a ﬁxed\nlength without distorting the audio. The segmentation pro-\ncess is supervised by an expert with a professional music\neducation background. Note that time-stretching not only\nreduces modeling complexity but also prevents annotators\nfrom memorizing the audio based on its duration.\nUltimately, we collect 235 structurally meaningful 5-\nsecond audios with labeled music memorability scores.\nOur goal is to determine which types of music pieces are\nmore likely to be memorized, rather than focusing on en-\ntire music clips, which are more complex and involve ad-\nditional factors. This research can facilitate various appli-\ncations, such as Netﬂix’s iconic “ta-dum” sound. The col-\nlected data can be found in the supplementary materials.\nFigure 2 illustrates the distribution of the collected audios\nconcerning their published geographical locations and to-\ntal views on YouTube, with view counts ranging from 10K\nto 100M.3.2 The Music Memory Game\nTo annotate the memorability of the collected musical data,\nwe follow the setting of image memory game [9] to de-\nsign a novel music listening experiment. During the ex-\nperiment, the recruited data annotators are asked to listen\nto 235 music pieces in total and answer whether the au-\ndio is repeated in the experiment or not. From a cognitive\nview, we deﬁne music memorability as long-term musical\nsalience and the extent to which a musical piece continues\nto be remembered over time. In the music memory game,\nmusic memorability is measured as the tendency to cor-\nrectly recognize a music piece when encountering it again\nin the experiment among all users. Speciﬁcally, let x(i)\njde-\nnote whether the i-th music piece can be recalled by the\nj-th data annotator, i.e., 1 if the annotator recognized the\ni-th music piece. The memorability score of music i, de-\nnoted bym(i), is then calculated by:\nm(i)=1\nnini/summationdisplay\nj=1x(i)\nj, x(i)\nj∈ {0,1} (1)\nwhereniis the total number of data annotators for the i-th\nmusic.\nTo make the ground truth unbeknownst to all partici-\npants, music excerpts are split into three task categories:\n“vigilance”, “target”, and “ﬁller”. Targets and vigilance\ntargets are both repeated in the experiment, while the for-\nmer are collected to be the true labels and the latter is used\nto make sure participants are attentive when labeling data.\nMoreover, ﬁllers are used to stuff the spacing between the\nﬁrst and second repetition of a target and therefore is only\npresented once. The overview of the music memory game\nexperimenting procedure is shown in Figure 1. The target-\nvigilance-ﬁller split details can be found in Table 1. Rig-\norous criteria are enforced to monitor the performances of\ndata annotators and preserve the quality of memorability\nlabels. Speciﬁcally, annotations from users who detect vig-\nilance repetition with an accuracy lower than 60% are au-\ntomatically discarded. Furthermore, to prevent gathering\nbiased memorability, all annotators only engage in label-\ning once. We recruited a total of 218 users from cam-\npus, with 163 clearing the vigilance accuracy level, 17%\nof passed annotators having professional music education\nbackgrounds, and over 98% being between the ages of 20\nand 29.\nDiffering from previous works on image memorability,\nour experiment is composed of three similar stages with\nbreaks inserted in between. The reasons for using stages\nand breaks are two-fold. First, audios are sequential, there-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n176Figure 3 . Memorability scores at various stages. The color\nsymbolizes the rank of short-term memorability, while the\nlines represent stage relationships. The plot also shows\nSpearman’s rank correlations ρbetween memorabilities\nmeasured at each stage.\nfore it is more exhausting to label the memorability score\nto audios as compared to static images. Second, it usu-\nally takes some time for the earworm phenomenon to hap-\npen when listening to music. Hence, we assume memo-\nrability should be invariant even after encountering breaks\nthat probably would reset the memory. The results of re-\nlations between repeat interval and memorability score are\nshown in Figure 3, where the lines exhibiting memorabil-\nity scores across short-term, medium-term, and long-term\nrepeats. The results manifest that the memorability score\nis indeed independent of the sequential context. Therefore,\nit is easier to memorize truly memorable pieces of music\neven after long breaks. The fact that Spearman’s rank cor-\nrelation [27] between short-term, medium-term, and long-\nterm are all greater than 0.64 also proves that the rank of\nmemorability score is preserved across variant repeat in-\ntervals.\n3.3 Labels and Consistency Analysis\nTo assure that collected labels are universal across all data\nannotators, we evaluate the human consistency according\nto previous work [9] by randomly splitting all participants\ninto 2 groups and examining how well the memorability\nscores measured in the ﬁrst groups matched the ones mea-\nsured in the second group by averaging Spearman’s rank\ncorrelation [27] between randomly separated two halves\nof the participants 25 times. The average Spearman’s rank\ncorrelation coefﬁcient ρis0.83, indicating the consistency\nof the collected data.\nFigure 4 shows the scattering plot of music memorabil-\nity and repeat interval. The graph demonstrates that mu-\nFigure 4 . Relations between memorability score and target\nrepeat interval in log scale. The hue represents the level of\nfatigue.\nsic possesses a linear relation between memorability score\nand log-scaled repeat interval. Please note that the fatigue\nlevel is another factor in the plot that also contributes to\nthe memorability score of audio. The fatigue level, de-\nﬁned as the amount of audios listened without a 3-minute\nbreak, is a direct result caused by staging experiment and\nparticipating in taking a break in the middle since listen-\ning to more music at one time without resting reduces par-\nticipants’ ability to identify repeated music pieces. The\nsetting of inserting audio to random positions in the exper-\niment procedure adds more context diversity to the process\nof memorizing music, thus making the labeled memorabil-\nity scores more robust.\n4. MUSIC MEMORABILITY PREDICTION\n4.1 Learning with Handcrafted Features\nAlthough feature extractions for deep learning models\ncan be data-driven without being handcrafted, leading to\na better result given sufﬁcient training data, handcrafted\nfeatures provide interpretable information for more in-\nsights. Therefore, we propose handcrafted features that\ncan more accurately depict the low-level acoustic features\nor high-level semantic features of musical clips as shown\nin Table 2. For the low-level acoustic features that can\nbe directly derived from the audio signal of music seg-\nments, we leverage the harmony, rhythm and timbre since\nthey are most easily recognizable fragments of a piece of\nmusic [17] and describe the fundamental elements of a\ntune. Moreover, zero crossings and zero crossing rate are\nalso extracted since they give the impressions into the fre-\nquency content of a signal. On the other hand, high-level\nsemantic features are more abstract descriptions. Since the\nprevious works in Psychology [28,29] mention the link be-\ntween music emotion and memory, we introduce valence\nand arousal, which represent the mood of music pieces as\nfeatures.\nAnother high-level feature is genre, which describesProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n177Level Category Feature Implementation\nLow-levelHarmony mean, std of 12 pitch class\nRhythm beat per minute (bpm)\nTimbre mean, std of 4-tracks (V ocals,\nBass, Drums, Others)\nZero Crossing # of zero crossings & avg, me-\ndian of zero crossings rate\nHigh-levelMood valence, arousal\nGenre Music, Musical Instrument\nTable 2 . Explainable handcraft features.\nhow likely a clip is belong to a certain type of music.\nSpeciﬁcally, due to the unstable performance of exist-\ning algorithms for detecting sequences of chord labels,\nwe employ chromagram (chroma) [30] as a representa-\ntion of harmony patterns. To extract timbre informa-\ntion, the Mel-Frequency Cepstral Coefﬁcient (MFCCs) is\nwidely utilized. Although MFCCs is representative for\ntimbre, its components are difﬁcult to grasp intuitively.\nAs a result, we treat MFCCs as a raw feature and ﬁnd\nan alternative solution by ﬁrst separating source audios\ninto four components using source separation software\nSpleeter [31], and calculating their respective amplitudes\nto represent the characteristics of different instruments and\nfrequency ranges. For the rhythmic pattern, although Tem-\npogram [32] captures the underlying rhythmic pattern of\nraw audios, it is unable to provide precise insights to con-\ncretely measure the audio’s groove. Therefore, we instead\nutilize beat per minute (bpm) to represent general rhythm\ncharacteristics. We also use static valence and arousal val-\nues to describe perceived music moods, which are pre-\ndicted by using Support Vector Regression (SVR) with a\nlinear kernel trained on the PMEmo dataset [33]. For genre\nfeatures, we use the predicted music tagging and instru-\nments from the downstream task of PANN [34]. Finally,\nSVR and Multilayer Perceptron (MLP) are employed as\npredictors to link audio features to memorability scores.\n4.2 End-to-End Deep Learning\nDeep learning [35] is featured by its ability to directly learn\nmeaningful information from raw data, as opposed to us-\ning hand-crafted features. As a result, we also test end-\nto-end models to ﬁnd if their feature-learning process im-\nproves performance. Our model uses spectrograms in Mel-\nscale as inputs, similar to previous end-to-end MIR tasks.\nMoreover, transfer learning [36], which applies previously\nlearned knowledge to new data, has been found to signif-\nicantly increase learning performance by skipping costly\ndata-labeling procedures. Here, we use the self-supervised\npre-trained Audio Spectrogram Transformer (SSAST) [37]\nsince SSAST has been proved to achieve state-of-the-art\nresults on numerous audio tasks, including audio event\nclassiﬁcation, keyword spotting, mood recognition, and\nspeaker identiﬁcation, after being trained on a vast amount\nof unlabeled data.Method Corr. MSE MSE STD\nchroma + MLP 0.1740 0.0326 -\nMFCCs + MLP 0.1179 0.0353 -\nconvnet features [38] + MLP 0.1889 0.0314 -\nEHC features + SVR 0.2988 0.0339 0.0128\nEHC features + SVR + PS 0.2084 0.0391 0.0129\nEHC features + MLP 0.2656 0.0263 0.0058\nEHC features + MLP + PS 0.2388 0.0275 0.0059\nmel-spectrograms + SSAST 0.0124 0.0298 0.0061\nmel-spectrograms + SSAST + PS 0.2658 0.0265 0.0074\nTable 3 . Spearman’s rank correlation and MSE loss\nbetween predicted and ground truth music memorability\nscore using different models. Note that EHC features stand\nfor explainable handcrafted features, PS stands for pitch\nshift data augmentation, and Corr. represents Spearman’s\nrank correlation.\n5. EXPERIMENT RESULTS\nEvaluation Metrics. Spearman’s rank correlation and\nmean squared error (MSE) loss are used as the metrics to\nevaluate the performance of music memorability predic-\ntion. The former indicates the ability to rank the relative\nmemorability of different audios, while the latter indicates\nthe absolute error of the predicted results.\nDifferent Baselines. Here, we leverage Chroma and\nMFCCs along with their respective derivatives as two\nhand-crafted feature representations and ﬁt the ground\ntruth by Multilayer Perceptron (MLP) as two simple base-\nlines. Moreover, we also use the convnet model as a base-\nline since it is the most referenced and available work in\ngeneral music representation. The convnet model [38] uti-\nlizes CNNs for music tagging in the pre-training stage, and\nthe extracted features serve as the representation for down-\nstream tasks. Finally, Self-Supervised Audio Spectrogram\nTransformer (SSAST) [37] is also used as the baseline,\nwhich is a Transformer-based model with more parame-\nters as compared to CNNs. SSAST pretrains the model\nwith joint discriminative and generative masked spectro-\ngram patch modeling.\nImplementation Details. All the feature classiﬁers are\npretrained without ﬁnetuning on the self-collected dataset.\nTo handle the instability stemming from the limited la-\nbeled data, we normalize labels by subtracting the mean\nvalue, i.e., predicting a relative value instead of an abso-\nlute value. For MLP and SSAST models, the learning rates\nare respectively set to 5e-5 and 5e-6 with the Adam opti-\nmizer [39]. We also conduct additional feature selection on\nthe handcrafted features to improve the convergence of the\nMLP/SVR model (only select 25 features) due the small\nnumber of data samples. In addition, techniques includ-\ning frequency masking, band stop ﬁltering, and reverbera-\ntion [26] are used to augment data, together with the pitch\nshifting augmentation. The results are reported by the av-\nerage of the 10-fold outputs.\n5.1 Prediction Results.\nTable 3 compares the results of different prediction mod-\nels, where SVR and MLP take explainable handcraftedProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n178Model Top-k feature selection Corr. MSE\nMLP k = 40 (no feature selection) 0.2160 0.0272\nMLP k = 35 0.2324 0.0270\nMLP k = 25 0.2656 0.0263\nMLP k = 20 0.2018 0.0271\nSVR k = 40 (no feature selection) 0.2168 0.0324\nSVR k = 35 0.2291 0.0340\nSVR k = 25 0.2988 0.0339\nSVR k = 20 0.2630 0.0354\nTable 4 . Spearman’s rank correlation and MSE loss for\nMLP/SVR models with different top-k feature selection.\nfeatures as inputs, and SSAST takes mel-spectrograms as\ninputs. The results indicate that chroma and MFCCs pro-\nduce the worst results due to the ineffective feature extrac-\ntion. For convnet features, the performance is better than\nchroma and MFCCs due to the pretraining. However, the\namount of training data is too small to ﬁnetune the model\non the music memorability regression task. SSAST outper-\nforms other baselines since it incorporates the prior knowl-\nedge of spectrograms pre-trained by using advanced meth-\nods. Finally, Explainable Handcrafted Features (EHC)\nmethod produces the best correlation results by combin-\ning both low- and high-level features that help improve\nmusic memorability. These quantitative ﬁndings manifest\nthat data-driven MIR tasks are notably reliant on huge data\nquantities to be resilient and general.\n5.2 Ablation Study\nTable 4 shows an ablation study on feature selection for\nhandcrafted features, indicating that selecting top-25 fea-\ntures leads to the best overall correlation results. More-\nover, Table 3 also shows an ablation study on extra pitch\nshifting for data augmentations. Small pitch shifts (less\nthan 5 semitones) make the altered audio seem natural to\nthe human ear according to [40]. Therefore, we add semi-\ntone shifts of -5 to 5 to our data. The results manifest that\npitch shifting is effective for the models that take sequence\ninformation into account because applying mean pooling\nacross time on harmony features in non-sequential models\nlike SVR and MLP just forces the model to forecast the\nsame value using multiple static chroma information. This\nmay confuse the model on harmony characteristics. On the\nother hand, models with sequential information, such as\nSSAST, learn pitch-invariance after pitch shifting. The per-\nformance of SSAST notably decreases without pitch shift\ndata augmentation, possibly due to its data-hungry nature\nas a Transformer-based model, i.e.requiring more data for\noptimal parameter tuning.\n5.3 Interpretability\nWe attempt to gain insight into the intrinsic memory uti-\nlizing XAI methodologies. One post-hoc strategy for ex-\npressing black box models in a human-interpretable man-\nner is SHAP [16]. Speciﬁcally, SHAP explanations are ob-\ntained by perturbing a speciﬁc instance in the data and ob-\nserving the impact of these perturbations on the black-box\nFigure 5 . SHAP summary of the SVR model with RBF\nkernel [41]. The most important features are listed in de-\ncreasing order and the fact that feature value rises after the\nSHAP value shows a positive relationship between the two.\nmodel’s output. As such, SHAP allows us to explore the\nfactors that the model considers when determining memo-\nrability. Figure 5 visualizes the directionality impact of the\ntop-5 features in SHAP, where the x-axis stands for SHAP\nvalue and each point is a SHAP value of a sample for a fea-\nture. Red color and blue colors respectively indicate higher\nand lower values of a feature. As such, we can observe the\nfeature directionality impact based on the distribution. For\nexample, the ﬁrst row shows that a higher arousal value\nleads to high memorability scores, while a lower arousal\nvalue can lead to both high and low memorability scores.\nThe important factors for the predictor among the EHC\nfeatures include arousal, bpm, harmony (the feature \"D\nmean\") and the timbre features extracted from the source\nother than vocals, drums, and bass (the feature \"other db\nmean\"). According to Psychology research [28], normal\nindividuals without brain damage ﬁnd it easier to recog-\nnize musical excerpts with high arousal. The melodies are\nthe main constituent elements of the source ”others” after\napplying 4-stem Spleeter separation. This ﬁnding supports\nour understanding that we often focus on the main melody\nin music, and thus the chorus or hook of the song with out-\nstanding melody usually represents the entire song.\n6. CONCLUSION AND FUTURE WORK\nIn this work, we explore the novel task of music memo-\nrability regression (MMR) using a data-driven approach.\nThe consistency of our newly proposed YouTube Music\nMemorability (YTMM) dataset supports our hypothesis\nthat music memorability indeed exists and can be pre-\ndicted. Furthermore, we investigate the use of feature engi-\nneering and self-supervised learning for predicting music\nmemorability, highlighting the importance of prior knowl-\nedge and other training approaches, such as label normal-\nization, for improving results with limited data. We make\nthe dataset available online to encourage further research\nand development in the ﬁeld of MMR. In the future, we\nplan to: 1) scale the dataset to better represent the mem-\norability of full music structures, 2) investigate the poten-\ntial of transfer learning trained on music-oriented datasets\nto enhance our current baselines, and 3) study the person-\nalization issue since music memorability can be strongly\nrelated to the past musical experience of individuals.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1797. ACKNOWLEDGEMENT\nThis work was supported in part by the National Science\nand Technology Council of Taiwan under Grant NSTC-\n109-2221-E-009-114-MY3.\n8. REFERENCES\n[1] M. Alexomanolaki, C. Loveday, and C. Kennett, “Mu-\nsic and memory in advertising: Music as a device of\nimplicit learning and recall,” Music, Sound, and the\nMoving Image , vol. 1, no. 1, pp. 51–71, 2007.\n[2] S. Hecker, “Music for advertising effect,” Psychology\n& Marketing , vol. 1, no. 3-4, pp. 3–8, 1984.\n[3] B. Snyder and R. Snyder, Music and memory: An in-\ntroduction . MIT press, 2000.\n[4] D. B. Ramsay, I. Ananthabhotla, and J. A. Paradiso,\n“The intrinsic memorability of everyday sounds,” AES\nInternational Conference on Immersive and Interactive\nAudio , 2019.\n[5] A. R. Halpern and D. Müllensiefen, “Effects of timbre\nand tempo change on memory for music,” Quarterly\nJournal of Experimental Psychology , vol. 61, no. 9, pp.\n1371–1384, 2008.\n[6] D. Müllensiefen and A. R. Halpern, “The role of fea-\ntures and context in recognition of novel melodies,”\nMusic Perception: An Interdisciplinary Journal ,\nvol. 31, no. 5, pp. 418–435, 2012.\n[7] S. McCullough Campbell and E. H. Margulis, “Catch-\ning an earworm through movement,” Journal of New\nMusic Research , vol. 44, no. 4, pp. 347–358, 2015.\n[8] K. Jakubowski, S. Finkel, L. Stewart, and D. Mül-\nlensiefen, “Dissecting an earworm: Melodic features\nand song popularity predict involuntary musical im-\nagery.” Psychology of Aesthetics, Creativity, and the\nArts, vol. 11, no. 2, p. 122, 2017.\n[9] P. Isola, J. Xiao, D. Parikh, A. Torralba, and A. Oliva,\n“What makes a photograph memorable?” IEEE trans-\nactions on pattern analysis and machine intelligence ,\nvol. 36, no. 7, pp. 1469–1482, 2013.\n[10] A. Khosla, A. S. Raju, A. Torralba, and A. Oliva, “Un-\nderstanding and predicting image memorability at a\nlarge scale,” in Proceedings of the IEEE International\nConference on Computer Vision (ICCV) , December\n2015.\n[11] R. Dubey, J. Peterson, A. Khosla, M.-H. Yang, and\nB. Ghanem, “What makes an object memorable?” in\nProceedings of the ieee international conference on\ncomputer vision , 2015, pp. 1089–1097.\n[12] S. Shekhar, D. Singal, H. Singh, M. Kedia, and\nA. Shetty, “Show and recall: Learning what makes\nvideos memorable,” in Proceedings of the IEEE Inter-\nnational Conference on Computer Vision Workshops ,\n2017, pp. 2730–2739.[13] A. Adadi and M. Berrada, “Peeking inside the black-\nbox: a survey on explainable artiﬁcial intelligence\n(xai),” IEEE access , vol. 6, pp. 52 138–52 160, 2018.\n[14] C.-Y . Li, P.-C. Yuan, and H.-Y . Lee, “What does a net-\nwork layer hear? analyzing hidden representations of\nend-to-end asr through speech synthesis,” in ICASSP\n2020-2020 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) . IEEE,\n2020, pp. 6434–6438.\n[15] J. V . Jeyakumar, J. Noor, Y .-H. Cheng, L. Garcia, and\nM. Srivastava, “How can i explain this to you? an em-\npirical study of deep neural network explanation meth-\nods,” Advances in Neural Information Processing Sys-\ntems, vol. 33, pp. 4211–4222, 2020.\n[16] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach\nto interpreting model predictions,” in Advances\nin Neural Information Processing Systems 30 ,\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett,\nEds. Curran Associates, Inc., 2017, pp. 4765–\n4774. [Online]. Available: http://papers.nips.cc/paper/\n7062-a-uniﬁed-approach-to-interpreting-model-predictions.\npdf\n[17] J. A. Burgoyne, D. Bountouridis, J. van Balen, H. Hon-\ninget al. , “Hooked: a game for discovering what\nmakes music catchy,” in Proceedings of the 14th So-\nciety of Music Information Retrieval Conference (IS-\nMIR) , 2013.\n[18] J. V . Balen, J. A. Burgoyne, D. Bountouridis, D. Mül-\nlensiefen, and R. C. Veltkamp, “Corpus analysis tools\nfor computational hook discovery,” in ISMIR , 2015, pp.\n227–233.\n[19] J. Van Balen et al. , “Audio description and corpus anal-\nysis of popular music,” Ph.D. dissertation, Utrecht Uni-\nversity, 2016.\n[20] I. R. Korsmit, J. A. Burgoyne, and H. Honing, “If you\nwanna be my lover... a hook discovery game to uncover\nindividual differences in long-term musical memory,”\ninProceedings of the 25th Anniversary Conference of\nthe European Society for the Cognitive Sciences of Mu-\nsic, 2017.\n[21] B. McFee, E. J. Humphrey, and J. P. Bello, “A software\nframework for musical data augmentation.” in ISMIR ,\nvol. 2015, 2015, pp. 248–254.\n[22] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le,\n“Randaugment: Practical automated data augmenta-\ntion with a reduced search space,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition Workshops , 2020, pp. 702–703.\n[23] H.-H. Wu, C.-C. Kao, Q. Tang, M. Sun, B. McFee,\nJ. P. Bello, and C. Wang, “Multi-task self-supervised\npre-training for music classiﬁcation,” in ICASSP 2021-\n2021 IEEE International Conference on Acoustics,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n180Speech and Signal Processing (ICASSP) . IEEE, 2021,\npp. 556–560.\n[24] R. Castellon, C. Donahue, and P. Liang, “Codiﬁed au-\ndio language modeling learns useful representations\nfor music information retrieval,” in ISMIR , 2021.\n[25] S. Pascual, M. Ravanelli, J. Serrà, A. Bona-\nfonte, and Y . Bengio, “Learning Problem-\nAgnostic Speech Representations from Multiple\nSelf-Supervised Tasks,” in Proc. of the Conf. of\nthe Int. Speech Communication Association (INTER-\nSPEECH) , 2019, pp. 161–165. [Online]. Available:\nhttp://dx.doi.org/10.21437/Interspeech.2019-2605\n[26] M. Ravanelli, J. Zhong, S. Pascual, P. Swietojanski,\nJ. Monteiro, J. Trmal, and Y . Bengio, “Multi-task self-\nsupervised learning for robust speech recognition,” in\nICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2020, pp. 6989–6993.\n[27] C. Spearman, “The proof and measurement of associ-\nation between two things,” The American journal of\npsychology , vol. 100, no. 3/4, pp. 441–471, 1987.\n[28] S. Samson, D. Dellacherie, and H. Platel, “Emotional\npower of music in patients with memory disorders:\nClinical implications of cognitive neuroscience,” An-\nnals of the New York Academy of Sciences , vol. 1169,\nno. 1, pp. 245–255, 2009.\n[29] P. Vuilleumier and W. Trost, “Music and emotions:\nfrom enchantment to entrainment,” Annals of the New\nYork Academy of Sciences , vol. 1337, no. 1, pp. 212–\n222, 2015.\n[30] C. Harte and M. Sandler, “Automatic chord identifca-\ntion using a quantised chromagram,” in Audio Engi-\nneering Society Convention 118 . Audio Engineering\nSociety, 2005.\n[31] R. Hennequin, A. Khlif, F. V oituret, and M. Moussal-\nlam, “Spleeter: a fast and efﬁcient music source sepa-\nration tool with pre-trained models,” Journal of Open\nSource Software , vol. 5, no. 50, p. 2154, 2020.\n[32] A. T. Cemgil, B. Kappen, P. Desain, and H. Hon-\ning, “On tempo tracking: Tempogram representation\nand kalman ﬁltering,” Journal of New Music Research ,\nvol. 29, no. 4, pp. 259–273, 2000.\n[33] K. Zhang, H. Zhang, S. Li, C. Yang, and L. Sun, “The\npmemo dataset for music emotion recognition,” in Pro-\nceedings of the 2018 acm on international conference\non multimedia retrieval , 2018, pp. 135–142.\n[34] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and\nM. D. Plumbley, “Panns: Large-scale pretrained au-\ndio neural networks for audio pattern recognition,”\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , vol. 28, pp. 2880–2894, 2020.[35] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,”\nnature , vol. 521, no. 7553, pp. 436–444, 2015.\n[36] S. J. Pan and Q. Yang, “A survey on transfer learning,”\nIEEE Transactions on knowledge and data engineer-\ning, vol. 22, no. 10, pp. 1345–1359, 2009.\n[37] Y . Gong, C.-I. J. Lai, Y .-A. Chung, and J. Glass, “Ssast:\nSelf-supervised audio spectrogram transformer,” in\nAAAI Conference on Artiﬁcial Intelligence , 2022.\n[38] K. Choi, G. Fazekas, M. Sandler, and K. Cho, “Trans-\nfer learning for music classiﬁcation and regression\ntasks,” in The 18th International Society of Music In-\nformation Retrieval (ISMIR) Conference 2017, Suzhou,\nChina . International Society of Music Information\nRetrieval, 2017.\n[39] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” in ICLR (Poster) , 2015.\n[40] J. Thickstun, Z. Harchaoui, D. P. Foster, and S. M.\nKakade, “Invariances and data augmentation for super-\nvised music transcription,” in 2018 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) . IEEE, 2018, pp. 2241–2245.\n[41] I. Steinwart, D. Hush, and C. Scovel, “An explicit de-\nscription of the reproducing kernel hilbert spaces of\ngaussian rbf kernels,” IEEE Transactions on Informa-\ntion Theory , vol. 52, no. 10, pp. 4635–4643, 2006.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n181"
    },
    {
        "title": "Chorus-Playlist: Exploring the Impact of Listening to Only Choruses in a Playlist.",
        "author": [
            "Kosetsu Tsukuda",
            "Masahiro Hamasaki",
            "Masataka Goto"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265403",
        "url": "https://doi.org/10.5281/zenodo.10265403",
        "ee": "https://zenodo.org/records/10265403/files/000093.pdf",
        "abstract": "When people listen to playlists on a music streaming service, they typically listen to each song from start to end in order. However, what if it were possible to use a function to listen to only the choruses of each song in a playlist one after another? In this paper, we call this music listening concept \"chorus-playlist,\" and we investigate its potential impact from various perspectives such as the demand and the objectives for listening to music with chorus-playlist. To this end, we conducted a questionnaire-based online user survey involving 214 participants. Our analysis results suggest reusable insights, including the following: (1) We show a high demand for listening to existing playlists with the chorus-playlist approach. We also reveal preferred options for chorus playback, such as adding crossfade transitions between choruses. (2) People listen to playlists with chorus-playlist for various objectives. For example, when they listen to their own self-made playlists, they want to boost a mood or listen to music in a specific context such as work or driving. (3) There is also a high demand for playlist creation on the premise of continuous listening to only the choruses of the songs in a playlist. The diversities of artists, genres, and moods are more important when creating such a playlist than when creating a usual playlist.",
        "zenodo_id": 10265403,
        "dblp_key": "conf/ismir/TsukudaHG23",
        "keywords": [
            "chorus-playlist",
            "music listening concept",
            "demand",
            "objectives",
            "playlist creation",
            "crossfade transitions",
            "self-made playlists",
            "mood boost",
            "specific context",
            "diversities of artists"
        ],
        "content": "CHORUS-PLAYLIST: EXPLORING THE IMPACT OF\nLISTENING TO ONLY CHORUSES IN A PLAYLIST\nKosetsu Tsukuda Masahiro Hamasaki Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\n{k.tsukuda, masahiro.hamasaki, m.goto}@aist.go.jp\nABSTRACT\nWhen people listen to playlists on a music streaming ser-\nvice, they typically listen to each song from start to end in\norder. However, what if it were possible to use a function\nto listen to only the choruses of each song in a playlist one\nafter another? In this paper, we call this music listening\nconcept “chorus-playlist,” and we investigate its potential\nimpact from various perspectives such as the demand and\nthe objectives for listening to music with chorus-playlist.\nTo this end, we conducted a questionnaire-based online\nuser survey involving 214 participants. Our analysis results\nsuggest reusable insights, including the following: (1) We\nshow a high demand for listening to existing playlists with\nthe chorus-playlist approach. We also reveal preferred op-\ntions for chorus playback, such as adding crossfade transi-\ntions between choruses. (2) People listen to playlists with\nchorus-playlist for various objectives. For example, when\nthey listen to their own self-made playlists, they want to\nboost a mood or listen to music in a speciﬁc context such\nas work or driving. (3) There is also a high demand for\nplaylist creation on the premise of continuous listening to\nonly the choruses of the songs in a playlist. The diversi-\nties of artists, genres, and moods are more important when\ncreating such a playlist than when creating a usual playlist.\n1. INTRODUCTION\nThe chorus of a song is one of the most distinctive parts in\nthe song. In terms of acoustic aspects, it has been reported\nthat the chorus tends to have louder sound, contain heav-\nier instrumentation and additional vocals, and include the\nhighest-pitch vocal note in a song [1–3]. In terms of cog-\nnitive aspects, the chorus tends to be the catchiest, most\nmemorable, and most salient part of a song for emotional\nexpression [4–7]. Moreover, the chorus is often character-\nized by the property of being a song’s most repeated sec-\ntion [8, 9]. Because of these characteristics, the chorus has\nattracted academic attention. For example, research has\nbeen conducted on music structure analysis including cho-\nrus detection [8–35] and its use for music summary gener-\nation [36, 37]. In addition, music datasets specializing in\nchoruses have been made publicly available [38].\n© K. Tsukuda, M. Hamasaki, and M. Goto. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: K. Tsukuda, M. Hamasaki, and M. Goto, “Chorus-Playlist:\nExploring the Impact of Listening to Only Choruses in a Playlist”, in\nProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.As for general music listening habits beyond cho-\nruses, the amount of time spent listening to music through\nplaylists on music streaming services has increased [39–\n42]. It has also been reported that this listening time is\nlonger than the time of listening to music via albums [43].\nOn the services, both playlists that are created by general\nusers and those that are created by professional curators or\nautomatically generated are widely available [42, 44–46].\nAs a result, Spotify has over 4 billion playlists, for exam-\nple [42]. With the popularity of playlists, many studies\nhave been conducted on playlist recommendation, genera-\ntion, and analysis [40, 42, 44, 47–76].\nGiven the importance of choruses and playlists, we fo-\ncus on a listening approach in which only the choruses of\nthe songs in a playlist are played one after another. Cer-\ntain smartphone music player applications such as V oca-\ncolle App by DWANGO Co., Ltd., MIXTRAX App by Pi-\noneer Corporation, and KENWOOD Music Control have\nprovided a function to continuously play the choruses of\nsongs or parts including the choruses. However, there has\nbeen no academic discussion on the impact of this listening\napproach. In this paper, we refer to the concept of contin-\nuous listening to only the choruses of songs in a playlist\nas “chorus-playlist.” This concept can be applied not only\nwhen a user listens to playlists that she created but also\nwhen she listens to playlists created by other users. Under\nthis concept, a user could create a playlist on the premise of\ncontinuous listening to only the choruses of the playlist’s\nsongs. Hence, the goal of this paper is to reveal the useful-\nness of chorus-playlist and provide reusable insights.\nTo achieve this goal, we conducted a questionnaire-\nbased online user survey involving 214 participants. Our\ncontributions can be summarized as follows.\n• To our knowledge, this is the ﬁrst study investigating\nthe impact of continuous listening to only the choruses\nof songs in a playlist.\n• We reveal user preferences for chorus playback in a\nplaylist ( e.g., users prefer to add crossfade transitions\nbetween choruses). We also show a high demand and\ncertain user objectives for listening to playlists with\nchorus-playlist. For example, people often want to lis-\nten to their own self-made playlists to boost a mood or\nin a speciﬁc context such as work or driving.\n• We show that people tend to be willing to create a\nplaylist for continuously listening to only the choruses\nof the songs in the playlist. We also reveal important\nproperties in creating such playlists ( e.g., the diversity\nof moods, genres, and artists).782• According to the survey results, we suggest new re-\nsearch topics for the music information retrieval (MIR)\ncommunity ( e.g., song recommendation for users who\nlisten to music with chorus-playlist). We also make sev-\neral proposals for music streaming platforms to attract\nusers ( e.g., when using the chorus-playlist approach,\npeople would be willing to listen to playlists contain-\ning hit songs to efﬁciently check them out).\n• We have made a portion of the survey results publicly\navailable on the web to support future studies1.\n2. RELATED WORK\n2.1 Chorus Analysis and Detection\nThe chorus is distinctive compared to other sections of a\nsong in terms of acoustic, structural, and cognitive aspects.\nIn terms of acoustic aspects, it has been known that cho-\nruses tend to be louder, to contain heavier instrumenta-\ntion and more vocals, and to have the highest-pitch vocal\nnote in a song [1–3]. More recently, Balen et al. [77] re-\nvealed that choruses have a smaller dynamic range and a\ngreater variety of MFCC-measurable timbres, as compared\nto other sections. Regarding structural aspects, choruses\nare usually repeated more than other sections such as in-\ntros and verses [8, 9]. As for cognitive aspects, choruses\nare the catchiest and most memorable sections for listen-\ners [4, 5, 7]. For artists, too, choruses are distinctive in\nthat they are the most salient sections for emotional ex-\npression [6]. Given these characteristics, we focus here on\nchoruses as song excerpts, rather than other sections. As\nwe will show in section 7, the chorus is more preferred\nthan other sections for continuous listening in a playlist.\nBecause of the importance of choruses, many studies\nhave addressed musical structure analysis, including cho-\nrus detection, based on music audio signals [8–15, 17–24,\n26,27,29–32,34,35] or lyrics [16,25,28,33]. The accuracy\nof identifying the chorus section is approximately 80-90%\nin terms of the F-measure [12,22,29,33]. Accordingly, the\nfeasibility of implementing the chorus-playlist concept on\nmusic streaming services is sufﬁciently high. For songs in\nwhich the correct chorus cannot be detected, it is also pos-\nsible to have users on a service manually correct choruses\nthrough a collective intelligence approach [78].\n2.2 Playlist Analysis and Recommendation\nOn today’s music streaming services, playlists have be-\ncome a central way to listen to music [40–42]. The ma-\njority of playlists on services are created by general users\nrather than professional music enthusiasts [44]. Users cre-\nate playlists not only for their own listening but also to\nshare their musical preferences with other users such as\nfriends and followers [44, 53, 70]. It has also been re-\nported that playlists can be characterized by certain proper-\nties [40, 62, 68, 69, 74, 76] such as song order [42] and low\ndiversity in terms of both artists and genres [70]. In this\npaper, too, we report objectives for listening to music with\n1The data can be downloaded from https://github.com/\nktsukuda/chorus_playlist .chorus-playlist (section 5) and important properties for cre-\nating a playlist to continuously listen to only the choruses\nof songs in the playlist (section 6).\nAlthough playlists are actively created by users, it is\ntime consuming to manually create a playlist [44]. To ease\nthe process, two approaches have been studied: assisted\nplaylist creation [49–52, 54, 55, 71] and automatic playlist\ngeneration [47, 48, 56, 58–61, 63–67, 72, 73, 75]. In song\nrecommendation for a playlist or generation of a playlist,\nit is typical to consider the song order and/or the audio\nsimilarity between songs. Furthermore, there are several\nstudies that automatically extract prominent sections (not\nonly limited to choruses) from individual songs and gen-\nerate DJ mixes [79] or medleys [80] by connecting them.\nMusic streaming services such as Spotify and Deezer pro-\nvide functions for automatic playlist generation to promote\nsong discovery by users [45, 46]. In this paper, according\nto our survey results, we discuss not only new approaches\nfor these research topics but also how to encourage users\non music streaming services to more actively interact with\nplaylists and discover novel songs.\n3. PARTICIPANTS\nWe recruited participants for our survey via an online re-\nsearch company in Japan. We limited the participants to\nthose who are Japanese, listened to music an average of at\nleast one day per week via any music streaming service,\nand had created at least 10 playlists on the service. We\npaid 51.6 USD (7,000 JPY) to each participant. Although\n222 participants answered the questionnaire in sections 4,\n5, 6, and 7 through a web browser, to make the analysis\nresults more reliable, we removed the answers from eight\nparticipants who submitted improper responses to a free-\nresponse question. The remaining 214 participants were\ndiverse in both gender and age range: 89 males (10s: 1;\n20s: 29; 30s: 35; 40s: 17; 50s: 7), and 125 females (10s:\n1; 20s: 45; 30s: 38; 40s: 25; 50s: 16).\n4. PREFERENCE FOR CHORUS PLAYBACK\n4.1 Chorus Playback Choices\nAs explained in section 1, the concept of chorus-playlist\nenables a user to continuously listen to only the choruses of\nsongs in a playlist. However, some users may prefer to add\ncrossfade transitions between choruses. In this section, we\ninvestigate the preferences for chorus playback in chorus-\nplaylist in terms of the following three choices.\n• TimePreChorus: the playback time before the chorus.\nThe options are “no playback,” “5 seconds,” and “10\nseconds.” “No playback” means that only the choruses\nare continuously played, without any part of the song\nbefore the chorus.\n• Crossfade: whether 1-second crossfade transitions are\nadded between songs. The options are “on” and “off.”\n• TimeChorus: the playback time for the chorus. The\noptions are “15 seconds,” “30 seconds,” and “adaptive.”\nIn the case of “15 (resp. 30) seconds,” 15 (resp. 30)\nseconds on a song is played from the beginning of theProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n783ﬁrst chorus2. In the case of “adaptive,” the ﬁrst chorus\nis played from beginning to end regardless of its length.\nHereafter, we refer to a combination of these three choices\nin the form of a triplet such as (TimePreChorus, Crossfade,\nTimeChorus) =(5 seconds, off, adaptive).\n4.2 Dataset\nIn this survey, the participants listened to playlists that we\nprovided. To reduce bias due to the played songs, we used\n120 songs created by professional musicians that we com-\nmissioned. That is, we guaranteed that the participants had\nnever listened to any of the 120 songs. Instead of using\nchorus detection methods introduced in section 2.1, a mu-\nsic expert manually labeled the start and end times of each\nsong’s ﬁrst chorus to prevent detection errors. For 26 songs\nthat started with the chorus, the expert labeled the start and\nend times of the second chorus, because the mood and/or\nbeat of such leading choruses are sometimes different from\nthose of the second and subsequent choruses. The average\nand standard deviation of chorus lengths for the 120 songs\nwere 29.0 and 7.66 seconds, respectively, and 52 songs had\nchoruses longer than 30 seconds.\nAs the participants had various music preferences, we\ncreated diverse playlists by sampling the songs to be in-\ncluded in playlists as follows. First, the 120 songs were\nplotted in the valence-arousal (V A) space according to their\naudio features. Next, we applied the k-means algorithm\nand classiﬁed the 120 songs in the V A space into three clus-\nters. We created three playlists by sampling ﬁve songs at\nrandom for each playlist from one cluster. Similarly, we\ncreated three more playlists from another cluster. Finally,\nwe created three playlists containing diverse songs in terms\nof their moods by randomly selecting one song from each\nof the two previous clusters and three songs from the re-\nmaining cluster. Each playlist’s song order was also de-\ntermined randomly. In total, we created nine playlists that\neach comprised ﬁve songs. Note that there were no song\noverlaps between any pairs of playlists.\nFor each playlist, there were 18 total option combina-\ntions (3 options for TimePreChorus ×2 options for Cross-\nfade×3 options for TimeChorus). For example, for (5\nseconds, on, adaptive), we ﬁrst created an MP3 ﬁle by cut-\nting each song in a playlist from ﬁve seconds before the\nﬁrst chorus to the end of the chorus and then connecting\nthe songs with crossfade transitions. We then created an\nMP4 ﬁle to simulate the participants’ experience of listen-\ning to the playlist on a smartphone application. Speciﬁ-\ncally, given an MP3 ﬁle, we created an MP4 ﬁle in which\nthe MP3’s playlist was played and images changed at the\nsame time the song changed in the playlist. Each image\ncontained a song’s title and artist name like in the music\nplayback screen of a music player application (Figure 1)3.\n4.3 Procedure\nFirst, we investigated the participants’ preferred option\ncombinations. To this end, three dropdown lists with the\n2Thus, if the ﬁrst chorus is less than 15 (resp. 30) seconds long, the\nsong continues play after the chorus until the total playback time reaches\n15 (resp. 30) seconds.\n3The images also include icons of pause, next, and previous buttons.Playback time before the chorus.\nNo playback\nCrossfade transition between tracks.\nOn\nPlayback time of the chorus.\n30 secondsPlaylist 1\nMISTAKE\nAuryPlaylist 2\nJelly fish\nJD WalkerPlaylist 3\nCinderella Girl\nKaty S\nFigure 1 . Interface example from our user survey. In this\nexample, when a participant selects the options (no play-\nback, on, 30 seconds), three playlists satisfying this com-\nbination are displayed.\noptions for each choice were presented to the participants.\nFor each participant, three playlists were selected at ran-\ndom from the set of playlists described in section 4.2. Once\nthe participant had selected an option for each choice, the\nthree playlists (MP4 ﬁles) satisfying that option combi-\nnation were displayed (Figure 1). When the participant\nchanged the combination, the displayed playlists were also\nchanged to those satisfying the new option combination4.\nAfter listening to playlists for any option combination,\nthe participants reported their most preferred combination\nsuch as (10 seconds, on, 30 seconds), by selecting those\noptions from the dropdown lists5.\nEven if a participant chose “adaptive” as the preferred\noption for TimeChorus, she may have liked “30 seconds”\nalmost as much. Accordingly, after the above investiga-\ntion, we investigated the option preferences including such\nsubtle differences. To this end, we displayed each of the\n18 options with a six-point Likert scale ranging from “not\npreferred at all” to “very preferred,” and we asked the par-\nticipants to rate their preferences for each option.\n4.4 Results\nTable 1 and Figure 2 indicate the results for the ﬁrst and\nsecond investigations, respectively. In Table 1, we can\nsee that the most popular combination was (5 seconds, on,\nadaptive). Even participants who chose other combinations\nalso tended to prefer each of these options. In fact, for the\nresults shown in Figure 2, paired Wilcoxon signed-rank\ntests with Bonferroni correction revealed that the median\nof “5 seconds” was statistically higher than the other two\noptions for TimePreChorus at p <0.016. Similarly, “on”\nfor Crossfade and “adaptive” for TimeChorus were statis-\ntically higher than the other options at p <0.01. In par-\nticular, it was not obvious that “5 seconds” was the most\npreferred option for TimePreChorus, making this a useful,\nreusable insight for realizing chorus-playlist.\nA music streaming service could offer the concept of\nchorus-playlist by implementing a function that enables\nusers to listen to only choruses for all existing playlists on\nthe service. If a service provided this function, it would be\nideal to enable users to play playlists with arbitrary option\ncombinations, as we did, to reﬂect users’ preferences. If it\n4Note that the songs contained in the three playlists did not change;\nonly the options for playing them were changed.\n5It was not mandatory to listen to the playlists for all 18 option com-\nbinations. In fact, most participants reported their most preferred combi-\nnation by narrowing down their preferences while switching options and\nlistening to the corresponding playlists.\n6Throughout this paper, ** (*) in a ﬁgure denotes a statistical differ-\nence atp <0.01(p <0.05).Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n784Table 1 . Preference distribution for option combinations.\nTimePreChorus Crossfade TimeChorus #participnats\nNo playbackOn15 seconds 10 (4.67%)\n30 seconds 3 (1.40%)\nadaptive 38 (17.76%)\nOff15 seconds 8 (3.74%)\n30 seconds 0\nadaptive 8 (3.74%)\n5 secondsOn15 seconds 26 (12.15%)\n30 seconds 14 (6.54%)\nadaptive 49 (22.90%)\nOff15 seconds 1 (0.47%)\n30 seconds 1 (0.47%)\nadaptive 16 (7.48%)\n10 secondsOn15 seconds 3 (1.40%)\n30 seconds 6 (2.80%)\nadaptive 22 (10.28%)\nOff15 seconds 1 (0.47%)\n30 seconds 1 (0.47%)\nadaptive 7 (3.27%)\n**\n**\n**\n****\nFigure 2 . Preference distributions for each option.\nis difﬁcult to implement such a ﬂexible function, chorus-\nplaylist should be provided with the option combination (5\nseconds, on, adaptive), which should maximize the aver-\nage user satisfaction according to this section’s results.\n5. DEMAND FOR CHORUS-PLAYLIST\nWe described above how to implement a chorus-playlist\nfunction. In this section, we investigate the demand for\nlistening to existing playlists with such a function.\n5.1 Procedure\nFirst, we showed the following description: “Suppose that\na function to play only the choruses of the songs in a\nplaylist has become available on the music streaming ser-\nvice that you usually use. Please rate on a scale from 1\n(unwilling) to 6 (very willing) how much you would like to\nuse this function to listen to existing playlists that you cre-\nated.” When the answer was “unwilling,” they were asked\nto respond freely on why he/she did not want to use it.\nOtherwise, when the answer was one of the remaining ﬁve\nitems, they were asked to respond freely with at least one\nobjective for listening to playlists with the function. We\ndid not set a cap on the number of responses.\nNext, in a similar way, we asked the participants to indi-\ncate their willingness on a 6-point scale to use the function\nto listen to existing playlists created by other users. Ac-\ncording to their willingness, they were asked to provide\nfree responses as they did for the ﬁrst question.\n5.2 Results\nFigure 3 shows the answer distribution. We refer to\nplaylists created by the participant and by other users “self-\nmade playlists” and “others’ playlists.” Only 11 and 3 par-\nticipants answered “unwilling” for self-made playlists and/aj17 /aj19/aj22 /aj22/aj17 /aj24/aj22 /aj18/aj17/aj17 /aj18/aj19/aj22 /aj18/aj22/aj17 /aj18/aj24/aj22 /aj19/aj17/aj17\n/aj47/aj86/aj78/aj67/aj70/aj83/aj1/aj80/aj71/aj1/aj81/aj66/aj83/aj85/aj74/aj68/aj74/aj81/aj66/aj79/aj85/aj84/aj48/aj85/aj73/aj70/aj83/aj84/aj8/aj1/aj81/aj77/aj66/aj90/aj77/aj74/aj84/aj85/aj84/aj52/aj70/aj77/aj71/aj14/aj78/aj66/aj69/aj70/aj1/aj81/aj77/aj66/aj90/aj77/aj74/aj84/aj85/aj84\n/aj18/aj27/aj1/aj86/aj79/aj88/aj74/aj77/aj77/aj74/aj79/aj72\n/aj19/aj27/aj1/aj79/aj80/aj85/aj1/aj87/aj70/aj83/aj90/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72/aj20/aj27/aj1/aj84/aj80/aj78/aj70/aj88/aj73/aj66/aj85/aj1/aj86/aj79/aj88/aj74/aj77/aj77/aj74/aj79/aj72\n/aj21/aj27/aj1/aj84/aj80/aj78/aj70/aj88/aj73/aj66/aj85/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72/aj22/aj27/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72\n/aj23/aj27/aj1/aj87/aj70/aj83/aj90/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72\nFigure 3 . Distribution of the willingness to listen to\nplaylists with the chorus-playlist function.\nothers’ playlists, respectively. The most popular reason for\nunwillingness was “I believe that there is value in listening\nto the entire song, including parts other than the chorus.”\nOn the other hand, because 75.2 % (161) and 82.7 % (177)\nparticipants for the two respective types of playlists an-\nswered “very willing,” “willing,” or “somewhat willing,”\nwe conclude that there is a sufﬁciently high demand for\nchorus-playlist.\nWe manually grouped the free responses on their objec-\ntives. When a response included multiple objectives cor-\nresponding to different groups, it was assigned to multiple\ngroups. Table 2 lists the top 10 objectives in terms of the\ngroup sizes for each of the two kinds of playlists. Each\nnumber in parentheses indicates the number of participants\nwho gave that objective. Below, we discuss the results.\nIn the case of self-made playlists, the top three objec-\ntives could be achieved just by continuously listening to\nthe choruses of songs in a playlist. For example, the ﬁrst\nobjective was “boost a mood.” As self-made playlists usu-\nally contain songs that match the user’s own music pref-\nerences, the user’s mood would be boosted even when lis-\ntening to playlists in the usual way [68]. Nevertheless, it is\ninteresting that the participants answered that they wanted\nto further boost their mood by listening to only choruses.\nIt would be beneﬁcial to recommend songs for a playlist\nthat are suitable for boosting a user’s mood when the user\nlistens to only the choruses in the playlist. For the sec-\nond objective, the participants answered with various con-\ntexts such as “work” and “driving.” When listening to a\nplaylist with the chorus-playlist function in a speciﬁc con-\ntext, there could be various reasons such as “increasing\nconcentration” and “relaxing.” It would be an interesting\nfuture work to conduct a more detailed analysis of the con-\ntexts in which the conventional playlist listening approach\nor the chorus-playlist approach are preferred. As for the\nfourth objective, it is known that people consider it valu-\nable to listen to music with others and let others listen to\ntheir favorite songs [81–84]. However, because it takes\nmuch time for others to listen to all the songs in a playlist,\npeople may hesitate to introduce their favorite songs. Thus,\nthe participants answered that they wanted to efﬁciently in-\ntroduce others such as friends or family to their favorite\nsongs when they listen to music together in person. That\nis, chorus-playlist could encourage people to interact with\nothers through music in the real world.\nOn the other hand, regarding the top six objectives\nfor others’ playlists, although those objectives could be\nachieved by conventional playlist listening, the participants\nwanted to use the chorus-playlist function to achieve the\nobjectives more efﬁciently in a shorter time. In particular,\nas seen from the ﬁrst, second, fourth, and ﬁfth objectives,\nthere is a strong demand for efﬁciently discovering and lis-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n785Table 2 . Top 10 free-response objectives for listening to playlists with the chorus-playlist function.\nRank Self-made playlist Others’ playlist\n1 Boost a mood (79) Find unfamiliar songs that suit my preference (109)\n2 Listen to a playlist in a speciﬁc context (68) Listen to hit songs (48)\n3 Listen to a playlist within a limited time (46) Learn other people’s music preferences (44)\n4 Recommend my favorite songs to others (36) Listen to songs by unfamiliar artists (37)\n5 Explore desired songs (35) Listen to songs in unfamiliar genres (26)\n6 Listen to many songs (23) Preview a playlist (18)\n7 Listen to various songs (20) Listen to a playlist in a speciﬁc context (13)\n8 Recall songs listened to in the past (18) Refer to a playlist for creating my playlists (12)\n9 Listen to only the choruses of my favorite songs (19) Listen to a playlist for a change of pace (9)\n10 Sing songs in a playlist (12) Boost a mood (6)\ntening to unfamiliar songs. Accordingly, if a music stream-\ning service provides playlists consisting of hit songs of the\npast week, songs by a speciﬁc artist, or songs in a speciﬁc\ngenre with the chorus-playlist function, many users would\nlikely listen to those playlists by using the function. This\nwould enable users to ﬁnd more new favorite songs and\nhelp increase their music listening activity.\nIn summary, we have revealed that chorus-playlist can\ngenerate new interactions between people and music espe-\ncially when they listen to self-made playlists. Moreover, as\ndiscussed above, the results in Table 2 can provide useful\ninsights for both researchers and music streaming services.\n6. IMPORTANT PROPERTIES FOR CREATING\nPLAYLISTS IN CHORUS-PLAYLIST CONCEPT\n6.1 Procedure\nIn section 5, we assumed application of the chorus-playlist\nconcept to existing playlists. However, by taking the con-\ncept a step further, a user could create a playlist on the\npremise of continuous listening to only the choruses in the\nplaylist’s songs (for simplicity, we refer to creating such\na playlist as “creating a chorus-playlist”). Therefore, we\nasked the participants how much they would like to create\nchorus-playlists on the music streaming service that they\nused regularly. They answered with their willingness on a\n6-point scale from “unwilling” to “very willing.” We also\ntold the participants that they could also listen to each song\nfrom beginning to end through, not just the chorus.\nWhen users create playlists, they consider certain prop-\nerties such as the diversity of artists and the song order.\nHence, we wondered whether there are any differences re-\ngarding the importance of these properties when creating a\nchorus-playlist as compared to creating a usual playlist. To\nanswer this question, we considered the following 11 prop-\nerties derived from past studies [42, 70]. (1) SongHit: in-\ncluding songs with high popularity. (2) SongNew: includ-\ning new songs in terms of the release dates. (3) ArtistSame:\nincluding as many songs by the same artist as possible. (4)\nArtistDiv: including songs by as many different artists as\npossible. (5) GenreSame: including as many songs in the\nsame genre as possible. (6) GenreDiv: including songs\nin as many different genres as possible. (7) MoodSame:\nincluding as many songs with the same musical mood as\npossible. (8) MoodDiv: including songs with as many dif-\nferent moods as possible. (9) SongOrder: the song order\nin the playlist. (10) SongTop: the ﬁrst song in the playlist.\n(11) SongLast: the last song in the playlist./aj17 /aj19/aj22 /aj22/aj17 /aj24/aj22 /aj18/aj17/aj17 /aj18/aj19/aj22 /aj18/aj22/aj17 /aj18/aj24/aj22 /aj19/aj17/aj17\n/aj47/aj86/aj78/aj67/aj70/aj83/aj1/aj80/aj71/aj1/aj81/aj66/aj83/aj85/aj74/aj68/aj74/aj81/aj66/aj79/aj85/aj84\n/aj18/aj27/aj1/aj86/aj79/aj88/aj74/aj77/aj77/aj74/aj79/aj72\n/aj19/aj27/aj1/aj79/aj80/aj85/aj1/aj87/aj70/aj83/aj90/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72/aj20/aj27/aj1/aj84/aj80/aj78/aj70/aj88/aj73/aj66/aj85/aj1/aj86/aj79/aj88/aj74/aj77/aj77/aj74/aj79/aj72\n/aj21/aj27/aj1/aj84/aj80/aj78/aj70/aj88/aj73/aj66/aj85/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72/aj22/aj27/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72\n/aj23/aj27/aj1/aj87/aj70/aj83/aj90/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72\nFigure 4 . Willingness to create a chorus-playlist.\nThe participants who answered the ﬁrst question with\nanything other than “unwilling” rated the importance of\neach property in creating a chorus-playlist and in creating a\nusual playlist on a 6-point scale from “not at all important”\nto “very important.” The 11 properties were displayed in a\nrandom order to each participant.\n6.2 Results\nFigure 4 shows that chorus-playlist creation has the poten-\ntial to be a new way of enjoying music, because 75.7%\n(162) participants answered “very willing,” “willing,” or\n“somewhat willing,” while only 6.07% (13) participants\nanswered “unwilling.” Next, as shown in Figure 5, paired\nWilcoxon signed-rank tests indicated that statistical differ-\nences between the two playlist types were observed for\nnine properties7. Hence, we can say that people tended\nto emphasize different properties when creating a chorus-\nplaylist as compared to creating a usual playlist. Ex-\nisting studies on song recommendation for playlists or\nplaylist generation have proposed various methods focus-\ning on the song order in a playlist [54, 61, 64, 73]. How-\never, for chorus-playlist, the SongOrder, FirstSong, and\nLastSong properties were relatively less important. In\ncontrast, hit songs and new songs were more important\nfor chorus-playlist. Furthermore, the results revealed the\nimportance of diversity in terms of artists, genres, and\nmoods for chorus-playlist. It has been reported that the\ndiversities of artists and genres tend to be low in usual\nplaylists [70]; however, to support users creating chorus-\nplaylists, it would be important to recommend songs to di-\nversify such properties. These results thus open up new\nrecommendation approaches in the MIR community.\n7. PLAYBACK METHOD COMPARISON\nWe have revealed a high demand to try chorus-playlist.\nIn this section, we investigate whether the chorus-playlist\nplayback method is preferred to other playback methods.\n7.1 Procedure\nFor comparison, we used the following two types of\nplaylists. (1) Head-playlist: a user continuously listens to\n7Figure 5 shows the results for the 201 participants besides the 13\nparticipants who answered “unwilling.” The same statistical differences\nwere obtained even with only the aforementioned 162 participants.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n786]**\n]**\n]*\n]**\n]**\n]*\n]**\n]**\n]**\nFigure 5 . Property importance for playlist creation.\nonly the head sections of each song in a playlist. (2) 30sec-\nplaylist: a user continuously listens to only the parts after\nthe ﬁrst 30 seconds of each song in a playlist. We adopted\n30 seconds according to the preview samples on a music\nstreaming service (Deezer) [85].\nSimilarly to the investigation described in section 4, we\nhad the participants listen to each type of playlist by select-\ning options for two choices. In the head-playlist case, the\ntwo choices were 1-second crossfade (options: “on” and\n“off”) and the playing time from the head of each song\n(options: “15 seconds” and “30 seconds”). In the case\nof 30sec-playlist, the two choices were 1-second cross-\nfade (options: “on” and “off”) and the playing time after\nthe ﬁrst 30 seconds of each song (options: “15 seconds”\nand “30 seconds”). The participants listened to playlists\nwith each playback method by using their favorite option\ncombinations. Here, each participant was assigned three\nplaylists containing the same songs as those in section 4.\nThen, they were asked to rate their willingness to listen to\nself-made playlists with each method on a 6-point scale.\n7.2 Results\nFigure 6 shows the results. Note that the chorus-playlist\nresults are repeated from “self-made playlists” in Figure 3.\nPaired Wilcoxon signed-rank tests with Bonferroni correc-\ntion revealed that the median for chorus-playlist was statis-\ntically higher than the medians for head-playlist and 30sec-\nplaylist. It thus became clear that it was not enough to sim-\nply play any part of the songs in a playlist continuously, but\nthat it was important for users to be able to play choruses\ncontinuously.\n8. DISCUSSION AND CONCLUSION\nIn this paper, we have studied the concept of chorus-\nplaylist. The reusable insights obtained from our user sur-\nvey can be summarized as follows.\n• We showed that there is a high demand for chorus-\nplaylist. When the participants listened to songs in a\nplaylist with chorus-playlist, they tended to prefer to\nlisten to 5 seconds before the chorus, add crossfade\ntransitions between songs, and listen to the chorus from**\n**\nFigure 6 . Willingness for three playlist types.\nbeginning to end. As discussed in section 7.2, it is more\nimportant to play choruses continuously than to play\nother sections continuously.\n• As seen in Table 2, the objectives for listening to mu-\nsic with chorus-playlist were largely different between\nself-made playlists and others’ playlists. In particular,\ncertain objectives for self-made playlists were unex-\npected, in that people wanted to enjoy music in a new\nway with chorus-playlist for objectives such as boost-\ning their mood. These results could provide guidelines\nfor researchers and services to consider new research\ntopics and activate user interaction, respectively.\n• We revealed a high demand for creating a chorus-\nplaylist. As in Figure 5, hit songs, new songs, and the\ndiversities of artists, genres, and moods are more im-\nportant when creating a chorus-playlist than when cre-\nating a usual playlist. These results also provide new\nviewpoints for studies on assisted playlist creation.\nWe acknowledge a limitation of this paper in that all\nthe participants in our user survey were Japanese. Because\npeoples’ music preferences, listening behaviors, and mu-\nsic itself vary widely from country to country [86–90], not\nall of the ﬁndings reported here can be generalized. Nev-\nertheless, we believe that this study provides a worthwhile\ncontribution as a ﬁrst step toward understanding the impact\nof the chorus-playlist concept. At the same time, this lim-\nitation indicates further possibilities such as investigating\nthe differences among countries and cultures. The publicly\navailable dataset of results from our user survey will enable\nresearchers to perform such comparisons.\nAnother limitation is that the participants did not expe-\nrience chorus-playlist on the music streaming services they\nusually used. However, because they answered the survey\nafter experiencing the chorus-playlist concept by listening\nto the playlists that we provided, we think that they could\nsufﬁciently imagine the situation of listening to self-made\nplaylists and others’ playlists with the chorus-playlist ap-\nproach. We currently provide the chorus-playlist function\nin a music-related smartphone application (V ocacolle App)\nand web service (Kiite8). In the future, we will investigate\nthe function’s usage in those more realistic environments.\nFinally, although we considered only the ﬁrst chorus of\na song (except when a song started with the chorus), the\nﬁnal chorus tends to be longer and contain heavier instru-\nmentation than other choruses [1]. Therefore, it would also\nbe an interesting future work to investigate the impact of\ndifferences between choruses in chorus-playlist listening.\nMoreover, the concept of listening to only choruses can\nbe applied not only to playlists but also to other song lists\nsuch as album track lists, which could further enrich and\ndiversify people’s music listening experience.\n8https://kiite.jpProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n7879. ACKNOWLEDGMENTS\nThis work was supported in part by JST CREST Grant\nNumber JPMJCR20D4 and JSPS KAKENHI Grant Num-\nber JP21H04917, Japan.\n10. REFERENCES\n[1] K. Stephenson, What to listen for in rock: A stylistic\nanalysis . Yale University Press, 2002.\n[2] J. Covach, “Form in rock music,” Engaging music: Es-\nsays in music analysis , pp. 65–76, 2005.\n[3] W. Everett, The foundations of rock: from “Blue suede\nshoes” to “Suite: Judy blue eyes” . Oxford University\nPress, 2008.\n[4] R. Middleton, “Song form,” The Continuum Encyclo-\npedia of Popular Music of the World , vol. 2, pp. 513–\n519, 2003.\n[5] A. Eronen, “Chorus detection with combined use of\nMFCC and chroma features and image processing ﬁl-\nters,” in Proceedings of the 10th International Confer-\nence on Digital Audio Effects , ser. DAFx 2007, 2007,\npp. 229–236.\n[6] S. Hult, L. B. Kreiberg, S. S. Brandt, and B. T. Jónsson,\n“Analysis of the effect of dataset construction method-\nology on transferability of music emotion recogni-\ntion models,” in Proceedings of the 2020 International\nConference on Multimedia Retrieval , ser. ICMR 2020,\n2020, pp. 316–320.\n[7] I. Salakka, A. Pitkäniemi, E. Pentikäinen, K. Mikko-\nnen, P. Saari, P. Toiviainen, and T. Särkämö, “What\nmakes music memorable? Relationships between\nacoustic musical features and music-evoked emotions\nand memories in older adults,” PlOS ONE , vol. 16,\nno. 5, pp. 1–18, 2021.\n[8] J. Paulus, M. Müller, and A. Klapuri, “State of the art\nreport: Audio-based music structure analysis,” in Pro-\nceedings of the 11th International Society for Music\nInformation Retrieval Conference , ser. ISMIR 2010,\n2010, pp. 625–636.\n[9] M. Müller, P. Grosche, and N. Jiang, “A segment-based\nﬁtness measure for capturing repetitive structures of\nmusic recordings,” in Proceedings of the 12th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ser. ISMIR 2011, 2011, pp. 615–620.\n[10] J. Foote, “Automatic audio segmentation using a mea-\nsure of audio novelty,” in Proceedings of the 2000\nIEEE International Conference on Multimedia and\nExpo , ser. ICME 2000, 2000, pp. 452–455.\n[11] M. Cooper and J. Foote, “Summarizing popular music\nvia structural similarity analysis,” in Proceedings of the\n2003 IEEE Workshop on Applications of Signal Pro-\ncessing to Audio and Acoustics , ser. WASPAA 2003,\n2003, pp. 127–130.[12] M. Goto, “A chorus section detection method for mu-\nsical audio signals and its application to a music listen-\ning station,” IEEE Transactions on Audio, Speech, and\nLanguage Processing , vol. 14, no. 5, pp. 1783–1794,\n2006.\n[13] M. Müller and S. Ewert, “Joint structure analysis with\napplications to music annotation and synchronization,”\ninProceedings of the 9th International Conference on\nMusic Information Retrieval , ser. ISMIR 2008, 2008,\npp. 389–394.\n[14] J. Paulus and A. Klapuri, “Music structure analysis us-\ning a probabilistic ﬁtness measure and a greedy search\nalgorithm,” IEEE Transactions on Audio, Speech, and\nLanguage Processing , vol. 17, no. 6, pp. 1159–1170,\n2009.\n[15] J. Serrà, M. Müller, P. Grosche, and J. L. Arcos, “Unsu-\npervised detection of music boundaries by time series\nstructure features,” in Proceedings of the 26th AAAI\nConference on Artiﬁcial Intelligence , ser. AAAI 2012,\n2012, pp. 1613–1619.\n[16] A. Baratè, L. A. Ludovico, and E. Santucci, “A\nsemantics-driven approach to lyrics segmentation,” in\nProceedings of the 8th International Workshop on Se-\nmantic and Social Media Adaptation and Personaliza-\ntion, ser. SMAP 2013, 2013, pp. 73–79.\n[17] O. Nieto and T. Jehan, “Convex non-negative matrix\nfactorization for automatic music structure identiﬁca-\ntion,” in Proceedings of the 2013 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning, ser. ICASSP 2013, 2013, pp. 236–240.\n[18] F. Kaiser and G. Peeters, “A simple fusion method of\nstate and sequence segmentation for music structure\ndiscovery,” in Proceedings of the 14th International\nSociety for Music Information Retrieval Conference ,\nser. ISMIR 2013, 2013, pp. 257–262.\n[19] H. Grohganz, M. Clausen, N. Jiang, and M. Müller,\n“Converting path structures into block structures using\neigenvalue decompositions of self-similarity matrices,”\ninProceedings of the 14th International Society for\nMusic Information Retrieval Conference , ser. ISMIR\n2013, 2013, pp. 209–214.\n[20] G. Peeters and V . Bisot, “Improving music structure\nsegmentation using lag-priors,” in Proceedings of the\n15th International Society for Music Information Re-\ntrieval Conference , ser. ISMIR 2014, 2014, pp. 337–\n342.\n[21] B. McFee and D. Ellis, “Analyzing song structure with\nspectral clustering,” in Proceedings of the 15th Inter-\nnational Society for Music Information Retrieval Con-\nference , ser. ISMIR 2014, 2014, pp. 405–410.\n[22] C.-H. Yeh, W.-Y . Tseng, C.-Y . Chen, Y .-D. Lin, Y .-R.\nTsai, H.-I. Bi, Y .-C. Lin, and H.-Y . Lin, “Popular musicProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n788representation: Chorus detection & emotion recogni-\ntion,” Multimedia Tools and Applications , vol. 73, pp.\n2103–2128, 2014.\n[23] T. Grill and J. Schlüter, “Music boundary detection us-\ning neural networks on combined features and two-\nlevel annotations,” in Proceedings of the 16th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ser. ISMIR 2015, 2015, pp. 531–537.\n[24] J. B. L. Smith and M. Goto, “Using priors to improve\nestimates of music structure,” in Proceedings of the\n17th International Society for Music Information Re-\ntrieval Conference , ser. ISMIR 2016, 2016, pp. 554–\n560.\n[25] K. Watanabe, Y . Matsubayashi, N. Orita, N. Okazaki,\nK. Inui, S. Fukayama, T. Nakano, J. B. L. Smith, and\nM. Goto, “Modeling discourse segments in lyrics using\nrepeated patterns,” in Proceedings of the 26th Interna-\ntional Conference on Computational Linguistics , ser.\nCOLING 2016, 2016, pp. 1959–1969.\n[26] G. Sargent, F. Bimbot, and E. Vincent, “Estimating\nthe structural segmentation of popular music pieces\nunder regularity constraints,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing , vol. 25,\nno. 2, pp. 344–358, 2017.\n[27] T. Cheng, J. B. L. Smith, and M. Goto, “Music struc-\nture boundary detection and labelling by a decon-\nvolution of path-enhanced self-similarity matrix,” in\nProceedings of the 2018 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing , ser.\nICASSP 2018, 2018, pp. 106–110.\n[28] M. Fell, Y . Nechaev, E. Cabrio, and F. Gandon, “Lyrics\nsegmentation: Textual macrostructure detection using\nconvolutions,” in Proceedings of the 27th International\nConference on Computational Linguistics , ser. COL-\nING 2018, 2018, pp. 2044–2054.\n[29] Y . Huang, S. Chou, and Y . Yang, “Pop Music High-\nlighter: Marking the emotion keypoints,” Transactions\nof the International Society for Music Information Re-\ntrieval , vol. 1, no. 1, pp. 68–78, 2018.\n[30] A. Maezawa, “Music boundary detection based on a\nhybrid deep model of novelty, homogeneity, repetition\nand duration,” in Proceedings of the 2019 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing , ser. ICASSP 2019, 2019, pp. 206–210.\n[31] G. Shibata, R. Nishikimi, E. Nakamura, and K. Yoshii,\n“Statistical music structure analysis based on a\nhomogeneity-, repetitiveness-, and regularity-aware hi-\nerarchical hidden semi-markov model,” in Proceedings\nof the 20th International Society for Music Information\nRetrieval Conference , ser. ISMIR 2019, 2019, pp. 268–\n275.[32] G. Shibata, R. Nishikimi, and K. Yoshii, “Music\nstructure analysis based on an LSTM-HSMM hybrid\nmodel,” in Proceedings of the 21th International So-\nciety for Music Information Retrieval Conference , ser.\nISMIR 2020, 2020, pp. 23–29.\n[33] K. Watanabe and M. Goto, “A chorus-section detec-\ntion method for lyrics text,” in Proceedings of the 21st\nInternational Society for Music Information Retrieval\nConference , ser. ISMIR 2020, 2020, pp. 351–359.\n[34] J. Wang, J. B. L. Smith, J. Chen, X. Song, and Y . Wang,\n“Supervised chorus detection for popular music using\nconvolutional neural network and multi-task learning,”\ninProceedings of the 2021 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing , ser.\nICASSP 2021, 2021, pp. 566–570.\n[35] J. Wang, Y . Hung, and J. B. L. Smith, “To catch a cho-\nrus, verse, intro, or anything else: Analyzing a song\nwith structural functions,” in Proceedings of the 2022\nIEEE International Conference on Acoustics, Speech\nand Signal Processing , ser. ICASSP 2022, 2022, pp.\n416–420.\n[36] S. Gao and H. Li, “Popular song summarization us-\ning chorus section detection from audio signal,” in Pro-\nceedings of the IEEE 17th International Workshop on\nMultimedia Signal Processing , ser. MMSP 2015, 2015,\npp. 1–6.\n[37] Y . Gao, Y . Shen, X. Zhang, S. Yu, and W. Li, “Music\nsummary detection with state space embedding and re-\ncurrence plot,” in Proceedings of the 6th Conference on\nSound and Music Technology , ser. CSMT 2019, 2019,\npp. 41–51.\n[38] K. Zhang, H. Zhang, S. Li, C. Yang, and L. Sun, “The\nPMEmo dataset for music emotion recognition,” in\nProceedings of the 2018 ACM on International Confer-\nence on Multimedia Retrieval , ser. ICMR 2018, 2018,\npp. 135–142.\n[39] J. H. Lee and N. M. Waterman, “Understanding user\nrequirements for music information services,” in Pro-\nceedings of the 13th International Society for Music\nInformation Retrieval Conference , ser. ISMIR 2012,\n2012, pp. 253–258.\n[40] L. Porcaro and E. Gomez, “20 years of playlists: A\nstatistical analysis on popularity and diversity,” in Pro-\nceedings of the 20th International Society for Music\nInformation Retrieval Conference , ser. ISMIR 2019,\n2019, pp. 130–136.\n[41] I. Kamehkhosh, G. Bonnin, and D. Jannach, “Effects of\nrecommendations on the playlist creation behavior of\nusers,” User Modeling and User-Adapted Interaction ,\nvol. 30, no. 2, pp. 285–322, 2020.\n[42] H. V . Schweiger, E. Parada-Cabaleiro, and M. Schedl,\n“Does track sequence in user-generated playlists mat-\nter?.” in Proceedings of the 22nd International SocietyProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n789for Music Information Retrieval Conference , ser. IS-\nMIR 2021, 2021, pp. 618–625.\n[43] K. Sakurai, R. Togo, T. Ogawa, and M. Haseyama,\n“Controllable music playlist generation based on\nknowledge graph and reinforcement learning,” Sen-\nsors, vol. 22, no. 10, pp. 135–142, 2022.\n[44] G. Bonnin and D. Jannach, “Automated generation of\nmusic playlists: Survey and experiments,” ACM Com-\nputing Surveys , vol. 47, no. 2, pp. 1–35, 2014.\n[45] K. Jacobson, V . Murali, E. Newett, B. Whitman, and\nR. Yon, “Music personalization at Spotify,” in Proceed-\nings of the 10th ACM Conference on Recommender\nSystems , ser. RecSys 2016, 2016, p. 373.\n[46] T. Bontempelli, B. Chapus, F. Rigaud, M. Morlon,\nM. Lorant, and G. Salha-Galvan, “Flow Moods: Rec-\nommending music by moods on deezer,” in Proceed-\nings of the 16th ACM Conference on Recommender\nSystems , ser. RecSys 2022, 2022, pp. 452–455.\n[47] J. J. Aucouturier and F. Pachet, “Scaling up music\nplaylist generation,” in Proceedings of the 2002 IEEE\nInternational Conference on Multimedia and Expo , ser.\nICME 2002, 2002, pp. 105–108.\n[48] B. Logan, “Content-based playlist generation: Ex-\nploratory experiments,” in Proceedings of the 3rd\nInternational Conference on Music Information Re-\ntrieval , ser. ISMIR 2002, 2002, pp. 295–296.\n[49] S. Pauws and B. Eggen, “PATS: Realization and user\nevaluation of an automatic playlist generator,” in Pro-\nceedings of the 3rd International Conference on Mu-\nsic Information Retrieval , ser. ISMIR 2002, 2002, pp.\n222–230.\n[50] R. V . Gulik and F. Vignoli, “Visual playlist generation\non the artist map,” in Proceedings of the 6th Interna-\ntional Conference on Music Information Retrieval , ser.\nISMIR 2005, 2005, pp. 520–523.\n[51] E. Pampalk, T. Pohle, and G. Widmer, “Dynamic\nplaylist generation based on skipping behavior,” in\nProceedings of the 6th International Conference on\nMusic Information Retrieval , ser. ISMIR 2005, 2005,\npp. 634–637.\n[52] S. Pauws and S. V . D. Wijdeven, “User evaluation of\na new interactive playlist generation concept,” in Pro-\nceedings of the 6th International Conference on Mu-\nsic Information Retrieval , ser. ISMIR 2005, 2005, pp.\n638–643.\n[53] S. J. Cunningham, D. Bainbridge, and A. Falconer,\n““more of an art than a science”: Supporting the cre-\nation of playlists and mixes,” in Proceedings of the 7th\nInternational Conference on Music Information Re-\ntrieval , ser. ISMIR 2006, 2006, pp. 240–245.[54] E. Pampalk and M. Gasser, “An implementation of\na simple playlist generator based on audio similarity\nmeasures and user feedback,” in Proceedings of the 7th\nInternational Conference on Music Information Re-\ntrieval , ser. ISMIR 2006, 2006, pp. 389–390.\n[55] N. Oliver and L. Kreger-Stickles, “PAPA: Physiology\nand purpose-aware automatic playlist generation,” in\nProceedings of the 7th International Conference on\nMusic Information Retrieval , ser. ISMIR 2006, 2006,\npp. 250–253.\n[56] S. Pauws, W. Verhaegh, and M. V ossen, “Fast gener-\nation of optimal music playlists using local search,”\ninProceedings of the 7th International Conference on\nMusic Information Retrieval , ser. ISMIR 2006, 2006,\npp. 138–143.\n[57] B. Fields, C. Rhodes, M. A. Casey, and K. Jacob-\nson, “Social playlists and bottleneck measurements:\nExploiting musician social graphs using content-based\ndissimilarity and pairwise maximum ﬂow values,” in\nProceedings of the 9th International Conference on\nMusic Information Retrieval , ser. ISMIR 2008, 2008,\npp. 559–564.\n[58] A. Flexer, D. Schnitzer, M. Gasser, and G. Widmer,\n“Playlist generation using start and end songs,” in Pro-\nceedings of the 9th International Conference on Mu-\nsic Information Retrieval , ser. ISMIR 2008, 2008, pp.\n173–178.\n[59] K. Bosteels, E. Pampalk, and E. E. Kerre, “Evaluat-\ning and analysing dynamic playlist generation heuris-\ntics using radio logs and fuzzy set theory,” in Proceed-\nings of the 10th International Society for Music Infor-\nmation Retrieval Conference , ser. ISMIR 2009, 2009,\npp. 351–356.\n[60] F. Maillet, D. Eck, G. Desjardins, and P. Lamere,\n“Steerable playlist generation by learning song simi-\nlarity from radio station playlists,” in Proceedings of\nthe 10th International Society for Music Information\nRetrieval Conference , ser. ISMIR 2009, 2009, pp. 345–\n350.\n[61] B. McFee and G. R. G. Lanckriet, “The natural lan-\nguage of playlists,” in Proceedings of the 12th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ser. ISMIR 2011, 2011, pp. 537–542.\n[62] J. H. Lee, “How similar is too similar?: Exploring\nusers’ perceptions of similarity in playlist evaluation,”\ninProceedings of the 12th International Society for\nMusic Information Retrieval Conference , ser. ISMIR\n2011, 2011, pp. 109–114.\n[63] B. McFee and G. R. G. Lanckriet, “Hypergraph models\nof playlist dialects,” in Proceedings of the 13th Inter-\nnational Society for Music Information Retrieval Con-\nference , ser. ISMIR 2012, 2012, pp. 343–348.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n790[64] J. L. Moore, S. Chen, T. Joachims, and D. Turnbull,\n“Learning to embed songs and tags for playlist predic-\ntion,” in Proceedings of the 13th International Society\nfor Music Information Retrieval Conference , ser. IS-\nMIR 2012, 2012, pp. 349–354.\n[65] G. Bonnin and D. Jannach, “Evaluating the quality\nof generated playlists based on hand-crafted samples,”\ninProceedings of the 14th International Society for\nMusic Information Retrieval Conference , ser. ISMIR\n2013, 2013, pp. 263–268.\n[66] D. Jannach, L. Lerche, and I. Kamehkhosh, “Beyond\n“hitting the hits”: Generating coherent music playlist\ncontinuations with the right tracks,” in Proceedings of\nthe 9th ACM Conference on Recommender Systems ,\nser. RecSys ’15, 2015, pp. 187–194.\n[67] T. Nakano, J. Kato, M. Hamasaki, and M. Goto,\n“PlaylistPlayer: An interface using multiple criteria to\nchange the playback order of a music playlist,” in Pro-\nceedings of the 21st International Conference on Intel-\nligent User Interfaces , ser. IUI 2016, 2016, pp. 186–\n190.\n[68] M. Pichl, E. Zangerle, and G. Specht, “Understand-\ning playlist creation on music streaming platforms,” in\nProceedings of the 2016 IEEE International Sympo-\nsium on Multimedia , ser. ISM 2016, 2016, pp. 475–\n480.\n[69] C. Chung, Y . Chen, and H. H. Chen, “Exploiting\nplaylists for representation of songs and words for text-\nbased music retrieval,” in Proceedings of the 18th In-\nternational Society for Music Information Retrieval\nConference , ser. ISMIR 2017, 2017, pp. 478–485.\n[70] R. Dias, D. Gonçalves, and M. J. Fonseca, “From man-\nual to assisted playlist creation: A survey,” Multimedia\nTools and Applications , vol. 76, no. 12, pp. 14 375–\n14 403, 2017.\n[71] L. F. Pontello, P. H. F. Holanda, B. Guilherme, J. P. V .\nCardoso, O. Goussevskaia, and A. P. C. D. Silva,\n“Mixtape: Using real-time user feedback to navigate\nlarge media collections,” ACM Transactions on Multi-\nmedia Computing, Communications, and Applications ,\nvol. 13, no. 4, pp. 1–22, 2017.\n[72] R. M. Bittner, M. Gu, G. Hernandez, E. J. Humphrey,\nT. Jehan, H. McCurry, and N. Montecchio, “Automatic\nplaylist sequencing and transitions,” in Proceedings of\nthe 18th International Society for Music Information\nRetrieval Conference , ser. ISMIR 2017, 2017, pp. 442–\n448.\n[73] S. Shih and H. Chi, “Automatic, personalized, and ﬂex-\nible playlist generation using reinforcement learning,”\ninProceedings of the 19th International Society for\nMusic Information Retrieval Conference , ser. ISMIR\n2018, 2018, pp. 168–174.[74] S. Y . Park, A. Laplante, J. H. Lee, and B. Kaneshiro,\n“Tunes together: Perception and experience of collab-\norative playlists,” in Proceedings of the 20th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ser. ISMIR 2019, 2019, pp. 723–730.\n[75] A. Patwari, N. Kong, J. Wang, U. Gargi, M. Covell, and\nA. Jansen, “Semantically meaningful attributes from\nco-listen embeddings for playlist exploration and ex-\npansion,” in Proceedings of the 21st International So-\nciety for Music Information Retrieval Conference , ser.\nISMIR 2020, 2020, pp. 527–533.\n[76] Z. Li, M. Song, S. Duan, and Z. Wang, “Are users\nattracted by playlist titles and covers? Understand-\ning playlist selection behavior on a music streaming\nplatform,” Journal of Innovation & Knowledge , vol. 7,\nno. 3, pp. 1–14, 2022.\n[77] J. V . Balen, J. A. Burgoyne, F. Wiering, and R. C.\nVeltkamp, “An analysis of chorus features in popular\nsong,” in Proceedings of the 14th Society of Music\nInformation Retrieval Conference , ser. ISMIR 2013,\n2013, pp. 107–112.\n[78] M. Goto, K. Yoshii, H. Fujihara, M. Mauch, and\nT. Nakano, “Songle: A web service for active music\nlistening improved by user contributions,” in Proceed-\nings of the 12th International Society for Music Infor-\nmation Retrieval Conference , ser. ISMIR 2011, 2011,\npp. 311–316.\n[79] A. Kim, S. Park, J. Park, J.-W. Ha, T. Kwon, and\nJ. Nam, “Automatic DJ mix generation using highlight\ndetection,” in Proceedings of the 18th International So-\nciety for Music Information Retrieval Conference , ser.\nISMIR 2017, 2017, pp. 1–2.\n[80] Y .-S. Huang, S.-Y . Chou, and Y .-H. Yang, “Generat-\ning music medleys via playing music puzzle games,” in\nProceedings of the 32nd AAAI Conference on Artiﬁcial\nIntelligence , ser. AAAI 2018, 2018, pp. 2281–2288.\n[81] A. Bassoli, J. Moore, S. Agamanolis, and H. C. Group,\n“tunA: Local music sharing with handheld Wi-Fi de-\nvices,” in Proceedings of the 5th Wireless World Con-\nference 2004 , ser. WWC 2004, 2004, pp. 1–23.\n[82] M. Håkansson, M. Rost, and L. E. Holmquist, “Gifts\nfrom friends and strangers: A study of mobile music\nsharing,” in Proceedings of the 10th European Confer-\nence on Computer-Supported Cooperative Work , ser.\nECSCW 2007, 2007, pp. 311–330.\n[83] M. Håkansson, M. Rost, M. Jacobsson, and L. E.\nHolmquist, “Facilitating mobile music sharing and so-\ncial interaction with Push!Music,” in Proceedings of\nthe 40th Annual Hawaii International Conference on\nSystem Sciences , ser. HICSS 2007, 2007, pp. 87–96.\n[84] K. Tsukuda, K. Ishida, M. Hamasaki, and M. Goto,\n“Kiite Cafe: A web service for getting together vir-\ntually to listen to music.” in Proceedings of the 22ndProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n791International Society for Music Information Retrieval\nConference , ser. ISMIR 2021, 2021, pp. 697–704.\n[85] K. M. Ibrahim, J. Royo-Letelier, E. V . Epure,\nG. Peeters, and G. Richard, “Audio-based auto-tagging\nwith contextual tags for music,” in Proceedings of\nthe 2020 IEEE International Conference on Acous-\ntics, Speech and Signal Processing , ser. ICASSP 2020,\n2020, pp. 16–20.\n[86] X. Hu and J. H. Lee, “A cross-cultural study of mu-\nsic mood perception between American and Chinese\nlisteners,” in Proceedings of the 13th International So-\nciety for Music Information Retrieval Conference , ser.\nISMIR 2012, 2012, pp. 535–540.\n[87] Y . Yang and X. Hu, “Cross-cultural music mood classi-\nﬁcation: A comparison on English and Chinese songs,”\ninProceedings of the 13th International Society for\nMusic Information Retrieval Conference , ser. ISMIR\n2012, 2012, pp. 19–24.\n[88] X. Hu, J. H. Lee, K. Choi, and J. S. Downie, “A cross-\ncultural study on the mood of K-POP songs,” in Pro-\nceedings of the 15th International Society for Music\nInformation Retrieval Conference , ser. ISMIR 2014,\n2014, pp. 385–390.\n[89] M. Liu, X. Hu, and M. Schedl, “Artist preferences and\ncultural, socio-economic distances across countries: A\nbig data perspective,” in Proceedings of the 18th Inter-\nnational Society for Music Information Retrieval Con-\nference , ser. ISMIR 2017, 2017, pp. 103–111.\n[90] C. Bauer and M. Schedl, “Global and country-speciﬁc\nmainstreaminess measures: Deﬁnitions, analysis, and\nusage for improving personalized music recommenda-\ntion systems,” PLOS ONE , vol. 14, no. 6, pp. 1–36,\n2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n792"
    },
    {
        "title": "Unveiling the Impact of Musical Factors in Judging a Song on First Listen: Insights From a User Survey.",
        "author": [
            "Kosetsu Tsukuda",
            "Tomoyasu Nakano",
            "Masahiro Hamasaki",
            "Masataka Goto"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265351",
        "url": "https://doi.org/10.5281/zenodo.10265351",
        "ee": "https://zenodo.org/records/10265351/files/000066.pdf",
        "abstract": "When a user listens to a song for the first time, what musical factors (e.g., melody, tempo, and lyrics) influence the user's decision to like or dislike the song? An answer to this question would enable researchers to more deeply understand how people interact with music. Thus, in this paper, we report the results of an online survey involving 302 participants to investigate the influence of 10 musical factors. We also evaluate how a user's personal characteristics (i.e., personality traits and musical sophistication) relate to the importance of each factor for the user. Moreover, we propose and evaluate three factor-based functions that would enable more effectively browsing songs on a music streaming service. The user survey results provide several reusable insights, including the following: (1) for most participants, the melody and singing voice are important factors in judging whether they like a song on first listen; (2) personal characteristics do influence the important factors (e.g., participants who have high openness and are sensitive to beat deviations emphasize melody); and (3) the proposed functions each have a certain level of demand because they enable users to easily find music that fits their tastes. We have released part of the survey results as publicly available data so that other researchers can reproduce the results and analyze the data from their own viewpoints.",
        "zenodo_id": 10265351,
        "dblp_key": "conf/ismir/TsukudaNHG23",
        "keywords": [
            "users decision to like or dislike the song",
            "musical factors",
            "online survey",
            "302 participants",
            "influence of 10 musical factors",
            "personal characteristics",
            "music streaming service",
            "reusable insights",
            "user survey results",
            "publicly available data"
        ],
        "content": "UNVEILING THE IMPACT OF MUSICAL FACTORS IN JUDGING A SONG\nON FIRST LISTEN: INSIGHTS FROM A USER SURVEY\nKosetsu Tsukuda Tomoyasu Nakano Masahiro Hamasaki Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\n{k.tsukuda, t.nakano, masahiro.hamasaki, m.goto}@aist.go.jp\nABSTRACT\nWhen a user listens to a song for the ﬁrst time, what mu-\nsical factors (e.g., melody, tempo, and lyrics) inﬂuence the\nuser’s decision to like or dislike the song? An answer to\nthis question would enable researchers to more deeply un-\nderstand how people interact with music. Thus, in this\npaper, we report the results of an online survey involving\n302 participants to investigate the inﬂuence of 10 musical\nfactors. We also evaluate how a user’s personal charac-\nteristics (i.e., personality traits and musical sophistication)\nrelate to the importance of each factor for the user. More-\nover, we propose and evaluate three factor-based functions\nthat would enable more effectively browsing songs on a\nmusic streaming service. The user survey results provide\nseveral reusable insights, including the following: (1) for\nmost participants, the melody and singing voice are im-\nportant factors in judging whether they like a song on ﬁrst\nlisten; (2) personal characteristics do inﬂuence the impor-\ntant factors (e.g., participants who have high openness and\nare sensitive to beat deviations emphasize melody); and (3)\nthe proposed functions each have a certain level of demand\nbecause they enable users to easily ﬁnd music that ﬁts their\ntastes. We have released part of the survey results as pub-\nlicly available data so that other researchers can reproduce\nthe results and analyze the data from their own viewpoints.\n1. INTRODUCTION\nWhen a user listens to a song for the ﬁrst time on a music\nstreaming service and it matches her taste, she may listen\nto it until the end or add it to her favorites or a playlist.\nOn the other hand, if the song does not match the user’s\npreferences, she may stop playing it partway through [1,2].\nBy accumulating logs of such listening behaviors, music\nstreaming services can estimate users’ music preferences\nand implement functions such as recommendations [3, 4].\nHowever, when a user ﬁrst listens to a song and de-\ncides whether or not she likes it, which musical factors\ninﬂuence the decision? For example, one user may like\na song because of its lyrics, another may like it because\nof its melody, and third may like it because of the sound\nof a musical instrument. Several prior studies investigated\n© K. Tsukuda, T. Nakano, M. Hamasaki, and M. Goto. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: K. Tsukuda, T. Nakano, M. Hamasaki, and\nM. Goto, “Unveiling the Impact of Musical Factors in Judging a Song\non First Listen: Insights from a User Survey”, in Proc. of the 24th Int.\nSociety for Music Information Retrieval Conf., Milan, Italy, 2023.people’s preferred musical factors [5–7]. However, those\nstudies targeted songs that the study participants already\nliked and investigated the reasons for liking those songs in\nterms of factors that were speciﬁc to the songs. Accord-\ningly, when a participant answered that she liked a certain\nsong because of its lyrics, it was unclear that she would\nalways judge whether she liked or disliked a song because\nof its lyrics. Thus, despite those studies, there is a lack\nof research on the musical factors that inﬂuence people’s\njudgment on whether they like a song on ﬁrst listen. This\nlack of research motivates our ﬁrst research question:\nRQ1 When people listen to a song for the ﬁrst time and\njudge whether they like it, which musical factors af-\nfect this judgment, and to what extent?\nTo more deeply understand how people interact with\nmusic, the effects of users’ personality traits and musical\nsophistication on their music preferences and listening be-\nhaviors have also been studied [5, 8–21]. For example, it\nhas been reported that people with high openness tend to\nshow a preference for folk music [16] and that musical so-\nphistication positively inﬂuences recommendation accep-\ntance [20]. Following such studies, we address the second\nresearch question:\nRQ2 How do people’s personality traits and musical so-\nphistication affect the importance of each musical\nfactor in judging whether they like a song?\nIf a certain musical factor inﬂuences judgments about\nsong preferences, it would be useful to propose practical\nexamples of its engineering use. In fact, proposed im-\nprovements to the functions of music streaming services\nfrom user study results have provided useful insights to\nthe music information retrieval (MIR) community [22–35].\nHence, we investigate a third research question:\nRQ3 What are the implications of musical factors for the\nfunctions of music streaming services?\nTo address these research questions, we targeted 10 mu-\nsical factors and conducted a questionnaire-based online\nuser survey involving 302 participants. Our main contribu-\ntions can be summarized as follows.\n• We reveal that the factors of melody andsinging voice\nhave large inﬂuences on music preference judgment,\nwhereas the factor of danceability has a small inﬂuence.\n• From a psychological perspective, we show that both\npersonality traits and musical sophistication affect the\nimportance of the various musical factors. Given these\nresults, we discuss the possibility that the important fac-\ntors for a particular user could be estimated from the\nuser’s listening behaviors on a music streaming service.561• From an engineering perspective, we propose three\nfunctions that would enable users to effectively browse\nsongs by leveraging musical factors, and we show that\neach function has a certain level of demand.\n• We have made the English translation of the survey\nquestionnaire and the survey results publicly available\non the web to support future studies1.\n2. RELATED WORK\n2.1 Musical Factors\nUnderstanding why people listen to music has been of in-\nterest to researchers. One typical research direction fo-\ncuses on the motivation to listen to music in daily life.\nThe main reasons include emotional reasons such as re-\nlaxation [18, 36–39] and relief [40, 41]. People also listen\nto music to concentrate and to pass time [42].\nAnother research direction investigates the reasons for\nlistening to speciﬁc preferred songs in terms of musical\nfactors. Greasley et al. [6] conducted interviews about par-\nticipants’ music collections. Among the main reasons why\nthe participants liked their collections were musical fac-\ntors such as the lyrics and instruments. Sanﬁlippo et al. [7]\nasked participants to sample two songs from their music\nlibrary on a listening device and answer questions such as\n“why do you enjoy listening to the track?” The participants\noften answered the questions by using a vocabulary of mu-\nsical factors. Boyle et al. [5] investigated the inﬂuence of\nmusical factors on young people’s pop music preferences.\nEach participant listed his/her three favorite pop songs and\nrated the importance of various musical factors in liking\nthose songs. The results revealed that melody, mood, and\nrhythm had large inﬂuences. Although these studies inves-\ntigated the inﬂuences of musical factors, they focused on\nonly songs that the participants already liked. Our study is\ndifferent in that we focus on the musical factors that peo-\nple emphasize when they listen to a song for the ﬁrst time.\nSince there is a vast number of songs that people have not\nyet listened to, investigating such factors is beneﬁcial to\nsupport ﬁnding songs that match their preferences.\n2.2 Personal Characteristics\nIn the music domain, user’s preferences, interests, and be-\nhaviors are inﬂuenced by personal characteristics. In par-\nticular, many studies have investigated the inﬂuences of\npersonality traits measured by the Big Five Inventory [8–\n14,16,17,43–47]. For example, personality has signiﬁcant\nassociations with genre preferences [11, 13, 14, 16, 43] and\naudio preferences [47]. It also inﬂuences the desired level\nof diversity in a recommended song list [46]. Ferwerda et\nal. [45] revealed that when a user browses for music, the\npreferred taxonomy (mood, activity, and genre) depends\non the user’s personality. Such personality-based results\ncan be used for personalization. In fact, several studies\nhave shown increased recommendation quality when per-\nsonality is incorporated [48–51]. Musical sophistication\nis another typical personal characteristic that inﬂuences\n1They can be downloaded from https://github.com/\nktsukuda/musical_factor .music preferences. For example, musically sophisticated\nusers listen to more diverse songs on both the artist and\ngenre levels [52], are more familiar with the songs in a\nrecommended song list [53], and prefer a less personalized\nplaylist [19]. These ﬁndings can also be used to improve\nmusic recommendations and user interfaces. Following\nthose studies, we investigate the inﬂuences of personality\ntraits and musical sophistication on the importance of mu-\nsical factors, and we suggest how its results can be used to\nimprove the recommendations.\n2.3 Design and Function Proposals\nFor user studies on music listeners’ needs, preferences, and\nbehaviors, it is common to not only report the results but\nalso propose designs and functions to improve music ser-\nvices by applying the results [22–35]. Such proposals have\nprovided reusable insights for the MIR community. Ex-\namples of these proposals include song recommendations\naccording to the user’s attention level [27], support for re-\nmote co-listening with a friend [31], and support for users\nto add their interpretations of lyrics [33]. Inspired by those\nprior studies, we propose three functions that enable music\nstreaming services to leverage musical factors. Whereas\nthe above studies only proposed designs and functions, we\nalso conducted a user study to evaluate users’ willingness\nto use the proposed functions.\n3. PARTICIPANTS\nWe recruited participants for our user study via an online\nresearch company in Japan. We limited the participants to\nthose who were Japanese and listened to music an average\nof at least one day per week via any music streaming ser-\nvice. The participants answered our questionnaire through\na web browser. We paid about 13.21 USD (1,750 JPY)\nto each participant. Although 354 participants answered\nthe survey, to make the analysis results more reliable, we\nremoved the answers from 52 participants who submitted\nimproper responses to a free-response question. The re-\nmaining 302 participants were diverse in both gender and\nage range: 147 male (10s: 4; 20s: 31; 30s: 33; 40s: 44;\n50s: 35) and 155 female (10s: 9; 20s: 39; 30s: 35; 40s: 34;\n50s: 38). Hereafter, we report the results obtained from the\n302 participants including section 6.\n4. INFLUENCE OF MUSICAL FACTORS\n4.1 Musical Factors\nReferring to prior studies on people’s favorite songs [5–7,\n54], we targeted the following 10 musical factors that may\ninﬂuence a person’s judgment of liking or disliking music\non ﬁrst listen: melody ,singing voice ,rhythm ,lyrics ,mood ,\ntempo ,harmony ,sentiment ,instruments , and danceability .\nAlthough these 10 factors are not completely independent\neach other (e.g., there would be relatively high correlation\nbetween mood andsentiment ), we adopted them to analyze\nas many factors as possible. In this study, all of these fac-\ntors were determined entirely from the music. That is, we\ndid not consider social factors that depend on the context\nof the music or the listener (e.g., the artist’s image, the pop-\nularity of music, and whether music was introduced by aProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n562/aj17 /aj22/aj17 /aj18/aj17/aj17 /aj18/aj22/aj17 /aj19/aj17/aj17 /aj19/aj22/aj17 /aj20/aj17/aj17/aj37/aj66/aj79/aj68/aj70/aj66/aj67/aj74/aj77/aj74/aj85/aj90/aj42/aj79/aj84/aj85/aj83/aj86/aj78/aj70/aj79/aj85/aj84/aj52/aj70/aj79/aj85/aj74/aj78/aj70/aj79/aj85/aj41/aj66/aj83/aj78/aj80/aj79/aj90/aj53/aj70/aj78/aj81/aj80/aj46/aj80/aj80/aj69/aj45/aj90/aj83/aj74/aj68/aj84/aj51/aj73/aj90/aj85/aj73/aj78/aj52/aj74/aj79/aj72/aj74/aj79/aj72/aj1/aj87/aj80/aj74/aj68/aj70/aj46/aj70/aj77/aj80/aj69/aj90\n/aj18/aj27/aj1/aj47/aj80/aj85/aj1/aj74/aj78/aj81/aj80/aj83/aj85/aj66/aj79/aj85\n/aj19/aj27/aj1/aj41/aj66/aj83/aj69/aj77/aj90/aj1/aj74/aj78/aj81/aj80/aj83/aj85/aj66/aj79/aj85/aj20/aj27/aj1/aj52/aj80/aj78/aj70/aj88/aj73/aj66/aj85/aj1/aj74/aj78/aj81/aj80/aj83/aj85/aj66/aj79/aj85\n/aj21/aj27/aj1/aj42/aj78/aj81/aj80/aj83/aj85/aj66/aj79/aj85/aj22/aj27/aj1/aj55/aj70/aj83/aj90/aj1/aj74/aj78/aj81/aj80/aj83/aj85/aj66/aj79/aj85\nFigure 1 . Importance distributions of musical factors (x-\naxis: number of participants).\nfriend). Rather, as this is an initial study on the inﬂuence of\nmusical factors for judging a song on ﬁrst listen, we leave\nthe investigation of such social factors for future work.\n4.2 Procedure\nFor each musical factor, we ﬁrst showed the participants\nthe factor’s name, its meaning, and a question. In the case\nofinstruments , for example, we showed the following de-\nscription to represent its meaning: “ Instruments means the\ntype of instruments used in the piece and their sounds.”\nSimilarly, we showed the following question: “How im-\nportant is the instruments in judging whether you like or\ndislike a song on ﬁrst listen?” The possible answers were\n“not important,” “hardly important,” “somewhat impor-\ntant,” “important,” and “very important.” When the an-\nswer for a factor was “not important” or “hardly impor-\ntant,” the participant was asked to respond freely on why\nit was unimportant. On the other hand, when the answer\nwas “somewhat important,” “important,” or “very impor-\ntant,” the participant was asked to respond freely with at\nleast one criterion for judging that he/she liked or disliked\na song according to the factor. The 10 musical factors were\ndisplayed in a random order to each participant.\nNote that in this survey, we asked the participants to an-\nswer the questions without actually listening to music to\navoid answer bias caused by the music they listened to for\nthe survey. Instead, they were asked to imagine daily situ-\nations where they listen to a song for the ﬁrst time and rate\nthe importance of each factor. This type of survey, which\ninvolves imagining a certain situation, is an established\nsurvey method in the MIR community [27, 31, 55–59].\n4.3 Results\nFigure 1 shows the importance distribution for each fac-\ntor. We can see that the importance was high for melody\nandsinging voice ; in fact, paired Wilcoxon signed-rank\ntests with Bonferroni correction revealed that their medi-\nans (i.e., 4) were statistically higher than the medians of the\nremaining eight factors at p <0.01. Among the remaining\neight factors, more than half of the participants gave a rat-\ning of 3, 4, or 5 for rhythm ,lyrics ,mood ,tempo ,harmony ,\nandsentiment . To more deeply understand the relation-\nships between factors, we show the Spearman’s rank corre-\nlations between them in Figure 2. There were high ( >0.4)\ncorrelations between rhythm andtempo ,mood andsenti-\nment , and melody andsinging voice . Although lyrics had a\nrelatively high average importance, it had low ( <0.3) cor-\nrelations with all other factors. Danceability , which hadMelody\nSinging voiceRhythmLyrics Mood T empoHarmony Sentiment InstrumentsSinging voice\nRhythm\nLyrics\nMood\nT empo\nHarmony\nSentiment\nInstruments\nDanceability0.42***\n0.35*** 0.27***\n0.21***0.28*** 0.12*\n0.22*** 0.31*** 0.25***0.22***\n0.29***0.26*** 0.57*** 0.11 0.30***\n0.25***0.21*** 0.32*** 0.19***0.25***0.25***\n0.23***0.29*** 0.17** 0.28*** 0.46*** 0.20*** 0.30***\n0.11* 0.21***0.29*** 0.13* 0.19** 0.22*** 0.36*** 0.17**\n0.01 0.02 0.13* 0.07 0.08 0.18** 0.21*** 0.11* 0.26***\n0.00.10.20.30.40.50.6\nSignificance of correlations\n                              *: p < 0.05\n                            **: p < 0.01\n                          ***: p < 0.001\nFigure 2 . Spearman’s rank correlations of importance be-\ntween musical factors.\nthe lowest average importance, showed a similar tendency.\nFor each factor, to analyze the free responses on crite-\nria for liking a song, we manually grouped the responses.\nBecause we allowed the participants to give more than one\ncriterion, each participant’s response could be assigned to\nmore than one group. Similarly, we grouped the responses\non criteria for disliking a song and reasons for the unim-\nportance of certain factors. Here, we omit the reasons for\nunimportance, because the most common response for all\nfactors was “I am not interested in this factor.” On the\nother hand, the criteria for liking or disliking a song were\ndiverse, as seen in Table 1, which lists the top three criteria\nfor each factor in terms of the group size. Many criteria in-\nvolved opposite terms for liked and disliked songs: in the\ncase of tempo , for example, participants who gave “fast” as\na criterion for liking a song tended to give “slow” as a cri-\nterion for disliking a song. In addition, the second column\nindicates that, for all factors, more participants gave crite-\nria for liking a song than for disliking a song, which means\nthat it was more common to have criteria for liking a song\nthan to have criteria for disliking a song. An interesting ap-\nplication of this ﬁnding would be to use criteria for liking a\nsong in explainable recommendation. For example, when\na song is recommended to a user who emphasizes melody ,\nshe may be more willing to listen to it if it appears with\nan explanation such as “this song is recommended to you\nbecause the melody is easy to remember.”\nThe results in Figure 1 are somewhat similar to those\nreported by Boyle et al. [5] (e.g., melody andrhythm had\nhigh importance, while danceability had low importance).\nNevertheless, we provide three contributions that are dis-\ntinct from their results: (1) our results are more general-\nized, because we did not focus on a speciﬁc genre and age\ngroup, whereas they focused on young people’s pop music\npreferences; (2) we analyzed the correlations between fac-\ntors and the criteria for each factor; and (3) we will publish\nthe survey results on the web to support later studies.\n5. INFLUENCE OF PERSONAL FACTORS\n5.1 Personality Traits\nProcedure. We measured the participants’ personality\ntraits in terms of ﬁve aspects (i.e., openness ,conscien-\ntiousness ,extraversion ,agreeableness , and neuroticism )\nby using the 29-item Big Five Inventory (BFI) on a 7-\npoint scale (1: strongly disagree - 7: strongly agree) [60].\nWe used the BFI because of its popularity in past stud-\nies [8–14, 16, 17, 43–47] compared to other traits such asProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n563Table 1 . Top three criteria for judging “like” and “dislike,” for each musical factor. Each number in parentheses indicates\nthe number of participants who responded with the corresponding criterion.\nFactor 1st 2nd 3rd\nMelodyLike (265) Easy to remember (35) Easy to sing or hum (33) Feels comfortable (28)\nDislike (193) Too loud (18) Difﬁcult to sing or hum (16) Feels uncomfortable (15)\nSinging voiceLike (261) Speciﬁc type (beautiful, powerful, soft, etc.) (74) V oice to my liking (54) Feels comfortable (51)\nDislike (203) Feels uncomfortable (50) Speciﬁc type (raspy, piercing, etc.) (47) V oice not to my liking (28)\nRhythmLike (237) Groovy (53) Feels comfortable (23) Rhythm to my liking (19)\nDislike (167) Rhythm not to my liking (17) Slow (16) Not groovy (15)\nLyricsLike (218) Sympathetic (71) Inspirational (41) Positive (10)\nDislike (164) Unclear meaning (41) Lack empathy (30) Pedestrian (26)\nMoodLike (219) Cheerful (51) Fits my mood/situation (25) Calm (21)\nDislike (162) Gloomy (32) Too loud (29) Feels uncomfortable (12)\nTempoLike (220) Fast (40) Groovy (29) Feels comfortable (24)\nDislike (163) Slow (48) Fast (31) Feels uncomfortable (15)\nHarmonyLike (174) Feels comfortable (43) Beautiful (23) Harmonious (22)\nDislike (116) Feels uncomfortable (25) Monotonous (7) Inharmonious (6)\nSentimentLike (163) Positive (33) Inspirational (30) Sympathetic (25)\nDislike (114) Negative (32) Evokes no emotion (12) Doesn’t ﬁt my mood/situation (7)\nInstrumentsLike (146) Include speciﬁc instruments (24) Fit the song (17) Feel comfortable (15)\nDislike (102) Too loud (24) Feel uncomfortable (11) Don’t ﬁt the song (7)\nDanceabilityLike (66) Body moves naturally to music (13) Groovy (11) Rhythmic (9)\nDislike (46) Not groovy (6) Gloomy (5) Rhythm is bad (4)\nTable 2 . Spearman’s rank correlations between personality traits and musical factor importance (N=302). Signiﬁcant\ncorrelations are shown in bold (*: p <0.05; **: p<0.01; ***: p<0.001).\nTrait Melody Singing voice Rhythm Lyrics Mood Tempo Harmony Sentiment Instruments Danceability\nOpenness 0.127* 0.135* 0.155** 0.177** 0.107 0.109 0.255*** 0.050 0.157** 0.151**\nConscientiousness 0.076 0.128* 0.062 0.031 0.127* 0.128* 0.125* 0.119* 0.028 0.013\nExtraversion 0.062 0.130* 0.172** 0.175** 0.098 0.114* 0.254*** 0.107 0.151** 0.219***\nAgreeableness 0.025 0.123* 0.048 0.088 0.158** 0.029 0.021 0.060 0.049 0.065\nNeuroticism 0.003 0.010 -0.081 0.036 -0.025 -0.004 -0.142* 0.109 -0.072 -0.120*\nopinion leadership [15].\nResults. Table 2 lists the Spearman’s rank correlations\nbetween the personality traits and the importance of each\nmusical factor. Openness had signiﬁcant correlations with\nas many as seven factors. That is, participants with higher\nopenness had more diverse criteria for judging whether a\nsong ﬁts their taste. This result is similar to a previous\nﬁnding that people with high openness tended to listen to\nmore diverse songs in terms of genres [16]. Similarly, ex-\ntraversion also had signiﬁcant correlations with many fac-\ntors, particularly, danceability . This result echoes a report\nthat people with high extraversion tended to listen to songs\nwith high danceability on a music streaming service [51].\nConscientiousness was the only trait that had a signiﬁcant\ncorrelation with sentiment . Both agreeableness andneu-\nroticism had signiﬁcant correlations with as few as two\nfactors. These results are similar to a previous ﬁnding that\nthose traits showed signiﬁcant correlations with few gen-\nres [16].\nPrior studies correlated personality traits with genre\npreferences and music audio preferences [16, 47]. For ex-\nample, people who often listen to folk music were found\nto have high openness [16]. As seen in Table 2, people\nwith high openness emphasize lyrics ; accordingly, for a\nuser who often listens to folk songs, it would be helpful to\nrecommend songs according to the similarity of lyrics.\n5.2 Musical Sophistication\nProcedure. To measure the musical sophistication, we\nused the following nine questions on a 7-point scale.\n1. InstExp: I engage in regular, daily practice of a musical\ninstrument (1: never - 7: ≥10years).2. DanceExp: I engage in regular, daily dancing (1: never\n- 7. more than 10 years).\n3. NoticeBeat: I can tell when people sing or play out of\ntime with the beat (1: strongly disagree - 7: strongly\nagree).\n4. NoticeTune: I can tell when people sing or play out of\ntune (1: strongly disagree - 7: strongly agree).\n5. LsnMusic: I listen to music (1: <15minutes per day -\n7:≥4hours per day).\n6. LsnNew: I listen to music that is new to me (1: <1\nsong per month - 7: ≥31songs per month).\n7. ViewLyrics: I view lyrics while listening to music (1:\n<1song per month - 7: ≥31songs per month).\n8. Karaoke: I sing karaoke (1: <1time per year - 7: ≥4\ntimes per week).\n9. AttEvt: I attend live music events as an audience mem-\nber (1:<1time per year - 7: ≥11times per year).\nQuestions 1, 3, 4, 5, and 9 derive from the Goldsmiths Mu-\nsical Sophistication Index (Gold-MSI) [61]. In addition,\nwe asked four questions of our own (questions 2, 6, 7, and\n8). For questions 5-9, we asked the participants to give the\naverage frequencies of those behaviors.\nResults. Table 3 lists the Spearman’s rank correlations\nbetween musical sophistication and the importance of each\nmusical factor. Overall, many of the results matched our\nintuition. For example, DanceExp had a signiﬁcantly high\ncorrelation with danceability ; participants who were sen-\nsitive to beat and tune deviations emphasized audio-based\nfactors such as melody ,singing voice , and harmony ; and\nViewLyrics had the highest correlation with lyrics . It is\nalso convincing that participants who often sang karaoke\nemphasized lyrics ; those who often attended live musicProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n564Table 3 . Spearman’s rank correlations between musical sophistication and the importance of each musical factor (N=302).\nSigniﬁcant correlations are shown in bold (*: p <0.05; **: p<0.01; ***: p<0.001).\nQuestion Melody Singing voice Rhythm Lyrics Mood Tempo Harmony Sentiment Instruments Danceability\nInstExp 0.100 0.061 -0.019 0.108 0.037 -0.099 0.134* 0.093 0.091 0.101\nDanceExp -0.041 0.039 -0.047 0.126* 0.030 -0.024 0.044 0.098 -0.005 0.341***\nNoticeBeat 0.228*** 0.228*** 0.126* 0.082 0.107 0.073 0.302*** 0.205*** 0.147* 0.072\nNoticeTune 0.272*** 0.231*** 0.099 0.088 0.121* 0.039 0.276*** 0.167** 0.078 0.001\nLsnMusic 0.041 0.054 0.111 0.141* 0.135* 0.101 0.078 0.108 0.051 0.090\nLsnNew 0.003 0.107 0.152** 0.152** 0.112 0.194*** 0.115* 0.101 0.126* 0.169**\nViewLyrics 0.001 0.085 0.118* 0.243*** 0.120* 0.147* 0.136* 0.128* 0.101 0.110\nKaraoke 0.085 0.087 0.005 0.210*** 0.154** -0.015 0.057 0.129* -0.033 0.081\nAttEvent -0.038 0.037 -0.023 0.200*** 0.004 0.016 0.039 -0.005 0.088 0.179**\nMelody\nRhythm\nLyrics\nTempoNot\nimportantHardly\nimportantSomewhat\nimportant ImportantVery\nimportant\n...(a)  Registration  of factor  importance\nLyrics\nMelody\nMood\nTempoAngel  Baby/  Betty\n...Lyrics\nMelody\nMood\nTempo...YOURevaluations EVERYONES evaluations\nAngel  Baby/  Betty(b)  Evaluation  of songs  by  factors\nI love  you  / Makiko  Hattori\nLyricsEternal  love\nTempoFastishInstruments G.,Pf.\nSentiment Positive(c)  Presentation  of factor  information\nFigure 3 . Overview of the three proposed functions. In the user study, these images were presented to the participants.\nevents emphasized both lyrics anddanceability ; and Ins-\ntExp had a signiﬁcant correlation with harmony . Table 3\nalso indicates certain high correlations that are not obvious\n(e.g., between LsnMusic/LsnNew and lyrics and between\nLsnNew and danceability ).\nCertain metrics, such as LsnMusic, LsnNew, and View-\nLyrics, can be computed for each user on a music stream-\ning service [59,62,63]. Thus, the results in Table 3 can also\nbe used to increase the conﬁdence in estimating the impor-\ntance of each factor to a user without explicitly asking the\nimportance. For example, if a user often listens to folk\nmusic (i.e., the user would have high openness as has been\nreported by Ferwerda et al. [16]) and new songs, we can es-\ntimate from the results in Tables 2 and 3 that rhythm is one\nof the user’s important factors. Hence, the user would be\nmore likely to accept recommendations by recommending\nsongs according to the similarity of their rhythms.\n6. FUNCTIONS BASED ON MUSICAL FACTORS\nIn section 4, we showed that certain musical factors in-\nﬂuence a person’s judgment of liking or disliking a song\non ﬁrst listen. Following those results, in this section,\nwe propose three functions, illustrated in Figure 3, that\ncould enrich and diversify the music listening experience\non streaming services. Then, we investigate the usefulness\nof these functions from the results of a user study.\n6.1 Functions\n6.1.1 Function 1: Registration of Factor Importance\nWith this function, shown in Figure 3 (a), users register the\nimportance of each of the 10 musical factors on a 5-point\nscale when judging whether they like or dislike music on\nﬁrst listen. It is not necessary to register the importance of\nall factors. For example, the importance of rhythm is not\nregistered in Figure 3 (a). The registration process only\nneeds to be done once, and the registered information can\nbe changed later.\nThis function supports the users as follows. Suppose\nthat a user is listening to her favorite song s. The user has\nregistered lyrics as “very important” and tempo as “hardlyimportant.” Hence, among songs that are new to this user,\nwe can recommend songs that have various tempos and\nsimilar lyrics to s. By listening to the recommended songs,\nthe user can ﬁnd new favorite songs.\n6.1.2 Function 2: Evaluation of Songs by Factors\nThis proposed function allows users to rate their song pref-\nerences on a factor-by-factor basis, as shown in Figure 3\n(b). The ratings are not mandatory: users only need to rate\nthe songs that they want to rate. In addition, they do not\nneed to rate songs in terms of all 10 factors. For exam-\nple, in the ﬁgure, the user does not rate mood . For each\nsong, by computing the average value of all users’ rating\nresults for each factor, we can display others’ evaluations\n(averaged ratings) like those shown in Figure 3 (b).\nThis function supports the users as follows. Suppose\nthat a user is interested in an artist named “Betty,” and\nthatdanceability is an important factor for the user. Then,\nsongs by “Betty” can be sorted and displayed in order of\nthe averaged ratings for danceability . This enables efﬁ-\ncient discovery of songs that match the user’s preferences.\n6.1.3 Function 3: Presentation of Factor Information\nWith this function, information on factors that a user wants\nto know for a song is displayed as shown in Figure 3 (c).\nThe information on each of the 10 factors can be automat-\nically estimated by using techniques from existing stud-\nies [64–70]. Thus, unlike the two previous functions, this\none does not require the user to input any information.\nThis function supports the users as follows. When a\nuser checks a list of newly released songs, usually only\nbasic information such as the artist and title is displayed\nfor each song. In contrast, our proposed function can dis-\nplay information on the musical factor for each song. For\nexample, if the user prefers slow-tempo songs with piano,\nshe can listen only to such songs by referring to the dis-\nplayed information on tempo andinstruments . This allows\nthe user to efﬁciently ﬁnd songs that match her preferences\namong a vast number of new songs.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n565Table 4 . Top three free-response reasons for “reasonably willing” or “willing” to use each of the proposed functions. Each\nnumber in parentheses indicates the number of participants who gave that reason.\nFunction 1: registration of factor importance Function 2: evaluation of songs by factors Function 3: presentation of factor information\n1st Easy to ﬁnd music that ﬁts my taste. (46) Would like to refer to others’ evaluations. (22) Easy to ﬁnd music that ﬁts my mood/situation. (27)\n2nd Helpful for listening to new songs. (33) Easy to understand others’ evaluations. (14) Easy to ﬁnd music that ﬁts my taste. (26)\n3rd Looks interesting to use. (11) Easy to ﬁnd music that ﬁts my taste. (13) Helpful for listening to new songs. (16)\n/aj17 /aj22/aj17 /aj18/aj17/aj17 /aj18/aj22/aj17 /aj19/aj17/aj17 /aj19/aj22/aj17 /aj20/aj17/aj17/aj39/aj86/aj79/aj68/aj85/aj74/aj80/aj79/aj1/aj20/aj39/aj86/aj79/aj68/aj85/aj74/aj80/aj79/aj1/aj19/aj39/aj86/aj79/aj68/aj85/aj74/aj80/aj79/aj1/aj18\n/aj18/aj27/aj1/aj86/aj79/aj88/aj74/aj77/aj77/aj74/aj79/aj72\n/aj19/aj27/aj1/aj79/aj80/aj85/aj1/aj87/aj70/aj83/aj90/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72/aj20/aj27/aj1/aj86/aj79/aj69/aj70/aj68/aj74/aj69/aj70/aj69\n/aj21/aj27/aj1/aj83/aj70/aj66/aj84/aj80/aj79/aj66/aj67/aj77/aj90/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72/aj22/aj27/aj1/aj88/aj74/aj77/aj77/aj74/aj79/aj72\nFigure 4 . Distribution of the willingness to use each of the\nproposed functions (x-axis: number of participants).\n6.2 Procedure\nFor each function, we showed the participants an overview\nof the function and examples of the user support that the\nfunction would enable as we described in section 6.12.\nThe participants were asked to indicate their willingness to\nuse the function, on a 5-point scale (“unwilling,” “not very\nwilling,” “undecided,” “reasonably willing,” and “will-\ning”), if it were implemented on the music streaming ser-\nvice that they used regularly. They were also asked to pro-\nvide free responses on their willingness. The three func-\ntions were displayed in a random order to each participant.\n6.3 Results\nFigure 4 shows the answer distribution for each function.\nFunctions 1 and 3 were more positively received than func-\ntion 2. To analyze the results, we manually grouped nega-\ntive responses (i.e., the free responses for “unwilling” and\n“not very willing”). As we had anticipated, a reason of “I\ndo not need the function” was common for all three func-\ntions. Regarding function 2, although we explained that the\nratings were not mandatory, a response of “It is tedious to\nrate songs” was also common. This is why the distribution\nfor function 2 was more biased in the negative direction.\nHere, note that our goal was not to propose functions that\nall participants would be willing to use. Rather, we sought\nto conﬁrm that the proposed functions would have a cer-\ntain level of demand; accordingly, the results in Figure 4\nindicate that we achieved our objective.\nWe also manually grouped the positive responses (i.e.,\nthe free responses for “willing” and “reasonably willing”).\nTable 4 lists the top three responses in terms of the group\nsize for each function. We can see that, in general, the par-\nticipants tended to appreciate functions that would make it\neasy to ﬁnd music that ﬁts their taste (all functions) and\neasy to listen to new songs (functions 1 and 3). The re-\nsponses for function 2 also indicate that they were inter-\nested in referring to other users’ evaluations of a song. We\ncan also see that the participants felt it was valuable to\nbe able to ﬁnd music according to their mood or situation\n(function 3). These responses provide reusable insights for\nlater studies: when researchers or streaming services pro-\n2We leave it as future work to actually implement these functions and\nconduct a long-term user study on them including how to visualize the\ninformation.pose a new function, such user demand could serve as a\nuseful guideline for its design.\nIf function 3 were implemented on a music streaming\nservice, it might be difﬁcult to estimate the information for\nall factors because of the platform’s resource limitations.\nIn such a case, a possible solution would be to decrease the\nnumber of displayed factors according to the results shown\nin Figure 2. For example, rhythm information could be\nomitted, because tempo has a high correlation with rhythm ,\nand users who emphasize rhythm could thus refer to tempo\ninformation instead. In contrast, lyrics should not be elimi-\nnated because it has low correlations with the other factors,\nand there would not be no alternative factor for users who\nemphasize lyrics .\n7. CONCLUSION\nIn this paper, we conducted an online user survey involving\n302 participants. The reusable insights obtained from our\nuser survey can be summarized as follows.\n• We showed that the melody andsinging voice are im-\nportant for most participants. Because there were trends\nin the criteria for each factor, as seen in Table 1, the cri-\nteria could be used to increase the explainability of song\nrecommendations, as discussed in section 4.3.\n• Personality and musical sophistication inﬂuence the im-\nportance of each musical factor. As discussed in sec-\ntions 5.1 and 5.2, these results would be useful for es-\ntimating which factors are important to a user from the\nuser’s listening behaviors on a streaming service.\n• The evaluation results for our proposed functions show\nthat there is a certain demand for functions that enable\nusers to browse songs according to musical factors. The\nreasons for each function’s demand in Table 4 could\nprovide guidelines for other researchers and services to\npropose novel factor-based functions.\nFinally, we acknowledge a limitation of this paper in\nthat all the participants in our user study were Japanese.\nBecause peoples’ music preferences and listening behav-\niors, as well as music itself, vary widely from country to\ncountry [26, 71–76], not all of the ﬁndings reported here\ncan be generalized. Nevertheless, we believe that our study\nprovides a worthwhile contribution to the MIR community\nas a ﬁrst step toward understanding how musical factors\ninﬂuence whether people like a song on ﬁrst listen. At\nthe same time, the above limitation can guide future work\nsuch as investigating the differences in important musical\nfactors among countries and cultures. The publicly avail-\nable dataset of results from our user study will enable re-\nsearchers not only to perform such comparisons but also to\nanalyze and compare results from different viewpoints.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n5668. ACKNOWLEDGMENTS\nThis work was supported in part by JST CREST Grant\nNumber JPMJCR20D4 and JSPS KAKENHI Grant Num-\nber JP21H04917, Japan.\n9. REFERENCES\n[1] H. Yakura, T. Nakano, and M. Goto, “FocusMusicRec-\nommender: A system for recommending music to lis-\nten to while working,” in Proceedings of the 23rd In-\nternational Conference on Intelligent User Interfaces ,\nser. IUI 2018, 2018, pp. 7–17.\n[2] B. Brost, R. Mehrotra, and T. Jehan, “The music\nstreaming sessions dataset,” in Proceedings of the\nWorld Wide Web Conference , ser. WWW 2019, 2019,\npp. 2594–2600.\n[3] P. Knees and M. Schedl, “A survey of music similarity\nand recommendation from music context data,” ACM\nTransactions on Multimedia Computing, Communica-\ntions, and Applications , vol. 10, no. 1, pp. 1–21, 2013.\n[4] Y . Deldjoo, M. Schedl, P. Cremonesi, and G. Pasi,\n“Recommender systems leveraging multimedia con-\ntent,” ACM Computing Surveys , vol. 53, no. 5, pp. 1–\n38, 2020.\n[5] J. D. Boyle, G. L. Hosterman, and D. S. Ramsey, “Fac-\ntors inﬂuencing pop music preferences of young peo-\nple,” Journal of Research in Music Education , vol. 29,\nno. 1, pp. 47–55, 1981.\n[6] A. Greasley, A. Lamont, and J. A. Sloboda, “Exploring\nmusical preferences: An in-depth qualitative study of\nadults’ liking for music in their personal collections,”\nQualitative Research in Psychology , vol. 10, no. 4, pp.\n402–427, 2013.\n[7] K. R. M. Sanﬁlippo, N. Spiro, M. Molina-Solana, and\nA. Lamont, “Do the shufﬂe: Exploring reasons for\nmusic listening through shufﬂed play,” PLOS ONE ,\nvol. 15, no. 2, pp. 1–21, 2020.\n[8] T. Chamorro-Premuzic and A. Furnham, “Personality\nand music: Can traits explain how people use music in\neveryday life?” British Journal of Psychology , vol. 98,\nno. 2, pp. 175–185, 2007.\n[9] M. J. Delsing, T. F. Ter Bogt, R. C. Engels, and W. H.\nMeeus, “Adolescents’ music preferences and person-\nality characteristics,” European Journal of Personality ,\nvol. 22, no. 2, pp. 109–130, 2008.\n[10] R. L. Zweigenhaft, “A do re mi encore: A closer look at\nthe personality correlates of music preferences,” Jour-\nnal of individual differences , vol. 29, no. 1, pp. 45–55,\n2008.\n[11] R. A. Brown, “Music preferences and personality\namong japanese university students,” International\nJournal of Psychology , vol. 47, no. 4, pp. 259–268,\n2012.[12] T. Chamorro-Premuzic, V . Swami, and B. Cermakova,\n“Individual differences in music consumption are pre-\ndicted by uses of music and age rather than emotional\nintelligence, neuroticism, extraversion or openness,”\nPsychology of Music , vol. 40, no. 3, pp. 285–300, 2012.\n[13] A. Langmeyer, A. Guglhör-Rudan, and C. Tarnai,\n“What do music preferences reveal about personality?”\nJournal of Individual Differences , vol. 33, no. 2, pp.\n119–130, 2012.\n[14] A. Laplante, “Improving music recommender systems:\nWhat can we learn from research on music tastes?”\ninProceedings of the 15th International Society for\nMusic Information Retrieval Conference , ser. ISMIR\n2014, 2014, pp. 451–456.\n[15] A. E. Krause and A. C. North, “Music listening in ev-\neryday life: Devices, selection methods, and digital\ntechnology,” Psychology of Music , vol. 44, no. 1, pp.\n129–147, 2016.\n[16] B. Ferwerda, M. Tkalcic, and M. Schedl, “Personality\ntraits and music genres: What do people prefer to listen\nto?” in Proceedings of the 25th ACM Conference on\nUser Modeling, Adaptation and Personalization , ser.\nUMAP 2017, 2017, pp. 285–288.\n[17] T. Schäfer and C. Mehlhorn, “Can personality traits\npredict musical style preferences? A meta-analysis,”\nPersonality and Individual Differences , vol. 116, pp.\n265–273, 2017.\n[18] W. M. Randall and N. S. Rickard, “Reasons for per-\nsonal music listening: A mobile experience sampling\nstudy of emotional outcomes,” Psychology of Music ,\nvol. 45, no. 4, pp. 479–495, 2017.\n[19] Y . Liang and M. C. Willemsen, “Personalized recom-\nmendations for music genre exploration,” in Proceed-\nings of the 27th ACM Conference on User Model-\ning, Adaptation and Personalization , ser. UMAP 2019,\n2019, pp. 276–284.\n[20] Y . Jin, N. Tintarev, and K. Verbert, “Effects of per-\nsonal characteristics on music recommender systems\nwith different levels of controllability,” in Proceedings\nof the 12th ACM Conference on Recommender Sys-\ntems, ser. RecSys 2018, 2018, pp. 13–21.\n[21] Y . Liang and M. C. Willemsen, “The role of preference\nconsistency, defaults and musical expertise in users’\nexploration behavior in a genre exploration recom-\nmender,” in Proceedings of the 15th ACM Conference\non Recommender Systems , ser. RecSys 2021, 2021, pp.\n230–240.\n[22] J. S. Downie and S. J. Cunningham, “Toward a the-\nory of music information retrieval queries: System de-\nsign implications,” in Proceedings of the 3rd Interna-\ntional Conference on Music Information Retrieval , ser.\nISMIR 2002, 2002, pp. 299–300.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n567[23] S. J. Cunningham, N. Reeves, and M. Britland, “An\nethnographic study of music information seeking: Im-\nplications for the design of a music digital library,” in\nProceedings of the 3rd ACM/IEEE-CS Joint Confer-\nence on Digital Libraries , ser. JCDL 2003, 2003, pp.\n5–16.\n[24] S. Jones, S. J. Cunningham, and M. Jones, “Organiz-\ning digital music for use: An examination of personal\nmusic collections,” in Proceedings of the 5th Interna-\ntional Conference on Music Information Retrieval , ser.\nISMIR 2004, 2004, pp. 397–402.\n[25] C. Inskip, R. Butterworth, and A. MacFarlane, “A\nstudy of the information needs of the users of a folk\nmusic library and the implications for the design of\na digital library system,” Information Processing &\nManagement , vol. 44, no. 2, pp. 647–662, 2008.\n[26] X. Hu, J. H. Lee, and L. K. Y . Wong, “Music infor-\nmation behaviors and system preferences of university\nstudents in Hong Kong,” in Proceedings of the 15th\nInternational Society for Music Information Retrieval\nConference , ser. ISMIR 2014, 2014, pp. 579–584.\n[27] J. H. Lee and R. Price, “Understanding users of com-\nmercial music services through personas: Design im-\nplications,” in Proceedings of the 16th International\nSociety for Music Information Retrieval Conference ,\nser. ISMIR 2015, 2015, pp. 476–482.\n[28] J. H. Lee, H. Cho, and Y .-S. Kim, “Users’ music infor-\nmation needs and behaviors: Design implications for\nmusic information retrieval systems,” Journal of the\nAssociation for Information Science and Technology ,\nvol. 67, no. 6, pp. 1301–1330, 2016.\n[29] J. H. Lee, Y . Kim, and C. Hubbles, “A look at the cloud\nfrom both sides now: An analysis of cloud music ser-\nvice usage,” in Proceedings of the 17th International\nSociety for Music Information Retrieval Conference ,\nser. ISMIR 2016, 2016, pp. 299–305.\n[30] L. Spinelli, J. Lau, L. Pritchard, and J. H. Lee, “Inﬂu-\nences on the social practices surrounding commercial\nmusic services: A model for rich interactions,” in Pro-\nceedings of the 19th International Society for Music\nInformation Retrieval Conference , ser. ISMIR 2018,\n2018, pp. 671–677.\n[31] J. H. Lee, L. Pritchard, and C. Hubbles, “Can we lis-\nten to it together?: Factors inﬂuencing reception of\nmusic recommendations and post-recommendation be-\nhavior,” in Proceedings of the 20th International So-\nciety for Music Information Retrieval Conference , ser.\nISMIR 2019, 2019, pp. 663–669.\n[32] J. H. Lee and A. T. Nguyen, “How music fans shape\ncommercial music services: A case study of BTS and\nARMY,” in Proceedings of the 21st International So-\nciety for Music Information Retrieval Conference , ser.\nISMIR 2020, 2020, pp. 837–845.[33] J. H. Lee, A. Bhattacharya, R. Antony, N. Santero, and\nA. Le, ““Finding home”: Understanding how music\nsupports listeners’ mental health through a case study\nof BTS,” in Proceedings of the 22nd International So-\nciety for Music Information Retrieval Conference , ser.\nISMIR 2021, 2021, pp. 358–365.\n[34] X. Hu, J. Chen, and Y . Wang, “University students’\nuse of music for learning and well-being: A qualitative\nstudy and design implications,” Information Process-\ning & Management , vol. 58, no. 1, pp. 1–14, 2021.\n[35] S. Y . Park and B. Kaneshiro, “Social music cura-\ntion that works: Insights from successful collabora-\ntive playlists,” Proceedings of the ACM on Human-\nComputer Interaction , vol. 5, no. CSCW1, pp. 1–27,\n2021.\n[36] J. A. Sloboda, S. A. O’Neill, and A. Ivaldi, “Functions\nof music in everyday life: An exploratory study using\nthe experience sampling method,” Musicae Scientiae ,\nvol. 5, no. 1, pp. 9–32, 2001.\n[37] A. Lamont and R. Webb, “Short- and long-term musi-\ncal preferences: What makes a favourite piece of mu-\nsic?” Psychology of Music , vol. 38, no. 2, pp. 222–241,\n2010.\n[38] A. B. Haake, “Individual music listening in workplace\nsettings: An exploratory survey of ofﬁces in the UK,”\nMusicae Scientiae , vol. 15, no. 1, pp. 107–129, 2011.\n[39] T. Schäfer, “The goals and effects of music listening\nand their relationship to the strength of music prefer-\nence,” PloS ONE , vol. 11, no. 3, pp. 1–15, 2016.\n[40] A. J. Lonsdale and A. C. North, “Why do we listen\nto music? A uses and gratiﬁcations analysis,” British\nJournal of Psychology , vol. 102, no. 1, pp. 108–134,\n2011.\n[41] S. Y . Park, E. Redmond, J. Berger, and B. Kaneshiro,\n“Hitting pause: How user perceptions of collaborative\nplaylists evolved in the united states during the covid-\n19 pandemic,” in Proceedings of the 2022 CHI Con-\nference on Human Factors in Computing Systems , ser.\nCHI 2022, 2022, pp. 1–16.\n[42] A. C. North, D. J. Hargreaves, and J. J. Hargreaves,\n“Uses of music in everyday life,” Music Perception:\nAn Interdisciplinary Journal , vol. 22, no. 1, pp. 41–77,\n2004.\n[43] P. J. Rentfrow and S. D. Gosling, “The do re mi’s of ev-\neryday life: The structure and personality correlates of\nmusic preferences.” Journal of Personality and Social\nPsychology , vol. 84, no. 6, pp. 1236–1256, 2003.\n[44] M. Tkal ˇciˇc, B. Ferwerda, D. Hauger, and M. Schedl,\n“Personality correlates for digital concert program\nnotes,” in Proceedings of the 23rd ACM Conference on\nUser Modeling, Adaptation and Personalization , ser.\nUMAP 2015, 2015, pp. 364–369.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n568[45] B. Ferwerda, E. Yang, M. Schedl, and M. Tkalcic,\n“Personality traits predict music taxonomy prefer-\nences,” in Proceedings of the 33rd Annual ACM Con-\nference Extended Abstracts on Human Factors in Com-\nputing Systems , ser. CHI EA 2015, 2015, pp. 2241–\n2246.\n[46] B. Ferwerda, M. Graus, A. Vall, M. Tkalcic, and\nM. Schedl, “The inﬂuence of users’ personality traits\non satisfaction and attractiveness of diversiﬁed recom-\nmendation lists,” in Proceedings of the 4th Workshop\non Emotions and Personality in Personalized Systems ,\nser. EMPIRE 2016, 2016, pp. 43–47.\n[47] A. B. Melchiorre and M. Schedl, “Personality corre-\nlates of music audio preferences for modelling music\nlisteners,” in Proceedings of the 28th ACM Confer-\nence on User Modeling, Adaptation and Personaliza-\ntion, ser. UMAP 2020, 2020, pp. 313–317.\n[48] R. Hu and P. Pu, “Enhancing collaborative ﬁltering sys-\ntems with personality information,” in Proceedings of\nthe 5th ACM Conference on Recommender Systems ,\nser. RecSys 2011, 2011, pp. 197–204.\n[49] I. Fernández-Tobías, M. Braunhofer, M. Elahi, F. Ricci,\nand I. Cantador, “Alleviating the new user problem\nin collaborative ﬁltering by exploiting personality in-\nformation,” User Modeling and User-Adapted Interac-\ntion, vol. 26, no. 2, pp. 221–255, 2016.\n[50] F. Lu and N. Tintarev, “A diversity adjusting strategy\nwith personality for music recommendation.” in Pro-\nceedings of the 5th Joint Workshop on Interfaces and\nHuman Decision Making for Recommender Systems ,\nser. IntRS 2018, 2018, pp. 7–14.\n[51] A. B. Melchiorre, E. Zangerle, and M. Schedl, “Per-\nsonality bias of music recommendation algorithms,” in\nProceedings of the 14th ACM Conference on Recom-\nmender Systems , ser. RecSys 2020, 2020, pp. 533–538.\n[52] B. Ferwerda and M. Tkal ˇciˇc, “Exploring online music\nlistening behaviors of musically sophisticated users,”\ninProceedings of the 27th Conference on User Model-\ning, Adaptation and Personalization , ser. UMAP 2019,\n2019, pp. 33–37.\n[53] B. Ferwerda, M. P. Graus, A. Vall, M. Tkalcic, and\nM. Schedl, “How item discovery enabled by diver-\nsity leads to increased recommendation list attractive-\nness,” in Proceedings of the 32nd ACM SIGAPP Sym-\nposium on Applied Computing , ser. SAC 2017, 2017,\npp. 1693–1696.\n[54] A. LeBlanc, “Outline of a proposed model of sources\nof variation in musical taste,” Bulletin of the Council\nfor Research in Music Education , no. 61, pp. 29–34,\n1980.\n[55] J. H. Lee and J. S. Downie, “Survey of music informa-\ntion needs, uses, and seeking behaviours: Preliminaryﬁndings,” in Proceedings of the 5th International Con-\nference on Music Information Retrieval , ser. ISMIR\n2004, 2004, pp. 441–446.\n[56] A. Laplante, “Users’ relevance criteria in music re-\ntrieval in everyday life: An exploratory study,” in Pro-\nceedings of the 11th International Society for Music\nInformation Retrieval Conference , ser. ISMIR 2010,\n2010, pp. 601–606.\n[57] J. H. Lee and N. M. Waterman, “Understanding user\nrequirements for music information services,” in Pro-\nceedings of the 13th International Society for Music\nInformation Retrieval Conference , ser. ISMIR 2012,\n2012, pp. 253–258.\n[58] M. Kamalzadeh, D. Baur, and T. Möller, “A survey on\nmusic listening and management behaviours,” in Pro-\nceedings of the 13th International Society for Music\nInformation Retrieval Conference , ser. ISMIR 2012,\n2012, pp. 373–378.\n[59] K. Tsukuda, M. Hamasaki, and M. Goto, “Toward an\nunderstanding of lyrics-viewing behavior while listen-\ning to music on a smartphone,” in Proceedings of the\n22nd International Society for Music Information Re-\ntrieval Conference , ser. ISMIR 2021, 2021, pp. 705–\n713.\n[60] T. Namikawa, I. Tani, T. Wakita, R. Kumagai,\nA. Nakane, and H. Noguchi, “Development of a short\nform of the Japanese Big-Five Scale, and a test of its\nreliability and validity,” The Japanese Journal of Psy-\nchology , vol. 83, no. 2, pp. 91–99, 2012.\n[61] D. Müllensiefen, B. Gingras, J. Musil, and L. Stew-\nart, “The musicality of non-musicians: An index for\nassessing musical sophistication in the general popula-\ntion,” PLOS ONE , vol. 9, no. 2, pp. 1–23, 2014.\n[62] G. Vigliensoni and I. Fujinaga, “The music listening\nhistories dataset,” in Proceedings of the 18th Interna-\ntional Society for Music Information Retrieval Confer-\nence, ser. ISMIR 2017, 2017, pp. 96–102.\n[63] M. Schedl, S. Brandl, O. Lesota, E. Parada-Cabaleiro,\nD. Penz, and N. Rekabsaz, “LFM-2b: A dataset of en-\nriched music listening events for recommender systems\nresearch and fairness analysis,” in Proceedings of the\n2022 Conference on Human Information Interaction\nand Retrieval , ser. CHIIR 2022, 2022, pp. 337–341.\n[64] K. Tsukuda, K. Ishida, and M. Goto, “Lyric Jumper:\nA lyrics-based music exploratory web service by mod-\neling lyrics generative process,” in Proceedings of the\n18th International Society for Music Information Re-\ntrieval Conference , ser. ISMIR 2017, 2017, pp. 544–\n551.\n[65] R. Delbouys, R. Hennequin, F. Piccoli, J. Royo-\nLetelier, and M. Moussallam, “Music mood detection\nbased on audio and lyrics with deep neural net,” in Pro-\nceedings of the 19th International Society for MusicProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n569Information Retrieval Conference , ser. ISMIR 2018,\n2018, pp. 370–375.\n[66] F. Karsdorp, P. van Kranenburg, and E. Manjava-\ncas, “Learning similarity metrics for melody retrieval,”\ninProceedings of the 20th International Society for\nMusic Information Retrieval Conference , ser. ISMIR\n2019, 2019, pp. 478–485.\n[67] M. Hamasaki, K. Ishida, T. Nakano, and M. Goto,\n“Songrium RelayPlay: A web-based listening interface\nfor continuously playing user-generated music videos\nof the same song with different singers,” in Proceed-\nings of the International Computer Music Conference\n2020 , ser. ICMC 2020, 2020, pp. 426–429.\n[68] A. A. Correya, D. Bogdanov, L. Joglar-Ongay, and\nX. Serra, “Essentia.js: A javascript library for music\nand audio analysis on the web,” in Proceedings of the\n21st International Society for Music Information Re-\ntrieval Conference , ser. ISMIR 2020, 2020, pp. 605–\n612.\n[69] G. Micchi, K. Kosta, G. Medeot, and P. Chanquion, “A\ndeep learning method for enforcing coherence in auto-\nmatic chord recognition,” in Proceedings of the 22nd\nInternational Society for Music Information Retrieval\nConference , ser. ISMIR 2021, 2021, pp. 443–451.\n[70] H. F. Garcia, A. Aguilar, E. Manilow, and B. Pardo,\n“Leveraging hierarchical structures for few-shot musi-\ncal instrument recognition,” in Proceedings of the 22nd\nInternational Society for Music Information Retrieval\nConference , ser. ISMIR 2021, 2021, pp. 220–228.\n[71] X. Hu and J. H. Lee, “A cross-cultural study of mu-\nsic mood perception between American and Chinese\nlisteners,” in Proceedings of the 13th International So-\nciety for Music Information Retrieval Conference , ser.\nISMIR 2012, 2012, pp. 535–540.\n[72] T. Schäfelr, A. Tipandjan, and P. Sedlmeier, “The func-\ntions of music and their relationship to music prefer-\nence in India and Germany,” International Journal of\nPsychology , vol. 47, no. 5, pp. 370–380, 2012.\n[73] Y . Yang and X. Hu, “Cross-cultural music mood classi-\nﬁcation: A comparison on English and Chinese songs,”\ninProceedings of the 13th International Society for\nMusic Information Retrieval Conference , ser. ISMIR\n2012, 2012, pp. 19–24.\n[74] X. Hu, J. H. Lee, K. Choi, and J. S. Downie, “A cross-\ncultural study on the mood of K-POP songs,” in Pro-\nceedings of the 15th International Society for Music\nInformation Retrieval Conference , ser. ISMIR 2014,\n2014, pp. 385–390.\n[75] M. Liu, X. Hu, and M. Schedl, “Artist preferences and\ncultural, socio-economic distances across countries: A\nbig data perspective,” in Proceedings of the 18th Inter-\nnational Society for Music Information Retrieval Con-\nference , ser. ISMIR 2017, 2017, pp. 103–111.[76] C. Bauer and M. Schedl, “Global and country-speciﬁc\nmainstreaminess measures: Deﬁnitions, analysis, and\nusage for improving personalized music recommenda-\ntion systems,” PLOS ONE , vol. 14, no. 6, pp. 1–36,\n2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n570"
    },
    {
        "title": "Quantifying the Ease of Playing Song Chords on the Guitar.",
        "author": [
            "Marcel A. Vélez Vásquez",
            "Mariëlle Baelemans",
            "Jonathan Driedger",
            "Willem H. Zuidema",
            "John Ashley Burgoyne"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265391",
        "url": "https://doi.org/10.5281/zenodo.10265391",
        "ee": "https://zenodo.org/records/10265391/files/000086.pdf",
        "abstract": "Quantifying the difficulty of playing songs has recently gained traction in the MIR community. While previous work has mostly focused on piano, this paper concentrates on rhythm guitar, which is especially popular with amateur musicians and has a broad skill spectrum. This paper proposes a rubric-based 'playability' metric to formalise this spectrum. The rubric comprises seven criteria that contribute to a single playability score, representing the overall difficulty of a song. The rubric was created through interviewing and incorporating feedback from guitar teachers and experts. Additionally, we introduce the playability prediction task by adding annotations to a subset of 200 songs from the McGill Billboard dataset, labelled by a guitar expert using the proposed rubric. We use this dataset to weight each rubric criterion for maximal reliability. Finally, we create a rule-based baseline to score each rubric criterion automatically from chord annotations and timings, and compare this baseline against simple deep learning models trained on chord symbols and textual representations of guitar tablature. The rubric, dataset, and baselines lay a foundation for understanding what makes songs easy or difficult for guitar players and how we can use MIR tools to match amateurs with songs closer to their skill level.",
        "zenodo_id": 10265391,
        "dblp_key": "conf/ismir/VasquezBDZB23",
        "keywords": [
            "Quantifying",
            "playability",
            "metric",
            "rubric",
            "proposed",
            "task",
            "annotations",
            "subset",
            "dataset",
            "baseline"
        ],
        "content": "QUANTIFYING THE EASE OF PLAYING SONG CHORDS ON THE GUITAR\nMarcel A. Vélez Vásquez1Mariëlle Baelemans1Jonathan Driedger2\nWillem Zuidema1John Ashley Burgoyne1\n1ILLC, University of Amsterdam, the Netherlands2Chordify, Groningen, the Netherlands\nm.a.velezvasquez@uva.nl\nABSTRACT\nQuantifying the difﬁculty of playing songs has recently\ngained traction in the MIR community. While previous\nwork has mostly focused on piano, this paper concentrates\non rhythm guitar, which is especially popular with ama-\nteur musicians and has a broad skill spectrum. This paper\nproposes a rubric-based ‘playability’ metric to formalise\nthis spectrum. The rubric comprises seven criteria that\ncontribute to a single playability score, representing the\noverall difﬁculty of a song. The rubric was created through\ninterviewing and incorporating feedback from guitar teach-\ners and experts. Additionally, we introduce the playability\nprediction task by adding annotations to a subset of 200\nsongs from the McGill Billboard dataset, labelled by a gui-\ntar expert using the proposed rubric. We use this dataset\nto weight each rubric criterion for maximal reliability. Fi-\nnally, we create a rule-based baseline to score each rubric\ncriterion automatically from chord annotations and timings,\nand compare this baseline against simple deep learning\nmodels trained on chord symbols and textual representa-\ntions of guitar tablature. The rubric, dataset, and baselines\nlay a foundation for understanding what makes songs easy\nor difﬁcult for guitar players and how we can use MIR tools\nto match amateurs with songs closer to their skill level.\n1. INTRODUCTION\nGuitars have seen a 1.25-million-instrument sales rebound\nsince the coronavirus pandemic, and the public’s fascin-\nation with fretted instruments has never been higher [1].\nWhile traditional methods of transferring musical playab-\nility knowledge via music schools or private teachers still\nexist, online resources have made learning to play the guitar\nmore accessible [2]. Indeed, online tools have led to a sig-\nniﬁcant increase in the accessibility of learning anymusical\ninstrument, with a growing number of children and adults\nlearning to play [3]. In addition, research suggests that\ninformal self-practice can enhance motivation compared\nto formal teaching [4]. Ultimate Guitar and Chordify are\n© M.A. Vélez Vásquez, M.C.E. Baelemans, J. Driedger,\nW.H. Zuidema, and J.A. Burgoyne. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0). Attribution: M.A.\nVélez Vásquez, M.C.E. Baelemans, J. Driedger, W.H. Zuidema, and J.A.\nBurgoyne, “Quantifying the Ease of Playing Song Chords on the Guitar”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.\nFigure 1 . Physical and cognitive criteria for evaluating the\nplayability of songs on the guitar position during guitar per-\nformance. Note that repetitiveness reﬂects both cognitive\nand physical factors, and that attentive listening to auditory\nfeedback, while not a criterion itself, is necessary for devel-\noping and reﬁning performative gestures.\nexamples of web-based music services that facilitate the\nautomatic extraction of chord progressions from audio re-\ncordings of songs or community-proposed chord transcrip-\ntions and present them in a simple and accessible format for\nthe growing group of amateur guitar players to use for prac-\ntice and pleasure. Currently, Ultimate Guitar and Chordify\nhave 39.7 million and 8 million users, respectively [5, 6].\nNavigating the abundance of online chord data on plat-\nforms such as Ultimate Guitar or Chordify can be over-\nwhelming, however, particularly for amateur learners seek-\ning suitable pieces to enhance their expertise. While\nChordify offers only a chord simpliﬁcation option, Ulti-\nmate Guitar offers four categories of playability: absolute\nbeginner, beginner, intermediate, and expert. Still, these\ncategories may be too broad to suit all individuals. There is\na need to establish a method that can predict a song’s difﬁ-\nculty level in a more ﬁne-grained, automated, and preferably\ninterpretable manner to assist learners in selecting appro-\npriate pieces based on their skill level and personal taste.725The Ultimate Guitar community has proposed a difﬁculty\nmeasurement system, which relied until recently on the in-\nput of multiple users, but like any system relying on human\nannotation, it is difﬁcult to scale and can suffer from low\nreliability unless annotators are well-qualiﬁed and familiar\nwith the annotation scheme.\nThis paper argues that a robust, reliable, and publicly\ndocumented difﬁculty prediction system could signiﬁcantly\nbeneﬁt music learners in selecting challenging and reward-\ning pieces. Our main contributions are: (1) an interpretable\nguitar playability metric; (2) an extension of the Billboard\ndataset of 200 playability annotated songs, tested for re-\nliability; and (3) a rule-based baseline for our playability\nmetric. Furthermore, we investigated how well a previously\nwell-performing model of piano playability compares to\nour rule-based baseline when trained on our dataset. The\nrule-based baseline and source code for all experiments are\navailable to download.1We also include dataset statistics\nand other information to aid future research on playability.\n2. RELATED WORK\nWe deﬁne playability as the level of musical proﬁciency\nrequired to perform a musical song on a speciﬁc instru-\nment. While it is a crucial aspect of musical analysis and\nperformance, it is a complex and challenging concept to\nmeasure or quantify. The playability of musical songs can\nbe inﬂuenced by various factors, such as the complexity of\nthe musical structure [7], the instrument of choice [8], and\nthe musical context in which it is played [9]. In addition, in-\ndividual musical competence for a particular song requires\ndeveloping physical and cognitive skills and is inﬂuenced\nby personality [10]. Physical skills for the guitar include\nreﬁning gestural mechanics, both left (fret ﬁngering) and\nright (strumming) hand positioning [11]; cognitive skills\ninclude a comprehensive understanding of music theory,\nthe ability to read musical scores, and attentive listening to\nthe auditory feedback of the instrument for monitoring and\nplanning of the performative gestures [12, 13].\nSeveral studies have attempted to develop methods for\nautomating the estimation of the difﬁculty level of piano\nsheet music. In 2012, researchers proposed a method that\nused MusicXML and seven high-level, instrument-agnostic\ncriteria to determine the difﬁculty level of a song [14]. They\nevaluated the accuracy of their criteria by testing them on\n50 piano pieces and validated their performance using prin-\ncipal component analysis and human judgement. Although\ntheir criteria were not instrument-speciﬁc, some of their\ncategories aligned with or were similar to those used in\nother studies. Another study focused on predicting the difﬁ-\nculty level of piano sheet music using regression [15]. The\nauthors proposed using RReliefF, a method for selecting\nrelevant symbolic music features, to improve their perform-\nance, yielding R2values of up to .40.\nIn a recent study, researchers developed a piano score\ndifﬁculty classiﬁcation task and a novel difﬁculty score\ndataset [16]. They used a gated recurrent unit (GRU) neural\n1https://github.com/Marcel-Velez/playability-\nbillboardnetwork with an attention mechanism and gradient-boosted\ntrees to train their model on segments of musical scores\nwith various piano-ﬁngering representations. They derived\nthe skill levels for each song from a musical practice-book\nseries, where the editions were ordered based on difﬁculty.\nBooks 1 and 2 were easier, classiﬁed as beginner by the\nauthors; Books 3 and 4 as intermediate; and Books 5 and 6\nas professional. They showed that novel piano ﬁngering fea-\ntures were indicative of difﬁculty. Both machine-learning\nmodels performed better than their simple baseline, with\nthe GRU with attention mechanisms performing best.\nThere has been limited research devoted to the investiga-\ntion of guitar playability. Some studies have incorporated\nalgorithmic proxies as a means of evaluating guitar play-\nability [17]. Meanwhile, others have primarily focused on\nleft-hand ﬁngering aspects [18]. However, a conspicuous\ngap in the existing literature is the lack of manual annota-\ntion of difﬁculty by human experts. Like the practice-book\ndataset, any automatic system for assessing playability re-\nquires good human-generated ground truth. To address this\nchallenge and move the scope from piano to guitar playab-\nility, we introduce a rubric-based metric to formalise the\nbroad spectrum of playability levels.\n3. A RUBRIC FOR GUITAR PLAYABILITY\nIn order to develop a rubric for guitar-playing difﬁculty, we\ninterviewed local guitar experts, including guitar teachers,\nto investigate what they believed makes a song challenging\nto play, and what they consider when developing teaching\nmaterial for a student (e.g., why it would or would not be\nsuitable for their students, and how they simplify the chord\nprogressions to make songs more accessible). Based on\nthese interviews, we created a list of categories appropriate\nfor evaluating playability and formulated four difﬁculty\nlevels within each criterion, with a textual description for\neach level. We revised this initial draft by considering\nwhether categories had too much overlap, and rephrased the\nnames and level descriptions for each criterion accordingly.\nWe requested and incorporated feedback on the updated\nrubric from two musical experts, and ﬁnally had a guitar\nexpert annotate ﬁve songs with the rubric and give feedback\nas to whether it allowed annotating the data efﬁciently.\nThe ﬁnal version of the rubric is in Table 1. It includes\nseven criteria: (1) ‘uncommonness of chord’, capturing\nthe possibility of the player having played the chords in\nthe speciﬁc song before, where unknown chords increase\ndifﬁculty; (2) ‘chord ﬁnger positioning’, capturing how\ncomfortably spaced the ﬁngers on the guitar fretboard are\npositioned, wherein chords are more difﬁcult to play if\nthey contain very stretched out or cramped ﬁnger positions\nthan when the ﬁngers are close together and in a relaxed\nposition; (3) ‘chord ﬁngering difﬁculty’, capturing how\nmany ﬁngers a chord requires and the ratio of barre chords\nplayed in a song, based on guitar teaching books’ build-up\nof number of ﬁngers used, and later on to barre chords;\n(4) ‘repetitiveness’, capturing that a song is easier to play if\nit has more repetition since it requires less task switching\nthan a less repetitive song; (5) ‘right hand complexity’,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n726Criterion Weight Very difﬁcult (3 points) Difﬁcult (2 points) Easy (1 point) Very Easy (0 points)\nUncommonness\nof chord3 A lot of uncommon\nchordsSome uncommon chords Few uncommon chords No uncommon chords\nChord ﬁnger\npositioning3 Very cramped or very\nwide ﬁngerspreadUncomfortable or spread\nout ﬁngersSlightly uncomfortable or\nspread out ﬁngersComfortable hand and\nﬁnger position\nChord ﬁngering\ndifﬁculty2 Mostly chords that\nrequire four ﬁngers or\nbarre chordsSome chords require four\nﬁngers to be played or are\nbarre chords (not A or E)Most chords require three\nﬁngers or are A or E barre\nchordsMost chords can be\nplayed with two or three\nﬁngers\nRepetitiveness 2 No repeated chord\nprogressionsA few repeated chord\nprogressionsQuite a bit of repetition of\nchord progressionsA lot of repetition of\nchord progressions\nRight-hand\ncomplexity2 For some chords multiple\ninner strings are not\nstrummedFor some chords one\ninner string is not\nstrummedFor some of the chords\none or more outer strings\nare not strummedFor the chords all strings\nare strummed\nChord\nprogression time1 Very quick chord\ntransitionsQuick chord transitions Slow chord transitions Very slow chord\ntransitions\nBeat difﬁculty\n(syncopes/ghostnotes)0 A lot of syncopes or\nghostnotesSome syncopes or\nghostnotesA few syncopes or\nghostnotesNo syncopes or\nghostnotes\nTable 1 . Proposed rubric for human annotators evaluating the difﬁculty of playing the chords of a song on the guitar.\nAlthough the rubric functions acceptably using the raw scores from the table header, it has even better predictive power\nwhen weighting the criteria according to the factor in the weight column. Note that the beat difﬁculty criterion provides so\nlittle extra information that we recommend omitting it (i.e., setting its weight to zero).\ncapturing how difﬁcult the strumming is, where dampening\nor skipping inner strings is thought to be more difﬁcult for\nstrumming than skipping outer strings or just strumming\nall strings; (6) ‘chord progression tempo’, covering the\ntempo at which the individual has to switch between chords,\nwherein matching the correct ﬁnger positions is linked to\nthe playability proﬁciency of the individual; and (7) ‘beat\ndifﬁculty’, which models the regularity of the beat within\na song, a more regular strum being easier to play than\nirregular strumming, and mixed regularity like that typical\nof the reggaeton genre being easier than fully irregular beat\npatterns. Figure 1 visualises these criteria in the context of\nactual guitar playing and organises them into physical and\ncognitive factors. The purpose of the rubric is to generate\na single, overall playability score as the sum of scores for\neach rubric category. As will be discussed in more detail\nbelow, while a simple unweighted sum of points for each\ncriterion already provides a reliable measure of playability,\nthe reliability is improved even further by using a weighted\nsum, with uncommonness and ﬁnger positioning receiving\nthe most weight and beat difﬁculty the least.\nOur playability rubric focuses on rhythm guitar playab-\nility over solo guitar playability: in other words, we are\nnot interested in melodies but rather in how difﬁcult it is\nfor guitar players to reproduce the chord progressions and\nrhythms of Western-style pop music. For MIR research\nsurrounding chords and timing in Western-style pop music,\none of the most frequently-used datasets is the McGill Bill-\nboard dataset [19]. The original Billboard dataset consists\nof 740 songs that were part of the Billboard Hot 100 chart\nbetween 1958 and 1991 and have been part of the MIREX\nchallenges. Each song has time-aligned chord transcriptions\nand higher-level structural information, including meter and\nphrase. Since its release, other researchers have enriched\nthe Billboard dataset with further information (e.g., the Bill-\nboard sub-corpus of the CoCoPops project [20] and theChord Annotator Subjectivity Dataset [21]). We decided to\ndo the same as a testing ground for our playability rubric,\ncreating the Billboard Playability Dataset.\n4. THE BILLBOARD PLAYABILITY DATASET\nAs a basis for our dataset, we started with the 50 songs that\nare included in the Chord Annotator Subjectivity Dataset.\nOf the remaining 690 songs that appear in the original\ndataset and CoCoPops, we chose a random sample of 150,\nbringing the total number of songs in Billboard Playability\nDataset to 200. In total, these 200 songs comprise 31 205\nchords, 27 190 bars, and 5852 phrases.\nFor each song, we acquired the audio and made an on-\nline annotation dashboard with an audio player on the top,\nthe Billboard chord transcriptions (including timing and\nphrasing information) on the left, and the rubric to the right.\nTo create the dataset, we enlisted the assistance of a guitar\nexpert who has previously demonstrated exceptional guitar\nskills and experience with other music annotation tasks. We\ninstructed the annotator to perform the songs as written\n(i.e., without using a capo or making other simpliﬁcations,\nand also not adding extensions beyond those notated in the\nBillboard dataset), but they were free to choose any appro-\npriate ﬁngering. After using our dashboard to listen and\nplay along with the song, the annotator ﬁlled in the rubric.\nSix of the songs appeared twice, unbeknownst to the an-\nnotator, and were scored similarly each time (maximally 5\npoints different on the weighted scale, whereas the standard\ndeviation across all scores in the dataset was 6.6 points).\nHistograms of the overall distributions per rubric cri-\nterion are in Figure 2. For one criterion, repetitiveness, the\nmost difﬁcult category was never used, which is somewhat\nto be expected given that all of the songs in the dataset\nare mainstream Western pop music. Most pop music tends\nto have some form of repetition, and not to consist of the\nunique chords and phrases that are characteristic of moreProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n727Commonness Finger Positioning Fingering Difﬁculty Repetitiveness RH Compexity Progression Speed Beat Difﬁculty\n0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3050100150\nFigure 2 . Histograms of playability scores per rubric criterion.\nBin Chords Bars Phrases\nAll songs 156.03 (87.92) 135.95 (55.34) 29.26 (12.41)\nEasy 25% 139.21 (85.66) 133.17 (44.64) 27.87 (10.87)\nModerate 25% 152.25 (73.79) 132.69 (42.13) 28.63 (9.48)\nHard 25% 158.29 (96.40) 135.59 (65.01) 28.27 (14.10)\nExpert 25% 175.20 (89.84) 142.47 (64.69) 31.35 (14.19)\nTable 2 . Mean, and standard deviation (in brackets) of the\nnumber of chords, bars, and phrases for the entire dataset\nand per playability bins. The playability bins are based on\nquartiles of the weighted total score of the songs, the easiest\nhaving a score lower than 8, moderate lower than 12.5, hard\nlower than 18, and expert higher than 18.\nexperimental genres [22].\n5. CAN PLAYABILITY BE MEASURED?\nGiven the inherent subjectivity in the concept of playability,\none could be forgiven for wondering whether predicting\nplayability is a well-posed question at all. Is there any com-\nmon underlying measure of playability for the guitar, or is it\nmerely a more-or-less arbitrary combination of criteria such\nas those we collected from guitar teachers for our rubric?\nTo address this concern, we checked our annotator’s scores\nforreliability : if one tries to predict our annotator’s rubric\nscores from a single parameter per song, what proportion\nof variance in that parameter is ‘true’ variance as opposed\nto measurement error? Reliability can also be seen as the\nextent to which the rubric criteria co-vary, with high reliab-\nility indicating high covariance (and thus that all criteria are\nmeasuring a common underlying phenomenon), or altern-\natively, as the proportion of variance explained by the ﬁrst\nprincipal component. Values of 0.7 or higher are desirable\nfor this type of assessment [23].\nFormally, we used a family of models known as partial\ncredit models to assess reliability [24, 25]:\nP[xni] =e/summationtextxni\nk=1αik(θn−δik)\n/summationtextK\nk/prime=0e/summationtextxni\nk=1αik/prime(θn−δik/prime), (1)\nwherexnidenotes the rubric score given to song nfor cri-\nterioni,xni∈ {0,1,...,K},θnrepresents the underlying\ndifﬁculty of song n, theδikare threshold parameters for\neach level of rubric criterion i, and theαik> 0 represent the\nincrease in difﬁculty score when moving from level k−1\nto levelkon rubric criterion i. We considered three variantsSong Artist Score\nStand By Me David and Jimmy Rufﬁn 1\nMiss You The Rolling Stones 2\nNo Charge Melba Montgomery 2\nJungle Boogie Kool and the Gang 2\nSunshine of Your Love Cream 2\nI Don’t Need You Kenny Rogers 28\nMan In The Mirror Michael Jackson 28\nOne Less Bell To Answer The 5th Dimension 30\nThat Girl Stevie Wonder 31\nDo I Do Stevie Wonder 34\nTable 3 . Easiest and most difﬁcult songs in the dataset with\ntheir weighted playability scores.\nof the model: (1) the simple partial credit model, for which\nallαikare ﬁxed to one (corresponding to a simple tally of\nrubric scores); (2) the generalised partial credit model, for\nwhichαikis allowed to vary in ibut not in k(correspond-\ning to the weighted rubric scores in Table 1); and (3) the\nextended partial credit model, for which the αikvary freely.\nWe ﬁt all three models to the Billboard Playability Data-\nset using a hierarchical Bayesian implementation in Stan.\nThe model included two hyperparameters µandσwith\npriorsµ∼N(0,1)andσ∼Exp(1). Given these hy-\nperparameters, the remaining priors were αik∼Exp(1),\nθn∼N(0,1),δik∼N(µ,σ). We computed reliability\naccording to the customary partial-credit formula [26]: the\nvariance of the estimated song difﬁculties θndivided by\nthe true difﬁculty variance. Because the true variance in\nour model is ﬁxed to unity, we could estimate reliability\ndirectly as the variance of the set of posterior means ˆθn.\nWe compared the three models using approximate leave-\none-out cross-validation [27]. The extended partial credit\nmodel performed best, but the generalised partial credit\nmodel was statistically indistinguishable from it (expected\nlog probability difference = 8.7, SE= 5.2). The simple\npartial credit model was somewhat worse (elpd = 105.5, SE\n= 14.2). All models, however, showed good reliability: 0.74\nfor the simple partial credit model, 0.84 for the generalised,\nand 0.86 for the extended.\nGiven these results, we recommend the generalised par-\ntial credit model, which is statistically indistinguishable\nfrom the extended model and more parsimonious. The\nsimple 3–3–2–2–2–1–0 weighting scheme accompanying\nthe rubric in Table 1 falls within 90% credible intervals\nfor allαikvalues from this model ﬁt. Table 2 providesProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n728some descriptive statistics for the dataset and quartile-based\n‘playability bins’ under this weighting, and Table 3 lists the\neasiest and most difﬁcult songs in the dataset. We can see\nan apparent increase in the mean number of chords, bars\nand phrases, which as described later in this paper, inspired\nus to try classifying difﬁculty based on length alone.\n6. CAN PLAYABILITY BE PREDICTED?\nIn short, the rubric we developed can be used by expert\nguitarists to measure playability reliably, especially when\nweighting the criterion scores according to the generalised\npartial credit model. Expert annotation is expensive, how-\never, and MIR can add value by automating this process.\n6.1 Rule-Based Model\nFirst, we developed a heuristic model as a baseline for\ncomparison against more sophisticated learning methods.\nFor those rubric criteria involving potentially different per-\nchord difﬁculties (e.g., ﬁngering difﬁculty), we used a TF-\nIDF weighted average of the difﬁculty heuristic over all\nchords in the song:\n/summationdisplay\ncTF(c)×IDF(c)×difﬁculty(c) (2)\nwhere difﬁculty(c)represents the difﬁculty score associated\nwith a speciﬁc chord, considering factors such as chord ﬁn-\nger positioning (CFP), chord ﬁngering difﬁculty (CFD), or\nRight-hand complexity (RHC). In our case, TF is how often\na chord appears in a song divided by the number of chords\nin the said song, and IDF is the log of the total number\nof songs divided by the number of songs that contain that\nchord. For the criteria that depend on ﬁngering, we assumed\none possible ﬁngering per chord based on an extensive list\nof set ﬁnger positions on the Chordify website. We also\nhad to simplify certain chords for which standard ﬁngerings\nproved difﬁcult to ﬁnd, for example, chord with extensions\nlike/sharp11; we added a simpliﬁcation penalty to compensate.\nUncommonness of chord (UC) uses a difﬁculty of one\nfor all chords (i.e., it is the average TF-IDF weight).\nChord ﬁnger positioning (CFP) requires the guitar dia-\ngram and is based on a naïve approach of counting\nthe distance between the lowest and highest played\nfret, not considering which strings they are played.\nCFP= (1+ simpliﬁed ×fsimple)×ﬁnger distance\nChord ﬁngering difﬁculty (CFD) is based on how many\nﬁngers are used, and if a ﬁnger is used for more than\none string, it is counted as a barre chord. For this\ncriterion, we had three learnable parameters, one for\nthe importance of how many ﬁngers were used, one\nthe importance of barre chords, and one for simpliﬁc-\nation.\nCFD= (1+ simpliﬁed ×fsimple)\n×(ﬁngers∗fﬁnger+bar∗fbar)Repetitiveness (R) is the number of unique phrases in a\nsong according to the Billboard annotations.\nRight-hand complexity (RHC) is based on apply the rub-\nric level descriptions to ﬁngering diagrams.\nRHC=\n\n0if no un-strummed strings\n1if outer strings not strummed\n2if one inner string not strummed\n3if multiple inner strings not strummed\nChord progression time (CPT) is the average chord dur-\nation (in s) according to the Billboard annotations.\nBeat difﬁculty (BD) is the ratio of chords that were longer\nor shorter than the most common chord duration in\nthe Billboard annotations.\nGiven these preliminary scores per criterion, averaged ac-\ncording to TF-IDF weights as necessary, we iterated over\nall annotations in Billboard Playability Dataset and grid-\nsearched for the three optimal thresholds, one between each\npair of adjacent difﬁculty levels. For categories with learn-\nable parameters, we extended the grid search accordingly.\n6.2 Classiﬁcation Experiments\nIn addition to the rule-based model we also trained neural\nnetworks on the playability prediction task using two archi-\ntectures: LSTMs and DeepGRU with attention, which have\nbeen applied recently to piano playability [16, 28]. We rep-\nlicated the same parameter settings as used in these papers.\nInspired by our ﬁndings on length and difﬁculty above, we\nalso included models using only representation length, with\nthresholds trained in the same way as the rule-based model.\nFor each architecture, we tested three distinct types of in-\nput: (1) processing each song character by character, which\ndoes not explicitly imply chord information (e.g. A:maj\n→‘A’, ‘:’, ‘m’, ‘a’, ‘j’); (2) splitting each chord into root\nand quality and treating those as unique input symbols, sim-\nilar to music-theoretical understanding (e.g. A:maj →‘A’,\n‘maj’); and (3) converting each chord into the corresponding\nguitar tablature, guitar-neck-like encodings displaying each\nof the six guitar strings with an ‘x’ label if it is skipped, ‘o’\nif it is open, or which ﬁnger goes on which fret otherwise\n(e.g., A:maj →[‘x’, ‘o’, ‘2:1’, ‘2:2’, ‘2:3’, ‘o’], where ‘2:1’\nrepresents the 2nd fret being played by the ﬁrst ﬁnger).\nGiven the characteristics of our rubric, we deﬁned a\ncustom loss function OL, which enforces an ordinal-like\nstructure in the class prediction:\nOL=3/summationdisplay\ni=0ρi×(target−i), (3)\nwhereρiis the predicted probability of level ifor the cri-\nterion in question. We trained the models in two settings:\nﬁrst to predict the total weighted playability score, and then\nto predict each individual criterion in turn. For all training\nconﬁgurations, we subdivided our dataset into 10 sections\nfor our experiments and conducted 10-fold cross-validation.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n729Model Input CFP ↓ CFD↓ UC↓ RHC↓ CPT↓ BD↓ R↓ Aggregate ↓\nRule-based - 1.04 (0.05) 0.85 (0.04) 0.95 (0.05) 0.78 (0.05) 0.90 (0.05) 1.20 (0.06) 0.93 (0.06) 12.38 (0.52)\nLength-based char 1.09 (0.03) 0.88 (0.03) 1.01 (0.03) 0.80 (0.01) 0.87 (0.03) 1.20 (0.04) 0.94 (0.06) 12.46 (0.36)\nLength-based split 1.09 (0.02) 0.88 (0.03) 1.02 (0.03) 0.80 (0.01) 0.86 (0.03) 1.19 (0.04) 0.95 (0.05) 12.46 (0.35)\nLength-based diagram 1.09 (0.02) 0.88 (0.02) 1.02 (0.04) 0.80 (0.01) 0.86 (0.03) 1.19 (0.04) 0.95 (0.05) 12.46 (0.36)\nLSTM char 0.75 (0.18) 0.50 (0.08) 0.68 (0.11) 0.34 (0.13) 1.25 (0.14) 0.74 (0.24) 0.70 (0.15) 5.27 (0.77)\nLSTM split 0.77 (0.14) 0.52 (0.11) 0.65 (0.08) 0.33 (0.13) 1.25 (0.12) 0.77 (0.24) 0.72 (0.14) 5.96 (1.40)\nLSTM diagram 0.78 (0.14) 0.51 (0.08) 0.65 (0.10) 0.35 (0.13) 1.27 (0.15) 0.79 (0.23) 0.72 (0.13) 6.20 (1.02)\nDeepGRU char 0.67 (0.18) 0.60 (0.24) 0.77 (0.21) 0.47 (0.39) 0.92 (0.42) 1.10 (0.54) 0.70 (0.15) 5.61 (1.17)\nDeepGRU split 0.69 (0.15) 0.50 (0.10) 0.66 (0.22) 0.30 (0.14) 1.22 (0.30) 1.00 (0.28) 0.80 (0.29) 5.88 (0.93)\nDeepGRU diagram 0.68 (0.17) 0.55 (0.18) 0.80 (0.27) 0.80 (0.53) 0.90 (0.48) 0.84 (0.20) 0.96 (0.58) 5.99 (1.12)\nTable 4 . Playability prediction performances after training on the Billboard Playability Dataset. The columns are the\nperformance when trained on and predicting each of the seven categories independently, followed by the error between all\nindividual categories added together for the baselines and the error when trained to directly predict the aggregated score for\nthe LSTM and DeepGRU models. Performances are reported in mean ordinal loss over 10 fold cross-validation with their\nstandard deviation. The overall best performing model is the LSTM with chords split into root and quality, except for the\ntwo time-dependant categories: chord progression time (CPT) and beat difﬁculty (BD).\n7. RESULTS\nOur rule-based model performs better than the length-based\ndifﬁculty predictions except for the chord progression time\nand beat difﬁculty category, as seen in Table 4. Since we\nuse three different chords representations, each of which\nyield different lengths, we show length-based classiﬁcation\nresults for each representation, but in practice, these differ-\nences seem to play a negligible role in playability prediction\nbased on length. All three length baselines-based achieve\nvery similar losses for all categories.\nWhen looking at the machine-learning models, we see\nthat they are more variable, but on average substantially\nbetter, than all baseline models, both in classifying each\ncriterion separately and predicting the weighted total difﬁ-\nculty. The only criterion where machine-learning models\nperform worse is the chord progression time. This criterion\nexpresses the speed difﬁculty, which is characterised by\nchord duration. The lack in performance can be explained\nby the fact that the chord transcriptions which form the\ninput to our model do not contain this duration information.\nOddly, both machine learning models do outperform the\nbaseline in predicting beat difﬁculty, which is also depend-\nent on chord duration. When taking the histogram for this\ncriterion into account, however, this performance can be\nexplained by class imbalance: trying to set thresholds is\nworse than simply settling on the largest class. The same\nclass imbalance is likely responsible for the partial-credit\nmodels assigning such a low weight.\nAlthough there is no obvious best model when looking\nacross performance on the individual criteria, the LSTM\ndoes show less variability than DeepGRU, and the LSTM\ntrained on character input performs signiﬁcantly better on\npredicting the weighted total score. We expected a bigger\ndifference in input type, with the guitar chord diagram per-\nforming the best because this chord representation encodes\nthe most guitar playing information, but this turned out to\nbe the worst performing input type of the three. We hypo-\nthesise this is caused by the sequential models not picking\nup on the guitar or hand-related physics.8. CONCLUSION\nIn this paper, we introduced a novel rubric that captures\nthe playability of guitar songs. This rubric comprises seven\ncriteria that can be combined into a single playability score.\nNext to this rubric, we also introduced the Billboard Play-\nability Dataset, 200 playability annotations for songs from\nthe Billboard dataset, which we used to validate the rubric’s\nreliability and conﬁrm that indeed, guitar playability can be\nmeasured. Following these results, we developed several\nmodels for playability prediction. As a baseline, we started\nwith a rule-based model that follows the rubric as mechan-\nically as possible. We then trained and evaluated an LSTM\nand DeepGRU on three different types of chord representa-\ntions. The representation encoding the least guitar – only\nusing textual characters – surprisingly performed best, and\nthe representation encoding the most guitar chord informa-\ntion – guitar tablature – performed the worst. Nevertheless\nboth LSTM and DeepGRU outperformed the rule-based\nmodel with the LSTM performing the best at predicting the\noverall playability. In future work, we aim to extend both\nthe dataset and the models to capture more nuances of play-\nability, and we hope this work will encourage and enable\nmore MIR researchers to explore the ﬁeld of playability and\nimprove online instrument learning environments. Addi-\ntionally, we envision the potential extension of our research\nto incorporate MusicXML or GuitarPro formats, enabling\nthe integration of our playability scores and models into\nwidely used music notation software.\n9. ACKNOWLEDGEMENTS\nWe are sincerely grateful to Jeanine Sier, Barbara de Bruin,\nRobin Willems, and Tom Strandberg for their valuable input\nand feedback on the rubric, and to Tom again for diligently\nannotating the songs. This research was supported by the\nDutch Research Council (NWO) as part of the project In-\nDeep (NWA.1292.19.399). Additionally, we would like\nto express our appreciation to Chordify for their generous\nsupport in funding the annotations.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n73010. REFERENCES\n[1]A. Williams, “Guitars are back, baby!” The\nNew York Times , Sep. 2020. [Online]. Avail-\nable: https://www.nytimes.com/2020/09/08/style/guitar-\nsales-fender-gibson.html\n[2]R. C. Rodriguez and V . Marone, “Guitar learning, ped-\nagogy, and technology: A historical outline,” Social\nSciences and Education Research Review , vol. 8, no. 2,\npp. 9–27, Dec. 2021.\n[3]The Associated Board of the Royal Schools of\nMusic, “Making music: Teaching, learning &\nplaying in the UK,” 2014. [Online]. Available: https:\n//gb.abrsm.org/media/12032/makingmusic2014.pdf\n[4]R. Reynolds and M. M. Chiu, “Formal and informal\ncontext factors as contributors to student engagement\nin a guided discovery-based program of game design\nlearning,” Learning, Media and Technology , vol. 38,\nno. 4, pp. 429–462, 2013.\n[5]“Chordify: Het Groningse paradepaardje van de IT-\nen muziekindustrie heeft al acht miljoen gebruikers,”\nGroninger ondernemers courant , Jan. 2023. [Online].\nAvailable: https://www.groningerondernemerscourant.\nnl/nieuws/chordify-het-groningse-paradepaard-van-de-\nit–en-muziekindustrie-heeft-al-acht-miljoen-gebruikers\n[6]Ultimate Guitar. [Online]. Available: https://www.\nultimate-guitar.com/forum/\n[7]J.-P. Boon, A. Noullez, and C. Mommen, “Complex\ndynamics and musical structure,” Journal of New Music\nResearch , vol. 19, no. 1, pp. 3–14, 1990.\n[8]T. Magnusson, “Of epistemic tools: Musical instru-\nments as cognitive extensions,” Organised Sound ,\nvol. 14, no. 2, p. 168176, 2009.\n[9]A. Chirico, S. Serino, P. Cipresso, A. Gaggioli, and\nG. Riva, “When music ﬂows. state and trait in musical\nperformance, composition and listening: A systematic\nreview,” Frontiers in Psychology , vol. 6, p. 906, 2015.\n[10] S. Swaminathan and E. G. Schellenberg, “Musical com-\npetence is predicted by music training, cognitive abilit-\nies, and personality,” Scientiﬁc Reports , vol. 8, no. 9223,\n2018.\n[11] J. De Souza, “Guitar thinking,” Soundboard Scholar ,\nvol. 7, no. 1, p. 1, 2022.\n[12] R. M. Brown, R. J. Zatorre, and V . B. Penhune, “Expert\nmusic performance: Cognitive, neural, and develop-\nmental bases,” Progress in Brain Research , vol. 217, pp.\n57–86, 2015.\n[13] C. Palmer, “Music performance,” Annual Review of\nPsychology , vol. 48, no. 1, pp. 115–138, 1997.[14] V . Sébastien, H. Ralambondrainy, O. Sébastien, and\nN. Conruyt, “Score analyzer: Automatically determin-\ning scores difﬁculty level for instrumental e-learning,”\ninProceedings of the 13th International Society for Mu-\nsic Information Retrieval Conference , Porto, Portugal,\n2012, pp. 571–576.\n[15] S.-C. Chiu and M.-S. Chen, “A study on difﬁculty level\nrecognition of piano sheet music,” IEEE International\nSymposium on Multimedia , pp. 17–23, 2012.\n[16] P. Ramoneda, N. C. Tamer, V . Eremenko, X. Serra,\nand M. Miron, “Score difﬁculty analysis for piano per-\nformance education based on ﬁngering,” in Proceed-\nings of the IEEE International Conference on Acoustics,\nSpeech and Signal Processing , 2022, pp. 201–205.\n[17] G. Hori and S. Sagayama, “Minimax Viterbi algorithm\nfor hmm-based guitar ﬁngering decision.” in Proceed-\nings of the 17th International Society for Music Inform-\nation Retrieval Conference , New York, New York, 2016,\npp. 448–453.\n[18] S. Ariga, S. Fukayama, and M. Goto, “Song2guitar:\nA difﬁculty-aware arrangement system for generating\nguitar solo covers from polyphonic audio of popular\nmusic.” in Proceedings of the 18th International Society\nfor Music Information Retrieval Conference , Suzhou,\nChina, 2017, pp. 568–574.\n[19] J. A. Burgoyne, J. Wild, and I. Fujinaga, “An expert\nground truth set for audio chord recognition and music\nanalysis.” in Proceedings of the 12th International Soci-\nety for Music Information Retrieval Conference , vol. 11,\nMiami, Florida, 2011, pp. 633–638.\n[20] N. Condit-Schulz and C. Arthur. (2023) Coordinated\ncorpus of popular music. [Online]. Available: https://\ngithub.com/Computational-Cognitive-Musicology-Lab\n[21] H. V . Koops, W. B. de Haas, J. A. Burgoyne, J. Bransen,\nA. Kent-Muller, and A. V olk, “Annotator subjectivity\nin harmony annotations of popular music,” Journal of\nNew Music Research , vol. 48, no. 3, p. 232252, 2019.\n[22] J. Pauwels, K. O’Hanlon, E. Gómez, and M. B. Sandler,\n“20 years of automatic chord recognition from audio,” in\nProceedings of the 20th International Society for Music\nInformation Retrieval Conference , Delft, the Nether-\nlands, 2019, pp. 54–63.\n[23] J. C. Nunnally, Psychometric Theory . New York:\nMcGraw–Hill, 1978.\n[24] G. N. Masters, “A Rasch model for partial credit scor-\ning,” Psychometrika , vol. 47, no. 2, pp. 149–174, 1982.\n[25] E. Muraki, “A generalized partial credit model: Ap-\nplication of an EM algorithm,” Applied Psychological\nMeasurement , vol. 16, pp. 159–176, 1992.\n[26] B. D. Wright and G. N. Masters, Rating Scale Analysis .\nChicago: MESA Press, 1982.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n731[27] A. Vehtari, A. Gelman, and J. Gabry, “Practical\nBayesian model evaluation using leave-one-out cross-\nvalidation and WAIC,” Statistics and Computing ,\nvol. 27, no. 5, pp. 1413–1432, 2017.\n[28] M. Maghoumi and J. J. LaViola, “DeepGRU: Deep\ngesture recognition utility,” in Advances in Visual Com-\nputing: 14th International Symposium on Visual Com-\nputing . Berlin: Springer, 2019, pp. 16–31.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n732"
    },
    {
        "title": "Towards Computational Music Analysis for Music Therapy.",
        "author": [
            "Anja Volk",
            "Tinka Veldhuis",
            "Katrien Foubert",
            "Jos De Backer"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265269",
        "url": "https://doi.org/10.5281/zenodo.10265269",
        "ee": "https://zenodo.org/records/10265269/files/000028.pdf",
        "abstract": "The research field of music therapy has witnessed a rising interest in recent years to develop and employ computational methods to support therapists in their daily practice. While Music Information Retrieval (MIR) research has identified the area of health and well-being as a promising application field for MIR methods to support health professionals, collaborations with experts in this field are as of today sparse. This paper provides an overview of potential applications of computational music analysis as developed in MIR for the field of active music therapy. We elaborate on the music therapy method of improvisation, with a particular focus on introducing therapeutic concepts that relate to musical structures. We identify application scenarios for analysing musical structures in improvisations, introduce existing analysis methods of therapists, and discuss the potential of MIR methods to support these analyses. Upon identifying a current gap between high-level concepts of therapists and low-level features from existing computational methods, the paper concludes further steps towards developing computational approaches to music analysis for music therapy in an interdisciplinary collaboration.",
        "zenodo_id": 10265269,
        "dblp_key": "conf/ismir/VolkVFB23",
        "keywords": [
            "Music therapy",
            "Computational methods",
            "Health and well-being",
            "Improvisation",
            "Therapeutic concepts",
            "Musical structures",
            "Active music therapy",
            "Music Information Retrieval (MIR)",
            "Collaborations with experts",
            "Interdisciplinary collaboration"
        ],
        "content": "TOWARDS COMPUTATIONAL MUSIC ANALYSIS FOR MUSIC THERAPY\nAnja Volk1Tinka Veldhuis1Katrien Foubert2Jos de Backer2\n1Department of Information and Computing Sciences, Utrecht University, the Netherlands\n2Faculty of Medicine, KU Leuven, LUCA School of Arts, Belgium\na.volk@uu.nl, tinkaveldhuis94@gmail.com, {katrien.foubert,jos.debacker}@kuleuven.be\nABSTRACT\nThe research ﬁeld of music therapy has witnessed a rising\ninterest in recent years to develop and employ computa-\ntional methods to support therapists in their daily practice.\nWhile Music Information Retrieval (MIR) research has\nidentiﬁed the area of health and well-being as a promising\napplication ﬁeld for MIR methods to support health pro-\nfessionals, collaborations with experts in this ﬁeld are as of\ntoday sparse. This paper provides an overview of potential\napplications of computational music analysis as developed\nin MIR for the ﬁeld of active music therapy. We elaborate\non the music therapy method of improvisation, with a par-\nticular focus on introducing therapeutic concepts that relate\nto musical structures. We identify application scenarios for\nanalysing musical structures in improvisations, introduce\nexisting analysis methods of therapists, and discuss the po-\ntential of MIR methods to support these analyses. Upon\nidentifying a current gap between high-level concepts of\ntherapists and low-level features from existing computa-\ntional methods, the paper concludes further steps towards\ndeveloping computational approaches to music analysis for\nmusic therapy in an interdisciplinary collaboration.\n1. INTRODUCTION\nThe use of music technology in the context of health and\nwell-being is becoming increasingly important, in line with\na growing interest in eHealth in medicine. Music’s affor-\ndances such as emotion regulation [1], motor coordina-\ntion [2], and social interaction [3], enable a broad range\nof therapeutic applications. They feed into research on de-\nveloping music technology for various contexts of music\ntherapy (MT) such as for supporting motor and cognitive\nrehabilitation through musical biofeedback [4] and through\nmusic-based applied games [5, 6], or through digital musi-\ncal instruments developed for speciﬁc patient groups [7].\nFor a broad overview on different use cases of music tech-\nnology for music therapy we refer to [8].\nOne of the main application ﬁelds envisioned for music\ntechnology in the context of health and well-being is the\n© A. V olk, T. Veldhuis, K. Foubert and J. de Backer. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: A. V olk, T. Veldhuis, K. Foubert and J. de\nBacker, “Towards Computational Music Analysis for Music Therapy”,\ninProc. of the 24th Int. Society for Music Information Retrieval Conf.,\nMilan, Italy, 2023.support of data analysis from therapeutic sessions, includ-\ning analysis and visualizations of musical structures. The\ncomputational analysis of musical structures has been one\nof the main research topics of music information retrieval\n(MIR) over the past decades. While MIR has identiﬁed the\nhealth context as one of its future challenges [9], there exist\nonly few attempts to employ MIR methods for the analysis\nof musical structures in the context of MT to date [10–15].\nThe goal of this paper is to provide an introduction and\noverview on how MIR methods for computational music\nanalysis can be of use for active music therapy (employing\nmusic making), speciﬁcally for analysing musical improvi-\nsations from therapy sessions to support music therapists.\nThe contributions of this paper are threefold: First, it\nprovides an overview of the different contexts in which\nmusic therapists analyse musical material from improvisa-\ntions, and of their analytical approaches (Section 2). Sec-\nond, it identiﬁes and describes different scenarios in MT\nwhich can beneﬁt from computational methods on music\nanalysis. (Section 3). Third, it identiﬁes a current gap be-\ntween high-level concepts of therapists and low-level fea-\ntures of current computational approaches, and concludes\ncollaboration perspectives for MIR and MT researchers on\ndeveloping computational approaches to musical structure\nanalysis of clinical improvisations (Section 4).\n2. OVERVIEW ON MUSIC THERAPY\n2.1 What is music therapy?\nThe American Music Therapy Association describes mu-\nsic therapy (MT) as \"the clinical and evidence-based use\nof music interventions to accomplish individualized goals\nwithin a therapeutic relationship by a credentialed profes-\nsional who has completed an approved music therapy pro-\ngram\" [16]. In a music intervention, therapists create musi-\ncal experiences in a holistic manner involving the patient’s\ncognition, emotion, movement and social interaction, to\napproach issues faced by their patients. Music therapists\ntheorize that musical processes are correlated with psy-\nchological processes [17, 18]: a musical change can indi-\ncate a change in the patient’s inner state or in the interre-\nlation between the patient and others. For instance, if a\npatient with ADHD learns to focus during MT, or a patient\nwith Parkinson learns to have more control over their body\nwhile playing music, these improvements can be generaliz-\nable to other areas in their lives, because of the interdepen-\ndence of human functioning [17]. For an overview on the247Figure 1 . Main forms and methods of music therapy with application areas for computational music analysis. This paper\nfocuses on the areas depicted by coloured blocks.\nvarious affordances of music for therapeutic use, and clini-\ncal and non-clinical contexts of music intervention see [8].\n2.2 Active music therapy\nMT is divided into active (music making) and receptive\n(music listening) MT [19], see Figure 1. A common re-\nceptive method is called Guided Imagery and Music (GIM)\n[20], in which the patient listens to music in a relaxed state.\nThe therapist guides the patient in bringing up the imagery\nthat emerges from their inner process in response to the\nmusic, to explore their inner conﬂicts.\nIn active MT, creative methods such as improvisation,\ncomposition and songwriting are employed, as well as\nrecreative methods such as singing an existing song. Cre-\native methods are used to unravel underlying psychological\npatterns [21]. For instance, if the patient acts mainly as the\nfollower in the interaction with the therapist during a musi-\ncal improvisation (i.e. only the therapist initiates changes\nin the music), this behaviour can help to unravel interac-\ntion patterns and corresponding associations in daily life\ninteractions of the patient.\nIn improvisation methods, the patient plays or sings mu-\nsic that they are creating themselves, either alone, with the\ntherapist or in a group. This paper focuses on the setting\nof therapist and patient playing together. During a MT ses-\nsion, the therapist is focused on creating the music together\nwith the patient (and the verbal evaluation of it), using mu-\nsical interventions, such as changes in one or more musical\nparameters like timbre, dynamics, or timing, to encourage\nchanges in the playing style of the patient. After the ses-\nsion, the therapist listens to the recording of the session\nand seeks to answer questions such as: In what way does\nthe patient interact? Where did it feel like we were in a\nﬂow together (instead of the patient just playing for them-\nselves) and what type of intervention caused this? In which\nmusical parameters is the patient very rigid and what inter-\nventions change that? The therapist then seeks to draw\nparallels to other aspects of the patient’s life and could ask\nthe patient in the next session if the way they interact and\nreact to the music is the same in other situations in their\nlife. After this verbal reﬂection, these habits can be further\nexplored when improvising again. In this way, therapist\nand patient try to slowly break out of typical habits in the\nprocess of consecutive MT sessions.\nIn composition methods, the patient uses their impro-visations to subsequently compose music. This could be\ndone for example by starting with a musical improvisation,\nthen selecting from the improvisations the parts or motifs\nthe patient ﬁnds most interesting to use in a composition,\nthen improvising again using these motifs, and so forth,\nhence employing an iterative approach. This fosters inter-\npersonal trust through a joint process working towards an\nexplicit artistic product [22]. In song writing, the use and\nanalysis of lyrics is important, in composition the focus is\non using musical parameters and structure.\n2.3 Analysis of music therapy improvisations\nThere exist many different approaches to analyse MT ses-\nsions. In some of them, therapists analyse only the be-\nhaviour of the patient, and not the created music. Ap-\nproaches that do analyse musical structures are called\nmusic-centered approaches , such as the Nordoff-Robbins\nmethod [23, 24]) and the so-called psychodynamic ap-\nproach [25]. This paper focuses on the psychodynamic\napproach, which suggests that producing music can help\naccessing the unconscious mind such that the patient’s un-\nderlying issues will surface within a musical improvisa-\ntion. While analysing musical structures can be useful in\nall MT methods and approaches, e.g. in receptive methods\npattern discovery could be helpful to investigate whether\nspeciﬁc patterns contribute to what patients prefer to lis-\ntening to in speciﬁc contexts, we will focus in this paper\non active, creative MT methods (see Figure 1).\nBruscia, one of the pioneers for analysis of MT improvi-\nsations (MTI), created the so-called Improvisation Assess-\nment Proﬁles (IAPs) [26], for which the therapist ﬁlls out\nquestionnaires based on their observations. The IAP con-\nsists of six different proﬁles (called Autonomy, Integration,\nTension, Variability, Salience and Congruence ). For exam-\nple, the Autonomy proﬁle explores the intermusical rela-\ntionship between patient and therapist, which could show\nthat the patient is a leader or a follower, where the patient\ndoes or does not initiate changes in the music when playing\ntogether. The therapist can observe this relationship in dif-\nferent musical dimensions, such as in rhythm, melody, or\nharmony, but also in lyrics [27]. The Tension proﬁle shows\nhow much tension is created through different aspects of\nthe music, relating to questions such as: is the tempo or\nmodality calm, or tense [28]? For a detailed description of\nVariability, Salience, and Congruence, we refer to [17,28].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n248Therapists typically do not use all proﬁles and dimen-\nsions, but focus on one proﬁle and ﬁll out the questionnaire\nfor all musical dimension or analyse one musical dimen-\nsion in all six proﬁles [10]. They choose the proﬁle and\nparameters based on the context: for instance, with MT\nfor a child with ADHD, it could be relevant to analyse the\npresence of hyper-activity and how this changes over time,\nfor which the therapist could use the Tension proﬁle.\nThe proﬁles describe high-level concepts from Brus-\ncia [17] that require ﬁne-grained descriptions on how they\nmight relate to concepts of musical structure for MIR re-\nsearchers. For instance, when can a tempo be described\nas tense? According to [10, 29, 30], there exists a variety\nof other methods for analysing MTI, see e.g. [24, 31–33],\nwhich all address various high-level MT concepts (such as\nautonomy or tension). In general, these analyses are car-\nried out by the therapist after the MT session by listening\nto the recorded music, using different analytical methods\nand the implicit musical knowledge of the therapist. There\nexists no systematic research on how music therapists use\nthe different music analysis methods in their practice, as\nmusic analysis methods in MT are a particularly under-\nresearched area according to [34].\n3. APPLICATION AREAS OF COMPUTATIONAL\nMUSIC ANALYSIS IN MT IMPROVISATIONS\nComputational methods for music analysis can be em-\nployed for several areas within the MT domain. In this\nsection, we describe for the creative methods within active\nMT (namely improvisation, composition and songwriting)\nthe following different application areas: initial clinical\nassessment; monitoring process; ﬁnding of so-called Mo-\nments of Interests; and enhancing the creative potential of\npatients within composition processes.\n3.1 Psychological Assessment\nPsychological assessment is the collection and analysis of\ninformation of a patient, resulting in hypotheses about the\nnature and causes of a patient’s personality, condition, re-\nsources and potentials. In the context of MTI, the informa-\ntion includes musical data.\nInitial clinical assessment. Hypotheses which are\nformed in the psychological assessment are used to deter-\nmine an effective treatment program [10], considering the\nskills the patient currently has and what kind of therapy\nwould ﬁt them. Data gathered in the psychological assess-\nment during the ﬁrst therapy session is used to determine\nif the patients’ symptoms are consistent with the diagnos-\ntic criteria for a speciﬁc mental disorder [35]. Computa-\ntional analyses of the improvisations in the ﬁrst therapy\nsession can support the initial clinical assessment. For in-\nstance, computationally analysing musical timing param-\neters of clinical improvisations can be promising in diag-\nnosing Borderline Personality Disorder [14].\nMonitoring process. Therapists use the data gathered\nwith psychological assessment in later stages of the ther-\napy for monitoring the process of the patient during treat-ment, such as for detecting any form of progress or devel-\nopment of the patient. One existing approach for assessing\nthis process is the so-called microanalysis [30] which fo-\ncuses on small changes in social, musical, and emotional\nbehaviour and experiences within one MT session. A com-\nputational tool could assist in performing the microanalysis\non the musical content on aspects such as identifying mu-\nsical dimensions of the patient’s improvisations that, for\ninstance, contain many repetitions for assessing the degree\nof rigidity in the playing style; identifying aspects of in-\nterventions of the therapist that caused changes in the pa-\ntient’s playing style on a micro level; determining the di-\nmensions which had the greatest inﬂuence on the musical\nchange observed in the improvisation. Gathering these in-\nsights over different sessions helps to establish what is typ-\nical of a patient’s improvisational style and how it changes\nover time as a result of the therapeutic interventions.\nMoments Of Interest . Effects of MT are often seen\nin speciﬁc moments within one musical improvisation ses-\nsion. When carrying out psychological assessment, ther-\napists seek to identify these speciﬁc moments which can\nbe turning points in the development of a patient. The fo-\ncus of the therapist’s analysis is to identify the so-called\nMoments of Interest (MOIs) [21], though there exist many\nother terms for MOIs, such as meaningful moments [36],\npivotal moments [37], and present moments [38].\nMOIs are chosen by therapists based on what they\nrecognise as an important change [30]. It could be a\nmistake (e.g. patient accidentally plays an unintentional\nnote), a mis-attunement between therapist and patient (e.g.\npatient does not listen to the therapist’s playing which\nleads to unsynchronized notes), a refreshing new harmonic\nchord, etc. The musical events right after this moment are\nalso of interest, since the therapist notices if the change\nleads indeed to something new within the improvisation\n(e.g. if a moment of interaction occurs between patient\nand therapist where they dissolve the mistake or continue\non the new chord). MOIs are not described by one single\nform of musical change, different musical elements could\nbe of importance in the identiﬁcation of MOIs for individ-\nual diseases and patients. For instance, for patients with\npsychosis it could be an important change if they stop play-\ning repetitively [39], and for borderline patients it could be\nan important change if they start alternating between lead-\ning and following the therapist in the improvisation [40].\n3.1.1 Case study: playing styles for psychosis patients\nA speciﬁcally interesting example for the potential applica-\ntion of computational music structure analysis, is the iden-\ntiﬁcation of different playing styles within MTI of patients\nwith psychosis, including the ﬁnding of speciﬁc MOIs,\nnamely Moments of Synchronicity (MOS). The spectrum\nof different playing styles as identiﬁed in [41] ranges from\nso-called sensorial play tocomplete musical form . In be-\ntween arise Moments of Synchronicity.\nSensorial play describes a style consisting of repetitive\nand monotonous, or chaotic and fragmented play, with a\nlack of phrasing, silences and dynamics. This style is typ-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n249ical for patients with psychosis who are perceptually and\nemotionally detached from their improvisation and are not\nreally engaging in the music, leading to an absence of in-\nteraction between patient and therapist. Typical character-\nistics of sensorial play are, e.g., random playing (tonal and\natonal), and a signiﬁcant lack of variation.\nMusical form denotes a playing style that is situated on\nthe other end of the range of observed playing styles in pa-\ntients with psychosis compared to sensorial play. It arises\nfrom an inter-subjective phenomenon between patient and\ntherapist, where both engage in a musical interaction. They\nexperience being autonomous and equal, and are able to in-\ntroduce their own new musical ideas to the improvisation.\nIt is characterized as an improvisation where dynamical\ndifferentiation, pulse, phrasing, pauses, repetition, varia-\ntion, rhythmic and melodic themes, and especially interac-\ntion between players can be observed. A clear beginning,\nending and development can be identiﬁed.\nMoments of Synchronicity (MOS) . For patients with\npsychosis, the goal is to progress from sensorial play to\nmusical form during several therapy sessions. In between,\nMOS between patient and therapist need to be established.\nThese are short moments where both players have a shared\nfeeling of an autonomous and free playing style, consti-\ntuting a moment of musical interaction. Often MOS are\nfuelled by interventions from the therapist. In these mo-\nments, attunement/synchronicity in the musical parameters\nof the patient and therapist can be observed and some vari-\nation, dynamics and phrasing emerge. A pulse, combined\nwith accents in the meter, are shared. MOS enrich the ther-\napeutic relationship, creating moments of trust, which en-\nable the patient to take more risks in the music playing,\nwhich in turn leads to changes in the patient. MOS are\nthe most basic form of a MOI; after these ﬁrst interactions,\nnew interactions can emerge (see [41,42] for a detailed de-\nscriptions of the playing styles and MOS).\nIn sum, MOS denote speciﬁc MOIs for patients with\npsychosis, marking their transition from sensorial play to\nmusical form. Recognizing these different playing styles\nidentiﬁed by the extent as to how much musical structures\nare present in a MTI, delivers an interesting case study for\nMIR on musical structure analysis and pattern discovery.\n3.2 Enhancing the creative potential for composing\nThe composition methods for enhancing the creative po-\ntential of patients as part of active music therapy (see Fig-\nure 1) offer a particularly interesting ﬁeld of application for\nautomatic pattern discovery. For creating compositions,\npatients start with improvising music. Afterwards, the pa-\ntient and therapist listen to the music together and seek to\nﬁnd parts that the patient would like to use for composing\ntheir own musical piece. The therapist writes down the mo-\ntifs they hear in musical notation, which can be time con-\nsuming. A pattern discovery tool could be used to identify\nand highlight all moments where the patient repeated their\nmotifs, and preferably these motifs could be automatically\ntranscribed to musical notation so that in the next phase of\nthe composition these motifs could be used immediately.4. DEVELOPING MIR METHODS FOR MT\n4.1 Existing Computational Approaches in MT\nA number of computational approaches have been devel-\noped to support psychological assessment in MTI, which\nwe summarize in this section. According to [10,12], Com-\nputer Aided Music Therapy Analysis System (CAMTAS)\nwas the ﬁrst attempt to organize and analyse audio and\nvideo data speciﬁcally from MT. Developed during the\nmid-90s, it was used for uploading recorded data and play-\ning back audio and video ﬁles simultaneously. Another an-\nnotation tool, the so-called Music Therapy Analysing Par-\ntitura (MAP) [29], helps to annotate events in MT based on\ntherapist’s manual transcriptions. The therapist can anno-\ntate the auditory material from a session, including the mu-\nsic itself, but also e.g. talking, silence, crying, and laugh-\ning, using a visual format with ﬁxed graphical codes, al-\nlowing the therapist to view the content of one improvisa-\ntion or over a whole session. Both CAMTAS and MAP\nrely on manual work by the therapist without any auto-\nmatic detection of events from music recordings, hence us-\ning these tools is rather time-consuming [10].\nComputational tools for MTI which analyse the musical\ncontent are the Music Therapy Logbook [12] and the Mu-\nsic Therapy Toolbox (MTTB) for MatLab [15,27,43]. The\nMusic Therapy Logbook was developed in collaboration\nbetween MIR and MT researchers. It can be used to gather\nevidence of changes in a patient‘s and therapist‘s use of\nmusic over time for psychological assessment. In a proof-\nof-concept study using simulated MTI where one expert\nwould improvise in the role of the patient and the other as\nthe therapist [12], existing MIR techniques e.g. for the de-\ntection of tempo changes or the identiﬁcation of rhythmic\npatterns, have been employed. It was tested whether com-\nputational methods can assist in evaluating whether ther-\napist’s tempo changes are effective in increasing the pa-\ntient’s tempo ﬂexibility. While it was possible to identify,\nfor instance, call and response type of play between ther-\napist and patient, it was concluded that addressing higher-\nlevel concepts about musical interactions with computa-\ntional means has yet to be fully explored in the future.\nThe MTTB tool takes MIDI ﬁles of therapist and patient\nas input and automatically extracts musical features, which\nit outputs into graphs depicting both the therapist and pa-\ntient over time. The musical features in the MTTB are\nbased on the Autonomy proﬁle of the IAP [27] described\nin section 2.3. For instance, the density graph is calcu-\nlated by averaging the number of notes played in a given\ntime window. Since theory suggests that increasing musi-\ncal density is a sign of increased arousal and therefore in-\ncreased emotional and physiological density [44], density\nshould be clinically relevant [43]. By manually reading\nand interpreting the two graphs of the therapist and patient,\nthe role-relationship can be determined for the feature. A\npilot study [45] investigated how the MTTB could support\nclinical assessment from improvisations when combined\nwith subjective experiences of the participants, delivering\nﬁrst insights on how this tool might be used in the futureProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n250for investigating Bruscia’s improvisation techniques, such\nas imitation and synchronisation through speciﬁc musical\nparameters. The MTTB is still under development.\nFor monitoring the process of patients over several ther-\napy sessions with computational methods, the concept of\nMusical Proﬁling was introduced in [10], comprising three\nparts: Typical Performance for establishing a patient’s in-\ndividual typical playing style, Temporal Evolution for mea-\nsuring the changes in different features in the improvisa-\ntions over some time or over different sessions, and In-\ndividual Tendencies , measuring relations between features\nthat are speciﬁc to that patient. In a case study with 6 par-\nticipants, they used e.g. averages of the features of dura-\ntion, note count, tempo, pulse clarity, dynamic centroid,\nand pitch centroid, to characterize a typical performance.\nThe Musical Proﬁling concept is intended to contribute to\nestablishing a systematic method of measuring and repre-\nsenting musical processes in improvisations in order to for-\nmalize assessment methods. To the best of our knowledge,\nthis concept has not yet been set into practice.\nIn sum, there exist promising ﬁrst steps in developing\ncomputational approaches to support the psychological as-\nsessment of the therapist when analysing clinical improvi-\nsations. They are not yet ready for use in clinical practice.\nLinking high-level MT concepts (such as moments of in-\nterest, tension, salience) to elements of the musical struc-\nture that can be extracted with computational features from\nthe music, is as of today not a solved problem (see [11,46]\nfor studies on linking computational features to clinical\nimprovisations). Moreover, from the perspective of their\npractical use, tools like MATLAB are not easy to use for\nall music therapists, and while MIDI is useful in the MT\nresearch context, most clinical contexts work with audio.\n4.2 Musical structure analysis and pattern discovery\nFor analysing the musical content of clinical improvisa-\ntions, MIR methods for musical structure analysis [47–51]\nas well as pattern discovery [52–64] could be of potential\nuse in the different application ﬁelds within MT described\nin section 3. Techniques to identify coherent segments us-\ning concepts such as homogeneity, novelty, repetition, or\nregularity, developed in music structure analysis [47, 48],\nmight be useful for describing structures emerging in clin-\nical improvisations. Pattern discovery methods could sup-\nport the identiﬁcation of different playing styles such as\nsensorial play or musical form, taking into account the\namount and kind of repetition and variation in musical pat-\nterns identiﬁed. However, these methods have been devel-\noped for different scenarios and styles, such as for popular\nmusic, jazz, classical music or folk songs. It needs to be\nexplored in how far these techniques need to be adapted for\nimprovisations in the MT context; e.g. in [13] it has been\nshown that repeated musical patterns identiﬁed in MTI\nwere rather different from patterns typically investigated\nin musicological analyses of compositions and corpora.\nApart from the difference in the musical material, the\nanalysis process of music therapists differs from musi-\ncological investigations of compositions. Typically, thetherapist has participated in the improvisation and analy-\nses afterwards the recorded musical material, taking into\naccount their own experience during the improvisation,\nwhich might already steer the attention to certain elements\nof the structure. This is different from a musicological\nanalysis of musical material that has been produced by\nother musicians (or musical novices).\nFor adapting MIR methods to the context of MT, the\ntypical high-level concepts addressed by music therapists\nneed to be investigated and deconstructed collaboratively\nin order to establish how these concepts are manifested\nin musical features and structures that can be described\nby computational means. Proof-of-concept studies such\nas [10, 12] provide a promising start into applying MIR\nfeatures for analysing MTI. Yet, in order to develop mean-\ningful computational features for the working context of\ntherapists, their implicit knowledge employed in analysing\nclinical improvisations needs to be made more explicit.\nOne example is given in [34] employing interviews with\ntherapists to determine implicit and explicit knowledge in\nmusic analysis of MTI. Working towards the explicating of\nthis implicit knowledge through collaboration would also\ncontribute to establish how much agreement exists between\ndifferent therapists using the same terminology and analy-\nsis methods. This constitutes an important step not only\nfor developing computational methods, but also in devel-\noping assessment methods that support the development of\nevidence-based methods in MT.\n4.3 Collaboration perspectives for computational\napproaches to musical structure in MT improvisations\nIn MIR, there exists a strong tradition of collaboration\nwith domain experts on investigating speciﬁc musical con-\ncepts for enabling computational modeling, such as on\nLeitmotifs [65–67], on cadences [68–70], on similarity\nof folk songs belonging to a tune family [71, 72], or on\nmelodic schemata and patterns of a certain musical style\n[54, 55, 72–74]. The establishment of data sets and an-\nnotations of experts regarding these concepts has been a\ncrucial factor for enabling collaboration. We expect this to\nbe a necessary step also for developing computational ap-\nproaches to music analysis for MTI. In the following we\nindicate examples for the envisioned collaboration for the\napplications described in section 3.\nInitial clinical assessment . For developing computa-\ntional methods for the analysis of improvisations within\nthe initial clinical assessment, data sets and descriptions of\ntypical playing styles for speciﬁc patient groups need to be\nestablished. For instance, an overview of which proﬁles\nand musical dimensions therapists typically select for spe-\nciﬁc patient groups within their manual assessment using\nIAPs, could serve as a starting point for explicating thera-\npists’ knowledge on how to describe playing styles using\nmusical features. In the future, once computational meth-\nods have been established, they could support therapists to\ninitially scan allproﬁles with the help of computation, in-\nstead of manually selecting a few, ensuring that nothing has\nbeen overlooked before concentrating on selected aspects.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n251Monitoring process . Comparing improvisations from\ndifferent sessions in order to monitor the therapeutic pro-\ncess requires a data-rich approach to musical structure for\nwhich computational methods are speciﬁcally apt. The\nconcept of Temporal Evolution within Musical Proﬁling\n[10] is a ﬁrst step to monitor process, using low-level mu-\nsical features such as note count or pitch centroid. For get-\nting closer to the MT practice, the high-level concepts of\nmusic analysis in MT need to be connected to appropri-\nate models in MIR, such as identifying recurring patterns\nand the amount of repetitiveness and variation observed in\nthese patterns, and the comparison of features over time.\nFinding MOIs and identifying playing styles . For an\noverview on envisioned steps in the collaboration between\nMIR and MT researchers on identifying MOIs as impor-\ntant changes in the therapeutic process, we refer to the dis-\ncussion in [8] on the creation of datasets and annotations,\nand the use of automatic pattern discovery and informa-\ntion theory. For the speciﬁc case of MOS as a progress\nfrom sensorial play to musical form (see Section 3.1.1),\nthe identiﬁcation of emerging synchronicity in the musi-\ncal parameters of patient and therapist requires the model-\ning of musical structure as emerging from an interaction.\nFor instance, MIR models on rhythm and meter could be\nadapted for detecting the establishment of a shared pulse\nin MOS, requiring a data collection with improvisations\nexhibiting different degrees of stability and variability in\ntemporal structures with annotations on MOS as identiﬁed\nby therapists. For distinguishing different playing styles,\nsuch as sensorial play and musical form as described in\nsection 3.1.1, computational methods for identifying mu-\nsical structure, repetition and variation along different mu-\nsical dimensions, can be employed.\nEnhancing the creative potential of patients for com-\nposing . Automatic pattern discovery has been success-\nfully employed in MIR for the automatic generation of\nmusic [75, 76]. For supporting the composition method\nas part of active MT, pattern discovery could assist in en-\nhancing the creativity of the patient in the iterative process\nof generating a composition from improvisations. For dis-\ncovering appropriate motifs in the patient’s improvisations,\nthe methods need to be able to ﬁnd non-exact matches\nthat might be perceptually meaningful. Appropriate visu-\nalization methods for displaying identiﬁed matches could\nsupport patients in choosing which matches they consider\nmeaningful to work with. Automatic music generation sys-\ntems such as [77] could be used to explore whether they\nmight support enhancing the patient’s creativity in MT (see\nthe discussions in [78] on AI and musical creativity, and [8]\non automatic music generation speciﬁcally for MT).\nWe conclude the following steps for establishing the\ncollaboration between MIR and MT on developing com-\nputational methods for music analysis of improvisations:\n• Investigate music analysis methods of therapists and cre-\nate data sets with clinical improvisations and music ana-\nlytic annotations from therapists. Using simulated ther-\napy sessions as in [12, 45] letting therapists imitate typi-\ncal playing styles of patients is a ﬁrst step, yet its useful-ness is limited by providing only stereotypical examples.\n• Establish a catalogue of typical intervention methods of\ntherapists in clinical improvisations as a ﬁrst step on\nﬁnding appropriate musical features for developing an-\nalytical methods to musical interactions in MTI.\n• Explore computational approaches from MIR for the\nautomatic detection of musical structures in MTI; start\nwith assessing existing MIR features, and determine\nsuitable adaptations for supporting therapists’ analytical\nconcepts. Taking into account the granularity of music\ninformation required, determine in which contexts sym-\nbolic and/or audio formats are appropriate.\n• Initiate case studies: For exploring the emergence of\nmusical structures in MTI, identifying different playing\nstyles of patients with psychosis as in Section 3.1.1 can\nprovide a particular interesting starting point for collab-\noration once datasets are established, since these playing\nstyles are well described in the MT literature.\n• Assess how much therapists (dis)agree in their individual\nintervention and analysis styles. Investigating the agree-\nment between therapists requires the building of dedi-\ncated annotation tools and methods to measure agree-\nment between annotators, such as in [79–82].\n• Assess how the intuitive knowledge of the music thera-\npist on their subjective experience in the improvisation\ncan be combined with and enhanced by the objective\nanalysis of the musical material through computation,\nto support therapists in their daily work, and to further\nestablish evidence-based treatments in MT.\n• Consider aspects of usability, including considerations\nfrom HCI, for developing appropriate tools for the daily\npractice of therapists; see [8, 42] for some general con-\nsiderations on developing tools for MT.\n5. CONCLUSION\nClinical improvisation from music therapy provide inter-\nesting and novel musical data for developing MIR methods\nfor music structure analysis and pattern discovery, aim-\ning to support therapists in their daily work. Interdisci-\nplinary efforts between MIR and MT researchers need to\nbe invested to close the gap between high-level concepts\nof music analysis used by therapists, and low-level fea-\ntures of current computational approaches to analyse MTI.\nInvestigating music analysis methods employed by thera-\npists, including the explication of therapists’ implicit mu-\nsical knowledge and the assessment of the agreement be-\ntween different therapists, constitutes a crucial step for de-\nveloping computational tools for MT. Speciﬁcally impor-\ntant herein is the establishing of data sets with clinical im-\nprovisations for different application areas, such as for ini-\ntial clinical assessment, including different patient groups\nand catalogues of typical interventions of therapists. Cre-\nating these data sets through interdisciplinary efforts will\nnot only prepare the ground for the appropriate computa-\ntional modeling of music structures in MTI, but also to a\nbetter understanding of music analysis methods in MT, ul-\ntimately contributing to ongoing research on establishing\nevidence-based methods in MT.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2526. REFERENCES\n[1] K. S. Moore, “A systematic review on the neural ef-\nfects of music on emotion regulation: Implications for\nmusic therapy practice,” Journal of Music Therapy ,\nvol. 50, no. 3, p. 198–242, 2013.\n[2] S. Dalla Bella, C. E. Benoit, N. Farrugia,\nM. Schwartze, and S. A. Kotz, “Effects of musi-\ncally cued gait training in Parkinson’s disease: Beyond\na motor beneﬁt,” Annals of the New York Academy of\nSciences , vol. 1337, pp. 77–85, 2015.\n[3] W. L. Magee and C. Bowen, “Using music in leisure\nto enhance social relationships with patients with com-\nplex disabilities,” NeuroRehabilitation , vol. 23, pp.\n305–311, 2008.\n[4] P. Kantan, E. G. Spaich, and S. Dahl, “A Techni-\ncal Framework for Musical Biofeedback in Stroke Re-\nhabilitation,” IEEE Transactions on Human-Machine\nSystems , vol. 52, no. 2, pp. 220–231, 2022.\n[5] V . Bégel, A. Seilles, and S. Dalla Bella, “Rhythm\nworkers: A music-based serious game for training\nrhythmic skills,” Music & Science , vol. 1, pp. 1–16,\n2018.\n[6] K. Agres and D. Herremans, “Music and motion-\ndetection: A game prototype for rehabilitation and\nstrengthening in the elderly,” in International Confer-\nence on Orange Technologies , 2017, pp. 95–98.\n[7] Z. Vamvakousis and R. Ramirez, “The eyeharp: A\ngaze-controlled digital musical instrument,” Frontiers\nin Psychology , vol. 7, 2016.\n[8] K. R. Agres, R. S. Schaefer, A. V olk, S. van Hooren,\nA. Holzapfel, S. Dalla Bella, M. Müller, M. de Witte,\nD. Herremans, R. Ramirez Melendez, M. Neerincx,\nS. Ruiz, D. Meredith, T. Dimitriadis, and W. L. Magee,\n“Music, computing, and health: a roadmap for the cur-\nrent and future roles of music technology for health\ncare and well-being,” Music & Science , vol. 4, pp. 1–\n32, 2021.\n[9] X. Serra, M. Magas, E. Benetos, M. Chudy, S. Dixon,\nA. Flexer, E. Gómez, F. Gouyon, P. Herrera, S. Jordà,\nO. Paytuvi, G. Peeters, J. Schlüter, H. Vinet, and\nG. Widmer, Roadmap for music information research .\nCreative Commons BY-NC-ND 3.0 License, 2013.\n[10] N. Letul ˙e, “Musical Proﬁling: Towards a Computer-\nBased Analysis of Clinical Improvisations,” Master’s\nthesis, Dept. of Music, Univ. of Jyväskylä, Jyväskylä,\nFinland, 2014.\n[11] G. Luck, K. Riikkilä, O. Lartillot, J. Erkkilä, P. Toivi-\nainen, A. Mäkelä, K. Pyhäluoto, H. Raine, L. Verkila,\nand J. Värri, “Exploring relationships between level of\nmental retardation and features of music therapy im-\nprovisations: A computational approach,” Nordic Jour-\nnal of Music Therapy , vol. 15, no. 1, pp. 30–48, 2006.[12] E. Streeter, M. E. Davies, J. D. Reiss, A. Hunt, R. Ca-\nley, and C. Roberts, “Computer aided music therapy\nevaluation: Testing the Music Therapy Logbook pro-\ntotype 1 system,” The Arts in Psychotherapy , vol. 39,\nno. 1, pp. 1–10, Feb. 2012.\n[13] C. Anagnostopoulou, A. Alexakis, and A. Triantafyl-\nlaki, “A Computational Method for the Analysis of\nMusical Improvisations by Young Children and Psy-\nchiatric Patients with No Musical Background,” in Pro-\nceedings of the 12th International Conference on Mu-\nsic Perception and Cognition and the 8th Triennial\nConference of the European Society for the Cognitive\nSciences of Music , E. Cambouropoulos, C. Tsougras,\nP. Mavromatis, and K. Pastiadis, Eds., Thessaloniki,\n2012.\n[14] K. Foubert, T. Collins, and J. De Backer, “Impaired\nmaintenance of interpersonal synchronization in musi-\ncal improvisations of patients with borderline person-\nality disorder,” Frontiers in Psychology , vol. 8, Apr.\n2017, Art. no. 537.\n[15] J. Erkkilä, “Music Therapy Toolbox (MTTB) – An\nImprovisation Analysis Tool for Clinicians and Re-\nsearchers,” in Microanalysis in Music Therapy: Meth-\nods, Techniques and Applications for Clinicians, Re-\nsearchers, Educators and Students . Jessica Kingsley\nPublishers, 2021, ch. 10, pp. 134–148.\n[16] American Music Therapy Association. \"Setting the\nrecord straight: What music therapy is and is\nnot.\" Musictherapy.org. Accessed at Aug. 23, 2021.\n[Online]. Available: https://www.musictherapy.org/\nabout/musictherapy/\n[17] K. E. Bruscia, Deﬁning Music Therapy , 3rd ed.\nGilsum, NH, USA: Barcelona Publishers, 2014.\n[18] H. Smeijsters, Sounding the self: analogy in improvi-\nsational music therapy . Gilsum, NH, USA: Barcelona\nPublishers, 2005.\n[19] B. L. Wheeler, Music Therapy Handbook . New York,\nNY , USA: The Guilford Press, 2015.\n[20] H. L. Bonny. \"Music Psychotherapy: Guided\nImagery and Music\". V oices.no. Accessed at\nOkt. 22, 2021. [Online]. Available: https:\n//voices.no/index.php/voices/article/view/1876/1640\n[21] J. Fachner, “Music, moments and healing processes:\nMusic therapy,” in The Routledge Companion to Music\nCognition , R. Ashley and R. Timmers, Eds. Abing-\ndon, UK: Routledge, 2017, ch. 1.8, pp. 89–100.\n[22] J. De Backer, B. Sebreghts, and K. Foubert, “Composi-\ntion Plus. A process-compositional approach in music\ntherapy to empower creative potential.” Journal of Ur-\nban Culture Research , vol. 25, pp. 161–175, 2022.\n[23] P. Nordoff and C. Robbins, Creative Music Therapy .\nNew York: John Day, 1977.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n253[24] K. Aigen, Being in music: Foundations of Nordoff-\nRobbins music therapy . Gilsum, NH, USA: Barcelona\nPublishers, 1996.\n[25] J. De Backer and J. Sutton, The Music in Music Ther-\napy: Psychodynamic Music Therapy in Europe : Clin-\nical, Theoretical and Research Approaches . Jessica\nKingsley Publishers, 2014.\n[26] K. E. Bruscia, Improvisational models of music ther-\napy. Springﬁeld, IL, USA: Charles C Thomas Pub-\nlisher, 1987.\n[27] J. Erkkilä and T. Wosch, “The Music Therapy Tool-\nbox,” in Music Therapy Assessment: Theory, Research,\nand Application . London, UK: Jessica Kingsley Pub-\nlishers, 2019, ch. 15, pp. 293–314.\n[28] Nordic Journal of Music Therapy. \"IAP Re-\nvisited\". Njmt.w.uib.no. Accessed at Apr.\n12, 2022. [Online]. Available: https:\n//njmt.w.uib.no/nordic-journal-of-music-therapy/\nforum-online-discussions-1998-2006/iap-revisited/\n[29] A. Gilboa, “Nordic Journal of Music Therapy Develop-\nments in the MAP: A method for describing and ana-\nlyzing music therapy sessions,” Nordic Journal of Mu-\nsic Therapy , vol. 21, no. 1, pp. 57–79, 2012.\n[30] T. Wigram and T. Wosch, Microanalysis in Mu-\nsic Therapy: Methods, Techniques and Applications\nfor Clinicians, Researchers, Educators and Students .\nLondon, UK: Jessica Kingsley Publishers, 2007.\n[31] H. T. Baxter, J. Berghofer, and L. MacEwan, The indi-\nvidualized music therapy assessment proﬁle: IMTAP .\nLondon, UK: Jessica Kingsley Publishers, 2007.\n[32] M. Forinash and D. Gonzalez, “A Phenomenological\nPerspective of Music Therapy,” Music Therapy , vol. 8,\nno. 1, pp. 35–46, 1989.\n[33] M. Langenberg, J. Frommer, and W. Tress, “A Qualita-\ntive Research Approach to Analytical Music Therapy,”\nMusic Therapy , vol. 12, no. 1, pp. 59–84, 1993.\n[34] N. Letul ˙e, E. Ala-Ruona, and J. Erkkilä, “Professional\nfreedom: A grounded theory on the use of music anal-\nysis in psychodynamic music therapy,” Nordic Journal\nof Music Therapy , vol. 27, no. 5, pp. 448–466, 2018.\n[35] Washington State University. \"Module\n3: Clinical Assessment, Diagnosis, and\nTreatment\". opentext.wsu.edu. Accessed\nat Dec. 18, 2021. [Online]. Available:\nhttps://opentext.wsu.edu/abnormal-psych/chapter/\nmodule-3-clinical-assessment-diagnosis-and-treatment/\n[36] J. Fachner, “Communicating change-meaningful mo-\nments, situated cognition and music therapy: A re-\nsponse to North (2014),” Psychology of Music , vol. 42,\nno. 6, pp. 791–799, 2014.[37] M. Gavrielidou and H. Odell-Miller, “An investigation\nof pivotal moments in music therapy in adult mental\nhealth,” The Arts in Psychotherapy , vol. 52, pp. 50–62,\nFeb. 2017.\n[38] G. Ansdell, J. Davidson, W. L. Magee, J. Meehan, and\nS. Procter, “From \"This F***ing life\" to \"that’s better\"\n. . . in four minutes: an interdisciplinary study of mu-\nsic therapy’s \"present moments\" and their potential for\naffect modulation,” Nordic Journal of Music Therapy ,\nvol. 19, no. 1, pp. 3–28, 2010.\n[39] J. De Backer and T. Wigram, “Analysis of Notated\nMusic Examples Selected from Improvisations of Psy-\nchotic Patients,” in Microanalysis in Music Therapy:\nMethods, Techniques and Applications for Clinicians,\nResearchers, Educators and Students , London, UK,\nch. 9, pp. 120–133.\n[40] K. Foubert, B. Sebreghts, J. Sutton, and J. De Backer,\n“Musical encounters on the borderline. Patterns of mu-\ntuality in musical improvisations with Borderline Per-\nsonality Disorder,” Arts in Psychotherapy , vol. 67,\n2020, Art. no. 101599.\n[41] J. De Backer, “Music and psychosis: A research report\ndetailing the transition from sensorial play to musical\nform by psychotic patients.” Nordic Journal of Music\nTherapy , vol. 17, no. 2, pp. 89–104, 2008.\n[42] T. Veldhuis, “Repeated musical patterns in music ther-\napy analysis,” Small Project Report, Utrecht Univer-\nsity, 2022.\n[43] J. Erkkilä, O. Lartillot, G. Luck, K. Riikkilä, and\nP. Toiviainen, “Intelligent music systems in music ther-\napy,” Music Therapy Today , vol. 5, no. 5, pp. 1–21,\n2004.\n[44] G. Husain, W. F. Thompson, and E. G. Schellenberg,\n“Effects of musical tempo and mode on arousal, mood,\nand spatial abilities,” Music perception , vol. 20, no. 2,\npp. 151–171, 2002.\n[45] A. Kurkjian, K. Skinner, and H. Ahonen, “Using\nmusic-adapted technology to explore Bruscia’s clin-\nical techniques introduced in autism research: Pilot\nstudy,” Approaches: An Interdisciplinary Journal of\nMusic Therapy , vol. 13, no. 2, pp. 178–204, 2021.\n[46] G. Luck, P. Toiviainen, J. Erkkilä, O. Lartillot, K. Ri-\nikkilä, A. Mäkelä, K. Pyhäluoto, H. Raine, L. Vark-\nila, and J. Värri, “Modelling the relationships between\nemotional responses to, and musical content of, music\ntherapy improvisations,” Psychology of Music , vol. 36,\nno. 1, pp. 25–45, 2008.\n[47] O. Nieto, G. J. Mysore, C.-i. Wang, J. B. Smith,\nJ. Schlüter, T. Grill, and B. McFee, “Audio-based mu-\nsic structure analysis: Current trends, open challenges,\nand applications,” Transactions of the International So-\nciety for Music Information Retrieval , vol. 3, no. 1,\n2020.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n254[48] F. Bimbot, O. Le Blouch, G. Sargent, and E. Vin-\ncent, “Decomposition into autonomous and compara-\nble blocks: a structural description of music pieces,”\ninProc. of the 11th International Society for Music In-\nformation Retrieval Conference , Utrecht, The Nether-\nlands, 2010, pp. 189–194.\n[49] J. Paulus, M. Müller, and A. Klapuri, “Audiobased mu-\nsic structure analysis,” in Proc. of the 11th Interna-\ntional Society for Music Information Retrieval Confer-\nence, Utrecht, The Netherlands, 2010, pp. 625–636.\n[50] J. Pauwels, F. Kaiser, and G. Peeters, “Combining\nHarmony-Based and Novelty-Based Approaches for\nStructural Segmentation,” in Proc. of the 14th Interna-\ntional Society for Music Information Retrieval Confer-\nence, Curitiba, Brazil, 2013, pp. 601–606.\n[51] G. Shibata, R. Nishikimi, E. Nakamura, and K. Yoshii,\n“Statistical music structure analysis based on a\nhomogeneity-, repetitiveness-, and regularityaware hi-\nerarchical hidden semi-Markov model,” in Proc. of the\n20th International Society for Music Information Re-\ntrieval Conference , Delft, The Netherlands, 2019, pp.\n268–275.\n[52] E. Cambouropoulos, “Musical parallelism and melodic\nsegmentation:: A computational approach,” Music Per-\nception , vol. 23, no. 3, pp. 249–268, 2006.\n[53] T. Collins, A. Arzt, S. Flossmann, and G. Widmer,\n“SIARCT-CFP: Improving Precision and the Discov-\nery of Inexact Musical Patterns in Point-Set Repre-\nsentations,” in Proc. of the 14th International Society\nfor Music Information Retrieval Conference , 2013, pp.\n549–554.\n[54] D. Conklin, “Discovery of distinctive patterns in mu-\nsic,” Intelligent Data Analysis , vol. 14, no. 5, pp. 547–\n554, 2010.\n[55] D. Conklin and C. Anagnostopoulou, “Comparative\npattern analysis of Cretan folk songs,” Journal of New\nMusic Research , vol. 40, no. 2, pp. 119–125, 2011.\n[56] R. B. Dannenberg and N. Hu, “Pattern discovery tech-\nniques for music audio,” Journal of New Music Re-\nsearch , vol. 32, no. 2, pp. 153–163, 2003.\n[57] J. Forth, “Cognitively-motivated geometric methods of\npattern discovery and models of similarity in music,”\nPh.D. dissertation, Goldsmiths, Univ. of London, Lon-\ndon, UK, 2012.\n[58] B. Janssen, W. Haas, A. V olk, and P. v. Kranen-\nburg, “Finding repeated patterns in music: State of\nknowledge, challenges, perspectives,” in International\nSymposium on Computer Music Multidisciplinary Re-\nsearch . Springer, 2013, pp. 277–297.[59] O. Lartillot, “Automated motivic analysis: An exhaus-\ntive approach based on closed and cyclic pattern min-\ning in multidimensional parametric spaces,” in Compu-\ntational Music Analysis , D. Meredith, Ed. Springer,\n2016, ch. 11, pp. 273–302.\n[60] D. Meredith, K. Lemström, and G. A. Wiggins, “Algo-\nrithms for discovering repeated patterns in multidimen-\nsional representations of polyphonic music,” Journal\nof New Music Research , vol. 31, no. 4, pp. 321–345,\n2002.\n[61] O. Nieto and M. M. Farbood, “Identifying polyphonic\npatterns from audio recordings using music segmen-\ntation techniques,” in Proc. of the 15th International\nSociety for Music Information Retrieval Conference ,\n2014, pp. 411–416.\n[62] M. Pesek, A. Leonardis, and M. Marolt, “Sym-\nCHM—An Unsupervised Approach for Pattern Dis-\ncovery in Symbolic Music with a Compositional Hi-\nerarchical Model,” Applied Sciences , vol. 7, no. 11, p.\n1135, 2017.\n[63] I. Y . Ren, H. V . Koops, A. V olk, and W. Swierstra, “In\nsearch of the consensus among musical pattern discov-\nery algorithms,” in Proc. of the 18th International Soci-\nety for Music Information Retrieval Conference , 2017,\npp. 671–678.\n[64] G. Velarde, D. Meredith, and T. Weyde, “A wavelet-\nbased approach to pattern discovery in melodies,” in\nComputational Music Analysis . Springer, 2016,\nch. 12, pp. 303–333.\n[65] M. Krause, M. Müller, and C. Weiß, “Towards Leitmo-\ntif Activity Detection in Opera Recordings,” Transac-\ntions of the International Society for Music Information\nRetrieval , vol. 4, no. 1, p. 127–140, 2021.\n[66] K. R. Page, T. Nurmikko-Fuller, C. Rindﬂeisch, Weigl,\nR. D. M., Lewis, L. Dreyfus, and D. De Roure, “A\ntoolkit for live annotation of opera performance: Ex-\nperiences capturing Wagner’s Ring cycle,” in Proc. of\nthe 16th International Society for Music Information\nRetrieval Conference , 2015, pp. 211–217.\n[67] L. Dreyfus and C. Rindﬂeisch, “Using digital libraries\nin the research of the reception and interpretation of\nRichard Wagner’s leitmotifs,” in Proceedings of the In-\nternational Workshop on Digital Libraries for Musicol-\nogy, 2014, pp. 1–3.\n[68] M. Rohrmeier and M. Neuwirth, “Towards a Syntax of\nthe Classical Cadence,” in What Is a Cadence? Theo-\nretical and Analytical Perspectives on Cadences in the\nClassical Repertoire. Leuven University Press, 2015,\npp. 287–338.\n[69] P. van Kranenburg and F. Karsdorp, “Cadence detec-\ntion in western traditional stanzaic songs using melodic\nand textual features,” in Proc. of the 15th InternationalProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n255Society for Music Information Retrieval Conference ,\n2014, pp. 391–396.\n[70] L. Bigo, L. Feisthauer, M. Giraud, and F. Levé, “Rel-\nevance of musical features for cadence detection,” in\nProc. of the 19th International Society for Music Infor-\nmation Retrieval Conference , 2018, pp. 671–678.\n[71] A. V olk and P. van Kranenburg, “Melodic similarity\namong folk songs: An annotation study on similarity-\nbased categorization in music,” Musicae Scientiae ,\nvol. 16, no. 3, pp. 317–339, 2012.\n[72] P. Boot, A. V olk, and W. B. de Haas, “Evaluating the\nrole of repeated patterns in folk song classiﬁcation and\ncompression,” Journal of New Music Research , vol. 45,\nno. 3, pp. 223–238, 2016.\n[73] R. Caro Repetto, R. Gong, N. Kroher, and X. Serra,\n“Comparision of the singing style of two Jingju\nschools,” in Proc. of the 16th International Society\nfor Music Information Retrieval Conference , Malaga,\nSpain, 2015, pp. 507–513.\n[74] V . I. S. Gulati, J. Serrà and X. Serra., “Mining Melodic\nPatterns in Large Audio Collections of Indian Art Mu-\nsic,” in Proc. of the Tenth International Conference on\nSignal-Image Technology and Internet-Based Systems ,\n2014, pp. 264–271.\n[75] D. Herremans and E. Chew, “MorpheuS: generat-\ning structured music with constrained patterns and\ntension,” IEEE Transactions on Affective Computing ,\nvol. 10, no. 4, pp. 510–523, 2019.\n[76] D. Conklin and G. Maessen, “Generation of melodies\nfor the lost chant of the mozarabic rite,” Applied\nSciences , vol. 9, no. 20, 2019. [Online]. Available:\nhttps://www.mdpi.com/2076-3417/9/20/4285\n[77] F. Pachet, “The Continuator: Musical Interaction With\nStyle,” Journal of New Music Research , vol. 32, no. 3,\npp. 333–341, 2003.\n[78] B. L. T. Sturm, A. L. Uitdenbogerd, H. V . Koops, and\nA. Huang, “Editorial for TISMIR special collection:\nAI and musical creativity,” Transactions of the Interna-\ntional Society for Music Information Retrieval , vol. 5,\nno. 1, pp. 67–70, 2022.\n[79] D. Tomaševi ´c, S. Wells, I. Y . Ren, A. V olk, and M. Pe-\nsek, “Exploring annotations for musical pattern discov-\nery gathered with digital annotation tools,” Journal of\nMathematics and Music , vol. 15, no. 2, pp. 194–207,\n2021.\n[80] A. Flexer and T. Grill, “The Problem of Limited Inter-\nrater Agreement in Modelling Music Similarity,” Jour-\nnal of New Music Research , vol. 45, no. 3, pp. 239–\n251, 2016.[81] A. Flexer, T. Lallai, and K. Rašl, “On Evaluation of\nInter- and Intra-Rater Agreement in Music Recommen-\ndation,” Transactions of the International Society for\nMusic Information Retrieval , vol. 4, no. 1, pp. 182–\n194, 2021.\n[82] H. V . Koops, W. B. de Haas, J. A. Burgoyne,\nJ. Bransen, A. Kent-Muller, and A. V olk, “Annotator\nsubjectivity in harmony annotations of popular music,”\nJournal of New Music Research , vol. 48, no. 3, pp.\n232–252, 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n256"
    },
    {
        "title": "Transfer Learning and Bias Correction With Pre-Trained Audio Embeddings.",
        "author": [
            "Changhong Wang",
            "Gaël Richard",
            "Brian McFee"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.8126908",
        "url": "https://doi.org/10.5281/zenodo.8126908",
        "abstract": "Pre-trained audio embeddings (VGGish, OpenL3, YAMNet) of IRMAS and OpenMIC-2018 datasets, releasedin the following paper:\n\nChanghong Wang, Brian McFee, and Gal Richard. Transfer Learning and Bias Correction with Pre-trained Audio Embeddings.Proceedings of theInternational Society for Music Information Retrieval (ISMIR) Conference, 2023.",
        "zenodo_id": 8126908,
        "dblp_key": "conf/ismir/WangRM23",
        "ee": "https://zenodo.org/records/8126908/files/embeddings.h5"
    },
    {
        "title": "Text-to-Lyrics Generation With Image-Based Semantics and Reduced Risk of Plagiarism.",
        "author": [
            "Kento Watanabe",
            "Masataka Goto"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265309",
        "url": "https://doi.org/10.5281/zenodo.10265309",
        "ee": "https://zenodo.org/records/10265309/files/000047.pdf",
        "abstract": "This paper proposes a text-to-lyrics generation method, aiming to provide lyric writing support by suggesting the generated lyrics to users who struggle to find the right words to convey their message. Previous studies on lyrics generation have focused on generating lyrics based on semantic constraints such as specific keywords, lyric style, and topics. However, these methods had limitations because users could not freely input their intentions as text. Even if such intentions can be given as input text, the lyrics generated from the input tend to contain similar wording, making it difficult to inspire the user. Our method is therefore developed to generate lyrics that (1) convey a message similar to the input text and (2) contain wording different from the input text. A straightforward approach of training a text-to-lyrics encoder-decoder is not feasible since there is no text-lyric paired data for this purpose. To overcome this issue, we divide the text-to-lyrics generation process into a two-step pipeline, eliminating the need for text-lyric paired data. (a) First, we use an existing text-to-image generation technique as a text analyzer to obtain an image that captures the meaning of the input text, ignoring the wording. (b) Next, we use our proposed image-to-lyrics encoder-decoder (I2L) to generate lyrics from the obtained image while preserving its meaning. The training of this I2L model only requires pairs of \"lyrics\" and \"images generated from lyrics\", which are readily prepared. In addition, we propose for the first time a lyrics generation method that reduces the risk of plagiarism by prohibiting the generation of uncommon phrases in the training data. Experimental results show that the proposed method can generate lyrics with different phrasing while conveying a message similar to the input text.",
        "zenodo_id": 10265309,
        "dblp_key": "conf/ismir/WatanabeG23",
        "keywords": [
            "text-to-lyrics generation method",
            "lyric writing support",
            "suggesting generated lyrics",
            "semantic constraints",
            "freely inputting intentions",
            "similar wording",
            "difficult to inspire",
            "two-step pipeline",
            "text-to-image generation",
            "image-to-lyrics encoder-decoder"
        ],
        "content": "TEXT-TO-LYRICS GENERATION WITH IMAGE-BASED SEMANTICS\nAND REDUCED RISK OF PLAGIARISM\nKento Watanabe Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST), Japan\n{kento.watanabe, m.goto}@aist.go.jp\nABSTRACT\nThis paper proposes a text-to-lyrics generation method, aim-\ning to provide lyric writing support by suggesting the gen-\nerated lyrics to users who struggle to ﬁnd the right words\nto convey their message. Previous studies on lyrics genera-\ntion have focused on generating lyrics based on semantic\nconstraints such as speciﬁc keywords, lyric style, and top-\nics. However, these methods had limitations because users\ncould not freely input their intentions as text. Even if such\nintentions can be given as input text, the lyrics generated\nfrom the input tend to contain similar wording, making it\ndifﬁcult to inspire the user. Our method is therefore devel-\noped to generate lyrics that (1) convey a message similar\nto the input text and (2) contain wording different from\nthe input text. A straightforward approach of training a\ntext-to-lyrics encoder-decoder is not feasible since there\nis no text-lyric paired data for this purpose. To overcome\nthis issue, we divide the text-to-lyrics generation process\ninto a two-step pipeline, eliminating the need for text-lyric\npaired data. (a) First, we use an existing text-to-image\ngeneration technique as a text analyzer to obtain an image\nthat captures the meaning of the input text, ignoring the\nwording. (b) Next, we use our proposed image-to-lyrics\nencoder-decoder (I2L) to generate lyrics from the obtained\nimage while preserving its meaning. The training of this\nI2L model only requires pairs of “lyrics” and “images gen-\nerated from lyrics”, which are readily prepared. In addition,\nwe propose for the ﬁrst time a lyrics generation method that\nreduces the risk of plagiarism by prohibiting the generation\nof uncommon phrases in the training data. Experimental\nresults show that the proposed method can generate lyrics\nwith different phrasing while conveying a message similar\nto the input text.\n1. INTRODUCTION\nAutomatic lyrics generation methods have been proposed\nas an important research topic in lyrics information pro-\ncessing [1]. With the aim of supporting users who already\nknow what they want to convey in their lyrics but struggle to\nﬁnd the appropriate words, the methods are used in writing\n© K. Watanabe and M. Goto. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribution:\nK. Watanabe and M. Goto, “Text-to-lyrics generation with image-based\nsemantics and reduced risk of plagiarism”, in Proc. of the 24th Int. Society\nfor Music Information Retrieval Conf., Milan, Italy, 2023.Input Text\nImage\n(Intermediate Representation)Generated Lyrics\nDriving a car on the seasideI 'm driving in my car\nJust like a fish on an ocean floor.\n(a) Existing image generation \nsuch as Stable Diffusion(b) Image-to-Lyrics\nEncode-Decoder\nFigure 1 . Overview of the proposed text-to-lyrics genera-\ntion method.\nsupport systems providing them with generated lyrics as a\nsource of new inspiration [2 –8]. Most previous studies have\nfocused on lyrics generation that is conditioned by semantic\nconstraints, including speciﬁc keywords, lyric style, and\ntopics. For example, Watanabe et al.’s system generates\nlyrics based on pre-deﬁned topics selected by the user, but\nthe limited range of topics results in similar styles of gen-\nerated lyrics [2]. Oliveira et al.’s system generates poems\nbased on keywords entered by the user, but it cannot gener-\nate poems based on sentences or paragraphs representing\nthe user’s intention [3, 4].\nTo provide more ﬂexible lyric writing support, we pro-\npose generating lyrics based on freely formatted text entered\nby the user. We believe this approach surpasses the use of\nsemantic constraints such as topics and keywords in terms\nof ﬂexibility. While existing paraphrase systems [9] can be\nconsidered useful for this approach, the paraphrased lyrics\nmay not provide sufﬁcient inspiration because they tend to\nbe similar in wording to the input text. For example, even if\na similar phrase “ Driving a car along the coastline ” is gen-\nerated from the input text “ Driving a car on the seaside” ,\nthe user is unlikely to get new inspiration.\nTherefore, the aim of this study is to develop a method\nfor generating lyrics that not only have meanings similar\nto the input text but also use wording different from the\ninput text. For example, if a user freely enters text that\nrepresents the content of the lyrics, such as “ Driving a car\non the seaside ”, our method generates lyrics with different\nwording, such as “ I’m driving in my car. Just like a ﬁsh\non an ocean ﬂoor. ”. As a simple way to achieve this aim,\nTransformer-based encoder-decoders [10] could be used for\ngenerating lyrics from text, but they require large text-lyric\npaired data for training, which is currently unavailable. To\naddress this issue, we could use text summarization and\nmachine translation to generate text from lyrics and obtain\npaired data automatically. However, since the generated text398and lyric pairs have similar wording, an encoder-decoder\ntrained using those paired data may generate lyrics with\nwording similar to the input text.\nTo achieve text-to-lyrics generation without using any\npaired text data for training, we propose a two-step pipeline\nframework: (a) using an existing text analyzer to obtain\nonly the semantic representation from the input text, and\n(b) generating lyrics from the obtained representation. The\ncore idea of this framework is to leverage a text-to-image\ngeneration technique such as Stable Diffusion [11] as the\ntext analyzer. An image generated from the input text can\nserve as a reasonable intermediate representation that cap-\ntures the meaning of the text while ignoring the details\nof its wording (Figure 1 (a)). Using the generated image,\nour image-to-lyrics encoder-decoder generates semantically\nrelated lyrics (Figure 1 (b)). It needs many image-lyric\npairs as training data, but we can readily prepare those pairs\nby generating images from lyrics of many songs. This is\nan advantage of using text-to-image generation. Another\nadvantage is that it can generate images without regard to\nthe input text’s format, i.e., whether it is a word, phrase,\nsentence, or paragraph. We can thus provide ﬂexible lyric\nwriting support that is not constrained by the format of the\ninput text.\nMachine learning-based generation methods may inad-\nvertently output portions of the training data directly with-\nout modiﬁcation. This output can be considered plagiarism\nin some cases [12, 13]. Therefore, this paper also proposes\nan anti-plagiarism method to reduce this risk. We assume\nthat generating common phrases (word sequences having\nhigh commonness [14]) used in many songs is not plagia-\nrism, and reduce the risk of plagiarism by prohibiting the\ngeneration of uncommon phrases used in only a few songs.\nTo the best of our knowledge, this is the ﬁrst study to include\nsuch an anti-plagiarism method in lyrics generation.\nExperimental results show that our text-to-lyrics gen-\neration method can generate lyrics with meaning similar\nto the input text but expressed differently. Another exper-\niment shows that lyrics generated without using our anti-\nplagiarism method would result in plagiarizing uncommon\nphrases in the training data, but those undesirable phrases\ncan successfully be removed by our method.\n2. RELATED WORK\nWhile natural language generation methods such as machine\ntranslations and chat systems have been actively studied\nand their performance greatly improved by deep neural\nnetworks (DNNs), automatic lyrics generation has also at-\ntracted attention as a research topic [1]. Most studies of\nlyrics generation have focused on lyric-speciﬁc musical\nconstraints such as melody [15 –20], rhyme [6, 8, 21 –25],\nand audio signal [26 –28]. While these lyric-speciﬁc musi-\ncal constraints are an important aspect of lyrics generation,\nthe main focus of this study is on the controllability of the\nsemantic content of the generated lyrics.\nOther studies have focused on lyrics generation that is\nconditioned by semantic constraints such as input keywords,\nstyles, and topics [2 –5, 29 –32]. However, although theseconstraints allow some control over the semantic content of\nthe generated lyrics, there may be differences between the\nuser’s intentions and the semantic content of the generated\nlyrics. To improve the usability of the lyrics generation\nmethod as a creative tool, we believe that users should be\nable to enter freely formatted text (words, phrases, sen-\ntences, paragraphs, etc.). Our proposed method therefore\nallows any text format, giving users greater control over the\nsemantic content of the generated lyrics.\nSome studies have proposed methods for generating\nlyrics that are semantically related to the input text [6, 7].\nRam et al. proposed a ﬁne-tuned T5 model [9] that gen-\nerates single-line lyrics that follow several lines of input\nlyrics [6]. This method allows the user not only to en-\nter sentences but also to control the rhyme and syllable\ncount of the generated lyrics by adding special tokens at\nthe end of the input sentence. In contrast to that method,\nin which the generated lyrics are a continuation of the in-\nput lyrics, ours generates lyrics that capture the semantic\ncontent of the input text. Zhang et al.’s research motivation\nis similar to ours, as they have also proposed a method for\ngenerating lyrics that capture the semantic content of the\ninput text (which they refer to as passage-level text) [7].\nTo overcome the problem of the lack of text-lyric paired\ndata for training the text-to-lyrics encoder-decoder, they\ncollected lyrics data and passage-level text data (such as\nshort novels and essays) separately and utilized an unsu-\npervised machine translation framework. Speciﬁcally, they\nprepared two encoder-decoders, one for lyric text and one\nfor passage-level text. They then aligned the latent rep-\nresentation space of these two encoder-decoders to build\na text-to-lyrics encoder-decoder. In this paper, we pro-\npose a novel approach to develop a text-to-lyrics generation\nmethod that requires only lyrics data. While Zhang et al.’s\nmethod requires the collection of both lyrics and input texts,\nours does not require additional text data, thus simplifying\nthe development of the lyrics generation method.\n3. TEXT-TO-LYRICS GENERATION WITH\nIMAGE-BASED SEMANTICS\nAs described in Section 1, the proposed text-to-lyrics gen-\neration method ﬁrst generates an image from the input text\nby leveraging an existing text-to-image generation method.\nIt then generates lyrics from the generated image by using\nour own image-to-lyrics encoder-decoder that we call I2L.\nSince the image serves as an intermediate representation to\nextract the meaning of the input text, the generated lyrics\ncan have similar meaning but different wording.\nThe network structure of the I2L is illustrated in Figure 2.\nBy assuming that one paragraph of lyrics can be represented\nin a single image, we set the unit of the generated lyrics to\na paragraph.\nWe ﬁrst uses the animation-style image generation\nmethod Anything V3.0.1to obtain an image having a uni-\nform style. The reasons for using Anything V3.0 here are\n1A ﬁne-tuned Stable Diffusion model. https://huggingface.\nco/Linaqruf/anything-v3.0Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n399Image\n(512º512 pixels)Linear Projection of Flattened Patches1234 46 47 48 49 ※0Patch + Positional\nEmbedding\nEmbedding Layer\nNZʪ\u00101ʫ\nʪ1ʫFully Connected LayerSoftmax Activation\n2º\n… … DBS ʪ-ʫ +VTU MJLF B JONZ DBS ʪ-ʫ +VTU MJLF BGJTI\n\u000f… …0O\nImage Features\n(50º768 dimension)\n… …\nLyrics (Context)Lyrics\n0 456789 10 16Word + Positional\nEmbedding\n(Tº768 dimension)\n ……\n…\nSequence of image patches\nResize the image to 224 º224 pixels and divide it into \npatches of 32 º32 pixels each, for a total of 49 patches.\nPre-trained Vision Transformer\n(Freezed Parameters )Transformer Decoder\n(Trainable Parameters )\n(※Extra embedding)\nFigure 2 . Image-to-lyrics encoder-decoder (I2L) for generating lyrics from an image that is generated from the input text.\n(1) it can generate images that represent the input text with-\nout prompt engineering, and (2) the use of images with\na uniform style facilitates I2L training. The image has a\nresolution of 512×512and corresponds to a paragraph of\nthe English lyrics.\nAs shown in Figure 2, we then segment the generated im-\nage into 49 patches and compute the features of the image\npatches by using a pre-trained Vision Transformer2[33] to\nobtain 50 features (each with 768 dimensions) per image.\nThese 50 image features are fed into the multi-head atten-\ntion layer of the Transformer decoder [10]. We feed each\nword in a paragraph into the word embedding and positional\nembedding layers to compute the word vectors, and feed\neach word vector into the masked multi-head attention layer\nof the Transformer decoder. The output of the Transformer\ndecoder is fed into the fully connected layer FCto obtain\na vector of vocabulary size dimensions. Finally, we apply\nthesoftmax activation function to this vector to calculate\nthe word probability distribution.\n3.1 Parameters\nWe use 768 as the number of embedding dimensions, 6\nas the number of multi-heads, 2 as the number of decoder\nlayers, 1024 as the number of feedforward layer dimensions,\nand GELU as the activation function. For optimization we\nuse AdamW [34] with a mini-batch size of 8, a learning\nrate of 0.001, and a warm-up step of one epoch. Training\nwas run for 40 epochs, and the I2L used for testing was the\none that achieved the best loss on the development set.\nWe dare to train our Transformer decoder from scratch\nusing only the lyrics data we have, without reusing available\npre-trained large-scale language models (LLMs) such as\nBERT [35] or GPT-2 [36]. This is because when the training\ndata of LLMs contain copyrighted literary works such as\nnovels, poems, or essays, reusing pre-trained LLMs can\nresult in plagiarizing those works. Since we would like to\nreduce the risk of plagiarism as described in Section 3.4,\nwe cannot leverage pre-trained LLMs.\n2https://huggingface.co/google/vit-base-patch\n32-224-in21k3.2 Training data\nWe sample 129,747 English songs from the Music Lyrics\nDatabase V .1.2.73so that each song contains at least three\nparagraphs. The resulting dataset contains 927,535 para-\ngraphs. This means that we can obtain 927,535 images\nby using Anything V3.0. We then split these songs into\ntraining (90%) and development (10%) sets. We use the top\n52,832 words with the highest document-frequency as the\nvocabulary for training, and convert the other words to a spe-\ncial symbol ⟨unknown⟩. This vocabulary includes ⟨L⟩tags\nfor line breaks, ⟨P⟩tags for the beginning of paragraphs,\nand⟨/P⟩tags for the end of paragraphs.\nWe applied the same procedure not only to the lyrics of\nEnglish songs but also to the lyrics of 142,772 Japanese\nsongs. This Japanese dataset contains 1,078,500 paragraphs,\nand the vocabulary size is 50,989 words. To extract word\nboundaries for Japanese lyrics, we apply the CaboCha\nparser [37]. Japanese lyrics are pre-translated into English\nby a Japanese-English translator4for use with Anything\nV3.0. We use these English and Japanese lyrics datasets to\ntrain two I2Ls (one for each language).\n3.3 Decoding algorithm\nWe expect that generating and suggesting different varia-\ntions of lyrics can give users new ideas for writing lyrics.\nTo generate such different variations, we use a sampling\nmethod rather than a beam search method. In the sampling\nmethod, we sample each word according to the probability\ndistribution calculated by the Transformer decoder. Sam-\npling words according to a probability distribution allows\na wide variety of words to be included in the generated\nlyrics, although some words that make the generated lyrics\nmeaningless may be included. To avoid generating such\nmeaningless lyrics, we use a Top- psampling method that\nprohibits sampling words with low generation probabili-\nties [38]. We can generate several lyrics simultaneously by\n3https://www.odditysoftware.com/page-datasales\n1.htm\n4https://huggingface.co/staka/fugumt-en-jaProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n400running Top- psampling in parallel. The probability distri-\nbution for word sampling in Top- psampling is calculated\nusing the formula softmax( z/τ): where zis the output\nof the fully connected layer FCandτis the temperature\nparameter. If τis less than 1, common words with high\nprobability values are more likely to be sampled. In model\ntraining we set τto1, while in lyrics generation the user\ncan setτfreely.\n3.4 Anti-plagiarism method for lyrics generation\nOne of concerns with lyrics generation based on machine\nlearning is the risk of plagiarism since the generated lyrics\nmay contain phrases that are identical to existing lyrics\nphrases in training data, potentially leading to copyright\ninfringement issues. To address this issue, we propose a\nmethod to reduce the risk of plagiarism in machine learning-\nbased lyrics generation. This method not only allows the\ngeneration of new phrases that are not present in the training\ndata, but also permits the use of commonly used phrases\nsuch as “ I love you ” in the generated lyrics. In contrast, it\nprohibits the use of uncommon phrases that we consider to\nbe a form of plagiarism. To achieve this, we create a list of\nuncommon phrases, UncommonPhrase , and prohibit the\ngeneration of phrases that are included in this list.\nFirst, we deﬁne the uncommon phrases included in\nUncommonPhrase , as well as the new phrases and com-\nmon phrases that are allowed to be generated. A phrase is\ndeﬁned by a word n-gram, denoted by {w1,...,wn}, where\nwis a word. We categorize a phrase as “new”, “common”,\nor “uncommon” according to SN({w1,...,wn})deﬁned\nas the number of songs in which the n-gram occurs in the\ntraining data:\n•IfSN({w1,...,wn}) = 0 , thisn-gram is a new\nphrase (i.e., it does not appear in the training data).\n•If3< SN({w1,...,wn}), thisn-gram is a common\nphrase (i.e., it appears frequently in the training data).\n•If1≤SN({w1,...,wn})≤3, thisn-gram is an\nuncommon phrase (i.e., it appears infrequently in the\ntraining data).5\nNote that there is a possibility of mistaking uncommon\nphrases for common phrases when duplicate lyrics are con-\ntained in the training data, which results in larger SNvalues\nthan they should be. It could happen when different artists\nsing the same lyrics, the same lyrics is repeatedly regis-\ntered, and so on. We therefore identify duplicate lyrics\naccording to the following two criteria: (1) we assume that\npairs of lyrics with the same 20-grams are duplicates, and\n(2) we assume that pairs of lyrics with a normalized edit\ndistance [39] of less than 0.5 are duplicates. To calculate\nSNaccurately, we then concatenate the identiﬁed duplicate\nlyrics and replace those lyrics with the single concatenated\nlyrics. When lyrics that do not duplicate are mistaken for\n5In this study, we tentatively set the threshold for SN at 3. Since\nthere is no established legal rule, we believe that this threshold will be\ndetermined by social consensus in the future. Providing the technical basis\nfor such discussions is also a contribution of this study.duplicate lyrics, a common phrase can be mistaken for an\nuncommon phrase, but it is better than vice versa from\nthe anti-plagiarism viewpoint. This reduced the number of\nEnglish songs in our lyrics data from 129,747 to 108,497.6\nBased on this SNcriteria, we collect uncommon phrases\nfrom our training data. However, it is important to note that\neven if a word 3-gram is a common phrase, it may become\nan uncommon phrase when it becomes a word 4-gram. For\ninstance, “ I love you ” is a common 3-gram with a large\nSN, while “ I love you darling ” is an uncommon 4-gram\nwith a small SN. Therefore we do not use a single value\nofnbut instead consider all values of nwithin a range\nfrom 1 to sufﬁciently large values. However, it is difﬁcult to\nstore all uncommon phrases in memory because the number\nofn-grams that have to be listed increases with n. To\novercome the memory limitation problem, we propose to\nuse the following procedure to minimize the number of\nuncommon phrases we need to store in memory: (1) we start\nby examining 1-grams, then move on to 2-grams, 3-grams,\nand so on until we have looked at all possible n-grams in\nthe training data. (2) For each target n-gram, we generate\nall possible sub- n-grams of length 1, 2, ..., n−1. If any of\nthese sub- n-grams are already in UncommonPhrase , we\ncan skip adding the target n-gram toUncommonPhrase\nbecause we know it is uncommon. Otherwise, we add\nthe target n-gram toUncommonPhrase . Following this\nprocedure, we collected approximately 22.3M uncommon\nn-grams with nranging from 1 to 21 for English lyrics.7\nAfter creating UncommonPhrase using the above\nprocedure, we prohibit their generation during Top- p\nsampling by the following two steps: (1) During word\ngeneration, we check whether any sub- n-grams derived\nfrom the word sequence {w1,...,wt}are included in\nUncommonPhrase . (2) If any of these sub- n-grams\nare found in UncommonPhrase , we prohibit the gen-\neration of word wtby setting its generation probability\nP(wt|{w1,...,wt−1})to zero.\n4. QUANTITATIVE EV ALUATION\nThe proposed text-to-lyrics generation method was quanti-\ntatively evaluated using two metrics:\nTest-set perplexity (PPL) : This is a standard evaluation\nmeasure for encoder-decoders. The PPL metric measures\nthe degree of predictability of the phrasing in the original\ntext in the test set [40]. A smaller PPL value is better since\nit indicates that the encoder-decoder has a higher ability to\ngenerate lyrics that capture the meaning of the input text.\nNormalized edit distance (NED) : The normalized edit\ndistance [39] between the generated lyrics and the input\ntext is calculated to evaluate whether the proposed method\ngenerates lyrics that differ in wording from the input text.\nA larger NED is better since it indicates that the generated\nlyrics have wording more different from the input text.\n6For Japanese lyrics, the number of songs was reduced from 142,772\nto 119,595.\n7For Japanese lyrics, we collected approximately 18.2M uncommon\nn-grams with nranging from 1 to 19.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n401English Japanese\nMethod PPL NED PPL NED\nI2L (proposed) 84.86 0.78 231.49 0.92\nS2L 346.73 0.69 306.19 0.86\nB2L 544.21 0.71 1051.58 0.66\nH2H 163.98 0.68 583.13 0.90\nTable 1 . Results of quantitative evaluation.\n4.1 Experimental dataset\nTo evaluate the proposed lyrics generation method, we con-\nstructed a small test dataset consisting of pairs of lyrics and\ninput text representing the semantic content of the lyrics.\nSince such a dataset is not available, for English songs, we\nprepared a test dataset that included plot texts from 20 Dis-\nney animated ﬁlms, taken from Wikipedia, along with their\ncorresponding theme song lyrics. We here assume that the\nlyrics of each theme song are written based on the content\nof that ﬁlm. For Japanese songs, we prepared 51 Japanese\nanimation plot texts and their theme song lyrics.\n4.2 Methods Compared\nTo compare the proposed method with possible differ-\nent methods, we prepared the following encoder-decoders\ntrained on paired data created in different suitable ways.\nImage-to-Lyrics encoder-decoder (I2L) This is the pro-\nposed encoder-decoder trained on image-lyric paired data.\nSummary-to-Lyrics encoder-decoder (S2L) We con-\nverted each lyric paragraph in the training data into a\nsummary using a text summarization method8to create\nsummary-lyric paired data. The data is then used to train a\nTransformer-based summary-to-lyric encoder-decoder.\nBack-translated-lyrics-to-Lyrics encoder-decoder (B2L)\nWe translated each lyric paragraph in the training data from\nEnglish to Japanese to English by using English-Japanese\nand Japanese-English translation methods9to create paired\ndata of the back-translated lyrics and the original lyrics.\nThe data is then used to train a Transformer-based back-\ntranslated-lyrics-to-lyrics encoder-decoder.\nHalf-to-Half encoder-decoder (H2H) Inspired by an exist-\ning text-to-lyrics encoder-decoder training method [6], we\nﬁrst split each lyrics paragraph in the training data into ﬁrst\nand second halves. We then used this split lyrics data to\ntrain a Transformer-based encoder-decoder that generates\nthe second half lyrics from the ﬁrst half lyrics.\nSince the above S2L, B2L, and H2H are also\nTransformer-based encoder-decoders, their parameter set-\ntings are the same as for the proposed I2L. Given one input\ntext, ﬁve lyrics were generated by each method. The pa-\nrameterpfor Top-psampling was set to 0.9andτwas set\nto0.4. The generation process stops when the symbol ⟨/P⟩\n8https://huggingface.co/google/pegasus-xsum for\nthe English summarization. https://huggingface.co/tsmatz/\nmt5\\_summarize\\_japanese for the Japanese summarization.\n9https://huggingface.co/staka/fugumt-en-ja for\nthe English to Japanese translation. https://huggingface.co/s\ntaka/fugumt-ja-en for the Japanese to English translation.0%5%10%15%20%\n1 2 3 4 5 6 7 8 9 10 11 12English Japanese\nnPercentage of generated n-grams \nincluded in UncommonPhrase\nFigure 3 . The percentage of generated lyric n-grams that\nare included in UncommonPhrase , a list of phrases that\nshould not be generated (plagiarized). For example, 18.4%\nat English 4-grams means that among all 4-gram phrases in\nthe generated lyrics, 18.4% are uncommon phrases, though\n81.6% are new or common phrases.\n(end of paragraph) is generated. For this comparison, we\ndid not use the proposed anti-plagiarism method.\n4.3 Experimental results\nTable 1 indicates that the proposed I2L method had the best\nPPL in both the English and Japanese experiments and that\nthe NED between the lyrics generated by this method and\nthe input text was the largest ( pt<0.05based on the paired\nt-test). As expected, the NEDs were smaller for the S2L\nand B2L methods, which were trained on paired data where\nthe wording of the input text and lyric pairs was similar. In\ncontrast, although the H2H method can generate lyrics with\nwording different from the input text, it cannot generate\nlyrics that are semantically related to the input text like the\nproposed method can. These ﬁndings conﬁrm that image-\nlyric pairs are more effective than other paired data sets as\ntraining data for encoder-decoders generating lyrics that are\nsemantically related to the input text but differ from it in\nwording.\n5. EFFECTIVENESS OF THE PROPOSED\nANTI-PLAGIARISM METHOD\nWe examined whether the absence of the anti-plagiarism\nmethod proposed in Section 3.4 results in plagiarizing un-\ncommon phrases found in existing lyrics. In the lyrics\ngenerated by the I2L method in Section 4, we calculated the\npercentage of n-grams included in UncommonPhrase .\nThe results with nranging from 1 to 12 are shown in\nFigure 3. The percentage of uncommon 1-grams and 2-\ngrams in the generated lyrics is almost 0%. This indicates\nthat almost all of the generated 1-grams and 2-grams are\ncommon phrases used in many existing lyrics, even with-\nout the use of the anti-plagiarism method. On the other\nhand, the percentage of uncommon 3-grams to 8-grams\nranged between 3% and 18%. This suggests that many\nphrases in the generated lyrics may plagiarize if the pro-\nposed anti-plagiarism method is not applied. Furthermore,\nasnincreases beyond 9, the n-gram combinations becomeProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n402Input text Image (intermediate representation) Generated lyrics\nA group of explorers are\nwalking through the grass neutral.⇒\n ⇒Out in the country, out of sight\nWe’ve got to get this together right now\nI’m going out with you today\nAnd some day we’ll make a lot better way\nWe meet again I guess our love is forever. ⇒\n ⇒This is the last time\nWe’ve been together for long years\nI’m here with you\nTo be forever yours\nTable 2 . Examples of lyrics generated by our text-to-lyrics generation method with the anti-plagiarism method.\nso vast that the generated n-grams are rarely included in\nUncommonPhrase . These results conﬁrm that our ma-\nchine learning-based lyrics generation method tends to sam-\nple common words, but the generated 3- to 8-gram phrases,\neven though they are composed of common words, may be\nuncommon enough to raise suspicion of plagiarism. Using\nthe proposed anti-plagiarism method, in contrast, ensures\nthat uncommon phrases contained in UncommonPhrase\nare never generated, thereby reducing the risk of plagiarism.\nWhile the proposed anti-plagiarism method is effective,\nit is important to note that it is not intended to be a fool-\nproof solution that ensures legal compliance. Rather, it is\ndesigned to provide a helpful guideline for those who wish\nto generate original lyrics while reducing the risk of plagia-\nrism. We hope that our approach can contribute to further\ndiscussions on a reasonable balance between encouraging\ncreativity and respecting intellectual property rights.\n6. QUALITATIVE EV ALUATION\nTable 2 shows two examples of lyrics generated using the\nproposed method. Given the input text, our method can\ngenerate any number of lines of lyrics, but here four lines\nare generated by stopping the generation process when four\n⟨L⟩(line break) symbols and the ⟨/P⟩(end of paragraph)\nsymbol are generated. In the ﬁrst example, the input text is\ntaken from the SICK dataset [41], while in the second ex-\nample the input text is taken from lyrics in the RWC Music\nDatabase [42]. In both examples, our method can generate\nlyrics that reﬂect the content of the input text. In the ﬁrst\nexample, it generates an image that represents the scene de-\nscribed in the input text and generates corresponding lyrics\nthat reﬂect the image. In contrast, in the second example,\nour method generates an image of a person with emotional\nexpression corresponding to the input text and generates\nlyrics that express the emotion depicted in the image. Other\nexamples can be found in the supplementary material A.10\nIn addition to the quantitative evaluation and the gen-\nerated examples, we evaluated the similarity between the\ninput text and the generated lyrics through a human eval-\nuator. To prepare the input text in an objective way, we\ncollected the titles of the “Hot 100 Songs” in 2022 on the\nBillboard year-end charts11, extracted the ﬁrst verse from\n10https://github.com/KentoW/ISMIR2023\n11https://www.billboard.com/charts/year-end/202their lyrics, and summarized each verse into a short sen-\ntence using ChatGPT.12Since 9 songs contained explicit\ncontent in either the input text or the generated lyrics, they\nwere excluded for ethical reasons.13We then showed the\nevaluator the input text and the lyrics generated from it, and\nasked to classify whether the impressions of the two were\nsimilar or not. As a result, the impressions of the input text\nand the generated lyrics were judged to be similar for 52\nof the 91 songs, conﬁrming that the proposed method can\ngenerate lyrics that express the content of the input text to\nsome extent. In cases where the impressions were classi-\nﬁed as dissimilar, most of the input texts contain complex\nsituations or abstract content that is difﬁcult to generate\nas images. Thus, the limitation of this approach is that it\ncannot generate lyrics for input texts that are difﬁcult to\nrepresent as images. Nevertheless, our method is useful\nas a writing support tool for many situations where users\nhave intentions that can be represented as images, and is\nalso valuable because it pioneered a novel lyric generation\napproach. Detailed results of the generated lyrics and the\njudgments are included in the supplementary material B.14\n7. CONCLUSION\nThis paper has described a method for generating lyrics that\nare similar in meaning to the input text but expressed differ-\nently. The contributions of this study are as follows: (1) We\nproposed a novel two-step pipeline framework. First, we\napply text-to-image generation as a text analyzer to extract\nonly the semantic content from the input text. Next, we use\nour proposed image-to-lyrics encoder-decoder to generate\nlyrics that capture the semantics of the generated image.\n(2) We proposed a method to reduce the risk of plagiarism\nby prohibiting the generation of uncommon phrases in the\ntraining data and veriﬁed its effectiveness. (3) We quantita-\ntively showed that our proposed method outperforms other\nmethods in generating lyrics for our purpose.\nFuture work will develop the ﬂexible lyric writing sup-\nport system incorporating the proposed lyrics generation\nmethod.\n2/hot-100-songs/\n12https://chat.openai.com/chat\n13As future work, we plan to incorporate a ﬁltering function that uses\nexplicit lyrics detection [43–46].\n14https://github.com/KentoW/ISMIR2023Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4038. ACKNOWLEDGMENTS\nThis work was supported in part by JST CREST Grant\nNumber JPMJCR20D4 and JSPS KAKENHI Grant Num-\nber JP20K19878, Japan.\n9. REFERENCES\n[1]K. Watanabe and M. Goto, “Lyrics information process-\ning: Analysis, generation, and applications,” in Pro-\nceedings of the 1st Workshop on NLP for Music and\nAudio (NLP4MusA) , 2020, pp. 6–12.\n[2]K. Watanabe, Y . Matsubayashi, K. Inui, T. Nakano,\nS. Fukayama, and M. Goto, “LyriSys: An interactive\nsupport system for writing lyrics based on topic transi-\ntion,” in Proceedings of the 22nd International Confer-\nence on Intelligent User Interfaces (ACM IUI) , 2017,\npp. 559–563.\n[3]H. G. Oliveira, T. Mendes, and A. Boavida, “Co-\nPoeTryMe: A co-creative interface for the composition\nof poetry,” in Proceedings of the 10th International Con-\nference on Natural Language Generation (INLG) , 2017,\npp. 70–71.\n[4]H. G. Oliveira, T. Mendes, A. Boavida, A. Nakamura,\nand M. Ackerman, “Co-PoeTryMe: Interactive poetry\ngeneration,” Cognitive Systems Research , vol. 54, pp.\n199–216, 2019.\n[5]R. Zhang, X. Mao, L. Li, L. Jiang, L. Chen, Z. Hu,\nY . Xi, C. Fan, and M. Huang, “Youling: An AI-assisted\nlyrics creation system,” in Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations (EMNLP) , 2020,\npp. 85–91.\n[6]N. Ram, T. Gummadi, R. Bhethanabotla, R. J. Sav-\nery, and G. Weinberg, “Say what? collaborative pop\nlyric generation using multitask transfer learning,” in\nProceedings of the 9th International Conference on\nHuman-Agent Interaction (HAI) , 2021, pp. 165–173.\n[7]L. Zhang, R. Zhang, X. Mao, and Y . Chang, “QiuNiu:\nA Chinese lyrics generation system with passage-level\ninput,” in Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics - System\nDemonstrations (ACL) , 2022, pp. 76–82.\n[8]N. Liu, W. Han, G. Liu, D. Peng, R. Zhang, X. Wang,\nand H. Ruan, “ChipSong: A controllable lyric genera-\ntion system for Chinese popular song,” in Proceedings\nof the First Workshop on Intelligent and Interactive\nWriting Assistants (In2Writing) , 2022, pp. 85–95.\n[9]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer,” Journal of Machine Learning Research ,\nvol. 21, no. 140, pp. 1–67, 2020.[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” Advances in neural informa-\ntion processing systems , vol. 30, pp. 1–11, 2017.\n[11] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer, “High-resolution image synthesis with la-\ntent diffusion models,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion (CVPR) , 2022, pp. 10 674–10 685.\n[12] A. Papadopoulos, P. Roy, and F. Pachet, “Avoiding pla-\ngiarism in markov sequence generation,” in Proceedings\nof the 28th AAAI Conference on Artiﬁcial Intelligence ,\n2014, pp. 2731–2737.\n[13] Q. Feng, C. Guo, F. Benitez-Quiroz, and A. M.\nMartínez, “When do GANs replicate? on the choice\nof dataset size,” in 2021 IEEE/CVF International Con-\nference on Computer Vision (ICCV 2021) , 2021, pp.\n6681–6690.\n[14] T. Nakano, K. Yoshii, and M. Goto, “Musical similar-\nity and commonness estimation based on probabilistic\ngenerative models of musical elements,” International\nJournal of Semantic Computing (IJSC) , no. 1, pp. 27–52,\n2016.\n[15] K. Watanabe, Y . Matsubayashi, S. Fukayama, M. Goto,\nK. Inui, and T. Nakano, “A melody-conditioned lyrics\nlanguage model,” in Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies (NAACL-HLT) , 2018, pp. 163–172.\n[16] X. Lu, J. Wang, B. Zhuang, S. Wang, and J. Xiao,\n“A syllable-structured, contextually-based conditionally\ngeneration of Chinese lyrics,” in Proceedings of the\n16th Paciﬁc Rim International Conference on Artiﬁcial\nIntelligence (PRICAI) , 2019, pp. 257–265.\n[17] Y . Chen and A. Lerch, “Melody-conditioned lyrics gen-\neration with SeqGANs,” in Proceedings of the 2020\nIEEE International Symposium on Multimedia (ISM) ,\n2020, pp. 189–196.\n[18] Y . Huang and K. You, “Automated generation of Chi-\nnese lyrics based on melody emotions,” IEEE Access ,\nvol. 9, pp. 98 060–98 071, 2021.\n[19] X. Ma, Y . Wang, M. Kan, and W. S. Lee, “AI-Lyricist:\nGenerating music and vocabulary constrained lyrics,” in\nProceedings of the 29th ACM International Conference\non Multimedia (ACM-MM) , 2021, pp. 1002–1011.\n[20] Z. Sheng, K. Song, X. Tan, Y . Ren, W. Ye, S. Zhang,\nand T. Qin, “SongMASS: Automatic song writing with\npre-training and alignment constraint,” in Proceedings\nof the 35th AAAI Conference on Artiﬁcial Intelligence ,\n2021, pp. 13 798–13 805.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n404[21] G. Barbieri, F. Pachet, P. Roy, and M. D. Esposti,\n“Markov constraints for generating lyrics with style,”\ninProceedings of the 20th European Conference on\nArtiﬁcial Intelligence (ECAI) , vol. 242, 2012, pp. 115–\n120.\n[22] J. Hopkins and D. Kiela, “Automatically generating\nrhythmic verse with neural networks,” in Proceedings\nof the 55th Annual Meeting of the Association for Com-\nputational Linguistics (ACL) , 2017, pp. 168–178.\n[23] E. Manjavacas, M. Kestemont, and F. Karsdorp, “Gen-\neration of hip-hop lyrics with hierarchical modeling and\nconditional templates,” in Proceedings of the 12th Inter-\nnational Conference on Natural Language Generation\n(INLG) , 2019, pp. 301–310.\n[24] L. Xue, K. Song, D. Wu, X. Tan, N. L. Zhang, T. Qin,\nW. Zhang, and T. Liu, “DeepRapper: Neural rap gen-\neration with rhyme and rhythm modeling,” in Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Process-\ning (ACL-IJCNLP) , 2021, pp. 69–81.\n[25] J. Chang, J. C. Hung, and K. Lin, “Singability-enhanced\nlyric generator with music style transfer,” Computer\nCommunications , vol. 168, pp. 33–53, 2021.\n[26] O. Vechtomova, G. Sahu, and D. Kumar, “Generation\nof lyrics lines conditioned on music audio clips,” in\nProceedings of the 1st Workshop on NLP for Music and\nAudio (NLP4MusA) , 2020, pp. 33–37.\n[27] ——, “LyricJam: A system for generating lyrics for\nlive instrumental music,” in Proceedings of the 12th\nInternational Conference on Computational Creativity\n(ICCC) , 2021, pp. 122–130.\n[28] K. Watanabe and M. Goto, “Atypical lyrics completion\nconsidering musical audio signals,” in Proceedings of\nthe 27th International Conference on Multimedia Mod-\neling (MMM) , vol. 12572, 2021, pp. 174–186.\n[29] K. Watanabe, Y . Matsubayashi, K. Inui, and M. Goto,\n“Modeling structural topic transitions for automatic\nlyrics generation,” in Proceedings of the 28th Paciﬁc\nAsia Conference on Language, Information and Com-\nputation (PACLIC) , 2014, pp. 422–431.\n[30] P. Potash, A. Romanov, and A. Rumshisky, “Ghost-\nWriter: Using an LSTM for automatic rap lyric gen-\neration,” in Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP) , 2015, pp. 1919–1924.\n[31] M. Ghazvininejad, X. Shi, Y . Choi, and K. Knight,\n“Generating topical poetry,” in Proceedings of the 2016\nConference on Empirical Methods in Natural Language\nProcessing (EMNLP) , 2016, pp. 1183–1191.[32] H. Fan, J. Wang, B. Zhuang, S. Wang, and J. Xiao, “A\nhierarchical attention based seq2seq model for Chinese\nlyrics generation,” in Proceedings of the 16th Paciﬁc\nRim International Conference on Artiﬁcial Intelligence\n(PRICAI) , vol. 11672, 2019, pp. 279–288.\n[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” in Proceedings\nof the 9th International Conference on Learning Repre-\nsentations (ICLR) , 2021.\n[34] I. Loshchilov and F. Hutter, “Decoupled weight decay\nregularization,” in Proceedings of the 7th International\nConference on Learning Representations (ICLR) , 2019.\n[35] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding,” in Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language\nTechnologies (NAACL-HLT) , 2019, pp. 4171–4186.\n[36] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever, “Language models are unsupervised\nmultitask learners,” OpenAI blog , vol. 1, no. 8, p. 9,\n2019.\n[37] T. Kudo and Y . Matsumoto, “Japanese dependency anal-\nysis using cascaded chunking,” in Proceedings of the 6th\nConference on Natural Language Learning (CoNLL) ,\n2002.\n[38] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi,\n“The curious case of neural text degeneration,” in Pro-\nceedings of the 8th International Conference on Learn-\ning Representations (ICLR) , 2020.\n[39] Y . Li and B. Liu, “A normalized Levenshtein distance\nmetric,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence , vol. 29, no. 6, pp. 1091–1095,\n2007.\n[40] C. Manning and H. Schutze, Foundations of statistical\nnatural language processing . MIT press, 1999.\n[41] M. Marelli, S. Menini, M. Baroni, L. Bentivogli,\nR. Bernardi, and R. Zamparelli, “A SICK cure for\nthe evaluation of compositional distributional semantic\nmodels,” in Proceedings of the 9th International Confer-\nence on Language Resources and Evaluation (LREC) ,\n2014, pp. 216–223.\n[42] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka,\n“RWC Music Database: Popular, classical and jazz mu-\nsic databases,” in Proceedings of the 3rd International\nConference on Music Information Retrieval (ISMIR) ,\n2002, pp. 287–288.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n405[43] H. Chin, J. Kim, Y . Kim, J. Shin, and M. Y . Yi, “Ex-\nplicit content detection in music lyrics using machine\nlearning,” in Proceedings of the 2018 IEEE Interna-\ntional Conference on Big Data and Smart Computing\n(BigComp) , 2018, pp. 517–521.\n[44] M. Fell, E. Cabrio, M. Corazza, and F. Gandon, “Com-\nparing automated methods to detect explicit content in\nsong lyrics,” in Proceedings of the International Con-\nference on Recent Advances in Natural Language Pro-\ncessing (RANLP) , 2019, pp. 338–344.\n[45] E. Egivenia, G. R. Setiawan, S. S. Mintara, and\nD. Suhartono, “Classiﬁcation of explicit music content\nbased on lyrics, music metadata, and user annotation,”\ninProceedings of the 6th International Conference on\nSustainable Information Engineering and Technology\n(SIET) , 2021, pp. 265–270.\n[46] M. Rospocher, “On exploiting transformers for detect-\ning explicit song lyrics,” Entertainment Computing ,\nvol. 43, p. 100508, 2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n406"
    },
    {
        "title": "CLaMP: Contrastive Language-Music Pre-Training for Cross-Modal Symbolic Music Information Retrieval.",
        "author": [
            "Shangda Wu",
            "Dingyao Yu",
            "Xu Tan 0003",
            "Maosong Sun"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265247",
        "url": "https://doi.org/10.5281/zenodo.10265247",
        "ee": "https://zenodo.org/records/10265247/files/000017.pdf",
        "abstract": "We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description. In comparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP demonstrated comparable or superior performance on score-oriented datasets. Our models and code are available at https://github.com/microsoft/muzic/tree/main/clamp.",
        "zenodo_id": 10265247,
        "dblp_key": "conf/ismir/WuY0S23",
        "keywords": [
            "CLaMP",
            "contrastive language-music pre-training",
            "large dataset of 1.4 million music-text pairs",
            "text dropout",
            "bar patching",
            "masked music model pre-training objective",
            "semantic search",
            "zero-shot classification",
            "WikiMusicText dataset",
            "score-oriented datasets"
        ],
        "content": "CLAMP: CONTRASTIVE LANGUAGE-MUSIC PRE-TRAINING FOR\nCROSS-MODAL SYMBOLIC MUSIC INFORMATION RETRIEV AL\nShangda Wu1Dingyao Yu2Xu Tan2Maosong Sun1,3\n1Central Conservatory of Music, China2Microsoft Research Asia\n3Tsinghua University, China\nshangda@mail.ccom.edu.cn, {v-dingyaoyu, xuta}@microsoft.com, sms@tsinghua.edu.cn\nhttps://ai-muzic.github.io/clamp\nABSTRACT\nWe introduce CLaMP :Contrastive Language- MusicPre-\ntraining, which learns cross-modal representations be-\ntween natural language and symbolic music using a mu-\nsic encoder and a text encoder trained jointly with a con-\ntrastive loss. To pre-train CLaMP, we collected a large\ndataset of 1.4 million music-text pairs. It employed text\ndropout as a data augmentation technique and bar patch-\ning to efﬁciently represent music data which reduces se-\nquence length to less than 10%. In addition, we devel-\noped a masked music model pre-training objective to en-\nhance the music encoder’s comprehension of musical con-\ntext and structure. CLaMP integrates textual information\nto enable semantic search and zero-shot classiﬁcation for\nsymbolic music, surpassing the capabilities of previous\nmodels. To support the evaluation of semantic search and\nmusic classiﬁcation, we publicly release WikiMusicText\n(WikiMT), a dataset of 1010 lead sheets in ABC notation,\neach accompanied by a title, artist, genre, and description.\nIn comparison to state-of-the-art models that require ﬁne-\ntuning, zero-shot CLaMP demonstrated comparable or su-\nperior performance on score-oriented datasets. Our mod-\nels and code are available at https://github.com/\nmicrosoft/muzic/tree/main/clamp .\n1. INTRODUCTION\nSymbolic Music Information Retrieval (MIR) is a ﬁeld that\ndeals with the automatic analysis and retrieval of music\nbased on symbolic representations such as sheet music or\nMIDI ﬁles. Symbolic MIR has numerous practical appli-\ncations, including music genre classiﬁcation [1, 2], auto-\nmatic music transcription [3, 4], and music recommenda-\ntion systems [5]. However, traditional symbolic MIR ap-\nproaches based on handcrafted features are often limited in\ntheir ability to capture the complex nature of music.\nDeep learning has become increasingly popular in sym-\nbolic MIR [6–9] due to its ability to extract complex and\n© S. Wu, D. Yu, X. Tan, and M. Sun. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: S. Wu, D. Yu, X. Tan, and M. Sun, “CLaMP: Contrastive\nLanguage-Music Pre-training for Cross-Modal Symbolic Music Informa-\ntion Retrieval”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.\n𝑀1\n𝑀2\n𝑀3\n𝑀𝑁…\n…\n………\n…\n…\n…\n…RoBERTa\nText\nEncoder\nM3\nMusic \nEncoderText Features\nMusic\nFeatures\n…\nText \nDescriptions\nSheet MusicFigure 1 . The architecture of CLaMP, including two en-\ncoders - one for music and one for text - trained jointly with\na contrastive loss to learn cross-modal representations.\nabstract music features from large datasets. However, ob-\ntaining sufﬁcient labelled data can be costly and time-\nconsuming, as most labelled symbolic music datasets are\nsmall in size [10–12]. To address this issue, semantic\nsearch and zero-shot classiﬁcation techniques can be used\nto retrieve and label extensive unlabelled data. These tech-\nniques enable the search for music by a given open-domain\nquery (e.g., \" upbeat music with a fast tempo \"), or the auto-\nmatic identiﬁcation of music characteristics based on cus-\ntomized labels without the need for training data.\nTo enable semantic search and zero-shot classiﬁca-\ntion for symbolic music, it is necessary to establish a\nconnection between music and language. This can be\nachieved through the use of contrastive learning [13–17]\nand pre-training [18–20]. Contrastive learning trains mod-\nels to learn a feature space where similar sample pairs\nare grouped and dissimilar pairs are separated, while pre-\ntraining involves training a model on a large dataset that\ncan be ﬁne-tuned or directly applied to a speciﬁc task.\nIn this paper, we introduce a solution for cross-modal\nsymbolic MIR that utilizes contrastive learning and pre-\ntraining. The proposed approach, CLaMP :Contrastive\nLanguage- Music Pre-training, is inspired by the success\nof vision-language models [13]. Unlike prior models that\nrely solely on symbolic music [9, 12, 21], CLaMP learns\nsemantically rich representations of musical concepts from\nboth sheet music and natural language. The contributions\nof this paper are as follows:157Nocturne in E-flat major, Op. 9, No. 2\nFrédéric Chopin\nChopin composed his best-known \nNocturne in E ♭major…\nThe A and B sections become \nincreasingly ornamented with each \nrecurrence.Frédéric Chopin\nChopin composed his best-known \nNocturne in E ♭major…The A and B sections become \nincreasingly ornamented with each \nrecurrence.Frédéric Chopin. The A and B \nsections become increasingly \nornamented with each \nrecurrence. Chopin composed \nhis best-known Nocturne in E ♭\nmajor…Frédéric Chopin\nChopin composed his best-known \nNocturne in E ♭major…The A and B sections become \nincreasingly ornamented with each \nrecurrence.\nNocturne in E-flat major, Op. 9, No. 2Candidate Texts Random Shuffle Random Selection Text Concatenation\nNocturne in E-flat major, Op. 9, No. 2Figure 2 . Text dropout is a data augmentation technique that involves a process in which candidate texts are shufﬂed\nrandomly and then selected to form a concatenated text. In this example, three candidate texts were randomly selected and\nconcatenated to produce the input text.\n• CLaMP is a cross-modal model for symbolic MIR,\nwhich is pre-trained on WebMusicText (WebMT), a\ndataset of 1.4 million music-text pairs. To the best of\nour knowledge, this is the ﬁrst model of its kind and\nit achieves comparable or better performance than\nexisting state-of-the-art models without training.\n• We propose multiple techniques to improve con-\ntrastive language-music pre-training. Our proposed\ntechniques include applying text dropout as a data\naugmentation method, utilizing bar patching for ef-\nﬁcient music representation, and implementing the\nmasked music model pre-training objective.\n• The cross-modal pre-training empowers CLaMP to\nperform tasks beyond the capabilities of unimodal\nmodels. It possesses unique features such as seman-\ntic search for desired music using open-domain text\nqueries and zero-shot classiﬁcation for new music.\n• To facilitate the evaluation of semantic search and\nmusic classiﬁcation, we release the WikiMusicText\n(WikiMT) dataset, which consists of 1010 music-\ntext pairs sourced from Wikifonia and Wikipedia.\n2. METHODOLOGY\nThis section presents CLaMP and its cross-modal sym-\nbolic MIR abilities. Additionally, we describe the WebMT\ndataset, which we created to pre-train our model.\n2.1 Model Design\n2.1.1 Contrastive Learning Objective\nCLaMP jointly trains music and text encoders to represent\nthe structural and semantic aspects of both modalities in a\nshared feature space. This is achieved using a batch con-\nstruction method and objective [22, 23], as illustrated in\nFig. 1, whereby the correct pairings of a batch of Nmusic-\ntext pairs are predicted. The music and text encoders em-\nploy global average pooling to obtain corresponding fea-\ntures from the last hidden states.\nThe objective of CLaMP is to minimize the distance be-\ntweenNpaired music-text examples while maximizing the\ndistance between N2−Nunpaired examples. We denote abatch ofNmusic-text pairs as (mi,ti)N\ni=1, wheremiandti\nrepresent the i-th music and text inputs, respectively. The\nmusic and text encoders are represented as fmandft. The\ncontrastive loss for (mi,ti)N\ni=1is deﬁned as follows:\nLCL=−1\n2NN/summationdisplay\ni=1(logexp(fm(mi)·ft(ti)/τ)\n/summationtextN\nj=1⊮i̸=jexp(fm(mi)·ft(tj)/τ)+\nlogexp(fm(mi)·ft(ti)/τ)\n/summationtextN\nj=1⊮i̸=jexp(fm(mj)·ft(ti)/τ)),(1)\nwhereτis a temperature hyper-parameter that controls the\nsharpness of the softmax distribution, and ⊮i̸=jis an in-\ndicator function that equals 1 if i̸=j, and 0 otherwise.\nThe two terms in Eq. 1 consider either music-to-text or\ntext-to-music logits.\n2.1.2 Text Encoder\nCLaMP includes a text encoder to extract musically rele-\nvant features from the input text. To achieve optimal per-\nformance, a pre-trained language model is used to initialize\nthe text encoder. Furthermore, text dropout is employed as\na data augmentation technique to prevent overﬁtting and\nimprove the generalization ability of the text encoder.\nPre-trained Language Model RoBERTa [24] is a\ntransformer-based language model pre-trained on a large\ncorpus of English text using the Masked Language Mod-\neling (MLM) objective [18]. This model is designed to be\nﬁne-tuned on downstream tasks and has demonstrated ex-\ncellent performance as a text encoder for the contrastive\nlanguage-audio pre-training [25]. To improve training\nefﬁciency, we used DistilRoBERTa [26] instead, which\nhas fewer parameters (82M) compared to RoBERTa-base\n(125M) while achieving comparable performance.\nText Dropout Text dropout is a data augmentation\ntechnique that encourages models to learn robust features\nfrom input texts. This technique involves using a dataset\nconsisting of multiple paired candidate texts from various\nsources for each musical composition. Similar to [27], for\na given composition with Lcandidates, text dropout shuf-\nﬂes the set of candidate texts and randomly selects Ktexts,\nwhereKis uniformly and randomly sampled from integers\nranging from 1 to L. These selected texts are concatenated\nto form a single input text for the text encoder, as shown inProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n158Table 1 . The average number of tokens per lead sheet in\nthe WikiMT dataset with different encoding methods.\nEncoding Bar Patching ABC Notation OctupleMIDI [9]\nTokens 47.07±21.60 749.16±379.56 469.09±256.43\nFig. 2. Text dropout offers a wider range of possible text\ncombinations and allows the model to learn more complex\nand diverse textual features.\n2.1.3 Music Encoder\nThe CLaMP music encoder is designed to understand the\ncomplex musical structure and context within ABC nota-\ntion. As a text-based format for symbolic music, ABC no-\ntation incorporates a wide range of musical symbols com-\nmonly used in sheet music. To keep all musical informa-\ntion while shortening sequence length, the encoding pro-\ncess utilizes the bar patching technique. To optimize per-\nformance, the music encoder is speciﬁcally designed for\nsymbolic music understanding based on bar patching.\nBar Patching The bar in musical notation groups\nphrases by deﬁning a ﬁxed number of beats and each bar\ncan be read and played as a single unit. It is separated by\nvertical lines, providing reference points for locating posi-\ntions within a score.\nPrevious models [28–31] for ABC notation utilized\ncharacter-based tokenization, resulting in sequences that\nare too lengthy to process efﬁciently. On the other hand,\nMeasureV AE [32] demonstrated the feasibility of encoding\nscores at the bar-level for music generation. To improve\nthe efﬁciency of processing, we proposed bar patching, in-\nspired by patch-based techniques in computer vision [33].\nBar patching divides a score into several small segments\ncorresponding to bars or headers (i.e. meta-information)\nin ABC notation. In our implementation, each patch is as-\nsigned a maximum of 64 characters, covering 98.8% of\nthe headers or bars in the pre-training dataset. We add an\n[END] token at the end of each patch to indicate the end\nof the sequence. Patches with fewer than 64 are padded\nwith[PAD] tokens, while those with over 64 characters\nare truncated. For the vocabulary, 95 ASCII printable char-\nacters and three special tokens (i.e., [PAD] ,[MASK] , and\n[END] ) are considered, resulting in a total of 98 tokens.\nThus, each patch can be represented as a 64 ×98 matrix.\nThese patches are then ﬂattened and projected into 768 di-\nmensions embeddings and used as input tokens, as illus-\ntrated in Fig. 3.\nBar patching effectively reduces the average sequence\nlength of the encoded music to less than 10% of the origi-\nnal ABC notation, as shown in Table 1. This technique im-\nproves the efﬁciency of representing music and facilitates\nfaster computation while preserving all musical informa-\ntion in the notation.\nMasked Music Model The Masked Music Model\n(M3) is a self-supervised model for symbolic MIR based\non bar patching representation. The primary concept of M3\nis to introduce random noise to certain patches of the in-\nput music, and then reconstruct the characters in the noise-\n|:SPFSP|:SPFSP|\nL:1/8  M:4/4   K:Emin  |: F | CGGC G2 CG | G2 FG BGFE |]L:1/8 M:4/4      K:Emin       F :| | CGGC G2 CG |        G2 FG BGFE |]\nABC NotationNoise-added \nBar Patches0Patch-level Transformer EncoderCharacter-level Transformer Decoder\nPosition +\nPatch EmbedsPatch FeaturesShifted OutputsOutput Chars\nL:1/8L:1/8END\n|END\nLinear Projection of Flattened Bar Patches1 2 3 4 5\nFigure 3 . The masked music model architecture, where\nthe encoder takes in a sequence of patches, and the decoder\nreconstructs character information of noise-added patches.\nadded bar patches based on the context. This pre-training\nenables M3 to learn from unlabelled musical data, making\nit useful for initializing the CLaMP music encoder.\nM3 is based on an asymmetric encoder-decoder archi-\ntecture, similar to MAE [34], as shown in Fig. 3. It uses\nan encoder to extract contextualized features of individual\npatches, along with a decoder, which is lightweight and\nautoregressively reconstructs the characters for each patch.\nAfter pre-training, the decoder is discarded and the encoder\nis used to initialize the music encoder of CLaMP.\nThe pre-training objective is inspired by MLM [18]. We\nﬁrst randomly select M%of the bar patches in the input\nmusic, and then the noise is added in three different ways:\n•Masking: 80% of the selected bar patches are re-\nplaced with a special patch ﬁlled with [MASK] to-\nkens. This encourages the model to learn to ﬁll in\nmissing information and understand the relationship\nbetween different musical elements.\n•Shufﬂing: 10% of the selected bar patches are ran-\ndomly shufﬂed internally. For example, a bar patch\n\"|: F | \" may be randomly shufﬂed to \" F :|\n|\" as shown in Fig. 3. This forces the model to\nlearn the patterns and structures within bar patches.\n•Unchanged: 10% of the selected bar patches are left\nunchanged. This can narrow down the gap between\npre-training and ﬁne-tuning.\nM3 is trained to predict the original characters in the\nnoise-added bar patches based on contextualized patch fea-\ntures. The model is optimized using the cross-entropy loss,\nwhich compares the predicted characters with the ground\ntruth characters. The ﬁnal objective is to minimize the\naverage loss over all the noise-added bar patches in the\ntraining set. By denoising these bar patches, M3 learns to\ncapture the dependencies and relationships between differ-\nent musical elements and structures, allowing it to extract\nmeaningful features from ABC notation.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n159This piece of \nmusic is \ncomposed by \nChopin.𝑀1RoBERTa\nText\nEncoder\nM3\nMusic \nEncoderText Features\nMusic\nFeature(b) Zero-shot Classification\nChopin\nLiszt\nMozart\nProkofievThis piece of \nmusic is \ncomposed by \n{composer}.\nAlbinoni\n…𝑀1\n𝑀2\n𝑀3\n𝑀𝑁\n…RoBERTa\nText\nEncoder\nM3\nMusic \nEncoderText Feature\nMusic\nFeatures\n…(a) Semantic Search\nThis well-known \nnocturne is in \nrounded binary \nform.\n…Figure 4 . The processes of CLaMP performing cross-modal symbolic MIR tasks, including semantic search and zero-shot\nclassiﬁcation for symbolic music, without requiring task-speciﬁc training data.\n2.2 Cross-Modal Symbolic MIR\nCLaMP is capable of aligning symbolic music and natural\nlanguage, which can be used for various cross-modal re-\ntrieval tasks, including semantic search and zero-shot clas-\nsiﬁcation for symbolic music.\nSemantic search is a technique for retrieving music\nby open-domain queries, which differs from traditional\nkeyword-based searches that depend on exact matches or\nmeta-information. This involves two steps: 1) extracting\nmusic features from all scores in the library, and 2) trans-\nforming the query into a text feature. By calculating the\nsimilarities between the text feature and the music fea-\ntures, it can efﬁciently locate the score that best matches\nthe user’s query in the library.\nZero-shot classiﬁcation refers to the classiﬁcation of\nnew items into any desired label without the need for\ntraining data. It involves using a prompt template\nto provide context for the text encoder. For exam-\nple, a prompt such as \" This piece of music is\ncomposed by {composer}. \" is utilized to form in-\nput texts based on the names of candidate composers. The\ntext encoder then outputs text features based on these in-\nput texts. Meanwhile, the music encoder extracts the mu-\nsic feature from the unlabelled target symbolic music. By\ncalculating the similarity between each candidate text fea-\nture and the target music feature, the label with the highest\nsimilarity is chosen as the predicted one.\n2.3 WebMusicText Dataset\nTo facilitate the learning of relationships between natu-\nral language and symbolic music, we developed a dataset\nnamed WebMusicText (WebMT) by crawling an extensive\ncollection of music-text pairs from the web. Our dataset\ncomprises 1,448,750 pairs of music-text data, where all\nmusic ﬁles are in score-oriented formats (e.g., MusicXML,\nLilyPond, and ABC notation). To reduce the disparity be-\ntween scores in different notations, we ﬁrst converted all\nmusic ﬁles to MusicXML and then to ABC notation1. In\naddition, to avoid information leakage, we removed any\nnatural language (e.g., titles, composers, and lyrics) in\nABC notation. The text parts of each pair were obtained\n1https://wim.vree.org/svgParse/xml2abc.htmlfrom corresponding meta-information (e.g., title and com-\nposer) or user comments, and are all in English. WebMT\nfeatures diverse musical compositions, from monophonic\nfolk music to polyphonic orchestral music, which enables\nthe model to learn a wide range of musical information.\n3. EXPERIMENTS\n3.1 Settings\n3.1.1 Models\n•MusicBERT [9] : This model combines unsu-\npervised pre-training with supervised ﬁne-tuning,\nwhich achieved state-of-the-art results. MusicBERT\nis available in two settings: MusicBERT-S/1024\n(MusicBERT small ), andMusicBERT-B/1024\n(MusicBERT base). MusicBERT-S/1024 consists\nof 4 layers and was pre-trained on the small-scale\nLakh MIDI Dataset (LMD, 148,403 pieces) [35],\nwhile MusicBERT-B/1024 has 12 layers and was\npre-trained on the large-scale Million MIDI Dataset\n(MMD, 1,524,557 pieces). Both models have a max-\nimum length of 1024.\n•M3: Our proposed music encoder is used to com-\npare the performances of unimodal and multimodal\nmodels trained on the same dataset (i.e., WebMT).\nM3 comes with two settings: M3-S/512 and\nM3-S/1024 , with maximum lengths of 512 and\n1024, respectively. In the following experiments,\nboth settings use the 6 encoder layers only.\n•CLaMP : Several variants were tested to verify\nthe effectiveness of the proposed techniques for\nimproving contrastive language-music pre-training.\nThese include CLaMP-S/512 which is the full\nmodel,CLaMP-S/512 (w/o TD) which re-\nmoves text dropout, CLaMP-S/512 (w/o M3)\nwhich has a randomly initialized music encoder,\nandCLaMP-S/512 (w/o M3, BP) which re-\nmoves both M3 and bar patching, and uses char-level\ntokenization to encode raw ABC notation instead.\nCLaMP-S/1024 was included to verify the effec-\ntiveness of an extended maximum length.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1603.1.2 Pre-training\nThe text encoder was initialized using DistilRoBERTa\n[26], with a maximum length of 128, and the music en-\ncoder was initialized using two settings: M3-S/512 and\nM3-S/1024. A length of 512 resulted in truncating 17.29%\nof compositions in WebMT, while a length of 1024 re-\nduced truncation to 7.7%. Both models were trained for\n40 epochs with 6 encoder layers and 3 decoder layers, an\nembedding size of 768, and a noise ratio of 45%. Based\non these two M3 encoders, we developed CLaMP-S/512\nand CLaMP-S/1024. Both of them were trained for 20\nepochs, using the AdamW optimizer [36] with β1= 0.9,\nβ2= 0.999,ϵ= 10−8, and a weight decay coefﬁcient\nof 0.01. The batch size is set to 640, and the temperature\nτ= 0.2. The training process was accelerated and mem-\nory was saved by using mixed precision [37].\n3.1.3 Evaluation Datasets\nWe introduce WikiMusicText (WikiMT)2, a new dataset\nfor the evaluation of semantic search and music classiﬁ-\ncation. It includes 1010 lead sheets (melodies with har-\nmonies) in ABC notation sourced from Wikifonia, each\naccompanied by a title, artist, genre, and description. The\ntitle and artist information is extracted from the score,\nwhereas the genre labels are obtained by matching key-\nwords from the Wikipedia entries and assigned to one of\nthe 8 classes that loosely mimic the GTZAN genres [38].\nThe description is obtained by utilizing BART-large [39]\nto summarize and clean the corresponding Wikipedia en-\ntry. Additionally, following WebMT, the natural language\ninformation within the ABC notation is removed.\nIn addition to WikiMT, we use two other datasets to\nevaluate music classiﬁcation: VGMIDI and Pianist8. VG-\nMIDI [11] includes 204 score-oriented MIDI arrange-\nments that were classiﬁed according to the valence-arousal\nmodel. Pianist8 [12] contains symbolic piano perfor-\nmances of 411 pieces from 8 composers with distinct\nstyles, which were automatically transcribed from audio\nusing a model presented in [8].\n3.1.4 Metrics\nWe use the following three metrics to evaluate the effec-\ntiveness of models in various downstream tasks:\n• Mean Reciprocal Rank (MRR) is used to evaluate\nranking systems. This metric calculates the average\nof the reciprocal ranks of the correct answers, which\nmeasures the effectiveness of the ranking.\n• Hit Ratio at K (HR@K) measures the accuracy of\nthe model by checking if the correct item is among\nthe top K recommendations, which is often used in\nrecommendation systems.\n• F1-macro score is a metric that assesses the overall\neffectiveness of a classiﬁcation model. It is com-\nputed using the arithmetic mean (i.e., unweighted\nmean) of all the per-class F1 scores.\n2https://huggingface.co/datasets/sander-wood/\nwikimtTable 2 . Semantic search performance of CLaMP on\nWikiMT (1010 music-text pairs) under different settings.\nSetting MRR HR@1 HR@10 HR@100\nS/512 0.2561 0.1931 0.3693 0.7020\nS/1024 0.2016 0.1436 0.3109 0.6554\nS/512 (w/o TD) 0.1841 0.1248 0.2911 0.6188\nS/512 (w/o M3) 0.1262 0.0802 0.1960 0.5119\nS/512 (w/o M3, BP) 0.0931 0.0525 0.1584 0.4426\n3.2 Results\n3.2.1 Semantic Search\nIn the semantic search evaluation, we assessed different\nversions of CLaMP for semantic search, aiming to test the\nefﬁcacy of contrastive language-music pre-training tech-\nniques. The pre-training dataset WebMT and the evalu-\nation dataset WikiMT have no overlap, thus guaranteeing\nthe validity of our evaluation results. In addition, as seman-\ntic search requires no additional training for this dataset, it\ndemonstrates the generalizability of CLaMP.\nTable 2 shows that our full model (CLaMP-S/512) out-\nperforms all other models across all metrics. Interest-\ningly, we discovered that increasing the maximum se-\nquence length to 1024 (CLaMP-S/1024) did not lead to\nan improvement in performance. We attribute this to the\nfact that all lead sheets in the WikiMT dataset, once en-\ncoded with bar patching, have a length smaller than 512,\nwhich limits the potential advantages of the longer se-\nquence length of CLaMP-S/1024. We also observed that\nthe removal of the proposed techniques from CLaMP had\na considerable negative impact on semantic search perfor-\nmance. Notably, the removal of M3 pre-training had the\ngreatest effect on model performance, followed by text\ndropout and bar patching.\nIn conclusion, our evaluation of CLaMP on WikiMT\nshows that CLaMP-S/512 with all proposed contrastive\nlanguage-music pre-training techniques is the most effec-\ntive for the semantic search task. This highlights the impor-\ntance of these techniques for effective pre-training and se-\nmantic search tasks. Additionally, increasing the sequence\nlength (CLaMP-S/1024) did not improve the model’s per-\nformance. These results emphasize the signiﬁcance of us-\ning appropriate pre-training techniques in language-music\nmodels and suggest that a longer sequence length may not\nnecessarily result in better outcomes.\n3.2.2 Music Classiﬁcation\nThe goal of the classiﬁcation evaluation is to assess how\nwell the zero-shot CLaMP models perform compared to\nother ﬁne-tuned models. In addition, to evaluate pre-\ntrained models, linear probes are used to train a linear clas-\nsiﬁer for the classiﬁcation based on the features from pre-\ntrained models. Despite being less powerful and relying on\npre-trained model features, linear classiﬁers offer a valu-\nable means of quantitatively assessing feature quality [40].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n161Table 3 . Classiﬁcation performance of different models on three datasets: WikiMT (1010 pieces, 8 genres), VGMIDI (204\npieces, 4 emotions), and Pianist8 (411 pieces, 8 composers).\nModelWikiMT VGMIDI [11] Pianist8 [12]\nF1-macro Accuracy F1-macro Accuracy F1-macro Accuracy\nLinear Probe MusicBERT-S/1024 0.2401 0.3507 0.4662 0.5350 0.8047 0.8102\nLinear Probe MusicBERT-B/1024 0.1746 0.3219 0.5127 0.5850 0.8379 0.8413\nZero-shot CLaMP-S/512 0.2660 0.3248 0.5217 0.6176 0.2180 0.2512\nZero-shot CLaMP-S/1024 0.2248 0.3406 0.4678 0.5049 0.1509 0.2390\nLinear Probe M3-S/512 0.2832 0.3990 0.5991 0.6667 0.6773 0.6909\nLinear Probe M3-S/1024 0.3079 0.4020 0.5966 0.6522 0.6844 0.6958\nLinear Probe CLaMP-S/512 0.3452 0.4267 0.6453 0.6866 0.7067 0.7152\nLinear Probe CLaMP-S/1024 0.3449 0.4416 0.6345 0.6720 0.7271 0.7298\nWikiMT was converted into the MIDI format using mu-\nsic21 [41] to be compatible with MusicBERT. In contrast,\nfor VGMIDI and Pianist8, we employed MuseScore3’s\nbatch conversion tool3to convert the scores into the Mu-\nsicXML format, which were then converted into ABC no-\ntation for use with M3 and CLaMP.\nWe conducted 5-fold cross-validation with the same\nfolds to assess all linear probe models, using identical ﬁne-\ntuning settings and a batch size of 10 to ensure consistency,\ngiven the limited size of the evaluation datasets. The linear\nprobe CLaMP models used the music encoder only, while\nthe text encoder was discarded. In the zero-shot classiﬁ-\ncation setting, CLaMP had no previous exposure to these\nevaluation datasets during pre-training. We utilized manu-\nally designed prompts for the zero-shot CLaMP models.\nThe top half of Table 3 presents the comparison of the\nperformance between linear probe MusicBERT and zero-\nshot CLaMP. The results found that the zero-shot CLaMP\nmodels demonstrated comparable or even superior perfor-\nmance compared to the linear probe MusicBERT mod-\nels on WikiMT and VGMIDI datasets. Interestingly, the\nsmaller zero-shot CLaMP-S/512 outperformed the larger\nlinear probe MusicBERT-B/1024, indicating that the pre-\ntraining of CLaMP has enabled it to learn more gener-\nalizable features that are useful for zero-shot music clas-\nsiﬁcation. However, this trend was not observed on Pi-\nanist8, where MusicBERT models performed much better\nthan zero-shot CLaMP models. This difference in perfor-\nmance can be attributed to the source of the datasets, as\nWikiMT and VGMIDI primarily focus on score informa-\ntion, whereas Pianist8 contains performance MIDI data de-\nrived from audio. Since both CLaMP and M3 were trained\nexclusively on score information, they lack knowledge of\nperformance MIDI. However, we noticed that the perfor-\nmances of linear probe CLaMP models on Pianist8 signif-\nicantly improved after ﬁne-tuning compared to the zero-\nshot ones. This suggests that incorporating ABC notation\nfrom performance MIDI into the pre-training of CLaMP\nmay enhance its ability to comprehend such data.\n3https://musescore.org/en/project/\nbatch-convertThe linear probe CLaMP models show better perfor-\nmance compared to the linear probe M3 models, as in-\ndicated in the bottom half of Table 3, despite being pre-\ntrained on the same dataset with the same architecture.\nThis is attributed to the use of contrastive learning, which\naligns the music encoder of CLaMP with the text modality,\nthus implicitly introducing textual information to the mu-\nsic encoder. Furthermore, we found that CLaMP-S/1024\nperformed better on Pianist8 than CLaMP-S/512, suggest-\ning that a larger maximum length is beneﬁcial for models\nto learn performance MIDI.\nIn summary, our evaluation demonstrates that zero-shot\nCLaMP performs comparably to state-of-the-art models\nin music classiﬁcation. Furthermore, the incorporation of\ncontrastive learning and textual information enhances the\nmusic encoder’s performance, resulting in better classiﬁca-\ntion accuracy when compared to M3 which employed the\nsame architecture. These results highlight the potential of\nCLaMP as a pre-training framework for symbolic MIR.\n4. CONCLUSIONS\nThis paper introduces CLaMP, a pre-trained model that\nutilizes contrastive language-music pre-training techniques\nto build cross-modal representations between natural lan-\nguage and symbolic music. The model was trained on\na dataset containing 1.4 million music-text pairs and has\ndemonstrated unique abilities of semantic search and zero-\nshot classiﬁcation for symbolic music. Compared to\nstate-of-the-art models that require ﬁne-tuning, zero-shot\nCLaMP exhibits comparable or superior performance in\nscore-oriented music classiﬁcation tasks without any train-\ning. However, the current version of CLaMP has limited\ncomprehension of performance MIDI, and still has room\nfor improvement. Future research will aim to expand its\ncapabilities by scaling it up and pre-training it on larger\ndatasets that incorporate a wider range of symbolic mu-\nsic formats beyond score-oriented ones. We expect that its\ncross-modal representations will facilitate research on new\ntopics in music analysis, retrieval, and generation, and pro-\nvide a foundation for the development of innovative sys-\ntems and applications that integrate music and language.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1625. REFERENCES\n[1] I. Karydis, “Symbolic music genre classiﬁcation\nbased on note pitch and duration,” in Advances\nin Databases and Information Systems, 10th East\nEuropean Conference, ADBIS 2006, Thessaloniki,\nGreece, September 3-7, 2006, Proceedings , ser.\nLecture Notes in Computer Science, Y . Manolopoulos,\nJ. Pokorný, and T. K. Sellis, Eds., vol. 4152.\nSpringer, 2006, pp. 329–338. [Online]. Available:\nhttps://doi.org/10.1007/11827252_25\n[2] C. Kofod and D. O. Arroyo, “Exploring the design\nspace of symbolic music genre classiﬁcation using data\nmining techniques,” in 2008 International Conferences\non Computational Intelligence for Modelling, Control\nand Automation (CIMCA 2008), Intelligent Agents,\nWeb Technologies and Internet Commerce (IAWTIC\n2008), Innovation in Software Engineering (ISE\n2008), 10-12 December 2008, Vienna, Austria ,\nM. Mohammadian, Ed. IEEE Computer Society,\n2008, pp. 43–48. [Online]. Available: https://doi.org/\n10.1109/CIMCA.2008.223\n[3] J. P. Bello, G. Monti, and M. B. Sandler, “Techniques\nfor automatic music transcription,” in ISMIR 2000,\n1st International Symposium on Music Information\nRetrieval, Plymouth, Massachusetts, USA, October\n23-25, 2000, Proceedings , 2000. [Online]. Available:\nhttp://ismir2000.ismir.net/papers/bello_paper.pdf\n[4] C. Raphael, “Automatic transcription of piano music,”\ninISMIR 2002, 3rd International Conference on\nMusic Information Retrieval, Paris, France, October\n13-17, 2002, Proceedings , 2002. [Online]. Available:\nhttp://ismir2002.ismir.net/proceedings/02-FP01-2.pdf\n[5] C. Walshaw, “A statistical analysis of the abc music\nnotation corpus: Exploring duplication,” 2014.\n[6] C. Hawthorne, I. Simon, R. Swavely, E. Manilow, and\nJ. H. Engel, “Sequence-to-sequence piano transcription\nwith transformers,” in Proceedings of the 22nd\nInternational Society for Music Information Retrieval\nConference, ISMIR 2021, Online, November 7-12,\n2021 , J. H. Lee, A. Lerch, Z. Duan, J. Nam,\nP. Rao, P. van Kranenburg, and A. Srinivasamurthy,\nEds., 2021, pp. 246–253. [Online]. Available: https:\n//archives.ismir.net/ismir2021/paper/000030.pdf\n[7] J. Gardner, I. Simon, E. Manilow, C. Hawthorne,\nand J. H. Engel, “MT3: multi-task multitrack music\ntranscription,” in The Tenth International Conference\non Learning Representations, ICLR 2022, Virtual\nEvent, April 25-29, 2022 . OpenReview.net, 2022.\n[Online]. Available: https://openreview.net/forum?id=\niMSjopcOn0p\n[8] Q. Kong, B. Li, X. Song, Y . Wan, and Y . Wang,\n“High-resolution piano transcription with pedals by\nregressing onset and offset times,” IEEE ACM Trans.\nAudio Speech Lang. Process. , vol. 29, pp. 3707–3717,2021. [Online]. Available: https://doi.org/10.1109/\nTASLP.2021.3121991\n[9] M. Zeng, X. Tan, R. Wang, Z. Ju, T. Qin, and T. Liu,\n“Musicbert: Symbolic music understanding with\nlarge-scale pre-training,” in Findings of the Association\nfor Computational Linguistics: ACL/IJCNLP 2021,\nOnline Event, August 1-6, 2021 , ser. Findings of ACL,\nC. Zong, F. Xia, W. Li, and R. Navigli, Eds., vol.\nACL/IJCNLP 2021. Association for Computational\nLinguistics, 2021, pp. 791–800. [Online]. Available:\nhttps://doi.org/10.18653/v1/2021.ﬁndings-acl.70\n[10] A. Ferraro and K. Lemström, “On large-scale\ngenre classiﬁcation in symbolically encoded music\nby automatic identiﬁcation of repeating patterns,”\ninProceedings of the 5th International Conference\non Digital Libraries for Musicology, DLfM 2018,\nParis, France, September 28, 2018 , K. R. Page,\nEd. ACM, 2018, pp. 34–37. [Online]. Available:\nhttps://doi.org/10.1145/3273024.3273035\n[11] L. N. Ferreira and J. Whitehead, “Learning to generate\nmusic with sentiment,” 2019.\n[12] Y . Chou, I. Chen, C. Chang, J. Ching, and\nY . Yang, “Midibert-piano: Large-scale pre-training\nfor symbolic music understanding,” CoRR , vol.\nabs/2107.05223, 2021. [Online]. Available: https:\n//arxiv.org/abs/2107.05223\n[13] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\nG. Krueger, and I. Sutskever, “Learning transferable\nvisual models from natural language supervision,” in\nProceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event , ser. Proceedings of Machine Learning\nResearch, M. Meila and T. Zhang, Eds., vol. 139.\nPMLR, 2021, pp. 8748–8763. [Online]. Available:\nhttp://proceedings.mlr.press/v139/radford21a.html\n[14] W. Kim, B. Son, and I. Kim, “Vilt: Vision-and-\nlanguage transformer without convolution or region\nsupervision,” in Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event , ser. Proceedings of Machine\nLearning Research, M. Meila and T. Zhang, Eds.,\nvol. 139. PMLR, 2021, pp. 5583–5594. [Online].\nAvailable: http://proceedings.mlr.press/v139/kim21k.\nhtml\n[15] J. Li, D. Li, C. Xiong, and S. C. H. Hoi, “BLIP:\nbootstrapping language-image pre-training for uniﬁed\nvision-language understanding and generation,” in\nInternational Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA ,\nser. Proceedings of Machine Learning Research,\nK. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári,\nG. Niu, and S. Sabato, Eds., vol. 162. PMLR,\n2022, pp. 12 888–12 900. [Online]. Available: https:\n//proceedings.mlr.press/v162/li22n.htmlProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n163[16] B. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang,\n“CLAP: learning audio concepts from natural language\nsupervision,” CoRR , vol. abs/2206.04769, 2022.\n[Online]. Available: https://doi.org/10.48550/arXiv.\n2206.04769\n[17] Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y . Li,\nand D. P. W. Ellis, “Mulan: A joint embedding\nof music audio and natural language,” CoRR , vol.\nabs/2208.12415, 2022. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2208.12415\n[18] J. Devlin, M. Chang, K. Lee, and K. Toutanova,\n“BERT: pre-training of deep bidirectional transformers\nfor language understanding,” in Proceedings of the\n2019 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers) , J. Burstein, C. Doran,\nand T. Solorio, Eds. Association for Computational\nLinguistics, 2019, pp. 4171–4186. [Online]. Available:\nhttps://doi.org/10.18653/v1/n19-1423\n[19] T. B. Brown, B. Mann, N. Ryder, M. Subbiah,\nJ. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss,\nG. Krueger, T. Henighan, R. Child, A. Ramesh, D. M.\nZiegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner,\nS. McCandlish, A. Radford, I. Sutskever, and\nD. Amodei, “Language models are few-shot learners,”\ninAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual , H. Larochelle, M. Ranzato, R. Hadsell,\nM. Balcan, and H. Lin, Eds., 2020. [Online]. Avail-\nable: https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\n[20] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu,\n“Exploring the limits of transfer learning with a\nuniﬁed text-to-text transformer,” J. Mach. Learn. Res. ,\nvol. 21, pp. 140:1–140:67, 2020. [Online]. Available:\nhttp://jmlr.org/papers/v21/20-074.html\n[21] Z. Wang and G. Xia, “Musebert: Pre-training\nmusic representation for music understanding and\ncontrollable generation,” in Proceedings of the 22nd\nInternational Society for Music Information Retrieval\nConference, ISMIR 2021, Online, November 7-12,\n2021 , J. H. Lee, A. Lerch, Z. Duan, J. Nam,\nP. Rao, P. van Kranenburg, and A. Srinivasamurthy,\nEds., 2021, pp. 722–729. [Online]. Available: https:\n//archives.ismir.net/ismir2021/paper/000090.pdf\n[22] K. Sohn, “Improved deep metric learning with multi-\nclass n-pair loss objective,” in Advances in Neural\nInformation Processing Systems 29: Annual Confer-\nence on Neural Information Processing Systems 2016,December 5-10, 2016, Barcelona, Spain , D. D. Lee,\nM. Sugiyama, U. von Luxburg, I. Guyon, and R. Gar-\nnett, Eds., 2016, pp. 1849–1857. [Online]. Avail-\nable: https://proceedings.neurips.cc/paper/2016/hash/\n6b180037abbebea991d8b1232f8a8ca9-Abstract.html\n[23] A. van den Oord, Y . Li, and O. Vinyals, “Repre-\nsentation learning with contrastive predictive coding,”\nCoRR , vol. abs/1807.03748, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1807.03748\n[24] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi,\nD. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and\nV . Stoyanov, “Roberta: A robustly optimized BERT\npretraining approach,” CoRR , vol. abs/1907.11692,\n2019. [Online]. Available: http://arxiv.org/abs/1907.\n11692\n[25] Y . Wu, K. Chen, T. Zhang, Y . Hui, T. Berg-Kirkpatrick,\nand S. Dubnov, “Large-scale contrastive language-\naudio pretraining with feature fusion and keyword-\nto-caption augmentation,” CoRR , vol. abs/2211.06687,\n2022. [Online]. Available: https://doi.org/10.48550/\narXiv.2211.06687\n[26] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distil-\nbert, a distilled version of bert: smaller, faster, cheaper\nand lighter,” ArXiv , vol. abs/1910.01108, 2019.\n[27] S. Doh, M. Won, K. Choi, and J. Nam, “Toward uni-\nversal text-to-music retrieval,” in ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , 2023, pp. 1–5.\n[28] B. L. Sturm, J. F. Santos, O. Ben-Tal, and\nI. Korshunova, “Music transcription modelling\nand composition using deep learning,” CoRR ,\nvol. abs/1604.08723, 2016. [Online]. Available:\nhttp://arxiv.org/abs/1604.08723\n[29] C. Geerlings and A. Meroño-Peñuela, “Interacting\nwith gpt-2 to generate controlled and believable mu-\nsical sequences in abc notation,” in NLP4MUSA , 2020.\n[30] S. Wu and M. Sun, “Tunesformer: Forming tunes with\ncontrol codes,” CoRR , vol. abs/2301.02884, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.\n2301.02884\n[31] ——, “Exploring the efﬁcacy of pre-trained\ncheckpoints in text-to-music generation task,”\nin The AAAI-23 Workshop on Creative AI\nAcross Modalities , 2023. [Online]. Available:\nhttps://openreview.net/forum?id=QmWXskBhesn\n[32] A. Pati, A. Lerch, and G. Hadjeres, “Learning to\ntraverse latent spaces for musical score inpainting,”\ninProceedings of the 20th International Society\nfor Music Information Retrieval Conference, ISMIR\n2019, Delft, The Netherlands, November 4-8, 2019 ,\nA. Flexer, G. Peeters, J. Urbano, and A. V olk,\nEds., 2019, pp. 343–351. [Online]. Available: http:\n//archives.ismir.net/ismir2019/paper/000040.pdfProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n164[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nand N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in 9th\nInternational Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021 . OpenReview.net, 2021. [Online]. Available:\nhttps://openreview.net/forum?id=YicbFdNTTy\n[34] K. He, X. Chen, S. Xie, Y . Li, P. Dollár,\nand R. B. Girshick, “Masked autoencoders are\nscalable vision learners,” in IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR\n2022, New Orleans, LA, USA, June 18-24, 2022 .\nIEEE, 2022, pp. 15 979–15 988. [Online]. Available:\nhttps://doi.org/10.1109/CVPR52688.2022.01553\n[35] C. Raffel, “Learning-based methods for comparing se-\nquences, with applications to audio-to-midi alignment\nand matching,” Ph.D. dissertation, PhD Thesis, 2016.\n[36] I. Loshchilov and F. Hutter, “Decoupled weight decay\nregularization,” in 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019 . OpenReview.net, 2019.\n[Online]. Available: https://openreview.net/forum?id=\nBkg6RiCqY7\n[37] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos,\nE. Elsen, D. García, B. Ginsburg, M. Houston,\nO. Kuchaiev, G. Venkatesh, and H. Wu, “Mixed\nprecision training,” in 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track\nProceedings . OpenReview.net, 2018. [Online]. Avail-\nable: https://openreview.net/forum?id=r1gs9JgRZ\n[38] B. L. Sturm, “An analysis of the GTZAN music genre\ndataset,” in Proceedings of the second international\nACM workshop on Music information retrieval with\nuser-centered and multimodal strategies, MIRUM\n’12, Nara, Japan, October 29 - November 02,\n2012 , C. C. S. Liem, M. Müller, S. K. Tjoa,\nand G. Tzanetakis, Eds., 2012, pp. 7–12. [Online].\nAvailable: https://doi.org/10.1145/2390848.2390851\n[39] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V . Stoyanov, and L. Zettlemoyer,\n“BART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension,” in Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\nD. Jurafsky, J. Chai, N. Schluter, and J. R.\nTetreault, Eds. Association for Computational Lin-\nguistics, 2020, pp. 7871–7880. [Online]. Available:\nhttps://doi.org/10.18653/v1/2020.acl-main.703\n[40] Y . M. Asano, C. Rupprecht, and A. Vedaldi, “A critical\nanalysis of self-supervision, or what we can learn froma single image,” in 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net, 2020.\n[Online]. Available: https://openreview.net/forum?id=\nB1esx6EYvr\n[41] M. S. Cuthbert and C. Ariza, “Music21: A toolkit\nfor computer-aided musicology and symbolic music\ndata,” in Proceedings of the 11th International\nSociety for Music Information Retrieval Conference,\nISMIR 2010, Utrecht, Netherlands, August 9-13,\n2010 , J. S. Downie and R. C. Veltkamp, Eds.\nInternational Society for Music Information Retrieval,\n2010, pp. 637–642. [Online]. Available: http:\n//ismir2010.ismir.net/proceedings/ismir2010-108.pdfProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n165"
    },
    {
        "title": "IteraTTA: An Interface for Exploring Both Text Prompts and Audio Priors in Generating Music With Text-to-Audio Models.",
        "author": [
            "Hiromu Yakura",
            "Masataka Goto"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265239",
        "url": "https://doi.org/10.5281/zenodo.10265239",
        "ee": "https://zenodo.org/records/10265239/files/000014.pdf",
        "abstract": "Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.",
        "zenodo_id": 10265239,
        "dblp_key": "conf/ismir/YakuraG23",
        "keywords": [
            "text-to-audio generation",
            "novice users",
            "freely generate music audio",
            "chord progressions",
            "instruments",
            "understanding music audios",
            "audio priors",
            "iterative comparison",
            "refining text prompts",
            "selecting favorable audio priors"
        ],
        "content": "IteraTTA: AN INTERFACE FOR EXPLORING\nBOTH TEXT PROMPTS AND AUDIO PRIORS\nIN GENERATING MUSIC WITH TEXT-TO-AUDIO MODELS\nHiromu Yakura\nUniversity of Tsukuba\nTsukuba, Japan\nhiromu.yakura@aist.go.jpMasataka Goto\nNational Institute of Advanced Industrial Science\nand Technology (AIST), Tsukuba, Japan\nm.goto@aist.go.jp\nFigure 1 . IteraTTA is an interface dedicated for allowing novice users to show their creativity in text-to-audio music\ngeneration processes. It provides a) computational guidance for constructing initial prompts and b) dual-sided iterative\nexploration of text prompts and audio priors.\nABSTRACT\nRecent text-to-audio generation techniques have the poten-\ntial to allow novice users to freely generate music audio.\nEven if they do not have musical knowledge, such as about\nchord progressions and instruments, users can try various\ntext prompts to generate audio. However, compared to the\nimage domain, gaining a clear understanding of the space\nof possible music audios is difﬁcult because users cannot\nlisten to the variations of the generated audios simultane-\nously. We therefore facilitate users in exploring not only\ntext prompts but also audio priors that constrain the text-\nto-audio music generation process. This dual-sided explo-\nration enables users to discern the impact of different text\nprompts and audio priors on the generation results through\niterative comparison of them. Our developed interface, It-\neraTTA, is speciﬁcally designed to aid users in reﬁning\ntext prompts and selecting favorable audio priors from the\ngenerated audios. With this, users can progressively reach\n© H. Yakura and M. Goto. Licensed under a Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). Attribution:\nH. Yakura and M. Goto, “IteraTTA: An interface for exploring both text\nprompts and audio priors in generating music with text-to-audio mod-\nels”, in Proc. of the 24th Int. Society for Music Information Retrieval\nConf., Milan, Italy, 2023.their loosely-speciﬁed goals while understanding and ex-\nploring the space of possible results. Our implementa-\ntion and discussions highlight design considerations that\nare speciﬁcally required for text-to-audio models and how\ninteraction techniques can contribute to their effectiveness.\n1. INTRODUCTION\nRecent advances in generative machine learning tech-\nniques open up novel ways for a diverse group of individu-\nals to engage in creative processes [1, 2]. Speciﬁcally, mu-\nsic generation models can foster creative expression among\nnovice users, who may not necessarily possess formal mu-\nsical knowledge [3, 4]. Consequently, several approaches\nhave been proposed to enable users to control various mu-\nsical attributes of generated audios, such as specifying the\nnote or rhythm density [5, 6] and chord progression [7–9].\nText-to-audio models [10,11] are promising in terms of al-\nlowing users who are not familiar with the concepts of such\nmusical attributes to generate their own sounds.\nNevertheless, there are still several gaps toward deploy-\ning such models to support the creativity of novice users.\nFor example, the models rely on annotated labels of music\nclips presented in their training datasets [12–14], which\nprimarily consist of musical descriptions such as genres,\ninstruments, and moods. Therefore, providing such infor-129mation as a text prompt is crucial for enabling ﬁne-grained\ncontrol over generated music audios. However, this may\nprove challenging for novice users due to disparities in\nartistic vocabulary among individuals with varying levels\nof musical knowledge [15]. Experimentally, it has been\nsuggested that non-musicians tend to rely more on abstract\nconcepts, such as the pleasantness or complexity of music,\nwhen appreciating musical pieces [16], which may pose\ndifﬁculties in fully exploring various text prompts.\nMoreover, understanding the space of possible results is\nalso challenging, particularly when compared to the use of\ntext-to-image models. In text-to-image generation, users\ncan look over various generation results at a glance, which\nfosters their understanding of the space and helps them de-\ncide on directions to explore [17]. From the perspective of\nexplainable AI (XAI), we can say that such results serve as\nexplanations by example [18] because the results implicitly\ninvite the users to infer the behavior of the models. How-\never, in text-to-audio generation, users cannot simultane-\nously listen to multiple generation results, thus impeding\ntheir comprehension and ability to efﬁciently explore the\nspace. These points imply that speciﬁc design considera-\ntions are necessary to fully leverage the potential of text-\nto-audio models and exploring them would also provide a\nnew perspective in terms of XAI.\nIn this paper, we introduce IteraTTA, an interface ded-\nicated to the text-to-audio (TTA) music generation pro-\ncesses of novice users. This interface enables iterative ex-\nploration of both text prompts and audio priors, allowing\nusers to gain a comprehensive understanding of the space\nof possible results by sufﬁciently constraining the genera-\ntion processes. We constructed this interface based on our\nobservations and related literature on creativity support,\nwhich emphasize the importance of 1) computational guid-\nance for constructing initial prompts and 2) dual-sided iter-\native exploration of text prompts and audio priors. More-\nover, we deployed the interface as a publicly-available Web\nservice and analyzed the diverse ways in which users uti-\nlized it in their creative processes. Our results and discus-\nsions shed light on ways to utilize models developed in the\nMIR community to unleash the creativity not only of ex-\npert users [19] but also of individuals with varying degrees\nof musical knowledge.\n2. RELATED WORK\n2.1 Music Generation Techniques\nMusic generation has been one of the central topics with\nthe MIR community [20–24], and recently, generative ma-\nchine learning techniques have been widely employed for\nthis purpose [24, 25]. While methods for symbolic music\ngeneration that output MIDI ﬁles have been popular [26–\n32], some methods use generative models to directly out-\nput audio, leveraging their expressiveness [33–36]. For ex-\nample, Jukebox [33] and RA VE [34] use variational au-\ntoencoders and autoregressive models trained on large-\nscale music datasets to generate diverse music audios.\nControllability in music generation has been also em-phasized [5–9, 37–39] because it is vital to open up its ap-\nplications for supporting users’ creative processes [40,41].\nFor instance, Music FaderNets [5] allows users to modify\nthe rhythm and note densities of generation results, while\nMusic SketchNet [6] enables them to specify pitch con-\ntours and rhythm patterns. Wang et al. [7] and Dai et al. [8]\nhave proposed methods to further constrain the chord pro-\ngression of generation results. However, as mentioned in\nSection 1, users are not always familiar with such con-\ncepts, and then, they would have difﬁculties in using these\nmethods to output music audios they want to generate. We\nacknowledge that some methods [38, 39] provide percep-\ntual control that does not require extensive musical knowl-\nedge: emotion-based musical generation. Nevertheless,\nthey are based on Russell’s valence-arousal model [42]\nconsisting of four classes, which limits the range of con-\ntrols and may hamper users’ agency [43] when the meth-\nods are used to support their creative processes.\nIn this context, recent text-to-audio models [10, 11] can\nbe an effective tool for such novice users. These models\nlearn the relationship between music audios and their text\ndescriptions (more speciﬁcally, latent representations en-\ncoded from the descriptions by RoBERTa [44]) and use\nit to guide results in generating new audios from an in-\nputted text ( i.e., text prompt). As RoBERTa can encode\ntext prompts with variable length and content, the mod-\nels can provide ﬂexible control without requiring speciﬁc\nmusical knowledge of rhythm patterns or chord progres-\nsions. Moreover, they allow users to constrain generation\nresults not only by text prompts but also by audio priors,\nensuring that the results have similar characteristics to the\npriors. For example, the diffusion model [45] employed by\nAudioLDM [11] usually uses Gaussian noise for the seed\nof its generation process, but by using a noise-infused au-\ndio prior, we can obtain generation results preserving the\ncharacteristics of the provided audio.\nHere, text-to-image models that use similar schemes\nhave been shown to unleash the creativity of novice users,\nallowing them to iteratively explore open-ended variations\nof text prompts [17] and customize their intermediate re-\nsults by specifying image prior constraints [46]. Similarly,\ntext-to-audio models can be leveraged to provide users\nwith such iterative exploration or customization. However,\nwe also expect that text-to-audio music generation pro-\ncesses may pose several speciﬁc difﬁculties, as explained\nin Section 1. Therefore, we explored how interaction tech-\nniques can address these challenges by developing an in-\nterface dedicated to text-to-audio models.\n2.2 Interfaces for Music Generation\nThere is a series of research on building interfaces to\nlet users interact with music generation techniques effec-\ntively [47–53]. MySong [47], for instance, involves a mu-\nsic accompaniment generation model, with which users\ncan control the happiness or jazziness of generation re-\nsults. Louie et al. [49] proposed an interactive interface for\nnovice users so that they can use a symbolic music gener-\nation technique with control of happiness or randomness.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n130The interface also allows users to constrain generation re-\nsults by providing music priors, which was experimentally\nconﬁrmed to be effective in iteratively reﬁning the results.\nZhou et al. [52,53] utilized a user-in-the-loop Bayesian op-\ntimization technique to enable novice users to iteratively\nexplore melodies composed by a generative model.\nThese interfaces underscore the signiﬁcance of provid-\ning controls and supporting iterative exploration in facili-\ntating the creativity of novice users using music generation\ntechniques. Consequently, the provision of recent text-to-\naudio models to novice users would be highly suitable for\nthis purpose, as they offer more ﬂexible control, compared\nto using several parameters such as happiness, while also\nallowing the use of audio priors. Our paper contributes to\nthis series of research by examining design considerations\nof interfaces for text-to-audio music generation processes,\naiming to expand the scope of applications of recent tech-\nniques developed in the MIR community.\n3. DESIGN REQUIREMENTS\nAs stated in Section 1, our goal is to leverage text-to-audio\nmodels to facilitate the creative expression of novice users\nregardless of their musical knowledge. To this aim, we\nembarked upon an examination of potential challenges that\nthese users may encounter during text-to-audio music gen-\neration processes and subsequently derived a set of design\nrequirements to address these issues. Guided by the princi-\nples of human-computer interaction, we utilized the think-\naloud protocol [54, 55] by involving three volunteers who\nself-reported that they possessed no formal musical train-\ning beyond compulsory education. Speciﬁcally, we pro-\nvided the volunteers with access to one of the latest text-\nto-audio models [11] on Google Colab using its ofﬁcial\nimplementation1, which enabled them to provide any text\nprompts and subsequently listen to three music audios gen-\nerated from the text prompts. Here, since the remotely-\nparticipated volunteers were Japanese speakers recruited\nvia word-of-mouth communication, we told them that they\ncan use DeepL Translator to translate text prompts into En-\nglish to obtain better results with the model that is mainly\ntrained on the dataset with English text labels [12–14].\nThey freely used the model for approximately 30 minutes\nwhile sharing their screens on a video call and verbaliz-\ning their thoughts and feelings. This allowed us to identify\nthe challenges that they encountered and the factors that\ncontributed to these challenges. We then conducted semi-\nstructured interviews to validate the challenges identiﬁed\nand to gain further insight into the reasons behind them.\nTheir responses were analyzed based on open coding [56],\nwhich yielded the following design requirements in line\nwith the existing literature on creativity support.\n3.1 Computational guidance for constructing initial\nprompts\nWe observed that the volunteers frequently encountered\ndifﬁculty in formulating appropriate text prompts to initi-\n1https://github.com/haoheliu/AudioLDMate their use of the model. For example, one volunteer en-\ntered the phrase “a song sounds like star wars,” resulting in\naudio containing a battle cry with a space-like sound effect.\nThis can be attributed to the characteristics of the text la-\nbels in the dataset used to train the model [12–14]. Speciﬁ-\ncally, the labels of music clips consist primarily of musical\ndescriptions such as genres, instruments, and moods, like:\n“An orchestra plays a happy melody while the strings and\nwind instruments are being played [14].” Therefore, pro-\nviding such a description would be essential to ensure that\nthe model trained on the dataset generates music audio as\nintended. The volunteer was unable to generate music-like\naudio until he attempted several prompts and ﬁnally en-\ntered “solemn music starting with a trumpet fanfare.”\nIn the context of creativity support, two underlying fac-\ntors could explain the aforementioned observation. First,\nan inherent gap in artistic vocabulary exists between ex-\npert and novice users [15]. Without deep musical knowl-\nedge, it can be challenging to conceive a precise descrip-\ntion of music audios. Additionally, novice users often\nhave loosely-speciﬁed goals when starting a creative en-\ndeavor [57–59]. They reﬁne their objectives gradually by\nexploring the space of possible results through iterative\nexploration [60, 61]. However, the dependency of text-\nto-audio models on precise descriptions of clearly-deﬁned\ngoals makes it difﬁcult for novice users to initiate such\nexploration. This suggests that supporting them compu-\ntationally in constructing initial prompts could potentially\nfacilitate the creativity of novice users.\n3.2 Dual-sided iterative exploration of text prompts\nand audio priors\nWe also observed that the volunteers encountered chal-\nlenges in efﬁciently exploring the generated results. One\nvolunteer who had prior experience with text-to-image\nmodels mentioned the point, as:\n“Unlike text-to-image models, comparing various\nresults at a glance was difﬁcult with the text-to-audio\nmodel. So, ﬁnding a text prompt reﬂecting my inten-\ntion most faithfully became much tough. ”\nIn other words, iteratively trying different text prompts\nwould not necessarily assist users in comprehending the\nspace of potential results, although it is necessary for\nnovice users to reﬁne their loosely-speciﬁed goals [60,61].\nTherefore, users cannot determine which direction would\nbe closest to their goals and what text prompt to try next.\nAnother volunteer mentioned an issue he faced, as:\n“I once found a generation result with a good\nmelody, but I wanted to change its tone. So, I added\n‘with a ﬂute’ to its text prompt and regenerated.\nHowever, the melody was then completely changed,\nwhich was frustrating. ”\nThis implies that we need to let users utilize not only text\nprompts but also audio priors to constrain the tune of gen-\neration results. In sum, supporting the creativity of novice\nusers in text-to-audio music generation processes requires\nenabling them to efﬁciently explore variations of both textProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n131Figure 2 . To facilitate the exploration of text prompts and audio priors, IteraTTA allows a) comparison of generation results\nwith an audio prior and b) instant edit of a text prompt.\nprompts and audio priors, allowing them to iteratively re-\nﬁne their goals by understanding the space of possible re-\nsults. This demands us to develop an interface specif-\nically tailored for text-to-audio models to provide such\ndual-sided exploration of text prompts and audio priors.\n4. IteraTTA\nBased on the above design requirements, we present Iter-\naTTA, a dedicated interface for text-to-audio music gener-\nation processes. It was implemented as a Web-based sys-\ntem, allowing novice users to instantly beneﬁt from the lat-\nest text-to-audio models in their creative processes.\n4.1 Design\nAs illustrated in Figure 1, our interface requires users to\nﬁrst input a theme phrase for music audios to generate. The\ninputted phrase need not include precise musical descrip-\ntions since IteraTTA leverages a large language model to\nderive such descriptions suitable for text-to-audio models\nusing knowledge embedded in the models [62]. Specif-\nically, the interface queries a large language model that\n“Please give me four variational lists of comma-separated\nphrases describing what does a music clip of \" [theme\nphrase] \" sound.” It then uses the four responded phrase\nlists as a variety of the ﬁrst text prompts to start the music\ngeneration processes in parallel. This feature allows novice\nusers to translate loosely-speciﬁed goals in their minds into\nmusical descriptions, which can also help them to envisage\nvariations of text prompts to explore.\nIteraTTA then generates three music audios for each of\nthe four prompts. The generated audios are arranged in\ntwo dimensions (see Figure 1), which enables novice users\nto understand how different music audios are generated by\ndifferent text prompts, and also, how different music au-\ndios are generated by the same text prompts. This is in-\ntended to assist users in identifying which text prompts\nand audio priors are closely aligned with their goals and\nwhich direction is worth exploring. If a user identiﬁesa suitable candidate text prompt, they can customize the\nprompt and generate new music audios with it. Alterna-\ntively, if the user discovers a suitable music audio, they\ncan use it as an audio prior to generate new music audios.\nIn essence, the user can explore the subspace of possible\nresults that are proximate to their goals by constraining ei-\nther text prompts or audio priors, while gradually reﬁning\ntheir goals by themselves.\nWe have incorporated several features to facilitate the\nexploration of text prompts and audio priors, as shown\nin Figure 2. For instance, when a user speciﬁes an audio\nprior, IteraTTA enables the user to compare generated re-\nsults with it. It also offers an instant editing feature of text\nprompts, allowing users to amplify or suppress the sound\nof a selected instrument. This is achieved by simply adding\na phrase of \"with strong [instrument] \" or \"with no [instru-\nment] \" into a text prompt, but it provides an example of\nhow they can modify generation results through prompts.\n4.2 Implementation\nAs mentioned, we developed IteraTTA as a Web-based\nsystem to invite novice users for trying music generation\nwith it. For the implementation of its back-end server,\nwe utilized Python with FastAPI and incorporated an API\nof GPT-3.52to construct initial prompts, while Audi-\noLDM [11] was employed to generate the music audios.\nThe length of music audios to generate was predeter-\nmined at 10 seconds so that our GPU server harnessing an\nNVIDIA RTX 2080 Ti can afford the generation of 12 au-\ndios (3 audios ×4 prompts) simultaneously. On average,\nthe generation process takes approximately 15 seconds. In\naddition, we used DeepL API to translate text prompts\ninto English when they were provided in non-English lan-\nguages because we observed that it led to better results in\nSection 3. For the front-end interface of IteraTTA, we uti-\nlized Vue.js, which enables users to download the gener-\nated music audios or share them on Twitter.\n2We usedgpt-3.5-turbo ofhttps://platform.openai.\ncom/docs/models/gpt-3-5 .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n132Figure 3 . Word cloud of theme phrases the 8,831 users inputted on our Web service.\n5. ANALYSIS\nTo investigate the effectiveness of IteraTTA in supporting\ndiverse users in the wild, we deployed it as a publicly-\navailable Web service in Japanese3. Within two days of\nrelease, 8,831 users generated 246,423 music audios. In\nthis section, we discuss the insights we extracted from their\nusage logs and their responses to a form that we put a link\nto it on the Web service so that they can share their opinion\nand feedback voluntarily.\n5.1 Diversity of theme phrases\nWe ﬁrst examined the theme phrases that users inputted to\ninitiate text-to-audio music generation processes and found\nthat they were highly varied. Some users provided music-\nrelated phrases, such as “nice city pop” and “cute future\nbass,” while others were more speciﬁc, like: “80’s hip\nhop that break dancers would dance to.” There were also\nphrases expressing more abstract ideas, such as “Arabian\ncaves” and “silent dream of a priestess.” Figure 3 visual-\nizes the words often used in the translated phrases in the\nform of a word cloud, showing their diversity.\nTo explore the role of IteraTTA, we compared the theme\nphrases inputted by the users and the text prompts derived\nfrom them by the large language model to the text labels\nin the dataset used for training the text-to-audio model.\nSpeciﬁcally, we randomly sampled 1,000 cases for each\nof the theme phrases, text prompts, and text labels4and\ncalculated their representation vectors using the same pre-\ntrained model of RoBERTa [44] as the text-to-audio model.\nWe then visualized the distribution of the vectors using t-\nSNE [63], as presented in Figure 4. This indicates that\nIteraTTA guided the large language model to derive text\nprompts that bridged the gap between the diverse users’\n3Its English version is currently available at\nhttp://iteratta.duckdns.org/ , and readers can try it on\ntheir Web browsers (Google Chrome is recommended).\n4For the text labels, we extracted labels containing “music” from Au-\ndioCaps [13].\nFigure 4 . Visualization of the representation vectors of\nthe theme phrases inputted by the users, the text prompts\ncomputationally derived from them, and the text labels in\nthe training dataset.\ntheme phrases and the text labels in the training dataset. In\nfact, we found that the large language model successfully\nderived text prompts containing musical descriptions even\nfrom abstract phrases, such as “otherworldly harmonies,\ndelicate strings, minimalistic percussion, dreamlike vo-\ncals” for “silent dream of a priestess.” These results sug-\ngest the effectiveness of guiding the construction of initial\nprompts to support the creative processes of novice users,\nas discussed in Section 3.1.\n5.2 Journey of iterative exploration\nWe also investigated how the users interacted with gen-\nerated results produced by IteraTTA. We analyzed the in-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n133Figure 5 . Visualization of how the users utilized the dual-sided exploration of IteraTTA.\nteraction log of the service and obtained Figure 5. While\nsome users just tried the exploration feature once, we\nfound that others made iterative use of the feature, alter-\nnating between providing text prompts and audio priors.\nInterestingly, one user repeated this reﬁnement process 32\ntimes, specifying text prompts 14 times and audio priors\n18 times before sharing their ﬁnal result on Twitter. These\npoints imply that our design, which enables dual-sided it-\nerative exploration, helped the users effectively utilize the\ntext-to-audio model.\n5.3 Unleashing the creativity of novice users\nWe lastly analyzed the users’ responses to the feedback\nform, which received 33 responses in total. Overall, most\nof them expressed their afﬁrmative experiences with the\ntext-to-audio music creation processes, like:\n“It was a very interesting trial. I can interact with it\nthroughout the day. ”\n“In my personal opinion, it can be used as a source\nof sampling materials and an idea generator. As a\nperson who usually composes music, I never had any\nnegative feelings about composing from text using\nthis. It is wonderful. ”\nThe latter comment suggests that the features of IteraTTA\nprepared for novice users can also beneﬁt experienced\nusers in different ways.\nIt is also notable that the users left comments imply-\ning the importance of the design requirements discussed in\nSection 3, such as how they enjoyed the open-ended explo-\nration starting from loosely-speciﬁed theme phrases.\n“It was fun to encounter songs that ﬁt the theme I\nprovided but I had never heard before. ”\n“I really enjoyed the points that I could take advan-\ntage of ChatGPT’s ability to associate and verbalize\neven seemingly unconnected ideas, which allowed\nme to provide crazy theme phrases that would not be\nunderstood by a human. I also learned a lot about\nhow to describe songs by looking at the derived text\nprompts. ”\nInterestingly, in the form, some users left a successful\nprompt that they reached after exploration:“I would like to report that including a phrase of\n‘simple progression’ or limiting the number of tracks\nyielded stabilized music audios, like: ‘Ideal harmo-\nnious song: balanced instrumentation, band sound,\nsimple chord progressions, rhythmic drum patterns,\ncatchy pop melody, up to 12 tracks. ’ ”\n“Adding ‘clear sound quality’ produces less noisy\naudios. ”\nIt is surprising that, even though we provided no ex-\nplicit description of the behavior of text-to-audio mod-\nels, the users were able to gain such knowledge by them-\nselves through the iterative exploration with IteraTTA.\nWhile such prompt modiﬁers (also known as quality boost-\ners) [64] that inﬂuence results in a speciﬁc way have\nbeen discovered for text-to-image models in a community-\ndriven manner [17, 64], the above comments would be\nthe ﬁrst examples for text-to-audio models, to the best of\nour knowledge. We assume that this is a manifestation\nof users’ creativity in text-to-audio music generation pro-\ncesses and would be hard to derive without IteraTTA.\n6. CONCLUSION\nThis paper introduces IteraTTA, an interface speciﬁcally\ndesigned for supporting novice users in their text-to-audio\nmusic generation processes. Its design is guided by two\nmain principles, providing a) computational guidance for\nconstructing initial prompts and b) dual-sided iterative ex-\nploration of text prompts and audio priors. The former\ncan help novice users translate their loosely-speciﬁed goals\ninto text prompts, which serve as starting points for ex-\nploration, even if they do not have rich artistic vocabu-\nlaries. The latter is important for enabling them to com-\nprehend the space of possible results and gradually re-\nﬁne their goals. To examine how diverse users utilize It-\neraTTA in their creative processes, we deployed it as a\npublicly-available Web service and analyzed users’ behav-\niors, which highlight the importance of these design con-\nsiderations in supporting the users’ creativity. Importantly,\nthese principles are applicable not only to the speciﬁc text-\nto-audio model but to other models, including those to be\nproposed in the near future. We believe that this paper can\nserve as a foundation for enabling novice users to beneﬁt\nfrom state-of-the-art models in the MIR community.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n1347. ACKNOWLEDGEMENT\nThis work was supported in part by JSPS KAKENHI Grant\nNumber JP21J20353, JST ACT-X Grant Number JPM-\nJAX200R, and JST CREST Grant Number JPMJCR20D4,\nJapan.\n8. REFERENCES\n[1] G. Franceschelli and M. Musolesi, “Creativity and ma-\nchine learning: A survey,” arXiv , vol. abs/2104.02726,\n2021.\n[2] M. J. Muller, L. B. Chilton, A. Kantosalo, C. P. Martin,\nand G. Walsh, “Proceedings of the GenAICHI work-\nshop: Generative AI and HCI,” in Extended Abstracts\nof the 2022 ACM SIGCHI Conference on Human Fac-\ntors in Computing Systems . ACM, 2022, pp. 110:1–\n110:7.\n[3] F. Carnovalini and A. Rodà, “Computational creativ-\nity and music generation systems: An introduction to\nthe state of the art,” Frontiers in Artiﬁcial Intelligence ,\nvol. 3, p. 14, 2020.\n[4] M. Rohrmeier, “On creativity, music’s AI complete-\nness, and four challenges for artiﬁcial musical creativ-\nity,” Transactions of the International Society for Mu-\nsic Information Retrieval , vol. 5, no. 1, pp. 50–66,\n2022.\n[5] H. H. Tan and D. Herremans, “Music FaderNets: Con-\ntrollable music generation based on high-level features\nvia low-level feature modelling,” in Proceedings of the\n21st International Society for Music Information Re-\ntrieval Conference . ISMIR, 2020, pp. 109–116.\n[6] K. Chen, C. Wang, T. Berg-Kirkpatrick, and S. Dub-\nnov, “Music SketchNet: Controllable music generation\nvia factorized representations of pitch and rhythm,” in\nProceedings of the 21st International Society for Music\nInformation Retrieval Conference . ISMIR, 2020, pp.\n77–84.\n[7] Z. Wang, D. Wang, Y . Zhang, and G. Xia, “Learning in-\nterpretable representation for controllable polyphonic\nmusic generation,” in Proceedings of the 21st Interna-\ntional Society for Music Information Retrieval Confer-\nence. ISMIR, 2020, pp. 662–669.\n[8] S. Dai, Z. Jin, C. Gomes, and R. B. Dannenberg,\n“Controllable deep melody generation via hierarchical\nmusic structure representation,” in Proceedings of the\n22nd International Society for Music Information Re-\ntrieval Conference . ISMIR, 2021.\n[9] Z. Wang and G. Xia, “MuseBERT: Pre-training mu-\nsic representation for music understanding and control-\nlable generation,” in Proceedings of the 22nd Interna-\ntional Society for Music Information Retrieval Confer-\nence. ISMIR, 2021, pp. 722–729.[10] A. Agostinelli, T. I. Denk, Z. Borsos, J. H. En-\ngel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen,\nA. Roberts, M. Tagliasacchi, M. Shariﬁ, N. Zeghidour,\nand C. H. Frank, “MusicLM: Generating music from\ntext,” arXiv , vol. abs/2301.11325, 2023.\n[11] H. Liu, Z. Chen, Y . Yuan, X. Mei, X. Liu, D. P. Mandic,\nW. Wang, and M. D. Plumbley, “AudioLDM: Text-to-\naudio generation with latent diffusion models,” arXiv ,\nvol. abs/2301.12503, 2023.\n[12] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter,\n“Audio Set: An ontology and human-labeled dataset\nfor audio events,” in Proceedings of the 2017 IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing . IEEE, 2017, pp. 776–780.\n[13] C. D. Kim, B. Kim, H. Lee, and G. Kim, “Audiocaps:\nGenerating captions for audios in the wild,” in Pro-\nceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies . ACL, 2019, pp.\n119–132.\n[14] K. Drossos, S. Lipping, and T. Virtanen, “Clotho: an\naudio captioning dataset,” in Proceedings of the 2020\nIEEE International Conference on Acoustics, Speech\nand Signal Processing . IEEE, 2020, pp. 736–740.\n[15] K. Swanwick, Musical Knowledge: Intuition, analysis\nand music education . London, UK: Routledge, 2002.\n[16] J. E. Gromko, “Perceptual differences between expert\nand novice music listeners: A multidimensional scal-\ning analysis,” Psychology of Music , vol. 21, no. 1, pp.\n34–47, 1993.\n[17] J. Oppenlaender, R. Linder, and J. Silvennoinen,\n“Prompting AI art: An investigation into the cre-\native skill of prompt engineering,” arXiv , vol.\nabs/2303.13534, 2023.\n[18] A. B. Arrieta, N. D. Rodríguez, J. D. Ser, A. Ben-\nnetot, S. Tabik, A. Barbado, S. García, S. Gil-Lopez,\nD. Molina, R. Benjamins, R. Chatila, and F. Her-\nrera, “Explainable artiﬁcial intelligence (XAI): con-\ncepts, taxonomies, opportunities and challenges toward\nresponsible AI,” Information Fusion , vol. 58, pp. 82–\n115, 2020.\n[19] K. Andersen and P. Knees, “Conversations with expert\nusers in music retrieval and research challenges for cre-\native MIR,” in Proceedings of the 17th International\nSociety for Music Information Retrieval Conference .\nISMIR, 2016, pp. 122–128.\n[20] C. Roads, “Research in music and artiﬁcial intelli-\ngence,” ACM Computing Surveys , vol. 17, no. 2, pp.\n163–190, 1985.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n135[21] J. D. Fernández and F. J. Vico, “AI methods in algorith-\nmic composition: A comprehensive survey,” Journal of\nArtiﬁcial Intelligence Research , vol. 48, pp. 513–582,\n2013.\n[22] C. Liu and C. Ting, “Computational intelligence in\nmusic composition: A survey,” IEEE Transactions on\nEmerging Topics in Computational Intelligence , vol. 1,\nno. 1, pp. 2–15, 2017.\n[23] J. Briot and F. Pachet, “Deep learning for music gen-\neration: challenges and directions,” Neural Computing\nand Applications , vol. 32, no. 4, pp. 981–993, 2020.\n[24] E. Deruty, M. Grachten, S. Lattner, J. Nistal, and\nC. Aouameur, “On the development and practice of\nAI technology for contemporary popular music pro-\nduction,” Transactions of the International Society for\nMusic Information Retrieval , vol. 5, no. 1, p. 35, 2022.\n[25] S. Ji, J. Luo, and X. Yang, “A comprehensive survey\non deep music generation: Multi-level representations,\nalgorithms, evaluations, and future directions,” arXiv ,\nvol. abs/2011.06801, 2020.\n[26] L. Yang, S. Chou, and Y . Yang, “MidiNet: A convo-\nlutional generative adversarial network for symbolic-\ndomain music generation,” in Proceedings of the 18th\nInternational Society for Music Information Retrieval\nConference . ISMIR, 2017, pp. 324–331.\n[27] H. Dong and Y . Yang, “Convolutional generative ad-\nversarial networks with binary neurons for polyphonic\nmusic generation,” in Proceedings of the 19th Interna-\ntional Society for Music Information Retrieval Confer-\nence. ISMIR, 2018, pp. 190–196.\n[28] C. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon,\nC. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman,\nM. Dinculescu, and D. Eck, “Music Transformer: Gen-\nerating music with long-term structure,” in Proceed-\nings of the 7th International Conference on Learning\nRepresentations . OpenReview.net, 2019.\n[29] Y . Huang and Y . Yang, “Pop Music Transformer: Beat-\nbased modeling and generation of expressive pop piano\ncompositions,” in Proceedings of the 28th ACM Inter-\nnational Conference on Multimedia . ACM, 2020, pp.\n1180–1188.\n[30] W. Hsiao, J. Liu, Y . Yeh, and Y . Yang, “Compound\nWord Transformer: Learning to compose full-song\nmusic over dynamic directed hypergraphs,” in Pro-\nceedings of the 35th AAAI Conference on Artiﬁcial In-\ntelligence . AAAI Press, 2021, pp. 178–186.\n[31] G. Mittal, J. H. Engel, C. Hawthorne, and I. Simon,\n“Symbolic music generation with diffusion models,” in\nProceedings of the 22nd International Society for Mu-\nsic Information Retrieval Conference . ISMIR, 2021,\npp. 468–475.[32] B. Yu, P. Lu, R. Wang, W. Hu, X. Tan, W. Ye, S. Zhang,\nT. Qin, and T. Liu, “Museformer: Transformer with\nﬁne- and coarse-grained attention for music genera-\ntion,” in Proceedings of the 36th Conference on Neural\nInformation Processing Systems . Curran Associates,\nInc., 2022, pp. 1376–1388.\n[33] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,\nand I. Sutskever, “Jukebox: A generative model for\nmusic,” arXiv , vol. abs/2005.00341, 2020.\n[34] A. Caillon and P. Esling, “RA VE: A variational autoen-\ncoder for fast and high-quality neural audio synthesis,”\narXiv , vol. abs/2111.05011, 2021.\n[35] T. Hung, B. Chen, Y . Yeh, and Y . Yang, “A bench-\nmarking initiative for audio-domain music generation\nusing the FreeSound loop dataset,” in Proceedings of\nthe 22nd International Society for Music Information\nRetrieval Conference . ISMIR, 2021, pp. 310–317.\n[36] M. Pasini and J. Schlüter, “Musika! Fast inﬁnite wave-\nform music generation,” in Proceedings of the 23rd\nInternational Society for Music Information Retrieval\nConference . ISMIR, 2022, pp. 543–550.\n[37] T. Akama, “Connective fusion: Learning transforma-\ntional joining of sequences with application to melody\ncreation,” in Proceedings of the 21th International So-\nciety for Music Information Retrieval Conference . IS-\nMIR, 2020, pp. 46–53.\n[38] H. Hung, J. Ching, S. Doh, N. Kim, J. Nam, and\nY . Yang, “EMOPIA: A multi-modal pop piano dataset\nfor emotion recognition and emotion-based music gen-\neration,” in Proceedings of the 22nd International So-\nciety for Music Information Retrieval Conference . IS-\nMIR, 2021, pp. 318–325.\n[39] P. Neves, J. Fornari, and J. B. Florindo, “Generating\nmusic with sentiment using Transformer-GANs,” in\nProceedings of the 23rd International Society for Mu-\nsic Information Retrieval Conference . ISMIR, 2022,\npp. 717–725.\n[40] K. Wang and J. V . Nickerson, “A literature review on\nindividual creativity support systems,” Computers in\nHuman Behavior , vol. 74, pp. 139–151, 2017.\n[41] C. A. Huang, H. V . Koops, E. Newton-Rex, M. Din-\nculescu, and C. Cai, “Human-AI co-creation in song-\nwriting,” in Proceedings of the 21th International So-\nciety for Music Information Retrieval Conference . IS-\nMIR, 2020, pp. 708–716.\n[42] J. A. Russell, “A circumplex model of affect,” Journal\nof Personality and Social Psychology , vol. 39, no. 6,\npp. 1161–1178, 1980.\n[43] J. Heer, “Agency plus automation: Designing artiﬁcial\nintelligence into interactive systems,” Proceedings of\nthe National Academy of Sciences , vol. 116, no. 6, pp.\n1844–1850, 2019.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n136[44] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov,\n“RoBERTa: A robustly optimized BERT pretraining\napproach,” arXiv , vol. abs/1907.11692, 2019.\n[45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer, “High-resolution image synthesis with la-\ntent diffusion models,” in Proceedings of the 2022\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition . IEEE, 2022, pp. 10 674–10 685.\n[46] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh,\nand Y . Taigman, “Make-a-scene: Scene-based text-to-\nimage generation with human priors,” in Proceedings\nof the 17th European Conference on Computer Vision .\nSpringer, 2022, pp. 89–106.\n[47] I. Simon, D. Morris, and S. Basu, “MySong: automatic\naccompaniment generation for vocal melodies,” in Pro-\nceedings of the 2008 ACM SIGCHI Conference on Hu-\nman Factors in Computing Systems . ACM, 2008, pp.\n725–734.\n[48] C. A. Huang, C. Hawthorne, A. Roberts, M. Din-\nculescu, J. Wexler, L. Hong, and J. Howcroft, “The\nBach doodle: Approachable music composition with\nmachine learning at scale,” in Proceedings of the 20th\nInternational Society for Music Information Retrieval\nConference . ISMIR, 2019, pp. 793–800.\n[49] R. Louie, A. Coenen, C. Z. Huang, M. Terry, and C. J.\nCai, “Novice-AI music co-creation via AI-steering\ntools for deep generative models,” in Proceedings of\nthe 2020 ACM SIGCHI Conference on Human Fac-\ntors in Computing Systems . ACM, 2020, pp. 610:1–\n610:13.\n[50] S. Rau, F. Heyen, S. Wagner, and M. Sedlmair, “Visual-\nization for AI-assisted composing,” in Proceedings of\nthe 23rd International Society for Music Information\nRetrieval Conference . ISMIR, 2022, pp. 151–159.\n[51] Y . Zhang, G. Xia, M. Levy, and S. Dixon, “COS-\nMIC: A conversational interface for human-AI music\nco-creation,” in Proceedings of the 21th International\nConference on New Interfaces for Musical Expression .\nnime.org, 2021.\n[52] Y . Zhou, Y . Koyama, M. Goto, and T. Igarashi, “Gen-\nerative melody composition with human-in-the-loop\nbayesian optimization,” in Proceedings of the 2020\nJoint Conference on AI Music Creativity . DiV A.org,\n2020.\n[53] ——, “Interactive exploration-exploitation balancing\nfor generative melody composition,” in Proceedings of\nthe 26th International Conference on Intelligent User\nInterfaces . ACM, 2021, pp. 43–47.\n[54] P. C. Wright and A. F. Monk, “The use of think-aloud\nevaluation methods in design,” ACM SIGCHI Bulletin ,\nvol. 23, no. 1, pp. 55–57, 1991.[55] O. Alhadreti and P. J. Mayhew, “Rethinking thinking\naloud: A comparison of three think-aloud protocols,”\ninProceedings of the 2018 ACM SIGCHI Conference\non Human Factors in Computing Systems . ACM,\n2018, p. 44.\n[56] A. Strauss and J. Corbin, Basics of Qualitative Re-\nsearch: Grounded Theory Procedures and Techniques .\nNewbury Park, CA: Sage Publications, 1990.\n[57] J. O. Talton, D. Gibson, L. Yang, P. Hanrahan, and\nV . Koltun, “Exploratory modeling with collabora-\ntive design spaces,” ACM Transactions on Graphics ,\nvol. 28, no. 5, pp. 1–10, 2009.\n[58] C. Lynch, K. D. Ashley, N. Pinkwart, and V . Aleven,\n“Concepts, structures, and goals: Redeﬁning ill-\ndeﬁnedness,” International Journal of Artiﬁcial Intelli-\ngence in Education , vol. 19, no. 3, pp. 253––266, 2009.\n[59] H. Yakura, Y . Koyama, and M. Goto, “Tool- and\ndomain-agnostic parameterization of style transfer ef-\nfects leveraging pretrained perceptual metrics,” in Pro-\nceedings of the 30th International Joint Conference on\nArtiﬁcial Intelligence . IJCAI, 2021, pp. 1208–1216.\n[60] L. Tweedie, “Interactive visualisation artifacts: How\ncan abstractions inform design?” in Proceedings of\nthe 10th BCS Conference on Human-Computer Inter-\naction . Cambridge University Press, 1995, pp. 247–\n265.\n[61] M. A. Terry and E. D. Mynatt, “Side views: Persis-\ntent, on-demand previews for open-ended tasks,” in\nProceedings of the 15th Annual ACM Symposium on\nUser Interface Software and Technology . ACM, 2002,\npp. 71–80.\n[62] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph,\nS. Borgeaud, D. Yogatama, M. Bosma, D. Zhou,\nD. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals,\nP. Liang, J. Dean, and W. Fedus, “Emergent abilities\nof large language models,” Transactions on Machine\nLearning Research , vol. in press , 2022.\n[63] L. van der Maaten and G. Hinton, “Visualizing data\nusing t-SNE,” Journal of Machine Learning Research ,\nvol. 9, no. 86, pp. 2579–2605, 2008.\n[64] J. Oppenlaender, “A taxonomy of prompt modiﬁers for\ntext-to-image generation,” arXiv , vol. abs/2204.13988,\n2022.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n137"
    },
    {
        "title": "Harmonic Analysis With Neural Semi-CRF.",
        "author": [
            "Qiaoyu Yang",
            "Frank Cwitkowitz",
            "Zhiyao Duan"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265379",
        "url": "https://doi.org/10.5281/zenodo.10265379",
        "ee": "https://zenodo.org/records/10265379/files/000080.pdf",
        "abstract": "Automatic harmonic analysis of symbolic music is an important\nand useful task for both composers and listeners.\nThe task consists of two components: recognizing harmony\nlabels and finding their time boundaries. Most of the\nprevious attempts focused on the first component, while\ntime boundaries were rarely modeled explicitly. Lack of\nboundary modeling in the objective function could lead to\nsegmentation errors. In this paper, we introduce a novel\napproach named Harana, to jointly detect the labels and\nboundaries of harmonic regions using neural semi-CRF\n(conditional random field). In contrast to rule-based scores\nused in traditional semi-CRF, a neural score function is\nproposed to incorporate features with more representational\npower. To improve the robustness of the model to\nimperfect harmony profiles, we design an additional score\ncomponent to penalize the match between the candidate\nharmony label and the absent notes in the music. Quantitative\nresults from our experiments demonstrate that the proposed\napproach improves segmentation quality as well as\nframe-level accuracy compared to previous methods.",
        "zenodo_id": 10265379,
        "dblp_key": "conf/ismir/YangCD23",
        "keywords": [
            "harmonic analysis",
            "symbolic music",
            "composers",
            "listeners",
            "two components",
            "recognizing harmony labels",
            "finding their time boundaries",
            "previous attempts",
            "explicit modeling",
            "segmentation errors"
        ],
        "content": "HARMONIC ANALYSIS WITH NEURAL SEMI-CRF\nQiaoyu Yang Frank Cwitkowitz Zhiyao Duan\nUniversity of Rochester\n{qyang15, fcwitkow}@ur.rochester.edu, zhiyao.duan@rochester.edu\nABSTRACT\nAutomatic harmonic analysis of symbolic music is an im-\nportant and useful task for both composers and listeners.\nThe task consists of two components: recognizing har-\nmony labels and ﬁnding their time boundaries. Most of the\nprevious attempts focused on the ﬁrst component, while\ntime boundaries were rarely modeled explicitly. Lack of\nboundary modeling in the objective function could lead to\nsegmentation errors. In this paper, we introduce a novel\napproach named Harana , to jointly detect the labels and\nboundaries of harmonic regions using neural semi-CRF\n(conditional random ﬁeld). In contrast to rule-based scores\nused in traditional semi-CRF, a neural score function is\nproposed to incorporate features with more representa-\ntional power. To improve the robustness of the model to\nimperfect harmony proﬁles, we design an additional score\ncomponent to penalize the match between the candidate\nharmony label and the absent notes in the music. Quantita-\ntive results from our experiments demonstrate that the pro-\nposed approach improves segmentation quality as well as\nframe-level accuracy compared to previous methods. The\nsource code used in this paper is available on GitHub1.\n1. INTRODUCTION\nIn music, harmony is the sound resulted from two or more\npitches being performed together. It is the vertical aspect of\nmusic [1], and is essential for both music creation and per-\nception. During music analysis, a harmony label is often\nassigned to a music segment that is harmonically coher-\nent. Many composers use harmonic progressions to set up\na musical template in which texture could then be ﬁlled [2].\nFor listeners, harmonic structure is a crucial mid-level rep-\nresentation of music that can inﬂuence the perception of\nother music elements such as melody and rhythm [3].\nThe task of harmonic analysis aims to ﬁnd the correct\nsegmentation of a music piece and to identify the corre-\nsponding label for each segmented region. These two goals\nare closely related. Regions with strong conﬁdence of a\ncandidate harmony label tend to possess the boundaries of\n1https://github.com/QiaoyuYang/harana\n© Q. Yang, F. Cwitkowitz, and Z. Duan. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Q. Yang, F. Cwitkowitz, and Z. Duan, “Harmonic Analysis\nwith Neural Semi-CRF”, in Proc. of the 24th Int. Society for Music\nInformation Retrieval Conf., Milan, Italy, 2023.a true segmentation [4]. On the other hand, the oracle seg-\nmentation could help the prediction of the true underlying\nharmony for the notes in each region [4]. Therefore, to\nachieve successful analysis of harmony, both of the two\ngoals as well as their relationship should be considered.\nTargeting the two indispensable components of har-\nmonic analysis simultaneously, we propose an approach\nto jointly predict the boundaries and labels of harmonic re-\ngions using neural semi-Markov conditional random ﬁeld\n(semi-CRF). It is well-known that the harmonic regions in\nmusic do not always share the same length [5]. Compared\nto conventional sequence labeling models, semi-CRF is\nmore suitable for the task because it allows for various\nlengths among the labeled regions [6].\nIn the original setting of semi-CRF, a score is com-\nputed in each segmented region using the weighted sum\nof rule-based features [6]. However, rule-based features\nare bounded by pre-deﬁned rules and might not exploit\nthe interaction between notes and other intermediate mu-\nsic representations deeply enough. To solve this problem,\nwe design a neural scoring function that ﬁrst estimates\nthe frame-level harmony distributions using a neural net-\nwork and then adapts them to candidate harmony labels\nwith an attention mechanism. The attention mechanism\ncould make the scoring module more efﬁcient by concen-\ntrating on sub-regions that are more harmonically related\nto the candidate label. In addition, an absence score is\nadded to the scoring function to improve the robustness\nof the model to imperfect harmony proﬁles of the music.\nThrough experiments we ﬁnd that the proposed architec-\ntural components collectively yield improvement on both\nsegmentation quality and harmony labels accuracy. We fo-\ncus on MIDI-like symbolic music input in our experiments\nbut the method could be easily adapted to audio.\nIn summary, our contributions include:\n• Proposing the ﬁrst neural semi-CRF model to jointly\nestimate harmony labels and their time boundaries;\n• Proposing an attention-based score function to alle-\nviate the inﬂuence of extra non-chordal notes and\nmissing chordal notes; and\n• Proposing a novel absence score to improve the ro-\nbustness to imperfect harmony proﬁles.\n2. RELATED WORKS\nDue to the importance of harmony in music, a substantial\namount of automatic systems have been designed for har-676monic analysis. Early systems tended to focus on using\nmusic audio as input and apply domain knowledge from\nmusic theory. To encode the audio waveform, a time-\nfrequency representation, or spectrogram, is usually ex-\ntracted using the short-time Fourier Transform. Then, with\nthe observation that it is the pitch class of notes rather than\nthe absolute pitch height that affects the harmonic content,\na common practice is to reduce the spectrogram to a chro-\nmagram with 12 bins corresponding to the 12 pitch classes.\nIn the decoding stage, the chromagram can be matched to\npredeﬁned chord proﬁles [7, 8] or made to emit explicit\nlabels using probabilistic models such as hidden Markov\nmodel (HMM) [9–11] or CRF [11].\nWith the increasing popularity of deep learning in the\npast decade, end-to-end models based on deep neural\nnetworks have received extensive attention [12–16]. To\nmodel the temporal evolution of music context, Boulanger-\nLewandowsk et al. extracted audio features using a re-\ncurrent neural network (RNN) [17]. To better aggregate\ncontext information and learn intermediate representations\nwith a temporal hierarchy, Zhou and Lerch used a convo-\nlutional neural network (CNN) with low-pass ﬁlters [12].\nMcFee and Bello further combined CNN and RNN in the\nfeature encoder for chord recognition [13]. As a pow-\nerful attention-based architecture designed for long-term\nsequence modeling, transformers have also been incor-\nporated in some recent approaches to harmonic analysis\n[14, 15].\nWhile the harmonic progression or context informa-\ntion can be modeled with various techniques, the majority\nof existing methods do not directly optimize for region-\nlevel output. Some methods adopt a two-stage approach,\nwhere the ﬁrst stage outputs frame-level chord labels and\nthe second stage smooths frame-level labels with post-\nprocessing [9–11, 18–20]. However, different from other\nsimple sequence labeling tasks such as part-of-speech tag-\nging, a harmonic label could correspond to a region span-\nning multiple frames. Although temporal smoothing by\nHMM or CRF regresses some sporadically outliers back\nto the harmonic streams, these models could still suffer\nfrom segmentation errors. Masada and Bunescu relaxed\nthe constraint on ﬁxed-size time-span of the output predic-\ntion [21]. They used a generalized variant of CRF, semi-\nCRF, to jointly detect chord labels and their boundaries.\nHowever, the features to the semi-CRF are entirely rule-\nbased, which means they are not necessarily optimal for\nthe end task. In this work, we build on the semi-CRF\nframework and explore neural features and scoring tech-\nniques that are jointly optimized for the end task - harmony\nlabeling and boundary prediction.\n3. METHODS\nIn our proposed model, Harana, we ﬁrst estimate the har-\nmony (including root, quality, and pitch activation in this\nwork) at the frame-level; then we aggregate the frame-level\nestimation into region-level segment scores based on can-\ndidate segments; ﬁnally, we use semi-CRF to ﬁnd the bestsegmentation candidate and its corresponding labels. We\nfocus on symbolic music input in our experiments. The\nfollowing subsections describe the model in detail.\n3.1 Data Representation\n3.1.1 Symbolic Music Input\nGiven a symbolic music piece, we slice it into short frames\nof one eighth of a beat long. We use beat instead of note\nduration in order to represent the basic time unit because\nmusic with different meters may have different distribu-\ntions on the note length. The pitch information in each\nframe is summarized with a 12-d pitch class distribution\nvector, which describes the normalized distribution of the\nduration of each pitch class in the frame. To help distin-\nguish between harmonies with the same pitch class vector,\nwe also include the bass note (the lowest note) in the input\nto the model; it is represented as a 12-d one-hot vector in-\ndicating the bass pitch class in each frame. Combining the\npitch class distribution and the bass note, the input to the\nmodel is a sequence of 24-d vectors.\n3.1.2 Harmony\nA popular representation of music harmony in symbolic\nmusic is the Roman numeral encoding, where the full har-\nmonic context of a label, including tonic and degree, is\nconsidered [22]. However, the combination of all the com-\nponents produces 47k different harmony labels, which are\nintractable for a classiﬁcation model with limited training\ndata. A possible solution is to classify each harmony com-\nponent independently, but this is incompatible with semi-\nCRF because the boundary of each component must be the\nsame. As a compromise, we use a subset of the harmony\ncomponents, root and quality, and model them jointly.\nThe root is represented as a 12-d one-hot vector cor-\nresponding to the 12 pitch classes. The quality is repre-\nsented as a 10-d one-hot vector corresponding to 10 com-\nmonly used classes. In addition to root and quality, we\nuse another harmony representation, the pitch class activa-\ntion vector, in the neural score function. Previous works\nhave shown its effectiveness as a label encoding for har-\nmonic analysis [13]. These vectors are 12-d multi-hot and\nare circularly shifted from the pitch-class activation vectors\nrooted at C.\n3.2 Semi-CRF\nSemi-CRF is a probabilistic model for sequence labeling\nwith a variable label-span. Given a sequence of input\nframesX=⟨X1,X2,...,XN⟩, semi-CRF provides the\nconditional probability of the sequence of contiguous non-\noverlapping labeled segments Y=⟨Y1,Y2,...,YK⟩, where\nNis the number of frames and Kis the number of seg-\nments. Since the labeled segments could span multiple\nframes, they are represented as three-dimensional tuples\nYi= (ui,vi,li), whereui,viandlirespectively denote\nthe onset, offset and label of the segment. In the context\nof harmonic analysis, Xrepresents the input music frames\nandYrepresents the harmonic regions.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n677Figure 1 : The semi-CRF architecture in the context of harmonic analysis. The total score is computed from music input and\na set of candidate harmony segments. Numbers in the blue squares are the frame indices. Numbers in the green rectangles\nare the indices of candidate harmony segments.\nThe conditional probability given by semi-CRF takes\nthe form of\nP(Y|X) =eWF(Y,X)\nZ(X), (1)\nwhereFis a feature vector computed from XandY,W\nis a learnable weight matrix, and Z=/summationtext\nYeWF(Y,X)is\na normalization factor summarizing all possible segmenta-\ntion and labeling of the input sequence. In this work, we\npropose to generalize the weighted feature score to a neural\nscore function S(Y,X)so that\nP(Y|X) =eS(Y,X)\nZ(X). (2)\nWith the assumption that the harmony labels are Marko-\nvian given the music input, the score function could be de-\ncomposed into the sum of segment-level scores that are de-\npendent only on the current and the previous segments.\nS(Y,X) =K/summationdisplay\ni=1Si(Yi,X;Yi−1). (3)\nTo simplify the notation, we treat Yi−1as a parameter for\nthei-th segment’s score function and omit it in the follow-\ning sections. Figure 1 demonstrates the structure of semi-\nCRF in the context of music harmonic analysis.\n3.3 Frame-Level Estimation\nFollowng Micci et al. [23], the frame-level estimation of\nharmony information is achieved with a DenseNet-GRU\narchitecture. The DenseNet-GRU module is followed by\nfully connected layers and ﬁnally the vectors correspond-\ning to different types of harmony information are estimated\nusing separate linear heads. The softmax function is used\nto produce the class distributions of the root and the qual-\nity, whereas sigmoid is used to ﬁnd the activation of each\npitch class. Mathematically, the computation of frame-\nlevel harmony estimation can be formulated as\nE(n) =MLP(GRU(DenseNet (Xn))),\nˆDR(n) =Softmax(FCR(E(n))),\nˆDQ(n) =Softmax(FCQ(E(n))),\nˆPC(n) =Sigmoid(FCPC(E(n))),(4)whereXnis thenthframe of the input music. ˆDR(n),\nˆDQ(n)andˆPC(n)represent the root distribution, quality\ndistribution and the pitch class activations of the estimated\nharmony for a frame.\n3.4 Attention-Based Score Function\nAs described in Eq. (3), the CRF model evaluates possible\nsequences of harmony labels and their segmentation. For\neach segment, i.e., a candidate harmony region, we need to\naggregate the frame-level harmony information (root, qual-\nity and pitch activation) computed from Eq. (4). A simple\nmethod would be taking the average or the mode, but we\nnote that a harmonic region is not likely to contain homo-\ngeneous harmonic content. In order to dynamically weigh\nthe harmonic importance of each frame within a region, an\nattention module is proposed to focus on the frames that\nare most similar to the candidate harmony label. In partic-\nular, the scaled dot-product attention [24] is used:\nA(Q,K,V) =/summationtextN\ni=1QTKiVi√\nd, (5)\nwhereQis the query vector, Kis the key sequence, Vis\nthe value sequence and dis the vector size.\nIn the context of our model, the estimated frame-level\nharmony sequence of a candidate harmony region serves\nas both the key and value while the candidate region-level\nharmony itself is the query. Then, the candidate-informed\n(CI) estimation can be computed as\nˆHCI(Yi) =A(H(li),ˆH(ui:vi),ˆH(ui:vi)),(6)\nwhereliis thei-th candidate harmony label, and H(li)is\nits harmony representation, which can be root DR, quality\nDQor pitch class activation PC as deﬁned in Eq. (4).\nVariables uiandviare the ﬁrst and last frames of the ith\nharmonic region Yi, andˆH(ui:vi)is the vector sequence\nof estimated frame-level harmony representations from the\nmusic input.\nNow that we have a single embedding vector to summa-\nrize the harmonic content in the i-th candidate region, the\nscore of assigning the candidate harmony label lito this\nregion can be described by the similarity between the can-\ndidate harmony label embedding H(li)and the candidate-\ninformed music embedding ˆHCI(Yi). Dot product is usedProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n678Figure 2 : The proposed pipeline of the neural encoder and scoring function.\nto calculate the similarity:\nSH\ni(Yi,X) =H(li)TˆHCI(Yi). (7)\nTo further model the transition probability between adja-\ncent harmony labels and enforce more inductive bias in de-\ncoding, a transition score between segments is computed:\nST\ni(Yi) =T[li−1,li]+(vi−ui)T[li,li], (8)\nwhereTis the transition matrix containing log-\nprobabilities of harmony transitions at the frame level. It is\npre-computed from the ground-truth labels in the training\ndata.\nCombining the similarity score and the transition score,\nthe score function of a candidate harmony region is\nSi(Yi,X) =/summationdisplay\nHSH\ni(Yi,X)+λST\ni(Yi), (9)\nwhereλis a hyperparameter to balance the two score com-\nponents. Figure 2 illustrates the overall structure of the\nneural front end and the scoring function.\n3.5 Absence Score\nIn Eq. (7), the comparison between the candidate-informed\nmusic embedding ˆHCIwith the candidate harmony repre-\nsentation Hindicates the likelihood of the candidate har-\nmony. However, this comparison may not be robust when\nthere are many non-chordal notes or missing chordal notes\nin the estimation. In this case, the estimated class distribu-\ntionsˆDRandˆDQin Eq. (4) would be relatively ﬂat and\nthe pitch class activation vector ˆPCwould not align well\nwith a chord template. In other words, the neural front-end\nmay not sufﬁciently suppress non-chordal notes and rec-\nognize missing chordal notes to produce class distributions\ndiscriminative enough for the semi-CRF to decode the har-\nmony. To improve the robustness of the model to such\nissues, we introduce an absence score to allow the model\nto ﬁlter out pitch activations that are not active within the\ninput music, the majority of which represent non-chordal\nnotes that should not intersect with chordal notes of the un-\nderlying harmony. To compute the absence score, the com-\nplement of the input pitch class vector is sent to the neuralfront-end. That means the input to Eq. (4) is transformed\nby\nXn[1 : 12] = 1 −Xn[1 : 12]. (10)\nThe harmony information estimated from the inactive mu-\nsicˆHinactare then compared with the candidate harmony\nvectorsH(li). The similarity between them should be min-\nimized. In summary, the absence score of a candidate har-\nmony region is\nASH\ni(Yi,X) =−H(li)TˆHinact\nCI(Yi). (11)\nWhen the absense score is used, the complete score func-\ntion becomes\nSi(Yi,X) =\n/summationdisplay\nHSH\ni(Yi,X)+ASH\ni(Yi,X)+λST\ni(Yi),(12)\n3.6 Optimization\nFor training, both the input music frames and the ground-\ntruth harmony label segments are provided. The goal is\nto update the model parameters such that the probability\ncomputed in Eq. (2) is maximized. This is equivalent to\nminimizing the negative log likelihood (NLL) loss:\nNLL(θ) =−logPθ(Y|X)\n=log(Zθ(X))−Sθ(Y,X),(13)\nwhereθare the model parameters. We then compute the\ngradient of the loss with respect to the parameters to train\nour model using gradient descent.\nDuring inference, where only the input music frames\nare provided, the goal becomes ﬁnding the correct seg-\nmentation and the corresponding labels that maximize the\nprobability P(Y|X). Since the normalization factor as a\nsum of exponential scores stays positive, maximizing the\nscore function S(Y,X)sufﬁces to decode the segments\nand labels.\nIn both training and inference, we used the algorithms\nbased on dynamic programming proposed in the original\nsemi-CRF paper to expedite the optimization process [6].Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6794. EXPERIMENTS\n4.1 Data\nA collection of datasets from various sources [22, 25–27]\norganized by Micchi et al. [28] is used to train and evaluate\nthe proposed architecture. Table 1 summarizes the statis-\ntics of the data included in our experiments. MusPy [29] is\nused to read the compressed MusicXML ﬁles and a parser\nadapted from [28] is employed to handle the proposed data\nrepresentations. To increase the size of the dataset and help\nalleviate possible data imbalance, each piece is transposed\nto 12 different keys. The dataset is split into disjoint sub-\nsets for training and testing with a 2:1 split.\n4.2 Implementation Details\nFollowing the original paper for faster training [30],\nDenseNet is implemented in three separate blocks. 1-D\nconvolution along the time frame dimension is used in\neach convolutional layer. Guided by the observation that\nharmony changes usually occur on average at a lower fre-\nquency than the frame rate, pooling layers are added be-\ntween blocks to reduce the temporal resolution of the har-\nmony output.\nTo ensure continuity and completeness of harmony re-\ngions in the training samples, we force the sample bound-\naries to be aligned with measure boundaries. A sample\nis chosen as 96 frames because it is divisible by all the\ncommon measure lengths existed in the dataset. Addition-\nally, to avoid over-sampling from music pieces with longer\nlength, the piece index is sampled uniformly ﬁrst before a\nmusic sample is selected from the piece.\nThe entire pipeline is implemented using PyTorch.\nAdam optimizer is applied with learning rate of 10−4and\nweight decay of 10−2. Dropout with rate 0.2 is added be-\ntween GRU layers and after each hidden fully-connected\nlayer to avoid over-ﬁtting. The λin Eq. (12) is chosen\nempirically to be 0.001.\n4.3 Evaluation Metrics\nThe task of music harmony analysis is two fold: recogniz-\ning the correct labels and ﬁnding the correct segmentation\ncorresponding to the labels. To obtain a full picture of the\nmodel performance, we used two types of evaluation met-\nrics to assess both aspects of the task.\nFirst, the frame-level accuracy is computed for both root\nand quality. The accuracy on a reduced dictionary of qual-\nity including only major and minor is also reported due to\nPieces Crotchet Chord Annotations\nBPSFH 32 23554 8615\nRoman Text 82 18208 7935\nTavern 27 20673 10723\nLopez 180 31367 16666\nTable 1 : Summary of statistics of the datasets.its prevalence in the literature and adequacy in many prac-\ntical uses. During training, the accuracy is computed at\nthe sample level. During inference, the result is averaged\nacross all frames in a song.\nThe other evaluation metric focuses on the segmenta-\ntion quality of the output. We use the standard segmenta-\ntion scores from the mir_eval package [31,32]. The scores\nare based on directional Hamming distance and consider\nthe overlap between the estimated harmony intervals and\nthe ground-truth intervals. The directional Hamming dis-\ntance between the set of estimated intervals ˆI={ˆIi}=\n{[ˆui,ˆvi]}and the set of ground-truth intervals I={Ii}is\ncomputed as the following:\nDHD(ˆI,I) =/summationtext\nˆIi∈ˆI(|ˆIi|−maxIj∈I|ˆIi∩Ij|))\n/summationtext\nˆIi∈ˆI|ˆIi|.\n(14)\nWhen a harmony boundary is missing from the estima-\ntion, an estimated harmony interval overlaps with mul-\ntiple ground-truth intervals, but the maximum overlap is\nbounded by the length of the ground-truth intervals, leav-\ning a large portion of the estimated interval not sub-\ntracted hence a large distance value. Therefore, a large\nDHD(ˆI,I)often indicates under-segmentation, while a\nlargeDHD(I,ˆI)often indicates over-segmentation. To\nsummarize the two directional distances in a single metric,\nthe overall segmentation quality score is computed as\nSQ= 1−max(DHD(I,ˆI),DHD(ˆI,I)). (15)\n4.4 Baseline Models\nThree baseline models are included in our experiments\nto demonstrate the performance improvement of our pro-\nposed method. The chosen baselines are all relevant to our\nmodel by sharing parts of the architecture. Since the neu-\nral front-end of Harana is CRNN, we ﬁrst test if a plain\nCRNN model [23] could achieve comparable results. A\nsecond baseline model, frog [28], also relies on CRNN to\nextract music features. In contrast to our model, it uses a\nneural autoregressive distribution estimator (NADE) to de-\ncode the harmony label. At the decoding stage, it deﬁnes\nan order of the harmony components and iteratively predict\nthe next component conditioned on the current component.\nThe same output harmony categories of root and quality\noutput are considered in the NADE decoder. A third base-\nline worth comparing to is the rule-based semi-CRF pro-\nposed by Masada and Bunescu [21]. It uses handcrafted\nrules as features to compute the segment scores in semi-\nCRF. For simplicity, we implemented the two most impor-\ntant features, chord coverage and segment purity, in our\nexperiment. Chord coverage measures what percentage of\nchordal notes are covered by the music segment while seg-\nment purity describes what proportion of notes in the music\nsegment are indeed chordal notes.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n680Model Root Acc Quality Acc Overall Acc Under Seg Over Seg Overall Seg\nHarana 0.744 0.743 0.651 0.722 0.747 0.649\nHarana - no semi-CRF 0.732 0.715 0.634 0.678 0.740 0.639\nHarana - no Attention Fusing 0.741 0.738 0.650 0.716 0.749 0.645\nHarana - no Absence Score 0.743 0.746 0.643 0.719 0.748 0.650\nTable 2 : The result of ablation studies summarizing the effect of removing each proposed component of the model on both\nframe-level accuracy and segmentation quality.\nModel Root Quality Majmin Overall\nCRNN 0.735 0.714 0.865 0.634\nfrog 0.733 0.542 0.815 0.459\nRuleSCRF 0.684 0.645 0.847 0.600\nHarana 0.744 0.743 0.886 0.651\nTable 3 : The frame-level accuracy for different models.\n5. RESULTS\n5.1 Frame-Level Accuracy\nTable 3 shows the result on frame-level accuracy. It can be\nseen that Harana outperforms the baseline models on all\nthe measures. The large gap between Harana and the rule-\nbased semi-CRF model demonstrates the value of a neural\nscore function. Without a neural front-end, the rule-based\nmodel even has weaker performance than the plain CRNN.\nWe also notice that frog has lower accuracy than the plain\nCRNN model. While the autoregressive decoding in frog\ncould help enforce coherence between harmony compo-\nnents, it may require the full spectrum of the harmony com-\nponents including key and degree. However, only root and\nquality were used in our experiments. Complete harmony\ninformation is difﬁcult to collect so we believe Harana has\na greater potential to leverage larger datasets in the future.\n5.2 Segmentation Quality\nAs shown in Table 4, Harana provides improvement on\nsegmentation quality compared to other models. Higher\nunder-segmentation score of Harana means there are fewer\nmissing boundaries in the estimation. Higher over-\nsegmentation score shows that most detected boundaries\nare indeed true boundaries. An interesting observation is\nthat the rule-based semi-CRF yields the most severe under-\nsegmentation even though it is optimized on the segmen-\ntation boundaries. The reason for this might be that rule\nbased-features are unable to clean noises such as the non-\nchordal notes and missing chordal notes in the input music\nbut directly compute features from them. The noise in the\nfeatures of short regions may be confused with the intrinsic\nnoise of longer regions.\n5.3 Ablation Studies\nTo show the effectiveness of each component of the ar-\nchitecture, we conduct additional ablation studies by re-\nmoving each component. Table 2 summarizes the results.Model Under Seg Over Seg Overall\nCRNN 0.681 0.738 0.639\nfrog 0.681 0.724 0.624\nRuleSCRF 0.666 0.741 0.625\nHarana 0.722 0.747 0.649\nTable 4 : The segmentation quality for different models.\nWe can see that the full architecture achieves the best re-\nsult overall. Among the missing components, semi-CRF\nleads to the largest performance drop. That conﬁrms semi-\nCRF is an indispensable component to capture boundary\ninformation in harmony analysis. The attention module,\nalthough also helpful, produces relatively smaller perfor-\nmance gain. It is expected because after the neural front-\nend, the frame-level estimations to be aggregated may be\nalready harmonically coherent; The attention module only\nhelps to focus on the most representative frames. The\neffect of removing the absence score is less signiﬁcant.\nWithout it, the quality accuracy and overall segmentation\nquality even slightly improved. The phenomenon could\nresult from the more difﬁcult training objective. Inactive\npitch class activations of the input music are an extreme\nscenario of noisy harmonic information. More data and a\nlarger neural front-end might be needed to fully leverage\nthe advantage of the absence score [33].\n6. CONCLUSIONS\nIn this paper, we proposed an automated approach for har-\nmonic analysis based on neural semi-CRF to jointly seg-\nment the harmonic regions and predict the labels. We de-\nveloped a neural encoder and an attention mechanism to\nreplace the conventional rule-based score function. We\nfurther proposed an absense score to improve the model\nrobustness to imperfect harmony proﬁles. Experiments\nshowed that our proposed architecture improves the per-\nformance on both frame-level accuracy and segmentation\nquality. Although our experiments focused on music in-\nput of symbolic format, the architecture could be adapted\nto audio input by simple modiﬁcations on the neural front-\nend. One limitation of the semi-CRF architecture is that\nit has quadratic time complexity with respect to sequence\nlength so it is difﬁcult to train the model on very long se-\nquences. To capture the long-term dependency of harmony\nprogression, more efﬁcient sequence modeling methods\ncould be explored in the future.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n6817. ACKNOWLEDGEMENTS\nThis work is partially supported by National Science\nFoundation grants No. 1846184 and 2222129. Frank\nCwitkowitz would like to thank the synergistic activities\nfunded by NSF grant DGE-1922591.\n8. REFERENCES\n[1] S. Kostka, D. Payne, and B. Almén, Tonal harmony .\nMcGraw-Hill Higher Education, 2012.\n[2] S. Bennett, “The process of musical creation: Inter-\nviews with eight composers,” Journal of Research in\nMusic Education , vol. 24, no. 1, pp. 3–13, 1976.\n[3] W. F. Thompson, “Modeling perceived relationships\nbetween melody, harmony, and key,” Perception Psy-\nchophysics , vol. 53, no. 1, pp. 13–24, 1993.\n[4] B. Pardo and W. P. Birmingham, “Algorithms for\nchordal analysis,” Computer Music Journal , vol. 26,\nno. 2, pp. 27–49, 2002.\n[5] J. Pauwels, K. O’Hanlon, E. Gómez, and M. Sandler,\n“20 years of automatic chord recognition from audio,”\ninInt. Society of Music Information Retrieval Conf. ,\n2019, pp. 54–63.\n[6] S. Sarawagi and W. W. Cohen, “Semi-Markov con-\nditional random ﬁelds for information extraction,”\ninConf. on Neural Information Processing Systems ,\n2004.\n[7] T. Fujishima, “Real-time chord recognition of musical\nsound: A system using common lisp music,” in Int.\nComputer Music Conf. , 1999.\n[8] C. Harte and M. Sandler, “Automatic chord identifca-\ntion using a quantised chromagram,” in Audio Engi-\nneering Society Convention , 2005.\n[9] A. Sheh and D. P. Ellis, “Chord segmentation and\nrecognition using em-trained hidden Markov models,”\ninInt. Society of Music Information Retrieval Conf. ,\n2003, pp. 185–191.\n[10] J. P. Bello and J. Pickens, “A robust mid-level repre-\nsentation for harmonic content in music signals,” in Int.\nSociety of Music Information Retrieval Conf. , 2005, pp.\n304–311.\n[11] J. A. Burgoyne, L. Pugin, C. Kereliuk, and I. Fujinaga,\n“A cross-validated study of modelling strategies for au-\ntomatic chord recognition in audio,” in Int. Society of\nMusic Information Retrieval Conf. , 2007, pp. 251–254.\n[12] X. Zhou and A. Lerch, “Chord detection using deep\nlearning,” in Int. Society of Music Information Re-\ntrieval Conf. , 2015.\n[13] B. McFee and J. P. Bello, “Structured training for large-\nvocabulary chord recognition,” in Int. Society of Music\nInformation Retrieval Conf. , 2017, pp. 188–194.[14] J. Park, K. Choi, S. Jeon, D. Kim, and J. Park, “A bi-\ndirectional transformer for musical chord recognition,”\ninInt. Society of Music Information Retrieval Conf. ,\n2019.\n[15] T.-P. Chen and L. Su, “Harmony transformer: Incorpo-\nrating chord segmentation into harmony recognition,”\ninInt. Society of Music Information Retrieval Conf. ,\n2019.\n[16] J. Jiang, K. Chen, W. Li, and G. Xia, “Large-\nvocabulary chord transcription via chord structure de-\ncomposition,” in Int. Society of Music Information Re-\ntrieval Conf. , 2019, pp. 644–651.\n[17] N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent, “Audio chord recognition with recurrent neural\nnetworks,” in Int. Society of Music Information Re-\ntrieval Conf. , 2013, pp. 335–340.\n[18] F. Korzeniowski and G. Widmer, “A fully convolu-\ntional deep auditory model for musical chord recogni-\ntion,” in Int. Workshop on Machine Learning for Signal\nProcessing , 2016, pp. 1–6.\n[19] Y . Wu and W. Li, “Automatic audio chord recognition\nwith MIDI-trained deep feature and BLSTM-CRF se-\nquence decoding model,” in Int. Workshop on Machine\nLearning for Signal Processing , 2019, p. 355–366.\n[20] J. Park, K. Choi, S. Jeon, D. Kim, and J. Park, “A bi-\ndirectional transformer for musical chord recognition,”\ninInt. Society of Music Information Retrieval Conf. ,\n2019.\n[21] K. Masada and R. C. Bunescu, “Chord recognition in\nsymbolic music using semi-Markov conditional ran-\ndom ﬁelds,” in Int. Society of Music Information Re-\ntrieval Conf. , 2017, pp. 272–278.\n[22] D. Tymoczko, M. Gotham, M. S. Cuthbert, and\nC. Ariza, “The romantext format: A ﬂexible and stan-\ndard method for representing roman numeral analy-\nses,” in Int. Society of Music Information Retrieval\nConf. , 2019.\n[23] G. Micchi, M. Gotham, and M. Giraud, “Not all roads\nlead to rome: Pitch representation and model archi-\ntecture for automatic harmonic analysis,” Trans. of the\nInternational Society for Music Information Retrieval ,\nvol. 3, no. 1, pp. 42–54, 2020.\n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Łukasz Kaiser, and I. Polo-\nsukhin, “Attention is all you need,” in Conf. on Neural\nInformation Processing Systems , 2017.\n[25] T.-P. Chen and L. Su, “Functional harmony recogni-\ntion of symbolic music data with multi-task recurrent\nneural networks,” in Int. Society of Music Information\nRetrieval Conf. , 2018, pp. 90–97.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n682[26] J. Devaney, C. Arthur, N. Condit-Schultz, and\nK. Nisula, “Theme and variation encodings with ro-\nman numerals (tavern): A new data set for symbolic\nmusic analysis,” in Int. Society of Music Information\nRetrieval Conf. , 2015.\n[27] N. N. López, Automatic harmonic analysis of classical\nstring quartets from symbolic score . Doctoral disser-\ntation, Universitat Pompeu Fabra, 2017.\n[28] G. Micchi, K. Kosta, G. Medeot, and P. Chanquion,\n“A deep learning method for enforcing coherence in\nautomatic chord recognition,” in Int. Society of Music\nInformation Retrieval Conf. , 2017, pp. 443–451.\n[29] H.-W. Dong, K. Chen, J. McAuley, and T. Berg-\nKirkpatrick, “Muspy: A toolkit for symbolic music\ngeneration,” in Int. Society of Music Information Re-\ntrieval Conf. , 2020.\n[30] G. Huang, Z. Liu, L. V . D. Maaten, and K. Q. Wein-\nberger, “Densely connected convolutional networks,”\ninConf. on Computer Vision and Pattern Recognition ,\n2017, pp. 4700–4708.\n[31] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, and D. P. W. Ellis, “mir_eval: A\ntransparent implementation of common MIR metrics,”\ninInt. Society of Music Information Retrieval Conf. ,\n2014, pp. 367–372.\n[32] C. Harte, Towards automatic extraction of harmony in-\nformation from music signals . Doctoral dissertation,\nQueen Mary University of London, 2010.\n[33] J. Clarysse, J. Hörrmann, and F. Yang, “Why adversar-\nial training can hurt robust accuracy,” in Int. Conf. on\nLearning Representations , 2023.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n683"
    },
    {
        "title": "Singing Voice Synthesis Using Differentiable LPC and Glottal-Flow-Inspired Wavetables.",
        "author": [
            "Chin-Yun Yu",
            "György Fazekas"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.13916489",
        "url": "https://doi.org/10.5281/zenodo.13916489",
        "ee": "https://zenodo.org/records/13916489/files/000079.pdf",
        "abstract": "This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for singing voice synthesis (SVS) that exploits the physical characteristics of the human voice using differentiable digital signal processing. GOLF employs a glottal model as the harmonic source and IIR filters to simulate the vocal tract, resulting in an interpretable and efficient approach. We show it is competitive with state-of-the-art singing voice vocoders, requiring fewer synthesis parameters and less memory to train, and runs an order of magnitude faster for inference. Additionally, we demonstrate that GOLF can model the phase components of the human voice, which has immense potential for rendering and analysing singing voices in a differentiable manner. Our results highlight the effectiveness of incorporating the physical properties of the human voice mechanism into SVS and underscore the advantages of signal-processing-based approaches, which offer greater interpretability and efficiency in synthesis.",
        "zenodo_id": 13916489,
        "dblp_key": "conf/ismir/YuF23",
        "keywords": [
            "GlOttal-flow LPC Filter",
            "singing voice synthesis",
            "differentiable digital signal processing",
            "glottal model",
            "IIR filters",
            "interpretable approach",
            "efficient synthesis",
            "state-of-the-art",
            "fewer synthesis parameters",
            "less memory"
        ],
        "content": "SINGING VOICE SYNTHESIS USING DIFFERENTIABLE LPC AND\nGLOTTAL-FLOW-INSPIRED WA VETABLES\nChin-Yun Yu György Fazekas\nCentre for Digital Music, Queen Mary University of London, UK\nchin-yun.yu@qmul.ac.uk, george.fazekas@qmul.ac.uk\nABSTRACT\nThis paper introduces GlOttal-ﬂow LPC Filter (GOLF), a\nnovel method for singing voice synthesis (SVS) that ex-\nploits the physical characteristics of the human voice using\ndifferentiable digital signal processing. GOLF employs\na glottal model as the harmonic source and IIR ﬁlters to\nsimulate the vocal tract, resulting in an interpretable and\nefﬁcient approach. We show it is competitive with state-\nof-the-art singing voice vocoders, requiring fewer synthe-\nsis parameters and less memory to train, and runs an or-\nder of magnitude faster for inference. Additionally, we\ndemonstrate that GOLF can model the phase components\nof the human voice, which has immense potential for ren-\ndering and analysing singing voice in a differentiable man-\nner. Our results highlight the effectiveness of incorporating\nthe physical properties of the human voice mechanism into\nSVS and underscore the advantages of signal-processing-\nbased approaches, which offer greater interpretability and\nefﬁciency in synthesis.\n1. INTRODUCTION\nSinging voice synthesis (SVS) has attracted substantial in-\nterest as a research topic over the last decades, and a va-\nriety of techniques have been developed. Early success-\nful SVS systems were usually based on sample concatena-\ntion [1–4], while parametric systems have become much\nmore prevalent. The actual synthesis process in parametric\nsystems is carried out by a vocoder controlled by synthesis\nparameters generated from a separate acoustic model given\nsome musical context factors (i.e. note number, duration,\nphoneme, etc.). Early systems of this kind use a linear\nsource-ﬁlter model as vocoder [5, 6]. Deep Neural Net-\nworks (DNNs) have subsequently become the dominant\napproach for state-of-the-art vocoders [7–13]. However,\nmel-spectrograms are often chosen as input features to\nthese models, which are less interpretable than traditional\nvocoder parameters (e.g. f0, aperiodicity ratios). Also, a\nsigniﬁcant amount of data is needed to cover various vocal\nexpressions to achieve generalisation.\n© C.-Y . Yu and G. Fazekas. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: C.-Y . Yu and G. Fazekas, “Singing V oice Synthesis Using Differ-\nentiable LPC and Glottal-Flow-Inspired Wavetables”, in Proc. of the 24th\nInt. Society for Music Information Retrieval Conf., Milan, Italy, 2023.In contrast, Differentiable Digital Signal Processing\n(DDSP) models [14–16] incorporate existing signal pro-\ncessing operations into neural networks as an inductive\nbias, making them more interpretable and generalisable.\nDDSP additive synthesis has been proposed for SVS by\nAlonso et al. [17]. Wu et al. [18] improved this further\nby using subtractive synthesis and sawtooth as the har-\nmonic source. Nercessian et al. [19] proposed a differ-\nentiable version of the WORLD vocoder [20] for doing\nend-to-end singing voice conversion. Yoshimura et al. [21]\nused Taylor expansion to approximate the mel-log spec-\ntrum approximation ﬁlter’s (MLSA) exponential function\nand embedded it into an SVS system. However, most of\ntheir architectures only assume the target signal is a mono-\nphonic instrument, which can potentially lead to solutions\nthat do not reﬂect some properties of voice. In their de-\nsign, the harmonic sources are ﬁxed to a speciﬁc shape\n(e.g. sawtooth, pulse train), and the ﬁlters are symmetric\nin the time domain, except Yoshimura et al. [21] which use\na minimum-phase MLSA ﬁlter. Incorporating constraints\nspeciﬁc to the human voice on the harmonic source and the\nﬁlters could lead to a more interpretable and compact SVS\nvocoder.\nIn this work, we propose GlOttal-ﬂow LPC Filter\n(GOLF), an SVS module informed by the physical prop-\nerties of the human voice. We build upon the Harmonic-\nplus-Noise architecture of DDSP [14] and the subtractive\nsynthesis of SawSing [18], but replace the harmonic source\nwith a glottal model and use IIR ﬁlters. We developed\na differentiable IIR implementation in PyTorch [22] for\ntraining efﬁciency. We then used this module as a neural\nvocoder and compared its performance with other DDSP-\nbased vocoders. Speciﬁcally, a simple and lightweight NN\nencoder converts the mel-spectrogram into synthesis pa-\nrameters, and the synthesiser decodes the signal from it.\nWe paired different synthesisers with the same encoder and\ntrained them jointly.\nOur contributions are twofold. First, GOLF has signiﬁ-\ncantly fewer synthesis parameters but is still competitive\nwith state-of-the-art SVS vocoders. Second, GOLF re-\nquires less than 40% of memory to train and runs ten times\nfaster than its alternatives for inference. Moreover, we in-\ndirectly show that GOLF could model the phase compo-\nnents of the human voice by aligning the synthesised wave-\nforms to the ground truth and calculating the differences.\nThis characteristic has excellent potential for analysing\nsinging voice in a differentiable manner. Decomposing the667human voice into the glottal source and vocal tract could\nalso enable us to adjust the singing style in different ways,\nsuch as altering the amount of vocal effort with varying\nshapes of the glottal pulse.\n2. BACKGROUND\nWe ﬁrst introduce the relevant notation. xidenotes the ith\ncolumn vector and xi,jdenotes the entry at the ithrow\nand thejthcolumn of the matrix X. Concatenating two\nmatrices along the column dimension is denoted by [;].xi\ndenotes the ithentry of the vector xor a time sequence\nxindexed by i.X(z)denotes the response of xnin the\nz-domain. Unless stated otherwise, we use nas the time\nindex and kas the frame index. Angular frequencies and\nperiods are normalised to the interval [0,1]. We use one-\nbased indexing for elements with ﬁnite dimensions.\n2.1 Glottal Source-Filter Model\nIn the source-ﬁlter model, we have the following simpliﬁed\nvoice production model:\nS(z) = (G(z)+N(z))H(z)L(z), (1)\nwhereG(z)represents the periodic vibration from the vo-\ncal folds, N(z)represents random components of the glot-\ntal source, H(z)represents the vocal-tract ﬁlter, and L(z)\nrepresents the radiation at the lips [23]. Since this for-\nmulation is linear, the radiation ﬁlter L(z)and the glot-\ntal pulse G(z)can be merged into a single source G′(z)\ncalled the radiated glottal pulse . If we assume L(z)is a\nﬁrst-order differentiator 1−z−1[24], then G′(z)is the\nderivative of the glottal pulse, which can be described by\nthe LF model [25], a four-parameter model of glottal ﬂow.\nH(z)is usually a Linear Predictive Coding (LPC) ﬁlter.\n2.2 Linear Predictive Coding\nLPC assumes that the current speech sample sncan be pre-\ndicted from a ﬁnite number of previous Msamplessn−1\ntosn−Mby a linear combination with residual errors en:\nsn=en−M/summationdisplay\ni=1aisn−i, (2)\nwhereaiare the linear prediction coefﬁcients. This is\nthe same as ﬁltering the residuals, equivalent to the glottal\nsource in our case, with an Mth-order all-pole ﬁlter, a ﬁl-\nter that has an inﬁnite impulse response (IIR). We can use\nthe LPC ﬁlter to represent the response of the vocal tract\nif the vocal tract is approximated by a series of cylindri-\ncal tubes with varying diameters [26], providing a physical\ninterpretation.\nUsing LPC for neural audio synthesis is not new [8,\n27, 28], and works have been conducted to incorporate\nIIR ﬁlters and train them jointly with deep learning mod-\nels [29–35]. The difﬁculty of training IIR in deep learning\nframework (e.g. PyTorch) using Eqn (2) is that its compu-\ntation is recursive, i.e. the output at each step depends onthe previous results, and to make the calculation differen-\ntiable, separated tensors are allocated in each step. This\ngenerates a signiﬁcant number of memory allocations and\noverheads for creating tensors, thus leading to performance\nissues, especially for long sequences. One way to miti-\ngate this is to allocate shared continuous memory before\ncomputation. However, in-place modiﬁcation is not differ-\nentiable in these frameworks. Some studies sidestep the\nrecursion by approximating IIR in the frequency domain\nusing Discrete Fourier Transform (DFT) [27, 30, 32–35],\nbut the accuracy of this approximation depends on the DFT\nresolution. Moreover, the IIRs used in practice are usually\nlow-order; in this case, it is faster to compute them directly,\nespecially on long sequences.\n3. PROPOSED MODEL\nUsually, N(z)in Eqn (1) is treated as amplitude-\nmodulated Gaussian noise [23, 24]. Our early experiments\nfound this formulation to be challenging to optimise. As\nan alternative, we move the noise components N(z)out-\nside the glottal source and ﬁlter it with time-varying ﬁlter\nC(z), resulting in\nS(z) =G′(z)H(z)+N(z)C(z). (3)\nThis resembles the classic Harmonic-plus-Noise\nmodel [36] and was used in previous DDSP-based\nSVS [17, 18]. Alonso et al. [17] modelled G′(z)H(z)\njointly using additive harmonic oscillators and time-\nvarying Finite Impulse Responses (FIRs) as C(z); Wu\net al. [18] introduced a sawtooth oscillator as G′(z)and\nzero-phase time-varying FIRs as H(z). In this work, we\nuse a glottal ﬂow model to synthesise harmonic sources\nand time-varying IIRs as ﬁlters.\n3.1 Glottal Flow Wavetables\nWe adopted the transformed-LF model [37] for generat-\ning glottal pulses. This model re-parameterises the LF\nmodel [25] using just one parameter Rd, which has been\nfound to correspond to the perceived vocal effort well and\ncovers a wide range of different glottal ﬂow shapes. We\nsampledKvalues of log(Rd)with equal spacing inside\n[log(0.3),log(2.7)]according to the value range suggested\nby [23]. We calculate the ﬂow derivative function g′(t;Rd)\nin continuous time tfor each sampled Rdand then sam-\npledLpoints in one period to get its discrete version. The\ndetails for calculating g′(t;Rd)were given in [38]. By\nstacking these sampled glottal ﬂows, we built wavetables\nD∈RK×L, with each row containing one period of a sam-\npled glottal pulse (see Fig. 1). The rows are sorted based\nonRd.\nThe model generates glottal pulses g′\nnby linearly inter-\npolating the two Daxes. The encoder network ﬁrst pre-\ndicts instantaneous frequency fn∈[0,0.5]and the frac-\ntional index τn∈[0,1]forRd. We then use the instan-\ntaneous phase φn=/summationtextn\ni=1fito interpolate the waveformProceedings of the 24th International Society for Music Information Retrieval Conference, Milan, Italy, Nov 5-9, 2023\n668Figure 1 . An example of the wavetables we used, corre-\nsponding to matrix DwithK= 31 .\nas:\ng′\nn= (1−p)/parenleftBig\n(1−q)ˆd⌊k⌋,⌊l⌋+qˆd⌊k⌋,⌈l⌉/parenrightBig\n+p/parenleftBig\n(1−q)ˆd⌈k⌉,⌊l⌋+qˆd⌈k⌉,⌈l⌉/parenrightBig\n,(4)\nwherel= (φnmod 1)L+ 1,k=τn(K−1) + 1,p=\nk−⌊k⌋,q=l−⌊l⌋, andˆD= [D;d1]∈RK×(L+1). The\nwavetables Dare ﬁxed in our case, contrary to [16], and\nwe only pick one wavetable at a time, not a weighted sum.\n3.2 Frame-Wise LPC Synthesis\nTime-varying LPC synthesis is usually done by linearly in-\nterpolating the LPC coefﬁcients to the audio resolution and\nﬁltering sample by sample. This is not parallelisable and\nslows down the training process. As an alternative, we ap-\nproximate LPC synthesis by treating each frame indepen-\ndently and using overlap-add:\nsn=/summationdisplay\nkLPC(g′\nnγnun−kT;ak)wn−kT, (5)\nwhereLPC(en;a)represents Eqn (2), ak∈RMare the\nﬁlter coefﬁcients at the kthframe,unandwnare the win-\ndowing functions, γn∈R+is the gain, and Tis the hop\nsize.unis ﬁxed to the square window. In this way, the\ncomputation can be parallelised. We found that the voice\nquality differences between overlap-add LPC and sample-\nby-sample LPC are barely noticeable if we use a sufﬁ-\nciently small hop size. We empirically found that a 200\nHz frame rate is sufﬁcient.\n3.3 LPC Coefﬁcients Parameterisation\nFor the LPC ﬁlter to be stable, all of its poles must lie in-\nside the unit circle on the complex plane. Stability can be\nguaranteed using robust representations, such as reﬂection\ncoefﬁcients [28]. The representation we chose in this work\nis cascaded 2nd-order IIR ﬁlters, and we solve the stabil-\nity issue by ensuring all the 2nd-order ﬁlters are stable. We\nuse the coefﬁcient representation from [33] to parameterisetheithIIR ﬁlter’s coefﬁcients 1+ηi,1z−1+ηi,2z−2from\nthe encoder’s outputs and cascade them together to form\nanMth-order LPC ﬁlter:\n(1+η1,1z−1+η1,2z−2)(1+η2,1z−1+η2,2z−2)\n···(1+ηM\n2,1z−1+ηM\n2,2z−2)\n= 1+a1z−1+a2z−2+···+aMz−M=A(z).(6)\n3.4 Unvoiced Gating\nThe instantaneous frequency fnpredicted by the encoder is\nalways non-zero and keeps the oscillator working. Without\nconstraint, the model would utilise these harmonics in the\nunvoiced region creating buzzing artefacts [18]. We pro-\npose to mitigate this problem by jointly training the model\nto predict the voiced/unvoiced probabilities as vn∈[0,1]\nand feeding the gated frequency ˆfn=vnfnto the oscilla-\ntor instead.\n4. OPTIMISATION\nTraining deep learning models is usually accomplished by\nbackpropagating the gradients evaluated at a chosen loss\nfunctionLthroughout the whole computational graph back\nto the parameters. Partially inspired by Bhattacharya et\nal. [29], we derived the closed form of backpropagation\nthrough time to utilise efﬁcient IIR implementation to solve\nthe problems we mentioned in Section 2.2 while keeping\nthe ﬁlter differentiable. Here, e∈RNis the input, a∈\nRMis the ﬁlter coefﬁcients, and s∈RNis the output.\nAssuming we know∂L\n∂s, we can get the derivatives∂L\n∂eand\n∂L\n∂a, using chain rules∂L\n∂s∂s\n∂eand∂L\n∂s∂s\n∂a.\n4.1 Backpropagation Through the Coefﬁcients\nTaking the derivatives of Eqn (2) with respect to aiwe get:\n∂sn\n∂ai=−sn−i−M/summationdisplay\nk=1ak∂sn−k\n∂ai, (7)\nwhich equals LPC(−sn−i;a).sn|n≤0does not depend on\naiso the initial conditions∂sn\n∂ai|n≤0are zeros. We can get\n∂sn\n∂awith one pass of ﬁltering because∂sn\n∂ajis∂sn\n∂aishifted by\nan offsetj−i. Lastly, we calculate∂L\n∂aias/summationtextN\nn=1∂L\n∂sn∂sn\n∂ai.\n4.2 Backpropagation Through the Input\nTo get the derivatives for input en, we ﬁrst re-write Eqn (2)\nas the following convolutional form:\nsn=n/summationdisplay\nm=1emhn−m, (8)\nwherehn=Z−1{H(z)},H(z) =1\nA(z). From Eqn (8)\nwe see that∂sn\n∂em=hn−m. The derivative of loss Lwith\nrespect to emdepends on all future samples sn, which is:\n∂L\n∂em=N/summationdisplay\nn=m∂L\n∂sn∂sn\n∂em=N/summationdisplay\nn=m∂L\n∂snhn−m. (9)Proceedings of the 24th International Society for Music Information Retrieval Conference, Milan, Italy, Nov 5-9, 2023\n669By swapping the variables n,m and considering the equiv-\nalence of Eqn (2) and Eqn (8), Eqn (9) can be simpliﬁed to\n∂L\n∂en=N/summationdisplay\nm=n∂L\n∂smhm−n\n=∂L\n∂sn−M/summationdisplay\ni=1ai∂L\n∂en+i.(10)\nEqn (10) shows that we can get the derivatives∂L\n∂enby just\nﬁltering∂L\n∂snwith the same ﬁlter, but running in backwards.\nThe initial conditions∂L\n∂en|n>N are naturally zeros.\nIn conclusion, backpropagation through an IIR ﬁlter\nconsists of two passes of the same ﬁlter and one matrix\nmultiplication1. We implemented the IIR in C++ and\nCUDA with multi-threading to ﬁlter multiple sequences si-\nmultaneously2. The differentiable IIR is done by register-\ning the above backward computation in PyTorch, and we\nsubmitted the implementation to TorchAudio [39] as part\nof thetorchaudio.functional.lfilter .\n5. EXPERIMENTAL SETUP\n5.1 Dataset\nWe test GOLF as a neural vocoder on the MPop600\ndataset [40], a high-quality Mandarin singing voice dataset\nfeaturing nearly 600 singing recordings with aligned lyrics\nsung by four singers. We used the audio recordings from\nthef1(female) and m1(male) singers. For each singer, we\nselected the ﬁrst three recordings as the test set, the follow-\ning 27 recordings as the validation set, and used the rest as\ntraining data (around three hours in total). All the record-\nings were downsampled to 24 kHz. The vocoder feature\nwe choose is the log mel-spectrogram. We computed the\nfeature with a window size of 1024 and 80 mel-frequency\nbins and set the hop size Tto 120. We normalised the fea-\nture to between zero and one and sliced the training data\ninto two seconds excerpts with 1.5 seconds overlap.\n5.2 Model Details\nWe adopted the encoder from SawSing but replaced the\ntransformer layers with three layers of Bi-LSTM for\nfavourable implementation, resulting in around 0.7M pa-\nrameters in total. A ﬁnal linear layer predicts the synthe-\nsis parameters {fk,vk,γk,βk,ak,bk}. The ﬁrst four pa-\nrameters are linearly upsampled to {fn,vn,γn,βn}.βn\nandbkare the Gaussian noise’s gain and ﬁlter coefﬁcients.\nWe added an average pooling layer with a size of 10 and\ntwo convolution layers after the encoder to predict the Rd\nfractional index τoat a lower rate and then linearly up-\nsampled to τn. This step avoids possible modulation ef-\nfects caused by switching the wavetables too quickly. A\n1The computation of the IIR does not need to fulﬁl the implementation\nrequirements set by the automatic differentiation framework, thus can be\nhighly optimised.\n2Although the single-core performance of a GPU is usually inferior to\na CPU, and we can only use at most one thread for each IIR, the GPU has\na much higher number of cores, which is beneﬁcial for training on a large\nnumber of sequences at once.system diagram of GOLF is shown in Fig. 2. We set\nK= 100,L= 2048 , Hanning window for wn, and\nM= 22 for both LPC ﬁlters. We used the same hop size\nTand a window size of 480 for frame-wise LPC. We nor-\nmalised all wavetables to have equal energy and aligned\nthem with the negative peak.\nFigure 2 . Overview of the GOLF synthesis process. phase\noffset is only introduced at test time, where the details are\ngiven in Section 6.1.\nWe compare GOLF with three DDSP-based baselines\nusing the same NN encoder to predict their synthesis pa-\nrameters. The ﬁrst two are the original DDSP [14] and\nSawSing [18]. We set their noise ﬁlter length to 80 and har-\nmonic ﬁlter length to 256 for SawSing. The third model is\nPUlse-train LPC Filter (PULF), which is similar to GOLF\nbut replaces the glottal ﬂow wavetables with a band-limited\npulse train [19] using additive synthesis, while the LPC or-\nder for the harmonic source is increased to 26 to accom-\nmodate the glottal pulse response. The number of oscil-\nlating sinusoids was set to over 150 for all the baselines.\nWe did not compare GOLF with Nercessian et al. [19] and\nYoshimura et al. [21] because these architectures are based\nclosely on the source-ﬁlter model, and use additional post-\nnets to enhance the voice, which makes it harder to com-\npare directly with GOLF.\n5.3 Training Conﬁgurations\nWe trained separate models for each singer, resulting in\n8 models. The loss function is the summation of theProceedings of the 24th International Society for Music Information Retrieval Conference, Milan, Italy, Nov 5-9, 2023\n670multi-resolution STFT loss (MSSTFT) and f0 loss from\nSawSing with FFT sizes set to {512,1024,2048}, plus a\nbinary cross entropy loss on voiced/unvoiced prediction.\nWe stopped the gradients from the harmonic source to the\nf0s and voiced decisions to stabilise the training. We used\nAdam [41] for running all optimisations. For DDSP and\nSawSing, the batch size and learning rate were set to 32\nand 0.0005; for GOLF and PULF, the numbers were 64\nand 0.0001. We used the ground truth f0s (extracted by\nWORLD [20]) for the harmonic oscillator of PULF during\ntraining due to stability issues. We trained all the models\nfor 800k steps to reach sufﬁcient convergence and picked\nthe checkpoint with the lowest validation loss as the ﬁnal\nmodel3.\n6. EV ALUATIONS\n6.1 Objective Evaluation\nThe objective metrics we choose are the MSSTFT, the\nmean absolute error (MAE) in f0, and the Fréchet audio\ndistance (FAD) [42] on the predicted singing of the test set.\nTable 1 shows that DDSP has the lowest MSSTFT and f0\nerrors, while SawSing reaches the lowest FAD. GOLF and\nPULF show comparable results in f0 errors to other base-\nlines. We report the memory usage when training these\nmodels and their real-time factor (RTF), both on GPU and\nCPU, in Table 2. The amount of memory required to train\nGOLF is around 35% of others, and it runs extremely fast,\nespecially on the CPU.\nSingers Models MSSTFT MAE-f0 (cent) FAD\nf1DDSP 3.09 74.47 ±1.19 0.50 ±0.02\nSawSing 3.12 78.91 ±1.18 0.38±0.02\nGOLF 3.21 77.06 ±0.88 0.62 ±0.02\nPULF 3.27 76.90 ±1.11 0.75 ±0.04\nm1DDSP 3.12 52.95 ±1.03 0.57 ±0.02\nSawSing 3.13 56.46 ±1.04 0.48±0.02\nGOLF 3.26 54.09 ±0.30 0.67 ±0.01\nPULF 3.35 54.60 ±0.73 1.11 ±0.04\nTable 1 . Evaluation results on the test set. We omit the\nstandard deviation if it is smaller than 0.01.\nAs an additional metric we use the L2 loss between the\npredicted and the ground truth waveform. The intuition\nbehind this is that GOLF and PULF are the only two mod-\nels introducing non-linear phase response because of IIR\nﬁltering. The ﬁlters in DDSP and SawSing are all zero-\nphase, and the initial phases of the sinusoidal oscillators\nare ﬁxed to zeros. We emphasise that this test is not target-\ning human perception but the phase reconstruction ability\nof the models. Humans cannot perceive the absolute fre-\nquency phase, but accurate reconstruction could be impor-\ntant in sound matching and mixing use cases. We evalu-\nate the loss on one of the test samples from m1we used\nin the subjective evaluation. We created a new parameter\n3The trained checkpoints, source codes, and audio samples are avail-\nable athttps://github.com/iamycy/golf .called phase offset sampled at 20 Hz. We linearly upsam-\npled phase offset and added it to the instantaneous phase\nφn, introducing a slowly varying phase shift. We optimised\nthis parameter by minimising the predicted waveform’s L2\nloss to the ground truth using Adam with a learning rate of\n0.001 and 1000 steps. We wrapped the differences between\nthe points of phase offset during optimisation to [-0.5, 0.5].\nWe ran this optimisation ﬁve times for each model. Each\ntime the phase offset was initialised randomly. We report\nthe minimum and maximum ﬁnal losses from these trials.\nTable 2 shows the lowest losses GOLF and PULF can reach\nare signiﬁcantly smaller than the others, with GOLF hav-\ning the smallest among all.\nModels MemoryRTF Waveform L2\nGPU CPU Min Max\nDDSP 7.3 0.015 0.237 71.83 88.77\nSawSing 7.3 0.015 0.240 75.72 93.16\nGOLF 2.6 0.009 0.023 21.98 64.82\nPULF 7.5 0.015 0.248 44.08 70.59\nTable 2 . The required number of VRAM (GB) for train-\ning with a batch size of 32, real-time factor (RTF), and\nthe minimum/maximum L2 loss on waveform using one\nof the test samples. The benchmark was conducted on\nan Ubuntu 20.04 LTS machine with an i5-4790k proces-\nsor and an NVIDIA GeForce RTX 3070 GPU.\n6.2 Subjective Evaluation\nWe conducted an online listening test using Go Listen [43].\nWe picked one short clip from each test set recording, re-\nsulting in 6 clips with duration ranging from 6 to 11 sec-\nonds. The test is thus divided into six examples, each con-\nsisting of one ground truth clip and four synthesised results\nfrom different models, and their loudness was normalised\nto –16dB LUFS. The order of the examples and the stimu-\nlus were randomised for each subject. Each subject was re-\nquested to rate the quality of these stimuli on a score from\n0 to 100. We collected responses from 33 anonymous par-\nticipants. We dropped one of the participants who did not\nindicate using headphones. We normalise scores to fall be-\ntween 1 to 5 and report the Mean Opinion Score (MOS)\nin Fig. 4. DDSP has the highest opinion scores overall,\nand a Wilcoxon signed-rank test shows that it is not statis-\ntically signiﬁcantly different from the f1ground truth ( p\n= 0.168). We applied the one-side Wilcoxon test on GOLF\nand PULF to compare them with SawSing, and the results\nshow that GOLF signiﬁcantly outperforms SawSing ( p<\n0.0001), and PULF performs better than SawSing on m1\n(p< 0.022).\n7. DISCUSSIONS\nGiven the evaluation results and the number of synthe-\nsis parameters in GOLF is roughly six times smaller than\nDDSP and SawSing, it is clear that GOLF’s synthesis pa-\nrameters are a more compact representation. ComparingProceedings of the 24th International Society for Music Information Retrieval Conference, Milan, Italy, Nov 5-9, 2023\n671Figure 3 . The predicted waveforms of a short segment from one of the m1test samples. The differences were computed\nby subtracting the predicted signal from the reference.\nFigure 4 . The MOS results of the vocoders trained on dif-\nferent singers with 95% conﬁdence interval.\nthe differences between GOLF and PULF in Table 2, we\ncan see that the performance gain is due to the use of\nwavetables. Other baselines synthesise band-limited har-\nmonic sources with many sinusoids oscillating simulta-\nneously, thus increasing the computational cost. PULF’s\nMOS score is much worse on f1, with noticeable arte-\nfacts in the unvoiced region and random components of\nthe voice. After investigation, we found the noise gains βn\npredicted by PULF ﬂuctuating at high speed, producing a\nmodulation effect. This behaviour is also found in PULF\ntrained onm1and GOLF, even on the harmonic gains γn.\nStill, the amount of ﬂuctuation is small and barely notice-\nable in the test samples. Given the available results, we\ncould only conclude that this effect relates to the type of\nharmonic source and the range of f0, i.e., female singers\nhave higher f0. This amplitude modulation effect cannot\nbe observed in spectrograms and thus is not captured by\nthe training loss we used. It could be an intrinsic drawback\nof using frame-wise LPC approximation, but more experi-\nments and comparisons with sample-wise LPC are needed.\nIn addition, SawSing produced low scores for both singers\nbecause of the buzzing artefacts in the unvoiced region.\nAlthough unvoiced gating (Sec. 3.4) reduces this problem\nto a large degree, human ears are susceptible to this effect.\nThis could be an inherent problem in using a sawtooth as\nthe harmonic source.\nThe L2 loss shown in Table 2 demonstrates that GOLF\nmatches phase-related characteristics more accurately thanother models. Fig. 3 shows GOLF produces the most sim-\nilar waveform to the ground truth. Other baselines’ wave-\nforms are similar because they use the same additive syn-\nthesiser. It is possible to reduce their L2 loss by optimising\nthe initial phases of the oscillators, but this cannot account\nfor time-varying source shapes. Low L2 loss is a positive\neffect of the deterministic phase responses embedded in\nGOLF. This opens up many possibilities, such as decom-\nposing and analysing the voice in a differentiable manner\nand training the vocoder using the time domain loss func-\ntion. The latter could be a possible way to reduce the\nﬂuctuation problem discussed in the previous paragraph.\nThe waveform matching of GOLF can be improved further\nby using a more ﬂexible glottal source model, adding FIR\nand all-pass ﬁlters to account for the voice’s mixed-phase\ncomponents and the recording environment’s acoustic re-\nsponse.\nLastly, we note that cascaded IIR ﬁlters provide an or-\nderless representation (i.e. the cascading order does not af-\nfect the outputs). This results in the responsibility prob-\nlem[44, 45] for the last layer of the encoder, which might\nbe one of the reasons why GOLF and PULF are less sta-\nble to train than other baselines. Developing architectures\nthat can handle orderless representation or switch to other\nrobust representations are possible ways to address this.\n8. CONCLUSIONS\nWe present a lightweight singing voice vocoder called\nGOLF, which uses wavetables with different glottal ﬂows\nas entries to model the time-varying harmonic components\nand differentiable LPC ﬁlters for ﬁltering both the harmon-\nics and random elements. We show that GOLF requires\nless memory to train and runs an order of magnitude faster\non the CPU than other DDSP-based vocoders, but still\nattains competitive voice quality in subjective and objec-\ntive evaluations. Furthermore, we empirically show that\nthe predicted waveforms from GOLF represent the voice’s\nphase response more faithfully, which could allow us to\nuse GOLF to decompose and analyse human voice.Proceedings of the 24th International Society for Music Information Retrieval Conference, Milan, Italy, Nov 5-9, 2023\n6729. ACKNOWLEDGEMENTS\nThe authors want to thank Moto Hira ( mthrok ), Christian\nPuhrsch (cpuhrsch ), and Alban Desmaison ( albanD )\nfor reviewing our pull requests to TorchAudio. We are\nincredibly grateful to Parmeet Singh Bhatia ( parmeet )\nfor implementing the ﬁrst version of efﬁcient IIR in the\nTorchAudio codebase as lfilter.cpp . We thank Ben\nHayes for giving feedback on the equations of backpropa-\ngation through an IIR. The ﬁrst author wants to exclusively\nthank Ikuyo Kita for giving positive, energetic support dur-\ning the writing process. The ﬁrst author is a research stu-\ndent at the UKRI Centre for Doctoral Training in Artiﬁcial\nIntelligence and Music, supported jointly by UK Research\nand Innovation [grant number EP/S022694/1] and Queen\nMary University of London.\n10. REFERENCES\n[1] M. W. Macon, L. Jensen-Link, J. Oliverio, M. A.\nClements, and E. B. George, “A singing voice synthesis\nsystem based on sinusoidal modeling,” in International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , vol. 1. IEEE, 1997, pp. 435–438.\n[2] J. Bonada, Ò. Celma Herrada, À. Loscos, J. Ortolà,\nX. Serra, Y . Yoshioka, H. Kayama, Y . Hisaminato,\nand H. Kenmochi, “Singing voice synthesis combining\nexcitation plus resonance and sinusoidal plus residual\nmodels,” in International Computer Music Conference\n(ICMC) , Havana, Cuba, 2001.\n[3] J. Bonada and X. Serra, “Synthesis of the singing voice\nby performance sampling and spectral models,” IEEE\nSignal Processing Magazine , vol. 24, no. 2, pp. 67–79,\n2007.\n[4] J. Bonada, M. Umbert Morist, and M. Blaauw, “Ex-\npressive singing synthesis based on unit selection for\nthe singing synthesis challenge 2016,” in INTER-\nSPEECH . International Speech Communication As-\nsociation (ISCA), 2016.\n[5] K. Saino, H. Zen, Y . Nankaku, A. Lee, and K. Tokuda,\n“An HMM-based singing voice synthesis system,” in\n9th International Conference on Spoken Language\nProcessing , 2006.\n[6] Y . Hono, S. Murata, K. Nakamura, K. Hashimoto,\nK. Oura, Y . Nankaku, and K. Tokuda, “Recent de-\nvelopment of the DNN-based singing voice synthesis\nsystem—Sinsy,” in Asia-Paciﬁc Signal and Informa-\ntion Processing Association Annual Summit and Con-\nference (APSIPA ASC) . IEEE, 2018, pp. 1003–1009.\n[7] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly,\nZ. Yang, Z. Chen, Y . Zhang, Y . Wang, R. Skerrv-\nRyan et al. , “Natural TTS synthesis by conditioning\nWaveNet on mel spectrogram predictions,” in Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2018, pp. 4779–4783.[8] J.-M. Valin and J. Skoglund, “LPCNet: Improving neu-\nral speech synthesis through linear prediction,” in In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) . IEEE, 2019, pp. 5891–\n5895.\n[9] R. Yoneyama, Y .-C. Wu, and T. Toda, “Uniﬁed source-\nﬁlter GAN with harmonic-plus-noise source excitation\ngeneration,” in INTERSPEECH . International Speech\nCommunication Association (ISCA), 2022.\n[10] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A\nﬂow-based generative network for speech synthesis,”\ninInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) . IEEE, 2019, pp. 3617–\n3621.\n[11] J. Liu, C. Li, Y . Ren, F. Chen, and Z. Zhao, “Diff-\nSinger: Singing voice synthesis via shallow diffusion\nmechanism,” in AAAI Conference on Artiﬁcial Intelli-\ngence , vol. 36, no. 10, 2022, pp. 11 020–11 028.\n[12] Y .-P. Cho, F.-R. Yang, Y .-C. Chang, C.-T. Cheng, X.-\nH. Wang, and Y .-W. Liu, “A survey on recent deep\nlearning-driven singing voice synthesis systems,” in\n2021 IEEE International Conference on Artiﬁcial In-\ntelligence and Virtual Reality (AIVR) . IEEE, 2021,\npp. 319–323.\n[13] N. Takahashi, M. Kumar, Singh, and Y . Mitsufuji, “Hi-\nerarchical diffusion models for singing voice neural\nvocoder,” arXiv preprint arXiv:2210.07508 , 2022.\n[14] J. Engel, L. H. Hantrakul, C. Gu, and A. Roberts,\n“DDSP: Differentiable digital signal processing,” in In-\nternational Conference on Learning Representations ,\n2020.\n[15] B. Hayes, C. Saitis, and G. Fazekas, “Neural wave-\nshaping synthesis,” in Proc. International Society for\nMusic Information Retrieval , 2021.\n[16] S. Shan, L. Hantrakul, J. Chen, M. Avent, and\nD. Trevelyan, “Differentiable wavetable synthesis,” in\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) . IEEE, 2022, pp. 4598–\n4602.\n[17] J. Alonso and C. Erkut, “Latent space explorations of\nsinging voice synthesis using DDSP,” arXiv preprint\narXiv:2103.07197 , 2021.\n[18] D.-Y . Wu, W.-Y . Hsiao, F.-R. Yang, O. Friedman,\nW. Jackson, S. Bruzenak, Y .-W. Liu, and Y .-H. Yang,\n“DDSP-based singing vocoders: A new subtractive-\nbased synthesizer and a comprehensive evaluation,” in\nProc. International Society for Music Information Re-\ntrieval , 2022.\n[19] S. Nercessian, “Differentiable WORLD synthesizer-\nbased neural vocoder with application to end-to-end\naudio style transfer,” arXiv preprint arXiv:2208.07282 ,\n2022.Proceedings of the 24th International Society for Music Information Retrieval Conference, Milan, Italy, Nov 5-9, 2023\n673[20] M. Morise, F. Yokomori, and K. Ozawa, “WORLD:\na vocoder-based high-quality speech synthesis system\nfor real-time applications,” IEICE TRANSACTIONS on\nInformation and Systems , vol. 99, no. 7, pp. 1877–\n1884, 2016.\n[21] T. Yoshimura, S. Takaki, K. Nakamura, K. Oura,\nY . Hono, K. Hashimoto, Y . Nankaku, and K. Tokuda,\n“Embedding a differentiable mel-cepstral synthesis ﬁl-\nter to a neural speech synthesis system,” arXiv preprint\narXiv:2211.11222 , 2022.\n[22] S. Li, Y . Zhao, R. Varma, O. Salpekar, P. Noordhuis,\nT. Li, A. Paszke, J. Smith, B. Vaughan, P. Damania\net al. , “PyTorch distributed: Experiences on acceler-\nating data parallel training,” Proceedings of the VLDB\nEndowment , vol. 13, no. 12, 2020.\n[23] G. Degottex, “Glottal source and vocal-tract separa-\ntion,” Ph.D. dissertation, Université Pierre et Marie\nCurie-Paris VI, 2010.\n[24] H.-L. Lu and J. O. Smith III, “Glottal source modeling\nfor singing voice synthesis.” in International Computer\nMusic Conference (ICMC) , 2000.\n[25] G. Fant, J. Liljencrants, Q.-g. Lin et al. , “A four-\nparameter model of glottal ﬂow,” STL-QPSR , vol. 4,\nno. 1985, pp. 1–13, 1985.\n[26] J. D. Markel and A. H. Gray, Linear Prediction of\nSpeech , ser. Communication and Cybernetics. Berlin,\nHeidelberg: Springer, 1976, vol. 12.\n[27] S. Oh, H. Lim, K. Byun, M.-J. Hwang, E. Song, and\nH.-G. Kang, “ExcitGlow: Improving a WaveGlow-\nbased neural vocoder with linear prediction analysis,”\ninAsia-Paciﬁc Signal and Information Processing As-\nsociation Annual Summit and Conference (APSIPA\nASC) . IEEE, 2020, pp. 831–836.\n[28] K. Subramani, J.-M. Valin, U. Isik, P. Smaragdis,\nand A. Krishnaswamy, “End-to-end LPCNet: A neu-\nral vocoder with fully-differentiable LPC estimation,”\narXiv preprint arXiv:2202.11301 , 2022.\n[29] P. Bhattacharya, P. Nowak, and U. Zölzer, “Optimiza-\ntion of cascaded parametric peak and shelving ﬁl-\nters with backpropagation algorithm,” in International\nConference on Digital Audio Effects , 2020, pp. 101–\n108.\n[30] S. Nercessian, “Neural parametric equalizer matching\nusing differentiable biquads,” in International Confer-\nence on Digital Audio Effects , 2020, pp. 265–272.\n[31] B. Kuznetsov, J. D. Parker, and F. Esqueda, “Differ-\nentiable IIR ﬁlters for machine learning applications,”\ninInternational Conference on Digital Audio Effects ,\n2020, pp. 297–303.[32] J. T. Colonel, C. J. Steinmetz, M. Michelen, and J. D.\nReiss, “Direct design of biquad ﬁlter cascades with\ndeep learning by sampling random polynomials,” in In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) . IEEE, 2022, pp. 3104–\n3108.\n[33] S. Nercessian, A. Sarroff, and K. J. Werner,\n“Lightweight and interpretable neural modeling of\nan audio distortion effect using hyperconditioned dif-\nferentiable biquads,” in International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2021, pp. 890–894.\n[34] T. Kim, Y .-H. Yang, A. Sincia, and J. Nam, “Joint esti-\nmation of fader and equalizer gains of dj mixers using\nconvex optimization,” in International Conference on\nDigital Audio Effects . DAFx, 2022, pp. 312–319.\n[35] C. J. Steinmetz, N. J. Bryan, and J. D. Reiss, “Style\ntransfer of audio effects with differentiable signal pro-\ncessing,” Journal of the Audio Engineering Society ,\nvol. 70, no. 9, pp. 708–721, 2022.\n[36] X. Serra and J. Smith, “Spectral modeling synthesis: A\nsound analysis/synthesis system based on a determin-\nistic plus stochastic decomposition,” Computer Music\nJournal , vol. 14, no. 4, pp. 12–24, 1990.\n[37] G. Fant, “The LF-model revisited. transformations and\nfrequency domain analysis,” Speech Trans. Lab. Q.\nRep., Royal Inst. of Tech. Stockholm , vol. 2, no. 3, p. 40,\n1995.\n[38] C. Gobl, “Reshaping the transformed LF model: Gen-\nerating the glottal source from the waveshape param-\neter Rd,” in INTERSPEECH . International Speech\nCommunication Association (ISCA), Aug. 2017, pp.\n3008–3012.\n[39] Y .-Y . Yang, M. Hira, Z. Ni, A. Astafurov, C. Chen,\nC. Puhrsch, D. Pollack, D. Genzel, D. Greenberg,\nE. Z. Yang, J. Lian, J. Hwang, J. Chen, P. Goldsbor-\nough, S. Narenthiran, S. Watanabe, S. Chintala, and\nV . Quenneville-Bélair, “Torchaudio: Building blocks\nfor audio and speech processing,” in International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2022, pp. 6982–6986.\n[40] C.-C. Chu, F.-R. Yang, Y .-J. Lee, Y .-W. Liu, and S.-H.\nWu, “MPop600: A mandarin popular song database\nwith aligned audio, lyrics, and musical scores for\nsinging voice synthesis,” in Asia-Paciﬁc Signal and In-\nformation Processing Association Annual Summit and\nConference (APSIPA ASC) . IEEE, 2020, pp. 1647–\n1652.\n[41] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” arXiv preprint arXiv:1412.6980 ,\n2014.Proceedings of the 24th International Society for Music Information Retrieval Conference, Milan, Italy, Nov 5-9, 2023\n674[42] K. Kilgour, M. Zuluaga, D. Roblek, and M. Shar-\niﬁ, “Fréchet audio distance: A metric for evaluat-\ning music enhancement algorithms,” arXiv preprint\narXiv:1812.08466 , 2018.\n[43] D. Barry, Q. Zhang, P. W. Sun, and A. Hines, “Go\nListen: An end-to-end online listening test platform,”\nJournal of Open Research Software , 2021.\n[44] Y . Zhang, J. Hare, and A. Prugel-Bennett, “Deep set\nprediction networks,” Advances in Neural Information\nProcessing Systems , vol. 32, 2019.\n[45] B. Hayes, C. Saitis, and G. Fazekas, “The\nresponsibility problem in neural networks with\nunordered targets,” 2023. [Online]. Available:\nhttps://openreview.net/forum?id=jd7Hy1jRiv4Proceedings of the 24th International Society for Music Information Retrieval Conference, Milan, Italy, Nov 5-9, 2023\n675"
    },
    {
        "title": "Stabilizing Training With Soft Dynamic Time Warping: A Case Study for Pitch Class Estimation With Weakly Aligned Targets.",
        "author": [
            "Johannes Zeitler",
            "Simon Deniffel",
            "Michael Krause 0002",
            "Meinard Müller"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265317",
        "url": "https://doi.org/10.5281/zenodo.10265317",
        "ee": "https://zenodo.org/records/10265317/files/000051.pdf",
        "abstract": "Soft dynamic time warping (SDTW) is a differentiable loss function that allows for training neural networks from weakly aligned data. Typically, SDTW is used to iteratively compute and refine soft alignments that compensate for temporal deviations between the training data and its weakly annotated targets. One major problem is that a mismatch between the estimated soft alignments and the reference alignments in the early training stage leads to incorrect parameter updates, making the overall training procedure unstable. In this paper, we investigate such stability issues by considering the task of pitch class estimation from music recordings as an illustrative case study. In particular, we introduce and discuss three conceptually different strategies (a hyperparameter scheduling, a diagonal prior, and a sequence unfolding strategy) with the objective of stabilizing intermediate soft alignment results. Finally, we report on experiments that demonstrate the effectiveness of the strategies and discuss efficiency and implementation issues.",
        "zenodo_id": 10265317,
        "dblp_key": "conf/ismir/ZeitlerD0M23",
        "keywords": [
            "soft dynamic time warping",
            "differentiable loss function",
            "neural networks",
            "weakly aligned data",
            "iterative computation",
            "refine soft alignments",
            "temporal deviations",
            "incorrect parameter updates",
            "stability issues",
            "pitch class estimation"
        ],
        "content": "STABILIZING TRAINING WITH SOFT DYNAMIC TIME WARPING:\nA CASE STUDY FOR PITCH CLASS ESTIMATION WITH WEAKLY\nALIGNED TARGETS\nJohannes Zeitler Simon Deniffel Michael Krause Meinard Müller\nInternational Audio Laboratories Erlangen, Germany\n{johannes.zeitler, michael.krause, meinard.mueller}@audiolabs-erlangen.de\nABSTRACT\nSoft dynamic time warping (SDTW) is a differentiable\nloss function that allows for training neural networks from\nweakly aligned data. Typically, SDTW is used to itera-\ntively compute and reﬁne soft alignments that compensate\nfor temporal deviations between the training data and its\nweakly annotated targets. One major problem is that a\nmismatch between the estimated soft alignments and the\nreference alignments in the early training stage leads to\nincorrect parameter updates, making the overall training\nprocedure unstable. In this paper, we investigate such sta-\nbility issues by considering the task of pitch class estima-\ntion from music recordings as an illustrative case study. In\nparticular, we introduce and discuss three conceptually dif-\nferent strategies (a hyperparameter scheduling, a diagonal\nprior, and a sequence unfolding strategy) with the objective\nof stabilizing intermediate soft alignment results. Finally,\nwe report on experiments that demonstrate the effective-\nness of the strategies and discuss efﬁciency and implemen-\ntation issues.\n1. INTRODUCTION AND RELATED WORK\nDeep neural networks (DNNs) have been commonly used\nin many music information retrieval (MIR) tasks, such as\nmusic transcription [1], or pitch class estimation (PCE) [2,\n3]. The latter provides a widely-used feature represen-\ntation for various subsequent processing pipelines, e.g.,\naudio thumbnailing [4], or chord recognition [3]. Deep\nlearning-based feature extractors yield the highest predic-\ntion accuracy when trained on data from the same dis-\ntribution, which is, however, often not readily available.\nThus, one major challenge is the acquisition of a sufﬁcient\namount of correctly labeled training data. In classical mu-\nsic, it is often difﬁcult to automatically annotate strongly\naligned targets (short: strong targets), i.e., with frame-wise\ntarget labels, due to changes of tempo. On the other hand,\nweakly aligned targets (short: weak targets) only globally\n© J. Zeitler, S. Deniffel, M. Krause, and M. Müller. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: J. Zeitler, S. Deniffel, M. Krause, and M.\nMüller, “Stabilizing Training with Soft Dynamic Time Warping: A Case\nStudy for Pitch Class Estimation with Weakly Aligned Targets”, in Proc.\nof the 24th Int. Society for Music Information Retrieval Conf., Milan,\nItaly, 2023.\n01Local align. cost(a)\nTarget seq. (fr.)Reference alignment Soft alignment\n01Local align. cost(b)\nTarget seq. (fr.)\n01Local align. cost(c)\nPredicted sequence (frames)Target seq. (fr.)\nFigure 1 : Deviation of strong reference alignments\n(dashed green) and soft alignments (red) and stabilizing\nstrategies. (a)Alignment mismatch of standard SDTW.\nStabilizing alignments with (b)hyperparameter schedul-\ning and (c)diagonal prior.\ncorrespond to the input without containing frame-wise lo-\ncal alignments [5,6]. These weak targets are relatively easy\nto obtain, e.g., by only annotating start and end of an audio\nsegment and deriving targets from the musical score. In\nour deﬁnition of weak targets, the order of the target vec-\ntors is correct, but their duration is unknown. Using weak\ntargets in DNN training requires a loss function that aligns\nnetwork predictions with the corresponding weak targets.\nIn classiﬁcation tasks, one widely used technique for\ntraining DNNs with weakly aligned targets is the connec-\ntionist temporal classiﬁcation (CTC) loss [7], which aligns\nnetwork predictions with a sequence of discrete labels.\nDespite being extendable to multi-label problems such as\nmulti-pitch estimation (MPE) [8], CTC remains limited to\ndiscrete targets and is algorithmically complex.\nIn contrast to CTC, dynamic time warping (DTW) can\nbe used to measure similarity between two real-valued se-\nquences and has been successfully applied in, e.g., music433synchronization and structure analysis [9]. Recently, dif-\nferentiable approximations of the minimum function [10–\n12] have been included in DTW, enabling the usage of\nthe DTW principle in gradient-based optimization algo-\nrithms. The algorithm proposed in [10], soft dynamic time\nwarping (SDTW), uses so-called soft alignments to com-\npute a differentiable cost measure between sequences of\ndifferent length. In [13], SDTW is used in the context\nof performance-score synchronization and [6] employed\nSDTW as a loss function to train DNNs for MPE with\nweakly aligned pitch annotations. Experiments in [6] indi-\ncated training instabilities with SDTW when the sequence\nlengths of inputs and targets are signiﬁcantly different.\nThis poses a severe problem in many MIR tasks, where\nsequences of input audio are typically very long, while\nweakly labeled targets, i.e., without note durations, are sig-\nniﬁcantly shorter.\nIn this paper, we investigate the cause of training in-\nstabilities under the SDTW loss and show that it is due\nto a mismatch between the estimated soft alignment and\nthe reference alignment (see Figure 1a) in the early stages\nof training. This mismatch causes incorrect parameter up-\ndates and the training may diverge. Therefore, we intro-\nduce and investigate strategies to decrease this alignment\nerror to stabilize training. In particular, we analyze a hy-\nperparameter scheduling strategy to yield smooth align-\nments in the early training phase (see Figure 1b) as well\nas the strategy of adding a diagonal prior to the SDTW\ncost matrix to initially favour diagonal alignments (see Fig-\nure 1c). Furthermore, we investigate a sequence unfold-\ning approach, where we uniformly stretch the weak target\nsequence to the length of the input sequence as proposed\nin [6]. We choose DNN-based PCE as an exemplary task to\nstudy the training process of standard SDTW and the im-\npact of our stabilizing strategies. We demonstrate that the\nhyperparameter scheduling and the diagonal prior strate-\ngies reliably reduce label mismatch in the early training\nstage and therefore lead to successful trainings. In addi-\ntion, these two strategies are computationally efﬁcient and\nrequire only small modiﬁcations to the standard SDTW al-\ngorithm.\nThe remainder of this article is structured as follows.\nFirst, in Section 2, we discuss the SDTW loss function and\ndeﬁne the concept of soft alignments. Next, in Section 3,\nwe introduce three conceptually different strategies for sta-\nbilizing DNN training under SDTW loss. After describing\nthe experimental setup in Section 4, we evaluate cause and\neffect of training problems with SDTW in Section 5, along\nwith the impact of our stabilizing strategies. Finally, we\nconclude with Section 6 and give an outlook to potential\nareas of future research regarding SDTW-based training in\nMIR.\n2. INTRODUCTION TO SDTW\nIn this section, we introduce SDTW as a loss function in\na DNN training framework and deﬁne the concept of soft\nalignments, closely following [10, 14].2.1 Deﬁnition\nLetX={x0,x1,...,xN−1}denote a sequence of DNN\npredictions, Y={y0,y1,...,yM−1}denote a sequence\nof weak targets and YS=/braceleftbig\nyS\n0,yS\n1,...,yS\nN−1/bracerightbig\ndenote a\nsequence of strong targets, where xn,ym,yS\nn∈RDfor\nn∈ {0,1,...,N−1}andm∈ {0,1,...,M−1}.\nWithout loss of generality, we assume N≥M.\nUsing the mean squared error (MSE) as a\nlocal cost function, the elements of the cost\nmatrixC:=CX,Y∈RN×Mare computed as\nCX,Y(n,m) =∥xn−ym∥2\n2. (1)\nWe next deﬁne binary alignment matrices A∈{0,1}N×M\nwhich align two sequences of length NandM. Each ma-\ntrixAencodes an alignment via a path of ones from cell\n(0,0)to(N−1,M−1)using only vertical, horizontal,\nand diagonal unit steps [10]. All cells not corresponding to\nthe alignment are set to zero. The set of all binary align-\nment matrices for sequences of length NandMis denoted\nAN,M. Using a differentiable approximation of the mini-\nmum function\nsoftminγ(S) =−γlog/summationdisplay\ns∈Sexp(−s/γ) (2)\nfor a given ﬁnite set S⊂Rand a hyperparameter γ∈R,\nthe SDTW cost is given by\nSDTWγ\nC= softminγ({⟨A,C⟩,A∈AN,M}) (3)\nand can be computed efﬁciently via dynamic program-\nming [10]. The inner product ⟨A,C⟩is the sum of all\nelements of Calong the alignment given by A.\n2.2 Soft Alignments\nThe expectation over all alignments Afor a cost matrix C\nis captured by the soft alignment matrix [14]\nEγ\nC=/summationdisplay\nA∈AN,Mpγ\nA,CA∈RN×M, (4)\nwhere the probability of an alignment is deﬁned as\npγ\nA,C=exp(−⟨A,C⟩/γ)/summationtext\nA′∈AN,Mexp(−⟨A′,C⟩/γ). (5)\nThe soft alignment matrix is of particular interest as it is the\nthe gradient of the SDTW cost w.r.t. the local cost matrix\n∇CSDTWγ\nC=Eγ\nC (6)\nand is computed during the backward pass of an SDTW\ntraining step with a dynamic programming algorithm [10,\n14]. In contrast to the binary alignments A, the entries\nof the soft alignment matrix Eγ\nC(n,m)can be interpreted\nas the probability of an alignment path going through\ncell(n,m). Only if this soft alignment assigns probabil-\nity mass to the correct alignments (n,m), the local cost\nterms (1) between the correct pairs of predictions xnandProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n434targetsymconstitute the overall SDTW cost and the DNN\nparameters can be successfully trained.\nThe hyperparameter γ, also termed temperature, con-\ntrols the smoothness of the softmin function (2). Larger\nvalues of γlead to smooth minima in (3), i.e., with contri-\nbutions of multiple alignments A, and therefore a “blurry”\nsoft alignment matrix Eγ\nC(see Figure 1b). On the other\nhand, small values of γpromote “sharp” soft alignments\nEγ\nCwith fewer non-zero entries (see Figure 1a), as (2) con-\nverges to the hard minimum function in the limit γ→0\nand a single binary alignment Abecomes dominant in (3)\nand (4).\n3. STABILIZING TRAINING WITH SDTW\nIn this section, we introduce three strategies for stabilizing\nSDTW-based training: hyperparameter scheduling, diago-\nnal prior, and sequence unfolding.\n3.1 Hyperparameter Scheduling\nAs described in Section 2, the softmin temperature param-\neterγcontrols the smoothness of the SDTW soft align-\nments. While a low value of γis desirable to ensure ex-\nact correspondences between predictions and targets due\nto sharp alignments, the latter are problematic in the initial\ntraining phase as inaccurate predictions from randomly ini-\ntialized network parameters lead to erroneous alignments,\nthus hampering convergence. Therefore, as a ﬁrst strat-\negy to stabilize SDTW training, we discuss an epoch-\ndependent scheduling of γ. Starting a training with a large\nsoftmin temperature γstart= 10 makes the soft alignment\nfuzzier, which leads to coarse, yet mostly meaningful tar-\nget assignments (see Figure 1b). After ten epochs with\nγ= 10 , when the trained network predicts meaningful fea-\ntures, we linearly reduce γduring the following ten epochs\nto a ﬁnal value of γﬁnal= 0.1, which stays constant for the\nremaining training.\n3.2 Diagonal Prior\nOn average, the correct alignment of two sequences with\narbitrary symbol durations has a higher probability to be\nclose to the diagonal than to deviate from it. Therefore, as\na second approach to stabilize the initial training phase, we\ninvestigate an additive prior P∈RN×Mwhich penalizes\nelements of the cost matrix Cthat are far from the diagonal\n(see Figure 2 for an illustration of a prior matrix). A sim-\nilar strategy was employed in [15] for restricting speech-\ntext alignments to the diagonal. Assuming equal symbol\ndurations, the diagonal alignment of a target ymstarts at\ninput frame qm=⌊Nm\nM⌋and ends at qm+1−1. To yield\nno penalty along the diagonal and a smoothly increasing\npenalty for distant alignments, we deﬁne the elements of\nthe prior matrix as\nP(n,m) = 1−\n\n1, q m≤n < qm+1\nexp/parenleftBig\n(n−qm)2\n−2ν/parenrightBig\n, n < q m\nexp/parenleftBig\n(n−qm+1)2\n−2ν/parenrightBig\n, n≥qm+1,\n(7)0 100 200 300 400 50001020304050\n01Prior\nPredicted sequence (frames)Target seq. (frames)\nFigure 2 : Diagonal prior matrix PforN= 500 ,M= 50\nandν= 1000 .\nwhere the parameter νcontrols the sharpness of the prior.\nIn our experiments, we use ν= 1000 . Finally, the prior\nmatrix is added to the cost matrix with a weight ωto obtain\nthe penalized cost matrix\nCP:=C+ωP, (8)\nwhich replaces Cin (3) to (6). Similarly to the hyper-\nparameter scheduling strategy, we choose a constant prior\nweightω= 3during the ﬁrst ﬁve epochs and then linearly\nreduce it to ω= 0during the following ﬁve epochs.\nNote that the numerical parameters for the strategies\npresented in Sections 3.1 and 3.2 were determined em-\npirically by the authors and small changes did not affect\nthe training performance. However, when training on se-\nquences of different length, with a different learning rate,\nor other DNN types, parameters should be adjusted on a\nvalidation set. As presented in Section 5, analysis of the\nsoft alignment matrix Eγ\nCprovides a good indication of\nthe current alignment stability.\n3.3 Sequence Unfolding\nBased on the observation that equal sequence lengths stabi-\nlize SDTW training, a third strategy is to uniformly unfold\nthe target sequence (see also [6]). The unfolded target se-\nquenceYU=/braceleftbig\nyU\n0,yU\n1,...,yU\nN−1/bracerightbig\nis constructed by uni-\nformly repeating elements from the weakly aligned target\nsequence, i.e., setting\nyU\nn←y⌊Mn\nN⌋ (9)\nto yield equal sequence lengths of the predictions Xand\nthe targets YU. Note that the repetition of target vectors\nintroduces ambiguities, leading to multiple optimum align-\nments.\n4. EXPERIMENTAL SETUP\nIn this section, we describe the task for our case study, the\nemployed dataset, as well as the used DNN architecture\nand the training procedure.\n4.1 PCE Task\nWe choose PCE from music recordings as an illustrative\ncase study to investigate the problems of the SDTW loss\nfunction and the effect of the stabilizing strategies. In our\nexperimental setting, a DNN takes Nframes of input au-\ndio (including context) and, for all frames, predicts twelve-\ndimensional pitch class activation vectors X(see Figure 3Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4350 100 200 300 400 500\nCDEF#G#A#\n(a) Strong target sequence (frames)Pitch class\n0 100 200 300 400 500CDEF#G#A#\nPredicted sequence (frames)Pitch classstrong alignment\n0 5 10 15 20 25\nCDEF#G#A#\n(b) Weak target sequence (frames)Pitch class\n0 100 200 300 400 500CDEF#G#A#\nPredicted sequence (frames)Pitch classsoft alignment\nFigure 3 : Alignment between training targets and pre-\ndicted pitch class features Xfor the running example from\nFrühlingstraum .(a)Strong reference alignment for MSE\nloss with strong targets YS.(b)Soft alignment for SDTW\nloss with weak targets Y.\nfor an illustration of predicted pitch class features). We\nwant to train the DNN such that the predictions Xmatch\nthe training targets as close as possible. In the case of\nstrong targets YS, each predicted frame xnis assigned to\nexactly one target frame yS\nnusing a strong alignment (see\nFigure 3a). When using weak targets Y, SDTW internally\ncomputes a soft alignment based on the cost matrix CX,Y\nto assign predictions and targets (see Figure 3b).\n4.2 Dataset\nThroughout all experiments, we use the Schubert Win-\nterreise dataset (SWD) [16] which contains audio record-\nings and strongly aligned pitch class annotations. Winter-\nreise is a song cycle for piano and singer, consisting of\n24 songs. For each song, SWD comprises nine different\nperformances, resulting in 9·24recorded songs with a to-\ntal duration of 10h 50min . We split the dataset for train-\ning, validation, and testing using a performance split [16].\nThe publicly available performances by Huesch (HU33,\nrecorded in 1933) and Scarlata (SC06, recorded in 2006)Layer Kernel Size Stride Output Shape\nPreﬁltering\nLayerNorm (N+74,216,5)\nConv2D 15×15 (1,1)(N+74,216,20)\nMaxPool 3×1 (1,1)(N+74,216,20)\nDropout\nBinning to MIDI pitches\nConv2D 3×3 (1,3)(N+74,72,20)\nMaxPool 13×1 (1,1)(N+74,72,20)\nDropout\nTime reduction\nConv2D 75×1 (1,1)(N,72,10)\nDropout\nChroma reduction\nConv2D 1×1 (1,1)(N,72,1)\nDropout\nConv2D 1×61 (1,12)(N,12,1)\nTable 1 : Musically motivated CNN architecture [3, 5].\nwere annotated manually [16] and constitute the test set.\nFor training and evaluation we choose sequences of length\nN= 500 , corresponding to approximately 8.7sof audio at\na sampling rate of 22050Hz and a hop length of 384sam-\nples. In order to generate weak training targets Yfrom\nSWD (which provides strongly aligned pitch class annota-\ntionsYS, see Figure 3a), we remove all adjacent repetitions\nof a pitch class vector (see Figure 3b) [5]. We choose an\nexcerpt from the song Frühlingstraum , performed by Ran-\ndall Scarlata (SC06), as a running example (see Figure 3)\nto visualize the soft alignment matrices (see Figure 4).\n4.3 DNN Architecture and Training\nWe adapt a conceptually simple and musically mo-\ntivated ﬁve-layer convolutional neural network (CNN)\nfrom [3, 5] with 43383 trainable parameters to predict\ntwelve-dimensional pitch class activation vectors from an\ninput sequence. Table 1 provides an overview of the ar-\nchitecture. We choose the harmonic constant-Q transform\n(HCQT) [17] with ﬁve harmonics as an audio feature rep-\nresentation, spanning six octaves at a resolution of three\nbins per semitone (resulting in 216 frequency bins starting\nfrom C1), a hop length of 384samples and a frame rate of\n57.4Hz. From an input sequence of length N+ 74 , the\nCNN sequentially predicts Nvectors of pitch class activa-\ntions. For the prediction of one frame, the CNN’s recep-\ntive ﬁeld covers 37 adjacent context frames on each side.\nLeaky ReLU with a negative slope of 0.3 is used as a non-\nlinearity after all hidden convolutional layers and sigmoid\nactivation is used after the ﬁnal layer. The dropout rate\nis set to 0.2. All models are trained using the Adam op-\ntimizer [18] with a batch size of 32 and an initial learning\nrate of 0.001. We reduce the learning rate by a factor of two\nif the validation loss did not decrease during the last four\nepochs, and terminate the training if the validation loss did\nnot decrease during the last twelve epochs. At the end of\ntraining, the model from the epoch with the lowest valida-\ntion loss is restored. The source code for reproducing our\nexperiments, as well as the trained models are available on\ngithub.com/groupmm/stabilizing_sdtw .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n436F-measure\nLoss Targets γ Strategy mean std\nMSE strong - - 0.82 0.07\nSDTW weak 0.1 - 0.56 0.37\nSDTW weak 0.3 - 0.63 0.32\nSDTW weak 1.0 - 0.24 0.36\nSDTW weak 3.0 - 0.31 0.38\nSDTW weak 10.0 - 0.57 0.37\nSDTW weak10→0.1hyp. sched. 0.80 0.04\nSDTW weak 0.1 diag. prior 0.81 0.02\nSDTW weak 0.1 seq. unfold. 0.53 0.04\nTable 2 : Averaged test results for DNNs trained on\nstrongly aligned reference targets as well as DNNs trained\nwith SDTW on weakly aligned targets using either the\nstandard conﬁguration or the discussed stabilization strate-\ngies. We report the mean (higher is better) and standard\ndeviation (lower is better) of the F-measure.\n5. EV ALUATION\nIn this section, we investigate the training process as well\nas the prediction accuracy under the standard SDTW loss,\nand compare it to the discussed stabilizing strategies. For\nquantitative evaluation, we repeat all DNN trainings ten\ntimes from random initializations. For the test set predic-\ntions of each trained model, we compute the F-measure\nw.r.t. time-pitch class bins using a threshold of 0.5. The\nmean and standard deviation of the F-measures from all\ntrained models are displayed in Table 2.\n5.1 Baseline: Strongly Aligned Targets\nAs a ﬁrst baseline and an upper bound for all fol-\nlowing experiments, we consider DNN training with\nstrongly aligned targets YS. For the sequence lengths\nM=N= 500 and an MSE loss function, the networks\nachieve the overall highest mean F-measure of 0.82with a\nstandard deviation of 0.07on the test set.\n5.2 Standard SDTW\nWe next analyze DNN training with weak targets Yand\nthe unmodiﬁed SDTW formulation from [10, 19] as a\nloss function. We investigate ﬁve different values of\nγ∈{0.1,...,10}which we keep constant during training.\nAnalyzing the mean F-measure on the test set in Table 2,\nthe ﬁve variants with standard SDTW yield comparably\nlow results between 0.24and0.57, and high standard de-\nviations between 0.32and0.38. Between 20% (γ= 0.3)\nand70% (γ= 1.0) of all training runs converged to the\nall-zero output, indicating a highly unstable training pro-\ncess of standard SDTW. In order to determine the cause\nof these instabilities, we analyze the quality of automati-\ncally generated soft alignments in the SDTW algorithm by\nvisualizing the soft alignment matrix for the running ex-\nample after training epochs one and 25, respectively. To\nhighlight the effects of small and large values of γ, we\nfocus on the edge cases γ∈{0.1,10.0}. Forγ= 0.1,\nthe estimated soft alignment exhibits a sharp structure (see\nFigure 4a), which, after a collapse to a single target frame\n(a)Epoch 1Target seq. (frames)Ref. align.\n01\nEγ\nCEpoch 25\n(b)\nTarget seq. (frames)\n 01\nEγ\nC\n(c)\nTarget seq. (frames)\n 01\nEγ\nC\n(d)\nTarget seq. (frames)\n 01\nEγ\nC\n(e)\nEstimated seq. (frames)Target seq. (frames)\n 01\nEγ\nC\nEstimated seq. (frames)\nFigure 4 : Reference alignment (green) and soft align-\nment matrix Eγ\nC(gray/black) for the running example af-\nter training epoch 1(left) and epoch 25(right) for different\ntraining strategies. (a)γ= 0.1,(b)γ= 10 ,(c)hyperpa-\nrameter scheduling, (d)diagonal prior, (e)sequence un-\nfolding.\nat epoch one, still only marginally overlaps with the refer-\nence alignment after 25 epochs. This sharp and erroneous\nsoft alignment causes unstable gradient updates and leads\nto the collapse of many training runs. When choosing a\nlarge softmin temperature γ= 10 , SDTW yields “blurry”\nsoft alignments (see Figure 4b) which at least partially cap-\nture the actual target frames in early epochs and coincide\nwell with the reference alignments as training progresses.\nHowever, a blurry soft alignment also leads to blurry net-\nwork predictions as multiple target frames are aligned to\neach predicted frame, thus resulting in a low F-measure\nwhen compared to strongly aligned targets..\n5.3 Stabilizing Strategies\nAfter evaluating the unsatisfactory training behavior of\nstandard SDTW, we investigate the effect of the previouslyProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n437introduced training strategies in the following section. We\nempirically choose γ= 0.1as the ﬁnal softmin tempera-\nture in all following experiments, as sharp alignments are\nnecessary for training an estimator with frame-wise preci-\nsion.\n5.3.1 Hyperparameter Scheduling\nFirst, we combine the advantages of high and low values\nofγin a hyperparameter scheduling strategy. Starting a\ntraining with γ= 10 , the soft alignment matrix for our run-\nning example after one epoch is blurry and at least partially\noverlapping with the reference alignment (see Figure 4c).\nThe successive reduction to γ= 0.1until epoch 20 permits\nsharp alignments at a later training stage. Indeed, Figure 4c\nshows a soft alignment after epoch 25 which is sharp and\ncoincides well with the reference. The mean F-measure\n(0.80) in Table 2, as well as the standard deviation ( 0.04),\nare the second best of all SDTW-based trainings. However,\nas the softmin function in (2) is a lower bound for the min-\nimum function [12] which becomes tight for γ→0, the\nSDTW loss is increasing when decreasing γ, despite un-\nchanged network parameters. Therefore, this strategy does\nnot allow for loss-based learning rate scheduling and early\nstopping before γis set to its ﬁnal value.\n5.3.2 Diagonal Prior\nThe second strategy stabilizes SDTW trainings with low\nvalues of γby adding a penalty cost to off-diagonal\nelements of the cost matrix. For our running example in\nFigure 4d, the soft alignment is indeed close to the di-\nagonal after the ﬁrst training epoch. As, on average, the\nalignments are diagonal, this often leads to correct assign-\nments of predictions and targets even for randomly initial-\nized DNNs. When the prior weight ωis reduced to zero\nafter the initial training phase, the network is still able to\nadapt to off-diagonal alignments, as seen in our running\nexample in Figure 4d. Analyzing the performance met-\nrics in Table 2, using a diagonal prior yields the highest\nmean F-measure ( 0.81) and the lowest standard deviation\n(0.02) of all SDTW variants, almost reaching the mean\nF-measure of the baseline experiments with strong targets\nand element-wise MSE loss. Moreover, when the prior\nweightωis reduced during training, the loss also decreases\nand therefore learning rate scheduling and early stopping\nare possible from the beginning.\n5.3.3 Sequence Unfolding\nLast, we investigate the strategy of unfolding the weak tar-\nget sequence to the length of the input, which was em-\nployed in [6]. For this strategy, we observe fully diagonal\nsoft alignments in the initial training phase, as visualized\nfor our running example in Figure 4e. This is caused by\nthe equal length of the predicted and the target sequence,\nwhich can be aligned using only diagonal steps. In the\nSDTW formulation from [10], the cost of a diagonal step\nis equal to the cost of a vertical or horizontal step. Thus,\nfor a uniform cost matrix (which is probable at the initialtraining phase due to random network initialization), tak-\ning a diagonal step only accumulates half the cost com-\npared to going “around the corner”, i.e., one step in the\nvertical and one in the horizontal direction, or vice versa.\nThis diagonalizing behavior leads, on average, to decent\nsoft alignments in the early training phase (as discussed in\nSection 5.3.2). However, in contrast to the additive diag-\nonal prior strategy, the implicit diagonalization of align-\nments is not reduced during the training, as can be seen\nin Figure 4e, which still exhibits strong diagonal compo-\nnents after 25 training epochs. Thus, the softly aligned\nSDTW targets seldom match the reference targets and per-\nformance remains low, resulting in a mean F-measure of\n0.53in Table 2.\nNote that the sequence unfolding strategy adds a sig-\nniﬁcant computational overhead compared to the previous\ntwo strategies, as unfolding always corresponds to using\na target sequence length of M=N. The forward and\nbackward pass of the SDTW loss function both have lin-\near complexity w.r.t. the sequence lengths O(MN)[10].\nThus, in our setting with N= 500 and a mean length of the\nweak target sequences in the test set of M= 24 , the un-\nfolding strategy leads to an increase in the computational\ncost of the SDTW loss by a factor of more than 20.\n6. CONCLUSION AND OUTLOOK\nIn this paper, we analyzed DNN training instabilities with\nSDTW as a loss function by the example of PCE. By anal-\nysis of the soft alignment matrix, we argued that align-\nment mismatch in the early training phase often causes\na collapse of the training procedure. Motivated by these\nﬁndings, we investigated three strategies for stabilizing the\nearly training phase. We found that the previously applied\nstrategy of unfolding the weakly aligned target sequence\nleads to almost exclusively diagonal alignments due to a\nnaïve weighting of horizontal, vertical, and diagonal align-\nment steps. Furthermore, this strategy is computation-\nally inefﬁcient, as it increases the target sequence length.\nIn contrast, the two introduced strategies of hyperparam-\neter scheduling and diagonal prior can be implemented\nwith negligible additional computational cost and stabi-\nlize SDTW-based training by two different mechanisms.\nThe hyperparameter scheduling strategy promotes smooth\nalignments in the early training phase, which increases the\nprobability of the predicted frame being at least partially\naligned to the correct target. Penalizing off-diagonal align-\nments in the SDTW cost matrix by an additive diagonal\nprior is a strategy that initially restricts the soft alignment\nto a region of high probability. Experimental evaluation\nshowed that these strategies reliably stabilize the SDTW\ntraining process. Implementing them as a default in the\nSDTW loss highly increases convergence rates.\nFuture research on SDTW-based loss functions in MIR\napplications might incorporate musically informed prior\ninformation, e.g., based on note durations or tempo anno-\ntations extracted from the musical score. Furthermore, the\npreference of diagonal alignment steps could be addressed\nby choosing different step weights.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n4387. ACKNOWLEDGEMENTS\nThis work was supported by the German Research Foun-\ndation (DFG MU 2686/7-2). The authors are with the In-\nternational Audio Laboratories Erlangen, a joint institution\nof the Friedrich-Alexander-Universität Erlangen-Nürnberg\n(FAU) and Fraunhofer Institute for Integrated Circuits IIS.\n8. REFERENCES\n[1] E. Benetos, S. Dixon, Z. Duan, and S. Ewert, “Auto-\nmatic music transcription: An overview,” IEEE Signal\nProcessing Magazine , vol. 36, no. 1, pp. 20–30, 2019.\n[2] F. Korzeniowski and G. Widmer, “Feature learning for\nchord recognition: The deep chroma extractor,” in Pro-\nceedings of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR) , New York City,\nNew York, USA, 2016, pp. 37–43.\n[3] C. Weiß, J. Zeitler, T. Zunner, F. Schuberth, and\nM. Müller, “Learning pitch-class representations from\nscore–audio pairs of classical music,” in Proceedings\nof the International Society for Music Information Re-\ntrieval Conference (ISMIR) , Online, 2021, pp. 746–\n753.\n[4] M. A. Bartsch and G. H. Wakeﬁeld, “Audio thumbnail-\ning of popular music using chroma-based representa-\ntions,” IEEE Transactions on Multimedia , vol. 7, no. 1,\npp. 96–104, 2005.\n[5] C. Weiß and G. Peeters, “Training deep pitch-class rep-\nresentations with a multi-label CTC loss,” in Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , Online, 2021, pp. 754–\n761.\n[6] M. Krause, C. Weiß, and M. Müller, “Soft dynamic\ntime warping for multi-pitch estimation and beyond,”\ninProceedings of the IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\nRhodes Island, Greece, 2023.\n[7] A. Graves, S. Fernández, F. J. Gomez, and J. Schmid-\nhuber, “Connectionist temporal classiﬁcation: La-\nbelling unsegmented sequence data with recurrent neu-\nral networks,” in Proceedings of the International\nConference on Machine Learning (ICML) , Pittsburgh,\nPennsylvania, USA, 2006, pp. 369–376.\n[8] C. Weiß and G. Peeters, “Learning multi-pitch esti-\nmation from weakly aligned score-audio pairs using\na multi-label CTC loss,” in Proceedings of the IEEE\nWorkshop on Applications of Signal Processing to Au-\ndio and Acoustics (WASPAA) , New Paltz, USA, 2021,\npp. 121–125.\n[9] M. Müller, Fundamentals of Music Processing – Us-\ning Python and Jupyter Notebooks , 2nd ed. Springer\nVerlag, 2021.[10] M. Cuturi and M. Blondel, “Soft-DTW: a differen-\ntiable loss function for time-series,” in Proceedings\nof the International Conference on Machine Learning\n(ICML) , Sydney, NSW, Australia, 2017, pp. 894–903.\n[11] A. Mensch and M. Blondel, “Differentiable dynamic\nprogramming for structured prediction and attention,”\ninProceedings of the International Conference on Ma-\nchine Learning (ICML) , Stockholmsmässan, Stock-\nholm, Sweden, 2018, pp. 3459–3468.\n[12] I. Hadji, K. G. Derpanis, and A. D. Jepson, “Repre-\nsentation learning via global temporal alignment and\ncycle-consistency,” in IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , Virtual,\n2021, pp. 11 068–11 077.\n[13] R. Agrawal, D. Wolff, and S. Dixon, “A convolutional-\nattentional neural framework for structure-aware\nperformance-score synchronization,” IEEE Signal Pro-\ncessing Letters , vol. 29, pp. 344–348, 2021.\n[14] M. Blondel, A. Mensch, and J. Vert, “Differentiable\ndivergences between time series,” in Proceedings of the\nInternational Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS) , Virtual, 2021, pp. 3853–3861.\n[15] K. Shih, R. Valle, R. Badlani, A. Ła ´ncucki, W. Pi-\nang, and B. Catanzaro, “RAD-TTS: Parallel ﬂow-\nbased TTS with robust alignment learning and diverse\nsynthesis,” in International Conference on Machine\nLearning (ICML), Third Workshop on Invertible Neu-\nral Networks, Normalizing Flows, and Explicit Likeli-\nhood Models , Virtual, 2021.\n[16] C. Weiß, F. Zalkow, V . Ariﬁ-Müller, M. Müller, H. V .\nKoops, A. V olk, and H. Grohganz, “Schubert Winter-\nreise dataset: A multimodal scenario for music anal-\nysis,” ACM Journal on Computing and Cultural Her-\nitage (JOCCH) , vol. 14, no. 2, pp. 25:1–18, 2021.\n[17] R. M. Bittner, B. McFee, J. Salamon, P. Li, and J. P.\nBello, “Deep salience representations for F0 tracking\nin polyphonic music,” in Proceedings of the Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR) , Suzhou, China, 2017, pp. 63–70.\n[18] D. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” in Proceedings of the International\nConference for Learning Representations (ICLR) , San\nDiego, California, USA, 2015.\n[19] M. Maghoumi, E. M. Taranta, and J. LaViola, “Deep-\nNAG: Deep non-adversarial gesture generation,” in\nProceedings of the International Conference on Intel-\nligent User Interfaces (IUI) , College Station, Texas,\nUSA, 2021, pp. 213–223.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n439"
    },
    {
        "title": "Symbolic Music Representations for Classification Tasks: A Systematic Evaluation.",
        "author": [
            "Huan Zhang",
            "Emmanouil Karystinaios",
            "Simon Dixon",
            "Gerhard Widmer",
            "Carlos Eduardo Cancino Chacón"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265421",
        "url": "https://doi.org/10.5281/zenodo.10265421",
        "ee": "https://zenodo.org/records/10265421/files/000101.pdf",
        "abstract": "Music Information Retrieval (MIR) has seen a recent surge in deep learning-based approaches, which often involve encoding symbolic music (i.e., music represented in terms of discrete note events) in an image-like or language-like fashion. However, symbolic music is neither an image nor a sentence intrinsically, and research in the symbolic domain is lacking a comprehensive overview of the different available representations. In this paper, we investigate matrix (piano roll), sequence, and graph representations and their corresponding neural architectures, in combination with symbolic scores and performances on three piece-level classification tasks. We also introduce a novel graph representation for symbolic performances and explore the capability of graph representations in global classification tasks. Our systematic evaluation shows advantages and limitations of each input representation.",
        "zenodo_id": 10265421,
        "dblp_key": "conf/ismir/ZhangKDWC23",
        "keywords": [
            "Music Information Retrieval",
            "Deep learning-based approaches",
            "Encoding symbolic music",
            "Intrinsic representations",
            "Matrix (piano roll)",
            "Sequence",
            "Graph representations",
            "Symbolic scores",
            "Performances",
            "Piece-level classification tasks"
        ],
        "content": "SYMBOLIC MUSIC REPRESENTATIONS FOR CLASSIFICATION TASKS:\nA SYSTEMATIC EV ALUATION\nHuan Zhang1Emmanouil Karystinaios2Simon Dixon1\nGerhard Widmer2Carlos Eduardo Cancino-Chacón2\n1Queen Mary University of London, United Kingdom\n2Johannes Kepler University, Austra\nABSTRACT\nMusic Information Retrieval (MIR) has seen a recent\nsurge in deep learning-based approaches, which often in-\nvolve encoding symbolic music (i.e., music represented in\nterms of discrete note events) in an image-like or language-\nlike fashion. However, symbolic music is neither an image\nnor a sentence, and research in the symbolic domain lacks a\ncomprehensive overview of the different available represen-\ntations. In this paper, we investigate matrix (piano roll), se-\nquence, and graph representations and their corresponding\nneural architectures, in combination with symbolic scores\nand performances on three piece-level classiﬁcation tasks.\nWe also introduce a novel graph representation for sym-\nbolic performances and explore the capability of graph rep-\nresentations in global classiﬁcation tasks. Our systematic\nevaluation shows advantages and limitations of each input\nrepresentation. Our results suggest that the graph represen-\ntation, as the newest and least explored among the three\napproaches, exhibits promising performance, while being\nmore light-weight in training.\n1. INTRODUCTION\nThe deep learning boom has profoundly impacted MIR,\nincluding research involving symbolic music representa-\ntions (MIDI, scores, etc.). A large body of recent literature\nfocuses on adapting existing architectures from computer\nvision and natural language processing to the ﬁeld of sym-\nbolic MIR. These approaches often treat music data as an\nimage (piano roll), as a sequence of language tokens, or,\nmore recently, as a graph. However, a piece of music is nei-\nther an image nor a sentence or graph, therefore, a critical\nquestion still remains open concerning the choice of input\nrepresentations for symbolic music.\nA source of complexity in symbolic music arises from\nthe different modalities of data such as scores and perfor-\nmances. A score contains information about music notation\n© H. Zhang, E. Karystinatos, S. Dixon, G. Widmer, C.E.\nCancino-Chacón. Licensed under a Creative Commons Attribution 4.0\nInternational License (CC BY 4.0). Attribution: H. Zhang, E. Karysti-\nnatos, S. Dixon, G. Widmer, C.E. Cancino-Chacón, “Symbolic Music\nRepresentations for Classiﬁcation Tasks: A Systematic Evaluation”, in\nProc. of the 24th Int. Society for Music Information Retrieval Conf., Milan,\nItaly, 2023.and often includes rich hierarchically structured informa-\ntion such as metrical structure and voicing. Symbolic music\nperformances, on the other hand, such as those recorded\non a MIDI-capable instrument, consist of a stream of con-\ntroller events. Extracting a hierarchical structure from such\na stream is not a trivial task [1 –3]. Furthermore, such perfor-\nmance data omit some of the rich information that a score\nprovides, such as pitch spelling and articulation markings,\nbut instead, it can include information about expression,\ntiming, local tempo, and performance dynamics.\nRecent research has produced relatively large datasets\ncontaining scores and performances at the symbolic level,\nincluding efforts to align these [4 –6]. Motivated by these\ndevelopments, we present an attempt to shed light on ques-\ntions revolving around the input representation of symbolic\nmusic for deep-learning-based MIR. We formulate an empir-\nical framework where we test multiple input representations,\nmodels, and piece-level classiﬁcation tasks.\nIn terms of input representations, we investigate piano\nrolls, tokenized sequences, and graphs. We evaluate multi-\nple models based on these representations on three differ-\nent tasks: composer classiﬁcation, performer classiﬁcation,\nand (playing) difﬁculty assessment. Furthermore, having\ndatasets containing both performances and their correspond-\ning scores such as ATEPP and ASAP [4, 5], allows us to\napply each combination of representation and task to ei-\nther score or performance. Our goal is to contribute an\nexperimental overview of different symbolic music repre-\nsentations. The contributions of this work are threefold:\n1.We investigate the performance and complexity of\nmatrix, sequence and graph input representations, and\ntheir corresponding neural architectures (respectively\nConvolutional Neural Networks, Transformers, and\nGraph Neural Networks).\n2.We compare the impact that the different information\ncontained in symbolic scores and performances has\non different piece-level classiﬁcation tasks.\n3.We introduce a new graph representation for symbolic\nperformances, and explore the capability of graph\nrepresentations in classiﬁcation tasks.\n2. RELATED WORK\nThe complexity of representing music data has been dis-\ncussed in the literature [7 –9]. Wiggins et al. [10] analyzed848the trade-offs of music representation systems with respect\nto expressive completeness and structural generality. In the\nage of deep learning, such considerations are still relevant\nregarding the variety of machine-readable representations\nsuch as piano rolls, MIDI-like sequences, NoteTuples, and\nMusical Spaces [11, 12]. In this section, we focus on three\nsymbolic representations (matrix, sequence, and graphs)\nand discuss their respective strengths and limitations.\nMusic as a Matrix: Similar to audio spectrograms, a\npitch-time representation that is typically used as input to\na CNN, the piano roll representation of music naturally\nemerges as the symbolic equivalent. Piano rolls have been\nwidely applied in tasks such as automatic music transcrip-\ntion [13, 14], classiﬁcation of piece-level attributes such as\ndifﬁculty and composer [15 –18], as well as generation of\nmusic accompaniment or performed dynamics [19, 20].\nA piano roll is a bare-bones representation of symbolic\nmusic data, and, therefore, information such as key signa-\ntures, articulation annotations, metrical structure, different\ninstrument parts, and voicing structure are not encoded in\nthe representation [11, 21].\nMusic as a Sequence: Modeling symbolic music as se-\nquences has a longstanding tradition in MIR. The multiple\nviewpoint system is a sequence representation that has been\nwidely used for music analysis, generation, and classiﬁca-\ntion [22 –25], as well as the basis for cognitively plausible\nmodels of expectation [26, 27]. In this system, musical\nelements are represented by viewpoints [28], which are ab-\nstract functions mapping musical events to abstract derived\nfeatures like pitch, interval, and melodic contour.\nWith the advances of deep learning-based language mod-\nels, sequential representation of music as language tokens\nhas recently received a lot of attention in sequence-to-\nsequence generative tasks from automatic orchestration [29]\nto description-based medley generation [30]. Similar to a\nstream of MIDI messages, various tokenization schemes\nencode music features such as pitch, onset time, duration,\nand velocity sequentially. Besides generation, large-scale\npre-training using music sequences has been applied to\ndownstream music understanding tasks [31, 32].\nHowever, tokenized music sequence representations cre-\nate difﬁculty for models to learn the dependency of long\ncontexts. Length reduction methods such as Byte Pair En-\ncoding (BPE) [29, 33] aim to address the length overﬂow\nproblem by replacing the occurrence of frequent subse-\nquences with new tokens.\nMusic as a Graph: A musical score can also be seen as a\ngraph where notes form the vertices and relations between\nnotes deﬁne the edges. Jeong and al. [34] introduced a\ngraph modeling of a musical score for generating expressive\nperformances. Recently, Karystinaios and Widmer [35]\npresented a new modeling of the score graph based on three\ndifferent note relations and a Graph Convolutional Network\nfor cadence detection in classical music. A score graph\ncan be homogeneous or heterogeneous, i.e. having one or\nseveral types of edges and/or vertices, respectively [36]. We\nwill investigate both heterogeneous and homogeneous score\ngraphs based on the representation used in [35].Graph Neural Networks have gained popularity in re-\ncent years, however, graph learning inherently presents\nsome limitations, such as over-smoothing in deep graph\nnetworks [37] and restrictions of Message Passing, where\ninformation in graph neural networks ﬂows only between\nedge relations predetermined by the representation (in con-\ntrast to a Transformer architecture where everything is in-\nterconnected [38]).\n3. METHODOLOGY\nIn this section, we describe the methodology followed, the\ncorpora used, and the experiments conducted to investigate\nin-depth the different symbolic representations.\n3.1 Representation Design\nWe brieﬂy introduce a formal deﬁnition of each representa-\ntion type, i.e. matrix, sequence, and graph. An example of\nthe three representations is shown in Figure 1.\n3.1.1 Matrix\nWe deﬁne as a matrix representation of music a 2-\ndimensional array M∈NH×Wthat depicts musical notes\non the time axis, commonly referred to as a piano roll. The\nvertical axis consists of 128 possible values attributed to\nthe MIDI pitch of note events, where we add three more\noptional ﬁelds for the una corda ,sostenuto , and sustain\npedals only applied on the MIDI performances.\nIn this work, we experimented with multiple channels\nas used in Onsets and Frames [39]. The onset channel is a\nbinarized roll with activations at onset timestamps, while\nthe frame channel encodes the duration of the note and the\nvelocity of the MIDI event. For scores, the velocity values\nare substituted by the voice index, i.e. the integer number\nassigned to a note to indicate the index among the number\nof independent voices.1\n3.1.2 Sequence\nA symbolic music sequence S∈N1×Nis deﬁned by a se-\nries of discrete tokens that represent attributes of notes. V o-\ncabularies such as Vpitch,VTimeShift ,VVelassign seman-\ntic meanings to tokens, and different tokenization schemes\ntranslate into different grammars of sequence construction.\nIn this work, we test three popular tokenization schemes:\nMIDILike [40, 41], REMI [42], and CompoundWord [43]\nand use the implementation of the MidiTok library [44].\nAs there is no existing tokenizer for processing scores,\nwe implemented custom MusicXML tokenizers following\nMidiTok’s framework, in the style of REMI as well as Com-\npoundWord . The major difference is the timing of bars and\nevent positions, as well as the addition of score-speciﬁc\ntokens such as VKeySig,VVoice .2\nByte Pair Encoding (BPE) is a tokenizer add-on tech-\nnique that has recently been applied to music sequence\nlearning [33]. It consists of a data compression technique\n1This voice information is commonly available in formats such as\nMusicXML, **Kern, and MEI.\n2Full documentation is provided with our open-source tokenizer in the\nproject repository.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n849Figure 1 . Excerpt of Schubert’s Impromptu Op. 90 No.4 and its input visualizations (from left to right): generic matrix,\nsequence (REMI-like) and graph.\nthat replaces the most common token subsequences in a\ncorpus with newly created tokens. BPE increases the vo-\ncabulary size and shortens the sequence length. We follow\nthe best results from [33] and adopt a BPE with 4 times\nthe original vocabulary size. On average, this reduced our\nsequence length between 55−65% in both datasets.\n3.1.3 Graph\nA homogeneous score graph Gis deﬁned by a tuple (V,E)\nof vertices and edges. Vis the set of notes in a musical score\nandE⊆V×V. Given a score with Nnotes, we extract\na matrix of k-dimensional note-wise features X∈RN×k\nbased on features contained in the score or performance.\nA heterogenous score graph G= (V,E,R)also includes\na set of relation types Rsuch that for every edge e∈E,\neis of type r∈ R if a condition deﬁned by rholds. In\nour work, we consider the following relations between two\nnotesu,vwhich deﬁne the edges e∈E:\n•uandvhave the same onset, i.e. on(v) =on(u),\nthenr=onset;\n•The offset of uis the onset of v, i.e.oﬀ(u) =on(v),\nthenr=consecutive;\n•The onset of ulies between the onset of vand the\noffset ofv, i.e.on(v)<on(u)∧on(u)<oﬀ(v),\nthenr=overlap.\nThe above relations only hold in the case of score graphs.\nTo adapt this to performance graphs, we use a window\ntolerance ttol, such that if two notes (u,v)∈Eand:\n•|on(v)−on(u)|< t tol, thenr=onset;\n•|oﬀ(u)−on(v)|< t tol, thenr=consecutive;\n•on(v)<on(u)∧on(u)<oﬀ(v), thenr=overlap .\nIn our conﬁgurations, for all graphs created from perfor-\nmance MIDI, we set ttol= 30ms , a perceptual threshold of\nexpressive timing [45]. In addition to the above relations,\nwe consider the possibility of adding an inversely directed\nedge for the overlap and the consecutive edge types, and\nwe name the inclusion of such edges inverse edges . For a\nhomogeneous graph Ghomand heterogeneous graph Ghet,\ne∈Ghom=⇒e∈Ghet.\nThe node features Xare divided into two categories,\nthe basic and the advanced features. The basic features are\nimplicitly contained in any score or performance note such\nas one-hot encoding of pitch class and octave of the note’s\npitch, and duration information. The advanced features\nFigure 2 . Left: front end for three representations, matrix,\ngraph, and sequence, from top to bottom. Right: ﬁxed back\nend with attention modules.\ncontains articulation, dynamics, and notation information\nfrom the Partitura python package [46]. The detailed com-\nputation of these features can be found in original partitura\npaper [47] and the basis mixer [48].\n3.1.4 Information Levels\nGiven the differences in information captured by symbolic\nscores and performances (Sec. 1), we run experiments with\nseparate levels of used information. For the base compar-\nison experiments, we input the basic level of information\nthat is present in both modalities: pitch, duration and onset.\nThe advanced level of information for performance includes\ndynamics (MIDI velocity) and pedals, while for score in-\ncludes the voice index (Sec. 3.1) as well as score markings\nsuch as articulation and dynamics. The results and com-\nparison of each level of information, also with respect to\ndifferent tasks, will be discussed in Section 4.3.\n3.2 Modelling Pipelines\nIn this work, we evaluate the input representations under the\nsame training pipeline of different piece-level classiﬁcation\ntasks, as discussed in Section 3.3. We split our training ar-\nchitecture into two parts, a front end that projects a window\nof musical context into a 64-dimensional embedding, and a\nback end that aggregates the embedding for ﬁnal prediction.\nThe front end is representation-speciﬁc while the back endProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n850ASAP-performance ASAP-score ATEPP-performance ATEPP-score\nACC F1 ACC F1 ACC F1 ACC F1\nMatrix\nResl Chnl\n400 On+Fm 0.59±0.04 0.18±0.02 0.59±0.03 0.18±0.01 0.24±0.05 0.20±0.04 0.25±0.02 0.16±0.03\n600 On+Fm 0.62±0.06 0.21±0.03 0.61±0.07 0.19±0.02 0.28±0.01 0.22±0.03 0.24±0.02 0.16±0.04\n800 Fm 0.62±0.04 0.21±0.02 0.58±0.06 0.18±0.03 0.22±0.03 0.17±0.01 0.22±0.02 0.18±0.03\n800 On+Fm 0.63±0.04 0.20±0.01 0.57±0.04 0.18±0.03 0.28±0.02 0.22±0.01 0.22±0.04 0.14±0.02\nSequence\nTokn BPE\nMidiLike × 0.53±0.05 0.16±0.02 N/A N/A 0.18±0.04 0.10±0.02 N/A N/A\nREMI × 0.51±0.04 0.15±0.02 0.43±0.04 0.14±0.01 0.23±0.04 0.10±0.02 0.23±0.04 0.13±0.02\nCP × 0.48±0.02 0.09±0.05 0.45±0.05 0.10±0.01 0.11±0.02 0.09±0.01 0.17±0.06 0.11±0.04\nMidiLike 4 0.52±0.04 0.15±0.02 N/A N/A 0.17±0.03 0.12±0.01 N/A N/A\nREMI 4 0.51±0.02 0.15±0.01 0.43±0.03 0.13±0.01 0.21±0.01 0.13±0.03 0.23±0.03 0.13±0.01\nGraph\nBi-dir Multi-rel\n× × 0.56±0.01 0.17±0.02 0.51±0.05 0.16±0.02 0.22±0.02 0.10±0.03 0.23±0.03 0.21±0.05\n× ✓ 0.58±0.03 0.19±0.01 0.54±0.05 0.17±0.02 0.27±0.03 0.13±0.02 0.29±0.10 0.18±0.06\n✓ ✓ 0.62±0.02 0.21±0.01 0.50±0.04 0.17±0.01 0.23±0.04 0.16±0.03 0.27±0.06 0.22±0.03\nTable 1 . Composer classiﬁcation results for all representations, on all target subsets of our datasets on the composer\nclassiﬁcation task using only basic level features. For each subset of data, we present the accuracy score and the macro F1\nscore with 8-fold cross-validation. See Section 4.1 for explanation of the parameters.\nrests ﬁxed. For a fair comparison, we ensure that the same\namount of musical context is given for different front ends\nto learn. For MIDI performances we ﬁx a window of 60s,\nand for symbolic scores, we choose a window of 120 beats\ngiven that 120bpm is a common tempo for music.\nFor the front end, we employ a commonly used architec-\nture for each respective representation domain:\nMatrix: Convolutional neural network based on ResNet\n[49] blocks with channel numbers adapted to our input.\nSequence: Transformer-encoder [50] front end with po-\nsitional encoding. Each layer includes multi-head attention\nwith 16 heads followed by an Add & Norm layer. For\nthe combined tokens CPWord we add separate embedding\nlayers for each token category in the front end.\nGraph: Our graph convolution network (GCN) is built\nby stacking GraphSAGE blocks [51] followed by a global\nmean pooling layer. We experiment with both heteroge-\nneous and homogeneous GraphSAGE. Note that a hetero-\ngeneous network has rtimes more parameters, where ris\nthe number of distinct edge relation types.\nFor the ﬁxed back end, we used a multi-head attention\nblock with linear projection heads to the desired number of\nclasses, as shown in Figure 2. To minimize the impact of\nmodel capacity on our comparative discussion, we carried\nout an ablation study to understand the size of the architec-\nture proportional to each kind of representation (Sec. 4.2).\n3.3 Tasks and Datasets\nIn this work, we focus on three tasks: composer classiﬁ-\ncation, performer classiﬁcation, and difﬁculty assessment.\nEach one of these tasks is a piece-level task since a label is\nattributed per piece. The composer classiﬁcation consists of\npredicting the composer of the piece. The performer clas-siﬁcation involves the prediction of the performer among\na list of predeﬁned performers included in the data source.\nFinally, difﬁculty assessment involves the prediction of a\nnumber between 1-9, with 1 being easy and 9 being hard.\nThe difﬁculty labels were assembled from Henle Music.3\nTo evaluate the aforementioned tasks, we use two large-\nscale collections of Western classical piano music that con-\ntain corresponding symbolic scores (MusicXML ﬁles) and\nperformances (MIDI ﬁles), ASAP (1067 performances, 245\nscores) and ATEPP (11742 performances, 415 scores). Both\ndatasets contain individual ﬁles per movement.\nFor the composer classiﬁcation task, we exclude the least\npopulated composer classes for balance in experiments,\nresulting in 10 classes for the ASAP dataset and 9 classes\nfor the ATEPP dataset. The performer classiﬁcation task\nuses MIDI performances of ATEPP with 20 classes. For\ndifﬁculty, given that both ASAP and ATEPP datasets focus\non concert repertoire, the actual classes used range from\ndifﬁculty 4-9.4For all experiments, we use an eight-fold\ncross-validation evaluation where 85% of our data is used\nfor training and 15% for testing in each fold.\n3.4 Training\nWe performed hyperparameter optimization sweeps to deter-\nmine the optimal learning rate and model hyperparameters.\nOur convergence criteria include early stopping at the 60\nepoch breakpoint with the patience parameter set at 0.005\non the validation accuracy. All our experiments are trained\non a single A5000 GPU, and the best models, training logs,\n3Henle Music difﬁculty labels, https://www.henle.de/en/\nabout-us/levels-of-difficulty-piano/\n4The full distribution of the classes for each task is shown in the\nsupplementary material.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n851and the code is available in the repository.5\n4. EXPERIMENTS AND RESULTS\nTo evaluate the different representations we performed three\nexperiments. Our ﬁrst experiment focuses on a detailed\ncomparison of the predictive accuracy of the three represen-\ntations/architectures applied to the composer classiﬁcation\ntask, since it is the most well-understood task among the\nthree. The second experiment studies the impact of model\ncapacity (number of trainable parameters) per representa-\ntion. Our last experiment investigates the effect of different\nlevels of input features (see Section 3.1.4) on the three tasks.\n4.1 Representations for Composer Classiﬁcation\nOur ﬁrst experiment is a comparative analysis of the three\nrepresentations on our two datasets, in the domains of both\nMIDI performance and MusicXML score with basic level\nfeatures. For each representation group we test different\nconﬁgurations, i.e. for matrix we experiment with the chan-\nnel (Chnl) and timestep resolution (Resl), for sequence we\nchange the tokenization scheme (Tokn) and apply BPE,\nand for graph we investigate the effect of homogeneous\nor heterogeneous graphs (Multi-rel) and the addition of in-\nverse edges (Bi-dir) (see Sec. 3.1). In Table 1, we present\nfor each data subset the accuracy score and the macro F1\nscore and their respective standard deviations under 8-fold\ncross-validation (see Sec. 3.3).\nIn terms of observations per representation, the matrix\nrepresentation results indicate no signiﬁcant differences un-\nder different experimental conﬁgurations. For sequence rep-\nresentations, the MIDILike andREMI tokenization schemes\nyield comparable performance. However, our experiments\nsuggest that CPWord is a more challenging representation\nto learn in the same setting. Concerning the BPE technique,\nno signiﬁcant difference is observed between results with 4\ntimes the original vocabulary and the non-BPE version.\nOur graph-based models exhibit similar performance\nregardless of the conﬁguration of the graph edges. In par-\nticular, the effect of reverse edges is not signiﬁcant, and\nhomogeneous graph convolution already achieves similar\nresults to heterogeneous graph convolutional models, which\nindicates that implicit structural information contained in\nthe heterogeneous approach is not strictly necessary for\npiece-level classiﬁcation tasks.\nOverall, we observe that three representations show\nsmall performance differences in given experiments, with\nthe matrix-CNN approach having the overall best metric\nacross the experiment groups and sequence have the worst.\nFinally, we would like to discuss the album effect , which\nconcerns the tendency of classiﬁcation models to learn non-\nintended features, such as acoustic features in pieces of\nthe same album [52]. In our case, this effect concerns\ndifferent performances of the same piece that may give\naway cues for classiﬁcation. Training with the entire corpus\nof performance MIDI, which involves different interpreta-\ntions of the same piece, yields an average accuracy of 90%\n5https://github.com/anusfoil/SymRep\nFigure 3 . Model capacity vs. macro F1 score for each\nrepresentation approaches on the ASAP-composer task.\n(see supplementary material), which is 30% higher for the\nASAP-perf group. To address this issue, we ﬁx the splits\nto only contain unseen pieces in the test set, which reduced\nthe accuracy score gap between performance and score.\nThis issue has often been overlooked in literature [53, 54]\nand a commonly-used dataset split is not piece-speciﬁc [16].\nGiven the recent development of large score-performance\ndatasets, we wish to establish a scientiﬁcally correct evalua-\ntion split taking into consideration the piece effect .\n4.2 Complexity\nIn our second experiment we investigate the impact of\nmodel capacity for each representation on the composer\nclassiﬁcation task using the ASAP dataset. We experiment\nwith different hidden dimensions hand the number of lay-\nersNon each architecture corresponding to each of the\nthree representations (Sec 3.2), and show our results in Fig-\nure 3. Overall, we observe that the GCN achieves its best\nperformance using 1.3M parameters, while architectures for\nmatrix and sequence achieve a similar accuracy at around\nthree times the number of parameters.\nAnother observation concerns the use of large models\nfor piece-level classiﬁcation tasks on symbolic data. Large\nconvolution models such as ResNet-18/34/50 [16] are sub-\nstantially over-parametrized, as our results suggest we can\nachieve similar results using a reduced version of ResNet-8,\nusing less than half the parameters of the smallest used\nResNet architecture. Similar observations can be made for\ntransformers, where scaling the model beyond 4.3M param-\neters does not further improve the performance. Our most\nefﬁcient transformer encoder consists of 4 layers of atten-\ntion modules with a hidden dimension of 256, signiﬁcantly\nless than transformers used in previous related work [33].\nFinally, we note one aspect of our results after scaling\nour graph network. While oversmoothing [37] (features\nof graph vertices converging to the same value) is a well-\nknown challenge to train deep GCN, our best performing\nmodel is a relatively deep and narrow network consisting\nof 5 layers with a hidden dimension of 64. One possible\ninterpretation is that convergence of node features does notProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n852Composer Performer Difﬁculty\nperf score perf (ATEPP) perf score\nMatrix\nbasic feats 0.625 0.572 0.364 0.403 0.420\nadvanced feats 0.618 0.577 0.342 0.411 0.415\nSequence\nbasic feats 0.530 0.447 0.287 0.438 0.368\nadvanced feats 0.513 0.393 0.292 0.426 0.349\nGraph\nbasic feats 0.607 0.545 0.305 0.373 0.361\nadvanced feats 0.598 0.697 0.323 0.356 0.405\nTable 2 . Accuracy of three identiﬁcation tasks on the ASAP\ndataset, with basic or higher-level features.\ncomplicate training in the graph-level classiﬁcation context.\n4.3 Comparison of Feature Levels and Tasks\nAs discussed in Section 3.1.4, we are also interested in\nunderstanding the impact of different levels of features on\nthe three classiﬁcation tasks. With this motivation, we\nperformed our third set of experiments, where we adopted\nthe best conﬁguration of models explored in experiment 1\n(see Section 4.1). We report the accuracy results in Table 2.\nOur results indicate that MIDI performances and Mu-\nsicXML scores have similar capabilities for distinguishing\ncomposers and difﬁculty. Furthermore, matrix and sequence\napproaches exhibit better results when learning with perfor-\nmances compared to scores. For the difﬁculty classiﬁcation\ntask, in particular, all three representations achieved approx-\nimately 40% accuracy on the 6 difﬁculty levels. Performer\nclassiﬁcation is more challenging since the difference lies in\nthe timing nuances and dynamic changes instead of the pitch\ninformation, which are more prominent in our input rep-\nresentations. In the 20-way classiﬁcation, our approaches\ngenerally achieved around 30% accuracy.\nOur observations suggest that the addition of advanced\nfeatures has a variable impact on the representations. Inter-\nestingly, the addition of advanced features does not improve\nthe training from sequence representations in most experi-\nments, which can possibly be explained by the increase in\nvocabulary size and relative sparsity of such information.\nGraph structures beneﬁt from the addition of voice edges,\nespecially in the representation of scores, where the perfor-\nmance boosts for both composer and difﬁculty classiﬁcation.\nNotably, the graph-score with advanced features con-\nﬁguration achieved the best result in score-based composer\nclassiﬁcation, when jointly compared with Table 1.\n4.4 Transformer vs. GNN: Are We Learning the Same\nSet of Musical Edges?\nA transformer can be seen as a special case of Graph Neural\nNetworks [38]. Assuming a fully connected graph where\nvertices are tokens in a sequence, we can draw parallels be-\ntween a GCN and learned attention in a transformer block.\nTherefore, we examine attention weights between\nNoteOn tokens in an effort to understand how our graph\nrepresentation of the score relates to the sequence-based\nrepresentation. For all pairs of NoteOn tokens from music\nFigure 4 . Visualization of graph edges (all edge types\naggregated) and the attention among NoteOn tokens for\nthe ﬁrst measures of Mozart Piano Sonata No.12, 1st mvt .\nsequences, we output their attention values and compute\nthe correlation with the aggregated adjacency matrix (with\nall musical edges constructed in Sec. 3.1). Across the test\nset of ASAP composer classiﬁcation on scores, there is a\nweak positive correlation, with Pearson’s value of 0.212.\nIn Figure 4, we visualize two measures of music with its\nconstructed graph edges, and the attention across NoteOn\ntokens. We can observe some structural similarities, espe-\ncially the overlap pattern in both measures, but overall the\nlearned attention spans are much more global while graph\nedges connect nodes within a local range.\n5. DISCUSSION AND FUTURE WORK\nIn this paper, we presented a series of systematic experi-\nments to investigate the impact of symbolic representations\nfor three piece-level tasks. In terms of simple classiﬁca-\ntion performance , we found that for a given task, different\nrepresentations showed small performance differences, but\nno clear pattern of superiority emerged. The matrix results\nwere marginally better on average, and usually more robust\nto hyper-parameter changes. More advanced features were\nbeneﬁcial only for certain tasks and representations.\nThe graph representation , as the newest and least ex-\nplored among the three approaches, exhibits promising\nperformance, while being more light-weight (in terms of\nrequired model complexity – cf. Fig. 3). We observe that\nhomogeneous graphs produce comparable results to het-\nerogeneous graphs for our piece-level classiﬁcation tasks,\nand deep GCNs perform better despite over-smoothing. As\ngraphs are arguably a more natural representation for struc-\ntured artifacts such as musical scores, we believe that they\nshould merit more detailed studies in the future.\nOur model complexity experiments demonstrated that\ncommonly used architectures in the literature are larger than\nnecessary for our tasks, as the same results can be achieved\nwith smaller architectures (Section 4.2). Furthermore, we\ndiscussed the album effect in score-performance datasets,\nwhere multiple interpretations of the same composition\nmay cause information leakage. Our results indicate the\nprofound impact of the album effect, and we introduce new\nevaluation splits to guard against this effect.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8536. ACKNOWLEDGEMENTS\nThis work is supported by the UKRI Centre for Doctoral\nTraining in Artiﬁcial Intelligence and Music, funded by UK\nResearch and Innovation [grant number EP/S022694/1],\nalso by the European Research Council (ERC) under the\nEU’s Horizon 2020 research and innovation programme,\ngrant agreement No. 101019375 ( Whither Music? ).\n7. REFERENCES\n[1]L. Liu, Q. Kong, V . Morﬁ, and E. Benetos, “Perfor-\nmance MIDI-to-score conversion by neural beat track-\ning,” in Proceedings of the International Society for\nMusic Information Retrieval Conference (ISMIR) , 2022.\n[2]D. Temperley, “A uniﬁed probabilistic model for\npolyphonic music analysis,” Journal of New Music\nResearch , vol. 38, no. 1, pp. 3–18, 2009. [Online].\nAvailable: https://doi.org/10.1080/09298210902928495\n[3]D. Temperley, The Cognition of Basic Musical Struc-\ntures . MIT Press, 2004.\n[4] H. Zhang, J. Tang, S. Rafee, S. Dixon, and G. Fazekas,\n“ATEPP: A Dataset of Automatically Transcribed Ex-\npressive Piano Performance,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2022.\n[5]S. D. Peter, C. E. Cancino-Chacón, F. Foscarin, A. P.\nMcLeod, F. Henkel, E. Karystinaios, and G. Widmer,\n“Automatic note-level score-to-performance alignments\nin the ASAP dataset,” Transactions of International So-\nciety for Music Information Retrieval (in press) , 2023.\n[6]F. Foscarin, E. Karystinaios, S. D. Peter, C. Cancino-\nChacón, M. Grachten, and G. Widmer, “The match\nﬁle format: Encoding alignments between scores and\nperformances,” in Proceedings of the Music Encoding\nConference (MEC) , 2022.\n[7]I. Xenakis, Formalized Music: Thoughts and Mathemat-\nics in Composition , 1992.\n[8]M. Harris, A. Smaill, and G. Wiggins, “Repre-\nsenting Music Symbolically,” in IX Colloquio di\nInformatica Musicale (Venice) , 1991. [Online]. Avail-\nable: http://citeseerx.ist.psu.edu/viewdoc/summary?\ndoi=10.1.1.43.473\n[9]M. Babbit, “The use of computers in musicological\nresearch,” Perspectives of New Music , vol. 3, no. 2, pp.\n74–83, 1965.\n[10] G. Wiggins, E. Miranda, A. Smaill, and M. Harris, “A\nFramework for the Evaluation of Music Representation\nSystems,” Computer Music Journal , vol. 17, no. 3, pp.\n31–42, 1993. [Online]. Available: https://about.jstor.\norg/terms\n[11] C. Walder, “Modelling symbolic music: Beyond the\npiano roll,” in Journal of Machine Learning Research ,\nvol. 63, 2016, pp. 174–189.[12] M. Prang, “Representation learning for symbolic music,”\nPh.D. dissertation, IRCAM, 2021. [Online]. Available:\nhttps://hal.archives-ouvertes.fr/tel-03329980\n[13] E. Benetos, A. Klapuri, and S. Dixon, “Score-informed\ntranscription for automatic piano tutoring,” in European\nSignal Processing Conference (EUSIPCO) , 2012.\n[Online]. Available: http://c4dm.eecs.qmul.ac.uk/rdr/\n[14] Q. Kong, B. Li, X. Song, Y . Wan, and Y . Wang, “High-\nResolution Piano Transcription with Pedals by Regress-\ning Onset and Offset Times,” IEEE/ACM Transactions\non Audio Speech and Language Processing , vol. 29, pp.\n3707–3717, 2021.\n[15] Y . Ghatas, M. Fayek, and M. Hadhoud, “A hybrid deep\nlearning approach for musical difﬁculty estimation of pi-\nano symbolic music,” Alexandria Engineering Journal ,\nvol. 61, no. 12, pp. 10 183–10 196, 2022.\n[16] S. Kim, H. Lee, S. Park, J. Lee, and K. Choi,\n“Deep Composer Classiﬁcation Using Symbolic\nRepresentation,” in International Society for Music\nInformation Retrieval (ISMIR) Late Breaking Demo\n(LBD) , 2020. [Online]. Available: http://arxiv.org/abs/\n2010.00823\n[17] G. Velarde, T. Weyde, C. E. Cancino-Chacón,\nD. Meredith, and M. Grachten, “Composer recognition\nbased on 2D-ﬁltered piano-rolls,” in Proceedings\nof the International Society for Music Information\nRetrieval Conference (ISMIR) , 2016. [Online].\nAvailable: https://www.semanticscholar.org/paper/\nComposer-Recognition-Based-on-2D-Filtered-Velarde-Weyde/\n2ee8df37e3f5363c573b2aeed2243034ea638f71\n[18] F. Foscarin, K. Hoedt, V . Praher, A. Flexer,\nand G. Widmer, “Concept-Based Techniques for\n\"Musicologist-friendly\" Explanations in a Deep Music\nClassiﬁer,” in Proceedings of the International Society\nfor Music Information Retrieval Conference (ISMIR) ,\n2022. [Online]. Available: http://arxiv.org/abs/2208.\n12485\n[19] H. W. Dong, W. Y . Hsiao, L. C. Yang, and Y . H.\nYang, “MuseGAN: Multi-track sequential generative\nadversarial networks for symbolic music generation\nand accompaniment,” in Proceedings of the 32nd AAAI\nConference on Artiﬁcial Intelligence , 2018. [Online].\nAvailable: https://salu133445.github.io/musegan/\n[20] S. van Herwaarden, M. Grachten, W. de Haas, and\nW. Bas de Haas, “Predicting expressive dynamics in\npiano performances using neural networks,” in Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , 2014.\n[21] J.-P. Briot, G. Hadjeres, and F.-D. Pachet, Deep\nLearning Techniques for Music Generation – A Survey ,\n2017. [Online]. Available: http://arxiv.org/abs/1709.\n01620Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n854[22] D. Conklin and I. H. Witten, “Multiple viewpoint\nsystems for music prediction,” Journal of New Music\nResearch , vol. 24, no. 1, pp. 51–73, 1995. [Online].\nAvailable: https://doi.org/10.1080/09298219508570672\n[23] D. Conklin, “Multiple viewpoint systems for music\nclassiﬁcation,” Journal of New Music Research ,\nvol. 42, no. 1, pp. 19–26, 2013. [Online]. Available:\nhttps://doi.org/10.1080/09298215.2013.776611\n[24] R. P. Whorley and D. Conklin, “Music generation\nfrom statistical models of harmony,” Journal of New\nMusic Research , vol. 45, no. 2, pp. 160–183, 2016.\n[Online]. Available: https://doi.org/10.1080/09298215.\n2016.1173708\n[25] D. Conklin, “Chord sequence generation with semiotic\npatterns,” Journal of Mathematics and Music , vol. 10,\nno. 2, pp. 92–106, 2016. [Online]. Available:\nhttps://doi.org/10.1080/17459737.2016.1188172\n[26] M. T. Pearce, “Statistical learning and probabilistic\nprediction in music cognition: Mechanisms of stylistic\nenculturation,” Annals of the New York Academy of\nSciences , vol. 1423, no. 1, pp. 378–395, 2018. [Online].\nAvailable: https://nyaspubs.onlinelibrary.wiley.com/\ndoi/abs/10.1111/nyas.13654\n[27] M. Pearce, “The Construction and Evaluation of Statis-\ntical Models of Melodic Structure in Music Perception\nand Composition,” Ph.D. dissertation, City University\nof London, UK, 2005.\n[28] D. Conklin and I. H. Witten, “Multiple Viewpoint Sys-\ntems for Music Prediction,” Journal of New Music Re-\nsearch , vol. 24, no. 1, pp. 51–73, 1995.\n[29] J. Liu, Y . Dong, Z. Cheng, X. Zhang, X. Li, F. Yu,\nand M. Sun, “Symphony Generation with Permutation\nInvariant Language Model,” in Proceedings of the\nInternational Society for Music Information Retrieval\nConference (ISMIR) , 2022. [Online]. Available: http:\n//arxiv.org/abs/2205.05448\n[30] D. von Rütte, L. Biggio, Y . Kilcher, and T. Hofmann,\n“FIGARO: Generating Symbolic Music with Fine-\nGrained Artistic Control,” in Proceedings of the\nInternational Conference on Learning Representations\n(ICLR) , 2023. [Online]. Available: http://arxiv.org/abs/\n2201.10936\n[31] M. Keller, G. Loiseau, and L. Bigo, “What\nMusical Knowledge Does Self-Attention Learn?” in\nProceedings of the 2nd Workshop on NLP for Music and\nSpoken Audio (NLP4MusA) , 2021, pp. 6–10. [Online].\nAvailable: https://aclanthology.org/2021.nlp4musa-1.2\n[32] M. Zeng, X. Tan, R. Wang, Z. Ju, T. Qin, and T. Y .\nLiu, “MusicBERT: Symbolic Music Understanding\nwith Large-Scale Pre-Training,” in Findings of the As-\nsociation for Computational Linguistics: ACL-IJCNLP ,\n2021.[33] N. Fradet, J.-P. Briot, F. Chhel, A. E. F. Seghrouchni,\nand N. Gutowski, “Byte Pair Encoding for Symbolic\nMusic,” 2023. [Online]. Available: http://arxiv.org/abs/\n2301.11975\n[34] D. Jeong, T. Kwon, Y . Kim, and J. Nam, “Graph neural\nnetwork for music score data and modeling expressive\npiano performance,” in International Conference on\nMachine Learning . PMLR, 2019, pp. 3060–3070.\n[35] E. Karystinaios and G. Widmer, “Cadence detection in\nsymbolic classical music using graph neural networks,”\ninProceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR) , 2022.\n[36] C. Shi, Y . Li, J. Zhang, Y . Sun, and S. Y . Philip, “A\nsurvey of heterogeneous information network analysis,”\nIEEE Transactions on Knowledge and Data Engineer-\ning, vol. 29, no. 1, pp. 17–37, 2016.\n[37] G. Li, M. Muller, A. Thabet, and B. Ghanem,\n“DeepGCNs: Can GCNs go as deep as CNNs?” in\nProceedings of the IEEE International Conference\non Computer Vision , 2019. [Online]. Available:\nhttps://sites.google.com/view/deep-gcns\n[38] P. Veli ˇckovi ´c, “Everything is Connected: Graph Neural\nNetworks,” Artiﬁcial Intelligence (AI) Methodology\nin Structural Biology , 2023. [Online]. Available:\nhttp://arxiv.org/abs/2301.08210\n[39] C. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Simon,\nC. Raffel, J. Engel, S. Oore, and D. Eck, “Onsets and\nframes: Dual-objective piano transcription,” in Proceed-\nings of the International Society for Music Information\nRetrieval Conference (ISMIR) , 2018, pp. 50–57.\n[40] S. Oore, I. Simon, S. Dieleman, D. Eck, and K. Si-\nmonyan, “This time with feeling: learning expressive\nmusical performance,” Neural Computing and Applica-\ntions , vol. 32, no. 4, pp. 955–967, 2018.\n[41] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer,\nI. Simon, C. Hawthorne, A. M. Dai, M. D. Hoffman,\nM. Dinculescu, and D. Eck, “Music Transformer,”\ninProceedings of the International Conference of\nLearning Representations (ICLR) , 2019. [Online].\nAvailable: http://arxiv.org/abs/1809.04281\n[42] Y . S. Huang and Y . H. Yang, “Pop Music Transformer:\nBeat-based Modeling and Generation of Expressive Pop\nPiano Compositions,” in Proceedings of the 28th ACM\nInternational Conference on Multimedia , 2020.\n[43] W.-Y . Hsiao, J.-Y . Liu, Y .-C. Yeh, and Y .-H. Yang,\n“Compound Word Transformer: Learning to Compose\nFull-Song Music over Dynamic Directed Hypergraphs,”\ninProceedings of the 35th AAAI Conference on Artiﬁ-\ncial Intelligence , 2021.\n[44] N. Fradet, J.-P. Briot, F. Chhel, A. El Fallah\nSeghrouchni, and N. Gutowski, “Miditok: a Python\nPackage for Midi File Tokenization,” in InternationalProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n855Society for Music Information Retrieval (ISMIR) Late\nBreaking Demo (LBD) , 2021.\n[45] W. Goebl, “Melody lead in piano performance:\nExpressive device or artifact?” The Journal of the\nAcoustical Society of America , vol. 110, p. 641, 2001.\n[Online]. Available: https://asa.scitation.org/doi/10.\n1121/1.1376133\n[46] C. Cancino-Chacón, S. D. Peter, E. Karystinaios, F. Fos-\ncarin, M. Grachten, and G. Widmer, “Partitura: A\nPython package for symbolic music processing,” in Pro-\nceedings of the Music Encoding Conference (MEC) ,\n2022.\n[47] C. Cancino-Chacón, S. D. Peter, E. Karystinaios,\nF. Foscarin, M. Grachten, and G. Widmer, “Partitura:\nA Python Package for Symbolic Music Processing,”\npp. 1–9, 2022. [Online]. Available: http://arxiv.org/abs/\n2206.01071\n[48] C. E. Cancino-Chacón, M. Grachten, W. Goebl, and\nG. Widmer, “Computational Models of Expressive Mu-\nsic Performance: A Comprehensive and Critical Re-\nview,” Frontiers in Digital Humanities , vol. 5, no. Octo-\nber, pp. 1–23, 2018.\n[49] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image recognition,” in Proceedings of the\nIEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2016.\n[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” in Proceedings of the 31st\nInternational Conference on Neural Information Pro-\ncessing Systems , 2017.\n[51] W. L. Hamilton, R. Ying, and J. Leskovec, “Inductive\nRepresentation Learning on Large Graphs,” in Proceed-\nings of the 31st International Conference on Neural\nInformation Processing Systems , 2017.\n[52] A. Flexer, “A closer look on artist ﬁlters for musical\ngenre classiﬁcation,” in Proceedings of the International\nSociety for Music Information Retrieval Conference\n(ISMIR) , 2007.\n[53] G. Micchi, “A neural network for composer classiﬁca-\ntion,” in Proceedings of the International Society for\nMusic Information Retrieval Conference (ISMIR) Late-\nBreading Demo (LBD) , 2018.\n[54] Q. Kong, K. Choi, and Y . Wang, “Large-Scale MIDI-\nBased Composer Classiﬁcation,” in arXiv , 2020.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n8568. APPENDIX\n8.1 Album effect\nAs mentioned in the main paper Section 4, the Album Effect\nremains a non-trivial issue in similar classiﬁcation tasks.\nHere we present in Table 4 the same content as the original\ntable (Table 1) from the paper which contains results from\nthe experiment that is trained on the entire performance\ncorpus with overlapping interpretations. Training under this\nnon-piece-speciﬁc split, we achieved comparable accuracy\n(93%) with the literature [16].\n8.2 Complexity\n8.2.1 Memory\nGiven that the same amount of music context is input into\nthe models, we are interested in understanding the memory\nefﬁciency of the representations. We used the native numpy\nandcuda functions to monitor the memory of data and\nmemory changes during training.\nIn terms the representation of a single piece of data, se-\nquence is the most compact one while matrix takes 70 ×\nmore space, given that a lot of redundant pixels are taken\nin the 2D representation. The size of graph varies depend-\ning on the number of nodes and edges, but overall it is in\nbetween that of the matrix and sequence.\nHowever, during training we can observe that the se-\nquence is the least memory-efﬁcient representation during\ntraining, and it takes 30 ×compares to the memory usage\nof matrix and graphs. Given the quadratic complexity of\ntransformer-like architectures, the training memory needed\nis one of the major limitation of sequence compared to the\nother representations.\nKB / seg KB / piece Training step (MB)\nMtr 819.2 5129.6 ± 3332.7 185.9 ± 105.9\nSeq 12.8 77.8 ± 56.7 5548.9 ± 1736.2\nGph 100.5 ± 57.3 610.9 ± 300.0 125.2 ± 103.4\nTable 3 . Size estimation of each representation with basic\nlevel features from ASAP-perf data. We include the average\nsize per segment (60s), average size per piece (as piece have\ndifferent length), as well as the average allocated memory\nincrease during each training step with a batch size of 1.\n8.2.2 Convergence epochs\nDuring training, we also observed a difference in the time\nit takes the models to convergence, given the 60 epochs\nconvergence criteria deﬁned in Sec 3.4. We ﬁrst performed\nlearning rate search using pytorch lightning’s learning rate\nﬁnder. Under the suggested learning rate, among different\nASAP-perf experiment of composer classiﬁcation, the\nmatrix have on average 143.0±24.7 epochs to converge, the\nsequence and the graph have 132.0±31.1 and 262.0±55.7\nepochs. During training, the graph models have relatively\nslower learning progress.8.3 Dataset class distributions\nWe present our dataset class distribution for each task in the\nTable 8.4.\n8.4 Silence and voice edges\nBesides the onset, consecutive and overlap edges in Sec 3.1,\nwe also add optional silence edges (edges that bridge over si-\nlence) to ensure a connected graph. A silence edge Esilence\nis added between a node that’s not connected by any con-\nsecutive edge and the time-wise closest node before it. The\nsilence edge doesn’t carry much music semantic meaning,\nand its main purpose is to prevent the disjoint subgraphs\nformed by distinct music sections, in which stops informa-\ntion ﬂow in training.\nIn the advanced representation of score graph, we input\nthe voicing information as voice edges. Given that we can’t\nguarantee the consistency of voice annotation in MusicXML\nscores (as they are mostly labeled for visual purposes like\nbeaming), we limit the voice edge connection within a\nmeasure: If two notes are labelled with the same voice, then\nthey are connected by a voice edge Evoice .Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n857ASAP-performance ASAP-score ATEPP-performance ATEPP-score\nACC F1 ACC F1 ACC F1 ACC F1\nMatrix\nResl Chnl\n400 On+Fm 0.926±0.02 0.796±0.06 0.598±0.03 0.177±0.01 0.905±0.04 0.796±0.03 0.246±0.02 0.156±0.03\n600 On+Fm 0.931±0.01 0.800±0.07 0.613±0.07 0.186±0.02 0.930±0.05 0.818±0.03 0.238±0.02 0.156±0.04\n800 Fm 0.925±0.02 0.723±0.11 0.583±0.06 0.182±0.03 0.891±0.02 0.737±0.02 0.221±0.02 0.181±0.03\n800 On+Fm 0.926±0.02 0.812±0.05 0.572±0.04 0.185±0.03 0.932±0.03 0.832±0.01 0.225±0.04 0.138±0.02\nSequence\nTokn BPE\nMidiLike × 0.860±0.03 0.674±0.11 N/A N/A 0.926±0.01 0.769±0.01 N/A N/A\nREMI × 0.783±0.04 0.521±0.05 0.431±0.04 0.138±0.01 0.910±0.01 0.729±0.02 0.229±0.04 0.129±0.02\nCP × 0.679±0.08 0.331±0.06 0.447±0.05 0.099±0.01 0.864±0.02 0.556±0.01 0.171±0.06 0.107±0.04\nMidiLike 4 0.905±0.02 0.727±0.06 N/A N/A 0.895±0.01 0.691±0.01 N/A N/A\nREMI 4 0.862±0.01 0.692±0.07 0.432±0.03 0.132±0.01 0.826±0.04 0.529±0.03 0.234±0.03 0.125±0.01\nGraph\nBi-dir Multi-rel\n× × 0.768±0.03 0.500±0.08 0.509±0.05 0.163±0.02 0.788±0.03 0.501±0.06 0.226±0.03 0.205±0.05\n× ✓ 0.861±0.03 0.763±0.03 0.545±0.05 0.174±0.02 0.928±0.01 0.781±0.03 0.289±0.10 0.176±0.06\n✓ ✓ 0.833±0.03 0.703±0.11 0.500±0.04 0.173±0.01 0.897±0.01 0.767±0.02 0.271±0.06 0.217±0.03\nTable 4 . Base experiment composer classiﬁcation results with the entire performance MIDI corpus and no piece-speciﬁc\nsplit.\nASAP composer ATEPP composer ATEPP performer ASAP difﬁculty\nBeethoven 195 Beethoven 3033 Richter 1581 9 164\nBach 163 Chopin 1739 Ashkenazy 1188 8 176\nChopin 162 Mozart 653 Arrau 833 7 132\nLiszt 67 Schubert 264 Brendel 743 6 150\nSchubert 55 Debussy 254 Kempff 609 5 56\nSchumann 26 Schumann 243 Barenboim 603 4 23\nHaydn 23 Bach 231 Schiff 595\nMozart 10 Ravel 169 Horowitz 576\nScriabin 9 Liszt 122 Gulda 459\nRavel 9 Gieseking 362\nGould 326\nGilels 322\nPerahia 288\nPollini 256\nArgerich 240\nSchnabel 240\nFrançois 234\nUchida 210\nCasadesus 164\nLugansky 125\nTable 5 . Dataset class distribution for the tasks. The performer task is in regards to the distribution of the performed MIDI,\nand the other three columns are in regards to the MusicXML score.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n858"
    },
    {
        "title": "Dual Attention-Based Multi-Scale Feature Fusion Approach for Dynamic Music Emotion Recognition.",
        "author": [
            "Liyue Zhang",
            "Xinyu Yang",
            "Yichi Zhang",
            "Jing Luo"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265259",
        "url": "https://doi.org/10.5281/zenodo.10265259",
        "ee": "https://zenodo.org/records/10265259/files/000023.pdf",
        "abstract": "Music Emotion Recognition (MER) refers to automatically extracting emotional information from music and predicting its perceived emotions, and it has social and psychological applications. This paper proposes a Dual Attention-based Multi-scale Feature Fusion (DAMFF) method and a newly developed dataset named MER1101 for Dynamic Music Emotion Recognition (DMER). Specifically, multi-scale features are first extracted from the log Mel-spectrogram by multiple parallel convolutional blocks. Then, a Dual Attention Feature Fusion (DAFF) module is utilized to achieve multi-scale context fusion and capture emotion-critical features in both spatial and channel dimensions. Finally, a BiLSTM-based sequence learning model is employed for dynamic music emotion prediction. To enrich existing music emotion datasets, we developed a high-quality dataset, MER1101, which has a balanced emotional distribution, covering over 10 genres, at least four languages, and more than a thousand song snippets. We demonstrate the effectiveness of our proposed DAMFF approach on both the developed MER1101 dataset, as well as on the established DEAM2015 dataset. Compared with other models, our model achieves a higher Consistency Correlation Coefficient (CCC), and has strong predictive power in arousal with comparable results in valence.",
        "zenodo_id": 10265259,
        "dblp_key": "conf/ismir/ZhangYZL23",
        "keywords": [
            "Music Emotion Recognition",
            "Automatically extracting emotional information",
            "Dynamic Music Emotion Recognition",
            "Dual Attention-based Multi-scale Feature Fusion",
            "Multi-scale features",
            "Emotion-critical features",
            "BiLSTM-based sequence learning model",
            "MER1101 dataset",
            "DEAM2015 dataset",
            "Consistency Correlation Coefficient"
        ],
        "content": "DUAL ATTENTION-BASED MULTI-SCALE FEATURE FUSION\nAPPROACH FOR DYNAMIC MUSIC EMOTION RECOGNITION\nLiyue Zhang1Xinyu Yang2Yichi Zhang2\nJing Luo2\n1The School of Software, Xi’an Jiaotong University , China\n2The School of Computer Science and Technology, Xi’an Jiaotong University , China\n{3121358019, datasonezyc, luojingl}@stu.xjtu.edu.cn, yxyphd@mail.xjtu.edu.cn\nABSTRACT\nMusic Emotion Recognition (MER) refers to automatically\nextracting emotional information from music and predict-\ning its perceived emotions, and it has social and psycholog-\nical applications. This paper proposes a Dual Attention-\nbased Multi-scale Feature Fusion (DAMFF) method and\na newly developed dataset named MER1101 for Dy-\nnamic Music Emotion Recognition (DMER). Speciﬁcal-\nly, multi-scale features are ﬁrst extracted from the log\nMel-spectrogram by multiple parallel convolutional block-\ns. Then, a Dual Attention Feature Fusion (DAFF) module\nis utilized to achieve multi-scale context fusion and cap-\nture emotion-critical features in both spatial and channel\ndimensions. Finally, a BiLSTM-based sequence learning\nmodel is employed for dynamic music emotion prediction.\nTo enrich existing music emotion datasets, we develope-\nd a high-quality dataset, MER1101, which has a balanced\nemotional distribution, covering over 10 genres, at least\nfour languages, and more than a thousand song snippets.\nWe demonstrate the effectiveness of our proposed DAMF-\nF approach on both the developed MER1101 dataset, as\nwell as on the established DEAM2015 dataset. Compared\nwith other models, our model achieves a higher Consisten-\ncy Correlation Coefﬁcient (CCC), and has strong predic-\ntive power in arousal with comparable results in valence.\n1. INTRODUCTION\nWith the rising demand for music consumption and the ex-\nplosive growth of music content, Music Emotion Recog-\nnition (MER) demonstrates its critical position in music\nunderstanding and applications. It has been widely used\nin personalized music recommendation [1], music thera-\npy [2], music education [3], music generation [4], etc.\nTo portray human emotions, two main types of models\nwere differentiated in the past [5]: discrete emotion mod-\nel [6, 7] and dimensional emotion model [8–11]. The dis-\ncrete emotion model describes human emotion as categor-\nc/circlecopyrtL. Zhang, X. Yang, Y . Zhang, J. Luo. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: L. Zhang, X. Yang, Y . Zhang, J. Luo, “Dual Attention-\nbased Multi-scale Feature Fusion Approach for Dynamic Music Emotion\nRecognition”, in Proc. of the 24th Int. Society for Music Information\nRetrieval Conf., Milan, Italy, 2023.ical adjectives, such as happiness, anger, sadness, joy, etc.\nHowever, limited words cannot adequately describe human\nemotions, different emotions are better described on a con-\ntinuous scale than as a set of discrete values. In Russel-\nl’s two-dimensional valence-arousal (V-A) emotional mod-\nel [12], emotions are described as points on the plane that\nis spanned by the arousal and valence axes. This turns the\nproblem of emotion prediction into a two-dimensional re-\ngression issue based on Russell’s emotion model. This pa-\nper is focused on the study of Dynamic Music Emotion\nRecognition (DMER), which predicts the emotion of mu-\nsic using continuous V-A values at a short interval.\nAmong the existing studies, Long Short-Term Memory\n(LSTM) has received extensive attention in the DMER due\nto its superiority in sequence modeling [8, 13–15]. Convo-\nlutional Neural Network (CNN) is used to extract features\nin many ﬁelds. Researchers have recently focused on im-\nproving emotion recognition accuracy using a combination\nof CNN and Recurrent Neural Network (RNN) [9,16–18].\nHowever, LSTM-based models still use handcrafted fea-\ntures as input, and some widely used handcrafted feature\noperations will lose high-level features. The CNN-RNN-\nbased model mainly uses a ﬁxed-scale CNN. Due to its\nﬁxed receptive ﬁeld, the learned CNN features are limit-\ned, and the emotional crucial features of different ﬁelds of\nview are not extracted. Moreover, various problems exist\nin existing music emotion datasets, which also hinder the\nprogress of DMER.\nThis paper proposes a novel Dual Attention-based\nMulti-scale Feature Fusion (DAMFF) model and devel-\nops the music emotion dataset MER1101 for DMER. On\nthe one hand, our model ﬁrst utilizes multi-scale convo-\nlution to extract features at different temporal-frequency\nspans from the log Mel-spectrogram. Then, we propose\na Dual Attention Feature Fusion (DAFF) module for fus-\ning multi-scale context features from spatial and channel\ndimensions to enhance the expressive ability of CNN. Fi-\nnally, the BiLSTM model processes these features and pre-\ndicts V-A emotional labels. On the other hand, we develop\na high-quality dataset named MER1101. Compared with\nthe existing publicly available datasets in the MER domain,\nMER1101 contains 1101 music snippets from 16 genres\nwith richer languages, more extensive size, and more bal-\nanced emotion label distribution. We evaluate our method\nusing the MER1101 dataset and DEAM2015 [19] dataset.207log Mel-spectrogramConv: 8@3×1\nBatchNorm+ReLUMaxPool2dConv: 8@3×1\nMaxPool2dConv: 8@1×5\nBatchNorm+ReLU\nMaxPool2d\nConv: 16@3×3\nBatchNorm+ReLU\nMaxPool2dConv: 16@5×5\nBatchNorm+ReLU\nMaxPool2d\nConv: 64@3×3\nBatchNorm+ReLU\nMaxPool2dBatchNorm+ReLU\nMaxPool2d\nConatenation,2\nConatenation,1 Input: 1×frame_len×dimSCAM\nConv:1@1×1\nBiLSTM\nFC Layer\nArousal/ValenceDual Attention Feature FusionTemporal-Frequency Multi-scale Convolution\n1-X'\nElement-wise Sum Matrix Multiplication\nConv: 64@5×5\nChannel Attention\nSpatial AttentionSigmoidX\nX X'X'X'Sequence Learning \nSCAMA\nB\nFigure 1 . DAMFF model architecture. The input is 2D spectrogram. The architecture combines temporal-frequency multi-\nscale feature extraction, dual attention feature fusion, and sequence learning to achieve dynamic emotion prediction for\nmusic.\nOn the MER1101 dataset, we achieve a Consistency Corre-\nlation Coefﬁcient (CCC) of 0.4223 for arousal and 0.1115\nfor valence. On the DEAM2015 dataset, we achieve a C-\nCC of 0.4203 for arousal and 0.0151 for valence. Exper-\nimental results show our method outperforming a number\nof baseline and SOTA models in DMER, by means of an\nimproved CCC metric.\n2. RELATED WORK\nResearchers have made many efforts in the DMER in the\npast few years. In the early days, RNN made a break-\nthrough in this ﬁeld due to their advantages in sequence\nprocessing. In the “Emotion in Music” task at MediaEval\nfrom 2013 to 2015, LSTM-based methods achieved state-\nof-the-art performance [20]. Li et al. [8] pointed out that in\nmusic composition, performance, and annotation, the emo-\ntion in music is related to the previous and future contexts.\nTherefore, they chose Bidirectional LSTM (BiLSTM) as\nthe regression model and proposed a multi-scale fusion\nmethod based on an Extreme Learning Machine (ELM)\nto improve the performance of the BiLSTM model. But\nthe LSTM-based models mentioned above use suboptimal\nhand-crafted features as input, making it difﬁcult to im-\nprove emotion recognition.\nLater, researchers began to employ CNN for high-level\ninvariant features extracted from raw music data [21–23].\nPons et al. [24] discussed how convolution ﬁlters with dif-\nferent shapes are suitable for speciﬁc musical concepts and\nexperimentally proved that the size of CNN ﬁlters can be\ninterpreted in both the temporal and frequency dimensions\nof the spectrogram. Researchers have combined CNN and\nRNN to improve the accuracy of emotion recognition, Ma-\nliket al. [16] proposed a two-dimensional V-A space con-\ntinuous emotion prediction method composed of stacked\nconvolution and recurrent neural network. Compared to\nusing BiLSTM [15] only, this method achieved better re-\nsults with fewer parameters; Dong et al. [9] replaced theconnection between the input layer and the hidden layer of\nthe RNN with a CNN to adaptively learn the sequential-\ninformation-included affect-salient features from the spec-\ntrogram; Zhang et al. [25] extracted MFCCs and Cochlea-\ngrams from raw music data as input features, and adopted\nan audio feature fusion method based on the combination\nof CNN and BiLSTM to predict the emotional V-A values\nin music. However, CNN-RNN-based models still have\nproblems with limited convolutional receptive ﬁelds. For\nMER, due to the limited size of the convolution kernel, the\nconvolution is mainly biased towards learning local infor-\nmation, which is insufﬁcient for learning the correlation\nbetween the spatial and channel axes.\nVarious attention mechanisms are devised to solve the\nabove problem in speech emotion recognition [21, 26, 27].\nGuo et al. [26] proposed a representation learning method\nwith spectral-temporal channel (STC) attention, which was\nintegrated with CNN to improve representation learning a-\nbility; Zhang et al. [21] applied multi-scale region attention\nin deep convolutional neural networks to focus on emo-\ntional features at different granularities; Zhang et al. [27]\nimplemented an attention layer on the arousal, valence,\nand dominance tasks and completed multi-task prediction-\ns to capture the contribution of different parts of each\ntask. Nonetheless, the attention mechanism is currently\nnot widely applied in the ﬁeld of DMER.\nIn this paper, we propose a novel attention module, the\nSpatial Channel Attention Module (SCAM), which consid-\ners spatial and channel dimensions to capture the relative\nimportance of features and integrates multi-scale convo-\nlutions for enhanced representation learning. We aim to\nbuild an attention mechanism that extracts salient informa-\ntion from multiple dimensions and can fuse contextual in-\nformation.\nJ. S. Gómez-Cañón et al. [28] summarized existing\nMER datasets. But they have some problems, for exam-\nple, some datasets have insufﬁcient number of music, and\nsome datasets have no dimension labels. After our com-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n208prehensive comparison, the three datasets CH818 [29], P-\nMEmo [30] and DEAM [19] are relatively suitable for the\nDMER task. However, all three datasets have some dis-\nadvantages. The songs in the CH818 dataset only contain\nChinese pop songs and are not public, while PMEmo only\nWestern pop songs; The annotators and annotating times\nof the training set and evaluation set in the DEAM2015\ndataset are different, resulting in a discrepancy in perfor-\nmance [19]. To enrich existing musical emotion datasets,\nwe develop a high-quality dataset, MER1101. MER1101\ncontains 1101 music snippets, which is better than most\ndatasets in the MER domain in terms of genre, language,\nnumber of music, and has more balanced distributed emo-\ntion annotations.\n3. METHODOLOGY\nThe proposed DMER processing method consists of three\nphases. Firstly, we build a Temporal-Frequency Multi-\nscale Convolution network using three different shapes of\nconvolutional ﬁlters. Secondly, we propose a Dual Atten-\ntion Feature Fusion network to focus more on the channel\nand spatial with important information and fuse multi-scale\nconvolutional features in different dimensions. And ﬁnally,\nwe employ BiLTSM, building a map from emotion-crucial\nfeatures to emotional space. The speciﬁcs are as follows.\n3.1 Temporal-Frequency Multi-scale Convolution\nCNN has been proven effective at tackling various visu-\nal tasks [31, 32]. In vision tasks, the ﬁlter dimension has\nspatial meaning, and the audio spectrogram ﬁlter dimen-\nsion corresponds to temporal and frequency [24]. We de-\nsign a temporal-frequency multi-scale convolution module\nwith three types of ﬁlters to capture various musical fea-\ntures. From the musical point of view, the temporal ﬁlter\n(1-by-n ) can learn temporal dependence in music; the fre-\nquency ﬁlter ( m-by-1 ) can learn pitch and timbre, and the\nsquare ﬁlter ( m-by-n ) can learn different musical features\naccording to the size of mandn. As shown in Figure 1,\nwe extract features through three layers of parallel con-\nvolutional blocks in the Temporal-Frequency Multi-scale\nConvolution module.\nFirstly, we take the 30-second log Mel-spectrogram as\ninput and perform distinct convolution operations on each\n0.5-second segment to keep the individual properties at\neach moment. Secondly, the ﬁrst layer introduces 3 ×1 and\n1×5 ﬁlters to capture features along the temporal and fre-\nquency axes, and their outputs are concatenated along the\ntime dimension. Finally, the concatenated results of the\nﬁrst layer are put into consecutive parallel convolutional\nlayers with kernel sizes 3 ×3 and 5×5. The output of the\nsecond layer is concatenated along the channel dimension,\nwhile the output of the third layer is fed into a dual atten-\ntion feature fusion module for feature fusion. After each\nconvolutional layer, batch normalization [33], the ReLU\nfunction [34] and max pooling are applied.\nInput X\nGlobalAvgPooling2d\nPoint-wise Conv2dC×1×1\nBN+ReLUView \nGlobalAvgPooling1d\nPoint-wise Conv1d(H×W)×C\nC'×1×1\nPoint-wise Conv2d(H×W)'×1\nC'×1×1\nPoint-wise Conv1d(H×W)'×1BN+ReLU\nView C×H×W\n(H×W)×1\nBN BNC×1×1(H×W)×1\n1×H×W\nSigmoid C×H×WFigure 2 . SCAM model architecture.\n3.2 Dual Attention Feature Fusion\nTo further enhance the representation ability of CNN and\ncapture the important information, we design a Dual Atten-\ntion Feature Fusion (DAFF) module to focus more on the\nchannel and spatial with important information for fusing\nmulti-scale convolutional features. As shown in Figure 1,\nthe DAFF module includes the Spatial Channel Attention\nModule (SCAM). By element-wise summing the output-\ns of 3×3 and 5×5 convolutions, we get a feature map\nX∈RC×H×Was input to SCAM, which is then fed into\nthe spatial and channel attention modules, respectively. In\nSections 3.2.1 and 3.2.2, we describe the proposed SCAM\nin detail.\n3.2.1 Channel Attention Module\nWe convert a single channel into 64 channels through the\nTemporal-Frequency Multi-scale Convolution, strengthen-\ning the temporal correlation between distinct channels. In\nthis case, we use the channel attention mechanism, which\nfocuses on what the essential features are. While tradi-\ntional attention mechanisms only focus on temporal struc-\ntures, channel attention can learn the importance of differ-\nent channels to deactivate features that do not contribute\nmuch to emotion. Figure 2 shows the channel attention\nmodule, similar to the Squeeze-and-Excitation block [35].\nThe module is mainly divided into two parts: squeeze and\nexcitation operations. Speciﬁcally, given an input feature\nX∈RC×H×W, we ﬁrst use Global Average Pooling in-\ndependently for each channel to aggregate spatial informa-\ntion and generate a channel attention map C∈RC×1×1.\nNext, we perform an excitation operation using two point-\nwise convolutions to enable cross-channel interaction. Fi-\nnally, the channel attention map C∈RC×1×1is obtained.\nIn short, the channel attention map is calculated as follows:\nC=β(Conv2d2(δ(β(Conv2d1(Pool2d(X)))))) (1)\nwhereδandβdenote the ReLU function and batch nor-\nmalization, respectively, and Pool2dandConv2drepre-\nsent the global average pooling2d and point-wise convolu-\ntion2d, respectively.\n3.2.2 Spatial Attention Module\nWe propose the spatial attention model, which exploits the\nspatial relationship between features to generate a spatialProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n209attention map. Spatial attention focuses on where the im-\nportant features are and supplements channel attention.\nThe spatial attention module obtains the spatial atten-\ntion map in four phases. First, the input feature through\nview operation is converted into a spatial feature map\nS/prime∈R(H×W)×C. Second, a global pooling operation is\napplied along the channel axis to compress the channels\nto obtain spatial-level features. Third, we use two point-\nwise convolutions to execute excitation operations and get\nfeature weights at distinct positions. Finally, the resulting\nspatial attention map is translated into S∈R1×H×W. In\nshort, the spatial attention map is calculated as follows:\nS=β(Conv1d2(δ(β(Conv1d1(Pool1d(X)))))) (2)\nwhereδandβdenote the ReLU function and batch nor-\nmalization, respectively, and Pool1dandConv1drepre-\nsent the global average pooling1d and point-wise convolu-\ntion1d, respectively.\nAfter that, we perform an element-wise sum opera-\ntion on the output of the dual attention and through the\nsigmoid function to obtain a new attention weight map\nX/prime∈RH×W×C.\nX/prime=Sigmoid (S⊕C) (3)\n3.2.3 Feature Fusion Strategy\nIn order to effectively aggregate multi-scale context infor-\nmation, we introduce the fusion strategy in [36], as shown\nby Dual Attention Feature Fusion in Figure 1. The output\nof SCAM is represented as X/prime,1−X/primeby the solid line and\ndotted line, respectively. Based on the SCAM, the multi-\nscale feature fusion can be expressed as:\nZ=X/prime⊗A+(1−X/prime)⊗B (4)\nwhereAandBrepresent the outputs of 3 ×3 and 5×5 con-\nvolutions respectively, Z∈RC×H×Wis the fused feature.\n3.3 Sequence Learning\nThrough the DAFF module, we get emotion-crucial fea-\ntures from multi-scale convolutional features. After reduc-\ning dimension, the features of the entire 30s of music snip-\npet are input into the Bidirectional LSTM (BiLSTM) for\nlong-term sequence learning. Finally, the emotional fea-\ntures are mapped to the emotional space with the help of a\nfully connected layer.\n4. EXPERIMENTS\n4.1 Dataset\nWe conduct our experiments on the DEAM2015 [19]\ndataset and our newly developed dataset MER1101. The\ndetails of each dataset are given below.\nDEAM: This dataset was developed in the “Emotion\nin Music” (EiM) task [37] of the MediaEval benchmark.\nWe utilized the DEAM2015 dataset, with the training set\nconsisting of 431 30-second samples and the evaluation\nset consisting of 58 full-length songs. This dataset is themost commonly used benchmark in dynamic music emo-\ntion recognition, but Cronbach’s αof the evaluation set\nis 0.29±0.94 for valence, which is relatively low [19].\nFurthermore, due to the different spatio-temporal environ-\nments and annotators of the emotion annotation process\nof the training set and the evaluation set [19], the perfor-\nmance derived from the training and evaluation set shows\na non-negligible discrepancy, especially in the valence di-\nmension.\nMER11011:Similar with DEAM, MER1101 is also\nbased on Russell’s valence-arousal emotion model. It con-\ntains 1101 music snippets gathered from the internet, with\neach ranging in duration from 16.5 seconds to 125.5 sec-\nonds. The dataset has both discrete and dimensional label-\ns. Every song in the dataset has been annotated by three\nmusic experts and ten college students. The annotators lis-\ntened to the song once and annotated the emotional adjec-\ntives of the song. After they were familiar with the song,\nthey listened to it twice and annotated the V-A values. An-\nnotators were only paid the full fee after their work had\nbeen reviewed. Student-labeled Cronbach’s αarousal is\n0.6295±0.3574, 0.5624 ±0.3766 for the valence. Expert-\nlabeled Cronbach’s αarousal is 0.3556 ±0.3442, 0.2420\n±0.3148 for the valence.\nCompared with other music datasets, the MER1101\ndataset has the following four advantages: 1) The dataset\ncontains more genres (16 genres), including pop, DJ dance,\nchinoiserie, electronic, hip-hop, etc.; 2) It contains richer\nlanguage, meeting the ratio of nearly 5:3:1:1 for Chinese,\nEnglish, Japanese and Korean, and other languages; 3) The\nsamples in our dataset distribute more balanced in the emo-\ntional quadrants and there are no more than three songs by\nthe same artist in each V-A quadrant; 4) The size of our\ndataset is relatively larger than the current music datasets.\nOur dataset can be used for a variety of music tasks,\nsuch as music genre classiﬁcation, music generation with\nemotion, music emotion recognition, etc.\n4.2 Evaluation Metrics\nWe use the Concordance Correlation Coefﬁcient (CCC),\nPearson Correlation Coefﬁcient (PCC), and Root Mean\nSquare Error (RMSE) as evaluation metrics. Each metric\nis computed by the ground-truth and predicted V-A values\nfor each song and averaged across songs. The CCC com-\nbines the characteristics of PCC and RMSE to evaluate not\nonly the trend of emotional changes but also the dispari-\nty between predictions and ground-truth. As a result, we\nconsider CCC to be the most important evaluation metric.\n4.3 Experimental Setup\nSince DEAM2015 predeﬁnes the training and evaluation\nset conﬁguration, we only describe the dataset division for\nMER1101 here. Firstly, we choose 925 songs lasting more\nthan 30 seconds from the MER1101 dataset and randomly\nsplit them into a training set (80% of the data) and an eval-\nuation set (20% of the data). Then, we split each song in\n1Seehttps://ismir-2023.github.io/MER1101/ for details.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n210(a) A-value CCC\n (b) A-value PCC\n (c) A-value RMSE\n (d) V-value CCC\n (e) V-value PCC\n (f) V-value RMSE\nFigure 3 . The CCC, PCC, and RMSE of arousal and valence with different hop sizes on the MER1101 dataset.\nthe training set into 30-second segments and kept complete\nsongs for the evaluation set. The ﬁnal training set contain-\ns 1526 30-second music snippets, and the evaluation set\ncontains 185 complete songs. The DEAM dataset uses the\nofﬁcial training and evaluation sets. To obtain a more ac-\ncurate comparison and minimize accidental errors, we use\n5-fold cross-validation on both datasets.\nThe log Mel-spectrogram is extracted using librosa\n[38], where the Mel band is 128, the sampling rate is\n44100Hz, and the window size and hop size are 60 ms\nand 10 ms, respectively. The size of the convolution k-\nernel is shown in Figure 1. We utilize the Adam optimizer\nfor training, with learning rate of 0.0003, training epoch\nof 100, and batch size of 32. To prevent overﬁtting, we\nadopt the early stopping strategy. In addition, we use C-\nCC and RMSE as loss functions for arousal and valence,\nrespectively.\n4.4 Experimental Results\n4.4.1 Hop Size Selection of Sliding Window\nFor the MER1101 dataset, we train the model with music\nsnippets of ﬁxed duration, while the durations of the mu-\nsic snippets are variable during the test. Thus, we could\nnot directly predict the emotion of the whole music. We\npropose a sliding window-based testing scheme to address\nthis issue and ensure the continuity of the predicted V-A\ncurves. During testing, we utilize the window size of T\nseconds and the hop size of t seconds. Each T second of\naudio in the window is input to the model, and the corre-\nsponding T seconds V-A curves are predicted. The ﬁrst\nwindow takes the prediction result of T seconds, and each\nsubsequent window only takes the result of the last t sec-\nonds.\nWe investigate the impact of hop size on the results of\nmusic emotion recognition on the MER1101 dataset. We\nset the window size to 30s, the same as the training set\nsample duration. Figure 3 shows the experimental results,\nCCC and PCC change signiﬁcantly and show a downward\ntrend with increasing hop size, and the change in RMSE\nis not obvious. We observe that with the increase of the\nhop size, the emotion prediction effect decreased signif-\nicantly, demonstrating that the shorter hop size performs\nbetter. During listening to music, the user’s emotion at a\ncertain moment is an accumulation of previous music con-\ntent. Therefore, providing the model with as much con-text as possible beneﬁts emotion recognition. A shorter\nhop size can provide more context for the model to pre-\ndict the current musical mood. In the experiments on the\nMER1101 dataset below, we adopt hop sizes of 2.5s and\n0.5s for arousal and valence, respectively.\n1×3\n5×13×3\n5×55×5\n5×5\n(a) Hybrid CNN\n3×1\n1×53×1\n1×53×1\n1×5 (b) T-F CNN\n3×3\n5×53×3\n5×55×5\n5×5 (c) Square CNN\n3×1\n3×13×3\n3×33×3\n3×3\n(d) T-S CNN\n1×5\n1×55×5\n5×55×5\n5×5 (e) F-S CNN\nFigure 4 . Five CNN architectures.\nModelArousal Valence\nCCC↑PCC∗↑RMSE∗↓CCC↑PCC↑RMSE↓\nHybrid CNN 0.4223 0.6856 0.1494 0.1115 0.2004 0.2595\nT-F CNN 0.4120 0.6787 0.1478 0.0846 0.1363 0.2684\nSquare CNN 0.4130 0.6894 0.1439 0.0732 0.1343 0.2703\nT-S CNN 0.4090 0.6881 0.1458 0.1085 0.1959 0.2542\nF-S CNN 0.4150 0.6804 0.1562 0.1046 0.1640 0.2800\n* The result of the signiﬁcance test (Student’s t test) show that there is\nno signiﬁcant difference between the results of this metric.\nTable 1 . Experimental results of different CNN architec-\ntures on the MER1101 dataset.\n4.4.2 Impact of CNN ﬁlters\nIn this section, we compare the inﬂuence of different CNN\narchitectures on the experimental results of the MER1101\ndataset. In this paper, we adapt three types of convolu-\ntion: temporal ﬁlters ( 1-by-n ), frequency ﬁlters ( m-by-1 ),\nand squared ﬁlters ( m-by-n ). Convolution ﬁlters of dif-\nferent shapes have different musical concepts. We com-\nbined them into ﬁve architectures. In Figure 4(a), the CN-\nN architecture used here is a “Hybrid CNN” architecture.\nFigure 4(b) uses the temporal ﬁlters and frequency ﬁlter-\ns, and we call it the “T-F CNN” architecture. Figure 4(c)\nonly uses a square ﬁlter, so we call it “Square CNN” archi-\ntecture. Figure 4(d) and Figure 4(e) are referred to as “T-S\nCNN” and “F-S CNN”, respectively. The experimental re-\nsults are shown in Table 1, which show that the “Hybrid C-\nNN” architecture has better expressiveness on the DMERProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n211MER1101 dataset DEAM2015 dataset\nModelArousal Valence Arousal Valence\nCCC↑PCC↑RMSE↓CCC↑PCC↑RMSE↓CCC↑PCC↑RMSE↓CCC↑PCC↑RMSE↓\nCRNN [16] 0.2798 0.5177 0.1625 0.0573 0.1033 0.2721 0.3488 0.5885 0.2197 0.0053 -0.0292 0.3542\nBCRSN [9] 0.1741 0.3770 0.3063 0.0660 -0.0647 0.4143 0.3168 0.5148 0.2397 0.0125 -0.0171 0.2914\nDNN [17] 0.0529 0.0903 0.2372 0.0118 0.0017 0.2734 0.2757 0.4282 0.2483 0.0075 0.0031 0.3353\nMCRNN [18] 0.0564 0.0918 0.2401 0.0155 0.0028 0.2752 0.2700 0.4396 0.2428 0.0137 0.0126 0.3135\nDAMFF 0.4223 0.6856 0.1494 0.1115 0.2004 0.2595 0.4203 0.6866 0.2401 0.0151 0.0366 0.3403\nTable 2 . Compared with the existing results.\ntask. It is shown that extracting them simultaneously is\nbeneﬁcial to obtain music emotion information from dif-\nferent perspectives, and the PCC and RMSE changes of\nArousal are not signiﬁcant.\n4.4.3 Comparison with the Existing Models\nWe compare the DAMFF to other DMER methods [9, 16–\n18] published in recent years. They differ from us in that\n[18] takes DEAM2014 [39] as the dataset, which consists\nof 744 songs. [16–18] take RMSE as the evaluation met-\nrics, and [9] translates numerical-type V-A values to binary\nrepresentation and independently predict emotion for each\n0.5s.\nIn this paper, we reproduce the models mentioned above\non the DEAM2015 and MER1101 datasets. All models’\nperformance is evaluated with the same experimental con-\nﬁgurations, i.e., the same dataset, evaluation metrics, and\nmetric calculation method. Table 2 shows the results of the\nexperiments. On the MER1101 dataset, our model is supe-\nrior to the others in all three metrics. On the DEAM2015\ndataset, our model shows powerful recognition ability for\narousal, but the valence slightly outperforms the previous\nmodels, which may stem from the less consistent valence\nannotations [15]. We believe predicted valence values on\nthe DEAM2015 dataset are relatively incapable of evaluat-\ning DMER since the predicted CCC value number in va-\nlence driven from all models is near zero. Experiments\nshow that our model can perform well in emotion recogni-\ntion on different datasets, especially in the arousal dimen-\nsion. Overall, valence values are more impoverished in\nboth datasets than arousal values, indicating that predict-\ning valence is more challenging. This is also consistent\nwith the conclusions of most works.\nModelArousal Valence\nCCC↑PCC∗↑RMSE∗↓CCC↑PCC↑RMSE↓\nDAMFF 0.4223 0.6856 0.1494 0.1115 0.2004 0.2595\nw/o Fusion Strategy 0.4097 0.6869 0.1563 0.1074 0.1722 0.2707\nw/o Channel Attention 0.4061 0.6740 0.1494 0.1071 0.1904 0.2650\nw/o Spatial Attention 0.4177 0.6819 0.1518 0.1009 0.1874 0.2720\nw/o DAFF 0.3982 0.6813 0.1562 0.0977 0.1693 0.2670\n* The result of the signiﬁcance test (Student’s t test) show that there is\nno signiﬁcant difference between the results of this metric.\nTable 3 . Ablation experiments of arousal and valence on\nthe MER1101 dataset.4.4.4 Ablation Study\nTo investigate the role of various modules, we construct-\ned four ablation modules. Among them, “w/o Fusion S-\ntrategy” directly inputs the result of the SCAM module\ninto BiLSTM, which explores the role of fusion strate-\ngy. In addition, the inﬂuence of dual attention is studied\nusing “w/o Channel Attention”, “w/o Spatial Attention”,\nand “w/o DAFF”. Table 3 shows the experimental results\non the MER1011 datasets. The results show that: 1) the\nnon-linear fusion strategy of the attention mechanism bet-\nter aggregates the multi-scale context and performs better;\n2) the attention mechanism increases the weights of emo-\ntional features, which is helpful for emotion recognition.\nAt the same time, dual attention is better than single at-\ntention, indicating that spatial and channel attention mech-\nanisms learn and emphasize what andwhere affect-salient\nfeatures, effectively improving CNN features. In summary,\nwe conclude that fusing multi-scale convolutional features\nfrom spatial and channel dimensions is more conducive to\ncapturing key emotional features, which is more evident on\nthe CCC metric.\n5. CONCLUSION\nThis paper proposes a novel Dual Attention-based Multi-\nscale Feature Fusion (DAMFF) network, which extracts\nmulti-scale convolutional features from spectrograms and\nexploits the dual-attention mechanism to capture impor-\ntant channel and spatial information. The network adopt-\ns the fusion mechanism that aggregates multi-scale con-\ntext information, effectively improving CNN features’ ex-\npressive ability. The music emotion dataset MER1101 we\ndeveloped contains 1101 music audio with 16 genres, 5\nlanguages and a balanced distribution of emotion labels.\nExperimental results show that our model outperforms the\ncomparison methods on the CCC metric on both MER1101\nand DEAM2015 datasets. Furthermore, our model has\nsubstantial prediction capabilities in terms of arousal and\ncomparable results in terms of valence.\nThe prediction of the valence dimension is still chal-\nlenging in DMER. In the future, we will focus on develop-\ning more effective techniques, such as pre-training audio\nfeatures for improving the recognition performance of va-\nlence.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n2126. REFERENCES\n[1] S. M. Florence and M. Uma, “Emotional detection and\nmusic recommendation system based on user facial ex-\npression,” in IOP Conference Series: Materials Sci-\nence and Engineering , vol. 912, no. 6. IOP Publish-\ning, 2020, p. 062007.\n[2] G. A. Dingle, P. J. Kelly, L. M. Flynn, and F. A. Baker,\n“The inﬂuence of music on emotions and cravings in\nclients in addiction treatment: A study of two clinical\nsamples,” The Arts in Psychotherapy , vol. 45, pp. 18–\n25, 2015.\n[3] T. Xia, Z. Li et al. , “Behavioral training of high-\nfunctioning autistic children by music education of\noccupational therapy,” Occupational Therapy Interna-\ntional , vol. 2022, 2022.\n[4] S. Ji, J. Luo, and X. Yang, “A comprehensive survey\non deep music generation: Multi-level representations,\nalgorithms, evaluations, and future directions,” arXiv\npreprint arXiv:2011.06801 , 2020.\n[5] X. Yang, Y . Dong, and J. Li, “Review of data features-\nbased music emotion recognition methods,” Multime-\ndia systems , vol. 24, pp. 365–389, 2018.\n[6] K. Trohidis, G. Tsoumakas, G. Kalliris, I. P. Vlahavas\net al. , “Multi-label classiﬁcation of music into emo-\ntions.” in International Conference on Music Informa-\ntion Retrieval (ISMIR) , vol. 8, 2008, pp. 325–330.\n[7] J.-H. Su, T.-P. Hong, Y .-H. Hsieh, and S.-M. Li, “Effec-\ntive music emotion recognition by segment-based pro-\ngressive learning,” in 2020 IEEE International Confer-\nence on Systems, Man, and Cybernetics (SMC) . IEEE,\n2020, pp. 3072–3076.\n[8] X. Li, H. Xianyu, J. Tian, W. Chen et al. , “A deep bidi-\nrectional long short-term memory based multi-scale\napproach for music dynamic emotion prediction,” in\n2016 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 2016,\npp. 544–548.\n[9] Y . Dong, X. Yang, X. Zhao, and J. Li, “Bidirectional\nconvolutional recurrent sparse network (BCRSN): an\nefﬁcient model for music emotion recognition,” IEEE\nTransactions on Multimedia , vol. 21, no. 12, pp. 3150–\n3163, 2019.\n[10] S. Chaki, P. Doshi, S. Bhattacharya, and P. Patnaik,\n“Explaining perceived emotion predictions in music:\nAn attentive approach.” in International Conference on\nMusic Information Retrieval (ISMIR) , 2020, pp. 150–\n156.\n[11] Z. Huang, S. Ji, Z. Hu, C. Cai, J. Luo, and X. Yang,\n“ADFF: Attention Based Deep Feature Fusion Ap-\nproach for Music Emotion Recognition,” in Inter-\nspeech 2022, 23rd Annual Conference of the Inter-\nnational Speech Communication Association, Incheon,\nKorea, 18-22 September 2022 , 2022, pp. 4152–4156.[12] J. A. Russell, “A circumplex model of affect.” Journal\nof personality and social psychology , vol. 39, no. 6, p.\n1161, 1980.\n[13] F. Weninger, F. Eyben, and B. Schuller, “On-line\ncontinuous-time music mood regression with deep re-\ncurrent neural networks,” in 2014 IEEE international\nconference on acoustics, speech and signal processing\n(ICASSP) . IEEE, 2014, pp. 5412–5416.\n[14] Y . Ma, X. Li, M. Xu, J. Jia, and L. Cai, “Multi-scale\ncontext based attention for dynamic music emotion\nprediction,” in Proceedings of the 25th ACM inter-\nnational conference on Multimedia , 2017, pp. 1443–\n1450.\n[15] X. Li, J. Tian, M. Xu, Y . Ning, and L. Cai, “Dblstm-\nbased multi-scale fusion for dynamic emotion predic-\ntion in music,” in 2016 IEEE International Conference\non Multimedia and Expo (ICME) . IEEE, 2016, pp.\n1–6.\n[16] M. Malik, S. Adavanne, K. Drossos, T. Virtanen,\nD. Ticha, and R. Jarina, “Stacked convolutional and\nrecurrent neural networks for music emotion recogni-\ntion,” in Proceedings. 14th Sound Music Comput. Con-\nf., 2017, pp. 208–213.\n[17] R. Orjesek, R. Jarina, M. Chmulik, and M. Kuba, “Dnn\nbased music emotion recognition from raw audio sig-\nnal,” in 2019 29th International Conference Radioelek-\ntronika (RADIOELEKTRONIKA) . IEEE, 2019, pp.\n1–4.\n[18] N. He and S. Ferguson, “Multi-view neural network-\ns for raw audio-based music emotion recognition,” in\n2020 IEEE International Symposium on Multimedia\n(ISM) . IEEE, 2020, pp. 168–172.\n[19] A. Aljanaki, Y .-H. Yang, and M. Soleymani, “Emotion\nin music task at mediaeval 2015,” in MediaEval , 2015.\n[20] Aljanaki, Anna and Yang, Yi-Hsuan and Soleymani,\nMohammad, “Developing a benchmark for emotion-\nal analysis of music,” PloS one , vol. 12, no. 3, p.\ne0173392, 2017.\n[21] M. Xu, F. Zhang, X. Cui, and W. Zhang, “Speech e-\nmotion recognition with multiscale area attention and\ndata augmentation,” in ICASSP 2021-2021 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2021, pp. 6319–6323.\n[22] J. Liu, Z. Liu, L. Wang, L. Guo, and J. Dang, “Speech\nemotion recognition with local-global aware deep rep-\nresentation learning,” in ICASSP 2020-2020 IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) . IEEE, 2020, pp. 7174–\n7178.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n213[23] W. Zhu and X. Li, “Speech emotion recognition with\nglobal-aware fusion on multi-scale feature representa-\ntion,” in ICASSP 2022-2022 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (I-\nCASSP) . IEEE, 2022, pp. 6437–6441.\n[24] J. Pons, T. Lidy, and X. Serra, “Experimenting with\nmusically motivated convolutional neural networks,”\nin2016 14th international workshop on content-based\nmultimedia indexing (CBMI) . IEEE, 2016, pp. 1–6.\n[25] C. Zhang, J. Yu, and Z. Chen, “Music emotion recog-\nnition based on combination of multiple features and\nneural network,” in 2021 IEEE 4th Advanced Informa-\ntion Management, Communicates, Electronic and Au-\ntomation Control Conference (IMCEC) , vol. 4. IEEE,\n2021, pp. 1461–1465.\n[26] L. Guo, L. Wang, C. Xu, J. Dang, E. S. Chng, and\nH. Li, “Representation learning with spectro-temporal-\nchannel attention for speech emotion recognition,” in\nICASSP 2021-2021 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2021, pp. 6304–6308.\n[27] Z. Zhang, B. Wu, and B. Schuller, “Attention-\naugmented end-to-end multi-task learning for emotion\nprediction from speech,” in ICASSP 2019-2019 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) . IEEE, 2019, pp. 6705–\n6709.\n[28] J. S. Gómez-Cañón, E. Cano, T. Eerola, P. Herrera,\nX. Hu, Y .-H. Yang, and E. Gómez, “Music emotion\nrecognition: Toward new, robust standards in person-\nalized and context-sensitive applications,” IEEE Sig-\nnal Processing Magazine , vol. 38, no. 6, pp. 106–114,\n2021.\n[29] X. Hu and Y .-H. Yang, “The mood of chinese pop mu-\nsic: Representation and recognition,” Journal of the\nAssociation for Information Science and Technology ,\nvol. 68, no. 8, pp. 1899–1910, 2017.\n[30] K. Zhang, H. Zhang, S. Li, C. Yang, and L. Sun, “The\npmemo dataset for music emotion recognition,” in Pro-\nceedings of the 2018 acm on international conference\non multimedia retrieval , 2018, pp. 135–142.\n[31] G. Li and Y . Yu, “Visual saliency based on multiscale\ndeep features,” in Proceedings of the IEEE conference\non computer vision and pattern recognition , 2015, pp.\n5455–5463.\n[32] M. Zhao, G. Cao, X. Huang, and L. Yang, “Hybrid\ntransformer-cnn for real image denoising,” IEEE Sig-\nnal Processing Letters , vol. 29, pp. 1252–1256, 2022.\n[33] S. Ioffe and C. Szegedy, “Batch normalization: Accel-\nerating deep network training by reducing internal co-\nvariate shift,” in International conference on machine\nlearning . pmlr, 2015, pp. 448–456.[34] V . Nair and G. E. Hinton, “Rectiﬁed linear units im-\nprove restricted boltzmann machines,” in Proceedings\nof the 27th international conference on machine learn-\ning (ICML-10) , 2010, pp. 807–814.\n[35] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation\nnetworks,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2018, pp.\n7132–7141.\n[36] Y . Dai, F. Gieseke, S. Oehmcke, Y . Wu, and\nK. Barnard, “Attentional feature fusion,” in Proceed-\nings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision , 2021, pp. 3560–3569.\n[37] “Mediaeval benchmarking initiative for multimedia e-\nvaluation,” http://www.multimediaeval.org/ .\n[38] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,\nE. Battenberg, and O. Nieto, “librosa: Audio and music\nsignal analysis in python,” in Proceedings of the 14th\npython in science conference , vol. 8. Citeseer, 2015,\npp. 18–25.\n[39] M. Soleymani, M. N. Caro, E. M. Schmidt, C.-Y . Sha,\nand Y .-H. Yang, “1000 songs for emotional analysis of\nmusic,” in Proceedings of the 2nd ACM international\nworkshop on Crowdsourcing for multimedia , 2013, pp.\n1–6.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n214"
    },
    {
        "title": "LyricWhiz: Robust Multilingual Zero-Shot Lyrics Transcription by Whispering to ChatGPT.",
        "author": [
            "Le Zhuo",
            "Ruibin Yuan",
            "Jiahao Pan",
            "Yinghao Ma",
            "Yizhi Li",
            "Ge Zhang",
            "Si Liu",
            "Roger B. Dannenberg",
            "Jie Fu 0001",
            "Chenghua Lin",
            "Emmanouil Benetos",
            "Wenhu Chen",
            "Wei Xue",
            "Yike Guo"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.10265295",
        "url": "https://doi.org/10.5281/zenodo.10265295",
        "ee": "https://zenodo.org/records/10265295/files/000040.pdf",
        "abstract": "We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the \"ear\" by transcribing the audio, while GPT-4 serves as the \"brain,\" acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copy-right license, based on MTG-Jamendo, and offer a human- annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task.",
        "zenodo_id": 10265295,
        "dblp_key": "conf/ismir/ZhuoYPMLZLDFLBC23",
        "keywords": [
            "LyricWhiz",
            "robust",
            "multilingual",
            "zero-shot",
            "automatic lyrics transcription",
            "state-of-the-art",
            "audio transcription",
            "GPT-4",
            "weakly supervised",
            "speech recognition"
        ],
        "content": "LYRICWHIZ: ROBUST MULTILINGUAL ZERO-SHOT LYRICS\nTRANSCRIPTION BY WHISPERING TO CHATGPT\nLe Zhuo1Ruibin Yuan2,3Jiahao Pan4Yinghao Ma5Yizhi Li6Ge Zhang2,7Si Liu1\nRoger Dannenberg3Jie Fu2Chenghua Lin6Emmanouil Benetos5Wenhu Chen7Wei Xue4Yike Guo4\n1Beihang University2Beijing Academy of Artiﬁcial Intelligence3Carnegie Mellon University\n4Hong Kong University of Science and Technology5Queen Mary University of London\n6University of Shefﬁeld7University of Waterloo\nzhuole1025@gmail.com, ruibiny@andrew.cmu.edu, fujie@baai.ac.cn\nABSTRACT\nWe introduce LyricWhiz, a robust, multilingual, and\nzero-shot automatic lyrics transcription method achieving\nstate-of-the-art performance on various lyrics transcrip-\ntion datasets, even in challenging genres such as rock and\nmetal. Our novel, training-free approach utilizes Whisper,\na weakly supervised robust speech recognition model, and\nGPT-4, today’s most performant chat-based large language\nmodel. In the proposed method, Whisper functions as the\n“ear” by transcribing the audio, while GPT-4 serves as the\n“brain,” acting as an annotator with a strong performance\nfor contextualized output selection and correction. Our ex-\nperiments show that LyricWhiz signiﬁcantly reduces Word\nError Rate compared to existing methods in English and\ncan effectively transcribe lyrics across multiple languages.\nFurthermore, we use LyricWhiz to create the ﬁrst pub-\nlicly available, large-scale, multilingual lyrics transcription\ndataset with a CC-BY-NC-SA copyright license, based on\nMTG-Jamendo, and offer a human-annotated subset for\nnoise level estimation and evaluation. We anticipate that\nour proposed method and dataset will advance the devel-\nopment of multilingual lyrics transcription, a challenging\nand emerging task.\n1. INTRODUCTION\nAutomatic lyrics transcription (ALT) is a crucial task in\nmusic information retrieval (MIR) that involves convert-\ning an audio recording into a textual representation of\nthe lyrics sung in the recording. The importance of this\ntask stems from the fact that lyrics are a fundamental as-\npect of many music genres and are often the main way in\nwhich listeners engage with and interpret a song’s mean-\ning. Additionally, ALT has numerous applications in the\nmusic industry, such as enabling better cataloging [1], mu-\nsic searching [2, 3], music recommendation [4], as well as\n© L Zhuo, R Yuan, and J Pan. Licensed under a Creative\nCommons Attribution 4.0 International License (CC BY 4.0). Attribu-\ntion: L Zhuo, R Yuan, and J Pan, “LyricWhiz: Robust Multilingual\nZero-shot Lyrics Transcription by Whispering to ChatGPT”, in Proc. of\nthe 24th Int. Society for Music Information Retrieval Conf., Milan, Italy,\n2023.\nFigure 1 . Concept illustration of the working LyricWhiz,\nwhere user prompts the two advanced models, Whisper\nand ChatGPT, to perform automatic lyrics transcription.\nfacilitating the creation of karaoke tracks and lyric videos.\nMoreover, ALT can assist in various music-related re-\nsearch tasks, including sentiment analysis [5], music genre\nclassiﬁcation [1], lyrics generation, which is further used\nfor music generation [6], security review, and music copy-\nright protection. Thus, accurate and efﬁcient ALT is essen-\ntial for advanced MIR and the development of new music-\nrelated applications.\nHowever, to date, no sufﬁciently robust and accurate\nALT system has been developed. Even major commercial\nmusic streaming platforms still rely heavily on manually-\nannotated lyrics, incurring high costs. One key reason is\nthe challenging nature of lyrics transcription. The diver-\nsity of singing styles and skills leads to varied timbres\nof the same pronunciation. Moreover, the phonemes in\nsinging may be pronounced in vastly different ways, such\nas longer duration, tone changes, or even vowel substitu-\ntions, to accommodate the melody. Lastly, the inclusion\nof various music accompaniments across different genres\nmakes it challenging to distinguish the vocal signals from\nother sounds. To surmount these challenges, a more robust\nALT system is necessary, capable of outperforming exist-\ning models in diverse scenarios, including the transcription\nof multilingual lyrics.\nAnother signiﬁcant factor hindering the progress of343ALT systems is the absence of large-scale singing datasets.\nCurrently, only two relatively sizable datasets [7, 8] exist\nfor ALT systems. However, all existing datasets are in\nEnglish, with no multilingual datasets available. Besides,\nthese datasets often have stringent copyright licensing re-\nstrictions, which signiﬁcantly hampers their utilization by\nresearchers. Consequently, developing a more comprehen-\nsive and representative dataset, encompassing multiple lan-\nguages and without copyright issues, is essential for sup-\nporting the creation of a robust and accurate system.\nIn this paper, we present LyricWhiz, a novel method for\nautomatic lyrics transcription. LyricWhiz surpasses exist-\ning methods on various ALT datasets, resulting in a signif-\nicant reduction in WER for English lyrics and providing\naccurate transcription results across multiple languages.\nOur system is robust, multilingual, and training-free. To\nachieve these results, we combined two powerful models\nfrom their respective domains as shown in Figure 1: Whis-\nper, a weakly supervised speech transcription model, and\nGPT-4, a large language model (LLM) from the ChatGPT\nfamily. Whisper acts as the “ear” while GPT-4 serves as\nthe “brain” by providing contextualized output selection\nand correction with strong performance [9]. We further\nuse LyricWhiz to build a multilingual lyrics dataset, named\nMulJam, which is the ﬁrst large-scale, multilingual lyrics\ntranscription dataset without copyright-related issues.\nThe contributions of our work are as follows:\n• We propose a novel, robust, training-free ALT\nmethod, LyricWhiz, which signiﬁcantly reduces\nWER on various ALT benchmark datasets, includ-\ning Jamendo, Hansen, and MUSDB18, and is close\nto the in-domain state-of-the-art system on DSing.\n• We introduce the ﬁrst ALT system that can perform\nzero-shot, multilingual, long-form ALT by integrat-\ning a large speech transcription model and an LLM\nfor contextualized post-processing.\n• We create the ﬁrst publicly-available, large-scale,\nmultilingual lyrics transcription dataset with a clear\ncopyright statement which eliminates further re-\nviewing of the users and facilitates public usage. We\nprovide a human-annotated subset to estimate noise\nlevels and evaluate multilingual ALT performance.\n2. RELATED WORK\n2.1 Automatic Lyrics Transcription\nAutomatic lyrics transcription (ALT) is an essential task\nin music information retrieval and analysis, aiming to rec-\nognize lyrics from singing voices. It remains challenging\ndue to facts such as the sparsity of training data and the\nunique acoustic characteristics of the singing voice that dif-\nfer from normal speech. Traditional methods treat ALT in\nthe automatic speech recognition (ASR) framework, which\ngenerally utilizes a hybrid of language model and acous-\ntic model, e.g., HMM-GMM. Music-related characteristics\nhave been used to further address these challenges [11–13].Despite integrating domain-speciﬁc music priors into\nsystem designs, the data scarcity issue persists. Recently,\nsome researchers have constructed datasets for end-to-end\nlearning, which greatly advances ALT, but most datasets\nare either noisy (DALI [7, 14], Hansen [15], DAMP-\nMVP1); not large (V ocadito [16]); or not diverse in terms\nof genre and language (MUSDB18 [17], DSing [8]).\nRecent rapid progress in ASR has greatly beneﬁted\nALT. Some work focuses on applying the ASR model ar-\nchitectures [18–20], such as the Transformers, to ALT,\nand other work leverages the vast amount of public anno-\ntated ASR datasets [19–21] to bridge between the speech\nand music data. For the ﬁrst time, a recent study [22]\ntransferred a large-scale self-supervised pre-trained ASR\nmodel, mus2vec 2.0, to the singing domain, and exhib-\nited superior performance on multiple benchmark datasets.\nNevertheless, this approach consists of pre-training, ﬁne-\ntuning, and transfer learning phases, thereby remaining rel-\natively complicated and still requiring singing datasets.\n2.2 Weakly Supervised Automatic Speech Recognition\nThe paradigm of large-scale unsupervised pretraining and\nnon-large annotated dataset ﬁnetuning has dominated end-\nto-end ASR research [23]. Well-known pretrained ASR\nmodels include contrastive learning based Wav2vec [24],\nWav2vec 2.0 [25], HuBert [26], WavLM [27], Whis-\nper [28], and Vall-E [29], which have performed impres-\nsively in various downstream tasks, including ASR and\nspeech synthesis. Among them, Whisper has been most\nrecognized for its ASR robustness across different datasets\nand its multilingual and multitasking capabilities, mak-\ning Whisper potentially applicable to music tasks. Be-\nsides, speciﬁcally for ALT, pre-trained musical audio mod-\nels including JukeBox [6], MusicLM [30], MULE [31],\nSingSong [32], music2vec [33], and MERT [34], may also\ncontribute to achieving strong performance.\n2.3 Chat-based Large Language Models\nChatGPT2, a chat-based large language model (LLM),\nhas found broad application in optimizing workﬂows\nacross a variety of domains, including multimodal intel-\nligence [35, 36]. Recent breaking AutoGPT3is even rec-\nognized as an embryonic form of artiﬁcial general intel-\nligence. Inspired by these developments, LyricWhiz col-\nlaborates with both Whisper [28] and ChatGPT to opti-\nmize the workﬂow of ALT. Prompt engineering is known\nto be important to navigate LLMs to perform better [37].\nLyricWhiz mainly adopts three primary strategies:\na) As shown in [38, 39], a well-formalized task de-\nscription prompt can effectively improve ChatGPT’s per-\nformance on downstream tasks with strict format require-\nments. We follow this empirical observation to strictly for-\nmalize the expected format of ALT post-processing out-\nputs. We also refer to the prompt pattern catalog in [39]\nfor an intuitive understanding of prompt engineering.\n1https://zenodo.org/record/2747436#.ZDqBQOzML0o\n2https://openai.com/blog/chatgpt\n3https://github.com/Torantulino/Auto-GPTProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n344Recordings\nVocal\nRecordingsPANNsWhisper\nWhisper ChatLLMLyrics Option 1\nLyrics Option 2\nLyrics Option n Lyrics Option nFinal Lyrics\n...\nAudio Event\nDetectionLanguage\nIdentificationLanguage\nTags\nDecoding\nPromptsDecoding\nPromptsCoT\nInstructionsFigure 2 . Framework of the proposed LyricWhiz. In the ﬁrst stage, we employ PANNS [10], to detect audio events and\nﬁlter out non-vocal recordings. In the second stage, we utilize the language identiﬁcation module in Whisper to predict\ninput audio language. We then construct language-speciﬁc prompts for Whisper and transcribe input audio multiple times.\nIn the ﬁnal stage, we request ChatGPT with CoT instructions to ensemble multiple predictions and generate the ﬁnal lyrics.\nb) Inspired by [40, 41], LyricWhiz utilizes prompt aug-\nmentation to ask ChatGPT to analyze the prompt and input\nlyrics, in order to select the most accurate prediction from\nmultiple Whisper trials, which is done in the ﬁrst phase as\nillustrated in Section 3.2. [41] designs a gradient-guided\nstrategy to select prompts. By contrast, we simply feed\nChatGPT with an instruction to select prompts for itself.\nc) The importance of a well-designed CoT [42], which\neffectively divides a complicated task into several phases\nand designs speciﬁc prompts for each phase, is widely ac-\nknowledged for enhancing LLM performance. We also\nproposes a concise CoT strategy, depicted in Section 3.2.\n3. METHODOLOGY\nThe overall framework of our method is presented\nin Figure 2. This section will provide an in-depth analysis\nof the design of the Whisper and ChatGPT components,\nand our multilingual dataset.\n3.1 Whisper as Zero-shot Lyrics Transcriptor\nIn the Whisper [28] paper, the authors scaled the weakly\nsupervised ASR to 680,000 hours of labeled audio data,\nwhich covers 96 languages and includes both multilin-\ngual and multitask training. This approach demonstrates\nhigh-quality results without the need for ﬁne-tuning, self-\nsupervision, or self-training techniques. By leveraging\nweak supervision and large-scale training, Whisper gen-\neralizes well to standard benchmarks and achieves robust\nspeech recognition in various downstream tasks.\nMotivated by this, we discovered that the weakly super-\nvised Whisper model, trained on speech data, also excels in\nlyrics transcription within the music domain. We directly\napply Whisper to transcribe lyrics of music from various\ngenres, including pop, folk, rock, and rap, and ﬁnd that the\nmodel consistently achieves accurate transcription results.\nThe model excels at long-form transcription and is robust\nto different song styles, even for challenging genres such\nas rock and electronic music, where Whisper still providesreasonable results. We further test Whisper on multiple\nbenchmark datasets for lyric transcription. The results in-\ndicate that Whisper, without any training or ﬁne-tuning,\ncan achieve or surpass SOTA performance across multiple\nlyric transcription datasets.\nUpon analyzing the transcription results from Whisper,\nwe observed that the model occasionally outputs content\nunrelated to lyrics, such as music descriptions, emojis,\nwebsite watermarks, and YouTube advertisements. We at-\ntribute this to the weakly supervised training of Whisper\non large-scale noisy speech datasets. To address this issue,\nwe utilize the input prompt designed in Whisper as a preﬁx\nprompt to guide it toward the lyric transcription task. Un-\nlike prompt designing philosophy in other large language\nmodels, Whisper’s preﬁx prompt does not work well with\nexplicit task instructions and has difﬁculty understanding\nlengthy explanations. In practice, we notice that using the\nsimplest prompt, “lyrics:\", effectively prevents the model\nfrom outputting descriptions of the music in most cases,\nresulting in a signiﬁcant improvement in transcription re-\nsults. Therefore, in the following sections, this prompt is\nconsistently used for Whisper’s transcription input.\nAdditionally, we apply post-processing tricks to Whis-\nper’s output, utilizing the model’s predicted no-speech\nprobability to handle situations where predictions are made\ndespite the absence of vocals in the song. Speciﬁcally, we\ndrop predicted lines of lyrics with a no speech probability\ngreater than 0.9. This effectively ﬁlters out watermarks and\nadvertisements, further enhancing the transcription results.\n3.2 ChatGPT as Effective Lyrics Post-processor\nAlthough we addressed some issues with Whisper’s pre-\ndictions through prompt design and post-processing, we\nstill cannot avoid transcription translation errors, as well\nas grammatical and syntactical errors. Furthermore, due\nto the inherently stochastic nature of temperature schedul-\ning in Whisper, the transcription predictions vary with each\nrun, leading to ﬂuctuations in evaluation metrics. To re-\nduce this variance and enhance overall accuracy, we gen-Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n345GPT-4 Instruction Prompt\nTask: As a GPT-4 based lyrics transcription post-processor,\nyour task is to analyze multiple ASR model-generated ver-\nsions of a song’s lyrics and determine the most accurate ver-\nsion closest to the true lyrics. Also ﬁlter out invalid lyrics\nwhen all predictions are nonsense.\nInput: The input is in JSON format:\n{“prediction_1”: “line1;line2;...”, ...}\nOutput: Your output must be strictly in readable JSON format\nwithout any extra text:\n{\n“reasons”: “reason1;reason2;...”,\n“closest_prediction”: <key_of_prediction>\n“output”: “line1;line2...”\n}\nRequirements: For the \"reasons\" ﬁeld, you have to provide\na reason for the choice of the \"closest_prediction\" ﬁeld. For\nthe \"closest_prediction\" ﬁeld, choose the prediction key that\nis closest to the true lyrics. Only when all predictions greatly\ndiffer from each other or are completely nonsense or mean-\ningless, which means that none of the predictions is valid,\nﬁll in \"None\" in this ﬁeld. For the \"output\" ﬁeld, you need\nto output the ﬁnal lyrics of closest_prediction. If the \"clos-\nest_prediction\" ﬁeld is \"None\", you should also output \"None\"\nin this ﬁeld. The language of the input lyrics is English.\nTable 1 . Instruction prompt for GPT-4 contextualized\npost-processing. We decompose this task into three con-\nsecutive phases, inspired by Chain-of-Thought prompting.\nNote that lines in blue indicate additional prompts used ex-\nclusively for multilingual dataset construction.\nerate3to5predictions for each input music under identical\nsettings and employ ChatGPT as an expert in lyrics to en-\nsemble these multiple predictions.\nThe crux of the problem lies in designing an effective\nprompt for ChatGPT to accomplish the ensemble task rea-\nsonably. As shown in Table 1, we ﬁrst assign ChatGPT\nthe role of a transcription post-processor, indicating that\nits task is to analyze multiple lyric transcription results and\nselect the one it deems most accurate. We then stipulate\nthat both input and output should be in JSON format to fa-\ncilitate structured processing and provide detailed descrip-\ntions for each output ﬁeld.\nDrawing on the Chain-of-Thought in large language\nmodels for reasoning, we devised a concise thought chain\nfor ChatGPT that decomposes lyrics post-processing into\nthree consecutive phases. This involves ﬁrst having Chat-\nGPT analyze multiple lyric inputs and provide reasons for\nselection, then making a choice, and ﬁnally outputting the\nchosen lyric prediction. We test this approach using GPT-\n3.5 and the newly released GPT-4. The results demon-\nstrate that using the analysis-selection-prediction prompt\nfor ChatGPT’s inference effectively enhances the ﬁnal\ntranscription results, with GPT-4 exhibiting a noticeably\nsuperior performance compared to GPT-3.5.\n3.3 Multilingual Lyrics Transcription Dataset\nBuilding upon the exceptional performance of the pro-\nposed framework in lyric transcription tasks, we further\nextend it to the challenging task of multilingual lyric tran-\nscription, introducing the ﬁrst large-scale, weakly super-\nvised, and copyright-free multilingual lyric transcription\ndataset. We utilize the publicly available MTG-JamendoDataset Languages Songs Lines Duraion\nDSing [8] 1 (en) 4,324 81,092 149.1h\nMUSDB18 [17] 1 (en) 82 2,289 4.6h\nDALI-train [14] 1 (en) 3,913 180,034 208.6h\nDALI-full [14] 30∗5,358∗- -\nMulJam (Ours) 6 6,031 182,429 381.9h\nTable 2 . Comparison between different lyrics transcription\ndatasets. Our model operates with a longer window (~ 30s),\nresulting in fewer lines compared to other datasets.\ndataset for music classiﬁcation, which comprises 55,000\nfull audio tracks, 195 tags, and music in various languages.\nSince the MTG dataset contains a considerable propor-\ntion of non-vocal music, we ﬁrst employ PANNs [10], a\nlarge-scale pre-trained audio pattern recognition model, to\ndetect audio events and ﬁlter out non-vocal music with\nvocal-related tag probabilities below a predeﬁned thresh-\nold. This ﬁltering method eliminates approximately 60%\nof the music, thereby substantially reducing the time and\nresources required for dataset construction. We then uti-\nlize Whisper to transcribe lyrics from the music.\nAs the music in the MTG dataset encompasses multi-\nple languages, we ﬁrst utilize the Language Identiﬁcation\nmodule within Whisper to predict the language of input\nmusic. Based on the predicted language, we translate the\npreﬁx prompt “lyrics:” into the corresponding language for\ninput, e.g., “paroles” in French, and “liedtext” in German.\nAfter obtaining the transcription results, we discard lyrics\nthat are too short or too long. When ensembling the pre-\ndiction results with ChatGPT, we also incorporate the lan-\nguage of lyrics as an input condition in the prompt. Given\nthe prevalence of nonsensical content in the transcription\nresults, we additionally require ChatGPT to evaluate the\nvalidity of the transcribed lyrics in the prompt. If all in-\nput lyrics are deemed nonsensical, e.g., all special Unicode\ncharacters, or extremely divergent, the transcription result\nfor that piece of music is considered invalid and discarded.\nTo prepare the dataset for training, it is essential to con-\nduct line-level annotation. Timestamps can be obtained\nfrom the output of Whisper by aligning the lyrics both\nbefore and after ChatGPT processing. For the alignment\nof strings, the Levenshtein distance [43] is employed. To\nexclude aligned lines of lower conﬁdence, the distance is\nnormalized, setting a threshold at 0.2. The quality of an-\nnotation is further enhanced through two subsequent ﬁlter-\ning stages. In the ﬁrst stage, lines that exhibit unusually\nhigh character rates, exceeding 37.5 Hz, are eliminated.\nThe second stage encompasses another Whisper iteration;\nsegments yielding a transcription of \"Thank you.\" are ex-\ncluded. These segments, which typically represent instru-\nmental sections, are believed to originate from Whisper’s\ntraining on data similar to video transcripts.\nFollowing the construction process outlined above,\nwe ultimately obtained a multilingual lyric transcription\ndataset, MulJam, consisting of 6,031 songs with 182,429\nlines and a total duration of 381.9 hours. The dataset’s\nstatistical information and comparisons with existing ALT\ndatasets are presented in Table 2.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n346Method Jamendo Hansen DSing\nTDNN-F [8] 76.37 77.59 19.60\nCTDNN-SA [44] 66.96 78.53 14.96\nGenre-informed AM [12] 50.64 39.00 56.90\nMSTRE-Net [13] 34.94 36.78 15.38\nDE2-segmented [45] 44.52 49.92 -\nW2V2-ALT [22] 33.13 18.71 12.99\nLyricWhiz (Ours) 24.25 7.85 13.78\nw/o ChatGPT Ens. 28.18 8.07 15.22\nw/o Whis. Prompt 33.21 8.75 13.40\nTable 3 . The WERs (%) of various ALT systems, in-\ncluding ablation methods, on multiple datasets. Note that\nW2V2-ALT is an in-domain baseline that natively train on\nDSing. The results of our method on Jamendo, Hansen are\nobtained from full-length transcription results, and the re-\nsults on DSing are obtained from utterance-level segments.\nTo our best knowledge, MulJam is the ﬁrst publicly\navailable large-scale dataset for multilingual lyrics tran-\nscription without copyright restrictions. While DALI [7]\nis another large-scale music dataset featuring multilingual\nlyrics, its restricted access and strict licensing requirements\nlimit its applicability for downstream tasks. In contrast,\nMulJam is free from copyright-related constraints and can\nbe utilized without approval, as the audio can be legally\ndownloaded directly from public sources without the need\nfor approval, making it easily accessible. This even in-\ncludes audio that is permitted for use in the development of\ncommercial software. Researchers are permitted to legally\nmodify our dataset for derivative works and redistribution,\nprovided they cite our work and adhere to the CC BY-NC-\nSA license. Furthermore, in contrast to the imbalanced\nlanguage distribution in DALI, where English songs ac-\ncount for over 80% of the total songs, our dataset includes\na greater proportion of songs in other languages, which is\nadvantageous for multilingual lyrics transcription.\n4. EXPERIMENTS\nIn this section, we ﬁrst outline our experimental setup, in-\ncluding datasets and evaluation metrics. Next, we report\nlyrics transcription results on various benchmark datasets.\nWe also conduct extensive ablation studies to verify the ef-\nfectiveness of our methods. Finally, we demonstrate the\nreliability of our dataset through noise level estimation.\n4.1 Experimental Setup\nDatasets. Our proposed method does not require any\ntraining; thus, we directly test it on several accessible lyric\ntranscription benchmark datasets, including Jamendo [46],\nHansen [15], MUSDB18 [17], DSing [8]. Among these,\nJamendo, Hansen, and DSing are widely used test datasets\nin music transcription. MUSDB18, originally a dataset\nfor music source separation, contains 150 rock-pop songs.\nThe authors in [17] provided line-level lyric annotations\nfor MUSDB18, making it a challenging real-world dataset\nfor lyric transcription. Additionally, we manually collected\n40 multilingual songs with lyrics annotations from MTG-Method a) b) c)\nCTDNN-SA-mixture [17] 76.06 78.44 89.24\nOurs-mixture 50.90 47.04 50.70\nCTDNN-SA-vocals [17] 37.83 30.85 58.45\nOurs-vocals 26.29 25.27 33.30\nTable 4 . The WERs (%) of our method and baseline [17]\non three subsets of annotated MUSDB18. The results of\nour method are obtained from utterance-level segments.\nJamendo as a test set for the proposed dataset, which can\nbe used to validate the reliability of our proposed dataset\nvia transcription accuracy.\nEvaluation. We report the Word Error Rate (WER) as\nthe evaluation metric, which is the ratio of the total num-\nber of insertions, substitutions, and deletions with respect\nto the total number of words. We calculate the average\nWER on the test sets. Since Whisper possesses the capa-\nbility for long-form transcription, we directly evaluate en-\ntire songs using Jamendo, Hansen, and the multilingual test\nset. We perform utterance-level evaluations on MUSDB18\nand DSing since they only have utterance-level annota-\ntions. We discovered that many songs in these evaluation\ndatasets are problematic, such as incorrect lyric annota-\ntions and excessively short song segments. One notable\nproblem is that sometimes there are prominent harmony\nparts in the background of a song. However, it is not pro-\nvided in the lyric annotations (e.g., Adele’s “Rolling in the\nDeep”). LyricWhiz is powerful enough to transcript both\nthe leading vocal and the background vocal with high accu-\nracy. Therefore, we removed these problematic data from\nour evaluations. Finally, we normalize the transcription re-\nsults to match the standardized ground truths. We remove\nall special Unicode characters, such as emojis. All text is\nconverted to lowercase, and numeric characters are con-\nverted to their alphabetic correspondence.\nBudget. To ensure fast and multi-round inference of\nthe Whisper-large model on various datasets, including\nthe large-scale MTG-Jamendo dataset, we conducted our\nexperiments concurrently on a server with 8xA100 80G\nGPUs. It takes approximately 9 hours to complete one\nround of inference, and each process uses up to 12G\nVRAM. The vocal probability threshold is set to 0.07 for\nPANNs-based vocal event detection. To carry out contex-\ntualized post-processing using ChatGPT, we invested a to-\ntal of US$2,000 on GPT-4 API for the entire project.\n4.2 Comparative Experiments\nIn order to verify the superiority of our approach, we\ncompare it with several previous studies on benchmark\ndatasets. W2V2-ALT [22], a transfer learning method\nbased on ASR self-supervised models, represents the cur-\nrent state-of-the-art in lyric transcription tasks. In our ex-\nperiments, we primarily compare our method with W2V2-\nALT, as well as other previous methods. The experimen-\ntal results, as shown in Table 3, indicate that our method\nachieves the best performance on Jamendo and Hansen\nand the second-best performance on DSing. In long-formProceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n347transcription datasets such as Jamendo and Hansen, our\nmethod signiﬁcantly outperforms all previous approaches\ndue to the strong contextual memory capabilities of both\nWhisper and ChatGPT. Furthermore, our method also\nleads by a considerable margin on MUSDB18, shown in\nTable 4, demonstrating the robust performance and re-\nsilience of our proposed method in more diverse and com-\nplex musical scenarios. It is worth noting that our method\ndid not surpass previous results on the DSing dataset,\nwhich we attribute to two factors. First, previous models\nwere trained on the DSing training set, making the DS-\ning test set an in-distribution dataset for the models, while\nour approach does not require any training and directly em-\nploys large-scale ASR models for zero-shot lyric transcrip-\ntion. Second, the segmented evaluation on DSing results\nin the loss of contextual information, which consequently\nleads to inaccurate transcriptions.\n4.3 Ablation Studies\nTo further substantiate the efﬁcacy of each component\nwithin our proposed approach, we conducted comprehen-\nsive ablation experiments.\nWhisper Prompt. In our experiments, we investigate\nthe Whisper prompt mechanism and test various prompts.\nFirst, we construct a complex prompt following the format\nof ChatGPT prompts, including task descriptions, format\nspeciﬁcations, and speciﬁc requirements. We then grad-\nually reduce the constituent elements of the prompt and\nobserve the results. We discover that, unlike general large\nlanguage models, Whisper has weaker task understanding\ncapabilities for complex prompts and can only compre-\nhend shorter task prompts. In practice, using the simplest\nprompt “lyrics:” yielded the best results. For multilingual\ntranscription, we translate \"lyrics:\" into the corresponding\nlanguage. As shown in Table 3, the designed prompt per-\nforms better in long-form transcription scenarios, assist-\ning the model in producing meaningful lyrics for difﬁcult\ntasks. However, its performance is less effective at the ut-\nterance level, possibly because predicting a single line of\nlyrics does not require additional contextual information.\nChatGPT Ensemble. In order to conﬁrm that ChatGPT\ncan analyze and infer the most accurate version of lyrics,\nwe ﬁrst conduct a simple experiment. In this experiment,\nwe add the ground truth lyrics to the predicted results and\ninput them together into ChatGPT for ensembling. We\nthen calculate the proportion of times ChatGPT ultimately\nchose the ground truth. If ChatGPT is able to choose the\nmost accurate lyrics, i.e., the ground truth, the ﬁnal pro-\nportion should be close to 100% . The computed results on\nthe Hansen dataset is 72.7%for ground truth data, which\nis sufﬁcient to demonstrate that ChatGPT can make cor-\nrect choices based on the constructed prompt and input\nlyrics. As further observed in Table 3, ChatGPT ensem-\nbling is particularly effective for long-form lyric transcrip-\ntion, suggesting that ChatGPT requires contextual infor-\nmation (the content of preceding and following lyrics, as\nwell as the content of different versions of predicted lyrics)\nfor inference. In contrast, utterance-level lyric inputs lackLanguage Songs train Songstest WERtest\nEnglish 3,791 20 21.86\nFrench 1,030 7 26.64\nSpanish 620 5 22.54\nItalian 311 3 44.01\nRussian 147 4 39.18\nGerman 132 1 25.43\nOverall 6,031 40 26.26\nTable 5 . The distribution of our dataset and WERs (%) on\ntest set. We manually constructed a test set of 40 songs\nfollowing the language distribution of the collected train-\ning set. Then, we applied our proposed method to the test\nset and computed the WER.\nboth context and diversity among different prediction re-\nsults, leading to inferior performance.\n4.4 Dataset Analysis\nIn order to demonstrate the reliability of the dataset con-\nstructed using Whisper and ChatGPT on MTG-Jamendo,\nwe manually create a multilingual test set for noise level\nestimation. Speciﬁcally, we ﬁrst select six languages from\nthe intersection of the languages in MTG and those in\nwhich Whisper performs best. We then conduct a strati-\nﬁed sampling of 40 songs on Jamendo and manually an-\nnotate their lyrics. We use these 40 songs as a test set,\nassessing the WER to estimate the noise level of our col-\nlected dataset. Table 5 presents the number of songs in\neach language and the WER results for the test set, where\nour method achieves decent WER levels for the majority\nof languages. As our goal is to construct a large-scale,\nmultilingual dataset for weak supervision, our method’s\ntranscription results are acceptable. Furthermore, we have\nnot implemented speciﬁc normalization for multilingual\ntranscription results, such as removing diacritical marks,\nwhich could be employed to enhance performance.\n5. CONCLUSION\nThis paper presents LyricWhiz, a novel zero-shot au-\ntomatic lyrics transcription system excelling in various\ndatasets and music genres. Combining Whisper and GPT-\n4, our approach signiﬁcantly reduces WER in English\nand efﬁciently transcribes multiple languages. LyricWhiz\nfurther generates the ﬁrst publicly accessible, large-scale,\nmultilingual lyrics dataset with a human-annotated subset\nfor noise level estimation and evaluation. The success-\nful integration of the large speech model and large lan-\nguage model in LyricWhiz offers a novel avenue for tradi-\ntional Music Information Retrieval (MIR) tasks, as previ-\nous task-speciﬁc solutions are being eclipsed by general-\npurpose models. Notably, large language models have\ndemonstrated their superior language understanding abil-\nities across various tasks. Hence, we anticipate further ap-\nplications of large language models to a broader spectrum\nof music-related domains, such as text-to-music genera-\ntion, to enhance the performance of various models.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n3486. ACKNOWLEDGEMENTS\nWe gratefully acknowledge the dataset post-processing\nwork described in Section 3.3 offered by Jiawen Huang.\nJiahao Pan and Wei Xue were supported by the Theme-\nbased Research Scheme (T45-205/21-N) and Early Career\nScheme (ECS-HKUST22201322), Research Grants Coun-\ncil of Hong Kong. Yinghao Ma is a research student at\nthe UKRI Centre for Doctoral Training in Artiﬁcial In-\ntelligence and Music, supported by UK Research and In-\nnovation [grant number EP/S022694/1]. Yizhi Li is fully\nfunded by an industrial PhD studentship (Grant number:\n171362) from the University of Shefﬁeld, UK.\n7. REFERENCES\n[1] A. Tsaptsinos, “Lyrics-based music genre classiﬁca-\ntion using a hierarchical attention network,” arXiv\npreprint arXiv:1707.04678 , 2017.\n[2] H. Fujihara, M. Goto, and J. Ogata, “Hyperlinking\nLyrics: A method for creating hyperlinks between\nphrases in song lyrics.” in ISMIR , 2008, pp. 281–286.\n[3] T. Hosoya, M. Suzuki, A. Ito, S. Makino, L. A. Smith,\nD. Bainbridge, and I. H. Witten, “Lyrics recognition\nfrom a singing voice based on ﬁnite state automaton\nfor music information retrieval.” in ISMIR , 2005, pp.\n532–535.\n[4] P. Knees and M. Schedl, “A survey of music similarity\nand recommendation from music context data,” ACM\nTransactions on Multimedia Computing, Communica-\ntions, and Applications (TOMM) , vol. 10, no. 1, pp.\n1–21, 2013.\n[5] E. Çano and M. Morisio, “MoodyLyrics: A sentiment\nannotated lyrics dataset,” in Proceedings of the 2017\ninternational conference on intelligent systems, meta-\nheuristics & swarm intelligence , 2017, pp. 118–124.\n[6] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,\nand I. Sutskever, “Jukebox: A generative model for\nmusic,” arXiv preprint arXiv:2005.00341 , 2020.\n[7] G. Meseguer-Brocal, A. Cohen-Hadria, and G. Peeters,\n“DALI: a large dataset of synchronized audio, lyrics\nand notes, automatically created using teacher-student\nmachine learning paradigm.” in 19th International So-\nciety for Music Information Retrieval Conference , IS-\nMIR, Ed., September 2018.\n[8] G. R. Dabike and J. Barker, “Automatic lyric transcrip-\ntion from karaoke vocal tracks: Resources and a base-\nline system.” in Interspeech , 2019, pp. 579–583.\n[9] P. Törnberg, “ChatGPT-4 outperforms experts\nand crowd workers in annotating political twitter\nmessages with zero-shot learning,” arXiv preprint\narXiv:2304.06588 , 2023.[10] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and\nM. D. Plumbley, “PANNs: Large-scale pretrained au-\ndio neural networks for audio pattern recognition,”\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , vol. 28, pp. 2880–2894, 2020.\n[11] C. Gupta, H. Li, and Y . Wang, “Automatic pronunci-\nation evaluation of singing.” in Interspeech , 2018, pp.\n1507–1511.\n[12] C. Gupta, E. Yılmaz, and H. Li, “Automatic lyrics\nalignment and transcription in polyphonic music: Does\nbackground music help?” in ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) . IEEE, 2020, pp. 496–\n500.\n[13] E. Demirel, S. Ahlbäck, and S. Dixon, “MSTRE-Net:\nMultistreaming acoustic modeling for automatic lyrics\ntranscription,” arXiv preprint arXiv:2108.02625 , 2021.\n[14] G. Meseguer-Brocal, A. Cohen-Hadria, and G. Peeters,\n“Creating DALI, a large dataset of synchronized audio,\nlyrics, and notes,” Transactions of the International So-\nciety for Music Information Retrieval , vol. 3, no. 1,\n2020.\n[15] J. K. Hansen and I. Fraunhofer, “Recognition of\nphonemes in a-cappella recordings using temporal pat-\nterns and mel frequency cepstral coefﬁcients,” in 9th\nSound and Music Computing Conference (SMC) , 2012,\npp. 494–499.\n[16] R. M. Bittner, K. Pasalo, J. J. Bosch, G. Meseguer-\nBrocal, and D. Rubinstein, “vocadito: A dataset of solo\nvocals with f_0, note, and lyric annotations,” arXiv\npreprint arXiv:2110.05580 , 2021.\n[17] K. Schulze-Forster, C. S. Doire, G. Richard, and\nR. Badeau, “Phoneme level lyrics alignment and text-\ninformed singing voice separation,” IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing ,\nvol. 29, pp. 2382–2395, 2021.\n[18] X. Gao, C. Gupta, and H. Li, “Genre-conditioned\nacoustic models for automatic lyrics transcription of\npolyphonic music,” in ICASSP 2022-2022 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2022, pp. 791–795.\n[19] S. Basak, S. Agarwal, S. Ganapathy, and N. Takahashi,\n“End-to-end lyrics recognition with voice to singing\nstyle transfer,” in ICASSP 2021-2021 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2021, pp. 266–270.\n[20] C. Zhang, J. Yu, L. Chang, X. Tan, J. Chen, T. Qin,\nand K. Zhang, “PDAugment: Data augmentation by\npitch and duration adjustments for automatic lyrics\ntranscription,” arXiv preprint arXiv:2109.07940 , 2021.\n[21] A. M. Kruspe and I. Fraunhofer, “Training phoneme\nmodels for singing with\" songiﬁed\" speech data.” in\nISMIR , 2015, pp. 336–342.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n349[22] L. Ou, X. Gu, and Y . Wang, “Transfer learning of\nwav2vec 2.0 for automatic lyric transcription,” in IS-\nMIR, 2022.\n[23] R. Tang, K. Kumar, G. Yang, A. Pandey, Y . Mao,\nV . Belyaev, M. Emmadi, C. Murray, F. Ture, and\nJ. Lin, “SpeechNet: Weakly supervised, end-to-end\nspeech recognition at industrial scale,” arXiv preprint\narXiv:2211.11740 , 2022.\n[24] S. Schneider, A. Baevski, R. Collobert, and M. Auli,\n“wav2vec: Unsupervised pre-training for speech\nrecognition,” arXiv preprint arXiv:1904.05862 , 2019.\n[25] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli,\n“wav2vec 2.0: A framework for self-supervised learn-\ning of speech representations,” Advances in neural in-\nformation processing systems , vol. 33, pp. 12 449–\n12 460, 2020.\n[26] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia,\nR. Salakhutdinov, and A. Mohamed, “HuBERT: Self-\nsupervised speech representation learning by masked\nprediction of hidden units,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing , vol. 29,\npp. 3451–3460, 2021.\n[27] S. Chen, C. Wang, Z. Chen, Y . Wu, S. Liu, Z. Chen,\nJ. Li, N. Kanda, T. Yoshioka, X. Xiao et al. , “WavLM:\nLarge-scale self-supervised pre-training for full stack\nspeech processing,” IEEE Journal of Selected Topics\nin Signal Processing , vol. 16, no. 6, pp. 1505–1518,\n2022.\n[28] A. Radford, J. W. Kim, T. Xu, G. Brockman,\nC. McLeavey, and I. Sutskever, “Robust speech recog-\nnition via large-scale weak supervision,” arXiv preprint\narXiv:2212.04356 , 2022.\n[29] C. Wang, S. Chen, Y . Wu, Z. Zhang, L. Zhou, S. Liu,\nZ. Chen, Y . Liu, H. Wang, J. Li et al. , “Neural codec\nlanguage models are zero-shot text to speech synthe-\nsizers,” arXiv preprint arXiv:2301.02111 , 2023.\n[30] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel,\nM. Verzetti, A. Caillon, Q. Huang, A. Jansen,\nA. Roberts, M. Tagliasacchi et al. , “MusicLM:\nGenerating music from text,” arXiv preprint\narXiv:2301.11325 , 2023.\n[31] M. C. McCallum, F. Korzeniowski, S. Oramas,\nF. Gouyon, and A. F. Ehmann, “Supervised and un-\nsupervised learning of audio representations for mu-\nsic understanding,” arXiv preprint arXiv:2210.03799 ,\n2022.\n[32] C. Donahue, A. Caillon, A. Roberts, E. Manilow, P. Es-\nling, A. Agostinelli, M. Verzetti, I. Simon, O. Pietquin,\nN. Zeghidour et al. , “SingSong: Generating musi-\ncal accompaniments from singing,” arXiv preprint\narXiv:2301.12662 , 2023.[33] Y . Li, R. Yuan, G. Zhang, Y . Ma, C. Lin, X. Chen,\nA. Ragni, H. Yin, Z. Hu, H. He et al. , “MAP-\nMusic2Vec: A simple and effective baseline for self-\nsupervised music audio representation learning,” arXiv\npreprint arXiv:2212.02508 , 2022.\n[34] Y . Li, R. Yuan, G. Zhang, Y . Ma, X. Chen, H. Yin,\nC. Lin, A. Ragni, E. Benetos, N. Gyenge et al. ,\n“MERT: Acoustic music understanding model with\nlarge-scale self-supervised training,” arXiv preprint\narXiv:2306.00107 , 2023.\n[35] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, and\nY . Zhuang, “HuggingGPT: Solving ai tasks with chat-\ngpt and its friends in huggingface,” arXiv preprint\narXiv:2303.17580 , 2023.\n[36] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu,\nM. Lomeli, L. Zettlemoyer, N. Cancedda, and\nT. Scialom, “Toolformer: Language models can\nteach themselves to use tools,” arXiv preprint\narXiv:2302.04761 , 2023.\n[37] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and\nG. Neubig, “Pre-train, prompt, and predict: A system-\natic survey of prompting methods in natural language\nprocessing,” ACM Computing Surveys , vol. 55, no. 9,\npp. 1–35, 2023.\n[38] J. White, S. Hays, Q. Fu, J. Spencer-Smith, and D. C.\nSchmidt, “ChatGPT prompt patterns for improving\ncode quality, refactoring, requirements elicitation, and\nsoftware design,” arXiv preprint arXiv:2303.07839 ,\n2023.\n[39] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea,\nH. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C.\nSchmidt, “A prompt pattern catalog to enhance\nprompt engineering with ChatGPT,” arXiv preprint\narXiv:2302.11382 , 2023.\n[40] T. Shin, Y . Razeghi, R. L. Logan IV , E. Wallace, and\nS. Singh, “AutoPrompt: Eliciting knowledge from lan-\nguage models with automatically generated prompts,”\narXiv preprint arXiv:2010.15980 , 2020.\n[41] K. Shum, S. Diao, and T. Zhang, “Automatic prompt\naugmentation and selection with chain-of-thought\nfrom labeled data,” arXiv preprint arXiv:2302.12822 ,\n2023.\n[42] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi,\nQ. Le, and D. Zhou, “Chain of thought prompting elic-\nits reasoning in large language models,” arXiv preprint\narXiv:2201.11903 , 2022.\n[43] V . I. Levenshtein et al. , “Binary codes capable of cor-\nrecting deletions, insertions, and reversals,” in Soviet\nphysics doklady , vol. 10, no. 8. Soviet Union, 1966,\npp. 707–710.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n350[44] E. Demirel, S. Ahlbäck, and S. Dixon, “Automatic\nlyrics transcription using dilated convolutional neural\nnetworks with self-attention,” in 2020 International\nJoint Conference on Neural Networks (IJCNN) . IEEE,\n2020, pp. 1–8.\n[45] E. Demirel, S. Ahlbäck, and S. Dixon, “Low re-\nsource audio-to-lyrics alignment from polyphonic mu-\nsic recordings,” in ICASSP 2021-2021 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 2021, pp. 586–590.\n[46] D. Stoller, S. Durand, and S. Ewert, “End-to-end lyrics\nalignment for polyphonic music using an audio-to-\ncharacter recognition model,” in ICASSP 2019-2019\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2019, pp.\n181–185.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n351"
    },
    {
        "title": "Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR 2023, Milan, Italy, November 5-9, 2023.",
        "author": [
            "Augusto Sarti",
            "Fabio Antonacci",
            "Mark Sandler 0001",
            "Paolo Bestagini",
            "Simon Dixon",
            "Beici Liang",
            "Gaël Richard",
            "Johan Pauwels"
        ],
        "year": "2023",
        "doi": "10.5281/zenodo.13916484",
        "url": "https://doi.org/10.5281/zenodo.13916484",
        "ee": null,
        "abstract": "TheProceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR 2023, Milan, Italy, November 5-9, 2023.",
        "zenodo_id": 13916484,
        "dblp_key": "conf/ismir/2023"
    }
]