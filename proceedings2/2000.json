[
    {
        "title": "The role of Music IR in the New Zealand Digital Music Library project.",
        "author": [
            "David Bainbridge 0001"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416260",
        "url": "https://doi.org/10.5281/zenodo.1416260",
        "ee": "https://zenodo.org/records/1416260/files/Bainbridge00.pdf",
        "abstract": "Digital Library (NZDL) project.  In keeping with the scope of the general project, the music work investigates data acquisition, retrieval, presentation and scalability.  These parts are described in turn in the text below.",
        "zenodo_id": 1416260,
        "dblp_key": "conf/ismir/Bainbridge00",
        "keywords": [
            "Digital Library (NZDL)",
            "project",
            "music work",
            "data acquisition",
            "retrieval",
            "presentation",
            "scalability",
            "scope",
            "general project",
            "text below"
        ],
        "content": "The Role of Music IR in the New Zealand Digital Library project \nDavid Bainbridge \nDepartment of Computer Science \nUniversity of Waikato, Hamilton \nNew Zealand \nd.bainbridge@cs.waikato.ac.nz   \nPhone: +64 7 838 4407 \nFax: +64 7 838 4155  \nExtended Abstract \nThis extended abstract describes the computer music work that forms part of the New Zealand \nDigital Library (NZDL) project.  In keeping with the scope of the general project, the music work investigates data acquisition, retrieval, presentation and scalability.  These parts are described in turn in the text below. \nIntroduction  \nMELDEX —the name given to our digital music library project—is a Web-based system that \nsupports searching through text and sung queries, and browsing through automatically compiled lists of metadata, such as titles.  Sample collections include 1,000 popular tunes derived from sheet music using Optical Music Recognition (OMR) software, 10,000 folksongs donated by two other computer music projects, and 100,000 MIDI files gathered from the Web at large.  Matching is monophonic to monophonic.  More specifically, it is monophonic query to monophonic track.  Collections are permitted to include polyphonic tracks, however no matching against these tracks occurs at present.  The software, written in C++ and Perl, implements a distributed architecture making it possible to serve different indexes (text or music) to the same collection on different computers is so desired. \nAcquisition  \nWe have worked with three principal acquisition methods: automatic conversion of sheet music \nusing OMR software, on-line MIDI files, and existing databases of music in symbolic form.  For sheet music we selected a book containing 1,200 popular tunes and digitized each page in black and white at 300 dots per inch (dpi). The resulting images took 48 hours to process using a 133 MHz Pentium processor.  Analyzing the errors related to notes, 9 edits per 100 notes were necessary to correct durational mistakes, but only 1 edit per 100 notes for pitch. This led us to the conclusion that a collection based on the uncorrected data indexed by pitch would yield a useful collection as the music matching algorithm employed already supports approximate pitch matching.  To boost accuracy rates we have since extended our OMR software to optionally merge the reconstructed score with a MIDI rendition of the same piece of music.  The technique developed uses a modified approximate string matching algorithm to align the notes detected in the image with the notes played in the MIDI file and to resolve the differences that arise.    Compared with sheet music, acquiring symbolic music information from a MIDI file is simpler.  Pitch calculations are unambiguous, however mapping the timing information to the musical durations quarter note, half note, etc. is more problematic.  Example collections formed from \nMIDI files are: M IDIMINI, based on 1,200 files from a Web site dedicated to MIDI data; and \nMIDIMAX, based on 100,000 MIDI files located through an Internet search engine. \n Working with existing databases of music in symbolic form minimizes the effort in developing a digital music collection as the task of acquisition has essentially been done for us.  All that is needed is a utility or two to convert the source database into our internal format.  Our 10,000 folk song collection is an example of this type of source data. \nRetrieval  \nDocument retrieval is through the activities of searching and browsing.  Text searching is \naccomplished using the full text retrieval system MG with supplementary code to support fielded searching.  For melody matching there is a choice of two algorithms: a state-based approach and a dynamic programming approach based on approximate string matching.  The latter gives a more precise answer, however it is more computationally expensive.  The algorithm used is controlled through a preferences page, as are the parameters use/ignore duration, contour/interval match, adaptive/fixed tuning, and match at start/anywhere.  The main browsing activity in M\nELDEX  takes the form of alphabetically sorted lists of metadata \n(principally title).  Using text-mining techniques we plan to improve on this by extracting additional metadata, cross referencing it and linking it up with other music resource sites.  An implemented example of this exists in the M\nIDIMINI collection where clicking on the MP3 icon \nnext to the title spawns a new search on an MP3 search site using the song title as the query.  Another form of browsing supported in M\nELDEX  encapsulates the notion of “find more tunes that \nare similar to this one.”  Based on previously calculated sequences of “interesting notes” that are repeated within a score—a motif as it were—a new query for a given document can be initiated by the clicking on a link next to its title to find tunes similar to the target tune; perhaps even a different version of the same tune.  Much hinges on how the pre-calculated motifs are derived (described more fully in the section on scalability below), and this browsing activity is not the only place we can take advantage of this information.  We use it to support a form of music document summary for “quick playback” as well as a way of reducing the size of the melody index for searching large collections.  We expand on both these points in the text below. \nPresentation  \nHaving located a music document, how should it be presented to the user?  The answer, of \ncourse, depends on the intended use. Three principal formats are audio playback, typeset music notation, and textual information.  For a given digital library collection, generation of these formats is strongly dependent on how the music was acquired.  For instance, generation of \ntypeset music in an OMR based collection is straightforward, but this is trickier if as occurs in \na MIDI based collection the source music has never existed in a written form, requiring some \nform of software tool.  In such situations we use an extended command-line version of the music notation application Rosegarden.  To provide an audio summary of a document we draw once again on our pre-calculated motifs, playing, for the segments of the score that correspond to the chosen motifs, all instruments that are involved during that passage of the score. Scalability  \nSo far we have described the core components of the M ELDEX  system.  In handling large \ncollections, such as 100,000 MIDI files, additional components come in to play to support scalability.  These are: the “followed by” operator, bounded memory, and reduction to motifs.  The “followed by” operator works in collections where there is at least one text index.  In this scenario the text query is dispatched first, and the document identifiers that match this query are used to form a local database of music documents.  The music query is then applied to this smaller, local database.  The bounded memory technique is a rudimentary approach that trades off memory usage for time.  Instead of reading in the entire database and matching against that, only part of the database is read in and matching is performed against that.  The returned results are stored and then the process is repeated for the next section of the database with the new results being merged with the stored results.  The final approach we have experimented with is to reduce the size of the musical index through the identification of repeated motifs.  More specifically we have reduced monophonic tracks to a text string that represents the pitches of the notes played, and then applied a highly efficient text-based phrase detection algorithm.  Up to the five longest and five most frequent phrases are chosen, and these are mapped back to their first occurrence in the music file. Applying this technique to the M\nIDIMINI collection, the index size decreased by 96%. \nConclusion \nIn conclusion, M ELDEX  is a rich environment in which to explore the emerging field of digital \nmusic libraries, and usage of the service has grown to the point where statistically significant transaction log analysis can be carried out.    Further technical developments are planned.  A key step is to develop tighter integration with the main NZDL project, thereby benefiting from a wider base of software developers.  This would, for example, increase the number of platforms the software runs on, and adds the option of producing standalone music collections that run off CD-ROM.  To learn more about our work visit the Website at www.nzdl.org . \n \nSuggested Readings \n \nBainbridge, D., Nevill-Manning, C., Witten, I., Smith, L. and McNab, J. 1999. Towards a digital \nlibrary of popular music. 4th ACM conference on Digital Libraries : 474−478. \n Witten, I., McNab, R., Jones, S., Apperley, M., Bainbridge, D. and Cunningham, S. 1999. \nManaging complexity in a distributed digital library. IEEE Computer  42(2): 74 −79."
    },
    {
        "title": "Automatic Segmentation for Music Classification using Competitive Hidden Markov Models.",
        "author": [
            "Eloi Batlle",
            "Pedro Cano"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416764",
        "url": "https://doi.org/10.5281/zenodo.1416764",
        "ee": "https://zenodo.org/records/1416764/files/BatlleC00.pdf",
        "abstract": "Music information retrieval has become a major topic in the last few years and we can find a wide range of applications that use it. For this reason, audio databases start growing in size as more and more digital audio resources have become available. However, the usefulness of an audio database relies not only on its size but also on its organization and structure. Therefore, much effort must be spent in the labeling process whose complexity grows with database size and diversity.",
        "zenodo_id": 1416764,
        "dblp_key": "conf/ismir/BatlleC00",
        "keywords": [
            "Music information retrieval",
            "digital audio resources",
            "audio databases",
            "labeling process",
            "size and diversity",
            "effort spent",
            "complexity grows",
            "organizing and structure",
            "usefulness of an audio database",
            "growth in size"
        ],
        "content": "Automatic Segmentation for Music Classification using \nCompetitive Hidden Markov Models  \nAbstract  \nMusic information retrieval has become a major topic in the last few years and we can find a wide \nrange of applications that use it. For this reason, audio data bases start growing in size as more and \nmore digital audio resources have become available. However, the usefulness of an audio \ndatabase relies not only on its size but also on its organization and structure. Therefore, much \neffort must be spent in the lab eling process whose complexity grows with database size and \ndiversity.  \n \nIn this paper we introduce a new audio classification tool and we use its properties to develop an \nautomatic system to segment audio material in a fully unsupervised way. The audio seg ments \nobtained with this process are automatically labeled in a way that two segments with similar \npsychoacoustics properties get the same label. By doing so, the audio signal is automatically \nsegmented into a sequence of abstract acoustic events. This is specially useful to classify huge \nmultimedia databases where a human driven segmentation is not practicable. This automatic \nclassification allow a fast indexing and retrieval of audio fragments. This audio segmentation is \ndone using competitive hidden Mark ov models as the main classification engine and, thus, no \nprevious classified or hand -labeled data is needed. This powerful classification tool also has a \ngreat flexibility and offers the possibility to customize the matching criterion as well as the \navera ge segment length according to the application needs.  \n \nThe first stage in a classification system is the parameterization part where some features are \nobtained from the raw audio signal. The aim of this parameterization stage is to calculate a set of \nvalue s that represent the main characteristics of the audio samples. The nature of the \nparameterization is strongly dependent on the application since it will determine the set of \nabstract acoustic units and, therefore, the underlying structure of a certain sou nd. \nThe main classification engine in our system is based on Hidden Markov Models (HMM). HMMs \nhave proven to be a very powerful tool to statistically model a process that varies in time. The \nidea behind them is very simple. Consider a stochastic process fr om an unknown source and we \nonly have access to its output in time. HMMs are well suited to model this kind of events. From \nthis point of view, HMMs can be seen as a doubly embedded stochastic process with a process \nthat is not observable (hidden process) and can only be observed through another stochastic \nprocess (observable process) that produces the time set of observations. However, after \nexamining the audio segmentation problem, we can see that usual HMM training is completely \nuseless because we do not  have a priori information about the segmentation. In traditional HMM \nthe training procedure uses a labeled observation sequence and parameter estimation is based on \nlearning from examples. For example, in speech recognition, the training database is label ed with \nthe phonemes of the speech sequences. By doing so, HMMs learn from examples of each \nphoneme and they use this knowledge to identify similar patterns in the recognition stage. But the \nproblem we are dealing with in automatic music segmentation is ve ry different (and by far more \ndifficult). Our main goal is to blindly identify similar (in some sense) audio segments and \ntherefore we do not have any previous knowledge to train traditional HMMs. Thus, this is clearly \nan example of HMMs unsupervised train ing. Competitive Hidden Markov Models have proved to \nbe specially well suited for this kind of situations. CoHMM are different to HMM mainly in the \ntraining stage since the recognition stage (or classification) shares the common algorithm.  \nIn this paper we  also show some results that prove that coHMM converge to a realistic \nsegmentation architecture.   \nThe main architecture for all experimental systems used in this paper is the same, even though \nother parameterizations are under study. We use mel -cepstrum an alysis to obtain the feature \nvectors. Each feature vector is calculated from a 25 ms frame with an overlap of 10 ms. In order \nto take into account some temporal structure beyond two frames, the feature vector is extended \nwith the mel -cepstrum first and sec ond derivatives. Each derivative is calculated using two \nframes before and two frames after each frame. Since mel -cepstrum coefficients are invariant to \nenergy changes (coefficient 0 is dropped out) energy information is added to the feature vector \nwith it s first and second derivatives. Raw energy is not included to avoid dependence on the \nmusic level. The coHMM topology can be different from experiment to experiment since the \nclassification criteria can be also different. However, the topology that better matches the average \nexperiment is a 3 three states left -to-right model and one model for each event.  \n \nAs it is shown with experimental results, this system opens the possibility to tackle new \nclassification problems and, even though the experiments are rel ated to audio, some other \nsegmentation, clustering and classification problems can be solved.  \n \nEven though only audio experiments were presented, competitive Hidden Markov Models are a \nstatistical tool that can be used in a wide range of applications, not only in the classification field \nbut also in the recognition area because the segmentation labels can be used to identify parts of \nthe audio stream.  \nAuthor Information  \nEloi Batlle  \nAudiovisual Institute. Universitat Pompeu Fabra  \nRambla 31, 08002 Barcelona  \nCatalunya - Spain  \neloi@iua.upf.es  \nhttp://www.iua.upf.es  Pedro Cano  \nAudiovisual Institute. Universitat Pompeu Fabra  \nRambla 31, 08002 Barcelona  \nCatalunya - Spain  \npcano@iua.upf.es  \nhttp://www.iua.upf.es  \nSuggested Readings  \nBatlle, E. et al. 1998.  Feature Decorrelation Methods in Speech Recognition. A Comparative Study. \nInternational Conference on Spoken L anguage Processing .  Vol. 3,  pp. 951 -954. \nBaum, L.E. et al. 1970.  A Maximization Technique Occurring in The Statistical Analysis of Probabilistic \nFunctions of Markov Chains. The Annals of Mathematical Statistics . Vol. 41, n. 1, pp. 164 -171. \nDempster, A.P . et al. 1977.  Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of \nthe Royal Statistical Society . Vol. 39, n. 1, pp. 1 -38. \nPeeters, G. et al. 2000.  Instrument Sound Description in the Context of MPEG -7. International Computer \nMusic C onference . \nRabiner, L.R. 1989.  A Tutorial on Hidden Markov Models and Selected Applications in Speech \nRecognition. Proceedings of the IEEE . Vol. 77, no. 2, pp. 257 -286. \nRaphael, C. 1999.  Automatic Segmentation of Acoustic Musical Signals Using Hidden Mar kov Models. \nIEEE Transactions on Pattern Analysis and Machine Intelligence . Vol. 21, no. 4.  \nRuggero, M.A. 1992.  The Mammalian Auditory Pathway: Neurophysiology. chap. Physiology and Coding \nof Sound in the Auditory Nerve, Springer -Verlag . \nSerra, X. 1997.  Musical Sound Modeling with Sinusoids plus Noise. in G.D. Poli, A. Picialli, S.T. Pope \nand C. Roads, editors, Musical Signal Processing. Swets \\& Zeitlinger Publishers . \nTorres, L. et al. 1990.  Signal Processing V: Theories and Applications. Elsevier Scien ce Publishers B.V ."
    },
    {
        "title": "Techniques for Automatic Music Transcription.",
        "author": [
            "Juan Pablo Bello",
            "Giuliano Monti",
            "Mark B. Sandler"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1414872",
        "url": "https://doi.org/10.5281/zenodo.1414872",
        "ee": "https://zenodo.org/records/1414872/files/BelloMS00.pdf",
        "abstract": "Two systems are reviewed than perform automatic music transcription. The first perform monophonic transcription using an autocorrelation pitch tracker. The algorithm takes advantage of some heuristic parameters related to the similarity between image and sound in the collector. The detection is correct between notes B1 to E6 and further timbre analysis will provide the necessary parameters to reproduce a similar copy of the original sound. The second system is able to analyse simple polyphonic tracks. It is composed of a blackboard system, receiving its input from a segmentation routine in the form of an averaged STFT matrix. The blackboard contents hypotheses database, an scheduler and knowledge sources, one of which is a neural network chord recogniser with the ability to reconfigure the operation of the system, allowing it to output more than one note hypothesis at the time. Some examples are provided to illustrate the performance and the weaknesses of the current implementation. Next steps for further development are defined.",
        "zenodo_id": 1414872,
        "dblp_key": "conf/ismir/BelloMS00",
        "keywords": [
            "Automatic music transcription",
            "Monophonic pitch tracking",
            "Similarity between image and sound",
            "Timbre analysis",
            "Polyphonic track analysis",
            "Blackboard system",
            "Segmentation routine",
            "Averaged STFT matrix",
            "Hypotheses database",
            "Scheduler"
        ],
        "content": "1TECHNIQUES FOR AUTOM ATIC MUSIC TRANSCRIP TION \nJuan Pablo Bello, Giuliano Monti and Mark Sandler  \nDepartment of Electronic Engineering, King’s College London,  \nStrand, London WC2R 2LS, UK  \njuan.bello_correa@kcl .ac.uk , giuliano.monti@kcl.ac.uk  \n \nAbstract  \n \nTwo systems are reviewed than perform automatic music \ntranscription. The first perform monophonic transcription using \nan autocorrelation pitch tracker. The algorithm take s advantage \nof some heuristic parameters related to the similarity between \nimage and sound in the collector. The detection is correct \nbetween notes B1 to E6 and further timbre analysis will provide \nthe necessary parameters to reproduce a similar copy of th e \noriginal sound. The second system is able to analyse simple \npolyphonic tracks. It is composed of a blackboard system, \nreceiving its input from a segmentation routine in the form of \nan averaged STFT matrix. The blackboard contents an \nhypotheses database, an scheduler and knowledge sources, one \nof which is a neural network chord recogniser with the ability \nto reconfigure the operation of the system, allowing it to output \nmore than one note hypothesis at the time. Some examples are \nprovided to illustrate the  performance and the weaknesses of \nthe current implementation. Next steps for further development \nare defined.  \n \n1. Introduction  \n \nMusical transcription of audio data is the process of taking a \nsequence of digital data corresponding to the sound waveform \nand extracting from it the symbolic information related to the \nhigh-level musical structures that might be seen on a score [1]. \nIn a very simplistic way, all the sounds employed in the music \nto be analysed may be de scribed by four physical parameters, \nwhich have corresponding physiological correlates [2]: \n \n1. Repetition rate or fundamental frequency of the \nsound wave, correlating with pitch.  \n2. Sound wave amplitude, correlating with loudness.  \n3. Sound wave shape, correlating with timbre.  \n4. Sound source location with respect to the listener, \ncorrelating with the listener’s spatial perception.  \n \nThe latter is not considered determinant for music transcription, \nand will be discarded for t his investigation. The other three \ngenerate the difference between the parts that can be defined in \na musical track [3]: the orchestra and the score. The orchestra is \nthe sound of the instrument itself, the specific characteris tics of \nthe instruments (timbre, envelope), which make it sound \nunique; the score consists of the general control parameters \n(pitch, onsets, etc), which define the music played by the \ninstrument. In an academic music representation, just the latter \ncan be described, i.e. which notes to play and when to play \nthem. The purpose of the present work is to automatically \nextract score “features” from monophonic and simple \npolyphonic music tracks, using an autocorrelation pitch tracker and a computational reasoning  model called blackboard system \n[4][5] and combining top -down (prediction -driven) processing \nwith the bottom -up (data -driven) techniques already \nimplemented in [6]. As the analysis of multitimbrical musical \npieces and the extraction of expression parameters are not in \nthe scope of the present work, just the parameters related with \npitch and loudness will be considered.  \n \n2. Monophonic Transcription with aut ocorrelation  \n \nIf the fundamental frequency of a harmonic signal is calculated, \nand the resulting track is visualised, it can be noticed that, for \nmost of the duration of the notes, the pitch maintains \napproximately constant. This relation, so clear to the eyes, \nrequires some comments. In order to implement some grouping \ncriteria and rules for sounds, emphasis should be given to the \nsimilarity in human perception between image and sound [7]. \nImportant clues can be obtained by obs erving carefully the plot \nof the pitch track. The current system doesn’t use a \nconventional (energy based) onset detector, instead, it \nimplements a pitch based onset detector, which is more robust \nwith slight note changes (glissando, legato).  \nMonophonic mu sic means that the performer is playing one \nnote at a time. More than one instrument can be played, but \ntheir sounds must not overlap. This is a big limitation for the \namount of input sounds that can be processed, however, it leads \nto fast and reliable res ults. Many commercial software tools are \nprovided on Internet to help musicians in the difficult task that \nis transcription. Few of them dare to perform polyphonic \ntranscription, but often the results are completely wrong.  \nWhich information is needed?  \nThe score is a sequence of note -events. Many music languages \nhave been developed until now and a new standard is arising \nunder the MPEG group [3]. The MIDI protocol [8] has been \nwidely accepted and uti lized by musicians and composers since \nits conception in 1982. It represents the most common example \nof a score file.  \nIn order to define a note -event, three parameters are essential:  \n \n• Pitch \n• Onset \n• Duration  \n \nEvery instrument is characterized by its own timbre, but the \nsounds created by different instruments playing the same note, \nwill have the same pitch. Therefore, determining the pitch is \nequivalent to knowing which note has been played.  \nThe onset time and the duration have also to be extracted in \norder to recreate the original melody.   \n  22.1. Autocorrelation Pitch Tracking  \n \nIn order to estimate the pitch in the musical signal, \nautocorrelation pitch tracking has been chosen, showing good \ndetection and smooth values during the steady part of a note. \nThe stead y part of a note is just after the attack, where all the \nharmonics become stable and clearly marked in the spectrum.  \nThe Autocorrelation function  \nAn estimate of the Autocorrelation of an N -length sequence \nx(k) is given by:  \n)()(1)(1\n0nkxkx\nNnrnN\nKxx+⋅ =∑−−\n=                   (1)  \nWhere n is the lag, or the period length, and x (n) is a time \ndomain signal. This function is particularly useful in \nidentifying hidden periodicities in a signal, for instance, in the \nweak fundamental situation. Peaks in the autocorrelation \nfunction correspond to the lags where periodicity is stronger. \nThe zero lag autocorrelation )0(xxr is the energy of the \nsignal. The autocorrelation function shows peaks for any \nperiodicity present in the signal, therefore it is necessary to \ndiscard the maximum relative to the multiple periodicities. If \nthe signal has high autocorrelation for a lag value, say K, it will \nhave maximum for n*K as well, where n is a positive integer. \nConsequently, the first peak in the autocorrelation function, \nafter the zero lag value, is considered as the inverse of the \nfundamental frequency, while the other peak values are \ndiscarded. The implementation takes advantage of some \nalgorithms implemented by Malcolm Slaney in the ‘Auditory \ntoolbox’ [9], a Matlab toolbox, freely available, implementing \nauditory models and functions to calculate the correlation \ncoefficients.  \nWhy autocorrelation ?  \nAutocorrelation is simple, fast and reliable. The equation (1) \nrepresents a very simple relation b etween the time waveform \nand the periodicities of the signal expressed by the \nautocorrelation coefficients.  \nThe calculation of the autocorrelation is computed through the \nFFT, which has a computational complexity of N ⋅log(N), where \nN is the length of the w indowed signal. The calculation \nprocess, therefore, it is very fast. The simulations performed \nconfirm the reliability of this method. In 1990, Brown \npublished results of a study where the pitch of instrumental \nsounds was determined using autocorrelation [10]; she \nsuggested this method to be a good frequency tracker for \nmusical sounds.  \n \n2.2. Transcription  \n \nThe transcription task is the translation from music to score. In \nthe score all the notes played are listed in a time seque nce, \nindicating the starting times, the durations and the pitches. The \nscheme of the monophonic transcription system implemented \nhere, is illustrated in figure 1.  \nThe outputs of the blocks are explained in the next figures. The \nPitch Tracker is based on th e autocorrelation method described \nin section 2.1. Its output is the instantaneous pitch of the signal.  \nBeside the pitch tracker, a block calculates the envelope of the \nsignal. This information goes to the pitch tracker, in order to \nskip the calculation of  the pitch when the energy of the signal falls below the audibility threshold. This procedure avoids \nineffective elaborations.  \n \n \nFigure 1. Scheme of the transcription system.  \n \nFigure 2 portrays the output of the pitch tracker. The  pitch is set \nto 0 in the silence parts.  \n \n Figure 2. Pitch from autocorrelation  \n \nThe conversion of the pitch (Hertz), to key number is the result \nof a rounding up to the nearest musical frequency. Unless the \npitch, the key numbers  keep the same value during the steady \npart of a note. The relation is given as follow [11]: \n \n\n\n\n×+=)2log()440/log(1249fkn                   (2)  \n \nWhere the [] operator calculates the nearest integer value. \n(Defined as piano keys from A 1 = 1, to C 9 = 88, with A 5 = 49 \nequivalent to the A at 440 Hz).  \n \n Figure 3. Pitch2MIDI conversion  \n \nIf we consider a violin vibrato, in the rounding up process all \nthe information regarding the frequency mod ulation are lost. \nHowever, the absence of frequency modulation in the \nsynthesized sound has little effect on the perceptual response to  3violin vibrato, while the absence of amplitude modulation \ncauses marked changes in sound quality [12]. Moreover, an \nalgorithm can extract the vibrato information from the signal \nenvelope, after the sound has been segmented note by note \n[13]. \nThe collector  extracts the score, considering the pre -elaborated \ntrack and t he signal amplitude, sorting: onsets, pitch and \noffsets. \nIf the short -time autocorrelation is calculated on a monophonic \nmusic signal and the results are plotted, the pitch information is \nalmost constant during the steady state parts of the notes. The \nattack part of a note is usually noisy; therefore, the pitch can \noscillate in a wide range of frequency before stabilizing. The \ntransient part can last a few tenths of msec and varies \ndepending on the instrument family [14]. In the attack part of \nthe signal, the pitch tracker cannot provide useful information \nfor the transcription system.  \nThe collector recognises when the pitch maintains the same \nvalue, and proposes a note onset in the first value of the \nconstant sequence. The onset  is confirmed when the pitch lasts \nfor the minimum note duration accepted. When a note is \nrecognized, the system is able to write in the score file: the \nonset and the pitch of the note.  \nThe minimum note duration  is the main parameter in the \ncollector. By m odifying its value, the system adapts to the \nspeed of the music, improving the performance of the \ntranscription. If the minimum note duration is set for instance \nto 40 msec, all the pitch sequences, with constant values, \nlasting less than 40 msec are disca rded. Hence, errors \nconcerning spurious notes are eliminated.  \nThe minimum duration parameter controls also the memory of \nthe system: when a note is detected, the pitch can vary inside \nthe 40 msec window before having again the same value, to be \nconsidered  part of the same note. This is very similar to the \nconsideration taken in sound restoration [15]: the human brain \ntakes information from the cochlea, and interprets them with \nthe knowledge of the previous samples; this behavio ur is called \nstreaming or integration process in psychoacoustics [7].  \nThe termination of a note is determined by the start of a new \nnote or by the recognition of silence. After an onset, the offset \ndetector checks if the signa l energy falls below the audibility \nthreshold. The duration of the note in the score is calculated by \nthe difference between its onset and the next onset/offset.  \nDuring the decaying part of a note, the pitch can slightly \nchange. The collector allows the p itch to have different values, \nuntil a new note is predicted. However, if the conditions for a \nnew note aren’t met, the system keeps the last note.  \n \n2.3. Results  \n \nThe number of lags considered in the autocorrelation \ndetermines the pitch range of the transc ription system.  The \nfollowing table gives an idea of the relation between the \nautocorrelation coefficients considered and the pitch range \ncovered (notes).  \nNo. coeff.  From to \n256 E2 C6 \n512 B1 E6 \n \nTable 1. Relation between the num ber of autocorrelation coefficients \nand pitch range in the transcription system.   \nThe configuration with 512 coefficients was chosen in the \ntranscription. The wider pitch range covered was preferred  to \nthe faster computational time with 256 coefficients.  \nTo verify that the pitch has been correctly tracked and the \nmelody of the original file has not been modified, the system \nwrites a Csound [16] score file. By providing an orchestra file, \nthe score can be converted into wav form at. The orchestra file \ncontains the description of the instrument. Hence, from the \nsame score, the same melody can be re -synthesised with \ndifferent instruments specifying different orchestra files.  \nThe test samples were obtained from a CD collection of br ass \ninstruments riffs. Comparative listening between the \nsynthesised score and the original riffs, reveal the transparency \nof the transcription. By transparency, I mean that the tempo and \nthe pitch are correctly extracted.  \nAs shown in Figure 2, the matlab script also plots the \nsegmentation of the signal (top); the black circles indicate \nonsets, the red circles indicate offsets. Then, the bottom figure \nportrays the midi notes of the score file in a “piano roll” form.  \n \n Figure 4. Original music (top) and score file (below).  \n \nIt was interesting to compare this system with a commercial \nprogram downloaded from Internet, performing WAV2MIDI \n[17]. Even if no specification about the transcription system \nwas give n, the two systems seem to work in a very similar way. \nThe minimum note duration can be modified in both the \nsystem. Finally, the simulations results are both fairly \nsuccessful.  \n \n2.4. Conclusions  \n \nThis part of the paper has reviewed a traditional method of  \nperforming pitch tracking, widely used in speech processing \nand has demonstrated to be also good for musical instruments. \nFurthermore, the implementation of a successful monophonic \ntranscription system has been illustrated.  \nThe transcription system descri bed doesn’t have an onset \ndetector based on the signal waveform. The onset is recognised \nonly at the beginning of the steady state part of the signal. As a \nresult the onset time precision can fail of a few tens of msec. \nThe great advantage of this approach , is that in glissando or \nlegato passages, the onset is easily detected. This is because the \nnew note is recognised analysing the pitch, instead of looking \nat the energy of the signal, which is usually ambiguous.   4The pitch and time of notes are the main fe atures in \ntranscription. However, other features like amplitude envelope, \ntimbre and vibrato are important to synthesize a close copy of \nthe original sound. The spectral analysis and the signal \nenvelope will be investigated in order to extract those \nparameters. Furthermore, in order to detect very low pitched \nnotes in the range of 30 -100 Hz, the pitch tracker has to be \nmodified to provide high frequency resolution renouncing to \nthe fast calculation of the autocorrelation function through the \nfft.  \n \n3. Simpl e Polyphonic Transcription: Blackboard System \nand neural network chord recogniser  \n \nThe blackboard system is a relatively complex problem -solving \nmodel prescribing the organisation of knowledge and data, and \nthe problem -solving behaviour within the overall organisation \n[5]. It receives its name from the metaphor of a group of \nexperts trying to solve a problem plotted on a blackboard, each \nacting only when his specific area of expertise is required in the \nproblem.  \nIn opposition to  the usual paradigm of signal processing \nalgorithms, where algorithms are described by data flowcharts \nshowing the progress of information along chains of modules \n[18], the architecture of the blackboard system is opportunistic, \nchoosing the specific module needed for the development of \nthe solution at each time step. Due to its open architecture \ndifferent knowledge can be easily integrated into the system, \nallowing the utilisation of various areas of expertise. The basic \nstructure of a blackboard system consist of three fundamental \nparts: the blackboard : global database where the hypotheses are \nproposed and developed, open to the interaction with all the \nmodules present in the system; the scheduler or opportunist ic \ncontrol system:  determines how the hypotheses are developed \nand by who; and the knowledge sources or “experts” of the \nsystem: modules that execute the actions intended to develop \nthe hypotheses present in the blackboard.  \nThe system operates in time step s, executing one action at the \ntime. The scheduler, prioritise within the existing list of \nknowledge sources, determining the order in which these \nactions are executed. Each knowledge source consists of a sort \nof  “if/then” (precondition/action) pair. When  the precondition \nof a certain knowledge source is satisfied, the action described \nin its programming body is executed, placing its output in the \nblackboard. These knowledge sources can perform different \nkinds of activities, such as detecting and removing unsupported \nhypothesis from the blackboard or stimulating the search for \nharmonics of a given note hypothesis.  \nTo achieve the transcription of a sound file the system can \nperform tasks such as:  \n \n1. The extraction of numeric parameters content in the \noriginal audio data -file, through the analysis of the \noutput generated for signal processing methods such \nas the Short Time Fourier Transform (STFT), the \nMultiresolution Fourier Transform (MFT) [19] or the \nlog-lag correl ogram [18][20]. \n2. Elimination of non -audible or “irrelevant” data for \nthe analysis performed, based on perceptual models \nof the ear and the brain. This helps the effici ency of the system, avoiding unnecessary computations and \nthe generation of “impossible” hypotheses.  \n3. The use of musical knowledge to discern the presence \nof patterns or forms in the musical composition being \nanalysed.  \n4. The use of “experience” for the recogn ition of \nmusical structures in the audio file.  \n \nThere are several implementations of blackboard systems in \nautomatic music transcription [4][20][21], however part of the \nknowledge a human being use to transcribe music is based on \nhis/her experience hearing music files and the inherent \nstructures present on these, and in those systems this \nknowledge is ignored. As [18] specify, the structure of the \nblackboard makes little distinction between explanatory and \npredictive operations; hypotheses generated for modules of \ninference can reconfigure the operation of the system and bias \nthe search within the solution space.  \n \n3.1 Top -Down and Bottom -up Processing  \n \nIn bottom -up processing, the information flows from the low -\nlevel stage, that of the analysis of the raw signal, to the highest \nlevel representation in the system, in our case that of  the note \nhypotheses. In this technique, the system does not know \nanything about the object of the analysis previous to the \noperation, and the result depends on the evolution of the data in \nits unidirectional flow through the hierarchy of the processor. \nThis approach is also called data -driven processing. In \ncontraposition, the approach when the different levels of the \nsystem are determined by predictive models of the analysed \nobject or by previous knowledge of the nature of the data is \nknown as top -down or  prediction -driven processing [22].  \nDespite of the fact that top -down processing is believed to take \nplace in human perception, most of the systems implemented \nuntil now are based on bottom -up processing, and j ust in the \nlast years the implementation of predictive processing to \nrecreate these perceptual tasks had become a common choice \nbetween researchers of this field [1][18][22][23]. The main \nreason for the implementation of top -down processing is the \nlack of abilities in the bottom -up systems to model important \nprocesses of the human  perception; also in tasks such as the \nautomatic transcription of music the “inflexibility” of these \nmodels make them unable to achieve results in a general \ncontext, in this particular case different types of sounds and \nstyles of music.  \nIn this work, the top-down processing is achieved through the \nimplementation of a connectionist system. This kind of systems \nconsists of many primitive cells (units), which are working in \nparallel and are connected via directed links. Through these \nlinks, activation pattern s are distributed imitating the basic \nmechanism of the human brain, reason why these models are \nalso called neural networks [24]. Knowledge is usually \ndistributed throughout the net and stored in the structure o f the \ntopology and the weights of the links; the networks are \norganized by automatic training methods, which help the \ndevelopment of specific applications. If adequately trained, \nthese networks can acquire the experience to make decisions in \nvery specific problems presented. As extensive documentation \nof neural networks is available, no further explanation of this  5topic will be developed here, just the basics of the implemented \nsystem are explained in section 3.2.3.  \n \n3.2. Implementation  \n \n3.2.1 Segmentation  \n \nAs is not the focus of this paper, just a brief explanation of the \nsystem’s front end is proportioned here. The onset detection \naims to evaluate the time instants when a new note is played in \na sound file. Whilst analysing the running spectrum of the \nsound it is possible to notice that when a new event occurs, the \nhigh frequency content is relevant. This property is exploited \nfrom the High Frequency Content method [25][26]. The \nmeasure of the high frequency content is given by:  \n \n( )∑+\n=• =1)2/(\n22\n)(N\nkkkX HFC                    (3)  \n \nWhere N is the FFT array length ( N/2 + 1  corresponds to the \nfrequency Fs/2, Fs  = sample rate) and X (k) is the kth bin of the \nFFT. The power spect rum is weighted linearly emphasizing the \nhigh frequencies in the frame. The Energy function E is the \nsum of the power spectra of the signal in the specified range:  \n \n( )∑+\n==1)2/(\n22\n)(N\nkkX E                                    (4)  \n \nIn both equations the first bin is discarded to avoid unwanted \nDC bias. These equations are calculated on each frame and \nused to build the detection function:  \n \nrr\nrr\nr\nEHFC\nHFCHFCDF ∗ =\n−1                                  (5)  \n \n \n Figure 5: The original signal (a tenor sax riff) and the de tected onsets \nand offsets (crosses) of this signal, the HFC and the Detection function.  \n \nAs can be seen in figure 5, this function shows sharp peaks in \nthe instant where the transient occurs. A criteria based on the slope of these peaks was used to determi ne the onset’s time. \nAfter this process, the segmentation is performed averaging the \nsignal’s STFT between onsets. This is used as the input of the \nblackboard system.  \n \n3.2.2 Blackboard Implementation  \n \nThe Blackboard system’s architecture implemented is bas ed on \nthat of Martin’s implementation [4] and is shown in figure 6.  \nAt the lower level, the system receives the averaged STFT of \nthe signal and identifies the peaks of the spectrum. Of this \ngroup just the peaks  higher than an amplitude threshold are \nconsider to build a Tracks matrix, containing the magnitude and \nfrequency of each. This information is feed to the database and \nexposed to the evaluation of the knowledge’s sources (KS) to \nproduce new hypotheses.  \nThere are three different levels of information present on the \ndatabase: tracks, partials and notes. The tracks information is \nautomatically provided at the beginning of the system \noperation, however the notes and partials  information are the \nproduct of the k nowledge’s sources interaction with the \ndatabase. It is the main task of the Scheduler to determine the \nneed for a specific kind of information and to activate the \ncorresponding knowledge source. In the present system a table \nof preconditions is evaluated at each time step and a rating is \ngiven to each knowledge source determining the order in which \nthese will operate.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAt tracks level, all the remaining peaks of the STFT have an \nequal chance of becoming notes, but as the operation  of the \nsystem goes forward and new hypotheses are produced and \nevaluated by the KS, ratings are given to narrow the search for \nmusical notes in the spectrum.  \nIn the case of the partials , the rating is based on the magnitude \nof the nearest peak (within a s pecific range) to the ideal \nfrequency of the hypothesis. For notes, rating is based on the \npresence and magnitude of peaks corresponding to the ideal  \nNotes \n \nPartials \n \nTracks Music representation  \nFFT Spectrum  Knowledge  \nSource \nKnowledge  \nSource \nKnowledge  \nSource  \nScheduler  \nFigure 6: The control structure and the data hierarchy of \nthe blackboard system   6partials this note should have [27]. All this information is \nstored in a matrix called “Hypotheses”.  \n \n3.2.3 Neural Network Implementation  \n  \nIn the neural network implemented, the information flows in \none way from input to output. There is no feedback, which \nmeans that the output of any layer does not affect that sam e \nlayer. This type of network is known as feed -forward.  \nThe structure of this implementation consists of three layers: an \ninput, an output and a hidden layer. The activation function \nimplemented for all the neurons is the sigmoid transfer \nfunction. The le arning is supervised. Training a feed -forward \nneural network with supervised learning consists of the \nfollowing procedure [24]: \n \n1. An input pattern is presented to the network. The \ninput is then propagated forward  in the net until \nactivation reaches the output layer. This is called the \nforward propagation phase.  \n2. The output of the output layer is then compared with \nthe teaching input. The error, i.e. the difference dj \nbetween the output oj and the teaching input tj of a \ntarget output unit j, is then used together with the \noutput oi of the source unit i to compute the necessary \nchanges of the link wij. To compute the deltas of \ninner units for which no input is available, (units of \nhidden layers) the deltas of the foll owing layer, \nwhich are already computed, are used in a formula \ngiven below. In this way the errors (deltas) are \npropagated backward, so this phase is called \nbackward propagation.  \n3. In this implementation offline learning is used, which \nmeans that the weights  changes Δωij are cumulated \nfor all patterns in the training file and the sum of all \nchanges is applied after one full cycle (epoch) \nthrough the training pattern file. This is also known \nas batch learning.   \n \n0 500 1000 1500 2000 2500 300010-5100105 Performance is 1.36477e-005, Goal is 0\n3000 EpochsTraining-Blue\n Figure 7: The learning performance of the neur al network \nimplemented.  \n \nHere, the input pattern consists of a 256 points spectrogram of a \npiano signal’s segment (either a note or a chord), part of the \nbatch of samples covering three octaves of the instrument. The \ntarget output is just represented for t he absence “0” or presence “1” of a chord in the sample. The weight changes were \ncalculated using the backpropagation weight update rule, also \ncalled generalized delta -rule, which reads as follows [24]: \n \nij ijohd w=Δ                                     (6)  \nwhere, \n) )(( j jj j jtnetf o d − ′=                                   (7) \nif unit j is an output unit or  \n∑′=kjkk j j jnetf wd d )(                                   (8) \nif unit j is a hidden unit    \n \nwhere: \nη learning factor eta (a constant)  \nδj error (difference between the real output and the \nteaching input) of unit j \ntj     teaching input of unit j \noi    output of the preceding unit i \ni      index of a predecessor to the current unit j with link \nwij from i to j \nj     index of the current unit  \nk index of a successor to the current unit j with link wjk \nfrom j to k \n \nThe learning performance of the network is shown in figure 7, \nwhere the value of the error through the cycles can be seen.  \n \n3.2.4 Neural Network Inter action with the Blackboard  \n \nThe network is trained off the process of automatic \ntranscription until it obtains a set of parameters adequate to the \ntask required, in this case the recognition of the presence of a \nchord in a spectrogram. When the overall sy stem is running, the \nnetwork receives as an input the same STFT data the \nblackboard system analyses. In the original blackboard’s \nprocess, just the note hypotheses with rating bigger than a cut -\noff threshold remained as valid hypotheses [5], in this version \nof the system, the output of the neural network change the \nperformance of the system allowing more than one note \nhypothesis to survive if necessary. This process reshapes the \nHypotheses  matrix and the routines that manipulate  it, allowing \nthe handling of a chord as a possible output of the system. In \nthis first approach, just chords of two or three notes can be \nidentified by the system.  \nAfter the selection of hypotheses is made, each of the \nfrequencies obtained is rounded towa rds the nearest ‘musical’ \nfrequency using equation (2) given in section 2.2. The key \nnumber obtained is rounded towards the nearest integer and \nintroduced in equation (9) [11], where fnote is the nearest \n‘musica l’ frequency:  \n \n()12/492440−×=kn\nnotef                                    (9)  \n \nThe output is given in two different ways: a graphical \nrepresentation and a score file in CSOUND™ language [28]. \nThe graphical repres entation is in the form of a ‘piano roll’, \nwhich is a common way of representing musical events in most \nMIDI sequencers. The score file, is a text file written in \nCSOUND™ protocol, which can be compiled and rendered  7with an Orchestra file (a sine wave soun d for these \nexperiments), obtaining an audio representation of the original \nsound. \n \n3.3 Examples  \n \nThree examples are shown here to illustrate how the system is \nworking and to define the next steps to follow. In the first \nexample, illustrated in figure 8, a  piano riff is plotted, \nconsisting on a succession of four notes (C 5 D5 E5 F5) followed \nby a C major chord (C 5 E5 G5). The notes and the chords are \nrecognized successfully by the system, which plot the output \naccording to the key number of each note. This example is \nintended just to show the main capabilities of the current \nsystem, notice that the notes and silences are well differentiated \nand the network identified the presence of a chord related with \nthe last onset, causing the blackboard to output the th ree higher \nrated hypotheses of the segment.  \n \nFigure 8: Example of automatic transcription of a piano riff.  \n \nFigure 9: System’s output of a piano riff with error in the chord \ntranscription.  \n \nIn figure 9 a note sequence (D 6 A5 E5) is represented followed \nby a D major chord (D 5 F#5 A5). It is plotted here because an \nerror is performed in the recognition of the chord showing one \nof the weaknesses of the current implementation. This error is \ndue to the presence of a high rated hypothesis for the A 4 note, \nproduct of the strong harmonics of the A 5 in the D major chord. \nThese make the hypothesis of the note A 4 better rated than the \nD5 missing note. As this problem became repetitive in some of \nthe experiments, an octave -error detection routine was implemented and p laced after the output of the blackboard. In \nthis example the error detection routine discarded the note A 4 in \nfavour of its higher octave equivalent, which was already \ndetected by the blackboard, leaving empty the output slot \ncorresponding to the D 5 note of the chord. This extra routine \ndisables the system’s handling of octave intervals.  \nThe last example shown in figure 10 represents a four measures \nsection of a piano song, including several chords. For this \nexample the octave error detector was disabled,  to avoid \nrestrictions in the kind of intervals the system can manage. Due \nto this, several mistakes are made in the transcription of the \nnotes of three chords (the first three of the figure), where \ncorrect note hypotheses were discarded by the system in f avour \nof their lower octave equivalents. In the plot can be noticed the \npresence of notes between the key numbers 18 and 29, when \njust notes of the key number 30 or higher were performed. The \nsame error was detected in the note before the last chord, where  \nthe note C 6 was selected over the correct C 5. Another error in \nthe transcription is related to the no detection of an onset in the \nseventh second of the song causing a wrong segmentation of \nthe piece. The spectrogram of this segment was identified as a \nchord for the neural network, probably due to the presence of \ntwo strong fundamental pitches in the time window averaged. \nAs can be seen in the figure 6 an inexistent chord was plotted \nbetween the times of 6.7164 and 7.3839 seconds, containing \nboth the origi nal notes played in that segment. The other twelve \nnotes of the piece and the last chord were correctly identified \nby the system.  \n \nFigure 10: Example of automatic transcription of a simple polyphonic \npiano song of four measures.  \n \n3.4. Conclusions and next  steps \n \nThe simple polyphonic system is achieving the automatic \nextraction of score parameters from simple polyphonic piano \nmusic, performed between the C4 and B6 notes, with up to \nthree notes played at the same time and without the octave \ninterval include d. This is less general than the purpose defined \non the introduction showing the necessity of some changes in \nthe current system  \nFirst, to manage the octave detection problem, new knowledge \nsources should be added to the blackboard architecture based \non the same principle implemented for the octave -error \ndetection routine, but with the possibility of allowing the \npresence of an octave interval when it is truly present in the  8input. To achieve that, more musical knowledge is necessary in \nthe system.  \nThe arc hitecture of the blackboard will be modified, \nincorporating dynamic structures to handle different sized \nhypotheses, in this case, chords of more than three notes. Also, \nthe training space of the network has to be expanded, \ncontemplating the recognition of  bigger chords and extending \nto all the octaves of the piano. As is showed in [6] the system is \nable to manage monophonic riffs of woodwinds and brass \ninstruments, however, to define the next steps towards the \nhandling of diffe rent instruments, more extensive testing has to \nbe performed.  \nAs a first approach, the results depicted here are very \nencouraging showing that further development of these ideas \ncould be the way for more robust and general results.  \n \n4. References  \n \n[1] Eric Sc heirer. “Extracting expressive performance \ninformation from recorded music”. Master’s thesis, MIT, \n1995. \n[2] R.F Moore. “Elements of Computer Music”. Prentice Hall, \nEnglewood Cliffs, New Jersey, 1990.  \n[3] Eric D. Scheirer, “ The MPEG -4 Structured Audio \nStandard ”, IEEE ICASSP Proc., 1998.  \n[4] Keith Martin. “A Blackboard system for Automatic \nTranscription of Simple polyphonic Music”. MIT Media \nLab, Technical Report # 385, 1995.  \n[5] R.S Engelmore and A.J. Morgan. “Blackboard Systems”. \nAddison -Wesley publishing, 1988.  \n[6] Bello J.P ., Monti and Sandler,” An implementation of \nautomatic transcription of monophonic music with a \nBlackboard system ”, Proc. of the ISSC, June 2000.  \n[7] Bregman A., “ Auditory Scene Analysis ”, MIT Press, 1990.  \n[8] MIDI Manufacturers Association. “ The Complete MIDI \n1.0 Detailed Specification ”, 1996.  \n[9] Slaney M. “ Auditory Toolbox for Matlab ” available at \nURL http://www.interval.com/papers/1998 -010/ \n[10] Brown , ” Musical frequency tracking using the methods of \nConventional and Narrowed Autocorrelation ” J.A.S.A. , \n1991. \n[11] James H. M cClellan, Ronald Schafer and Mark Yoder. \n“DSP First: A Multimedia Approach”. Prentice Hall, \nUSA. 1998  \n[12] Wakefield G.H., “ Time-frequency characteristic of violin \nvibrato: modal distibution analysis and synthesis ”, JASA, \nJan-Feb 2000.  \n[13] Bendor D, Sandler M., “ Time domain extraction of \nVibrato from monophonic instruments ”, to be published in \nMusic IR 2000 Conference, October 2000.  \n[14] Martin K., ” Sound-Source recognition ”, PhD thesis, MIT, \nftp://sound.media.mit.edu/pub/Papers/kdm -phdthesis.pdf, \n1999. \n[15] Ellis D, “ Hierarc hic models of sound for separation and \nrestoration ”,Proc. IEEE Mohonk Workshop, 1993.  \n[16] Csound web page URL: http://music.dartmouth.edu/  \n        /∼dupras  /wCsound/ csoundpage.html.  \n[17] WAV2MIDI, URL: http://www.audiowork.com.  \n[18] Daniel Ellis. “Prediction -driven co mputational auditory \nscene analysis”. PhD Thesis, MIT, June 1996.  [19] E.R.S Pearson. “ The Multiresolution Fourier Transform \nand its Application to the Analysis of Polyphonic Music”. \nPhD Thesis, Warwick University, 1991.  \n[20] Keith Martin. “Automatic Transcription of Simple \npolyphonic Music: Robust Front End Processing”. MIT \nMedia Lab, Technical Report # 399, December 1996.  \n[21] Daniel Ellis. “Mid -level Representation for computational \nauditory scene analysis”. In Proc. Of the Computational \nAuditory Scene Analysis Worksh op; 1995 International \nJoint Conference on Artificial intelligence, Montreal, \nCanada, August 1995.  \n[22] Anssi Klapuri. “Automatic Transcription of Music”. MSc \nThesis, Tampere University of Technology, 1998.  \n[23] Malcolm Slaney. “A critique of pure audition”. In Proc . Of \nthe Computational Auditory Scene Analysis Workshop, \nMontreal, Canada, August 1995.  \n[24] Stuttgart Neural Network Simulator. User Manual, version \n4.1. University of Stuttgart, Institute for Parallel and \nDistributed High Performance Systems. Report No. 6/95.  \n[25] Tristan Jehan. “Music Signal Parameter Estimation”. \nCNMAT Berkeley, USA. 1997.  \n[26] P. Masri and A. Bateman. “Improved Modelling of Attack \nTransient in Music Analysis -Resynthesis”. University of \nBristol. 1996.  \n[27] Randall Davis, Bruce Buchanan, and Edward Shortlif fe. \n“Production Rules as a representation for a Knowledge -\nBased Consultation Program”. Artificial Intelligence, \n8:15-45, 1977.  \n[28] Barry Vercoe. “CSOUND A Manual for the Audio \nProcessing System and Supporting Programs with \nTutorials”. Media Lab, M.I.T, Massach usetts, USA. 1992"
    },
    {
        "title": "Time Domain Extraction of Vibrato from Monophonic Instruments.",
        "author": [
            "Daniel Bendor",
            "Mark B. Sandler"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416810",
        "url": "https://doi.org/10.5281/zenodo.1416810",
        "ee": "https://zenodo.org/records/1416810/files/BendorS00.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416810,
        "dblp_key": "conf/ismir/BendorS00",
        "content": "1Time Domain Extraction of Vibrato from Monophonic Instruments\nIntroduction\nVibrato is an essential ingredient in the expressive nature of many musical instruments. Creating\nslight oscillations in the pitch and/or volume of the musical tone, vibrato allows a long sustained note\nto become more lively and dynamic. Musical instruments differ in both the technique used to create\nvibrato as well as the physical characteristics of the vibrato-enhanced sound produced. The goal of\nthis research is to extract information describing the amplitude, frequency, and phase of the vibrato\nfrom a section of monophonic music.\nProcedure\nThe steady state of a musical note can be approximated as follows:\n                           ¥\nG(t)=A(t)* S [H n*cos(n*(f 0+F(t))+qn)]\n                          n=1\nG(t)- Musical signal\nn- harmonic number (1 (fundamental), 2, 3, etc.)H\nn- amplitude of harmonic\nqn- phase of harmonic\nf0- fundamental frequency\nA(t)- amplitude modulation signalF(t)- frequency modulation signal\nThe amplitude and frequency modulation signals, A(t) and F(t), can be extracted using envelope and\npitch extraction techniques respectively. In non-vibrato signals, A(t)=1 and F(t)=0, however with theonset of vibrato they both take on the form A\nv*cos(f v*t+qv). The amplitude (A V), frequency (f V),\nand phase ( qV), of each these signals can then be analyzed. These methods only work on\nmonophonic music, and are suitable for real-time implementation.\nEnvelope  extraction\nVibrato, a low frequency signal, sinusoidally varies the envelope of the instrumentÕs waveform\n(composed of higher frequency components). We can reconstruct the original vibrato signal, by using\nthe standard deviation within a 36.4 ms window as a relative measure of envelopeÕs amplitude. This isthe smallest window that will fit the entire wavelength of the musical note A0 (27.5 Hz), the lowest\nnote we are interested in detecting.\nThe unfiltered vibrato signal is found by sliding the 36.4 ms window one sample at a time and\nrecalculating the standard deviation within the window.  We then filter this with a bandpass filter\n1\n(passing 4-7 Hz)2, to obtain the original vibrato signal causing the amplitude modulation.  \nPrior to using the bandpass filter, the signal can be put through an anti-aliasing low pass filter and\ndownsampled. (We downsampled the sampling rate from 44.1 kHz to 44.1 Hz in our experiment).  \nThis reduces the order needed for the bandpass filter.  \nIf the onsets and offsets are known, a Hanning window can be used to remove the transients they produce in the\nsignal.  Otherwise these transients may be confused with vibrato when passed through the bandpass filter.\nPitch Extraction\n                                                \n1 The bandpass filter used is a FIR least squares filter Order=100, f p1=4Hz, f p2=7Hz\n2 4-7 Hz was the observed range of vibrato frequency in our sound files.2Vibrato sinusoidally varies the fundamental pitch (and harmonics) of the instrumentÕs waveform.\nBecause all the frequency components are affected, just observing the fundamental pitch will be\nenough for determining where the vibrato is present.  To monitor the vibrato in the fundamental, thefollowing pitch tracking scheme was used.\nWe can obtain the fundamental frequency by dividing the sampling rate by the average number of\nsamples within one wavelength of the sound.  To find the wavelength, an autocorrelation of a 72.7\nms windowed signal is used.  This is the minimum window size that would contain two wavelengths of\nthe musical note A0 (27.5 Hz), the lowest note we are interested in detecting.  With the\nautocorrelation data, the distance (number of samples) between the two largest peaks is calculated,\nand the most likely wavelength (number of samples) is determined. The windowed autocorrelation is\npassed over the whole signal, sliding 36.4 ms (half of a window length) each time.\nThe frequency modulation signal in this form can be put through a bandpass filter (4-7 Hz) as for the\namplitude modulation signal, to clean up the non-vibrato noise.  \nConclusion\nThis is an approach to a completely time based analysis of vibrato induced variation of frequency andamplitude in a monophonic musical signal.  Amplitude changes are detected by envelope extraction\nwith a sliding windowed standard deviation.  Frequency changes are detected by pitch tracking with a\nsliding windowed autocorrelation of the signal.\nThese algorithms are suitable for real-time implementation, and could be utilized as a teaching aid for\nmusicians wishing to see what physical parameters affect their vibrato quality.  In addition, this\ninformation could be fed into a blackboard system as a preprocessing step before music transcription.\nThe parameters of the amplitude, frequency, and phase of the vibrato generated frequency and\namplitude modulating signals, can be obtained with this method and used in classification systems and\nneural network models.\nAuthor Information\nDaniel Bendor\nUndergraduate School of Electrical Engineering\nUniversity of Maryland at College Park\ndbendor@glue.umd.eduMark Sandler\nDepartment of Electronic Engineering\nKingÕs College London\nmark.sandler@kcl.ac.uk   \nSuggested Readings\nHerrera, Perfecto; Bonada, Jordi.  1998. Vibrato Extraction and Parameterization in the Spectral\nModeling Synthesis Framework.   Proceedings 98 Digital Audio Effects Workshop.\nRossignol, S., Depalle, P., Soumagne, J. Rodet, X. and Collette J.-L. 1999. Vibrato Detection,\nEstimation, Extraction and Modification.   Proceedings 99 Digital Audio Effects Workshop.\nSkovenborg, Esben. 1998.  Modelling the Timbre of Musical Sound.   Dissertation submitted to the\nDepartment of Electronics and the Music Technology Group of the University of York."
    },
    {
        "title": "IR for Contemporary Music: What the Musicologist Needs.",
        "author": [
            "Alain Bonardi"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415912",
        "url": "https://doi.org/10.5281/zenodo.1415912",
        "ee": "https://zenodo.org/records/1415912/files/Bonardi00.pdf",
        "abstract": "Active listening is the core of musical activity Listening does not only concern receiving musical information. On the contrary, it is “active” and based on a set of interactions between listeners and musical documents—including automatic music information research and extraction—so as to discover intentions. This recognition process is based on the observation of regularities and rules, in order to build “forms” from all indications, information and redundancies. The listener interprets all the signs that are meaningful for him as intentions, attributed to the composer. Features of computer assisted listening Let us specify further the active listening situation for the musicologist, taking for example the consultation of a document in such a digital library as IRCAM’s. The musicologist is facing a computer screen, while handling scores and books. This terminal allows him, among many other possibilities, to listen to music, to access musical data bases and hypermedia analyses. The musicologist is handling several devices on several media at the same time. First of all, the listener needs a framework that takes him/her into account. The purpose is to set the conditions of possibility of listening by restricting the heuristics of “forms”. It is therefore necessary to set a listening framework for the musicologist, to assist him in discovering the “intentions” of music. The main feature of this listening environment is thus its capacity to enable its user to vary the music representation. In the same way that working on a musical piece leads us either to read it silently or to sol-fa, or to hum it, or to play it, the musicologist’s environment must enable rapid changes of the This is very important: a critical part of the analysis work consists in associating varied representations and contexts. Its purpose is the emergence of meaning from numerous and dissimilar elements that views imagined by the musicologist manage to reconcile. Musical databases help weave these links. In such an environment as the IRCAM Digital Library, we can either: •= associate various representations of music: hypermedia analyses offer sonograms as well as scores or formal schemes. •= or associate various contexts: the musicologist can easily know which works are contemporary with the Marteau sans Maître by Boulez using roughly the same instruments, or explore the musical production of the year 1954. Musicology and contemporary music We use here the expression “contemporary music” for Western art-tradition music written since 1945. However this definition is controversial, and a lot of ambiguity remains in identifying works that belong to it. But it avoids, at least provisionally, the stumbling block of the stylistic definition. To characterize it, we use the musicologist’s point of view, with his/her tools, either computerized or not. The musicologist is at the same time a listener and a composer, since analyzing a piece a music leads to “rewriting” it. The first difficulty the musicologist faces in contemporary music is the confusion in listening. Looking for reference marks, the listener is in a way considerably free, but he is deprived from the listening guiding towards intentions we evoked before. Let us examine the consequences of this confusion in terms of automatic research of musical information: •= The first point is the decline of melody, which used to be the fundamental basis in musical composition. Structural objects used by composers are no longer the ones which are part of the horizontal entities perceived (cells, figures). This raises two kinds of problems: ∗= the first one concerns the computation of the musical surface, that is to say the automatic determination of outstanding elements of a polyphonic music, by combination of structural and sound criteria (instrumental features, timbres, masking effects, etc.). ∗= the second one deals with the research of melodic elements which play a structural role, such as mottos. •= The second point concerns the difficulty of extracting and interpreting harmonic data from contemporary music works. There is no longer a reference system. This problems has arisen since Debussy: in his music, he often “forgets” to resolve certain dissonances, which progressively become part of his harmonic vocabulary. •= The relationship to form: in his effort to recognize intentions, the listener eliminates time from movement, to get the trajectory and the form. The latter derives from a spatial conception; in its expression, it is often mistaken for its structure. The classical sonata is often described according to a rhetorical and intemporal scheme exposition/development/reexposition. Contemporary music proposes two major evolutions in that field: ∗= form can be attached to other representations than symbolic structures: spectral music thus often uses paths through sound spectra called interpolations that contribute to the form. To our knowledge, there is no automatic tool able to interpret this kind of data at the scale of a form. ∗= many contemporary composers do not want form to reveal itself simply, as a reducing logical scheme, for instance the ABA scheme for the aria da capo. But the possibilities of using “operating filters” as algorithms in automatic search, to be able to handle complex forms, are still limited. •= Last but not least, we have only evoked music using traditional instruments. Electroacoustic and mixed (computer plus instruments) raise the representation problem: which information structures is it possible to extract from the audio file of such a piece as Artikulation by Ligeti? How can we handle inharmonic sounds? A synthesis of the musicologist’s needs Our first remark is that the choice of the contemporary catalogue does not modify the nature of the musicologist’s work nor his/her purposes. However it seems that this task is more difficult and less systematic for contemporary music. The musicologist must set together by himself “formal filters” enabling him to account for the composer’s intention, starting from directories of simple form bearing elements and classical structures. We state that the musicologist’s workstation must have the following features: •= It proposes varied representations of music and circulation possibilities among them, breaking away from physical separations between their producers. It must thus propose to the musicologist the computerized tools and files uses by the composer as he/she was elaborating the piece, if possible. The representations that the musicologist will link together during his/her analysis work are: ∗= the graphical representations (paper score, sonogram, formal scheme, etc.), ∗= the sound representations (audio), ∗= the symbolic representations (MIDI, computerized description of the score, etc.), ∗= if necessary, the tool-representations that concern the working steps of the composer using the computer. •= It allows for the active listening of music in a broad meaning, by consulting different musical documents, each of them being associated to one or several representations we have described. •= It must allow for the reconciliation of reading and writing on the same media. In traditional music analysis, these two phases are split, since the musicologist reads a score and moreover writes a document in a literary form, without any possibility of dynamic link between the two. On a terminal, it is important that the musicologist may have possibilities of writing and annotating on musical objects represented on the screen. In the case of mixed musics, associating computer and instruments, the musicologist has to be able to use the sketch computerized environment run by the composer. •= To face the difficulty of analyzing contemporary music, the system proposes to the listener/musicologist to build his own adequate structures to look for forms using specific languagues to encode the patterns, either global or local. The form bearing elements may be: ∗= either musical , to be looked for in symbolic representations (as different kinds of intervals of notions such as the longest sequence of joint intervals, or the longest sequence of disjoint intervals, etc.), ∗= or related to sound, to be looked for in signal representations (search for harmonic zones, peaks, etc.) ∗= or operating processes (fractals, mathematic transformations, etc.). These form bearing elements can be hierarchically set together as trees, to build a form search profile. •= It includes learning mechanisms, such as: ∗= saving of works achieved by each musicologist, ∗= saving of approaches lead for each of the analyzed pieces, ∗= enriching the directories including form bearing elements and elementary structures. Examples of implementation in IRCAM projects Two European projects including IRCAM partnership answer part of these expectations: •= the CUIDADO (Content-based Unified Interfaces and Descriptors for Audio/music Databases available Online) project deals with description and search of audio files, in conjunction with the proposed MPEG7 standard. Form bearing elements are here low level data extracted from signal to be statically correlated to high level descriptors such as genre information. Among proposed functionalities, let us notice: the fast search of sound files, of similarities between audio contents, the creation of user profiles, the editing and classification of sounds. •= The WEDELMUSIC (Web Delivery of Music Scores) project proposes a system of online distribution of scores or fragments of scores. The musicologist has possibilities of research of musical structures, of comparison and annotation. There is still much work remaining to propose a coherent set of software and devices dedicated to contemporary music. Suggested Readings [Berio 1981] Berio, Luciano, 1983. Intervista sulla musica, a cura di Rossana Dalmonte, Roma, Bari, Laterza. [Lerdahl 1988] Lerdahl, Fred, 1988. « Structure de prolongation dans l’atonalité », in La musique et les sciences cognitives, Bruxelles, Mardaga, pp. 103-135. [Rousseaux 1990] Rousseaux, Francis, 1990. Une contribution de l’intelligence artificielle et de l’apprentissage symbolique automatique à l’élaboration d’un modèle d’enseignement de l’écoute musicale, Thèse de doctorat, Université Paris VI.",
        "zenodo_id": 1415912,
        "dblp_key": "conf/ismir/Bonardi00",
        "keywords": [
            "active listening",
            "musicologist",
            "computer assisted listening",
            "framework",
            "heuristics",
            "listening environment",
            "music representation",
            "critical part",
            "meaning",
            "musical databases"
        ],
        "content": "IR for Contemporary Music: What the Musicologist Needs \n \nAlain Bonardi \nIRCAM \n1, place Igor Stravinsky \n75004 Paris \nalain.bonardi@ircam.fr \n  \nAbstract\n \nActive listening is the core of musical activity \nListening does not only concern receiving musical information. On the contrary, it is “active” and based \non a set of interactions between listeners and musical documents—including automatic music information research and extraction—so as to discover intentions. This recognition process is based on the observation of regularities and rules, in order to build “forms” from all indications, information and redundancies. The listener interprets all the signs that are meaningful for him as intentions, attributed to the composer. \nFeatures of computer assisted listening \nLet us specify further the active listening situation for the musicologist, taking for example the consultation of a document in such a digital library as IRCAM’s. The musicologist is facing a computer screen, while handling scores and books. This terminal allows him, among many other possibilities, to listen to music, to access musical data bases and hypermedia analyses. The musicologist is handling several devices on several media at the same time. \nFirst of all, the listener needs a framework that takes him/her into account. The purpose is to set the \nconditions of possibility of listening by restricting the heuristics of “forms”. It is therefore necessary to set a listening framework for the musicologist, to assist him in discovering the “intentions” of music. The main feature of this listening environment is thus its capacity to enable its user to vary the music representation. In the same way that working on a musical piece leads us either to read it silently or to sol-fa, or to hum it, or to play it, the musicologist’s environment must enable rapid changes of the representation of abstract objects. \nThis is very important: a critical part of the analysis work consists in associating varied representations \nand contexts. Its purpose is the emergence of meaning from numerous and dissimilar elements that views imagined by the musicologist manage to reconcile. Musical databases help weave these links. In such an environment as the IRCAM Digital Library, we can either: \n•=associate various representations of music: hypermedia analyses offer sonograms as well as scores or \nformal schemes. \n•=or associate various contexts: the musicologist can easily know which works are contemporary with \nthe Marteau sans Maître  by Boulez using roughly the same instruments, or explore the musical \nproduction of the year 1954. Musicology and contemporary music \nWe use here the expression “contemporary music” for Western art-tradition music written since 1945. \nHowever this definition is controversial, and a lot of ambiguity remains in identifying works that belong to it. But it avoids, at least provisionally, the stumbling block of the stylistic definition. \nTo characterize it, we use the musicologist’s point of view, with his/her tools, either computerized or not. \nThe musicologist is at the same time a listener and a composer, since analyzing a piece a music leads to “rewriting” it.  \nThe first difficulty the musicologist faces in contemporary music is the confusion in listening. Looking \nfor reference marks, the listener is in a way considerably free, but he is deprived from the listening guiding towards intentions we evoked before. Let us examine the consequences of this confusion in terms of automatic research of musical information: \n•=The first point is the decline of melody, which used to be the fundamental basis in musical \ncomposition. Structural objects used by composers are no longer the ones which are part of the horizontal entities perceived (cells, figures). This raises two kinds of problems: \n∗=the first one concerns the computation of the musical surface, that is to say the automatic \ndetermination of outstanding elements of a polyphonic music, by combination of structural and sound criteria (instrumental features, timbres, masking effects, etc.). \n∗=the second one deals with the research of melodic elements which play a structural role, \nsuch as mottos. \n•=The second point concerns the difficulty of extracting and interpreting harmonic data from \ncontemporary music works. There is no longer a reference system. This problems has arisen since Debussy: in his music, he often “forgets” to resolve certain dissonances, which progressively become part of his harmonic vocabulary. \n•=The relationship to form: in his effort to recognize intentions, the listener eliminates time from \nmovement, to get the trajectory and the form. The latter derives from a spatial conception; in its expression, it is often mistaken for its structure. The classical sonata is often described according to a rhetorical and intemporal scheme exposition/development/reexposition. Contemporary music proposes two major evolutions in that field: \n∗=form can be attached to other representations than symbolic structures: spectral music \nthus often uses paths through sound spectra called interpolations that contribute to the form. To our knowledge, there is no automatic tool able to interpret this kind of data at the scale of a form. \n∗=many contemporary composers do not want form to reveal itself simply, as a reducing \nlogical scheme, for instance the ABA scheme for the aria da capo . But the possibilities of \nusing “operating filters” as algorithms in automatic search, to be able to handle complex forms, are still limited. \n•=Last but not least, we have only evoked music using traditional instruments. Electroacoustic and \nmixed (computer plus instruments) raise the representation problem: which information structures is it possible to extract from the audio file of such a piece as Artikulation  by Ligeti? How can we \nhandle inharmonic sounds? \nA synthesis of the musicologist’s needs \nOur first remark is that the choice of the contemporary catalogue does not modify the nature of the musicologist’s work nor his/her purposes. However it seems that this task is more difficult and less systematic for contemporary music. The musicologist must set together by himself “formal filters” enabling him to account for the composer’s intention, starting from directories of simple form bearing elements and classical structures. \nWe state that the musicologist’s workstation must have the following features: \n•=It proposes varied representations of music and circulation possibilities among them, breaking away \nfrom physical separations between their producers. It must thus propose to the musicologist the computerized tools and files uses by the composer as he/she was elaborating the piece, if possible. The representations that the musicologist will link together during his/her analysis work are: \n∗=the graphical representations (paper score, sonogram, formal scheme, etc.), \n∗=the sound representations (audio), \n∗=the symbolic representations (MIDI, computerized description of the score, etc.), \n∗=if necessary, the tool-representations that concern the working steps of the composer \nusing the computer. \n•=It allows for the active listening of music in a broad meaning, by consulting different musical \ndocuments, each of them being associated to one or several representations we have described. \n•=It must allow for the reconciliation of reading and writing on the same media. In traditional music \nanalysis, these two phases are split, since the musicologist reads a score and moreover writes a document in a literary form, without any possibility of dynamic link between the two. On a terminal, it is important that the musicologist may have possibilities of writing and annotating on musical objects represented on the screen. In the case of mixed musics, associating computer and instruments, the musicologist has to be able to use the sketch computerized environment run by the composer. \n•=To face the difficulty of analyzing contemporary music, the system proposes to the \nlistener/musicologist to build his own adequate structures to look for forms using specific languagues to encode the patterns, either global or local. The form bearing elements may be: \n∗=either musical , to be looked for in symbolic representations (as different kinds of \nintervals of notions such as the longest sequence of joint intervals, or the longest sequence of disjoint intervals, etc.), \n∗=or related to sound, to be looked for in signal representations (search for harmonic \nzones, peaks, etc.) \n∗=or operating processes (fractals, mathematic transformations, etc.). These form bearing elements can be hierarchically set together as trees, to build a form search \nprofile. \n•=It includes learning mechanisms, such as: \n∗=saving of works achieved by each musicologist, \n∗=saving of approaches lead for each of the analyzed pieces, \n∗=enriching the directories including form bearing elements and elementary structures. \nExamples of implementation in IRCAM projects \nTwo European projects including IRCAM partnership answer part of these expectations: \n•=the CUIDADO (Content-based Unified Interfaces and Descriptors for Audio/music Databases \navailable Online) project deals with description and search of audio files, in conjunction with the proposed MPEG7 standard. Form bearing elements are here low level data extracted from signal to be statically correlated to high level descriptors such as genre information. Among proposed functionalities, let us notice: the fast search of sound files, of similarities between audio contents, the creation of user profiles, the editing and classification of sounds. \n•=The WEDELMUSIC (Web Delivery of Music Scores) project proposes a system of online \ndistribution of scores or fragments of scores. The musicologist has possibilities of research of musical structures, of comparison and annotation. \nThere is still much work remaining to propose a coherent set of software and devices dedicated to \ncontemporary music. \nSuggested Readings \n[Berio 1981] Berio, Luciano, 1983. Intervista sulla musica , a cura di Rossana Dalmonte, Roma, Bari, \nLaterza. \n[Lerdahl 1988] Lerdahl, Fred, 1988. « Structure de prolongation dans l’atonalité », in La musique et les \nsciences cognitives , Bruxelles, Mardaga, pp. 103-135. \n[Rousseaux 1990] Rousseaux, Francis, 1990. Une contribution de l’intelligence artificielle et de \nl’apprentissage symbolique automatique à l’élaboration d’un modèle d’enseignement de l’écoute musicale , Thèse de doctorat, Université Paris VI."
    },
    {
        "title": "Using User Models in Music Information Retrieval Systems.",
        "author": [
            "Wei Chai",
            "Barry Vercoe"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415898",
        "url": "https://doi.org/10.5281/zenodo.1415898",
        "ee": "https://zenodo.org/records/1415898/files/ChaiV00.pdf",
        "abstract": "Most websites providing music services only support category-based browsing and/or text-based searching. There has been some research to improve the interface either for pull applications, e.g. query-by-humming systems, or for push applications, e.g. collaborative-filtering-based or feature- based music recommendation systems. However, for content-based search or feature-based filtering systems, one important problem is to describe music by its parameters or features, so that search engines or information filtering agents can use them to measure the similarity of the target (userÕs query or preference) and the candidates. MPEG7 (formally called ÒMultimedia Content Description InterfaceÓ) is an international standard, which describes the multimedia content data to allow universal indexing, retrieval, filtering, control, and other activities supported by rich metadata. However, the metadata about the multimedia content itself are still insufficient, because many features of multimedia content are quite perceptual and user-dependent. For example, emotional features are very important for multimedia retrieval, but they are hard to be described by a universal model since different users may have different emotional responses to the same multimedia content. We therefore turn to user modeling techniques and representations to describe the properties of each user, so that the retrieval will be more accurate. Besides, user modeling can be used to reduce the search space, make push service easier and improve the user interface. There are several important issues in user modeling for music information retrieval purpose or even more general multimedia retrieval. 1) How to model the user? User-programmed, machine- learning and knowledge-engineered methods can be used. 2) What information is needed to describe a user for music IR purpose? It may include both the userÕs indirect information (e.g. age, sex, citizenship, education, music experience, etc.) and direct information (e.g. userÕs interests, definition of qualitative features, appreciation habit, etc.). 3) How to represent, use and share the user model? Similar to MPEG7 concepts, we can use a standard language in text format to represent the user model, so that search engines or information filtering agents can use it to refine the result easily and efficiently, without repeating the long-time observation and learning of the userÕs behavior. User modeling can be done on client-side or server-side. Issues including easy/hard to obtain the user information, hard/easy to use collaborative filtering model, far from/close to the music data, more/less privacy or safety, more scalable/higher load on the server, etc., need to be considered when choosing either of the paradigms. We adopted the client-side user modeling paradigm in our MusicCat system. It is an agent that allows the user to define contexts and corresponding features of music that he wants to hear in those contexts correspondingly. Besides, the user can also define qualitative features of music based on quantitative features. For examples, the user just needs to tell the agent what kind of music he prefers to hear at what kind of context, like ÒI need fast and exciting music when IÕm happyÓ, ÒI need soft music to wake me up every morning at 8:00Ó,  ÒI need slow classical music, when IÕm thinkingÓ, ÒI need rhythmic music when IÕm walkingÓ, etc. Or, the user can define qualitative features, like ÒRomantic music for me means slow music with titles or lyric including word loveÓ, ÒMy favorite music includes ÉÓ etc. Then, when the moment comes - the user tells the agent or a pre-defined time approaches, the agent can automatically, randomly and repeatedly choose music from the userÕs collection according to the pre-defined constraints. So far, MusicCat uses a profile-based user interface. And only midi files are used in the system. In our system, we categorize the quantitative features of music into textual features including title, composer, genre, country, etc.; notation features including key signature, time signature, tonality, BPM, etc.; perceptual features including slow or fast, short or long, quiet or loud, non- rhythmic or rhythmic, and soft or exciting. User can define music corresponding to a particular context or qualitative feature based on these quantitative features. We manually input the textual information. Notation features were automatically extracted from midi files. Perceptual features were computed from the parameters including average duration per note, average tempo, tempo deviation, average pitch change per note, etc. The weights of each parameter contributing to each perceptual feature were set manually. Some psychoacoustic experiments should be necessary for more accuracy.  To make the system more appealing, wireless communication technology can be used to make the system portable. We may also use sensors that can automatically detect the user context and then play corresponding music. User information is very valuable and needs to be shared in the future to make universal information retrieval possible. Thus we propose an XML-like language, UMIRL (User Modeling for Information Retrieval Language), in which different systems may describe the user in this standard format to make the user model sharable and reusable. Although it is designed for music information retrieval purpose in this paper, it can be extended for general multimedia information retrieval as well. To make music information retrieval systems more efficient, both user modeling techniques and descriptions are important. Those are prerequisites for open and efficient personalized services. We propose the user modeling language because there hasnÕt been much attention on the ways to share the valuable user data. We believe the main issues discussed in this paper will be the most significant but also the hardiest points in this research area. Author Information Wei Chai Media Laboratory Massachusetts Institute of Technology chaiwei@media.mit.edu Barry Vercoe Media Laboratory Massachusetts Institute of Technology bv@media.mit.edu Suggested Readings Giorgio Braijnik, Giovanni Guida, Carlo Tasso.  1990. User Modeling in Expert Man-Machine Interfaces: a Case Study in Intelligent Information Retrieval . IEEE Transactions Transactions on Systems, Man, and Cybernetics, Vol 20. No.1, pp166-185. Upendra Shardanand and Pattie Maes. 1995. Social Information Filtering: Algorithms for Automating ÒWord of MouthÓ. Conference proceedings on Human factors in computing systems, pp210-217. Eric Scheirer.  2000.  Music-Listening Systems, PhD dissertation.",
        "zenodo_id": 1415898,
        "dblp_key": "conf/ismir/ChaiV00",
        "keywords": [
            "Music information retrieval",
            "User modeling",
            "Multimedia content description interface",
            "User modeling techniques",
            "User modeling representations",
            "User modeling paradigms",
            "User modeling language",
            "User modeling for information retrieval",
            "User modeling for multimedia retrieval",
            "User modeling for music information retrieval"
        ],
        "content": "Using User Models in Music Information Retrieval Systems\nAbstract\nMost websites providing music services only support category-based browsing and/or text-based\nsearching. There has been some research to improve the interface either for pull applications, e.g.query-by-humming systems, or for push applications, e.g. collaborative-filtering-based or feature-\nbased music recommendation systems. However, for content-based search or feature-based\nfiltering systems, one important problem is to describe music by its parameters or features, so thatsearch engines or information filtering agents can use them to measure the similarity of the target\n(userÕs query or preference) and the candidates. MPEG7 (formally called ÒMultimedia Content\nDescription InterfaceÓ) is an international standard, which describes the multimedia content datato allow universal indexing, retrieval, filtering, control, and other activities supported by richmetadata. However, the metadata about the multimedia content itself are still insufficient, because\nmany features of multimedia content are quite perceptual and user-dependent. For example,\nemotional features are very important for multimedia retrieval, but they are hard to be describedby a universal model since different users may have different emotional responses to the same\nmultimedia content. We therefore turn to user modeling techniques and representations to\ndescribe the properties of each user, so that the retrieval will be more accurate. Besides, usermodeling can be used to reduce the search space, make push service easier and improve the userinterface.\nThere are several important issues in user modeling for music information retrieval purpose or\neven more general multimedia retrieval. 1) How to model the user? User-programmed, machine-\nlearning and knowledge-engineered methods can be used. 2) What information is needed to\ndescribe a user for music IR purpose? It may include both the userÕs indirect information (e.g.age, sex, citizenship, education, music experience, etc.) and direct information (e.g. userÕs\ninterests, definition of qualitative features, appreciation habit, etc.). 3) How to represent, use and\nshare the user model? Similar to MPEG7 concepts, we can use a standard language in text formatto represent the user model, so that search engines or information filtering agents can use it torefine the result easily and efficiently, without repeating the long-time observation and learning of\nthe userÕs behavior.\nUser modeling can be done on client-side or server-side. Issues including easy/hard to obtain the\nuser information, hard/easy to use collaborative filtering model, far from/close to the music data,\nmore/less privacy or safety, more scalable/higher load on the server, etc., need to be consideredwhen choosing either of the paradigms.\nWe adopted the client-side user modeling paradigm in our MusicCat system. It is an agent that\nallows the user to define contexts and corresponding features of music that he wants to hear inthose contexts correspondingly. Besides, the user can also define qualitative features of music\nbased on quantitative features. For examples, the user just needs to tell the agent what kind of\nmusic he prefers to hear at what kind of context, like ÒI need fast and exciting music when IÕmhappyÓ, ÒI need soft music to wake me up every morning at 8:00Ó,  ÒI need slow classical music,\nwhen IÕm thinkingÓ, ÒI need rhythmic music when IÕm walkingÓ, etc. Or, the user can define\nqualitative features, like ÒRomantic music for me means slow music with titles or lyric includingword loveÓ, ÒMy favorite music includes ÉÓ etc. Then, when the moment comes - the user tellsthe agent or a pre-defined time approaches, the agent can automatically, randomly and repeatedly\nchoose music from the userÕs collection according to the pre-defined constraints. So far,\nMusicCat uses a profile-based user interface. And only midi files are used in the system.In our system, we categorize the quantitative features of music into textual features including\ntitle, composer, genre, country, etc.; notation features including key signature, time signature,tonality, BPM, etc.; perceptual features including slow or fast, short or long, quiet or loud, non-rhythmic or rhythmic, and soft or exciting. User can define music corresponding to a particular\ncontext or qualitative feature based on these quantitative features. We manually input the textual\ninformation. Notation features were automatically extracted from midi files. Perceptual featureswere computed from the parameters including average duration per note, average tempo, tempo\ndeviation, average pitch change per note, etc. The weights of each parameter contributing to each\nperceptual feature were set manually. Some psychoacoustic experiments should be necessary formore accuracy.  To make the system more appealing, wireless communication technology can be\nused to make the system portable. We may also use sensors that can automatically detect the user\ncontext and then play corresponding music.\nUser information is very valuable and needs to be shared in the future to make uni versal\ninformation retrieval possible. Thus we propose an XML-like language, UMIRL (User Modeling\nfor Information Retrieval Language), in which different systems may describe the user in thisstandard format to make the user model sharable and reusable. Although it is designed for music\ninformation retrieval purpose in this paper, it can be extended for general multimedia information\nretrieval as well.\nTo make music information retrieval systems more efficient, both user modeling techniques and\ndescriptions are important. Those are prerequisites for open and efficient personalized services.We propose the user modeling language because there hasnÕt been much attention on the ways toshare the valuable user data. We believe the main issues discussed in this paper will be the most\nsignificant but also the hardiest points in this research area.\nAuthor Information\nWei Chai\nMedia Laboratory\nMassachusetts Institute of Technology\nchaiwei@media.mit.eduBarry Vercoe\nMedia Laboratory\nMassachusetts Institute of Technology\nbv@media.mit.edu\nSuggested Readings\nGiorgio Braijnik, Giovanni Guida, Carlo Tasso.  1990. User Modeling in Expert Man-Machine Interfaces: aCase Study in Intelligent Information Retrieval . IEEE Transactions Transactions on Systems, Man, and\nCybernetics , Vol 20. No.1, pp166-185.\nUpendra Shardanand and Pattie Maes. 1995. Social Information Filtering: Algorithms for Auto mating\nÒWord of MouthÓ. Conference proceedings on Human factors in computing systems , pp210-217.\nEric Scheirer.  2000.  Music-Listening Systems , PhD dissertation."
    },
    {
        "title": "Music Representation, Indexing and Retrieval at NTHU.",
        "author": [
            "Arbee L. P. Chen"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1417981",
        "url": "https://doi.org/10.5281/zenodo.1417981",
        "ee": "https://zenodo.org/records/1417981/files/Chen00.pdf",
        "abstract": "summarized. We treat the rhythm, melody, and chords of a music object as music features and develop various data structures and algorithms to efficiently perform approximate and partial matching for the retrieval of music data [Liu99a], [Chen98], [Chou96]. In [Chen00a], we present the techniques for retrieving songs by music segments. A music segment consists of a segment type and the associated beat and pitch information. We also propose multi-feature index structures for exact and approximate searching on different features [Lee00]. The problem of feature extraction is also studied. The repeating pattern is defined as a sequence of notes, which appears more than once in music objects. Choosing repeating patterns as the feature to represent the music objects meets both efficiency and semantic-richness requirements for content-based music data retrieval. We propose approaches to efficiently discover the repeating patterns of music objects in [Hsu98], [Liu99b]. We have also implemented Muse, a prototype system for content-based music data retrieval to illustrate the feasibility of the concepts we propose.",
        "zenodo_id": 1417981,
        "dblp_key": "conf/ismir/Chen00",
        "keywords": [
            "music features",
            "approximate and partial matching",
            "music data retrieval",
            "music segments",
            "multi-feature index structures",
            "feature extraction",
            "repeating patterns",
            "Muse",
            "content-based music data retrieval",
            "system implementation"
        ],
        "content": "Arbee L.P. Chen \nDepartment of Computer Science, National Tsing Hua University \nHsinchu, Taiwan, R.O.C. \nEmail: alpchen@cs.nthu.edu.tw  \n \nIn this extended abstract, our work on the representation, indexing and retrieval of music data is \nsummarized. We treat the rhythm, melody, and chords of a music object as music features and develop various data structures and algorithms to efficiently perform approximate and partial matching for the retrieval of music data [Liu99a], [Chen98], [Chou96]. In [Chen00a], we present the techniques for retrieving songs by music segments . A music segment consists of a segment type and the associated beat \nand pitch information. We also propose multi-feature index structures for exact and approximate searching on different features [Lee00].  \nThe problem of feature extraction is also studied. The repeating pattern  is defined as a sequence of \nnotes, which appears more than once in music objects. Choosing repeating patterns as the feature to represent the music objects meets both efficiency and semantic-richness requirements for content-based music data retrieval. We propose approaches to efficiently discover the repeating patterns of music objects in [Hsu98], [Liu99b].  \nWe have also implemented Muse , a prototype system for content-based music data retrieval to \nillustrate the feasibility of the concepts we propose.\n  \nReferences \n[Chen00] Chen, A. L. P., M. Chang, J. Chen, J. L. Hsu, C. H. Hsu, and S. Y. S. Hua, “Query by Music \nSegments: An Efficient Approach for Song Retrieval,” in Proc. of IEEE International \nConference on Multimedia and Expo , 2000. \n[Chen98] Chen, J. C. C. and A. L. P. Chen, “Query by Rhythm: An Approach for Song Retrieval in \nMusic Databases,” in  Proc. of IEEE International Workshop on Research Issues in Data \nEngineering , 1998. \n[Chou96] Chou, T. C., A. L. P. Chen, and C. C. Liu, “Music Databases: Indexing Techniques and \nImplementation,”  in Proc. of IEEE International Workshop on Multimedia Data Base \nManagement System , 1996. \n[Hsu98] Hsu, J. L., C. C. Liu, and A. L. P. Chen, “Efficient Repeating Pattern Finding in Music \nDatabases,” in  Proc. of ACM International Conference on Information and Knowledge \nManagement , 1998. \n[Lee00] Lee, W. and A. L. P. Chen, “Efficient Multi-Feature Index Structures for Music Data \nRetrieval,” in Proc. of SPIE Conference on Storage and Retrieval for Image and Video \nDatabases , 2000. \n[Liu99a] Liu, C. C., J. L. Hsu, and A. L. P. Chen, “An Approximate String Matching Algorithm for \nContent-Based Music Data Retrieval,” in Proc. of IEEE International Conference on \nMultimedia Computing and Systems , 1999. \n[Liu99b] Liu, C. C., J. L. Hsu, and A. L. P. Chen, “Efficient Theme and Non-Trivial Repeating Pattern \nDiscovering in Music Databases,” in Proc. of IEEE International Conference on Data \nEngineering , 1999. Music Representation, Indexing and Retrieval at NTHU"
    },
    {
        "title": "Optical Music Recognition System within a Large-Scale Digitization Project.",
        "author": [
            "G. Sayeed Choudhury",
            "M. Droetboom",
            "Tim DiLauro",
            "Ichiro Fujinaga",
            "Brian Harrington 0001"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415730",
        "url": "https://doi.org/10.5281/zenodo.1415730",
        "ee": "https://zenodo.org/records/1415730/files/ChoudhuryDDFH00.pdf",
        "abstract": "An adaptive optical music recognition system is being de- veloped as part of an experiment in creating a comprehen- sive framework of tools to manage the workflow of large- scale digitization projects. This framework will support the path from physical object and/or digitized material into a digital library repository, and offer effective tools for incorporating metadata and perusing the content of the resulting multimedia objects. 1",
        "zenodo_id": 1415730,
        "dblp_key": "conf/ismir/ChoudhuryDDFH00",
        "keywords": [
            "adaptive optical",
            "music recognition",
            "experiment",
            "comprehensive framework",
            "workflow",
            "large-scale digitization",
            "digital library repository",
            "metadata",
            "content",
            "multimedia objects"
        ],
        "content": "OpticalMusicRecognition SystemwithinaLarge-Scale\nDigitization Project\nG.SayeedChoudhury\n\u0000,TimDiLauro\n\u0000,MichaelDroettboom\u0001,\nIchiroFujinaga\u0001,BrianHarrington\n\u0000andKarlMacMillan\u0001\u0002DigitalKnowledgeCenter,MiltonS.EisenhowerLibrary\nJohnsHopkinsUniversity\nand\u0003PeabodyConservatoryofMusic\nJohnsHopkinsUniversity\nAbstract\nAnadapti veoptical music recognition system isbeing de-\nveloped aspartofanexperiment increating acomprehen-\nsiveframe workoftools tomanage theworkﬂo woflarge-\nscale digitization projects. This frame workwill support\nthepath from physical object and/or digitized material\ninto adigital library repository ,andoffereffectivetools\nforincorporating metadata andperusing thecontent ofthe\nresulting multimedia objects.\n1Introduction\nThe project involvesdigitization oftheLester S.Levy\nCollection ofSheet Music (Milton S.Eisenho werLibrary ,\nJohns Hopkins University). Central tothisproject isthe\noptical music recognition program, which convertsbit-\nmapped images ofscanned music intomachine-readable\nformats. InPhase One, images ofthemusic andlyrics,\nandcolor images ofthecoversoftheLevyCollection\nwere digitized andadatabase oftextindexrecords was\ncreated. Phase Twoconsists ofconverting thedigitized\nmusic tocomputer -readable music notation format along\nwith full-te xtlyrics, generating sound renditions, andcre-\nating metadata toenhance search capabilities.\n2Lester S.Levycollection\nDuring Phase One, the researchers atthe Eisen-\nhower Library created adatabase oftextindexrecords, images ofthemusic and lyrics and color im-\nages ofthecoversheets from theLevyCollection.\nThis database isavailable tothe general public at\nhttp://levysheetmusic.mse.jhu.edu .\nTheLevyCollection consists ofover29,000 pieces of\npopular American music. While theCollection spans the\nyears 1780 to1960, itsstrength lieswithin itsthorough\ndocumentation ofnineteenth andearly twentieth-century\nAmerica through popular music. TheCollection alsopro-\nvides asocial commentary onAmerican lifeandadistinc-\ntiverecord oftheir time.\nCurrently ,theCollection can besearched inthree\nmodes. First, users cansearch bysubject, akeyword\nsearch onthetextrecord. Each ofthepieces hasbeen\nindexedforthesubject ofthesong and/or coverimage.\nUsers may also browse theCollection bythetopical ar-\nrangement ofthephysical collection. The physical col-\nlection’ sorganization scheme includes 38topics, such as\nthecircus; dance; drinking, temperance andsmoking; fra-\nternal orders; presidents; romantic andsentimental songs;\nschools andcolle ges; andtransportation. Finally ,users\nwith interest inthegraphical elements canexamine the\nCollection byfocusing onthecoverart.\n3Digital workﬂo wmanagement\nTheentire project isanexperiment indeveloping acom-\nprehensi veframe workoftools tomanage theworkﬂo w\noflarge-scale digitization projects. This frame workwill\nsupport thepath from physical object and/or digitized ma-terial into adigital library repository ,andoffereffective\ntools forincorporating metadata andperusing thecontent\noftheresulting multimedia objects. TheLevyCollection,\nwith itslargesizeandavailability indigital format, isan\nideal subject fordevelopment andevaluation ofthispro-\nposed frame work.\nPhase One oftheLevyProject focused ondigitally\nscanning themusic into TIFF ﬁles, converting toJPEG\nimages andthumbnails, andthen mounting theimages\nontheWeb.Online indexing wasalso created atthe\nsheet music item level.Anindexrecord wascreated for\neach piece ofmusic, which included when available: the\nunformatted transcription oftitle, statement ofresponsi-\nbility ,ﬁrst lineoflyric, ﬁrst line ofchorus, dedication,\nperformer ,artist/engra ver,publication information, plate\nnumber ,andboxanditem number .Wealso introduced\ncontrolled vocab ulary intheform ofbrief subject terms\nforboth thecontent ofsheet music coversandcontent\nofsongs from theLibrary ofCongress’ sThesaurus for\nGraphic Materials. Currently theinformation isavailable\nasunformatted freetextﬁles thatcanbesearched bysim-\nplekeywordorphrase.\nInPhase Two,anadapti veoptical music recognition\n(AOMR) softw are(Fujinaga 1997) isused toconvert\ntheTIFF image ofscanned sheet music into computer\nreadable-formats, which includes GUIDO andMIDI ﬁles\nalong with full-te xtofthelyrics. These digital objects\nwill bedeposited into thedata repository along with the\nscanned sheet music (TIFF ,JPEG, thumbnail, andassoci-\nated metadata).\n4Adapti veoptical music\nrecognition\nThe AOMR softw areoffers ﬁveimportant advantages\noversimilar commercial offerings. First, itcanberunin\nbatch processing mode, anessential feature fortheLevy\nCollection givenitslargenumber ofmusic sheets. Itis\nimportant tonote that most commercial softw areisin-\ntended forthecasual user anddoes notscale foralarge\nnumber ofobjects. Second, thesoftw areiswritten inC\nandtherefore isportable across platforms. Third, thesoft-\nwarecan“learn” torecognize different music symbols—\nanissue considering thediversity oftheLevyCollection\nspeciﬁcally ,andtheuniverse ofnotated music, ingen-\neral. Fourth, thesoftw areisopen-sourced. Finally ,this\nsoftw arecanseparate full-te xtlyrics thatcanbefurther\nFigure 1:Overall architecture oftheAOMR system\nprocessed using third-party optical character recognition\n(OCR) softw are. Preliminary attempts atusing theexist-\ningOMR system forOCR alsoshowsome promise.\nBoth output formats oftheAOMR, GUIDO andMIDI,\nrepresent non-proprietary ﬁleformats. GUIDO isaﬁle\nformat designed toallowtheinterchange ofmusic nota-\ntiondata between andamong music notation editing and\npublishing programs andmusic scanning programs (Hoos\nandHamel 1997). MIDI provides low-bandwidth trans-\nmission ofmusic overtheInternet sothattheusers can\nlisten tothemusic with their web browsers.\nRecently theAOMR softw arewasported totheLinux\nenvironment using theGTK+ library (Pennington 1999).\nTheoverall architecture ofthesystem isshowninFigure\n1.The console-only version also compiles andruns on\nMicrosoft Windowsusing themingw32/gcc toolchain.\nUsing vertical run-length coding andprojection anal-\nysis thestafflines areremo vedfrom theinput image\nﬁle.Lyrics arealsoremo vedusing various heuristic rules.\nThemusic symbols arethen segmented using connected-\ncomponent analysis. Asetoffeatures, such aswidth,\nheight, area, number ofholes, andlow-order central mo-\nments, isstored foreach segmented graphic object and\nused asthebasis fortheadapti verecognition system based\nonexamples.\nThe exemplar -based classiﬁcation model isbased ontheidea that objects arecategorized bytheir similarity\ntostored examples. The model canbeimplemented by\nthek-nearest-neighbor (k-NN) algorithm (CoverandHart\n1967), which isaclassiﬁcation scheme todetermine the\nclass ofagivensample byitsfeature vector .Distances be-\ntween feature vectors ofanunclassiﬁed sample andpre-\nviously classiﬁed samples arecalculated. Theclass repre-\nsented bytheclosest neighbor isthen assigned totheun-\nclassiﬁed sample. Besides itssimplicity andintuiti veap-\npeal, theclassiﬁer canbeeasily modiﬁed, bycontinually\nadding newsamples thatit“encounters” intothedatabase,\ntobecome anadapti vesystem (Aha 1997). Infact,“the\nnearest neighbor algorithm isoneofthesimplest learn-\ningmethods known,andyetnoother algorithm hasbeen\nshowntooutperform itconsistently” (Cost andSalzber g\n1993). Furthermore, theperformance oftheclassiﬁer can\nbedramatically increased byusing weighted feature vec-\ntors. Finding agood setofweights, however,isextremely\ntime-consuming, thus agenetic algorithm (Holland 1975)\nisused toﬁndasolution (Wettschereck, Aha, andMohri\n1997). Note thatthegenetic algorithm canberunoff-line\nwithout affecting thespeed oftherecognition process.\n5Inter pretation ofmusical\nsemantics\nAfter thesymbols havebeen classiﬁed, their musical se-\nmantics areinterpreted (Droettboom 2000). While other\napproaches tothis problem haveused relati velyhigh-\nleveldata structures such asgraph grammars (Fahmy\nandBlostein 1998) ortemporal trees (Diener 1990), our\nmethod provesthatispossible toperform most operations\nquite conveniently using asimple, temporally-sorted list.\nTheﬁrststepintheinterpretation phase istoconnect all\ninter-related symbols. Forinstance, inorder todetermine\nthepitch ofthenote, itmust berelated toasetofstaff\nlines, aclefandakeysignature. Manyrhythmic errors\ncanalso becorrected byadjusting themetric placement\nofnotes relati vetotheir vertical alignment with notes in\nother parts.\nTheinterpretation isperformed using ahighly dynamic\nobject-oriented system implemented inPython. This de-\nsignmakesitpossible todeﬁne thesemantics ofnewsym-\nbols atrun-time, further facilitating theextensible nature\noftheAOMR system.6Curr entdevelopments\nAninteracti vegraphic editor suitable tobeinterf aced with\ntheAOMR program isbeing developed jointly with the\ngroup working ontheGUIDO editor (Renz 2000). The\npurpose ofthiseditor istocorrect anyerrors generated by\ntheAOMR sothatthecorrected version then canbecon-\nverted toGUIDO format. The GUIDO editor wascho-\nsenbecause itsinternal data structure, GUIDO Graphic\nStream (GSS), isverysimilar totheoriginal Postscript\noutput ofAOMR.\nToenable powerful andfastmusic search retrie val,we\nareadapting anadvanced natural language search en-\ngine tolocate various musical attrib utes, such asmelodic\nphrase, interv al,andcontour .Toenable thissearching and\nprovide auser-friendly navigational mechanism, Phase\nTwooftheLevyProject will include astrong metadata\ncomponent. Commonly deﬁned as“data about data,”\nmetadata isstructured representational information. The\nkinds ofmetadata important forLevyinclude descrip-\ntive(toenable searching, browsing andidentiﬁcation of\nitems), structural (toenable thecreation ofaninterf ace\nforoptimum browsing andnavigation), andadministra-\ntive(tomanage thedigital components ofthecollection\nandaidusers inidentiﬁcation ofitems).\nThecurrent index-textwillbeconverted intostructured\nmetadata using XML (Extensible Markup Language).\nThis metadata willbebound, along with themusic nota-\ntion, image, sound, andlyric data, intoacomplete digital\nsheet music object. WewillusetheXML markup tocre-\nateindexesthatwillallowusers tomovebetween general\nkeywordandprecise searches. Wewillextract richname\ninformation from theunstructured index-textintospeciﬁc\nindexessuch ascomposer ,lyricist, orarranger ,andpos-\nsibly performer ,artist, engra ver,lithographer ,dedicatee,\nandpublisher .Cross-references will direct searchers to\nindexrecords containing varying forms ofnames, includ-\ningpseudon yms, transcribed from thesheet music pieces.\nAllrecords willhavethe“authoritati ve”version ofnames.\nThe subject terms will also recei vemark-up tofacilitate\nsubject keywordsearching.\nTofurther enhance thescholarly value oftheLevyCol-\nlection, aweb interf acewill bedeveloped foramusic\nresearch toolkit, forexample, Humdrum (Huron 1997).\nThese toolkits aresoftw aretools intended toassist inmu-\nsicresearch andaresuitable foruseinawide variety of\ncomputer -based musical investigations, such asmotivic,\nstylistic, andmelodic analysis andconcordance studies.Wealsopropose toextend plans fordeveloping automated\nmeans ofmining authoritati vename information andcre-\nating evenricher name indexes.\n7Conclusions\nThe LevyCollection, with itslargesize andavailability\nindigital format, isanideal subject fordevelopment and\nevaluation ofacomprehensi veframe workoftools toman-\nagetheworkﬂo woflarge-scale multimedia digitization\nprojects. Theadapti veoptical music recognition program\nwillplay acentral roleinthisendea vor.\nTheendresult should signiﬁcantly increase thecollec-\ntion’sresearch value toscholars, educators, writers, musi-\ncians, andthegeneral public, taking advantage ofsearch-\ningboth musical andtextual data, viewing thedigitized\nsheet music andcovers,linking tofull-te xtlyrics, and\nhearing sound ﬁles.\n8Ackno wledgements\nThis project isfunded inpart bytheNational Science\nFoundation, theInstitute forMuseum andLibrary Ser-\nvices, andtheLevyfamily .\nRefer ences\nAha, D.W.(1997). Lazy learning. Aritiﬁcial Intelli-\ngence Review11(1),7–10.\nCost, S.and S.Salzber g(1993). Aweighted near-\nestneighbour algorithm forlearning with symbolic\nfeatures. Machine Learning 10,57–78.\nCover,T.andP.Hart (1967). Nearest neighbour pat-\nternclassiﬁcation. IEEE Transactions onInforma-\ntionTheory 13(1),21–7.\nDiener ,G.(1990). Modeling Music Notation: AThree-\nDimensional Appr oach.Ph.D.thesis, Stanford\nUniversity .Stan-M-69.\nDroettboom, M.(2000). Design and implementa-\ntion ofanoptical music interpreter .Technical re-\nport, Peabody Conserv atory oftheJohns Hop-\nkins University .http://www .peabody .jhu.ed u/md-\nboom/omi/.Fahmy ,H.andD.Blostein (1998). Agraph-re writing\nparadigm fordiscrete relaxation: Application to\nsheet-music recognition. International Journal of\nPattern Reco gnition andArtiﬁcial Intellig ence.\nFujinaga, I.(1997). Adaptive optical music recogni-\ntion.Ph.D.thesis, McGill University .\nHolland, J.H.(1975). Adaptation innatur alandarti-\nﬁcial systems .Ann Arbor: University ofMichigan\nPress.\nHoos, H.H.and K.Hamel (1997). GUIDO music\nnotation version 1.0: Speciﬁcation part I,basic\nGUIDO. Technical Report TI20/97, Technische\nUniversit¨atDarmstadt. http://www .informatik.tu-\ndarmstadt.de/AFS/GUIDO/docu/spec1.htm.\nHuron, D.(1997). Humdrum andkern: Selecti vefea-\ntureencoding. InE.Selfridge-Field (Ed.), Beyond\nMIDI: TheHandbook ofMusical Codes ,pp.375–\n401. Cambridge, MA: MIT Press.\nPennington, H.(1999). GTK+/Gnome Application De-\nvelopment .Indianapolis, IN:NewRiders.\nRenz, K.(2000). Design andimplementation ofaplat-\nform independent guido notation engine. InInter -\nnational Computer Music Confer ence,pp.469–\n472.\nWettschereck, D.,D.W.Aha, andT.Mohri (1997). A\nreviewandempirical evaluation offeature weight-\ningmethods foraclass oflazy learning algorithms.\nArtiﬁcial Intellig ence Review11,272–314."
    },
    {
        "title": "PROMS: A Web-based Tool for Searching in Polyphonic Music.",
        "author": [
            "Michael Clausen",
            "Roland Engelbrecht",
            "Dirk Meyer",
            "Jürgen Schmitz"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1417139",
        "url": "https://doi.org/10.5281/zenodo.1417139",
        "ee": "https://zenodo.org/records/1417139/files/ClausenEMS00.pdf",
        "abstract": "One major task of a digital music library (DML) is to provide techniques to locate a queried musical pattern in all pieces of music in the database containing that pattern. For a survey of several computational tasks related to this kind of data retrieval we refer to Crawford et al. [3]. Existing DMLs like MELDEX [1], Themefinder [4], and the Sonoda-Muraoka-System [7] work with melody databases relying on score-like information. Retrieval and matching are performed in a fault-tolerant way by string-based methods which mainly take into account pitch information. Generally, rhythm plays only a subordinate role. The music dictionary of Barlow and Morgenstern [2] shows that music retrieval based on pitch information only leads to results with typically too many false matches. (An example of such absurd matches is given in Selfridge-Field [6], p. 27.) We are convinced that both pitch and rhythm are crucial for recognizing melodies. In the more general context of polyphonic music, one is even forced to consider pitch and rhythm information. PROMS, a web-based computer-music service under development at the University of Bonn, Germany, is part of the MiDiLiB project [5]. The aim of PROMS is to design and to implement PROcedures for Music Search. Our discussion will take place in a rather general setting: we assume that our database contains several kinds of music such as polyphonic and homophonic music as well as melodies. We also use score-like information. A query to the database is a fragment of a piece of music. This could be a melody or a certain figure of an accompaniment. The task is now to locate efficiently all occurences of this fragment in all pieces of music in the database. We have designed and implemented a web-based computer-music service that enables seaching in polyphonic music. In contrast to the above mentioned systems, PROMS is not string-based but set-oriented. The basic objects within the PROMS system are single notes, specified by its onset time t, its pitch p, and its duration d. A piece of music is then a finite set M of notes. Our database consists of a sequence of N pieces of music in the MIDI format: D1, . . . , DN. Similarly, a query is itself a finite set Q of notes. Thus there is no principal difference between a piece Di of music in the database and a query Q. However, typically, Di is much larger than Q. An occurrence is a pair ∗This work was supported in part by Deutsche Forschungsgemeinschaft grants CL 64/3-1 and CL 64/3-4. (i, v) such that the v-shifted version of Q is a subset of Di: Q + v ⊆Di. Combining methods from Computer Algebra with well-established techniques from Full-Text-Retrieval, we obtain a time and space efficient polyphonic music information retrieval system. This is best illustrated by some performance data (on a Pentium II, 333 MHz, 256 MB RAM, Windows NT 4.0): The PROMS database consists of 327 MB of MIDI data, our index consumes only 22 MB, the time to construct our index is about 40 seconds. Finally, the average response time is about 80 milliseconds. Here are some highlights of our system: PROMS considers pitch and rhythm simultaneously. It supports polyphonic queries. The processing time depends essentially on the number of notes in the query. Queries might contain “gaps”. It supports user- and problem-adapted indexing on- the-fly. It allows fuzzy search and transposition-invariant search. With little additional effort, it allows to compute all occurences of a query with at most k mismatches.",
        "zenodo_id": 1417139,
        "dblp_key": "conf/ismir/ClausenEMS00",
        "keywords": [
            "digital music library",
            "query musical pattern",
            "locate efficiently",
            "polyphonic music",
            "score-like information",
            "pitch and rhythm",
            "polyphonic queries",
            "time and space efficient",
            "user-adapted indexing",
            "fuzzy search"
        ],
        "content": "PROMS: A Web-based Tool for Searching in\nPolyphonic Music∗\nM. Clausen, R. Engelbrecht, D. Meyer, J. Schmitz\nInstitut f¨ ur Informatik V, Universit¨ at Bonn,\nD-53117 Bonn, Germany\n{clausen,roland,meyerd,schmitz1 }@cs.uni-bonn.de\nOctober 4, 2000\nAbstract\nOne major task of a digital music library (DML) is to provide techniques to locate a queried\nmusical pattern in all pieces of music in the database containing that pattern. For a survey of\nseveral computational tasks related to this kind of data retrieval we refer to Crawford et al. [3].\nExisting DMLs like MELDEX [1], Themeﬁnder [4], and the Sonoda-Muraoka-System [7] work\nwith melody databases relying on score-like information. Retrieval and matching are performed in\na fault-tolerant way by string-based methods which mainly take into account pitch information.\nGenerally, rhythm plays only a subordinate role. The music dictionary of Barlow and Morgenstern\n[2] shows that music retrieval based on pitch information only leads to results with typically too\nmany false matches. (An example of such absurd matches is given in Selfridge-Field [6], p. 27.)\nWe are convinced that both pitch and rhythm are crucial for recognizing melodies. In the more\ngeneral context of polyphonic music, one is even forced to consider pitch and rhythm information.\nPROMS, a web-based computer-music service under development at the University of Bonn,\nGermany, is part of the MiDiLiB project [5]. The aim of PROMS is to design and to implement\nPROcedures for Music Search. Our discussion will take place in a rather general setting: we assume\nthat our database contains several kinds of music such as polyphonic and homophonic music as\nwell as melodies. We also use score-like information. A query to the database is a fragment of a\npiece of music. This could be a melody or a certain ﬁgure of an accompaniment. The task is now\nto locate eﬃciently all occurences of this fragment in all pieces of music in the database.\nWe have designed and implemented a web-based computer-music service that enables seaching\nin polyphonic music. In contrast to the above mentioned systems, PROMS is not string-based but\nset-oriented. The basic objects within the PROMS system are single notes, speciﬁed by its onset\ntime t, its pitch p, and its duration d. A piece of music is then a ﬁnite set Mof notes. Our database\nconsists of a sequence of Npieces of music in the MIDI format: D1, . . . , D N. Similarly, a query is\nitself a ﬁnite set Qof notes. Thus there is no principal diﬀerence between a piece Diof music in\nthe database and a query Q. However, typically, Diis much larger than Q. An occurrence is a pair\n∗This work was supported in part by Deutsche Forschungsgemeinschaft grants CL 64/3-1 and CL 64/3-4.(i, v) such that the v-shifted version of Qis a subset of Di:Q+v⊆Di. Combining methods from\nComputer Algebra with well-established techniques from Full-Text-Retrieval, we obtain a time\nand space eﬃcient polyphonic music information retrieval system. This is best illustrated by some\nperformance data (on a Pentium II, 333 MHz, 256 MB RAM, Windows NT 4.0): The PROMS\ndatabase consists of 327 MB of MIDI data, our index consumes only 22 MB, the time to construct\nour index is about 40 seconds. Finally, the average response time is about 80 milliseconds.\nHere are some highlights of our system: PROMS considers pitch and rhythm simultaneously.\nIt supports polyphonic queries. The processing time depends essentially on the number of notes\nin the query. Queries might contain “gaps”. It supports user- and problem-adapted indexing on-\nthe-ﬂy. It allows fuzzy search and transposition-invariant search. With little additional eﬀort, it\nallows to compute all occurences of a query with at most kmismatches.\nReferences\n[1] David Bainbridge. MELDEX: A Web-based Melodic Index Service. In: Melodic Similarity:\nConcepts, Procedures, and Applications. Computing in Musicology , volume 11, chapter 12,\npages 223–230. MIT Press, 1998.\n[2] Harold Barlow and Sam Morgenstern. A Dictionary of Musical Themes . Crown Publishers,\n1948.\n[3] Tim Crawford, Costas S. Iliopoulos, and Rajeev Raman. String-Matching Techniques for Mu-\nsical Similarity and Melodic Recognition. In: Melodic Similarity: Concepts, Procedures, and\nApplications. Computing in Musicology , volume 11, chapter 3, pages 73–100. MIT Press, 1998.\n[4] Andreas Kornst¨ adt. Themeﬁnder: A Web-based Melodic Search Tool. In: Melodic Similarity:\nConcepts, Procedures, and Applications. Computing in Musicology , volume 11, chapter 13, pages\n231–236. MIT Press, 1998.\n[5]MiDiLiB project. Content-based indexing, retrieval, and compression of data in digital music\nlibraries.\nhttp://leon.cs.uni-bonn.de/forschungprojekte/midilib/english/ .\n[6] Eleanor Selfridge-Field. Conceptual and Representational Issues in Melodic Comparison. In:\nMelodic Similarity: Concepts, Procedures, and Applications. Computing in Musicology , vol-\nume 11, chapter 1, pages 3–64. MIT Press, 1998.\n[7] Tomonari Sonoda and Yoichi Muraoka. A www-based melody-retrieval-system — an indexing\nmethod for a large melody database. In Proc. ICMC 2000, Berlin , pages 170–173, 2000."
    },
    {
        "title": "Exploration of Point-Distribution Models for Similarity-based Classification and Indexing of Polyphonic Music.",
        "author": [
            "Dave Cliff",
            "Heppie Freeburn"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416572",
        "url": "https://doi.org/10.5281/zenodo.1416572",
        "ee": "https://zenodo.org/records/1416572/files/CliffF00.pdf",
        "abstract": "Similarity is an intuitive criterion for indexing and classification of digital audio files in music information retrieval systems. While significant work has been done on similarity-based approaches to monophonic music, methods for reliably dealing with databases of arbitrary polyphonic music remain elusive. In this paper we describe our ongoing research in exploring the use of high-order multivariate statistical techniques for similarity-based classification of polyphonic music in digital audio files. The statistical techniques we employ, known as point distribution models (PDMs), have recently proven to be of surprising value in computer vision research for rating visual similarity; here we are attempting to apply PDMs to musical similarity. This involves creating neural networks that approximate the statistical processing, to save on potentially explosive storage and processor requirements. This paper reports on work in progress: our results to date are inconclusive and somewhat negative. We describe our rationale for exploring PDMs in polyphonic music similarity-rating and discuss the problems we have encountered so far, with the intention of encouraging other members of the music information retrieval community to explore this and related approaches. Overview The approach we are currently exploring is, in essence, to perform sophisticated statistical analysis on the results of applying standard signal processing techniques to digital audio polyphonic music data. The key difference between our approach and much prior research is that we are explicitly rejecting the need for an intermediate symbolic representation (such as audio-to- MIDI) of the content of digital-audio music files. Rather, we view individual digital audio files as individual data-points in an ultra-high dimensional space (the space of all possible digital audio files for a given duration, sampling rate, bit depth, and number of channels).  We are exploring the use of multivariate statistical analysis techniques to find transformations that map from this input space (which could have many millions of dimensions) onto new spaces with several orders of magnitude fewer dimensions, where the new spaces have similarity-like distance metrics and classification-like partitions. This might seem like a hopelessly na•ve approach, had not a similar approach recently been shown to be a successful foundation for similarity-based indexing in other ultra-high-dimensional spaces: namely those found in computer vision. In the past decade, new statistical approaches have been developed for computer vision in medical imaging (Wolfson, 1999) and automated surveillance of pedestrians (Leeds, 1998). In essence, these new statistical methods involve computing a characterization of the mean shape, along with a characterization of the primary modes of variation in the shape (i.e., the main ways in which the samples of the shape differ from the mean of the samples). Surprisingly, these primary modes often correspond to intuitive visual notions such as variation in viewing angle of a particular object (such as a human face), or the variation seen between two classes of object (such between female faces and male faces). The approach we have explored in music classification is, in essence, to apply these vision techniques to visual objects that are representations of portions of digital audio. The visual objects are created by taking the spectrograms (amplitude surfaces over a time-frequency space) of the mono sum from fixed-duration samples of stereo CD-quality digital-audio music files and then relatively-coarsely quantizing the time and frequency axes into discrete bins to give data- compression in excess of 90%. The quantized spectrogram is decorated with one amplitude ÒpointÓ per time-frequency bin, and the collections of points for a particular music sample are treated as coordinates for a single point in a high-dimensional space (the space of possible quantized spectrograms). For example, working with 30-second audio samples and dividing the spectrogram frequency axis into 1000 bins and the time axis into 50 bins per second gives 1000*50*30=1.5m points, so each processed sample is a single point in a 1.5m-dimensional space of possible inputs. We compute a collection of such points, one per sample, for samples from a variety of music files, to give a cloud of points in input-space. We then apply linear principal components analysis (PCA) on deviations from the mean of this cloud to identify principal modes of variation, and then explore whether these principal modes correspond to intuitive notions of similarity. We also explore whether the early principal components can be interpreted as a set of basis vectors for a a subspace in which simple distance metrics correspond to measures of musical similarity. The dimensionality of the input space is so large that neural-network approximators to PCA (Sanger, 1989) are used rather than analytic matrix-manipulation code. So far, our results (from samples of commercial recordings) are inconclusive. We are attempting to better understand the nature of our approach by working with music generated from MIDI files where we have full experimental control over various aspects of the music such as tempo, number of voices, instrument type, and so on. We are also looking to use nonlinear PCA neural networks (e.g. Fotheringhame & Baddeley, 1997) to test the possibility that our current use of linear methods represents an over-simplification. Author Information Dave Cliff Digital Media Systems Department Hewlett-Packard Labs Bristol BS34 8QZ England U.K. dave_cliff@hp.com Phone +44 117 312 8189 Heppie Freeburn Digital Media Systems Department Hewlett-Packard Labs Bristol BS34 8QZ England U.K. cf@hplb.hpl.hp.com Phone +44 117 312 8718 Suggested Readings Fotheringhame, D. and Baddeley, R. 1997. ÒNonlinear Principal components analysis of neuronal spike train dataÓ. Biological Cybernetics 77: 282-288. Leeds, 1998. University of Leeds, School of Computer Studies, Computer Vision Group. http://www.scs.leeds.ac.uk/imv/ Sanger, T.D. 1989. ÒOptimal unsupervised learning in a single-layer linear feedforward neural networkÓ. Neural Networks, 2:459-473. Wolfson 1999. Wolfson Medical Imaging Unit, University of Manchester. http://www.wiau.man.ac.uk/research/Flexible_Models/index.html",
        "zenodo_id": 1416572,
        "dblp_key": "conf/ismir/CliffF00",
        "keywords": [
            "Similarity-based classification",
            "Multivariate statistical techniques",
            "Polyphonic music",
            "Digital audio files",
            "High-order point distribution models",
            "Neural networks",
            "Ultra-high dimensional space",
            "Spectrograms",
            "Principal components analysis",
            "MIDI files"
        ],
        "content": "Exploration of Point-Distribution Models for Similarity-based\nClassification and Indexing of Polyphonic Music.\nAbstract\nSimilarity is an intuitive criterion for indexing and classification  of digital audio files in music\ninformation retrieval systems. While significant work has been done on similarity-based\napproaches to monophonic music, methods for reliably dealing with databases of arbitrarypolyphonic music remain elusive. In this paper we describe our ongoing research in exploring the\nuse of high-order multivariate statistical techniques for similarity-based classification of\npolyphonic music in digital audio files. The statistical techniques we employ, known as point\ndistribution models  (PDMs), have recently proven to be of surprising value in computer vision\nresearch for rating visual similarity; here we are attempting to apply PDMs to musical similarity.\nThis involves creating neural networks that approximate the statistical processing, to save onpotentially explosive storage and processor requirements. This paper reports on work in progress:our results to date are inconclusive and somewhat negative. We describe our rationale for\nexploring PDMs in polyphonic music similarity-rating and discuss the problems we have\nencountered so far, with the intention of encouraging other members of the music informationretrieval community to explore this and related approaches.\nOverview\nThe approach we are currently exploring is, in essence, to perform sophisticated statistical\nanalysis on the results of applying standard signal processing techniques to digital audio\npolyphonic music data. The key difference between our approach and much prior research is thatwe are explicitly rejecting the need for an intermediate symbolic representation (such as audio-to-\nMIDI) of the content of digital-audio music files.\nRather, we view individual digital audio files as individual data-points in an ultra-high\ndimensional space (the space of all possible digital audio files for a given duration, sampling rate,\nbit depth, and number of channels).  We are exploring the use of multivariate statistical analysis\ntechniques to find transformations that map from this input space (which could have manymillions of dimensions) onto new spaces with several orders of magnitude fewer dimensions,\nwhere the new spaces have similarity-like distance metrics and classification-like partitions.\nThis might seem like a hopelessly nave approach, had not a similar approach recently been\nshown to be a successful foundation for similarity-based indexing in other ultra-high-dimensional\nspaces: namely those found in computer vision.\nIn the past decade, new statistical approaches have been developed for computer vision in\nmedical imaging (Wolfson, 1999) and automated surveillance of pedestrians (Leeds, 1998). In\nessence, these new statistical methods involve computing a characterization of the mean shape,along with a characterization of the primary modes of variation  in the shape (i.e., the main ways\nin which the samples of the shape differ from the mean of the samples). Surprisingly, these\nprimary modes often correspond to intuitive visual notions such as variation in viewing angle of aparticular object (such as a human face), or the variation seen between two classes of object (suchbetween female faces and male faces).\nThe approach we have explored in music classification is, in essence, to apply these vision\ntechniques to visual objects that are representations of portions of digital audio. The visual\nobjects are created by taking the spectrograms (amplitude surfaces over a time-frequency space)of the mono sum from fixed-duration samples of stereo CD-quality digital-audio music files and\nthen relatively-coarsely quantizing the time and frequency axes into discrete bins to give data-\ncompression in excess of 90%. The quantized spectrogram is decorated with one amplitudeÒpointÓ per time-frequency bin, and the collections of points for a particular music sample aretreated as coordinates for a single point in a high-dimensional space (the space of possible\nquantized spectrograms). For example, working with 30-second audio samples and dividing the\nspectrogram frequency axis into 1000 bins and the time axis into 50 bins per second gives1000*50*30=1.5m points, so each processed sample is a single point in a 1.5m-dimensional\nspace of possible inputs.\nWe compute a collection of such points, one per sample, for samples from a variety of music\nfiles, to give a cloud of points in input-space. We then apply linear principal components analysis\n(PCA) on deviations from the mean of this cloud to identify principal modes of variation, andthen explore whether these principal modes correspond to intuitive notions of similarity. We alsoexplore whether the early principal components can be interpreted as a set of basis vectors for a a\nsubspace in which simple distance metrics correspond to measures of musical similarity.\nThe dimensionality of the input space is so large that neural-network approximators to PCA\n(Sanger, 1989) are used rather than analytic matrix-manipulation code. So far, our results (from\nsamples of commercial recordings) are inconclusive. We are attempting to better understand thenature of our approach by working with music generated from MIDI files where we have full\nexperimental control over various aspects of the music such as tempo, number of voices,\ninstrument type, and so on. We are also looking to use nonlinear PCA neural networks (e.g.Fotheringhame & Baddeley, 1997) to test the possibility that our current use of linear methodsrepresents an over-simplification.\nAuthor Information\nDave Cliff\nDigital Media Systems Department\nHewlett-Packard Labs\nBristol BS34 8QZ\nEngland U.K.\ndave_cliff@hp.com\nPhone +44 117 312 8189Heppie Freeburn\nDigital Media Systems Department\nHewlett-Packard Labs\nBristol BS34 8QZ\nEngland U.K.\ncf@hplb.hpl.hp.com\nPhone +44 117 312 8718\nSuggested Readings\nFotheringhame, D. and Baddeley, R. 1997. ÒNonlinear Principal components analysis of neuronal spiketrain dataÓ. Biological Cybernetics  77: 282-288.\nLeeds, 1998. University of Leeds, School of Computer Studies, Computer Vision Group.\nhttp://www.scs.leeds.ac.uk/imv/\nSanger, T.D. 1989. ÒOptimal unsupervised learning in a single-layer linear feedforward neural networkÓ.Neural Networks , 2:459-473.\nWolfson 1999. Wolfson Medical Imaging Unit, University of Manchester.\nhttp://www.wiau.man.ac.uk/research/Flexible_Models/index.html"
    },
    {
        "title": "Audio Information Retrieval (AIR) Tools.",
        "author": [
            "Perry R. Cook",
            "George Tzanetakis"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416716",
        "url": "https://doi.org/10.5281/zenodo.1416716",
        "ee": "https://zenodo.org/records/1416716/files/CookT00.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416716,
        "dblp_key": "conf/ismir/CookT00"
    },
    {
        "title": "Finding Motifs with Gaps.",
        "author": [
            "Maxime Crochemore",
            "Costas S. Iliopoulos",
            "Yoan J. Pinzón"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415986",
        "url": "https://doi.org/10.5281/zenodo.1415986",
        "ee": "https://zenodo.org/records/1415986/files/CrochemoreIP00.pdf",
        "abstract": "This paper focuses on a set of string pattern-matching problems that arise in musical analysis, and especially in musical information retrieval. A musical score can be viewed as a string: at a very rudimentary level, the alphabet could simply be the set of notes in the chromatic or diatonic notation, or the set of intervals that appear between notes (e.g. pitch may be represented as MIDI numbers and pitch intervals as number of semitones). An important example of flexibility required in score searching arises from the nature of polyphonic music. Within a certain time span each of the simultaneously-performed voices in a musical composition does not, typically, contain the same number of notes. So ‘melodic events’ occurring in one voice may be separated from their neighbours in a score by intervening events in other voices. Since we cannot generally rely on voice information being present in the score we need to allow for temporal ‘gaps’ between events in the matched pattern. Typically, the magnitude of such a gap (which might be expressed as a maximum time value, or, more probably, as a maximum number of skipped event-time-slots) will be a parameter set by the user. In our mathematical treatment the allowance for gaps in the query and the score being searched is represented by the constant α. Fig. 1 shows a short example from a musical score in monophonic format in which we attempt to match a pattern y (also known as the ‘query’) within a music score t (that we will call the ‘text’). This pattern can only be matched by allowing gaps of up to two spaces between pitches in the pattern; Note that the matching of the pattern to the score can be actually ‘approximate’ (see Cambouropolos et al 1999), in that the difference between the pitches and the sequence of musical events could be bounded by a constant δ (for simplicity we set δ to be zero in this example). We can see that there is an occurrence of the pattern in the text, starting at position three because y1, y2, y3, y4 and y5 matches exactly x3, x5, x8, x9 and x11 , respectively,  with a sequence of gaps G=(g1=1, g2=2, g3=0, g4=1). g1  is the number of spaces between the first two matched pitches in the text (i.e. between (x3=8) and (x5=3)), g2  is the number of spaces between the second and the third matched pitches in the text, and son on. Clearly, this is a valid match because all the gaps are less than or equal to two, which was the given gap restriction. If we want to find matches with a gap of up to one, then this match won’t be a valid one. Fig. 2 shows a similar example but for δ=1 so that the matched pitches don’t need to be exact, an ‘error’ of up to 1 is now allowed. Therefore, the first pitch in the pattern (8) matches the third pitch in the text (7) because |8-7|=1 and that is less or equal to our allowed error of one. The second pith in the pattern (3) matches the fifth pith in the text (4) and so on. The sequence of gaps remains exactly the same. The problem of matching with gaps, can be formally defined as follows: given a musical sequence x (call the ‘text’) and a motif y (call the ‘pattern’) find all occurrences of y in x such that yi = xji ∀ i ∈ {1..m}, where m is the length of y. Note that y occurs at position j1 of x with a gap sequence G=(g1, g2, …, gm-1), where gi =|ji - ji+1-1| ∀ i ∈ {1..m-1}. We will consider this problem under a variety of conditions: the motif matching can be either exact or approximate. The gaps can be bounded, unbounded or all the same length. We have design efficient algorithms and implementations of all the above variants. 8 3 2 3 7 4 6 8 1 3 3 5 8 2 3 5 7 1 y : pattern x : text j g1 g2 g4 g3 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 m=5 n=12 Figure 1 (displayed a δ-occurrence of y with α-bounded gaps, for δ=0 and α=2.) 8 3 2 3 7 4 6 7 1 4 3 5 8 2 4 5 6 1 y : pattern x : text j g1 g2 g4 g3 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 m=5 n=12 Figure 2 (displayed a δ-occurrence of y with α-bounded gaps, for δ=1 and α=2.) Author Information Maxime Crochemore Institut Gaspard-Monge, Laboratoire d'informatique, Université de Marne-la-Vallée mac@univ-mlv.fr www-igm.univ-mlv.fr/~mac Wojciech Rytter Uniwersytet Warszawski, Banacha 2, 02--097, Warszawa, Poland, and Department of Computer Science, University of Liverpool, Liverpool L69 7ZF, UK. rytter@csc.liv.ac.uk www.csc.liv.ac.uk/~rytter Costas S. Iliopoulos and Yoan J. Pinzon Dept. Computer Science, King's College London, London WC2R 2LS, UK, and School of Computing, Curtin University of Technology, GPO Box 1987 U, WA. Australia {csi,pinzon}@dcs.kcl.ac.uk www.dcs.kcl.ac.uk/staff/csi, www.dcs.kcl.ac.uk/pg/pinzon Suggested Readings T. Crawford, C. S. Iliopoulos, R. Raman. 1998, String Matching  Techniques for Musical Similarity and Melodic Recognition,  Computing in Musicology, Vol 11: 73--100. E. Cambouropolos, M. Crochemore, C. S. Iliopoulos, L. Mouchard, Y. J. Pinzon. 1999, Algorithms for Computing Approximate Repetitions in Musical Sequences, Proceedings of the 10--th Australasian Workshop on Combinatorial Algorithms, (Eds J. Simpson and R. Raman), Curtin University Press, Vol 3: 114--128.",
        "zenodo_id": 1415986,
        "dblp_key": "conf/ismir/CrochemoreIP00",
        "content": "Finding Motifs with Gaps\nAbstract\nThis paper focuses on a set of string pattern-matching problems that arise in musical analysis, and\nespecially in musical information retrieval. A musical score can be viewed as a string: at a very\nrudimentary level, the alphabet could simply be the set of notes in the chromatic or diatonic\nnotation, or the set of intervals that appear between notes (e.g. pitch may be represented as MIDI\nnumbers and pitch intervals as number of semitones).\nAn important example of flexibility required in score searching arises from the nature of\npolyphonic music. Within a certain time span each of the simultaneously-performed voices in a\nmusical composition does not, typically, contain the same number of notes. So ‘melodic events’\noccurring in one voice may be separated from their neighbours in a score by intervening events in\nother voices. Since we cannot generally rely on voice information being present in the score we\nneed to allow for temporal ‘ gaps ’ between events in the matched pattern. Typically, the\nmagnitude of such a gap (which might be expressed as a maximum time value, or, more probably,\nas a maximum number of skipped event-time-slots) will be a parameter set by the user. In our\nmathematical treatment the allowance for gaps in the query and the score being searched is\nrepresented by the constant α.\nFig. 1 shows a short example from a musical score in monophonic format in which we attempt to\nmatch a pattern y (also known as the ‘query’) within a music score t (that we will call the ‘text’).\nThis pattern can only be matched by allowing gaps of up to two spaces between pitches in the\npattern; Note that the matching of the pattern to the score can be actually ‘approximate’ (see\nCambouropolos  et al 1999) , in that the difference between the pitches and the sequence of\nmusical events could be bounded by a constant δ (for simplicity we set δ to be zero in this\nexample). We can see that there is an occurrence of the pattern in the text, starting at position\nthree because  y1, y2, y3, y4 and y5 matches exactly x3, x5, x8, x9 and x11 , respectively,  with a\nsequence of gaps G=(g1=1, g2=2, g3=0, g4=1). g1  is the number of spaces between the first two\nmatched pitches in the text (i.e. between ( x3=8) and ( x5=3)), g2  is the number of spaces between\nthe second and the third matched pitches in the text, and son on. Clearly, this is a valid match\nbecause all the gaps are less than or equal to two, which was the given gap restriction. If we want\nto find matches with a gap of up to one, then this match won’t be a valid one. Fig. 2 shows a\nsimilar example but for δ=1 so that the matched pitches don’t need to be exact, an ‘ error ’ of up to\n1 is now allowed. Therefore, the first pitch in the pattern (8) matches the third pitch in the text (7)\nbecause |8-7|=1 and that is less or equal to our allowed error of one. The second pith in the pattern\n(3) matches the fifth pith in the text (4) and so on. The sequence of gaps remains exactly the\nsame.\nThe problem of matching with gaps, can be formally defined as follows: given a musical\nsequence x (call the ‘text’) and a motif y (call the ‘pattern’) find all occurrences of y in x such that\nyi = xji ∀ i ∈ {1..m}, where m is the length of y. Note that y occurs at position j1 of x with a gap\nsequence G=(g1, g2, …, gm-1), where gi =|ji - ji+1-1| ∀ i ∈ {1..m-1}. We will consider this problem\nunder a variety of conditions: the motif matching can be either exact or approximate. The gaps\ncan be bounded, unbounded or all the same length. We have design efficient algorithms and\nimplementations of all the above variants.83237\n46813\n35823571 y : pattern\n x : text\nj g1 g2 g4 g31 2 3 4 5 6 7 8 9 10 11 121 2 3 4 5\n m=5\n n=12\nFigure 1 (displayed a δ-occurrence of y with α-bounded gaps, for δ=0 and α=2.)\n83237\n46714\n35824561 y : pattern\n x : text\nj g1 g2 g4 g31 2 3 4 5 6 7 8 9 10 11 121 2 3 4 5\n m=5\n n=12\nFigure 2 (displayed a δ-occurrence of y with α-bounded gaps, for δ=1 and α=2.)\nAuthor Information\nMaxime Crochemore\nInstitut Gaspard-Monge, Laboratoire\nd'informatique, Université de Marne-la-Vallée\nmac@univ-mlv.fr\nwww-igm.univ-mlv.fr/~mac\nWojciech Rytter\nUniwersytet Warszawski, Banacha 2, 02--097,\n Warszawa, Poland, and Department of Computer\nScience, University of Liverpool, Liverpool L69\n7ZF, UK.\nrytter@csc.liv.ac.uk\n www.csc.liv.ac.uk/~rytterCostas S. Iliopoulos and Yoan J. Pinzon\nDept. Computer Science, King's College London,\nLondon WC2R 2LS, UK,\nand School of Computing, Curtin University of\nTechnology,\n GPO Box 1987 U, WA. Australia\n{csi,pinzon}@dcs.kcl.ac.uk\n www.dcs.kcl.ac.uk/staff/csi,\nwww.dcs.kcl.ac.uk/pg/pinzon\nSuggested Readings\nT. Crawford, C. S. Iliopoulos, R. Raman. 1998, String Matching  Techniques for Musical Similarity and\nMelodic Recognition,  Computing in Musicology, Vol 11: 73--100.\nE. Cambouropolos, M. Crochemore, C. S. Iliopoulos, L. Mouchard, Y. J. Pinzon. 1999, Algorithms for\nComputing Approximate Repetitions in Musical Sequences, Proceedings of the 10--th\nAustralasian Workshop on Combinatorial Algorithms, (Eds J. Simpson and R. Raman), Curtin\nUniversity Press, Vol 3: 114--128."
    },
    {
        "title": "Beyond VARIATIONS: Creating a Digital Music Library.",
        "author": [
            "Jon W. Dunn"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415148",
        "url": "https://doi.org/10.5281/zenodo.1415148",
        "ee": "https://zenodo.org/records/1415148/files/Dunn00.pdf",
        "abstract": "This presentation will focus primarily on work being done at Indiana University in the area of digital music libraries, with some discussion of related efforts.",
        "zenodo_id": 1415148,
        "dblp_key": "conf/ismir/Dunn00",
        "keywords": [
            "variations",
            "digital music library",
            "multimedia",
            "sound recordings",
            "music education",
            "intellectual property",
            "metadata",
            "usability",
            "content-based ir",
            "synchronization"
        ],
        "content": "Beyond VARIATIONS: Creating a Digital Music Library \nJon W. Dunn \nDigital Library Program \nIndiana University \njwd@indiana.edu  \n \nAbstract \nThis presentation will focus primarily on work being done at Indiana University in the area of digital music libraries, with some discussion of related efforts.  \nIndiana University’s VARIATIONS system serves as both an early example of a working digital \nlibrary supporting music content and an early application of World Wide Web technologies to \nmusic.  Since April 1996, the system has provided online access within the William and Gayle \nCook Music Library to sound recordings from the library’s collections.  Unlike many early university-based digital library projects whose primary goals were to provide broader access to \nunique and/or archival collections, VARIATIONS has built its digital collection from standard \nmusical repertoire identified as central to the teaching mission of the Indiana University School of \nMusic. \n At present, the collection available in VARIATIONS encompasses over 6800 sound recording \ntitles, representing a broad range of musical material reflecting the curriculum of the IU School of \nMusic, including operas, songs, instrumental music, jazz, rock, and world music.  Recordings are \ndigitized at CD-quality by library staff and stored as both WAV-format “archival” copies and \nMPEG service copies for delivery to users.  Users are able to locate sound recordings either by \nbrowsing reserve lists (organized by course number, instructor, composer, and title) or by \nsearching IU’s online library catalog system, IUCAT.  Sound files are streamed to users via an IBM VideoCharger server and a customized player application. \n \nVARIATIONS has become an integral part of music library services and the instructional process \nwithin the School of Music at IU, but despite its success, VARIATIONS lacks many elements \nthat one might expect to find in a “digital music library.”  One such element is access to formats other than audio.  In fact, the very name VARIATIONS was intended in part to convey the idea \nthat musical information by its nature comes in many different formats (including sound, \nnotation, time-based information, text, video) and that access to all of these formats in an \nintegrated fashion is a desirable goal for a digital music library system. \n Indiana University has recently embarked on a four-year project to dramatically expand upon the \ncurrent VARIATIONS system to create a Digital Music Library with funding from the National \nScience Foundation and National Endowment for the Humanities as part of the federal Digital \nLibrary Initiatives – Phase 2 (DLI2) program.  Lead Principal Investigator for the project is Dr. \nMichael A. McRobbie, Vice President for Information Technology and Professor of Philosophy and Computer Science at IU.  This project, involving faculty from music, law, and library and \ninformation science, and librarians and staff from both the library system and information \ntechnology services department, centers around three main tasks: First, the project will create a \nDigital Music Library (DML) testbed system.  Secondly, applications will be developed for music \neducation and research based upon the collections and functionality provided by the DML \ntestbed. Finally, the DML will be used as a foundation for research in the areas of instruction, \nusability, and intellectual property rights.  \nTestbed system: The testbed system created in the DML project will support not only audio but \nimages of musical scores, score notation files, time-based representations of music (such as \nMIDI), and possibly video.  The system will provide navigation, search, and retrieval functions \nincluding bibliographic search; retrieval and synchronized playback of sound recordings, MIDI \nfiles, and music notation files; access control and authentication services; and rights management services.  Synchronization and linking of multiple representations of the same musical work will \nbe enabled through development of structural metadata formats for music.  In addition to IU, \nseven institutions in the US, UK, and Japan have agreed to participate as “satellite sites” to \nexplore issues involved in remote access to the system and its collections and provide additional \nfeedback on the system’s design and development.  Testing and evaluation of access across national and international networks, including both the commodity Internet and experimental \nhigh-performance networks, will be carried out in the project. \n \nMusic instruction:  The promise of a successful digital music library in education can be \nrealized only when users are able to integrate access to digital library content—including digital \naudio, score notation, structural representations, and annotations—easily with other interface \nelements in the creation of class presentations, learning activities, and publications.  The DML project will develop an object-oriented component-based software framework to allow creation of \nsuch applications, including support for synchronization between media and visualization of \nmusical forms.  In addition, the project will evaluate the instructional impact created through use \nof the digital library in both major and non-major courses, including distance education courses. \n \nUsability: There are many different categories of users who might make use of a digital music \nlibrary system, including students, researchers, instructors, librarians, and the general public, and \ndesign of the system must take this into account.  Questions to be addressed include: How do we \noffer users control of multiple information formats in a single interface?  How do we represent \nthe DML information space to support accurate navigation and retrieval?  How do we make creation and modification of instructional tools as easy as possible?  What constitutes an \nappropriate set of measures to assess the usability of this technology?  To help answer these \nquestions, both formative and summative testing will be pursued throughout the design process.  \nParticipation groups will be formed to allow users to be engaged directly in the design of the \nsystem through input into the requirements process and participation in usability tests. \n \nIntellectual property: The creation of almost any digital library raises complex issues associated \nwith copyright and other legal forms of intellectual property, and music poses particular copyright \ncomplexities given the multiple rights holders involved (for both the underlying musical work \nand the sound recording of that work).  Research in this area will include determinations as to when fair use and other exceptions provided by copyright law may be invoked and what the most \npractical options are for securing permissions and licenses when these exceptions do not apply.  \nVarious management alternatives, both organizational and technical (authentication, access \ncontrol, rights management metadata, etc.) will be explored. \n \nContent-based IR:   While content-based music searching is not a primary focus of this project, it \nis certainly a very important part of a digital music library, and we are very interested in working with others to integrate content-based IR capabilities into the system to complement the \nmetadata-based capabilities that we are developing.\n \n Suggested Readings \nBurroughs, Michael, and Fenske, David. 1990. Variations: A Hypermedia Project Providing Integrated \nAccess to Music Information. In International Computer Music Conference Glasgow 1990: \nProceedings . \n \nDunn, Jon W., and Mayer, Constance A. 1999. VARIATIONS: A Digital Library System at Indiana \nUniversity. In DL ’99: Proceedings of the Fourth ACM Conference on Digital Libraries , 12-19. \n Digital Music Library project web site. < http://dml.indiana.edu/\n> \n \nMultimedia Music Theory Teaching Project web site. < http://theory.music.indiana.edu/mmtt/ > \n VARIATIONS project web site. < http://www.dlib.indiana.edu/variations/\n>"
    },
    {
        "title": "ARTHUR: Retrieving Orchestral Music by Long-Term Structure.",
        "author": [
            "Jonathan Foote"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416644",
        "url": "https://doi.org/10.5281/zenodo.1416644",
        "ee": "https://zenodo.org/records/1416644/files/Foote00.pdf",
        "abstract": "We introduce an audio retrieval-by-example system for orchestral music. Unlike many other approaches, this sys- tem is based on analysis of the audio waveform and does not rely on symbolic or MIDI representations. ARTHUR retrieves audio on the basis of long-term structure, specifi- cally the variation of soft and louder passages. The long- term structure is determined from envelope of audio energy versus time in one or more frequency bands. Similarity between energy profiles is calculated using dynamic pro- gramming. Given an example audio document, other docu- ments in a collection can be ranked by similarity of their energy profiles. Experiments are presented for a modest corpus that demonstrate excellent results in retrieving dif- ferent performances of the same orchestral work, given an example performance or short excerpt as a query.",
        "zenodo_id": 1416644,
        "dblp_key": "conf/ismir/Foote00",
        "keywords": [
            "audio retrieval-by-example",
            "orchestral music",
            "analysis of audio waveform",
            "long-term structure",
            "variation of soft and louder passages",
            "dynamic programming",
            "energy profiles",
            "experiments",
            "modest corpus",
            "different performances"
        ],
        "content": "1/G36/G37/G54/G55/G53/G36/G38/G55\nWe introduce an audio retrieval-by-example system for\norchestral music. Unlike many other approaches, this sys-\ntem is based on analysis of the audio waveform and does\nnot rely on symbolic or MIDI representations. A RTHUR\nretrieves audio on the basis of long-term structure, specifi-\ncally the variation of soft and louder passages. The long-\nterm structure is determined from envelope of audio energyversus time in one or more frequency bands. Similarity\nbetween energy profiles is calculated using dynamic pro-\ngramming. Given an example audio document, other docu-ments in a collection can be ranked by similarity of their\nenergy profiles. Experiments are presented for a modest\ncorpus that demonstrate excellent results in retrieving dif-ferent performances of the same orchestral work, given an\nexample performance or short excerpt as a query.\nKeywords : music retrieval, audio analysis, acoustic simi-\nlarity\n1. INTRODUCTION\nRecent years have seen an increasing interest in musicretrieval by similarity. Because of the large amount of\nmusic available on the Web, there is starting to be signifi-cant commercial interest in music retrieval. Multiple start-up companies are offering audio-based music retrieval to\ninternet users (for example, gigabeat.com  and mongo-\nmusic.com ). An intriguing business model is offered by\n“*CD” (starcd.com ), which logs radio station playlists\nusing automatic music identification. The company offers a\nservice that lets users find the artist and title of a work(and, naturally, the opportunity to purchase it) based on the\nair time and the radio station ID.\nThe structure of most music is sufficient to characterize the\nwork. As proof by example, human experts can identify\nmusic and sound by visual structure alone. Professor VictorZue of MIT teaches a course in “reading” sound spectro-\ngraphs. In a double-blind test, Arthur G.Lintgen of Phila-\ndelphia proved able to identify unlabeled classicalphonograph recordings by the softer and louder passagesvisible in the LP grooves [1,2]. His example indicates that\nthe long-term musical structure can be used for identifica-tion and retrieval. This paper presents a automatic music\nretrieval system inspired by Mr. Lintgen’s approach, and is\nthus named \nARTHUR in his honor. Like its namesake,\nARTHUR retrieves audio on the basis of long-term structure,\nspecifically the variation of soft and louder passages. The\nsystem thus works best on music that has substantial\ndynamic variation, such as works in the orchestral canon.Unfortunately, this technique is not robust for much popu-\nlar music, which generally has much less dynamic range (a\nshortcoming shared with Mr. Lintgen).\n2. PREVIOUS WORK\nMuch work in music retrieval has concentrated on sym-bolic or MIDI representations, perhaps due to the difficultyof extracting useful features from audio. Despite this, a\ngrowing number of researchers are investigating music and\naudio retrieval in the waveform domain [3]. A particularapproach to rapid audio search was done by a group atNTT [4]. In this method known audio segments were\ndetected in longer recordings by comparing histograms of\nthe power spectrum in 7 frequency bands, and/or zerocrossing rate. This method was optimized for speed, and\ncould locate signals in the presence of noise, but relied on\nthe identical signal being present in the search corpus. \nWork at Muscle Fish LLC has resulted in a audio retrieval-\nby-similarity demonstration for small audio clips\n1. Muscle\nFigure 1. Arthur G. Lintgen identifying a phono-\ngraph record by examining the groovesARTHUR: Retrieving Orchestral Music by Long-Term Structure\nJonathan Foote\nFX Palo Alto Laboratory, Inc.\n3400 Hillview Avenue\nPalo Alto, CA 94304\nfoote@pal.xerox.com2Fish’s feature set includes loudness, pitch, bandwidth and\nharmonicity [5]. A Gaussian model is constructed from\ntraining data, so that a covariance-weighted Euclidean\n(Mahalonobis) distance can be used as a measure of simi-larity. For retrieval, the distance is computed between a\ngiven sound example and all other sound examples (about\n400 in the demonstration). Sounds are ranked by distance,with the closer ones being more similar. \nWork by the author, using an entirely different approach,\nhas resulted in a similar retrieval application [6]. Here dis-\ntance measures are computed between histograms derived\nfrom a discriminatively-trained vector quantizer. A histo-gram is computed for each audio file by counting the rela-\ntive frequencies of samples in each quantization bin. If\nhistograms are considered vectors, then simple Euclideanor cosine measures can determine the similarity, and thus\nrank the audio. Unlike previous approaches, this works for\nmulticomponent audio sources such as music\n1. David Pye\nat ATT UK Research has developed another audio retrieval\nmethod using Gaussian models [7] that improves on the\nvector-quantizer approach in many respects.\n3. THE ALGORITHM\nThe retrieval algorithm is relatively straightforward. First,an “energy profile ” is computed for every audio document\nin the collection. The energy profile is a representation of\nthe average acoustic energy versus time. In the experimentsof the next section, this is determined by computing the\nRMS signal power across 1-second windows. Audio sourcefiles were obtained from CD recordings in 16-bit 44.1kHzstereo PCM format. To facilitate computation, files weremixed to mono and decimated to a 22.05 kHz sampling\nrate. An even more practical system could derive the audio\npower directly from encoded audio formats without theexpense of decoding [7]. Though the similarity calculationis reasonably robust to scale changes, energy measure-ments are normalized so the maximum value is the sameacross all audio documents. The result of the analysis is a\n1-d time series of power measurements at a rate of one per\nsecond. Figure 2 shows plots of energy versus time for twodifferent performances of Beethoven ’s Fifth Symphony .\nThough the performances are clearly different (notice thedifferent time scales), the overall energy structure of thedocuments is quite similar. This property is exploited bythe \nARTHUR system.\nOnce the energy profile is computed, it can be compared\nwith other profiles. The similarity between them is calcu-lated using dynamic programming (DP). Because DP iswell-documented in the literature [8,9,10], the algorithmicdetails will only be summarized here. The particular vari-\nant used here is often called “Discrete Time Warping ” in\nthe speech recognition literature. This was originally devel-oped for template-based speech recognition, where it helpsaccount for variations in speech timing and pronunciation.One string is aligned to the other via a lattice, with theextent of one “test” signal on the vertical axis and the other\n“reference” signal on the horizontal. Every point ( i,j) in the\nlattice corresponds to the alignment of the reference signalat time i to the test signal at time j. The DP algorithm first\n1http://www.musclefish.com\n1The reader is invited to try the demonstration at \nhttp://www.fxpal.xerox.com/people/foote/musicr/0 50 100 150 200 250 300 350 400 45000.050.10.150.2energy\n0 50 100 150 200 250 300 350 400 450 50000.050.10.150.2\ntime (s)energy\nFigure 2. Energy profiles for two different performances of the first movement of Beethoven ’s Fifth Symphony\nTop: Herbert von Karajan/Berliner Philharmoniker. Bottom: Eric Leinsdorf/Boston Symphony Orchestra.3recursively computes the best possible path through all\npoints in the lattice. At every point, the cost of extendingthe path is calculated as the minimum of the cost of the best\npath (so far), plus the cost of extending the path. The latter\ncost is simply the distance between the test and referencesignals at ( i,j), plus a penalty for insertions or deletions.\nThe latter are permitted by considering paths from neigh-\nbors not on the diagonal; in the current system paths from\nthe nearest left or bottom neighbor are permissible. A pen-alty is added to the cost to discourage excessive insertionsor deletions. Besides the cost of the best path, a pointer to\nthe previous best path is also stored at each point. Once the\nbest paths have been computed, the minimum-cost path isselected by minimizing over the last row of the lattice: this\nis the cost of the best-matching path. The actual path trajec-\ntory can be determined by backtracking using the pointerssaved during the forward computation, as in Figure 3.\nThe DP algorithm returns two results: the best alignment\npath that takes one signal into the other, and the matchingcost of that path. The last is an excellent measure of signalsimilarity: identical signals will have a diagonal best path\nand a cost of zero, while increasing differences will\nincrease the matching cost. For retrieval, the cost is used torank corpus documents by similarity to the query.\nThe DP algorithm is especially well suited to matching\nenergy profiles. Unlike simple matching (as in [4]) or cor-relation, which require relevant documents to be exact rep-licas of the query, DP accounts for differences in both the\nfeatures and the relative timing. In other words, signal\namplitudes need not match exactly, nor is it required for thevarious features to occur exactly at the same relative times.\nThus the DP algorithm smoothly matches performanceswith variable dynamics, tempos, and tempo changes. Fig-ure 3 shows the best alignment path between the twoBeethoven performances of Figure 2. Deviations from thediagonal show the relative tempo differences: overall thelonger work has a slower tempo, except at the very begin-ning where it is slightly faster. In addition, DP easily han-dles the case when query and corpus files do not start andend at exactly the same time. Allowing insertions and dele-tions accounts for any offsets at the beginning or end. Thisfeature is particularly useful for IR, because it allows que-ries to be any shorter fragment of longer works. Note thatthis application of dynamic programming is far from novel:as one of the earliest approaches to automatic speech rec-ognition, it has been in use for more than three decades[10]. However, the features used here are rather differentfrom a speech application in that they use a much longertime scale.\n4. EXPERIMENTS\nThis paper presents results using an extremely modest cor-pus of less than 100 documents. Thus the experiments hereare more to demonstrate the feasibility of the approach thanto offer any convincing retrieval results. Many aspects ofthe system could likely be improved. In particular, there isa wide space of parameters still to be explored; for examplethe time window of 1 second for the energy calculation waschosen arbitrarily, and is likely to be suboptimal.\n4.1 Experiment I: symphonic music\nThe corpus for the first experiment contained 58 docu-ments, each of which was a single track from a CD record-ing. Three versions of Brahms ’ Symphony No.3 , including\nperformances from two different conductors (Furtwanglerand Celibidache), provide the queries and relevant docu-ments for the experiment (see Table 1 for performancedetails). Other corpus documents were movements fromBeethoven ’s Fifth, Sixth, and Seventh symphonies, includ-\ning two separate performances of Beethoven ’s Fifth Sym-\nphony. A “Classics Greatest Hits ” collection provided yet a\nthird performance of the Fifth’s first movement, as well as\nfifteen of the usual classical warhorses, including the Alle-\ngro movements from Bach ’s Brandenburg Concerto No.3\nand Eine Kleine Nachtmusik , as well as an excerpt from the\nWilliam Tell Overture, and similarly well-known works .\nThe corpus was also “salted” with the nine tracks from\nPink Floyd ’s Dark Side of the Moon (to speak of classic\nwarhorses)  plus two Beatles songs and four tracks from the\nJohn Coltrane album Blue Train . The queries were chosen\nfrom the Brahms symphony as each movement has twoalternate performances that are considered relevant. Thecorpus thus contains highly relevant documents (different050100150200250300350400450050100150200250300350400\ntime (s)time (s)\nFigure 3. Dynamic-programming best-path alignment\nof energy profiles from Figure 2. Note deviation from\ndiagonal (dotted line) due to performance differences. 4performances of the same movement), moderately similar\ndocuments (different movements from the same work), dif-\nferent works in a similar genre (Beethoven and Rossini),\nand non-relevant documents from unrelated rock and jazz\ngenres (Pink Floyd and John Coltrane). As expected from\nthe mostly orchestral genre, audio documents were rather\nlong: the mean length of documents was 393 seconds with\na range of 83 to 660 seconds. For experiments I, II, and III,entire audio documents, i.e. symphonic movements, were\nused as queries. For evaluation, different performances of\nthe same movement were considered relevant, while differ-\nent movements or works were not. The three performances\nof the first movement of Beethoven ’s Fifth Symphony  were\nused to tune the retrieval algorithm, specifically the inser-\ntion/deletion penalties and the distance measure. The dis-\ntance measure used was the squared Euclidean distance,\nand the insertion/deletion penalties were set to 0.1, which\nappeared to maximize the difference in DP scores between\nthe relevant and non-relevant documents.\nFor the actual experimental evaluation, each of the three\nperformances of the four movement of Brahms ’ Third Sym-\nphony (Op.90 in F major)  was used as a query. Each of the\n58 corpus documents was then ranked by similarity to each\nof the 12 queries. For every query, the other two perfor-\nmances of the same movement ranked higher than any\nother document, thus yielding recall and precision rates of\n100% on this corpus.4.2 Experiment II: Piano concertos\nBecause the previous experiment proved suspiciously suc-\ncessful, it was desired to make the retrieval task more diffi-\ncult, if for no other reason than to provide more credible\nresults. Investigation revealed that piano music was notretrieved nearly as well as purely orchestral music. This isbecause the energy profile of piano music is highly vari-\nable between performances of the same work, even by the\nsame performer. The acoustic energy is highly sensitive toboth performance idiosyncrasies (such as use of the soste-\nnuto pedal), the acoustic environment, microphone place-\nment, recording post-production, and perhaps even the\ninstrument make. For a more challenging retrieval task, thecorpus of Experiment I was augmented with four perfor-mances of the three movements of the Beethoven  Piano\nConcerto No. 2  (Op.19 in B flat major) as well as the\nChopin Concerto No. 2  (B 43/Op. 21 in F minor). The four\nperformances of the six concerto movements resulted in aquery set of 24 documents (see Tables 2 and 3 for perfor-\nmance information). These additional documents increased\nthe overall corpus size to 82.\nDate Conductor Ensemble\n1954 Wilhelm Furtw ängler Berlin Philharmonic\n1959 Sergiu Celibidache  Italian Radio Symphony1979 Sergiu Celibidache Munich Philharmonic\nTable 1. Performances for Brahms query set\n50100 150 200 250 300 350 400 450 500 55010203040\n50 100 150 200 250 300 350 400 45010203040\nFigure 4. Spectrograms of different performances of the second movement of Beethoven ’s Piano Concerto No.2 .\nTop: Arthur Rubenstein/Erich Leinsdorf. Bottom: Levin/John Eliot Gardiner. time (s)5As expected, retrieval on this query set was gratifyingly\npoorer. Each of the 24 queries had 3 relevant documents inthe corpus, so this was chosen as the cutoff point for mea-suring retrieval precision. Thus there were 72 relevant doc-uments for this query set. For each query, documents were\nranked by DP score, and a cutoff of 3 was used. From the\n72 documents retrieved at this cutoff, 60 were relevant,giving a retrieval precision of 83%. More sophisticatedanalyses such as ROC curves, are probably not warranteddue to the small corpus size. Retrieval performance for theoriginal Brahms query set was not affected by the corpusexpansion, and remained at 100%. \n4.3 Experiment III: spectral features\nBecause the energy profile of piano music did not yield sat-isfactory performance, we attempted to improve retrievalby using features more informative than pure energy.Though many possible audio parameterizations are avail-able, a spectral representation was chosen for its simplicity.For every audio document, a long-term spectral representa-tion was computed using the Short-Time Fourier Trans-form. In the examples presented here, windows ( “frames”)\nare 1 second long. Each analysis frame is windowed with aHamming window, and a fast Fourier transform (FFT) esti-mates the spectral components in the window. The loga-\nrithm of the magnitude of the result is used as an estimate\nof the power spectrum of the windowed frame. Because thecomparatively long window has a high frequency resolu-tion, the result was linearly quantized into 40 spectralbands ranging from 0 to F\ns/4. For the 22.05 kHz data, this\nresulted in bands approximately 140 Hz wide. The result-ing vector of 40 frequency components characterizes thespectral content of each 1-second window. The sequence ofspectral vectors represents the frequency content of the sig-nal over time (often called the spectrogram). Figure 4\nshows spectrograms of two performances of the secondmovement of Beethoven ’s Piano Concerto No.2 . The\nspectrogram can be used in the dynamic programming in asimilar manner to the energy. In this case the distance mea-\nsure used is the squared Euclidean distance between spec-\ntral vectors. As in speech recognition, normalizing eachdocument by subtracting the spectral mean improvedretrieval considerably. Using spectral features resulted in69 relevant documents retrieved out of the possible 72;thus the spectral features increased the retrieval perfor-mance from 83% to 96% on the piano query set. Theretrieval performance on the original Brahms query setremained at 100% when using spectral features. \n4.4 Experiment IV: variable query length\nUsing an entire audio track as a query seems to yield rea-sonable results, at least on this admittedly miniscule cor-pus. However, it might be desirable to use smaller audioclips as queries, if for no other reason than to speed up thesearch time (which is proportional to the product of thelengths of the query and corpus documents). Halving thequery length reduces the overall search time by the samefactor. To this end, the last experiment investigatesretrieval accuracy as a function of query length. The que-\nries were fragments of the piano queries from Experiment\nII formed by extracting a variable-length excerpt starting(arbitrarily) 40 seconds into each document. Once again,the DP algorithm can find the best match, regardless ofwhere the clip starts or ends. Figure 5 shows the results ofthe experiment. As might be expected, longer queries per-form better, and spectral features substantially outperformpurely energetic features. Queries needed to be truncated at130 seconds so as not to exceed the length of the shortestDate Artist conductor Ensemble\n1931 Artur Rubenstein  Sir John Barbirolli London Symphony Orchestra\n1946 Artur Rubenstein William Steinberg Pittsburgh Symphony Orchestra1958 Artur Rubenstein Alfred Wallenstein Symphony of the Air1968 Artur Rubenstein Eugene Ormandy Philadelphia Orchestra\nTable 2. Piano query set: performances of Chopin ’s Concerto No.2  (B 43/Op.21 in F minor)\nDate Artist conductor Ensemble\n1956 Artur Rubenstein Josef Krips Boston Symphony Orchestra\n1967 Arthur Rubenstein Erich Leinsdorf Symphony of the Air1975 Artur Rubenstein Daniel Barenboim London Philharmonic Orchestra1996 Robert Levin John Eliot Gardiner Orchestre R évolutionnaire et Romantique\nTable 3. Piano query set: performances of Beethoven ’s Piano Concerto No. 2  (Op. 19 in B flat major)6query document; this is one reason the best results in this\nexperiment do not approach the precision achieved whenusing the full-length query documents. The non-monotonicresults are no doubt due to the small test corpus: experi-ments on larger corpora should yield smoother curves.\n5. DISCUSSION\nThese experiments are primarily a proof of concept giventhe admittedly small corpus size. There is considerablescope for improving the retrieval performance yet further.\nTuning the algorithm on a larger development corpus\nshould increase the differences between relevant and non-relevant document scores, and thus improve retrieval.Many aspects of the work here are arbitrary, such as the 1-second window size as well as the number of frequencybins, and also could be tuned. Better parameterizations\nmight include a weighted frequency distance giving more\nimportance to the middle frequencies, or even using ceps-tral features as in [7]. Obviously more evaluation on a big-ger corpus would also not go amiss. However, we hope thatthese modest experiments have shown the utility of theapproach. \n6. ACKNOWLEDGEMENTS\nThanks to Stephen Smoliar for discussions and providingmuch of the test corpus data; also to Lynn Wilcox formanuscript suggestions. The photograph in Figure 1 wastaken from reference [2], and is reproduced here under the\nFair Use provision of the Copyright Act of 1976 (17 USCS\n§107). 7. REFERENCES\n[1] Holland, Bernard. “A Man Who Sees What Others Hear. ”\nThe New York Times . p.C28, 19 November 1981\n[2]http://www.snopes.com/music/media/reader.htm\n[3] Foote, J., “An Overview of Audio Information Retrieval, ” in\nMultimedia Systems , 7(1), pp.2-11, January 1999, ACM\nPress/Springer-Verlag.\n[4] Kashino, K., Smith, G., and Murase, H., “Time-Series\nActive Search for Quick Retrieval of Audio and Video, ” in\nProc.International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP) 1999 , Phoenix, AZ. IEEE\n[5] Wold, E., Blum, T., Keislar, D., and Wheaton, J., “Classifica-\ntion, Search and Retrieval of Audio ,” in Handbook of Multi-\nmedia Computing , ed. B. Furht, pp.207-225, CRC Press,\n1999. \n[6] Foote, J. “Content-Based Retrieval of Music and Audio, ” in\nMultimedia Storage and Archiving Systems II, Proc.SPIE,\nVol. 3229, Dallas, TX.\n[7] Pye, D., “Content-based Methods for the Management of\nDigital Music, ” in Proc. International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) 2000,  vol. IV\npp. 2437, IEEE\n[8] J. Kruskal and D. Sankoff, “An Anthology of Algorithms\nand Concepts for Sequence Comparison, ” in Time Warps,\nString Edits, and Macromolecules: the Theory and Practice\nof String Comparison , eds.D. Sankoff and J. Kruskal, CSLI\nPublications, (Stanford) 1999\n[9] Rabiner, L., and Juang, B.-H., Fundamentals of Speech Rec-\nognition, Englewood Cliffs, NJ, 1993\n[10] Vintsyuk, T. K., “Speech Discrimination by Dynamic Pro-\ngramming, ” in Kibernetika  4(2), pp.81-88, Jan. 1968  0  20  40  60  80 100 120 14000.10.20.30.40.50.60.70.80.91\nquery length (s)precision (at recall = 3)energyspectral features\nFigure 5. Retrieval performance versus query length,\nfor piano query set (24 queries)."
    },
    {
        "title": "A Music Interface for Visually Impaired People in the WEDELMUSIC Environment. Design and Architecture.",
        "author": [
            "Anastasia Georgaki",
            "Spyros Raptis",
            "Stelios Bakamidis"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1417291",
        "url": "https://doi.org/10.5281/zenodo.1417291",
        "ee": "https://zenodo.org/records/1417291/files/GeorgakiRB00.pdf",
        "abstract": "integrated in the WEDELMUSIC1 environment which is under development at ILSP2. Our scope is to facilitate the access of visually impaired  persons  to musical databases (scores, audio and MIDI files) via Internet and give them the possibility  to edit  and create musical scores.",
        "zenodo_id": 1417291,
        "dblp_key": "conf/ismir/GeorgakiRB00",
        "keywords": [
            "integrated",
            "WEDELMUSIC1",
            "environment",
            "under",
            "development",
            "ILSP",
            "access",
            "visually",
            "impaired",
            "musical"
        ],
        "content": "A music interface for Visually Impaired people in the\nWEDELMUSIC environment. Design and Architecture\nAnastasia Georgaki,  Spyros Raptis , Stelios Bakamidis\nInstitute for Language and Speech Processing, Speech Technology Department,\nEpidavrou & Artemidos 6, GR - 151 25 Marousi, Athens, GREECE\nAbstract:  In this poster we present the architecture of a new music interface for blind musicians,\nintegrated in the WEDELMUSIC1 environment which is under development at ILSP2. Our scope is to\nfacilitate the access of visually impaired  persons  to musical databases (scores, audio and MIDI files)via Internet and give them the possibility  to edit  and create musical scores.\n1.Important Key issues for the design of an interface for Visually impaired\npersons\na)Accessible visualisation of the current location\nb)Accessible visualisation of searching information\nc)Visualisation of information / feedback of above results.\n2. Architecture of the interfaceThe W\nEDEL MUSIC VIP M ODULE  is part of the overall W EDEL MUSIC SYSTEM (DSI,\nUNIVSRSITY OF FLORENCE )1, and is itself divided into smaller modules and\ncomponents. The most important modules are:\n• a (normal or Braille) printer using Braille Music format and to the Braille bar.\n• Spoken Music Interpreter. The main purpose of this module is to interpret\ninformation from the W EDEL  object to a descriptive textual form. This text is then\nconverted to speech via a speech synthesiser.\nPr interPr inter\nBraille PrinterBraille Printer\nBr aille BarBr aille Bar\nAudioAudio\nMusi c fi l eMusi c fi l eBraille\nInterpreterBraille\nInterpreter\nSpoken Music\nInterpreterSpoken Music\nInterpreter\nMIDI\nInterpreterMIDI\nInterpreter\nMILLA\nInterpreterMILLA\nInterpreterTtSTtS\nScreenScreen\nGr aphi csGr aphi cs Graphics FileGraphics FileWedel\nFormatWedel\nFormat\nFig.1.  Data interpretations and target output devices.\n                    \n1 The WEDELMUSIC main project ( www.wedelmusic.org ) is a European  IST project which started in\nJanuary 2000.\n23. General components of the VIP Module\nFig.2. Layout of the system architecture\n4. The W EDEL MUSIC VIP E DITOR  User Interface Component\nFig. 3. An indicative version of the W EDEL MUSIC VIP INTERFACE\n5. Conclusions\nThrough our work, we want to give the blind musician access to the same information\navailable to the sighted musician: the unfiltered, written message of the composer\ndistributed via the new  WEDELMUSIC environment. In addition WEDELMUSIC\nwill also precipitate a renewed interest in the use and teaching of Braille and speech-\nmusic. In particular, it will help sighted music educators who do not know the Braille\nmusic system  to teach blind to be literate musicians.VIP Editor ModuleVIP Editor ModuleSpoken Music ModuleSpoken Music Module\nBraille ModuleBraille Module\nWedelToolkitWedelToolkit\nOperating SystemOperating System\n(MS-Windows 9x, NT)(MS-Windows 9x, NT)WedelM usic WedelM usic VIP ModuleVIP Module\nWedelMusic WedelMusic SystemSystemBraille Printing\nModule InterfaceBraille Printing\nModule InterfaceBraille Printer \nDriverBraille Printer \nDriver\nMicr osoft\nSpeech\nAPIMicr osoft\nSpeech\nAPIEnglish TtS Engine English TtS Engine\nFrench TtS Engine French TtS Engi ne\nItalian TtS Engine Italian TtS EngineCommercial\nScreen ReaderCommercial\nScreen Reader\nN otationN otation\nCommands\nCode Link &\nData ExchangeGUI Element\nShortcuts &Shortcuts &\nHotkeysHotkeysSynthetic Voice\nOptions\nDialog BoxSynthetic Voice\nOptions\nDialog BoxSpoken Music\n Settings\nDialog BoxSpoken Music\n Settings\nDialog BoxSpoken Music\nSettingsSpoken Music\nSettings\nBraille Interpreter\nSettings\nDialog BoxBraille Interpreter\nSettings\nDialog BoxBraille Music\nInterpreter\nSettingsBraille Music\nInterpreter\nSettings\nLanguage\nIDLanguage\nID\nTtS Scheduling\nand RoutingTtS Scheduling\nand RoutingWedelMusic\nVIP EditorWedelMusic\nVIP EditorMultilingual  TtS\nSynthetic\nVoice\nSettingsSynthetic\nVoice\nSettingsto WedelToolkit\nDocumentSettingsto WedelToolkit\nBraille PrinterSettings\nBraille Music InterpreterBraille Music InterpreterSpoken Music\nInterpreterSpoken Music\nInterpreter"
    },
    {
        "title": "Representing Music Using XML.",
        "author": [
            "Michael Good 0002"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415032",
        "url": "https://doi.org/10.5281/zenodo.1415032",
        "ee": "https://zenodo.org/records/1415032/files/Good00.pdf",
        "abstract": "Why does the world need another music representation language? Beyond MIDI describes over 20 different languages or musical codes (Selfridge-Field, 1997). Most commercial music programs have their own internal, proprietary music representation and file format. Music's complexity has led to this proliferation of languages and formats. Sequencers, notation programs, analysis tools, and retrieval tools all need musical information optimized in different ways.",
        "zenodo_id": 1415032,
        "dblp_key": "conf/ismir/Good00",
        "keywords": [
            "music representation language",
            "MIDI",
            "proliferation of languages",
            "complexity",
            "proprietary music representation",
            "internal formats",
            "Sequencers",
            "notation programs",
            "analysis tools",
            "retrieval tools"
        ],
        "content": "Representing Music Using XML \nAbstract \nWhy does the world need another music representation language? Beyond MIDI  describes over \n20 different languages or musical codes (Selfridge-Field, 1997). Most commercial music \nprograms have their own internal, proprietary music representation and file format. Music's complexity has led to this proliferation of languages and formats. Sequencers, notation programs, \nanalysis tools, and retrieval tools all need musical information optimized in different ways. \n \nYet no music interchange language has been widely adopted since MIDI. MIDI has contributed to \nenormous growth in the electronic music industry, but has many well-known limitations for notation, analysis, and retrieval. These include its lack of representation of musical concepts such \nas rests and enharmonic pitches (distinguishing Db from C#), as well as notation concepts such as \nstem direction and beaming. Other interchange formats such as NIFF and SMDL overcome these \nrestrictions, but have not been widely adopted. \n Successful interchange formats such as MIDI and HTML share a common trait that NIFF and \nSMDL lack. MIDI and HTML skillfully balance simplicity and power. They are simple enough \nfor many people to learn, and powerful enough for many real-world applications. The simplicity \nmakes it easy for software developers to implement the standards and to develop encoding tools \nfor musicians. This helps circumvent the “chicken-and-egg” problem with new formats.  \nXML (Extensible Markup Language) is a World Wide Web Consortium (W3C) recommendation \nfor representing structured data in text, designed for ease of usage over the Internet by a wide \nvariety of applications. XML is a meta-markup language that lets designers and communities \ndevelop their own representation languages for different applications. Like HTML and MIDI, it \nbalances simplicity and power in a way that has made it very attractive to software developers. \n The common base of XML technology lets developers of new languages focus on representation \nissues instead of low-level software development. All XML-based languages can be processed by \na variety of XML tools available from multiple vendors. Since XML files are text files, users of \nXML files always have generic text-based tools available as a lowest common denominator. \nXML documents are represented in Unicode, providing support for international score exchange.  \nMusicXML is an XML-based music interchange language. It represents common western musical \nnotation from the 17\nth century onwards, including both classical and popular music. The language \nis designed to be extensible to future coverage of early music and less standard 20th and 21st \ncentury scores. Non-western musical notations would use a separate XML language. As an interchange language, it is designed to be sufficient, not optimal, for diverse musical applications. \nMusicXML is not intended to supersede other languages that are optimized for specific musical \napplications, but to support sharing of musical data between applications. \n \nThe current MusicXML software runs on Windows. As of September 2000, it reads 100% of the MuseData format plus portions of NIFF and Finale’s Enigma Transportable Files (ETF). It writes \nto Standard MIDI Files in Format 1, MuseData files, and Sibelius. The NIFF, ETF, and MIDI \nconverters use XML versions of these languages as intermediate structures. MusicXML is \ndefined using an XML Document Type Definition (DTD) at www.musicxml.com/xml.html\n. \nXML Schemas address some shortcomings of DTDs, but are not yet a W3C recommendation.  MusicXML adapts the MuseData and Humdrum languages to XML, adding features needed to \ncover more of 19th-21st century musical usage. These were chosen as starting points because they \nare two of the most powerful languages currently available for musical analysis and interchange. \nOne of Humdrum’s important features is its explicitly two-dimensional representation of music \nby part and by time. A hierarchical representation like XML cannot directly support this type of \nlattice structure, but programs written in XSLT (Extensible Style Language Transformations) support automatic conversion between these two orderings. \n \nMusicXML score files do not represent presentation concepts such as pages and systems. The \ndetails of formatting will change based on different paper and display sizes. In the XML \nenvironment, formatting is handled separately from structure and semantics. The same applies for detailed interpretive performance information. \n \nOne limitation to computer-based musical analysis and retrieval has been the tight coupling of \nrepresentations to development tools (e.g. Humdrum requires Unix familiarity; MuseData tools \nrequire TenX). In contrast, XML programming tools are available for all major industry \nprogramming languages and platforms. This lets the user rather than the representation language \nchoose the programming environment, making for simpler development of musical applications.  \nSay we want to investigate whether Bach’s pieces really have 90% of its notes in one of two \ndurations—e.g., quarters and eighths, or eighths and sixteenths. We can do this by plotting a \ndistribution of note durations on a bar chart, displayed together with a simple spreadsheet. (The \nfull poster includes a picture.) Writing this program in Visual Basic took only half a day, including learning to use the display controls. In the 2\nnd movement of Bach’s Cantata No. 6, for \nexample, the top two note durations make up nearly 87% of the notes, a more uneven distribution \nthan often seen with other composers. For retrieval purposes, an extended program could then \nlook for the works in a given corpus with the most uneven distribution of note durations. \n Music information retrieval faces a tower-of-Babel problem. There is no representation language \nin widespread use today that overcomes MIDI's limitations for music interchange. Past efforts \nsuffered from the overall absence of popular standardized formats for complex structured data. \nXML provides the technical foundation for a more powerful and expressive music interchange \nlanguage. Developing converters between existing formats and a single music XML language could greatly simplify the tasks of music information retrieval. MusicXML attempts to provide an \ninterchange language that is well designed from musical, human, and computer perspectives. \nAuthor Information \nMichael Good \nRecordare \ngood@recordare.com  \nwww.recordare.com  \nCopyright © 2000 Michael Good \nSuggested Readings \n Castan, Gerd. Notation site. http://www.s-line.de/homepages/gerd_castan/compmus/index_e.html\n. \n Harold, Elliotte Rusty. 1999. XML Bible . Foster City, CA: IDG Books. http://metalab.unc.edu/xml\n/. \n Selfridge-Field, Eleanor (ed.). 1997. Beyond MIDI: The Handbook of Musical Codes.  Cambridge, MA: \nMIT Press. http://www.ccarh.org/publications/\n. Includes MuseData, Humdrum, NIFF, SMDL, and MIDI."
    },
    {
        "title": "Towards Instrument Segmentation for Music Content Description: a Critical Review of Instrument Classification Techniques.",
        "author": [
            "Perfecto Herrera-Boyer",
            "Xavier Amatriain",
            "Eloi Batlle",
            "Xavier Serra"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416768",
        "url": "https://doi.org/10.5281/zenodo.1416768",
        "ee": "https://zenodo.org/records/1416768/files/Herrera-BoyerABS00.pdf",
        "abstract": "A system capable of describing the musical content of any kind of sound file or sound stream, as it is supposed to be done in MPEG7-compliant applications, should provide an account of the different moments where a certain instrument can be listened to. In this paper we concentrate on reviewing the different techniques that have been so far proposed for automatic classification of musical instruments. As most of the techniques to be discussed are usable only in 'solo' performances we will evaluate their applicability to the more complex case of describing sound mixes. We conclude this survey discussing the necessity of developing new strategies for classifying sound mixes without a priori separation of sound sources.",
        "zenodo_id": 1416768,
        "dblp_key": "conf/ismir/Herrera-BoyerABS00",
        "keywords": [
            "automatic classification",
            "musical instruments",
            "sound mixes",
            "MPEG7-compliant applications",
            "solo performances",
            "sound streams",
            "different moments",
            "instrumental listening",
            "new strategies",
            "sound source separation"
        ],
        "content": "Towards instrument segmentation for music content description: a\ncritical review of instrument classification techniques\nPerfecto Herrera, Xavier Amatriain, Eloi Batlle, Xavier Serra\nAudiovisual Institute - Pompeu Fabra University\nRambla 31, 08002 Barcelona, Spain\n{perfecto.herrera, xavier.amatriain, eloi.batlle, xavier.serra}@iua.upf.es\nA system capable of describing the musical content of any kind of sound file or sound stream, as it is\nsupposed to be done in MPEG7-compliant applications, should provide an account of the different\nmoments where a certain instrument can be listened to . In this paper we concentrate on reviewing the\ndifferent techniques that have been so far proposed for automatic classification of musical instruments . As\nmost of the techniques to be discussed are usable only in \"solo\" performances we will evaluate their\napplicability to the more complex case of describing sound mixes. We conclude this survey discussing the\nnecessity of developing new strategies for classifying sound mixes without a priori separation of sound\nsources.\nKeywords: classification, timbre models, segmentation, music content processing, multimedia content\ndescription, MPEG-7\nIntroduction\nThe need for automatically classifying sounds1 arises in contexts as different as bioacoustics or military\nsurveillance. Our focus, anyway, will be that of multimedia content description, where segmentation of\nmusical audio streams can be done in terms of the instruments that can be listened to (for example in\norder to locate a “solo” in the middle of a song). Two main different objectives can be envisioned in this\ncontext:\n• segmentation according to the played instrument, where culturally accepted labels for all the\nclasses have to be associated with certain feature vectors (hence it is a clear example of\nsupervised learning problem);\n• segmentation according to perceptual features, where there are no universal labels for classifying\nsegments but similarity distance functions derived from psychoacoustical studies on what humans\nintend as “ timbral similarity” [1;2;3;4] .\nThe first point will be the subject of this paper, whereas the second one has been partially pursued in one\nof our recent contributions to the MPEG-7 process [5].\nAlthough a blind or completely bottom-up approach could be feasible for tackling the problem, we can\nassume that some additional meta-information (e.g. title of the piece, composer, players…) will be\navailable in the moment of performing the classification, because these and other metadata are expected\nto be part of the MPEG7 standard that should be approved by the end of 2001 [6]. Descriptions compliant\nwith that standard will include, alongside all those textual metadata, other structural, semantic and\ntemporal data about the instruments or sound sources that are being played in a specific moment, the\nnotes/chords/scales they are playing, or the types of expressive musical resources (e.g. vibrato,\nsforzando…) used by the players. Extracting all those non-textual data by hand is an overwhelming task\n                                                          1 The construction of a classification procedure from a set of data for which the true classes are known has also been\nvariously termed pattern recognitio n, discriminatio n, or supervised learning (in order to distinguish it from\nunsupervised learning or clustering in which the classes are inferred from the data) [55]. The aim of supervised\nlearning is to derive, from correctly classified cases, a rule whereby we can classify a new observation into one of\nthe existing classes.and therefore automatic procedures have to be found to perform what has been called the “signal-to-\nsymbol transformation” [7].\nInstrument segmentation of complex mixtures of signals is still far from being solved (but see [8], [9],\n[10] for different approaches). Therefore, one preliminary way of overriding the obnoxious stage of\nseparating components is reducing the scope of the classification systems to only deal with isolated\nsounds. There is an obvious tradeoff in endorsing this strategy: we gain simplicity and tractability, but we\nlose contextual and time-dependent cues that can be exploited as relevant features for classifying the\nsounds. As this has been the preferred strategy in the current literature on instrument classification, this\npaper will concentrate on them. A review of those studies would not be complete without discussing the\nfeatures used for classification, but space constraints have prevented us of including it here.\nClassification of monophonic sounds\nK-Nearest Neighbors\nThe K-Nearest Neighbors algorithm is one of the most popular algorithms for instance-based learning. It\nfirst stores the feature vectors of all the training examples and then, for classifying a new instance, it finds\n(usually using an Euclidean distance) a set of k nearest training examples in the feature space, and assigns\nthe new example to the class that has more examples in the set. Although it is an easy algorithm to\nimplement, the K-NN technique has several drawbacks: as it is a lazy algorithm [11] it does not provide a\ngeneralization mechanism (because it is only based on local information), it requires having in memory\nall the training instances, it is highly sensitive to irrelevant features (as they can dominate the distance\nmetrics), and it may require a significant load of computation each time a new query is done.\nA K-NN algorithm classified 4 instruments almost with complete accuracy in [12]. Unfortunately, they\nused a small database (with restricted note range to one octave, although including different dynamics),\nand conclusions should be taken with caution, moreover if we consider the following more thoroughful\nworks.\nMartin and Kim [13] (but also see [14]) developed a classification system that used the K-NN with  31\nfeatures extracted from cochleagrams. The system also used a hierarchical procedure consisting on first\ndiscriminating pizzicati from continuous notes, then discriminating between different “families” (sustained\nsounds furthermore divided into strings, woodwind and brass), and finally, specifically classifying sounds\ninto instrument categories. With a database of 1023 sounds they achieved 87% of successful classifications at\nthe family level and 61% at the instrument level when no hierarchy was used. Using the hierarchical\nprocedure increased the accuracy at the instrument level to 79% but it degraded the performance at the family\nlevel (79%). Without including the hierarchical procedure performance figures were lower than the ones they\nobtained with a Bayesian classifier (see below).\n[15] used a combination of Gaussian classifier2 and k-NN for classifying 1498 samples into specific\ninstrumental families or specific instrument labels. Using an architecture very similar to Martin and\nKim’s hierarchy (sounds are first classified in broad categories and then the classification is refined inside\nthat category) they reported a success of 75% in individual instrument classification (and 94% in “family”\nclassification) . Additionally they report a small accuracy improvement by only using the best features for\neach instrument and no hierarchy at all (80%).\n                                                          2 The Gaussian classifier was only used for rough discrimination between pizzicati and sustained soundsA possible enhancement of the K-NN technique consisting on weighting each feature according to its\nrelevance for the task has been used by the Fujinaga team3 [16;17;18] [19]. In a series of three\nexperiments using over 1200 notes from 39 different timbres taken from the McGill Master Samples CD\nlibrary the success rate of 50%, observed when only the spectral shape of steady-state notes was used,\nincreased to 68% when tristimulus, attack position and features of dynamically changing spectrum\nenvelope, such as the change rate of the centroid, were added. In the most recent paper, a real-time\nversion of this system was reported.\nThe fact the best accuracy figures are around 80% and that Martin and Fujinaga have settled into similar\nfigures, can be interpreted as an estimation of the limitations of the K-NN algorithm (provided that the\nfeature selection has been optimized with genetic or other kind of techniques). Therefore, more powerful\ntechniques should be explored.\nNaive Bayesian Classifiers\nThis method4 involves a learning step in which the probabilities for the classes and the conditional\nprobabilities for a given feature and a given class are estimated, based on their frequencies over the\ntraining data. The set of these estimates corresponds to the learned hypothesis, which is formed without\nsearching, simply by counting the frequency of various data combinations within the training examples,\nand can be used then to classify each new instance.\nThis technique has been used with 18 Mel-Cepstrum Coefficients in [20]. After clustering the feature\nvectors with a K-means algorithm, a Gaussian mixture model from their means and variances was built.\nThis model was used to estimate the probabilities for a Bayessian classifier. It then classified 30 short\nsounds of oboe and sax with an accuracy rate of 85%. Martin [14] enhanced a similar Bayesian classifier\nwith context-dependent feature selection procedures, rule-one-out category decisions, beam search, and\nFisher discriminant analysis for estimating the Maximum A Priori probabilities. In [13] performance of\nthis system was better than that of a K-NN algorithm at the instrument level (71% accuracy) and\nequivalent to it at the family level (85% accuracy).\nDiscriminant Analysis\nClassification using categories or labels that have been previously defined can be done with the help of\ndiscriminant analysis , a technique that is related to multivariate analysis of variance and multiple\nregression . Discrimination analysis attempts to minimize the ratio of within-class scatter to the between-\nclass scatter and builds a definite decision region between the classes. It provides linear, quadratic or\nlogistic functions of the variables that \"best\" separate cases into two or more predefined groups, but it is\nalso useful for determining which the most discriminative features are and the most alike/different groups.\nOne possible drawback of the technique is its reduced generalization power, although Jackknife tests\n(cross-validating with leave-one-case-out) can protect against overfitting to the observed data.\nSurprisingly the only study using this technique, and not thoroughly, has been the one by Martin and\nKim. They only used LDA for estimation of the mean and variance for the gaussians of each class to be\nfed to an enhanced naive Bayesian classifier. Perhaps it is commonly assumed that the classification\nproblem is much more complex than that of a quadratic estimation, but it means taking from granted\nsomething that has not been experimentally verified, and maybe it should be done.\nFollowing this line, in a pilot study carried in our laboratory with 120 sounds from 8 classes and 3\nfamilies we have got 85% (Jackknifed: 75%) accuracy using quadratic linear discriminant functions in\n                                                          3 The feature relevance was determined with a genetic algorithm4 Here naive means that it assumes feature independencetwo steps (sounds are first assigned to family, and then they are specifically classified). Given that the\nfeatures we used were not optimized for segmentation but for searching by similarity, we expect to be\nable to get still better results when we include other valuable features.\nBinary trees\nBinary trees , in different formulations, are pervasively used for different machine learning and\nclassification tasks. They are constructed top-down, beginning with the feature that seems to be the most\ninformative one, that is, the one that maximally reduces entropy. Branches are then created from each one\nof the different values of this descriptor (in the case of non binary valued descriptors a procedure for\ndichotomic partition of the value range must be defined). The training examples are sorted to the\nappropriate descendant node, and the entire process is then repeated recursively using the examples of one\nof the descendant nodes, then with the other. Once the tree has been built, it can be pruned to avoid\noverfitting and to remove secondary features. Although building a binary tree is a recursive procedure, it\nis anyway faster than the training of a neural network.\nBinary trees are best suited for approximating discrete-valued target functions but they can be adapted to\nreal-valued features as Jensen’s binary decision tree [21], which exemplifies their application to\ninstrument classification. In his system the trees are constructed by asking a large number of questions\n(e.g. “attack time longer than 60 ms?”), then, for each question, data are split into two groups, goodness\nof split (average entropy) is calculated and finally the question that renders the best goodness is chosen.\nOnce the tree has been built using the learning set, it can be used for classifying new sounds (each leaf\ncorresponds to one specific class) but also for making explicit rules about which features better\ndiscriminate an instrument from another. Unfortunately results regarding the classification of new sounds\nhave not yet been published (but see Jensen’s thesis [22] for an attempt on log-likelihood classification\nfunctions).\nAn application of the C4.5 algorithm [23] can be found in [24], where a database of 18 classes and 62\nfeatures was classified with accuracy rates between 64% and 68% depending on the test procedure.\nA final example of a binary tree for audio classification, although not specifically tested with musical\nsounds, is that of Foote [25]. His tree-based supervised vector quantization with maximization of mutual\ninformation uses frame-by-frame 12 Mel-cepstral coefficients plus energy for partitioning the feature\nspace into a number of discrete regions. Each split decision in the tree involves comparing one element of\nthe vector with a fixed threshold, that is chosen to maximize the mutual information between the data and\nthe associated labels that indicate the class of each datum. Once the tree is built, it can be used as a\nclassifier by computing histograms of frequencies of classes in each leaf of the tree and using distance\nmeasures between histogram templates derived from the training data and the resulting histogram for the\ntest sound.\nSupport Vector Machines\nSVMs are a very recently developed technique that is based on statistical learning theory [26]. The basic\ntraining principle behind SVMs is finding the optimal linear hyperplane such that the expected\nclassification error for unseen test samples is minimized (i.e . they look for  good generalization\nperformance). According to the structural risk minimization inductive principle, a function that classifies\nthe training data accurately and which belongs to a set of functions with the lowest complexity will\ngeneralize best regardless of the dimensionality of the input space. Based on this principle, a linear SVM\nuses a systematic approach to find a linear function with the lowest complexity. For linearly non-\nseparable data, SVMs can (nonlinearly) map the input to a high dimensional feature space where a linear\nhyperplane can be found. Although there is no guarantee that a linear solution will always exist in the\nhigh dimensional space, in practice it is quite feasible to construct a working solution. In sum, training a\nSVM is equivalent to solving a quadratic programming with linear constraints and as many variables asdata points. A SVM was used in [27] for the classification of eight solo instruments playing musical\nscores from well-known composers. The best accuracy rate was a 70% using 16 MCCs and sound\nsegments that were 0.2 seconds long. When she attempted classification on longer segments an\nimprovement was observed (83%) although there were two instruments very difficult to classify\n(trombone and harpsichord). Another worth-mentioning feature of this study is the use of truly\nindependent sets for the learning and for the test sets (and they were mainly “solo” phrases from\ncommercial recordings).\nArtificial Neural Networks\nA very simple feedforward network with a backpropagation training algorithm was used, along with K-NN,\nin [12]. The network (a 3/5/4 architecture) learnt to classify sounds from 4 very different instruments (piano,\nmarimba, accordion and guitar) with a high accuracy (best figure 97%), although slightly better figures were\nobtained using the simplest K-NN algorithm (see above).\nA comparison between a multilayer perceptron, a time-delay network, and a hybrid self-organizing\nnetwork/radial basis function can be found in [28]. Although very high success rates were found (97% for the\nperceptron, 100% for the time-delay network, and 94% for the self-organizing network) it should be noted\nthat the experiments used only 40 sounds from 10 different classes and ranging one octave only.\nExamples of self-organizing map [29] usage can be found in [30], [31],[32], [33]. All these studies use some\nkind of auditory pre-processing for getting the features that are fed to the network, then build the map, and\nfinally compare the clustering of sounds made by the network with human subjects similarity judgments ( [1],\n[34]). From these maps and comparisons the authors advance timbral spaces to be explored, or\nconfirm/disconfirm theoretical models that explain the data. It can be seen then, that the classification we get\nfrom these kind of systems are not directly usable for instrument recognition, as they are not provided with\nany label to be learnt. Nevertheless, a mechanism for associating their output clusters to specific labels seems\nfeasible to be implemented (e.g. the radial basis function used by Cemgil, see above). The ARTMAP\narchitecture [35] eventually implements this strategy by a complex  topology: an associative memory is\nconnected with an input network that self-organizes binary input patterns, with an output network that\ndoes the same with binary and real-valued patterns, and with an orienting subsystem that may alter the\ninput processing depending on output and associative memory states. Fragoulis et al [36] successfully\nused an ARTMAP for the classification of 5 instruments with the help of only ten features (slopes of the\nfirst five partials, time delays of the first 4 partials respective to the fundamental, and high frequency\nenergy). The errors (2%) were attributed to not having taken into account different playing dynamics in\nthe training phase.\nThe most thorough study on instrument classification using neural networks is, perhaps, that of Kostek’s [37],\nalthough it has been a bit neglected in the relevant literature . Her team has carried out several studies [38]\n[39] on network architecture, training procedures, and number and type of features, although the number of\nclasses to be classified has been always too small. They have used a feedforward NN with one hidden layer,\nand their classes were trombone, bass trombone, English horn and contrabassoon, instruments with somehow\nsimilar sound. Accuracy rates use to be higher than 90%, although they vary depending on the type of\ntraining and number of descriptors.\nAlthough some ANN architectures are capable of approximate any function, and therefore neural\nnetworks are a good choice when the function to be learned is not known in advance, they have some\ndrawbacks: first of all, the computation time for the learning phase is very long, tweaking of their\nparameters can also be tedious and prohibitive, and over-fitting (excessive number of bad selected\nexamples) can degrade their generalization capabilities. On the positive side, figures coming from\navailable studies do not quite outperform other simpler algorithms but anyway neural networks mayexhibit one advantage in front of some of them: once the net has learnt, the classification decision is very\nfast (compared to K-NN or to binary trees).\nHigher Order Statistics\nWhen signals have Gaussian density distributions, we can describe them thoroughly with second order\nmeasures like the autocorrelation function or the spectrum. There are some authors who claim that musical\nsignals, as they have been generated through non-linear processes, do not fit a Gaussian distribution. In that\ncase, using higher order statistics  or polyspectra, as for example skewness of bispectrum and kurtosis of\ntrispectrum, it is possible to capture all information that could be lost if using a simpler Gaussian model. With\nthese techniques, and using a Maximum Likelihood classifier, Dubnov and his collaborators [40] have\nshowed that discrimination between 18 instruments from string, woodwind and brass families is possible\nalthough they only provide figures for a classification experiment that used generic classes of sounds (not\nmusical notes).\nRough Sets\nRough sets  [41] are a novel technique for evaluating the relevance of the features used for description and\nclassification. It has been developed in the realm of knowledge-based discovery systems and data mining\n(although similar, not to be mistaken with fuzzy sets ). In rough set theory any set of similar or\nindiscernible objects is called an elementary set and forms a basic granule of knowledge about the\nuniverse; on the other hand, the set of discernible objects are considered rough (imprecise or vague).\nVague concepts cannot be characterized in terms of information about their elements; however they may\nbe replaced by two precise concepts, respectively called the lower approximation  and the upper\napproximation  of the vague concept. The lower approximation consists of all objects that surely belong to\nthe concept whereas the upper approximation contains all objects that possibly belong to the concept. The\ndifference between both approximations is called the boundary region  of the concept. The assignment of\nan object to a set is made through a membership function that as a probabilistic flavor. Once information\nis conveniently organized into information tables this technique is used to assess the degree of vagueness\nof the concepts, the interdependency of attributes and therefore the alternatives for reducing complexity in\nthe table without reducing the information it provides. Information tables regarding cases and features can\nbe interpreted as conditional decision rules of the form IF {feature x} is observed THEN { isanYobject},\nand consequently they can be used as classifiers. An elementary but formal introduction to rough sets can\nbe found in [42]. Applications of this technique to different problems, including those of signal\nprocessing [43], alongside with discussion of software tools implementing these kinds of formalisms, are\npresented in [44]. When applied to instrument classification [45] reports accuracy rates higher than 80%\nfor classification of the same 4 instruments mentioned in the ANN’s section. The main cost of using\nrough sets is however the need for quantization of features’ values, a non-trivial issue indeed, because in\nthe previous study different results were obtained depending on the quantization method (see also [46]\nand [47]). On the other hand, when compared to neural networks or fuzzy sets rules, rough sets have\nseveral benefits: they are cheaper in terms of computational cost and the results are similar to those\nobtained with the other two techniques.\nTowards classification of sounds in more complex contexts\nAlthough we have found that there are several techniques and features which provide a high percent of\nsuccess when classifying isolated sounds, it is not clear that they can be applied directly and successfully\nto the more complex task of segmenting monophonic phrases or complex mixtures. Additionally, many of\nthem would not accomplish the requirements discussed in [14] for real-world sound-source recognition\nsystems. Instead of assuming a preliminary source separation stage that facilitates the direct applicability\nof those algorithms, we are committed with an approach of signal understanding without separation  [48].This means that with relatively simple signal-processing and pattern-classification techniques we\nelaborate judgments about the musical qualities of a signal (hence, describing content). Provided that\ndesideratum, we can enumerate some apparently useful strategies to complement the previously discussed\nmethods:\n• Content awareness  (i.e. using metadata when available): the MPEG-7 standard provides\ndescriptors that can help to partially delimitate the search space for instrument classification. For\nexample, if we know in advance that the recording is a string quartet, or a heavy-metal song,\nseveral hypotheses regarding the sounds to be found can be used for guiding the process.\n• Context awareness : contextual information can be conveyed not only from metadata, nor from\nmodels in a top-down way. It also can spread from local computations at the signal level by using\ndescriptors derived from analysis of groups of frames. Note transition analysis, for example, may\nprovide a suitable context [49].\n• Use of synchronicities and asynchronicities:  co-modulations or temporal coherence of partials\nmay be used for inferring different sources, as some CASA systems do [50;8].\n• Use of spatial cues:  in stereophonic recordings we can find systematic instrument positioning that\ncan be tracked for reducing the candidate classes.\n• Use of partial or incomplete cues : contrasting with the problems of source separation or analysis\nfor synthesis/transformation, our problem does not demand any complete characterization or\nseparation of signals and, consequently, incomplete cues might be enough exploited.\n• Use of neglected features : as for example articulations between notes, expressive features (e.g.\nvibrato, portamento) or what has been called “specificities” of instrument sounds [3].\n• Combining different subsystems : different procedures can make different estimations and errors.\nTherefore a wise combination may yield better results than figuring out what is the best or what is\ngood in each one [51] [52]. Combinations can be done at different processing stages: at the\nfeature computation (concatenating features), at the output of the classification procedures\n(combining hypothesis), or also in a serial layout where the output of one classification procedure\nis the input to another procedure (as Martin’s MAP+Fisher projection  exemplifies).\n• Use of more powerful algorithms for representing sequences of states:  Hidden Markov Models\n[53] are good candidates for representing long sequences of feature vectors that define an\ninstrument sound, as [54] have demonstrated for generic sounds.\nConclusions\nWe have discussed the most commonly used techniques for instrument classification. Although they\nprovide a decent starting point for the more realistic problem of detection and segmentation of musical\ninstruments in real-world audio, conclusive statements after performance figures can be misleading\nbecause of inherent biases in each one of the algorithms. Enhancing or tuning them for the specificities of\ndealing with realistic musical signals seems a more important task than selecting the best existing\nalgorithm. Consequently other complementary strategies should be addressed in order to achieve the kind\nof signal understanding we aim at.\nReferences\n[1] Grey, J. M., \"Multidimensional perceptual scaling of musical timbres,\" Journal of the Acoustical Society of\nAmerica , 61, pp. 1270-1277, 1977.\n[2] Krumhansl, C. L., \"Why is musical timbre so hard to understand ?,\" in Nielzenand, S. and Olsson, O. (eds.)\nStructure and perception of electroacoustic sound and music  Amsterdam: Elsevier, 1989, pp. 43-53.[3] McAdams, S., Winsberg, S., de Soete, G., and Krimphoff, J., \"Perceptual scaling of synthesized musical\ntimbres: common dimensions, specificities, and latent subject classes,\" Psychological Research , 58, pp.\n177-192, 1995.\n[4] Lakatos, S., \"A common perceptual space for harmonic and percussive timbres,\" Perception and\nPsychophysics ,  in press, 2000.\n[5] Peeters, G., McAdams, S., and Herrera, P. Instrument Sound Description in the context of MPEG-7.  Proc.\nof the ICMC 2000.\n[6] ISO/MPEG-7 . Overview of the MPEG-7 Standard . 15-6-2000. Electronic document :\nhttp://drogo.cselt.stet.it/mpeg/standards/mpeg-7/mpeg-7.htm\n[7] Green, P. D., Brown, G. J., Cooke, M. P., Crawford, M. D., and Simons, A. J. H., \"Bridging the Gap\nbetween Signals and Symbols in Speech Recognition,\" in Ainsworth, W. A. (ed.) Advances in Speech,\nHearing and Language Processing  JAI Press, 1990, pp. 149-191.\n[8] Ellis, D. P. W., \"Prediction-driven computational auditory scene analysis.\" Ph.D. thesis MIT . Cambridge,\nMA, 1996.\n[9] Bell, A. J. and Sejnowski, T. J., \"An information maximisation approach to blind separation and blind\ndeconvolution,\" Neural Computation , 7 (6), pp. 1129-1159, 1995.\n[10] Varga, A. P. and Moore, R. K. Hidden Markov Model decomposition of speech and noise.  Proc. of the\nICASSP . pp. 845-848, 1990.\n[11] Mitchell, T. M., Machine Learning  Boston, MA: McGraw-Hill, 1997.\n[12] Kaminskyj, I. and Materka, A. Automatic source identification of monophonic musical instrument sounds.\nProc. of the IEEE International Conference On Neural Networks . 1, 189-194, 1995.\n[13] Martin, K. D. and Kim, Y. E. Musical instrument identification: A pattern-recognition approach. Proc. of\nthe 136th meeting of the Acoustical Society of America . 1998.\n[14] Martin, K.  D., \"Sound-Source Recognition: A Theory and Computational Model.\" Ph.D. thesis, MIT .\nCambridge, MA, 1999.\n[15] Eronen, A. and Klapuri, A. Musical instrument recognition using cepstral coefficients and temporal\nfeatures. Proc. of the ICASSP . 2000.\n[16] Fujinaga, I., Moore, S., and Sullivan, D. S. Implementation of exemplar-based learning model for music\ncognition.  Proc. of the International Conference on Music Perception and Cognition . 171-179, 1998.\n[17] Fujinaga , I. Machine recognition of timbre using steady-state tone of acoustical musical instruments.  Proc.\nof the 1998 ICMC.  207-210, 1998.\n[18] Fraser, A. and Fujinaga, I. Toward real-time recognition of acoustic musical instruments... Proc. of the\nICMC. 175-177, 1999.\n[19] Fujinaga, I. and MacMillan, K. Realtime recognition of orchestral instruments.  Proc. of the ICMC . 2000.\n[20] Brown, J. C., \"Musical instrument identification using pattern recognition with cepstral coefficients as\nfeatures,\" Journal of the Acoustical Society of America , 105 (3), pp. 1933-1941, 1999.\n[21] Jensen, K. and Arnspang, J. Binary decission tree classification of musical sounds. Proc. of the 1999\nICMC. 1999.\n[22] Jensen, K, \"Timbre models of musical sounds.\" Ph.D. thesis University of Copenhaguen , 1999.\n[23] Quinlan, J. R., C4.5: Programs for Machine Learning  San Mateo, CA: Morgan Kaufmann, 1993.\n[24] Wieczorkowska, A. Classification of musical instrument sounds using decision trees. Proc. of the 8th\nInternational Symposium on Sound Engineering and Mastering, ISSEM'99, pp. 225-230, 1999.\n[25] Foote, J. T. A Similarity Measure for Automatic Audio Classification.  Proc. of the AAAI 1997 Spring\nSymposium on Intelligent Integration and Use of Text, Image, Video, and Audio Corpora . Stanford, 1997.\n[26] Vapnik, V. Statistical Learning Theory .  New York: Wiley. 1998.\n[27] Marques, J., \"An automatic annotation system for audio data containing music.\" BS and ME thesis . MIT.\nCambridge, MA, 1999.\n[28] Cemgil, A. T. and Gürgen, F. Classification of Musical Instrument Sounds using Neural Networks.  Proc.\nof SIU97 . 1997.\n[29] Kohonen, T., Self-Organizing Maps  Berlin: Springer-Verlag, 1995.\n[30] Feiten, B. and Günzel, S., \"Automatic indexing of a sound database using self-organizing neural nets,\"\nComputer Music Journal , 18 (3), pp. 53-65, 1994.\n[31] Cosi, P., De Poli, G., and Lauzzana, G., \"Auditory Modelling and Self-Organizing Neural Networks for\nTimbre Classification,\" Journal of New Music Research , 23, pp. 71-98, 1994.\n[32] Cosi, P., De Poli, G., and Parnadoni, P. Timbre characterization with Mel-Cepstrum and Neural Nets . Proc.\nof the 1994 ICMC, pp. 42-45, 1994.[33] Toiviainen, P., Tervaniemi, M., Louhivuori, J., Saher, M., Huotilainen, M., and Näätänen, R., \"Timbre\nSimilarity: Convergence of Neural, Behavioral, and Computational Approaches,\" Music Perception , 16 (2),\npp. 223-241, 1998.\n[34] Wessel, D., \"Timbre space as a musical control structure,\" Computer Music Journal , 3 (2), pp. 45-52, 1979.\n[35] Carpenter, G. A., Grossberg, S., and Reynolds, J. H., \"ARTMAP: Supervised real-time learning and\nclassification of nonstationary data by a self- orgnising neural network,\" Neural Networks , 4, pp. 565-588,\n1991.\n[36] Fragoulis, D. K., Avaritsiotis, J. N., and Papaodysseus, C. N. Timbre recognition of single notes using an\nARTMAP neural network. Proc. of the 6th IEEE International Conference on Electronics, Circuits and\nSystems . Paphos, Cyprus. 1999.\n[37] Kostek, B., Soft computing in acoustics: applications of neural networks, fuzzy logic and rough sets to\nmusical acoustics  Heidelberg: Physica Verlag, 1999.\n[38] Kostek, B. and Krolikowski, R., \"Application of artificial neural networks to the recognition of musical\nsounds,\" Archives of Acoustics , 22 (1), pp. 27-50, 1997.\n[39] Kostek, B. and Czyzewski , A. An approach to the automatic classification of musical sounds.  Proc. of the\nAES 108th convention . Paris. 2000.\n[40] Dubnov, S., Tishby, N., and Cohen, D., \" Polyspectra as Measures of Sound Texture and Timbre,\" Journal\nof New Music Research , vol. 26, no. 4, 1997.\n[41] Pawlak, Z., \"Rough sets,\" Journal of Computer and Information Science , 11 (5), pp. 341-356, 1982.\n[42] Pawlak, Z., \"Rough set elements,\" in Polkowski, L. and Skowron, A. (eds.) Rough Sets in Knowledge\nDiscovery  Heidelberg: Physica-Verlag, 1998.\n[43] Czyzewski, A., \"Soft processing of audio signals,\" in Polkowski, L. and Skowron, A. (eds.) Rough Sets in\nKnowledge Discovery  Heidelberg: Physica Verlag, 1998, pp. 147-165.\n[44] Polkowski, L. and Skowron, A., Rough Sets in Knowledge Discovery  Heidelberg: Physica-Verlag, 1998.\n[45] Kostek, B., \"Soft computing-based recognition of musical sounds,\" in Polkowski, L. and Skowron, A.\n(eds.) Rough Sets in Knowledge Discovery  Heidelberg: Physica-Verlag, 1998.\n[46] Kostek, B. and Wieczorkowska, A., \"Parametric representation of musical sounds,\" Archives of Acoustics ,\n22 (1), pp. 3-26, 1997.\n[47] Wieczorkowska, A., \"Rough sets as a tool for audio signal classification,\" in Ras, Z. W. and Skowron, A.\n(eds.) Foundations of Intelligent Systems: Proc. of the 11th International Symposium on Foundations of\nIntelligent Systems (ISMIS-99)  Berlin: Springer-Verlag, 1999, pp. 367-375.\n[48] Scheirer, E. D., \"Music-Listening Systems.\" Ph.D. thesis. MIT. Cambridge, MA. 2000.\n[49] Kashino, K. and Murase, H. Music recognition using note transition context. Proc. of the 1998 IEEE\nICASSP . Seattle. 1998.\n[50] Cooke, M., Modelling auditory processing and organisation  Cambridge: Cambridge University Press,\n1993.\n[51] Elder IV, J. F. and Ridgeway, G. Combining estimators to improve performance.  1999. Proc. of the 5th\nInternational Conference on Knowledge Discovery and Data Mining. 1999.\n[52] Ellis, D. P. W. Improved recognition by combining different features and different systems . To appear in\nProc. of the AVIOS-2000, San Jose, CA. May, 2000.\n[53] Rabiner , L. R. A tutorial on Hidden Markov Models and selected applications in speech recognition.  Proc.\nof the IEEE, 77, pp. 257-286. 1989.\n[54] Zhang, T. and Jay Kuo, C.-C. Heuristic approach for generic audio data segmentation and annotation. ACM\nMultimedia Conference, pp. 67-76. Orlando, FLA. 1999.\n[55] Michie, D., Spiegelhalter, D. J., and Taylor, C. C., Machine Learning, Neural and Statistical Classification.\nChichester: Ellis Horwood ;  1994."
    },
    {
        "title": "Perceptual and Cognitive Applications in Music Information Retrieval.",
        "author": [
            "David Huron"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1414794",
        "url": "https://doi.org/10.5281/zenodo.1414794",
        "ee": "https://zenodo.org/records/1414794/files/Huron00.pdf",
        "abstract": "Music librarians and cataloguers have traditionally created indexes that allow users to access musical works using standard reference information, such as the name of the composer or the title of the work. While this basic information remains important, these standard reference tags have surprisingly limited applicability in most music-related queries. Music is used for an extraordinary variety of purposes: the restaurateur seeks music that targets a certain clientele; the aerobics instructor seeks a certain tempo; the film director seeks music conveying a certain mood; an advertiser seeks a tune that is highly memorable; the physiotherapist seeks music that will motivate a patient; the truck driver seeks music that will keep him/her alert. Although there are many other uses for music, music’s preeminent functions are social and psychological. The most useful retrieval indexes are those that facilitate searching according to such social and psychological functions. Typically, such indexes will focus on stylistic, mood, and similarity information. In attempting to build such musical indexes, two general questions arise: (1) What is the best taxonomic system by which to classify moods, styles, and other musical characteristics? (2) How can we create automated systems that will reliably characterize recordings or scores? Internet-based music distribution has brought these two questions to the fore. In the case of proprietary musical databases, the second problem can be centrally managed, and perhaps addressed using manual",
        "zenodo_id": 1414794,
        "dblp_key": "conf/ismir/Huron00",
        "keywords": [
            "music information retrieval",
            "mood characterization",
            "music summarization",
            "perception",
            "cognition",
            "web crawlers",
            "digital music distribution",
            "emotional taxonomy",
            "music indexing",
            "arousal"
        ],
        "content": "Perceptual and Cognitive Applications in Music Information Retrieval\nDavid Huron\nCognitive and Systematic Musicology Laboratory\nSchool of Music\nOhio State University\nhuron.1@osu.edu\nAbstract\nMusic librarians and cataloguers have traditionally created indexes that allow users to access musical\nworks using standard reference information, such as the name of the composer or the title of the work.While this basic information remains important, these standard reference tags have surprisingly limitedapplicability in most music-related queries.\nMusic is used for an extraordinary variety of purposes: the restaurateur seeks music that targets a certain\nclientele; the aerobics instructor seeks a certain tempo; the ﬁlm director seeks music conveying a certainmood; an advertiser seeks a tune that is highly memorable; the physiotherapist seeks music that willmotivate a patient; the truck driver seeks music that will keep him/her alert. Although there are manyother uses for music, music’s preeminent functions are social and psychological. The most usefulretrieval indexes are those that facilitate searching according to such social and psychological functions.Typically, such indexes will focus on stylistic, mood, and similarity information.\nIn attempting to build such musical indexes, two general questions arise: (1) What is the best taxonomic\nsystem by which to classify moods, styles, and other musical characteristics? (2) How can we createautomated systems that will reliably characterize recordings or scores?\nInternet-based music distribution has brought these two questions to the fore. In the case of proprietary\nmusical databases, the second problem can be centrally managed, and perhaps addressed using manualmethods. However, past experience with Internet access to text documents implies that non-proprietarydecentralized indexing is likely to pr ove more popular and successful. That is, future music indexes will\nlikely resemble web-wide search engines (like Infoseek or Google) rather than closed proprietary systems(like Beatscape or Encyclopedia Britannica).\nThe problem of building musical web crawlers that traverse the web and index music-related ﬁles (such as\nMP3) should pr ove challenging. The enabling technology for such musical web crawlers will need to\ndraw extensively on research in music perception and cognition. Consider two sample problems: musicsummarization and mood characterization.\nMusic Summarization\nBefore downloading or streaming an entire work, there is great beneﬁt to hearing a brief illustrativeexcerpt — a musical equivalent of the “thumbnails” commonly used in electronic picture galleries. Notallportions of a musical work are equally representative of a giv en piece, and so the practice of extracting the\ninitial few seconds ( incipit) is not optimum for identifying or recognizing a work. Most well-known\nmusical themes or hooks do not appear within the ﬁrst ten seconds. Moreover, the optimum musicalthumbnail may consist of two or more brief passages edited into a single ﬁve-second sound bite.\nOne way of viewing this problem is as a musical equivalent of the problem of text summarization (Mani\n& Maybury, 1999). In our lab, we have initiated a research project intended to develop useful algorithmsfor automatic music summarization. We are currently building a database of listener responses indicatingthe most important segments in various musical works. Speciﬁcally, we are marking those segments in asample of sound recordings which are most prototypical (Rosch & Lloyd 1978). Salient passages tend tobe (1) easily recalled by listeners from one day to the next, and (2) generate many false-positiverecognition responses (i.e., listeners incorrectly claim to have heard the passage before, having beenexposed to other excerpts from the same work). This database will provide a test suite that allows us toassess the effectiveness of different summarization algorithms.\nMood Characterization\nThe most well-known models of mood entail two factors: valence(happy/anxious) and arousal\n(calm/energetic) (e.g., Thayer, 1989). Musical moods can be be usefully characterized using the resultingtwo-dimensional space. Exemplars of the four quadrants might be Rossini’s William Tell Overture\n(happy/energetic), Bach’s, Jesu, Joy of Man’s Desiring (happy/calm), Berg’s Lulu, (anxious/energetic),\nand the opening of Stravinsky’s Firebird (calm/anxious). Of the two factors, arousal is more\ncomputationally tractable and can be estimated using simple amplitude-based measures (e.g. Huron,1992).\nMore recent research has focussed on speciﬁc emotional connotations, such as auditory correlates of\ncuteness, fear, disgust, aggressivity, submissiveness, and pleasure (e.g., Ohala, 1980; Huron, Kinney &Precoda, MS).\nSuggested Readings\nHuron, D. 1992. The ramp archetype and the maintenance of auditory attention. Music\nPerception, 10(1)83-92.\nHuron, D., Kinney, D. & Precoda, K. (2000). Relation of pitch height to perception of\ndominance/submissiveness in musical passages. In review.\nMani, I. & Maybury, M.T. (eds.) 1999. Advances in Automatic Text Summarization. Cambridge,\nMassachusetts: MIT Press.\nOhala, J. 1980. The acoustic origin of the smile. Journal of the Acoustical Society of America, 68, S33.\nRosch, E. & Lloyd, B. (eds.) 1978. Cognition and categorization. Hillsdale, N.J.: Erlbaum.\nThayer, R.E. 1989. The Biopsychology of Mood and Arousal. New York: Oxford University Press."
    },
    {
        "title": "Subject Search for Music: Quantitative Analysis of Access Point Selection.",
        "author": [
            "Mari Itoh"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1414950",
        "url": "https://doi.org/10.5281/zenodo.1414950",
        "ee": "https://zenodo.org/records/1414950/files/Itoh00.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1414950,
        "dblp_key": "conf/ismir/Itoh00",
        "content": "Subject Search for Music:\nQuantitative Analysis of Access Point Selection\nBackground\nNon-book materials have unique characteristics in subject analysis that influence their\ninformation retrieval process.  It is obvious that online search studies intended for books are not\nappropriate for music materials.  There are not enough empirical studies based on what objects focus on\nmusic materials.  This study aims to explore useful tools for subject search of music scores in online\nenvironment.  To comprehend problems regarding the issue, the author surveyed 21,177 online catalog\nsearch logs in an academic library of music.\nSmiraglia (1989, p. 64-73) describes the characteristics of music and divided them into 4\ngroups according to the subject analysis of textual materials: 1) intellectual form; 2) topicality; 3) the\nintended audience; 4) physical form.  In the study such attributes to music are considered as subject\naspects for searching.  The elements include topic, genre, form of composition, technique or style,\nmedium of performance, physical format or version, language of text, historical style period or time.  The\nauthor examined these elements for analysis in the survey.\nPurpose\nThe purpose of this survey is to comprehend how the searchers selected a variety of access\npoints for examining what search tools are most effective and necessary for searching music, and in what\nway they should be provided, especially for an unknown item search.  The survey was carefully treated\nnot to include the data restricted by the system requirements from which they were collected, because the\npurpose of the study is not to explore problems on system designing, or user interface.  Manual analysis\nof data carried out here excludes the problems resulting in the system limitations that may differ in each\nonline system, such as misspelling correction, stemming standardization, truncation, stop word lists, and\nso on.  Instead, the researcher focused on which access point the user chose, not on how s/ he input\nterms.\nMethods\nSamples of transaction logs were taken from the OPAC system in the Library at Kunitachi\nCollege of Music in Tokyo, Japan.  The data were collected for 82 days from September 1st to December\n20th, 1999.  This is a part of a second semester that does not include an examination period and is not as\ndisorganized as at the beginning of a school year (April in Japan).  The number of queries conducted by\nlibrary users in the sample data was 39,811 of which search sessions for music search were 21,177.  The\nnumber of initial queries was 11,435.  A log for a query consisted of “ID”,”Date”,”Time”,”Terminal\nno.”,”Database no.”,”Language”,”Search field”,”Input terms”,”Number of results”,”Time taken for\nsearching (second)”.\nInitial queries of each search session were used to analyze types of access points.  The reason\nfor it is because an initial query can be considered to most express the searchers’ intention, and access\npoints of the sequential queries in the same session are the ones chosen from the results of interaction\nwith the initial search results.  Only when a user made simple mistakes, for example, inputting personal\nname as a title search, the initial access points were ignored.   These were corrected to the proper ones by\nthe researcher.  If access points of the successive queries seemed appropriate for the searcher’s intention,\nthey were taken into account.  After adequate access points were determined, subject terms were focused\nfor further analysis and examined according to the categories of music characteristics.   Finally, searches\nby subject terms after zero-hit results or information overload of initial queries were analyzed.\nResults and Discussion\nThere were two major findings in the quantitative analysis of access point selection.  First,\nmore than half of the initial searches were combinations of access points.  It became clear that searcherstend to choose more than two access points at the beginning of their search.  Second, the combinations of\nsearch types varied according to the searchers’ intentions.  Less usage of only a single access point\nmeans that its selection is not efficient enough for music search.\nAs to searches by subject terms, terms for medium of performance and genre were most\nfrequently used in initial queries as well as in secondary ones to refine initial queries.  When initial\nsearches with title/ uniform title or opus number were modified in order to raise their recall rate, subject\nterms for medium of performance were selected in secondary queries.  It was found particularly for\nunique musical instruments like marimba or electronic organ.  Searchers selected related terms of initial\ninput words to refine their initial search results instead of changing types of access points.  The narrower\nterm of the subject word used in an initial query was selected, e.g., “trombones (3)” for initial search\nterm “brass trios”.  In both cases to raise recall rate and precision rate, subject terms were used with\nvarious access points to construct search queries.  In order to specify initial queries by adding other\naccess points to them, publisher’s name, editor, edition and physical format of music were important\nelements.\nSelection of genre terms were seen when initial searches failed to find a portion of a large\nmusical work or a piece in larger collections of a certain genre.  In this case, personal name and title\nsearch was not effective enough.  Therefore, searchers selected subject terms expressing genre to expand\nthe search results.  When genre terms were used in their initial queries, they were added terms of medium\nof performance in the secondary queries to limit search results.  Also, genre terms were selected to\nmodify initial queries with words expressing general topic, for example, “death” to which adding\n“ceremonial music.”\nQuantitative analyses show that personal name or title search is not as useful in music\ninformation retrieval as it is in the textual one.  In most cases, subject terms provide more comprehensive\nsearches compared with those of title words as access points.  However, such subject terms which are\nrelated to the attributes to music can be accessible from more than one input fields in online search\nsystem.  These different choices for a particular element of music hinder effective searches.  It is closely\nrelated to the organization of bibliographic records which describe intellectual contents and physical\ncharacteristics of the bibliographic entity.  For instance, the medium of performance is accessible in title,\nsubject headings or classification numbers that are not clearly distinguished between them.  In addition, it\nis necessary to supply various types of access points for all attributes of music that were found important\nin online information retrieval for music.  Proper control of these elements may improve subject searches\nof music.  A qualitative survey must be conducted to study that which cannot be covered by the\ntransaction log analyses.  This further study will clarify the issues of the exact search purpose of each\nlibrary user and his/ her satisfaction level in search results through it in order to devise a way of effective\nsearching.\nMari Itoh\nGraduate School of Library and Information Science\nAichi Shukutoku University\n989043x@asu.aasa.ac.jp"
    },
    {
        "title": "Using a Spectral Flatness Based Feature for Audio Segmentation and Retrieval.",
        "author": [
            "Özgür Izmirli"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416438",
        "url": "https://doi.org/10.5281/zenodo.1416438",
        "ee": "https://zenodo.org/records/1416438/files/Izmirli00.pdf",
        "abstract": "A method that utilizes a spectral flatness based tonality feature for segmentation and content- based retrieval of audio is outlined. The method uses the tonality measure which is derived from the discrete bark spectrum as a means of detecting transitions between tonal and noise-like parts of the audio input. The meaning of ‘tonal’ in this context is different from the music-theoretical meaning and implies that there are dominant sinusoidal components in the spectrum, but, does not indicate that they are consonant or harmonic in any sense. Segmentation is performed by determining the times of these transitions, hence providing reference points for search purposes. Search is carried out by pivoting the query information on these reference points. The cumulative distance between the tonality pattern in successive frames of the query and the candidate sound fragments is used as a measure of similarity. In order to quantify the tonality, the input is processed as follows : the signal is sampled at 22050 Hz and a 2048-point FFT is performed on each frame using a Hanning analysis window. The window is hopped every 71 msecs. which corresponds to approximately 30 % overlap with the previous window. A pre-emphasis filter is applied to compensate for the reduced sensitivity of the human ear at low frequencies. The bark-band filter outputs are calculated from the FFT output by integration of the power spectral density within the critical bands. This is followed by a stage in which tonality determination is carried out for each critical band. The Spectral Flatness Measure (SFM) and the corresponding tonality coefficient (Johnston 1988) are used to quantify the tonal quality, i.e. how much tone-like the sound is as opposed to being noise-like. SFM is defined by the ratio of the geometric mean to the arithmetic mean of the power spectral density components in each critical band. A tonality vector is defined to be the collection of tonality coeficients for a single frame. More specifically, the tonality vector contains a tonality coeficient for each critical band. In order to perform segmentation, the tonal-to-noise and the noise-to-tonal transitions are obtained. These transitions are calculated, as a sequence bj (j is the time index), from the rate of change of the smoothed tonality vector with respect to time. The smoothing occurs due to averaging of the tonality vectors across several frames. By detecting rapid changes in the summary tonality in either direction, segmentation decisions are made. The sequence bj is an indicator for the overall tonal quality change of the input signal. Whenever tonal inputs become dominant in the input signal, for example, with the start of a vocal or solo instrument, the value of the indicator increases during the transition. That is to say, an increase in the value of b (over time) is interpreted as a transition from a noisy part to a more tonal part. Conversely, the decrease in the value of b, possibly extending to the negative extreme, indicates a transition from tonal to noisy spectra. The search for an audio fragment is done by comparing a short sequence of tonality vectors in the database with the same number of tonality vectors in the search sequence. As an exhaustive search is very costly for this purpose, a selective search that uses segments corresponding to relatively high perceptual entropy is performed. The determination of these segments is performed by the use of the b sequence. The starting times of these segments are called anchor times and are classified as either ‘tonal-to-noise’, or, ‘noise-to-tonal’. The noise-to-tonal anchor time is determined by a local maximum in b. In order for it to qualify as an anchor point, the difference between this maximum and the first minimum that precedes it must be larger than a threshold, ρnt. Similarly, the tonal-to-noise anchor is found for a minimum with a threshold, ρtn. As new audio files are added to the database they are processed to obtain the anchor times and tonality vectors for all frames. This information is stored, to be referenced later in the search phase. A query consists of a short audio fragment. Once a search is initiated, the query is processed to obtain the compatible form of information in the search database, i.e. tonality vectors and anchor times. The tonality vectors starting from the anchor times in the query are aligned with the anchor times of the audio in the database. The search is based on an objective of finding the minimum distance between the tonality variation pattern of the query and the variation pattern in fragments of the audio data in the database. The distance measure is simply the sum of differences of the corresponding tonality values between a candidate sound fragment and the search sequence. The search for a fragment that is already in the database leads to an exact match and therefore is found without error. An experiment carried out using 14 2-3 minute long pieces from pop, classical and jazz recordings showed that an exact match is always found. When a copy of the query does not exist in the database the most similar fragment is pulled out. The similarity determined by this method is based solely on the variation of tonality in the audio fragments. This means that characteristics such as pitch, loudness or consonance are not dealt with. Hence, the type of similarity found by this method, in general, is characterized by the unfolding of bark-band spectral envelopes and more specifically, reveals vocal or instrumental onset pattern resemblance and likeness of onset patterns of percussive perturbations to steady tonal sounds. The work described here explores the applicability of a tonality feature based on the Spectral Flatness Measure to segmentation and content-based retrieval for audio data. As explained above, audio fragments that are found to be similar in this categorization are similar in terms of tonal and noise characteristics spanning time and frequency. For various applications, multi-feature systems have been reported to perform successfully on arbitrary audio signals (Scheirer and Slaney 1997; Tzanetakis and Cook 1999). Correspondingly, the tonality feature described here could be combined with other features to further narrow down the ‘most similar’ list and make the resemblance perceptually more relevant. Author Information Ozgur Izmirli Center for Arts and Technology, Department of Mathematics and Computer Science, Connecticut College oizm@conncoll.edu",
        "zenodo_id": 1416438,
        "dblp_key": "conf/ismir/Izmirli00",
        "keywords": [
            "audio",
            "segmentation",
            "content-based retrieval",
            "spectral flatness",
            "tonality",
            "audio input",
            "query information",
            "reference points",
            "search sequence",
            "anchor times"
        ],
        "content": "Using a Spectral Flatness Based Feature for\nAudio Segmentation and Retrieval\nAbstract\nA method that utilizes a spectral flatness based tonality feature for segmentation and content-\nbased retrieval of audio is outlined. The method uses the tonality measure which is derived from\nthe discrete bark spectrum as a means of detecting transitions between tonal and noise-like parts\nof the audio input. The meaning of ‘tonal’ in this context is different from the music-theoretical\nmeaning and implies that there are dominant sinusoidal components in the spectrum, but, does not\nindicate that they are consonant or harmonic in any sense. Segmentation is performed by\ndetermining the times of these transitions, hence providing reference points for search purposes.\nSearch is carried out by pivoting the query information on these reference points. The cumulative\ndistance between the tonality pattern in successive frames of the query and the candidate sound\nfragments is used as a measure of similarity.\nIn order to quantify the tonality, the input is processed as follows : the signal is sampled at 22050\nHz and a 2048-point FFT is performed on each frame using a Hanning analysis window. The\nwindow is hopped every 71 msecs. which corresponds to approximately 30 % overlap with the\nprevious window. A pre-emphasis filter is applied to compensate for the reduced sensitivity of the\nhuman ear at low frequencies. The bark-band filter outputs are calculated from the FFT output by\nintegration of the power spectral density within the critical bands. This is followed by a stage in\nwhich tonality determination is carried out for each critical band. The Spectral Flatness Measure\n(SFM) and the corresponding tonality coefficient (Johnston 1988) are used to quantify the tonal\nquality, i.e. how much tone-like the sound is as opposed to being noise-like. SFM is defined by\nthe ratio of the geometric mean to the arithmetic mean of the power spectral density components\nin each critical band.\nA tonality vector is defined to be the collection of tonality coeficients for a single frame. More\nspecifically, the tonality vector contains a tonality coeficient for each critical band. In order to\nperform segmentation, the tonal-to-noise and the noise-to-tonal transitions are obtained. These\ntransitions are calculated, as a sequence bj (j is the time index), from the rate of change of the\nsmoothed tonality vector with respect to time. The smoothing occurs due to averaging of the\ntonality vectors across several frames. By detecting rapid changes in the summary tonality in\neither direction, segmentation decisions are made. The sequence  bj is an indicator for the overall\ntonal quality change of the input signal. Whenever tonal inputs become dominant in the input\nsignal, for example, with the start of a vocal or solo instrument, the value of the indicator\nincreases during the transition. That is to say, an increase in the value of b (over time) is\ninterpreted as a transition from a noisy part to a more tonal part. Conversely, the decrease in the\nvalue of b, possibly extending to the negative extreme, indicates a transition from tonal to noisy\nspectra.\nThe search for an audio fragment is done by comparing a short sequence of tonality vectors in the\ndatabase with the same number of tonality vectors in the search sequence. As an exhaustive\nsearch is very costly for this purpose, a selective search that uses segments corresponding to\nrelatively high perceptual entropy is performed. The determination of these segments is\nperformed by the use of the b sequence. The starting times of these segments are called anchor\ntimes and are classified as either ‘tonal-to-noise’, or, ‘noise-to-tonal’. The noise-to-tonal anchortime is determined by a local maximum in b. In order for it to qualify as an anchor point, the\ndifference between this maximum and the first minimum that precedes it must be larger than a\nthreshold, ρnt. Similarly, the tonal-to-noise anchor is found for a minimum with a threshold, ρtn.\nAs new audio files are added to the database they are processed to obtain the anchor times and\ntonality vectors for all frames. This information is stored, to be referenced later in the search\nphase. A query consists of a short audio fragment. Once a search is initiated, the query is\nprocessed to obtain the compatible form of information in the search database, i.e. tonality\nvectors and anchor times. The tonality vectors starting from the anchor times in the query are\naligned with the anchor times of the audio in the database. The search is based on an objective of\nfinding the minimum distance between the tonality variation pattern of the query and the variation\npattern in fragments of the audio data in the database. The distance measure is simply the sum of\ndifferences of the corresponding tonality values between a candidate sound fragment and the\nsearch sequence.\nThe search for a fragment that is already in the database leads to an exact match and therefore is\nfound without error. An experiment carried out using 14 2-3 minute long pieces from pop,\nclassical and jazz recordings showed that an exact match is always found. When a copy of the\nquery does not exist in the database the most similar fragment is pulled out. The similarity\ndetermined by this method is based solely on the variation of tonality in the audio fragments. This\nmeans that characteristics such as pitch, loudness or consonance are not dealt with. Hence, the\ntype of similarity found by this method, in general, is characterized by the unfolding of bark-band\nspectral envelopes and more specifically, reveals vocal or instrumental onset pattern resemblance\nand likeness of onset patterns of percussive perturbations to steady tonal sounds.\nThe work described here explores the applicability of a tonality feature based on the Spectral\nFlatness Measure to segmentation and content-based retrieval for audio data. As explained above,\naudio fragments that are found to be similar in this categorization are similar in terms of tonal and\nnoise characteristics spanning time and frequency. For various applications, multi-feature systems\nhave been reported to perform successfully on arbitrary audio signals (Scheirer and Slaney 1997;\nTzanetakis and Cook 1999). Correspondingly, the tonality feature described here could be\ncombined with other features to further narrow down the ‘most similar’ list and make the\nresemblance perceptually more relevant.\nAuthor Information\nOzgur Izmirli\nCenter for Arts and Technology,\nDepartment of Mathematics and Computer Science,\nConnecticut College\noizm@conncoll.edu\nReferences\nJohnston, J. D. 1988. “Transform Coding of Audio Signals Using Perceptual Noise Criteria,” IEEE Journal\nof Selected Areas in Communication, Vol. 6, pp. 314-323.\nScheirer, E. and Slaney, M. 1997. “Construction and Evaluation of a Robust Multifeature Speech/Music\nDiscriminator,” Proceedings ICASSP’97, pp.1331-1334.\nTzanetakis G. and Cook, P. 1999. “Multifeature Audio Segmentation for Browsing and Annotation,”\nProceedings IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp. 103-106."
    },
    {
        "title": "Score-based Style Recognition Using Artificial Neural Networks.",
        "author": [
            "Francis J. Kiernan"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416626",
        "url": "https://doi.org/10.5281/zenodo.1416626",
        "ee": "https://zenodo.org/records/1416626/files/Kiernan00.pdf",
        "abstract": "Overview The original idea was to develop a system for musicological analysis that was capable of assisting in the resolution of issues concerning compositional authenticity. Based on explicit rule-based interrogation of a musical score the system gathers statistical information by way of a data habitual characteristics within the composition. It must not be assumed that this system aims towards the modeling of human musical perception, as it is the authorÕs belief that score-based analyses cannot adequately meet this task. Initially the system was developed to explore the authenticity of flute compositions attributed to Frederick II ÒThe GreatÓ. Since there has long been musicological debate concerning this matter it was decided to acquire information from both performers of period instruments and musicologists concerning the characteristics and signatures required in the discrimination task. Based on free and multiple-choice reports returned a rule base was compiled that was then implemented into the system. Methodology For the purpose of this study it was decided to create a corpus of score representations in ALMA1, an antiquated and relatively forgotten format. The primary reason for this was the ease of coding without the necessity to sacrifice any of the printed score attributes. Since the system is based on intervallic difference of note relations it was not necessary to transpose the selected material to a common key-signature, which appears to be common practice in similar studies. Aside from the standard frequency of note distribution statistics, horizontal pitch-class ÒsnapshotsÓ are used in order to obtain a rough image of the tonal contents of each measure. Other data is collected from functions stemming from Lerdahl & Jackendoff (1983) theories on tonal music. In a similar manner vertical pitch-class analysis is employed, this method involves the weighting of individual events based on their metric position to obtain a single vector. Weighting of importance of the vertical pitch-class data is crucial for the perceived tonality of each measure (Cook 2000). Strong-weak interplay between parts provides major individualistic cues in the identification process. Auxiliary and passing notes in this instance are not considered since the resolution is restricted to minimal values of 16th notes. Efforts to locate modulations were developed by monitoring the frequency of note occurrence over time. Having established the initial key a scan is run over the score to track any deviation, which is assumed if recurring accidental tones match ÒexpectedÓ modulatory practices. For example when a piece initially established as being in C-major displays a recurring F# it is highly likely that a modulation to the dominant key of G-major has occurred. The sequence recognition algorithm involves the staggered parsing of a back-to-back pair of arrays checking sequences of intervals on the major metric points of the melody. Their function is to identify recurring interval sequences; chromatic, diatonic or pentatonic. The sequences being sought must be concurrent. Motivic interplay as yet cannot be detected.",
        "zenodo_id": 1416626,
        "dblp_key": "conf/ismir/Kiernan00",
        "keywords": [
            "system",
            "musical",
            "score",
            "analysis",
            "compositional",
            "authenticity",
            "rule-based",
            "statistical",
            "information",
            "data"
        ],
        "content": "Score-based style recognition using artificial neural networks.\nAbstract\nOverview\nThe original idea was to develop a system for musicological analysis that was capable of assisting\nin the resolution of issues concerning compositional authenticity. Based on explicit rule-basedinterrogation of a musical score the system gathers statistical information by way of a dataextraction engine, the subsequent neural network implicitly forms an abstract impression of\nhabitual characteristics within the composition. It must not be assumed that this system aims\ntowards the modeling of human musical perception, as it is the authorÕs belief that score-basedanalyses cannot adequately meet this task.\nInitially the system was developed to explore the authenticity of flute compositions attributed to\nFrederick II ÒThe GreatÓ. Since there has long been musicological debate concerning this matterit was decided to acquire information from both performers of period instruments and\nmusicologists concerning the characteristics and signatures required in the discrimination task.\nBased on free and multiple-choice reports returned a rule base was compiled that was thenimplemented into the system.\nMethodology\nFor the purpose of this study it was decided to create a corpus of score representations in ALMA1,\nan antiquated and relatively forgotten format. The primary reason for this was the ease of coding\nwithout the necessity to sacrifice any of the printed score attributes. Since the system is based onintervallic difference of note relations it was not necessary to transpose the selected material to a\ncommon key-signature, which appears to be common practice in similar studies.\nAside from the standard frequency of note distribution statistics, horizontal pitch-class\nÒsnapshotsÓ are used in order to obtain a rough image of the tonal contents of each measure.\nOther data is collected from functions stemming from Lerdahl & Jackendoff (1983) theories ontonal music. In a similar manner vertical pitch-class analysis is employed, this method involvesthe weighting of individual events based on their metric position to obtain a single vector.\nWeighting of importance of the vertical pitch-class data is crucial for the perceived tonality of\neach measure (Cook 2000). Strong-weak interplay between parts provides major individualisticcues in the identification process. Auxiliary and passing notes in this instance are not considered\nsince the resolution is restricted to minimal values of 16\nth notes.\nEfforts to locate modulations were developed by monitoring the frequency of note occurrence\nover time. Having established the initial key a scan is run over the score to track any deviation,\nwhich is assumed if recurring accidental tones match ÒexpectedÓ modulatory practices. Forexample when a piece initially established as being in C-major displays a recurring F# it is highlylikely that a modulation to the dominant key of G-major has occurred.\nThe sequence recognition algorithm involves the staggered parsing of a back-to-back pair of arrays\nchecking sequences of intervals on the major metric points of the melody. Their function is to identifyrecurring interval sequences; chromatic, diatonic or pentatonic. The sequences being sought must beconcurrent. Motivic interplay as yet cannot be detected.\n                                                            \n1 ALMA was developed by Murray J. Gould, as a successor of the “Plaine and Easie Code” (Gould 1963).Vector responses from the data extraction engine is fed into a neural network based on KohonenÕs\n(1996) Self-organizing map algorithm (SOM). In the learning cycle 30 compositions of known\norigin were supplied to the SOM with identification tags. The tested material was similar innature, style and structure Ð i.e. Common-time allegro movements from flute compositions by\nFrederick II, C.P.E. Bach and J.Quantz.\nThe testing cycle involved the passing of a further 6 compositions (2 by each composer) through\nthe system. In order to obtain a correlative estimate of the systemÕs response, both musicologists\nand period performers were questioned as to whom they thought composed these additional\nworks.\nResults\nHaving successfully learnt the required corpus the test material was presented to the system. Theresulting map showed that the compositions of Frederick, Quantz and Bach were located in three\ndistinctive regions, thus confirming the initial hypothesis that statistical data is sufficient in the\nidentification of individual musical characteristics. It is interesting to note that the works ofFrederick II were more closely located to those of Quantz to those of C.P.E. Bach. Thus\nsupporting historical speculation concerning musical allegiances (Helm 1969, Kiernan 1997).\nComparing the results obtained from all sources, it was found that the systemÕs accuracy exceeds\nthat of the human participant, thus prompting the continued development of such an application.\nAuthor Information\nFrancis J. Kiernan\nPythagoras Graduate School\nDepartment of Music, University of Jyvskyl, Finland\nkiernan@cc.jyu.fi\nTel: +358 (40) 75 81 777\nSuggested Readings\nCatstn, Marcus. 1997. Pitch Class Set Theory . Helsinki: Sibelius Academy.\nCook,  Norman. 2000. Explaining Harmony: The independence of interval dissonance and chordal tension.\nLecture presentation ICP2000, Sweden\nGould, Murray J. 1965. ALMA: Alpha-numeric Language for Music Analysis. Musicology and Computer\nMusicology 1966-2000.  American Musicological Soc. Symposium Proceedings.\nKohonen, Teuvo. 1997. Self-Organizing Maps . Helsinki: Springer\nîÕMaidn, Donncha. 1998. CPN-view: Technical Specification. Limerick, Ire.: Univ. of Limerick\nTodd, P. & Loy, D. 1991. Music and Connectionism . Cambridge: M.I.T. Press\nToiviainen, Petrie. 1996. Modeling Musical Cognition with Neural Networks. Jyvskyl: Univ. of\nJyvskyl"
    },
    {
        "title": "Analysis of a Contour-based Representation for Melody.",
        "author": [
            "Youngmoo E. Kim",
            "Wei Chai",
            "Ricardo García",
            "Barry Vercoe"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416760",
        "url": "https://doi.org/10.5281/zenodo.1416760",
        "ee": "https://zenodo.org/records/1416760/files/KimCGV00.pdf",
        "abstract": "Identifying a musical work from a melodic fragment is a task that most people are able to accomplish with relative ease. For some time now researchers have worked to give computers this ability as well, as it would be the cornerstone of any query-by-humming system. To accomplish this, it is reasonable to study how humans are able to perform this task, and to assess what features we use to determine melodic similarity. Research has shown that melodic contour is an important feature in determining melodic similarity, but it is also clear that rhythmic information is important as well. The goal of this research is to explore what variation of contour and rhythmic information can result in the most efficient, robust, and scalable representation for melody. We intend for this to be the basis of a query-by-humming system that will be used to test the validity of our proposed representation. The importance of melodic contour The literature suggests that a coarse melodic contour description is more important to listeners than strict intervals in determining melodic similarity. Experiments have shown that interval direction alone (i.e. the 3-level +/-/0 contour representation) is an important element of melody recognition. There is, of course, anecdotal and experimental evidence that humans use more than just interval direction (a 3-level contour) in assessing melodic similarity. In an experiment by Lindsay (1996), subjects were asked to repeat (sing) a melody that was played for them. He found that while there was some correlation between sung interval accuracy and musical experience, even musically inexperienced subjects were able to negotiate different interval sizes fairly successfully. From a practical standpoint, a 3-level representation will generally require longer queries to arrive at a unique match. Given the perceptual and practical considerations, we chose to explore finer (5- and 7-level) contour divisions for our representation. Proposed melody representation We used a triple  to represent each melody, where T is the time signature of the song, P is the pitch contour vector, and B is the beat number vector. The range of values of P vary depending on the number of levels of contour used, but follow the pattern of 0, +, -, ++, --, +++, etc. The first value of B is the location of the first note within its measure in beats (according to the time signature). Successive values of B are incremented according to the number of beats between successive notes. Values of B are quantized to the nearest whole beat. Additionally, we used a vector Q to represent different contour resolutions and quantization boundaries. The length of Q indirectly reveals the number of levels of contour being used, and the individual values of Q indicate the absolute value of the quantization boundaries (in number of half-steps). For example, Q = [0 1] represents that we quantize interval changes into three levels, 0 for no change, + for an ascending interval (a boundary at one half-step or more), and - for a descending interval. This representation is equivalent to the popular +/-/0 or U/D/R (up/down/repeat) representation. Q = [0 1 3] represents a quantization of intervals into five levels, 0 for no change, + for an ascending half-step or whole-step (1 or 2 half-steps), ++ for ascending at least a minor third (3 or more half-steps), - for a descending half-step or whole-step, and -- for a descent of at least a minor third. Thus far, we have assembled a data set of 50 multi-track MIDI files, containing a mixture of popular and classical music. The popular music selections span a variety of different countries. All selected songs had a separate monophonic melody sound track. Results In spite of anecdotal evidence, we wanted to explicitly verify the usefulness of rhythmic information in comparing melodic similarity. To test this, we used the simplest contour (3-levels, Q=[0 1]) for queries with and without the rhythmic information vector, B. Our results clearly indicate that rhythmic information allows for much shorter (and thus more efficient) queries. For the 5- and 7-level contours, we also examined a variety of quantization boundaries (different vectors Qk). Our results showed that the performance of 5-level contours are generally better than the 3-level contour, and 7-levels is better than that. For quantization vectors, we limited our search to Qk = [0 1 x …] cases only. Other values would have caused  repeated notes (no interval change) to be grouped in the same quantization level as some amount of interval change, which does not make sense perceptually. What is illuminating is that the best 5-level contour was able to equal the performance of the 7- level contour. This suggests that a 5-level contour may be an optimal tradeoff between efficiency and robustness to query variation (more levels will cause more variations in queries). Given this result, it is revealing to examine the histogram of interval occurrences in our data set. An optimal quantizer would divide the histogram into sections of equal area. This was approximately true for the Q = [0 1 3] case, which has the best performance. No interval change (0) occurs about 23% of the time. Ascending half-steps and whole-steps (+1 and +2) are about 21% of the intervals, whereas descending half- and whole-steps (-1 and -2) represent approximately 23%. Other choices for quantization boundaries clearly have less-optimal probability distributions, which is why they do not perform as well. While this result is dependant on the statistics of the data set, it is worth noting that it also correlates well with our knowledge of melody perception. Others have noted the apparent correlation of statistical independence and perceptual importance in acoustic features, which supports a theory of perception evolving from statistical efficiency. Perhaps it is not surprising that these relationships may exist in higher-level features, such as melody, as well. Some surely will argue the reverse causality: that human perception has driven the statistics of melody, resulting in a distribution of intervals that is pleasing to human perception. Either way, it is a useful relationship that perhaps has not yet been fully exploited. The statistical features of this description for melody result in an efficient representation. And since the representation correlates well with our perception of melody, the representation becomes more robust since our queries are likely to be more accurate. Author Information Youngmoo E. Kim, Wei Chai, Ricardo Garcia, Barry Vercoe Machine Listening Group MIT Media Lab {moo,chaiwei,rago,bv}@media.mit.edu Suggested Readings Lindsay, Adam T. 1996. Using contour as a mid-level representation of melody. Unpub. MS thesis. MIT Media Lab. McNab, R. J. et al. 1996. “Toward the digital music library: tune retrieval from acoustic input.” Proc. ACM Digital Libraries, Bethesda. http://www.nzdl.org. MiDiLiB, University of Bonn, http://leon.cs.uni-bonn.de/forschungprojekte/midilib/english. Themefinder™, Stanford University, http://www.ccarh. org/themefinder. TuneServer, University of Karlsruhe, http://wwwipd.ira. uka.de/tuneserver.",
        "zenodo_id": 1416760,
        "dblp_key": "conf/ismir/KimCGV00",
        "keywords": [
            "melodic contour",
            "rhythmic information",
            "query-by-humming system",
            "query-by-humming system",
            "query-by-humming system",
            "query-by-humming system",
            "query-by-humming system",
            "query-by-humming system",
            "query-by-humming system",
            "query-by-humming system"
        ],
        "content": "Analysis of a Contour-based Representation for Melody\nAbstract\nIdentifying a musical work from a melodic fragment is a task that most people are able to\naccomplish with relative ease. For some time now researchers have worked to give computers\nthis ability as well, as it would be the cornerstone of any query-by-humming  system. To\naccomplish this, it is reasonable to study how humans are able to perform this task, and to assess\nwhat features we use to determine melodic similarity. Research has shown that melodic contour is\nan important feature in determining melodic similarity, but it is also clear that rhythmic\ninformation is important as well. The goal of this research is to explore what variation of contour\nand rhythmic information can result in the most efficient, robust, and scalable representation for\nmelody. We intend for this to be the basis of a query-by-humming  system that will be used to test\nthe validity of our proposed representation.\nThe importance of melodic contour\nThe literature suggests that a coarse melodic contour description is more important to listeners\nthan strict intervals in determining melodic similarity. Experiments have shown that interval\ndirection alone (i.e. the 3-level +/-/0 contour representation) is an important element of melody\nrecognition. There is, of course, anecdotal and experimental evidence that humans use more than\njust interval direction (a 3-level contour) in assessing melodic similarity. In an experiment by\nLindsay (1996), subjects were asked to repeat (sing) a melody that was played for them. He found\nthat while there was some correlation between sung interval accuracy and musical experience,\neven musically inexperienced subjects were able to negotiate different interval sizes fairly\nsuccessfully. From a practical standpoint, a 3-level representation will generally require longer\nqueries to arrive at a unique match. Given the perceptual and practical considerations, we chose\nto explore finer (5- and 7-level) contour divisions for our representation.\nProposed melody representation\nWe used a triple < T P B > to represent each melody, where T is the time signature of the song, P is\nthe pitch contour vector, and B is the beat number vector. The range of values of P vary\ndepending on the number of levels of contour used, but follow the pattern of 0, +, -, ++, --, +++,\netc. The first value of B is the location of the first note within its measure in beats (according to\nthe time signature). Successive values of B are incremented according to the number of beats\nbetween successive notes. Values of B are quantized to the nearest whole beat. Additionally, we\nused a vector Q to represent different contour resolutions and quantization boundaries. The length\nof Q indirectly reveals the number of levels of contour being used, and the individual values of Q\nindicate the absolute value of the quantization boundaries (in number of half-steps). For example,\nQ = [0 1] represents that we quantize interval changes into three levels, 0 for no change, + for an\nascending interval (a boundary at one half-step or more), and - for a descending interval. This\nrepresentation is equivalent to the popular +/-/0 or U/D/R (up/down/repeat) representation.\nQ = [0 1 3] represents a quantization of intervals into five levels, 0 for no change, + for an\nascending half-step or whole-step (1 or 2 half-steps), ++ for ascending at least a minor third (3 or\nmore half-steps), - for a descending half-step or whole-step, and -- for a descent of at least a\nminor third. Thus far, we have assembled a data set of 50 multi-track MIDI files, containing a\nmixture of popular and classical music. The popular music selections span a variety of different\ncountries. All selected songs had a separate monophonic melody sound track.Results\nIn spite of anecdotal evidence, we wanted to explicitly verify the usefulness of rhythmic\ninformation in comparing melodic similarity. To test this, we used the simplest contour (3-levels,\nQ=[0 1]) for queries with and without the rhythmic information vector, B. Our results clearly\nindicate that rhythmic information allows for much shorter (and thus more efficient) queries. For\nthe 5- and 7-level contours, we also examined a variety of quantization boundaries (different\nvectors Q k). Our results showed that the performance of 5-level contours are generally better than\nthe 3-level contour, and 7-levels is better than that. For quantization vectors, we limited our\nsearch to Qk = [0 1 x …] cases only. Other values would have caused  repeated notes (no interval\nchange) to be grouped in the same quantization level as some amount of interval change, which\ndoes not make sense perceptually.\nWhat is illuminating is that the best 5-level contour was able to equal the performance of the 7-\nlevel contour. This suggests that a 5-level contour may be an optimal tradeoff between efficiency\nand robustness to query variation (more levels will cause more variations in queries). Given this\nresult, it is revealing to examine the histogram of interval occurrences in our data set. An optimal\nquantizer would divide the histogram into sections of equal area. This was approximately true for\nthe Q = [0 1 3] case, which has the best performance. No interval change (0) occurs about 23% of\nthe time. Ascending half-steps and whole-steps (+1 and +2) are about 21% of the intervals,\nwhereas descending half- and whole-steps (-1 and -2) represent approximately 23%. Other\nchoices for quantization boundaries clearly have less-optimal probability distributions, which is\nwhy they do not perform as well.\nWhile this result is dependant on the statistics of the data set, it is worth noting that it also\ncorrelates well with our knowledge of melody perception. Others have noted the apparent\ncorrelation of statistical independence and perceptual importance in acoustic features, which\nsupports a theory of perception evolving from statistical efficiency. Perhaps it is not surprising\nthat these relationships may exist in higher-level features, such as melody, as well. Some surely\nwill argue the reverse causality: that human perception has driven the statistics of melody,\nresulting in a distribution of intervals that is pleasing to human perception. Either way, it is a\nuseful relationship that perhaps has not yet been fully exploited. The statistical features of this\ndescription for melody result in an efficient representation. And since the representation\ncorrelates well with our perception of melody, the representation becomes more robust since our\nqueries are likely to be more accurate.\nAuthor Information\nYoungmoo E. Kim, Wei Chai, Ricardo Garcia, Barry Vercoe\nMachine Listening Group\nMIT Media Lab\n{moo,chaiwei,rago,bv}@media.mit.edu\nSuggested Readings\nLindsay, Adam T. 1996. Using contour as a mid-level representation of melody . Unpub. MS thesis. MIT\nMedia Lab.\nMcNab, R. J. et al. 1996. “Toward the digital music library: tune retrieval from acoustic input.” Proc. ACM\nDigital Libraries, Bethesda. http://www.nzdl.org.\nMiDiLiB, University of Bonn, http://leon.cs.uni-bonn.de/forschungprojekte/midilib/english.\nThemefinder™, Stanford University, http://www.ccarh. org/themefinder.\nTuneServer, University of Karlsruhe, http://wwwipd.ira. uka.de/tuneserver."
    },
    {
        "title": "Searching for Meaning: Melodic Patterns, Combinations, and Embellishments.",
        "author": [
            "Steve Larson"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415738",
        "url": "https://doi.org/10.5281/zenodo.1415738",
        "ee": "https://zenodo.org/records/1415738/files/Larson00.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1415738,
        "dblp_key": "conf/ismir/Larson00",
        "content": "SEARCHING FOR MEANING:  MELODIC PATTERNS,  \nCOMBINATIONS, AND EMBELLISHMENTS \n  \nABSTRACT \n \nSteve Larson \nAssociate Professor of Music \nUniversity of Oregon \nSchool of Music \nsteve@darkwing.uoregon.edu \n \n I am interested in the search for musical patterns -- not so much because I want to find particular patterns, but because I want to understand musical meaning and I believe that musical meaning is something that listeners create when they relate musical patterns to one another, and when they relate musical patterns to other sorts of patterns.    I describe a theory of musical meaning that argues that experienced listeners of tonal music hear musical motion metaphorically, as purposeful action within a dynamic field of musical forces (musical gravity, magnetism, and inertia).  That theory has been used to clarify issues in Schenkerian theory, to analyze music in many styles, to improve the training of musicians, to account for experimental results in melodic expectation, to explain striking regularities in published analyses of tonal music, and even to illuminate the phenomenon of \"swing\" in jazz.    The assumptions of that theory generate a small set of patterns and pattern combinations.  This small set of patterns crops up over and over again in tonal music.  Recognizing these patterns and their significance requires seeing how they are embellished in particular pieces. I illustrate these patterns, their combinations and embellishments, and something about their meaning by analyzing several folk songs, including \"Ah, vous dirai-je, Maman\" and Mozart's variations on it.    The ubiquity of this small set of patterns raises interesting questions about searching for musical patterns, about the role of computers in information retrieval vs their role in musical artificial intelligence, and about musical meaning.  My presentation ends with a series of such questions."
    },
    {
        "title": "SEMEX - An efficient Music Retrieval Prototype.",
        "author": [
            "Kjell Lemström",
            "Sami Perttu"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415908",
        "url": "https://doi.org/10.5281/zenodo.1415908",
        "ee": "https://zenodo.org/records/1415908/files/LemstromP00.pdf",
        "abstract": "We present an efficient prototype for music information retrieval. The prototype uses bit- parallel algorithms for locating transposition invariant matches of monophonic query melodies within monophonic or polyphonic music stored in a database. When dealing with monophonic music, we employ a fast approximate bit-parallel algorithm with special edit distance metrics. The fast scanning phase is succeeded by verification where a separate metrics is used for ranking matches. We also offer the possibility to search for exact occurrences of a ‘distributed’ melody within polyphonic databases via a bit-parallel filtering technique. In our experiments with a database of 2 million musical elements (notes in a monophonic and chords in a polyphonic database) the responses were obtained within one second in both cases. Furthermore, our pro- totype is capable of using various interval classes in matching, producing more approximation when it is needed. Key words: music retrieval, bit parallelism, polyphonic databases 1",
        "zenodo_id": 1415908,
        "dblp_key": "conf/ismir/LemstromP00",
        "keywords": [
            "music information retrieval",
            "bit parallelism",
            "pattern matching",
            "polyphonic search",
            "monophonic databases",
            "transposition invariance",
            "query processing",
            "midi",
            "edit distance",
            "music retrieval algorithms"
        ],
        "content": "SEMEX– AnEfﬁcientMusicRetrievalPrototype\n/#03\nKjell LemströmandSami Perttu\nDepartment of Computer Science\nP.O.Box 26 FIN-00014, University of Helsinki\nFINLAND\n{klemstro,perttu}@cs.helsinki.ﬁ\nAbstract\nWe present an efﬁcient prototype for music information retrieval. The prototype uses bit-\nparallel algorithmsfor locating transpositioninvariantmatchesof monophonicquery melodies\nwithin monophonicor polyphonicmusic stored in a database. When dealing with monophonic\nmusic, we employ a fast approximate bit-parallel algorithm with special edit distance metrics.Thefastscanningphaseissucceededbyveriﬁcationwhereaseparatemetricsisusedforranking\nmatches. We also offerthe possibility to search forexact occurrencesof a ‘distributed’melody\nwithin polyphonic databases via a bit-parallel ﬁltering technique. In our experiments with adatabase of 2 million musical elements (notes in a monophonic and chords in a polyphonic\ndatabase) the responses were obtained within one second in both cases. Furthermore, our pro-\ntotype is capable of using various interval classes in matching, producingmore approximation\nwhenit isneeded.\nKey words: musicretrieval,bitparallelism,polyphonicdatabases\n1 Introduction\nTheorigins ofmusicinformation retrieval (MIR)systemsareinmanualcollections of incipits, short\nmelodic fragments obtained from the beginning of pieces of music. The collections were manuallycompiledbyresearchers orlibrarians, andusually coveredonenarrowﬁeldofmusic. Morerecently,computerized content-based MIR systems have begun to appear. The systems interpret melodiesas strings that can be matched against query patterns using standard methods from general stringmatching.\nInthispaper,weintroduceourMIRprototypecalledSEMEX(SearchEngineforMelodicExcerpts),\nwhichisanefﬁcientimplementation ofvarious ideasthatwehaveintroduced earlier(forasummaryoftheseideas,see(Lemström2000)). Thecoresofthematchingalgorithms inSEMEXarebasedonso-called bitparallelism . Weclaimthatthelengthofan‘ideal’MIRquerypatternisshorterthanthe\nusual length ofthecomputer word(i.e.32bits). Thus,byusing thebit-parallel techniques presented\nlater, wecanexpect that thecores ofthe algorithms run inlinear time (withrespect tothe size ofthe\ndatabase). Such fast implementations are of crucial importance since music databases can be verylarge. Notonlyistheextensive useofbit-parallelism something newforaMIRsystem,butSEMEX/#03A work partly supported by Nokia Research Center.also differs from the previous systems in other respects, which are discussed below (see Appendix\nfor a comparison table of current MIRsystems).\nSomeMIRsystems, such astheprototype ofGhias,Logan,Chamberlin &Smith(1995) orPollastri\n(1999); MELDEX (Bainbridge, Nevill-Manning, Witten, Smith & McNab 1999); and Melodis-\ncov (Rolland, Raskinis &Ganascia 1999), accept digital input which is subsequently converted into\nan internal representation for matching. An apparent problem there is that the digital input is likelyto be somewhat distorted. For instance, if a query is given by humming, one cannot expect the in-tervals to be exactly correct. To overcome this problem, the music is often represented by contour,\ngiving only the direction of the intervals (up, down, or repeat). This representation, however, re-quires long query patterns inorder toreach good discriminatory power (Downie1999). InSEMEX,\nwe can achieve better discrimination while still maintaining tolerance to errors by using the QPI\nclassiﬁcation introduced by Lemström &Laine (1998).\nThe representation of choice for melodies in current MIR systems is a symbolic string of pitches.\nDurational information can be used in some systems but the emphasis is usually put on pitch infor-mation. Whendealing withqueries onpitches, transposition invariance isaveryimportant property\nthat should be considered. An algorithm taking into account transposition invariance usually ig-nores the musical key of the pitch sequence by using interval representation that considers only the\nsemitonic pitch distances between consecutive elements within the musical sequences. However, as\nnoted by Cambouropoulos, Crawford & Iliopoulos (1999), some problems are involved with that\nencoding as well. Lemström & Ukkonen (2000) presented how transposition invariance can beachieved without using interval representation, and thus, some of those shortcomings are avoidable.We apply their transposition invariant distance metrics in SEMEX.\nApropertyofrealmusicdatabasesoftencompletelyignoredinMIRsystemsisthatthedatabasesare\nlikely to contain polyphony. Although a few algorithms for locating occurrences of query patternsin polyphonic databases have been presented recently in literature (see e.g., Uitdenbogerd & Zobel1998, Dovey 1999, Holub, Iliopoulos, Melichar & Mouchard 1999, Lemström & Tarhio 2000), toour knowledge, besides SEMEX, only the system by Uitdenbogerd & Zobel (1999) considers thisproblem. However, the approaches of these two systems are different. Uitdenbogerd and Zobelreduce the polyphonic surface to a monophonic melody by a melody extraction algorithm, while inSEMEX it is possible to locate occurrences that are distributed among several voices by using theMonoPoly algorithm by Lemström &Tarhio (2000).\n2 Problem Setting\nIn our current prototype, we use a simpliﬁed representation for music comprising only the pitchlevels of the notes. The pitch levels are represented as small integers/0 /;/:/:/: /;rwhich form our\nalphabet /#06. Here,three values of rare of particular interests; r /=/2(representing musical contour);r /=/1 /0(representing the QPI classiﬁcation); and r /= /1/2/7(as in MIDI(MID 1996)).\nA musical source S /= S/1\nS/2\n/#01/#01/#01 Snis a sequence of sets of integers. Each Simodels a chord and\nis formally a subset of the alphabet /#06. Moreover, each Sicorresponds to a chord of notes, and is\ncomprised of notes having their onsets simultaneously.\nA musical query pattern p /= p/1\np/2\n/#01/#01/#01 pmis a sequence of integers, more precisely, pi\n/2 /#06,f o ri /=/1 /;/:/:/: m. Obviously,thedegeneratecasefor S(thatis,whenthesourceismonophonic), denoteds,is represented by a similar structure to that of the pattern representation.\nThe setting for a content-based music information retrieval problem is as follows. Given a long\nsource string S /= S/1\n/#01/#01/#01 Snand a relatively short music query pattern p /= p/1\n/#01/#01/#01 pm, the task is to\nﬁnd all locations in Swhere poccurs as a subsequence. Here an occurrence might mean an exact,\ntransposed ,o re v e napproximate occurrence .\nWedeﬁne that there isan exact occurrence of patposition j,if pi\n/2 Sj /+ i /, /1holds for i /=/1 /;/:/:/: /;m,\nand there is a transposition invariant occurrence of pat position j, if there is an integer dsuch that\neach /#28 pi\n/+ d /#29 /2 Sj /+ i /, /1. Currently, wedo not allow approximate queries to a polyphonic source.\nWhen the source sis monophonic, exact and transposition invariant occurrences of pwithin smean\nthat pi\n/= sj /+ i /, /1and /#28 pi\n/+ d /#29 /= sj /+ i /, /1for each i, respectively. An approximate occurrence of p\nis found if there is a subsequence p\n/0of s, such that p\n/0can be obtained from pby using kor fewer\neditingoperations (anapproximatetransposed occurrence isdeﬁnedaccordingly). Theconventional\nediting operations are:/#0Freplacement: the aligned symbols piand sj /+ i /, /1are distinct,/#0Finsertion: the symbol sjis missing in p,/#0Fdeletion: the symbol piis missing in s.\n3 ApproximatePatternMatching\nApproximate string pattern matching is based on the concept of edit distance (Crochemore &\nRytter 1994, Gusﬁeld 1997). The edit distance D /#28 A/; B /#29between strings A /= a/1\n/;/:/:/: /;amandB /= b/1\n/;/:/:/: /;bn\n/; A/; B /2 /#06\n/#03( /#06\n/#03denotes the set of all sequences over /#06), is the minimum number\nof editing operations (given above) required to transform string Ainto string B. Thespecial case in\nwhich deletions and insertions are not allowed is called the Hamming distance .\nA standard way to locate approximate occurrences of pin s, is to search for subsequences p\n/0of s,\nsuch that D /#28 p/; p\n/0/#29 /#14 k( kis a threshold value used to control the accuracy of the pattern matching).\nThiscanbedoneefﬁcientlybyusing dynamicprogramming . Suchaproceduretabulatesalldistancesdij\n/= /#28 p/1\n/#01/#01/#01 pi\n/;sj\n/0/#01/#01/#01 sj\n/#29(where j\n/0/#14 j) between the preﬁxof pand anyfactor(substring) of s\n(Sellers 1980, Ukkonen 1985 a). The pattern matching problems associated with edit and Hamming\ndistances are called kdifferences and kmismatches , respectively.\nBit-Parallelism. Thedevelopment ofdynamic programming methodscomputing theeditdistance\nhas lead to an algorithm family applying a fast bit-parallel approach. When replacing a conven-tional implementation where each variable resides in its own memory location or register with acorresponding implementation where the variables can be manipulated in parallel with bit opera-tions,asigniﬁcant speed-up canbeachieved. Oneoftheﬁrstalgorithms ofthefamilywaspresented\nby Baeza-Yates & Gonnet (1992). Their shift-or algorithm was originally designed for the exact\nmatching. Denoting byWthe number of bits in a computer word used, their algorithm runs in\ntime O /#28 d\nmW\ne n /#29, i.e., in linear time if the pattern is short enough as compared to the length of the\n(used) computer word (if m /#14 W). The MonoPoly algorithm, which we will use with polyphonic\ndatabases, is based on the shift-or algorithm.Recently Myers (1998) presented a fast bit-parallel implementation for the kdifferences problem.\nIt also runs in time O /#28 d\nmW\ne n /#29. The performance of the algorithm rests on a certain property of\nthe dynamic programming table; the difference between adjacent vertical cells is always -1, 0, or1 (Ukkonen 1985 b). In Myers’ algorithm, the current column in the matrix is represented by two\nbit mask variablesPand Mthat store positive and negative changes (deltas) in the matrix values,\nrespectively. We apply Myers’ algorithm when dealing with monophonic databases.\n4 Semex Prototype\n4.1 Supported Formats\nSEMEX supports three ﬁle formats: the Standard MIDI File (SMF) format (MID 1996), which isa common interchange format for symbolically encoded music; the IRP (Inner RePresentation) for-mat(Lemström &Laine 1998), which is astraightforward ASCIIﬁle format for encoding symbolicmusic; and MDB (Melody DataBase), which is a simple ﬂat-ﬁle database format used in the proto-type. MDB ﬁles contain a single string of pitch values encoding all the music in the database and atrack list for mapping indices in the pitch string to tracks in source ﬁles. The mapping is needed asotherwise the Query Engine component would have no way of telling in which source ﬁle a match\noriginated; neither would it be able to discern matches straddling a track boundary.\nRecall that, though there is more information available in the formats, we are currently considering\nonly the pitch levels of notes.\n4.2 System Architecture\nThecomponentsoftheSEMEXprototypesystemareshowninFig.1. TheUserInterfacecomponent\nmakes use of the other components in order to present the user with a single interface. A separate\nPitchEstimatorcomponent (Tolonen&Karjalainen 2000)facilitatesconversion ofdigitalaudiodatainto symbolic form. The core of the system, consisting of the components Song Manager, Databaseand Query Engine, performs queries and allows the user to compile MIDI ﬁles into a database.\nEngineQueryUser\nInterfaceSong\nManager\nDatabaseCOMPONENTSCORE\nEstimatorPitch\nFigure 1: Components of the SEMEXPrototype.\nThe core of the prototype is written in C++ with a careful object-oriented design, allowing differentsoftware components to interact nearly orthogonally. For instance, all matching algorithms can use\nany interval reduction scheme and can be applied on any type of source data. The latter property is\ndue to the algorithms being implemented as template functions.\n4.3 UserInterface\nCurrently, the prototype has a simple Unix-like user interface: it is invoked from the command lineand different options are selected with‘switches’. Asample invocation of the prototype is shown inFig.2. Inthe example, the10 best matches of aquery pattern are shown from midi.mdb ,whichis\na database ﬁle containing monophonic reductions from a hand-picked collection of classical MIDIﬁles. In addition to textual output, the prototype can extract the parts corresponding to matchesfrom the actual MIDI source ﬁles referred to in the database and play them back using an externalprogram, with appropriate volume fading.\nFigure 2: A sample invocation of the SEMEXprototype.\n4.4 Retrieving from Monophonic Sources\nIn the special case when the database is monophonic, we apply Myers’ algorithm (Myers 1998).\nBit-parallel string matching algorithms have proven themselves to be very ﬂexible; it is ratherstraightforward to apply the transposition invariant distance measures by Lemström & Ukkonen(2000) and different classiﬁcations ofpitches (such ascontour and QPIclassiﬁcations (Lemström&Laine 1998)), while still preserving the linear time complexity.\nIn Fig. 3, we illustrate the performance of the adapted Myers’ algorithm (including the pruning but\nexcluding the veriﬁcation phase of the following paragraphs) by comparing it to the standard dy-namic programming algorithm and Ukkonen’s cutoff algorithm (Ukkonen 1985 b). The comparison\nwas run in a 700 MHz Pentium III with 768 MB of RAM under the Linux operating system. Thelength of a machine word was 32 bits. The database consisted of 208 classical MIDI ﬁles, which\nwerereduced inthiscomparison toamonophonic formbyincluding only thehighest notesfromthe\npolyphonic texture. The reported scanning time is the average of the scanning times of 100 patternsof length 12 selected randomly from the database. The approximation parameter k(which mainly\naffects Ukkonen’s cutoff algorithm) was set to 3 during the runs.\n101001000\n2000004000006000008000001e+061.2e+061.4e+061.6e+061.8e+062e+06average scanning time in milliseconds\ndatabase length in notesbasic algorithm\ncutoff algorithm\nMyers' algorithm\nFigure 3: A comparison of the running times of the basic dynamic programming algorithm, Ukko-\nnen’s cutoff algorithm, and Myers’ bit-parallel algorithm.\nPruning. It is useful to eliminate some matches instead of reporting all of them. Speciﬁcally,\nmatches which are super- or substrings of a better match should be pruned. One source of suchmatches is that, as for columns, the difference in edit distance between two successive entries on arow of the dynamic programming matrix is at most one. This results in an undesirable proliferationof matches on both sides of a center match with an edit distance belowk.\nIn order to combat such ‘pollution’, the matching algorithms are modiﬁed to report only positions\nwheretheeditdistance doesnotgrowandwhicharenotfollowedbyabettermatch. Furtherpruningis done during veriﬁcation.\nVeriﬁcation. Inthescanning phasetheexactsubstring ofscorresponding toamatchisnotknown\n- Myers’ algorithm computes only edit distances. Therefore, to verify a matching position j,i ti s\nfed to the basic dynamic programming algorithm, which resolves the corresponding match. The\nveriﬁcation algorithm alsocomputes theﬁnaldistance D\nvmjofthematch,employing avariant ofthe\nedit distance metrics. The sole purpose of this metrics is to ﬁne-tune the sorting of matches withequal edit distances. We use a metrics that, in addition to the usual edit distance, includes as a termthetotal modulation required to convertpto s. The modulation imposed by a single edit operation\nis the absolute difference between the intervals being compared (in the case of replacement) or theabsolute value of the interval being inserted or deleted (in the case of insertion or deletion). Thetotal modulation is simply the sum of these:D\nvi/;j\n/=min\n/8/#3E/#3C/#3E/:\nD\nvi /, /1 /;j /, /1\n/+/#28if pi\n/= sjthen /0else /1/+ j /0 /: /0/0/1/#28 pi\n/, sj\n/#29 j /#29 /;D\nvi /, /1 /;j\n/+/1 /+ j /0 /: /0/0/1 pi\nj /;D\nvi/;j /, /1\n/+/1 /+ j /0 /: /0/0/1 sj\nj /:(1)\nWhen the exact substring of sis known, matches are pruned so that only the best match is retained\nout of matches beginning from the same position in s. Together with the initial pruning performed110100100010000100000\n2000004000006000008000001e+061.2e+061.4e+061.6e+061.8e+062e+06\ndatabase length in chordscandidates\noccurrences\n110100100010000100000\n2000004000006000008000001e+061.2e+061.4e+061.6e+061.8e+062e+06average time in mseconds\ndatabase length in chordsAlgorithm C\nMonoPoly algorithm, first query\nMonoPoly algorithm, re-queries\nFigure 4: Running MonoPoly algorithm on the test database. On the left, the numbers of candi-\ndates and proper occurrences (note the logarithmic scale). On the right, the times for a ﬁrst query(preprocessing+marking+checking) andre-queries (marking+checking) ofMonoPolyAlgorithm,ascompared to a ‘stand-alone’ version of the checking phase (referred to as Algorithm C).\nbythescanning algorithm,thesemeasuressufﬁcetoﬁlteroutmatcheswhicharesuper-orsubstrings\nof a better match.\n4.5 Retrieving fromPolyphonicSources\nIn SEMEX, we can deal with databases containing polyphony in two ways. First, the database canbe reduced in a monophonic form by considering only the highest notes of chords (as suggested\nby Uitdenbogerd & Zobel (1999)), and then applying the techniques described in the previous sub-\nsection (thus, the performance ofthis approach in SEMEXrelies onMyers’ algorithm). Second, wecan search transposed exact occurrences by using the MonoPoly algorithm.\nThe MonoPoly algorithm runs in two phases, out of which the latter comprises two subphases. In\nthe ﬁrst phase, the algorithm preprocesses the source in timeO /#28 nq /#29( qdenotes the maximum size\nof the chords). The second phase applies a ﬁlteringtechnique. First a marking phase applying bit\nparallelism searches possible occurrences (called candidates ) of the given query pattern. If m /#14W, this is done in linear time. Then a checking phase scans through the candidates to ﬁnd the\nproper occurrences among them. For this phase, we use a somewhat slower algorithm with a time\ncomplexity O /#28 nq /#28 q /+ m /#29/#29(Lemström & Tarhio 2000). Therefore, the speed of the MonoPoly\nalgorithm depends rather heavily on the ﬁltration ability of the marking phase.\nIn Figs. 4 one can see how the MonoPoly algorithm performs with the test database. We illustrate\nboth the efﬁciency of the ﬁltration (number of proper occurrences / candidate occurrences), andcompare the running times of the MonoPoly algorithm (for a ﬁrst query to a database, and for ‘re-queries’ to the same database) to the straightforward algorithm which is also used in the checkingphase of the MonoPoly algorithm. The settings for this comparison were as in the previous com-parison, but in this case the polyphony was preserved. Theaverage polyphony degree (within MIDI\ntracks) varied usually between 1 and 3, but the maximal degree was as high as 41. As can be seen\ninthe ﬁgures, though the number of candidates grows faster than thenumber ofproper occurrences,the MonoPoly algorithm clearly outperforms the comparison algorithm. When using the MonoPolyalgorithm, even the ﬁrst queries to the database were completed within one second in all the cases.5 Conclusion\nWe have presented a prototype of an efﬁcient music information retrieval system. It is the ﬁrst of\nits kind to utilize extensively fast bit-parallel algorithms in matching. These algorithms have otheradvantages besides speed over the traditional algorithms: any character in the pattern can matchan arbitrary set of source characters at no additional cost, making, e.g., different interval reduction\nschemes easy to implement. Our prototype is also the ﬁrst MIR software system to provide for\nmatching monophonic patterns against sources containing true polyphony.\nAcknowledgments\nThe authors are grateful to Professor Esko Ukkonen for his support to this project.\nReferences\nBaeza-Yates, R. & Gonnet, G. H. (1992), ‘A new approach to text searching’, Communications of\nthe ACM 35(10), 74–82.\nBaeza-Yates,R.&Perleberg,C.H.(1992),Fastandpracticalapproximatestringmatching, in‘Com-\nbinatorial Pattern Matching, Third Annual Symposium’, pp. 185–192.\nBainbridge, D., Nevill-Manning, C., Witten, I., Smith, L. & McNab, R. (1999), Towards a digital\nlibrary of popular music, in‘Proceedings of the fourth ACM conference on digital libraries’,\nBerkeley, CA,pp. 161–169.\nCambouropoulos, E., Crawford, T. & Iliopoulos, C. S. (1999), Pattern processing in melodic se-\nquences: Challenges, caveats & prospects, in‘Proceedings of the AISB’99 Symposium on\nMusical Creativity’, Edinburgh, pp. 42–47.\nCrochemore, M.&Rytter, W.(1994), Text Algorithms , Oxford University Press.\nDovey, M. J. (1999), An algorithm for locating polyphonic phrases within a polyphonic musical\npiece,in‘Proceedings oftheAISB’99Symposium onMusical Creativity’, Edinburgh, pp.48–\n53.\nDownie, J. S. (1999), Evaluating a Simple Approach to Music Information Retrieval: Conceiving\nMelodic n-grams as Text, PhD thesis, The University of Western Ontario, Faculty of Informa-\ntion and Media Studies.\nGhias, A.,Logan, J.,Chamberlin, D.&Smith,B.C.(1995), Query byhumming -musical informa-\ntion retrieval in an audio database, in‘ACM Multimedia 95 Proceedings’, San Francisco, CA,\npp. 231–236.\nGusﬁeld, D.(1997), Algorithms on Strings, Trees, and Sequences , Cambridge University Press.\nHolub, J., Iliopoulos, C.S.,Melichar, B.& Mouchard, L.(1999), Distributed string matching using\nﬁnite automata, in‘Proceedings of the 10th Australasian Workshop On Combinatorial Algo-\nrithms’, Perth, WA,Australia, pp. 114–128.Kornstädt, A. (1998), ‘Themeﬁnder: A web-based melodic search tool’, Computing in Musicology\n11, 231–236.\nLemström, K. (2000), String Matching Techniques for Music Retrieval, PhD thesis, University of\nHelsinki, Department of Computer Science. (to appear).\nLemström, K. & Laine, P. (1998), Musical information retrieval using musical parameters, in‘Pro-\nceedingsofthe1998International ComputerMusicConference’, AnnArbor,MI,pp.341–348.\nLemström, K. & Tarhio, J. (2000), Detecting monophonic patterns within polyphonic sources,\nin‘Content-Based Multimedia Information Access Conference Proceedings (RIAO’2000)’,\nParis, pp. 1261–1279.\nLemström, K. & Ukkonen, E. (2000), Including interval encoding into edit distance based music\ncomparison and retrieval, in‘Proceedings of the AISB’2000 Symposium on Creative & Cul-\ntural Aspects and Applications of AI & Cognitive Science’, Birmingham, pp. 53–60.\nMID(1996), The Complete Detailed MIDI 1.0 Speciﬁcation .\nMyers, G. (1998), A fast bit-vector algorithm for approximate string matching based on dynamic\nprogramming, in‘ProceedingsofCombinatorialPatternMatching’,Piscataway,N.J.,pp.1–13.\nPollastri, E.(1999), Melody-retrieval basedonpitch-tracking andstring-matching methods, in‘Pro-\nceedings of the XIIth Colloquium on Musical Informatics’.\nRolland, P.-Y.(1999), ‘Discovering patterns in musical sequences’, Journal of NewMusic Research\n28(4), 334–350.\nRolland, P.-Y.,Raskinis, G.& Ganascia, J.-G. (1999), Musical content-based retrieval: an overview\nofthemelodiscov approach andsystem, in‘ACMMultimedia 99Proceedings’, Orlando, FLO.\nSellers, P. H. (1980), ‘The theory and computation of evolutionary distances: Pattern recognition’,\nJournal of Algorithms 1(4), 359–373.\nTolonen, T. & Karjalainen, M. (2000), ‘A computationally efﬁcient multi-pitch analysis model’,\nIEEETransactions on Speech and Audio Processing . (in press).\nUitdenbogerd, A. L. & Zobel, J. (1998), Manipulation of music for melody matching, in‘ACM\nMultimedia 98 Proceedings’, Bristol, pp. 235–240.\nUitdenbogerd, A. L.& Zobel, J. (1999), Melodic matching techniques for large music databases, in\n‘ACMMultimedia 99 Proceedings’, Orlando, FLO.\nUkkonen, E. (1985 a), ‘Algorithms for approximate string matching’, Information and Control\n64, 100–118.\nUkkonen, E. (1985 b), ‘Finding approximate patterns in strings’, Journal of Algorithms 6, 132–137.\nWu, S. & Manber, U. (1992), ‘Fast text searching allowing errors’, Communications of the ACM\n35(10), 83–91.APPENDIX: MIRSystemComparison\nPitch Rhythm APTIPTOR Other Matching Typeof Time\nrepresentation representation factors algorithm matching complexity\nGhiaset al.(95) contour - -++- Baeza-Yates& kmism O /#28 mn /#29\nPerleberg(92)\nMELDEX intervals durations -+++ dpyaugmented O /#28 mn /#29\nBainbridgeetal.(99) &contour kdiff\nTHEMEFINDER several - -+-- ? exact ?\nKornstädt(98)\nUitdenbogerd& intervals - ++-- dp kdiff O /#28 mn /#29\nZobel(99)\nPollastri(99) abs.pitches durations& -++- Ukkonen(85b)& kdiff O /#28 kn /#29\n&interv als durationratios\nMELODISCOV several durations& -++-takesinto FlExPat,Rolland(99) augmented O /#28 mn /#29\nRollandetal.(99) (incl. intervals durationratios accountmetric kdiff\n&contour) positions\nSEMEX abs. pitches, - +++-various bp,Myers(98) exact& O /#28 n /#29z\ncontour& distances augmented\nQPI kdiff\nAPAllowspolyphony\nTITranspositioninvariant\nPTPitch trackingOROpticalmusicrecognition\ndpDynamicprogramming(conventional)\nbpBit parallelism\nyAstraightforwardimplementationofWu &Manber’s(92) O /#28 kn /#29bit parallelalgorithmisavailable;dpispreferred\nzCoresofthealgorithms(assumingthat m /#14 W)"
    },
    {
        "title": "Intellectual Property Rights in Musical Works: Overview, Digital Library Issues and Related Initiatives.",
        "author": [
            "Mary Levering"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416786",
        "url": "https://doi.org/10.5281/zenodo.1416786",
        "ee": "https://zenodo.org/records/1416786/files/Levering00.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1416786,
        "dblp_key": "conf/ismir/Levering00",
        "content": "Intellectual Property Rights in Musical Works  \n \n Mary Levering, Associate Register for National Copyright Programs \n U.S. Copyright Office, Library of Congress  mlev@loc.gov  \n \nAbstract  \n \n1.  Intellectual Property Rights in Musical Works  \nOur nation’s Founding Fathers incorporated copyright principles into the U.S. Cons titution in order to \nfoster the creation and dissemination of intellectual works for the public good.1  The federal copyright law \nsupports these goals by permitting authors to reap rewards from their creative efforts for limited periods of time, while also benefitting subsequent creators by allowing them to use and build on prior works under certain circumstances.    Thus, in order to stimulate originality and creativity, creators and owners of musical works are granted an exclusive bundle of rights in their copyrighted musical works for the enrichment of our society.  Awareness of these rights is essential for all potential users, even for academics and scholars who wish to use these musical works for educational or other research purposes.  Many musical works, especially those embodied in phonorecords, CD-ROMs and other audio-visual formats, have multiple layers of intellectual property rights involved.\n2  Scholars and institutions must respect these rights when digitizing, transmitting, \nretrieving or otherwise using copyrighted musical works in electronic formats.  Even when underlying musical compositions are in the public domain, the version being used may include new copyrightable authorship that must be considered.  Such authorship could include new arrangements, revised lyrics or music, other editorial revisions of the musical work itself, or—where the work is fixed as a series of sounds—it could include all or some new sounds, a remix of sounds, and so forth.  \nUnderstanding concepts such as (1) the whole range of rightsholders’ exclusive rights in musical \nworks,\n3 (2) the statutory limitations on these rights,4 including how the Fair Use  5 doctrine can be applied \nreasonably for educational and scholarly purposes, (3) when authorization is needed, (4) who the rightsholders are,\n6 and (5) how to secure required permissions efficiently and easily,7 have all become increasingly important \n                                                 \n1  U.S. Constitution, Article 1, Section 8. \n2 Layers of rights in musical works include reproduction/print, distribution, mechanical, \nsynchronization, performance, dramatic adaptation and others. \n3  Exclusive rights of creators, 17 U.S.C. § 106.  \n4  Limitations on exclusive rights of copyright creators/owners (17 U.S.C. § 107-121). \n5  Fair Use (17 U.S.C. § 107) — four statutory factors. \n6  U.S. Copyright Office records in machine-readable form cataloged from 1978 to the present, \nincluding registration and renewal information and recorded documents, are available for searching on the \nInternet through LOCIS (Library of Congress Information System); connect to LOCIS through the Web <www.loc.gov/copyright/rb.html> or directly by using Telnet to <locis.loc.gov>. \n7  Collective rights organizations that manage and license rights in musical works include: \n· ASCAP, BMI, SESAC: performing rights societies \n· Harry Fox Agency: music publishers’ licensing for scholars and educational users in this age of new online works, mushrooming digital distance education, \nand ever-expanding networked access to resources in digital form.  \n2.  Copyright and Digital Library Issues for Music  \nFair Use.  Fair use is the broadest and most general limitation on the exclusive rights of copyright \nowners.  Codified in the copyright law since 1978, fair use is a flexible, technology-neutral doctrine, allowing reasonable and socially desirable uses of copyrighted works, even when they are not authorized by the copyright owner.  Although flexibility is a major benefit of the fair-use doctrine, the corollary is a degree of uncertainty, giving rise to many controversies over the years and supporting the need for negotiated fair-use \nguidelines to give practical guidance for day-to-day decision-making in the educational context.  Fair-use \nguidelines involving musical works include the classroom guidelines for music negotiated in the late 1970’s\n8  \nand guidelines for creation of multimedia works agreed on in 1996,9 in an effort parallel to the 1994-97 \nCONFU (Conference on Fair Use) initiative.  \nOther Digital Library Issues.  Earlier fair-use guidelines do not translate easily to new technological \nuses of copyrighted works in digital form, nor do they give practical guidance for the novel applications emerging for digital works that can be so easily accessed and transmitted over campus networks and global communications networks.  Many rightsholders and educational organizations participating in CONFU recognized the growing need for fair-use guidelines in the context of higher education, addressing such new capabilities as electronic reserves, electronic course packs, distance education, and digital archives. But CONFU participants were unable to achieve a general consensus in these areas, though electronic preservation (converting older analog collections to digital form for preservation purposes) was addressed in CONFU and those concerned were referred to the U.S. Congress for relief.  Digital preservation did receive Congressional approval in the Digital Millennium Copyright Act of 1998.  \nDigital Rights Management vs. Access Management.   One major challenge in the area of digital \nrights management is understanding the distinction and  relationship between managing “rights” in copyrighted works and managing “access” consistent with those rights.  Managing rights in online copyrighted works is the right and responsibility of rightsholders and their agents, e.g., authors, publishers, and others in the distribution chain; managing “access” to those works in a way that consistently respects rightsholders’ rights, and is also responsive to the needs and privileges of scholars and other users, is the responsibility of digital libraries and archives who store online copyrighted works and provide users with access to them.  Managing access is more \nchallenging in the online world than in the analog world. \n \n3.  Digital Library Initiatives at U.S. Copyright Office and Library of Congress  \nU.S. Copyright Office.  The U.S. Copyright Office is a national office of public record, where claims \nto copyright are registered and documents pertaining to copyright (such as assignments, transfers, exclusive licenses, and so forth) are recorded.  Over 600,000 claims to copyright are received and processed by the Office annually, along with more than a million copyright deposits, including thousands of works in the performing arts.   The U.S. Copyright Office’s innovative new CORDS system (Copyright Office Electronic Registration, Recordation & Deposit System) enables fully automated copyright registration and deposit by \n                                                 \n8 U.S. Copyright Office.  Reproduction of Copyrighted Works by Educators and Librarians.  Circular \n21, <http://lcweb.loc.gov/copyright/circs/>. \n9 Fair Use Guidelines for Educational Multimedia.  Nonlegislative Report of the Subcommittee on \nCourts and Intellectual Property, Committee on the Judiciary, U.S. House of Representatives.  September 27, \n1996. <http:// www.indiana.edu/~ccumc/mmfairuse.html>. \n claimants.  Through CORDS, rightsholders can register their copyrighted works (including musical works) in \ndigital form faster and more easily, making copyright catalog records about registered works quickly and easily accessible online.  Copyright records on all copyright registrations and recordations, containing millions of records about copyrighted works, are available in the Office’s national databases, available online since 1978 and over the Internet since 1993.  \nCopyright deposits of musical works received in the Copyright Office as part of the copyright \nregistration process are one of the Library’s primary sources of new musical works.  Working with the National Music Publishers Association (over 20,000 music publishers) and its licensing subsidiary, the Harry Fox Agency, and cooperating music publishers, the Copyright Office has opened its CORDS electronic registration and deposit system to new claims for musical works with accompanying digital deposits in MP3 format.  The system is in production with several major music publishers already cooperating and is growing.  New musical deposits received in electronic form will be included in the Library’s digital repository.  Technical issues facing CORDS system developers include: \n· authentication and security · metadata (structural, administrative, and descriptive) · digital object models (including complex digital objects) · unique persistent identifiers \n \nLibrary of Congress.  As part of its multi-faceted Digital Futures Project, the Library is currently \ndeveloping a digital repository prototype (2000-01) for different types of works in its digital research collections; this includes musical works digitized as part of the Library’s American Memory Project as well as new musical works received in digital audio format through copyright deposit.  The project is a collaborative activity that prototypes new approaches for the acquisition, storage, maintenance, preservation and retrieval of various categories of digitally formatted works.  The Library’s digital repository prototype is testing Artesia’s TEAMS software for digital asset management purposes to help determine what is needed for long-term storage and preservation, and access controls for authorized retrieval of copyrighted works by researchers and other users.  Stored digital works are uniquely identified by “handles” (a technology developed by CNRI\n10) and \nmanaged by the Library’s Local Handle Management System.  \nThe Library is also building an innovative National Audio-Visual Conservation Center at Culpeper, \nVirginia, projected to be operational in 2003, under the Management of the Motion Picture, Broadcasting and Recorded Sound Division (MBRS).  The new center will be experimenting with new ways to store, retrieve and deliver these works in a secure manner to researchers at the Library’s main campus in Washington, D.C.  Goals of the A-V prototype project (1999-2002) include advancing the Library’s understanding of acquiring, maintaining and preserving audio-visual collections in digital formats and serving readers needs effectively and innovatively with these digitized works.  The prototype is focusing on data structure, system architecture, and human administration, with content persisting through many cycles of obsolescence.  Defining, capturing and storing appropriate metadata along with complex digital objects is of paramount importance, including essential structural, administrative, and descriptive metadata for each stored digital object.\n  \n \n Suggested Readings  \nWWW.loc.gov/copyright/cords/ \nWWW.loc.gov/standards/metadata.html WWW.loc.gov/rr/mopic/avprot/avprhome.html \n \n                                                 \n10  The Corporation for National Research Initiatives’ Handle System, <www.cnri.reston.va.us/>."
    },
    {
        "title": "Integrating Paper and Digital Music Information Systems.",
        "author": [
            "Karen Lin",
            "Tim Bell 0001"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416544",
        "url": "https://doi.org/10.5281/zenodo.1416544",
        "ee": "https://zenodo.org/records/1416544/files/LinB00.pdf",
        "abstract": "Active musicians generally rely on extensive personal paper-based music information retrieval systems containing scores, parts, compositions, and arrangements of published and hand-written music. Many have a bias against using computers to store, edit and retrieve music, and prefer to work in the paper domain rather than using digital documents, despite the flexibility and powerful retrieval opportunities available. In this paper we propose a model of operation that blurs the boundaries between the paper and digital domains, offering musicians the best of both worlds. A survey of musicians identifies the problems and potential of working with digital tools, and we propose a system using colour printing and scanning technology that simplifies the process of moving music documents between the two domains. Keywords : user interfaces, user needs, optical music recognition",
        "zenodo_id": 1416544,
        "dblp_key": "conf/ismir/LinB00",
        "keywords": [
            "music",
            "personal",
            "paper-based",
            "information",
            "retrieval",
            "computers",
            "flexibility",
            "powerful",
            "digital",
            "domains"
        ],
        "content": "Integrating Paper and Digital Music \nInformation Systems  \nKaren Lin and Tim Bell  \nUniversity of Canterbury, Christchurch, New Zealand  \n \nAbstract  \nActive musicians generally rely on extensive personal paper -based music information \nretrieval systems containing sc ores, parts, compositions, and arrangements of published \nand hand -written music. Many have a bias against using computers to store, edit and \nretrieve music, and prefer to work in the paper domain rather than using digital \ndocuments, despite the flexibility  and powerful retrieval opportunities available. In this \npaper we propose a model of operation that blurs the boundaries between the paper and \ndigital domains, offering musicians the best of both worlds. A survey of musicians \nidentifies the problems and po tential of working with digital tools, and we propose a \nsystem using colour printing and scanning technology that simplifies the process of \nmoving music documents between the two domains.  \nKeywords : user interfaces, user needs, optical music recognition  \n1. Introduction  \nTraditionally musicians have stored and retrieved music scores using paper -based \nsystems. Many musicians have built up personal libraries of music books, compositions, \narrangements and sheet music. The acquisition or creation of documents is \nstraightforward, but the retrieval or modification of scores is hindered by the inflexibility \nof the paper medium. A digital music library would have a number of benefits, including \nconvenient retrieval (instead of searching through piles of music), ease of  processing \n(such as part extraction), and communication (sending electronic copies to other \nperformers). However working with digital documents also poses significant barriers for \nusers more familiar with traditional paper documents. In this paper we expl ore the \nrelationship between the paper and digital domains, and the possibility of allowing easy \nconversion between the two to allow documents to exist in both domains and be \nprocessed in whichever domain is the most convenient.  \nWe begin by proposing a thr ee-state model that provides a framework for reasoning \nabout the different domains. Then we report on a survey of musicians to determine the \nkinds of retrieval and processing tasks they carry out, and their preferences for carrying \nout these tasks in each domain. We then report on a system in which color manuscript is \nused to simplify the transition between paper and digital media.  \n2. The states of a music document  \nRather than just use two states of a document (paper and digital), we propose a model \nthat di vides the digital state into two: image data and semantic data. Figure 1 shows the resulting three -state model, with transitions showing how data can be converted from one \nstate to another. This model is intended to help us to reason about the representation of \nmusical scores. Digital image data is typically a bitmap representation of the pixels, \nwhereas the digital semantic data stores information about pitch and rhythm. This \nacknowledges the difference between, for example, a scanned  image and one that has \nbeen recognised so that meaning is associated with the symbols. Methods for converting \nbetween the three states are also shown. For example, digital image data can be converted \nto digital semantic data using an OMR (optical music re cognition) system.  \n \nFigure 1: Three -state model of data  \nA document may well exist in all three formats at once. For example, a score in a \nmusician’s library might be scanned as a digital image and stored on a portable device, \nbut also be recognised as best as possible to allow semantic information to be stored for \nconvenient retrieval. This kind of approach is already common with textual documents. \nFor example, the Adobe Acrobat Capture system [Merz 1997] can be used to put a paper  \ndocument on -line in a form displaying the original scanned image, but ‘hiding’ an \nOCRed version behind it to allow searching and copying of the text. The more accurate \nthe recognition, the more useful the retrieval process will be. Retaining the image dat a \nprevents mistakes in recognition being reflected in the displayed version of the document.  \nPaper documents are still widely used for writing, storing, practising and performing \nmusic. Paper is reliable, inexpensive, has a high resolution, is easily marke d up, and is \nvery portable. We are interested in removing the boundaries between the three states so \nthat, for example, an arranger could write a score on paper, scan it, print out an edited \nversion, annotate it with pencil, and scan it again to have the d ifferences incorporated in \nthe digital version of the document.  \n3. How musicians manage music documents  \nAfter some preliminary interviews with professional musicians we designed a survey in \norder to assess how digital techniques could aid musicians with mu sic information management. \nThe musicians interviewed and surveyed came from a variety of backgrounds from classical to \ncontemporary music. We were interested in musical activities of all kinds including composition, \narrangement, performance, teaching, mus icology, recording, accompanying, transcribing and so \non. We asked about the tools that they use, their working procedures, difficulties that they face, \nand ways that digital storage and retrieval might be applied.  \nThe survey was distributed to about fifte en Internet newsgroups. There were 40 respondents \nfrom differing backgrounds. Twelve of the 40 were full -time musicians, and 75% of the \nrespondents perform musical tasks for at least ten hours per week. All the respondents peform \nmultiple tasks; for instan ce, some are composers, arrangers and performers. It was interesting to \nfind that six respondents compose exclusively by hand and seventeen respondents use both paper \nand computer software. This means that over half of the respondents still use paper for composition and arrangement, despite the survey being biased towards computer users because it \nwas conducted on the Internet.  \nThe survey identified a number of existing problems that with the use of computers for \nmanaging musical documents. These are as fol lows:  \nThe cost of computer hardware and software  is one of the main concerns for at least 30% \nof participants. Expensive hardware such as fast computers for running modern software, high -\nquality printers for a good printing result, CD -writers and hard disk s for storage, as well as \nsynthesizers and expensive music notation programs are beyond the means of many musicians. \nMoreover, rapidly changing software and hardware make the problem worse.  \nThe size of a computer screen  is another problem. Even though 17 -inch screens are \nrelatively common and larger screens are available, this is still very limiting for an orchestral \nscore that includes several parts for different instruments. Some participants mentioned that it \nwould be very helpful to have an affordable f lat screen as big as an ordinary desk. Another \nproblem is that screens can be tiring to look at for long periods.  \nUsing computers can cause a loss of inspiration and creativity.  This was an issue \nmentioned twice in the interviews, and by five respondents f rom the Internet survey. The \ncomputer imposes an extra cognitive step between having an idea and recording it in a document. \nSome musicians overcome this with experience, but at the cost of lost productivity while \nbecoming accustomed to the new method. Fur thermore, with handwriting any symbol can be \nplaced anywhere on the page, whereas an infrequently used symbol (such as a glissando) may be \nvery difficult to invoke in music processing software, leading to loss -of-activation errors for the \nuser.  \nLoss of eff iciency is experienced  by some computer users because it can take more time to \nenter music into music software. Some musicians said that they could write music by hand \nseveral times faster than using a computer; one of them claiming it could be ten times f aster. \nThere is some research supporting this statement: Anstice [1996] found that hand -writing music \ntakes about one -third of the time. Improving software by employing flexible input techniques \nmay reduce this problem, and an experienced user might become  quite efficient. One of the \nparticipants mentioned that he could enter music on the computer very quickly — he has used \nthat software since 1985.  \nEven the start -up time for a computer can be a problem. Several participants said that when \nthey had an idea,  they would be unlikely to be bothered to turn on a computer and run music \nnotation programs to compose the music. By the time they have waited several minutes for the \ncomputer and software to start, they have forgotten the music they thought of and the in spiration \nhas flown. Other musicians said that although they do not write music by hand often, they \nparticularly prefer to write this way when they are in a rush or some excellent ideas come to mind \nsuddenly. They agree that grabbing a piece of paper to wr ite music down by hand is the quickest \nway under such situations.  \nThe long learning curve  required by complicated and badly designed software was \nmentioned by several participants. Musicians have to spend their valuable time learning software, \nrather than creating music.  \nSoftware compatibility  was another issue raised. One participant found that one new \nversion of software had a different data format to the old version. Consequently, he had to enter \nall of his music again. Other respondents had similar exp eriences. One said that he has kept an \nancient computer just to be able to print out his master’s thesis composition. Compatibility is also \na problem between different vendors, particularly in the absence of widely used standards for \nnotation representatio n. Hand -writing music on paper first also poses problems. As mentioned above, half of the \nparticipants write music by hand, and some then further develop or arrange their idea on the \ncomputer. In order to do so, they must enter the already -written music by  using computer or \nsynthesiser keyboards. This creates double handling. In particular, when the composition is large, \nsuch as an orchestral score, the whole process of moving handwritten music from paper to \ncomputer becomes very cumbersome. One musician wi th two children finds quality time is \nprecious and limited. Once she completes an arrangement or composition, she does not want to \ntake the time to copy or format the parts. She just wishes someone or something could take it \nfrom there and finish the routi ne work.  \nWe have also observed that another problem with paper documents is the choice of writing \nimplement. A pen is better if the music is to be photocopied and distributed, but a pencil is more \nflexible for correction of errors. Computer manuscripts off er the best of both worlds: they \nphotocopy well, yet they are amenable to changes including copying phrases, transposition, and \npart splitting. Moreover, music written on the computer can be played back for ‘proof reading’ \nfor rehearsal.  \nWe were also inte rested in which criteria are most useful for searching for a piece of music \nin personal collections, music libraries or music shops. In the future the distinction between these \nis likely to be blurred by digital libraries. The survey showed that composer, title and instrument \nwould be the most popular criteria for searching a digital music library. These can all be extracted \nfrom the documents’ text, without using Optical Music Recognition.  \nOne musician said that he would like a database collection that cat egorises music according \nto musical content – time signature, type of piece, source, and melodic characteristics. When he \ngets results from searches he would like to be able to see the themes, not just their titles. Another \nmusician would like a completely  customizable database. Other ideas suggested for a music \ndatabase are: recording files in a database directly linked to the midi software, comparing audio \ninput and scores to find a title, and entering a melody on a keyboard and have the computer \nsearch f or close matches. We note that some of these features are already available in \nexperimental systems such as the New Zealand Digital Library [Bainbridge et. al. 1999].  \nWe also asked the musicians to suggest improvements to existing music information \nsystems . \nOne musician surveyed pointed out the difficulty of scanning bound sheet music and books. \nA fast system for automatically scanning these would be invaluable for converting from the paper \ndomain to digital form. This might be achieved by a high -resolution  overhead camera that could \ntake ‘snapshots’ as the user turns the pages.  \nMany of the musicians surveyed would like a page turning device. On printed music, page \nbreaks occur during a rest or one -handed section if possible. However, if there is no suitable  \nposition, unless playing from memory, musicians are required to stop playing to turn the page. In \nan orchestra the music is typically shared between two players, so one stops to turn the page, \nresulting in a ‘dip’ in the music level [Graefe et. al. 1996].  It would be very helpful if there were a \ndigital music display with a system that could follow the music and do the page turning for the \nperformer. Some research has been conducted [Graefe et. al. 1996, McPherson 1999, Bellini \n1999] but the quality, bulki ness, cost and reliability of computer screens make a paper based \nsystem more attractive at present.  \nAnother problem raised was the unreliability of scanning and recognising hand -written \nscores and old imprints. Even a system that is accurate 99% of the ti me could have an error in \nevery few bars of a complex piano piece. The time taken to find and correct such errors may not \nbe much less than the time to enter the music correctly by hand in the first place [Hewlett and Selfridge -Field 1994]. For older music  or handwritten music being studied by a musicologist, \nexisting OMR systems are not able to do a very accurate job.  \nOther things that the musicians surveyed would find useful include: a virtual accompanist; \nvideo and music being integrated more elegantly; compression technologies improved to save \nhard disk space; more efficient and stable software; and a ‘brilliant’ pen input system.  \n4. Music processing using colour  \nFrom the surveys it was clear that composers and arrangers would benefit greatly from a \nsystem that allows them to write music by hand and later transfer it to the digital domain. \nThe recognition of handwritten music is difficult, but if the writer knows that the music \nwill be transferred to a computer at a later stage, then the manuscript and n otation can be \nadapted to increase the chances of this being done accurately.  \nPreviously we had explored the use of a pen -based computer so that the computer was \nable to track all writing [Anstice et. al. 1996, Ng et. al. 1998], but this required specialis t \nequipment that to date has not become commonplace.  \nAnother approach is to use color on the manuscript to simplify the task of separating \nsymbols from the stave line [Lin and Bell 1999]. A major source of error in OMR \nsystems is trying to determine the sh apes of objects superimposed on a stave. A simple \nand effective way of using color is to print the manuscript lines in a color (such as red), \nso that color as well as position can be used to distinguish any handwritten symbols. \nSince inexpensive color inkj et printers and color scanners are available and widely used, \nthis method is readily available to any user who has access to a fairly conventional \ncomputer setup.  \nHowever, identifying colored staves is not as simple as matching pixels of a given color, \nas the color of a scanned line can be quite variable, and imperfections in printing the \nimage can mean that a line that appears to be red can contain many different colored \npixels. Likewise, grey or black writing on the stave can vary in color, and where it \nintersects with the colored lines there can be smudging (where the colored ink is picked \nup by the pen or pencil) or blurring (where the ink shows through the superimposed \nsymbol).  \nTo avoid these problems we use traditional stave line identification techniq ues to provide \nclues about where the boundaries of symbols are expected to be. The variation in colors is \novercome by mapping the scanned colors into a color space that distinguishes the key \nfeatures of the different colors we are looking for.  \n \nFigure 2: Two sample pieces of music with blue staff lines  Figure 2 shows two sample pieces of music that have bright blue staff lines and grey \nnotation written using a 4B pencil. The bright blue staff lines were printed one point thick with \nthe bright blue color (R GB 0,0,1), and then printed out onto white paper by an inkjet printer at a \nresolution of 300dpi. The notation was written by hand using a 4B pencil, and then the image was \nscanned through an inexpensive color scanner with default scan settings at a resolut ion of 300dpi.  \nThere are three colors that need to be distinguished in the image: the white background, the \ngrey symbols, and the blue lines. In the scanned image there is substantial variation in the \nscanned colors of pixels belonging to all three of thes e sets.  \nWe found it useful to transform the colors to the HSV (Hue, Saturation, Value) color space, \nwhich matches human perception better than the RGB (Red, Green, Blue) space used by scanners \nand screens, or CMYK (Cyan, Magenta, Yellow, Black) used by pri nters. The Hue identifies a \ncolor of the spectrum, and the Saturation indicates how intense the color is. The Saturation is \nparticularly useful for distinguishing the blue stave lines because a value significantly greater \nthan zero indicates that a pixel i s ‘colorful’ instead of being a shade of grey (from black to white). \nA nice side effect of using saturation is that it does not matter which color the stave lines are as \nlong as they are not a shade of grey.  \nExperiments have shown that a saturation of more  than 20% indicates a colored stave, while \n2 to 10% indicates pencil writing, and less than 2% indicates the white background.  \nWe also found it useful to convert an image to the CMYK color space to get additional \ninformation. The K (Black) value indicates the ‘darkness’ of a pixel, and was useful for \nidentifying the writing that tended to have a higher K value. Because CMYK uses four \nparameters to describe a three -dimensional space, it is under -constrained, and several different \nformulae are available to ca lculate K. The most trivial is to use K=0, since the three other colors, \nCMY, also cover a valid color space, although this provides no help for our system. The \nmaximum value that could be used for K is the minimum value of C, M and Y in a CMY \nrepresentati on. This value is useful, although we found that some of the more sophisticated \nformulae used by commercial systems worked even better. These formulae are not widely \navailable because they are commercially sensitive, and could only be inferred from samples  of \ncolors given to the systems.  \nFigure 3 shows the result of using these techniques to extract the pencil symbols from the \nblue stave -lines of Figure 2. We can see that the shapes of the grey notations have been almost \nentirely preserved and the stave -lines are almost completely removed. The accuracy is even better \nif a black pen is used instead of pencil, although this is less flexible for the user.  \n \n \nFigure 3: The sample of music in Figure 2 after the stave -lines have been removed  We have also experimen ted with using color for the annotation of music, and have found \nthat similar processing techniques to those described above can be very effective for extracting \nnotation. For example, and arranger might use a colored pen to mark a section of music to be \ncopied, deleted, or transposed.  \n5. Conclusion  \nThere is a demand for merging the paper and digital domains for musicians who would \nlike to be able to take advantage of the ease of use of paper documents, and the flexibility \nand retrieval properties of digita l documents. Giving composers and arrangers the option \nof working with pencil in the paper domain eliminates the cognitive hurdle of using \nsoftware while engaged in a creative endeavour. The survey has identified a number of \nfeatures that would be useful i n systems that operate in both domains. By using colored \nstave -lines the conversion to digital semantic data is provided with extra information to \nhelp with the accurate removal of stave -lines, and to help identify special symbols.  \nReferences  \nJ Anstice, T Bell, A Cockburn and M Setchell: The design of a pen -based musical input system, \nOZCHI 96 (Hamilton, New Zealand)  (1996) 260 -267 \nD Bainbridge, C Nevill -Manning, I Witten, L Smith and R McNab: Towards a digital library of \npopular music, The 4th ACM conferen ce on Digital Libraries, ( Berkeley)  (1999) 161 -169. \nP Bellini, F Fioravanti and P Nesi:: Managing Music in Orchestras, IEEE Computer, 32(9), \n(1999) 26 -34 \nC. Graefe, D Wahila, J Maguire and O Dasna: Designing the muse: A digital music stand for the \nsymphony  musician, CHI 96 (Vancouver, BC, Canada)  (1996) 436 -441 \nW B Hewlett and E Selfridge -Field: How practical is music recognition as an input method?, in  \nComputing and Musicology: An international directory of applications , CCARH, \nStanford, California (1994) 159-166 \nK Lin and T Bell: Music processing using colour, IVCNZ99 (Christchurch, New Zealand)  (1999)  \nJ McPherson 1999: Page Turning — score automation for musicians, Honours project report, \nDepartment of Computer Science, University of Canterbury, Christchu rch, NZ (1999)  \nT Merz: Postscript and Acrobat PDF, Springer -Verlag, Berlin (1997)  \nE Ng, T Bell and A Cockburn: Improvements to a pen -based musical input system, OZCHI 98 \n(1998)"
    },
    {
        "title": "Mel Frequency Cepstral Coefficients for Music Modeling.",
        "author": [
            "Beth Logan"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416444",
        "url": "https://doi.org/10.5281/zenodo.1416444",
        "ee": "https://zenodo.org/records/1416444/files/Logan00.pdf",
        "abstract": "We examine in some detail Mel Frequency Cepstral Coefficients (MFCCs) - the dominant features used for speech recognition - and investigate their applicability to modeling music. In particular, we examine two of the main assumptions of the process of forming MFCCs: the use of the Mel frequency scale to model the spectra; and the use of the Discrete Cosine Transform (DCT) to decorrelate the Mel-spectral vectors. We examine the first assumption in the context of speech/music discrimination. Our results show that the use of the Mel scale for modeling music is at least not harmful for this problem, although further experimentation is needed to verify that this is the optimal scale in the general case. We investigate the second assumption by examining the basis vectors of the theoretically optimal transform to decorrelate music and speech spectral vectors. Our results demonstrate that the use of the DCT to decorrelate vectors is appropriate for both speech and music spectra.",
        "zenodo_id": 1416444,
        "dblp_key": "conf/ismir/Logan00",
        "keywords": [
            "Mel Frequency Cepstral Coefficients (MFCCs)",
            "speech recognition",
            "music modeling",
            "Mel frequency scale",
            "Discrete Cosine Transform (DCT)",
            "speech/music discrimination",
            "optimal scale",
            "theoretical optimal transform",
            "decorrelate vectors",
            "speech and music spectra"
        ],
        "content": "\u0000\u0002\u0001\u0004\u0003\u0006\u0005\b\u0007\t\u0001\u000b\n\r\f\u000e\u0001\u0010\u000f\u000e\u0011\u0013\u0012\u0015\u0014\u0016\u0001\u0010\u0017\u0019\u0018\u001b\u001a\u001c\u0007\u001e\u001d\u001f\u0003\u0006\u0014! \"\u0001\u0013#$\u0011\u0010%&\u0001\u0004\u000f'\u001a(\u0018*)+ ,\u0007-\u0000\u0002\f.\u0018(%&\u0011/\u0000\u0002 \"0\u0019\u0001\u0004\u00031%1\u000f.23\r4+57698;:=<\u001b>@?A>@BDC\u001eEGFIH\u001e<@4KJ\b4MLN4O>PENQR6S8T>@C\u0010:=EN>P57:=EVUA:=BXWY>@Z\nA:@B\u0006W\u001e[(574\\E\nA:=EGW\u0004:@E7>P5NFI:=?]?\u001e4\nA>@BDC\u001eEGFIH\u001e<@4\nA4\\?\u001b574MEA>@BDC\u001eEGFIH\u001e<@4_^9`ba\u001bc(dOefc3\r4+576hgi8;:=<\u001b>@?\u0013j\u000eQM:=B\u0006W\u001e>@Z\u0013giQM:=Bkml\u000bnRo+p\\qfrsotvu\u001fu7w\u001bxsyKz|{fu}z|{~1yKu\u001f\u001buR&xsz|PXuP\u001b&u@\u001bu{f7.uR(~11Vxs@Pu7mRz|u{&~\u0006\u0004~GY\t&\u001bu\u001ffyKz|{fxs{PuxO&\u001bVu~f~1u\ns~1\u001euRuV&u7s{\tz&z{mhxs{\tmz|{Psu~1&zxO1u\"&fuzxO\t\t|z|RxO\tz |z¡\u00191¢y¢=\u001bu|z|{\u001b.y.f~Vz|O£¤¥{\u0006\txO&&z|R\t|xO¦f§u.u7w\u001bxsyKz|{fu\b¡§\n&\u001bu.y_xsz|{Xxs~&~Vfy¢f&z{\t~,\n&\u001bu\u0019f&=7u~&~,\n}sVy_z|{\u001b_¨\u0004,~©&\u001bu\"f~1uª\n&\u001bu\"\u0006u\n&u@\u001bu{f7m~&Rxsuª1¢y¢@fu(&fuª~&\u001cu71Gx=«=xs{\tm&fuª\t~1u'\n&\u001bu\"¬ªz|~&7&uR1u\b~&z {\u001bu­\nVxs{\t~\nsGy®¬­\n\u001f1mfu7s&&u|xO1u&fuXu¯¡~&\u001cu71GxsYsu71sV~R£tvumu7w\u001bxsyKz|{fu_&\u001bu_°\tV~1\bxs~V~&fy¢f&z|{±z|{±&fum7{P1u7w@\u000e\n~1\u001euRuV\u001c²\\y.f~Vz|Kfz|~V7Vz|yKz|{\txO&z{Y£D³\"\u001b&u~&f&~}~&\u001b+§!&fxO\u001f&\u001buf~1u\n&\u001bu\u0006u(~&Rxs|u\nsy¢=\u001bu|z|{\u001by\u0019f~&z z|~}xOuxs~1}{\u001bsfxOVy\nf\ns}&fz ~f&s\tuy´¦Pxs&\u001bf\n\u001b&&\u001buR\u001fu7w@\u001euRVz y¢u{&xO&z|{_z|~\u001f{\u001buRu\u001bum1.suRVz\n¢&fxO&fz|~\u001fz|~T&fu'sf&z yKxs\t~&Rxs|uz|{\u000e&\u001bu,su{\u001buRGxsfRxs~1us£;tvuz|{Psu~1&zxO1u'&\u001bu,~1u7{f_xs~&~&fy¢\t&z{\u000eP\u0019u7w\u001bxsyKz|{fz|{f\r&fu\txs~&z|~hsu71sV~\u001f\n&\u001bu.&fuRs&uR&z|Rxs|µs\t&z|yKxs\u00041Vxs{f~\nsVy¶1\u001bu7sV&u|xO1u¢y.\t~&z|\bxs{\t\u0006~1\u001euRuG\u0006~1\u001eu71Vxs\u0004su71sV~R£\b³ªf&u~&f&~fuy¢{f~11VxO1u\b&fxO,&\u001buf~&u\"\n&fu¬\"­\n1_\u001bu7s&Vu|xO1usu71sV~\rz|~xOffVsfVz|xO1u\ns,\u001es&~1\u001euRuVXxs{\tµy.f~Vz|\"~1\u001eu71Vx=£·X¸P¹Yº\u0019»\npM¼\u000bns½¨f~&z|ª&uR\t&u~1u{P&xO&z{Y¦fy\u0019f~&z|\nuxO&\u001b&u~R¦(\u0006\u0004\nuxO&\u001b&u~R£¾ ¿fÀ\rÁ\u001eÂYÃÄKÅ\u000eÆ\u000bÁ\u001eÇVÃ'À³\nxs|\u0004&\u001bu¢yKxs{=¡yKxsfu\u0019~1\t{ff~'§fz|G\u0006z|{\u001bÈ(\u001bu{f7ufª zsu~R¦Y~1\u001euRuVÉxs{fÉy\u0019f~&z|.xO&u¢xO&fxO\tµ&fuy¢~1\u0019f&|z¯°\u001cO£´Ê=\u001cuRuG9\txs~.{fX\u001euRu{±\u001euRV7uzsuËxs~\u0019x\u0006{fxO&fVxs}z|{P1uR\nxs7um\u001euRÌ§\u001fuRu{Ë\u001euRs\tu_xs{\t7y¢\t\u001b1uRV~\u000bxs{f\u0019\u001bu{f7ufxs~\u0010Vu7uzsu.y\u0019fG\n@R\t~1u.xO11u{P&z{Y£;¬\ruRxs\u001bu~\u000b\n&u~1uxOGV\u000ez|{&fu\u001f~1\u001euRuV7yKy.\t{fz¡µfxs~'u´1f~&xO\t|u~1@~&1uyK~xs{f\u00067{suRVsu{f7u.\n&fu\nuxO&f&u~'xs{\t´y¢=\u001bu|~,f~&u\ns~1\u001euRuVXxs{\txs@~Vz|~R£¤¥{\u0016&fu±y.f~Vz|É7yKy.\t{fz¡!\u001b+§uRsuR¦\"xs&\u001bf\u0016&\u001buÉ°\tu|!\n~1@{P&\u001bu~&z ~z ~DsuR&!y_xO&\u001b&us¦ªx\u001byKz|{fxs{P(xOVxsfzy¶fxs~,suR\r1muyKuR&su.1D~1su.s&\u001buRªf&s\t|uyK~,~&fG\u0006xs~\ry\u0019f~&z|R xs~&~&z¯°(RxO&z|{s.1Vxs{\t~&7Vzf&z|{Y£D{\t~1uPfu{&s¦hyKxs{Pv&uRf&u~&u{&xO&z{\t~\ns.y\u0019f~&z|K\txsu\u001cuRu{±f&s\u001e~1uÍIus£Î\u001b£\u0006xO&&z {Y¦=Ê=G\u001buz&uRÏ\u0016Ð;uRV7@u.ÑÒsÒsÓG¦\u0013\u0006xOV&z|{´ÑÒsÒsÓG¦\u0013ÔÊ=Vfuz&uR\u001fÏÕÊ\u001b|xs{\u001buRDÑÒsÒÖsG¦Y×|\tyµ¦Ø\"uz|~V|xO¦t!\u001buxO1{±ÏÙtv|SÑÒsÒsÒ1G£¤Ú{\u0006&fz|~'\txO\u001euR¦(§u\u0019u7w=xsyKz {\u001bu\u0019~&y¢u\u000e\n&\u001bu\u000exs~&~&fy¢\t&z{f~,\nXuh\u001b&u7P\u001bu{\t7µuR(~11Vxs\u0010@u7mRzu{P&~\u0006\u000b,~G\u001fT&fu\u001byKz|{fxs{P\nuxO&f&u~,f~1u\ns~1\u001euRuGµ&u7s{fz&z|{\u001fxs{fµu7w\u001bxsyKz|{\u001bu\"§'\u001buR&\u001buR&\u001bu~1u.xs~&~&fy¢\t&z{f~xO&uOxs|z|\ns'y¢=\u001bu|z {\u001b\u0019y\u0019f~&z O£\u0006\u000b,~\u000bxO&u}~&\u001bsVÚÔ1uRVyÕ~1\u001eu71VxsÔ\txs~1u\nuxO&\u001b&u~£­\nfuR&u\nsVus¦\ns\u0013&\u001bu\u001f\t\u001b&\u001e~1u~\u001e\n&fz|~\u0013\txO\u001euR¦§\u001fu\u000e§z| \u0013xs~&~&fyKu.&fxO\r§u\u000exOVu.{f|´z|{1uRVu~11uÉz|{\u0006~V\u001bs&ÚÔ1uRVy$~1\u001eu71Vxs¯Ô(xs~1u\nuxO&\u001b&u~ª\ny.f~Vz|O£t!fz u\bzz ~,RuxO\r&fxOs&\u001buR\nuxO&\u001b&u~\ryKzP,\u001eu\nf~1u\ns'x_\txO&&z|R\t|xO,f&s(uy®Ius£Î\u001b£VPP&\tys¢\u001cuxO&~\u0019s¢xs{PÛs&\u001buR\u000e\n&fu\nuxO&\u001b&u~\u000e&u\nuR&u{f7uSxO\u001e+suMG¦z\u000ez|~.xs ~1¨R|uxO¢&fxO.&fuD~&\u001cu71Gxs7y¢\u001e~&z&z{Ë\nx¨~Vz{fxs7{&xsz {f~Ky\u0019fVÜz|{\nsVy_xO&z{Y£­\nfz ~\u0019z {\nsGyKxO&z{Ë7f|Ü7uR&&xsz|{\t±\u001euxs\u001by¢u{P1u´@Dxs\tfz&z{fxs\nuxO&\u001b&u~'z\n&u@fzVuDsxsRRfy\u0019f|xO1u´MsuR'{fsuR,&z|y¢u\"§z {f\u001b+§'~R£\u0006\u000b,~hfxsu\u001euRu{\bf~&u@\"s&\u001buR\u000bxsf&\u001bsV~\u00131ªy¢=\u001buy\u0019f~&z Txs{f\bxs\tfz~1\t{ff~R£\u000b\u001bs\u0004u7w\u001bxsy¢\t|us¦z|{m\u001b@s1u\"ÑÒsÒÖs\u0013x&uR1VzuROxs~&@~11uy-z|~\u0010\tfz|\u001e\txs~&u{\u0019x7uR\t~11Gxs&uRfVu~1u{&xO&z|{\b\n~1\t{ff~R£\u0010×|\tyuRR£TxsÝ£;xs|~1|z|~&T\u0006\u0004~}xs~;{fu,\n&\u001bu\nuxO&\u001b&u~}z {\u000e&\u001buzT&uR1GzuR\\xs\t~1@~&1uyb×|fyÞuR}xsÔ£\tÑÒsÒsÒG£}ßy.f~Vz|~&fyKyKxOGzàxO&z{\u0019~1@~&1uyÞ\txs~1u\u0019{¢7uR\t~11Gxs\nuxO&\u001bVu~Tz|~h\u001bu~&7Vz|\u001cu\u0019z|{µá\u0013sxs{mÏâP¢ãOäsäsäG£­\n\u001bu~1u\"§s&å=~,\u001b+§uRsuRªf~1u7uR\t~11Gxs\nuxO&f&u~,y¢uR&u|m\u001euRxsf~1u\"&fuRfx+su\b\u001euRu{µ~1_~VfR7u~&~\nf\ns~1\u001euRuV´Vu7s{fz&z{µ§'z&\u001b\u001bu7w\u001bxsyKz|{fz|{f\u0019&fuxs~&~&fy¢\t&z{f~yKxs\u001bu.z|{Ds&uxO\u001buR&xsz Ý£­\n\u001buµs&xs{\tzàxO&z{Ü\n&fz|~\u000e\txO\u001euRKz|~¢xs~\n||+§~R£Stvuµ°(V~1K\u001bu~&7Gz\u001eu¨\u0004,~\ns_yK@\u001bu z|{\u001b~1\u001euRuVY£tvu\u0019&\u001bu{Xz {su~1&z|xO1u.§fuR&\u001buR'~1uRsuRGxs\u0010\n&\u001bu\u0019xs~&~&fy¢\t&z{f~,yKxsfu\b§\u001bu{\nsVy_z|{\u001bK&\u001bu~&uuxO&\u001bVu~;xO&uxO\tf&sfVz xO1u\ns;y.f~Vz|O£\u0004hz|{fxs ª§\u001fuf&u~&u{h7{fR|f~Vz{f~\u000bxs{f.~V\u001bssu~1&z{f~\ns\n\u001b&f&u§\u001fs&å\u001c£æ ç èÉéÕé\u0016êìëÃ'Âîí.ïðTðTÆ\u0010ñ\u0015òÞðTÆ;Ã'ó'À¢ÇÚÁYÇ&Ã\rÀ\u0006\u0004~.\txsu\u001cuRu{Û&\u001bumfyKz|{fxs{P\nuxO&\u001b&u~\u0019f~1u\ns\u000e~1\u001euRuV±Vu7s{fz&z{\ns\u0019~1y¢um&z|y¢u¨Ius£Î\u001b£õôTf{\u001b\u001b¦@tvP=f|xs{fmÏâ×PG{\u001bu\bÑÒsÒsö1G£­\n\u001buzT~&fR7u~V~}fxs~}\u001cuRu{mf\u001bu1.&\u001buz}xO\tz |z¡.1\b&uR\t&u~1u{P&\u001bu~&\u001cuRuG9xsyK\t|z&ffu_~1\u001eu71Vfy÷z|{Ûx¨7y¢\txs7\nsVyµ£Xø\u001fxsVS~11uRÜz|{±&fu_f&=7u~&~.\n7&uxO&z|{f\u0006\u0004\nuxO&f&u~\"z|~ªy¢s&zOxO1uÉP\u0006\u001euRV7uRf&fxs\u000bs7y¢\tf&xO&z{fxsh7{f~&z|fuRVxO&z{f~R£\"tvu_u7w=xsyKz {\u001bu&\u001bu~1u\b~11uR(~,z|{Dy¢s&u\bfuR&xsz|Yz|{&\u001bu\n|M§z|{\u001b\u0019\txOVxOsVxO(f~R£Tß-y¢s&u7y¢\t|uR1u\b\u001bu~&7Vz|f&z{\n&fuÑf&=7u~&~,xs{f´xs~V~&fy¢f&z|{f~z|~zsu{´z {ÛùxO\tz {\u001buRÏûú\txs{\u001b´ÑÒsÒsöG£hz\u001b&uµÑ¢~&\u001bM§~ª&\u001bu\u000ef&=7u~&~ª\n7&uxO&z|{\u001b´¨\u0004,\nuxO&\u001b&u~R£­\n\u001bu.°\tV~&ª~&1uR±z|~\r1´fz=z|\u001bu\b&fu~1\u001euRuVD~&z{fxs\u001cz|{P1\nVxsyKu~R¦@\t~&fxs|\u000e@_xOf\t|@z|{f\bx\u0019§z|{f\u001bM§z|{\u001b\nf{f7&z|{mxO°fw=uDz|{1uRV\\xs|~£­\n\u001buxsz|yü\u001buR&u\u0006z ~_1Üy¢=\u001bu\r~&yKxs|\u0019I¡@\tz|Rxs|!ãOäsyK~G_~1u7&z{f~\n&\u001buv~&z|{fxs,&fxOµxO&uÉ~1&xO&z|~&&z|Rxs|~1&xO&z{fxO&s£­\n\u001bu.§z|{f\u001bM§\nf{f7&z{\u0013¦\t¡@\tz|Rxs µx_ý\rxsyKyKz|{\u001bm§z|{ff+§¦(&uy¢+su~\ru\u001bsu\u0019u7þ\u001eu7&~R£'tvusu{\u001buRVxO1u.x_7uR\t~11Vxs\nuxO&\u001b&u\bsu71s\ns,uxsV\nVxsy¢us£\nConvert to Frames\nTake discrete\nFourier transform\nTake Log of \namplitude spectrum\nMel-scaling and\nsmoothing\nDiscrete cosine transformWaveform\nMFCC Featureshz\u001b&uKÑO©}ÿ}&=7u~&~1_7&uxO1u\u0019\u0006\u000b,\nuxO&\u001b&u~­\n\u001bu\"{\u001bu7w=,~11uR´z|~1¢&xOåsu&\u001bu\"¬ªz|~&7VuR1u\u001b\u001bVzuR­\nVxs{f~\nsVy ¬ª­\n\u001f\nuxsV\nVxsy¢us£;tvu&fu{&uR&xsz|{\u0019{f&fu|sxOVz&fyì\n&\u001buxsy¢(|z&f\u001bu\u001f~1\u001eu71Vfyµ£\u0004tvu,fz|~&RxOV.\tfxs~&uz|{\nsVy_xO&z{\b\u001euRxsf~&u\u001cuRG7uRf&fxs\u001c~1&ffzu~\u001ffxsu\"~&f+§{_&fxO&\u001bu'xsy¢(|z&f\u001bu\n&\u001bu\r~&\u001cu71Gfy z|~y\u0019fVy¢s&uªz|y¢\u001es&&xs{P&fxs{v&\u001bu¢\t\txs~1us£\u000etvuK&xOåsuK&\u001buKsxOGz&fy$\n&\u001buKxsy¢\t|z|&f\u001bu\u000e~1\u001eu71Vfy$\u001euRxsf~1u¢&\u001bu¢\u001euRV7uz|suff{\u001bu~V~\nx¢~Vz{fxsYfxs~\u001euRu{\nf{fD1_\u001cuxO\tf&w\u001bz|yKxO1u|sxOGz&fyKz|O£­\n\u001bu_{fu7w@\b~&1uRÛz|~\"1¨~&y¢@s&v&\u001bu_~1\u001eu71V\ty xs{fvuy¢(fxs~&zàRu¢\u001euRV7uRf&\txs|\u0006yKuxs{fz|{\u001b\nf\nVu7ãP\u001bu{\tRzu~R£­\nfz|~.z|~\bxsGfzuRsuS@Û7|u7&z|{fX&\u001buv~&xf\u0019ã\u0001\u0003\u0002\n~1\u001eu71Vxs7y¢\u001e{\u001bu{&~\u0019z|{P1S~Vxf\u0005\u0004ä&u@\u001bu{f7X\tz|{f~,xs~ª~&f+§{¨z|{¨\u000bz|\u001b&u\u0019ã@£ªß\r&\u001b\u001b\u0006{\u001bu\u0019§\u001ff Xu7w=\u001eu7'&\u001bu~&u.\tz {f~,1\u001eu\bu@fxs|~1\txs7uËz|{\n&u@\u001bu{f7s¦;z.\txs~\u001euRu{\nf{f±&fxO\ns\u000e~1\u001euRuVY¦h&\u001bu+§uR\n&u@\u001bu{fRzu~.xO&u\u001cuRG7uR\u001b&fxs|´y¢sVu\u0019z|yK\u001csV&xs{'&fxs{¨&\u001bu\u0019\tz\u001buR\n&u@\u001bu{fRz|u~R£­\nfuR&u\nsVus¦(&\u001bu\u000e\tz {X~1\txsRz {\u001b\n|M§~&fu~1O¡Rxs|u\u0007\u0006 Xu\t\b\n&u@\u001bu{f7D~&Rxsus£\naverage spectral components over bins\nsmoothed Mel spectra\u000bz|\u001b&u.ã@©Xu\u0004~&Rxs|z|{\u001bKxs{\tX~&y¢@s&fz|{f¢\n&fu\bsmxsyK\t|z&ffuª~&\u001cu71Gfyµ£Ê@\u001eu71Vxs\u00107y¢\u001e{\u001bu{&~xO&u\bxsuRGxOsu\u0006MsuR'\u0006u¯¡~1\txs7uX\tz|{f~1mf&=ff7u\"xK~&y¢@s&\u001bu´~&\u001cu71Gfyµ£­\n\u001bu\u0019Xuh~&Rxsu\u000ez|~\txs~1u\u0006{¨xyKxOf(z|{\u001b¢\u001euR¡§uRu{vxs7&fxs\n&u@\u001bu{f7´xs{\tX\u001euRV7uzsu\u0006\tz&G\u0006xs~xOf\txO&u{P&±&\u001buD@fyKxs{Sxsffz1sV±~1@~&1uy fPu~\u000e{\u001bs\u000e\u001euRV7uzsu\tz&G9z|{ËxÉ|z|{\u001buxO\u0019yKxs{f{\u001buR+£­\n\u001buyKxOf\tz|{fXz|~\bxO\tf&w\u001bz|yKxO1u|É z|{\u001buxO\b\u001euM§¶Ñ7å=ýàDxs{f9sxOVz&fy_z|KxO\u001e+sus£¨\u000bz|\u001b&uöX~&\u001bM§~\b&fuXu\nf{\t7&z{Y£­\n\u001bu7y¢\u001e{\u001bu{P&~,\n&\u001bu\u0006u¯¡~1\u001eu71VxsYsu71sG~'Rxs|R\t|xO1u\ns,uxsV\nGxsy¢uxO&ufz\tm7s&Vu7|xO1uY£Ê@\u001euRuG\nuxO&f&u~,xO&uÌP(z|Rxs|y¢@fuuDPDyKz¯w=&\u001b&u~\n\u000b\nxs\t~&~&z|xs{µ\u001bu{\t~&z&zu~R£­\nfuR&u\nsVus¦z|{µsV\u001buR1K&uff7u\"&fu\b{P\ty\b\u001euR\n\txOVxsy¢uR1uRG~z|{D&\u001bu.~1@~&1uyµ¦f&\u001bu\b xs~1,~11uRX\n¨\u0004,\nuxO&\u001bVu7{f~11Vf7&z|{Ëz|\b1ÉxOf\t±x\u00061Vxs{f~\nsVy÷1\u0006&fu\u0006u¯¡~1\u001eu71Vxs\u001fsu71sV~\u000e§fz VË\u001bu7s&&u|xO1u~\u0019&\u001buz|7y¢\u001e{\u001bu{&~£­\n\u001buRs&uR&z Rxs|s¦=&\u001buØxOV@f{\u001bu{\u001b¡á\u0013@uRsu_Øá\u000b\u001f1Vxs{\t~\nsGy Is,uP\tz\\xs|u{&mÿTVz|{\tRz\txsy¢\u001e{\u001bu{P&~ß\r{fxs=~&z|~\rÿß\"1}xsVfzuRsu~&\tz|~R£h¤¥{m&\u001buª~1\u001euRuVD7yKy\u0019f{fzÌs¦=&\u001bu\rØ\bá\u00061Gxs{f~\nsVyÙz ~xOff&+w=z|y_xO1uX@µ&\u001bu\u000e¬\rz ~&7&uR1uK~&z|{\u001bu­\nVxs{\t~\nsGy÷¬­\n.\u0006xOGfxÉÏûá\u0013uRuDÑÒsÒsöG£\r\f'~Vz|{\u001bK&fz ~1Vxs{f~\nsVyµ¦\u0013Ñö´Is\r~1P7uR\t~&1Vxs\nuxO&\u001b&u~xOVusf&xsz|{\u001bu\nsuxsG\nVxsy¢us£ö1001011021031040500100015002000250030003500\nFrequency (Hz)Melhz\u001b&u\bö@©­\n\u001bu\u0006u\u0013~&Rxsus£\u000e ç èÉéÕé\u0016êìëÃ'ÂÞç Å\u000eê\u001eÇ&Æ\u0010\u000fÙÀ\u0012\u0011\u0014\u0013\u0016\u0015\bê\u001eÇ&êß'~\u001ffz|~&Rf~V~1u\u000exO\u001cMsus¦P&fuf&=7u~&~;\nRxs|Rf|xO&z {\u001b\"\u0006\u0004~\ns}~1\u001euRuV_7{f~&z|~1&~T\n°\tsu'yKxsz|{¢~11uR\t~£ÑO£'¬ªz=z|\u001bu\"~&z{\txsYz|{1\nVxsy¢u~£ã@£ª³\r\t&xsz|{D&\u001buxsy¢\t z&f\u001bu\"~1\u001eu71Vfy´£ö@£­\nxOåsu&\u001bu\b|sxOVz&fyµ£\u0004\u001b£ª{PsuR&,1mXu\u0013~&\u001cu71Gfyµ£\u0001\n£­\nxOåsu&\u001bu\b¬­\n£tvu'{\u001bM§ ~1uRuRå\u000e1\b\u001buR1uRGyKz|{\u001bu§\u001buR&fuRT&fz|~hfV@7u~&~}z|~;~&fz&xO(u\ns}7VuxO&z|{\u001b\nuxO&\u001b&u~;1.y¢@fuy.f~Vz|O£'³\n&\u001bu.~&1uR\t~' z|~11uXxO\u001eMsus¦\u001c§u.~Vfxs|Yu7w\u001bxsyKz|{\u001bu\u0019~11uR\t~\u0017\u0004mxs{f\u0001\n£­\n\u001bu.s&\u001buRª~11uR\t~\rxO&u.u~V~7{1&MsuRV~&z|xs\u0010~&z|{f7uªy.\t~&z|O¦=|zåsuª~1\u001euRuVµz|~{\u001b{=¡~&&xO&z{fxO&\u0006~&1uR±Ñ+xs{f\tfxs~1u7¡z|{\t\u001buR\u001eu{f\u001bu{P'\nsy¢{\u001b\u0019\u0018,&u7sV\tz|{\u001b~Gª~11uR¨ãG£\u001fß\r|~1\u001b¦fxs&\u001bf´y.\t~&z|ªfxs~,x¢|xOVsuR\u001b={fxsyKz|\rVxs{\u001bsu\b&fxs{´~1\u001euRuV\u0013¦&\u001buR&u\bz|~{fK&uxs~1{µ1m~&f~1\u001eu7&fxO|ff{\u001bu~&~§'z||\u001c{\u001bs,\u001eu\"\u001euRV7uzsuµsxOVz|&fyKz|Rxs||\u0006~11uR¨öG£ýM§\u001fuRsuR+¦\u001fz\u0019z|~\u001e~&~&z|\tu¢&fxO\u000e&\u001buXu~1\u001eu71Vx\u0006yKxÛ{fs.\u001eu_s\t&z|yKxs\ns\u000ey.f~Vz|_~&z|{fxs|~\bxs~&\u001buR&uyKx+m\u001eu\"y¢s&u\"z|{\nsVyKxO&z|{z {~VxDfz\u001buR\n&uPfu{fRzu~~11uR\u001a\u0004PG£Ê\u001bz|yKz||xOGs¦&\u001bu¬­\ny_x{\u001bs\u0019xO\tf&w\u001bz|yKxO1um&\u001bumØ\báÍ1Gxs{f~\nsVy\ns\u000ey.f~Vz|X~11uR\u0001\nG£´tvuµ~&fxs ;&\u001buR&u\ns&uu7w=xsyKz {\u001bu_&\u001bu~&uxs~&~&fy¢\t&z{f~z|{D&\u001bu\n||+§z|{f\u0019~1u7&z|{f~R£\u001b\u001d\u001c\u001f\u001e! #\"$\"\u0003 #\u001e\t%\u0016&\u0017'( )\u001e*'(%,+(%- \u0004.0/\u001d1 2\u0010354\u0017687:9<;>=83@?BADCFEG3IHKJLAM?B4N2\u0010OQP\u000b354\u001d;>=8R­\n z|{Psu~1&zxO1uv§'\u001buR&\u001buR´zµz|~DxOffVsfVz|xO1u¨1 \u001buR1uRVyKz {\u001bu\u0006&fu±~1\u001eu71Vfy\u0015\ny\u0019f~&z|¨f~&z {\u001b9&fuXuT~VRxsus¦\u0013§u¢u7w=xsyKz {\u001bu¢&\u001bu¢\u001euR\nsVyKxs{f7u\u000e\nxµ~Vz|y¢\tu\u000e~1\u001euRuV\u001c²\\y.f~Vz|\u000efz|~&7Vz yKz|{fxO1s£\btvu_fxsuxOxsz||xO\tu\u000exO&f{\t¨öm\u001b\u001bV~'\n|xO\u001euu¨z|{=¡ff~1u\bfxO&x\n&y$x_\t&xsfRxs~1\"{\u001buR§~\r~&\u001bM§£­\n\u001bu\u0019~&\u001b+§7{&xsz|{\t~\rz {1uR&=zuR§~'xs{f¨7yKy¢uRVRz|xs ~'xs{f¨fxs~\rxm{@fy.\u001euR'\n~1uRyKu{&~ª\ny\u0019f~&z O£­\n\u001bu\u0019fxO&xmz ~fz=z|\u001bu¨z|{1XãDf\u001bV~\r\n1Gxsz|{fz|{\u001bDfxO&xµxs{fS\u0004äµy_z|{Pf1u~ª\n1u~1&z|{f´fxO&x=£¢ß&\t{fSÑRäMT \n&fu1Vxsz|{fz|{f\u000e\txO&xKxs{f±ÑU\u0004VT \n&\u001bu\b1u~1&z|{\u001bKfxO&x_z|~,|xO\u001euuDxs~y\u0019f~&z O£tvuX7{PsuR&K&\u001buµ1Vxsz {fz|{\u001bÉfxO&xv1W\u0006 \u0006u\t\bTxs{fX\u0006 á\u0004z|{\u001buxOY\b;7uR\t~11Gxs\nuxO&\u001bVu~Kxs~\n|+§'~R£­\n\u001bu~&z{fxs;z|~ª~&xsyK\tuvxOKÑ\u0002\nå=ýà_xs{\t±7{suRV1u±1Xã\u0001\n£\u0002\nyK~\nVxsyKu~\"+suRV xOf\u001euÉPËÑRäsyK~R£K\u001bs&fu\u0006 Xu\t\b\nuxO&\u001bVu~R¦§u,&\u001bu{K7{PsuR&\u001fuxsV\nVxsy¢u1\u0005\u0004ä\u0006u¯¡~1\txs7u\n&u@\u001bu{f7\u000e7y¢\u001e{\u001bu{P&~R£;\u001bs}&fu\u0006 á\u0010z|{\u001buxOZ\b\nuxO&\u001b&u~¦\u0010xµ|z|{\u001buxO\n&u@\u001bu{f7\u0006~VRxsu¢z|~\"f~1uY£\u0019ù,u\nuR&Vz|{fm1´hz\u001bVu¢ã@¦\u0010&fz|~\"7s&&u~&\u001c{\tf~1´f~&z|{\u001bD\u0006u¯¡~1\txs7uv(z|{f~\rz|{¨&\u001bu\u0019°\tV~1\"Rxs~1uKxs{fÉ|z {\u001buxOVP¡~1\txs7u¨\tz|{\t~'z|{¨&\u001bu¢~1u7{\tY£.¤Ú{¨\u001es&Rxs~1u~R¦\u001b§u\r&fu{D&xOåsu\"&\u001bu\r|sxOVz&fy xs{fD¬­\nxs~f~&fxs(1\u000esf&xsz|{ÉÑö\u000e7uR\t~11Gxs\nuxO&\u001b&u~\nsuxsVVxsyKus£\f'~Vz|{\u001b,x,~1&xs{\tfxOVªsuRV~&z|{\"\n&fu}ø;w=\u001eu7&xO&z{=¡¨x\\w=z|y_zàxO&z{_øT¡É\u001exs|ssVz&fyûIus£Î\u001b£,×xsfyµ¦ÿ\u0004uR1Gzus¦@Ê@f|u~hÏÍtvuz|~V~ÑÒÖ\\ä1G¦P§u1Vxsz|{\u000eyKz¯w=&\u001b&u\nB\nxs\t~&~&z|xs{\u0019R|xs~&~&z°\tuRV~\nsh&\u001bu|xO\u001euu\u0019~1\u001euRuVxs{f´y\u0019f~&z|\"~1uRyKu{&~'z|{µ&\u001bu\"1Vxsz {fz|{\u001b¢fxO&x=£³ª\u001b\n\nxsf~V~&z|xs{´\u001bu{\t~&z&zu~fx+su\bfz|xOs{\txs\u00137+OxOVz|xs{f7uyKxO1Vz|7u~,~Vz|{f7uª§\u001fuxs~V~&fy¢u\"&fxOfÑö¢7uR\t~11VxsY7yK\u001c{fu{&~,xOVuf{f7s&&u xO1uY£;tvu&\u001bu{µR|xs~1~&z\nµuxsVÉ~1uRy¢u{P\"z|{´&\u001bu\u00191u~1ªfxO&xmP´&fu.y¢=\u001bu\u0010~1\u001euRuV¨sªy.\t~&z|\"§fz VX\txs~&\u001bu\u000efzfu~1xsuRVxOsu\u000e|zåsu|z \u001bP=m+suR\r&\u001bu1u~1~&uRy¢u{R£­\nxO(uKÑ~&\u001bM§~&\u001bu~1uRyKu{&xO&z{\u0006uR&&s\nsy¢=\u001bu|~§'z&D\\xOV@z|{f\u000e{@fy.\u001euR\nyKzw@&\u001bVu\r7y\u000e\u001c{fu{&~\u000e\t\tz|\b§'z&Ë&\u001buD¡§±fz¯þ\u001euR&u{\nuxO&\u001b&u~\u000eÌP\u001eu~R£9¤Ì¢~&\u001bM§~\u000e&fxO\ns¢&fz|~\u000e~1\u001euRuV\u001c²\\y.f~Vz|R|xs~&~&z¯°\u001cRxO&z{Û\t&s\tuyµ¦h&\u001buD&u~Vf&~\bxOVuv~&&xO&z|~1&z|Rxs||\u001b\b~&z{fz°(Rxs{&|¨\u001euR11uR¢z\n\u0006u¯Ô\txs~&uË7uRf~11Vxs\nuxO&\u001b&u~VxO&\u001buR,&\txs{X|z|{fuxO1Ô\txs~1uµ7uR\t~&1Vxs\nuxO&\u001b&u~xOVu\bf~1uY£\u001fýM§\u001fuRsuR¦\u001c§\u001buR&fuR&fz|~z ~~&z|y¢\t|\b\u001euRxsf~1u'&\u001bu\u0006u\t~&Rxsu\ry¢=\u001bu|~T~1\u001euRuG_\u001euR11uR}s\u001f\u001euRxsf~1u'z|}xs|~1\byK@\u001bu ~Ty.\t~&z|\u001euR11uR\u001fz ~{\u001bs;RuxO£h¤Ìhz|~hxs|~1ª{\u001bs;RuxO;§fuR&\u001buR\u000bxªfzþ\u001cuR&u{P\n&u@\u001bu{f7\b§xO&(z|{\u001bªxs{f(²Ms;xª|xO&suR;~&xsy¢\t z|{\u001bVxO1u§f|D\u001euª\u001cu{fu7°(Rz|xsÝ£;ß,§sV~1R¦\u001b§u\bRxs{´7{fR f\u001buª&fxOf~Vz|{\u001b\u0019&\u001bu\b\u0006uY7uR\t~11Vfy/1Ky¢@fuy.f~Vz|,z|{¢&\tz|~T~1\u001euRuV\u001c²\\y.f~Vz|,fz|~V7Vz|yKz|{\txO&z{\u0019f&s\tuy z|~T{\u001bsfxOVy\nfÝ£\u000bf\u001b&&\u001buR\u001f1u~1&~xO&u'{\u001buRufu1KsuRVz\nm&\txO,&\u001bu\b\u0006u\u00137uR\t~11Gfy/z|~xOf\t&sfVz|xO1u\nsy¢=\u001bu|z {\u001b\u0019y\u0019f~&z \"z|{D&\u001bu\"su{\u001buRVxs\u0004Rxs~1us£.0/\\[ ]\u00077L;\u001d=\u000bR^J`_83WacbedfJ\u0003OhgSE\u000bEiA`O5j8;>kl?mJ\u00033\u0007J`_\u000b3Wnc9odpA`?q=\u000b7sr-O!AMktvuª{f+§*z|{Psu~1&zxO1u\"&\u001bu\ru7þ\u001cu7&z|su{\u001bu~&~\nf~Vz|{\u001b&\u001bu\"¬­\n1¢\u001bu7s&&u|xO1u\"\u0006u(~&\u001cu71Gxs\nuxO&\u001b&u~£ß'~¢y¢u{P&z{\u001buÜz {ÜÊ=u7&z{ ã@¦}&\u001buµ7s&Vu7¢§x+Ë1v\u001bu7sV&u|xO1u´7y¢\u001e{\u001bu{P&~Kz|~\u00191±f~1u&\u001bu´Øá1Vxs{f~\nsVyµ£ûtvuv§'z||,&fuR&u\nsVuÉ°\tV~1Dz|{Psu~1&zxO1u±\u001bM§ §\u001fu|\r&\u001buv¬­\nxOf\t&w\u001bz|yKxO1u~D&\u001buvØá1Vxs{f~\nsVy\ns~1\u001euRuV\u0006~1\u001eu71Vx=£­\n\u001bu\bØ\báÛ1Vxs{\t~\nsGy/7{suRV&~'su71s\u0005t9\nfz|yKu{f~&z{eu 1msu71spvË\nfz|y¢u{\t~&z{xw±§fuR&uwzy{u xs{fÜ&fuX7y¢\u001e{\u001bu{P&~K\nvìxOVuXf{f7s&Vu|xO1uìIus£Î\u001b£Þ\n\nfxs{\u001buy ÑÒsÒ=Ñ+1G£­\nfz ~¢Rxs{ \u001eu\u0001|'\ty\b\u001euR Ê@\u001eu71Vfy Ê@uRyKu{&xs\u0006zw\u001c£ ø}&&s\b(TK\u0004 \u0006u ÖP£|Ñá\u0010z|{\u001buxO ÑU\u0004\u001b£ öÓ \u0006u ÑO£ Óá\u0010z|{\u001buxO Ó@£ ÒV}Ñ\u0002\n\u0006u ö@£\u0002á\u0010z|{\u001buxO ÑRä=£iÖM}­\nxO\t|uÑO©' xs~&~&z¯°(RxO&z|{´&u~&\t&~\ns\r1u~1&z|{\u001bDfxO&xm1u~11uÉ§z&\u0006y¢@fu|~§'z&´OxO&=z|{\u001bm{@fy.\u001euR\nyKz¯w=&\u001b&u7y¢\u001e{\u001bu{P&~,xs{f´\tz¯þ\u001cuRVu{ÌP\u001eu~\n~1\u001eu71Vx=£G}\u0019z|{ffz|RxO1u~Vu~&f,z|~~Vz{fz¯°(Rxs{P&m§\u001fsV~&u&fxs{µ&\u001buVu~&fz|yKyKufz|xO1uxO\u001cMsu\b§z|&´Ò\u0001\nT 7{=°(fu{f7uf~&z|{f\u000ex_\u0006xO&G\u001bu=¡ÿhxszV~1u~1R£u7w@fVu~&~1u´xs~vS~t\u000b ÚÑ+ýuR&ubz|~&\u001buw\u001ewKu 1Gxs{f~\nsVyKxO&z{´y_xO1Vz¯w\u001e£\u001f¤¥Rxs{´\u001eu\b~V\u001b+§{´&fxO&\u001bu\u00197y¢\u001e{\u001bu{P&~,\nv9§'z||\u001cu,\t{f7s&&u|xO1u¢z\n&\u001buV+§~ûxO&u&fu,s&&\u001b{\u001bsGyKxs|zàRu\u000euzsu{Psu71sV~\u001f\n&fu,7+OxOVz|xs{f7u\ryKxO1Vz¯w¶\nt}£­\n\u001bu,uz|su{Oxs|\u001bu~;\nîVxO1u,&fuz|y¢\u001es&&xs{f7u\nuxsGm7s&Vu~1\u001e{ffz|{\u001bªuzsu{Psu71s£}fs\u001f~1\u001euRuV~&z{fxs|~¦@ÌP(z|Rxs|w^u uz|su{Oxs|\u001bu~xO&u~Vz{fz¯°(Rxs{Pz|y¢\t=z|{\u001b\b&fxOuxsV\nuxO&\u001bVu\"su71spt±Rxs{\u001cu.1Vxs{f~\nsVy¢u\u00061su71s\rv9\n~VyKxs|uR\rfz|y¢u{f~Vz{±Ius£Î\u001b£\u000eø}\t\u001bVxsz y¶Ï ÐTxs{­\nVuRu~\u0019ÑÒsÒ\u0001\n1G£­\n\tz|~&uff{f\txs{f7Du7w=z|~&&~\u001euRxsf~1u.&\u001bu\u0019Vxs{\u001båµ\n z|~'u~&~'&fxs{uv¦\u001cz|yK\t=z|{\u001b\u000e&fxOª\u001buR\u001eu{f\u001bu{fRzu~,u7w=z|~&\u001cuRÌ§\u001fuRu{K&\u001bu7yK\u001c{fu{&~h\ntT£­\n@f~\u000b¡@\tz|Rxs|s¦O§u7{suR&\u001fx\r\u0006u¯¡~1\u001eu71Vxs@su71sT\nfz|y¢u{\t~&z{\u0004äK1_x¢7uR\t~&1Vxs\u0013su71s\r\n\tz|y¢u{f~&z|{¨Ñö@£hz\u001b&u\u0005\u0004~&\u001bM§~&\u001bu.uzsu{Oxs|\u001bu~'xs{fD°\tG~1Ñ\u0001\nuzsu{Psu71sV~\r\n\ns\rxK~1u@\u001bu{f7u.\n\u0006u\u0010|s~1\u001eu71Vxs\tsu71sG~,7|u71u\n&yÙxO\u001e\u001bö.\u001b\u001bV~\u001f\n~1\u001euRuV\u0013£­\n\u001bu'~&\u001cuRuGDf~1uz|~}&fu'1Vxsz {fz|{\u001b\b~&uRs­\n¤¥\u0006¤­\n\n\nxO&\nuR\u000bxsÝ£ÑÒsÒsö\u0013§fz|Gz|~\u0013x~1\u001euxOåsuR\u0004z|{ffuR\u001cu{\t\u001bu{R¦MRuxs{\b~1\u001euRuG\bfxO&xO\txs~&us£­\n\u001bu\u001buRxµ\n&\u001bu\"uzsu{P\\xs \u001bu~z|~R|uxOVm~&uRu{Xz|y¢(@z {\u001b\b&fxO{f_&fuª°\tV~1ÑRä¢s'~1KxO&u\b~&z|{fz¯°(Rxs{PR£­\n\u001bu\r¬\"­\n§\tz|Vmz|~}f~&u_z|{_&\u001bu'~1\u001euRuGm7yKy\u0019f{fz|¡¢1\u000exOff&+w=z|y_xO1u&\u001bu\rØá¨1Vxs{f~\nsVy Rxs{\u001cu\"§Vz11u{µxs~r~h:t\u000b ÝãýuR&u¢&fu¢uuy¢u{P&~\"\nrxOVu¢&\u001bu¢7uR\t~11VxsT7@u7mRzu{P&~\"\nt xs{f z|~\"xew\u001ew\u0019u yKxO1Vz¯w¨\n7~&z|{fu\txs~&z|~\nf{f7&z|{f~\"zÝ£Îus£}7~&z|{fu\n\t{f7&z{f~GG£hz\u001b&u\u0001\n~&\u001bM§~&\u001bu\b°\tV~1Ñ\u0001\n7~&z|{\u001bu(xs~&z|~\nf{f7&z{\t~R£y¢(xOVz|{\u001bKhz\u001bVu\u0005\u0004K§z&´&fz|~°\tf&u§\u001fuX~&uRu´&fxO¢&\u001bu´uz|su{su71s1¡fuRVzsuÍ\txs~&z|~\nf{f7&z{\t~.\n&\u001bu´Ø\báâ1Vxs{f~\nsVy xOVu\u0006 7~Ú¡|zåsu\u0003\b\u001fz|{{fxO&\u001b&us¦P\txO&&z|Rf xOV\ns}&\u001bu'y¢s&uz|yK\u001csV&xs{h°\tG~1\nuR§\n\t{f7&z{f~h§z|&¢|xO&suR}uz|su{Oxs|\u001bu~R£­\n\tz|~\u00020 10 20100102104Eigenvalues\n0 20 40−0.500.51st Eigenvector\n0 20 40−0.500.52nd\n0 20 40−0.500.53rd\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\u000bz|\u001b&u\u0014\u0004\u001b©;ø}z|su{Oxs|\u001bu~\u001fxs{f\u000e°\tV~&Ñ\u0001\nuzsu{Psu71sV~\ns}&\u001bu\r7+OxOVz|xs{f7u\ryKxO1Vz¯w¢\nXu\u001cs\b~&\u001cu71Gxssu71sV~\ns'ö¢f\u001bV~\nRuxs{´~1\u001euRuGs\t~1uR&OxO&z{¨z|~'xs ~1m{\u001bs1uÉz|{ËýuRGyKxs{f~1å@\u0006ÏÙ\u0006xs|x+xO&ÜÑÒsÒsÓG£­\nPf~'§u.fx+su\u000efuy¢{f~11VxO1u&fxO,&\u001bu\b¬­\nz|~R~1u1_&\u001busf&z|y_xsY\u001bu7s&&u|xO&z|{\n\t{f7&z{\ns~1\u001euRuV\u0006sK~1\u001eu71Vx=£tvu_{\u001bM§Þu7w=xsyKz {\u001bu\u000e&\u001buKØáS1Vxs{f~\nsVy\nsy\u0019f~&z|\u0019~1\u001eu71Vx=£\u000e\u000bz|\u001b&u\u0002\n~&\u001b+§'~ª&\u001bu\u000euzsu{Oxs|\u001bu~xs{fD°\tV~&ªÑ\u0001\nuzsu{su71sG~'\n\ns,x_~1uPfu{f7u\nXu\u0013|sK~1\u001eu71VxsYsu71sV~\r7|u71u\n&y ÑRäsäxOff&+w=z|y_xO1uËö¨yKz|{@\u001b1uD×uxO&u~K~1{\u001b~´ÝãsÓsÒ±yKz|{@\u001b1u~\u0019\ny\u0019f~&z|m1s&xsõG£Sßxsz|{\u0013¦T§\u001fuµ~1uRuµ&\txO&\u001buDuzsu{Oxs|\u001bu~¢fuRxËVxO\tz|f|Éxs{\t9&\txO¢&\u001buD\txs~&z|~\nf{f7&z{f~\u0019xO&u\u0006 7~Ú¡|z|åsu\u0003\bTz|{9{\txO&\u001b&us¦}xOxsz|{\txO&&z|Rf xOVÛ&fuµ°\tV~1\nuR§£­\nP\t~¢§\u001fu¨7{fR|ffuµ&fxOK&\u001bu¨f~1uµ\n&\u001buX¬­\ns\u001bu7s&&u|xO&z {\u001by.f~Vz|\"sK~1\u001eu71VxKz ~xOff&sfGz|xO1us£-Bs%0 +\u001d&$%,+! 8s\u001c\u001f\u001e\t\u001eq$s,'( #$\u001eq\u001e\ts \u0016!p\u001e!\u001e\t#'(¡ p&Z¡¢`%,+(%-'q'( i'($%-¡+!- \u001f+\t+(%-\u001e\t\"\u0003 \u001fL&Z£$- \u001f\u001e\t£$%F$s,'( \u001fs\u001e-¤IBs%, #\u001e\ts%<s$,'( #$\u001e)\u001c¥+(%\u0017\u001e\t$ ¥!¦ \u001f+\u001d&Z%,+ §£$,+(%\u0016\u001c\u001e\t£$¨+(%\u0016©s%-$, ª\u001c+(#$«G%-'\u0016¤Bs%\u00148e$s,'( \u001fs\u001e\u001c¥+(%\u0017 +\u001d&$%,+(%-&\u0012Y '($%-¡+8- \u001f+\t+(%,\u001e\t\"\u0003 #L&ZsN%-\u001f%-Y¬#\u001c£$%!s\u001d\u0012&$ %,\u001eQs \u001f'8$%--%-\u001e\t\u001e(\u001c¥+( ª#¬U%%>­s\u001c,'(¡ \u0005'(s%)\u001e(\u001c«G% \u001f+\u001d&Z%,+\u0016¤G®\\¯\"s\u001c+\t'(-s£\u001c¥+i$ \u001f'(%'(s\u001c'0'($%)°¥+\u001d&\u0005\u001c\u001fL&\r±'(¯%-\u001f%-Y¬U%-,'( +(\u001e0 \u001f5'(s%\u000b\u001e\t\"\u0003%-%-\u001dª&$\u001c'\u001d\u001c\u0014\u001c\u001f\"$\"\u0003%\u0016\u001c+F'( \u0017\u0003%² +(%-¬U%,+(\u001e\t%\u0016&`³`- #«G\"s\u001c+(%\u0016&N'( \u0014'(s%\u000b- #\u001e\t$%)L\u001c\u001e\t\u001e´s$,'( #s\u001e-¤Iµ§,'(L\u001c£¡ ¶'($%·%-\u001f%-Y¬#\u001c£$%-\u001eB +B'(s%,\u001e\t%·'¸m )%-\u001f%-Y¬U%-,'( +(\u001eq\u001c+(%Q\u001c«G #\u001e*'B'($%·\u001e(\u001c«G%FYs«i\u0003%,+B\u001c\u001eq\u001e\ts \u0016!< #\u0014'(s%%,£\u001f%-Y¬#\u001cs%Q\"s '\u0016¤Ö0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−101\n0 20 40−1010 20 40−101hz\u001bVu\u0001\n©}hzV~1Ñ\u0001\n7~Vz|{\u001bu\"\txs~&z ~\n\t{f7&z{f~¹é!Ã\rÀ\u000eÆ8\u0013VÅ\u000eê\u001eÇ&Ã\rÀ\u000eê\u0011,À\u000eÄ í.Å\u000eó'óðTê\tÁ\u001eÇVÃ'À\u000eê ëÃÂÞè¢Å.ÁYÅ\u000eÂYðoº Ã'Âq»¤¥{.&fz|~\u0010\txO\u001euR¦M§u\u001f\txsu~1\u001bP\u00041\r\tfz| ª{\b&\u001bu~&fR7u~&~hz|{&\u001bu~&\u001cuRuG\bVu7s{fz&z{.7yKy.\t{fz¡\"@z|{su~&&zxO&z|{\u001b\u001bM§ xOf\t z|RxO\tuz;z|~h1\bf~&u&fu,\u001byKz|{fxs{P\nuxO&f&u~\ns}y¢=\u001bu|z|{\u001bª~1\u001euRuVK1y¢@fuy.f~Vz|O£Ttvu°(V~1,fz|~VRf~&~1u&\u001bu\t&@7u~V~\nhsVy_z|{\u001b¢\u0006\u000b,\nuxO&\u001b&u~\ns\r~1\u001euRuVY¦\tfu~&7Vz\tz {\u001b.&fu&uxs~1{f~\nsxs{f xs~&~&\ty¢f&z{f~¢y_xs\u001buXxOKuxsGÍ~11uR\u0010£ tvuX&fu{ z|{Psu~1&zxO1u ¡§Û\n&\u001buXy¢sVu7{1&MsuRV~&z|xs\u000b~11uR\t~z|{D&\u001bu\b7{P1u7w@'\ny\u0019f~&z \"y¢@fu|z|{\u001b\u001b£hzV~1R¦,§uÉu7w=xsy_z|{\u001buÍ&\u001buvf~1u¨\n&\u001bu¨\u001cuRG7uRf&fxsªXu\n&u@\u001bu{f7 ~&Rxsuvz|{Í&\u001buÉ7{1u7w=µ\n~1\u001euRuV(²\\y\u0019f~&z|'fz|~&7Gz|yKz|{fxO&z|{Y£\u0013tvu\n\t{fK&fxO&\u001bu\rXu\u001e~&Rxsu'§xs~xOuxs~1{\u001bs\u001f\txOVy\nf\ns\u001f&fz ~f&s\tuy´¦Pxs&\u001bf\n\u001b&&\u001buR\u001fu7w@\u001euRVz y¢u{&xO&z|{_z|~\u001f{\u001buRu\u001bum1.suRVz\n¢&fxO&fz|~\u001fz|~T&fu'sf&z yKxs\t~&Rxs|usyK@\u001bu z|{\u001b¢y.\t~&z|ª~1\u001eu71VxKz|{D&fu\"su{\u001buRVxs\u0010Rxs~&us£Ê@u7{fY¦Y§uKz|{Psu~1&zxO1uÉ&\u001bu¢xs~&~Vfy¢f&z{\u0006&fxO\"&\u001bu¢|xs~&ª~&1uRv\n}sVyKz|{\u001b7uR\t~11Vxs\nuxO&\u001b&u~&u~&f&~z {fu7s&&u|xO1uµsu71sG~R£\u001f×_u7w\u001bxsyKz|{\tz|{\u001b\u000e&\u001buª\txs~&z|~su71sG~,\n&fu\"&\u001buRs&uR&z|Rxs||_sf&z|y_xs1Vxs{f~\nsVyÞ1\bfu7s&&u|xO1u,su71sG~R¦P§u\u001buyK{f~11VxO1u¢&\txOT&fz|~;xs~&~Vfy¢f&z{¢z ~TxOff&s\tVz|xO1u\u001es&s~&\u001cuRuGXxs{fXy.f~Vz|ª~1\u001eu71Vx=£f\u001b&\u001bVu\u0019§s&å¨~&\u001bf|\n@Rf~ª{Éxµy¢sVu\u0019&fs&\u001b¨u7w=xsy_z|{fxO&z{É\n&\u001bu¢~1\u001eu71Vxs\u000b\txOVxsy¢uR1uRV~1\u000e\t~1u\r~VfVDxs~&\u001buª~&xsy¢\t|z {\u001bVxO1uª\n&fu\r~&z|{fxsÝ¦&fu\n&u@\u001bu{f7K~&Rxs z|{\u001bµXu\u001css&\u001buR&§z ~1uMTxs{\t&\u001bu.{@fy.\u001cuR\n(z|{f~1f~1u§fu{´~&y¢@s&fz|{\u001b\u001b£ß\r|~1K§s&&Pµ\nz {su~1&z|xO&z{\u0006z ~&\u001bu\b§'z|{f\u001bM§z|{\u001b~&zàRu\bxs{f\nVxsy¢u\bVxO1us£Ó0 10 20100102104Eigenvalues\n0 20 40−0.500.51st Eigenvector\n0 20 40−0.500.52nd\n0 20 40−0.500.53rd\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\n0 20 40−0.500.5\u000bz|\u001b&u\u0002\n©\u000bø\u001fzsu{Oxs|\u001bu~hxs{f.°\tG~1Ñ\u0001\nuz|su{su71sV~\ns;&\u001bu7+OxOVz|xs{\t7u,yKxO1Vz¯w\nsT\u0006u@|sª~&\u001cu71Gxssu71sV~\nsxO\u001e\u001bG\u0004XÑ+²Oã_\u001b\u001bV~\n×uxO&u~'~1{\u001b~Ò¼ \u000fûÆ§»\bÀ\u000eÃ\u000b½¾\u0013&ðTÄ¢óp¿ ð}ÀªÁ\u001cê­\n\u001bu\"xs\u001b&\u001bs§f|D|zåsu\r1¢&fxs{\u001båm&\u001bu\"~1\u001euRuVs&\u001b´xOù'á\ns\u001bu|\n\t\tfz|~VRf~&~&z{\t~\u001fxs{fDy¢sGxs~&\u001bf\u001es&R£òÞðTëðTÂYð}À\u000eÆ;ð}ê×xs\tyµ¦Pá}£ø,£¦Pÿ\u000buR1Vz|us¦­\n£¦@Ê=fu~R¦\n\n£@Ï\u0016tvuz|~&~R¦K|\b£\u001cÚÑÒÖ\\äG¦I\u0006 ß!yKx\\w\u001bz|yKzàxO&z|{\u000e1uVf{fz P\u001bu=RR\u001bVz {\u001bz|{!&\u001buv~1&xO&z|~&&z|Rxsªxs{fxs=~&z|~\nfVs\txO\tz||z ~1&z|\nf{f7&z{f~\n\u0006xO&åsM!Gfxsz|{f~Y\b¦\rÀpÁ5ÁIÂ\u0017ÃÅÄ\u0003Æ¸ÇKÂÈÆ(Ä\u0003Æ*É\\Ê#Æ(ÂBË·Ìf¦\u0010Ñ\u0002\n\u0004sÍ(Ñ+Ö@ÑO£× fyµ¦­\n£\u001eáT£¦\u0013Ø\"uz|~V|xO¦\u001e¬.£\u0013£¦Yt\u0016\u001buxO1{Y¦\u0004úf£\u001cß.£\u0013Ïîtv|Y¦Yø,£Yý.£TÚÑÒsÒsÒG¦0ÃÅÎYÆ¸ÇKÏYÐÄLÁIÐÄLÑUÆ*É*ÒYÓÔÎÕÏ-Ö×\nÄLÁ@ØUÖÄ`ÒYÆ*Ø\u0019Ñ¥Î\u0017ÖUÏLÑÒ\u001fÏLÁ5Æ\u001dÎÁ5Æ*Ù\u0016ÚÄsÊÎÐSÄLÁIÄLÓ´ÛsÊ\u001fÉ\\ÊÜ<ÊUÆ(ÏLÑ\u0016ÄYÝVÎUÜ<Ñ\u0016ÎYÆ*Ñ\u001fÉ(ÎUÞLÄLÓ¨ÜpÄLÁIÐßÊÎ\u001dÝ×\nÎUÁIÆ(Ä\u0003Æ*É*Ï\u0003ÁàÏ\u0016Ö¦Ä\u0003ØÐ\u0003É*ÏÉ¸ÁYÖUÏ\u0003Ñ×\nÄ\u0003Æ*É\tÏLÁf¦B\f\b£iÊ\u001e£fÿ\u000bxO1u{P\u0001\n¦(Ò=ÑÓ@¦\u001cãsãsö@£øT\t\u001bGxsz|yµ¦Yô\u000e£\u0004Ï ÐTxs{­\n&uRu~¦hý.£\u0010á}£\u001fÚÑÒsÒ\u0001\nG¦G\u0006 ß ~&z{fxs}~&\u001b\t~1(xs7uKxOff&xsG\ns.~1\u001euRuG±u{ffxs{\t7u7y¢u{P\b¦Bá\u0016âiâ\u000bâäãÑ\u0016ÄLÁÊÄ`ÒYÆ*É\tÏLÁÊ¦ÏLÁ\nÈ\u0003åÎÎÒUÇÄLÁmÐÀpØ@ÐLÉ*Ï\u0012æÑ\u0016ÏYÒ#Î#ÊÊ#É\\ÁKÝªç\u0004¸\u0004PG¦\u001cã\u0001\nÑ\u001fÍ@ã\u0002\u0003\u0001\n£\u001bPs1us¦Yú\u001b£­\n£\u0010ÚÑÒsÒÖsG¦Y{P1u{PÚÔ\txs~1u\u0006&uR1VzuROxs\u001e\ny.f~Vz|\"xs{fµxsffz|\u001b¦qÉ\\Á\u001a\u0006 Ê\u001bÿT¤Úø\u0014\b¦\u001bf\u0013£\u0013ÑösÓZÍ(ÑU\u0004PÖP£\nxO&\n\u001b¦=úf£PÊ\u001c£suR}xsÝ£\tÚÑÒsÒsöG¦\u001b¬ªxO&\tx&z|yKzR£xs7f~&&z|NÔ\t\u001b{\u001buR&z 7{&z|{@\u001bf~}~1\u001euRuV_7sV\tf~R£s{fz|~&&z\u0004ÒsöOä=¦­\nuGf{fz|RxsYVuR\u001csVR¦f¬,ß'ù'ÿ\u001eß\b£\nfxs{\u001buyµ¦(ù£\n\n£\u0004ÚÑÒsÒ=Ñ+G¦\nÈÆ(ÏYÒÇKÄsÊUÆ*É*Ò\u0014è8Á@É*Æ\u001dÎªÎÓÔÎ×\nÎUÁ5Æ¸Êé<À\nÈ\u0003åÎÒYÆ*Ñ\u0016ÄLÓqÀ\nå`åÑ\u0016ÏYÄ`ÒÇ¦\u0013Ê@fVz|{fsuR1ÔÐ;uRV|xO\u001b£ýuRVy_xs{f~1å@s¦ý\b£sÏ!\u0006xs|x+xO&Y¦M|.£=ÚÑÒsÒsÓG¦5\u0006 Ê@\u001eu71VxsP\txs~&z|~\n\t{f7&z{f~\n&y-fz|~V7Vz|yKz|{\txs{\u0013xs{fxs|@~&z ~\b¦á\u001fÁ5Æ\u001dÎÑ\u001fÁIÄ`Æ*É*ÏLÁmÄLÓêQÏ\u0003ÁYÖÎÑ\u0016ÎÁIÒ\u001fÎÏLÁ\nÈ\u0003åÏsë`ÎUÁßì!ÄLÁKÝ\u0003Ø@ÄYÝVÎæÑ\u0016ÏYÒ\u001fÎUÊÊ\u001fÉ¸ÁVÝ£á\u0013sxs{Y¦;×\r£­\n£\u000bÏ¶@Y¦\u000bÊ\u001c£ÝãOäsäsäG¦}\u0006\t~&z|¢~&fy_yKxOVzàxO&z{Ûf~&z|{\u001bµåsuRv(\u001bVxs~1u~R¦iÉ\\ÁÅ\u0006 ÿT&=7uRufz|{f~¤Úø}ø}ø ¤¥{P1uRV{fxO&z|{fxs\u0004{\nuR&u{f7u\"{Xß'7f~&&z|R~R¦\u001cÊ@\u001euRuGY¦fxs{f\u0006Ê=z|{fxsYÿT&=7u~&~&z {\u001b\b£\u0006xOV\tx\u001c¦B|.£\u001cÏ á\u0013uRus¦\u0013\r£ ¡ý.£hÚÑÒsÒsöG¦0\u0006 ³ª{X&\u001bu.xs~&@y¢\t1s&z|\b~1&xO&z ~1&z|Rxs\u0004\u001cu\tx=zs\nuy¢(zVz|Rxs\u00137uRf~11Vxs\u00107Pu7mRz|u{&~\b¦qá\u0016â\u000bâiâzã@Ñ-Ä\u0003ÁÊUÄ`ÒZÆ*É*Ï\u0003ÁÊ\u0012ÏLÁ\nÈÉÔÝ\u0003ÁmÄLÓBæGÑ-ÏYÒ#Î#Ê\u001fÊ\u001fÉ¸ÁVÝË·Ìf¦\u0010ÑÒsÒOä$Í(ÑÒsÒsö@£\u0006xO&&z {Y¦\rØK£¬\u0019£\bÚÑÒsÒsÓG¦­\n+§xOVâxs\u001b1y_xO&z|v~1f{f\u0016~1fV7uÉ&u7s{fz&z{\u0013©Ëz|fu{&z\n=z|{\u001b9y\u0019f~&z Rxsz|{f~&1Vfy¢u{P&~R¦BÉ\\Áß\u0006 ÿT&=O£@|\rß­\n³ yK\t\u001b&xO&z{fxsYý'uxOVz|{fKß'\u001bOxs{f7u\u0006Ê=&f\u001b_¤Ú{f~1&z|&\u001b1u\u0003\b£\u0006xO&&z {Y¦Ø¢£\u001f¬\u0019£¦'Ê=G\u001buz&uR¦ø£\u001f¬\u0019£Ï Ð;uRV7Pus¦'×'£áT£\rÚÑÒsÒsÓG¦'¨f~&z|´7{P1u{PDxs{\txs@~Vz|~¢&\u001b&fy¢=\u001bu|~.\nxsffz&z|{Y¦iÉ¸Ác\u0006 ÿT&=O£hß ¨f&z|y¢u\tz|xDtvs&å=~&\u001bsS{ {1u{P\u000eÿT&=7u~&~&z|{fX\n¨f~&z|\ns¨f&z|y¢u\tz|x.ß'f\t z|RxO&z{f~\b£ùxO\tz|{fuR¦\u001báT£\tù\b£\tÏûúfxs{f\u001b¦f×'£(ý\b£\u0004ÚÑÒsÒsöG¦!íBØ\u0019ÁIÐMÄ×\nÎUÁ5Æ(ÄLÓ¨Ê\u0012Ï\u0016Ö\nÈ\u0003åÎÎÒUÇeî\u0014ÎÒÏ¥Ý`Á@É*Æ*É*ÏLÁ\t¦\u001cÿ}&u{&z 7u#ÍPý'xs|Ô£ÑRäÊ=V\u001buz|&uR¦ø,£Ï Ê= xs{\u001buRs¦Û£\"ÚÑÒsÒÖsG¦ª{\t~11Vf7&z{Íxs{f uROxs|fxO&z|{ \nxÛ&s\tf~1my.f|&z\nuxO&f&u~1\u001euRuV\u001c²\\y.f~Vz|,fz|~V7Vz|yKz|{\txO1s¦É\\Á\u0006 ÿT&=7uRufz|{f~T¤Úø}ø}ø9¤Ú{1uRV{\txO&z{fxs\u001c{\nuRVu{f7u,{ß'7f~1&z|R~R¦\u001eÊ@\u001euRuVY¦fxs{f¨Ê=z{fxs\u001eÿTV@7u~&~Vz|{\u001b\b£ôTf{\u001b\u001b¦(Ê\u001c£\u001bú\u001b£¦@tvP=f|xs{\tY¦@ÿ\u000b£\t\r£\u001bÏ-×\u001f@V{\u001bus¦@t-£\u001búf£\u001eÚÑÒsÒsöG¦qïÕãqð\u0012éBï\u0005É*Ð`ÐVÎUÁxÃÅÄLÑ\u001fëLÏLÞ\rÃ\u0007ÏYÐMÎÓiã5ÏZÏLÓ£ësÉ¸ÆñFò`Â\\ós¦(xsy\b\tVz|\u001bsup\f'{fz|suRV~&zÌ_ø}{\u001bz {\u001buRuRVz|{\u001b.¬'uR(xO&&y¢u{P'Ê@\u001euRuV\n\n&\u001bDxs{fDø}{P1&s\tz 'ùu7~1uxOVGXá\u0004xO\u001csGxO1sVzu~¤Ú{fO£ÑsÑ"
    },
    {
        "title": "MuTaTeD&apos;ll: A System for Music Information Retrieval of Encoded Music.",
        "author": [
            "Donald MacLellan",
            "Carola Boehm"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1417755",
        "url": "https://doi.org/10.5281/zenodo.1417755",
        "ee": "https://zenodo.org/records/1417755/files/MacLellanB00.pdf",
        "abstract": "MuTaTeD'II started in November 1999, building on the results of the MuTaTeD project. Its aim is to design and implement a music information retrieval system with delivery/access services for encoded music. The prototype service will provide a user friendly, web-based search/browse/query interface to access musical content.",
        "zenodo_id": 1417755,
        "dblp_key": "conf/ismir/MacLellanB00",
        "keywords": [
            "MuTaTeDII",
            "November 1999",
            "MuTaTeD project",
            "Music information retrieval system",
            "Encoded music",
            "Prototype service",
            "User friendly",
            "Web-based",
            "Search/browse/query interface",
            "Access musical content"
        ],
        "content": "MuTaTeD'II: A SYSTEM FOR MUSIC INFORMATION RETRIEVAL\nOF ENCODED MUSIC\nAbstract\nMuTaTeD'II started in November 1999, building on the results of the MuTaTeD project.\nIts aim is to design and implement a music information retrieval system with delivery/accessservices for encoded music. The prototype service will provide a user friendly, web-based\nsearch/browse/query interface to access musical content.\nIntroduction\nThe Objectives of the project MuTaTeD were: to integrate SMDL - ISO/IEC DIS 10743\n(Newcomb, 1991) as the Model with NIFF as one possible View, and establish a standard Meta-DTD for music tagging languages, which could be used by the wide user community.\nAdditionally, it was to research into the development and integration of a SMDL DTD for the\nwider music user community. The System in MuTaTeD was implemented using LEX/YACC.The work also heavily influenced the \"Structured Music MPEG7 proposals\" (Boehm/Hall, 1999)\nwhich were proposed in order to ensure an SMDL-compliant standard.\nBesides proving the concept of integrating these two standards, the use of the\nLEX/YACC technology in MuTaTeD enabled us to implement the converters, but to certain\nextent also restricted our flexibility. This restriction was the major reason for implementing the\nfollow-up system in the project MuTaTeD'II in a different way. The MuTaTeD'II team decided touse the Groveminder Technology, which, besides other advantages, caters for the reading-in ofdifferent DTDs.\nNIFF, being binary, also posed some problems of accessing and searching the content.\nThe fact that any compilation/search procedure has to always be preceded by a reading in or outof the whole binary data in the case of binary formats, has consequences in the design of intuitive\ncontent music retrieval, specifically for displaying dynamically parts of music. So although NIFF\nhas proved to be a powerful format, we are contemplating using other text based representationlanguages for future implementations. Additional information on the two projects can be found attheir relevant websites. (MuTaTeD1 WWW 1999, MuTaTeD2 WWW 2000)\nMuTaTeDÕII\nIn MuTaTeDÕII, we will use Groveminder's support for tagged based languages,\nspecifically SGML based languages. The prototype service will provide a user friendly,expandable web-based search/browse/query interface to access musical content.\nBoth high level searches and more complex low-level searches will be catered for. High\nlevel searches will include searches for specific metadata regarding composer or date ofcomposition. As SMDL supports the storage of a certain set of bibliographic metadata, this will\nbe utilised within this system. Low level searches will concentrate on musical aspects within each\nscore or across a pre-selected collection of scores. This low level searching will enable the user tosearch for a certain pitch, note or articulation patterns. In the next version of the system, it isplanned to provide any combination of pitch, note and articulation searches, and other SMDL\nelements, such as lyrics will be supported. This will seamlessly transform a search activity into a\ntool for music analysis.\nTwo main API's are used, both written in C++. The first is the Groveminder system itself\nwhich is used to validate, parse and search SMDL files. The second is GNU Cgicc. This is a gnu\nopen source API which provides functions for talking to a HTTP server and also creating HTMLon the fly. The system internal steps involved in the successful indexing for the searching process\ninclude:\n1. Opening and parsing a SMDL document for validation, thus building the grove.\n2. Parsing the resulting tree/grove to locate the information searched for.\n3. Create containers and store search specific data in the containers.\n4. Apply the search functions to the containers and return the results.\n5. Store results and return to client via web page.\nSummary\nIn the past the availability of music information on the net has been hampered for a\nvariety of reasons, one of the main ones being the propriety nature of the file formats used in\nmost score notation packages. Throughout the Mutated projects, we have tried to overcome someof these restrictions. In MuTaTeD! the task to make SMDL files viewable was undertaken by\ncreating SMDL to NIFF converters. This opened the door for the next obvious step which was\ncarried out in MuTaTeD'II. SMDL being a tag-based language was one of the obvious choices fortransporting music information over the internet.\nWhile the process of creating a fully interactive online music information search and\nanalysis tool is still in its embryonic stage it is immediately obvious that a solution of this nature\ncould have a variety of benefits to the music community.\n1. high-level to low-level music search facilities\n2. adaptable, expandable analysis tools\n3. use of an underlying powerful, standardized description language, which does not\nnecessarily restrict the handling of purely encoded music\n4. the expansion possibilities into areas of  audio/video tagging\nFinally, the expansion of the web and the general increase in tag-based languages like\nXML and SGML mean that, in future, there will be a greater provision for developingapplications that deal in these languages. The next generation of browsers (i.e. Netscape 6) will\nbe fully XML compliant and, given time, SGML (or something of a similar level of complexity)\nwill be more commonly used. This drive towards easier interchange of information is inevitableand provides an opportunity to put data under the control of users rather than leaving them prone\nto the fickle trends of commercial application developers who more often than not have been\nresponsible for the bottleneck in information interchange.\nAuthor Information\nDonald MacLellan\nDepartment of Music\nUniversity of Glasgow\ndon@music.gla.ac.uk\ntel: +44 (0) 141 330 6065Carola Boehm\nDepartment of Music\nUniversity of Glasgow\ncarola@music.gla.ac.uk\ntel: +44 (0) 141 330 4903\nSuggested Readings\nCover, Robin. The XML Cover Pages Website.  Http://www.oasis-open.org/cover  Especially sections on\nSMDL, groves and property sets.\nBoehm, Carola and MacLellan, Donald. MuTaTeDÕII website. http://www.pads.ahds.ac.uk/mutated2.html\nHall, Cordy and Boehm, Carola. MuTaTeD website http://www.pads.ahds.ac.uk/mutated\nNewcomb, Steven. 1991 Standards: Standard Music Description Language Complies with Hypermedia\nStandard.  IEEE Computer 24/7/91: p76-79. ISSN:0018-9162"
    },
    {
        "title": "A Comparison of Language Modeling and Probabilistic Text Information Retrieval Approaches to Monophonic Music Retrieval.",
        "author": [
            "Jeremy Pickens"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415100",
        "url": "https://doi.org/10.5281/zenodo.1415100",
        "ee": "https://zenodo.org/records/1415100/files/Pickens00.pdf",
        "abstract": "With interest in music information retrieval increasing the need for retrieval systems unique to music is also growing. Despite its unique properties music shares many similarities with text. The goal of this paper is to explore some of the capabilities and limitations of current text information retrieval systems as applied to the task of music retrieval. Monophonic music is converted into text and retrieval experiments are run using two different text information retrieval systems in various configurations. Finally, we will discuss whether the techniques applied here are generalizable to the larger problem of polyphonic music retrieval. 1",
        "zenodo_id": 1415100,
        "dblp_key": "conf/ismir/Pickens00",
        "keywords": [
            "music information retrieval",
            "text information retrieval",
            "monophonic music",
            "polyphonic music",
            "text similarity",
            "text retrieval experiments",
            "text information retrieval systems",
            "limitations",
            "generalization",
            "larger problem"
        ],
        "content": "AComparison ofLanguage Modeling andProbabilistic TextInformation\nRetrievalApproachestoMonophonic MusicRetrieval\nJeremyPickens\nDepartment ofComputer Science\nUniversityofMassachusetts\nAmherst, MA01002USA\njeremy@cs.umass.edu\nAbstract\nWithinterest inmusic information retrie valincreasing theneed forretrie valsystems unique to\nmusic isalso growing. Despite itsunique properties music shares manysimilarities with text.The\ngoal ofthispaper istoexplore some ofthecapabilities andlimitations ofcurrent textinformation\nretrie valsystems asapplied tothetask ofmusic retrie val.Monophonic music isconverted intotext\nandretrie valexperiments arerunusing twodifferent textinformation retrie valsystems invarious\nconﬁgurations. Finally ,wewilldiscuss whether thetechniques applied here aregeneralizable tothe\nlargerproblem ofpolyphonic music retrie val.\n1Introduction\nThemodern ﬁeld ofinformation retrie val(IR) beganinthe1950s with theaimofusing computers to\nautomatically search collections ofunstructured online text.Since thattime, andparticularly inthelast\ndecade with thepopular arrivaloftheInternet, interest invaried, “multimedia” information retrie val\napplications hasexploded. Notonly hastextretrie valbranched outbyincorporating audio speech recog-\nnition techniques, butimage retrie valandvideo retrie valhaverecei vedconsiderable attention. Music\ninformation retrie valisathird.\nTextIRhasalargehead start onthese other retrie valsystems. Techniques havebeen intensi velystudied\nandreﬁned forabout four decades. Methods andmodels formusic IR,ontheother hand, arestillin\ntheir infancy(Byrd &Crawford, 2000). Music retrie valwillcontinue tomature. However,rather than let\nitmature inisolation, theattitude takeninthispaper isthatweshould leverage traditional information\nretrie valworkandexamine theextent towhich techniques developed fortextcanbeapplied tomusic.\nTwoprimary reasons wecantakethisapproach havetodowith theassumptions wemakeabout our\nmusic representation format. First, instead ofdigitized audio, weassume explicit knowledge ofthe\nmusical structure ofapiece ofmusic. Inother words, thevarious notes andtheir pitches, start times, and\ndurations areknown.(General audio music recognition, some what akin tospeech recognition fortext,is\nanunsolv edproblem andwillnotbeaddressed here.) This structure, or“music text’,could beanything\nfrom time-stamped MIDI tofullmusic notation, such astheNightingale music notation format (AMNS,\n2000). Second, weassume thatthemusic wearesearching ismonophonic. Only onepitch is“active”at\nanygiventime slice. Withthese twoassumptions, itwillbepossible toapply textretrie valtechniques to\nmusic.2BackgroundandRelatedWork\n2.1InferenceNetworks(Inquery)\nTheﬁrsttextretrie valsystem under consideration isInquery (Callan, Croft, &Harding, 1992; Turtle &\nCroft, 1991). This isaprobabilistic retrie valsystem, based onthegeneralized frame workofaBayesian\ninference netw ork. ABayesian netisadirected acyclic graph. Thenodes inthegraph represent propo-\nsitional variables andthearcsrepresent dependencies between those variables.\n            \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\nFigure 1:Anexample inference netw orkfordocument retrie val\nAsCallanetal.,1992 explain, theInquery Bayesian netisdivided intotwoprincipal components: the\ndocument netw orkandthequery netw ork. The document netw orkhastwolayers. The ﬁrst layer is\ncomposed ofdocument nodes (\n\u0000\u0002\u0001).Thedocument nodes aretheleaves,andaretheonly observ ables in\ntheentire system. There isonenode foreach document inacollection andthevalue ofthenode isthe\nproposition thatthisdocument satisﬁes auser’ sinformation need. This value isalwayssettotrue for\neverydocument. Theinternal nodes inthegraph, ontheother hand, arenotdirectly observ able andmust\nbeinferred from thedocuments.\nThesecond layer inthedocument netw orkiscomposed ofdocument content representation nodes ( \u0003\u0002\u0004),\nwhich represent theproposition that agivenconcept hasbeen observ ed. Asasimple example, one\nrepresentation ofonepartofadocument might beaword, aspace- orpunctuation-delimited sequence\nofalphanumerics. Other possible representations include integervalues, dates, named entities andso\non.One document canhavemultiple representations, andonerepresentation canbeshared bymultiple\ndocuments. Forexample, astring ofnumbers (such as“1225”) could betreated either asatextstring or\nasanintegervalue orboth, inwhich case tworepresentation nodes would beused. Keepinmind thatthe\nconcept inarepresentation node isnotdirectly observ able; itmust beinferred from thedocument. The\narcbetween adocument node (\n\u0000\u0002\u0001)andarepresentation node ( \u0003\u0005\u0004)istheconditional probability \u0006\b\u0007\t\u0003\n\u0004\f\u000b\n\u0000\r\u0001\u000f\u000e.\nTheprior probability ofanydocument\u0006\b\u0007\n\u0000\f\u0001\u0010\u000eis\u0011\n\u0012,where\u0013isthenumber ofdocuments inthecollection.\nThedocument content representation nodes connect with theﬁrstlayer ofthequery netw orkatthequery\nconcept nodes. Thequery netw orkisarepresentation ofauser information need. Arepresentation node\u0003\u0014\u0004cantakeontwovalues, trueorfalse. Thearcbetween the\u0003\u0015\u0004andaquery concept node\u0016\u0014\u0017isthebelief\nintheproposition \u0003\u0005\u0004,theproposition thattheconcept \u0003\u0018\u0004hasbeen observ edinthedocument. Bayes’\nruleisused toinfer beliefs about document representation nodes\u0003from documents, then beliefs about\nquery concept nodes \u0016from document represenation nodes. Justasmultiple arcsareallowed from docu-\nment nodes todocument content representation nodes, multiple arcsarealsoallowed between document\ncontent representation nodes andquery concept nodes. This willbeuseful later intheconstruction ofphrases, inwhich multiple words arecombined insome desired manner tocreate asinglephrasequery\nconcept node.\nTherootofthetree, thenode towhich allarcseventually lead, isthequery node itself, \u0019.This represents\ntheproposition thattheuser’ sinformation need ismet, andisalwayssettotrue. Thevarious concept\nnodes \u0016which comprise aquery arecombined using aweighted summation operator ,summing thebeliefs\nateach ofthequery concept nodes. Documents arerankedbythissum. Thehigher thebelief thatagiven\ndocument meets theinformation need, thehigher itisranked.\n2.2Language Modeling\nAsecond, lesswell-kno wnprobabilistic approach totextinformation retrie valislanguage modeling. The\nprimary difference inthisapproach isthat, unlik emost other models inwhich thedocument indexing and\ndocument retrie valtasks aredissimilar ,language modeling combines indexing andretrie valintoasingle\nmodel. Thelanguage model isused notonly asanindex,butasamethod ofestimating theprobability\nofgenerating aquery .Unlik etheInquery inference netw orkapproach, concepts arenotinferred from\ndocuments asanintermediate step. The language modeling approach deals with theprobabilities of\ngenerating thequery words themselv es,directly from thedocuments.\nThe language model wewill useformusic retrie valisthesystem developed byPonte &Croft, 1998.\nForagivenuser query ,documents inthecollection arerankedbytheir probability ofgenerating that\nquery .This ranking formula, deﬁned foreach document model \u001a\u001c\u001binthecollection, isaproduct ofthe\nprobabilities ofgenerating each term inthequery ,times theproduct oftheprobabilities ofnotgenerating\nalltheterms thatarenotinthequery:\u001d\u001e\u0007 \u001f!\u000b \u001a\"\u001b\n\u000e$#&%')(\n*\n\u001d\u001e\u0007)+,\u000b \u001a-\u001b\n\u000e/.0%'\u000f1 (2*\b3\n46587\n\u001d\u001e\u0007)+,\u000b \u001a-\u001b\n\u000e(1)\nTheprobability ofgenerating aterm, giventhemodel ofthedocument, isgivenby:\u001d\u001e\u0007)+,\u000b \u001a-\u001b\n\u000e9#(2)\u001d\u001e;:\u0017<\u0007)+\u0014\u000b \u001a\"\u001b\n\u000e\u0011>= ?A@CB\nDFEHG I.\u001d\u001e;JLKNM\u0007)+\n\u000eB\nDOEHG IQPSR+UTWV\n')X\u001bLY[Z5\\\u000f]\nE\\\u000f^ _2`badc,ebf\nPHgc\nWhen theterm isfound within thedocument (+UTZ5),wecanusetheﬁrstprobability estimate. There\naretwopart tothisestimate:\n\u001d\u001eh:\u0017<\u0007)+,\u000b \u001a-\u001b\n\u000eand\n\u001d\u001e;JLKNM\u0007)+\n\u000e.The ﬁrst part,\n\u001d\u001eF:\u0017,isthemaximum likelihood\nestimate oftheterm giventhedistrib ution ofthatterm inthecurrent document, \u001ai\u001b.This issimply\nthefrequenc yofoccurrence oftheterm inthatdocument normalized bythelength ofthedocument.\nThesecond part,\n\u001d\u001eFJLKNM,istheaverage overalldocuments inwhich +occursofthemaximum likelihood\nestimate ofthatterm ineach document. Finally ,\n\u001djisariskfunction. Details arefound inthePonte &\nCroft, 1998 paper ,butitisenough tosaythat\n\u001djestimates towhat degree thefrequenc yofthecurrent\nterm +inadocument varies from thenormalized mean across alldocuments. Themore thisfrequenc y\ndiffersfrom themean, theriskier itistousethemaximum likelihood estimate, andthesafer itistouse\ntheaverage estimate. Hence,\n\u001djisamixing parameter between\n\u001d\u001eh:\u0017 \u0007)+,\u000b \u001a-\u001b\n\u000eand\n\u001d\u001eOJ>KNM\u0007)+\n\u000e.\nWhen theterm isnotfound within thedocument, thebestwecandoistouseanestimate based upon alldocuments inthecollection. Thus\n\\\u000f]\nE\\\u000f^issimply thenumber oftimes thatterm appears inthecollection\nasawhole, overthesizeof(number ofdocuments in)thecollection.\nOurrankingformula Although theranking formula used byPonte &Croft, 1998 includes anestimate\nforallterms which donotappear inthequery ,ourexperiment does notconsider these terms important.\nWearelooking forspeciﬁc melodies inoursearches, andthenotes thatwearenotlooking forhave\nlittle bearing onwhether ornotamelody ispresent inadocument. Preliminary results (notshownhere)\nindicate much better performance when thenon-query terms areignored, butfurther analyses need tobe\ndone before weareconﬁdent ofthis. Nevertheless, thisistheintuition wehavedeveloped andsoweuse\nthefollowing simpliﬁed language model ranking formula:\u001d\u001e\u0007 \u001f!\u000b \u001a\"\u001b\n\u000ek#%')(\n*\n\u001d\u001e\u0007)+,\u000b \u001a-\u001b\n\u000e(3)\n3Experimental Design\n3.1MeldexCollection\nThemusic document collection weused consists ofalmost 9,400 North American, British, Irish andGer-\nman folksongs, German ballads, andChinese ethnic andprovincial songs (Schaf frath, 1995; DT,1996;\nMcNab, Smith, Bainbridge, &Witten, 1997). These folksongs areallmonophonic; notes areofvarious\ndurations, butnotmore than asingle note issounded atanygiventime slice. Ignoring durations, McNab\netal.,1997 showthatsevencontiguous note pitches (orsixcontiguous pitch interv als)arenecessary ,on\naverage, touniquely identify asong.\nWewillnotbeworking with contiguous notes ofthislength (asexplained inSections 3.2and4.1), but\nitisimportant tonote thislimit andhowitmight affectourresults. Ifwecreated sequences thatwere\ntoolong, ourevaluation might beuseless; theverylength ofthesequences would beenough toachie ve\ngood retrie valresults. Interested readers canﬁndadetailed analysis oftheinfometric properties ofthis\ncollection inDownie, 1999.\n3.2Document Creation\nNowthatwehaveacollection wemust convertourcollection documents andqueries intoaform suitable\nforevaluation byourtwotextretrie valsystems.             \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\nFigure 2:LucyintheSkywith Diamonds\nAsanexample wehavethepiece ofmusic givenbyFigure 2(Lennon &McCartne y,1967). Westart\nwith anabstract representation ofthispiece which includes thefollowing information: time signatures,\nkeysignatures, note pitches andnote durations. From thisrepresentation weconstruct thegraphic in\nFigure 2.From thisrepresentation wealso contruct atextdocument which represents thissame piece.\nWealsousethisabstract representation toconstruct textqueries with which wewilltesttheeffectiveness\nofourretrie valsystems.Inorder toconvertmusic documents totextdocuments, some simplifying assumptions need tobemade.\nThemain assumption isthat, while important, note durations arenotnearly asimportant asnote pitches.\nWetherefore ignore duration aswell asrests when converting music totext.Furthermore, wemake\nthesame assumption asother researchers thatpitch interv alsarebetter features than absolute pitches,\nbecause ofpossible transpositions from onekeytoanother (Downie, 1999).\nUnigrams Theﬁrsttype ofpitch interv alweuseisinterv alunigrams. Forexample, theﬁrstnote in\nFigure 2isanE.Thesecond note rises toanA.Subsequent notes risetoanE,falltoaG,risetoanE,\nthen falltoanA.Pitch interv alsaremeasured insemitones. Sothesequence ofpitch interv alsbegins:l\u0018mon\u0002pLm8qWp7sr\npLmr\np7\nqWp4,4,4)t(4)\nIntheMelde xcollection there arenointerv alslargerthan +24orsmaller than -24. Forsimplicity andto\navoidworking with “negative”textterms, weadd25toeach interv al.Thus, thepitch interv alunigrams\nrange from 1to49.Thesong inFigure 2becomes thefollowing textdocument:u\n\u0000Wv\u00163\nZiw5\nw2x3\u0014y\nw\u0018z3\u0014{|4,4,4\nu~}\n\u0000\rv\u00163\nZ (5)\nBigrams Thesecond type ofpitch interv alused isinterv albigrams. Whereas unigrams only considered\ntwocontiguous notes, bigrams turnthree contiguous notes intoasingle “text”term. Thesong inFigure 2\nbegins with thefollowing ordered pairs ofunigrams:l\u0007\nm8n\u0002pLm8q\u000e\np\u0007\nm8qWp7r\n\u000e\np\u00077r\npLmr\n\u000e\np\u0007\nmr\np7\nq\u000e\np\u00077\nq\u0002p4,4\u00144\n\u000e\np4,4,4t(6)\nUsing thesame convention foreliminating “negative”terms aswedidwith unigrams, thisbecomes:l\u0007w5\npw2x\n\u000e\np\u0007w2x\np3\u0014y\n\u000e\np\u00073\u0005y\npw\u0018z\n\u000e\np\u0007w\u0018z\np3\u0014{\n\u000e\np\u00073{\np4\u00144,4\n\u000e\np4,4,4t(7)\nWecanagain takeadvantage ofthefactthatthevocabulary sizeofthecollection islimited andknown.\nConsider anarbitrary ordered unigram pair\u0007\t\npb\u000e.Neithernor\naregoing tobelargerthan 49or\nsmaller than 1.Thus, wecanusethefollowing formula toconvert \u0007\t\npb\u000einto aunique single value\nbigram:A\u0003\n\u0002\n#zr\n\nm(8)\nThis ﬁnally yields thefollowing bigram “text”document:u\n\u0000\rv\u00163\nZ3\nn5\nx3\nn{\nz{\f3\u0014{3\u0014y2{\nz4,4,4\nui}\n\u0000Wv\u00163\nZ (9)\nNote thateventhough these converted textterms arenumbers, itisimportant nottothink ofthem as\nintegers, inthesense thattheycanbeordered, added, subtracted andsoon.Theterm “1226”=(25,1)\nmight actually bemore similar to“1177”=(24,1)than itisto“1225”=(24,49),eventhough theinteger\n1226 iscloser totheinteger1225. Theterms aresimply anenumeration, asimple wayofconverting pitch\ninterv alstotext.\n4Experiment One\n4.1QueryConstruction\nTheprocess forturning amusic query intoatextquery issimilar tothatofturning amusic document into\natextdocument. Thequery “wrapper”, thesyntactic sugar ,differsforeach targetsystem, butthebasicmethod isthesame. Inourexperiments queries arecomposed ofpitch interv alunigrams orbigrams.\nNote thatthisdiffersfrom theDownie, 1999 approach, which uses longer 4,5and6-gram queries as\nwell as4,5and6-gram document terms. Wewanttoseehowwell ourapproach does using these basic\nunits, ignoring thelargerdiscriminatory power(butsmaller ﬂexibility andgeneralizability) thatlonger\nn-grams afford.\nForInquery ,each ofourqueries isaweighted sum ofeither unigrams orbigrams. ForLanguage Mod-\neling, aquery isaproduct oftheestimate ofeach unigram orbigram probability ,giventhemodel ofthe\ndocument. Essentially ,ourquery hasbecome a“grab bag”, asetofterms. Theonly sequentiality that\nhasbeen preserv edisthatoftwocontiguous notes (forinterv alunigrams) orthree contiguous notes (for\nintevalbigrams). Forexample:\nInquery query Language Model query\n#q1=#WSUM(1.0\n\u001d\u001e\u0007 \u001f!\u000b \u001a\u001b\n\u000e=\n1.0 30\n\u001d\u001e\u0007w5\n\u000b \u001a-\u001b\n\u000e\n1.0 32 *\n\u001d\u001e\u0007w2x\n\u000b \u001a-\u001b\n\u000e\n1.0 16 *\n\u001d\u001e\u00073\u0014y\n\u000b \u001a-\u001b\n\u000e\n1.0 34 *\n\u001d\u001e\u0007w\u0018z\n\u000b \u001a-\u001b\n\u000e\n1.0 18 *\n\u001d\u001e\u00073\u0014{\n\u000b \u001a-\u001b\n\u000e\n... ...\n);\nAdditionally ,weconstruct queries thataresome where between unigrams andbigrams. Inquery allows\nphraseoperators which takeastheir argument multiple query terms (query content representation nodes)\nandaninteger,.The; (unordered windo w)operator look forquery terms which appear inanyorder\nwithin asize document windo w.The\nv\n\u0000(ordered windo w)operator looks forphrase terms which\nappear intheexact same order asinthephrase operator ,within asizedocument windo w.\nForexample, ifthesong from Figure 2isused asaquery ,itmight betransformed into thefollowing\nweighted sum ofInquery phrase operators:\n#q1=#WSUM(1.0\n1.0 #od3(30, 32)\n1.0 #od3(32, 16)\n1.0 #od3(16, 34)\n1.0 #od3(34, 18)\n...\n);\nWeusefour different Inquery phrase operators: #uw1, #od5, #od3 and#od1. The purpose oftheun-\nordered windo w(#uw1) istoexamine theimportance thatsequencing hasinmusic andwhat happens\nwhen sequencing isignored. Thepurposed oftheordered windo wsatvarious lexical distances (#od1,\n#od3, #od5) istoexamine tighter andlooser matching schemes, toseetowhat degree wearehelped or\nharmed byallowing, ineffect, non-contiguous bigrams (non-contiguous ordered unigram pairs).4.2Results\nThanks toTREC, thetextretrie valcommunity hasseverallargecollections ofdocuments along with\nafairly extensi verelevance judgements oneach collection (Harman, 1995). The music retrie valcom-\nmunity does notyethavethissame standardization anddocument-to-query relevance information. A\nname-that-tune ,orknown-item search, isanalternati veevaluation technique. Asong isselected from\nthecollection, aquery iscreated based onthatsong, andthesystem isevaluated bytheranking ofthis\nitem.\nFifty songs were selected atrandom from theMelde xcollection andtranformed intoqueries ofvarious\nlengths. Full textqueries contain anaverage of50notes (49interv alunigrams, or48interv albigrams).\n12-note and7-note incipits arequeries constructed byusing theinitial 12or7notes ofasong.\nTable 1lists theInquery andLanguage Modeling approaches using unigrams andbigrams, andtheIn-\nquery approach using unigram phrases (uw1, od5, od3andod1). Thevalue shownistheaverage rank\nachie vedbyeach retrie valapproach overqueries from 50songs. Thelowertherank, thebetter thesystem\nperformance.\nTable 1:Average rank ofknownitem over50queries\nSystem QueryType full-te xt 12-note incipit 7-note incipit\nINQUER Y unigram 259 717 1420\nLANGMOD unigram 1377 1168 1578\nINQUER Y bigram 1 14 162\nLANGMOD bigram 1 12 221\nINQUER Y Phrase uw1 2155 2916 3731\nINQUER Y Phrase od5 22 180 667\nINQUER Y Phrase od3 5 98 507\nINQUER Y Phrase od1 1 15 164\n4.3Discussion\nQueryLength One might question theefﬁcacyofusing thefulltextofaknownitem forquery con-\nstruction. This wasdone because itgivesanupper bound ontheperformance ofeach retrie valtechnique.\nIntextretrie val,ifthefulltextofadocument isused tocreate aquery ,performance would likelydrop.\nThere aretoomany“noisy” words inadocument, words which conceptually havelittle todowith the\naboutness ofthatdocument. Ontheother hand, intuition says thatthelonger amusical passage, themore\nthatpassage willbeabout thesong from which itistaken.Theresults intable 1showthatthisintuition\nholds forallthese retrie valsystems andqueries, except Language Modeled unigrams. Thelargerthesize\nofthequery ,thebetter thesystem performs, onaverage.\nInqueryversusLanguage Modeling Forunigrams, theInquery approach does much better than the\nLanguage Modeling approach. Neither system, however,hasanabsolute performance levelanywhere\nnear acceptable. This isnotsurprising, since unigrams contain solittle sequential information necessary\nformusic. Withbigrams, ontheother hand, thedifference between thetwoapproaches narro ws. For\nfull-te xtqueries, Language Modeling does worse than Inquery inonly 2of50instances, andequal to\nInquery intheremaining 48instances. Itisnotcurrently knownwhy language modeling failswith uni-\ngrams, relati vetoInquery ,butsucceeds with bigrams. Unigrams donotdiscriminate verywell between\nsongs, andperhaps Inquery’ sinverse document frequenc yweighting handles thisproblem better thanthelanguage modeling approach. Abigram encapsulates alonger sequence ofnotes than aunigram,\nincreasing implicit term discrimination perhaps toapoint where Inquery’ sweighting scheme nolonger\nprovides signiﬁcant impro vement overtheLanguage Modeling scheme.\nInqueryPhrases One main difference between thelanguage modeling approach andInquery isthat\ntheformer does notcontain thenotion ofphrases, orterm non-contiguity ,while thelatter does. The\nﬁrstphrase experiment, #uw1, performs asexpected: poorly .The unordered windo wsize1simulates\ncontiguity between unigrams, increasing thediscrimination poweroftheterm, butthedisre gard forthe\norder ofthecontiguity destro ysanydiscrimination wemight havegained. The#uw1 phrases doeven\nworse than unigrams bythemselv es,onaverage.\nThere areexceptions. Aquery-by-query analysis showsthat#uw1 succeeds with (approximately three)\nqueries thathavealargeproportion ofcontiguous notes with thesame pitch: #uw1(25, 25). Insuch\ncases, theﬁrstandsecond term inaphrase arethesame, andthesymmetry isnotbrokenbyanunordered\nwindo w.A#uw1 phrase effectivelybehaveslikean#od1 phrase, soperformance impro ves.Inthevast\nmajority ofinstances, however,theunigrams arenotthesame andresults arehurtsigniﬁcantly; toomany\nspurious matches arefound.\nBypaying attention totheordering ofcontiguous unigrams, recapturing theoriginal sequentiality ofa\nsong,odphrases provide better performance. The#od1 phrases areextremely similar tobigrams. When\nthedistance between theordered unigrams islooser (#od3 and#od5), perhaps toallowmore ﬂexibility\ninmatching, performance drops; more spurious matches arefound. (More relevantsongs might befound\naswell, butwecannot testthiswithout better collections andrelevance judgements.) The amount of\nthedrop appears tobeinversely proportional tothesizeofthequery .Thelonger thequery ,thelessthe\nranking isaffected byspurious matches duetolargerwindo wsizes.\n5Experiment Two\n5.1QueryConstruction\nWecannot control thelength oftheuser query ,butwecancontrol howthatquery isused. This leads to\nasecond experiment, based onInquery’ sﬂexible phrase operators. (Wedonotcontinue with Language\nModeling because current text-based approaches donotallowfornon-contiguous n-grams, orphrases.)\nRecall thatInquery’ sphrase operators takeastheir arguments query content representation nodes (sec-\ntions 2.1and4.1). Intheprevious experiment, these nodes were pitch interv alunigrams andbigrams.\nHowever,phrase operators arethemselv esquery content representation nodes. Phrase concept nodes can\ntherefore beinferredfromotherphraseconceptnodes .Simply put,phrases canbenested.\nAsample query with nested phrases isthis#od3 phrase of#od5 phrases:\n#q1=#WSUM(1.0\n#od3( #od5(30, 32)\n#od5(32, 16)\n#od5(16, 34)\n#od5(34, 18)\n...\n)\n);Anexponential number ofphrase combinations andnestings arepossible. Phrases canbenested toan\narbitrary depth, phrase distance arguments canbevaried atarbitrary nodes, indeed theentire shape ofthe\nquery content node netw orkcanbemodiﬁed andrearranged. Forsimplicity ,weonly test#odphrases\nof#od\nphrases. Trials arerunforqueries ofeach length: full-te xt,12-note incipits, and7-note incipits.\n5.2Results\nTable 2:Average rank ofknownitem over50queries\nSystem QueryType full-te xt 12-note incipit 7-note incipit\nINQUER YPhrase od5ofod5 1 9 218\nINQUER YPhrase od5ofod3 1 4 116\nINQUER YPhrase od5ofod1 1 1 18\nINQUER YPhrase od3ofod5 1 2 99\nINQUER YPhrase od3ofod3 1 2 84\nINQUER YPhrase od3ofod1 1 1 13\nINQUER YPhrase od1ofod5 1 1 13\nINQUER YPhrase od1ofod3 1 1 11\nINQUER YPhrase od1ofod1 1 1 8\n5.3Discussion\nTheresults areencouraging forthenestedodxoperators. Foranygivenquery ataspeciﬁed query length,\nthenested phrase equals oroutperforms theunigram, bigram, andstraight #od5, #od3, and#od1 forms\nofthatquery .Theonly exception isthe#od5 of#od5s on7-note queries; thephrase windo wsaretoo\nloose andthequery length tooshort. Otherwise, everyother query construct outperforms straight #od1\nphrases andbigrams.\nQuery size still matters, butusing nested phrase operators inamanner which attempts torecapture\ntheoriginal sequentiality ofthesong produces more precise results. Theadvantage isthatthese looser\nmatching schemes (#od5 of#od3s, andsoon)allowpotentially better recall without sacriﬁcing toomuch\nprecision. When doing aknown-item search thisisnotinteresting. However,once largercollections and\nrelevance judgements become available (once multiple documents arejudged relevanttoasingle query),\ntheability toallowformore ﬂexible matching without hurting precision isimportant. Itistheauthor’ s\nopinion thatthere arebeneﬁts indifferent query formulations andshapes. Different music features and\nvariations could beemphasized orignored, based onhowthequery isconstructed.\n6Conclusion\nThegoal ofthispaper wastoevaluate current textinformation retrie valsystems asapplied tothemono-\nphonic music retrie valtask. Although other probabilistic methods such astheInquery system havebeen\ntried, theLanguage Modeling approach, tothebestoftheauthor’ sknowledge, hasneverbeen applied to\nmusic. This latter approach didnotyield impressi veresults.\nHowever,theframe workoffered byLanguage Modeling isstilluseful. Inmost current textretrie val\nsystems, aseparate model isused fordocument represenation andforquery-to-document-re presentation\nmatching. Additional layers ofabstraction areadded, “concepts” areinferred from documents andfrom\nqueries, andthen these concepts areretrie ved. Fortext,thisadditional abstraction does notalways\npose asigniﬁcant problem. There isoften high correlation between tokenized alphanumeric sequences\n(words) andconcepts. Withmusic, ontheother hand, concepts aremuch more difﬁcult toﬁnd. Where\nismeaning found inmusic? Inasingle note? Inanote interv al?Insixcontiguous note interv als? In\nsixnon-contiguous note interv als(i.e.: because ofimpro visation orsome other variation)? Theanswerremains elusive,andaretrie valsystem which bases itseffectiveness onitsability toextract some sort\nofmusical terms orconcepts from amusic document willhavetosolvethisproblem before tackling the\nactual retrie valtask.\nThe Language Modeling frame workbypasses thisintermediate step andgoes directly from document\ntoquery .Itasks howlikelyitisthatagivendocument could havegenerated aquery ,regardless ofthe\n“concepts” thatarefound ineither thedocument orthequery .This frame workthus becomes veryuseful\nformusic, where concept extraction isdifﬁcult.\nDespite these rantings about thedifﬁculty ofﬁnding orinferring musical concepts from documents, this\npaper also showsthatanadequate workaround ispossible. While other approaches tomusic indexing\nhaveconcentrated onlonger -grams, weﬁndthatitispossible toindexseemingly meaningless terms,\npitch interv alunigrams, andstillgetgood results. TheInquery phrase operators letauser construct a\nquery which takesintoaccount theordered distance ofunigrams, creating asemblance oftheoriginal\nsequence. Aconﬁgurable windo wsizeaswell astheability tonest phrases toanarbitrary depth allows\nforﬂexibility inprecision andrecall.\nHowever,thisInquery phrase technique isstrictly limited tomonophonic music. Forpolyphonic music,\nitisunclear howonemight indexinterv alunigrams. Ifpolyphonic music consisted exclusi velyofchords,\nthetaskwould bemuch simpler .Butmost interesting music hasmultiple voices, multiple parallel threads,\nallofwhich arenotalwayseasily resolv edinto asingle one-dimensional sequence. Other techniques\nneed tobedeveloped.\n7Acknowledgements\nThis material isbased onworksupported inpartbytheNational Science Foundation, Library ofCongress\nandDepartment ofCommerce under cooperati veagreement number EEC-9209623, andNSF grant num-\nberIIS-9905842. Anyopinions, ﬁndings andconclusions orrecommendations expressed inthismaterial\naretheauthor(s) anddonotnecessarily reﬂect those ofthesponsor(s).\nReferences\nAMNS (2000). Nightingale music notation softw are.http://www .ngale.com.\nByrd, D.&T.Crawford (2000). Problems ofinformation retrie valinpolyphonic music.Submitted for\npublication .\nCallan, J.,W.Croft, &S.Harding (1992). The inquery retrie valsystem. InProceedings ofthe3rd\nInternational ConferenceonDatabase andExpertSystemsApplications ,pp.78–83.\nDownie, J.S.(1999).Evaluating aSimpleApproachtoMusicInformation Retrieval:Conceiving\nMelodicN-gramsasText.Ph.D.thesis, University ofWestern Ontario.\nDT (1996). Digital tradition folk song database. The Digital Tradition.\nhttp://web2.x erox.com/digit rad.\nHarman, D.(1995). Thetrecconferences. InR.Kuhlen &M.Rittber ger(Eds.),Hypertext-Informa-\ntionRetrieval-Multimedia; SynergieeffekteElektronischerInformationssysteme ,Proceedings of\nHIM'95,pp.9–28. Universitaetsforlag Konstanz.\nLennon, J.&P.McCartne y(1967). Lucyintheskywith diamonds. Recording, from thealbum”Sgt.\nPepper’ sLonely Hearts Club Band”.\nMcNab, R. J., L. A. Smith, D. Bainbridge, & I.H. Witten (1997). The\nnewzealand digital library melody index. InD-LibMagazine .Available at:\nhttp://www .dlib.org/dlib/may 97/meld ex/05witt en.html.\nPonte, J.M.&W.B.Croft (1998). Alanguage modeling approach toinformation retrie val.InPro-\nceedingsofACMSIGIR ,pp.275–281.Schaf frath, H.(1995). The essen folksong collection. (Four computer disks containing\n6,255 folksong transcriptions and 34-page research guide. http://www .musicog.ohio-\nstate.edu/Huron/public ations.html) .Center forComputer Assisted Research intheHumanities\n(Stanford, CA).\nTurtle, H.&W.B.Croft (1991). Evaulation ofaninference netw ork-based retrie valmodel.ACM\nTransactions onInformation Systems9(3),187–222."
    },
    {
        "title": "XML4MIR: Extensible Markup Language for Music Information Retrieval.",
        "author": [
            "Perry Roland"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1417167",
        "url": "https://doi.org/10.5281/zenodo.1417167",
        "ee": "https://zenodo.org/records/1417167/files/Roland00.pdf",
        "abstract": "adoption of XML standards for music representation and meta-data to serve as the basis for music information retrieval.",
        "zenodo_id": 1417167,
        "dblp_key": "conf/ismir/Roland00",
        "keywords": [
            "music information retrieval",
            "eXtensible Markup Language (XML)",
            "music representation",
            "information exchange",
            "meta-data",
            "unicode",
            "musicat",
            "data communication",
            "standards",
            "music data"
        ],
        "content": "Perry Roland \nDigital Library Research Group \nUniversity of Virginia \n \nAbstract: This paper evaluates the role of standards in information exchange and suggests the \nadoption of XML standards for music representation and meta-data to serve as the basis for \nmusic information retrieval. \n \nKeywords: Music Information Retrieval (MIR), eXtensible Markup Language (XML), music \nrepresentation, information exchange standards, Unicode, MusiCat \n \n \n \n \nXML4MIR: \nExtensible Markup Language for Music Information Retrieval \n \nThe most important word in the phrase “Music Information Retrieval” is “information”.  \nPonder why it’s not “Music Data Retrieval”.  Information is not just data.  Instead, information is \ncommunication, the exchanging of data and meta-data.  A great deal of effort has been expended \non capturing music data; however, for the full potential of MIR to be realized, we must \nconcentrate on how the communication of the data will take place.  Standards, e.g. common \nlanguages, are the enablers of communication. \n \nWhy do we need standards? \n Standards are good for scholars.  They extend the scale, breadth, and accessibility of \nscholarly evidence and encourage innovation in learning and teaching.  In addition, standards \nfacilitate innovation and collaboration in scholarly discourse (Greenstein, 254). \n Standards are also good for business.  They reduce the costs associated with acquiring \nand preparing data and help create new sources of revenue for that data.  For example, publishers \ncan syndicate material to other information providers or target their material to traditional \npublishing formats or even electronic formats, such as Internet devices or CD-ROMs, more \neasily.  Additional revenue can be generated from the same material with minimal cost and effort \n(Floyd, 48). \n Bob Metcalfe has stated \"the usefulness of a network grows exponentially with its \nnumber of users\" (qtd. in St. Laurent).  The clearest example of this statement in action is the \nrapid growth of the Internet.  Of course, Metcalfe's Law can be applied to other technologies as \nwell, including data encoding standards: With a standard, more users will take advantage of the \ntechnology; as the number of users increases, the price of implementing the technology falls; \nfalling prices create competition to improve the technology; and, finally, improved technology \nincreases the usefulness of the standard. \n The implications of this economic model of technology are that continuing the status quo, \nwithout a standard, is shortsighted.  In the long-term, more readily available data means more \ncustomers, at least for tools and value-added information, if not for the raw data itself.  At least \none company, Muze Incorporated, which plans to provide electronic meta-data for music (Luh), \nbelieves that more efficient access to information may also speed up consumers' purchase decisions, presumably creating more purchases.  Also, in this economic model, both commercial \n(publishing) and non-commercial (scholarly) users of the technology benefit as they each \nfunction in a symbiotic relationship as both data providers and data consumers. \n \nWhy not use existing standards? \n While it might be possible to adopt an existing music representation, there are several \nproblems with this approach. \n Most existing music representations are inappropriate due to their scope.  The analytical \ndomain, which we seek to exploit in MIR, is most often the first thing to be defined as “out of \nscope”.  Many representations define their approach to music encoding too narrowly, \nconcentrating on a particular use of the data such as printing or automated performance.  Most of \nthese representations are useful as input codes, but they have limited use as intermediate \nrepresentations (Selfridge-Field, 571).  Other representations, such as the Standardized Music \nDescription Language, have attempted to represent music too broadly.  SMDL has been unable \nto attract a large user group in part because it is difficult for potential users and tool developers to \nsee how SMDL might apply to their particular situation.  A standard that is defined generally \nenough to represent all music can only be made to work for a particular subset with great effort. \nIn addition to problems of scope, many existing solutions are hardware or software \ndependent.  The lack of acceptance of specialized hardware input devices for music, such as \ntablets and touch screens, outdated storage mechanisms, like punched cards and paper tape, and \nincompatible file formats should stress the necessity of hardware neutrality. \n Many existing codes are also proprietary.  After expending a great deal of effort to create \nthem, their owners are reluctant to divulge their inner workings.  Therefore, their use for \ninformation exchange is severely limited. \n Not only is there a need for a music representation standard; we also need to examine \nexisting meta-data standards in order to judge their suitability for MIR purposes. \n As compromise solutions for a wide variety of materials, most meta-data systems are \ninadequate for MIR.  For example, using MARC (MAchine Readable Catalog) records it is \ndifficult to express the complex relationships frequently found in music meta-data.  The TEI \n(Text Encoding Initiative) scheme is designed for text representation, not meta-data exchange.  \nThe EAD (Encoded Archival Description) format is designed for the use of the archival \ncommunity and so lacks elements necessary for the complete description of music objects.  For \nthese reasons, a meta-data standard for music has been proposed which not only addresses these \nissues, but also allows the leveraging of existing music meta-data, e.g. thematic catalogs \n(Roland, MusiCat). \n \nWhy use XML? \n The foremost reason for using XML is that it is a platform-independent, open standard.  \nXML is a simplified subset of the Standard Generalized Markup Language (ISO8879) standard.  \nDespite its name, XML isn't really a markup language, but a meta-language, designed to support \nthe definition of community-specific languages.  Because there are no limits on the use of \nelements across multiple namespaces or on the structural depth that a markup language might \nemploy, XML is very powerful.  Furthermore, it is easy to implement.  Developers need not \ncreate their own tools for authoring, parsing, transforming or displaying XML.  There is already \nan ever-growing set of free tools available.  The fact that the SGML/XML approach is nearing ubiquity also provides a strong reason \nfor employing it for music representation and meta-data.  XML is the foundation of a broad \nrange of industry and academic standards, such as CML (chemistry), MML (mathematics), and \nThML (theology), PGML and SVG (graphics), and SMIL (presentation media).  Although most \nof them are not yet fully developed, there are even a few XML markup schemes for music. \n Outside the music field there is a large, organized SGML/XML community already in \nexistence.  Users include academic institutions, government agencies, such as the U.S. \nGovernment Printing Office, Department of Defense and Internal Revenue Service, as well as \ncommercial interests such as publishers, airlines, and manufacturers.  Organizations promoting \nthe development of XML include ISO (International Standards Organization), OASIS \n(Organization for the Advancement of Structured Information Standards), and W3C (World-\nWide Web Consortium).  A large community of SGML/XML users indicates a potential \nconsumer base for encoded music materials.  It also means that the resources and skills of this \ncommunity are transferable to the task of encoding music. \n Internationalization provides another argument in favor of adopting XML.  XML was \ndesigned from the beginning to support the inclusion of multiple languages and symbols.  Since \nmusic often includes text as labels, lyrics and performance directions, it is appropriate to employ \nan encoding that uses Unicode, an extensive international character set, for both content and \nmarkup. \n \nWhy use XML for music representation? \n XML is grammar-based.  XML documents can be validated using a Document Type \nDefinition or DTD, which is a formal statement of the rules governing the document’s grammar.  \nRoads enumerates several advantages of grammars: there is a long history of research into the \nuse of grammars in music description, because any music that can be segmented can be \nrepresented, grammars are very powerful, grammars provide a generative model (Roads, 408-\n409).  In addition, utilizing a grammar speeds encoding and reduces encoding errors. \nXML is declarative.  According to Desain and Honing, a declarative representation is \npreferable to a procedural one because declarative knowledge is accessible.  That is, it can easily \nbe examined and combined with information from other sources.  A declarative representation is \nalso composable, e.g. the meaning of a complex expression is based on or can be derived from \nthe meaning of its parts and their combinations (Desain and Honing, 142).  Also, since no \ninteractions occur between structural entities, the representation is extremely modular (Desain \nand Honing, 131).  The importance of modularity is discussed in more detail below. \nXML is structured hierarchically.  The hierarchical view of music suggested by Buxton \nseems natural to most musicians because they are trained to approach music this way.  Teachers \noften implore students to perform phrases, not notes.  Similarly, when we analyze the structure of \na piece of music, it is natural to name its parts and show how some elements are more important \nthan others.  This is the essence of analysis.  In his design principles for music representation, \nDavid Huron (Huron1992, 26) has suggested that it is appropriate for representation to be \nisomorphic with the thing represented.  One might say XML is structurally isomorphic with \nmusic. \nIn addition, the tree data structure is conceptually easy and provides efficient, non-linear \ndata retrieval. Also, this data structure makes it possible to apply transformations to groups of \nobjects (Gourlay, 393). Of course, the simplest hierarchy, e.g. a sequence of events, is also conceptually simple, \nbut it is difficult to encode structural relationships between events using a single sequence.  \nDefining the structural relationships in music is perhaps the most important goal of \nrepresentation.  Music encoded as a sequence of events (e.g. as in MIDI) makes automated error \ndetection impossible when more than one kind of event is included. \nOn the other hand, the most problematic thing about defining structure in music is that a \nsingle hierarchy is inadequate to describe the multiple, and sometimes overlapping, hierarchies \nsuch as those created by beams, measures, phrases, voices, and sections.  However, the \nSGML/XML community has developed several methods of representing multiple hierarchies in \ntexts (Barnard, et al) that can be applied to music.  First, attributes (or perhaps other elements) \ncan be used to mark an element as belonging to one or more hierarchies.  Figure 1 shows how an \nelision of two phrases might be indicated using attributes. \n \n<note id=”n1” hier=”phrase1”/> \n<note id=”n2” hier=”phrase1”/> \n<note id=”n3” hier=”phrase1 phrase2”/> \n<note id=”n4” hier=”phrase2”/> \n<note id=”n5” hier=”phrase2”/> \n<note id=”n6” hier=”phrase2”/> \nFigure 1 \n \n Second, boundary elements that have no content themselves can be used to mark the \nbeginning and end of events.  Figure 2 illustrates the use of these elements to mark the same \nelided phrases.  Note that the position of the phrase-defining elements is significant. \n \n<beginphrase id=”p1”/><note id=”n1”/><note id=”n2”/> \n<beginphrase id=”p2”/><note id=”n3”/><endphrase id=”p1”/> \n<note id=”n4”/><note id=”n5”/><note id=”n6”/><endphrase id=”p2”/> \nFigure 2 \n \n Finally, separate analysis elements can be used to create references that show how other \nelements are to be organized.  In figure 3 the position of the phrase elements is irrelevant to the \norganization of the notes.  They could be placed elsewhere in the document or, using other \nXML-related standards, such as XML Linking Language (XLink) and XML Path Language \n(XPath), even in a separate document. \n \n<note id=”n1”><note id=”n2”><note id=”n3”> \n<phrase id=”p1” start=”n1” end=”n3”/> \n<note id=”n4”><note id=”n5”><note id=”n6”> \n<phrase id=”p2” start=”n3” end=”n6”/> \nFigure 3 \n \n Of course, more than one of these approaches could be implemented at a time, giving the \nuser the opportunity to select the method most appropriate for the music being encoded.  Also, \neven though it introduces a certain amount of inconsistency, different methods can be applied to \ndifferent hierarchies.  For instance, phrases might be marked with analysis elements while beams \nare indicated by boundary elements. \n XML is modular.  Music is often thought of as having separate visual, analytical, and \nperformance aspects or domains.  Furthermore, each of these domains is frequently described in terms of separate facets, e.g. time, pitch, harmony, etc.  Of course, not every musical task \ninvolves all the domains or facets.  Printing perhaps requires the most representationally \ncomplete encoding, while analysis requires less, and automated performance less still.  Using \nXML would facilitate a general encoding which allowed the facets most important to the task at \nhand to be represented completely and those of less importance to be minimized or left out \nentirely, a concept Huron calls “selective feature encoding.” (Huron1997, 375)  In other words, \nthe XML Document Type Definition can be written to allow multiple levels of representational \ncompleteness.  A modular DTD also allows multiple encoding styles.  For example, for \nrepresenting pitch one encoder prefers MIDI note numbers while another would rather use ANSI \nstandard pitch designations such as “C#4”.  Either style can be allowed in the representation \nsimply by “switching on” the appropriate module.  Of course, each of the other domains and \nfacets can be handled in a similar fashion. \n XML is extensible, an absolute requirement for music representation systems.  \nExtensibility functions to “resolve ambiguity already latent within the existing scheme” \n(Huron1992, 35).  Extensibility also allows the representation to absorb some changes in coding \nrequirements.  For example, a DTD can provide a mechanism for making arbitrary changes to \nelement names and content specifications.  This feature might be useful for translating English \nelement names into Russian for a colleague in Moscow or redefining the content model for note \nelements to accommodate future developments, such as a successor to MIDI. \nAs long as the extensions are simply additions, the representation may be extended \nwithout necessarily making existing documents non-conformant.  Of course, deciding when to \nextend and when to abandon the representation for a new one becomes difficult when the \nextensions affect the structure of the representation.  The ultimate in extensibility, however, is a \ngood exit strategy.  Since a DTD is not absolutely required for XML, one could opt for a well-\nformed representation, that is, where there are no overlapping markup structures or, with the \ntools discussed below, transform the old grammar into a new one. \nXML is human-readable.  Human-readability makes data creation and maintenance easier \nand functions as a protection against technological obsolescence.  These considerations are very \nimportant for music because the body of material to be encoded is so vast and the investment in \nencoding is so large. \nAt least two of Huron’s design principles are addressed by human-readable encoding.  \nFirst, an encoding that is human-readable can be made mnemonic, that is, meaningful to the \nencoder who must learn little or no new vocabulary.  Second, human-readable representations \nare non-cryptic.  They make it easier to use the encoding because the relationship between the \nrepresentation and the thing represented is made clear.  They also contain redundancy that aids in \nthe recognition of errors (Huron1992, 26). \n \nIML \n’=CLEF G’ ’*KEY UFS LC’ ’+TIME(C)’ \nLD2 UE4 *F4 / F4* LD8 C8 UUA4.  G8 / FN1 // \n \nPLAINE AND EASIE CODE \n(#FC, C) ’2D 4E F_/ 8D C ’’4.A 8G / 1NF // \n \nDARMS \n!G !K2# !MC 20H 1Q 2J / 2 (20 19) 31Q 30E/ 9*W /!/ XML CODE \n<mdl> \n<clef type=”G” pos=”treble”/> \n<key sig=”2#” tonic=”D”/> \n<time sig=”C” norm=”4/4”/> \n<bar startline=”invis”> \n<note pitch=”D4” dur=”2”/> \n<note pitch=”E4” dur=”4”/> \n<note pitch=”F#4” dur=”4” tie=”initial”/> \n</bar> \n<bar> \n<note pitch=”F#4” dur=”4” tie=”final”/> \n<note pitch=”D4” dur=”8”/> \n<note pitch=”C#4” dur=”8”/> \n<note pitch=”A5” dur=”4.”/> \n<note pitch=”G5” dur=”8”/> \n</bar> \n<bar endline=”endbar”> \n<note pitch=”Fn5” dur=”1”/> \n</bar> \n   </mdl> \nFigure 4 \n \n Figure 4 illustrates the impact of human readability.  While it cannot be said that the \nXML code is more compact than the others, it is clearly not a private code.  Even to someone \nwith minimal training, it is immediately obvious what the code represents.  Furthermore, \nunambiguous labeling of the data also enhances machine readability. \n While XML may be somewhat verbose, reduction of the code can be achieved by several \nmethods.  User-defined entities and standard character entity references may be employed.  A list \nof 220 music symbols (Roland) has been approved by the Unicode Technical Committee and is \nawaiting the next step on the path to approval by the International Standards Organization.  Code \nreduction for network transmission can be accomplished via standard loss-less compression \nschemes, such as Run Length Encoding (RLE) and Lempel-Ziv-Walsh (LZW) compression, and \nby network acceleration technology currently in development (XML Growth).  However, as \nprocessors get faster and more bandwidth becomes available, terseness for machine processing \nand transmission will become less of a concern.  We must be careful not to repeat the mistakes of \nthe past, i.e. encoding of dates with two digits instead of four, when immediate processing needs \nwere allowed to take precedence over protecting the data from technological changes. \n The final, and perhaps most important, reason for choosing XML for MIR is that it \nseparates content and structure from presentation and behavior.  Taking data creation into the \nequation, the problem of music representation can be seen as consisting of three components -- \ninput, communication, and output (Gourlay, 389).  Separation into these components alleviates \nthe problems of strong coupling of the syntax and semantics of the music encoding language to \nparticular processors or processing techniques (Page, 48-49).  The main disadvantage of \ndecoupling is that additional mechanisms are required for associating behavior with the data.  \nIndustry agreement on the creation and processing of a specific grammar is required -- no small \ntask given that music notation is complex, ambiguous, and redundant, and that there is a very \nwide range of processing requirements.  Fortunately, companion standards to XML, XSLT and \nXPath, offer standard methods for processing, transforming, and querying XML documents.  The advantages of decoupling content from presentation outweigh the disadvantages.  \nFirst, high-level abstraction of the content allows particular representations to be generated as \nneeded.  The content can be re-used more effectively since the creator provides a single data \nstream for both data exchange, e.g. machine consumption, and publishing, e.g. human \nconsumption.  Media independence in publishing can be achieved without altering the basic data.  \nDifferent devices, such as web browsers and MIDI file players, may render the same data \ndifferently.  Downstream document processing, such as that required for some MIR tasks like \ndisplaying selected portions of the data, may be carried out independently of the data.  User \nautonomy, an important consideration in music representation (Dannenberg, 24; Byrd, 20) can be \nprovided through user-configurable views of the data.  Second, rendition on the client reduces \nthe server load and transmission time in a network environment.  Finally, accommodating a large \nnumber of encoders is essential for encoding the massive amount of music required for large-\nscale MIR.  By separating the input and communication phases, input can be accomplished using \ntools, such as word processors, databases, MIDI devices, graphical interfaces, or text processors, \nwhich accommodate a wide range of user preferences and budgets (Gourlay, 391). \n \nWhy use XML for music meta-data? \n All of the advantages of XML enumerated above apply not only to music \nrepresentation, but also to the representation of music meta-data.  The primary advantage of \nadopting XML for both is improved integration of the music and its meta-data.  XML music \nrepresentations may contain XML meta-data or XML meta-data may contain XML music \nrepresentations.  Figure 5 illustrates how common elements can be shared between the two \nstructures.  In the MusiCat markup the notation is used to uniquely identify the object of the \nmeta-data while in the mdl markup the notation is a complete representation of the principal \nobject. \n \n<MusiCat>      <mdl> \n<title>Musicke in Bbb</title>   <title>Musicke in Bbb</title> \n<agent>Roland, Perry</agent>   <agent>Roland, Perry</agent> \n<incipit>      <notation> \n<notation>(...)</notation>   <note/> \n</incipit>       <note/>(...) \n<analysis>(...)</analysis>   </notation> \n</MusiCat>      </mdl> \nFigure 5 \n \nTo summarize, XML provides the music community with a method for achieving \ninteroperability of content and style, freedom from vendor control of the data, creator control of \nthe markup syntax, and user control of the behavior of the data.  Given the complexity of the task \nof creating a large-scale music information retrieval system, these are advantages that we cannot \nafford to disregard. Works Cited \n \n\"XML Growth Continues -- In More Ways Than One.\"  IT Director 10 May 2000.  Available: \nhttp://www.it-director.com/00-05-10-2.html.  May 12, 2000. \n \nBarnard, David T., et al.  \"Hierarchical Encoding of Text:Technical Problems and SGML \nSolutions.\" The Text Encoding Initiative: Background and Contents.  Guest Ed. Nancy Ide and \nJean Véronis.  Computers and the Humanities 29 (1995): 211-231.  Available: http://www.oasis-\nopen.org/cover/barnardHier-ps.gz.  May 12, 2000. \n \nBuxton, William, et al.  \"The Use of Hierarchy and Instance in a Data Structure for Computer \nMusic.\"  Foundations of Computer Music.  Ed. Curtis Roads and John Strawn.  Cambridge, MA: \nMIT Press, 1985.  443-466. \n \nByrd, Donald.  \"Music Notation Software and Intelligence.\"  Computer Music Journal 18 (Spring \n1994): 17-20. \n \nDannenberg, Roger B.  \"Music Representation Issues, Techniques, and Systems.\"  Computer \nMusic Journal 17 (Fall 1993): 20-30. \n \nDesain, Peter and Henkjan Honing.  Music, Mind and Machine: Studies in Computer Music, \nMusic Cognition and Artificial Intelligence.  Amsterdam: Thesis Publishers, 1992. \n \nFloyd, Michael.  \"Separating Body from Soul.\"  Web Techniques 5 (July 2000): 46-49. \n \nGourlay, John S.  \"A Language for Music Printing.\"  Communications of the ACM 5 (May \n1986): 388-401. \n \nGreenstein, Daniel.  \"Publishing Scholarly Information in a Digital Millennium.\"  Computers in \nthe Humanities 32 (1998): 253-256. \n \nHuron, David.  \"Design Principles in Computer-Based Representation.\"  Computer \nRepresentations and Models in Music.  Ed. Alan Marsden and Anthony Pople.  New York: \nAcademic Press, 1992.  5-39. \n \nHuron, David.  \"Humdrum and Kern: Selective Feature Encoding.\"  Beyond MIDI: The \nHandbook of Musical Codes.  Ed. Eleanor Selfridge-Field.  Cambridge, MA: MIT Press, 1997.  \n375-401. \n \nLuh, James C.  \"A Company Trying to Drive a Real-World Use of XML.\"  Internet World 18 \nJan. 1999: 17. \n \nPage, Stephen Dowland.  Computer Tools for Music Information Retrieval.  Diss.  Oxford Univ., \n1988.  Ann Arbor: UMI, 1988.  AAGD-96200. \n Roads, Curtis.  \"Grammars as Representations for Music.\"  Foundations of Computer Music.  \nEd. Curtis Roads and John Strawn.  Cambridge, MA: MIT Press, 1985.  403-442. \n \nRoland, Perry.  MusiCat DTD.  Latest version available: \nhttp://www.lib.virginia.edu/~pdr4h/MusiCat/DTD/. \n \nRoland, Perry.  \"Proposed Musical Characters in Unicode (ISO/IEC 10646).\"  Beyond MIDI: \nThe Handbook of Musical Codes.  Ed. Eleanor Selfridge-Field.  Cambridge, MA: MIT Press, \n1997.  553-562.  Latest version available: \nhttp://www.lib.virginia.edu/dmmc/Music/UnicodeMusic. \n \nSelfridge-Field, Eleanor.  \"Beyond Codes: Issues in Musical Representation.\" Beyond MIDI: \nThe Handbook of Musical Codes.  Ed. Eleanor Selfridge-Field.  Cambridge, MA: MIT Press, \n1997.  565-572. \n \nSt. Laurent, Simon.  Letting Go: The Futures of XML and SGML.  Available: \nhttp://www.simonstl.com/articles/lettinggo.htm.  May 12, 2000. \n \nUnicode Consortium.  The Unicode Standard, Version 3.0.  Reading, MA: Addison-Wesley, \n2000."
    },
    {
        "title": "MCML - Music Contents Markup Language.",
        "author": [
            "Jochen Schimmelpfennig",
            "Frank Kurth"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415526",
        "url": "https://doi.org/10.5281/zenodo.1415526",
        "ee": "https://zenodo.org/records/1415526/files/SchimmelpfennigK00.pdf",
        "abstract": "We present an XML-based description interface for various types of musical contents - MCML, Music Contents Markup Language. An application of  MCML currently developed within our group is a music browser system which enables a content based navigation in digital music files. Another major application of a music contents annotation interface is the description and handling of query results to digital music libraries. Motivation and Goals The main goal of the MCML project was to specify a description language useful for both the presentation of results to content based queries to a music database (in particular the database developed within the MiDiLiB-project [1]) and a content based navigation in digital music files. Although this includes structuring of musical information on a score-like level, we did not intend to develop a language with the capabilities of a complete music notation system. Instead, we focused on a universal, easy to use and to process language for supporting the former applications. Research Contributions In developing MCML, the following research topics had to be addressed: • Which classes of musical contents and information are important for the above applications such as navigation and which are not? • How to describe the contents on which navigation will be performed? • How can a reasonable content based navigation in music files be realized? • Is it reasonable to use state-of-the-art content description interfaces such as XML to realize music contents description? • How can a linking concept improve navigation?",
        "zenodo_id": 1415526,
        "dblp_key": "conf/ismir/SchimmelpfennigK00",
        "keywords": [
            "XML-based description interface",
            "Music browser system",
            "Digital music files",
            "Content-based navigation",
            "Music contents annotation interface",
            "Query results",
            "Digital music libraries",
            "Music database",
            "MiDiLiB-project",
            "Music notation system"
        ],
        "content": "MCML - Music Contents Markup Language\nAbstract\nWe present an XML-based description interface for various types of musical contents - MCML,\nMusic Contents Markup Language . An application of  MCML currently developed within our group is\na music browser system which enables a content based navigation in digital music files. Another major\napplication of a music contents annotation interface is the description and handling of query results\nto digital music libraries.\nMotivation and Goals\nThe main goal of the MCML project was to specify a description language useful for both the\npresentation of results to content based queries to a music database (in particular the database\ndeveloped within the MiDiLiB -project [1]) and a content based navigation in digital music files.\nAlthough this includes structuring of musical information on a score-like level, we did not intend to\ndevelop a language with the capabilities of a complete music notation system. Instead, we focused on\na universal, easy to use and to process language for supporting the former applications.\nResearch Contributions\nIn developing MCML, the following research topics had to be addressed:\n• Which classes of musical contents and information are important for the above applications\nsuch as navigation and which are not?\n• How to describe the contents on which navigation will be performed?\n• How can a reasonable content based navigation in music files be realized?\n• Is it reasonable to use state-of-the-art content description interfaces such as XML to realize\nmusic contents description?\n• How can a linking concept improve navigation?\nIntroduction to MCML\nIn the following, we shall give a brief overview on MCML’s main concepts.\nConcepts\nMCML describes the contents of one or more digital instances of a particular piece of music. Those\ninstances may be given in several different file formats (e.g. WAV, MIDI, MP3 etc.). While not being\nan extension of one of those file formats, MCML provides meta-information related to both the\ncontents of the single music files as well as to relations between several of those files. As a data\nformat, MCML uses textual XML-files.\nBesides some global information on a particular piece of music, each MCML file contains several\n<PIECEOFMUSIC>-structures, each of which contains all of the information related to one particular\ninstance of this piece of music. E.g., there could exist one structure containing all information about a\nWAV-file of a live recording of  The Beatles Yesterday and another structure for a MIDI file of\nsomeone interpreting the same piece. As an important concept, MCML’s content descriptions are\ndirectly linked to the timeline of the underlying music file.\nThe various types of a piece’s content-related information (e.g. score, structure or mood) mostly\nhave very different structures and properties and thus require different description schemes.\nTherefore, we defined different content categories (realized by the so called contents tracks ). The\ncurrent version of MCML provides contents tracks for score, structure, mood, textual, and indexinformation. The set of content categories may however be extended according to application-\ndependent demands.\nAn important concept is the usage of links. MCML provides special structures to describe\nrelationships among different content elements. Such links may be relational , i.e., define groups of\nelements (undirected links) or functional , i.e., serve as pointers to related elements (e.g. variations,\nsimilar pieces). Links are an important tool for targeted browsing in music files.\nWhat MCML is not\nAs already mentioned above, MCML is not intended to be a complete and powerful notation and\nmusic description system. Within MCML, musical contents can only be described to a certain level of\ndetail. Although even more detailed notation structures may be described by extending MCML, there\nis a trade off between the detail of the described information and the complexity of the MCML data.\nWhy XML?\nXML, a recommendation of the W3 group for the description of structured information [2], provides\nfunctionality which does not perfectly but sufficiently meet the requirements induced by the above\nconcepts for a music contents description. (For example in modeling score based information, there is\nfrequently a need for grouping notes, e.g. to represent melodies, chords, or bars. As XML naturally\nonly supports tree-like grouping structures, it is not appropriate to allow for an elegant realization of\na general grouping concept.) XML is widely accepted and has recently been used with various\napplications. Moreover, it is platform independent and a large number of XML processing tools are\nalready available. Even more important, MPEG-7, a future standard for description of multimedia\ncontents [3], which is currently under development, will most likely be based on an XML- (or XML\nschema-) like description definition language (DDL).\nConclusions and Ongoing Work\nAt the point of writing, some preliminary experiments have been carried out using a prototypic\nMCML-based browsing system. A more elaborate version of the system is about to be finished within\nthe next months. Extensive tests including design and implementation of visualization and\nauralization methods will be important topics of our future work. Those tests will give deeper insights\nand lead to further improvements of MCML.\nAuthor Information\nJochen Schimmelpfennig and Frank Kurth\nDepartment of Computer Science\nUniversity of Bonn\nRömerstraße 164, D 53117 Bonn, Germany\n{schimmel,frank}@cs.uni-bonn\nSuggested Readings\n[1] MiDiLiB-Project, Department of Computer Science, University of Bonn, Germany,\nhttp://leon.cs.uni-bonn.de/forschungprojekte/midilib/english/\n[2] XML, Extensible Markup Language, World Wide Web Consortium,    http://www.w3.org/XML/    \n[3] MPEG7, Motion Pictures Experts Group,    http://www.darmstadt.gmd.de/mobile/MPEG7/"
    },
    {
        "title": "From Raw Polyphonic Audio to Locating Recurring Themes.",
        "author": [
            "Thomas von Schroeter",
            "Shyamala Doraisamy",
            "Stefan M. Rüger"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1416124",
        "url": "https://doi.org/10.5281/zenodo.1416124",
        "ee": "https://zenodo.org/records/1416124/files/SchroeterDR00.pdf",
        "abstract": "the automatic transcription of raw audio from a single polyphonic instrument with discrete pitch (eg piano) and the location of recurring themes from a Humdrum score. 1",
        "zenodo_id": 1416124,
        "dblp_key": "conf/ismir/SchroeterDR00",
        "keywords": [
            "automatic transcription",
            "raw audio",
            "polyphonic instrument",
            "discrete pitch",
            "Humdrum score",
            "reccurring themes",
            "theme detection",
            "instrumental music",
            "audio processing",
            "music analysis"
        ],
        "content": "From Raw Polyphonic Audio to Locating Recurring Themes\nThomas von Schroeter1, Shyamala Doraisamy2and Stefan M R¨ uger3\n1T H Huxley School of Environment, Earth Sciences and Engineering\nImperial College of Science, Technology and Medicine\nPrince Consort Road, London SW7 2BZ, England\nts9@ic.ac.uk\n2Department of Multimedia\nFaculty of Computer Science and Information Technology\nUniversity Putra Malaysia, 43400 UPM Serdang, Selangor D.E., Malaysia\nshyamala@fsktm.upm.edu.my\n3Department of Computing\nImperial College of Science, Technology and Medicine\n180 Queen’s Gate, London SW7 2BZ, England\ns.rueger@ic.ac.uk\nAbstract. We present research studies of two related strands in content-based music retrieval:\nthe automatic transcription of raw audio from a single polyphonic instrument with discrete pitch\n(eg piano) and the location of recurring themes from a Humdrum score.\n1 Introduction\nIn the age of digitalisation the production, recording and storage of music is easier than ever\nbefore. This calls for intelligent, content-based retrieval methods, and it would seem by the\nsheer volume of audio data that these methods need to be fully automated.\nDesigning and searching a truly musical database depends on 1) the chosen encoding of the\nmusical data and 2) the method for comparison of musical sequences. Three diﬀerent levels of\nencoding are generally considered: a) unstructured raw audio ﬁles based on digitised samples\nof sound waves, b) semi-structured such as MIDI (Selfridge-Field 1997) and c) one of many\nhighly structured formats such as Humdrum (Huron 1997), Plaine and Easie (Howard 1997),\nDARMS (Selfridge-Field 1997) etc. The former is just the way performances are stored, whereas\nthe latter contains musical features that describe music at a more appropriate level for content-\nbased retrieval. It seems desirable to have access to all levels of encoding for a music piece,\nso that a particular performance can be archived and played in raw audio, but retrieved using\nfeatures of a higher-level encoding.\nIt has been widely acknowledged that achieving automatic conversion between any of the\naforementioned levels is extremely diﬃcult if this is to pass the critical assessment of an ex-\nperienced musician (even the audio playback from MIDI is hard when real-instrument sound\nis required). However, for the purposes of music retrieval, simple musical representations have\nproven to be successful, eg the n-gram encoding of successive pitch intervals as text strings\nwhere each letter stands for an interval or interval class (Downie and Nelson 2000). Indeed,\n1other retrieval systems such as the New Zealand Digital library MELDEX system (McNab,\nSmith, Bainbridge and Witten 1997; Bainbridge 1998), the joint ThemeFinder project of Stan-\nford University and Ohio State University (Kornst¨ adt 1998), or various “query by humming”\napproaches (Ghias, Logan, Chamberlin and Smith 1995; Blackburn and DeRoure 1998) use\nsimple encoding schemes, eg as simple as a sequence of pitch directions (up, down, rest).\nAll these systems work more or less successfully on databases of folksongs or other mono-\nphonic music pieces, where a representation in terms of pitch and duration is relatively straight-\nforward. In contrast to this, most Western-style music is essentially polyphonic. Here, the\ntranscription from raw audio to a higher-level encoding is much more challenging. Section 2 sur-\nveys several approaches to the task of transcribing Western-style music that uses the diatonic\nscale and introduces some new ones. We do not address the issue of instrument identiﬁcation at\nall; instead we limit our analysis to a single polyphonic keyboard string instrument with discrete\npitch such as piano or harpsichord.\nThe second part of this article is concerned with comparison of musical sequences of poly-\nphonic music. Humans, normally, will ﬁnd it not diﬃcult recognizing similarities between slightly\nmodiﬁed or decorated musical sequences. However, one major computational diﬃculty lies in\nmodelling the human perception of musical similarity; this and related problems have been de-\nscribed in (Selfridge-Field 1998). One example for the importance of similarity matching is the\nidentiﬁcation of the recurrence of a theme in a given piece of musical score. The theme here\nrefers to the main melody or musical idea that forms the basis of the composition. Composers\nusually repeat this theme throughout the composition and upon repetition, this theme is usually\nmodiﬁed to add variety to their composition or is repeated using some method deﬁned by the\nform of the composition.\nIn Section 3, we discuss the problem of locating recurring themes in polyphonic music using\nthe Humdrum score format (Huron 1997) — thereby addressing the similarity problem in a way\nthat is more amenable to evaluation. Standard musical sequence matching algorithms use simple\npitch-and-duration-based distance measures to compute matches or similarities (for a review see\n(Crawford, Iliopoulos and Raman 1998)) and our algorithm, a modiﬁcation of (Mongeau and\nSankoﬀ 1990), is no exception. We used Bach’s Fugues for testing. The data set was encoded\nin Humdrum format where the various voices are clearly separated.\n2 Experiments on polyphonic music transcription\nWe consider transcription algorithms which convert raw audio into a list of fundamental fre-\nquencies over, and possibly varying in, time. We believe that for the purposes of retrieval this\nis suﬃcient to capture some essential (if crude) details of a performance, while avoiding the\nmore involved interpretation problems usually associated with transcription, such as approxi-\nmating the relative durations of neighbouring notes in terms of the fractions expressible by a\nconventional score, and introducing heuristics about how to group notes to parts.\nConceptually we divide the task into two subtasks: time-frequency spectral analysis and\nfundamental line extraction . Most research to date seems to have followed such a two-step\napproach, with some notable exceptions, for instance (Walmsley, Godsill and Rayner 1999).\n2.1 Time-frequency spectral analysis\nOf the many algorithms that have been used or proposed for time-frequency analysis of musical\nand speech signals, we implemented and tested the following three:\n2²short-time least mean squared (LMS) ﬁltering (Choi 1997) extended to a sum of sinusoids with\nexponentially spaced frequencies, based on singular value decomposition;\n²a decimated version of the constant-Q spectrogram due to Brown (Brown 1991; Brown and\nPuckette 1993); and\n²a decimated version of the Phase Vocoder (Flanagan and Golden 1966; Puckette and Brown\n1998).\nNone of them gave satisfactory results for polyphonic signals:\nTests of the LMS approach with synthesised signals consisting of no more than 2-3 sinusoids\nshowed signiﬁcant bias in the amplitude estimation when the actual frequencies did not exactly\ncoincide with grid frequencies.\nTheconstant-Q spectrogram and the Phase Vocoder were tested in detail with synthesised\nand acoustic piano signals with one and two parts. We were rarely able to see more than 2 or\n3 partials (see Fig. 1 and 2 (a)); higher partials were usually too weak to be detected against\nthe noise background. Moreover, the frequency resolution of spectrogram methods is limited\nto one sinusoid per channel. Thus spectral lines belonging to diﬀerent tones which happen to\nfall into the same channel cannot be resolved. The channels must therefore have passbands\nof at most a semitone in width, with transition bandwidths in the region of a quarter tone.\nIn order to achieve these ﬁlter speciﬁcations, very long ﬁlters are required, resulting in poor\ntime resolution. Furthermore, spectral leakage leads to substantial amounts of energy in bands\nneighbouring a strong component. This is not a problem for the detection of a single component\nsince the frequency estimates will be very close for both bands; however, for the same reason,\nPhase Vocoder estimates are subject to considerable bias when a neighbouring band contains\nenergy from a diﬀerent component. This eﬀect has been studied quantitatively in (Puckette and\nBrown 1998).\nThus we eventually decided to abandon Fourier methods altogether in favour of auto-\nregressive (AR) estimators. We believe that we are the ﬁrst to have applied AR methods to\nmusical signals. A number of estimation schemes based on auto-regressive models have been pub-\nlished; initially we implemented and tested four of them with synthesised signals (von Schroeter\n2000). Marple’s MODCOVAR algorithm (Marple 1987) turned out the most accurate.\nIts comparative advantage over Fourier methods is illustrated by the larger number of partials\nwhich it detects, typically 5 or 6 per note for the same signals in which Fourier and Phase Vocoder\nmethods detect only 3 or 4 in all (see Fig. 2). In high quality piano recordings recently made\navailable to us by Eric Scheirer at the MIT Media Lab, up to 14 partials are detected in a\nmonophonic passage! We measured their anharmonicity and found it in good agreement with\nFletcher’s model (1964).\nWe therefore used Marple’s algorithm as the basis for all further experiments. Its output\nis a list of poles in the complex plane for each frame. Poles are accepted or rejected according\nto their distance from the unit circle, which gives a measure of their relative weakness. The\nangle of the pole locations with the real axis gives the digital frequency !which can easily be\nconverted to pitch k(in semitones above some reference frequency !0) using the relation\n!=!0¢2k=12:\n30\n10\n20\n30\n40Channel\n01000020000\nTime\n0\n10\n20\n30\n40ChannelFigure 1: Constant-Q spectrogram of a musical signal (bars 2-3 of a recording of Bach’s Fugue in\nC Major from part 1 of the Well-Tempered Clavier, sampled at 5kHz and low-passed to 2.5kHz),\nobtained with a ﬁlterbank of Kaiser windows with transition bandwidth of 1/4 tone. The powers\n(vertical axis) are normalised according to ˆp=p=(1 +kpk), where pdenotes the spectrogram\npowers in each channel and kpktheir vector 1-norm). The reference frequency is 220 Hz for\nchannel 0.\n2.2 Fundamental line extraction\n2.2.1 Prior work\nIn contrast to music synthesis for creative purposes, the transcription problem seems to have\nreceived comparatively little attention, even though the ﬁrst attempts in this area date from the\nlate 1970’s (Moorer 1977). The methods which have so far been proposed essentially fall into\none or more of four categories which can be labelled as follows:\n²Correlation methods. These methods are motivated by the use of correlations to ﬁnd similarities\nbetween signals. Separately in each time frame, they compute either multiple autocorrelations\nof the spectrum (Tanguiane 1993), or convolutions of the spectrum with the spectral pattern of\na single tone (Brown 1992). In both cases, tone hypotheses will appear as peaks in the resulting\nsequences. Neither method seems to have been tested with polyphonic acoustic signals.\n²Tone data bases. This approach consists in the use of pre-recorded training data, reﬂecting\nthe individual acoustical properties of the instrument when playing single notes, to aid detec-\ntion of chords in the piece to be analysed (Rossi, Girolami and Leca 1997). The approach has\nbeen tested on acoustic piano signals with detection rates of 98% for scales and 92% for 4-part\npolyphony, albeit under somewhat idealized conditions.\n²Bayesian Networks (Walmsley, Godsill and Rayner 1999). This is the most recent and perhaps\nthe most principled approach to date as it makes explicit use of a mathematical tone model;\nhowever it also seems computationally very expensive. The interdependencies of parameters\nare modelled as a probabilistic network with a priori probabilities reﬂecting prior knowledge.\nParameters are then estimated using Markov chain Monte-Carlo methods.\n²Context enlargement. This approach complements the recorded and digitised signal by higher-\nlevel information in order to reduce the search space of tones compatible with the measured spec-\ntrum. Such additional information can be given in the form of AI-style rules, for instance rules\ngoverning the formation and rejection of note hypotheses based on signal shapes (Fernandez-Cid\n4100 200 300 400 500152025303540\n50 100 150 200152025303540Figure 2: (a) Left: Phase Vocoder time-pitch spectrum for the musical signal of Fig. 1 with 50\nsamples step size between frames. For each frame, dots indicate the precise pitch of components\nfound in the channels in Fig. 1 satisfying ˆpi>0:05. (b) Right: AR spectrum obtained from the\nsame signal by Marple’s algorithm using 40 poles, 250 samples per frame (without overlaps),\nand a pole acceptance width of 0.01 to both sides of the unit circle.\nand Casajus-Quiros 1998) or assumptions about a particular musical style, see (Tanguiane 1993)\nand references there.\n2.2.2 Towards a topological approach to transcription\nBased on what little prior expertise was available, and guided by experimental results, we de-\nveloped a suite of transcription algorithms, starting from a generalisation of the correlation\napproach, but ﬁnding ourselves naturally led to applying concepts with increasingly topological\ncontent. Here we sketch the beginning and the current stage of this development; typical results\nare shown in Fig. 3.\n(a) Correlation peaks with a tone pattern. This approach is a modiﬁcation of the one\ndue to Brown (Brown 1992) to allow for continuous analysis frequencies, where the tone pattern\nis realized as a list of intervals with equal width on the pitch scale. Instead of computing the\ncorrelation with the tone pattern, we simply count the number of components covered by it. For\neach frame, this is a step function of the pattern oﬀset. The algorithm extracts the mid point k\nof the ﬁrst pitch interval in which the highest value of this function occurs, and discards from\nthe frame all other pitches covered by the tone pattern centred at k, taking kas the fundamental\npitch of a note hypothesis. This process is repeated until the remaining spectrum is empty (or\nso sparse that it does not give rise to a further note hypothesis).\nExperimental results even with monophonic signals show that the convolution peaks found\nby this simple scheme are often below the spectrum, falsely suggesting a tone with missing\nfundamental. When we restricted the search of fundamental pitches to the range of pitches\noccurring in each frame (up to a tolerance), our algorithm detected about 90% of the notes in\na 2-part acoustic piano signal, but also some spurious components (see Fig. 3(a)).\n(b) Connectivity patterns in pitch and time. Closer scrutiny of Fig. 2 (b) reveals that\nin general the partials of a note have asynchronous onsets and vary in their decay time. Hence\na purely frame-based algorithm is likely to fail. This led us to model tones as two-dimensional\nsubsets of the time-pitch spectrum whose points are connected by two kinds of relations, namely\n550 100 150 200152025303540\n25 50 75100 125 150 175 200152025303540Figure 3: Transcription results for the spectrum shown in Fig. 2(b) using (a) the restricted\ncorrelation peak heuristic (left), and (b) connectivity patterns (right).\nconnectivity in time as continuation of a spectral line across neighbouring frames, and connec-\ntivity in pitch as a simultaneous pattern relation between points in the same frame. There is a\ntwo-level hierarchy of such combined time-pitch relations:\n(P) two points can belong to a single tone pattern;\n(T) they can belong to a single tone pattern with speciﬁed fundamental.\n(T) implies (P). Thus (P) can be used to break down the input spectrum into connected\ncomponents such that each tone pattern belongs to exactly one of these components; no prior\nknowledge of the fundamentals is necessary for this step. Within the (P) components we then list\nthe maximal (T) components based at each of the lines, where delays of the fundamentals within\nthe (T) components are tolerated up to a tunable threshold. This list is what we call a covering\ntable; the partial ordering of its entries by inclusion reﬂects a partial ordering of possible tone\nhypotheses by their “explanatory power”, although not necessarily in any probabilistic sense.\nIn each covering table we admit as chord hypotheses any minimal combination of its entries\nwhich covers the entire component. It can be shown that except in rather artiﬁcial circumstances,\nthese minimal covering sets are unique and identical with the set of maximal elements with\nrespect to inclusion, both up to multiply attributed lines. Thus we form this set, discard\nelements with too little, inharmonic, or disconnected “essential support” (deﬁned as the set of\npoints not shared with any other element of the covering table), and for all remaining components\nwe extrapolate the fundamental to all frames intersecting the essential support in which it is\nnot detected. The result is a point spectrum in which each point indicates an instantaneous\nfundamental pitch of a note hypothesis; thus the output format is richer in pitch details than\nthe ordinary MIDI format and would in principle also accommodate pitch-variable instruments.\nPreliminary experimental results for this method show a crucial dependence on the choices\nof parameters; with appropriate settings, detection was reliable for monophonic signals. For\npolyphonic signals with up to 6 visible partials per note, detection of correct notes is as good\nas with the convolution peak heuristic, but more of their decay phase is captured, and spurious\ncomponents can almost completely be eliminated. However, multiple detections still occur (i.e.\nnotes which are struck only once but detected more than once). See Fig. 3(b). Results for the\nhigh quality piano signals referred to earlier were badly aﬀected by anharmonicity in combination\nwith the implicit bias caused by alignment of tone patterns with the fundamental. Such signals\nwould seem to require a combination of connectivity and clustering methods; these will be the\n6object of further study.\n3 Locating recurrent themes\n3.1 Related work\nEarly work in the area of comparing two musical sequences includes a system by Dillon and\nHunter (1982) where the system was designed to identify variants of Anglo-American folk songs.\nFor every song, 5 variants were generated based on the initial phrase and each variant was\ndesigned to capture one aspect of the melody in a form suitable for variant matching operations.\nTherefore, given a melody, the type of variant being seeked is generated from the query tune\nand this is matched with the database of songs indexed by the incipit and its variants using\nBoolean matching techniques. However, the idea of stating the variant before it is sought seems\nto defeat the purpose of automatically identifying the recurring themes.\nAnother system described in (Blackburn and DeRoure 1998) compares two musical patterns\nbased on the contours, and one of their objectives is to retrieve songs through query by humming.\nThe song database is indexed by sub-contours (pitch directions). A particular song is encoded\nas a long sequence of pitch directions (up, down, rest). Sub-contours based on the key length\ndeﬁned are obtained iteratively as segments from the long sequence. To query, part of a song\nto be retrieved is sung and this sub-contour is used to search the database of sub-contours. A\nnear match set is obtained using a tree search. The concept of obtaining sub-contours can be\nused in breaking up a fugue into smaller sections. One problem is that the key-length has to be\nspeciﬁed.\n3.2 A basic comparison algorithm\nIn locating recurring themes, an algorithm is preferred which takes a more general approach\nwhere no speciﬁc type of modiﬁcation is emphasised. The algorithm of Mongeau and Sankoﬀ\n(1990) was chosen as a baseline and is detailed below.\nLeta= (a1; a2; : : : ; a A) be a sequence of a certain number Aof notes, each of which is\nencoded as a pair of pitch and duration and b= (b1; b2; : : : ; b B) be another sequence of Bnotes.\nWe compute the dissimilarity dA;Bof the two sequences aandbrecursively as follows:\nBoundary conditions\nd0;0= 0\ndi;0=di¡1;0+w(ai;0); i¸1\nd0;j=d0;j¡1+w(0; bj); j¸1\nGeneral step, i= 1; : : : ; A andj= 1; : : : ; B\ndi;j= min8\n>>>><\n>>>>:di¡1;j +w(ai;0) (deletion)\ndi¡1;j¡1+w(ai; bj) (replacement)\ndi;j¡1 +w(0; bi) (insertion)\ndi¡1;j¡k+w(ai; bj¡k+1; : : : ; b j);2·k·min(j; F) (fragmentation)\ndi¡k;j¡1+w(ai¡k+1; : : : ; a i; bj);2·k·min(i; C) (consolidation)\nThe underlying idea is the one of the edit distance, and dynamic programming is used to\nobtain the series of transformations with the minimum distance. w(ai; bj) is the distance score\nor weight associated with the ith note of sequence aand the jth note of sequence b. This score\nis a weighted sum\nw(ai; bj) =winterval (ai; bj) +k1wlength (ai; bj)\n7Figure 4: The eﬀect of diﬀerent pitch weights on the alignment of sequences.\nof pitch and duration scores, winterval (ai; bj) and wlength (ai; bj), the former being the weight\nassigned for a particular diﬀerence in pitch and the latter is the weight assigned for the diﬀerence\nin duration. The factor k1can be varied to reﬂect the relative contribution of pitch and duration.\nw(ai;0), the weight for deletion, is the length of the deleted note aitimes k1, since this can be\nviewed as a note aireplaced by a note of length zero. Here, the pitch weightings would be zero\nand the weight contribution would only be based on duration weightings. Similarly, w(0; bj) is\nthe length of the inserted note bjtimes k1.\nFor a fragmentation, winterval is the sum of the interval weights between each note fragment\nand the original, and wlength is the diﬀerence between the total length of the replacing notes and\nthe length of the replaced one; similarly in the case of consolidation. The constant Fcan be\nobtained by considering, where it would cost less to insert a number of terms than to fragment\nmore than Felements. Therefore, it is not necessary to consider fragmentations of aiinto more\nthan Felements (or, similarly, to consider consolidations of more than Celements into bj).\nParameter and weight values are discussed in detail in (Doraisamy 1995). It should be\nnoted that parameter and weight values aﬀect the optimal alignment of the sequences under the\nalgorithm. As an example, consider Fig. 4, where two sequences are compared which are a 4th\napart. Weight measures that are sensitive to musical diﬀerences and the consonance of intervals\nwere used in the left hand side comparison. However, diﬀerent and perhaps less intuitive weight\nvalues yield a more appropriate optimal alignment in the right hand side. This also illustrates\nthe diﬃculty this algorithm faces in dealing with two transposed melody lines.\nOur experiments with Mongeau and Sankoﬀ’s algorithm used the ﬁrst few notes of Bach’s\nFugue I of The Well-Tempered Clavier, Book I, with a number of variations such as key change,\nskipping notes, augmentation and diminution. We found good overall dissimilarity measure\nexcept for the following variations: 1) changing the rhythm of a melody line and 2) transposing\na melody line into a diﬀerent key. Both, unfortunately, are quite common variations employed\nby composers.\n83.3 Suggested enhancements\nIn order to be able to identify transposition, augmentation and diminution we suggest a change\nin input format which incorporates the generation of a melodic and rhythmic contour.\n3.3.1 Melodic contour\nFrom the experimental results, one limitation identiﬁed is that the algorithm requires the pitch\nto be encoded based on the distance from the tonic. This poses a problem when sequences\nare automatically extracted from a music score, especially where sequences are extracted from\nmodulated portions of the score. For such sequences, the pitch of that sequence is encoded based\non its new tonic. Thus, pitches in the sequence would be considered not to belong to the original\nscale and this would cause weights based on semitone diﬀerences to be used, which happen to\nbe much higher, resulting therefore in a high dissimilarity score!\nFor sequences that have been extracted from a modulated portion of the score pitches can\nbe re-encoded as distances from the new Tonic. This means that some preprocessing would\nbe required where one has to analyse the score where modulation had taken place accordingly.\nHowever, this defeats the purpose of automatically extracting sequences for comparison of a\nscore.\nIf the data was encoded as pitch oﬀset (the distance and direction each note moves from\nthe note that precedes it) instead of absolute pitches (the note itself), then the algorithm would\ncompare melodic contours (the patterns of the melody) instead.\n3.3.2 Rhythmic contour\nIn the case of changing the rhythm, ie the duration lengths of the notes, the dissimilarity\nmeasure turns out to be high. If the duration was encoded as rhythmic ratio, one would arrive\nat arhythmic contour which is invariant under the actual rhythmic value.\n3.4 Implementation of a theme locator system\nA system to locate a recurring theme was implemented in the following steps:\nExtraction. We extracted the pitch and duration values from the kern representation of Hum-\ndrum. The theme is extracted from this simpliﬁed sequence. For now, the theme is taken as\nthe ﬁrst subject of the fugue. This was taken to be the voice with the ﬁrst entry and ends when\nthe answer begins on any other voices. The next voice (column) that comes in with the melody\nnote is the ﬁrst voice to be extracted as a sequence for comparison. This process continues until\nwe obtain sequences for all the voices.\nContours. The absolute pitch and duration values are used to obtain rhythmic and melodic\ncontours which are used as input to the comparison algorithm.\nComparison. In extraction, the theme and the voices were separated. Each voice is now one\nlong sequence, and the theme is trying to be located in each of these long sequences. For\nparticular long sequences, shorter sequences are extracted for comparison to be made whether\nthat particular sequence extracted contains a recurrence of the theme.\nAnalysis. The obtained dissimilarities are compared against a threshold, and if below a certain\nthreshold, the theme is deemed to recur at this position in the music piece.\nThe system developed is able to detect themes varied with three common methods of modi-\nﬁcation which are transposition, augmentation/diminution and addition or skipping of notes.\n94 Conclusions and future work\nWe believe that our work has important implications for both spectral analysis and fundamental\ntracking as subtasks of polyphonic transcription. As for spectral analysis, we have shown that\nauto-regressive estimators are superior to spectrogram and Phase Vocoder methods in their\ncapacity to resolve a suﬃcient number of partials. With respect to fundamental tracking, we\nbelieve that a synthesis of topological and clustering concepts can lead to a better model of\na tone, lends itself to straightforward implementations in terms of standard graph searching\nalgorithms, and thus oﬀers considerable promise for more reliable note detection.\nAlthough the resulting output of such a polyphonic analysis is somewhat richer in format\nthan ordinary MIDI, it does not contain the details and the quality of a high-level encoding\nsuch as Humdrum. We are currently investigating how this gap can be closed with an automatic\nprocedure. One major challenge seems to be the separation of voices from the audio recording.\nOnce these challenges have been overcome, a traditional approach based on monophonic melody\ncomparisons as outlined in Section 3 could be used to locate recurring themes or, more generally,\nto compare musical sequences. One would hope that, for the purposes of music retrieval and\ntheme location, these challenges do not have to be mastered at a level to satisfy an experienced\nmusician.\nAcknowledgements: This work is partially supported by the EPSRC, UK.\nReferences\nBainbridge, D. (1998). Meldex: A web-based melodic index search service. Computing in\nMusicology 11 , 223–230.\nBlackburn, S. and D. DeRoure (1998). A tool for content-based navigation of music. In ACM\nMultimedia 98 - Electronic Proceedings .\nBrown, J. C. (1991). Calculation of a constant Qspectral transform. J. Acoust. Soc. Am. 89 ,\n425–434.\nBrown, J. C. (1992). Musical fundamental frequency tracking using a pattern recognition\nmethod. J. Acoust. Soc. Am. 92 , 1394–1402.\nBrown, J. C. and M. S. Puckette (1993). A high resolution fundamental frequency determi-\nnation based on phase changes of the Fourier transform. J. Acoust. Soc. Am. 94 , 662–667.\nChoi, A. (1997). Real-Time Fundamental Frequency Estimation by Least-Squares Fitting.\nIEEE Transactions on Speech and Audio Processing 5 , 201–205.\nCrawford, T., C. S. Iliopoulos and R. Raman (1998). String matching techniques for musical\nsimilarity and melodic recognition. Computing in Musicology 11 , 73–100.\nDillon, M. and M. Hunter (1982). Automated identiﬁcation of melodic variants in folk music.\nComputers and the Humanities 16 , 107–117.\nDoraisamy, S. (1995). Locating recurrent themes in musical sequences . MSc Thesis, University\nMalaysia Sarawak.\nDownie, S. and M. Nelson (2000). Evaluation of a simple and eﬀective music information\nretrieval method. In Proceedings of the 23rd International ACM SIGIR Conference .\nFernandez-Cid, P. and F. J. Casajus-Quiros (1998). Multi-pitch estimation for polyphonic\nmusical signals. In Proc. ICASSP , Volume 6, pp. 3565–3568.\n10Flanagan, J. L. and R. M. Golden (1966). Phase vocoder. Bell Syst. Tech. J. 45 , 1493–1509.\nFletcher, H. (1964). Normal Vibration Frequencies of a Stiﬀ Piano String. J. Acoust. Soc.\nAm. 36 , 203–209.\nGhias, A., J. Logan, D. Chamberlin and B. C. Smith (1995). Query by humming — musical\ninformation retrieval in an audio database. In ACM Multimedia 95 - Electronic Proceed-\nings.\nHoward, J. (1997). Plaine and Easie Code: a code for music bibliography. In (Selfridge-Field\n1997) .\nHuron, D. B. (1997). Humdrum and Kern: selective feature encoding. In (Selfridge-Field\n1997) , pp. 375–401.\nKornst¨ adt, A. (1998). Themeﬁnder: A web-based melodic search tool. Computing in Musi-\ncology 11 , 231–236.\nMarple, S. L. (1987). Digital spectral analysis with applications . Prentice-Hall (Englewood\nCliﬀs, New Jersey).\nMcNab, R. J., L. A. Smith, D. Bainbridge and I. H. Witten (1997). The New Zealand Digital\nLibrary Melody index. D-Lib Magazine .\nMongeau, M. and D. Sankoﬀ (1990). Comparison of musical sequences. Computers and the\nHumanities 24 , 161–175.\nMoorer, J. A. (1977). On the Transcription of Musical Sounds by Computer. Computer Music\nJournal , 32.\nPuckette, M. S. and J. C. Brown (1998). Accuracy of Frequency Estimates Using the Phase\nVocoder. IEEE Trans. Speech and Audio Processing 6 , 166–176.\nRossi, L., G. Girolami and M. Leca (1997). Identiﬁcation of polyphonic piano signals. Acus-\ntica 83 , 1077–1084.\nvon Schroeter, T. (2000). Auto-regressive spectral line analysis of piano tones. Technical\nreport.\nSelfridge-Field, E. (Ed) (1997). Beyond MIDI: the handbook of musical codes . MIT Press,\nCambridge, MA.\nSelfridge-Field, E. (1998). Conceptual and representational issues in melodic comparison.\nComputing in Musicology 11 , 3–64.\nTanguiane, A. (1993). Artiﬁcial perception and music recognition . Number 746 in Lecture\nnotes in artiﬁcial intelligence. Springer-Verlag, Berlin/London.\nWalmsley, P. J., S. J. Godsill and P. J. W. Rayner (1999). Polyphonic pitch tracking using joint\nBayesian estimation of multiple frame parameters. In IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics, New Paltz (NY), 17th-20th October .\n11"
    },
    {
        "title": "What Motivates a Musical Query?.",
        "author": [
            "Eleanor Selfridge-Field"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1415970",
        "url": "https://doi.org/10.5281/zenodo.1415970",
        "ee": "https://zenodo.org/records/1415970/files/Selfridge-Field00.pdf",
        "abstract": "[TODO] Add abstract here.",
        "zenodo_id": 1415970,
        "dblp_key": "conf/ismir/Selfridge-Field00",
        "content": "\u0001\u0002\u0003\u0004\u0005\u0006\u0007\u0004\b/G09\u0003\u0004/G0A\u000b\u0005\u0003\u0005\u0006\f\u000b\b/G0D\u0003\u000e\u0005\u000f\f/G0A\u0010\u0011\u0012\n\u0001\u0002\u0003\u0004\u0004\u0004\u0002\u0005\u0006\u0007\b/G09/G0A\u000b\u0002\f\u0007\u0006/G0D\u000b\u000e\u000f\u0010\u0007\u0011\u0012\u000e\u0007\u0006\u000f\n\u0013/G0A/G09\u0014\u0015\u0006\u0016\u000e/G09\u0010\u0002\u0017\u000b/G0A/G0D\u0007\u0014\u0014/G0A\u000b\u0002\u0018\u0019\u0015\u0014\u000e\u001a\u001b\u0013\u0007/G09\u0016\u0007\u000b\u0002/G0D/G0A\u000b\u0002\u0013/G0A\u001c\u001d\u0015\u0016\u0007\u000b\u0002\u001e\u0014\u0014\u000e\u0014\u0016\u0007\u000f\u0002\u001f\u0007\u0014\u0007\b\u000b\u001a \u0002\u000e/G09\u0002\u0016 \u0007\u0002!\u0015\u001c\b/G09\u000e\u0016\u000e\u0007\u0014\f\u0016\b/G09/G0D/G0A\u000b\u000f\u0002\"/G09\u000e#\u0007\u000b\u0014\u000e\u0016$\f\u0016\b/G09/G0D/G0A\u000b\u000f%\u0002\u0013\u001e\u0002&’(\u0004)\u0011(\u0004*+\n/G0A\u000b\u0013\u0014/G0D/G0D\u0010\u0015\u0003\u0016\u000b\u0004\u0003\u0017\u0013\u0007\u0010\u0018\u0016/G0A\u0018\f\n\u0002\u0004\u0004\u0019\u001a\u001b\u001b/G0D/G0D\u0010\u0015\u0003\u001c\u001d\u001d\u001d\u0016\u000b\u0004\u0003\u0017\u0013\u0007\u0010\u0018\u0016/G0A \u0018\f\u001b\u001e/G0A\u000b\u0013\n, \u0007\u000b\u0007\u0002\b\u000b\u0007\u0002\u0016 \u000b\u0007\u0007\u0002\u001d\u0006\b\u001a\u0007\u0014\u0002-\u0007\u0002\u001c\u000e\u0010 \u0016\u0002\u0006/G0A/G0A.\u0002\u0016/G0A\u0002\u0007\u0014\u0016\b/\u0006\u000e\u0014 \u0002\b\u0002\u000b\b/G09\u0010\u0007\u0002/G0A/G0D\u00020\u0015\u0007\u0014\u0016\u000e/G0A/G09\u0014\u0002\u0016 \b\u0016\u0002\u001c\u000e\u0010 \u0016\u0002\u001d\u000b/G0A/G0D\u000e\u0016\b/\u0006$\u0002/\u0007\u0002\b\u0014.\u0007\u000f\n/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0011\u000f\b\u0016\b\u0002\u0014\u0007\b\u000b\u001a \u0007\u00141\u0002\u0002\u0012\u000e\u000b\u0014\u0016%\u0002-\u0007\u0002\u001c\u000e\u0010 \u0016\u0002\u0006/G0A/G0A.\u0002\b\u0016\u0002 \u001d\u0002\u0007\u0002\u001c\u000e\u0010 \u0016\u0002-\b/G09\u0016\u0002\u0016/G0A\u0002\u0014\u0007\b\u000b\u001a \u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000f\b\u0016\b\u0002\b/G09\u000f\u0002- $1\u0002\n2\u00073\u0016%\u0002-\u0007\u0002\u001c\u000e\u0010 \u0016\u0002\u0006/G0A/G0A.\u0002\b\u0016\u0002 \u001d\u0002\u0003\u0004\u0002 \b\u0014\u0002\b\u0006\u000b\u0007\b\u000f$\u0002/\u0007\u0007/G09\u0002\b\u0016\u0016\u0007\u001c\u001d\u0016\u0007\u000f%\u0002-\u000e\u0016 \u0002\b\u0002#\u000e\u0007-\u0002\u0016/G0A\u0002\u0007\u0014\u0016\b/\u0006\u000e\u0014 \u000e/G09\u0010\u0002- \b\u0016\u0002 \b\u0014\n-/G0A\u000b.\u0007\u000f\u0002\u0018/G0A\u000b\u0002/G09/G0A\u0016\u001b\u0002\b/G09\u000f\u0002- $1\u0002\u0002, \u000e\u000b\u000f%\u0002-\u0007\u0002\u001c\u000e\u0010 \u0016\u0002\u0006/G0A/G0A.\u0002\b\u0016\u0002 \u0002\u0007\u001d\u0002\u0014\u0007\b\u000b\u001a \u0007\u0014\u0002 \b#\u0007\u0002/\u0007\u0007/G094\b\u000b\u0007\u0002/\u0007\u000e/G09\u0010\u0002\u001a\b\u000b\u000b\u000e\u0007\u000f\u0002/G0A\u0015\u0016%\n-\u000e\u0016 \u0002\b\u0002#\u000e\u0007-\u0002\u0016/G0A-\b\u000b\u000f\u0002\u0007\u0014\u0016\b/\u0006\u000e\u0014 \u000e/G09\u0010\u0002\u0016 \u0007\u0002\u001a/G0A/G09\u0014\u0016\u000b\b\u000e/G09\u0016\u0014\u0002\u0016 \b\u0016\u0002\u001c\b/G09\u0011\u001c\b\u001a \u000e/G09\u0007\u0002\u000b\u0007\u001a\u000e\u001d\u000b/G0A\u001a\u000e\u0016$\u0002\u000e\u001c\u001d/G0A\u0014\u0007\u0002/G0A/G09\u0002\u001c\u0015\u0014\u000e\u001a\b\u00060\u0015\u0007\u000b\u000e\u0007\u00141\u0002\u0002\u0002\n\u0001\u0002\u0007\u0005\u0003\u0017\u0018\u0005\u0001\u0002\u0003\u0004\u001a\u0005\u001f\u0007\u0015/G0A\u0005 \u0003\u0010\u000e\u0011\u0005\u000f\f/G0A\u0010\u0011\u0005!\u0010\u0007\"/G0A/G0D\u0004\u000b\n\u0005/G0D/G0D/G0A\u000b\u0016\u0014\u0002\u0016/G0A\u0002\u0014\u0007\b\u000b\u001a \u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000f\b\u0016\b\u0002\b/G09\u000f\u0002\u0014\u0007\u0006\u0007\u001a\u0016\u0002\u001d/G0A\u000b\u0016\u000e/G0A/G09\u0014\u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u001c\b\u0016\u0007\u000b\u000e\b\u0006\u0002\u0016 \b\u0016\u0002\u001a/G0A\u000b\u000b\u0007\u0014\u001d/G0A/G09\u000f\u0002\u0016/G0A\u0002\b\u0002\u0015\u0014\u0007\u000b\u0011\u000f\u0007/G0D\u000e/G09\u0007\u000f\u001c/G0A\u000f\u0007\u0006\u0002 \b#\u0007\u0002/\u0007\u0007/G09\u0002\u000f\u000e\u0014\u001a\u0015\u0014\u0014\u0007\u000f\u0002\u0014\u000e/G09\u001a\u0007\u0002\u0016 \u0007\u00025&+\u0004\u00141\u0002\u0002\u001e\u0016\u0002/G0D\u000e\u000b\u0014\u0016\u0002\u0016 \u0007\u0002\u000f\u000e\u0014\u001a\u0015\u0014\u0014\u000e/G0A/G09\u0002-\b\u0014\u0002\u0016 \u0007/G0A\u000b\u0007\u0016\u000e\u001a\b\u0006%\u0002/G0D/G0A\u000b\u0002\u0016 \u0007\u000b\u0007\u0002-\b\u0014\u0002/G09/G0A\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000f\b\u0016\b\u0002\u0016/G0A\u0002\u0014\u0007\b\u000b\u001a 1\u0002\u0002\u001e\u001d\b\u000b\u0016\u0002/G0D\u000b/G0A\u001c\u0002\u001a/G0A\u001c\u001d/G0A\u0014\u000e\u0016\u000e/G0A/G09%\u0002 /G0A-\u0007#\u0007\u000b%\u0002\u0016 \u0007\u0002\u001d\u000b/G0A\u000f\u0015\u001a\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002/G09/G0A\u0016\b\u0016\u0007\u000f\u0002\u0014\u001a/G0A\u000b\u0007\u0014\u0002-\b\u0014\u0002\u0016 \u0007\u001d\u000b\u000e\u001c\b\u000b$\u0002\u0007\u001c\u001d \b\u0014\u000e\u0014\u0002/G0A/G0D\u0002\u001a/G0A\u001c\u001d\u0015\u0016\u0007\u000b\u0002\b\u001d\u001d\u0006\u000e\u001a\b\u0016\u000e/G0A/G09\u0014\u0002\u000e/G09\u0002\u001c\u0015\u0014\u000e\u001a\u0002\u000e/G09\u0002\u0016 \u0007\u00025&+\u0004\u0014\u0002\b/G09\u000f\u00025&*\u0004\u00141\u0002\u0002\f\u000e/G09\u001a\u0007\u0002/G09\u0007\u0016-/G0A\u000b.\u000e/G09\u0010\u0002\b\u0014\u0002-\u0007./G09/G0A-\u0002\u000e\u0016\u0002\u0016/G0A\u000f\b$\u0002-\b\u0014\u0002/G09/G0A\u0016\u0002\u000f\u0007#\u0007\u0006/G0A\u001d\u0007\u000f%\u0002\u0007\b\u001a \u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u0006\b\u000b\u0010\u0007\u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u0014\u0002\u000f\u0007#\u0007\u0006/G0A\u001d\u0007\u000f\u0002\u000e\u0016\u0014\u0002/G0A-/G09\u0002\u001c\u0007\u0016 /G0A\u000f\u0002/G0D/G0A\u000b\u000b\u0007\u001d\u000b\u0007\u0014\u0007/G09\u0016\u000e/G09\u0010\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000f\b\u0016\b1\u0002\u0002\f/G0A\u001c\u0007\u0002\u0006\u0007\b\u000f\u000e/G09\u0010\u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u0014\u0002-\u0007\u000b\u0007\u0002/\b\u0014\u0007\u000f\u0002/G0A/G09\u00027\u001e\u001f\u0019\f\u0002\u0018\u0016 \u0007\u00027\u000e\u0010\u000e\u0016\b\u0006\u0002\u001e\u0006\u0016\u0007\u000b/G09\b\u0016\u0007\u001f\u0007\u001d\u000b\u0007\u0014\u0007/G09\u0016\b\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u0019\u0015\u0014\u000e\u001a8\u0002\u0013/G0A\u0006\u0015\u001c/\u000e\b\u0002\"/G09\u000e#\u0007\u000b\u0014\u000e\u0016$%\u0002/G0D\u000b/G0A\u001c\u00025&+)\u001b8\u0002\f\u00139\u001f\u0005\u0002\u0018\f\u0016\b/G09/G0D/G0A\u000b\u000f\u0002\b/G09\u000f\u0002\u0013/G0A\u0006\u0010\b\u0016\u0007\"/G09\u000e#\u0007\u000b\u0014\u000e\u0016\u000e\u0007\u0014%\u0002/G0D\u000b/G0A\u001c\u00025&*\u0003\u001b8\u0002\u0019\"\f,\u001f\u001e2\u0002\u0018:/G09\u000f\u000e\b/G09\b\u0002\"/G09\u000e#\u0007\u000b\u0014\u000e\u0016$%\u0002/G0D\u000b/G0A\u001c\u0002\u0016 \u0007\u0002\u0007\b\u000b\u0006$\u0002*\u0004\u0014\u001b%\u0002\b/G09\u000f\u0002\u0013\u0005\u001f;\u0002\u0018:\u0006\u0006\u000e/G09/G0A\u000e\u0014\"/G09\u000e#\u0007\u000b\u0014\u000e\u0016$%\u0002/G0D\u000b/G0A\u001c\u00025&*(\u001b1\u0002<\u0012/G0A\u000b\u0002\u000f\u0007\u0016\b\u000e\u0006\u0014\u0002/G0A/G0D\u0002\u0016 \u0007\u0014\u0007\u0002\u0014$\u0014\u0016\u0007\u001c\u0014\u0002\b/G09\u000f\u0002\u001c\b/G09$\u0002/G0A\u0016 \u0007\u000b\u0014%\u0002\u0014\u0007\u0007\u0002\f\u0007\u0006/G0D\u000b\u000e\u000f\u0010\u0007\u0011\u0012\u000e\u0007\u0006\u000f\u0002\u00185&&*\u001b1=2/G0A/G09\u0007\u0002/G0A/G0D\u0002\u0016 \u0007\u0014\u0007\u0002\u0014$\u0014\u0016\u0007\u001c\u0014\u0002-\b\u0014\u0002\u001d\u000b\u000e\u001c\b\u000b\u000e\u0006$\u0002/G0D/G0A\u001a\u0015\u0014\u0007\u000f\u0002/G0A/G09\u0002\u0016 \u0007\u0002\u0014\u0015\u001d\u001d/G0A\u000b\u0016\u0002/G0A/G0D\u0002\u000f\b\u0016\b\u00020\u0015\u0007\u000b$\u0002\b/G09\u000f\u0002\b/G09\b\u0006$\u0014\u000e\u00141\u0002\u0002\n\u001e/G09\u0002\u0007\b\u000b\u0006$\u0002\u0014$\u0014\u0016\u0007\u001c\u0002\u0016 \b\u0016\u0002\u000f\u000e\u000f\u0002 \b#\u0007\u0002\u0016 \u0007\u0002\u001d\u000b\u000e\u001c\b\u000b$\u0002\u0010/G0A\b\u0006\u0002/G0A/G0D\u0002/G0D\b\u001a\u000e\u0006\u000e\u0016\b\u0016\u000e/G09\u0010\u0002\u0016 \u0007\u00020\u0015\u0007\u000b$\u0002/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000f\b\u0016\b/\b\u0014\u0007\u0014\n/G0A\u000b\u000e\u0010\u000e/G09\b\u0016\u0007\u000f\u0002\b\u0016\u0002\u0017\u000b\u000e/G09\u001a\u0007\u0016/G0A/G09\u0002\"/G09\u000e#\u0007\u000b\u0014\u000e\u0016$\u0002\u000e/G09\u0002\u0016 \u0007\u0002\u001c\u000e\u000f\u0011\f\u000e3\u0016\u000e\u0007\u00141\u0002\u00027\u0007#\u0007\u0006/G0A\u001d\u0007\u000f\u0002/$\u0002\u0005\u000b\u000e\u001a\u0002\u001f\u0007\u0010\u0007/G09\u0007\u000b%\u0002\u0019\u000e\u001a \b\u0007\u0006\u0002>\b\u0014\u0014\u0006\u0007\u000b%\b/G09\u000f\u0002/G0A\u0016 \u0007\u000b\u0014%\u0002\u000e\u0016\u0002-\b\u0014\u0002/\b\u0014\u0007\u000f\u0002/G0A/G09\u0002\u0014/G0A\u001c\u0007\u0016 \u000e/G09\u0010\u0002\u001a\b\u0006\u0006\u0007\u000f\u0002:\u0019;4\u0019:\u001f\u0002<:/G09\u0016\u0007\u000b\u001c\u0007\u000f\u000e\b\u0016\u0007\u0002\u0019\u0015\u0014\u000e\u001a\b\u0006\u0002;\b/G09\u0010\u0015\b\u0010\u00074\u0019\u0015\u0014\u000e\u001a:/G09/G0D/G0A\u000b\u001c\b\u0016\u000e/G0A/G09\u0002\u001f\u0007\u0016\u000b\u000e\u0007#\b\u0006=1\u0002\u0002:/G09\u0002\u001a/G0A/G09\u0016\u000b\b\u0014\u0016\u0002\u0016/G0A\u0002\u0016 \u0007\u0002\u0007\u001c\u001d \b\u0014\u000e\u0014\u0002/G0A/G0D\u0002\u0007\b\u000b\u0006$\u00027\u001e\u001f\u0019\f\u0011/\b\u0014\u0007\u000f\u0002\b/G09\b\u0006$\u0014\u000e\u0014\u0002/G0A/G09\u0002\u001c/G0A/G09/G0A\u001d /G0A/G09\u000e\u001a\u001c\u0015\u0014\u000e\u001a\u0002\b/G09\u000f\u0002\u0014\u0007\u0016\u0011\u0016 \u0007/G0A\u000b\u0007\u0016\u000e\u001a\u0002\u001d\u000b/G0A\u001a\u0007\u000f\u0015\u000b\u0007\u0014%\u0002\u0016 \u0007\u0002\u0017\u000b\u000e/G09\u001a\u0007\u0016/G0A/G09\u0002-/G0A\u000b.\u0002-\b\u0014\u0002\u00073\u001a\u0006\u0015\u0014\u000e#\u0007\u0006$\u0002\u001a/G0A/G09\u001a\u0007\u000b/G09\u0007\u000f\u0002-\u000e\u0016 \u0002\u001d/G0A\u0006$\u001d /G0A/G09\u000e\u001a\u001c\u0015\u0014\u000e\u001a\u0002\u0018/$\u0002\u001f\u0007/G09\b\u000e\u0014\u0014\b/G09\u001a\u0007\u0002\u001a/G0A\u001c\u001d/G0A\u0014\u0007\u000b\u0014\u001b\u0002\b/G09\u000f\u0002\u0016 \u0007\u0002\u001c\u000e/G09\u0015\u0016\u000e\b\u0007\u0002/G0A/G0D\u0002\u000e\u0016\u0014\u0002/G09/G0A\u0016\b\u0016\u000e/G0A/G09%\u0002- \u000e\u001a \u0002\b\u000b\u0007\u0002\u0007\u0014\u0014\u0007/G09\u0016\u000e\b\u0006\u0002\u0016/G0A\u0002\b\u0014\u0014\u0007\u0014\u0014\u000e/G09\u0010//G0A\u0016 \u0002\u001d\u000b/G0A\u001d/G0A\u000b\u0016\u000e/G0A/G09\b\u0006\u0002\u000b $\u0016 \u001c\u000e\u001a\u0002#\b\u0006\u0015\u0007\u0014\u0002\b/G09\u000f\u0002\u001a/G0A/G09\u0016\u00073\u0016\u0015\b\u0006\u0002\b\u0014\u001d\u0007\u001a\u0016\u0014\u0002/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\u0002\u0016 \u0007/G0A\u000b$1\u0002\u0002\u0002\u0002\n, \u0007\u0002\u0017\u000b\u000e/G09\u001a\u0007\u0016/G0A/G09\u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u0002\u000e/G09\u000e\u0016\u000e\b\u0006\u0006$\u0002\u0007/G09\u001a/G0A\u000f\u0007\u000f\u0002\u001c\u0015\u001a \u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u001c\u0015\u0014\u000e\u001a\u0002/G0A/G0D\u0002?/G0A\u00140\u0015\u000e/G09\u0002\u000f\u0007\u0014\u0002\u0017\u000b\u0007@%\u0002\b\u0002\u001f\u0007/G09\b\u000e\u0014\u0014\b/G09\u001a\u0007\n\u001a/G0A\u001c\u001d/G0A\u0014\u0007\u000b\u0002/G09/G0A\u0016\u0007\u000f\u0002/G09/G0A\u0016\u0002/G0A/G09\u0006$\u0002/G0D/G0A\u000b\u0002\u0016 \u0007\u0002/G0A#\u0007\u000b\b\u0006\u0006\u0002\b\u001a\u001a/G0A\u001c\u001d\u0006\u000e\u0014 \u001c\u0007/G09\u0016\u0002\b/G09\u000f\u0002\b\u000b\u0016\u000e\u0014\u0016\u000b$\u0002/G0A/G0D\u0002\u0016 \u000e\u0014\u0002\u001c\u0015\u0014\u000e\u001a\u0002/\u0015\u0016\u0002\b\u0006\u0014/G0A\u0002\u000e/G09\u0016\u000b\u000e\u001a\b\u0016\u0007\b\u0006\u0006\u0015\u0014\u000e/G0A/G09\u0014\u0002-/G0A#\u0007/G09\u0002\u000e/G09\u0016/G0A\u0002\u0016 \u0007\u0002/G0D\b/\u000b\u000e\u001a\u0002/G0A/G0D\u0002 \u000e\u0014\u0002-/G0A\u000b.\u00141\u0002\u0002\u0019\b/G09$\u0002-/G0A\u000b.\u0014\u0002-\u0007\u000b\u0007\u0002\u0007/G09\u001a/G0A\u000f\u0007\u000f\u0002/G0D\u000b/G0A\u001c\u0002\u001c\u0015\u0006\u0016\u000e\u001d\u0006\u0007%\u0002\u000f\u000e#\u0007\u000b\u0010\u0007/G09\u0016\u0014/G0A\u0015\u000b\u001a\u0007\u00141\u0002\u0002A \b\u0016\u0002\u000f\u000e\u000f\u0002\u001d\u0007/G0A\u001d\u0006\u0007\u0002-\b/G09\u0016\u0002\u0016/G0A\u0002\u0006\u0007\b\u000b/G09\u0002\u0016 \b\u0016\u0002\u0016 \u0007$\u0002\u001a/G0A\u0015\u0006\u000f\u0002/G09/G0A\u0016\u0002\b\u0006\u000b\u0007\b\u000f$\u0002/G0D\u000e/G09\u000f\u0002/G0A\u0015\u0016B\u0002\u0002:\u0016\u0002-\b\u0014\u0002\u0007/G09#\u000e\u0014\u000e/G0A/G09\u0007\u000f\u0002\u0016 \b\u0016\u0016 \u0007\u0002\u0007/G09\u001a/G0A\u000f\u0007\u000f\u0002\u0014/G0A\u0015\u000b\u001a\u0007\u0014\u0002\u00185\u001b\u0002-/G0A\u0015\u0006\u000f\u0002\u0014\u0015\u001d\u001d/G0A\u000b\u0016\u0002\u0016 \u0007\u0002\u000b\u0007\u001a/G0A/G09\u0014\u0016\u000b\u0015\u001a\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u0016 \u0007\u0002/G0A\u000b\u000f\u0007\u000b\u0002\u000e/G09\u0002- \u000e\u001a \u0002\u001c\b/G09\u0015\u0014\u001a\u000b\u000e\u001d\u0016\u0002\u0014/G0A\u0015\u000b\u001a\u0007\u0014-\u0007\u000b\u0007\u0002\u001a\u000b\u0007\b\u0016\u0007\u000f8\u0002\u0018\u0003\u001b\u0002-/G0A\u0015\u0006\u000f\u0002\u0007/G09\b/\u0006\u0007\u0002\u001c\u0015\u0014\u000e\u001a\u0002\u0016 \u0007/G0A\u000b\u000e\u0014\u0016\u0014\u0002\u0016/G0A\u0002\u0007\u0014\u0016\b/\u0006\u000e\u0014 \u0002\u0016 \u0007\u0002\u000b\b/G09\u0010\u0007\u0002\b/G09\u000f\u0002\u000e/G09\u001a\u000e\u000f\u0007/G09\u001a\u0007\u0002/G0A/G0D\u0002\u000f\u000e/G0D/G0D\u0007\u000b\u0007/G09\u0016\u0002.\u000e/G09\u000f\u0014/G0A/G0D\u0002 \b\u000b\u001c/G0A/G09\u000e\u001a\u0002\u001d\u000b/G0A\u001a\u0007\u000f\u0015\u000b\u0007\u0014%\u0002\u0014\u0015\u001a \u0002\b\u0014\u0002\u0016 \u0007\u0002\u001d\u000b\u0007\u001d\b\u000b\b\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u0014\u0015\u0014\u001d\u0007/G09\u0014\u000e/G0A/G09\u0014\u0002\u000e/G09\u0002\u001a\b\u000f\u0007/G09\u001a\u0007\u00148\u0002\u0018(\u001b\u0002-/G0A\u0015\u0006\u000f\u0002\u000e\u001c\u001d\u000b/G0A#\u0007\u001c\u0007\u0016 /G0A\u000f\u0014\u0002/G0D/G0A\u000b\u0002\u0014\u0016\u0015\u000f$\u000e/G09\u0010\u0002\u0016 \u0007\u0002\u001a/G0A\u001c\u001d\u0006\u00073\u0002 \u000e\u0007\u000b\b\u000b\u001a \u000e\u0007\u0014\u0002/G0A/G0D\u0002\u000b $\u0016 \u001c\u000e\u001a\u0002\u000b\u0007\u0006\b\u0016\u000e/G0A/G09\u0014 \u000e\u001d\u0014\u0002- \u000e\u001a \u0002-\u0007\u000b\u0007\u0002\u0007/G09\b/\u0006\u0007\u000f\u0002/$\u001c\u0007/G09\u0014\u0015\u000b\b\u0006\u0002/G09/G0A\u0016\b\u0016\u000e/G0A/G09\u0002\u0018\b\u0002\u001c\u0015\u0006\u0016\u000e\u0011\u0006\u0007#\u0007\u0006\u0002\u0016\u000b\u0007\u0007\u0002\u0014\u0016\u000b\u0015\u001a\u0016\u0015\u000b\u0007\u0002/G0A/G0D\u0002\u0014/G0A\u001c\u0007\u0016\u000e\u001c\u0007\u0014\u0002\u0016-\u000e\u0014\u0016\u0007\u000f\u0002\u001d\b\u000e\u000b\u0014\u0002/G0A/G0D\u0002/\b\u0014\u0007\u0011\u0003\u0002\b/G09\u000f\u0002/\b\u0014\u0007\u0011(\u0010\u000b/G0A\u0015\u001d\u000e/G09\u0010\u0014\u001b1\u0002\u0002\u0012\u0007-\u0002/G0A/G0D\u0002\u0016 \u0007\u0014\u0007\u0002/G0A/6\u0007\u001a\u0016\u000e#\u0007\u0014\u0002-\u0007\u000b\u0007\u0002\u001c\u0007\u0016%\u0002/\u0015\u0016\u0002\u0016 \u0007\u0002-/G0A\u000b.\u0002\u001d\u000b/G0A\u001c\u001d\u0016\u0007\u000f\u0002\u0016 \u0007\u0002\u0007/G09\u001a/G0A\u000f\u000e/G09\u0010\u0002/G0A/G0D\u0002/G0A\u0016 \u0007\u000b\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014\u0002/G0A/G0D\u0002\u001f\u0007/G09\b\u000e\u0014\u0014\b/G09\u001a\u0007\u0002\u001c\u0015\u0014\u000e\u001a\u0002\b/G09\u000f\u0002 \b\u0014\u0002\u0015/G09\u000f/G0A\u0015/\u0016\u0007\u000f\u0006$\u0002\u001a/G0A/G09\u0016\u000b\u000e/\u0015\u0016\u0007\u000f\u0002\u0016/G0A\u0002\u0014/G0A\u001c\u0007\u0002\u000e\u001c\u001d/G0A\u000b\u0016\b/G09\u0016\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a \u0002/G0A/G09\u001c/G0A\u000f\b\u0006\u0002\b/G09\u000f\u0002\u0016/G0A/G09\b\u0006\u0002\u0016 \u0007/G0A\u000b$\u0002\u0018\u00071\u00101%\u0002\u0017/G0A-\u0007\u000b\u0014\u00025&C58\u0002\u0013/G0A\u0006\u0006\u000e/G09\u0014\u0002?\u0015\u000f\u000f\u00025&&\u0003\u001b1\u0002<:\u000b/G0A/G09\u000e\u001a\b\u0006\u0006$%\u0002\u0016 \u0007\u0014\u0007\u0002\u0006\b\u0016\u0007\u000b\u0002\u0014\u0016\u0015\u000f\u000e\u0007\u0014\u0002 \b#\u0007/\u0007\u0007/G09\u0002\u001a/G0A/G09\u000f\u0015\u001a\u0016\u0007\u000f\u0002\u0006\b\u000b\u0010\u0007\u0006$\u0002\b/G09\b\u0006/G0A\u0010\u000e\u001a\b\u0006\u0006$%\u0002-\u000e\u0016 \u0002\u001a/G0A\u001c\u001d\u0015\u0016\u0007\u000b\u0002\u000e/G09#/G0A\u0006#\u0007\u001c\u0007/G09\u0016\u0002\u001d\u000b\u000e/G09\u001a\u000e\u001d\b\u0006\u0006$\u0002/G0D/G0A\u000b\u0002\u0016 \u0007\u0002\u001c\b/G09\b\u0010\u0007\u001c\u0007/G09\u0016\u0002/G0A/G0D\n\u001c\u0007\u0016\b\u0011\u000f\b\u0016\b1\u0002\u0002\u0019\u0015\u001a \u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u0007/G09\u001a/G0A\u000f\u0007\u000f\u0002\u000f\b\u0016\b\u0002-\b\u0014\u0002\u000f\u000e\u0014\u001a\b\u000b\u000f\u0007\u000f\u0002\b/G0D\u0016\u0007\u000b\u0002\u001a\b\u000b\u000f\u0011\u000b\u0007\b\u000f\u0007\u000b\u0014\u0002 \b\u000f\u0002/\u0007\u001a/G0A\u001c\u0007\u0002\u001d\b\u0014\u0014D=1\n\u001e\u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u0002/G0A/G0D\u0002\u0010\u000b\u0007\b\u0016\u0007\u000b\u0002\b\u001a \u000e\u0007#\u0007\u001c\u0007/G09\u0016%\u0002\u000e/G09\u0002\u0016\u0007\u000b\u001c\u0014\u0002/G0A/G0D\u0002\u001d\u000b\u0007\u0014\u0007\u000b#\u0007\u000f\u0002\u0007/G09\u001a/G0A\u000f\u000e/G09\u0010\u0002\b/G09\u000f\u0002\u0015\u0014\u0007/G0D\u0015\u0006\u0002\b/G09\b\u0006$\u0014\u000e\u0014%\u0002\u000e\u0014\u0002/G0A/G09\u0007\u0002/\u0007\u0010\u0015/G09\u0002\u000e/G09\n5&C\u0003\u0002\b\u0016\u0002\u0016 \u0007\u0002E\u0007\u0014\b\u001c\u0016 /G0A\u001a \u0014\u001a \u0015\u0006\u0007\u0002/G0D\u0015\u0007\u000b\u0002\u0019\u0015\u0014\u000e.\u0002\b\u0016\u0002\u0016 \u0007\u0002\"/G09\u000e#\u0007\u000b\u0014\u000e\u0016$\u0002\u000e/G09\u0002\u0005\u0014\u0014\u0007/G09%\u0002E\u0007\u000b\u001c\b/G09$\u0002\u0018\u0014\u0007\u0007\u00027\b \u0006\u000e\u0010%\u00025&&*%\b/G09\u000f\u0002\u000e\u0016\u0014\u0002\u0006\u000e\u0014\u0016\u0002/G0A/G0D\u0002\u000b\u0007/G0D\u0007\u000b\u0007/G09\u001a\u0007\u0014\u001b1\u0002\u0002F\u0015\u000e\u0006\u000f\u000e/G09\u0010\u0002/G0A/G09\u0002\b\u0002\u0006/G0A/G09\u0010\u0011\u0014\u0016\b/G09\u000f\u000e/G09\u0010\u0002\u0016\u000b\b\u000f\u000e\u0016\u000e/G0A/G09\u0002\u000e/G09\u0002/G0D/G0A\u0006.\u0011\u001c\u0015\u0014\u000e\u001a\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a %\u0002\u0016 \u0007\u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u0006\u0007\b\u000f\u0007\u000b%\u0002!\u0007\u0006\u001c\u0015\u0016\u0002\f\u001a \b/G0D/G0D\u000b\b\u0016 %\u0002\b\u000e\u001c\u0007\u000f\u0002\u0016/G0A\u0002\u0007/G09\u001a/G0A\u000f\u0007%\u0002\u0014\u0016/G0A\u000b\u0007%\u0002\u000f\u000e\u0014\u001d\u0006\b$%\u0002\b/G09\u000f\u0002\u0014\u0007\b\u000b\u001a \u0002\u001c/G0A/G09/G0A\u001d /G0A/G09\u000e\u001a\u0002\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014\u0002/G0D\u000b/G0A\u001c\u000f\u000e#\u0007\u000b\u0014\u0007\u0002\u001d\b\u000b\u0016\u0014\u0002/G0A/G0D\u0002E\u0007\u000b\u001c\b/G09$\u0002\b/G09\u000f\u0002E\u0007\u000b\u001c\b/G09\u0011\u0014\u001d\u0007\b.\u000e/G09\u0010\u0002\u0006\b/G09\u000f\u00141\u0002\u0002, \u0007\u0002\u0015/G09\u000f\u0007\u000b\u0006$\u000e/G09\u0010\u0002\u001c\b\u0016\u0007\u000b\u000e\b\u0006\u0014\u0002\u0007/G09\u001a/G0A\u000f\u0007\u000f\u0002-\u0007\u000b\u0007#\b\u000b\u000e/G0A\u0015\u0014\u0006$\u0002\u000b\u0007\u001a/G0A\u000b\u000f\u000e/G09\u0010\u0014\u0002\b/G09\u000f\u0002\u0014\u001a/G0A\u000b\u0007\u00141\u0002\u0002F\u0007\u001a\b\u0015\u0014\u0007\u0002\u0016 \u0007\u0002\u001c\u0015\u0014\u000e\u001a\u0002-\b\u0014\u0002\u001c/G0A/G09/G0A\u001d /G0A/G09\u000e\u001a%\u0002\u000e\u0016\u0002-\b\u0014\u0002\u001d/G0A\u0014\u0014\u000e/\u0006\u0007\u0002\u0016/G0A\u0002\u0007/G09\u001a/G0A\u000f\u0007\u0002\b\u0010\u000b\u0007\b\u0016\u0002\u000f\u0007\b\u0006\u0002/G0A/G0D\u0002\u000e\u0016\u0002/G0D\b\u000e\u000b\u0006$\u00020\u0015\u000e\u001a.\u0006$%\u0002\b/G09\u000f\u0002\u0016 \u0007\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a \u00020\u0015\u000e\u001a.\u0006$\u0002\u001c/G0A#\u0007\u000f\u0002/G0A/G09\u0002\u0016/G0A\u0002\u0016 \u0007\u0002\u000f\u0007#\u0007\u0006/G0A\u001d\u001c\u0007/G09\u0016\u0002/G0A/G0D\u0002\b\u00020\u0015\u0007\u000b$\b\u001d\u001d\b\u000b\b\u0016\u0015\u00141\u0002\u0002, \u0007\u0002\u0010\u000b/G0A\u0015\u001d\u0002 \b\u000f\u0002\u0007/G09\u001a/G0A\u000f\u0007\u000f\u0002\u000b/G0A\u0015\u0010 \u0006$\u00025)%\u0004\u0004\u0004\u0002\u0014/G0A/G09\u0010\u0014\u0002/\u0007/G0D/G0A\u000b\u0007\u0002\u0017\u000b/G0A/G0D\u0007\u0014\u0014/G0A\u000b\u0002\f\u001a \b/G0D/G0D\u000b\b\u0016 G\u0014\u0002\u0015/G09\u0016\u000e\u001c\u0007\u0006$\u000f\u0007\b\u0016 \u0002\u000e/G09\u00025&&’1\u0002\u0002\n\f\u001a \b/G0D/G0D\u000b\b\u0016 G\u0014\u0002\u000f\b\u0016\b%\u0002- \u000e\u001a \u0002\u000e\u0014\u0002\u000f\u000e\u0014\u0016\u000b\u000e/\u0015\u0016\u0007\u000f\u0002\u0015/G09\u000f\u0007\u000b\u0002\u0006\u000e\u001a\u0007/G09\u0014\u0007%\u0002 \b\u0014\u0002\u001d\u000b/G0A#\u0007\u000f\u0002\u0016/G0A\u0002/\u0007\u0002\b\u0002\u0006\u0007\u0010\b\u001a$\u0002/G0A/G0D\u0002\u001a/G0A/G09\u0014\u000e\u000f\u0007\u000b\b/\u0006\u0007\n\u000e\u001c\u001d/G0A\u000b\u0016\b/G09\u001a\u0007\u0002\u0016/G0A\u0002\u0016 /G0A\u0014\u0007\u0002\u001a/G0A/G09\u001a\u0007\u000b/G09\u0007\u000f\u0002-\u000e\u0016 \u0002\u0016 \u0007\u0002\u000f\u0007#\u0007\u0006/G0A\u001d\u001c\u0007/G09\u0016\u0002/G0A/G0D\u00020\u0015\u0007\u000b$\u0002\u000b/G0A\u0015\u0016\u000e/G09\u0007\u00141\u0002\u0002:/G09\u0002/G0D\b\u001a\u0016%\u0002\u001a/G0A/G09\u0016\u000b\b\u0014\u0016\u0007\u000f\u0002-\u000e\u0016 \u0002\u0016 \u0007\u0017\u000b\u000e/G09\u001a\u0007\u0016/G0A/G09\u0002-/G0A\u000b.%\u0002\u0015\u0014\u0007\u0002/G0A/G0D\u0002 \u000e\u0014\u0002\u000f\b\u0016\b\u0002\b/G09\u000f\u0002\u001d\u000b/G0A\u0010\u000b\b\u001c\u001c\u000e/G09\u0010\u0002\u001a/G0A/G09\u001a\u0007\u001d\u0016\u0014\u0002 \b#\u0007\u0002\u000b\u0007#\u0007\b\u0006\u0007\u000f\u0002\b/G09\u0002\u000e\u001c\u001d/G0A\u000b\u0016\b/G09\u0016\u0002/G0D\b\u0006\u0006\b\u001a$\u0002\u000e/G09\u0002/G0A\u0015\u000b\u0016 \u000e/G09.\u000e/G09\u0010\u0002\b//G0A\u0015\u0016\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0011\u000f\b\u0016\b\u00110\u0015\u0007\u000b$1\u0002\u0002:\u0016\u0002\u0014\u0007\u0007\u001c\u0014\u0002\u000b\u0007\b\u0014/G0A/G09\b/\u0006\u0007\u0002\u0016/G0A\u0002\u0014\u0015\u001d\u001d/G0A\u0014\u0007\u0002\u0016 \b\u0016\u0002\b\u0006\u0006\u00020\u0015\u0007\u000b\u000e\u0007\u0014\u0002\u001d\u0007\u000b\u0016\u000e/G09\u0007/G09\u0016\u0002\u0016/G0A\u0014\u000e\u001c\u001d\u0006\u0007\u0002\u0018\u000e1\u00071%\u0002\u001c/G0A/G09/G0A\u001d /G0A/G09\u000e\u001a\u001b\u0002\u000f\b\u0016\b\u0002\u001a\b/G09\u0002/\u0007\u0002\u000f\u0007\u000b\u000e#\u0007\u000f\u0002 \u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\u0002/G0D\u000b/G0A\u001c\u0002\u0014/G0A/G0D\u0016-\b\u000b\u0007\u0002\u000f\u0007#\u0007\u0006/G0A\u001d\u0007\u000f\u0002/G0D/G0A\u000b\u0002\u0016 \u0007\u00020\u0015\u0007\u000b$\u0002/G0A/G0D\n\u001a/G0A\u001c\u001d\u0006\u00073\u0002\u000f\b\u0016\b1\u0002\u0002, \u0007\u0002\u001a\b\u0014\u0007\u0002\u0016\u0015\u000b/G09\u0014\u0002/G0A\u0015\u0016\u0002\u0016/G0A\u0002/\u0007\u0002/G0A\u0016 \u0007\u000b-\u000e\u0014\u0007\u0002/\u0007\u001a\b\u0015\u0014\u0007\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000f\b\u0016\b\u0002\u001a\b/G09\u0002/\u0007\u0002\u0014\u000e\u001c\u001d\u0006\u0007\u0002/G0A\u000b\u0002\u001a/G0A\u001c\u001d\u0006\u00073\u0002\u000e/G09\b/G09\u0002\u0015/G09\u0016/G0A\u0006\u000f\u0002/G09\u0015\u001c/\u0007\u000b\u0002/G0A/G0D\u0002-\b$\u00141\u0002\u00029/G09\u0007\u0002.\u000e/G09\u000f\u0002/G0A/G0D\u0002\u001a/G0A\u001c\u001d\u0006\u00073\u000e\u0016$\u0002\u000f/G0A\u0007\u0014\u0002/G09/G0A\u0016\u0002/G09\u0007\u001a\u0007\u0014\u0014\b\u000b\u000e\u0006$\u0002\u000e/G09\u001a/G0A\u000b\u001d/G0A\u000b\b\u0016\u0007\u0002\u0007#\u0007\u000b$\u001a/G0A/G09\u001a\u0007\u000e#\b/\u0006\u0007\u0002.\u000e/G09\u000f\u0002/G0A/G0D\u0002\u0014\u000e\u001c\u001d\u0006\u000e\u001a\u000e\u0016$1\u0002\u0002, \u0007\u0002\u0005\u0015\u000b/G0A\u001d\u0007\b/G09\u0002\u001a\u0015\u0006\u0016\u000e#\b\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u0007\u0006\b//G0A\u000b\b\u0016\u0007\u0002 \b\u000b\u001c/G0A/G09\u000e\u001a\u0002\u0016 \u000e/G09.\u000e/G09\u0010\u0002 \b\u0014\u001d\u0007\u000b \b\u001d\u0014\u0002\u000f\u000e\u001c\u000e/G09\u000e\u0014 \u0007\u000f\u0002\u000b\u0007\u0014\u001d\u0007\u001a\u0016\u0002/G0D/G0A\u000b\u0002\u0016 \u0007\u0002\u001c/G0A\u000f\b\u0006\u0002\b/G09\u000f\u0002\u000b $\u0016 \u001c\u000e\u001a\u0002\u001a/G0A\u001c\u001d\u0006\u00073\u000e\u0016\u000e\u0007\u0014\u0002/G0A/G0D\u0002/G0A\u0016 \u0007\u000b\u0002\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u00141\n:/G09\u0002A\u0007\u0014\u0016\u0007\u000b/G09\u0002\b\u000b\u0016\u0002\u001c\u0015\u0014\u000e\u001a%\u0002/G0D/G0A\u000b\u0002\u00073\b\u001c\u001d\u0006\u0007%\u0002-\u0007\u0002\u0006\u000e.\u0007\u0002\u0016/G0A\u0002\u001a\b\u0016\u0007\u0010/G0A\u000b\u000e@\u0007\u0002\u001d\u000e\u0007\u001a\u0007\u0014\u0002\b\u001a\u001a/G0A\u000b\u000f\u000e/G09\u0010\u0002\u0016/G0A\u0002\u0016 \u0007\u000e\u000b\u0002\u001c/G0A\u000f\u0007\u0002\u0018\u001c\b6/G0A\u000b%\n\u001c\u000e/G09/G0A\u000b\u001b\u0002/G0A\u000b\u0002.\u0007$\u0002\u0018\u001e11E\u001b1\u0002\u00029/G09\u0007\u0002/G0A/G0D\u0002\f\u001a \b/G0D/G0D\b\u0016 G\u0014\u00020\u0015\u0007\u000b\u000e\u0007\u0014\u0002\b\u0014\u0014\u0007\u0014\u0014\u0007\u0014\u0002\u0016 \u0007\u0002\u000b\b/G09\u0010\u0007\u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u001d\u000e\u0007\u001a\u00078\u0002\b/G09\u0002\b\u0014\u0014/G0A\u001a\u000e\b\u0016\u0007\u000f\u0002/G0A/G09\u0007\u0006\u000e\u0014\u0016\u0014\u0002\u0016 \u0007\u0002\u0016/G0A/G09\u0007\u0014\u0002\u001d\u000b\u0007\u0014\u0007/G09\u0016%\u0002-\u000e\u0016 /G0A\u0015\u0016\u0002/\u000e\b\u0014\u0002\u0016/G0A-\b\u000b\u000f\u0014\u0002\u001a\b\u0016\u0007\u0010/G0A\u000b\u000e@\b\u0016\u000e/G0A/G091\u0002\u0002:/G09\u0002\u0016 \u000e\u0014\u0002\u000e/G09\u0014\u0016\b/G09\u001a\u0007\u0002\u0018\u0012\u000e\u0010\u0015\u000b\u0007\u00025\u001b\u0002\u0016 \u0007\u0002\u000e/G09\u001a\u000e\u000f\u0007/G09\u001a\u0007/G0A/G0D\u0002\u0007\b\u001a \u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u0016/G0A/G09\u0007\u0014\u0002\u000e\u0014\u0002\b\u0006\u0014/G0A\u0002\u000e/G09\u000f\u000e\u001a\b\u0016\u0007\u000f1\u0002\u0002\n:/G09\u0002/G0D/G0A\u0006.\u0011\u0014/G0A/G09\u0010\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a %\u0002\b\u0002\u001d\b\u000b\u0016\u000e\u001a\u0015\u0006\b\u000b\u0002\u001d\u000e\u0016\u001a \u0011\u0006\u000e\u0014\u0016\u0002\u001c\b$\u0002/\u0007\u0002\u0015\u0014\u0007/G0D\u0015\u0006\u0002\u000e/G09\u0002\u000e\u000f\u0007/G09\u0016\u000e/G0D$\u000e/G09\u0010\u0002\u0016 \u0007\u0002\u000b\u0007\u0010\u000e/G0A/G09\u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u0014/G0A/G09\u0010G\u0014\n/G0A\u000b\u000e\u0010\u000e/G091\u0002\u001e/G09/G0A\u0016 \u0007\u000b\u0002\u000f\u0007\u001d\b\u000b\u0016\u0015\u000b\u0007\u0002/G0D\u000b/G0A\u001c\u0002\b\u000b\u0016\u0011\u001c\u0015\u0014\u000e\u001a\u0002H\u0006/G0A\u0010\u000e\u001aI\u0002\u000e\u0014\u0002\u0016 \u0007\u00020\u0015\u0007\u000b$\u0002- \u000e\u001a \u0002\u000b\u0007\u0016\u000b\u000e\u0007#\u0007\u0014\u0002/G0A/G09\u0006$\u0002\u0016 \u0007\u0002/G0D\u000e/G09\b\u0006\u0002\u0016/G0A/G09\u0007\u0014\u0002/G0A/G0D\u0007\b\u001a \u0002\u001d \u000b\b\u0014\u00071\u0002\u0002\n:/G09\u0002\u0012\u000e\u0010\u0015\u000b\u0007\u0002\u0003\b\u0002-\u0007\u0002\u0014\u0007\u0007\u0002/G0D\u000e\u000b\u0014\u0016\u0002\u0016 \u0007\u0002\u0006\b\u0014\u0016\u0002\u0016/G0A/G09\u0007\u0002/G0A/G0D\u0002\u0007\b\u001a \u0002\u001d \u000b\b\u0014\u0007\u0002/G0A/G0D\u0002\u0016 \u0007\u0002E\u0007\u000b\u001c\b/G09\u0002\b/G09\u0016 \u0007\u001c\u0002H7\u0007\u0015\u0016\u0014\u001a \u0006\b/G09\u000f\u0002\u0015\u0007/\u0007\u000b\n\u001e\u0006\u0006\u0007\u00141I\u0002\u0002\n, \u0007\u0002#\b\u0006\u0015\u0007\u0002/G0A/G0D\u0002\u0014\u0015\u001a \u0002\b\u0002\u0014\u001a \u0007\u001c\b\u0016\u000e\u001a\u0002#\u000e\u0007-\u0002\u0016/G0A\u0002/G0D/G0A\u0006.\u0011\u0014/G0A/G09\u0010\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a \u0007\u000b\u0014\u0002\u000e\u0014\u0002\u0007\b\u0014\u000e\u0006$\u0002\b\u001d\u001d\u000b\u0007\u001a\u000e\b\u0016\u0007\u000f\u0002- \u0007/G09\u0002\u0016 \u000e\u0014\u0002\u00073\b\u001c\u001d\u0006\u0007\n\u000e\u0014\u0002\u001a/G0A\u001c\u001d\b\u000b\u0007\u000f\u0002-\u000e\u0016 \u0002\u0016 \u0007\u0002H\u001c\u0007\u0006/G0A\u000f\u000e\u001a\u0002\u0014\u001d\u000e/G09\u0007I\u0002\u0014 /G0A-/G09\u0002\u000e/G09\u0002\u0012\u000e\u0010\u0015\u000b\u0007\u0002\u0003/1\u0002\u0002, \u000e\u0014\u0002\u0014\u001d\u000e/G09\u0007\u0002\u000e\u0014\u0002\b\u0002\u001c\u0007\u001a \b/G09\u000e\u001a\b\u0006\u0002\u000b\u0007\u000f\u0015\u001a\u0016\u000e/G0A/G09\u0002\b\u0016\u0016 \u0007\u0002 \b\u0006/G0D\u0011/G09/G0A\u0016\u0007\u0002\u0006\u0007#\u0007\u0006\u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u0007/G09\u0016\u000e\u000b\u0007\u0002\u0014/G0A/G09\u00101\u0002\u00027\u000e/G0D/G0D\u0007\u000b\u0007/G09\u001a\u0007\u0014\u0002/G0A/G0D\u0002\u000b $\u0016 \u001c\u0002\u0018\u000e/G09\u001a\u0006\u0015\u000f\u000e/G09\u0010\u0002\u001d\u000b\u0007\u0014\u0007/G09\u001a\u0007\u0002/G0A\u000b\u0002\b/\u0014\u0007/G09\u001a\u0007\u0002/G0A/G0D\u000b\u0007\u001d\u0007\b\u0016\u0007\u000f\u0002/G09/G0A\u0016\u0007\u0014%\u0002\u000b\u0007\u0014\u0016\u0014%\u0002\u0014$/G09\u001a/G0A\u001d\b\u0016\u000e/G0A/G09\u0014%\u0002\b/G09\u000f\u0002\u0014\u0015\u0014\u001d\u0007/G09\u0014\u000e/G0A/G09\u0014\u001b\u0002\u001a\b/G09\u0002$\u000e\u0007\u0006\u000f\u0002\b\u0002\u000f\u0007\u0014\u001a\u000b\u000e\u001d\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u0016 \u0007\u0002- /G0A\u0006\u0007\u0002- \u000e\u001a \u0002\u000e\u0014\u0002\n/G09/G0A\u0016\u0002\u0007/G09\u0016\u000e\u000b\u0007\u0006$\u0002H\u0016\u000b\u0015\u0007I\u0002\u0016/G0A\u0002/G0A\u0015\u000b\u0002\u001a/G0A\u0010/G09\u000e\u0016\u000e#\u0007\u0002\u000e\u001c\u001d\u000b\u0007\u0014\u0014\u000e/G0A/G091\u0002\u0002:/G09\u0002/G0D/G0A\u0006.\u0011\u0014/G0A/G09\u0010\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a \u0002\u0014/G0A\u000b\u0016\u000e/G09\u0010\u0002/$\u0002\u001d \u000b\b\u0014\u0007\u0011/G0D\u000e/G09\b\u0006\u0014\u0002 \b\u0014/\u0007\u0007/G09\u0002\u0014 /G0A-/G09\u0002\u0016/G0A\u0002\u001a\u0015\u0006\u0006\u0002\u001c/G0A\u000b\u0007\u0002\u000b\u0007\u001a/G0A\u0010/G09\u000e@\b/\u0006\u0007\u0002\u001a\u0006\u0015\u0014\u0016\u0007\u000b\u0014\u0002/G0A/G0D\u0002\u000b\u0007\u0006\b\u0016\u0007\u000f\u0002\u0014/G0A/G09\u0010\u0014\u0002\u0016 \b/G09\u0002\u0014/G0A\u000b\u0016\u000e/G09\u0010\u0002/$\u0002\u000e/G09\u000e\u0016\u000e\b\u0006\u0002\u001d \u000b\b\u0014\u0007\u0002\u001a/G0A/G09\u0016\u0007/G09\u00161\n#\u0007\u001d\u0012\u0005\u001f\u0007\u0015/G0A\u0005\u000f\f/G0A\u0010\u0011\u0005$\u0007\u0017\u000b\b\u0018/G0A\u0010\u0003\u0004\b\u0007\u0017\u000b\u0005\b\u0017\u0005$\f\u0010\u0010/G0A\u0017\u0004\u0005!\u0010\u0007\"/G0A/G0D\u0004\u000b\n, \u0007\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a \u0007\u000b\u0002\u000e/G09\u0016\u0007/G09\u0016\u0002/G0A/G09\u0002\u000f\u0007#\u000e\u0014\u000e/G09\u0010\u0002\u0014\u0016\u000b\b\u0016\u0007\u0010\u000e\u0007\u0014\u0002/G0D/G0A\u000b\u0002/G0D\b\u001a\u000e\u0006\u000e\u0016\b\u0016\u000e/G09\u0010\u0002\u001c\u0015\u0014\u000e\u001a\u00020\u0015\u0007\u000b$\u0002/G0D\b\u001a\u0007\u0014\u0002\b\u0002/G0D/G0A\u000b\u001c\u000e\u000f\b/\u0006\u0007\u0002\u000b\b/G09\u0010\u0007\u0002/G0A/G0D/G0A/\u0014\u0016\b\u001a\u0006\u0007\u00141\u0002\u0002\u001e\u0002/G0D\u0015/G09\u000f\b\u001c\u0007/G09\u0016\b\u0006\u0002/G0A/G09\u0007\u0002\u000e\u0014\u0002\u0016 \b\u0016\u0002\u001c\u0015\u0014\u000e\u001a\u00110\u0015\u0007\u000b$\u0002\u0016\u0007\u001a /G09\u000e0\u0015\u0007\u0014\u0002\b\u000b\u0007\u0002/G09/G0A\u0016\u0002\u0014\u000e\u001c\u001d\u0006$\u0002\u0014\u0016\u000b\b\u000e\u0010 \u0016/G0D/G0A\u000b-\b\u000b\u000f\u00073\u0016\u000b\b\u001d/G0A\u0006\b\u0016\u000e/G0A/G09\u0014\u0002/G0A/G0D\u0002\u0016\u00073\u0016\u00110\u0015\u0007\u000b$\u0002\u0016\u0007\u001a /G09\u000e0\u0015\u0007\u00141\u0002\u0002\u001e/G09/G0A\u0016 \u0007\u000b\u0002\u000e\u0014\u0002\u0016 \b\u0016\u0002\u0016 \u0007\u0002\u0016\u0007\u000b\u001c\u0002H\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000f\b\u0016\bI\u0002\u000e\u0014\u0002 \u000e\u0010 \u0006$\b\u001c/\u000e\u0010\u0015/G0A\u0015\u0014J\u0002\u0016 \u0007\u0002\u000e/G09/G0D/G0A\u000b\u001c\b\u0016\u000e/G0A/G09\u0002\u001a/G0A/G09\u0016\u0007/G09\u0016\u0002/G0D/G0A\u000b\u0002\b/G09$\u0002\u0010\u000e#\u0007/G09\u0002-/G0A\u000b.\u0002\u001a\b/G09\u0002/\u0007\u0002\u0014\u0007\u0006\u0007\u001a\u0016\u000e#\u0007\u0006$\u0002\u0007/G09\u001a/G0A\u000f\u0007\u000f\u0002\u000e/G09\u0002\b\u0002 /G0A\u0014\u0016\u0002/G0A/G0D\u0002-\b$\u00141\u0002\u001e\u0002\u0016 \u000e\u000b\u000f\u0002\u000e\u0014\u0002\u0016 \b\u0016\u0002\u000b\u0007\u001a\u0007/G09\u0016\u0002/G0D\u000e/G09\u000f\u000e/G09\u0010\u0014\u0002\u000e/G09\u0002 \u0015\u001c\b/G09\u0011/G0D\b\u001a\u0016/G0A\u000b\u0014\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a \u0002\u0014\u0015\u0010\u0010\u0007\u0014\u0016\u0014\u0002\u0016 \b\u0016\u0002\u0014/G0A\u001c\u0007\u0002/G0A/G0D\u0002\u0016 \u0007\u0002/G0D\u0007\b\u0016\u0015\u000b\u0007\u0014\u0002\u0016 \b\u0016\u001c\b\u0016\u0016\u0007\u000b\u0002\u001c/G0A\u0014\u0016\u0002\u0016/G0A\u0002\u0006\u000e\u0014\u0016\u0007/G09\u0007\u000b\u0014\u0002\u001d\u0007\u000b\u0016\b\u000e/G09\u0002\u0016/G0A\u0002\b\u0002\u001d\b\u000b\u0016\u000e\u001a\u0015\u0006\b\u000b\u0002\u001d\u0007\u000b/G0D/G0A\u000b\u001c\b/G09\u001a\u0007\u0002/G0A/G0D\u0002\b\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002-/G0A\u000b.\u0002\u000b\b\u0016 \u0007\u000b\u0002\u0016 \b/G09\u0002\u0016/G0A\u0002\u000e/G09 \u0007\u000b\u0007/G09\u0016/G0D\u0007\b\u0016\u0015\u000b\u0007\u0014\u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u001a/G0A\u001c\u001d/G0A\u0014\u000e\u0016\u000e/G0A/G091\u0002\n51\u0002\u0019\u0015\u0014\u000e\u001a\u0011K\u0015\u0007\u000b$\u0002#\u00141\u0002,\u00073\u0016\u0011K\u0015\u0007\u000b$\n, \u0007\u0002\u000f\u000e#\u0007\u000b\u0014\u000e\u0016$\u0002/G0A/G0D\u0002\u0010/G0A\b\u0006\u0014\u0002- \u000e\u001a \u0002 \b\u0014\u0002\b\u0006-\b$\u0014\u0002\u001a \b\u000b\b\u001a\u0016\u0007\u000b\u000e@\u0007\u000f\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u00020\u0015\u0007\u000b$\u0002\u000e\u0014\u0002\u001a/G0A\u001c\u001d/G0A\u0015/G09\u000f\u0007\u000f\u0002/$\u0002\b/G09\b\u0014\u0014\u0015\u001c\u001d\u0016\u000e/G0A/G09\u0002\u0016 \b\u0016\u0002\b\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u00020\u0015\u0007\u000b$\u0002\u000e\u0014\u0002\u0007\u0014\u0014\u0007/G09\u0016\u000e\b\u0006\u0006$\u0002\b\u0014\u0002 /G0D\u0007\u0017/G0D\b\u000b/G0A%\u0005\u000b\u0019/G0A/G0D\b\u0013\b/G0D%\u0005\f\u0017\b\u0018\b\u0015/G0A\u0017\u000b\b\u0007\u0017\u0003\u000e%\u0005\u0003\u0017\u0018\n\f\u0017\b\u0018\b\u0010/G0A/G0D\u0004\b\u0007\u0017\u0003\u000e \u0002\b\u0014\u0002\b\u0002\u0016\u00073\u0016\u0002\u0014\u0007\b\u000b\u001a 1\u0002\u0002:/G09\u0002\b\u000f\u000f\u000e\u0016\u000e/G0A/G09%\u0002\u001d/G0A\u0016\u0007/G09\u0016\u000e\b\u0006\u0002\u001c\b\u0016\u001a \u0007\u0014\u0002\b\u000b\u0007\u0002\u0016\b\u001a\u000e\u0016\u0006$\u0002\b\u0014\u0014\u0015\u001c\u0007\u000f\u0002\u0016/G0A\u0002 \b#\u0007\u0002\b\u0002\u0006\u0007/G09\u0010\u0016 \n\u001a/G0A\u000b\u000b\u0007\u0014\u001d/G0A/G09\u000f\u000e/G09\u0010\u0002\u0016/G0A\u0002\u0016 \b\u0016\u0002/G0A/G0D\u0002\u0016 \u0007\u0002/G0A\u000b\u000e\u0010\u000e/G09\b\u0006\u00020\u0015\u0007\u000b$1\u0002\u0002, \u0007\u0014\u0007\u0002\b\u000b\u0007\u0002\u001d/G0A\u0016\u0007/G09\u0016\u000e\b\u0006\u0006$\u0002\u001d\b\u000b\b\u0006$@\u000e/G09\u0010\u0002\u00073\u001d\u0007\u001a\u0016\b\u0016\u000e/G0A/G09\u00141\u0002, \u00070\u0015\u0007\u0014\u0016\u000e/G0A/G09\u0014\u0002\u0016 \b\u0016\u0002\u001a\b/G09\u0002/\u0007\u0002\b/G09\u0014-\u0007\u000b\u0007\u000f\u0002/$\u0002\u001c\u0007\b/G09\u0014\u0002\b\u0014\u0002\u0014\u000e\u001c\u001d\u0006\u0007\u0002\b\u0014\u0002\u0016 /G0A\u0014\u0007\u0002\u0015\u0014\u0007\u000f\u0002\u000e/G09\u0002\u0016\u00073\u0016\u0002\u0014\u0007\b\u000b\u001a \u000e/G09\u0010\u0002\b\u000b\u0007\u0002\u0010\u0007/G09\u0007\u000b\b\u0006\u0006$0\u0015\u0007\u0014\u0016\u000e/G0A/G09\u0014\u0002\u0016/G0A\u0002- \u000e\u001a \u0002\u0014\u001a /G0A\u0006\b\u000b\u0014\u0002\b/G09\u000f\u0002\u001d\u0007\u000b/G0D/G0A\u000b\u001c\u0007\u000b\u0014\u0002/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\u0002\b\u0016\u0016\b\u001a \u0002\u0006\u000e\u0016\u0016\u0006\u0007\u0002#\b\u0006\u0015\u00071\u0002\nA\u0007\u0002 \b#\u0007\u0002\u001a/G0A\u001c\u0007\u0002\u0016/G0A\u0002\u000b\u0007\u001a/G0A\u0010/G09\u000e@\u0007\u0002\u0016 \u0007\u0014\u0007\u0002/G0A/\u0014\u0016\b\u001a\u0006\u0007\u0014\u0002\u0006\b\u000b\u0010\u0007\u0006$\u0002\u0016 \u000b/G0A\u0015\u0010 \u0002/G0A\u0015\u000b\u0002\u00073\u001d\u0007\u000b\u000e\u0007/G09\u001a\u0007\u0002\u000e/G09\u0002\u0016-/G0A\u0002\u0006/G0A/G09\u0010\u0011\u0016\u0007\u000b\u001c\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a \n\u001d\u000b/G0A6\u0007\u001a\u0016\u0014\u0002\b\u0016\u0002\u0016 \u0007\u0002\u0013\u0007/G09\u0016\u0007\u000b\u0002/G0D/G0A\u000b\u0002\u0013/G0A\u001c\u001d\u0015\u0016\u0007\u000b\u0002\u001e\u0014\u0014\u000e\u0014\u0016\u0007\u000f\u0002\u001f\u0007\u0014\u0007\b\u000b\u001a \u0002\u000e/G09\u0002\u0016 \u0007\u0002!\u0015\u001c\b/G09\u000e\u0016\u000e\u0007\u0014\u0002\b\u0016\u0002\f\u0016\b/G09/G0D/G0A\u000b\u000f\u0002\"/G09\u000e#\u0007\u000b\u0014\u000e\u0016$1\u0002\u00185\u001b, \u0007\u0002 /G09/G0A\u000b\u0002\f\u0005\u0007\u0005\nL\u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u0002\u000e\u0014\u0002\u001a/G0A/G09\u001a\u0007\u000b/G09\u0007\u000f\u0002-\u000e\u0016 \u0002\u0016 \u0007\u0002\u000f\u0007#\u0007\u0006/G0A\u001d\u001c\u0007/G09\u0016\u0002/G0A/G0D\u0002\b\u0002\u001a/G0A\u0006\u0006\u0007\u001a\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002/G0D\u0015\u0006\u0006$\u0002\u0007/G09\u001a/G0A\u000f\u0007\u000f\u0002\u0007\u0006\u0007\u001a\u0016\u000b/G0A/G09\u000e\u001a\n\u0014\u001a/G0A\u000b\u0007\u00141\u0002\u0002;\u000e.\u0007\u0002\u0016 \u0007\u0002\u0017\u000b\u000e/G09\u001a\u0007\u0016/G0A/G09\u0002-/G0A\u000b.%\u0002\u000e\u0016\u0002\u001a/G0A/G09\u0016\b\u000e/G09\u0014\u0002/G0A/G09\u0006$\u0002\u001d/G0A\u0006$\u001d /G0A/G09\u000e\u001a\u0002\u000f\b\u0016\b\u0002\b/G09\u000f\u0002\u000e\u0014\u0002\u000f\u0007\u0014\u000e\u0010/G09\u0007\u000f\u0002/G0D/G0A\u000b\u0002\u001c\u0015\u0006\u0016\u000e\u001d\u0006\u0007\u0002.\u000e/G09\u000f\u0014/G0A/G0D\u0002\b\u001d\u001d\u0006\u000e\u001a\b\u0016\u000e/G0A/G09\u0014\u0002\u0018\u0014\u001a/G0A\u000b\u0007\u0002\u0007\u000f\u000e\u0016\u000e/G09\u0010\u0002\b/G09\u000f\u0002\u001d\u000b\u000e/G09\u0016\u000e/G09\u0010%\u00020\u0015\u0007\u000b$\u0002\b/G09\u000f\u0002\b/G09\b\u0006$\u0014\u000e\u0014%\u0002\u0007\u0016\u001a1\u001b1\u0002\u0018\u0003\u001b\u0002, \u0007\u0002 /G0D\u000e\u0002\u000f\u0002\u0004\u0010\u0011\u0001\u0002\u0012\nL\u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u0002\u000e\u0014\n\u001a/G0A/G09\u001a\u0007\u000b/G09\u0007\u000f\u0002-\u000e\u0016 \u0002\u000f\b\u0016\b\u0002\u000f\u0007#\u0007\u0006/G0A\u001d\u001c\u0007/G09\u0016\u0002\b/G09\u000f\u00020\u0015\u0007\u000b$\u0002\u0014/G0A/G0D\u0016-\b\u000b\u0007\u0002/G0D/G0A\u000b\u0002\b\u0002\u001c\u0015\u0006\u0016\u000e/G0D\b\u001a\u0007\u0016\u0007\u000f\u0002\u001c\u0007\u0006/G0A\u000f\u000e\u001a\u0011\u0014\u0007\b\u000b\u001a \u0002\u0016/G0A/G0A\u00061\u0002\u0002;\u000e.\u0007\u0016 \u0007\u0002\u0005\u0014\u0014\u0007/G09\u0002-/G0A\u000b.\u0002\u0018\b/G09\u000f\u0002\u000e/G09\u0002/G0D\b\u001a\u0016\u0002\u000e/G09\u001a/G0A\u000b\u001d/G0A\u000b\b\u0016\u000e/G09\u0010\u0002\u000f\b\u0016\b\u0002/G0D\u000b/G0A\u001c\u0002\u0016 \u0007\u0002\u0005\u0014\u0014\u0007/G09\u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u001b%\u0002\u000e\u0016\u0002\u001a/G0A/G09\u0016\b\u000e/G09\u0014\u0002/G0A/G09\u0006$\u0002\u001c/G0A/G09/G0A\u001d /G0A/G09\u000e\u001a\u001c\u0015\u0014\u000e\u001a\u0002\b/G09\u000f\u0002\u000e\u0014\u0002\u000f\u0007\u0014\u000e\u0010/G09\u0007\u000f\u0002/G0D/G0A\u000b\u0002\b\u0002\u0014\u000e/G09\u0010\u0006\u0007\u0002\b\u001d\u001d\u0006\u000e\u001a\b\u0016\u000e/G0A/G09M\u0016 \u0007\u001c\b\u0016\u000e\u001a\u0002\u0014\u0007\b\u000b\u001a \u000e/G09\u00101\u0002\u0002 /G09/G0A\u000b\u0002\f\u0005\u0007\u0005\nL\u0002/G09/G0A-\u0002\u001a/G0A/G09\u0016\b\u000e/G09\u0014\u0002\u001c/G0A\u000b\u0007\n\u0016 \b/G09\u00025%\u0004\u0004\u0004\u0002\u0014\u001a/G0A\u000b\u0007\u0014\u0002/G0A/G0D\u0002\u001a\u0006\b\u0014\u0014\u000e\u001a\b\u0006\u0002\u001c\u0015\u0014\u000e\u001a1\u0002\u0002, \u0007\u0014\u0007\u0002\b\u000b\u0007\u0002\u001d\u000b\u0007\u0014\u0007\u000b#\u0007\u000f\u0002\u000e/G09\u0002\b\u0002\u000b\b/G09\u0010\u0007\u0002/G0A/G0D\u0002/G0D/G0A\u000b\u001c\b\u0016\u0014\u0002\u0014\u0015\u000e\u0016\u0007\u000f\u0002\u0016/G0A\u0002\u000f\u000e/G0D/G0D\u0007\u000b\u0007/G09\u0016.\u000e/G09\u000f\u0014\u0002/G0A/G0D\u0002\b\u001d\u001d\u0006\u000e\u001a\b\u0016\u000e/G0A/G09\u00141\u0002\u0002 /G0D\u000e\u0002\u000f\u0002\u0004\u0010\u0011\u0001\u0002\u0012 %\nL\u0002\b\u0002A\u0007/\u0011/\b\u0014\u0007\u000f\u0002\u0016/G0A/G0A\u0006%\u0002/G09/G0A-\u0002\u001a/G0A/G09\u0016\b\u000e/G09\u0014\u0002\u001c/G0A\u000b\u0007\u0002\u0016 \b/G09\u0002()%\u0004\u0004\u0004\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\n\u000e/G09\u001a\u000e\u001d\u000e\u0016\u0014%\u0002\b\u0010\b\u000e/G09\u0002\u000e/G09\u0002\b\u0002\u000b\b/G09\u0010\u0007\u0002/G0A/G0D\u0002/G0D/G0A\u000b\u001c\b\u0016\u00141\u0002\u0002, \u0007\u0002\"\u001f;\u0014\u0002/G0D/G0A\u000b\u0002//G0A\u0016 \u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u0014\u0002\b\u000b\u0007\u0002\u0010\u000e#\u0007/G09\u0002\b\u0016\u0002\u0016 \u0007\u0002\u0007/G09\u000f\u0002/G0A/G0D\u0002\u0016 \u000e\u0014\u0002\b\u000b\u0016\u000e\u001a\u0006\u00071\n, \u0007\u00020\u0015\b/G09\u0016\u000e\u0016$\u0002/G0A/G0D\u0002\u000f\b\u0016\b\u0002\u000e\u0014\u0002\u001c\u0015\u001a \u0002\u0006\u0007\u0014\u0014\u0002\u000e/G09\u0016\u0007\u000b\u0007\u0014\u0016\u000e/G09\u0010\u0002\u0016 \b/G09\u0002\u0016 \u0007\u0002\u0014\u001d\u000b\u0007\b\u000f\u0002/G0A/G0D\u0002\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014%\u0002\u0016 \u0007\u0002/G09\b\u0016\u0015\u000b\u0007\u0002/G0A/G0D\u0002\u0016 \u0007\n\u0014\u0007\b\u000b\u001a \u0007\u0014\u0002\u000e\u001c\u001d\u0006\u0007\u001c\u0007/G09\u0016\u0007\u000f%\u0002\b/G09\u000f\u0002\u0016 \u0007\u0002\u001d/G0A\u0016\u0007/G09\u0016\u000e\b\u0006\u0002\u0015\u0014\u0007\u0014\u0002/G0D/G0A\u000b\u0002\u000b\u0007\u0016\u000b\u000e\u0007#\u0007\u000f\u0002\u000f\b\u0016\b\u0002\u000e/G09\u0002/G09\u0007-\u0002\u001a/G0A/G09\u0016\u00073\u0016\u00141\u0002\u0002F\u000b\u000e\u0007/G0D\u0006$%\u0002 /G0D\u000e\u0002\u000f\u0002\u0004\u0010\u0011\u0001\u0002\u0012L\n\u000e/G09\u001a/G0A\u000b\u001d/G0A\u000b\b\u0016\u0007\u0014\u0002\u000e/G09\u001a\u000e\u001d\u000e\u0016\u0014\u0002/G0D\u000b/G0A\u001c\u0002\u0014\u0007#\u0007\u000b\b\u0006\u0002 \u000e\u0010 \u0006$\u0002\u000f\u000e#\u0007\u000b\u0014\u0007\u0002\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014%\u0002\u000e/G09\u001a\u0006\u0015\u000f\u000e/G09\u0010\u0002/G0A\u000b\u001a \u0007\u0014\u0016\u000b\b\u0006\u0002\b/G09\u000f\u0002\u001a \b\u001c/\u0007\u000b\u0002\u001c\u0015\u0014\u000e\u001a\n\u0018\u001a \u000e\u0007/G0D\u0006$\u0002/G0D\u000b/G0A\u001c\u0002\u0016 \u0007\u0002\u0007\u000e\u0010 \u0016\u0007\u0007/G09\u0016 \u0002\b/G09\u000f\u0002/G09\u000e/G09\u0007\u0016\u0007\u0007/G09\u0016 \u0002\u001a\u0007/G09\u0016\u0015\u000b\u000e\u0007\u0014\u001b%\u0002\u001d/G0A\u0006$\u001d /G0A/G09\u000e\u001a\u0002#/G0A\u001a\b\u0006\u0002\u001c\u0015\u0014\u000e\u001a\u0002\u0018\u001a \u000e\u0007/G0D\u0006$\u0002/G0D\u000b/G0A\u001c\u0002\u0016 \u0007\u0014\u000e3\u0016\u0007\u0007/G09\u0016 \u0002\u001a\u0007/G09\u0016\u0015\u000b$\u001b%\u0002\b/G09\u000f\u0002/G0D/G0A\u0006.\u0002\u001c\u0015\u0014\u000e\u001a\u0002\u0018\u001a \u000e\u0007/G0D\u0006$\u0002/G0D\u000b/G0A\u001c\u0002\u001a\u0007/G09\u0016\u000b\b\u0006\u0002\u0005\u0015\u000b/G0A\u001d\u0007\u0002\b/G09\u000f\u0002/G0D\u000b/G0A\u001c\u0002\u0013 \u000e/G09\b\u001b1\u0002\u0002, \u000e\u0014\u0002/\u000b\u0007\b\u000f\u0016 \u0002\u0014\u001d\b/G09\u0014\u001c\u0015\u0006\u0016\u000e\u001d\u0006\u0007\u0002/G09/G0A\u0016\b\u0016\u000e/G0A/G09\b\u0006%\u0002\u0016/G0A/G09\b\u0006%\u0002\b/G09\u000f\u0002\u001c\u0007/G09\u0014\u0015\u000b\b\u0006\u0002\u0016\u000b\b\u000f\u000e\u0016\u000e/G0A/G09\u00141\u0002\u0002, \u0007\u000b\u0007/G0D/G0A\u000b\u0007%\u0002\b\u0006\u0016 /G0A\u0015\u0010 \u0002\u0016 \u0007\u0002/\u000b\u0007#\u000e\u0016$\u0002/G0A/G0D\u0002\u000e/G09\u001a\u000e\u001d\u000e\u0016\u0014\u0014\u000e\u001c\u001d\u0006\u000e/G0D\u000e\u0007\u0014\u0002\u0014/G0A\u001c\u0007\u0002/G09\u0007\u0007\u000f\u0014%\u0002\u0016 \u0007\u0002\u000b\b/G09\u0010\u0007\u0002/G0A/G0D\u0002\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014\u0002\u0014\u0007\u0007\u001c\u0014\u0002\u0016/G0A\u0002\u001a/G0A\u001c\u001d\u0006\u000e\u001a\b\u0016\u0007\u0002/G0A\u0016 \u0007\u000b\u00141\u0002\u0002\n/G0D\u000e\u0002\u000f\u0002\u0004\u0010\u0011\u0001\u0002\u0012\nL\u0002\u001a\u0015\u000b\u000b\u0007/G09\u0016\u0006$\u0002\u0014\u0007\b\u000b\u001a \u0007\u0014\u0002\b\u0016\u0002/G0D\u000e#\u0007\u0002\u0006\u0007#\u0007\u0006\u0014\u0002/G0A/G0D\u0002\u0014\u001d\u0007\u001a\u000e/G0D\u000e\u001a\u000e\u0016$4\u0010\u0007/G09\u0007\u000b\b\u0006\u000e\u0016$N/$\u0002\u001d\u000e\u0016\u001a \u0002\u0014\u0016\u000b\u000e/G09\u0010%\u0002\u000e/G09\u0016\u0007\u000b#\b\u0006\u0006\u000e\u001a\u0002\u0014\u0016\u000b\u000e/G09\u0010%\n\u001d\u000e\u0016\u001a \u0011\u001a\u0006\b\u0014\u0014\u0002\u0014\u0016\u000b\u000e/G09\u0010%\u0002\u0010\u000b/G0A\u0014\u0014\u0002\u001a/G0A/G09\u0016/G0A\u0015\u000b\u0002\u0018\u000b\u0007\u0006\b\u0016\u000e#\u0007\u0002\u000f\u000e\u000b\u0007\u001a\u0016\u000e/G0A/G09\u0002/G0A/G09\u0006$\u001b%\u0002\b/G09\u000f\u0002\u000b\u0007/G0D\u000e/G09\u0007\u000f\u0002\u001a/G0A/G09\u0016/G0A\u0015\u000b\u0002\u0018\u000b\u0007\u0006\b\u0016\u000e#\u0007\u0002\u000f\u000e\u000b\u0007\u001a\u0016\u000e/G0A/G09%\u0002-\u000e\u0016 \u0014\u0016\u0007\u001d4\u0014.\u000e\u001d\u0002\u000f\u000e\u0014\u001a\u000b\u000e\u001c\u000e/G09\b\u0016\u000e/G0A/G09\u001b1\u0002\u0002:\u0016\u0007\u001c\u0014\u0002\u000b\u0007\u0016\u0015\u000b/G09\u0007\u000f\u0002/$\u0002\u0016 \u0007\u0002\u0014\u0007\b\u000b\u001a \u0002\u001a\b/G09\u0002/\u0007\u0002\u001d\b\u0014\u0016\u0007\u000f\u0002/G0A/G09\u0016/G0A\u0002\b\u0002\u001a\u0006\u000e\u001d//G0A\b\u000b\u000f\u0002\b/G09\u000f\u0002\u00073\u001d/G0A\u000b\u0016\u0007\u000f/G0D/G0A\u000b\u0002/G0D\u0015\u000b\u0016 \u0007\u000b\u0002\u0007#\b\u0006\u0015\b\u0016\u000e/G0A/G091\u0002\u0002, \u0007$\u0002\u001a\b/G09\u0002\b\u0006\u0014/G0A\u0002/\u0007\u0002 \u0007\b\u000b\u000f\u0002\b\u0014\u0002\u0019:7:\u0002/G0D\u000e\u0006\u0007\u00141\u0002\u0002\"\u0014\u0007\u000b\u0002/G0D\u0007\u0007\u000f/\b\u001a.\u0002\u000e\u0014\u0002-\u0007\u0006\u001a/G0A\u001c\u0007\u000f%\u0002\b\u0014\u0002\b\u000b\u0007\u0002\u0015\u0014\u0007\u000b\u0014\u0015/\u001c\u000e\u0014\u0014\u000e/G0A/G09\u0014\u0002\u0016 \b\u0016\u0002\u001c\u0007\u0007\u0016\u0002\u0016 \u0007\u0002/G0A\u0016 \u0007\u000b\u0002\u001a\u000b\u000e\u0016\u0007\u000b\u000e\b\u0002/G0A/G0D\u0002\u000e/G09\u001a\u0006\u0015\u0014\u000e/G0A/G091\n/G0D\u000e\u0002\u000f\u0002\u0004\u0010\u0011\u0001\u0002\u0012\nL\u0002 \b\u0014\u0002\b\u0002\u0010\u000b/G0A-\u000e/G09\u0010\u0002\u0014\u0015\u001d\u001d\u0006\u0007\u001c\u0007/G09\u0016\u0002/G0A/G0D\u0002\u0016\u00073\u0016\u0015\b\u0006\u0002\u000f\b\u0016\b1\u0002\u0002, \u0007\u0002/G0D\u000e\u0007\u0006\u000f\u0014\u0002\u000e/G09\u001a\u0006\u0015\u000f\u0007\u000f\u0002\u000e\u000f\u0007/G09\u0016\u000e/G0D$\u0002\u000e/G09/G0D/G0A\u000b\u001c\b\u0016\u000e/G0A/G09\n\u0014\u001d\u0007\u001a\u000e/G0D\u000e\u001a\u0002\u0016/G0A\u0002\u000e/G09\u000f\u000e#\u000e\u000f\u0015\b\u0006\u0002\u001d\u000e\u0007\u001a\u0007\u0014\u0002/G0A\u000b\u0002\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014J\u0002\u00071\u00101%\u0002\u001a/G0A\u001c\u001d/G0A\u0014\u0007\u000b%\u0002\u000f\b\u0016\u0007\u0002/G0A/G0D\u0002\u001a/G0A\u001c\u001d/G0A\u0014\u000e\u0016\u000e/G0A/G09%\u0002\b/G09\u000f\u0002\u000e/G09\u0014\u0016\u000b\u0015\u001c\u0007/G09\u0016\b\u0016\u000e/G0A/G09\b/G09\u000f\u0002\u0010\u0007/G09\u000b\u0007\u0002/G0D/G0A\u000b\u0002\u001a\u0006\b\u0014\u0014\u000e\u001a\b\u0006\u0002\u000e/G09\u0014\u0016\u000b\u0015\u001c\u0007/G09\u0016\b\u0006\u0002\u001c\u0015\u0014\u000e\u001a8\u0002\u0014/G0A\u001a\u000e\b\u0006\u0002/G0D\u0015/G09\u001a\u0016\u000e/G0A/G09%\u0002\u001a/G0A\u0015/G09\u0016\u000b$%\u0002\b/G09\u000f\u0002\u000b\u0007\u0010\u000e/G0A/G09\u0002/G0D/G0A\u000b\u0002/G0D/G0A\u0006.\u0011\u001c\u0015\u0014\u000e\u001a1\u0002\u0002;\u000e/G09.\u0014\n\u0007/G09\b/\u0006\u0007\u0002\u0016 \u0007\u0002\u00073\u0016\u000b\b\u001a\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\b\u0006\u0006\u0002\u001c\b\u0016\u0007\u000b\u000e\b\u0006\u0014\u0002\u000b\u0007\u0006\u0007#\b/G09\u0016\u0002\u0016/G0A\u0002/G0A/G09\u0007\u0002\u0006/G0A/G09\u0010\u0002-/G0A\u000b.\u0002\u0018\u00071\u00101%\u0002\b\u0006\u0006\u0002\u0016 \u0007\u0002\u0016 \u0007\u001c\u0007\u0014\u0002/G0A/G0D\u0002\b\u0002\u0014$\u001c\u001d /G0A/G09$\u001b/G0A\u000b\u0002\b\u0006\u0006\u0002\u0016 \u0007\u0002-/G0A\u000b.\u0014\u0002\u000e/G09\u0002\b\u0002\u001a/G0A\u0006\u0006\u0007\u001a\u0016\u000e/G0A/G09\u0002\u0018\u00071\u00101%\u0002\u0016 \u0007\u0002 \u0013\u0002\u0014\u0014\u0015/G0D\u0002\u000f\u0016\u0002\u0012\u0002\u0001\u0003\u0017\u0014\u0005\u0018\u0010\u0002\u0012 \u001b1\u0002\u0002,\u00073\u0016\u0011/G0D\u000e\u0007\u0006\u000f\u0014\u0002\u0007/G09\b/\u0006\u0007\u0002\u001d\u000b\u0007\u0006\u000e\u001c\u000e/G09\b\u000b$\u0002\u000b\u0007\u0014\u0015\u0006\u0016\u0014\n\u0016/G0A\u0002/\u0007\u0002/G0D\u0007\u000f\u0002\u0016/G0A\u0002/G0A\u0016 \u0007\u000b\u0002\u001d\u000b/G0A\u0010\u000b\b\u001c\u00141\u0002\u0002:/G09\u0002\u000b\u0007\u0006\b\u0016\u0007\u000f\u0002-/G0A\u000b.%\u0002/G0D/G0A\u000b\u0002\u00073\b\u001c\u001d\u0006\u0007%\u0002\u001e\b\u000b\u000f\u0007/G09\u0002\b/G09\u000f\u0002!\u0015\u000b/G0A/G09\u0002\u0018\u0003\u0004\u0004\u0004\u001b\u0002 \b#\u0007\u0002/G0D\u0007\u000fH\u000b\u0007\u0010\u000e/G0A/G09I\u0002\u000f\b\u0016\b\u0002\u0016/G0A\u0002\b\u0002\u0010\u0007/G0A\u0010\u000b\b\u001d \u000e\u001a\b\u0006\u0002\u001c\b\u001d\u001d\u000e/G09\u0010\u0002\u001d\u000b/G0A\u0010\u000b\b\u001c\u0002\u0016/G0A\u0002\u0014 /G0A-\u0002\u0016 \u0007\u0002\u000f\u000e\u0014\u0016\u000b\u000e/\u0015\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002/G0D\u0007\b\u0016\u0015\u000b\u0007\u00141\u0002\u0002\u0002\u0002<, \u0007\u0002\u0014\u0007\b\u000b\u001a \u0002\b\u0006\u0010/G0A\u000b\u000e\u0016 \u001c\u0014\u0002/G0D/G0A\u000b\u0002 /G0D\u000e\u0002\u000f\u0002\u0004\u0010\u0011\u0001\u0002\u0012\nL\u0002-\u0007\u000b\u0007\u0002/G0A\u000b\u000e\u0010\u000e/G09\b\u0006\u0006$\u0002\u000f\u0007#\u000e\u0014\u0007\u000f\u0002/$\u00027\b#\u000e\u000f\u0002!\u0015\u000b/G0A/G091\u0002\u0002, \u0007\u0002\u001d\u000b/G0A\u0010\u000b\b\u001c\n\u000e\u001c\u001d\u0006\u0007\u001c\u0007/G09\u0016\b\u0016\u000e/G0A/G09\u0002\b/G09\u000f\u0002A\u0007/\u0002\u000e/G09\u0016\u0007\u000b/G0D\b\u001a\u0007%\u0002\b\u0014\u0002-\u0007\u0006\u0006\u0002\b\u0014\u0002\u000f\b\u0016\b\u0002\u0016\u000b\b/G09\u0014\u0006\b\u0016\u000e/G0A/G09\u0002\b/G09\u000f\u0002\u0016 \u0007\u0002\u001c\b/G09$\u0002\u0016\b\u0014.\u0014\u0002/G0A/G0D\u0002#\u0007\u000b\u000e/G0D\u000e\u001a\b\u0016\u000e/G0A/G09% \b#\u0007\u0002/\u0007\u0007/G09\u0002\u000f/G0A/G09\u0007\u0002\u0006\b\u000b\u0010\u0007\u0006$\u0002/$\u0002\u0013\u000b\b\u000e\u0010\u0002\f\b\u001d\u001d%\u0002-\u000e\u0016 \u0002\u001a/G0A/G09\u0016\u000b\u000e/\u0015\u0016\u000e/G0A/G09\u0014\u0002/$\u0002\u001e/G09\u000f\u000b\u0007\b\u0014\u0002>/G0A\u000b/G09\u0014\u0016\b\u0007\u000f\u0016\u0002\b/G09\u000f\u0002\b\u0002 /G0A\u0014\u0016\u0002/G0A/G0D\u0002/G0A\u0016 \u0007\u000b\f\u0016\b/G09/G0D/G0A\u000b\u000f\u0002\u0014\u0016\u0015\u000f\u0007/G09\u0016\u0014\u0002\b/G09\u000f\u0002#\u000e\u0014\u000e\u0016/G0A\u000b\u00141=\u0002\u0002\nA\u0007\u0002\b\u000b\u0007\u0002\u0006\u0007\u000f\u0002\u0016/G0A\u0002/\u0007\u0006\u000e\u0007#\u0007\u0002\u0016 \b\u0016\u0002\u001c\u0015\u001a \u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u001c/G0A\u0014\u0016\u0002\u0015\u0014\u0007/G0D\u0015\u0006\u0002\u0014\u0007\b\u000b\u001a \u000e/G09\u0010\u0002\u000e/G09\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014\u0002-\u000e\u0006\u0006\u0002/\u0007\u0002/G0A/G0D\u0002\u0016 \u0007\n/G0D\u0015@@$\u0002#\b\u000b\u000e\u0007\u0016$1\u0002\u0002\u0002, \u0007\u0002\u000f\u0007\u0010\u000b\u0007\u0007\u0002/G0A/G0D\u0002\u000b\u0007\u0006\b\u0016\u000e/G0A/G09\u0014 \u000e\u001d\u0002/\u0007\u0016-\u0007\u0007/G09\u0002\u001d\u000e\u0007\u001a\u0007\u0014\u0002\b\u0006\u000b\u0007\b\u000f$\u0002./G09/G0A-/G09\u0002\u0016/G0A\u0002/\u0007\u0002\u0014\u000e\u001c\u000e\u0006\b\u000b\u0002\u000e\u0014\u0002/G0D\u0015/G09\u000f\b\u001c\u0007/G09\u0016\b\u0006\u0016/G0A\u0002\u001c\b/G09$\u00020\u0015\u0007\u0014\u0016\u000e/G0A/G09\u0014\u0002\u0016\u000b\b\u000f\u000e\u0016\u000e/G0A/G09\b\u0006\u0006$\u0002\u001d/G0A\u0014\u0007\u000f1\u0002\u0002:\u0016\u0002\u001a\b/G09\u0002\b\u000e\u000f\u0002\u0016 \u0007\u0002\u000e\u000f\u0007/G09\u0016\u000e/G0D\u000e\u001a\b\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u0016\u0015/G09\u0007\u0011/G0D\b\u001c\u000e\u0006\u000e\u0007\u0014\u0002\u000e/G09\u0002/G0D/G0A\u0006.\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u00148\u0002\u0016 \u0007\u0002\u000f\u0007\u0010\u000b\u0007\u0007\u0002/G0A/G0D\u0002\u000b\u0007\u0006\b\u0016\u000e/G0A/G09\u0014 \u000e\u001d\u0002/\u0007\u0016-\u0007\u0007/G09\u0002\u000e/G09\u0016\u0007/G09\u000f\u0007\u000f\u0002#\b\u000b\u000e\b\u0016\u000e/G0A/G09\u0014\u0002\u000e/G09\u0002\u001a\u0006\b\u0014\u0014\u000e\u001a\b\u0006\u0002\u000e/G09\u0014\u0016\u000b\u0015\u001c\u0007/G09\u0016\b\u0006\u0002\u001c\u0015\u0014\u000e\u001aG\u0002/G0A\u000b\u0016 \u0007\u0002\u001a/G0A#\u0007\u000b\u0016\u0002\b\u0010\u0007/G09\u000f\b\u0002/G0A/G0D\u0002\u001d\b\u000b/G0A\u000f$\u0002\b/G09\u000f\u0002\u001d\b\u000b\b\u001d \u000b\b\u0014\u0007\u0002\u000e/G09\u0002#/G0A\u001a\b\u0006\u0002\u001c\u0015\u0014\u000e\u001a1\u0002\u0002\u001e\u001c/\u000e\u0010\u0015\u000e\u0016$\u0002\b/G09\u000f\u0002\u000e\u001c\u001d\u0006\u000e\u001a\b\u0016\u000e/G0A/G09\u0002 \b#\u0007\u0002\u001d\u0006\b$\u0007\u000f\u000e\u001c\u001d/G0A\u000b\u0016\b/G09\u0016\u0002\u000b/G0A\u0006\u0007\u0014\u0002\u000e/G09\u0002\u0016 \u0007\u0002\u000f\u0007\u0016\b\u000e\u0006\u0007\u000f\u0002\u001a\u000b\b/G0D\u0016\u0002/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u001a/G0A\u001c\u001d/G0A\u0014\u000e\u0016\u000e/G0A/G09\u0002/G0A#\u0007\u000b\u0002\u0016 \u0007\u0002\u001d\b\u0014\u0016\u0002\u0016 \u000b\u0007\u0007\u0002 \u0015/G09\u000f\u000b\u0007\u000f\u0002$\u0007\b\u000b\u00141\u0002, \u0007\u0014\u0007\u0002\u0014\u0015/\u0016\u0006\u0007\u0016\u000e\u0007\u0014\u0002/\u0007\u0010\u0002/G0D/G0A\u000b\u0002\u0014\u0015\u000e\u0016\b/\u0006\u0007\u0002-\b$\u0014\u0002/G0A/G0D\u0002\u0014\u0007\b\u000b\u001a \u000e/G09\u0010\u0002- \u000e\u001a \u0002\b\u000b\u0007\u0002\u0006\u000e.\u0007\u0006$\u0002\b\u0014\u0002 \u0007\u0016\u0007\u000b/G0A\u0010\u0007/G09\u0007/G0A\u0015\u0014\u0002\b\u0014\u0002\u0016 \u0007\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014\u0002\u0016 \u0007\u001c\u0014\u0007\u0006#\u0007\u00141\u0002\u0002\n\u00031\u0002\f/G0A\u0015/G09\u000f\u00027\b\u0016\b\u0002#\u00141\u0002/G09/G0A\u0016\b\u0016\u000e/G0A/G09\u00027\b\u0016\b\n, \u0007\u0002\u000b\u0007\u0014\u0015\u0006\u0016\u0014\u0002/G0A/G0D\u0002/G0D\u0015@@$\u0002\u0014\u0007\b\u000b\u001a \u000e/G09\u0010\u0002/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u001c\b\u0016\u0007\u000b\u000e\b\u0006\u0002-\u000e\u0006\u0006\u0002/\u0007\u0002\u001d\b\u000b\u0016\u000e\b\u0006\u0006$\u0002\u000f\u0007\u001d\u0007/G09\u000f\u0007/G09\u0016\u0002/G0A/G09\u0002 /G0A-\u0002\u0016 \u0007\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u000f\b\u0016\b\u0002 \b#\u0007\u0002/\u0007\u0007/G09\u0002\u000f\u0007/G0D\u000e/G09\u0007\u000f1\u0002\u0002\u0012/G0A\u000b\u0002/G0A/G09\u0007\u0002\u001d\u0007\u000b\u0014/G0A/G09\u0002H\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000f\b\u0016\bI\u0002\u000e\u0014\u0002/G09\u0007\u001a\u0007\u0014\u0014\b\u000b\u000e\u0006$\u0002\u000f\u0007\u000b\u000e#\u0007\u000f\u0002/G0D\u000b/G0A\u001c\u0002\u0014/G0A\u0015/G09\u000f\u0002\b/G09\u000f\u0002\u000e\u0014/G0D\u0015/G09\u000f\b\u001c\u0007/G09\u0016\b\u0006\u0006$\u0002\u0016\u0007\u001c\u001d/G0A\u000b\b\u0006\u0006$\u0011/G0A\u000b\u000f\u0007\u000b\u0007\u000f8\u0002\u0016/G0A\u0002\b/G09/G0A\u0016 \u0007\u000b\u0002\u000e\u0016\u0002\u000e\u0014\u0002/G09\u0007\u001a\u0007\u0014\u0014\b\u000b\u000e\u0006$\u0002\u000f\u0007\u001d\u0007/G09\u000f\u0007/G09\u0016\u0002/G0A/G09\u0002/G09/G0A\u0016\b\u0016\u000e/G0A/G09%\u0002\b/G09\u000f\u0002\u000e\u0014\u0016 \u0007\u000b\u0007/G0D/G0A\u000b\u0007\u0002\u0014\u001d\b\u0016\u000e\b\u0006\u0006$\u0002/G0A\u000b\u000f\u0007\u000b\u0007\u000f1\u0002\u0002\u0012/G0A\u000b\u0002\u0014\u0007\b\u000b\u001a \u0002\u0014\u0016\u000b\b\u0016\u0007\u0010\u000e\u0007\u0014%\u0002\u0016 \u0007\u0002\u000f\u000e/G0D/G0D\u0007\u000b\u0007/G09\u001a\u0007\u0002\u000e\u0014\u0002\u001a\u000b\u000e\u0016\u000e\u001a\b\u00061\u0002\u0002\u0005/G09\u001a/G0A\u000f\u000e/G09\u0010\u0002\u0014\u001a \u0007\u001c\u0007\u0014\u0002- \u000e\u001a \u001d\u000b\u000e#\u000e\u0006\u0007\u0010\u0007\u0002/G0A/G09\u0007\u0002\u000f/G0A\u001c\b\u000e/G09\u0002\u0016\u0007/G09\u000f\u0002\u0016/G0A\u0002\u0014 /G0A\u000b\u0016\u0011\u001a \b/G09\u0010\u0007\u0002\u0016 \u0007\u0002/G0A\u0016 \u0007\u000b%\u0002/\u0015\u0016\u0002\u001c\u0015\u0006\u0016\u000e\u0011\u000f/G0A\u001c\b\u000e/G09\u0002\u0007/G09\u001a/G0A\u000f\u000e/G09\u0010\u0002-\u000e\u0016 \u000e/G09\u0002\u0014\u000e/G09\u0010\u0006\u0007\u0002/G0D\u000e\u0006\u0007\u0014\u0002\u000e\u0014\u0007/G09/G0A\u000b\u001c/G0A\u0015\u0014\u0006$\u0002\u001a/G0A\u001c\u001d\u0006\u00073\u0002\b/G09\u000f\u0002\u000f\u000e/G0D/G0D\u000e\u001a\u0015\u0006\u0016\u0002/G0D/G0A\u000b\u0002\u001d\u000b/G0A\u0010\u000b\b\u001c\u001c\u0007\u000b\u0014\u0002\u0016/G0A\u0002\u000f\u0007\u001a/G0A\u000f\u0007\u0002\b\u001a\u001a\u0015\u000b\b\u0016\u0007\u0006$%\u0002\u0007/G0D/G0D\u000e\u001a\u000e\u0007/G09\u0016\u0006$%\u0002\b/G09\u000f\u0002\u001a/G0A\u001c\u001d\u0006\u0007\u0016\u0007\u0006$1\u0002\u0019\u0015\u0014\u000e\u001a\u0002\u000e\u0014\u0002/G0D\u0015\u0006\u0006\u0002/G0A/G0D\u0002\u001a/G0A/G09\u0016\u00073\u0016\u0015\b\u0006\u0002\u0015/G09\u000f\u0007\u000b\u0014\u0016\b/G09\u000f\u000e/G09\u0010\u0014\u0002\b/G09\u000f\u0002\u000e\u001c\u001d\u0006\u000e\u001a\u000e\u0016\u0002\u001c\u0007\b/G09\u000e/G09\u0010\u00141\u0002\u0002, \u0007\u0002\u001d\u000b/G0A\u0010\u000b\b\u001c\u001c\u000e/G09\u0010\u0002/G0A#\u0007\u000b \u0007\b\u000f\u0002/G0A/G09/G0D\u0007\u000b\u000b\u0007\u0016\u000e/G09\u0010\u0002/G0A\u0015\u0016\u0002\u0007\u000e\u0016 \u0007\u000b\u0002/G0D\u000b/G0A\u001c\u0002\b\u0002\u001d/G0A\u0006$\u001d /G0A/G09\u000e\u001a\u0002\u0014\u001a/G0A\u000b\u0007\u0002\u000e\u0014\u0002\u001a/G0A/G09\u0014\u000e\u000f\u0007\u000b\b/\u0006\u00071\u0002\u0002\u0002\u0002\n\u001e\u0002\u0014\u000e\u001c\u001d\u0006\u0007\u000b\u0002\b/G09\u000f\u0002$\u0007\u0016\u0002#\u0007\u000b$\u0002\u0016\b3\u000e/G09\u0010\u0002\u001d\u000b/G0A/\u0006\u0007\u001c\u0002\u000e\u0014\u0002\u0014\u000e\u001c\u001d\u0006$\u0002\u0016 \b\u0016\u0002\u0016 \u0007\u0002\u0014\u000e/G09\u0010\u0006\u0007\u0002\u0014$\u001c//G0A\u0006\u0002- \u000e\u001a \u0002\u000e/G09\u0002\u001a/G0A\u001c\u001c/G0A/G09\u0002A\u0007\u0014\u0016\u0007\u000b/G09\n/G09/G0A\u0016\b\u0016\u000e/G0A/G09\u0002- \u000e\u001a \u0002\u000b\u0007\u001d\u000b\u0007\u0014\u0007/G09\u0016\u0014\u0002\b\u0002H/G09/G0A\u0016\u0007I\u0002\u001a/G0A\u001c/\u000e/G09\u0007\u0014\u0002\u001c\u0015\u0006\u0016\u000e\u001d\u0006\u0007\u0002\u001d\b\u000b\b\u001c\u0007\u0016\u0007\u000b\u0014\u0002\u0018\u001d\u000e\u0016\u001a \u0002\b/G09\u000f\u0002\u000f\u0015\u000b\b\u0016\u000e/G0A/G098\u0002/G0A/G0D\u0016\u0007/G09\b\u000f\u000f\u000e\u0016\u000e/G0A/G09\b\u0006\u0002/G0D\u0007\b\u0016\u0015\u000b\u0007\u0014\u0002/G0A/G0D\u0002\b\u000b\u0016\u000e\u001a\u0015\u0006\b\u0016\u000e/G0A/G09%\u0002\u000f$/G09\b\u001c\u000e\u001a\u0014%\u0002\u0007\u0016\u0002\b\u00061\u001b%\u0002\u0007\b\u001a \u0002/G0A/G0D\u0002- \u000e\u001a \u0002\u001c\b$\u0002\u000b\u00070\u0015\u000e\u000b\u0007\u0002\u001a/G0A/G09\u0016\u00073\u0016\u0015\b\u0006\u000e/G09/G0D/G0A\u000b\u001c\b\u0016\u000e/G0A/G09\u0002\u0016/G0A\u0002\u000f\u0007\u001a/G0A\u000f\u00071\u0002\u0002, \u0007\u0002\u0016\u0007\u000b\u001c\u0002H\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u000f\b\u0016\bI\u0002\u000e\u0014\u0002\b\u0006\u001c/G0A\u0014\u0016\u0002\u0015/G09/\u0007\b\u000b\b/\u0006$\u0002#\b\u0010\u0015\u00071\n\u001e\u0006\u0016 /G0A\u0015\u0010 \u0002\u000e\u0016\u0002\u000e\u0014\u0002\u0010\u0007/G09\u0007\u000b\b\u0006\u0006$\u0002\u0016\u000b\u0015\u0007\u0002\u0016 \b\u0016\u0002\u001c\u0007\b/G09\u000e/G09\u0010\u0002\u000e\u0014\u0002\u000f\u0007\u0016\u0007\u000b\u001c\u000e/G09\u0007\u000f\u0002/$\u0002\u0016 \u0007\u0002/G0A\u000b\u000f\u0007\u000b\u0002\u000e/G09\u0002- \u000e\u001a \u0002\u0014$\u001c//G0A\u0006\u0014\u0002\b\u000b\u0007\u0002\b\u000b\u000b\b/G09\u0010\u0007\u000f%\n\u0016\u00073\u0016\u0002\u000e\u0014\u0002\b\u0006-\b$\u0014\u0002\u0014\u0007\b\u000b\u001a \u0007\u000f\u0002\b/G09\u000f\u0002\u001c\b\u0016\u001a \u0007\u000f\u0002\u000e/G09\u0002/G0A/G09\u0007\u0002\u000f\u000e\u000b\u0007\u001a\u0016\u000e/G0A/G09\u0002\b/G09\u000f\u0002/G0A/G09\u0002/G0A/G09\u0007\u0002\u000f\u000e\u001c\u0007/G09\u0014\u000e/G0A/G091\u0002\u0002\f/G0A\u001c\u0007\u0002\b\u001d\u001d\u000b/G0A\u001d\u000b\u000e\b\u0016\u0007\u001c\b\u0016\u001a \u0007\u0014\u0002\u000e/G09\u0002\u001c\u0015\u0014\u000e\u001a\u0002\u001c\b$\u0002\u000b\u00070\u0015\u000e\u000b\u0007\u0002\u0014\u0007\u0006\u0007\u001a\u0016\u000e/G0A/G09\u0002/G0D\u000b/G0A\u001c\u0002\b\u0002\u0006\b\u000b\u0010\u0007\u0002/G0D\u000e\u0007\u0006\u000f\u0002/G0A/G0D\u0002/G0A/6\u0007\u001a\u0016\u0014\u0002\u0018-\u000e\u0016 \u0002\u001a/G0A/G09\u0014\u00070\u0015\u0007/G09\u0016\u0002-\u0007\u000e\u0010 \u0016\u000e/G09\u0010\u0014\b\u001c/G0A/G09\u0010\u0002\u0016 \u0007\u001c\u001b\u0002/G0A\u000b\u0002\u0014\u0006\b/G09\u0016\u0007\u000f\u0002#\u000e\u0007-\u0014\u0002\u0016/G0A\u0002\u0006/G0A\u001a\b\u0016\u0007\u0002\u001a/G0A\u001c\u001d/G0A\u0014\u000e\u0016\u0007\u0002\u001d\b\u0016\u0016\u0007\u000b/G09\u0014\u0002\u000f\u000e\u0014\u0016\u000b\u000e/\u0015\u0016\u0007\u000f\u0002\b\u001a\u000b/G0A\u0014\u0014\u0002\u0014\u0007#\u0007\u000b\b\u0006\u0002\u001d\b\u000b\u0016\u0014\u000e/G09\u0014\u0016\u000b\u0015\u001c\u0007/G09\u0016\u0014%\u0002/G0A\u000b\u0002\b\u0006\u0016\u0007\u000b\u0007\u000f\u0002\u0016\u0007\u001c\u001d/G0A\u000b\b\u0006\u0006$\u0002/G0A\u000b\u0002\u0016/G0A/G09\b\u0006\u0006$\u0002/G0D\u000b/G0A\u001c\u0002/G0A/G09\u0007\u0002\u001c/G0A#\u0007\u001c\u0007/G09\u0016\u0002\u0016/G0A\u0002\u0016 \u0007\u0002/G09\u00073\u0016\u0002\u0018/G0A\u000b\u0002\u000e/G09\u000f\u0007\u0007\u000f\u0002/G0D\u000b/G0A\u001c\u0002/G0A/G09\u0007-/G0A\u000b.\u0002\u0016/G0A\u0002\u0016 \u0007\u0002/G09\u00073\u0016\u001b1\u0002\u0002\u0002\u0002\u0002\n\u0012/G0A\u000b\u0002\b\u0002\u0016\u00073\u0016\u0015\b\u0006\u0002\u00073\u001d\u0006\b/G09\b\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u0014\u0015\u001a \u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u001a/G0A/G09\u001a\u0007\u001d\u0016\u0014%\u0002\u0006\u0007\u0016\u0002\u0015\u0014\u0002\u001a/G0A/G09\u0014\u000e\u000f\u0007\u000b\u0002\b\u0002\u001a\u000b$\u001d\u0016/G0A\u0010\u000b\b\u001c1\u0002\u0002:/G09\u0002\b\u0002\u001a\u000b$\u001d\u0016/G0A\u0010\u000b\b\u001c\u0002\b\n\u001a\u000b/G0A\u0014\u0014\u0011-/G0A\u000b\u000f\u0002\u001d\u0015@@\u0006\u0007\u0002\u000e\u0014\u0002\u001a/G0A\u001c\u001d\u0006\u0007\u0016\u0007\u000f\u0002\u00185\u001b\u0002/$\u0002\u000f\u0007\u001a/G0A\u000f\u000e/G09\u0010\u0002#\u0007\u000b/\b\u0006\u0002\u001a\u0006\u0015\u0007\u0014%\u0002\u0018\u0003\u001b\u0002/$\u0002\u0016\u000b\b/G09\u0014/G0D\u0007\u000b\u000b\u000e/G09\u0010\u0002\u0016 \u0007\u0002\u0006\u0007\u0016\u0016\u0007\u000b\u0014\u0002\b\u0014\u0014/G0A\u001a\u000e\b\u0016\u0007\u000f-\u000e\u0016 \u0002\u0007\b\u001a \u0002/G09\u0015\u001c/\u0007\u000b\u0002\u000e/G09\u0002\u0007\b\u001a \u0002\b/G09\u0014-\u0007\u000b\u0011\u0006\u000e/G09\u0007\u0002\u0016/G0A\u0002\u0016 \u0007\u0002\b\u001d\u001d\u000b/G0A\u001d\u000b\u000e\b\u0016\u0007\u0002//G0A3\u0002\u000e/G09\u0002\u0016 \u0007\u0002\u001d\u0015@@\u0006\u0007%\u0002\b/G09\u000f\u0002\u0018(\u001b\u0002/$\u0002\u001a/G0A/G09/G0D\u000e\u000b\u001c\u000e/G09\u0010\u0002\u0016 \u0007\u000b\u0007\u0014\u0015\u0006\u0016\u0002\u0018\b\u00020\u0015/G0A\u0016\b\u0016\u000e/G0A/G09\u0002\u00073\u0016\u000b\b\u001a\u0016\u0007\u000f\u0002/G0D\u000b/G0A\u001c\u0002\b\u0002//G0A/G0A.\u001b\u0002/$\u0002\u000f\u0007\u001a/G0A\u000f\u000e/G09\u0010\u0002\u0016 \u0007\u0002\u0016\u000e\u0016\u0006\u0007\u0002/G0A/G0D\u0002\u0016 \u0007\u0002//G0A/G0A.1\u0002\u0002, \u000e\u0014\u0002\u000e\u0014\u0002\u000f/G0A/G09\u0007\u0002/$\b\u0014\u0014\u0007\u001c/\u0006\u000e/G09\u0010\u0002\u0016 \u0007\u0002/G0D\u000e\u000b\u0014\u0016\u0002\u0006\u0007\u0016\u0016\u0007\u000b\u0002/G0A/G0D\u0002\u0007\b\u001a \u0002\b/G09\u0014-\u0007\u000b\u0002-/G0A\u000b\u000f\u0002/G0A\u000b\u0002\u001d \u000b\b\u0014\u0007\u0002\u000e/G09\u0016/G0A\u0002\b\u0002\u0014\u000e\u001c\u001d\u0006\u0007\u0002\u0016\u00073\u0016\u0011\u0014\u0016\u000b\u000e/G09\u00101\u0002\u0002\u0002\u0019\b/G09$\u0002.\u000e/G09\u000f\u0014\u0002/G0A/G0D\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\b/G09\b\u0006$\u0014\u000e\u0014\u0002\u000e/G09\u0002\u0015\u0014\u0007\u0002\u0016/G0A\u000f\b$\u0002\b\u000b\u0007\u0002\u001c/G0A\u000b\u0007\u0002\b/G09\b\u0006/G0A\u0010/G0A\u0015\u0014%\u0002\u000e/G09\u0002\u0016 \u0007\u000e\u000b\u0002\u000b\u0007\u000f\u0015\u001a\u0016\u000e#\u0007\u0002\u0016\u0007/G09\u000f\u0007/G09\u001a\u000e\u0007\u0014\u0002\u0016/G0A\u0002\u0018(\u001b\u0002\u0016 \b/G09\u0002\u0016/G0A\u0002\u00185\u001b\u0002/G0A\u000b\u0018\u0003\u001b1\u0002\u0002A \u0007\u000b\u0007\b\u0014\u0002\u0016\u00073\u0016\u0002\b/G09\b\u0006$\u0014\u000e\u0014\u0002\b\u0006\u001c/G0A\u0014\u0016\u0002\b\u0006-\b$\u0014\u0002\u000f\u0007\u001d\u0007/G09\u000f\u0014\u0002/G0A/G09\u0002\u0006\u0007\u0016\u0016\u0007\u000b\u0014\u0002\u0014\u0016\b$\u000e/G09\u0010\u0002\b\u0014\u0014\u0007\u001c/\u0006\u0007\u000f\u0002\u000e/G09\u0016/G0A\u0002-/G0A\u000b\u000f\u0014\u0002\b/G09\u000f\u0002-/G0A\u000b\u000f\u0014\u000e/G09\u0016/G0A\u0002\u0014\u0007/G09\u0016\u0007/G09\u001a\u0007\u0014%\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\b/G09\b\u0006$\u0014\u000e\u0014\u0002\b\u0006\u0006/G0A-\u0014\u0002/G0D/G0A\u000b\u0002\b\u0002\u0006/G0A\u0016\u0002/G0A/G0D\u0002\u000b\u0007\u000f\u0015\u001a\u0016\u000e#\u0007\u0002\u0014\u0007\u0006\u0007\u001a\u0016\u000e/G0A/G091\u0002;\u0007\u0016\u0002\u0015\u0014\u0002\u001a/G0A/G09\u0014\u0016\u000b\u0015\u001a\u0016\u0002\b\u0002\u0014\u0007\u000b\u000e\u0007\u0014\u0002/G0A/G0D\u001d/G0A\u0014\u0014\u000e/\u0006\u0007\u0002\u001c\b\u0016\u001a \u0007\u0014\u0002/G0D/G0A\u000b\u0002\u0016 \u0007\u0002\u0016\b\u000b\u0010\u0007\u0016\u0002 \u0019\u0005\u001a1\u0002\u0002, \u0007\u0002\u000b\u0007\u0014\u0015\u0006\u0016\u0014\u0002/G0A/G0D\u0002\b\u0002\u0016\u00073\u0016\u0002\u0014\u0007\b\u000b\u001a \u0002\u0018\u000e\u0010/G09/G0A\u000b\u000e/G09\u0010\u0002\u001a\b\u0014\u0007\u0002\u000f\u000e/G0D/G0D\u0007\u000b\u0007/G09\u001a\u0007\u0014\u001b\u0002-/G0A\u0015\u0006\u000f\u0002/\u0007\n\u0016 \u0007\u0014\u0007J\u0017\u000b\u000e/G09\u001a\u000e\u001d\u0006\u0007J\u0002/G0A\u000b\u000f\u0007\u000b\u0002\u000e\u0014\u0002/G0A/\u0014\u0007\u000b#\u0007\u000f\n\u001e\u001a\u001a\u0007\u001d\u0016\b/\u0006\u0007\u0002\u001c\b\u0016\u001a \u0007\u0014O/\b\u0010\u0002F\b\u0010%\u0002F\u001eE\"/G09\b\u001a\u001a\u0007\u001d\u0016\b/\u0006\u0007J\u0002/G0A\u000b\u000f\u0007\u000b\u0002\u000e\u0014\u0002#\u000e/G0A\u0006\b\u0016\u0007\u000fO\u0010\b/%\u0002\b/\u0010%\u0002/\u0010\b\n\u0017\u000b\u000e/G09\u001a\u000e\u001d\u0006\u0007J\u0002\u000f\u000e\u000b\u0007\u001a\u0016\u000e/G0A/G09\u0002/G0D/G0A\u000b\u0002\u001a/G0A\u001c\u001d\b\u000b\u000e\u0014/G0A/G09O; \u0001\u001f\u0002\n\u001e\u001a\u001a\u0007\u001d\u0016\b/\u0006\u0007\u0002\u001c\b\u0016\u001a \u0007\u0014J\u0002\u001d\u000b/G0A\u001a\u0007\u0014\u0014\u0007\u000f\u0002; \u0001\u001f\n\"/G09\b\u001a\u001a\u0007\u001d\u0016\b/\u0006\u0007J\u0002\u000f\u000e\u000b\u0007\u001a\u0016\u000e/G0A/G09\u0002\u000e\u0014\u0002#\u000e/G0A\u0006\b\u0016\u0007\u000f\u0002O\u001f \u0001;8\u0002,\u0001F8\u0002F\u0001,8\u0002\u000f\u000e\b\u0010/G0A/G09\b\u0006\u0014\n\u0017\u000b\u000e/G09\u001a\u000e\u001d\u0006\u0007J\u0002\u0006\u000e\u0016\u0007\u000b\b\u0006\u0002\u001a/G0A/G09\u0016\u0007/G09\u0016\u0002\u000b\u00070\u0015\u000e\u000b\u0007\u000f\u0002/G0D/G0A\u000b\u0002\u001c\b\u0016\u001a O/\b\u0010\n\u001e\u001a\u001a\u0007\u001d\u0016\b/\u0006\u0007\u0002\u001c\b\u0016\u001a \u0007\u0014O \u0019\u0005\u001a\u0010\b\u0010\u0007%\u0002\u001a\b/ \u0019\u0005\u001a\u0007\n\"/G09\b\u001a\u001a\u0007\u001d\u0016\b/\u0006\u0007J\u0002\u0006\u000e\u0016\u0007\u000b\b\u0006\u0002\u001a/G0A/G09\u0016\u0007/G09\u00164\u0014\u0007\u001c\b/G09\u0016\u000e\u001a\u0002\u001c\u0007\b/G09\u000e/G09\u0010\u0002\u000e\u0014\u0002#\u000e/G0A\u0006\b\u0016\u0007\u000fO \u0019\u0007\u0005\u001a\u0006\u0007%\u0002\u0019\u0005/G09\u001a%\u0002\u0019\b\u000f\u0002\u0005/G09\u000f\u0002\u001a/G0A/G0A\u000f\nF\u000b\u000e\u0007/G0D\u0006$\u0002\u0014\u0016\b\u0016\u0007\u000f%\u0002\u001a\u0007\u000b\u0016\b\u000e/G09\u0002\u001a/G0A/G09\u001a\u0007\u001d\u0016\u0015\b\u0006\u0002\u001d\u000b\u000e/G09\u001a\u000e\u001d\u0006\u0007\u0014\u0002\u0015\u0014\u0007\u000f\u0002\u000e/G09\u0002\u0015/G09\u000f\u0007\u000b\u0014\u0016\b/G09\u000f\u000e/G09\u0010\u0002\b/G09\u000f\u0002\u000f\u0007\u0014\u001a\u000b\u000e/\u000e/G09\u0010\u0002\u001c\u0015\u0014\u000e\u001a\u0002\u001c\b$\u0002/\u0007\n\u0014\b\u0016\u000e\u0014/G0D\u000e\u0007\u000f\u0002/$\u0002\u0016 \u0007\u0002\u00070\u0015\u000e#\b\u0006\u0007/G09\u0016\u0002/G0A/G0D\u0002\u0014/G0A\u001c\u0007\u0002/G0A/G0D\u0002\u0016 \u0007\u0002\u0015/G09\b\u001a\u001a\u0007\u001d\u0016\b/\u0006\u0007\u0002\u001c\b\u0016\u001a \u0007\u0014\u0002\u0014 /G0A-/G09\u0002 \u0007\u000b\u00071\u0002\u0002, \u0007\u0002\u001a \b/G09\u0010\u0007\u000f\u0002/G0A\u000b\u000f\u0007\u000b\u001c\b$\u0002/\u0007\u0002\b\u001a\u001a\u0007\u001d\u0016\b/\u0006\u0007\u0002\u000e/G09\u0002\u0014\u0007\u0016\u0011\u0016 \u0007/G0A\u000b\u0007\u0016\u000e\u001a\u0002\u0014\u0007\b\u000b\u001a \u0007\u00141\u0002\u0002, \u0007\u0002\u001a \b/G09\u0010\u0007\u000f\u0011\u000f\u000e\u000b\u0007\u001a\u0016\u000e/G0A/G09\u0002\u001c\b\u0016\u001a \u0007\u0014\u0002\u001c\b$\u0002/\u0007\u0002\b\u001a\u001a\u0007\u001d\u0016\b/\u0006\u0007\u0002\u000e/G09\u0014\u0007\u0007.\u000e/G09\u0010\u0002\u000b\u0007\u0016\u000b/G0A\u0010\u000b\b\u000f\u0007\u0002\u0014\u0016\b\u0016\u0007\u001c\u0007/G09\u0016\u0014\u0002\u000e/G09\u0002\u001a/G0A/G09\u0016\u000b\b\u001d\u0015/G09\u0016\b\u0006\u0006$\u0011\u001a/G0A/G09\u001a\u0007\u000e#\u0007\u000f\u0002\u001c\u0015\u0014\u000e\u001a1\u0002\u0002, \u0007\u0002\u000f\u000e\u0014\u001d\u0007\u000b\u0014\u0007\u000f\u0002\u001c\b\u0016\u001a \u0007\u0014\u0002\u0018\u0006\b\u0014\u0016\u00073\b\u001c\u001d\u0006\u0007\u001b\u0002\u001c\u000e\u001c\u000e\u001a\u0002\u0014/G0A\u001c\u0007\u0002\u000e\u001c\u001d/G0A\u000b\u0016\b/G09\u0016\u0002\u000b\u0007\u000f\u0015\u001a\u0016\u000e/G0A/G09\u000e\u0014\u0016\u0002\u0014\u0016\u000b\b\u0016\u0007\u0010\u000e\u0007\u0014\u0002\u0015\u0014\u0007\u000f\u0002/G0D\u000b\u00070\u0015\u0007/G09\u0016\u0006$\u0002\u000e/G09\u0002\u001a/G0A/G09\u0016\u0007\u001c\u001d/G0A\u000b\b\u000b$\u0002\u001c\u0015\u0014\u000e\u001a\u0016 \u0007/G0A\u000b$1\u0002\u0002\u001e\u0014\u0002-\u0007\u0002/G0A/\u0014\u0007\u000b#\u0007\u000f\u0002\u000e/G09\u0002\u0012\u000e\u0010\u0015\u000b\u0007\u0002\u0003%\u0002\u000b\u0007\u000f\u0015\u001a\u0016\u000e#\u0007\u0002\u0014\u0007\u0006\u0007\u001a\u0016\u000e/G0A/G09%\u0002\b/G09\u0002\b\u001d\u001d\u000b/G0A\b\u001a \u0002- \u000e\u001a \u0002\u000e\u0014\u0002\u0007\u0014\u0014\u0007/G09\u0016\u000e\b\u0006\u0002\u0016/G0A\u0002\u001c\b/G09$.\u000e/G09\u000f\u0014\u0002/G0A/G0D\u0002\u001c\u0007\u0006/G0A\u000f\u000e\u001a\u0002\u0014\u0007\b\u000b\u001a \u000e/G09\u0010\u0002\u0018\f\u0007\u0006/G0D\u000b\u000e\u000f\u0010\u0007\u0011\u0012\u000e\u0007\u0006\u000f%\u00025&&C\u001b%\u0002\u001a\b/G09\u0002\u000e\u0016\u0014\u0007\u0006/G0D\u0002/\u0007\u0002\u001d\u000b\b\u001a\u0016\u000e\u001a\u0007\u000f\u0002\u000e/G09\u0002\u001c\b/G09$\u0002-\b$\u0014\u0002\b/G09\u000f\u0002\u000e\u0014\u0002\u0016 \u0015\u0014/G09/G0A\u0016\u0002\b/G09\u0002\b\u0015\u0016/G0A\u001c\b\u0016\u000e\u001a\u0002\u0014/G0A\u0006\u0015\u0016\u000e/G0A/G09\u0002\u0016/G0A\u0002\u0016 \u0007\u0002/G09\u0007\u0007\u000f\u0002/G0D/G0A\u000b\u0002\u000f\b\u0016\b\u0002\u0010\u0007/G09\u0007\u000b\b\u0006\u000e@\b\u0016\u000e/G0A/G091\n(1\u0002\u0013 \b\u0006\u0006\u0007/G09\u0010\u0007\u0014\u0002/G0D\u000b/G0A\u001c\u0002!\u0015\u001c\b/G09\u0011\u0012\b\u001a\u0016/G0A\u000b\u0014\u0002\u001f\u0007\u0014\u0007\b\u000b\u001a \n\u001f\u0007\u001a\u0007/G09\u0016\u0002 \u0015\u001c\b/G09\u0011\u0014\u0015/6\u0007\u001a\u0016\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a \u0002/G0A/G09\u0002\u0006\u000e\u0014\u0016\u0007/G09\u000e/G09\u0010\u0002 \b\u0014\u0002\u000b\u0007#\u0007\b\u0006\u0007\u000f\u0002\u0014/G0A\u001c\u0007\u0002\u0014\u0015\u000b\u001d\u000b\u000e\u0014\u000e/G09\u0010\u0002\u001a/G0A\u000b\u000b\u0007\u0006\b\u0016\u000e/G0A/G09\u0014\u0002\u0016 \b\u0016\u0002/\u0007\b\u000b\u0002/G0A/G09\u0014\u0007\b\u000b\u001a \u0002\u0014\u0016\u000b\b\u0016\u0007\u0010\u000e\u0007\u00141\u0002\u0002, \u0007\u0002/G0D/G0A\u0006\u0006/G0A-\u000e/G09\u0010\u0002\b\u000b\u0007\u0002/G0A/G09\u0006$\u0002\b\u0002/G0D\u0007-\u0002\u00073\b\u001c\u001d\u0006\u0007\u00141\n\u001e1\u0002\u001e\u001c/G0A/G09\u0010\u0002\u0015/G09\u0016\u000b\b\u000e/G09\u0007\u000f\u0002\u001d/G0A\u001d\u0015\u0006\b\u000b\u0011\u001c\u0015\u0014\u000e\u001a\u0002\u0007/G09\u0016 \u0015\u0014\u000e\b\u0014\u0016\u0014%\u0002\u0016\u0007\u001c\u001d/G0A\u0002\u001c\b$\u0002/\u0007\u0002\b\u0002\u001d\u000b\u000e/G09\u001a\u000e\u001d\b\u0006\u0002\u001a/G0A\u001c\u001d/G0A/G09\u0007/G09\u0016\u0002/G0A/G0D\nH\u0014\u0016$\u0006\u0007I\u0011\u0014\u000e\u001c\u000e\u0006\b\u000b\u000e\u0016$\u00026\u0015\u000f\u0010\u001c\u0007/G09\u0016\u00141\u0002\u0002,\u0007\u001c\u001d/G0A\u0002\u000e\u0014%\u0002/G0A/G0D\u0002\u001a/G0A\u0015\u000b\u0014\u0007%\u0002/G09/G0A\u0016\u0002\b/G09\u0002\b\u0016\u0016\u000b\u000e/\u0015\u0016\u0007\u0002/G0A/G0D\u0002\b\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002-/G0A\u000b.\u0002/\u0015\u0016\u000b\b\u0016 \u0007\u000b\u0002/G0A/G0D\u0002\u000e\u0016\u0014\u0002\u001d\u0007\u000b/G0D/G0A\u000b\u001c\b/G09\u001a\u0007\u0002\b/G09\u000f\u0002-\u000e\u0006\u0006\u0002/\u0007\u0002\u0014\u001d\u0007\u001a\u000e/G0D\u000e\u001a\u0002\u0016/G0A\u0002\u0007\b\u001a \u0002\u001d\u0007\u000b/G0D/G0A\u000b\u001c\b/G09\u001a\u00071\nF1\u0002\u0002, \u0007\u0002\u0006\u0007#\u0007\u0006\u0002/G0A/G0D\u0002#\u000e/\u000b\b\u0016/G0A\u0002\b/G0D/G0D\u0007\u001a\u0016\u0014\u00026\u0015\u000f\u0010\u001c\u0007/G09\u0016\u0014\u0002/G0A/G0D\u0002H\u00073\u001d\u000b\u0007\u0014\u0014\u000e/G0A/G09I\u0002\u000e/G09\u0002\u001d\u0007\u000b/G0D/G0A\u000b\u001c\b/G09\u001a\u0007\u0002\u0018\u001f\b\u001d/G0A\u001d/G0A\u000b\u0016\u00025&&+\u001b1\u0002\n, \u000e\u0014\u0002\u000e\u0014\u0002\u0014\u0015\u000b\u001d\u000b\u000e\u0014\u000e/G09\u0010\u0002/\u0007\u001a\b\u0015\u0014\u0007\u0002\u001d\u0007\u000f\b\u0010/G0A\u0010$\u0002\u001d\u000b\u0007\b\u001a \u0007\u0014\u0002\u0016 \b\u0016\u0002\u000f$/G09\b\u001c\u000e\u001a\u0014\u0002\b/G09\u000f\u0002\u0016\u0007\u001c\u001d/G0A\u0002\u001a \b/G09\u0010\u0007\u0002\b\u000b\u0007\u0002\b\u001c/G0A/G09\u0010\u0016 \u0007\u0002\u001d\u000b\u000e/G09\u001a\u000e\u001d\b\u0006\u0002\u001c\u0007\b/G09\u0014\u0002/G0A/G0D\u0002\u00073\u001d\u000b\u0007\u0014\u0014\u000e/G0A/G091\u0002\u0002P\u000e/\u000b\b\u0016/G0A\u0002\u000e\u0014\u0002\b\u0006\u0014/G0A\u0002\b\u00020\u0015\b\u0006\u000e\u0016$\u0002/G0A/G0D\u0002\u001d\u0007\u000b/G0D/G0A\u000b\u001c\b/G09\u001a\u0007%\u0002/G09/G0A\u0016\u001a/G0A\u001c\u001d/G0A\u0014\u000e\u0016\u000e/G0A/G091\u0002\n\u00131\u0002\u0002\f\u0016$\u0006\u0007\u0002\b/G09\u000f\u0002\u0010\u0007/G09\u000b\u0007\u00026\u0015\u000f\u0010\u001c\u0007/G09\u0016\u0014\u0002/G0A/G0D\u0002/\u000b/G0A\b\u000f\u001a\b\u0014\u0016\u0002\u001c\u0015\u0014\u000e\u001a\u0002\u001a\b/G09\u0002/\u0007\u0002\u001c\b\u000f\u0007\u0002-\u000e\u0016 \u0002’\u0004Q\u0002\b\u001a\u001a\u0015\u000b\b\u001a$\u0002\u000e/G09\u00021\u0004\u0003)\n\u0014\u0007\u001a/G0A/G09\u000f\u0014\u0002/$\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0006$\u0002H\u0015/G09\u0016\u000b\b\u000e/G09\u0007\u000fI\u0002\u0016\u0007\u0007/G09\b\u0010\u0007\u0002\u0014\u0015/6\u0007\u001a\u0016\u0014%\u0002\u000e/G0D\u0002\u0016 \u0007\u0002\u0016\u0007\u0014\u0016\u0002\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014\u0002\b\u000b\u0007\u0002/G0D\b\u001c\u000e\u0006\u000e\b\u000b\u0002\u0018\u0017\u0007\u000b\u000b/G0A\u0016\u0016\b/G09\u000f\u0002E6\u0007\u000b\u000f\u000e/G09\u0010\u0007/G09\u00025&&&\u001b1\u0002\u0002, \u000e\u0014\u0002\u000e\u0014\u0002\u0014\u0015\u000b\u001d\u000b\u000e\u0014\u000e/G09\u0010\u0002/\u0007\u001a\b\u0015\u0014\u0007\u0002\u0014\b\u001c\u001d\u0006\u0007\u0014\u0002\b\u000b\u0007\u0002\u0014 /G0A\u000b\u0016\u0007\u000b\u0002\u0016 \b/G09\u0002\u001c\b/G09$\u0002\u0014\u000e/G09\u0010\u0006\u0007\u0019:7:\u0002H\u0007#\u0007/G09\u0016\u00141I\u0002\u0002!\u0007\u000b\u0007\u0002 \u0015\u001c\b/G09\u0002\u001d\u0007\u000b\u001a\u0007\u001d\u0016\u0015\b\u0006\u0002\b/G09\u000f\u0002\u001a/G0A\u0010/G09\u000e\u0016\u000e#\u0007\u0002\b/\u000e\u0006\u000e\u0016\u000e\u0007\u0014\u0002\u0014\u0007\u0007\u001c\u0002\u0016/G0A\u0002\u000f\u0007/G0D$\u0002\u0016 \u0007\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006-\u000e\u0014\u000f/G0A\u001c\u0002/G0A/G0D\u0002\b\u0010\u0007\u00141\u0002, \u0007\u0002\u0010\u0007/G09\u0007\u000b\b\u0006\u0002\u000b\u0007\u0006\u000e\b/G09\u001a\u0007\u0002/G0A/G09\u0002H\u0007#\u0007/G09\u0016\u0014I\u0002\u0016/G0A\u0002\u000f\u0007\u0014\u001a\u000b\u000e/\u0007\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u001a/G0A/G09\u0016\u0007/G09\u0016\u0002\u000e/G09\u0002\u001a/G0A\u001c\u001d\u0015\u0016\u0007\u000b\b\u001d\u001d\u0006\u000e\u001a\b\u0016\u000e/G0A/G09\u0014\u0002\u001a/G0A\u0015\u0006\u000f\u0002/G0D\b\u000e\u0006\u0002\u000e/G0D\u0002\u000e\u0016\u0002\u001a\b/G09\u0002/\u0007\u0002\u0014 /G0A-/G09\u0002\u0016 \b\u0016\u0002\u0016 \u0007\u0002\u0016 \u000b\u0007\u0014 /G0A\u0006\u000f\u0002/G0D/G0A\u000b\u0002H\u0014\u0016$\u0006\u0007I\u0002\u000b\u0007\u001a/G0A\u0010/G09\u000e\u0016\u000e/G0A/G09\u0002\u000e\u0014\u0002\u001c/G0A\u000b\u0007\u0010\u000b\b/G09\u0015\u0006\b\u000b%\u0002/G0A\u000b\u0002\u001c/G0A\u000b\u0007\u0002\u000f\u0007#\u000e\b/G09\u0016%\u0002/G0A\u000b\u0002\u001c/G0A\u000b\u0007\u0002\u0014\u0015\u001d\u0007\u000b/G0D\u000e\u001a\u000e\b\u0006\u0002\u0016 \b/G09\u0002\u0016 \u0007\u0002\u0007\u0014\u0014\u0007/G09\u001a\u0007\u0002/G0A/G0D\u0002\u0016 \b\u0016\u0002\u000b\b/G09\u0010\u0007\u0002/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006H\u001a/G0A/G09\u0016\u0007/G09\u0016I\u0002- \u000e\u001a \u0002\u0007#\u0007/G09\u0016\u0014\u0002\u000f\u0007\u0014\u001a\u000b\u000e/\u00071\u0002\n$\u0007\u0017/G0D\u000e\f\u000b\b\u0007\u0017\n\u0019$\u0002\u001d\u0015\u000b\u001d/G0A\u0014\u0007\u0002\u000e/G09\u0002\u001a/G0A/G09\u0016\u000b\b\u0014\u0016\u000e/G09\u0010\u0002\u0016 \u0007\u0002\u000f\u000e#\u0007\u000b\u0010\u0007/G09\u0016\u0002\u0010/G0A\b\u0006\u0014\u0002/G0A/G0D\u0002//G0A\u0016 \u0002\u0007\b\u000b\u0006\u000e\u0007\u000b\u0002\b/G09\u000f\u0002\u001a\u0015\u000b\u000b\u0007/G09\u0016\u0002\u001d\u000b/G0A6\u0007\u001a\u0016\u0014\u0002 \b\u0014\u0002/\u0007\u0007/G09\u0002\u0016/G0A\n\u000f\u0007\u001c/G0A/G09\u0014\u0016\u000b\b\u0016\u0007\u0002 /G0A-\u0002/\u000b/G0A\b\u000f\u0002\u0016 \u0007\u0002\u000b\b/G09\u0010\u0007\u0002/G0A/G0D\u0002\u0010/G0A\b\u0006\u0014\u0002\u000e/G09\u0002\u001c\u0015\u0014\u000e\u001a\u00020\u0015\u0007\u000b$\u0002\u001c\b$\u0002/\u00071\u0002\u0002:\u0002 \b#\u0007\u0002\b\u0016\u0016\u0007\u001c\u001d\u0016\u0007\u000f\u0002\u0016/G0A\u0002\u0014 /G0A-\u0002 /G0A-\u001c\u0015\u001a \u00020\u0015\u0007\u000b$\u0002/G0A/6\u0007\u001a\u0016\u000e#\u0007\u0014\u0002\u001c\b$\u0002/\u0007\u0002\u001a/G0A\u0006/G0A\u000b\u0007\u000f\u0002\b/G09\u000f\u0002\u001a/G0A/G09\u0014\u0016\u000b\b\u000e/G09\u0007\u000f\u0002/$\u0002\u0016 \u0007\u0002\u000b\u0007\u001d\u0007\u000b\u0016/G0A\u000b\u000e\u0007\u0014\u0002/G0D\b\u001c\u000e\u0006\u000e\b\u000b\u0002\u0016/G0A\u0002\u000b\u0007\u0014\u0007\b\u000b\u001a \u0007\u000b\u00141\u0002\n, \u0007\u0002/G0A/G09\u0006$\u0002\u001a/G0A\u001c\u001c/G0A/G09\u0002\u0016 \u000b\u0007\b\u000f\u0002\u000e/G09\u0002\u001c\u0015\u0014\u000e\u001a\u00110\u0015\u0007\u000b$\u0002\u001c/G0A\u0016\u000e#\b\u0016\u000e/G0A/G09\u0014%\u0002/\u000b/G0A\b\u000f\u0006$\u0002\u000f\u0007/G0D\u000e/G09\u0007\u000f%\u0002\u000e\u0014\u0011\u0011\b\u0006\b\u0014\u0011\u0011 \u0015\u001c\b/G09\u0002\u001a\u0015\u000b\u000e/G0A\u0014\u000e\u0016$1\u0002\n\"/G09\u0006\u0007\u0014\u0014\u0002-\u0007\u0002\u0006/G0A\u0014\u0007\u0002\u0016 \b\u0016%\u0002\u000f\u0007\u0014\u000e\u0010/G09\u0007\u000b\u0014\u0002/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\u00110\u0015\u0007\u000b$\u0002\u0014/G0A/G0D\u0016-\b\u000b\u0007\u0002\u001a\b/G09\u0002\u00073\u001d\u0007\u001a\u0016\u0002\u0016/G0A\u0002\u001a\b\u0016\u0007\u000b\u0002/G0D/G0A\u000b\u0002\b/G09\u0002\u0015/G09\u0007/G09\u000f\u000e/G09\u0010\u0002\u0014\u0016\u000b\u0007\b\u001c\u0002/G0A/G0DH\u0014\u001d\u0007\u001a\u000e\b\u0006I\u0002/G09\u0007\u0007\u000f\u0014%\u0002\b\u0014\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u001d\u000b\u0007/G0D\u0007\u000b\u0007/G09\u001a\u0007\u0014\u0002\u001a/G0A/G09\u0016\u000e/G09\u0015\u0007\u0002\u0016/G0A\u0002\u0007#/G0A\u0006#\u0007\u0002\b/G09\u000f\u0002\u001a \b/G09\u0010\u00071\u0002\u0002&/G0A\u0013/G0A\u0010/G0A\u0017/G0D/G0A\u000b\n\u001e\b\u000b\u000f\u0007/G09%\u0002F\u000b\u0007\u0016%\u0002\b/G09\u000f\u00027\b#\u000e\u000f\u0002!\u0015\u000b/G0A/G09\u0002\u0018\u0003\u0004\u0004\u0004\u001b1\u0002\u0002 HE\u0007/G0A\u0010\u000b\b\u001d \u000e\u001a\b\u0006\u0002;/G0A\u001a\b\u0006\u000e@\b\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u0019\u0015\u0014\u000e\u001a\b\u0006\u0002\u0012\u0007\b\u0016\u0015\u000b\u0007\u0014J\n\u0019\b\u001d\u001d\u000e/G09\u0010\u0002\u0005\u0015\u000b/G0A\u001d\u0007\b/G09\u0002\u0012/G0A\u0006.\u0014/G0A/G09\u0010%I\u0002\u0013/G0A\u001c\u001d\u0015\u0016\u000e/G09\u0010\u0002\u000e/G09\u0002\u0019\u0015\u0014\u000e\u001a/G0A\u0006/G0A\u0010$\u00025\u0003\u0002\u00185&&&\u0011\u0003\u0004\u0004\u0004\u001b%\u000255)\u00115\u0003C</G0D/G0A\u000b\u0016 \u001a/G0A\u001c\u000e/G09\u0010=1\n\u0013/G0A\u0006\u0006\u000e/G09\u0014\u0002?\u0015\u000f\u000f%\u0002\u0013\u000b\u000e\u0014\u0016\u0006\u0007\u0002\u00185&&\u0003\u001b1\u0002\u0002H\u0019/G0A\u000f\b\u0006\u0002\u0016$\u001d\u0007\u0014\u0002\b/G09\u000f\u0002R\u0015\u0016%G\u0002R\u000b\u0007%G\u0002R\u001c\u000eG\u0002\u0016/G0A/G09\b\u0006\u000e\u0016\u000e\u0007\u0014J\u0002,/G0A/G09\b\u0006\u0002\u001a/G0A \u0007\u000b\u0007/G09\u001a\u0007\u0002\u000e/G09\u0002\u0014\b\u001a\u000b\u0007\u000f\n#/G0A\u001a\b\u0006\u0002\u001d/G0A\u0006$\u001d /G0A/G09$\u0002/G0D\u000b/G0A\u001c\u0002\b//G0A\u0015\u0016\u00025)\u0004\u0004%I\u0002 \u001b\b/G0A\u0012\u0011\u0005\u0014\u0003\b\u0004\u0003\u0007\u000e\u0002\u0003\u001c\u000f\u0002\u0012\u0010\u0006\u0005\u0011\u0003/G09/G0A\u000b\u0010\u0006\b\u0014\b\u001a\u0010\u0006\u0005\u0014\u0003\u001d\b\u0006\u0010\u0002\u0007\u001e \u0002’)4(%\u0002\u0002’\u0003C\u0011+*1\n7\b \u0006\u000e\u0010%\u0002\u0005-\b\u0002\u00185&&*\u001b1\u0002\u0002H, \u0007\u0002\u0005\u0014\u0014\u0007/G09\u0002\u001e\u0014\u0014/G0A\u001a\u000e\b\u0016\u000e#\u0007\u0002\u0013/G0A\u000f\u0007J\u0002\u001e\u0002\u0013/G0A\u000f\u0007\u0002/G0D/G0A\u000b\u0002\u0012/G0A\u0006.\u0014/G0A/G09\u0010\u0002\u001e/G09\b\u0006$\u0014\u000e\u0014I\u0002\u000e/G09\u0002\f\u0007\u0006/G0D\u000b\u000e\u000f\u0010\u0007\u0011\n\u0012\u000e\u0007\u0006\u000f\u0002\u00185&&*\u001b%\u0002(’(\u0011(+51\u0002\n\u0017\u0007\u000b\u000b/G0A\u0016\u0016%\u00027\b#\u000e\u000f%\u0002\b/G09\u000f\u0002\u001f/G0A/\u0007\u000b\u0016\u000291\u0002E6\u0007\u000b\u000f\u000e/G09\u0010\u0007/G09\u0002\u00185&&&\u001b1\u0002H\f\u001a\b/G09/G09\u000e/G09\u0010\u0002\u0016 \u0007\u00027\u000e\b\u0006J\u0002\u001e/G09\u0002\u00073\u001d\u0006/G0A\u000b\b\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002/G0D\b\u001a\u0016/G0A\u000b\u0014\u0002\u000e/G09\n\u0016 \u0007\u0002\u000e\u000f\u0007/G09\u0016\u000e/G0D\u000e\u001a\b\u0016\u000e/G0A/G09\u0002/G0A/G0D\u0002\u001c\u0015\u0014\u000e\u001a\b\u0006\u0002\u0014\u0016$\u0006\u0007%I\u0002 \u001d\b\u0006\u0010\u0002\u0007\u001e\u0003\u0004\b\u0012\u0003/G09/G0A\u000b\u0010\u0006\u0003\u001f\u0002\u0012\u0006\u0002\u0016\u0007\u0010\b\u0011\u0003\u0005\u0011\u0001\u0003\u0017\b\u001a\u0011\u0010\u0007\u0010\b\u0011 \u0003\u001c\u0011\u0011/G0A\u0005\u0014\u0003/G09\u0002\u0002\u0007\u0010\u0011\u001a\n!\"###$%\u0002\u001c\u0019\u000b\u0007\u0012\u0005\u0006\u0007\u000b%\u0002\u0018\u0013 \u000e\u001a\b\u0010/G0AJ\u00022/G0A\u000b\u0016 -\u0007\u0014\u0016\u0007\u000b/G09\u0002\"/G09\u000e#\u0007\u000b\u0014\u000e\u0016$%\u0002\u0016$\u001d\u0007\u0014\u001a\u000b\u000e\u001d\u0016\u001b%\u0002\u001d1\u0002CC1\n\u0017/G0A-\u0007\u000b\u0014%\u0002!\b\u000b\u000b$\u0002\u00185&C5\u001b1\u0002\u0002H,/G0A/G09\b\u0006\u0002,$\u001d\u0007\u0014\u0002\b/G09\u000f\u0002\u0019/G0A\u000f\b\u0006\u0002\u0013\b\u0016\u0007\u0010/G0A\u000b\u000e\u0007\u0014%I\u0002 \u001b\b/G0A\u0012\u0011\u0005\u0014\u0003\b\u0004\u0003\u0007\u000e\u0002\u0003\u001c\u000f\u0002\u0012\u0010\u0006\u0005\u0011\u0003/G09/G0A\u000b\u0010\u0006\b\u0014\b\u001a\u0010\u0006\u0005\u0014\u0003\u001d\b\u0006\u0010\u0002\u0007\u001e\n(’4(%\u0002\u001d\u001d1\u0002’\u0003C\u0011’*\u00041\n\u001f\b\u001d/G0A\u001d/G0A\u000b\u0016%\u0002\u0005\u0006\u000e\u0007@\u0007\u000b\u0002\u00185&&+\u001b1\u0002\u0002H\u0005\u001c/G0A\u0016\u000e/G0A/G09\b\u0006\u0002\u00053\u001d\u000b\u0007\u0014\u0014\u000e/G0A/G09\u0002\u0013/G0A\u000f\u0007\u0002\u000e/G09\u00029\u001d\u0007\u000b\b\u0002\b/G09\u000f\u0002;\u000e\u0007\u000f\u0002\f\u000e/G09\u0010\u000e/G09\u0010%I\u0002 \u001b\b/G0A\u0012\u0011\u0005\u0014\u0003\b\u0004\u0003%\u0002&\n/G09/G0A\u000b\u0010\u0006\u0003’\u0002\u000b\u0002\u0005\u0012\u0006\u000e \u0002\u0003)%\u00025\u0004&\u0011’&1\n\f\u0007\u0006/G0D\u000b\u000e\u000f\u0010\u0007\u0011\u0012\u000e\u0007\u0006\u000f%\u0002\u0005\u0006\u0007\b/G09/G0A\u000b%\u0002\u0007\u000f1\u0002\u00185&&*\u001b1\u0002\u0002 (\u0002\u001e\b\u0011\u0001\u0003/G09)\f) \u0003/G0D\u000e\u0002\u0003*\u0005\u0011\u0001\u0019\b\b+\u0003\b\u0004\u0003/G09/G0A\u000b\u0010\u0006\u0005\u0014\u0003\u0017\b\u0001\u0002\u000b 1\u0002\u0002\u0013\b\u001c/\u000b\u000e\u000f\u0010\u0007J\u0002, \u0007\n\u0019:,\u0002\u0017\u000b\u0007\u0014\u00141\n\f\u0007\u0006/G0D\u000b\u000e\u000f\u0010\u0007\u0011\u0012\u000e\u0007\u0006\u000f%\u0002\u0005\u0006\u0007\b/G09/G0A\u000b\u0002\u00185&&C\u001b1\u0002\u0002H\u0013/G0A/G09\u001a\u0007\u001d\u0016\u0015\b\u0006\u0002\b/G09\u000f\u0002\u001f\u0007\u001d\u000b\u0007\u0014\u0007/G09\u0016\b\u0016\u000e/G0A/G09\b\u0006\u0002:\u0014\u0014\u0015\u0007\u0014\u0002\u000e/G09\u0002\u0019\u0007\u0006/G0A\u000f\u000e\u001a\u0002\u0013/G0A\u001c\u001d\b\u000b\u000e\u0014/G0A/G09I\n\u000e/G09\u0002/G09\u0002\u0014\b\u0001\u0010\u0006\u0003\u001d\u0010\u000f\u0010\u0014\u0005\u0012\u0010\u0007\u001e \u0003\u0017\b\u0011\u0006\u0002\u0016\u0007\u000b,\u0003\u001f\u0012\b\u0006\u0002\u0001/G0A\u0012\u0002\u000b,\u0003\u0005\u0011\u0001\u0003\u001c\u0016\u0016\u0014\u0010\u0006\u0005\u0007\u0010\b\u0011\u000b \u0002\u0018\u0013/G0A\u001c\u001d\u0015\u0016\u000e/G09\u0010\u0002\u000e/G09\u0002\u0019\u0015\u0014\u000e\u001a/G0A\u0006/G0A\u0010$\u000255\u001b%\u0002\u0007\u000f1\nA\b\u0006\u0016\u0007\u000b\u0002F1\u0002!\u0007-\u0006\u0007\u0016\u0016\u0002\b/G09\u000f\u0002\u0005\u0006\u0007\b/G09/G0A\u000b\u0002\f\u0007\u0006/G0D\u000b\u000e\u000f\u0010\u0007\u0011\u0012\u000e\u0007\u0006\u000f\u0002\u0018\u0013\b\u001c/\u000b\u000e\u000f\u0010\u0007J\u0002\u0019:,\u0002\u0017\u000b\u0007\u0014\u0014\u001b%\u0002(\u0011+’1\n\"\u001f;\u0014J\n/G09/G0A\u000b\u0002\f\u0005\u0007\u0005JL\u0002\u001d\u001d\u001d\u0016/G0D/G0D\u0003\u0010\u0002\u0016\u0007\u0010’\u001b\u0015\f\u000b/G0A\u0018\u0003\u0004\u0003\u001b\n/G0D\u000e\u0002\u000f\u0002-\u0010\u0011\u0001\u0002\u0012 JL\u0002\u001d\u001d\u001d\u0016\u0004\u0002/G0A\u0015/G0A\u0013\b\u0017\u0018/G0A\u0010\u0016\u0007\u0010’\u001b"
    },
    {
        "title": "Music IR: Past, Present, and Future.",
        "author": [
            "Alexandra L. Uitdenbogerd"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.7595459",
        "url": "https://doi.org/10.5281/zenodo.7595459",
        "ee": "https://zenodo.org/records/7595459/files/Traditions ISBN 978-81-956242-0-1 (1)-201-214.pdf",
        "abstract": "Arts education in School curriculum: Present and Future Trends and Issues\n\nThe National education policy, 2020, stresses the promotion of holistic and multidisciplinary education to develop the intellectual, aesthetic, social, physical, emotional and moral capacity of learners. Arts have had an important role to play both as a subject to promote artistic, aesthetic and creative skills and also as a pedagogical tool for experiential and joyful learning among the K 12 students. The NEP has emphatically stressed the importance of a learner outcome-based curriculum framework concerning curricular expectations. The present research examines the national school curriculum in arts against this background. The paper explores current trends in combining learner outcomes with assessment and envisages a standardized framework for arts education in the country.",
        "zenodo_id": 7595459,
        "dblp_key": "conf/ismir/Uitdenbogerd00",
        "keywords": [
            "Holistic education",
            "Multidisciplinary education",
            "Intellectual development",
            "Aesthetic skills",
            "Creative skills",
            "Pedagogical tool",
            "K-12 students",
            "Learner outcome-based curriculum",
            "Curricular expectations",
            "Assessment"
        ],
        "content": "Traditions of Indian Classical Music and Dance  \n \n197 \n Arts education in School curriculum:  Present and Future Trends and Issues  \nAuthor  \nDr. Radhika Puthenedam  \nAssistant Professor (III)  \nSchool of Performing and Visual Arts  \nIndira Gandhi National Open University,  \nNew Delhi - 110068  \nAbstract  \nThe National education policy, 2020, stresses the promotion of holistic and multi -\ndisciplinary education to develop the intellectual, aesthetic, social, physical, emotional and \nmoral capacity of learners. Arts have had an important role to play both as a s ubject to promote \nartistic, aesthetic and creative skills and also as a pedagogical tool for experiential and joyful \nlearning among the K 12 students. The NEP has emphatically stressed the importance of a \nlearner outcome -based curriculum framework concerni ng curricular expectations. The present \nresearch examines the national school curriculum in arts against this background. The paper  \nexplores current trends in combining learner outcomes with assessment and envisages a \nstandardized framework for arts educat ion in the country.  \nKey Words : Arts education, learning outcomes, NEP 2020, standards in arts learning  \nIntroduction  \nArts play a very important role in the holistic development of an individual and are \nconsidered an essential component of comprehensive edu cation, particularly among school \nchildren. The present school curriculum framework stresses that every student is given an equal \nopportunity to participate in art activities, take an active part in the art -making or doing \nprocesses and be encouraged to ex press themselves. The National education policy 2020, \nstresses on the promotion of holistic and multi -disciplinary education for capacity building in \nintellectual, aesthetic, social, physical, emotional and moral spheres. It upholds UNESCO’s \ninitiative on global citizenship education and 21st-century skills or competencies required for the ISBN: 978 -81-956242 -0-1 \n \n198 \n new age knowledge society with emphasis on creating “global citizens with global \nconsciousness and global competence deeply connected with Indian roots” 1. The role of arts is \nvery pertinent here not only as a marker of the quintessential cultural ethos of our country but \nalso for providing a seamless ground for enabling better competency -based education in \nteaching -learning approaches from the schooling ye ars itself. As Dewey remarked, arts are “not \nluxuries of education, but emphatic expressions of that which makes any education worthwhile” \n(1916, 247).  Education in arts or through arts has been shown to balance and develop \nconceptual, creative and critic al skills in students thereby providing a seamless ground and \nmaking the learner better equipped from the schooling years itself. The presence of arts in \neducation as arts for art’s sake or as arts as an instrument for the better educational outcome \nhas to  take into account the very nature and purpose of arts and its role in the holistic support \nand development of a student. Moreover, the idea of “art for our sake” as Hetland and Heller \nremarked is also quite pertinent as they say, “people hunger for art be cause it allows them to \nconnect the rational with the intuitive, the brain and the body... It allows them to express a sense \nof the whole human being” (2017)  \nThe various national curriculum frameworks, from 1975 to 2000, had noted the need for \ndeveloping o f aesthetic sensibility and free expression, awareness and appreciation of national \nheritage using art education. The NCF 2005 stressed the point that education in performing and \nvisual arts or arts education should be imparted as one of the compulsory sub jects from pre -\nprimary to secondary stages of school education (NCERT,2006). Also, as per the new \nassessment reforms (2021) that have shifted the emphasis from content -based assessment to \ncompetency -based assessment, the focus is on high -order thinking ski lls in the learning \noutcome with an emphasis on critical thinking. This is where the competency -based, also known \nas a performance -based assessment, becomes primarily linked to creating competency or \nperformance -based learning outcomes that can then be ass essed.  In this background, the \npresent research analysis has looked at the arts education pedagogy in India concerning the \n \n1 Talk by Prof.D.P.Singh, Chairman,University Grants Commission,Visitor’s conference: Implementation of National \neducational policy 2020, 21 Sep 2020. Held virtually.   Traditions of Indian Classical Music and Dance  \n \n199 \n content standard framework adopted at the national level since 2005 and examines it against \nthe prominent performance -based arts sta ndards available in art education in India and abroad \nand the new pedagogic formulations adopted in NEP 2020. Based on this, the paper has made \nsuggestions for adopting content standards based on art processes to effectively implement \nperformance -based lea rning outcomes as a necessity for arts education across all the arts \ndisciplines in the country. It looks at taking a step ahead by exploring current trends in \ncombining learner outcomes with assessment and envisages a standardized framework for arts \neduca tion in the country.  The objectives of this research are to  \n1. analyse and evaluate the available frameworks of art education,  \n2. discuss the need for standards in learner outcomes /curriculum expectations  \n3. discuss the scope of the outcomes -based framework and compare it with other \nsignificant model frameworks finally  \n4. suggest standards and criteria for a successful arts education pr ogramme.  \nLearning approaches  \nThe kinesthetic learning theories of Dewey (1915) stressed on ‘learning by doing’ \napproach through experiential learning and brought focus towards the importance of art \neducation and art -integrated learning. Other constructivi st instructional theorists like Jerome \nBruner (1960), Jean Piaget (1974) and Vygotsky (1978) too gave a learner -centric approach to \neducation and focussed on the importance of the learner in constructing their knowledge. It \nchanged the behaviourist view th at treated the teacher as a dispenser of knowledge to the \nteacher as a facilitator who helps students acquire understandings and put them to individual \nuse. The NCF (2005), gives ample scope for art education on these lines and recommends that \n“emphasis sh ould be given to learning rather than teaching, and should use participatory, \ninteractive and experiential approaches rather than instructive because the goal of art education \nis to promote aesthetic and personal awareness and the ability to express onesel f in different \nforms”. Further, the multiple intelligences theory of Gardner (1983) suggests that individuals ISBN: 978 -81-956242 -0-1 \n \n200 \n may possess many different kinds of intelligence, showing the importance of art -integrated \nlearning that has gained new grounds.  \nThe constructivi st learning approach brought the focus on the learner and the \nexperiential theories, upon the learning processes. The need for 21st-century skills in a \nglobalised world has shown the need for core competencies or skill frameworks namely \ncollaboration, crea tivity, critical thinking and communication. In this regard, the NEP 2020 \nassessment reforms2 recommend a shift from content -based assessment to competency -based \nassessment with emphasis on testing high -order thinking skills, based on the achievement of \nessential learning outcomes or acquiring minimal standards of competence. This draws our \nfocus to the development of ‘outcomes -based assessment’ that is inevitable shortly. The \ndevelopments in curriculum concerning learner outcomes and different frameworks about \ncontent and pedagogical standards employed in the national school curriculum over the years \nare analysed in the following section.  \nCurriculum developments  \nArts education consists of ‘learning in the arts’ and ‘learning through the arts’, which can \nbe effectively overlapped in the context of school education. As CBSE (2020) notes, art \neducation is meant to be a precursor to art integrated learning. It points out that art education \nentails instruction in various art forms (visual as well as performing) to help children develop an \ninterest in arts and encourage them to enthusiastically participate in related activities, thus, \npromoting abilities such as imagination, creativity, valuing arts and cultural heritage at the same \ntime help in inculcating art -based enquiry skills in children. Learning through the arts is said to \npromote a joyful and creative learning experience. NEP 2020, on the topic of curriculum and \npedagogy in schools, states that learning should be holistic, integrated, enjoyable and engagin g \nwith an emphasis on experiential learning. It gives importance to art integration as one of the \n \n2 NEP 2020 Assessment reforms, NCERT. Accessed 28th Feb 2021 \nhttps://www.education.gov.in/shikshakparv/docs/Examination_and_Assessment_Reforms.pdf  Traditions of Indian Classical Music and Dance  \n \n201 \n standard pedagogies within each subject, and with explorations of relations among different \nsubjects as part of experiential learning.   \nThe CBSE and NCERT Art education approaches  \nA two -pronged approach has been adopted by CBSE for the implementation of art \ncurricula in schools for classes 1 -12. It says,   \n(i) “Art education will continue to be an integral part of the curriculum, as a co -scholastic \narea ti ll secondary level. The schools may also promote and offer visual and performing \narts-based subjects at the secondary and senior secondary level  \n(ii) Art shall be integrated with the teaching and learning process of all academic subjects \nfrom classes 1 -12, to p romote active and experiential learning for connecting knowledge \nto life outside the school, ensuring that learning shifts away from rote methods and for \nenriching the curriculum, so that it goes beyond textbooks” (2020)   \nAccording to CBSE’s document on A rt education, the different art forms under the \numbrella of creative education are the two major domains –Visual and Performing arts, \ndifferentiated mainly by the mode of expression and the tools used. The performing arts are \nsometimes further classified as  movement arts, musical arts including music (vocal and \ninstrumental), movement and dance and creative drama and puppetry. Visual arts too \nsometimes include components of media arts and craft making at schools apart from two -\ndimensional or Pictorial arts t hat includes drawing and painting, collage making, printing, \nphotography and computer graphics and three -dimensional artworks that include clay modelling \nand pottery, carving and sculpture  \nBased on the developmental stages of a child, NCERT’s curriculum o f art education \n(2006, ) suggests different ways in which art education is to be followed in schools.  ISBN: 978 -81-956242 -0-1 \n \n202 \n Pre-primary: The visual and performing art is taught in a fully integrated manner at this stage. All \nsubjects are taught through drawing, painting, clay modelling, role -play, dance, story -telling, \nsinging, etc.   \nPrimary stage (1 -5 classes): Art is a mediu m of self -expression at this stage and art education is \naimed at ‘promoting self -expression, creativity, sense of freedom and thus, psychological \nhealth’.  \nUpper -Primary stage (6 -8 classes): In these classes, children appreciate different arts - both \nclassic al and folk - involving both aesthetic appreciation and knowledge of the art. Art education \nemphasizes students’ participation in cultural activities and to become conscious of the rich \nheritage of the nation.  \nSecondary stage (9 -10 classes): Art is treated as a specialized subject and art education at this \nstage comprises the study of visual and aural resources and their exploration; projects to \npromote creative expression and exhibition of their works, exploration of traditional art forms of \nthe community   \nHigher Secondary stage (11 & 12th classes): Art is treated as a discipline from now on. This \nstage provides knowledge of aesthetics, criticism, art history, exploring career options in the arts \nand understanding the roles and their contribution to society .  \nThe DBAE and NCCAS frameworks of Art Education  \nThe Discipline -Based Art Education (DBAE, 1991), developed by the Getty centre for \neducation in the Arts in 1982 focuses on a systematic and sequential program for studying art \nhistory, art criticism, aesth etics and art production. It is a comprehensive art education approach \nthat attempts to develop students’ ability to understand and appreciate art using knowledge of \ntheories and contexts of art, and to respond to and create art. This includes four discipl ines or \ncomponents - art production, art history, art criticism and aesthetics  Traditions of Indian Classical Music and Dance  \n \n203 \n The National core arts standards (NCCAS, 2014) of the USA’s framework for advancing \nstudents’ artistic understanding that is followed across the majority of the states in America  is a \ncase in point. The NCCAS arts standards have developed common core arts standards in line \nwith international arts standards. It consists of an important component known as the anchor \nstandards. As the document points out, “there are 11 of these stand ards that describe the \ngeneral knowledge and skill that teachers expect students to demonstrate throughout their \neducation in the arts and serve as tangible expressions of artistic literacy. In each stage, \nstudents build creative habits as they learn to   \n1. Generate and conceptualize artistic ideas and work  \n2. Organize and develop ideas and work  \n3. Refine and complete artistic work  \n4. Select, analyze and interpret artistic work for presentation  \n5. Develop and refine artistic techniques and work for presentation  \n6. Convey meaning through the presentation of artistic work  \n7. Perceive and analyze artistic work  \n8. Interpret intent and meaning  \n9. Evaluate artistic work by applying criteria  \n10. Make art by synthesizing and relating knowledge and personal experiences  \n11. Deep en understanding by relating artistic ideas to societal, historical and cultural \ncontexts”  \nThis structure helps to translate artistic learning in the developmental stages of children \nfrom pre -school through high school into specific and measurable learning  goals. It is an \noutcomes -based approach to teaching and learning in the arts that is based on four artistic \nprocesses - creating, presenting/performing, producing and finally responding, articulated by the \nfive different arts disciplines - dance, music, the atre, visual arts and media arts.  The artistic \nprocesses highlighted below correspond to the different strands of standards:  ISBN: 978 -81-956242 -0-1 \n \n204 \n Standards 1 -3 concern with Creating; 4-6 standards are about Presenting/ Performing \n/Producing;  7-9 correspond to Responding  and the final stages 10 &11 involve Connecting.   \n \nAnalysis and Discussion of frameworks of learning  \nThe syllabus of arts education in the NCERT 2008 syllabus had brought forth curricular \nexpectations and learning expectations together concerning the content t aught under different \nheadings as given in table -1. Till class five, an integrated learning approach is used where the \nusage of arts is in a composite way with a common framework for classes I -V with two content \nstandards as given below  \n1. class I and II - sub-themes in languages and maths   \n2. class III to V - themes in languages, maths and environmental studies  \nTable -1 \nTheme and questions  Objectives  Suggested \nactivities  Suggested resources/notes for the \nteacher  \n \nFor upper primary classes (VI -VIII) and secondary classes (VIII -X), the syllabus of arts \neducation in areas of visual arts, theatre, music and dance is designed to have a content -\noriented framework as illustrated in table -2 with rare inclusion of learner o bjectives (theatre: XI -\nXII classes) and a common set of learner outcomes (visual arts: VI -X classes) or content -based \nlearning outcomes for class groups (dance: VI -VIII and VIII -X classes and theatre: VI -VIII).  \nTable -2 \nTopic/theme  Contents  Activities/ process/ways of \napplication/methods &materials  Suggestions/notes for \nthe teacher   \n   \nIn its document on ‘Learning indicators and learning outcomes’, NCERT (2014) \ncorrelates learning indicators with curricular expectations/learning outcomes and pedagogical Traditions of Indian Classical Music and Dance  \n \n205 \n processes. Learning indicators are different attributes that help to assess a child a nd decide \ntheir learning outcomes. Here, common learning outcomes are suggested for every subject at \neach school stage. Moreover, learning domains that are processes in knowledge acquisition \nbecome parameters for specific learning outcomes/ curricular expe ctations and pedagogical \nprocesses for these stages, though it has not been the case with all the subjects. Learning \nindicators for these learning processes further help to show progression in learning in every \nclass.  In the case of art education, apart f rom common learning outcomes for upper -primary \n(VI-VIII), the common parameters on which pedagogical processes and learning indicators are \nlisted are:   \n1. understand the basics of different art forms;  \n2. develop artistic skills;  \n3. appreciate different art forms;   \n4. awareness of the cultural diversity of the country;  \n5. self-expression and creativity.  \n  The first of the common standard arts process given above are evaluated in Table 3 \nagainst the learning outcomes listed for each subject domain.  \nTable -3 \n1. Understand the basics of different art forms  \nSubject Domains  Learning Outcomes  \nVisual arts and \ncrafts  \n Elements of art and design; Line, colour, form, texture, space, tone, \nharmony, balance and rhythm  \nTechniques of painting, sculpture and other three -dimensional art \nforms  \nMusic  Exposure to many types of Music – classical, semi -classical, regional  ISBN: 978 -81-956242 -0-1 \n \n206 \n  Singing and playing musical instruments in tune and rhythm  \nIdentifying the diverse type of Music prevalent in the country  \nRecognising different time measures /chhand /taal  \nTheatre  \n Experimenting with different types of sounds and movements from the \nimmediate environment/surroundings to enhance the following skills:  \nObservation, Listening, Comprehension, Expression, Performance, \nSelf-exploration Movement and Mime  \nDance  \n Warm -up movements to prepare the learner for dance  \nAlert mind  \nBalance of the body for an aesthetic posture required in dance  \nFlexible body to be able to create many types of movement  \nUse of all parts of the body to express creatively, aesthetically  \n \n  Though the domains act as standard processes/ outcomes, the specific learner \noutcomes under each of these standard domains are largely varied. They are process -oriented \n(theatre, music) but still, content -based as in the case of dance and visual arts. The learner \noutcomes as processes too are content specific and do not reflect the levels of learning or \ncognition and therefore provide inadequate sequential standards in the learning outcomes.  \nThe latest CCE document (2019) states that “the pedagogy employed  to accomplish the \ndesired goals of learning, should be such that it emphasises the processes of learning specific \nto each curricular area”. Assessment and teaching are complementary to each other and so it is \nonly natural that processes of assessment focu s on understanding how children learn which is \nexplicated in their corresponding learning outcomes. Table 4 shows the template integrating Traditions of Indian Classical Music and Dance  \n \n207 \n learning outcomes with teaching -learning and assessment as recommended in the new CCE \ndocument.  \nTable -4 \nIntegrating learning outcomes with teaching -learning and assessment  \nLearning  \nOutcomes  Suggested \nPedagogical  \nProcesses  Assessment  \nStrategies  Data  \nSources   \nThe learner outcomes in the current matrix include processes that are content specific \nand hence not entirely focussed on the processes of learning applicable to the domains of the \nsubject. The point here is that if learner outcomes continue to be dependent on the contents of \nthe subject, the question of recognising stages of learning and cognition is sel ective upon the \nchosen theme and content. The scope for developing standards of stages as in ‘remembering’, \n‘understanding’, ‘’applying’, ‘analysing’, ‘evaluating’ and finally ‘creating’ as prescribed in \nBloom’s taxonomy, has not been effectively implement ed in such a learning programme. If there \nare to be standards to be upheld in every subject that is dealt with, the learning processes need \nto tag their line with effective stages of learning. As the NEP outlines, testing to focus on the \nachievement of cri tical or essential learning outcomes is to become an important part of \nexaminations in the primary and secondary stages of learning3. Critical learning outcomes are \nthe major processes associated with learning that promote critical thinking, conceptual cl arity \nand analytical skills. The new assessment reforms that are to shift focus to testing core \nconcepts, high -order thinking skills and foundational skills also have to have a clear bearing on \ntheir respective learning outcomes. With assessment strategies  changing from content -based \nassessment to competency -based assessment, it is time that the content -based criteria also \nchange to outcomes -based assessment. Therefore, the present practice of learning outcomes \nthat is content/curriculum -based learner outco mes has to change its focus to process -based \nlearning outcomes where content areas are to be redesignated under domains of art learning. \n \n3 -do- ISBN: 978 -81-956242 -0-1 \n \n208 \n These, which are largely major learning processes or domains in learning and specific learning \noutcomes too are to refl ect sequential process standards. Such a framework would provide \nstandards in learning outcomes for every topic or content in a subject and also help to offer a \nstandardised frame of learning outcomes that can be adopted and adapted for every class.  \n \nStandards in art education thus have an important role to play in guiding the design of \ncurriculum, instruction and assessment. Standards can be  \n1. sequential to reflect the learning stages and also be  \n2. process / domain based to blend -in aspects of content and at the same time be  \n3. effective common core outcomes of teaching -learning.  \nStandards in art education serve two purposes. They   \n● act as verifiable standards be it in learning outcomes or assessment and  \n● help to offer a standardised framework for all the disciplines across art disciplines \n(Puthenedam, 2021)  \n \nConclusion  \nThe national curriculum frameworks in arts education consist of components for \ndeveloping aesthetic sensibility, free expression, awareness and appreciation of national \nheritage, and using arts in integrative learning. The paper has looked at developments in art \npedagogy regarding the standards of curriculum and learner expectations. The various \nframeworks analysed here have shown the trends and issues pertaining to locating learner \noutcomes in curriculum development. Education in arts should reflect the go als of arts practice. \nIt must prepare our children with knowledge in artistic skills to help them to analyse the \nsignificance of artworks, to evaluate and engage in the creative process and to become aware \nof the cultural and historical background of the a rts- both ancient and modern. This research \nhas shown the need and importance of standards in art education and has delineated its scope \nin creating new outcome -based common core frameworks. The criteria, as given below, are vital Traditions of Indian Classical Music and Dance  \n \n209 \n to fulfilling the require ments of a successful arts education programme and envisage the \nproposed measures for a new arts education standards framework that will include       \n• Designing common artistic standards across all disciplines  \n• Adopting standards for imparting knowledge an d skills in art disciplines according to the \ncognitive development of children during school years.  \n• Focussing major outcomes on core concepts, higher order and foundational skills  \n• Provision of a framework for discipline -wise performance/assessment standards in \nassessing the artistic processes adopted  \n• Evolving outcomes -based assessment through a comprehensive standards framework \ninvolving both outcomes/process standards and assessme nt standards.  \nPolicy documents  \n1. Arts Education Document (Code 502), CBSE, New Delhi   \nhttp://cbseacademic.nic.in/web_material/CurriculumMain20/Coscholastic/ART_EDUCATION.\npdf \n2. Arts Integration, CBSE, 2019   \nhttp://cbseacademic.nic.in/web_material/Circulars/ 2019/art_integration.pdf  \n3. Continuous and Comprehensive evaluation guidelines, NCERT, 2019  \nhttps://ncert.nic.in/pdf/announcement/CCE -Guidelines.pdf  \n4. Learning indicators and Learning outcomes at the Elementary stage, NCERT, 2014  \nhttp://www.dsek.nic.in/misc/learningoutcome.pdf  \n5. National Core Arts Standards: A Conceptual Frameworks for Arts Learning, National \nCoalition for Core Arts Standards, USA. 2014  \nhttps://www.nationalartsstandards.org/sites/default/files/Conceptual%20Framework% 2007 -\n21-16_0.pdf  \n6. Position Paper - National Focus group on Arts, Music, Dance and Theatre, NCERT, 2006  \n     https://ncert.nic.in/pdf/focus -group/art_education.pdf  \n7. Secondary School curriculum: 2020 -21, CBSE, New Delhi.2020  ISBN: 978 -81-956242 -0-1 \n \n210 \n http://cbseacademic.nic.in/web_mate rial/CurriculumMain21/Main -\nSecondary/Intitial_pages_sec_2020 -21.pdf  \n8. Syllabus of Arts Education, NCERT, New Delhi,2008  \n     https://ncert.nic.in/pdf/syllabus/Art_Educationfinal_syllabus.pdf  \n \nBibliography  \n1. Bruner, Jerome. The Process of Education. Vintage, NewYork,1960  \n2. Dewey, J. The school and society. Chicago: The University of Chicago Press, 1915.  \n3. Dewey,,J. Democracy and Education: An Introduction to the Philosophy of Education. \nNew York: Macmillan. 1916  \n4. Gardner, H. Frames of mind: The theory of multiple intelligences. New York: Basic \nBooks. 1983.  \n5. Hetland, Lois and Rafael Heller. 2017. On the Goals and Outcom es of Arts Education: \nAn Interview with Lois Hetland. The Phi Delta Kappan.  Vol. 98, No. 7. pp. 15 –20. \nJSTOR, http://www.jstor.org/stable/26386929.  Accessed 28th Feb 2021  \n6. Piaget, Jean. To Understand is to Invent: The Future of Education. New York: Viking, \n1974.  \n7. Puthenedam, Radhika. Perfor ming and Visual Arts. Eds Life Skills education. New Delhi: \nIndira Gandhi National Open University, 2021  \n8. The J.Paul  Getty Trust. Discipline based Art education: A curriculum sampler. Eds Kay, \nAlexander & Day, Michael. California: Getty Books, 1991,2001. Accessed 28th Feb 2021  \n9. http://www.g etty.edu/publications/virtuallibrary/0892361719.html   \n10. Vygotsky, Lev S. Mind in Society. Cambridge: Harvard University Press, 1978."
    },
    {
        "title": "Music Ranking Techniques Evaluated.",
        "author": [
            "Alexandra L. Uitdenbogerd",
            "Justin Zobel"
        ],
        "year": "2000",
        "doi": "10.5281/zenodo.1414990",
        "url": "https://doi.org/10.5281/zenodo.1414990",
        "ee": "https://zenodo.org/records/1414990/files/UitdenbogerdZ00.pdf",
        "abstract": "Several techniques have been proposed for matching melody queries to stored music. In previous work [2], we found that local alignment, a technique derived from bioinformatics, was more effective than the n-gram methods derived from information retrieval that are used in other work. In this paper we explore a broader range of n-gram techniques, and test them with both manual queries and queries automatically extracted from MIDI files. Our experiments show that alternative - indeed, simpler - n-gram matching techniques than those tested in earlier work can be as effective as local alignment; one highly effective technique is to simply count the number of 5-grams in common between query and stored piece of music. N-grams are particularly effective for short queries and manual queries, while local alignment is superior for the automatic queries. In our experiments we have used a database of 10 466 standard MIDI files down loaded from the internet. These were processed to extract their melodies as described in [1]. In that earlier paper, we used volunteer listeners to judge a small set of extracted melodies. We discovered that our simple algorithm that chooses notes starting at each instance to be melody notes and that we have named all mono, was the most successful in extracting melodies. However, we continued to test all of our melody extraction algorithms in our second paper [2] in the context of melodic similarity measurement. In this experiment we used three methods of melodic standardisation to produce the representations that would be used for matching. They were: contour, which uses a three-symbol alphabet to represent where a melody goes up, down or stays the same; modulo-12 interval (pitch distance between notes), which keeps the size of intervals and their direction but reduced to one octave; and exact interval, which keeps the exact interval size and direction.  All these represented pitch information only. A set of 28 automatically extracted melody queries were used to locate other versions of the same piece of music. The results were evaluated by computing the eleven-point precision average and the precision at a recall level of 20. Again the all mono method was successful, as were the entropy channel and all channels methods. The entropy channel method applies the all mono technique to each channel and then selects the melody channel based on its first order predictive entropy. The all channels technique also applies the all mono algorithm to each channel and then retains them all for the matching process. In addition, we confirmed that contour was insufficient for good matching, but that the use of interval sequences worked well.  Queries of 30 intervals produced good answers.  The similarity measurement techniques tested were local alignment, longest common subsequence, an n-gram counting technique that included the frequency of occurrence of each n-gram, and the Ukkonen n-gram measure. Local alignment was a clear winner. However, local alignment is a slow technique. It would be useful to have a good n-gram- based technique as indexes can be built to speed up matches. This led to our current exploration of n-grams. In this current set of experiments we used the same experimental framework as that used in [2], but also tested a set of manually produced queries. These manual queries were created by a classically trained pianist with perfect pitch, who listened to the source MI DI files and played a melodic fragment to represent each piece. We explored the variables of n-gram  length, n-gram counting method, song-length normalisation, melody extraction and query length.   The results clearly showed that n-grams of length five work best using the count distinct method, where term frequency is ignored. Some length normalisation was found to be best, such as dividing by the log of the number of intervals. Query lengths of 20 and 30 each gave similar results, but those of length 10 were much less effective. The all channels technique was found to be far superior to the other methods when manual queries were used. For the automatic queries the results are less distinct. It is quite clear from our experiments that an index-based approach to melody  matching using n- grams can be used to produce good answers to melody queries and we recommend  an n-gram length of five for this purpose. Term frequency should be ignored in this approach. The method of melody extraction and standardisation used within the collection is also important in implementing a successful system. Author Information Alexandra L. Uitdenbogerd, Justin Zobel Department of Computer Science, RMIT University {alu, jz}@cs.rmit.edu.au Suggested Readings Downie, Stephen J. and Nelson, Michael.  2000, “Evaluation of a simple and effective music information retrieval method”, Proc ACM-SIGIR Conference, Athens, Greece. Uitdenbogerd, Alexandra and Zobel, Justin. 1998. “Manipulation of music for melody matching”, Proc. ACM International Multimedia Conference, Bristol, England. Uitdenbogerd, Alexandra and Zobel, Justin. 1999. “Melodic matching techniques for large music databases”, Proc. ACM International Multimedia Conference, Orlando, Florida, USA.",
        "zenodo_id": 1414990,
        "dblp_key": "conf/ismir/UitdenbogerdZ00",
        "keywords": [
            "local alignment",
            "n-gram methods",
            "bioinformatics",
            "information retrieval",
            "MIDI files",
            "melody queries",
            "stored music",
            "melodic similarity measurement",
            "melody extraction algorithms",
            "pitch information"
        ],
        "content": "Music Ranking Techniques Evaluated\nAbstract\nSeveral techniques have been proposed for matching melody queries to stored music. In previous\nwork [2], we found that local alignment, a technique derived from bioinformatics, was more\neffective than the n-gram methods derived from information retrieval that are used in other work.\nIn this paper we explore a broader range of n-gram techniques, and test them with both manual\nqueries and queries automatically extracted from MIDI files. Our experiments show that\nalternative - indeed, simpler - n-gram matching techniques than those tested in earlier work can\nbe as effective as local alignment; one highly effective technique is to simply count the number of\n5-grams in common between query and stored piece of music. N-grams are particularly effective\nfor short queries and manual queries, while local alignment is superior for the automatic queries.\nIn our experiments we have used a database of 10 466 standard MIDI files down loaded from the\ninternet. These were processed to extract their melodies as described in [1]. In that earlier paper,\nwe used volunteer listeners to judge a small set of extracted melodies. We discovered that our\nsimple algorithm that chooses notes starting at each instance to be melody notes and that we have\nnamed all mono, was the most successful in extracting melodies. However, we continued to test\nall of our melody extraction algorithms in our second paper [2] in the context of melodic\nsimilarity measurement. In this experiment we used three methods of melodic standardisation to\nproduce the representations that would be used for matching. They were: contour, which uses a\nthree-symbol alphabet to represent where a melody goes up, down or stays the same; modulo-12\ninterval (pitch distance between notes), which keeps the size of intervals and their direction but\nreduced to one octave; and exact interval, which keeps the exact interval size and direction.  All\nthese represented pitch information only.\nA set of 28 automatically extracted melody queries were used to locate other versions of the same\npiece of music. The results were evaluated by computing the eleven-point precision average and\nthe precision at a recall level of 20. Again the all mono method was successful, as were the\nentropy channel and all channels methods. The entropy channel method applies the all mono\ntechnique to each channel and then selects the melody channel based on its first order predictive\nentropy. The all channels technique also applies the all mono algorithm to each channel and then\nretains them all for the matching process. In addition, we confirmed that contour was insufficient\nfor good matching, but that the use of interval sequences worked well.  Queries of 30 intervals\nproduced good answers.  The similarity measurement techniques tested were local alignment,\nlongest common subsequence, an n-gram counting technique that included the frequency of\noccurrence of each n-gram, and the Ukkonen n-gram measure. Local alignment was a clear\nwinner. However, local alignment is a slow technique. It would be useful to have a good n-gram-\nbased technique as indexes can be built to speed up matches. This led to our current exploration\nof n-grams.\nIn this current set of experiments we used the same experimental framework as that used in [2],\nbut also tested a set of manually produced queries. These manual queries were created by a\nclassically trained pianist with perfect pitch, who listened to the source MI DI files and played a\nmelodic fragment to represent each piece. We explored the variables of n-gram  length, n-gram\ncounting method, song-length normalisation, melody extraction and query length.   The results\nclearly showed that n-grams of length five work best using the count distinct method, where termfrequency is ignored. Some length normalisation was found to be best, such as dividing by the log\nof the number of intervals. Query lengths of 20 and 30 each gave similar results, but those of\nlength 10 were much less effective. The all channels technique was found to be far superior to the\nother methods when manual queries were used. For the automatic queries the results are less\ndistinct.\nIt is quite clear from our experiments that an index-based approach to melody  matching using n-\ngrams can be used to produce good answers to melody queries and we recommend  an n-gram\nlength of five for this purpose. Term frequency should be ignored in this approach. The method of\nmelody extraction and standardisation used within the collection is also important in\nimplementing a successful system.\nAuthor Information\nAlexandra L. Uitdenbogerd, Justin Zobel\nDepartment of Computer Science, RMIT University\n{alu, jz}@cs.rmit.edu.au\nSuggested Readings\nDownie, Stephen J. and Nelson, Michael.  2000, “Evaluation of a simple and effective music\ninformation retrieval method”, Proc ACM-SIGIR Conference, Athens, Greece.\nUitdenbogerd, Alexandra and Zobel, Justin. 1998. “Manipulation of music for melody matching”,\nProc. ACM International Multimedia Conference, Bristol, England.\nUitdenbogerd, Alexandra and Zobel, Justin. 1999. “Melodic matching techniques for large music\ndatabases”, Proc. ACM International Multimedia Conference, Orlando, Florida, USA."
    },
    {
        "title": "ISMIR 2000, 1st International Symposium on Music Information Retrieval, Plymouth, Massachusetts, USA, October 23-25, 2000, Proceedings",
        "author": [],
        "year": "2000",
        "doi": "10.5281/zenodo.3527920",
        "url": "https://doi.org/10.5281/zenodo.3527920",
        "ee": null,
        "abstract": "Ever since the first International Symposium on Music Information Retrieval in 2000, the proceedings have been made publicly available to interested researchers. After 20 years of annual conferences and workshops, this number has grown to an impressive amount of almost 2,000 papers. When restricted to linear search and retrieval in a document collection of this size, it becomes inherently hard to identify topics, related work and trends in scientific research. Therefore, this paper presents and evaluates a map-based user interface for exploring 20 years of ISMIR publications. The interface visualizes k-nearest neighbor subsets of semantically similar papers. Users may jump from one neighborhood to the next by selecting another paper from the current subset. Through animated transitions between local k-nn maps, the interface creates the impression of panning a large global map. Evaluations results of a small user study suggest that users are able to discover interesting links between papers. Due to its generic approach, the interface is easily applicable to other document collections as well. The search interface and its source code will be made publicly available.",
        "zenodo_id": 3527920,
        "dblp_key": "conf/ismir/2000",
        "keywords": [
            "International Symposium on Music Information Retrieval",
            "publicly available proceedings",
            "20 years of conferences and workshops",
            "linear search and retrieval",
            "document collection of size",
            "k-nearest neighbor subsets",
            "semantically similar papers",
            "user interface for exploring",
            "small user study",
            "generic approach"
        ]
    }
]