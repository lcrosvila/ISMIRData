[
    {
        "title": "Genre Classification Using Bass-Related High-Level Features and Playing Styles.",
        "author": [
            "Jakob Abeßer",
            "Hanna M. Lukashevich",
            "Christian Dittmar",
            "Gerald Schuller"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417697",
        "url": "https://doi.org/10.5281/zenodo.1417697",
        "ee": "https://zenodo.org/records/1417697/files/AbesserLDS09.pdf",
        "abstract": "Considering its mediation role between the poles of rhythm, harmony, and melody, the bass plays a crucial role in most music genres. This paper introduces a novel set of transcription-based high-level features that characterize the bass and its interaction with other participating instruments. Furthermore, a new method to model and automatically retrieve different genre-specific bass playing styles is presented. A genre classification task is used as benchmark to compare common machine learning algorithms based on the presented high-level features with a classification algorithm solely based on detected bass playing styles.",
        "zenodo_id": 1417697,
        "dblp_key": "conf/ismir/AbesserLDS09",
        "keywords": [
            "mediation",
            "rhythm",
            "harmony",
            "melody",
            "bass",
            "genre",
            "interaction",
            "instrument",
            "genre-specific",
            "classification"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nGENRE CLASSIFICATIONUSING BASS-RELATED HIGH-LEVEL\nFEATURES ANDPLAYING STYLES\nJakob Abeßer, HannaLukashevich,ChristianDittmar,Geral d Schuller\nFraunhoferIDMT, {abr,lkh,dmr,shl }@idmt.fraunhofer.de\nABSTRACT\nConsideringitsmediationrolebetweenthepolesofrhythm,\nharmony,andmelody,thebassplaysacrucialroleinmost\nmusic genres. This paper introduces a novel set of\ntranscription-basedhigh-levelfeaturesthatcharacteri zethe\nbassanditsinteractionwithotherparticipatinginstrume nts.\nFurthermore, a new method to model and automatically\nretrieve different genre-speciﬁc bass playing styles is pr e-\nsented. Agenreclassiﬁcationtaskisusedasbenchmarkto\ncompare common machine learning algorithms based on\nthepresentedhigh-levelfeatureswithaclassiﬁcationalg o-\nrithmsolelybasedondetectedbassplayingstyles.\n1. INTRODUCTION\nAfter prolonged series of publications focusing on low-\nand mid-level features, many works within the MIR com-\nmunity nowadays emphasize the importance of musical\nhigh-level features. Their application is expected to sig-\nniﬁcantlyincreasetheprecisionin automaticmusicclassi -\nﬁcation and similarity search tasks that have limits using\nconventional modeling paradigms [2]. Various automatic\ntranscription techniques allow the extraction of score pa-\nrameterslikenotepitch,velocity(volume),onsettimeand\ndurationfrompolyphonicmixtures. Theseparametersem-\nbody the prior foundationfor a subsequent feature extrac-\ntion. Due to their close relation to musicological expres-\nsions, high-levelfeaturescan be easily understoodby mu-\nsicologists. Thus, they offer a promising opportunity to\ntranslate existing musicological knowledge into automati -\ncallyretrievablepropertiesofanalyzedmusic.\nThe remainder of this paper is organized as follows. In\nSec. 2, we illustrate the goals of this publication and give\nan overview over related work in the subsequent section.\nWe present both novel transcription-based high-level fea-\ntures and a new framework to model concepts and classes\nfor the purpose of music classiﬁcation in Sec. 4. Evalua-\ntion resultsfrom differentscenariosare presentedand dis -\ncussed in Sec. 5 and a ﬁnal conclusion is given in the last\nsection.\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2009 International Society for MusicInformation Retrieva l.2. GOALS &CHALLENGES\nOur goal is to design transcription-based high-level fea-\ntures that enable a better characterizationof the bass trac k\nin different songs. Furthermore,we aim to developa gen-\neralmethodtotranslatemusicologicalknowledgeintorule s\non feature values that can be easily evaluated. This ap-\nproachisintendedtofacilitatethedesignofaninstrument -\nrelated classiﬁer that is trained by musicological knowl-\nedge – similar to an expert system. When analyzing real\naudio data, the strong dependence of a well-performing\ntranscriptionsystemstill remainsthebiggestchallenge.\n3. PREVIOUS APPROACHES\nVarious bass transcription algorithms have been proposed\nso far in [13], [11], [6], and [18]. They extract the score\nparametersofabasstrackinpolyphonicaudiorecordings.\nStill, transcription errors related to pitch and onset valu es\nappear due to the high complexity of overlapping instru-\nment spectra. These errors affect the accuracy of the de-\nduced high-level features. As shown in [16], high-level\nfeatures can be derivedfrom different music domains like\ninstrumentation, texture, rhythm, dynamics, pitch statis -\ntics, melody, and chords. Offering a direct access to the\nrelevant score parameters, symbolic audio data like MIDI\nreceivespreferentialtreatmentin manypublications.\nThe authors of [4] applied several statistical methods to\nderive high-level features from note onsets, pitches, and\nintervals. The versatility of complexity-based descripto rs\nbased on entropy, compression, and prediction has been\nshown in [15]. A set of musical features derived from the\nbass part was introduced in [19]. The authors restricted\nthemselves to pitch-related features and distinguished be -\ntween features characterizing the pitch variability and th e\npitch motion. Rhythmical aspects like the swing or syn-\ncopationshavebeeninvestigatedinvariouspublicationsa s\nforinstancein[12]and[9]. In[16],[4],[19],and[1],genr e\nclassiﬁcation solelyonhigh-levelfeatureswascovered.\n4. NEW APPROACHES\n4.1 High-levelfeatures\nHigh-level features allow to model and quantify musical\npropertiesthat are directlyobservableby experiencedmu-\nsicologists. These are for instance the key, the time signa-\nture or measure of the harmonic consonance in a piece of\nmusic. Theycanbededucedfromthepitch,theonsettime,\nandthedurationvaluesofall notes.\n453Poster Session 3\nMelody-relatedfeatures\nBy analyzing the course of the absolute pitch pA, we de-\nrive features from the incidence rate of typical pitch pro-\ngressions, such as notes with constant pitch orchromatic\nnote sequences related to the overall number of notes and\nthe overall pitch range in halftones. With reference to the\nsimultaneously soundingchordsof the harmonytrack, we\nderive a feature from the ratio of chord notes within the\nbass line. Besides, we convert the absolute pitch of each\nnote into its functional pitch pA,F. It represents the inter-\nval type between each bass note and the root note of the\nsimultaneously sounding chord. We consider all interval\ntypes from primes to sevenths ( pA,F∈[1,7]), bigger in-\ntervals are mapped into this interval range. The incidence\nratesofallpossiblevaluesof pA,Fareusedasfeaturesthat\nprovide key-independentinformation about the frequency\nof occurrenceof differentinterval types related to the har -\nmonyaccompaniment.\nThe prior use of root notes, octaves, and ﬁfths of the cur-\nrent chord within a bass line does not allow a conclusive\ndifferentiation between major and minor based chords by\nexclusivelyinvestigatingthebassaccompaniment. Thus,a\nmeasure of harmonic ambiguity is calculated proportional\nto the occurrence rate of primes and ﬁfths and inversely\nproportional to the occurrence rate of thirds as\nFHA=P(pA,F= 1) + P(pA,F= 5)−P(pA,F= 3).\nWe use a simple bar-wise distance measure combining\nrhythmic and melodic similarity to detect the dominant\nbass pattern . Therefore, we compute a square matrix Dτ\ncontainingthe similarity betweenthe notesin each pair of\nbars. We use Dτ(k, m) = 0 .5[(1−Nk,m/Nk)+\n(1−Nm,k/Nm)]where Nidenotes the number of notes\nin bar iandNi,jdenotesthe numberof notes of bar ithat\nhaveanoteequivalentinbar jwiththesamepitch( pA)and\nonset[mod(τ,1)]. We choose the notes of bar ndom=n\nthat minimizes/summationtext\niDi,nas the dominantpattern since this\nbarhasthelowestoveralldistancetotheotherbars. Subse-\nquently, measures of tonalandrhythmic variation are de-\nrivedfromthemeandistancebetweenallbarstobar ndom.\nFor the rhythmical variation, only the aforementionedon-\nset conditionofthenoteequivalentistakenintoaccount.\nThe interval progression of the bass line is characterized\nbythreedifferentrepresentations,namelytherelativepi tch\npR∈[−12,12](mapped down to a two octave range),\nthe relative pitch mapped to functional intervals pR,F∈\n[−7,7](toprovidearepresentationindependentofthekey-\ntypeasdescribedabove),andtheintervaldirection pR,D∈\n[−1,1]. Subsequently,severalstatistical propertiessuchas\nentropy and relative number of non-zero elements of the\nprobabilities of all parameter values are extracted as fea-\ntures. The measures of constant direction FCD&domi-\nnant direction FDDfurthermorequantify the temporal ra-\ntio of note passages with constant interval direction and\ncharacterize the dominant direction. Thus, they measure\nto what extend a melody appears to be ﬂuent. We use\nFCD =N[pR,D(i)≡pR,D(i+ 1)]/NIntervals and\nFDD=N(pR,I= 1)/NIntervals.Rhythm-relatedfeatures\nThebeat grid contains the temporal positions and indices\nof all beats corresponding to the current time signature.\nAfteritsextraction,allnoteonset tanddurationvalues ∆t\naremappedfromsecondstocertainmultiplesofthecorre-\nspondingbar lengths (resultingin τand∆τ). This allows\natempo-independentextractionofrhythm-relatedfeature s.\nWe applied a similar approach as described in [12] to de-\nrive theswing ratio related to the 8th- and the 16th-note\ngrid.\nA measure of syncopation related to the both aforemen-\ntioned temporal grids is derived by retrieving binary pat-\nterns (like for instance “1001” in an 16th-note grid rep-\nresenting two notes whereas the ﬁrst one is played on a\ndownbeat and the other one on the adjacent off-beat re-\nlatedto the8th-notegrid).\nBased on the dominant bass pattern and its dynamic pro-\ngression, we take the number of bass notes within each\nbar with a velocity above 60% of the maximum occuring\nbass note velocity as the measure of accent sparsity . Per-\ncussionists often use the bass-drum to “double” the main\naccents of the bass line. We measure the ratio of the num-\nber of notes that both instruments played rhythmically in\nunisonto the sum of all notes played by the bass and the\nbassdrumindividually.\nStructure-relatedfeatures\nIn addition, features characterizing repeating melodic an d\nrhythmicsegmentsarederived. Therefore,weapplyasim-\nplepatternsearchalgorithm( CorrelativeMatrixApproach\n[14])oncharacterstringsderivedfromtheaforementioned\nscoreparameters pA,τ, and∆τ.\nWe use the statistical properties mean, median, standard\ndeviation, minimum, and maximum from each of the pat-\ntern parameters length, incidence rate, and mean distance\nbetween similar patters as features. Overall, all single-\nand multidimensionalhigh-levelfeatures result in an 154-\ndimensionalfeaturevector.\n4.2 Concept-basedframework\nTo improvegenreclassiﬁcation, we aim at modelingcom-\nmon bass playing styles that are typical for certain mu-\nsic genres. Therefore, we apply a generic framework to\ntranslate known musicological properties into explicit re -\nstrictions on feature values. The assignment of weighting\nfactors furthermore allows to take the importance of each\npropertyintoaccount. Inthefollowingsubsections,wein-\ntroducetheterms concept,class,andpropertyasthemajor\ncomponentsoftheframework. Afterwardsweexplainhow\nrelevancevalues forbothpropertiesandclassesarederived\nto measure their signiﬁcance to the investigated piece of\nmusicandclosewithadetailedexample. Hereafter,multi-\ndimensionalvariablesaredenotedinboldprint.\nConcepts& classes\nTheterm conceptrepresentsa generalapproachtocatego-\nrize music. Each concept is deﬁned by a set of classesas\n45410th International Society for Music Information Retrieval Conference (ISMIR 2009)\ng1\nF4C1PF,1 \nPF,2 \nFeatures Classes Properties \nP\nPF3\ng2\nCC2\nProperty \nrelevance Property \nweighting F2F1PM,2 PM,1 \nClass \nrelevance Concept\n(F) (P) (C)Property \nFigure1. Concept-basedframework\nshowninFig. 1. Inthispaper,weapplytheconcepts Bass-\nPlayingStyle (denotedas SB),whichrepresentsacommon\nwayofplayingthebassinaspeciﬁcmusicgenreand Genre\n(denoted as G), which is a common category for musi-\ncologists. One well-known example is the bass playing\nstyle walking bass (deﬁned as class WalkingBass ), which\niswidelyappliedbybassplayersindifferentmusicgenres\nrelated to Swing. It is coveredas an examplein the end of\nthis section. The assignment between classes of both con-\nceptsisshownin Fig.2.\nProperties\nEach class is deﬁned by a number of properties P. They\ntranslate its musicologicaldescriptioninto explicitres tric-\ntionsonthevaluesofcertain features F.\nWe discern mandatory properties (M) andfrequent prop-\nerties(F).Mandatorypropertiesarestrictlyneedtobeful-\nﬁlled whereas frequent propertiesare not mandatory for a\ncertain class. A weighting factor 0≤gi≤1is assigned\nto each frequent property. giis proportionalto the impor-\ntanceofthecorrespondingpropertywithregardtothecur-\nrentclass.\nFurthermore, properties are either omnipresent (O) or\nconditional (C). Omnipresent properties are constantly\nvalid, whereas the validity of conditional properties de-\npendsona certain condition. Thismayfor instance be the\npresenceofaninstrumentthatafeatureandthusaproperty\nis related to. Only if the condition is fulﬁlled, the corre-\nsponding property needs to be considered. Generally, the\nindices of Pimply the corresponding property type. Ex-\namplesaregivenintheendofthissection. We derivedthe\nweighting factors and thresholds of all properties used in\nthis paper from experiments with development data sam-\nples, whichdidnotbelongtotheevaluationset.\nRelevancevalues\nTheproperty relevance value γPmeasures to what extent\na property Pis fulﬁlled ( γP= 1) or not ( γP= 0). It is\nderived from the corresponding feature value Fby using\narating function r(F). This functiondependson the type\nof restriction on the feature value Fthat is deﬁned by P.\nFor instance, we use γP=r(F) = 0.5[sgn(F−V) + 1]\nto match the property P→FisBiggerThan V. TheAfrequent use of chord tones is mandatory.\nP1,MO→FChordToneRatio isBiggerThan 0.3\n2) The melodic direction is often constant within each bar.\n(important property -weighting factor g2= 0.7)\nP2,F O→FConstantDirection isBiggerThan 0.7\n3) If quarter notes are primarily used (such as in slow and\nmid-tempoJazz songs), thereisahighswingfactor relatedt o\nthe eighth note grid. (important property - weighting facto r\ng3= 0.8)\nifCondition (FDominantRhythmicalGrid is4)\nP3,F C→FSwingF actor, 8isBiggerThan 0.7\n4)Ifeighthnotesareprimarilyused(suchasinup-tempoJaz z\nsongs), there is a high swing factor related to the sixteenth\nnote grid. (important property - weightingfactor g4= 0.8)\nifCondition (FDominantRhythmicalGrid is8)\nP4,F C→FSwingF actor, 16isBiggerThan 0.7\n5) Chromatic note passages are occasionally used. (less im-\nportant property -weighting factor g5= 0.3)\nP5,F O→FChromatics isRelativelyHigh\nTable 1. Properties of the class WalkingBass (concept\nBassPlayingStyle )\nratingfunctionis designedin sucha way that 0≤γP≤1\nisassured.\nSubsequently, the class relevance value γCis derived for\neach class Cfrom its corresponding property relevance\nvalues. γCquantiﬁestowhatextendacertainclassisrele-\nvantforthemusicologicaldescriptionofananalyzedpiece\nofmusic.\nWe suggest the following algorithm to comply with the\ndifferent property types. If all mandatory properties are\ngiven to be true, γCis calculated as a weighted sum over\nall frequent properties γPFaccording to their normalized\nweightingfactors /hatwideg(/summationtext/hatwidegi= 1). Otherwiseitissettozero.\nThisalgorithmcanbesummarizedasfollows:\nγC=/braceleftBigg/summationtext\ni/hatwidegiγPF,iifγPM,j= 1∀PM,j∈PM,\n0 else(1)\nExample\nAsshowninTable1,theclass WalkingBass oftheconcept\nBassPlayingStyle isdeﬁnedby5feature-relatedproperties\nthatarederivedfrommusicologicalpropertiesofthisstyl e.\n5. EVALUATION\nWe use two data sets consisting of symbolic (MIDI) and\nreal audio(AUDIO)each with 50respectively40 excerpts\nfrom each of the genres PopRock (POP),Swing\n(SWI),Latin(LAT),Funk(FUN),Blues(BLU), and Met-\nalHardRock (MHR). All excerptsare derived from instru-\nmental solo parts of the melody instruments between 20\nand 35 seconds of length. Fig. 3 depicts all processing\nstepsthatprecedetheevaluation.\n455Poster Session 3\nConcepts BassPlayingStyle \nClasses Genre \nWalkingBass FunkSyncopated \nBossaNovaBass Swing Funk \nLatin \nSB \nSB \nSB \n G\nG\nG\nFigure2. Assignmentbetweentheclassesoftheconcepts BassPlayingStyle andGenre\nMIDI AUDIO \nTranscription \nFeature extraction Score parameters \nHigh-level features \nEvaluation Concept-based \nclassification Pattern recognition \nbased classification \nFigure3. Processingﬂow-chart\n5.1 Transcription& Pre-processing\nWe used the TranscriptionToolbox[6]to extractthe score\nparametersfrom real audio data. It providesalgorithmsto\nextract the bass, melody, harmony, and drum part as well\nas the beat grid information. As explainedin Sec. 4.1, our\naim istofocusonthebassanditsinteractionwith thepar-\nticipating instruments. Concerning symbolic audio data,\nscoreparametersaredirectlyextracted.\n5.2 FeatureSelection(FS)andFeatureSpace\nTransformation(FST)\nThefollowingfeatureselectionandfeaturespacetransfor -\nmationtechniqueshavebeenutilizedto reducethe dimen-\nsionalityofthefeaturespace.\nInertia Ratio Maximization using Feature Space Pro-\njection(IRMFSP).\nIRMFSP wasproposedin[17]. ThisFS algorithmismoti-\nvatedbytheideassimilartoFisher’sdiscriminantanalysi s.\nDuringeachiterationofthealgorithm,welookforthefea-\nture maximizing the ratio of between-class inertia to the\ntotal-classinertia. Toavoid thenextchosenfeatureto pro -\nvidethesameinformationonthenextiteration,allfeature s\nare orthogonalized to the selected one. In this evaluation\nweusetheISMFSPalgorithmswiththemodiﬁcationspro-\nposedin[8].\nLinear DiscriminantAnalysis(LDA)\nLDA is one of the most often used supervised FST meth-\nods [10]. It is successfully applied as a pre-processingfor\naudiosignalclassiﬁcation. Originalfeaturevectorsarel in-\nearly mapped into new feature space guaranteeing a max-imal linear separability by maximization of the ratio of\nbetween-class variance to the within-class variance. This\nmapping is conductedby multiplying the original K×N\ndimension feature matrix Xwith the transformation ma-\ntrixT. Reducingthedimensionofthetransformedfeature\nvector from NtoD≤Nis achievedby consideringonly\ntheﬁrst Dcolumnvectorsof Tformultiplication.\nGeneralizedDiscriminantAnalysis(GDA)\nReal-world classiﬁcation routines often have to deal with\nnon-linearproblems,thuslineardiscriminationintheori g-\ninalfeaturespaceisoftennotpossible. TheideaoftheFST\ntechnique GDA [3] is to map the features into higher di-\nmensional (sometimes inﬁnity dimensional) space, where\nthe linear discrimination is possible. Dealing with a high\ndimensionalspace leads to an increase of the computation\neffort. Toovercomethisproblem,thesocalled kerneltrick\nis applied. The key idea of the kernel trick is to replace\nthe dot product in a high-dimensionalspace with a kernel\nfunctionintheoriginalfeaturespace.\n5.3 Classiﬁcation\nWeapplied4knownmethods(SVM,GMM,NB,andkNN)\nas well as a novelconcept-basedapproachfor the purpose\nofclassiﬁcation.\nSupport VectorMachines\nASupportVectorMachine(SVM)isadiscriminativeclas-\nsiﬁer,attemptingtogenerateanoptimaldecisionplanebe-\ntween feature vectors of the training classes [20]. Com-\nmonly for real-world applications, classiﬁcation with lin -\near separationplanesis notpossible in the originalfeatur e\nspace. Thetransformationtothehigherdimensionalspace\nis done using above mentioned kernel trick (we applied\nthe RBF kernel in this paper). Transformed into a high-\ndimensional space, non-linear classiﬁcation problems can\nbecomelinearlysolvable.\nGaussian MixtureModels\nGaussianMixtureModels(GMM)arecommonlyusedgen-\nerativeclassiﬁers.Singledatasamplesoftheclassareint er-\npreted as being generated from various sources and each\nsource is modeled by a single multivariate Gaussian. The\nprobability density function (PDF) is estimated as a\nweightedsumofthemultivariatenormaldistributions. The\nparameters of a GMM can be estimated using the\nExpectation-Maximizationalgorithm[5].\n45610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nNaiveBayesClassiﬁer\nNaive Bayes classiﬁer (NB) is a simple probabilistic clas-\nsiﬁer. NB uses a strong assumption of feature dimensions\nbeing statistically independentand thustakes into accoun t\nonly means and variances over the feature dimensions for\nalltrainingdataoftheclass. Recently,applicabilityand ef-\nﬁciencyofNBclassiﬁerswerediscussedindetailin [21].\nk-NearestNeighbor\nWithk-NearestNeighbor(kNN),theclassiﬁcationisbased\nontheclassassignmentoftheclosest trainingexamplesin\nthefeaturespace[7]. WeusedtheEuclideandistancehere.\nThis type of discriminativeclassiﬁer is also referredas in -\nstance based learning. The level of generalizationof kNN\ncanbetunedbyadjustingthenumberofnearestneighbors\nktakenintoaccount.\nNovelapproach: concept-basedclassiﬁer\nUsingEq. 1,wederiveaclassrelevancevalue γSBi≡γCi\nforeachclassoftheconcept BassPlayingStyle . Wedeﬁned\none common bass playing style for each of the 6 genres\nthatwereconsideredintheevaluation(seeSec.5),namely\nWalkingBass (SWI),BluesShufﬂe (BLU),FunkSyncopated\n(FUN),SteadyRiff (MHR),BossaNovaBass (LAT), and\nChordRootAccompaniment (POP).Forourexperiments,we\nused5differentpropertiesforeachclass.Usingtheassign -\nment between the classes of both concepts as depicted in\nFig.2,theconcept-basedclassiﬁerestimatesthegenre /hatwideG=\nGjthat is assigned to the bass playing style SBiwith the\nhighest class relevance value γSBi. In case two or more\nbass playing styles related to different genres obtain the\nsame class relevance values, the classiﬁcation is consid-\neredtobecorrectifatleastoneofthecandidatesisrelated\ntothecorrectgenreandfalseifnot. Asaproofofconcept,\nweperformedtheevaluationexperimentusingtheconcept-\nbasedclassiﬁer ontheMIDIdataset.\n6. RESULTS\nTable 3 gives an overview over the classiﬁcation scores\nfor different FS / FST combination. For each combina-\ntion, the parametrization with the best results is depicted .\nFurtherevaluationparameterssuchasthenumberofgaus-\nsians for the GMM classiﬁers, kfor the kNN classiﬁers,\nand the number of dimensions after IRMFSP are given\nin brackets. We performed a 25-fold cross validation to\nderive mean classiﬁcation scores and their standard devi-\nations (given in brackets below) for each classiﬁer. As\nshown there, best mean classiﬁcation accuracies for the\nMIDI and AUDIO data set of 81.47%and46.85%have\nbeen achieved applying a combined IRMFSP - GDA pre-\nprocessing for both data sets. Above all, we expect trans-\ncriptionerrorsaffectingnotepitchvalues,onsetvaluesa nd\nbeat grid information to cause signiﬁcantly lower classiﬁ-\ncation scores for real audio data. For both data sets, the\napplicationof feature selection and feature space transfo r-\nmationalgorithmsclearlyincreasestheaccuracyvaluesof\nthesubsequentclassiﬁers.BLU FUN LAT MHR POP SWI\nBLU 68.0- 4.0 - - 28.0\nFUN 28.046.04.0 4.0 4.0 14.0\nLAT 16.0 - 70.0- 2.0 12.0\nMHR 34.0 8.0 6.0 34.0 2.0 16.0\nPOP 36.0 - 20.0 2.0 6.036.0\nSWI 36.0 - 22.0 - - 42.0\nTable 2. Confusion matrix of the concept-basedclassiﬁer\n(MIDIdataset)in %\nAs depicted in Table 2, the concept-based classiﬁer\nachieved a mean classiﬁcation accuracy of 44.3%varying\ninastrongwayfordifferentgenres. Bestresultshavebeen\nobtainedfor Latin(70.0%)andBlues(68.0%). Thelowre-\nsults forPop(6.0%) andMetalHardRock (34.0%) lead to\nthe assumption, that modelingonly one bass playingstyle\nper genreis not sufﬁcient due to the highvariabilityin the\napplieddataset. Furtherstepsincludetheevaluationbase d\nona largerdatabase.\n7. CONCLUSIONS & FUTUREWORK\nIn this paper, we introduced a novel set of transcription-\nbased high-levelfeaturesrelated to the rhythmic,melodic ,\nharmonic, and structural description of bass lines. Fur-\nthermore, we presented a new approach to model musi-\ncalknowledgeofmusicalstylesaspropertiesrelatedtothe\nvaluesoftranscription-basedhigh-levelfeatures. Thema in\nadvantage of concept-basedclassiﬁcation approach is that\nsigniﬁcantly fewer features are necessary to model each\nclass as in common machine learning approaches. Future\nstepsincludemodelingadditionalgenre-speciﬁcbassplay -\ningstylesaswellastransferringtheproposedmethodonto\notherfrequentlyusedinstrumentsliketheguitar.\n8. ACKNOWLEDGMENTS\nThis work has been partly supported by the german re-\nsearch project GlobalMusic2One1funded by the Federal\nMinistry of Education and Research (BMBF-FKZ:\n01/S08039B).TheauthorswouldliketothankPaulBr¨ auer\nforthefruitfuldiscussions.\n9. REFERENCES\n[1] J. Abeßer, C. Dittmar, and H. Großmann. Automatic genre\nand artist classiﬁcation by analyzing improvised solo part s\nfrommusicalrecordings.In ProceedingsoftheAudioMostly ,\n2008.\n[2] J.-J. Aucouturier, B. Defreville, and F. Pachet. The bag -of-\nframe approach to audio pattern recognition: A sufﬁcient\nmodel for urban soundscapes but not for polyphonic music.\nJournal of the Acoustical Society of America , 122(2):881–\n891, 2007.\n[3] G. Baudat and F. Anouar. Generalized discriminant analy sis\nusing a kernel approach. Neural Computation , 12(10):2385–\n2404, 2000.\n[4] P. J. Ponce de L´ eon and J. M. I˜ nesta. Pattern recognitio n ap-\nproach for music style identiﬁcation using shallow statist ical\n1http://www.globalmusic2one.de\n457Poster Session 3\nDataset FS /FST Dim. SVM GMM(2) GMM(3) GMM(5) GMM(10) NB kNN(1) kNN(5) kNN(10)\nMIDI - 154 69.13 67.45 66.28 59.52 60.31 60.03 66.88 64.17 62.69\n(8.34) (9.08) (9.45) (6.21) (5.97) (7.27) (6.51) (5.61) (7.39)\nLDA 5 63.06 59.24 61.22 53.44 59.23 60.50 60.14 62.15 62.38\n(7.20) (5.10) (6.69) (7.43) (14.43) (7.33) (6.97) (7.66) (8.53)\nGDA(γ= 2−7) 5 77.60 77.60 77.60 44.04 18.73 18.37 77.60 77.60 77.60\n(7.65) (7.65) (7.65) (8.31) (9.54) (6.27) (7.65) (7.65) (7.65)\nIRMFSP(20) 20 73.82 58.70 65.07 63.75 64.21 57.99 78.06 71.70 68.85\n(7.89) (8.08) (8.23) (10.20) (7.08) (6.38) (7.31) (6.84) (8.72)\nIRMFSP(80) +LDA 5 72.15 69.87 69.34 67.95 65.20 69.65 69.45 70.48 69.66\n(8.93) (10.67) (7.60) (9.81) (11.70) (8.52) (9.02) (7.93) (6.65)\nIRMFSP(40) +GDA( γ= 2−5)5 76.99 19.32 20.15 13.30 16.09 18.37 81.10 81.47 81.47\n(13.88) (5.07) (9.26) (5.60) (2.85) (6.27) (6.39) (6.20) (6.20)\nAUDIO - 154 41.33 33.45 34.95 33.73 33.80 27.24 36.54 31.42 32.25\n(8.33) (8.59) (8.98) (10.33) (10.62) (8.33) (10.35) (11.61) (9.66)\nLDA 5 32.98 32.82 30.16 28.90 28.76 34.25 31.16 33.84 34.10\n(7.39) (7.46) (6.88) (7.95) (8.33) (7.58) (9.00) (8.66) (8.28)\nGDA(γ= 2−9) 5 42.74 42.74 42.74 27.09 15.02 12.79 42.74 42.74 42.74\n(11.53) (11.53) (11.53) (15.08) (6.90) (6.63) (11.53) (11.53) (11.53)\nIRMFSP(40) 40 43.26 39.19 38.31 39.83 36.62 26.69 45.23 42.04 37.83\n(11.76) (10.83) (10.56) (12.93) (9.86) (8.87) (11.70) (12.06) (10.67)\nIRMFSP(20) +LDA 5 43.80 41.66 42.28 41.32 40.58 43.90 35.29 40.69 41.48\n(10.61) (11.16) (10.68) (11.50) (13.52) (12.09) (9.10) (10.75) (10.24)\nIRMFSP(40) +GDA( γ= 2−5)5 46.85 46.85 46.85 26.59 16.64 12.79 46.85 46.85 46.85\n(9.73) (9.73) (9.73) (13.75) (7.10) (6.63) (9.73) (9.73) (9.73)\nTable 3. Meanclassiﬁcationaccuracy[%]fortheMIDIandAUDIO data set (standarddeviation[%] givenin brackets)\ndescriptors. IEEE Transactions on System, Man and Cyber-\nnetics - Part C : Applications and Reviews , 37(2):248–257,\nMarch 2007.\n[5] A.P.Dempster,N.M.Laird,andD.B.Rdin.Maximum like-\nlihoodfromincomplete dataviatheemalgorithm. Journal of\nthe Royal Statistical Society, Series B ,39:1–38, 1977.\n[6] C. Dittmar, K. Dressler, and K. Rosenbauer. A toolbox for\nautomatic transcription of polyphonic music. In Proceedings\nof the AudioMostly , 2007.\n[7] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁca-\ntion (2nd Edition) . Wiley-Interscience, 2nd edition, Novem-\nber 2000.\n[8] S. Essid. Classiﬁcation automatique des signaux audio-\nfr´ equences : reconnaissance des instruments de musique .\nPhD thesis, Universit´ e Pierre et Marie Curie, Paris, Franc e,\nDecember 2005.\n[9] P. Flanagan. Quantifying metrical ambiguity. In Proceedings\nof the Int. Conf. on Music Information Retrieval (ISMIR) ,\npages 635–640, 2008.\n[10] K.Fukunaga. Introduction toStatistical PatternRecognition .\nAcademic Press,2nd edition, September 1990.\n[11] M. Goto. A real-time music-scene-description system -\npredominant-f0 estimation for detecting melody and bass\nlines in real-world audio signals. Speech Communication ,\n43:311–329, 2004.\n[12] F. Gouyon. A computational approach to rhythm description\n- audio features for the computation of rhythm periodicity\nfunctions andtheir useintempoinductionand musiccontent\nprocessing . PhD thesis,UniversityPompeu Fabra,2005.\n[13] S. W. Hainsworth and M. D. Macleod. Automatic bass line\ntranscription from polyphonic audio. In Proceedings of the\nInt. Computer Music Conf.(ICMC) ,2001.[14] J.-L.Hsu,C.-C.Liu,andA.L.P.Chen. Discoveringnont riv-\nial repeatingpatterns inmusic data. In IEEETransactions on\nMultimedia , volume 3 of IEEE Transactions on Multimedia ,\npages 311–324, September 2001.\n[15] S. T. Madsen and G. Widmer. A complexity-based approach\nto melody track identiﬁcation in midi ﬁles. In Proceedings\nof the Int. Workshop on Artiﬁcial Intelligence and Music\n(MUSIC-AI) ,January 2007.\n[16] C.McKayandI.Fujinaga.Automaticgenreclassiﬁcatio nus-\ning large high-level musical feature sets. In Proceedings of\nthe Int. Conf. in Music Information Retrieval (ISMIR) , pages\n525–530, 2004.\n[17] G. Peeters and X. Rodet. Hierarchical gaussian tree wit h in-\nertiaratiomaximizationfortheclassiﬁcationoflargemus ical\ninstrumentsdatabases.In Proceedingsofthe6thInt.Conf.on\nDigital AudioEffects(DAFx) ,London, UK,2003.\n[18] M.P.Ryyn¨ anenandA.P.Klapuri.Automatictranscript ionof\nmelody,bassline,andchordsinpolyphonicmusic. Computer\nMusic Journal , 32:72–86, 2008.\n[19] Y. Tsuchihashi, T. Kitahara, and H. Katayose. Using bas s-\nline features for content-based mir.In Proceedings of the Int.\nConf. on Music Information Retrieval (ISMIR) , pages 620–\n625, 2008.\n[20] V. N. Vapnik. Statistical learning theory . Wiley New York,\n1998.\n[21] H. Zhang. The optimality of naive bayes. In V. Barr and\nZ. Markov, editors, Proceedings of the FLAIRS Conf. AAAI\nPress, 2004.\n458"
    },
    {
        "title": "Genre Classification Using Harmony Rules Induced from Automatic Chord Transcriptions.",
        "author": [
            "Amelie Anglade",
            "Rafael Ramírez 0001",
            "Simon Dixon"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414944",
        "url": "https://doi.org/10.5281/zenodo.1414944",
        "ee": "https://zenodo.org/records/1414944/files/AngladeRD09.pdf",
        "abstract": "We present an automatic genre classification technique making use of frequent chord sequences that can be applied on symbolic as well as audio data. We adopt a first-order logic representation of harmony and musical genres: pieces of music are represented as lists of chords and musical genres are seen as context-free definite clause grammars using subsequences of these chord lists. To induce the contextfree definite clause grammars characterising the genres we use a first-order logic decision tree induction algorithm. We report on the adaptation of this classification framework to audio data using an automatic chord transcription algorithm. We also introduce a high-level harmony representation scheme which describes the chords in term of both their degrees and chord categories. When compared to another high-level harmony representation scheme used in a previous study, it obtains better classification accuracies and shorter run times. We test this framework on 856 audio files synthesized from Band in a Box files and covering 3 main genres, and 9 subgenres. We perform 3-way and 2-way classification tasks on these audio files and obtain good classification results: between 67% and 79% accuracy for the 2-way classification tasks and between 58% and 72% accuracy for the 3-way classification tasks.",
        "zenodo_id": 1414944,
        "dblp_key": "conf/ismir/AngladeRD09",
        "keywords": [
            "automatic",
            "genre",
            "classification",
            "technique",
            "frequent",
            "chord",
            "sequences",
            "symbolic",
            "audio",
            "data"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nGENRE CLASSIFICATION USING HARMONY RULES INDUCED FROM\nAUTOMATIC CHORD TRANSCRIPTIONS\nAm´elie Anglade\nQueen Mary\nUniversity of London\nCentre for Digital Music\namelie.anglade@elec.qmul.ac.ukRafael Ramirez\nUniversitat Pompeu Fabra\nMusic Technology Group\nrramirez@iua.upf.eduSimon Dixon\nQueen Mary\nUniversity of London\nCentre for Digital Music\nsimon.dixon@elec.qmul.ac.uk\nABSTRACT\nWe present an automatic genre classiﬁcation technique mak-\ning use of frequent chord sequences that can be applied on\nsymbolic as well as audio data. We adopt a ﬁrst-order logic\nrepresentation of harmony and musical genres: pieces of\nmusic are represented as lists of chords and musical gen-\nres are seen as context-free deﬁnite clause grammars using\nsubsequences of these chord lists. To induce the context-\nfree deﬁnite clause grammars characterising the genres we\nuse a ﬁrst-order logic decision tree induction algorithm.\nWe report on the adaptation of this classiﬁcation frame-\nwork to audio data using an automatic chord transcription\nalgorithm. We also introduce a high-level harmony rep-\nresentation scheme which describes the chords in term of\nboth their degrees and chord categories. When compared\nto another high-level harmony representation scheme used\nin a previous study, it obtains better classiﬁcation accura-\ncies and shorter run times. We test this framework on 856\naudio ﬁles synthesized from Band in a Box ﬁles and cov-\nering 3 main genres, and 9 subgenres. We perform 3-way\nand 2-way classiﬁcation tasks on these audio ﬁles and ob-\ntain good classiﬁcation results: between 67% and 79% ac-\ncuracy for the 2-way classiﬁcation tasks and between 58%\nand 72% accuracy for the 3-way classiﬁcation tasks.\n1. INTRODUCTION\nTo deal with the ever-increasing amount of digital music\ndata in both personal and commercial musical libraries some\nautomatic classiﬁcation techniques are generally needed.\nAlthough metadata such as ID3 tags are often used to sort\nsuch collections, the MIR community has also shown a\ngreat interest in incorporating information extracted from\nthe audio signal into the automatic classiﬁcation process.\nWhile low-level representations of harmonic content have\nbeen used in several genre classiﬁcation algorithms (e.g.\nchroma feature representation in [1]), little attention has\nbeen paid to how harmony in its temporal dimension, i.e.\nchord sequences, can help in this task. However, there\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.seems to be a strong connection between musical genre and\nthe use of different chord progressions [2]. For instance, it\nis well known that pop-rock tunes mainly follow the classi-\ncal tonic-subdominant-dominant chord sequence, whereas\njazz harmony books propose different series of chord pro-\ngressions as a standard. We intend to test the extent to\nwhich harmonic progressions can be used for genre classi-\nﬁcation.\nIn a previous article [3] we have shown that efﬁcient and\ntransparent genre classiﬁcation models entirely based on\na high-level representation of harmony can be built using\nﬁrst-order logic. Music pieces were represented as lists of\nchords (obtained from symbolic ﬁles) and musical genres\nwere seen as context-free deﬁnite-clause grammar using\nsubsequences of any length of these chord lists. The gram-\nmar representing the genres were built using a ﬁrst-order\nlogic decision tree induction algorithm. These resulting\nmodels not only obtained good classiﬁcation results when\ntested on symbolic data (between 72% and 86% accuracy\non 2-class problems) but also provided a transparent ex-\nplanation of the classiﬁcation to the user. Indeed thanks\nto the expressiveness of ﬁrst-order logic the decision trees\nobtained with this technique can be presented to the user\nas sets of human readable rules.\nIn this paper we extend our harmony-based approach to\nautomatic genre classiﬁcation by introducing a richer har-\nmony representation and present the results of audio data\nclassiﬁcation. In our previous article we used the inter-\nvals between the root notes of consecutive chords. Root\ninterval progressions capture some degree information and\ndo not depend on the tonality. Thus when using root in-\ntervals no key extraction is necessary. However, one root\ninterval progression can cover several degree sequences.\nFor instance the degree sequences “IV-I-IV” and “I-V-I”\nare both represented by the root interval sequence “perfect\nﬁfth-perfect fourth”. To avoid such generalisations we in-\ntroduce here another representation of harmony based on\nthe degrees (i.e. I, V , etc.) and chord categories (i.e. min,\n7, maj7, etc.). In addition such a representation matches\nthe western representation of harmony and thus our clas-\nsiﬁcation models (i.e. decision trees or sets of classiﬁca-\ntion rules describing the harmony) can be more easily in-\nterpreted by the users. Finally since degrees are relative\nto the key, a key estimation step is now needed. This is a\nrequirement but not a limitation as nowadays many chord\ntranscription algorithms from audio (e.g. [4,5]) do also per-\n669Poster Session 4\nform key estimation.\nThe paper is organised as follows: In Section 2 we re-\nview some existing studies using high-level representation\nof harmony for automatic genre classiﬁcation. In Section\n3 we present the details of our methodology, including the\nknowledge representation and the learning algorithm em-\nployed in this study. In Section 4 we present the classiﬁca-\ntion results of our ﬁrst-order logic classiﬁcation technique\nbefore concluding in Section 5.\n2. RELATED WORK\nOnly a few studies have considered using higher level har-\nmonic structures, such as chord progressions, for automatic\ngenre recognition.\nIn [6], a rule-based system is used to classify sequences\nof chords belonging to three categories: Enya, Beatles and\nChinese folk songs. A vocabulary of 60 different chords\nwas used, including triads and seventh chords. Classiﬁ-\ncation accuracy ranged from 70% to 84% using two-way\nclassiﬁcation, and the best results were obtained when try-\ning to distinguish Chinese folk music from the other two\nstyles, which is a reasonable result as both western styles\nshould be closer in terms of harmony.\nPaiement et al. [7] also used chord progressions to build\nprobabilistic models. In that work, a set of 52 jazz stan-\ndards was encoded as sequences of 4-note chords. The au-\nthors compared the generalization capabilities of a proba-\nbilistic tree model against a Hidden Markov Model (HMM),\nboth capturing stochastic properties of harmony in jazz,\nand the results suggested that chord structures are a suit-\nable source of information to represent musical genres.\nMore recently, Lee [8] has proposed genre-speciﬁc\nHMMs that learn chord progression characteristics for each\ngenre. Although the ultimate goal of this work is using the\ngenre models to improve the chord recognition rate, he also\npresented some results on the genre classiﬁcation task. For\nthat task a reduced set of chords (major, minor, and dimin-\nished) was used.\nFinally, Perez-Sancho et al. [9] have investigated if 2,\n3 and 4-grams of chords can be used for automatic genre\nclassiﬁcation on both symbolic and audio data. They report\nbetter classiﬁcation results when using a richer vocabulary\n(seventh chords) and longer n-grams.\n3. METHODOLOGY\nContrary to n-grams that are limited to sequences of length\nnthe ﬁrst-order logic representation scheme that we adopt\ncan employ chord sequences of variable length to charac-\nterise a musical genre. A musical piece is represented as\na list of chords. Each musical genre is illustrated by a se-\nries of musical pieces. The objective is to ﬁnd interesting\npatterns, i.e. chord sequences, that appear in many songs\nof one genre and do not (frequently) appear in the other\ngenres and use such sets of patterns to classify unknown\nmusical pieces into genres. As there can be several inde-\npendent patterns and each of them can be of any length\nwe use a context-free deﬁnite-clause grammar formalism.Finally to induce such grammars we use TILDE [10], a\nﬁrst-order logic decision tree induction algorithm.\n3.1 Knowledge representation\nIn the deﬁnite clause grammar (DCG) formalism a sequence\nover a ﬁnite alphabet of letters is represented as a list of\nletters. Here the chords (e.g. G7, Db, BM7, F#m7, etc.)\nare the letters of our alphabet. A DCG is described using\npredicates. For each predicate p/2 (orp/3) of the form\np(X,Y) (orp(c,X,Y) ),Xis a list representing the se-\nquence to analyse (input) and Yis the remaining part of the\nlistXwhen its preﬁx matching the predicate p(or property\ncof the predicate p) is removed (output). In the context-\nfree grammar (CFG) formalism, a target concept is deﬁned\nwith a set of rules.\nHere our target predicate is genre/4 , where genre(g,\nA,B,Key) means the song A(represented as its full list\nof chords) in the tonality Key belongs to genre g. The\nargument B, the output list (i.e. an empty list) is neces-\nsary to comply with the deﬁnite-clause grammar represen-\ntation. We are interested in degrees and chord categories\nto characterise a chord sequence. So the predicates consid-\nered to build the rules are degreeAndCategory/5 and\ngap/2 , deﬁned in the background knowledge (cf. Table\n1). degreeAndCategory(d,c,A,B,Key) means\nrootNote(c ,[c|T],T,Key). rootNote(c ,[cm |T],T,Key).\nrootNote(c s,[cs |T],T,Key). rootNote(c s,[csm |T],T,Key).\n. . . . . .\ncategory(min,[cm |T],T). category(maj,[c |T],T).\ncategory(min,[csm |T],T). category(maj,[cs |T],T).\n. . . . . .\ndegree(1 ,A,B,cmajor) :- rootNote(c ,A,B,cmajor).\ndegree(1 s,A,B,cmajor) :- rootNote(c s,A,B,cmajor).\n. . .\ndegreeAndCategory(Deg,Cat,A,B,Key) :-\ndegree(Deg,A,B,Key), category(Cat,A,B).\ngap(A,A).\ngap([ ,A],B) :- gap(A,B).\nTable 1 . Background knowledge predicates used in the\nﬁrst-order logic decision tree induction algorithm. For\neach chord in a chord sequence its root note is identiﬁed us-\ning the rootNote/4 predicate. The degrees are deﬁned\nusing the degree/4 predicate and the key. The chord\ncategories are identiﬁed using the category/3 predicate\nand ﬁnally degrees and categories are united in a single\npredicate degreeAndCategory/5 .\nthat the ﬁrst chord of the list Ahas degree dand category c.\nThegap/2 predicate matches any chord sequence of any\nlength, allowing to skip uninteresting subsequences (not\ncharacterised by the grammar rules) and to handle large\nsequences (for which otherwise we would need very large\ngrammars). In addition we constrain the system to use at\nleast two consecutive degreeAndCategory predicates\nbetween two gap predicates. This guarantees that we are\nconsidering local chord sequences of a least length 2 (but\nalso larger) in the songs.\n67010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n∧ degreeAndCategory(6_,min,E,F,Key)g1g2?degAndCat(1_,maj,A,C,Key)degAndCat(7_,min,A,C,Key)gap(A,C) ∧ degAndCat(1_,maj,C,D,Key)   ∧ degAndCat(5_,7,D,E,Key)gap(A,C) ∧ degAndCat(5_,7,C,D,Key)   ∧ degAndCat(6_,min,D,E,Key)gap(A,C) ∧ degAndCat(5_,7,C,D,Key)   ∧ degAndCat(2_,min,D,E,Key)gap(A,C) ∧ degAndCat(7_,min,C,D,Key)   ∧ degAndCat(1_,maj,D,E,Key)gap(A,C) ∧ degAndCat(5_,7,C,D,Key)   ∧ degAndCat(3_,min,D,E,Key)gap(A,C) ∧ degAndCat(3_,min,C,D,Key)   ∧ degAndCat(5_,7,D,E,Key) g1g2g2 | g1g3 g1g3 | g1g2g2g1g1g2g2 | g3 * g1g1g2 | g2g3 g2 | g1g1g2g3 g1g3 | g1g2g2 g2 | g1g1g2g3 g2 | g1g1g2g3Learning examples : [C,G7,Am] g1      [C,G7,Dm] g2      [Bm,C] g3      [Bm,C,G7,Am] g1      [C,G7,Em,G7,Am] g2\ndegAndCat(6_,min,E,F,Key)degAndCat(2_,min,E,F,Key)degAndCat(3_,min,E,F,Key)gap(E,F) ∧ degAndCat(3_,min,F,G,Key)    ∧ degAndCat(5_,7,G,H,Key)gap(E,F) ∧ degAndCat(5_,7,F,G,Key)    ∧ degAndCat(6_,min,G,H,Key) g1g1 | g2g2 *  g2 | g1g1g2 g2 | g1g1g2 g2 | g1g1g2 g2 | g1g1g2gap(A,C) ∧ degreeAndCategory(1_,maj,C,D,Key)               ∧ degreeAndCategory(5_,7,D,E,Key)g3Equivalent set of rules (Prolog program):genre(g1,A,B,Key) :-   gap(A,C),degAndCat(1_,maj,C,D,Key),  degAndCat(5_,7,D,E,Key),degAndCat(6_,min,E,F,Key),!genre(g2,A,B,Key) :-    gap(A,C),degAndCat(1_,maj,C,D,Key),   degAndCat(5_,7,D,E,Key),!genre(g3,A,B,Key).gap(A,C) ∧ degreeAndCategory(1_,maj,C,D,Key)              ∧ degreeAndCategory(5_,7,D,E,Key)g3?TrueFalseTrueFalseTrueFalseFigure 1 . Schematic example illustrating the induction of a ﬁrst-order logic tree for a 3-genre classiﬁcation problem (based\non the 5 learning examples on top). At each step the partial tree (top) and each literal (or conjunction of literals) considered\nfor addition to the tree (bottom) are shown together with the split resulting from the choice of this literal (e.g. g1g1g2 |g2\nmeans that two examples of g1 and one of g2 are in the left branch and one example of g2 is in the right branch). The literal\nresulting in a the best split is indicated with an asterisk. The ﬁnal tree and the equivalent ordered set of rules (or Prolog\nprogram) are shown on the right. The key is C Major for all examples. For space reasons degAndCat is used to represent\ndegreeAngCategory .\nAn example of a simple and short grammar rule we can\nget using this formalism is:\ngenre(genre1,A,B,Key) :-\ngap(A,C),degreeAndCategory(5 ,7,C,D,Key),\ndegreeAndCategory(1 ,maj,D,E,Key),gap(E,B).\nWhich can be translated as : “Some music pieces of genre1\ncontain a dominant 7th chord on the dominant followed by\na major chord on the tonic” (i.e. a perfect cadence).\nBut more complex rules combining several local patterns\n(of any length larger than or equal to 2) separated by gaps\ncan also be constructed with this formalism.\n3.2 Learning algorithm\nTo induce the harmony grammars we apply TILDE’s deci-\nsion tree induction algorithm [10]. TILDE is a ﬁrst order\nlogic extension of the C4.5 decision tree algorithm [11].\nLike C4.5 it is a top-down decision tree induction algo-\nrithm: at each step the test resulting in the best split is\nused to partition the examples. The difference is that at\neach node of the trees instead of attribute-value pairs, con-\njunctions of literals are tested. TILDE uses by default the\ngain-ratio criterion [11] to determine the best split and the\npost-pruning is the one from C4.5. TILDE builds ﬁrst-\norder logic decision trees which can also be represented\nas ordered sets of rules (or Prolog programs). In the case\nof classiﬁcation, the target predicate of each model rep-\nresents the classiﬁcation problem. A simple example il-\nlustrating the induction of a tree from a set of examples\ncovering three genres is given in Figure 1.\nFirst-order logic enables us to use background knowl-\nedge (which is not possible with non relational data min-\ning algorithms). It also provides a more expressive way\nto represent musical concepts/events/rules which can be\ntransmitted as they are to the users. Thus the classiﬁcation\nprocess can be made transparent to the user.4. EXPERIMENTS AND RESULTS\n4.1 Training data\n4.1.1 Audio data\nThe data used in the experiments reported in this paper has\nbeen collected, annotated and kindly provided by the Pat-\ntern Recognition and Artiﬁcial Intelligence Group of the\nUniversity of Alicante. It consists in a collection of Band\nin a Box1ﬁles (i.e. symbolic ﬁles containing chords) from\nwhich audio ﬁles have been synthesised and it covers three\ngenres: popular, jazz, and academic music. The symbolic\nﬁles have been converted into a text format in which only\nthe chord changes are available. The Popular music set\ncontains pop, blues, and celtic (mainly Irish jigs and reels)\nmusic; jazz consists of a pre-bop class grouping swing,\nearly, and Broadway tunes, bop standards, and bossanovas;\nand academic music consists of Baroque, Classical and\nRomantic Period music. All the categories have been de-\nﬁned by music experts who have also collaborated in the\ntask of assigning meta-data tags to the ﬁles and rejecting\noutliers. The total amount of pieces is 856 (Academic\n235; Jazz 338; Popular 283) containing a total of 120,510\nchords (141 chords per piece in average, a minimum of 3\nand a maximum of 522 chords per piece).\nThe classiﬁcation tasks that we are interested in are rela-\ntive to the three main genres of this dataset: academic, jazz\nand popular music. For all our experiments we consider\neach time the 3-way classiﬁcation problem and each of the\n2-way classiﬁcation problems. In addition we also study\nthe 3-way classiﬁcation problem dealing with the popu-\nlar music subgenres (blues, celtic and pop music). We do\nnot work on the academic subgenres and jazz subgenres\nas these two datasets contain very unbalanced subclasses,\n1http://www.pgmusic.com/products bb.htm\n671Poster Session 4\nsome of them being represented by only a few examples.\nBecause of this last characteristic removing examples to\nget the same number of examples per class would lead to\npoor models built on too few examples. Finally resampling\ncan not be used as TILDE automatically removes identical\nexamples.\nFor each classiﬁcation task we perform a 5-fold cross-\nvalidation. The minimal coverage of a leaf (a parameter in\nTILDE) is set to 5.\nacademic/jazz/popular Root Int D&C 3 D&C 7th\nAccuracy (baseline = 0.40) 0.619 0.759 0.808\nStderr 0.017 0.015 0.014\n# nodes in the tree 40.8 31.0 18.4\n# literals in the tree 66.2 90.6 50.8\nacademic/jazz Root Int D&C 3 D&C 7th\nAccuracy (baseline = 0.59) 0.861 0.872 0.933\nStderr 0.014 0.014 0.011\n# nodes in the tree 11.0 16.4 10.4\n# literals in the tree 19.0 46.0 30.8\nacademic/popular Root Int D&C 3 D&C 7th\nAccuracy (baseline = 0.54) 0.731 0.824 0.839\nStderr 0.020 0.017 0.016\n# nodes in the tree 17.0 12.4 11.0\n# literals in the tree 27.6 36.4 31.8\njazz/popular Root Int D&C 3 D&C 7th\nAccuracy (baseline = 0.55) 0.828 0.811 0.835\nStderr 0.015 0.016 0.015\n# nodes in the tree 13.4 17.0 10.6\n# literals in the tree 23.2 50.6 29.0\nblues/celtic/pop Root Int D&C 3 D&C 7th\nAccuracy (baseline = 0.36) 0.709 0.703 0.746\nStderr 0.027 0.028 0.026\n# nodes in the tree 11.4 16.2 14.0\n# literals in the tree 20.4 45.8 40.4\nTable 2 . Classiﬁcation results on manual chord transcrip-\ntions using a 5-fold cross-validation. The number of nodes\nand literals present in a tree gives an estimation of its com-\nplexity. “Root Int” refers to the root intervals representa-\ntion scheme. “D&C 3” and “D&C 7th” refers to the degree\nand chord category representation scheme respectively ap-\nplied on triads only and on triads and seventh chords.\n4.1.2 Chord transcription\nThe chord transcription algorithm based on harmonic pitch\nclass proﬁles (HPCP [12]) we apply is described in [13]. It\ndistributes spectral peak contributions to several adjacent\nHPCP bins and takes peak harmonics into account. In ad-\ndition to using the local maxima of the spectrum, HPCPs\nare tuning independent (i.e. the reference frequency can be\ndifferent from the standard tuning), and consider the pres-\nence of harmonic frequencies. In this paper, the resulting\nHPCP is a 36-bin octave independent histogram represent-\ning the relative intensity of each 1/3 of the 12 semitones of\nthe equal tempered scale. We refer to [13] for a detailed\ndescription of the algorithm.\nThe algorithm can be tuned to either extract triads (lim-\nited to major and minor chords) or triads and seventh chords(limited to major seventh, minor seventh and dominant sev-\nenth). Other chords such as diminished and augmented\nchords are not included in the transcription (as in many\ntranscription systems) because of the tradeoff between pre-\ncision and accuracy. After pre-processing, only the chord\nchanges (i.e. when either the root note or the chord cate-\ngory is modiﬁed) are kept. Notice that when dealing with\nthe symbolic ﬁles (manual transcription) the mapping be-\ntween the representations is based on the third (major or\nminor). Since only the chord changes were available in the\nsymbolic ﬁles (no timing information) it was not possible\nto compute the transcription accuracy.\n4.2 Validating our new harmony representation\nscheme\nWe ﬁrst study if our new harmony representation scheme\nbased on degrees and chord categories (D&C) can compete\nwith our previous representation scheme based on root in-\ntervals (Root Int.). For that we test these two harmony\nrepresentations on clean data, i.e. on the manual chord\ntranscriptions. We test the degree and chord category rep-\nresentation scheme on both triads-only (D&C 3) and triads\nand seventh manual transcriptions (D&C 7th). The results\n(i.e. test results of the 5-fold cross-validation) of these ex-\nperiments are shown in Table 2.\nThe D&C representation scheme obtains better results,\nwith accuracies always as high as or higher than the root in-\nterval representation scheme classiﬁcation accuracies. Fur-\nthermore the complexity of the models is not increased\nwhen using the D&C representation compared to the root\ninterval representation. Indeed, the number of nodes and\nliterals in the built models (trees) are comparable. Using\nthe seventh chord categories leads to much higher accu-\nracies, lower standard errors and lower complexity than\nwhen only using the triads.\nWe also tested these representation schemes when the\nlearning examples are audio ﬁles (cf. Section 4.3 for more\ndetails on these experiments). However the root interval\nexperiments on audio data were so slow that we were un-\nable to complete a 5-fold cross-validation. We estimate the\ntime needed to build one (2-class) model based on the root\ninterval audio data to 12 hours in average, whereas only 10\nto 30 minutes are needed to build a D&C 3 (2-class) model\non audio data and around 1 hour and a half for a D&C\n7th (2-class) model. In conclusion the degree and category\nrepresentation scheme outperforms the root interval repre-\nsentation scheme on both classiﬁcation accuracy and run\ntimes.\n4.3 Performances on audio data\nWe now test if our ﬁrst-order logic classiﬁcation frame-\nwork can build good classiﬁcation models when the learn-\ning examples are automatic chord transcriptions from au-\ndio ﬁles (i.e. noisy data). This is essential for the many\napplications in which no symbolic representation of the\nharmony is available. The results of this framework when\nusing the degree and chord category representation scheme\non audio data are shown in Table 3.\n67210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nacademic/jazz/popular D&C 3 D&C 7th\nAccuracy (baseline = 0.39) 0.582 0.575\nStderr 0.017 0.017\n# nodes in the tree 59.2 66.8\n# literals in the tree 171.2 198.4\nacademic/jazz D&C 3 D&C 7th\nAccuracy (baseline = 0.59) 0.759 0.743\nStderr 0.018 0.018\n# nodes in the tree 26.4 31.8\n# literals in the tree 76.0 93.8\nacademic/popular D&C 3 D&C 7th\nAccuracy (baseline = 0.55) 0.685 0.674\nStderr 0.020 0.021\n# nodes in the tree 25.8 26.4\n# literals in the tree 72.2 74.0\njazz/popular D&C 3 D&C 7th\nAccuracy (baseline = 0.54) 0.789 0.773\nStderr 0.016 0.017\n# nodes in the tree 22.4 28.8\n# literals in the tree 66.0 86.0\nblues/celtic/pop D&C 3 D&C 7th\nAccuracy (baseline = 0.35) 0.724 0.668\nStderr 0.027 0.028\n# nodes in the tree 13.2 14.8\n# literals in the tree 38.8 43.2\nTable 3 . Classiﬁcation results on audio data using a 5-fold\ncross-validation.\nAlthough the accuracies are still good (signiﬁcantly abo-\nve the baseline), it is not surprising that they are lower than\nthe results obtained for clean data (i.e. manual transcrip-\ntions). The noise introduced by the automatic chord tran-\nscription also leads to a higher complexity of the models\nderived from audio data. Also using the seventh chords\nleads to slightly less accurate models than when using tri-\nads only. The opposite result was obtained with the manual\ntranscription data, where the seventh chord representation\nscheme outperformed the triads representation scheme. We\nsurmise that the reason for this difference is the fact that the\nautomatic chord transcription algorithm we use is much\nless accurate when asked to use seventh chords than when\nasked to use triads only.\nConcerning the classiﬁcation tasks, all the 2 and 3-class\nproblems are solved with accuracies well above chance\nlevel. The 3-class popular music subgenres classiﬁcation\nproblem seems particularly well handled by our framework\nwith 72% and 67% accuracy when using respectively tri-\nads and seventh chords. The best 2-class classiﬁcation re-\nsults (between 74% and 79% accuracy) are obtained when\ntrying to distinguish jazz from another genre (academic or\npopular). Indeed the harmony of classical and popular mu-\nsic can be very similar, whereas jazz music is known for its\ncharacteristic chord sequences, very different from other\ngenres harmonic progressions.\n4.4 Transparent classiﬁcation models\nTo illustrate the transparency of the classiﬁcation models\nbuilt using our framework we present here some interest-ing rules with high coverage extracted from classiﬁcation\nmodels generated from symbolic data. Notice that the clas-\nsiﬁcation models are trees (or ordered sets of rules), so a\nrule in itself can not perform classiﬁcation both because of\nhaving a lower accuracy than the full model and because\nthe ordering of rules in the model is important to the classi-\nﬁcation (i.e. some rule might never be used on some exam-\nple because one of the preceding rules in the model covers\nthis example). To illustrate this for each of the following\nexample rules we provide its absolute coverage (i.e. if the\norder was not taken into account) on each genre.\nThe following rule was found in the popular subgenres\nclassiﬁcation models:\n[coverage: blues=42/84; celtic=0/99; pop=2/100 ]\ngenre(blues,A,B,Key) :-\ngap(A,C),degreeAndCategory(1 ,7,C,D,Key),\ndegreeAndCategory(4 ,7,D,E,Key),gap(E,B).\n“Some blues music pieces contain a dominant seventh chord\non the tonic directly followed by a dominant seventh chord\non the subdominant (IV). ”\nThe following rules were found in the academic/jazz/pop-\nular classiﬁcation models:\n[cov.: jazz=273/338; academic=42/235; popular=52/283 ]\ngenre(jazz,A,B,Key) :-\ngap(A,C),degreeAndCategory(2 ,min7,C,D,Key),\ndegreeAndCategory(5 ,7,D,E,Key),gap(E,B).\n“Some jazz music pieces contain a minor seventh chord on\nthe supertonic (II) directly followed by a dominant seventh\nchord on the dominant. ”\n[cov.: jazz=173/338; academic=1/235; popular=17/283 ]\ngenre(jazz,A,B,Key) :-\ngap(A,C),degreeAndCategory(6 ,7,C,D,Key),\ndegreeAndCategory(2 ,min7,D,E,Key),gap(E,B)\n“Some jazz music pieces contain a dominant seventh chord\non the submediant (VI) directly followed by a minor sev-\nenth chord on the supertonic (II). ”\nFinally the following rules were found in the academic/\njazz classiﬁcation models:\n[cov.: academic=124/235; jazz=6/338; popular=78/283 ]\ngenre(academic,A,B,Key) :-\ngap(A,C),degreeAndCategory(1 ,maj,C,D,Key),\ndegreeAndCategory(5 ,maj,D,E,Key),gap(E,B).\n“Some academic music pieces contain a major chord on\nthe tonic directly followed by a major chord on the domi-\nnant. ”\n[cov.: academic=133/235; jazz=10/338; popular=68/283 ]\ngenre(academic,A,B,Key) :-\ngap(A,C),degreeAndCategory(5 ,maj,C,D,Key),\ndegreeAndCategory(1 ,maj,D,E,Key),gap(E,B).\n“Some academic music pieces contain a major chord on\nthe dominant directly followed by a major chord on the\ntonic. ”\nNote that the lack of sevenths distinguishes this last com-\nmon chord change from its jazz counterparts. Indeed the\nfollowing rule has a high coverage on jazz:\n[cov.: jazz=146/338; academic=0/235; popular=15/283 ]\ngenre(jazz,A,B,Key) :-\ngap(A,C),degreeAndCategory(5 ,7,C,D,Key),\ndegreeAndCategory(1 ,maj7,D,E,Key),gap(E,B).\n673Poster Session 4\n5. CONCLUSION AND FUTURE WORK\nIn this paper we showed that our genre classiﬁcation frame-\nwork based on harmony and ﬁrst-order logic and previ-\nously tested on symbolic data in [3] can also directly learn\nclassiﬁcation models from audio data that obtain a classi-\nﬁcation accuracy well above chance level. The use of a\nchord transcription algorithm allows us to adopt a high-\nlevel representation of harmony even when working on\naudio data. In turn this high-level representation of har-\nmony based on ﬁrst-order logic allows for human-readable,\ni.e. transparent, classiﬁcation models. We increased this\ntransparency by introducing a new harmony representa-\ntion scheme, based on the western representation of har-\nmony which describes the chords in terms of degrees and\nchord categories. This representation is not only musi-\ncally more meaningful than a previous representation we\nadopted, it also got better classiﬁcation results and the clas-\nsiﬁcation models using it were built faster. Testing our\nmodel on manual transcriptions we observed that using\nseventh chords in the transcription task could consider-\nably increase the classiﬁcation accuracy. However the au-\ntomatic transcription algorithm we used for these experi-\nments was not enough accurate when using seventh chords\nand we could not observe such improvements when using\naudio data.\nFuture work includes testing several other chord tran-\nscription algorithms to see if they would lead to better clas-\nsiﬁcation models when using seventh chords. We also plan\nto use these chord transcription algorithms to study how\nthe accuracy of classiﬁcation models built on transcriptions\nevolves with the accuracy of these transcriptions. In addi-\ntion the audio data used in these experiments was gener-\nated with MIDI synthesis. This is generally cleaner than\nCD recordings, so we expect a further degradation in re-\nsults if we were to use audio recordings. Unfortunately we\ndo not possess the corresponding audio tracks that would\nallow us to make this comparison. We intend to look for\nsuch recordings and extend our audio tests to audio ﬁles\nthat are not generated from MIDI. Finally with these ex-\nperiments we showed that a classiﬁcation system based\nonly on chord progressions can obtain classiﬁcation results\nwell above chance level. If such a model based only on\none dimension of music (harmony) can not compete on its\nown with state-of-the-art classiﬁcation models, we believe\n– and intend to test this hypothesis in future experiments\n– that if such an approach is combined with classiﬁcation\nmodels based on other dimensions (assumed orthogonal)\nsuch as rhythm and timbre we will improve on state-of-\nthe-art classiﬁcation accuracy.\n6. ACKNOWLEDGMENTS\nWe would like to thank the Pattern Recognition and Arti-\nﬁcial Intelligence Group of the University of Alicante for\nproviding the data. This work is supported by the EPSRC\nproject OMRAS2 (EP/ E017614/1) and the Spanish TIN\nproject ProSeMus (TIN2006-14932-C02-01). During her\ninternship at the Music Technology Group the ﬁrst author\nwas supported by the EPSRC Platform grant EP/E045235/1.7. REFERENCES\n[1] G. Tzanetakis, A. Ermolinskiy, and P. Cook. Pitch his-\ntograms in audio and symbolic music information re-\ntrieval. In Proceedings of ISMIR 2002 , Paris, France,\n2002.\n[2] W. Piston. Harmony . Norton, W. W. & Company, Inc.,\n5th edition, 1987.\n[3] Am ´elie Anglade, Rafael Ramirez, and Simon Dixon.\nFirst-order logic classiﬁcation models of musical gen-\nres based on harmony. In Proceedings of the 6th Sound\nand Music Computing Conference , Porto, Portugal,\n2009.\n[4] T. Yoshioka, T. Kitahara, K. Komatani, T. Ogata,\nand H. G. Okuno. Automatic chord transcription with\nconcurrent recognition of chord symbols and bound-\naries. In Proceedings of ISMIR 2004 , pages 100–105,\nBarcelona, Spain, 2004.\n[5] K. Lee and M. Slaney. Acoustic chord transcription and\nkey extraction from audio using key-dependent HMMs\ntrained on synthesized audio. IEEE Transactions on\nAudio, Speech, and Language Processing , 16(2):291–\n301, 2008.\n[6] M.-K. Shan, F.-F. Kuo, and M.-F. Chen. Music style\nmining and classiﬁcation by melody. In Proceedings\nof 2002 IEEE International Conference on Multimedia\nand Expo , volume 1, pages 97–100, 2002.\n[7] J.-F. Paiement, D. Eck, and S. Bengio. A probabilistic\nmodel for chord progressions. In Proceedings of ISMIR\n2005 , pages 312–319, London, UK, 2005.\n[8] K. Lee. A system for automatic chord transcription us-\ning genre-speciﬁc hidden markov models. In Proceed-\nings of the International Workshop on Adaptive Multi-\nmedia Retrieval , Paris, France, 2007.\n[9] C. Perez-Sancho, D. Rizo, S. Kersten, and R. Ramirez.\nGenre classiﬁcation of music by tonal harmony. In In-\nternational Workshop on Machine Learning and Mu-\nsic, Helsinki, Finland, 2008.\n[10] Hendrik Blockeel and Luc De Readt. Top down in-\nduction of logical decision trees. Artiﬁcial Intelligence ,\n101(1-2):285–297, 1998.\n[11] J. Ross Quinlan. C4.5: programs for machine learning .\nMorgan Kaufmann Publishers Inc., 1993.\n[12] T. Fujishima. Realtime chord recognition of musical\nsound: a system using common lisp music. In Proceed-\nings of the 1999 International Computer Music Confer-\nence, pages 464–467, Beijing, China, 1999.\n[13] E. G ´omez. Tonal Description of Music Audio Signals .\nPhD thesis, MTG, Universitat Pompeu Fabra, 2006.\n674"
    },
    {
        "title": "Supporting Folk-Song Research by Automatic Metric Learning and Ranking.",
        "author": [
            "Korinna Bade",
            "Andreas Nürnberger",
            "Sebastian Stober",
            "Jörg Garbers",
            "Frans Wiering"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418165",
        "url": "https://doi.org/10.5281/zenodo.1418165",
        "ee": "https://zenodo.org/records/1418165/files/BadeNSGW09.pdf",
        "abstract": "In folk song research, appropriate similarity measures can be of great help, e.g. for classification of new tunes. Several measures have been developed so far. However, a particular musicological way of classifying songs is usually not directly reflected by just a single one of these measures. We show how a weighted linear combination of different basic similarity measures can be automatically adapted to a specific retrieval task by learning this metric based on a special type of constraints. Further, we describe how these constraints are derived from information provided by experts. In experiments on a folk song database, we show that the proposed approach outperforms the underlying basic similarity measures and study the effect of different levels of adaptation on the performance of the retrieval system.",
        "zenodo_id": 1418165,
        "dblp_key": "conf/ismir/BadeNSGW09",
        "keywords": [
            "appropriate similarity measures",
            "classification of new tunes",
            "further",
            "weighted linear combination",
            "special type of constraints",
            "expert information",
            "retrieval task",
            "basic similarity measures",
            "performance of the retrieval system",
            "effect of different levels of adaptation"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSUPPORTING FOLK-SONG RESEARCH BY AUTOMATIC METRIC\nLEARNING AND RANKING\nKorinna Bade, Andreas N ¨urnberger, Sebastian Stober\nOtto-von-Guericke University Magdeburg\nFaculty of Computer Science\n{korinna.bade,andreas.nuernberger,\nstober}@ovgu.deJ¨org Garbers, Frans Wiering\nUtrecht University\nDepartment of Information\nand Computing Sciences\n{garbers,fransw }@cs.uu.nl\nABSTRACT\nIn folk song research, appropriate similarity measures can\nbe of great help, e.g. for classiﬁcation of new tunes. Sev-\neral measures have been developed so far. However, a par-\nticular musicological way of classifying songs is usually\nnot directly reﬂected by just a single one of these measures.\nWe show how a weighted linear combination of different\nbasic similarity measures can be automatically adapted to\na speciﬁc retrieval task by learning this metric based on a\nspecial type of constraints. Further, we describe how these\nconstraints are derived from information provided by ex-\nperts. In experiments on a folk song database, we show that\nthe proposed approach outperforms the underlying basic\nsimilarity measures and study the effect of different levels\nof adaptation on the performance of the retrieval system.\n1. INTRODUCTION\nFolk song researchers detect and document relations be-\ntween folk songs and their performances. This helps to\nunderstand oral transmission. Today, folk song researchers\ncan digitally encode their transcriptions using common mu-\nsic notation editors and use computational methods to de-\ntect similarities between songs. However, as there are dif-\nferent ways to detect features in music, there are many dif-\nferent ways to compare songs. Usually, a single compu-\ntational similarity value will not match directly with the\nclassiﬁcation criteria that a musicologist applies. There-\nfore, we choose a weighted linear combination of different\nbasic similarity measures. Depending on the retrieval task\nat hand, the optimal weighting (with best retrieval perfor-\nmance) of such a complex similarity measure may differ.\nIn this paper, we describe a metric learning approach\nthat can derive a good weighting in a semi-supervised man-\nner. We apply constraint-based metric learning and formal-\nize the weight adaptation as an optimization problem that\nis solved by gradient descent. Constraints that guide the\nadaptation process can be derived from an existing classi-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.ﬁcation of tunes from a collection. We compare different\nways of employing the derived similarities to support dif-\nferent browsing and classiﬁcation tasks in a system that\naccepts both previous classiﬁed and unclassiﬁed queries.\nThe main contribution of this paper lies in describ-\ning and evaluating a general methodology that allows folk-\nsong researches to automatically generate complex task-\nspeciﬁc similarity metrics from basic similarity measures.\n2. RELATED WORK\nMetric learning has been a topic of interest in general in-\nformation retrieval for some time, as using a suitable simi-\nlarity measure is crucial for the performance of many com-\nmonly used approaches for clustering, classiﬁcation or rank-\ning. The general objective is either to get a query closer to\nthe relevant objects (in a classic retrieval scenario) or to re-\nﬁne the decision boundary between relevant and irrelevant\nobjects (in a classiﬁcation scenario which does not nec-\nessarily require a query). The highly subjective nature of\nperception and the large variety of ways to represent and\ncompare music in many “plausible” ways make it hard to\nmanually deﬁne and tweak a metric according to the char-\nacteristics of the input data and the speciﬁc retrieval task.\nConsequently, there exist only few approaches for direct\nmanipulation of the metric as described e.g. in [1] and [2].\nIn contrast to this, our approach allows a semi-supervised\nmetric adaptation. This requires some labeled objects as\ntraining data. For the experiments discussed in this paper,\nsuch data was already provided. However, if such informa-\ntion is not available a priori, a relevance feedback approach\nis usually taken where a user is asked to judge on the rele-\nvance of some objects.\nThe idea of incorporating relevance feedback to improve\nthe performance of an information retrieval system goes\nback to the 1970s [3]. Since, it has been widely applied\nand further elaborated – primarily in text but also in im-\nage retrieval. Recently, in the ﬁeld of music information\nretrieval, several approaches using explicit feedback have\nbeen presented that adaptively combine the results of dif-\nferent music representation schemes [4], associate differ-\nent music similarity perception models with users [5], learn\nto discriminate between similar and dissimilar pieces [6],\nadapt to the way of querying by taking into account user-\nspeciﬁc humming errors [7], or generate user-adaptive play\n741Oral Session 9-A: Folk Songs\nlists [8–11]. Alternatively, the required information may\nbe collected through the analysis of user actions such as\nthe skipping behavior [12] or manual rearrangement of ob-\njects on a map through drag & drop [13].\nAll these approaches are related to the one presented in\nthis paper in that they rely on some form of metric adapta-\ntion. Our approach differs from these in two ways. First, it\ntargets a signiﬁcantly different application scenario. Sec-\nond, none of the above approaches is based on constrained\nmetric learning, which is applied here, except for the ap-\nproach presented in [13] that uses similar constraints to\nguide the clustering of a self-organizing map.\n3. OUR APPROACH\nThe goal of this work is to assist a folk song researcher\nin classifying new tunes (Section 3.1). Fundamental to\nthis task is the computation of similarities between tunes.\nDifferent measures were developed in the past (Section\n3.2). However, a particular musicological way of classi-\nfying songs is usually not directly reﬂected by just one of\nthese measures. We show here how a weighted measure\nderived from a certain classiﬁcation scheme can be auto-\nmatically learned (Section 3.3). Based on this measure, we\ncan support the classiﬁcation of new items by presenting\na ranked list (ordered by similarity) of already classiﬁed\ntunes (Section 3.4).\n3.1 Expert classiﬁcation support\nFolk song researchers at the Meertens Institute study and\nclassify folk song variants. Besides other means, songs\nare traditionally classiﬁed by assigning them a so called\nmelody norm . This classiﬁcation captures aspects of musi-\ncal similarity and historical relationships. One tune cannot\nbe part of more than one class.\nThe WITCHCRAFT project supports researchers by pro-\nviding a system that enables browsing by musical content.\nThe system’s similarity measures operate on symbolic rep-\nresentations of tunes (Humdrum ∗∗kern and MIDI for-\nmat). A query melody is usually speciﬁed by clicking on a\nsearch link besides a database item. The system then ranks\ndatabase tunes according to a chosen similarity measure.\nTwo types of ranking lists are supported by switching\non/off a ﬁlter that is based on tune classiﬁcation. In unﬁl-\ntered mode, the tune-ranking-list presents all tunes ordered\nby similarity. This is handy when looking for all variants\nof a given song. In ﬁltered mode, the class-ranking-list\npresents only the best ranked melody from each class.1\nTherefore, much fewer items are shown. This is handy\nwhen classifying a previously unclassiﬁed song or when\nquestioning an existing classiﬁcation.\n3.2 Basic tune similarity measures\nIn this paper, we distinguish between basic similarity mea-\nsuressimj(t1,t2)as introduced in the following and lin-\near combinations thereof (Eq. (1)). Note that the basic\n1We found that taking the maximum leads to better results than taking\nthe average of all the similarities.similarity measures in this paper are themselves complex\nconstructions of often more basic musical and mathemati-\ncal transformations [14, 15]. However, in future work, we\nplan to also use more basic building blocks. In our ex-\nperiments, we consider 14 similarity measures. However,\nthe methods proposed in the following work with any set\nof measures. 11 of the 14 measures are taken from the\nSimile package.2These are rawEd ,diffEd ,nGrSumCo ,\nnGrUkkon ,harmCorE ,rhytFuzz ,rhytGaus ,opti1 ,opti3 ,\naccents opti1 andaccents opti2 . Two distance measures\nare based on the spectra of Laplacean andAdjacency graphs\n[16] and one is an unpublished pitch sequence edit dis-\ntance, implemented by us. All distance measures were\ntransformed to a similarity through sim = (1 + dist)−1.\n3.3 Estimating a weighted similarity\nHaving given a certain number of expert classiﬁcations, the\nquestion remains whether we can ﬁnd an optimal weight-\ning of different tune similarity measures to reﬂect the simi-\nlarity underlying these expert classiﬁcations. In particular,\nwe are interested in the following weighted sum of nsim-\nilarity measures:\nsimw(t1,t2) =n/summationdisplay\nj=1wjsimj(t1,t2), (1)\nwithwj≥0, and/summationtextn\nj=1wj= 1.\nThe weight vector wcan be learned by methods of con-\nstrained clustering, which target on learning a metric [17,\n18]. In particular, must-link-before (MLB) constraints [18]\ncan be used. Originally, MLB constraints were proposed\nfor hierarchical clustering to describe the hierarchical rela-\ntion between three different items. The constraint (ix,iy,iz)\nstates that items ixandiyshould be linked on a lower hier-\narchy level than items ixandiz. For our problem at hand,\nwe can use a similarity interpretation instead, i.e., items ix\nandiyshould be more similar than items ixandiz.\nGiven a certain query tune q, we know from the expert\nclassiﬁcation, which other tunes trbelong to the same (rel-\nevant) class and which tunes tiare irrelevant. As the tunes\nof the same class should be ranked ﬁrst and, thus, should\nbe more similar, we can build MLB constraints of the fol-\nlowing form: (q,tr,ti), which implies that\nsim(q,tr)>sim(q,ti). (2)\nHence, the goal is to learn a weight vector w, with which\nthe fewest of the MLB constraints known for a certain\nquery are violated. This can be achieved with a gradient\ndescent search similar to the work in [18]. During learn-\ning, all constraint triples (q,tr,ti)are presented to the al-\ngorithm several times until convergence is reached. If a\nconstraint is violated by the current similarity measure, the\nweighting is updated by trying to maximize\nobj(q,tr,ti) = sim w(q,tr)−simw(q,ti), (3)\n2http://doc.gold.ac.uk/isms/mmm/SIMILE_algo_\ndocs_0.3.pdf\n74210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nwhich can be directly derived from (2). This leads to the\nweight update rule of each individual weight wj\nwj=wj+η∆wj,with (4)\n∆wj=∂obj(q,tr,ti)\n∂wj= sim j(q,tr)−simj(q,ti)(5)\nwhereηis the learning rate deﬁning the step width of each\nadaptation step.\nHowever, this computation does not ensure the bounds\nonwjgiven earlier. To achieve this, an additional step is\nadded that, ﬁrst, sets all negative weights to 0 and then\nnormalizes the weights to sum up to 1. The complete algo-\nrithm is summarized in Figure 1.\nlearnWeights (query tune q, tunesT, expert classiﬁ-\ncationC,∀k= 1..n:similarity simk)\nDetermine constraints MLB fromTandC\nInitialize w:∀j:wj:= 1/n\nrepeat\nfor all (q,tr,ti)∈MLB do\nifsimw(q,tr)≤simw(q,ti)then\n∀j:compute ∆wj\n∀j:wj:= max(0,wj+η∆wj)\nsumw=/summationtext\njwj\n∀j:wj:=wj/sumw\nend if\nend for\nuntil convergence\nreturnw\nFigure 1 . The weight learning algorithm\nThis algorithm learns an individual weighting wqbased\non the set of MLB constraints for a single query q. How-\never, a weighting that works well for several queries would\nbe more useful. In speciﬁc, it is interesting to learn class\nweightings wcl(t)that hold for all queries of the same class\ncl(t)of a tunetand an overall weighting wathat holds\nfor all queries. These can be computed by the same algo-\nrithm using the combined constraint sets from all consid-\nered rankings.\n3.4 Querying with unclassiﬁed tunes\nThe approach from the previous section can learn an op-\ntimal similarity measure based on an expert classiﬁcation.\nIf new tunes are added to a collection, no expert classiﬁca-\ntion is available at ﬁrst and, hence, no weights optimized\nfor this query are available based on which a ranking list\ncould be build. If there is no perfect global measure that\ncan be applied to all queries, a different strategy can be\nfollowed for this query to build the ranking.\nThis is based on the already known good weightings of\nall database tunes, which were determined by the method\ndescribed in the previous section. If we assume that similar\nsongs also have a similar optimal weighted similarity, we\ncan estimate a weighting for the new query tune qby pick-\ning the weighting scheme of the closest tune tbestin the\ndatabase. This can be seen as a case-based approach [19]where each tune in the database and its associated weighted\nsimilarity correspond to a case and these stored cases are\nused to decide how to handle the new case, i.e., how to\nweight concerning the query tune.\nHowever, this does not yet fully solve the problem, be-\ncause a weighting scheme is already required to ﬁnd the\nclosest tune tbestfor a query. A straight forward approach\nis to use an overall weighting wa. Alternatively, the more\nspeciﬁc class weighting wcl(t)or the individual tune weight-\ningwtassociated with each database tune tcan be used,\nbecause we already know that these similarities are well\nsuited for comparing any tune with t.3We will select the\ncase with the largest (local) similarity and use its weight-\ning to ﬁnally rank all tunes in the database according to the\nquery tuneq:wbest= arg max wtsimwt(q,t).\nFor ranking, we can also use any of the weightings asso-\nciated with the closest case tbest, i.e.,wtbest,wcl(tbest)and\nwa, where the latter is obviously the same for any database\ntune. In Section 4.4, we use the notation w2◦w1to in-\ndicate that the ﬁrst step (closest case selection) was per-\nformed using a similarity based on w1and the second step\n(ranking) is based on w2.\n4. EXPERIMENTS\nWe conducted experiments to study how well the different\nweighting techniques perform for already classiﬁed (Sec.\n4.2) and unclassiﬁed tunes (Sec. 4.4) with respect to the\ntwo different ranking lists (Sec. 4.1). Further, we analyzed\nthe stability of the learned weighting schemes (Sec. 4.3).\n4.1 Dataset and measure evaluation method\nOur evaluation is done on 360 well understood single melo-\ndic strophes (one strophe per recording) described in [20].\nThe tunes are classiﬁed into 26 disjunct classes. For each\npair of tunes all 14 basic similarity measures are consid-\nered. These 14·3602similarity values are precalculated\nand need not be recomputed in the learning algorithm and\nin the construction of ranking lists.\nAs in the application system (Sec. 3.1), our algorithms\nproduce for each (combined) similarity measure and query\ntune a tune-ranking-list of all database tunes and a class-\nranking list. Both are ordered by decreasing computed\nsimilarity. All tunes with the same class as the query are\nmarked as being a relevant result. This gives the ground\ntruth for evaluating the ranked lists. As measures, we com-\npute the average precision andaverage recall per rank on\nthe tune-ranking-lists for the set of evaluated queries. For\nevaluation of class-ranking lists, we are interested in the\nposition of the correct class in such a list. We present\nhere the number of misclassiﬁcations at rank 1, the average\nrank of the correct class and the average inverse rank over\nall considered queries. The latter average is less sensitive\nfor single extremely bad class retrievals.\n3Please note that in this case different weightings are used to compute\nthe similarity to different database tunes, which leads to local distortions\nof the similarity space around each case. While such a locally distorted\nmetric is unsuitable for the computation of the entire ranking, it may still\nbe useful to retrieve only tbest as shown in the experiments in Sec. 4.4.\n743Oral Session 9-A: Folk Songs\n4.2 Querying with classiﬁed tunes\nIn this section we study the retrieval performance of learned\nweights (cf. Sec. 3.3) and, thus, whether an automatically\ndetermined combination of different existing similarity mea-\nsures performs better than the individual ones. We con-\nsider three weightings of different speciﬁcity as motivated\nin Section 3.3: query-speciﬁc weighting wq, class-speciﬁc\nweighting wcl(q)and overall weighting wa. The preci-\nsion/recall curves for these cases are shown in Fig. 3 (left).\nAdditionally, the performance plots of the two best basic\nsimilarity measures, rawEd andopti1have been included\ninto the ﬁgure for comparison. Table 1 (top) shows the cor-\nresponding evaluation of the class-ranking-lists, ordered\nby best performance, which gives the same order for all\nthree measures, i.e., average rank of correct class (smaller\nis better), average inverse rank (larger is better), and the\nnumber of wrong classiﬁcations (inspecting the ﬁrst rank).\nNot surprisingly, using wqfor similarity computation\nresults in the best retrieval performance. It marks the upper\nbound of what can be achieved with the learning algorithm.\nFurther, it can be observed that wcl(q)indeed performs bet-\nter than wa. However, if weights get more speciﬁc, the\ndanger of overﬁtting exists. We will discuss this problem\nin Section 4.4. Nevertheless, this evaluation indicates that\nthere might not be a single perfect overall similarity mea-\nsure that can be used in general. Instead data/problem spe-\nciﬁc measures might be needed, which are especially inter-\nesting if they can be determined automatically as through\nour presented method.\nIt is interesting to see, that the overall weight performs\nworse than the best basic similarity measure ( rawEd ) in\nmost precision/recall regions (although only slightly) but\nthatrawEd performs worse than all shown measures for\nthe class-ranking-lists. This is caused by the convergence\nbehavior of the algorithm, which is not guaranteed to ﬁnd\na global optimum but a local one.\n4.3 Stability of the weighting scheme\nIn order to assess the stability of the individual weighting\nschemes throughout the different classes, we conducted\ntwo experiments. In the ﬁrst experiment, we analyzed the\nindividual weightings obtained for the classiﬁed tunes (cf.\nSection 4.2) with respect to the classes. The following two\nmeasures were computed for each class:\nTheaverage pairwise inner-class similarity is computed\nas the average of the similarity of the weightings for all\npairs of tunes within the speciﬁc class:\nsiminner (C) = average\nt1,t2∈C,t1/negationslash=t2/braceleftbig\nsimcos/parenleftbig\nwt1,wt2/parenrightbig/bracerightbig\n.(6)\nAnalogously, the average pairwise cross-class similarity is\ncomputed as the average of the similarity of the weightings\nfor all pairs of tunes where one tune belongs to the speciﬁc\nclass and the other to a different class:\nsimcross(C) = average\nt1∈C,t2/∈C/braceleftbig\nsimcos/parenleftbig\nwt1,wt2/parenrightbig/bracerightbig\n.(7)\n 0 0.2 0.4 0.6 0.8 1\n 5  10  15  20  25similarity\nclass (melody norm)avg pairwise inner class sim\navg pairwise cross class simFigure 2 . Average pairwise inner- and cross-class similar-\nity of the individual weightings per class (sorted).\nFor the comparison of two weightings the cosine similarity\nsimcos/parenleftbig\nwt1,wt2/parenrightbig\n=wt1·wt2\n/bardblwt1/bardbl/bardblwt2/bardbl(8)\nwas used. Fig. 2 shows the computed values for all classes\n(sorted by descending inner-class value for better readabil-\nity). The inner-class value is always signiﬁcantly higher\nthan the respective cross-class value. It can be concluded\nthat the individual weightings of tunes belonging to the\nsame class are in general distinct from those belonging\nto others which explains the usefulness of class weights\n(wcl(q)in Sec. 4.2). The generally high cross-class values\n(above 0.5) can be interpreted as an indicator for the exis-\ntence of a useful overall weighting scheme ( wain Sec. 4.2).\nFor the second experiment, we left out one to ﬁve rel-\nevant tunes selected randomly from a ranking during the\nlearning process for individual weights. The procedure\nwas repeated ten times for each number of excluded tunes.\nWe then compared the 5·10resulting weighting schemes\nwith the one learned with all available information. To save\ntime, we limited the number of tunes used as queries to two\nfor each of the 26 classes resulting in 2·26·5·10 = 2600\nsamples compared.\nThe number of excluded tunes did not seem to have a\nlarge observable effect in our experiment. The different\nweights learned for the same tune with a differing set of\nrelevant tunes were almost identically with an average sim-\nilarity of 0.969(σ= 0.086). Only a few outliers could be\nmeasured, the worst with a minimal similarity of 0.278.\nFrom the results we can conclude that the learning algo-\nrithm still produces stable results even if almost half of the\nrelevant tunes are removed.\n4.4 Querying with unclassifed tunes\nIn this section we study the retrieval performance for pre-\nviously unclassiﬁed tunes, i.e., for which no previously\nlearned weight wqorwcl(q)exists. Following the case-\nbased approach described in Sec. 3.4, Fig. 3 (middle and\nright) shows the respective precision/recall curves. We there-\nby consider two different real-world situations.\nIn the ﬁrst case (Fig. 3; middle) the query tune repre-\nsents a new tune and is therefore not part of the weight\n74410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nlearning. However, the other tunes of the same class are\nalready in the database and therefore used for learning.\nBut of course, the system does not know during ranking\nwhich ones these are. In the other case (Fig. 3; right) all\ntunes from the query tune’s class were not used for learn-\ning, simulating an entirely new class that shall be added\nto the database. Thus, no information on this class was\navailable for learning. This is of course an even harder\ncase. For computation of the precision and recall values,\nall tunes were ranked according to the query tune, includ-\ning the songs of the unknown class. For both scenarios, we\nused weights with different speciﬁcity in the two steps of\nthe case-based approach (cf. Sec. 3.4).\nAs expected, the comparison of both diagrams shows\nthat the performance of the learned measures is lower for\nthe harder case of a new class than in the case of a new\ntune from a known class. The rawEd measure can thereby\nbe used as a point of comparison, because it has the same\ncurve in both cases. It showed that the more speciﬁc weight-\nings, most notably wt◦wt, are much better in the middle\ngraph than in the right one. This is because members of the\nsame tune family can be detected as a case, if they are al-\nready in the database. However, speciﬁc weightings from\nother tune families are less ﬁtting. Thus, the approach fails\nfor a new tune family, where no good speciﬁc measures\nare in the database yet. In the middle graph, wais best\nused to establish a case and wtof that case to ﬁnally rank:\nwt◦wa. This approach is also quite good in the right\ngraph, although using only wais slightly better. rawEd\nis better at the end of the ranking, while it is worse at the\nbeginning. This is also reﬂected in the evaluation of class-\nranking-lists (Tab. 1). Here, rawEd performs worst. With\nrespect to automatic classiﬁcation, waperforms best with\nthe fewest errors in the ﬁrst rank. A comparison of wt◦wa\nwithwcl(t)◦wawould be interesting, but the experimental\ndata for wcl(t)◦wais not available, yet.\nAs a remark it shall be noted that the described method-\nology of simulating new tunes is very time-consuming be-\ncause for each considered query the learning has to be re-\ndone without the respective information. Therefore, the\nexperiments were done with only 78 query melodies, three\nmelodies from each melody norm. For the development of\nnew similarity measures, the biased evaluation without re-\nsampling as used in Section 4.2 can be used to get a rough\nidea of which measure might be more promising. How-\never, it can never replace a ﬁnal unbiased evaluation as in\nthis section. Furthermore, the choice of melodies showed a\nsigniﬁcant impact on the results. Using, e.g., only the ref-\nerence melodies from [20], the learned measures perform\nmuch better in comparison to rawEd - also in the most\nchallenging case, while other query tunes are harder to\nhandle. For our evaluations we used the reference melody\nand two randomly picked other melodies.\n5. CONCLUSION\nWe described an adaptive metric learning approach based\non constrained clustering that can be used in folk song re-\nsearch to learn a task-speciﬁc similarity measure in form ofTable 1 . Evaluation of the class-ranking-lists. Top: clas-\nsiﬁed tunes (Sec. 4.2). Middle: unclassiﬁed tunes of a\nknown class (Sec. 4.4). Bottom: unclassiﬁed tunes of an\nunknown class (Sec. 4.4).\nMeasure Rank Inverse 1st Wrong\nwq1.042 0.989 6 / 360\nwcl(q)1.083 0.985 9 / 360\nopti1 1.169 0.975 14 / 360\nwa1.172 0.974 14 / 360\nrawEd 1.233 0.967 16 / 360\nwt◦wa1.218 0.969 4 / 78\nwa1.231 0.981 2 / 78\nwt◦wt1.244 0.957 5 / 78\nwcl(t)◦wcl(t)1.346 0.976 2 / 78\nrawEd 1.410 0.946 5 / 78\nwa1.218 0.982 2 / 78\nwt◦wa1.244 0.971 3 / 78\nwt◦wt1.282 0.942 7 / 78\nwcl(t)◦wcl(t)1.359 0.970 3 / 78\na weighted linear combination of several basic similarity\nmeasures. Individual, class and overall weightings provide\ndifferent levels for speciﬁcity of the adaptation. Experi-\nments on a data set of pre-classiﬁed folk songs showed that\nthe combined similarity measures using these weightings\ncan outperform the original basic similarities for ranking\nand automatic classiﬁcation.\nFuture experimental work comprises incorporating more\nbasic similarity measures that capture different aspects of\nthe tunes to be classiﬁed. Further, the impact of the dif-\nfering value distributions (within the ﬁxed [0,1]interval)\nfor the different basic similarities needs to be studied in\nfurther experiments as it might cause a bias in the learned\nweighting schemes.\nFuture musicological work includes studying clusters of\nsimilar weightings. As different weightings represent dif-\nferent metrics, they select different features that separate\nmelody classes. Within a melody norm, several distinct\nweight clusters suggest the introduction of sub-melody-\nnorms that might be helpful for folk song research. On\nthe other hand, weight clusters shared by different melody\nnorms could be studied to improve the case-based approach.\nIf, e.g., rhythmically ragged melodies generally lead to\nhigher weighted rhythmical similarity measures, then rag-\ngedness should be used to select weights instead of rhyth-\nmical similarity. For a better support of folk song research-\ners, the algorithm should be integrated into a graphical user\ninterface. In this context, possible interaction scenarios,\ne.g., for expert-driven development of new similarity mea-\nsures, could be examined.\nAcknowledgments. This work was supported by the Nether-\nlands Organization for Scientiﬁc Research within the WITCH-\nCRAFT project NWO 640-003-501, the German Research Foun-\ndation (DFG), and the German National Merit Foundation. Spe-\ncial thanks to contributing developers and data providers.\n745Oral Session 9-A: Folk Songs\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1average precision\naverage recallwq\nwcl(q)\nwa\nopti1\nrawEd\n 0 0.2 0.4 0.6 0.8 1\n 0 0.2 0.4 0.6 0.8 1average recall 0 0.2 0.4 0.6 0.8 1\n 0 0.2 0.4 0.6 0.8 1average recallwt o wtwcl(t) o wcl(t)wa o wawt o warawEd\n 0 0.2 0.4 0.6 0.8 1\n 0 0.2 0.4 0.6 0.8 1average recallwt o wtwcl(t) o wcl(t)wa o wawt o warawEd 0 0.2 0.4 0.6 0.8 1\n 0 0.2 0.4 0.6 0.8 1average recallFigure 3 . Precison / Recall plots for tune-ranking-lists. Left: classiﬁed tunes. Middle: unclassiﬁed tunes of a known class.\nRight: unclassiﬁed tunes of an unknown class.\n6. REFERENCES\n[1] S. Baumann and J. Halloran. An ecological approach to\nmultimodal subjective music similarity perception. In\nProc. of Conf. on Interdisciplinary Musicology , 2004.\n[2] F. Vignoli and S. Pauws. A music retrieval system\nbased on user driven similarity and its evaluation. In\nISMIR , 2005.\n[3] J. J. Rocchio. Relevance feedback in information re-\ntrieval. In The SMART Retrieval System - Experiments\nin Automatic Document Processing , 1971.\n[4] T. Mandl and C. Womser-Hacker. Learning to cope\nwith diversity in music retrieval. In ISMIR , 2002.\n[5] D. N. Sotiropoulos, A. S. Lampropoulos, and G. A.\nTsihrintzis. MUSIPER: a system for modeling music\nsimilarity perception based on objective feature subset\nselection. User Modeling and User-Adapted Interac-\ntion, 18(4):315–348, 2008.\n[6] M. I. Mandel, G. E. Poliner, and D. P. W. Ellis. Sup-\nport vector machine active learning for music retrieval.\nMultimedia Systems , 12(1):3–13, 2006.\n[7] D. Little, D. Raffensperger, and B. Pardo. A query by\nhumming system that learns from experience. In IS-\nMIR, 2007.\n[8] K. Wolter, C. Bastuck, and D. G ¨artner. Adaptive user\nmodeling for content-based music retrieval. In Proc.\nof the 6thInt. Workshop on Adaptive Multimedia Re-\ntrieval , 2008.\n[9] K. Hoashi, K. Matsumoto, and N. Inoue. Personaliza-\ntion of user proﬁles for content-based music retrieval\nbased on relevance feedback. In MULTIMEDIA’03:\nProc. of the 11th ACM Int. Conf. on Multimedia , 2003.\n[10] M. Grimaldi and P. Cunningham. Experimenting with\nmusic taste prediction by user proﬁling. In MIR ’04:Proc. of the 6th ACM SIGMM Int. Workshop on Multi-\nmedia Information Retrieval , 2004.\n[11] S. Pauws and B. Eggen. PATS: Realization and user\nevaluation of an automatic playlist generator. In ISMIR ,\n2002.\n[12] E. Pampalk, T. Pohle, and G. Widmer. Dynamic\nplaylist generation based on skipping behavior. In IS-\nMIR, 2005.\n[13] S. Stober and A. N ¨urnberger. Towards user-adaptive\nstructuring and organization of music collections. In\nProc. of 6thInt. Workshop on Adapt. Multimedia Retr. ,\n2008.\n[14] D. M ¨ullensiefen and K. Frieler. The SIMILE algo-\nrithms documentation 0.3 , 2006.\n[15] D. M ¨ullensiefen and K. Frieler. Cognitive adequacy in\nthe measurement of melodic similarity: Algorithmic\nvs. human judgments. Computing in Musicology , 13,\n2004.\n[16] A. Pinto, R. H. van Leuken, M. F. Demirci, F. Wier-\ning, and R. C. Veltkamp. Indexing music collections\nthrough graph spectra. In ISMIR , 2007.\n[17] K. Bade and A. N ¨urnberger. Personalized hierarchical\nclustering. In Proc. of the IEEE / WIC / ACM Int. Conf.\non Web Intelligence , 2006.\n[18] K. Bade and A. N ¨urnberger. Creating a cluster hierar-\nchy under constraints of a partially known hierarchy. In\nProc. of the SIAM Int. Conf. on Data Mining , 2008.\n[19] A. Aamodt and E. Plaza. Case-based reasoning: foun-\ndational issues, methodological variations, and system\napproaches. AI Communications , 7(1):39–59, 1994.\n[20] A. V olk, P. Van Kranenburg, J. Garbers, F. Wiering,\nR. C. Veltkamp, and L. P. Grijp. A manual annotation\nmethod for melodic similarity and the study of melody\nfeature sets. In ISMIR , 2008.\n746"
    },
    {
        "title": "Smarter than Genius? Human Evaluation of Music Recommender Systems.",
        "author": [
            "Luke Barrington",
            "Reid Oda",
            "Gert R. G. Lanckriet"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417803",
        "url": "https://doi.org/10.5281/zenodo.1417803",
        "ee": "https://zenodo.org/records/1417803/files/BarringtonOL09.pdf",
        "abstract": "Genius is a popular commercial music recommender system that is based on collaborative filtering of huge amounts of user data. To understand the aspects of music similarity that collaborative filtering can capture, we compare Genius to two canonical music recommender systems: one based purely on artist similarity, the other purely on similarity of acoustic content. We evaluate this comparison with a user study of 185 subjects. Overall, Genius produces the best recommendations. We demonstrate that collaborative filtering can actually capture similarities between the acoustic content of songs. However, when evaluators can see the names of the recommended songs and artists, we find that artist similarity can account for the performance of Genius. A system that combines these musical cues could generate music recommendations that are as good as Genius, even when collaborative filtering data is unavailable.",
        "zenodo_id": 1417803,
        "dblp_key": "conf/ismir/BarringtonOL09",
        "keywords": [
            "collaborative filtering",
            "music recommender system",
            "user data",
            "music similarity",
            "acoustic content",
            "user study",
            "recommendations",
            "system evaluation",
            "system performance",
            "system improvement"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSMARTER THAN GENIUS?\nHUMAN EV ALUATION OF MUSIC RECOMMENDER SYSTEMS.\nLuke Barrington Reid Oda∗Gert Lanckriet\nElectrical & Computer Engineering,∗Cognitive Science\nUniversity of California, San Diego\nlukeinusa@gmail.com roda@ucsd.edu gert@ece.ucsd.edu\nABSTRACT\nGenius is a popular commercial music recommender sys-\ntem that is based on collaborative ﬁltering of huge amounts\nof user data. To understand the aspects of music similarity\nthat collaborative ﬁltering can capture, we compare Genius\nto two canonical music recommender systems: one based\npurely on artist similarity, the other purely on similarity of\nacoustic content. We evaluate this comparison with a user\nstudy of 185 subjects. Overall, Genius produces the best\nrecommendations. We demonstrate that collaborative ﬁlter-\ning can actually capture similarities between the acoustic\ncontent of songs. However, when evaluators can see the\nnames of the recommended songs and artists, we ﬁnd that\nartist similarity can account for the performance of Genius.\nA system that combines these musical cues could generate\nmusic recommendations that are as good as Genius, even\nwhen collaborative ﬁltering data is unavailable.\n1. INTRODUCTION\nThe popularity of the online radio station Pandora.com (20\nmillion users) and Apple iTunes’ “Genius” feature (released\nin September 2008 and available to over 10 million regis-\ntered iTunes users) has brought the perennial MIR research\ntopic of music similarity and recommender systems into\nthe public spotlight. Apple, the largest music retailer in\nthe world, collects massive amounts of data about music\npurchase and listening habits of its users. Our experiments\ndemonstrate that collaborative ﬁltering of this data allows\nGenius to produce better music recommendations than sys-\ntems based on simple metadata- or content-based analysis.\nHowever, Genius fails on music for which collaborative\nﬁltering data is unavailable, such as the huge volume of\nundiscovered content in the “long tail” of the music market.\nIn this paper, we seek to understand the musical cues that\nGenius’ collaborative ﬁltering identiﬁes to capture music\nsimilarity. We can then develop MIR recommender sys-\ntems that use the same cues, without the need for massive\namounts of user data. Since we do not have access to the\ncollaborative ﬁltering input to the Genius algorithm, we\ncompare its output to two canonical recommender systems\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.where we have complete knowledge of their available musi-\ncal information. We discover that, despite not basing its rec-\nommendations directly on the audio content, collaborative\nﬁltering can capture information about acoustic similarity,\nas well as metadata similarity, for playlist generation . Us-\ning a blind user study, we determine the inﬂuence of certain\nmetadata (e.g., familiarity, afﬁnity, visibility) and musical\nfactors (e.g., styles, sounds, artists) on playlist evaluation .\n2. THE BLACK ART OF PLAYLIST GENERATION\nA playlist is a collection of songs grouped together under a\nparticular principle. The principle could be general, such\nas “rock songs from the 70’s” or personal like “songs that\nremind me of Melanie”. Cunningham et al.[ 1] make the\ndistinction between playlists and “mixes”. While a mix can\nhave abstract themes and the sequence of songs is important,\na playlist simply embodies a mood or desired emotional\nstate or acts as a background to an activity (work, romance,\nsports, etc.). The order of songs in a playlist is not impor-\ntant and it is often played on shufﬂe. Cunningham et al.’s\nuser study reports that 50 percent of requests for help in\ncreating a playlist included a song as an example. Our work\nfocuses on this “query by example” paradigm where the\nuser provides a song as a query or “seed” and the recom-\nmender system’s task is to generate a playlist of more music\nthat somehow “ﬁts well” with the seed song. The meaning\nof “ﬁts well” may depend on a variety of the factors below.\n2.1 Factors that impact playlist generation\nPlaylists may be generated (either automatically or by hand)\nto reﬂect a mood , accompany an activity or explore novel\nsongs for music discovery. Recommendations can be based\non similarity to one or more seed examples or songs may\nbe grouped based on semantic descriptions. The top orga-\nnization schemes for playlists in [ 1] were similar artists ,\ngenres andstyles so we focus on the impact of these factors\nfor automating playlist generation.\n2.2 Factors that inﬂuence playlist evaluation\nIt is rare that a playlist is rated explicitly by the conditions\nused to generate it. The playlist’s purpose plays a large role\nin evaluating it. Since music is often experienced within\na social context[ 2], factors such as song popularity ,famil-\niarity and the perception of the recommender system as an\nexpert can play a large role in the perceived quality of the\nplaylist. Even systems that generate novel or serendipitous\nplaylists for song discovery must include some familiar and\n357Oral Session 4: Music Recommendation and Playlist Generation\nrelevant items to inspire users to trust the recommender\nsystem[ 3]. This may be achieved by offering some trans-\nparency of the recommendations, e.g., by showing matching\nartists or using descriptive tags.\n3. MUSIC RECOMMENDER SYSTEMS\nA variety of approaches to music recommendation and\nplaylist generation have been proposed by the MIR commu-\nnity. Aucouturier and Pachet [ 4] used acoustic similarity\nto group songs together. Flexer et al. [ 5] propose using\nKL divergence between acoustic song models to make a\nplaylist that transitions coherently from a start to an end\nsong. Xiao et al.[6] describe songs’ acoustic content using\nautomatically generated tags drawn from a variety of se-\nmantic categories. They derive a music similarity metric\nby learning the optimum weighting of these categories and\nﬁnd genre similarity to be the most important predictor of\nsubjective evaluations.\nFields et al.[ 7] extract social-network ﬂow between\nartists on MySpace and use the resulting artist association\nmetadata to build playlists. Vignoli and Pauws [ 8] designed\na recommender system that allows users to control how\nacoustic timbre information is combined with genre, mood,\nyear and tempo metadata. The resulting playlists rated\nhigher than less transparent controls in a user evaluation.\n3.1 Two Types of Recommender System\nSection 2 details a variety of inﬂuences that may be used\nby music recommender systems but they can be broadly\ncategorized into two different approaches:\nContent-based systems “listen” to the audio content of the\nmusic and build playlists by ﬁnding songs that sound similar\n(e.g., [ 4,5]) or that have similar semantic descriptions (e.g.,\n[9,6]). For example, the popular online radio station Pan-\ndora.com1employs professional musicologists to listen to\neach of the 1 million songs in its “music genome” database\nand objectively characterize their acoustic content using\n400 semantic descriptors (e.g., major or minor tonality, the\namount of syncopation, the gender of the vocalist, etc.).\nMetadata-based systems use information associated with\nthe music that is not directly related to the acoustic content\nsuch as artist names (e.g., [ 7]), genre or other tag infor-\nmation, purchase data, popularity, etc. For example, the\nGenius playlist algorithm uses collaborative ﬁltering based\non the purchase history of millions of iTunes users (i.e.,\nlisteners who bought thissong also bought thatsong).\nFor this paper, we evaluate the Genius recommender\nsystem against one content-based and one metadata-based\napproach to generating playlists, as well a system that gen-\nerates playlists randomly. All systems take a seed song\nand return a playlist of ﬁve recommended songs. Each\nalgorithm that we consider is described in detail below.\n3.2 Genius\nThe iTunes Genius recommender system2uses the Gra-\ncenote MusicID service[ 10] to ﬁngerprint songs in a user’s\n1www.Pandora.com\n2Our experiments use Genius incorporated in iTunes version 8.0.music library and identify the name of the song, artist, al-\nbum, etc. This metadata is then used to identify the songs\nin Genius’ database. Although the exact details of the algo-\nrithm are a trade-secret of Apple Inc., Genius appears to use\ncollaborative ﬁltering to compare the seed song’s metadata\nto iTunes’ massive database of music sales (over 50 million\ncustomers who have purchased over 5 billion songs), as well\nas play history and song rating data collected from iTunes\nusers3. When it is ﬁrst initialized, Genius analyzes a user’s\nmusic library and compiles all of the collaborative ﬁltering\ndata necessary to build playlists from the library, based on\nany given seed song. While this ﬁngerprinting and database\ncommunication takes some time ( ∼1hour for our 12,000-\nsong library), the only acoustic analysis involved seems to\nbe ﬁngerprinting for the purpose of metadata information\nand not content-based recommendation.\nInformal experiments with Genius give some clues into\nits operation and verify that it does not use content anal-\nysis directly. For example, if we delete the ID3 metadata\ninformation associated with a given MP3 ﬁle, or add a song\nto the library which is unknown to Gracenote (e.g., a new\nrecording by an obscure band), Genius fails to recommend\nany music. Furthermore, if we choose a seed song that is\nvery atypical of the style of the artist or album that features\nthe song, Genius recommends music that represents the\nmore common aspects of the artist. For example, using\nthe seed song “Beautiful World”, a country-folk ballad that\nis an outlying anomaly on the album “Renegades” by the\nmetal band “Rage Against the Machine”, Genius recom-\nmends a playlist of aggressive, thrash-metal songs by bands\nsuch as “Incubus” and “Nirvana”. Although these artists are\nrelated to the seed artist, the sound and style of the resulting\nplaylist is very dissimilar to that of the seed song. Based\non this analysis, we expect Genius to perform well when\nrecommending playlists based on popular seed songs but to\nsuffer when analyzing less well-known music.\n3.3 Artist Similarity\nTo provide a second, more transparent playlist algorithm\nthat, like Genius, is not based on acoustic analysis, we\nconsider building playlists based on artist similarity. The\nsocial music-streaming website last.fm offers lots of user-\ngenerated information about songs and artists4. In particu-\nlar, for any given artist, our artist similarity system retrieves\na ranked list of the 100 most similar artists to the seed song’s\nartist. We use this last.fm metadata to build a playlist by\nmoving down the ranked list and choosing a random song\nby each artist that we ﬁnd in our library. Comparisons be-\ntween these music recommendations and Genius’ playlists\nwill illuminate the degree to which collaborative ﬁltering\ncaptures artist similarity .\n3.4 Semantic Similarity of Automatic Tags\nTo examine Genius’ ability to capture acoustic similarity\nbetween songs, we compare it to a purely content-based\napproach. This recommender system is modeled on Pan-\ndora.com in that it ﬁnds similar songs by matching semantic\n3Based on http://www.apple.com/pr/ as well as a meeting\nbetween the authors and iTunes in January 2009.\n4www.last.fm/api\n35810th International Society for Music Information Retrieval Conference (ISMIR 2009)\ndescriptions of the audio content. Pandora’s semantic data\nand its music library are proprietary, so we recreate a similar\nsystem using computer audition.\nWe use an automatic tagging algorithm, described in\ndetail in [ 11], to describe any song using 149 different se-\nmantic tags. These tags include descriptors of the genre,\nemotion, instruments, vocals and usages of the song. For a\ngiven song, the output of this “auto-tagger” is a set of prob-\nabilities that indicate the relevance of each tag to the song.\nThese probabilities may be interpreted as the parameters of\na “semantic” multinomial distribution that characterizes the\nsong, just as a human listener might use words to describe\na song’s acoustic content (e.g., “very jazzy, features a lot of\nsaxophone and piano, and good to listen to on a date”).\nThe auto-tag system computes similarity between two\nsongs by comparing the Kullback-Liebler (KL) divergence\nbetween their semantic multinomial distributions. To build\na playlist, we return songs with minimum KL-divergence\nfrom the seed song. Abstracting multimedia representa-\ntions using semantics has shown improvements over di-\nrect feature-based similarity for retrieval of images[ 12],\nvideo[ ?] and sound effects[ 9] and this system was among\nthe top four performing algorithms in the 2007 MIREX\naudio similarity challenge [13].\n4. PLAYLIST EV ALUATION EXPERIMENT\nOne of the biggest challenges when designing music recom-\nmendation systems lies in evaluating any proposed method.\nThere is no standard “ground truth” data set on which to\ntest, let alone train, music similarity algorithms. Widely\navailable surrogates for similarity exist, such as deciding\nthat songs should be deemed “similar” if they come from\nsimilar genres [ 6,14], artists [ 15] or albums [ 9]. Playlists\ncan be evaluated by examining their intersection with exist-\ning, human-generated playlists [ 16,6] but this requires that\nthe same music libraries be used to generate both the new\nand the reference playlists.\nA more accurate, but less scalable or ﬂexible approach\nuses humans to evaluate music recommendations. This was\nthe approach taken in the 2007 MIREX contest[ 13] and,\nthough great effort was required to collect this information,\nthe resulting evaluation was very rich. This data has not\nbeen released to the MIR community. Human computation\ngames such as Tag-A-Tune[ 17] may provide another source\nof human-derived music similarity data.\nSince the goals of this paper are both to evaluate the\nperformance of different music recommender systems in\nvarious simulated scenarios anddetermine the factors that\ninﬂuence these evaluations, we built a new platform for\nhumans to evaluate playlists as well as collect information\nabout the strengths and weaknesses of each system.\n4.1 The Interface\nA new subject arriving at the experiment website sees brief\ninstructions explaining the task and the playlist evaluation\nprocedure. The subject then logs in, so that they can return\nto the experiment at a later date and not repeat trials.\nA single evaluation or “trial” consists of three stages: 1)\nListen to and evaluate a seed song. 2) Listen to and evaluate2 playlists. 3) Indicate factors that inﬂuenced the playlist\nevaluation. 50 seed songs were chosen in advance and, on\neach trial, one seed song is randomly assigned to a subject\n(without repetition). In the ﬁrst stage, the subject listens to\nthe seed song and rates how familiar they are with the song\nand how much they likethe song, both on a 5-point scale.\nOnce the subject rates the seed song, stage 2 displays two\nplaylists, each containing 5 songs, generated by one of the\n4 possible recommender systems (Genius, Artist Similarity,\nSimilar Tags or a random playlist). The two systems in\na given trial are randomly chosen but not the same. The\nsubject can listen to the songs from each playlist in any\norder or re-listen to the seed song by pressing corresponding\nplay buttons. Beside each song is a button to indicate any\nbad song that “doesn’t ﬁt” in the playlist. After listening to\nthe playlists, the subject evaluates which playlist is better,\non a 5-point scale: “Playlist 1 much better”, “Playlist 1\nsomewhat better”, “Equal”, “Playlist 2 somewhat better”,\n“Playlist 2 much better”.\nAfter choosing the winning playlist, stage 3 asks the sub-\nject to indicate factors that inﬂuenced their evaluation. Six\nfactors are presented that may have affected the subject’s\nevaluation of why either playlist was a good match with\nthe seed song: similar sounds, genres, artists, energy, in-\nstrumentation andlyrics . These factors only examine the\nrelationship between the seed song and the playlists. Other\nfactors (e.g., usage, time) are assumed to be implicit in the\nchoice of the seed song and are not tested in this work. The\nsubject can select as many factors as they deem relevant or\nindicate that the factor was not relevant (this choice was not\npre-selected) before continuing on to the next trial. Subjects\ncan quit at any stage and their progress is saved.\n4.2 The Music\nThe playlists are built from the authors’ personal music\nlibrary of over 12,000 relatively popular songs that span the\nmost common genres of Western popular music, with very\nlittle music outside these genres. The genres include rock,\nalternative, punk, soft rock, classic rock, folk, pop, elec-\ntronica, experimental, blues, jazz, soul and hip-hop. The 50\nseed songs were chosen to represent these genres in propor-\ntions roughly equal to those observed in the library5. For\neach seed song, we pre-calculate a ﬁve-song playlist using\neach of the recommender systems described in Section 3.\nWe used 30-second song clips, beginning 30 seconds\nfrom the start of the song. 30 seconds is generally enough\nto give a good impression of a song (e.g., it is a standard\nlength for previewing songs in online music stores) while\nbeing sufﬁciently short to make each trial manageable, since\nsubjects are required to listen to 11 clips.\nIn half the trials, song and artist names for both the seed\nsong and the playlist songs are hidden from subjects. This\nallows us to investigate the inﬂuence of the song and, in\nparticular, artist names on subjects’ evaluation of playlists.\nIn certain music recommendation scenarios, listeners may\nread the names of the songs they hear (e.g., album track-\nlists, record stores, music players such as iTunes, WinAmp,\n5The list of seed songs and music library can be found at\nhttp://cosmal.ucsd.edu/cal/projects/playlist/\n359Oral Session 4: Music Recommendation and Playlist Generation\n(484) (494) (503) (897) (313)  \ngenius artistauto−tags equal random0102030405060 much better, names shown\nsomewhat better, names shown\nsomewhat better, names hidden\nmuch better, names hiddenFigure 1 . Percent wins for each music recommender sys-\ntem, divided over trials where the song names were hidden\nor shown. X-axis displays the system (and number of tri-\nals where this system was presented). Y-axis displays the\npercentage of trials where the system was the winner.\nand last.fm) while in other cases, they only hear - and do\nnot see - the playlist (e.g., most stereos, iPods on shufﬂe\nmode, on the radio, at parties or clubs).\n5. RESULTS\nExperimental subjects were recruited from psychology and\nengineering classes at UCSD, via email to friends, col-\nleagues and the Music-IR mailing list and from online blogs\nand social networks. During the three week experiment,\n185 subjects completed 894 trials with, on average, 4.8 tri-\nals per subject, including a maximum of 44 trials by one\nuser. Seed songs were chosen randomly (without repeating\na seed for any subject) and each of the 50 seed songs was\npresented an average of 18 times with a minimum of 11 and\na maximum of 30 trials. Each of the three playlist genera-\ntion methods was presented in at least 638 trials and each\nof the 150 playlists (a {seed song, playlist method }pair)\nwas presented in, on average, 11.8 trials.\nFigure 1 displays how often each recommender system\nwon, as a proportion of all the trials in which it appeared.\nFigure 2 indicates how each system fared against the others,\nin head-to-head comparisons. The fading between colors\nin Figures 2-4 indicates the variance over 50 random sub-\nsamplings of 75% of the data for each condition. It is clear\nthat Genius outperforms both the Artist Similarity and Sim-\nilar Tags methods in most cases although a more detailed\nexamination is given below.\n5.1 Trial Lengths\nTable 1 demonstrates that subjects spent, on average, 226\nseconds on each trial, indicating that they listened to almost\nall of each 30-second song clip (11 songs x 30 seconds =\n330 seconds). This time was signiﬁcantly less for trials\nwhere the song and artist names were visible (196 seconds)\nand signiﬁcantly longer when the names were hidden (258\nseconds), indicating that subjects were often able to evaluate\ngenius\n42.2%equal\n26.6%artist\n31.2%(a)\ngenius\n53.1%equal\n13.8%auto−tags\n33.2%(b)\nartist\n42.3%equal\n14.4%auto−tags\n43.3%(c)Figure 2 . Head-to-head playlist comparisons over all condi-\ntions. Ignoring the equal votes, all systems are signiﬁcantly\nbetter than random and Genius is signiﬁcantly better than\nthe content-based system using similar tags (Chi-square\ntest for ﬁt to a uniform distribution, α= 0.05). All other\ndifferences are not signiﬁcant.\nplaylists (or, at least, some of the songs in a playlist) simply\nby looking at the names of the song and artist. Thus, we\nexpect trials where the names were hidden to estimate better\nthe impact of the “sounds” of the songs while those with\nnames shown will demonstrate the impact of artist similarity.\nTrial Length (sec) Mean Median\nAll Trials 226 150\nNames Shown 258 165\nNames Hidden 196 139\nTable 1 . Average seconds spent per trial as well as for trials\nwhere the song and artist names were shown or hidden.\n5.2 Knowing the Names\nVisibility of song and artist names had a large inﬂuence on\nhow subjects evaluated each playlist. Showing the names\nbeneﬁted the metadata-based systems where, as evidenced\nby the shorter time spent on these trials, subjects made\nuse of this metadata information to make their evaluations.\nComparisons between each pair of algorithms are summa-\nrized in Figure 3. Of particular note is the comparison\nbetween the two metadata-based systems. When the names\nare shown, we see in Figure 3(a) that subjects actually rate\nthe Artist Similarity playlists slightly better than the Genius\nplaylists. This may indicate that social or visual cues are,\nat times, more salient than acoustic similarity or that, given\nsome “explanation” of how a playlist is built, listeners are\nmore forgiving of acoustic mismatches [ 3]. However, when\nthe names are hidden, and subjects must base their judge-\nments on the acoustics alone, Genius is overwhelmingly\nsuperior (Figure 3(b)).\n5.3 Familiar and Liked Songs\nThe effects of subjects’ familiarity with the seed song is\nshown in Figure 5. The effect of afﬁnity for the seed song\nwas qualitatively almost identical and is not shown. In both\ncases, Genius beneﬁts from decreased familiarity or afﬁnity\nwhile the Artist Similarity method suffers. In other words,\nwhen subjects did not know (or like) a song, and presumably\ncould make less use of artist associations, they preferred\nGenius’ recommendations. This is a strong indication that\nGenius does not just average over artists but determines\nsong-speciﬁc similarities. The only statistically-signiﬁcant\n36010th International Society for Music Information Retrieval Conference (ISMIR 2009)\ngenius\n33.7%equal\n28.3%artist\n38.0%(a)\ngenius\n51.9%equal\n24.7%artist\n23.5%(b)\ngenius\n54.9%equal\n14.7%auto−tags\n30.4%(c)\ngenius\n51.1%equal\n12.8%auto−tags\n36.2%(d)\nartist\n46.9%equal\n13.3%auto−tags\n39.8%(e)\nartist\n37.3%equal\n15.7%auto−tags\n47.1%(f)\n0 50 100Figure 3 . Head-to-head playlist comparisons over the con-\ndition where song and artist names are shown (a),(c)&(e)\nor hidden (b),(d)&(f). When names are shown (a), Artist\nSimilarity outperforms Genius, but suffers signiﬁcantly\nwhen names are hidden (b) (Chi-square test for indepen-\ndence,α= 0.05). The content-based system always ben-\neﬁts when names are hidden (d),(f), forcing subjects to\nconsider acoustics.\nchange between these conditions (familiar / unfamiliar or\nliked / not liked) is the reversal in ratings for the Artist\nand Tag Similarity methods. When familiar with the seed\nsong, subjects were able to appreciate similar artists in the\nArtist Similarity playlists but, in the absence of this prior\nknowledge, acoustic similarity prevailed.\n5.4 Content Similarity from Collaborative Filtering\nWe have seen that collaborative ﬁltering ﬁnds similarities\nbetween songs, not just artists. Can collaborative ﬁltering\nbased on usage and purchase metadata actually capture\nsimilarity in acoustic content? To answer this question,\nwe consider trials where subjects were unfamiliar with the\nseed song and where the names of the songs and artists were\nhidden . This removes the inﬂuence of song familiarity and\nartist associations so that subjects’ evaluations are based\nonly on acoustic similarity. We also required that subjects\nlikethe seed song so that they had sufﬁcient motivation\nand experience with the genre to make relevant evaluations\n(many subjects reported that they felt unwilling or unable\nto evaluate songs they disliked). The outcome is shown in\nFigure 4 where it can be seen that Genius now performs\nat the same level as the system based solely on acoustic\ncontent. This agrees with the ﬁndings of Baccigalupo et\nal.[16] who provide evidence that information about song\nassociations discovered from social playlists can be used to\nderive genre afﬁnities i.e., collaborative ﬁltering data can\nbe used to derive aspects of acoustic similarity.\ngenius\n42.1%equal\n15.8%auto−tags\n42.1%\n0 50 100\nFigure 4 . Genius captures content. When subjects were\nunfamiliar with a seed song that they liked and had no\ninformation about song and artist names (26 trials), Genius\nmatches the performance of the content-based system.\ngenius\n37.9%equal\n27.3%artist\n34.8%(a)\ngenius\n44.9%equal\n26.2%artist\n29.0%(b)\ngenius\n45.1%equal\n14.1%auto−tags\n40.8%(c)\ngenius\n57.6%equal\n13.6%auto−tags\n28.8%(d)\nartist\n46.9%equal\n19.4%auto−tags\n33.7%(e)\nartist\n38.5%equal\n10.3%auto−tags\n51.3%(f)\n0 50 100Figure 5 . Head-to-head playlist comparisons over the con-\ndition where subjects are familiar (a),(c),&(e) or unfamil-\niar(b),(d)&(f) with the seed song . There is a signiﬁcant\ndifference between (e) and (f) where the content-based Tag\nSimilarity system is more effective than Artist Similiarity\nwhen the seed song is not familiar. (Chi-square test for\nindependence, α= 0.05).\n5.5 Bad Songs\nTable 2 examines the “bad songs” in each playlist that sub-\njects felt did not ﬁt well with the seed song. Overall, the\nplaylist with fewer bad songs won in 81% of trials.\nGenius 1.30\nSimilar Artist 1.18\nSimilar Tags 1.33\nRandom 2.56Trial Winner 1.20\nTrial Loser 1.80\nTable 2 . Average “bad songs” in playlists from each system\nas well as the average for the winner and loser of each trial.\n6. SMARTER THAN GENIUS\nWhile Genius performs as well or better than the metadata-\nand content-based systems on our test collection of popular\nmusic, it is unable to make recommendations from the large\n“long tail” of new, undiscovered music. We now consider\nhow a music recommender system could take advantage of\nboth content-based information and metadata, when avail-\nable, to perform as well or better than Genius, without the\nneed for massive amounts of user data.\n6.1 Balancing Content and Metadata\nTable 3 quantiﬁes the competing inﬂuences of artist and\nacoustic similarity. We show the average artist similarity\nand auto-tag KL divergence between the seed songs and all\nthe songs from playlists generated by each recommender\nsystem. These measures are also shown for all the bad\nsongs. By design, the content-based system has minimum\nKL and, although it can only access artist information in-\ndirectly through acoustics, it captures artist similarity at\na better-than-random level. Though they produce good\nrecommendations, both Genius and the Artist Similarity\nsystems have signiﬁcantly higher KL. This indicates that\nsimply minimizing divergence between semantic descrip-\ntions will not produce perfect recommendations. Likewise,\nrecommending similar artists is not sufﬁcient as many bad\nsongs had high artist similarity. Table 2 indicates that a rec-\nommender should avoid bad songs with very large semantic\n361Oral Session 4: Music Recommendation and Playlist Generation\nArtist Similarity Tag KL Divergence\nGenius 19.8 0.81\nSimilar Artists 44.5 0.89\nSimilar Tags 5.0 0.14\nRandom 1.1 1.15\nBad Songs 16.9 1.20\nTable 3 . Average artist similarity (between 0 and 100) and\nauto-tag KL divergence (larger means less similar) between\na seed song and playlist songs recommended by each system\nas well as for bad songs produced by all systems.\ndifferences (high KL divergence) while also making sure\nto include some clearly similar artists. For example, in 14\nof the 50 playlists tested, Genius recommended a song by\nthesame artist as the seed song, a simple way to enhance\nperception of the relevance of the recommendations.\n6.2 Musical factors inﬂuencing playlist evaluations\nStage 3 of our experiment asked subjects to indicate how\nwell the playlist songs matched the seed song on six differ-\nent musical cues: similar style (genre), sound, artist, energy,\ninstruments and lyrics. Subjects could indicate that a par-\nticular factor was most relevant to either playlist, even the\none they had deemed inferior in stage 2. Figure 6 displays\nthe percentage of trials where each system best manifested\nthese factors. Genius playlists often match the styles (47%)\nand sounds (53%) of the seed song while, predictably, the\ncontent-based Similar Tags system rarely returns similar\nartists (26%). The percentages below the x-axis in Figure\n6 indicate how often each factor was cited as a favorable\ninﬂuence (subjects were not required to mark these inﬂu-\nences). Similarity between the sound of the seed and the\nplaylist was the most frequently cited factor (82%) while\nsimilar lyrics rarely inﬂuenced playlist evaluation (36%).\n7. CONCLUSIONS\nWe ﬁnd that Genius’ collaborative ﬁltering approach, which\nessentially captures the wisdom of the crowds, performs\nwell on a test collection of popular music. By removing\nevaluator bias resulting from artist names and song familiar-\nity, we show that Genius captures song-speciﬁc aspects of\nacoustic similarity, as can be derived from a purely content-\nbased system. Thus, for exploring the long tail, a content-\nbased recommender can be expected to perform similarly\nto Genius, ifcollaborative ﬁltering data were available.\nWe discover that seeing song and artist names has a\nsigniﬁcant effect on how a playlist is evaluated, indicating\nthat recommender systems must be designed with appli-\ncations in mind. We highlight the most inﬂuential factors\non similarity evaluation and suggest that balancing content\nanalysis to avoid bad songs with metadata similarity to pro-\nvide transparent recommendations can help build smarter\nmusic recommender systems.\n8. ACKNOWLEDGEMENTS\nWe thank Sarah Creel and Brian McFee for their advice.\nGL is supported by NSF grant DMS-MSPA 062540922.\nstylesound artistenergyinstruments lyrics10 0102030405060\nwin\nlose\n73% 82% 66% 71% 66% 36%  genius\nsimilar artist\nsimilar tags\nrandomFigure 6 . Musical factors inﬂuence song similarity. Y-axis\nshows how often each recommender system best matched\nmusical factors of the seed song, averaged over trials eval-\nuating that system (win or lose). Below the x-axis is the\npercentage of total trials where each factor was an inﬂuence.\n9. REFERENCES\n[1]S. Cunningham, D. Bainbridge, and A. Falconer. More of an\nart than a science: Supporting the creation of playlists and\nmixes. In ISMIR , 2006.\n[2]M. Salganik, P. Dodds, and D. Watts. Experimental study of\ninequality and unpredictability in an artiﬁcial cultrural market.\nScience , 311(5762):854–856, 2006.\n[3]O. Celma and P. Herrera. A new approach to evaluating novel\nrecommendations. In RecSys , 2008.\n[4]J.-J. Aucouturier and F. Pachet. Music similarity measures:\nWhat’s the use? In ISMIR , 2002.\n[5]A. Flexer, D. Schnitzer, M. Gasser, and G. Widmer. Playlist\ngeneration using start and end songs. In ISMIR , 2008.\n[6]L. Xiao, L. Liu, F. Seide, and J. Zhou. Learning a music\nsimilarity measure on automatic annotations with application\nto playlist generation. In ICASSP , 2009.\n[7]B. Fields, C. Rhodes, and M. Casey. Social playlists and bottle-\nneck measurements: Exploiting musician social graphs using\ncontent-based dissimilarity and pairwise maximum ﬂow val-\nues. In ISMIR , 2008.\n[8]F. Vignoli and S. Pauws. A music retrieval system based on\nuser-driven similarity and its evaluation. In ISMIR , 2005.\n[9]L. Barrington, A.B. Chan, D. Turnbull, and G. Lanckriet. Au-\ndio information retrieval using semantic similarity. In ICASSP ,\n2007.\n[10] Gracenote Inc. Automatic identiﬁcation of sound recordings.\nUS Patent Number 7,328,153, 2008.\n[11] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Se-\nmantic annotation and retrieval of music and sound effects.\nIEEE TASLP , 16(2):467–476, February 2008.\n[12] N. Rasiwasia, N. Vasconcelos, and P. Moreno. Bridging the\ngap: Query by semantic example. IEEE Trans. on Multimedia ,\n2007.\n[13] J.S.Downie. Music Information Retrieval eXchange. ISMIR ,\n2007.\n[14] E. Pampalk, A. Flexer, and G. Widmer. Improvements of\naudio-based music similarity and genre classiﬁcation. In IS-\nMIR, 2005.\n[15] A. Berenzweig, B. Logan, D. Ellis, and B. Whitman. A large-\nscale evalutation of acoustic and subjective music-similarity\nmeasures. Computer Music Journal , pages 63–76, 2004.\n[16] C. Baccigalupo, E. Plaza, and J. Donaldson. Uncovering afﬁn-\nity of artists to multiple genres from social behavior data. In\nISMIR , 2008.\n[17] E. Law and L vonAhn. Input-agreement: A new mechanism\nfor collecting data using human computation games. In ACM\nCHI, 2009.\n362"
    },
    {
        "title": "Accelerating Non-Negative Matrix Factorization for Audio Source Separation on Multi-Core and Many-Core Architectures.",
        "author": [
            "Eric Battenberg",
            "David Wessel"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417020",
        "url": "https://doi.org/10.5281/zenodo.1417020",
        "ee": "https://zenodo.org/records/1417020/files/BattenbergW09.pdf",
        "abstract": "Non-negative matrix factorization (NMF) has been successfully used in audio source separation and parts-based analysis; however, iterative NMF algorithms are computationally intensive, and therefore, time to convergence is very slow on typical personal computers. In this paper, we describe high performance parallel implementations of NMF developed using OpenMP for shared-memory multicore systems and CUDA for many-core graphics processors. For 20 seconds of audio, we decrease running time from 18.5 seconds to 2.6 seconds using OpenMP and 0.6 seconds using CUDA. These performance increases allow source separation to be carried out on entire songs in a number of seconds, a process which was previously impractical with respect to time. We give insight into how such significant speed gains were made and encourage the development and use of parallel music information retrieval software.",
        "zenodo_id": 1417020,
        "dblp_key": "conf/ismir/BattenbergW09",
        "keywords": [
            "OpenMP",
            "CUDA",
            "NMF",
            "audio source separation",
            "parts-based analysis",
            "iterative NMF algorithms",
            "computational intensive",
            "time to convergence",
            "personal computers",
            "high performance parallel implementations"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nACCELERATING NON-NEGATIVE MATRIX FACTORIZATION FOR\nAUDIO SOURCE SEPARATION ON MULTI-CORE AND MANY-CORE\nARCHITECTURES\nEric Battenberg\nParallel Computing Laboratory\nUniversity of California, Berkeley\nericb@eecs.berkeley.eduDavid Wessel\nCenter for New Music and Audio Technologies\nUniversity of California, Berkeley\nwessel@cnmat.berkeley.edu\nABSTRACT\nNon-negative matrix factorization (NMF) has been suc-\ncessfully used in audio source separation and parts-based\nanalysis; however, iterative NMF algorithms are compu-\ntationally intensive, and therefore, time to convergence is\nvery slow on typical personal computers. In this paper,\nwe describe high performance parallel implementations of\nNMF developed using OpenMP for shared-memory multi-\ncore systems and CUDA for many-core graphics proces-\nsors. For 20 seconds of audio, we decrease running time\nfrom 18.5 seconds to 2.6 seconds using OpenMP and 0.6\nseconds using CUDA. These performance increases allow\nsource separation to be carried out on entire songs in a\nnumber of seconds, a process which was previously im-\npractical with respect to time. We give insight into how\nsuch signiﬁcant speed gains were made and encourage the\ndevelopment and use of parallel music information retrieval\nsoftware.\n1. INTRODUCTION\nEven though music information retrieval (MIR) research\nis growing in importance and popularity, we have yet to\nsee widespread adoption of MIR techniques in end-user\napplications. Part of this may be due to the ubiquity of on-\nline music recommendation services such as Pandora and\nLast.fm that use hand-labeled data and collaborative ﬁl-\ntering as a basis for their recommendations, but also, the\noverall computational complexity of many MIR techniques\nmakes their use outside of powerful compute clusters in-\nfeasible. The rate of progress of MIR research could be\ngreatly improved if the execution time of MIR techniques\nwas reduced enough to allow for quicker evaluation and\ntuning of algorithm parameters and more frequent real-\nworld usage.\nAn emphasis on creating fast implementations has seen\nsome attention, though not nearly enough. Tzanetakis pro-\nduced submissions to MIREX 2007 using the Marsyas au-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.dio processing framework that ran orders of magnitude faster\nthan the submissions of competitors while producing com-\nparable results [1]. For example, in the audio mood clas-\nsiﬁcation task, the multi-core Tzanetakis implementation\ncompleted in 2 minutes, while competing implementations\ntook between 8 minutes and 3 hours. Even for research\nimplementations, such large speed differences can signiﬁ-\ncantly impact the usability of MIR software.\nIn this paper, we describe our efforts to speed up percus-\nsive source separation based on non-negative matrix fac-\ntorization (NMF), an unsupervised learning technique that\nhas been used in audio source separation and parts-based\nanalysis [2] [3] [4] [5]. Since NMF dominates the compu-\ntation time in such a source separation task, it is an impor-\ntant computational procedure to optimize.\nThe goal of this paper is to demonstrate the dramatic\nspeedup that can be achieved by multi-core and many-core\nimplementations of multimedia applications and to encour-\nage MIR researchers to develop and reuse high performance\nparallel implementations of important MIR procedures.\nIn Section 2, we explain the importance of producing\nparallel MIR applications. Section 3 covers the practical\nconsiderations for audio source separation based on NMF.\nIn Section 4, we introduce the OpenMP and CUDA parallel\nprogramming models. Section 5 details the design of our\nparallel implementations and gives insight into techniques\nimportant to parallelizing MIR applications. Finally, Sec-\ntion 6 concludes with suggestions on how MIR can most\nbeneﬁt from parallel computing.\n2. PARALLELIZING MULTIMEDIA\nAPPLICATIONS\nPercussive source separation is a useful ﬁrst step in such\nMIR tasks as drum transcription, rhythm summarization,\nand beat tracking. By extracting an audio signal containing\nonly percussive instruments, the task of rhythmic analysis\ncan be greatly simpliﬁed. Helen and Virtanen [6] use NMF\nalong with a support vector machine (SVM) to accomplish\nthis. The drum track extractor we use as a target for perfor-\nmance optimization is similar to that presented in [6] but\nincludes additional complexity optimizations and percus-\nsive features introduced in [7].\nComputation time in this system is dominated by NMF,\nwhich makes up about 80% of the CPU time (18.5 seconds\n501Poster Session 3\nof the 23.1 seconds total) in a Matlab implementation run\non 20 seconds of audio. In order to increase throughput,\nthe NMF step must be optimized.\nBecause single-core CPU performance increases have\nbeen hindered by power concerns, limits on memory speed,\nand diminishing returns on instruction level parallelism,\nthe focus of computer science research has turned strongly\ntowards parallel architectures and programming models [8].\nApplications programmers can no longer develop a sequen-\ntial implementation of their software and hope that future\nuniproccessor speedups will provide the necessary com-\nputing power to make their application useful. Instead, the\nexponentially increasing number of processing elements,\nor cores, in current architectures must be exploited to max-\nimize performance.\nMulti-core CPU architectures are already commonplace\nin workstations, servers, and laptops, so parallelizing code\nto utilize available cores will lead to signiﬁcant perfor-\nmance increases for most users. In addition, the majority\nof personal computers today ship with many-core graph-\nics processors contained on the system’s video card. Cur-\nrent high-end graphics processors (GPUs) ship with tens of\nprocessors each capable of executing operations on large\ndata vectors. The end result is a highly data-parallel archi-\ntecture that can be used for general computation (not just\ngraphics rendering) thanks to programming frameworks like\nOpenCL [9] and Nvidia’s CUDA [10].\nCUDA has been successfully used to achieve very high\nperformance on a variety of applications that rely on signal\nprocessing and machine learning. Examples include a fast\nGPU-based support vector machine implementation that\nachieves up to 135×speedup over LIBSVM [11], a large\nvocabulary speech recognition engine with 10 ×speedup\nover sequential versions [12], and an image contour detec-\ntor that achieves 114 ×speedup [13]. To help put these\nnumbers in perspective, the 114 ×speedup represents a re-\nduction in runtime from 4 minutes to 2 seconds.\nWe aim to achieve such dramatic performance gains\nwith NMF-based source separation.\n3. NON-NEGATIVE MATRIX FACTORIZATION\nFOR AUDIO SOURCE SEPARATION\nNon-negative matrix factorization can be used for audio\nsource separation by decomposing a spectrogram matrix\ninto two matrices which contain source-wise spectral con-\ntributions and time-varying gains. NMF can be phrased as\nthe optimization problem:\nGiven an M×Nnon-negative matrix X∈RM×N\n+ ,\nﬁnd matrices W∈RM×K\n+ andH∈RK×N\n+ that mini-\nmize the cost function f(X,WH ).\n3.1 Cost Function\nRather than using the mean-squared error between Xand\nthe product WH as the cost function, we use a matrix ver-\nsion of the Kullback-Leibler divergence:D(X/bardblWH ) =/summationdisplay\nij/parenleftbigg\nXijlogXij\n(WH )ij−Xij+ (WH )ij/parenrightbigg\n(1)\nIt has been shown in [3] that this divergence cost func-\ntion achieves better audio source separation results than\nmean-squared error.\n3.2 Multiplicative Updates\nLee and Seung [14] have proposed an algorithm based on\ngradient-based multiplicative updates for minimizing the\nabove optimization problem. For the divergence cost func-\ntion, we alternate between updates on the two matrices us-\ning the following expressions\nH←H.∗WTX\nWH\nWT1,W←W.∗X\nWHHT\n1HT(2)\nWhere division is carried out element-wise, “ .∗” is element-\nwise multiplication, and 1represents an M×Nmatrix of\nones and is used to compute row and column sums.\nIt is important to note that, because the optimization\nproblem is not convex in both WandH, the above up-\ndates do not necessarily converge to a global minimum.\nTo address this problem, researchers typically use multiple\nrandom initializations and choose the best result. Adding\nextra computation time by running multiple trials cannot\nbe done without signiﬁcant justiﬁcation since time to con-\nvergence can be in the minutes when operating on just sec-\nonds of audio.\ncomponents1\n2\n3components\n1 2 3hi-hat\nsnare drum\nbass drum\nWHframesfrequency bands100 200 300 400 500 600\n100200300400500\nFigure 1 .A spectrogram matrix for a basic rock beat sur-\nrounded by its factor matrices WandHcomputed using NMF.\nThe component-wise gain matrix Hhas been aligned with the\ncorresponding drum score.\n3.3 Initialization\nOther approaches use a deterministic initialization based\non the structure or statistics of the matrix Xor derived\nfrom knowledge about the domain. We use an approach\nbased on the latter [7], which uses a subset of discrete co-\nsine transform basis functions and typical drum spectra as\n50210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nthe initial columns of W. For our purposes, the initializa-\ntion choice does not directly affect the speed with which\nthe updates in eq. (2) are executed, but it can affect the\noverall number of iterations required for convergence. To\neliminate this dependence, we will only focus on optimiz-\ning the speed of a set number of iterations rather than time\nto convergence.\n3.4 Matrix Dimensions\nAn additional consideration that must be made is the di-\nmensionality of the spectrogram matrix that is to be fac-\ntorized. To adequately represent drum sounds in both time\nand frequency, a length 4096 Hann window is used to ex-\ntract each analysis frame and a hop size of 256 is used to\nshift the window in time. For 20 seconds of audio sam-\npled at 44.1kHz, this gives us a matrix of size 2049×3445\n(number of positive frequency bins ×number of analysis\nframes). Since such high frequency resolution ( ∼10Hz)\nis not required at higher frequencies, we use a Bark-based\nperceptual dimensionality reduction [7] on the columns of\nXto arrive at a matrix of size 512×3445 . After NMF\nis carried out on this smaller matrix, we can interpolate to\nreturn to the original frequency scale if necessary. Lastly,\nwe choose an inner dimension for the factor matrices W\nandHofK= 30 . This represents the number of sources\ninvolved in the separation.\nUsing these dimensions, our implementations require\nabout 60MB of memory per minute of audio, making entire-\nsong decomposition feasible from a memory standpoint.\nNext we introduce the programming models that will be\nused to parallelize the NMF algorithm.\n4. OPENMP AND CUDA\n4.1 OpenMP\nOpenMP is a standardized API that enables parallel execu-\ntion on shared-memory multi-core machines [15]. OpenMP\nhas been implemented for C, C++, and Fortran and is sup-\nported in Visual C++ 2005, the Intel compiler, and gcc 4.2\nand above. The beauty of OpenMP lies in its ability to par-\nallelize existing sequential code by annotating it with com-\npiler directives. OpenMP automatically forks threads that\nexecute on separate processors according to the directives.\nOpenMP very conveniently parallelizes loops contain-\ning independent iterations using a single directive. The\nelement-wise array multiplication shown below can be split\namongst ntcores using a leading #pragma directive.\n#pragma omp parallel for num_threads(nt)\nfor(i=0;i<N;i++)\nc[i] = a[i] *b[i];\nA reduction, which operates on multiple pieces of data\nand returns a single result, can be carried out using a re-\nduction clause in the forpragma. In the example below,\nthe reduction operator is addition, so we are returning the\nsum of an array. The ﬁrst pragma creates a team of nt\nthreads that are each assigned a chunk of the work in the\nfor loop. After each thread completes its work, the valuescontained in each thread’s private variable sare summed\ninto a single ﬁnal variable s.\ns = 0;\n#pragma omp parallel num_threads(nt)\n#pragma omp for reduction(+:s)\nfor(i=0;i<N;i++)\ns += a[i];\n4.2 CUDA\nCUDA encompasses both the parallel device architecture\nused in newer Nvidia GPUs and the extensions to the C\nlanguage used to program the CUDA architecture for gen-\neral purpose computation. CUDA code compiled using\nNvidia’s nvcc is executed on the host, or CPU, which then\nissues instructions to the device or GPU. Host code typi-\ncally contains control ﬂow instructions and memory move-\nment operations between host memory and device mem-\nory, while device code is made up of kernels , which are\nfunctions written to execute in a Single Program, Multi-\nple Data (SPMD) fashion, i.e. each thread running on the\ndevice during kernel invocation executes the kernel code\nindependently on whatever chunk of data is assigned to the\nthread.\nTeams of threads can also share memory. As of CUDA\n2.1, threads can be grouped into thread blocks of up to size\n512. Threads within the same block are executed on the\nsame processor and can all access special on-chip shared\nmemory, which is necessary for inter-thread communica-\ntion. Because separate thread blocks cannot share data,\nthey can be executed independently on separate proces-\nsors. Therefore, a kernel that uses a large number of thread\nblocks should scale well on future GPUs with more pro-\ncessors.\nIn the box below, we see a kernel that performs element-\nwise addition. Each thread runs the vecAdd function sep-\narately and computes an array index from its thread ID,\nblock ID, and block size, and operates on the array ele-\nments located at that index. In the main function, the kernel\nis invoked with Bthread blocks each containing Nthreads,\nsoB×Nshould be equal to the size of the arrays.\n// kernel definition\n__global__ void vecAdd(float *a,\nfloat *b, float *c){\nint i = threadIdx.x+blockIdx.x *blockDim.x;\nc[i] = a[i] + b[i];\n}\nint main(){\n. . .\n// kernel invocation\nvecAdd<<<B,N>>>(a,b,c);\n}\nDevice kernels are physically executed in groups of 32\nadjacent threads called warps . Warps are most efﬁcient\nwhen the group of threads can be executed in a completely\nSIMD (Single Instruction, Multiple Data) manner, i.e. each\nthread in the warp does the exact same thing but to dif-\nferent data. Inserting control ﬂow statements into a ker-\nnel that cause threads within the same warp to execute\n503Poster Session 3\ndifferent code (this is referred to as a “divergent” warp)\nforces the affected threads to be run sequentially rather\nthan concurrently. Double-precision hardware support is\ncurrently lacking in CUDA, which is why we focus on\nsingle-precision implementations in this work.\nCUDA is designed to achieve high throughput on highly\ndata-parallel computations. Luckily, most multimedia ap-\nplications (especially music) exhibit a large amount of data\nparallelism.\n5. PARALLEL IMPLEMENTATION\n5.1 Important Kernels\nTo help organize our NMF implementation, we decompose\nthe updates in eq. (2) into the most important computa-\ntional kernels, including dense matrix multiplication, col-\numn and row sums, and element-wise vector arithmetic.\nEach of the kernels will be called sequentially, but individ-\nual kernels will be heavily parallelized and optimized.\nThe kernel that will do the most work in terms of ﬂoat-\ning point operations (ﬂops) is the Single-precision GEneral\nMatrix Multiply, or SGEMM . For the matrix dimensions\nlisted at the end of Section 3.4, the four SGEMMs in eq. (2)\nrequire about 423 Mﬂops. The element-divides require\nabout 3.6 Mﬂops, the sums about 0.1 Mﬂops, and the element-\nmultiplies about 0.1 Mﬂops. To prevent dividing by zero,\na small constant (called EPS) is added to every element in\neach divisor matrix, which produces a non-trivial amount\nof work (3.6 Mﬂops). Also, in order to check for conver-\ngence, we compute the divergence cost function (1) every\n25 iterations, which computes the sum of 1.8×106log-\nbased values.\nEven though the SGEMMs contain the vast majority of\nthe work, other operations, namely the slow ﬂoating-point\ndivides and the sums, can end up using a lot of compute\ntime. Divides are inherently slow operations and can take\ntens of clock cycles on certain architectures. While the\nsums contain relatively few total operations, a parallelized\nsum will require inter-thread communication which can be\nvery slow. Since a highly optimized SGEMM routine is\navailable in most vendor BLAS libraries, our implemen-\ntation goal was to tune the remaining kernels so that the\nSGEMMs dominate the overall computation time. Practi-\ncally speaking, signiﬁcantly outperforming our Matlab im-\nplementation (which takes 18.5 seconds to run 200 itera-\ntions on a Core 2 Duo T9300) was a more exciting goal.\n5.2 OpenMP Implementation\nAs stated before, OpenMP makes it very easy to parallelize\nexisting sequential code for a multi-core shared-memory\nmachine. Using the two types of forpragmas from Sec-\ntion 4.1 we can parallelize the sums and element-wise arith-\nmetic. Since the element divides are numerous, slow, and\ndo not require inter-thread communication, it makes sense\nto parallelize their loop. The row and column sums, how-\never, require a lot of communication for the amount of ad-\ndition work done per core (since the partial sum computedby each core must be sent to another core), so paralleliz-\ning the reduction loop actually led to a slower kernel. The\nlarger sum in the divergence cost function not only con-\ntains lots of addition but a slow log-based computation,\nso the work to communication ratio was beﬁtting parallel\nspeedup.\nFor the SGEMMs, we use Intel’s Math Kernel Library\n(MKL) ver. 10.0.1.014, which is heavily optimized to take\nadvantage of memory hierarchy and SIMD instructions.\nMKL uses OpenMP under the hood, so the number of threads\nused for the SGEMMs can be controlled in the same way\nas our parallel loops.\nPerformance results for the OpenMP implementation\nare shown in Figure 2 for a dual-socket Intel Core i7 920\nmachine which has 8 cores and 16 hardware threads. The\nbest performance is seen at 14 threads and is about 4.3 ×\nfaster than the single-threaded run. The most signiﬁcant\nspeed up is seen in the SGEMM since it has the highest\nwork to communication ratio, but other time-consuming\nkernels beneﬁt as well. Running this implementation on\nthe Core 2 Duo T9300 with 2 threads takes 8.9 seconds,\nwhich is 2×faster than our optimized Matlab implementa-\ntion using 2 threads.\n12345678910111213141516024681012\nElement Multiply\nRow/Col Sums\nAdd EPS\nDivergence\nElement Divide\nSGEMM \nThreadsTime for 200 iterations [sec]11.25\n2.60\nFigure 2 .Performance results for the OpenMP implementation\non a dual-socket Intel Core i7 920\n5.3 CUDA Implementation\nWriting a CUDA implementation takes a bit more thought.\nFirst, the matrices must be copied to GPU memory. Copies\nbetween CPU and GPU are relatively slow (ideally 3 GB/s\nover the PCI bus), and it’s best to avoid them except dur-\ning initialization or when returning results. This means\nthat in our case it’s better to perform all of the matrix com-\nputations on the GPU to avoid extra copies even if certain\noperations are better suited for the CPU.\nElement-wise arithmetic is completely data-parallel and\nis easily accomplished with code similar to that in Sec-\ntion 4.2. Other kernels, including the SGEMMs and sums,\nrequire a bit of inter-thread communication and are not so\ntrivially parallelized on CUDA.\n5.3.1 SGEMM\nLuckily, an optimized SGEMM routine is available in the\nCUBLAS 2.1 library that achieves 60% of theoretical peak\n50410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nperformance for large matrices on current GPUs [17]. For\nthe Geforce GTX 280, 60% of peak amounts to 373 Gﬂops/s.\nFor our particular matrix multiplications of dimensions [512×\n30×3445] ,[30×512×3445] , and [512×3445×30], the\nCUBLAS SGEMM achieves 117, 147, and 104 Gﬂops/s\nrespectively on this GPU. Even though these are relatively\nsmall SGEMMs, we should still be able to do better.\nUpon inspection of the paper [17] that describes the\nmethods used in the current CUBLAS SGEMM, we dis-\ncovered that threads operate on matrix sub-blocks with di-\nmensions 16 and 64. With this in mind, we tried zero\npadding our matrices to multiples of 16, 32, and 64. We\nfound that simply padding the matrices to multiples of 32\nresulted in an effective throughput (not counting operations\non zero-padded areas) of 264, 196, and 85 Gﬂops/s for\neach SGEMM size. Since the NMF algorithm uses two\nSGEMMs of the ﬁrst size, this results in an SGEMM run-\nning time reduction from 0.71 to 0.52 seconds for 200 iter-\nations.\n5.3.2 Reduction\nBecause parallel reductions, such as sums, mins, and maxes,\nare not included in standard libraries, we will have to write\nour own routines. A tutorial on optimizing reductions in\nCUDA is available in the CUDA SDK [18]. This overview\npresents optimization strategies that can be used to greatly\nimprove the speed of large power-of-2-size reductions and\nshows how a 30×speedup can be achieved for a 4.2×106\nlength sum over a naive binary tree implementation.\nA binary tree reduction can be constructed in various\nways. Using the shared memory of a thread block, we\ncan perform a series of two-element reductions. Two ways\nto organize the overall reduction are shown in Figure 3.\nIn both versions, each thread in the thread block starts by\nreading an array element from global memory into shared\nmemory. Then threads are assigned to carry out two-element\nsums.\nThe difference lies in which threads work on which ar-\nray elements. Method 1 interleaves working and non-working\nthreads which act on adjacent elements. Method 2 se-\nquentially assigns working threads so there are contigu-\nous blocks of working and non-working threads. This de-\ncreases the number of divergent warps. Also, the memory\naccesses are strided rather than adjacent to reduce the num-\nber of simultaneous memory bank accesses (since shared\nmemory locations are cyclically assigned to memory banks)\n[16].\nIn addition to reorganizing the tree traversal, other op-\ntimizations –such as explicit loop unrolling and allowing\neach thread to read and sum multiple array elements into its\nshared memory location before the tree traversal begins–\nimprove performance a bit. These techniques had to be\nadapted for non-power-of-2-size arrays, but they greatly\nimproved the speed of the large 1.8×106length diver-\ngence sum.\nFor the smaller 512 and 3445 length column and row\nsums, these techniques were not quite enough, and the CUDA\nkernel ran much slower than a sequential CPU version. In\n0 1 23\n0 1\n00 2\n04 6\n0 40 1 23 4 56 7 0 1 23 4 56 7time steps12 3 4 5 6 78\n6 12 10 8\n14 226 7 8\n12 10 6 7 8 5\n8 36 22 12 10 6 7 5512 3 4 5 6 78\n3 4 7 2\n10 26 15 8\n4 7 6 15 8 2611\n36 2 4 7 6 15 8 2612 3 4 5 6 78 12 3 4 5 6 78\nThread\nIDs\nShared\nMemoryInterleaved Reductio n Strided Sequential R eduction\nGlobal\nMemoryMethod 1 Method 2Figure 3 .Two methods of shared memory reduction\norder to produce more concurrent work (in terms of thread\nblocks), we can compute all 30 of the column or row sums\nsimultaneously. This is accomplished by launching a 2D\ngrid of thread blocks, in which the ﬁrst dimension rep-\nresents which of the 30 sums is being computed and the\nsecond dimension indexes the thread blocks within the in-\ndividual sum. This ﬁnal optimization produced staggering\nspeedup for the 30 smaller sums as shown in Figure 4.\nincreasing optimization0.00000.02000.04000.06000.08000.10000.12000.14000.16000.1800Time to sum 30 columns of length 512 (200 iterations) original inter­\nleaved\n+sequential thread \nassignments\n+strided memory \naccesses\n+unroll last warp\n+completely unroll\n+multiple \nreads/thread\n+concurrent reduc ­\ntionstime [sec]0.155\n0.0027\nFigure 4 .Cumulative effect of various optimizations on running\ntime of 200 iterations of the 30 column sums\n5.3.3 CUDA Performance Results\nThe results for the CUDA implementation compared to\nOpenMP and Matlab implementations are shown in Fig-\nure 5. The Matlab implementation is optimized for single-\nprecision vector operations and uses the dimensionality re-\nduction technique mentioned in Section 3.4. Our Matlab\nimplementation runs about 3 ×faster than a naive Matlab\nimplementation that doesn’t use dimensionality reduction.\nThe OpenMP version runs more than twice as fast as the\nMatlab version on the same machine, and shows signiﬁcant\nspeedup when using more threads on the Core i7; however,\nthe non-linear speedup between 1 and 14 threads suggests\nthat the OpenMP version will not scale well to more cores.\nOur CUDA implementation shows great performance\non the older Geforce 8600 GTS, which has 4 multiproces-\nsors at 1.46 GHz. The newer Geforce GTX 280, with 30\nmultiprocessors at 1.3GHz, runs the CUDA implementa-\ntion over 30×faster than the optimized Matlab implemen-\ntation and 18×faster than the single-threaded OpenMP\n505Poster Session 3\nCore 2 Duo T9300 [2]Core i7 920 [1]\nCore 2 Duo T9300 [2]Core i7 920 [14]\n8600 GTS [GPU]GTX280 [GPU]02468101214161820\nElement Multiply\nRow/Col Sums\nAdd EPS\nDivergence\nElement Divide\nSGEMM \nProcessor Model [threads used]\nMatlab OpenMP CUDA0.60318.5\n11.2\n8.87\n2.592.41Figure 5 .Running time comparison for 200 iterations of\n512×30×3445 NMF using optimized implementations in Mat-\nlab, OpenMP, and CUDA on different architectures\nversion on the Core i7 920. Both of these GPUs are mar-\nketed to consumers for desktop gaming and graphics so\nare quite affordable compared to many of the professional-\ngrade cards.\nAdditional speedup is possible with future GPUs with\nmore multiprocessors and greater memory bandwidth. As\nstated earlier, CUDA programs scale well if kernels have\na large number of independent thread blocks. The rela-\ntively small size of the matrix operations doesn’t guaran-\ntee strong scaling in the future, but in this case, additional\nspeedup is not necessarily required. For audio source sep-\naration, the NMF already performs at 33×real-time on the\nGTX 280.\n6. DISCUSSION AND FUTURE WORK\nAfter achieving such signiﬁcant speedup on the NMF step\nof percussive source separation, the next step would be\nto parallelize the remaining pieces of the complete source\nseparation process. As with the bulk of signal process-\ning and machine learning routines, these steps are all very\ndata-parallel (since individual audio frames can be pro-\ncessed independently) so would beneﬁt from paralleliza-\ntion.\nWhen choosing between OpenMP and CUDA for pro-\ngramming MIR applications, it is important to note that\nwhile CUDA can achieve superior performance on newer\nGPUs, the programmer effort required is much greater than\nwith OpenMP, which is a better starting point for those who\nalready know how to program in C. We must also remem-\nber that parallel MIR applications do not necessarily have\nto be coded from scratch. Many MIR techniques can be as-\nsembled from basic building blocks that already have fast\nparallel implementations. In addition to standard libraries\nlike MKL, fftw, and CUBLAS, many researchers have re-\nleased parallel implementations of important routines.\nWe will be releasing Python modules for the implemen-\ntations described in this paper so that other researchers\ncan beneﬁt from the speed gains. We feel that sharing\nhigh-performance, user-friendly tools in order to encour-\nage more widespread use of parallel implementations withinthe MIR community is an important step in increasing the\npracticality of MIR techniques.\n7. REFERENCES\n[1] G. Tzanetakis: “Marsyas submissions to MIREX\n2007”, MIREX 2007 , 2007. URL: http://www.music-\nir.org/mirex/2008/abs/mirex2007.pdf\n[2] D. Lee and H. Seung: “Learning the parts of objects by non-negative\nmatrix factorization,” Nature , V ol. 401, pp. 788–791, 1999.\n[3] T. Virtanen: “Monaural sound source separation by nonnegative ma-\ntrix factorization with temporal continuity and sparseness criteria,”\nIEEE Transactions on Audio, Speech, and Language Processing ,\nV ol. 15, No. 3, pp. 1066–1074, 2007.\n[4] P. Smaragdis and J. Brown: “Non-negative matrix factorization for\npolyphonic music transcription,” IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics , pp. 177–180, 2003.\n[5] A. Cont, S. Dubnov, D. Wessel: “Realtime Multiple-Pitch and\nMultiple-Instrument Recognition for Music Signals Using Sparse\nNon-Negative Constraints,” Proceedings of the International Confer-\nence on Digital Audio Effects (DAFx) , 2007.\n[6] M. Helen and T. Virtanen: “Separation of drums from polyphonic\nmusic using nonnegative matrix factorization and support vector ma-\nchine,” Proc. EUSIPCO , 2005.\n[7] E. Battenberg: “Improvements to Percussive Compo-\nnent Extraction Using Non-Negative Matrix Factoriza-\ntion and Support Vector Machines,” Masters Thesis, Uni-\nversity of California, Berkeley, December 2008. URL:\nhttp://cnmat.berkeley.edu/publications/author/Battenberg\n[8] K. Asanovic, R. Bodik, et al.: “The landscape of parallel computing\nresearch: A view from Berkeley,” Electrical Engineering and Com-\nputer Sciences, University of California at Berkeley, Technical Report\nNo. UCB/EECS-2006-183 , December, 2006.\n[9] A. Munschi: “OpenCL: Parallel computing on the GPU and CPU,”\nSIGGRAPH08: ACM SIGGRAPH 2008 classes , 2008.\n[10] J. Nickolls, I. Buck, et al.: “CUDA: Scalable parallel programming,”\nACM Queue , April, 2008.\n[11] B. Catanzaro, N. Sundaram, and K. Keutzer: “Fast support vector ma-\nchine training and classiﬁcation on graphics processors,” Proceedings\nof the 25th international conference on Machine learning , pp. 104–\n111, 2008.\n[12] J. Chong, Y . Yi, et al.: “Data-Parallel Large V ocabulary Continuous\nSpeech Recognition on Graphics Processors,” Proceedings of the 1st\nAnnual Workshop on Emerging Applications and Many Core Archi-\ntecture (EAMA) , pp. 23–25, 2008.\n[13] B. Catanzaro, B. Su, et al.: “Efﬁcient, high-quality image contour\ndetection,” International Conference on Computer Vision , 2009.\n[14] D. Lee and H. Seung: “Algorithms for Non-negative Matrix Fac-\ntorization’,” Advances In Neural Information Processing Systems ,\npp. 556–562, 2001.\n[15] Open MP Architecture Review Board: OpenMP application pro-\ngramming interface , Ver. 2.5, May 2005.\n[16] “Nvidia CUDA Programming Guide,” Ver. 2.1, URL: devel-\noper.download.nvidia.com/compute/cuda/2 1/toolkit/\ndocs/NVIDIA CUDA Programming Guide 2.1.pdf, 2008.\n[17] V . V olkov and J. Demmel: “Benchmarking GPUs to tune dense linear\nalgebra,” Supercomputing 08 , 2008.\n[18] M. Harris: “Optimizing parallel reduction in CUDA,” Nvidia Cuda\nSDK 2.1 , URL: http://developer.download.nvidia.com/compute/cuda/\nsdk/website/projects/reduction/doc/reduction.pdf, 2008.\nResearch supported by Microsoft and Intel funding (Award #20080469)\nand by matching funding by U.C. Discovery (Award #DIG07-10227)\n506"
    },
    {
        "title": "Shades of Music: Letting Users Discover Sub-Song Similarities.",
        "author": [
            "Dominikus Baur",
            "Tim Langer",
            "Andreas Butz"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417844",
        "url": "https://doi.org/10.5281/zenodo.1417844",
        "ee": "https://zenodo.org/records/1417844/files/BaurLB09.pdf",
        "abstract": "Many interesting pieces of music violate established structures or rules of their genre on purpose. These songs can be very atypical in their interior structure and their different parts might actually allude to entirely different other songs or genres. We present a query-by-example-based user interface that shows songs related to the one currently playing. This relation is not based on overall similarity, but on the similarity between the part currently playing and parts of other songs in the collection along different dimensions (pitch, timbre, bars, beats, loudness). The similarity is initially computed automatically, but can be corrected by the user. Once a sufficient number of corrections has been made, we expect the similarity measure to reach an even higher precision. Our system thereby allows users to discover hidden similarities on the level of song sections instead of whole songs.",
        "zenodo_id": 1417844,
        "dblp_key": "conf/ismir/BaurLB09",
        "keywords": [
            "query-by-example",
            "user interface",
            "genre structures",
            "atypical songs",
            "different parts",
            "entirely different songs",
            "collection similarity",
            "dimensional analysis",
            "song sections",
            "hidden similarities"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSHADES OF MUSIC: LETTING USERS DISCOVER SUB-SONG\nSIMILARITIES\nDominikus Baur, Tim Langer, Andreas Butz\nMedia Informatics Group\nUniversity of Munich (LMU), Munich, Germany\n{dominikus.baur,andreas.butz }@ifi.lmu.de, tim.langer@campus.lmu.de\nABSTRACT\nMany interesting pieces of music violate established struc-\ntures or rules of their genre on purpose. These songs can\nbe very atypical in their interior structure and their differ-\nent parts might actually allude to entirely different other\nsongs or genres. We present a query-by-example-based\nuser interface that shows songs related to the one currently\nplaying. This relation is not based on overall similarity,\nbut on the similarity between the part currently playing and\nparts of other songs in the collection along different dimen-\nsions (pitch, timbre, bars, beats, loudness). The similarity\nis initially computed automatically, but can be corrected\nby the user. Once a sufﬁcient number of corrections has\nbeen made, we expect the similarity measure to reach an\neven higher precision. Our system thereby allows users to\ndiscover hidden similarities on the level of song sections\ninstead of whole songs.\n1. INTRODUCTION\nAll music is based on repetition on different levels: From\nthe lowest level of sounds in different frequencies to the\nhighest, cultural aspects of genres and trends, every song\nis contained in an intricate network of repeating segments.\nOne of the best known of these patterns is the verse-chorus\nform [1] that has been deﬁning for the last half century\nof popular music and implies inherent repetitive structures,\npossibly to increase recognition. Nevertheless, certain parts\nsuch as the intro, outro or especially the bridge can stand in\ncomplete contrast to the rest of the song, sometimes form-\ning a mini-song of their own (and sometimes even digress-\ning along this path and never returning to their origin).\nMusic recommendation and visualization often relies\non an abstract idea of ”similarity” between songs, which\nis actually a measure for repetition. It is mostly generated\nby collaborative ﬁltering or content-based measures, but\nthis similarity normally works on the level of whole songs,\nwith a set of related songs based on their averaged close-\nness. While some systems access songs on a lower level to\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.extract segments, they do so to ﬁnd the most representative\npart of the song to, again, do an overall comparison.\nIn this way, parts of songs with a high inner diversity\n(as in the bridge parts mentioned above) simply disappear\nin the similarity measure: While the overall impression of\nsong A might be very similar to song B regarding content\nand sound, its bridge might be an allusion to a third song\nC and its outro even closer to another song D, neither of\nwhich is reﬂected in a generalized, one-dimensional simi-\nlarity value. Query-by-example/humming systems, in con-\ntrast, have to rely on these deeper structures within a song,\nas they mostly have to work with incomplete input. Still,\ntheir main use is not to reveal hidden connections between\nparts of songs but to retrieve the one song that the user has\nin mind - multiple songs are only displayed because of in-\naccuracies in retrieval.\nIn this paper we present our web-based system Shades\nof Music that provides users with an interface to retrieve\nand discover connections between songs at the level of\nparts or sections. The user can listen to songs and see\nwhich other songs are similar to the currently playing sec-\ntion and in which of their parts. To stay with the example\nfrom above: For most of song A, sections from song B\nare shown as the most similar ones, but during A’s bridge\nsong C and during A’s outro, song D appear. A similar-\nity between these sections is initially calculated using the\nweb service Echo Nest[2], but our system then encourages\nusers to give feedback and improve its classiﬁcation. In\nthe rest of this paper we present related work in the areas\nof music user interfaces, then describe our system, the way\nusers can give feedback, and the underlying calculations.\n2. RELATED WORK\nQuery-by-example is an active ﬁeld of research that aims\nfor retrieving an item with only insufﬁcient information.\nAs the input mostly represents a part of the full item, ex-\ntracting segments and being able to compare them is an\nimportant ﬁrst step. Older QBE systems for audio mostly\nworked with symbolic MIDI-ﬁles[3], but more recent sys-\ntems evaluate the actual audio signals. Various attributes,\nsuch as note sequences[3], melody[4] (e.g., with Query-\nby-humming), or beat[5] are used. Since these systems\nalways try to retrieve one speciﬁc item, the segmentation\nis used to create a ranked list of possible candidates. More\ncreative approaches to QBE such as [6] are trying to let\n111Poster Session 1\nFigure 1 . Shades of Music: Listening to a song and ﬁnding related songs based on different attributes\nthe user ”sketch” aspects of a song in various ways as an\ninput to the system. Applications for representing larger\nmusic collections follow two courses: One approach is to\nvisualize the collection in a global way, for example us-\ning the popular self-organizing maps (Islands of Music [7],\nbut also [8] and [9]) or force-directed layouts[10]. Another\nway is to display related items based on one currently ac-\ntive item (in principle QBE) as in Musicream[11] or the\nExpressive Music Jukebox[12].\nTo make up for the shortcomings in automatic content-\nbased similarity analysis and allow for personalization, user\nfeedback is incorporated in various systems. Recommender\nsystems[13], for example based on ratings [14] or implicit\ndata such as listening histories in the online community\nLast.fm[15] offer the user suggestions for novel music. Con-\nnections between song parts are central, for example, for\nthe music website Who Sampled? [16] whose community\nadds samples and their origin to the database.\n3. SHADES OF MUSIC\nShades of Music is a (prototype of a) web-based service\nthat allows users to listen to songs and ﬁnd related sec-\ntions. Based on the currently playing song, related sections\nof other songs are displayed. Echo Nest does an initialsimilarity classiﬁcation, but as the interface collects user\nfeedback this similarity measure becomes more accurate.\nWe implemented Shades of Music with the Ruby on Rails\nframework on the server side and a browser application\nbased on Adobe Flash on the client side.\n3.1 The User Interface\nInitially, a list of all songs in her or his collection is dis-\nplayed. Additional songs can easily be uploaded from the\ncomputer or retrieved from online sources. After choosing\na song, the application starts to play this song and displays\nthe main interface (see ﬁgure 1). A horizontal bar at the\nbottom represents the current song and its sections. A play\nhead and additional color highlighting show the currently\nplaying section of the song. With the check boxes below\nthe bar, the user can choose the criteria based on which re-\nlated sections are displayed. Pitch, timbre, bar, beat and\nloudness plus a cumulative total value are available. For\neach selected attribute, an additional line of songs is dis-\nplayed (”Pitch”, ”Timbre” and ”Total” in ﬁgure 1). Their\norder reﬂects the similarity: The most similar song sec-\ntion appears on the left followed by less similar ones to\nthe right. For each of these sections, the complete song is\ndisplayed including artist and title. Each of these songs is\n11210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nagain divided into its subsections with unrelated sections\ntransparent and related sections with a ﬁve-step color cod-\ning that shows similarity for the current attribute from low\nto high. Once familiar with this visualization, the user can\nsee at ﬁrst glance that, for example, the intro of another\nsong is similar to the active section. It is important to note\nhere that the same song might appear not only once but\nseveral times: Once among each of the different attributes\nbut also within the list for one attribute if more than one\nsection of the song corresponds to the current section (see\n”Faithless - God is a DJ” in ﬁgure 2). The lists contain only\nthe ﬁve most similar songs along that particular dimension.\nSince the sections of a song are often very similar to other\nsections of the same song, related sections from the current\nsong are not displayed.\nFigure 2 . Detail of ﬁgure 1: The same song might be rep-\nresented by several sections\nShades of Music can be used as a web-based radio: If\none song is over, the system automatically picks the overall\nmost similar song and starts playing that (which makes the\nﬁrst chosen song the seed song of the playlist[17]). With\nour similarity metric (see below) being symmetrical, this\nwould lead to two similar songs playing in an endless loop\n(as the one most similar to the ﬁrst would in turn have\nthe ﬁrst song as its top candidate). Therefore, the system\nonly plays each song once. The user can of course also\nuse the system to actively navigate her or his collection:\nUpon double-clicking one of the suggested songs, the sys-\ntem starts playing it.\n3.2 Segmentation\nSeparating music into relevant subsections is a topic of ac-\ntive research. Methods learned from extracting representa-\ntive audio thumbnails [1] can also be used to analyse the\nstructure of an audio source [18]. Echo Nest is a web ser-\nvice that provides among others such an analysis for audio\ndata. Besides retrieving meta-data for songs and values\nsuch as their current popularity (based on mentioning on\nwebpages), it also performs segmentation and analysis of\nsongs. Details can be found in [19].\nOne useful feature in our case is the automatic division\ninto longer sections of several seconds length (e.g., verse\nor chorus) and very short segments that form short stable\nelements of a song. For each of these segments, Echo Nestreturns a value for variations in loudness plus a chroma\nvector for pitch and another twelve-dimensional vector for\ntimbre. The pitch chroma vector reﬂects the relative distri-\nbution of the acoustic content along the twelve semitones,\nwhile the timbre vector tries to capture the spectral surface\nof sound in an Echo Nest speciﬁc format with weights for\ntwelve basis functions [2].\nAdditionally, positions of beats and bars for the whole\nsong can also be retrieved. To calculate the similarity be-\ntween sections of songs, we use the following procedure:\nFirst, the positions of segments and sections are retrieved.\nThe longer sections form the basis for the comparison and\nare displayed in the interface as separate areas. Two sec-\ntions’ beat- or bar-wise similarity is determined by count-\ning the number of beats and bars within a section and com-\nparing these numbers.\nFor all segments within one section, values for changes\nin loudness, pitch and timbre are available for a more so-\nphisticated comparison. Variations in loudness can be very\neasily compared by calculating their difference in decibels.\nTo compare the pitch and timbre vectors, the positions of\nthe vectors within one section are averaged and the result-\ning vectors compared using the euclidean distance between\nthem.\nThe ﬁnal comparison value for two sections is formed\nby normalizing all ﬁve values (beats, bars, loudness, pitch,\ntimbre) and calculating the average difference, which leads\nto a ﬁnal similarity between 0 and 1.\nThis very simple algorithm provides an initial compar-\nison that is sufﬁcient for our purposes, as the given values\ncan be adjusted by the users anyway. Adding weights to\nthe different features could also improve the classiﬁcation,\nbut since this would need more ﬁne-tuning, at the moment\nall attributes have the same inﬂuence on the ﬁnal result.\nFigure 3 . User feedback for one suggestion of the system\n3.3 User Feedback\nAutomatically extracted similarity naturally has its limits.\nAlthough the hypothesized glass ceiling [20] for content-\nbased extraction might be circumventable [21], some in-\nherent problems will remain: Especially the issue of per-\nsonalization is crucial. One user’s idea of similarity might\ncompletely differ from another’s who has a different taste\nin music or a more sophisticated sense for it. Thus, we\nare convinced that a metric based on automation is only a\nﬁrst step. For a ﬁnal classiﬁcation, user input has to be in-\ncorporated into the interface and its underlying algorithms.\nLast.fm [15] is a prominent example of a robustly classi-\nﬁed music library based on user feedback.\n113Poster Session 1\n3.3.1 Ratings for the automatic suggestions\nIn its current version, Shades of Music provides a very\nstraightforward mechanism with which the user can cor-\nrect the suggestions of the system. In the general play view,\neach song has a button to the left of its icon that makes a\nsmall window pop up (see ﬁgure 3). Here, the user can rate\nthe suggestion made by the system on a scale from 1 to 5.\nAs songs can appear more than once in the list of sugges-\ntions (for example, if the current song section corresponds\nto the repeated chorus of the other song) and even in sev-\neral lists (for example for beat and pitch), the user can also\ncriticize certain suggestions while promoting others. This\nmeans that the feedback is very speciﬁc and doesn’t sim-\nply rate the computed similarity, but actually the aspect on\nwhich it was based.\nIf the user makes the effort to actually rate a sugges-\ntion, this overrides the respective computed value. The\nuser is unable to see the actual internal similarity values\nand is shown the most similar sections only, so a negative\nrating always results in a reduction of the calculated simi-\nlarity (and possibly a removal of the rated section from the\nlist). Therefore, a vote replaces (for the user who made it)\nthe initial similarity calculated by the system for the two\nsections concerned. The rating of a speciﬁc aspect is inter-\npreted as a similarity of 0.0 (1), 0.25 (2), 0.5 (3), 0.75 (4)\nor 1.0 (5) and stored in the database. If the user votes on\nthe total cumulative value, the rating is used as a factor for\nall the other attributes, so that their average corresponds\nwith the rating value.\n3.3.2 Incorporation of multiple users and feedback\nFrom an initial ﬁve-dimensional metric of similarity be-\ntween sections, the additional user feedback leads for a\nnumber of users to a higher-dimensional similarity. In the\nsimplest case, only one user accesses the system and up-\nloads songs from her or his own collection. The system\ncalculates similarity values for existing sections and the\nuser rates these suggestions as replacements for the au-\ntomatically extracted similarity. In the end, the system\nreaches an optimal suggestion for this theoretical single\nuser (of course with the overhead of rating millions of sec-\ntion combinations).\nAs Shades of Music is a web-based system, it is in-\nherently targeting multiple users who all upload their own\nsongs. This is used to reduce the analytical overhead by\nusing meta-data to identify identical songs within separate\ncollections. For these songs, existing classiﬁcation data\ncan be used. To counter erroneous meta-data, audio thumb-\nnails could also be used for identiﬁcation as the data is ex-\ntracted anyway. Previous ratings by other users work as a\nreﬁnement of the system-generated similarity: All ratings\nfor an attribute of a pair of sections are again converted to\na similarity value and, together with the system-generated\none, averaged to reach a ﬁnal value. In this way, we are\nable to improve suggestions even for new users (as long\nas they upload existing songs which were already rated by\nother users). Once a user starts rating suggestions within\nher or his own collection, these ratings are of course againdirectly applied (see 3.3.1).\n4. SUMMARY AND FUTURE WORK\nWe presented Shades of Music, a web-based system that\nlets users discover connections between parts of songs within\ntheir music collection. For an exemplary song, a number\nof similar song sections are displayed, regarding the ﬁve\nattributes beats, bars, loudness, pitch and timbre and an\naverage total. The user can give a rating for a suggestion\nand thus improve the system’s results for himself and oth-\ners. Informal ﬁrst feedback showed great potential for the\napplication as especially users with large song collections\nwere curious what connections might be discovered. As\na user study for our system should show the merits of the\nunderlying idea and not, for example, the usability of the\ninterface, we plan to open the system for multiple users\nover a longer period of time and collect our observations.\nIn this way, we will also be able to investigate the value of\nthe integrated rating system.\nExtensive testing showed that our prototype also has\nsome shortcomings. First of all, we used a rather simple\nand not state-of-the-art algorithm for calculating the sim-\nilarity between sections. When improving this, we would\nalso address the lack of scalability caused by the pair-wise\ncomparison of sections, for example by indexing [22]. With\nour initial test set of ten songs and an average number of\ntwenty extracted sections, adding one song already leads\nto a total of 20.000 comparisons (4.000 for each of the ﬁve\nattributes).\nThe user interface can also be improved in several ways:\nThe representation of the current song as grey section blocks\ndoes not help in understanding its structure. Labels with\n’verse’ or ’chorus’ might help, but automation to do that\nis probably not feasible. Heuristics, such as ”repeated sec-\ntions are a chorus” will most likely be insufﬁcient. Inter-\nface elements for labelling could be included to let users\ndo that (and maybe also add the lyrics to the song for addi-\ntional orientation).\nThe ways in which users are able to give feedback could\nalso be expanded: Adjustment of section borders or sug-\ngestion of new songs (or sections) are only two ideas. Based\non our algorithm of averaging all users’ votes and the Echo\nNest value for novel users we also face the problem of\nchanging suggestions if new votes arrive. To avoid con-\nfusing our users with ever-changing suggestions, it might\nhelp to only initially use this method and don’t update the\nresults every time the interface is launched. Finally, with\nthe generated database of related song sections, additional\nprojects are also feasible: Novel visualizations for a global\nmusic collection as a network of interconnected song sec-\ntions could prove interesting just as clustering the user com-\nmunity (”neighbors” in Last.fm) based on their votes.\n5. ACKNOWLEDGMENTS\nThis work was funded by the University of Munich and the\nstate of Bavaria. We would like to thank the members of\nthe echonest.com forum for valuable feedback and support.\n11410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n6. REFERENCES\n[1] M.A. Bartsch, and G.H. Wakeﬁeld: “To catch a chorus:\nUsing chroma-based representations for audio thumb-\nnailing,” IEEE Workshop on the Applications of Signal\nProcessing to Audio and Acoustics , pp. 15–18, 2001.\n[2] The Echo Nest, 2009-05-11, http://the.echonest.com\n[3] Y .H. Tseng: “Content-based retrieval for music collec-\ntions,” Proceedings of the 22nd annual international\nACM SIGIR conference on Research and development\nin information retrieval , pp. 176–182, 1999.\n[4] W.H. Tsai, H.M. Yu, and H.M. Wang: “A query-by-\nexample technique for retrieving cover versions of pop-\nular songs with similar melodies,” Proceedings of the\nInternational Symposium on Music Information Re-\ntrieval , pp. 183–190, 2005.\n[5] A. Kapur, M. Benning, and G. Tzanetakis: “Query-by-\nbeat-boxing: Music retrieval for the DJ,” Proceedings\nof the International Conference on Music Information\nRetrieval , pp. 170–177, 2004.\n[6] G. Tzanetakis, A. Ermolinskyi, and P. Cook: “Be-\nyond the query-by-example paradigm: New query in-\nterfaces for music information retrieval,” Proceedings\nof the 2002 International Computer Music Conference ,\npp. 177–183, 2002.\n[7] E. Pampalk, A. Rauber, and D. Merkl: “Content-based\nOrganization and Visualization of Music Archives,”\nProceedings of the tenth ACM international conference\non Multimedia , pp. 570–579, 2002.\n[8] F. Morchen, A. Ultsch, M. Nocker, and C. Stamm:\n“Databonic visualization of music collections accord-\ning to perceptual distance,” Proceedings of the 6th\nInternational Conference on Music Information Re-\ntrieval , 2005.\n[9] P. Knees, M. Schedl, T. Pohle, and G. Widmer: “An\ninnovative three-dimensional user interface for explor-\ning music collections enriched with meta-information\nfrom the web,” Proceedings of the ACM Multimedia ,\npp. 17–24, 2006.\n[10] R. van Gulik, F. Vignoli, and H. van de Wetering:\n“Mapping music in the palm of your hand, explore and\ndiscover your collection,” Proceedings of the 5th IS-\nMIR Conference , 2004.\n[11] M. Goto, and T. Goto: “Musicream: New music play-\nback interface for streaming, sticking, sorting, and re-\ncalling musical pieces,” Proceedings of the 6th Inter-national Conference on Music Information Retrieval ,\npp. 404–411, 2005.\n[12] F. Vignoli, and S. Pauws: “A music retrieval sys-\ntem based on user-driven similarity and its evaluation,”\nProceedings of 6th ISMIR Conference , pp. 272–279,\n2005.\n[13] G. Adomavicius, and A. Tuzhilin: “Toward the next\ngeneration of recommender systems: A survey of state-\nof-the-art and possible extensions,” IEEE Transactions\non Knowledge and Data Engineering , V ol. 17, No. 6,\npp. 734–749, 2005.\n[14] K. Hoashi, K. Matsumoto, and N. Inoue: “Person-\nalization of user proﬁles for content-based music re-\ntrieval based on relevance feedback,” Proceedings of\nthe eleventh ACM internation conference on Multime-\ndia, pp. 110–119, 2003.\n[15] Last.fm, 2009-05-11, http://www.last.fm\n[16] Who Sampled?, 2009-05-17,\nhttp://www.whosampled.com\n[17] E. Pampalk, T. Pohle, and G. Widmer: “Dynamic\nplaylist generation based on skipping behavior,” Pro-\nceedings of the 6th ISMIR Conference , pp. 634–637,\n2005.\n[18] J. Paulus, and A. Klapuri: “Music structure analysis\nby ﬁnding repeated parts,” Proceedings of the 1st ACM\nWorkshop on Audio and music computing multimedia ,\npp. 59–68, 2006.\n[19] T. Jehan: Creating Music by Listening , PhD Thesis in\nMedia Arts and Sciences. MIT, 2005.\n[20] F. Pachet, and J.J. Aucouturier: “Improving timbre\nsimilarity: How high is the sky?,” Journal of nega-\ntive results in speech and audio sciences , V ol. 1, No. 1,\n2004.\n[21] T. Lidy, A. Rauber, A. Pertusa, and J. M. Inesta: “Im-\nproving Genre Classiﬁcation By Combination of Au-\ndio and Symbolic Descriptors Using a Transcription\nSystem,” Proceedings of the International Symposium\non Music Information Retrieval , pp. 61–66, 2007.\n[22] R. Cai, C. Zhang, L. Zhang, and W.-Y . Ma: “Scal-\nable Music Recommendation by Search,” Proceedings\nof the 15th international conference on Multimedia ,\npp. 1065–1074, 2007.\n115"
    },
    {
        "title": "Evaluation of Multiple-F0 Estimation and Tracking Systems.",
        "author": [
            "Mert Bay",
            "Andreas F. Ehmann",
            "J. Stephen Downie"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418241",
        "url": "https://doi.org/10.5281/zenodo.1418241",
        "ee": "https://zenodo.org/records/1418241/files/BayED09.pdf",
        "abstract": "Multi-pitch estimation of sources in music is an ongoing research area that has a wealth of applications in music information retrieval systems. This paper presents the systematic evaluations of over a dozen competing methods and algorithms for extracting the fundamental frequencies of pitched sound sources in polyphonic music. The evaluations were carried out as part of the Music Information Retrieval Evaluation eXchange (MIREX) over the course of two years, from 2007 to 2008. The generation of the dataset and its corresponding ground-truth, the methods by which systems can be evaluated, and the evaluation results of the different systems are presented and discussed.",
        "zenodo_id": 1418241,
        "dblp_key": "conf/ismir/BayED09",
        "keywords": [
            "Multi-pitch estimation",
            "music information retrieval systems",
            "competing methods",
            "fundamental frequencies",
            "polyphonic music",
            "Music Information Retrieval Evaluation eXchange (MIREX)",
            "ground-truth",
            "evaluation results",
            "systems",
            "evaluation"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nEVALUATION OF MULTIPLE-F0 ESTIMATION AND TRACKING\nSYSTEMS\nMert Bay Andreas F. Ehmann J. Stephen Downie\nInternational Music Information Retrieval Systems Evaluation Laboratory\nUniversity of Illinois at Urbana-Champaign\n{mertbay, aehmann, jdownie }@illinois.edu\nABSTRACT\nMulti-pitch estimation of sources in music is an ongoing\nresearch area that has a wealth of applications in music in-\nformation retrieval systems. This paper presents the sys-\ntematic evaluations of over a dozen competing methods\nand algorithms for extracting the fundamental frequencies\nof pitched sound sources in polyphonic music. The eval-\nuations were carried out as part of the Music Information\nRetrieval Evaluation eXchange (MIREX) over the course\nof two years, from 2007 to 2008. The generation of the\ndataset and its corresponding ground-truth, the methods by\nwhich systems can be evaluated, and the evaluation results\nof the different systems are presented and discussed.\n1. INTRODUCTION\nA key aspect of many music information retrieval (MIR)\nsystems is the ability to extract useful information from\ncomplex audio, which may then be used in a variety of\nuser scenarios such as searching and organizing music col-\nlections. Among these extraction techniques, the goal of\nmultiple fundamental frequency (multi-F0) estimation is\nto extract the fundamental frequencies of all (possibly con-\ncurrent) notes within a polyphonic musical piece. The ex-\ntracted representations usually either take the form of a\n1) list of pitches vs. time; or, 2) a MIDI-like representa-\ntion that contains individual notes and their onset and off-\nset times. These representations represent an intermediary\nbetween the audio and the score. While automatic tran-\nscriptions systems concern themselves with generating the\nactual score of music being analyzed, the intermediate rep-\nresentation generated by multi-F0 systems is useful in its\nown right. Such information can be very useful for other\nMIR systems as higher level features: to deﬁne the struc-\nture of the song, to make a better search or recommenda-\ntion based on the score, or for F0-guided source separation.\nRecently, there has been great interest in multi-F0 estima-\ntion.\nTo understand the current state of art, starting in 2007,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.the MIREX [3] organized a multi-F0 evaluation task. This\ntask can be considered as an evolution and superset of the\nprevious MIREX audio melody extraction tasks. For more\ninformation on audio melody extraction, we refer the reader\nto [13]. The MIREX multiple-F0 task consists of two sub-\ntasks built around the two pitch representations mentioned\nearlier. The ﬁrst subtask is called Multiple-F0 Estimation\n(MFE). In MFE, systems are required to return a list of\nactive pitches at ﬁxed time steps (analysis frames) of a\npolyphonic recording. The second subtask is called Note\nTracking (NT). In the NT subtask, systems are required to\nreturn the note F0, onsets and offsets of note events in the\npolyphonic mixture, similar to a piano-roll representation.\nThe MIREX multiple-F0 task attracted many researchers\nfrom around the world. In the 2007 MFE subtask, there\nwere a total of 16 algorithms from 12 labs. For the NT\nsubtask, there were 11 algorithms from 7 labs. In 2008,\nthere were a total of 15 algorithms from 10 labs for MFE\nand 13 algorithms from 8 labs for NT.\nThis paper serves to discuss the current performance of\nmulti-F0 systems and to analyze the results of the MIREX\nalgorithm evaluations. The paper is organized as follows.\nThe rest of Section 1 describes the main approaches and\nchallanges to MFE and NT. Section 2 describes the eval-\nuation process. Section 2.1 describes the dataset and Sec-\ntion 2.2 deﬁnes the evaluation metrics. Section 3 discusses\nthe results and some approaches from the MIREX 2007\nand 2008 MFE and NT subtasks. Section 4 provides some\nconcluding remarks.\n1.1 An Overview of Multiple-F0 Estimation and Note\nTracking Methods\nThere are many methods for F0 estimation and note track-\ning and an in-depth coverage of the many possible tech-\nniques is beyond the scope of this paper. Instead, we will\nprovide a very brief overview of methods. Table 1 shows\nthe participants of the MIREX 2007 and 2008 MFE and\nNT subtasks and their proposed methods. All systems use\na time-frequency representation of the input signal as a\nfront-end. The time-frequency representations include short-\ntime Fourier transforms [1,2,6,10,11,13,15], auditory ﬁlter\nbanks [16, 17], wavelet decompositions [5] and sinusoidal\nanalysis [18]. Characteristics of the spectrum such as har-\nmonicity [5,10,14,17,19], spectral smoothness [11], onset\nsynchronicity of harmonics [18] are often used to extract\n315Poster Session 2\nF0s either by grouping harmonics together or calculating\nscores for different F0 hypotheses.\nA large cross-section of techniques use nonnegative ma-\ntrix factorization (NMF) to decompose the observed mag-\nnitude spectrum into a sparse basis. Fundamental frequen-\ncies can then be determined for each basis vector, and the\nonsets/offsets are computed from the amplitude weight of\neach basis throughout a piece. Some systems follow classi-\nﬁcation approaches which attempt to ﬁnd pre-trained notes\nin the mixture. In general, it is possible to categorize the\nmethods used into two groups in terms of how they ap-\nproach polyphony. In the ﬁrst group, systems extract F0s\nfor the predominant source in the polyphonic mixture. The\nsource is subsequently canceled or suppressed and the next\npredominant F0 is estimated. This procedure goes on iter-\natively until all sources are estimated. In the second group,\nsystems attempt to estimate all F0s jointly.\n2. EVALUATION\nExtracting pitch information from polyphonic music is a\ndifﬁcult problem. This is why we choose to subdivide the\ntask into the two MFE and NT subtasks. MFE deﬁnes a\nlower level representation for multiple-F0 systems. In this\nsubtask, the systems estimate the F0s of active sources for\neach analysis frame.In many multi-F0 systems, frame-level\nF0 estimation is a precursor to the NT subtask. In the NT\nsubtask, the systems are required to report the F0, onset\nand offset times of every note in the input mixture. Origi-\nnally, additional timbre-tracking subtasks were envisioned\nfor the MIREX multi-F0 task. Timbre tracking requires\nthat the systems return the F0 contour and the notes of\neach individual source (e.g., oboe, ﬂute, etc.) separately.\nHowever these subtasks were canceled due to lack of par-\nticipation.\n2.1 Creating the Dataset and the Ground-truth\nThe MIREX multi-F0 dataset consists both of recordings\nof a real-world performance and pieces generated from\nMIDI. The real-world performance is a recording of L.\nvan Beethoven Variations from String Quartet Op.18 N.5.\nwhich is adapted and arranged for a woodwind quintet which\nconsists of bassoon, clarinet, ﬂute, horn and oboe. The\npiece was chosen due to its highly contrapuntal nature where\nthe lines of each instrument are fairly different but sound\nharmonious when played together. Also, the predominant\nmelodies alternate between instruments. The recording was\ndone at the School of Music at the University of Illinois at\nUrbana-Champaign. First, the members of the quintet were\nrecorded playing together where each performer was close\nmic‘ed. Second, each part was then recorded in complete\nisolation while the performer listened to and played along\nwith the other parts previously recorded through headphones.\nThe rerecording was done in isolation because there was\nsigniﬁcant bleed through of other sources into each instru-\nments microphone during the ensemble recording. The\nMIREX 2007 dataset consisted of ﬁve different 30-second\nsections that were chosen from the nine minute recording.The MIREX 2008 data set added two more 30-second sec-\ntions for a total of seven. The sections were chosen based\non high activity of all sources. The isolated instruments\nfrom those sections were mixed to form mixtures start-\ning from duet (two polyphony) to quintet (ﬁve polyphony).\nThis results in four clips per section where each clip is\ngenerated by introducing an extra instrument to the mix-\nture. There was no normalization during mixing, so each\nsource‘s loudness in the mixture depends on how it was\nperformed by the musician.\nTo create the ground-truth set, monophonic pitch detec-\ntors were used on the isolated instrument tracks using a 46\nms window and a 10 ms hop size. The pitch detectors used\nwere Wavesurfer ,Praat andYIN. The pitch contours gen-\nerated were manually inspected and corrected by experts to\nget rid of common monophonic pitch detector errors such\nas voiced / unvoiced detection and octave errors. To cre-\nate the ground-truth for the NT subtask, the isolated instru-\nment recordings were annotated by hand to determine each\nnote’s onset, offset and its F0 by inspecting the extracted\nmonophonic pitch contour, the time domain amplitude en-\nvelope and the spectrogram of the recording.\nThe second, MIDI-based, portion of the dataset comes\nfrom two different sources. The ﬁrst set was generated\nby [18] by creating monophonic tracks rendered and syn-\nthesized from MIDI ﬁles using real instrument samples\nfrom the RWC database [8]. The monophonic tracks were\ncreated such that no notes overlap so that each frame in the\ntrack is strictly monophonic. The ground-truth for MFE\nwas extracted using YIN. The ground-truth for the NT sub-\ntask was generated using the MIDI ﬁle. Two 30-seconds\nsections with 4 clips from two to ﬁve polyphony were used\nfrom this data. The second set, which was used only for the\nnote tracking subtask, was generated by [12] by record-\ning a MIDI-controlled Disklavier playback piano. Two\none-minute clips were used from this dataset for the note\ntracking subtask. The ground-truth was generated using\nthe MIDI ﬁles.\n2.2 Evaluation Methods and Metrics\nThis section describes the evaluation methods used in MIREX\n2007 and 2008. The MFE and NT subtasks have different\nmethods for evaluation.\n2.2.1 Multi-F0 Estimation Evaluation\nAs mentioned earlier, the multi-F0 task represents a frame-\nlevel estimation of F0s where submitted systems were re-\nquired to report active F0s every 10 ms. Many different\nmetrics are used to evaluate this subtask. We begin by\ndeﬁning precision, recall and F-Measure as:\nP recision =∑T\nt=1T P(t)\n∑T\nt=1T P(t) +F P(t)(1)\nRecall =∑T\nt=1T P(t)\n∑T\nt=1T P(t) +F N(t)(2)\nF-measure =2×precision ×recall\nprecision +recall(3)\n31610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSystems Code Front End F0-Est Method Note Tracking method Ref\nCont AC STFT NMF with sparsity constraints NMF with sparsity constraints [2]\nCao, Li CL STFT Subharmonic sum, cancel-iterate N/A [1]\nYeh et al. YRC Sinusoidal an. Joint Estimation based on spectral features HMM tracking [18]\nPoliner, Ellis PE STFT SVM classiﬁcation HMM tracking [13]\nLeveau PL Matching pursuit Matching Pursuit with harmonic atoms N/A [10]\nRaczy ´nski et al. SR Constant-Q trans. Harmonicity constrained NMF N/A [14]\nDurrieu et al. DRD STFT GMM source model, cancel-iterate N/A [4]\nEmiya et al. EBD STFT Derived from note tracking HMM Tracking [6]\nEgashira et al. EOS Wavelets Derived from note tracking EM ﬁt of Harmonic Temp. Models [5]\nGroble STFT MG Scoring on pre-trained pitch models. N/A [9]\nPertusa, I ˜nesta PI STFT Joint Estimation based on spectral features Merge notes [11]\nReis et al. RFF STFT Derived from note tracking Genetic Alg. [15]\nRyyn ¨anen, Klapuri RK Auditory model Derived from note tracking HMM note and key models [16]\nVincent et al. EBD ERB ﬁlter-bank Derived from note tracking Harmonicity constrained NMF [17]\nZhou, Reiss ZR RTFI N/A Harmonic grouping, onset detection [19]\nTable 1 . Summary of submitted multi-F0 and note tracking systems.\nSince not all sources are active during any given analysis\nframe, the number of F0s in each time step of the ground-\ntruth varies with time. For that reason, T P,F PandF N\nare deﬁned as a function of time (frame index, t) as fol-\nlows: “true positives” T P(t)are calculated for frame t,\nbased on the number F0s that correctly correspond between\nthe ground-truth F0 set and the reported F0 set for that\nframe. “False positives” F P(t)are calculated as the num-\nber of F0s detected that do not exist in the ground-truth\nset for that frame. The notion of “false negatives” F N(t)\nhowever, becomes more problematic. We ﬁrst begin by\ndeﬁning the notion of a negative. We deﬁne negatives\nbased on the maximum polyphony of a each musical clip.\nTherefore, a quartet clip has a polyphony of four. Nega-\ntives in the ground-truth for each frame are calculated as\nthe difference of the total polyphony and the number of\nF0s in the ground-truth. Similarly, the number of negatives\nfor each frame in the reported F0 transcriptions are the dif-\nference between the total polyphony and the number of re-\nported F0s. Therefore, the false negatives for each frame,\nF N(t), is calculated as the difference between the number\nof reported negatives at frame tand the number of nega-\ntives in the ground-truth at frame t. Therefore, false nega-\ntives represent the number of active sources in the ground-\ntruth that are not reported. The T P(t),F P(t)andF N(t)\nare summed across all frames to calculate the total num-\nber of T Ps,F Ps and F Ns for a given musical clip. From\nthese measures, we can calculate an overall accuracy score\nas:\nAccuracy =∑T\nt=1T P(t)\n∑T\nt=1T P(t) +F P(t) +F N(t)(4)\nThis is a measure of overall performance bounded be-\ntween 0 and 1 where 1 corresponds to perfect transcrip-\ntion. However, it does not explain the types of errors that\ncan happen. Therefore, we turn our attention to measures\nwhich better identify the types of errors multi-F0 systems\nmake. We ﬁrst note that not every instrument is active\nat every time frame. For example, an instrument in the\nmixture might be inactive through most of a piece’s dura-\ntion and active for only a relatively short amount of time.There are different kind of errors that can happen in es-\ntimating and reporting F0 candidates. An F0 of a source\ncan be missed altogether, substituted with a different F0,\nor an extra F0 can be inserted (“false alarm” or false pos-\nitive). To explain these types of errors, a measure called\nthe frame-level transcription error score deﬁned by [7] and\nused for music transcription by [12] is used. The beneﬁt\nof this error measure is that this single error score can be\ndecomposed into the three aforementioned types of errors,\nnamely a miss, substitution, or false alarm. The total error\nscore is deﬁned as\nEtot=∑T\nt=1max( Nref(t), Nsys(t))−Ncorr(t)\n∑T\nt=1Nref(t)(5)\nwhere Nref(t)is the number of F0s in the ground-truth\nlist for frame t, Nsys(t)is the number of reported F0s and\nNcorr(t)is the number of correct F0s for that frame. This\nerror counts the number of returned F0s that are not correct\n(they are either extra or substituted F0s) and the number of\nF0s that are missed. The total error is calculated by sum-\nming the frame level errors and normalizing by the the total\nnumber of F0s in the ground-truth. The maximum bound\nof this error score is directly correlated with the number\nof F0s returned. Not returning anything will result in a\nscore of 1 while perfect transcription will yield a score of\n0. However, the total error is not necessarily bounded by\n1. This total error can be decomposed into the sum of three\nsub-errors. The substitution error is deﬁned as\nEsub=∑T\nt=1min(Nref(t), Nsys(t))−Ncorr(t)\n∑T\nt=1Nref(t)(6)\nThe substitution error counts the number of ground-truth\nF0s for each frame that were not returned, but some other\nincorrect F0s were returned instead. These types of errors\ncan be considered substitutions. This score is bounded be-\ntween 0 and 1.\nMissed errors are deﬁned as\nEmiss =∑T\nt=1max(0 , Nref(t)−Nsys(t))\n∑T\nt=1Nref(t)(7)\n317Poster Session 2\nwhich counts the number of F0s in the ground-truth that\nwere missed by the system with no substitute F0s being\nreturned. This error is also bounded between 0 and 1.\nFalse alarms are deﬁned as\nEfa=∑T\nt=1max(0 , Nsys(t)−Nref(t))\n∑T\nt=1Nref(t)(8)\nwhich counts the number of extra F0s returned that are\nnot substitutes. Every extra F0 after the number of F0s in\nthe ground-truth list is counted as false alarm. The upper\nbound of this error depends on the number of F0s returned.\nAll errors are normalized by the total number of F0s in the\nground-truth. The error is good measure for this task be-\ncause it enables us to explain different types of errors and\ncan also provide a single measure for comparison.\n2.2.2 Note Tracking Evaluation\nIn the note tracking subtask, systems are required to return\na list of notes where each note is designated by its F0, on-\nset and offset time. The evaluation of this subtask is more\nstraightforward then the frame-level subtask. We can think\nof the ground-truth list as a ﬁxed collection of events where\neach event is deﬁned by three variables, F0, onset and off-\nset. Due to the difﬁculty of detecting offsets in a highly\npolyphonic mixture, the evaluations were calculated using\ntwo different scenarios. In the ﬁrst scenario, a returned\nnote event is assumed to be correct if its onset is within\na +/-50 millisecond range of a ground-truth onset and its\nF0 is within +/- a quarter tone (3%) of the ground-truth\npitch. Here, the offset times are ignored. In the second sce-\nnario, in addition to the previous onset and pitch require-\nments, the correct returned note is required to have an off-\nset time within 20% of ground-truth note’s duration around\nthe ground-truth note’s offset value, or within 50 millisec-\nonds of the ground-truth note’s offset, whichever is larger.\nFor these two cases, precision, recall and F-measure are\ncalculated where true positives are deﬁned as the returned\nnotes that conform to the previously mentioned require-\nments and false positives were deﬁned as the ones that do\nnot. We also deﬁne an additional measure called Overlap\nRatio (OR). The OR for a ith correct note in the returned\nlist is deﬁned as\nORi=min(tref\ni,off, tsys\ni,off)−max(tref\ni,on, tsys\ni,on)\nmax(tref\ni,off, tsys\ni,off)−min(tref\ni,on, tsys\ni,on)(9)\nwhere tsys\ni,offandtsys\ni,onare the offset and the onset times\nof the correctly returned note and tref\ni,offandtref\ni,onare the\noffset and onset times of the corresponding ground-truth\nnote. An average OR score is a good measure of how much\nthe correct returned note overlaps with the corresponding\nground-truth note. This information is especially useful\nwhen the correct notes are calculated based on the onset\nonly.3. RESULTS AND DISCUSSION\nThe evaluation results of two iterations of the MIREX multi-\nF0 estimation task (2007-2008) are presented here. We\nﬁrst turn our attention to the frame-level MFE subtask.\nFigure 1 shows the precision, recall, and accuracy scores\nfor all submitted MFE systems over the two years. In gen-\neral, systems have improved in accuracy over the course of\nthe two years.\nIn Figure 2, a bar graph of the total error is shown for\neach of the systems. Each total error bar is subdivided into\nthe three types of errors that constitute it namely, miss er-\nrors, substitution errors, and false alarm errors. It is evident\nthat different systems present different trade-offs in terms\nof the types of errors. Referring back to Fig. 1, one can see\nthat some systems have a very high precision compared to\ntheir accuracy such as those by PI, EBD and PE [6,11,13].\nPI has the highest precision in both years. The reason be-\nhind this is that most of the F0s reported by these systems\nare correct, but they tend to under-report and miss a lot of\nactive F0s in the ground-truth. This type of behavior is also\nevident in Fig. 2. While PI systems have the lowest total\nerror score, there are very few false alarms compared to\nmiss errors. PI achieves a low number of local false pos-\nitives by taking into account a temporal salience of each\ncombination of pitches. The results are post-processed by\neither merging/ignoring note events or using a weighted\ndirected acyclic graph (wDAG).\nSimilarly, EBD and PE use hidden Markov models for\ntemporal smoothing, and also have a relatively high miss\nerror. RK [16] and YRC [18] have balanced precision, re-\ncall, as well as a balance in the three error types, and as\na result, have the highest accuracies for MIREX 07 and\nMIREX 08, respectively. On the other hand, some sys-\ntems like half of the CL submissions, have a high recall\ncompared to their precision accuracy. CL returned a ﬁxed\n(maximum) number of F0s for every frame regardless of\nthe input polyphony in order to maximize recall.\nThe top two submissions share similar approaches. Both\nYRC and PI(1,2) generate a pool of candidate F0s for each\nframe and combine the candidates into hypotheses to jointly\nevaluate the present F0s. YRC ﬁrst estimates an adap-\ntive noise level, and extracts sinusoidal components. The\nalgorithm then extracts F0 candidates until all the sinu-\nsoidal components are explained in the signal, as well as\na polyphony inference stage that estimates the number of\nconcurrent sources. All combinations of F0 candidates\nare evaluated by a score function based on smoothness\nand harmonicity, among others, and the best set is cho-\nsen. Finally, a tracking method is performed by ﬁrst con-\nnecting F0 candidates across frames to establish candidate\ntrajectories and then pruning them using HMMs. PI takes\na similar approach in that, once again, joint F0 hypothe-\nses are evaluated using saliency scores based on properties\nsuch as spectral smoothness and candidate loudness. Post-\nprocessing either takes into account local signal character-\nistics taken from adjacent frames or uses wDAGs for F0\nnote merging or pruning. The top performing algorithm\nfrom 2007, RK uses an auditory inspired model for anal-\n31810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n00.10.20.30.40.50.60.70.8\n  \nYRC2−08PI2−08\nYRC1−08RK−07RK−08PI1−08YRC−07ZR−07PI−07\nVBB2−07VBB−08CL1−07DRD−08CL2−08SR−07\nVBB1−07EOS−08EBD1−08EBD2−08PE−07MG−08PL−07CL1−08CL2−07EOS2−07EOS1−07AC2−07AC1−07RFF1−08RFF2−08EBD−07Accuracy\nPrecision\nRecallFigure 1 . Precision, recall and accuracy for MIREX 07 and MIREX 08 MFE subtask ordered by accuracy.\nysis, and uses HMMs for note models and for note transi-\ntions, after a musical key estimation stage, in an attempt to\nincorporate some musicological information into the pro-\ncess.\nFor the NT subtask, Fig. 3 shows the precision, recall,\nand F-measures of the onset-offset based evaluation of the\nnote tracking systems. We notice that in the NT onset-\noffset evaluation, performance is relatively poor. The likely\nexplanation of this performance stems from the difﬁculty\nin properly deﬁning an offset ground-truth in the data sets.\nIn the woodwind data set, offset ground-truth was deﬁned\non the monophonic recordings of each track where the off-\nset was labeled at very low loudness. Once mixed, other\nsignals can dominate the low level of a source at the tail\nend of its decay such that the offset within the mixture\nis somewhat ambiguous. For the MIDI-generated piano\ndataset, offset is deﬁned based on the MIDI ﬁle, and does\nnot take into account the natural decay and the reverbera-\ntion of the piano. Therefore, in the woodwind dataset, the\noffset time may be overestimated, whereas in the MIDI-\ngenerated dataset, the offset may be underestimated. Due\nto the inherent difﬁculty of properly deﬁning offset, we\nalso evaluate based strictly on note onset. The onset-based\nevaluation results of the NT subtask can be seen in Fig. 4.\nMore detailed results and signiﬁcance tests can be found at\nthe MIREX wiki pages.1\n4. CONCLUSION\nInspecting the methods used and their performances, we\ncannot make generalized claims as to what type of ap-\nproach works best. In fact, statistical signiﬁcance testing\nshowed that the top three methods were not signiﬁcantly\ndifferent. However, systems that go beyond simple frame-\nlevel estimation methods and incorporate temporal con-\nstraints or other note tracking methods seem to perform\nbetter. It is plausible that timbral/instrument tracking can\nimprove MFE even more. A future direction for evaluation\nwould then be to add an instrument tracking subtask that\n1http://www.music-ir.org/mirex/2007/index.php/MIREX2007 Results\nhttp://www.music-ir.org/mirex/2008/index.php/MIREX2008 Results\n00.20.40.60.811.21.41.6\n  PI2−08PI1−08\nYRC2−08\nPI−07\nYRC−07\nRK−07RK−08\nYRC1−08\nZR−07\nVBB2−07\nVBB−08\nVBB1−07\nCL2−08\nEBD2−08EBD1−08\nPE−07\nPL−07\nEOS−08\nSR−07\nCL1−07\nDRD−08\nRFF2−08RFF1−08\nMG−08\nEBD−07\nAC2−07\nEOS2−07EOS1−07\nAC1−07\nCL1−08CL2−07Esubs\nEmiss\nEfaFigure 2 . Error scores for MIREX 07 and MIREX 08 MFE\nsubtask ordered by total error.\n00.050.10.150.20.250.30.350.40.45\n  YRC−08\nRK−07RK−08\nZR3−08\nPE−07\nZR2−08ZR1−08\nPI1−08\nEOS−08\nPI1−07\nVBB2−07\nVBB−08\nPI2−08\nEBD1−08\nVBB1−07\nEBD2−08\nPI2−07\nEOS1−07\nEBD−07\nEOS1−07\nRFF2−08RFF1−08\nAC2−07AC1−07Precision\nRecall\nAve. F−measure\nFigure 3 . Precision, recall and F-measure based on note\nonset and offset for the MIREX 07 and MIREX 08 NT\nsubtask.\n319Poster Session 2\n00.10.20.30.40.50.60.70.8\n  RK−07RK−08\nYRC−08\nZR3−08\nVBB2−07\nVBB−08\nZR2−08ZR1−08\nEOS−08\nPE−07\nPI1−08\nVBB1−07\nEBD1−08\nPI1−07PI2−08\nEBD2−08\nEOS2−07EOS1−07\nPI2−07\nEBD−07\nRFF1−08RFF2−08\nAC2−07AC1−07Precision\nRecall\nAve. F−measure\nAve. OverlapFigure 4 . Precision, recall and F-measure based on note\nonset only for the MIREX 07 and MIREX 08 NT subtask.\nwould lead to a more complete music transcription task.\nThe music transcription ﬁeld is advancing but the problem\nis still far from being solved and there is a great room for\nimprovement.\n5. REFERENCES\n[1] C. Cao and M. Li. Multiple F0 Estimation in\nPolyphonic Music, Avaliable at http://www.music-\nir.org/mirex/2008/abs/mirex08 multiF0 Cao.pdf.\n[2] A. Cont, S. Dubnov, and D. Wessel. Realtime multiple-\npitch and multiple-instrument recognition for music\nsignals using sparse non-negative constraints. In Pro-\nceedings of the International Conference on Digital\nAudio Effects (DAFx). Bordeaux, France , 2007.\n[3] J.S. Downie. The music information retrieval evalu-\nation exchange (2005–2007): A window into music\ninformation retrieval research. Acoustical Science and\nTechnology , 29(4):247–255, 2008.\n[4] J.L. Durrieu, G. Richard, and B. David. Singer melody\nextraction in polyphonic signals using source sep-\naration methods. In IEEE International Conference\non Acoustics, Speech and Signal Processing, 2008.\nICASSP 2008 , pages 169–172, 2008.\n[5] K. Egashira, N. Ono, and S. Sagayama. Sequen-\ntial Estimation of Multiple Fundamental Fre-\nquencies Through Harmonic-Temporal-Structured\nClustering, Avaliable at http://www.music-\nir.org/mirex/2008/abs/F0 egashira.pdf.\n[6] V. Emiya, R. Badeau, and B. David. Multipitch esti-\nmation of inharmonic sounds in colored noise. In Proc.\nInt. Conf. Digital Audio Effects (DAFx), Bordeaux,\nFrance , pages 93–98, 2007.\nWe thank Andrew W. Mellon Foundation for their ﬁnancial support.[7] J.G. Fiscus, N. Radde, J.S. Garofolo, A. Le, J. Ajot,\nand C. Laprun. The rich transcription 2005 spring\nmeeting recognition evaluation. Lecture Notes in Com-\nputer Science , 3869:369, 2006.\n[8] M. Goto. Development of the RWC music database.\nInProceedings of the 18th International Congress on\nAcoustics (ICA 2004) , volume 1, pages 553–556, 2004.\n[9] M. Groble. Multiple fundamental frequency\nestimation, Avaliable at http://www.music-\nir.org/mirex/2008/abs/F0 groble.pdf.\n[10] P. Leveau, D. Sodoyer, and L. Daudet. Automatic In-\nstrument Recognition in a Polyphonic Mixture using\nSparse Representations. In Proc. of Int. Conf. on Music\nInformation Retrieval (ISMIR), Vienne, Autriche , 2007.\n[11] A. Pertusa and J.M. Inesta. Multiple fundamental\nfrequency estimation using Gaussian smoothness. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing, 2008. ICASSP 2008 , pages\n105–108, 2008.\n[12] G.E. Poliner and D.P.W. Ellis. A discriminative model\nfor polyphonic piano transcription. EURASIP Journal\non Advances in Signal Processing , 2007:1–9, 2007.\n[13] G.E. Poliner, D.P.W. Ellis, A.F. Ehmann, E. Gomez,\nS. Streich, and B. Ong. Melody Transcription From\nMusic-Audio: Approaches and Evaluation. IEEE\nTransactions on Audio Speech and Language Process-\ning, 15(4):1247, 2007.\n[14] S.A. Raczynski, N. Ono, and S. Sagayama. Multipitch\nanalysis with harmonic nonnegative matrix approxima-\ntion. In Proc. Int. Conf. on Music Information Retrieval\n(ISMIR) , pages 381–386, 2007.\n[15] G. Reis, N. Fonseca, F.F. de Vega, and A. Ferreira.\nHybrid Genetic Algorithm Based on Gene Fragment\nCompetition for Polyphonic Music Transcription. Lec-\nture Notes in Computer Science , 4974:305, 2008.\n[16] M. P. Ryynanen and A. Klapuri. Polyphonic mu-\nsic transcription using note event modeling. In IEEE\nWorkshop on Applications of Signal Processing to Au-\ndio and Acoustics, 2005 , pages 319–322, 2005.\n[17] E. Vincent, N. Bertin, and R. Badeau. Harmonic and\ninharmonic nonnegative matrix factorization for poly-\nphonic pitch transcription. In IEEE International Con-\nference on Acoustics, Speech and Signal Processing,\n2008. ICASSP 2008 , pages 109–112, 2008.\n[18] C. Yeh. Multiple fundamental frequency estimation of\npolyphonic recordings . PhD thesis, Ph. D. dissertation,\nUniversit Pierre et Marie Curie, Paris, Jun, 2008.\n[19] R. Zhou and J.D. Reiss. A Real-Time Frame-\nBased Multiple Pitch Estimaiton Method Using\nThe Resonator Time-Frequency Image, Avaliable at\nhttp://www.music-ir.org/mirex/2008/abs/F0 zhou.pdf.\n320"
    },
    {
        "title": "Grouping Recorded Music by Structural Similarity.",
        "author": [
            "Juan Pablo Bello"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414876",
        "url": "https://doi.org/10.5281/zenodo.1414876",
        "ee": "https://zenodo.org/records/1414876/files/Bello09.pdf",
        "abstract": "This paper introduces a method for the organization of recorded music according to structural similarity. It uses the Normalized Compression Distance (NCD) to measure the pairwise similarity between songs, represented using beat-synchronous self-similarity matrices. The approach is evaluated on its ability to cluster a collection into groups of performances of the same musical work. Tests are aimed at finding the combination of system parameters that improve clustering, and at highlighting the benefits and shortcomings of the proposed method. Results show that structural similarities can be well characterized by this approach, given consistency in beat tracking and overall song structure.",
        "zenodo_id": 1414876,
        "dblp_key": "conf/ismir/Bello09",
        "keywords": [
            "organization",
            "recorded music",
            "structural similarity",
            "Normalized Compression Distance (NCD)",
            "pairwise similarity",
            "songs",
            "beat-synchronous self-similarity matrices",
            "clustering",
            "musical work",
            "system parameters"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nGROUPING RECORDED MUSIC BY STRUCTURAL SIMILARITY\nJuan Pablo Bello\nMusic and Audio Research Lab (MARL), New York University\njpbello@nyu.edu\nABSTRACT\nThis paper introduces a method for the organization of re-\ncorded music according to structural similarity. It uses\nthe Normalized Compression Distance (NCD) to measure\nthe pairwise similarity between songs, represented using\nbeat-synchronous self-similarity matrices. The approach is\nevaluated on its ability to cluster a collection into groups of\nperformances of the same musical work. Tests are aimed at\nﬁnding the combination of system parameters that improve\nclustering, and at highlighting the beneﬁts and shortcom-\nings of the proposed method. Results show that structural\nsimilarities can be well characterized by this approach, gi-\nven consistency in beat tracking and overall song structure.\n1. INTRODUCTION\nCharacterizing the temporal structure of music has been\none of the main goals of the MIR community, with ex-\nample applications including thumbnailing, long-term seg-\nmentation and synchronization between multiple record-\nings [1, 2]. Despite this focus, however, there has been lit-\ntle in terms of using structure as the main driver of audio-\nbased retrieval and organization engines.\nThis paper proposes and evaluates a methodology for\nthe characterization of structural similarity between musi-\ncal recordings. The approach models similarity in terms\nof the information distance between music signals repre-\nsented using self-similarity matrices. These matrices are\nwell-known for their ability to characterize recurring pat-\nterns in structured data, and are thus widely used in MIR\nfor the analysis of musical form. However, in retrieval ap-\nplications they are mostly used as intermediate representa-\ntions from which a ﬁnal representation (e.g. beat spectrum,\nsegment labels) is derived. In this paper we argue that self-\nsimilarity matrices can be used directly in the computa-\ntional modeling of texture-, tempo- and key-invariant rela-\ntionships between songs in a collection. Our approach is\nmainly inspired by the work in [3], which uses the same\nprinciple to compare the structure of protein sequences.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.1.1 Background\nThe use of structure for audio-based MIR was ﬁrst pro-\nposed in [4]. This approach is based on the idea that long-\nterm structure can be characterized by patterns of dynamic\nvariation in the signal. In this approach, song similarity is\nmeasured as the cost of DP-based pairwise alignment be-\ntween sequences of local energy or magnitude spectral co-\nefﬁcients. Experimental results, albeit preliminary, show\nthe potential of this idea for retrieval.\nA similar concept is explored in [5], and more exten-\nsively in [6], where variations of spectral content are quan-\ntized into a symbolic sequence, obtained via vector quanti-\nzation or HMMs. In these works, pairwise song similarity\nis measured using the edit distance or, more efﬁciently, lo-\ncality sensitive hashing [6].\nThe mentioned sequences are not only able to represent\nthe texture and harmony of musical pieces, but also struc-\ntural patterns, from motifs and phrases to global form. Mu-\nsical sequences sharing style, origin or functionality will\nbe likely to show structural similarity, despite differences\nin actual sequence content. Hence, a change of key does\nnot preclude listeners from identifying a 12-bar blues, and\nthe relationship between different variations and renditions\nof a work remain close, despite changes of instrumenta-\ntion, ornamentation, tempo, dynamics and recording con-\nditions. Unfortunately, all representations discussed above\nare sensitive to one or more of these variables. As a result,\ntheir success at characterizing music similarity depends on\ntheir ability to marginalize those changes. Examples in-\nclude the use of modiﬁed distance metrics and suboptimal\nfeature transposition methods [2, 5].\nStructure comparison has been extensively studied in\nother ﬁelds, such as bioinformatics. For protein sequences,\nfor example, structures are usually characterized using con-\ntact maps , which are, simply put, binary self-similarity\nmatrices where a 1characterizes a contact (i.e. similar-\nity higher than a certain threshold) and a 0the lack of it.\nThe problem of comparing protein topologies using con-\ntact maps is known as maximum contact map overlap , with\nmany proposed solutions in the literature. In this paper we\nconcentrate on the one proposed in [3], which uses an ap-\nproximation of the information distance between two con-\ntact maps known as the normalized compression distance\n(NCD), to be discussed in more detail in section 2.2.\nIn music, the NCD has been used on raw MIDI data\nfor clustering and classiﬁcation based on genre, style and\nmelody [7, 8]. More recently, it has been used on audio\ndata for sound and music classiﬁcation [9] and, with lim-\n531Oral Session 6: Similarity\nFigure 1 . (a) Self similarity matrix of the ﬁrst 248 bars of a performance of Beethoven 5thSymphony; MDS projection of\na quarter (b), half (c) and full matrix (d) to 3 dimensions; (e) comparison of two different performances.\nited success, in cover-song identiﬁcation [10]. To the best\nof our knowledge this paper proposes the ﬁrst use of NCD\nto characterize structural similarity between music audio\nrecordings.\n1.2 Example\nFigure 1(a) shows a self-similarity matrix of the ﬁrst 248\nbars of the ﬁrst movement of Beethoven’s 5thsymphony.\nThe recording is of a 2006 performance by the Russian\nNational Orchestra conducted by Mikhail Pletnev. Figures\n1(b-d) are the result of taking the distances in the matrix\nand projecting them into a 3-dimensional space using clas-\nsical multidimensional scaling (MDS). The ﬁgures show\nthe trajectory of the piece at a quarter, half and full seg-\nment length, respectively. Figures 1(b) and (c) depict the\nfamous opening section of this symphony as a loop, while\nﬁgure 1(d) shows the recapitulation as simply another, ap-\nproximate instance of the same loop. This example clearly\nshows how self-similarity matrices are able to character-\nize primary (the trajectory itself) and, at least, secondary\n(local motifs such as the loop) structure in music. Fig-\nure 1(e) shows the full segment trajectory described above\n(in black), and a new trajectory, corresponding to a 1963\nrecording by the Berlin Philharmonic conducted by Her-\nbert von Karajan (in red). The goal of our approach is to\nquantify the (dis)similarity of these representations, and to\nuse the results to group related music together.\n2. APPROACH\nThe proposed approach consists of three main parts: (a)\nrepresentation, where a self-similarity matrix is generated\nfrom the analysis of the audio signal; (b) similarity, where\nthe pairwise distance between the representations is com-\nputed using the NCD; and (c) clustering, where the matrix\nof NCDs is used for the grouping of songs. The details are\nexplained in the following.\n2.1 Representation\nIn our implementation we use a beat-synchronous feature\nsetF, composed of either MFCC or chroma features. The\nﬁrst 20 MFCCs are calculated using a 36-band ﬁlterbank,\nframe size of 23.22ms and 50% overlap. The chroma fea-\ntures are computed via the constant-Q transform using a\nminimum frequency of 73.42 Hz, 36 bins per octave and\na 3-octave span, on a signal downsampled to fs= 5512.5Hz. The resulting features are tuned and their dimension-\nality reduced to 12 with a weighted sum across each 3-bin\npitch class neighborhood. For beat tracking we use the al-\ngorithm in [11], and average the extracted features between\nconsecutive beats. Beat tracking is used to reduce the size\nof the self-similarity matrix and to minimize the effect of\ntempo-variations on the representation.\nThe feature set is smoothed using zero-phase forward-\nbackward ﬁltering with a second order Butterworth ﬁlter.\nFilter cutoff is at 1/ 128thof the feature rate. Finally, the\nfeatures are standardized (separately for each song).\nThe computation of self-similarity matrices has been\ndiscussed extensively elsewhere in the literature and will\nnot be discussed in any detail here. Sufﬁces to say that for\nour tests we use both the euclidean and cosine distances.\nOnce computed, matrices are normalized (per song) to the\n[-1,1] range, their upper triangular part extracted, and the\nvalues uniformly quantized and encoded into Bbits. In\nour experiments Bassumes the values 2, 3 and 4. It is\nworth noting that we have favored the notion of “fuzzy”\nrather than binary self-similarity, as it is not clear what an\nadequate deﬁnition of contact may be in the context of this\nwork. For the same reason we have favored the use of uni-\nform quantization over other possible partitions of the sim-\nilarity range.\n2.2 Similarity\nWe measure similarity using the normalized compression\ndistance (NCD), which will be brieﬂy introduced here (For\na comprehensive discussion the reader is referred to [7]).\nIt can be shown that the information distance between\ntwo objectso1ando2, up to a logarithmic additive term, is\nequivalent to:\nID(o1,o2) =max{K(o1|o2),K(o2|o1)} (1)\nwhereK(.)denotes the Kolmogorov complexity. The con-\nditional complexity K(o1|o2)measures the resources need-\ned by a universal machine to specify o1giveno2.\nThe information distance in Eq. 1 suffers from not con-\nsidering the size of the input objects, and from the non-\ncomputability of K(.). To solve the ﬁrst problem, a nor-\nmalized information distance can be deﬁned as:\nNID (o1,o2) =max{K(o1|o2),K(o2|o1)}\nmax{K(o1),K(o2)}(2)\nTo solve the second problem, we can approximate K(.)us-\ningC(.), the size in bytes of an object when compressed\n53210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nusing a standard compression algorithm. Using this prin-\nciple, it can be shown that equation 2 can be approximated\nby the normalized compression distance:\nNCD (o1,o2) =C(o1o2)−min{C(o1),C(o2)}\nmax{C(o1),C(o2)}(3)\nwhereC(o1o2)is obtained by compressing the concatena-\ntion of objects o1ando2[7]. For our implementation the\nobjects are the encoded self-similarity matrices for each\nsong. We use the NCD implementation in the CompLearn\ntoolkit1with the bzip2 and PPMd compression algorithms.\n2.3 Clustering\nWe use an algorithm from Matlab’s statistics toolbox that\nbuilds a hierarchical cluster tree using the complete linkage\nmethod [12]. The clusters are deﬁned by ﬁnding the small-\nest height in the tree at which a cut across all branches will\nleaveMaxClust or less clusters. The output of the pro-\ncess is a vector containing the cluster number per item in\nthe test set.\n3. EXPERIMENTAL SET-UP\n3.1 Test Data\nWe use two datasets in our experiments. The ﬁrst set,\nwhich we call P56, consists of 56 recordings of piano mu-\nsic, including excerpts of 8 works by 3 composers (Beetho-\nven, Chopin and Mozart), played by 25 famous pianists\nbetween 1946 and 1998. It was collected as part of the\ncomputational study of expressive music performance dis-\ncussed in [13]. Each work has, at least, 3 associated rendi-\ntions and at most 13, with audio ﬁle lengths in the range of\n1 to 8 minutes.\nThe second set (S67, collected by the authors) includes\n67 recordings of symphonic music, including one move-\nment for each of 11 works by 7 composers (Beethoven,\nBerlioz, Brahms, Mahler, Mendelssohn, Mozart and Tchai-\nkovsky). The set includes instances from 56 different re-\ncording sessions scattered between 1948 and 2008, featur-\ning 34 conductors. Each work has 6 associated renditions,\nwith the sole exception of the 3rd movement of Brahm’s\nSymphony No. 1 in C minor, for which 7 performances are\navailable. The duration of the recorded movements range\nfrom 3 to 10 minutes.\nClassical music is used as, apart from the odd repeti-\ntion of a motif or section, the structure of renditions can\nbe expected to be the same. The two sets are composed\nof recordings using similar instrumentation (piano, orches-\ntra), to emphasize the difference with timbre-base simi-\nlarity approaches. Both sets, however, present signiﬁcant\nvariations in recording condition and interpretation (no-\ntably in dynamics and tempo). All ﬁles are 128 kb/s MP3s\nwith sampling frequency of 44.1kHz.\n1http://www.complearn.org3.2 Methodology\nClustering methods are highly sensitive to both the num-\nber and relative size of partitions in a dataset. To account\nfor variations of those factors and avoid overﬁtting, every\ntest is performed Itimes, each using a random sample of\nsizeN < M , whereMis the number of items in the\ndataset. For every test, we report the mean accuracy of\nclusters across the Isubsets, measured as follows.\nGiven a partition of the dataset into Rgroups,Q=\n{q1,...,q R}, produced by the clustering algorithm, and a\ntarget partition, T={t1,...,t P}, we can validate Qusing\nthe Hubert-Arabie Adjusted Rand (AR) index as:\nAR=/parenleftbigN\n2/parenrightbig\n(a+d)−[(a+b)(a+c) + (c+d)(b+d)]\n/parenleftbigN\n2/parenrightbig2−[(a+b)(a+c) + (c+d)(b+d)]\n(4)\nwhere/parenleftbigN\n2/parenrightbig\nis the total number of object pairs in our dataset.\nAR measures the correspondence between QandT, as a\nfunction of the number of the following types of pairs: ( a)\npairs with objects in the same group both in QandT; (b)\nobjects in the same group in Qbut not inT; (c) objects\nin the same group in Tbut not inQ; and (d) objects in\ndifferent groups in both QandT. The AR index accounts\nfor chance assignments and does not require arbitrary as-\nsignment of cluster labels not P=R, as might be the case\nwhen using classiﬁcation accuracy to validate clustering.\nReaders unfamiliar with the AR index might ﬁnd the fol-\nlowing guidelines useful: AR = 1 means perfect clustering,\nwhile values above 0.9, 0.8 and 0.65 reﬂect, respectively,\nexcellent, good and moderate cluster recovery. Random\npartitions of the dataset result on AR →0(can also as-\nsume small negative values). For a detailed discussion of\nthe properties and beneﬁts of the AR index see [14].\n4. RESULTS AND DISCUSSIONS\nThe main goal of our experiments is to test the capacity of\nthe proposed approach in characterizing structural similar-\nity. As similarity is an elusive concept which is not easily\nquantiﬁed, we test an approximate scenario: the task of\nclustering a music collection into groups of renditions of\nthe same work. Thus, for example, a partition Qof S67,\ngenerated using the approach in section 2 with parame-\ntersθ, is validated using AR and a target partition Tof\n11 groups, where each group contains the 6 or 7 renditions\nof one of the works in the collection.\nSpeciﬁcally, our experiments seek to: (1) ﬁnd the pa-\nrameterization θthat maximizes AR, (2) assess the impact\nof the used clustering methodology, and (3) highlight the\nstrengths and shortcomings of our approach.\n4.1 Parameterization\nIn our experiments θ={F,d,B,C,MaxClust }, where\nFis the feature set (MFCC or chroma), dthe distance met-\nric used to compute the self-similarity matrix (euclidean or\ncosine),Bthe number of bits used to quantize the matrix\n(2, 3 or 4),Cthe compression method used for the com-\nputation of the NCD (bzip2 or PPMd), and MaxClust the\n533Oral Session 6: Similarity\nFigure 2 . Comparison of mean AR results for all F,dcom-\nbinations on sets P56 (left) and S67 (right).\nFigure 3 . Comparison of mean AR results for B=\n{2,3,4}on sets P56 (left) and S67 (right).\nFigure 4 . Comparison of mean AR results for C=\n{bzip2,PPMd}on sets P56 (left) and S67 (right).\nmaximum number of clusters to be retrieved from the tree\n(between 6 and 35).\nAll possible combinations of θare testedI= 50 times2,\nusing random samples of size N= 0.75×M(42 for P56,\n50 for S67). In all tests, both collections are tested inde-\npendently.\nFigure 2 shows results for all F,dcombinations for C=\nbzip2andB= 3. As with most ﬁgures in this section, it\nseparately shows AR values for P56 (left) and S67 (right),\nacross the range of MaxClust values. For both datasets,\nchroma features outperform MFCCs, clearly for P56 and\nslightly for S67. This is consistent with the notion of har-\nmonic content as a reliable indicator of structure in music,\nas has been repeatedly found in the segmentation litera-\nture [1,2]. The better performance of MFCCs in S67 com-\npared to P56 is to be expected, as within-song timbre dif-\n2We tested I={10,20,50,100,200,500,1000}and found varia-\ntions of mean AR to be minimal for I≥50.ferences and dynamic changes are more pronounced in or-\nchestral than in piano music. For chroma features, the use\nof euclidean or cosine distances in the computation of the\nself-similarity matrix makes little difference. For MFCCs,\nhowever, the euclidean distance results in signiﬁcantly bet-\nter performance, indicating that dynamics are as important\nas timbre changes in deﬁning the structure of a piece.\nFigure 3 illustrates the importance of the number of\nbitsBused in the encoding and quantization of the self-\nsimilarity matrix, for F=chroma ,d=euclidean and\nC=bzip2. Apart from B= 2 giving the best results\nfor S67, no clear trend is visible in these plots (at least not\ncommon to both sets). This hints at process independence\nfrom the choice of B. The good performance of B= 2,\nhowever, opens the door for a binary deﬁnition of contacts\nin music, although more extensive testing is necessary to\ndeﬁne an appropriate threshold.\nFinally, ﬁgure 4 compares two compression methods for\nthe computation of NCD. In these plots, F=chroma ,\nd=euclidean andB= 3. In all cases bzip2outperforms\nPPMd , which is unfortunate as the latter is much faster\nthan the former. This result seems to contradict ﬁndings in\nthe literature where the PPM family of compression meth-\nods usually works best for the NCD computation [7].\nFigure 5 . Variation of mean AR according to random sam-\nple sizeN(P56 in black, S67 in gray).\n4.2 Clustering methods\nOn a separate experiment, we tested our system against\nvariations of the random sample size Nfor both collec-\ntions.Nvalues ranged from 30 to 52 for P56, and 64 for\nS67. We used F=chroma ,d=euclidean ,B= 3 and\nC=bzip2. Figure 5 shows results for P56 (in black) and\nS64 (in gray, skewed towards the right), across a range of\nMaxClust values ranging from N/2−20toN/2 + 10 .\nEach curve corresponds to a value of N. Variations of peak\nAR acrossNappear to be uniformly distributed in the de-\npicted range for each test set. Their location within this\nrange does not follow any obvious trend. For example, for\nP56, the minimum peak corresponds to the N= 32 curve,\nwhile the maximum peak is for N= 30 (closely followed\n53410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nbyN= 48 ). All other peaks are randomly located in be-\ntween.\nNotably, the location of peaks appears to be a function\nofN, with most peaks in (N/2−5)±3for P56 and in\n(N/2 + 3)±2for S67. The difference between the sets,\nhowever, also indicates that the size of the collection M,\nthe number of groups within that collection and the size of\nthose groups have a hand in the results. While NandM\nare always known, it is unreasonable to expect the number\nand size of groups to be known, making the choice of value\nfor the critical MaxClust parameter a complex one. Our\ninability to deﬁne MaxClust with prior information is a\nmajor shortcoming of the proposed approach.\nAs an alternative we have tested a different clustering\nalgorithm, which operates by merging clusters whose sep-\naration, measured in their connecting node, is less than a\npre-speciﬁed Cutoff value, ranging between 0 and 1. No-\ntably, this method does not require any prior information\nabout cluster numbers. Additionally, we test building the\nhierarchical cluster tree using single, average and weighted\nlinkage in addition to the complete linkage method used in\nthe rest of this paper [12]. Figure 6 shows the results of\nthese tests using F=chroma ,d=euclidean ,B= 2\nandC=bzip2. The AR = 0.63 result for weighted linkage\nand Cutoff = 0.85 in S67 is the highest obtained in our ex-\nperiments, a signiﬁcant increase on our previous best (vis-\nible in the “complete” curve of the same graph). It clearly\nshows that gains can be made by improving our cluster-\ning stage. However, this result is not indicative of a gen-\neral trend, as illustrated by the low results obtained for the\nsame method in the P56 dataset. An in-depth exploration\nof the space of clustering methods and their parameteriza-\ntions will be the focus of future work.\nFigure 6 . Test of cutoff clustering with 4 linkage methods.\n4.3 An example tree\nFigure 7 is generated using yet another linkage algorithm\non the full S67 dataset, the quartet method described in\n[7], usingF=chroma ,d=euclidean ,B= 3 and\nC=bzip2. Clustering on this tree using MaxClust = 36\nresults onAR= 0.55, which makes this graph represen-\ntative of system performance using the best parameteriza-\ntion.\nThe tree branches out into 10 clusters, each correspond-\ning to a work in the collection. Four of those clusters group\nall renditions of a given work. Figure 7(a) shows a detailof the tree exemplifying one such cluster, corresponding to\nthe 7 renditions of the third movement of Brahm’s Sym-\nphony No. 1 in C minor. Two clusters group 5 out of 6\nperformances, for example those for the third movement\nof Mozart’s Symphony No. 4 in G minor k550 depicted\nin Figure 7(c). One cluster, for the second movement of\nMahler’s Symphony No. 1 in D major “Titan”, groups 4\nout of 6 performances as shown in Figure 7(b). The three\nremaining clusters group only 3 or 2 performances out of 6.\nOnly one work results in no clusters of any kind. In total,\n47 out of 67 recordings are correctly assigned to a group.\nUngrouped recordings are located in the stem of the tree,\nwhich has been gray-shaded in the graph.\nFigures 7(b) and (c) also help illustrate the effect of beat\ntracking accuracy on the proposed approach. The number\nof detected beats in the missing performance of Mozart’s\nk550, visible in the stem of the tree in Fig. 7(b), is ap-\nproximately twice as many as those detected in all other\nperformances of the same piece. Octave errors act as ﬁl-\nters on the feature set, which can result on a signiﬁcant loss\nof detail in the corresponding self-similarity matrix and, as\nthe tree shows, a poor characterization of structural simi-\nlarity between the recordings. This is an important draw-\nback of our approach as octave errors are common in beat\ntracking. Another example of the same problem are the\ntwo missing recordings in Mahler’s Symphony 1 cluster in\nFig. 7(b), which are located in the lower end of the stem of\nthe tree. An informal analysis of the results shows that a\ngood portion of overall clustering errors are associated to\ninconsistencies in beat tracking. It is worth noting that “in-\nconsistency” is the right word in this case, as what is really\nimportant is not that beats are correctly tracked, but that\ntheir relation to the actual tempo of the piece is the same\nfor all performances.\nAn additional observation relates to the six performances\nof the fourth movement of Berlioz’s “Symphonie Fantas-\ntique”. The score includes a repetition of the ﬁrst 77 bars\nof this movement before entering its second half, roughly\ndescribing an AAB structure. Half of the performances\nin our dataset, however, ignore that repetition resulting on\na shorter AB structure. Correspondingly, the cluster in\nthe tree related to this piece groups only the latter, while\nthe other three performances appear close together in the\nlower end of the tree. While in theory the common part\nof the structure should be enough to identify the similarity\nbetween all six recordings, in practice this is clearly not\nthe case. This sensitivity to common structural changes,\ne.g. repetitions, raises questions about the potential use of\nNCD-based similarity in the modeling of the relationships\nthat exist amongst variations, covers, remixes and other\nderivatives of a given work. Further research is now be-\ning conducted to fully explore this issue.\n5. CONCLUSIONS AND FUTURE WORK\nThis paper presents a novel approach for the organization\nof recorded music according to structural similarity. It\nuses the Normalized Compression Distance (NCD) on self-\nsimilarity matrices extracted from audio signals, using stan-\n535Oral Session 6: Similarity\nAbba74Tch_op740.998\n1.000Abba82Mah_sym 11.003\n1.004Abba85Men_op900.9460.949\nAshk95Men_op90\n0.937Berg00Bra_op680.972\n0.986Berg00Bra_op98\n0.9570.968\nBern63Ber_op14\n0.984 0.993Bern83Bra_op680.963\n0.984Bern83Bra_op980.976\nBoul00Mah_sym 41.003\n1.001\nBoul97Ber_op14 1.003Boul99Mah_sym 10.953\n0.962\nBoyd04Bee_op67\n1.0011.008Brue85Moz_k5500.986\n0.999\nBrue90Men_op90\n1.000Brue91Moz_k5500.920\n0.957Chri06Mah_sym 4\n1.003Gard07Bra_op6 80.9680.966\nGard88Moz_k38 5\n0.9710.984\nGerg03Ber_op14\n0.979\nGerg95Tch_op740.997\n1.009Giul81Tch_op741.005Hait03Bra_op68Hait92Bra_op98\n0.9610.966\nHarn80Moz_k385\n0.964Harn91Bee_op68 0.9470.946\nJoch96Bra_op98\nLepp77Men_op90\nLevi06Mah_sym 41.008Levi88Bra_op680.984\nLevi88Moz_k385\n0.964Levi99Bra_op68\nMack88Men_op9 00.9560.977\nMarr86Moz_k385\n0.9620.957\nMarr89Moz_k5500.950\n0.970\nMink06Moz_k550Munc54Ber_op14Norr04Mah_sym 1\nNorr04Men_op90\nNorr88Bee_op6 7\n1.007Norr88Bee_op6 80.954\n0.966\nNorr89Ber_op14Norr91Bra_op68\nPlet06Bee_op671.000\n1.000Plet06Bee_op680.961\nPlet91Tch_op741.010\nRatt08Ber_op14Sawa90Bra_op98\nSege93Mah_sym 11.003Solt64Mah_sym 1\n0.974\nSolt77Tch_op74\n1.0081.001\nSolt83Mah_sym 41.004Szel02Moz_k385\nSzel02Moz_k550\nVanI01Moz_k550VonD94Mah_sym 4\nVonK63Bee_op6 7VonK63Bee_op6 8\nVonK64Tch_op74\n1.003Walt48Mah_sym 1\nWalt59Moz_k38 5Walt95Bra_op9 80.979\nWeil04Bee_op67Weil04Bee_op68\nZand01Mah_sym 4\nZinm97Bee_op67Zinm97Bee_op68\nBerg00Bra_op680.972\n0.986Bern83Bra_op680.963\n0.984Gard07Bra_op680.968\nHait03Bra_op68Levi88Bra_op680.984\nLevi99Bra_op68Norr91Bra_op68\n         06Bee_op6868\nBrue91Moz_k5500.920\n0.957Marr89Moz_k5500.950\n0.970\nMink06Moz_k550Szel02Moz_k550\nVanI01Moz_k550A\nB\nC\nA B CBoul99Mah_sym10.953\n0.962Brue85Moz_k5500.986\n0.999Norr04Mah_sym1\nSolt64Mah_sym1\n0.974Walt48Mah_sym1Figure 7 . Uprooted binary tree of S67 using the quartet method. Details show a perfect cluster (A) and two partial clusters\n(B and C)\ndard features and distance metrics. The approach is eval-\nuated on its ability to facilitate the clustering of different\nperformances of the same piece together. Experimental re-\nsults on piano and orchestral music datasets show that the\napproach is able to successfully group the majority of per-\nformances in a collection, resulting on average AR values\nin the 0.5-0.6 range. Our tests show that best results are ob-\ntained for self-similarity matrices computed using chroma\nfeatures and the euclidean distance, and encoded using 2-3\nbits. They also show that the NCD works best when using\nthebzip2compression algorithm. Preliminary results also\nindicate that further gains can be made by improving the\nclustering stage.\nOn the downside, the approach has shown sensitivity\nto octave errors in beat tracking and, predictably, to struc-\ntural changes, which limit the potential application of the\ncurrent implementation to the retrieval and organization of\nother types of musical variations. To address these issues,\nfuture work will concentrate on two main areas. First, the\nimprovement of the self-similarity representation, along\nthe lines of work in [2], to include transposition invariance,\npath following and the merging of matrices computed at\n1/2, 1 and 2 times the tracked tempo. Second, we will ex-\nplore alternatives to the use of NCD for the maximum con-\ntact map overlap problem. We plan to explore solutions\nbased on the branch and cut approach (e.g. [15]) and adapt\nthem to the speciﬁcities of music data.\n6. ACKNOWLEDGEMENTS\nThe author would like to thank Gerhard Widmer and Werner\nGoebl for the P56 dataset, and Dan Ellis and the Com-\npLearn team for free distribution of their code libraries.\nThis material is based upon work supported by the NSF\n(grant IIS-0844654) and by the IMLS (grant LG-06-08-\n0073-08).\n7. REFERENCES\n[1] M. A. Bartsch and G. H. Wakeﬁeld. To catch a chorus:\nUsing chroma-based representations for audio thumb-\nnailing. In WASPAA-01, NY, USA , pages 15–18, 2001.\n[2] M. M ¨uller. Information Retrieval for Music and Mo-\ntion. Springer-Verlag New York, Inc., Secaucus, NJ,\nUSA, 2007.[3] N. Krasnogor and D. A. Pelta. Measuring the similarity\nof protein structures by means of the universal similar-\nity metric. Bioinformatics , 20(7):1015–1021, 2004.\n[4] J. Foote. Arthur: Retrieving orchestral music by long-\nterm structure. In ISMIR , 2000.\n[5] J.-J. Aucouturier and M. Sandler. Using long-term\nstructure to retrieve music: Representation and match-\ning. In ISMIR 2001, Bloomington, Indiana, USA , 2001.\n[6] M. Casey and M. Slaney. Song intersection by approx-\nimate nearest neighbour retrieval. In ISMIR-06, Victo-\nria, Canada , 2006.\n[7] R. Cilibrasi and P. M. B. Vit ´anyi. Clustering by com-\npression. IEEE Transactions on Information Theory ,\n51(4):1523–1545, 2005.\n[8] Ming Li and Ronan Sleep. Genre classiﬁcation via an\nLZ78 string kernel. In ISMIR-05, London, UK, , 2005.\n[9] M. Hel ´en and T. Virtanen. A similarity measure for au-\ndio query by example based on perceptual coding and\ncompression. In DAFx-07, Bordeaux, France , 2007.\n[10] T. Ahonen and K. Lemstr ¨om. Identifying cover songs\nusing normalized compression distance. In MML’08,\nHelsinki, Finland , 2008.\n[11] D. Ellis. Beat Tracking by Dynamic Programming.\nJournal of New Music Research , 36(1):51–60, March\n2007.\n[12] R. Xu and D. Wunsch. Survey of clustering algorithms.\nNeural Networks, IEEE Transactions on , 16(3):645–\n678, 2005.\n[13] G. Widmer, S. Dixon, W. Goebl, E. Pampalk, and\nA. Tobudic. In search of the Horowitz factor. AI Mag. ,\n24(3):111–130, 2003.\n[14] D. Steinley. Properties of the Hubert-Arabie adjusted\nRand index. Psychological methods , 9(3):386–396,\nSeptember 2004.\n[15] W. Xie. and N. V . Sahinidis. A branch-and-reduce al-\ngorithm for the contact map overlap problem. Research\nin Computational Biology (RECOMB 2006), Lecture\nNotes in Bioinformatics , 3909:516–529, 2006.\n536"
    },
    {
        "title": "Pitched Instrument Onset Detection based on Auditory Spectra.",
        "author": [
            "Emmanouil Benetos",
            "Andre Holzapfel",
            "Yannis Stylianou"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416174",
        "url": "https://doi.org/10.5281/zenodo.1416174",
        "ee": "https://zenodo.org/records/1416174/files/BenetosHS09.pdf",
        "abstract": "In this paper, a novel method for onset detection of music signals using auditory spectra is proposed. The auditory spectrogram provides a time-frequency representation that employs a sound processing model resembling the human auditory system. Recent work on onset detection employs DFT-based features, such as the spectral flux and group delay function. The spectral flux and group delay are introduced in the auditory framework and an onset detection algorithm is proposed. Experiments are conducted on a dataset covering 11 pitched instrument types, consisting of 1829 onsets in total. Results indicate the superiority of the auditory representations over the DFT-based ones, with the auditory spectral flux exhibiting an onset detection improvement by 2% in terms of F-measure when compared to the DFT-based feature.",
        "zenodo_id": 1416174,
        "dblp_key": "conf/ismir/BenetosHS09",
        "keywords": [
            "onset detection",
            "music signals",
            "auditory spectra",
            "sound processing model",
            "human auditory system",
            "spectral flux",
            "group delay function",
            "pitched instrument types",
            "1829 onsets",
            "F-measure"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nPITCHED INSTRUMENTONSET DETECTION BASED ONAUDITORY\nSPECTRA\nEmmanouilBenetos,Andr ´eHolzapfel, andYannisStylianou\nInstitute ofComputer Science,FORTH,Greece,\nandMultimedia Informatics Lab,Computer ScienceDepartmen t, University ofCrete, Greece\n{benetos,hannover,yannis }@csd.uoc.gr\nABSTRACT\nIn this paper, a novel method for onset detection of music\nsignals using auditory spectra is proposed. The auditory\nspectrogram provides a time-frequency representation tha t\nemploys a sound processing model resembling the human\nauditory system. Recent work on onset detection employs\nDFT-based features, such as the spectral ﬂux and group\ndelay function. The spectral ﬂux and group delay are in-\ntroducedintheauditoryframeworkandanonsetdetection\nalgorithm is proposed. Experiments are conducted on a\ndatasetcovering11pitchedinstrumenttypes,consistingo f\n1829 onsets in total. Results indicate the superiority of\ntheauditoryrepresentationsovertheDFT-basedones,with\ntheauditoryspectralﬂuxexhibitinganonsetdetectionim-\nprovement by 2% in terms of F-measure when compared\ntothe DFT-based feature.\n1. INTRODUCTION\nThe detection of the starting time of each musical note\nplays an important role in the analysis of music signals.\nThis process is referred to as musical instrument onset de-\ntection and it is an essential step for music transcription\napplications,aswellasformusicsignalcompression,beat\ntracking, and music information retrieval. The goal of an\nonset detection system is the accurate estimation of note\nonset times, regardless of the instrument type or perfor-\nmance style. Several approaches for pitched instrument\nonset detection have been proposed in the literature, how-\never they are mostly limited to a small number of instru-\nment classes.\nIn [1], an onset detection system combining both en-\nergy and phase information was proposed. The employed\ndataset contained pitched nonpercussive, pitched percus-\nsive,nonpitchedpercussive,andcomplexsounds. Reported\nresults indicated an improvement over energy and phase-\nbased approaches. An improved version of the system in\n[1] was proposed in [4], tested on the same dataset. In [3],\na system for onset detection employing a constant-Q pitch\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbear this noticeand thefull citation ontheﬁrst page.\nc/circlecopyrt2009International Society forMusic InformationRetrieval .detectorwasproposed,testedonthepitchednonpercussive\nsoundsalsoemployedin[1]. Itisalsosuggestedin[3]that\na detector based on a computational auditory model might\nimprove onset detection performance. Gainza et al. em-\nployedFIRcombﬁltersonaframebyframebasiscombin-\ning the inharmonicity properties with the energy increases\nofthesignalonset[5]. Resultsreportanimprovementoffer\nenergy-based and phase-based approaches. Finally in [6],\nthe group delay function was proposed for onset detection\ninabeattrackingapplication. Multibandanalysiswasper-\nformed on two datasets, the ﬁrst from the MIREX 2006\nbeat tracking task and the second containing samples of\ntraditional Cretan music.\nIn this paper, a novel approach for onset detection is\nproposed by employing auditory spectrograms instead of\nDFT-derived spectrograms for the computation of onsets\ndetectionfeatures. Theauditoryspectra,basedonthemode l\npresented in [11], are designed to mimic the functions of\nthe human auditory system. In the auditory domain, the\ngroup delay and spectral ﬂux features are introduced, and\nanonsetdetectionsystemisproposed. Comparativeexper-\niments on onset detection were performed using the same\nfeatures in the DFT domain. The dataset used for exper-\nimentation contains a wide variety of pitched instrument\ntypes, not limited to western instruments, containing 1829\nonsets in total. Results indicate that the auditory feature s\noutperform DFT-based features for onset detection, with\ntheauditoryspectralﬂuxreachinganF-measureof75.9%.\nThe outline of the paper is as follows. Section 2 is de-\nvoted to the DFT-based features and system for onset de-\ntection. In Section 3, the auditory model and features are\npresented,alongwiththeproposedonsetdetectionsystem.\nTheemployeddataset,themethodsusedforevaluationand\nthe experimental results are discussed in Section 4. Con-\nclusions are drawn and future directions are indicated in\nSection 5.\n2. DFT-BASED ONSET DETECTION\n2.1 Group Delay\nAs described in [6], phase information can be used for on-\nset detection by considering the group delay τ(ω), which\nfor a given signal x[n]with a phase spectrum Φ(ω)is de-\nﬁned as the derivative of phase over frequency:\nτ(ω) =−dφ(ω)\ndω(1)\n105Poster Session 1\nThe average of the group delay is determined by the dis-\ntance between the center of the analysis window and the\nposition of an impulse within the window, even when the\nimpulse has been ﬁltered by a causal and stable ﬁlter. As\nthe onset of a musical instrument might be modelled by\nan impulse sent into a causal and stable system, in [6] the\naverage group delay is used as an onset detection func-\ntion: using a large overlap, an analysis window is shifted\nover the signal and for each window position the average\ngroup delay is computed. The obtained sequence of aver-\nage group delays is referred to as phase slope function. In\nFigure 1, an example of a phase slope function is depicted\nby the dashed line which has positive zero crossings at the\nposition of impulses in the signal. In order to avoid error\nproblemswhenunwrappingphaseinthegroupdelaycom-\nputation, the slope of the phase function can be computed\nas [10]:\nτ(ω) =XR(ω)YR(ω) +XI(ω)YI(ω)\n|X(ω)|2(2)\nwhere\nX(ω) = XR(ω) +jXI(ω)\nY(ω) = YR(ω) +jYI(ω)\naretheFourierTransformsof x[n]andnx[n],respectively.\nThephaseslopeisthencomputedasthenegativeoftheav-\nerageofthegroupdelayfunction. Inthispaper,theimple-\nmentationofthephaseslopeonsetdetectoraspresentedin\n[7] has been used, which includes a multiband processing\nof the complex DFT spectra and band-wise zero-crossing\nselection for increased accuracy. The resulting group de-\nlay onset detection signal, computed from the band-wise\nzero-crossing selection, contains peaks located at the tim e\ninstants of thedetected onsets.\n2.2 Spectral Flux\nSpectral ﬂux (SF) isbased on thedetection of sudden pos-\nitive energy changes in the signal which indicate attack\nparts of new notes. The accuracy of onset detection us-\ning SF and its computational simplicity were presented\nin[2,4]. SF iscomputed as:\nSF(k) =/summationdisplay\nωHW(|X(ω,k)| − |X(ω,k−1)|)(3)\nwhere HW(x) =x+|x|\n2is the half wave rectiﬁer function,\nandX(ω,k)istheSTFTofthesignalwith 5.6mshopsize\nand a window length hof46ms. For the experiments in\nthispaper,theL1-normSFisusedasshownin(3),sinceit\nwas shown in[4] that itoutperforms theL2-norm.\n2.3 DFT-based Onset Detection System\nOnsets are detected by selecting the zero crossings of the\nphase slope and the local maxima of the spectral ux de-\ntection signals. The onset detection method has been mo-\ntivated by the processing steps proposed in [1]: ﬁrst, the\ndetection signals are smoothed using a Hanning windowof length 51ms, which was found to be crucial for im-\nproving onset detection results. Afterwards, the signals\nare normalized using z-score. In [7], the application of\nanadaptivethresholdhasbeenshowntoimproveaccuracy\nfor SF, while it was found be impaired in case of PS. For\nthat reason, an adaptive threshold is applied to SF only. It\nis computed by applying a moving median lter of length\n97ms which is subtracted from the SF detection signals.\nFinally, a peak selection algorithm is performed in order\nto produce the detected onsets, by selecting peaks that are\nseparated by aminimum peak distance of 40ms.\n3. AUDITORY SPECTRUM-BASED ONSET\nDETECTION\nIn this Section the auditory model is presented, followed\nby the deﬁnition of the group delay function and spectral\nﬂux in the auditory spectrum domain. Finally, an onset\ndetection system using auditory spectra isproposed.\n3.1 Auditory Model\nThe auditory model was ﬁrst introduced in [13] and for-\nmalized in [11]. It is inspired by physiological, psychoa-\ncoustical and computational studies in the human primary\nauditory cortex. The model consists of two stages, a spec-\ntralestimationmodel(designedtomimicthecochleainthe\nauditorysystem)andspectralanalysismodel(whichmim-\nics the primary auditory cortex). The spectral estimation\nmodel produces theso-called auditory spectrogram.\nThe auditory spectrum produces a time-frequency rep-\nresentation of the signal on a logarithmically scaled fre-\nquency axis, referred as the tonotopic axis. The auditory\nspectrogramconsistsof128log-frequencybinsandcanbe\napproximated as:\nXA[n,l] = max( ∂lg(∂nx[n]∗nh[n,l]),0),(4)\nwhere x[n]isthe original signal and h[n,l]isaminimum-\nphaseseedbandpassﬁlterwhere h[n,l] =αh[αn,l0],with\nscaling factor α= 2l−l0andl= 1,... ,129. The convo-\nlutionof x[n]withh[n,l]isanapplicationofaconstant-Q\nﬁlter-bank wavelet transform. ∂istands for differentiation\noveri,andg(m) =1\n1+e−m−1\n2isasigmoid-likefunction,\nwhichisusedtomodelthehaircellresponseinthehuman\nauditory system. It should be noted that in (4) two oper-\nations are not mentioned for simplicity purposes, they are\nhowever employed for the auditory spectra computation.\nTheﬁrstconsistsofatemporalsmoothingoperationwhich\nﬁltersoutresponsesbeyond4kHzandthesecondconsists\nofatemporalintegrationof XA[n,l],whichisfollowedby\nsubsampling.\n3.2 Auditory Group Delay\nAccordingto(2),andbynotingthat XA[n,l]hasnoimag-\ninary values like the DFT-based group delay, the proposed\nfunction for computing the group delay in the auditory\n10610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSamples0 500 1000 1500 2000 2500 3000 3500-1.5-1-0.500.511.5\nFigure 1 . A sequence of impulses with linearly time\nvarying amplitudes, the associated DFT-based group de-\nlay function (dashed line), and the associated auditory\nspectrum-basedgroupdelayfunction(dashed-dottedline) .\nspectrum isdeﬁned as:\nAUD GRD[n,l] =YA[n,l]\nXA[n,l], (5)\nwhere YA[n,l]is the auditory spectrum of nx[n]. Due to\nthe differentiation factor ∂nin (4), onsets are detected by\ndeterminingthepositionsofpositivepeaksratherthanpos -\nitive zero-crossings. In Figure 1, the auditory spectrum-\nbased group delay that is obtained when shifting an anal-\nysis window over a sample signal is depicted as a dashed-\ndotted line. Note that the term group delay was preferred\nfor the detection function instead of auditory phase slope,\nbecausenoaveragevaluehasbeencomputedforneighbor-\ning bands as isthecase forthe DFT-based phase slope.\nThe processing steps for the computation of the onset\ndetectionsignal,basedontheauditoryspectrumgroupde-\nlay function, can be seen in Figure 2. The auditory spec-\ntrum was computed using the NSL toolbox [9]. For the\ncomputation of the auditory spectrum the window length\nissetto0.1s,with4.5mshopsizeandtheresultedspectro-\ngram is computed for a bandwidth of 76-3242 Hz. In pro-\ncessing block 2, the auditory group delay function is com-\nputed from auditory spectrograms XA[n,l]andYA[n,l]\nusing(5). Forouranalysis,tonotopicbands b= 10,... ,39\nof the auditory spectrogram were utilized, thus ignoring\nbands containing high-frequency noise, as well as bands\nranging from 76-104 Hz which are not crucial for onset\ndetection purposes, because these frequencies are below\ntheF0rangeoftheinvestigatedinstruments. Inprocessing\nblock3ofFigure2,eachbandissmoothedintimeusinga\n3rddegreeSavitzky-Golayﬁlterwithwindowsizeequalto\n12samples[12]. TheSavitzky-Golayﬁlteruseslocalpoly-\nnomial regression and is considered superior compared to\nFIR ﬁlters or moving average ﬁlters, preserving the local\nmaxima of the signal while rejecting noise. In processing\nblock 4, for each group delay band, peak picking is per-\nformed in order to select candidate onsets. For each band,\nan onset detection signal is constructed containing either\nthe value zero when no peak has been detected, or the am-\nplitudeofthedetectedpeak. Ineachband b,athresholdfor\npeak detection is determined separately by the mean value\nofthehalf-waverectiﬁedgroupdelayfunctionforthepar-Sample0100200300400500600700 800 900 1000\nFigure 3. The spectral ﬂux onset strength signals of a\ntanbur recording. The lower-placed signal depicts the au-\nditory spectrum-derived spectral ﬂux, while the higher-\nplaced signal depicts the DFT-based spectral ﬂux. The ‘x’\nmarker corresponds totheannotated onset time.\nticular band. Finally, all band-wise detection signals are\nsummed, creating a single onset detection signal based on\ntheauditory group delay.\n3.3 Auditory Spectral Flux\nThe spectral difference in the auditory domain is deﬁned\ninasimilarmannertothegroupdelay. Thespectralﬂuxin\ntheauditory spectrum isdeﬁned usingthe L1norm:\nAUD SF[n] =/summationdisplay\nlHW(XA[n,l]−XA[n−1,l]).(6)\nFor the auditory spectral ﬂux, the original signal is re-\nsampled to 8kHz and the spectral ﬂux is computed with\na step size of 8 ms. It should be noted that no band-wise\nsmoothing or band selection was performed on the audi-\ntory spectral ﬂux, since it was found to degrade onset de-\ntection performance. In Figure 3, the auditory spectrum-\nbased and DFT-based spectral ﬂux onset strength signals\nof a tanbur (plucked string instrument) recording are de-\npicted. Theannotatedonsettimescanalsobeseen,aswell\nasafalsedetectionfortheDFT-basedspectralﬂuxatsam-\nple790.\n3.4 Auditory Spectrum-based Onset Detection System\nOnsets from the auditory group delay and spectral ﬂux\ndetection signals are detected using roughly the same ap-\nproach as for the DFT representations, by selecting the lo-\ncal maxima of the signals. First, each detection function\nis normalized using z-score standardization. Afterwards,\na moving median ﬁlter of length 0.2s is computed as an\nadaptive threshold, which is a robust method for detecting\nimpulses in audio signals [8]. The adaptive threshold is\nthen subtracted from the detection signals. Finally, peak\npicking is performed, by selecting peaks that are higher\nthanthreshold δandareseparatedbyaminimumpeakdis-\ntance of 40ms.\n107Poster Session 1\nwavAUDITORY\nSPECTR.GROUP\nDELAYBAND\nSMOOTHINGPEAK\nSELECTION+AUD_GRD_OSS\nFigure 2. Block diagram of thecomputation of the auditory spectrum- based group delay.\nInstrument No. of onsets No. of ﬁles\nCello 150 5\nClarinet 149 5\nGuitar 174 5\nKemenc ¸e 186 5\nNey 147 7\nUd 211 5\nPiano 195 5\nSaxophone 148 5\nTanbur 156 5\nTrumpet 140 5\nViolin 173 5\nTotal 1829 57\nTable 1. Onset dataset details.\n4. EXPERIMENTS\n4.1 Dataset\nIn our experiments, the dataset introduced in [7] was em-\nployed. Itconsistsof57recordingsofpitchedinstruments ,\nincluding11instrumenttypes,asseeninTable1. Thevar-\nious instrument types can be organized into three classes:\npitched-percussive instruments (guitar, ud, piano, and ta n-\nbur),windinstruments(clarinet,ney,saxophone,andtrum -\npet),andbowedstringinstruments(cello,kemenc ¸e,andvi -\nolin). It should be noted that the set is not limited to west-\nern instruments, but also contains middle-eastern instru-\nment samples. In total, the recordings contain 1829 anno-\ntated onsets, while each instrument type contains roughly\nthesamenumberofonsets. Allrecordingsaremonophonic,\nsampled at 44.1kHz.\n4.2 Evaluation Methods\nFor evaluating the results of the proposed onset detection\nsystems, the recall ( R), precision ( P), and F-measure ( F)\nare employed as ﬁgures of merit. Let Ntpstand for the\nnumber of correctly detected onsets, Nfpthe number of\nfalse positives, and Nfnthe number of missed onsets. P\nandRare deﬁned as:\nP=Ntp\nNtp+Nfp, R=Ntp\nNtp+NFN(7)\nwhile theF-measure iscomputed from PandR:\nF=2PR\nP+R(8)\nItshouldbenotedthat P,R,andFareutilizedforeval-\nuation in the MIREX onset detection contests. An onsetFeature GRD SFAUDGRDAUDSF\nF-measure 73.7% 73.9% 73.8% 75.9%\nTable 2. F-measures for the various onset detection fea-\ntures.\nis correctly matched if it is detected within 50ms of the\nground truth onset time. By varying parameter δin small\nsteps, P/R-curves can be created by placing Rvalues on\nthe horizontal axis and Pvalues on the vertical one. The\nP/R-curve which is closer to the upper right corner of the\ndiagram is considered to be the best detector with regards\ntoF.\n4.3 Results\nThe performance of the various onset detection features is\nshown in P/R-curves in Figure 4. In Figure 3(a) the per-\nformance of the complete dataset as described in Table 1\nis shown. Regarding the optimum F-measure, the DFT-\nbased group delay and spectral ﬂux along with the audi-\ntorygroupdelayseemtoperformalmostequallygood,but\nthey are surpassed by the auditory spectral ﬂux. The best\nF-measures on the complete dataset can be seen in Table\n2, where it can be seen that the auditory spectral ﬂux out-\nperforms the other three features by about 2% in terms of\nF-measure. Theauditorygroupdelayperformsmarginally\nbetter than its DFT-based counterpart, achieving high pre-\ncisionrates. Ingeneral,theauditory-basedfeaturesoutp er-\nformtheir respective DFT-based features.\nAsfarastheindividualinstrumenttypesareconcerned,\nthe auditory group delay exhibits very high precision rates\nfor the set of string instruments in Figure 3(b), making it\nusefulforbeattrackingtasks. However,theauditorygroup\ndelay is vastly outperformed by the remaining three fea-\ntures when pitched percussive instruments are employed\nin Figure 3(c), with the DFT-based spectral ﬂux achieving\nvery high precision and recall rates. The DFT-based spec-\ntral ﬂux slightly outperforms the auditory spectrum-based\nspectralﬂuxforpitchedpercussiveinstruments,whichcan\nbeattributedtothelimitedfrequencyrangeoftheauditory\nspectrum, since percussive onsets are detected in high fre-\nquencybands[2]. Itshouldbenotedthatallfeaturesreport\nhigh rates for pitched percussive instruments compared to\nstringandwindinstruments. Finally,thesetofwindinstru -\nmentsinFigure3(d)showslowerprecisionratescompared\ntotheothersets. Theauditoryfeaturesachieveroughlythe\nsame best F-measure, outperforming the DFT-based fea-\ntures.\n10810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nALLINSTRUMENTS\nSF\nPS\nAUDSF\nAUDGRD\n0 20 40 60 80 100020406080100\n(a)BOWED STRINGS\nSF\nGRD\nAUDSF\nAUDGRD\n0 20 40 60 80 100020406080100\n(b)\nPITCHED PERCUSSIVE\nSF\nGRD\nAUDSF\nAUDGRD\n0 20 40 60 80 100020406080100\n(c)WIND INSTRUMENTS\nSF\nGRD\nAUDSF\nAUDGRD\n0 20 40 60 80 100020406080100\n(d)\nFigure 4. Performance curves of the various onset detection feature s. Recall and Precision values are plotted on the\nhorizontal and vertical axis,respectively.\n5. CONCLUSIONS\nIn this paper a new approach for onset detection using au-\nditoryspectrawasproposed. Thegroupdelayfunctionand\nspectralﬂuxintheauditorydomainwereintroducedasfea-\ntures for onset detection, and a system was proposed. The\nonset detection performance of the auditory spectral ﬂux\nwas found to be superior compared to the DFT-based fea-\nture, reaching an F-measure of 75.9% compared to 73.9%\nof the DFT-based spectral ﬂux. While the performance\nof the auditory spectral ﬂux for pitched percussive instru-\nments was inferior compared to DFT-based features, it is\nrelatively superior when string and wind instruments are\ntested.\nIn the future, a fusion of the onset detection features in\nthe auditory domain will be performed, in an attempt to\nmaximize onset detection performance. The system could\nalso consider onsets produced by non-pitched percussive\ninstruments,whichcanbeeasilydetectedusingenergyde-\nscriptors. In addition, the creation of an onset detection\nsystem which is dependent of the instrument family can\nleadtoimprovedresults. Finally,theaforementionedtech -niques can be developed for usage in polyphonic record-\nings.\n6. REFERENCES\n[1] J. P. Bello, C. Duxbury, M. Davies, and M. Sandler,\n“On the use of phase and energy for musical onset de-\ntectioninthecomplexdomain,” IEEESignalProc.Let-\nters,Vol. 11, No. 6, pp. 553-556, June 2004.\n[2] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M.\nDavies, and M. Sandler, “A tutorial on onset detec-\ntion of music signals,” IEEE Trans. Speech and Audio\nProc., Vol. 13, No. 5, pp. 1035-1047, Sep. 2005.\n[3] N.Collins,“Usingapitchdetectorforonsetdetection, ”\ninProc.6thInt.Conf.MusicInformationRetrieval ,pp.\n100-106, September 2005.\n[4] S. Dixon, “Onset detection revisited,” in Proc. 9th Int.\nConf. Digital Audio Effects , pp. 133-137, 2006.\n[5] M. Gainza, E. Coyle, and B. Lawlor, “Onset detection\n109Poster Session 1\nusing comb ﬁlters,” in Proc. IEEE Workshop Applica-\ntions of Signal Processing to Audio and Acoustics , pp.\n263-266, 2005.\n[6] A. Holzapfel and Y. Stylianou, “Beat tracking using\ngroup delay based onset detection,” in Proc. 9th Int.\nConf. MusicInformationRetrieval , Sep. 2008.\n[7] A. Holzapfel, Y. Stylianou, A. C. Gedik, and B.\nBozkurt “Three dimensions of pitched instrument on-\nset detection,” IEEE Trans. Audio, Language, and\nSpeech Processing , accepted for publication.\n[8] I. Kauppinen, “Methods for detecting impulsive noise\nin speech and audio signals,” in Proc. 14th Int. Conf.\nDigitalSignal Proc. , Vol. 2, pp. 967-970, July2002.\n[9] T. Chi and S. A. Shamma, “NSL Matlab Tool-\nbox,”http://www.isr.umd.edu/Labs/NSL/\nSoftware.htm , Neural Systems Lab., Univ. Mary-\nland.\n[10] A. V. Oppenheim, R. W Schafer, and J. R. Buck,\nDiscrete-TimeSignal Processing , Prentice Hall,1998.\n[11] P. Ru, “Multiscale multirate spectro-temporal audito ry\nmodel,”PhD Thesis, Univ. Maryland College Park ,\n2001.\n[12] A. Savitzky, and M. J. E. Golay, “Smoothing and dif-\nferentiation of data by simpliﬁed least squares proce-\ndures,”AnalyticalChemistry ,Vol.36,No.8,pp.1627-\n1639, July1964.\n[13] X. Yang, K. Wang, and S. A. Shamma, “Auditory rep-\nresentationsofacousticsignals,” IEEETrans.Informa-\ntionTheory , Vol. 38, No. 2, pp. 824-839, March 1992.\n110"
    },
    {
        "title": "Music Mood and Theme Classification a Hybrid Approach.",
        "author": [
            "Kerstin Bischoff",
            "Claudiu S. Firan",
            "Raluca Paiu",
            "Wolfgang Nejdl",
            "Cyril Laurier",
            "Mohamed Sordo"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417317",
        "url": "https://doi.org/10.5281/zenodo.1417317",
        "ee": "https://zenodo.org/records/1417317/files/BischoffFPNLS09.pdf",
        "abstract": "Music perception is highly intertwined with both emotions and context. Not surprisingly, many of the users’ information seeking actions aim at retrieving music songs based on these perceptual dimensions – moods and themes, expressing how people feel about music or which situations they associate it with. In order to successfully support music retrieval along these dimensions, powerful methods are needed. Still, most existing approaches aiming at inferring some of the songs’ latent characteristics focus on identifying musical genres. In this paper we aim at bridging this gap between users’ information needs and indexed music features by developing algorithms for classifying music songs by moods and themes. We extend existing approaches by also considering the songs’ thematic dimensions and by using social data from the Last.fm music portal, as support for the classification tasks. Our methods exploit both audio features and collaborative user annotations, fusing them to improve overall performance. Evaluation performed against the AllMusic.com ground truth shows that both kinds of information are complementary and should be merged for enhanced classification accuracy.",
        "zenodo_id": 1417317,
        "dblp_key": "conf/ismir/BischoffFPNLS09",
        "keywords": [
            "music perception",
            "emotions",
            "context",
            "information seeking",
            "music retrieval",
            "latent characteristics",
            "moods",
            "themes",
            "social data",
            "Last.fm music portal"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMUSIC MOOD AND THEME CLASSIFICATION - A HYBRID APPROACH\nKerstin Bischoff, Claudiu S. Firan,\nRaluca Paiu, Wolfgang Nejdl\nL3S Research Center,\nAppelstr. 4, Hannover, Germany\n{bischoff,firan,paiu,nejdl }@L3S.deCyril Laurier, Mohamed Sordo\nMusic Technology Group,\nUniversitat Pompeu Fabra\ncyril.laurier@upf.edu\nmohamed.sordo@upf.edu\nABSTRACT\nMusic perception is highly intertwined with both emotions\nand context. Not surprisingly, many of the users’ informa-\ntion seeking actions aim at retrieving music songs based\non these perceptual dimensions – moods and themes, ex-\npressing how people feel about music or which situations\nthey associate it with. In order to successfully support mu-\nsic retrieval along these dimensions, powerful methods are\nneeded. Still, most existing approaches aiming at inferring\nsome of the songs’ latent characteristics focus on identi-\nfying musical genres. In this paper we aim at bridging\nthis gap between users’ information needs and indexed mu-\nsic features by developing algorithms for classifying mu-\nsic songs by moods and themes. We extend existing ap-\nproaches by also considering the songs’ thematic dimen-\nsions and by using social data from the Last.fm music por-\ntal, as support for the classiﬁcation tasks. Our methods\nexploit both audio features and collaborative user annota-\ntions, fusing them to improve overall performance. Eval-\nuation performed against the AllMusic.com ground truth\nshows that both kinds of information are complementary\nand should be merged for enhanced classiﬁcation accuracy.\n1. INTRODUCTION\nGeneral music perception – i.e.how we think and talk about\nmusic – is heavily inﬂuenced by emotions and context.\nConsequently, users’ music information seeking behavior\nalso reﬂects the importance of opinion/mood and theme\nassociations for music songs. Searching for music usu-\nally is an exploratory and social process, in which peo-\nple make use of collective knowledge, as well as the opin-\nions and recommendations of other people [1]. Related is\ntheir need for contextual metadata expressing, for exam-\nple, which situations/events are often associated with the\nsongs. Thus, besides directly searching or browsing music\nby artist or title, associated usage, theme/main subject and\nmood/emotional state are used in every third (navigational)\nquery [1]. Similarly, [2] found that the majority of music\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.queries from a search engine log falls into these categories\n– 30% of the queries are theme-related ( e.g.“party music”,\n“wedding songs”) and 15% target mood information. Such\nstatistics thus show the necessity of indexing music collec-\ntions according to mood and theme classes.\nHence, our goal in this paper is to automatically derive\nmood and theme metadata for music tracks to better cover\ndiverse facets reﬂecting the complex real-world music in-\nformation needs of users. With the “mood of a song” we\ndenote the state or the quality of a particular feeling in-\nduced by listening to that song ( e.g. aggressive ,happy ,sad,\netc.). The “theme of a song” refers to the context or situa-\ntion which ﬁts best when listening to the song, e.g. at the\nbeach ,night driving ,party time ,etc.\nCurrently available state-of-the-art music search engines\nstill do not explicitly support music retrieval based on mood\nand theme information, and content-based approaches try-\ning to address this problem mainly focus on identifying\nthe moods of songs and do not tackle the thematic aspects\nof the music resources. Several works in Music Informa-\ntion Retrieval have shown a potential to model the mood\nfrom audio content (like [3–6], see [7] for an extensive\nreview). Although this task is quite complex, satisfying\nresults can be achieved if the problem is reduced to sim-\nple models [7]. However, an important limitation of these\napproaches is that they concentrate on the mood only ex-\npressed in the audio signal itself and can not capture other\nsources of emotionality.\nApart from analyzing the low-level features of music\nresources to identify the songs’ corresponding mood or\ntheme, another powerful source of information that can be\nused are Web 2.0 portals. Collaborative tagging platforms\nhave become extremely popular in recent years – users as-\nsociate descriptive keywords to various types of content\n(e.g.pictures, Web pages, music). Especially for multi-\nmedia data, such as music, the gain provided by the newly\navailable textual information is substantial, since with most\nprominent search engines on the Web, users are currently\nstill constrained to search for music using textual queries.\nThe contributions of the paper are twofold:\n•We show the feasibility of automatic music classiﬁ-\ncation according to contextual aspects like themes.\n•We successfully exploit collective knowledge in form\nof tags in order to complement the intrinsic informa-\ntion derived from audio features.\n657Poster Session 4\nThe algorithms can be used in various ways: predicted\nmood and theme labels can be indexed to enrich the meta-\ndata index of music search engines enabling a more social\nand context-aware search (or browsing). Besides, such la-\nbels will be valuable for recommendation and playlist gen-\neration, e.g.for listening to “Party Time”-like songs.\n2. RELATED WORK\nMusic enrichment recently focuses on deriving mood in-\nformation based on extracted acoustic data [3–5]. [3] pro-\nposes a content-based method, tailored to classical music,\nthat uses the Thayer’s model [8] for classiﬁcation. For de-\ntecting the mood of music, timbre, intensity and rhythm,\nfeatures are extracted and a Gaussian Mixture Model is\nused to model each feature set. In [4], the authors propose\na schema such that music databases are indexed on four la-\nbels of music mood: “happiness”, “sadness”, “anger” and\n“fear”. The relative tempo of the music tracks, the mean\nand standard deviation of average silence ratio are used to\nclassify moods, using a neural network as classiﬁer. For\nautomatically detecting mood for music tracks, [5] uses a\nset of 12 mood classes which are not mutually exclusive.\nHowever, the main focus of the paper is creating a ground\ntruth database for music mood classiﬁcation.\nSeveral existing papers aim at automatically inferring\nadditional information from available content as well as\n(user generated) metadata. [9] present a music retrieval sys-\ntem that uses supervised multiclass Na ¨ıve Bayes classiﬁca-\ntion for learning a relationship between acoustic features\nand words from expert reviews on songs, thus enabling\nquery-by-text for music. Similarly, [10, 11] aim at enrich-\ning songs with textual descriptions for improving music\nIR. [10] uses a variant of the AdaBoost algorithm, Filter-\nBoost, in order to predict social tags of the songs based\non the information captured in the audio features. Never-\ntheless, the tags learned by the classiﬁer pertain to multi-\nple categories of tags (genres, styles, moods and contexts)\nand there is no special focus on mood and theme-related\ntags, like in our case. [11] compares ﬁve methods for col-\nlecting tags: user surveys, harvesting social tags, annota-\ntion games, mining web documents and auto-tagging audio\ncontent. Again, here there is no discussion about the per-\nformance of the described methods for predicting mood\nand theme tags. Moreover, both [10, 11] are not compara-\nble with our approach, since there is no clear deﬁnition for\nmood and theme classes and the data sets on which evalu-\nation was performed differ from ours.\n[12] and [13] investigate social tags for improving mu-\nsic recommendations – [12] to attenuate the cold-start prob-\nlem by automatically predicting additional tags based on\nthe learned relationship between existing tags and acous-\ntic features, [13] to make better recommendations based\non the latent factors hidden in user-tag-item relations. For\nthis, the authors successfully apply Higher Order Singular\nValue Decomposition on the triplets. Again, while both ap-\nproaches make use of Last.fm to predict (the likelihood) of\nall kinds of tags, our work explicitly focuses on inferring\nmood and theme annotations.In [14], Last.fm user tags have been used together with\ncontent-based features for automatic genre classiﬁcation.\nTwo classiﬁcation strategies are proposed that make im-\nplicit use of tags: A graph of music tracks is constructed\nthat captures their semantic similarity in terms of tags as-\nsociated. Both the baseline low-level feature only clas-\nsiﬁer as well as a single-layer classiﬁer, considering au-\ndio features and implicit tag similarity simultaneously, are\nclearly outperformed by a double-layer classiﬁer, which\nﬁrsts learns genre labels based on audio information and\nthen iteratively updates its models considering the tag-based\nneighborhood of tracks.\nThus it seems that especially for multimedia user gen-\nerated tags are valuable, since low-level features may not\nbe expressive enough. [15] found that Last.fm tags deﬁne a\nlow-dimensional semantic space which - especially at the\ntrack level highly organized by artist and genre - is able\nto effectively capture sensible attributes as well as mu-\nsic similarity. Somewhat complementary to our approach,\n[16] aims at studying the relationships between moods and\nartists, genres and usage metadata. As a test set for the\nexperiments, the authors use AllMusic.com ,Epinions.com\nand a subset of Last.fm data. The authors point out an in-\nteresting ﬁnding: Many of the individual mood terms were\nhighly synonymous, or described aspects of the same un-\nderlying mood space. The experiments also showed that\ndecreasing the mood vocabulary size in some ways clari-\nﬁed the underlying mood of the items being described.\nWe use Last.fm ’s valuable folksonomy1information for\ninferring mood and theme labels for songs. While in ear-\nlier experiments only tags were used for deriving moods,\nthemes and styles/genres [17], in this paper we also inves-\ntigate fusion with audio-based methods. Extending exist-\ning music metadata enrichment studies, we fuse social tags\nand low-level audio features of the tracks to infer mood\nor theme labels showing that both sources provide helpful\ncomplementary information.\n3. DATA SETS\nAllMusic.com .In 1995, the AllMusic.com (AMG) web-\nsite was created as a place and community for music fans.\nAlmost all music genres and styles are covered, ranging\nfrom the most commercial/popular to very obscure ones.\nNot only genres can be found on AllMusic.com , but also\nreviews of albums and artists within the context of their\nown genres, as well as classiﬁcations of songs and albums\naccording to themes, moods or instruments. All these re-\nviews and classiﬁcations are manually created by music\nexperts from the AllMusic.com team, therefore the data\nfound here serves as a good ground truth corpus.\nFor our experiments, we collected AllMusic.com pages\ncorresponding to music themes and moods, ﬁnding 178\ndifferent moods and 73 themes. From the pages corre-\nsponding to moods and themes, we also gathered informa-\ntion related to which music tracks fall into these categories.\nThis way, we ended up with 5,770 songs. Looking at the\n1folk + taxonomy: collaboratively created classiﬁcation scheme\n65810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nsongs identiﬁed in each of the categories, we have 8,158\ntrack-mood and 1,218 track-theme assignments. On aver-\nage songs are annotated with 1.73 moods and 1.21 themes\nrespectively, with maximum number of annotations of 12\nand 6 respectively.\nLast.fm .For the tracks collected from AllMusic.com ,\nwe obtained the Last.fm tags users had assigned to these\nsongs together with the corresponding frequencies. Last.fm\nis a popular UK-based Internet radio and music commu-\nnity website. In a comparative study on tagging [2] found\nthat the majority of the generally accurate and reliable user\ntags on Last.fm fall into the genre category (60%). Consid-\nerably less frequent are tags referring to moods/opinions/\nqualities (20%) or themes/context/usage (5%) of the music\nsongs. According to [15], at the track level the tags often\nname the genre and artist of a song. As not all AllMu-\nsic.com songs have user tags in Last.fm , our set of tracks\nis reduced to 4,737. Using the AudioScrobbler API, we\ncollected in total 59,525 different tags for this set of songs.\nAudio. For each track from the previous collections\nfound in our audio database, we have a 30 seconds excerpt\nin mp3 format with a bitrate of 192 kbps. From these au-\ndio tracks, we automatically extracted several state-of-the-\nart MIR audio features of different type: timbral, tonal,\nrhythmic including MFCCs, BPM, chroma features, spec-\ntral centroid and others. Please refer to [7] for a complete\nlist. For each excerpt of the data set, its 200ms frame-based\nextracted features were summarized with their component-\nwise means and variances. At the end of the process, we\nobtained 240 low-level and mid-level audio features.\n4. MOOD AND THEME CLASSIFICATION\nFor predicting themes and moods, we base our solution on\nsocial knowledge – i.e.collaboratively created tags asso-\nciated to music tracks – extracted from Last.fm , as well as\non audio information. Building upon already provided user\ntags, on the audio content of music tracks, or on combina-\ntions of both, we build multiclass classiﬁers to infer addi-\ntional annotations corresponding to moods and themes.\n4.1 AllMusic.com Class Clustering\nGiven that the number of classes existing in AllMusic.com\nis quite large ( e.g.178 different moods) with many of the\nindividual terms being highly synonymous or denoting the\nsame concept in well known models of emotions2[16],\nclustering was applied to the initial set of AllMusic.com\nmoods as well as the themes.\nMood Clustering. For comparison reasons, we choose\nthe ﬁve mood categories used for the MIREX Audio Music\nMood Classiﬁcation Track (see Table 1). Each of the clus-\nters is a collection of ﬁve to seven AllMusic.com mood la-\nbels that together deﬁne the cluster. These categories were\nproposed in [16], derived from a popular set (of Top Songs,\nTop Albums). The MIREX mood clusters effectively re-\nduce the diverse mood space, and yet root in the social-\n2Moods are considered to be similar to emotions, but being longer in\nduration, less intensive and missing object directednessNr. MOOD CLUSTERS – MIREX\nMM1 Passionate, Rousing, Conﬁdent, Boisterous, Rowdy\nMM2 Rollicking, Cheerful, Fun, Sweet, Amiable/Good natured\nMM3 Literate, Poignant, Wistful, Bittersweet, Autumnal, Brooding\nMM4 Humorous, Silly, Campy, Quirky, Whimsical, Witty, Wry\nMM5 Aggressive, Fiery, Tense/Anxious, Intense, V olatile, Visceral\nNr. MOOD CLUSTERS – THAYER\nMT1high energy / high stress: Tense/Anxious, Angst-Ridden,\nSpooky, Eerie, Rowdy, Fiery, Angry, Fierce, Provocative,\nBoisterous, Hostile, Aggressive, V olatile, Rebellious,\nConfrontational, Paranoid, Outrageous, Unsettling, Brittle\nMT2high energy / low stress: Rollicking, Exuberant, Happy,\nSexy, Exciting, Energetic, Party/Celebratory, Intense,\nGleeful, Lively, Cheerful, Fun, Rousing, Freewheeling,\nCarefree, Passionate, Playful, Gritty, Joyous,\nMT3low energy / low stress: Calm/Peaceful, Sentimental,\nCathartic, Soft, Romantic, Springlike, Warm, Precious,\nLaid-Back/Mellow, Conﬁdent, Hypnotic, Naive, Intimate,\nInnocent, Relaxed, Soothing, Dreamy, Smooth, Gentle\nMT4low energy / high stress: Sad, Melancholy, Detached,\nWhimsical, Gloomy, Ironic, Snide, Somber, Autumnal,\nWry, Wintry, Plaintive, Yearning, Austere, Bittersweet,\nFractured, Bleak, Cynical/Sarcastic, Bitter, Acerbic\nTable 1 . (Samples from) Mood clusters\ncultural context of pop music3. Restricting our data set\nto tracks whose assigned moods fall into exactly one of\nthese categories, we had 1192 distinct songs left for ma-\nchine learning. To balance cluster size for our multiclass\nclassiﬁers the cutoff was set to 200 instances per cluster.\nSince many AllMusic.com mood labels and thus the cor-\nresponding songs classiﬁed by human experts are not used\nin MIREX, we as well experimented with the well known\ntwo-dimensional models of emotion/mood. In the Thayer\nenergy-stress model [8], emotions are classiﬁed along the\ntwo axes of (low - high) energy and (low - high) stress.\nThus, the two factors divide the mood space into the four\nclusters “exuberance”, “anxious/frantic”, “depression” and\n“contentment”. Similarly, Russell/Thayer’s bipolar model\ndifferentiates emotions based on arousal and valence. In\nthe psychological literature there is little agreement on the\nnumber of basic emotional categories or dimensions. How-\never, the Thayer model has been proven useful for mu-\nsic classiﬁcation and the four categories resulting seem\na fair compromise: reducing the mood space to enable\nclear classiﬁcatory distinction and still providing valuable\nextra-musical metadata for exploratory information needs.\nDuring clustering all AllMusic.com labels were manually\nmapped into the two-dimensional mood space by the au-\nthors adopting a similarity sorting method as described be-\nlow for themes. The four resulting clusters are shown to-\ngether with some example AllMusic.com labels in Table 1.\nAgain, clusters were balanced by randomly choosing 403\ninstances for each cluster.\nTheme Clustering. Since AllMusic.com themes do\nnot directly correspond to human emotions, mapping the\n73 theme terms into the mood spaces used before was not\npossible (though themes may often be strongly related to\nspeciﬁc moods). For manual clustering, we adopted a simi-\nlarity sorting procedure, in which all AllMusic.com themes\nwritten on cards were sorted by the authors into as many\nand as high piles as appropriate. Co-occurrence matrices\n3http://www.music-ir.org/mirex2007/index.php/\nAudio_Music_Mood_Classification\n659Poster Session 4\nNr. THEME CLUSTERS\nT1Party Time, Birthday Party, Celebration, Prom, Late Night,\nGuys Night Out, Girls Night Out, At the Beach, Drinking,\nCool & Cocky, TGIF, Pool Party, Club, Summertime\nT2Sexy, Seduction, Slow Dance, Romantic Evening, In Love\nNew Love, Wedding, Dinner Ambiance\nT3Background Music, Exercise/Workout Music, Playful\nThe Sporting Life, Long Walk, The Great Outdoors, Picnic,\nMotivation, Empowering, Afﬁrmation, The Creative Side,\nVictory, Day Driving, Road Trip, At the Ofﬁce\nT4D-I-V-O-R-C-E, Heartache, Feeling Blue, Breakup, Regret,\nLoss/Grief, Jealousy, Autumn, Rainy Day, Stay in Bed,\nSolitude, Reminiscing, Introspection, Reﬂection, Winter,\nSunday Afternoon\nTable 2 . Theme clusters\nwere built and added to ﬁnd good groupings by analyzing\nthe clusters. Unclear membership of singular labels was\nresolved after discussion. Applying this method resulted\nin a theme list with 13 labels. Classes containing too few\nsongs are discarded in order to have a minimal representa-\ntive learning corpus for the classiﬁer, such that the remain-\ning four theme clusters (Table 2) contain 74 songs each.\n4.2 Classiﬁcation\nThe core of our mood and theme classiﬁcation methods are\nmulticlass classiﬁers trained on the AllMusic.com ground\ntruth using tags or audio information as features. We ex-\nperiment both with classiﬁers created separately for the\ntwo different types of features we consider, which are then\ncombined in order to produce for each song a ﬁnal mood/\ntheme classiﬁcation, as well as with a classiﬁer taking as\ninput a combination of audio and tag features. After sev-\neral experiments, we could observe that SVM classiﬁers\nwith Radial Basis Function (RBF) kernel performed best\nfor the case of audio input features (it outperformed Lo-\ngistic Regression, Random Forest, GMM, K-NN and De-\ncision Trees), whereas in the case of tag features, Na ¨ıve\nBayes Multinomial achieved the best performance. Addi-\ntionally, the linear combination of the separate classiﬁers\nfor audio and tag features performed better than the clas-\nsiﬁer trained on the combination of audio and tag features.\nOnly the best obtained classiﬁcation results are presented\nin this paper. We have classiﬁers trained for the whole set\nof classes ( i.e.either for moods or themes) and these classi-\nﬁers produce for every song in the test set a probability dis-\ntribution over all classes ( e.g.over all moods). The highest\nprobability is considered in order to assign the songs to the\ncorresponding class. We experimented with feature selec-\ntion based on automatic methods ( e.g.Information Gain)\nbut the results showed that the full set is better suitable for\nlearning, even though it contains some noise.\nAlgorithm 1 presents the main steps of our classiﬁca-\ntion approach, where classiﬁers are trained separately for\nthe two different types of input features – tags and audio\ninformation. We show the algorithm for mood classiﬁca-\ntion, the case of themes classiﬁcation being similar.\nStep 1 (optional) of the algorithm above aims at reduc-\ning the number of mood classes to be predicted for the\nsongs. If two classes are clustered, the resulted class will\ncontain all songs which have been originally assigned toany of the composing classes. As we need a certain amount\nof input data in order to be able to consistently train the\nclassiﬁers, we discard those classes containing less than a\ncertain number of songs4assigned (step 2).\nAlg. 1. Mood classiﬁcation\nInput:ftype – feature type\nftype =/braceleftbigg\n0,for tag features ;\n1,for audio features .\nM – mood classes to be learned\nStotal – set of songs\n1:Apply clustering method to cluster moods (see Section 4.1)\n2:Select classes of moods Mto be learned\nFor each mood class\nIf the class does not contain at least Xsongs\nDiscard class\n3:Classiﬁer learns a model\n3a:Split song setStotal into\nStrain = songs used for training the classiﬁer\nStest = songs used for testing the classiﬁers’ learned model\n3b: Select features for training the classiﬁer\nIf (ftype = 0) // tag features\nFor each song si∈Strain\nCreate feature vector Ft(si) ={tj|tj∈T}, where\nT= set of tags from all songs in all mood classes\ntj=/braceleftbigg\nlog(freq (tj) + 1),ifsihas tagtj;\n0,otherwise.\nElse // audio features\nFor each song si∈Strain\nCreate feature vector Fa(si) ={aj|aj∈A}, where\nA= set of audio features from all songs in all mood classes\naj=standardize (aj)\n3c:Train and test classiﬁer\nIf (ftype = 0) // tag features\nTrain Na ¨ıve Bayes (NB) on Strain using{Ft(si);si∈Strain}\nTest Na ¨ıve Bayes (NB) on Stest\nElse // audio features\nTrain SVM on Strain using{Fa(si);si∈Strain}\nTest SVM onStest\n4:Classify songs into mood classes\nFor each song si∈Stotal\nIf (ftype = 0) // tag features\nCompute probability distribution Pt(si)as\nPt(si) ={pNB(mj|si);mj∈M}\nAssignsitomj, wheremax (pNB(mj|si))\nElse // audio features\nCompute probability distribution Pa(si)as\nPa(si) ={pSV M (mj|si);mj∈M}\nAssignsitomj, wheremax (pSV M (mj|si))\nAfter selecting separate sets of songs for training and\ntesting in step 3a, we build the feature vectors correspond-\ning to each song in the training set (step 3b). In the case\nof features based on tags, the vectors have as many ele-\nments as the total number of distinct tags assigned to the\nsongs belonging to the mood classes. The elements of a\nvector will have values depending on the frequency of the\ntags occurring along with the song. We experimented with\ndifferent variations for computing the vector elements, but\nthe formula based on the logarithm of the tag frequency\nprovided best results. Audio features are standardized for\nbetter suitability with the SVM classiﬁer. Here, a one-\nvs-one multiclass approach was taken with the parameters\nselected via grid search (C and gamma with 3-fold cross\nvalidation method). Probability estimations are made by\npairwise coupling [18].\nOnce the feature vectors are constructed, they are fed\ninto the classiﬁer and used for training. The assignment\nof a song to a class is done based on the maximum pre-\ndicted probability for a song among all possible classes\n4The threshold depends on the type of clustering and class type. The\nexact numbers are given in Section 4.1.\n66010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nClassiﬁer Class R P F1 Acc\nSVM (audio) Mood MIREX 0.450 0.442 0.420 0.450\nNB (tags) Mood MIREX 0.565 0.566 0.564 0.565\nComb (α= 0.7) Mood MIREX 0.575 0.573 0.572 0.575\nSVM (audio) Mood THAYER 0.517 0.515 0.515 0.517\nNB (tags) Mood THAYER 0.539 0.542 0.539 0.539\nComb (α= 0.8) Mood THAYER 0.570 0.569 0.569 0.569\nSVM (audio) Themes clustered 0.528 0.581 0.522 0.527\nNB (tags) Themes clustered 0.595 0.582 0.575 0.595\nComb (α= 0.9) Themes clustered 0.625 0.617 0.614 0.625\nTable 3 . Experimental results: P,R,F1,Acc for the dif-\nferent classiﬁers and mood/theme classes\n(step 4). As already mentioned, we also experiment with a\nlinear combination of the predictions of the two separately\ntrained classiﬁers (details are presented in Algorithm 2).\nAlg. 2. Mood classiﬁcation – classiﬁers’ linear combination\nInput: M – mood classes to be learned\nStotal – set of songs\n1:For each song si∈Stotal\nComputePa(si) ={pSV M (mj|si)}={pa(mj|si)}\nandPt(si) ={pNB(mj|si)}={pt(mj|si)}(see Alg. 1, step 4)\n2:For eachα=0.1,...,0.9,step =0.1\nFor each song si∈Stotal\nFor each mood mj∈M\npat(mj|si) =α·pa(mj|si) + (1−α)·pt(mj|si)\nAssignsitomj, wheremax (pat(mj|si))\nComputeP,R,Acc ,F1\n3:Selectα=αbest that produces best results for P,R,Acc ,F1\n4:Classify songs into mood classes, using αbest for weighting the\nprobabilities outputted by the audio-based classiﬁer and (1−αbest)for\nweighting the probabilities predicted by the tag-based classiﬁer.\nThe two different classiﬁers are ﬁrst trained to make\npredictions for all songs in the collection. For producing\na linear combination of the classiﬁers as ﬁnal output, we\nthen experiment with different values of the αparameter.\n5. EV ALUATION\nFor measuring the quality of our theme and mood predic-\ntions, we compare our output against the AllMusic experts’\nassignments, using Precision ( P), Recall (R), Accuracy\n(Acc) and F1-Measure ( F1) for the evaluation. We present\nthe best results achieved among all our experimental runs\n(10-fold cross validations) in Table 3. These runs corre-\nspond to the different combinations of classiﬁers (audio-\nbased, tag-based, or linear combinations of the two) and\nclasses to be predicted (themes or moods clustered accord-\ning to Mirex or Russell/Thayer resp.).\nFor both moods and themes, we observe that the clas-\nsiﬁers relying solely on audio features perform worse than\nthe pure tag-based classiﬁers. However, combining the two\ntypes of classiﬁers leads to improved overall results. For\nthe moods clustered according to Mirex, Russell/Thayer\nand themes manually clustered, the best values of αare\n0.7, 0.8 and 0.9 respectively. These values indicate a higher\nweight for the audio-based classiﬁers, though their achieved\nperformance is poorer than that of the tag-based classiﬁers.\nThis fact is easily explainable, due to the different types of\nclassiﬁers considered: SVM for audio features and Na ¨ıveBayes for tag features. It is known that Na ¨ıve Bayes pro-\nduces probabilities close to 1 for the most likely identiﬁed\nclass, whereas for the rest of classes, the probabilities are\ncloser to 0. On the other hand, SVM produces more even\nprobability distributions, therefore the high probabilities\noutputted by Na ¨ıve Bayes need to be evened out through\na lowerαweight. The variations of the F1measure with\nαare depicted in Figure 1. The biggest variations are to\nbe found in the case of moods clustered according to the\nRussell/Thayer model, where for values of alpha starting\nwith 0.7 we observe a sharp drop of the F1value. For\nMirex mood classes the F1values start to deprecate with\nαvalues greater than 0.8.\nThe baseline accuracy for a random classiﬁer trying to\nassign songs to the Russell/Thayer mood classes or to the\ntheme clusters is 0.25, while for the Mirex mood classes it\nwould be 0.2. The linear combination of the classiﬁers im-\nproves accuracy in the range of 10 to 27.7% for moods and\n18.5% for themes over audio-based classiﬁers. Overall,\nresults are better for theme classiﬁcation, indicating that\nthemes are easier to distinguish.\nFigure 1 . F1 values when varying the αparameter\nAnalyzing the confusion matrices for the best perform-\ning approaches (Figure 2), we observe some prominent\nconfusion patterns: in the case of Mirex clustering, in-\nstances belonging to class MM 1are often misclassiﬁed\nintoMM 2,MM 4instances into MM 3. Similarly,MT3\ninstances are wrongly classiﬁed into MT4for the case of\nRussell/Thayer clustered moods; also MT1andMT4are\noften confused. For the latter, the energy dimension does\nnot seem to ease differentiation, given that high stress (neg-\native valence) is characteristic for both classes. T3andT1\nare the difﬁcult theme classes. Further reﬁnement of these\nclasses should be considered for future work, in order to\neliminate this kind of ambiguities ( e.g.Exercise/Workout\nmusic might be as well considered Party-like music).\nIt is difﬁcult to directly compare our results to the re-\nlated work cited, as each paper uses a different number of\nclasses. Moreover, experimental goals, ground truth and\nevaluation procedures vary as well, or detailed descriptions\nare missing. Comparing to the best algorithms submitted to\nthe MIREX task, we achieve results with lower accuracy.\nHowever, knowing that the algorithm used in this paper for\naudio classiﬁcation is the same as submitted to MIREX in\n661Poster Session 4\nFigure 2 . Confusion matrix for the best approaches\n2007 [19] (obtaining 60.5% accuracy), our conclusion is\nthat the difference comes from the ground truth data. The\nhypothesis is that our results here are lower because we\ndid not ﬁlter the training and test instances using listeners.\nMoreover for the MIREX collection, listeners were asked\nto focus on audio only (not lyrics, context or other), which\nmakes it much easier then to classify using audio-based\nclassiﬁers. In that context, the classiﬁcation task on our\nMIREX-like AllMusic.com ground truth is more difﬁcult.\n6. CONCLUSION\nPrevious attempts to associate mood labels to music songs\noften rely on lyrics or audio information for clustering or\nclassifying song corporas. Our algorithms exploit both au-\ndio information and social annotations from Last.fm for\nautomatically classifying songs according to moods and\nthemes and thus enriching music tracks with information\noften queried for by users. Themes capturing contextual\naspects of songs, in particular, is a facet not considered so\nfar in the literature. The algorithms proposed in this paper\nrely either on user tags, on audio features, or on combina-\ntions of both. Results of an evaluation performed against\nAllMusic.com experts’ ground truth indicate that provid-\ning such mood and theme information is feasible. The re-\nsults show that audio and tag information is complemen-\ntary and should be merged in order to achieve improved\noverall classiﬁcation performance. Using our algorithm,\nmusic also becomes searchable by associated themes and\nmoods, providing a ﬁrst step towards effectively searching\nmusic by textual, descriptive queries.\nFor future work, some of the promising ideas to be fur-\nther investigated refer to reﬁnements of the moods and\nthemes clusters, as well as to other possible combinations\nof the audio and tag-based classiﬁers, i.e.metaclassiﬁers.\n7. ACKNOWLEDGMENTS\nThis work was partially supported by the PHAROS project\nfunded by the European Commission under the 6th Frame-\nwork Programme (IST Contract No. 045035).\n8. REFERENCES\n[1] J. H. Lee and J. S. Downie: “Survey of music information\nneeds, uses, and seeking behaviours: Preliminary ﬁndings,”\nISMIR , 2004.\n[2] K. Bischoff, C. S. Firan, W. Nejdl, and R. Paiu: “Can all tags\nbe used for search?,” CIKM , pp. 193–202, 2008.[3] D. Liu, L. Lu, and H.-J. Zhang: “Automatic mood detection\nfrom acoustic music data,” ISMIR , 2003.\n[4] Y . Feng, Y . Zhuang, and Y . Pan: “Popular music retrieval by\ndetecting mood,” SIGIR , 2003.\n[5] J. Skowronek, M. McKinney, and S. van de Par: “A demon-\nstrator for automatic music mood estimation,” ISMIR , 2007.\n[6] Y .-H. Yang, Y .-C. Lin, Y .-F. Su, and H. H. Chen: “A regres-\nsion approach to music emotion recognition,” IEEE Transac-\ntions on Audio, Speech, and Language Processing , V ol. 16,\nNo. 2, pp. 448–457, 2008.\n[7] C. Laurier and P. Herrera: “Automatic detection of emotion\nin music: Interaction with emotionally sensitive machines,”\nHandbook of Research on Synthetic Emotions and Sociable\nRobotics: New Applications in Affective Computing and Ar-\ntiﬁcial Intelligence , pp. 9–32, 2009.\n[8] R. E. Thayer: The biopsychology of mood and arousal , Ox-\nford University Press, 1989.\n[9] D. Turnbull, L. Barrington, and G. Lanckriet: “Modeling mu-\nsic and words using a multi-class nave bayes approach,” IS-\nMIR, 2006.\n[10] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere: “Au-\ntotagger: A model for predicting social tags from acoustic\nfeatures on large music databases,” Journal of New Music Re-\nsearch , V ol. 37, No. 2, pp. 115–135, 2008.\n[11] D. Turnbull, L. Barrington, and G. Lanckriet: “Five ap-\nproaches to collecting tags for music,” ISMIR , 2008.\n[12] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green: “Auto-\nmatic generation of social tags for music recommendation,”\nNIPS , 2007.\n[13] P. Symeonidis, M. Ruxanda, A. Nanopoulos, and\nY . Manolopoulos: “Ternary semantic analysis of social\ntags for personalized music recommendation,” ISMIR , 2008.\n[14] L. Chen, P. Wright, and W. Nejdl: “Improving music genre\nclassiﬁcation using collaborative tagging data,” WSDM , pp.\n84–93, 2009.\n[15] M. Levy and M. Sandler: “A semantic space for music de-\nrived from social tags,” ISMIR , 2007.\n[16] X. Hu and J. S. Downie: “Exploring mood metadata: Re-\nlationships with genre, artist and usage metadata,” ISMIR ,\n2007.\n[17] K. Bischoff, C. S. Firan, W. Nejdl, and R. Paiu: “How do\nyou feel about ”dancing queen”?: deriving mood & theme\nannotations from user tags,” JCDL , pp. 285–294, 2009.\n[18] T.-F. Wu, C.-J. Lin, and R. C. Weng: “Probability estimates\nfor multi-class classiﬁcation by pairwise coupling,” Journal\nof Machine Learning Research , V ol. 5, pp. 975–1005, 2004.\n[19] C. Laurier and P. Herrera: “Audio music mood classiﬁcation\nusing support vector machine,” MIREX Audio Music Mood\nClassiﬁcation contest, ISMIR , 2007.\n662"
    },
    {
        "title": "Calculating Similarity of Folk Song Variants with Melody-based Features.",
        "author": [
            "Ciril Bohak",
            "Matija Marolt"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416516",
        "url": "https://doi.org/10.5281/zenodo.1416516",
        "ee": "https://zenodo.org/records/1416516/files/BohakM09.pdf",
        "abstract": "As folk songs live largely through oral transmission, there usually is no standard form of a song each performance of a folk song may be unique. Different interpretations of the same song are called song variants, all variants of a song belong to the same variant type. In the paper, we explore how various melody-based features relate to folk song variants. Specifically, we explore whether we can derive a melodic similarity measure that would correlate to variant types in the sense that it would measure songs belonging to the same variant type as more similar, in contrast to songs from different variant types. The measure would be useful for folk song retrieval based on variant types, classification of unknown tunes, as well as a measure of similarity between variant types. We experimented with a number of melodic features calculated from symbolic representations of folk song melodies and combined them into a melodybased folk song similarity measure. We evaluated the measure on the task of classifying an unknown melody into a set of existing variant types. We show that the proposed measure gives the correct variant type in the top 10 list for 68% of queries in our data set.",
        "zenodo_id": 1416516,
        "dblp_key": "conf/ismir/BohakM09",
        "keywords": [
            "oral transmission",
            "standard form",
            "song variants",
            "song type",
            "melodic features",
            "variant type",
            "melodic similarity measure",
            "folk song retrieval",
            "classification of unknown tunes",
            "variant type similarity"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nCALCULATING SIMILARITY OF FOLK SONG V ARIANTS WITH\nMELODY-BASED FEATURES\nCiril Bohak, Matija Marolt\nFaculty of Computer and Information Science\nUniversity of Ljubljana, Slovenia\n{ciril.bohak, matija.marolt }@fri.uni-lj.si\nABSTRACT\nAs folk songs live largely through oral transmission, there\nusually is no standard form of a song - each performance of\na folk song may be unique. Different interpretations of the\nsame song are called song variants, all variants of a song\nbelong to the same variant type. In the paper, we explore\nhow various melody-based features relate to folk song vari-\nants. Speciﬁcally, we explore whether we can derive a\nmelodic similarity measure that would correlate to variant\ntypes in the sense that it would measure songs belonging to\nthe same variant type as more similar, in contrast to songs\nfrom different variant types. The measure would be useful\nfor folk song retrieval based on variant types, classiﬁcati on\nof unknown tunes, as well as a measure of similarity be-\ntween variant types. We experimented with a number of\nmelodic features calculated from symbolic representation s\nof folk song melodies and combined them into a melody-\nbased folk song similarity measure. We evaluated the mea-\nsure on the task of classifying an unknown melody into a\nset of existing variant types. We show that the proposed\nmeasure gives the correct variant type in the top 10 list for\n68% of queries in our data set.\n1. INTRODUCTION\nWith the rapid growth of digitization and appearance of\ndigital libraries, folk song archives are (slowly but surel y)\nentering the digital age. More and more folk song and mu-\nsic archives are being digitized, while most new data are\nalready being collected in digital form.\nFolk music is music that lives in oral tradition. It was\ncomposed by everyday people, and has in most cases never\nbeen written down or at least never published. It was mostly\npassed on to the next generation verbally and not in written\nform. Until folk music researchers started to put together\nfolk music collections containing transcriptions, lyrics and\nother metadata, melodies were never put down in scores\nor any other symbolic representation. Several folk song\ncollections are widely available; probably the most well\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .known of them is the Essen Folksong Database [1] that in-\ncludes 20.000 songs, mostly from Germany, Poland and\nChina and minor collections from some other (mostly Eu-\nropean) countries. The digital archive of Finnish Folk Tune s\n[2] is also a well known collection, containing approxi-\nmately 9.000 folk tunes that were published as a collec-\ntion of books between 1898 and 1933 and were digitized\nin 2002-2003. Some other collections are: The American\nFolk Song Collection [3], Australian Folk Songs [4], etc.\nWe conducted our researh on songs from the the Ethno-\nmuse archive [5], which contains folk music and dance col-\nlections of the Institute of Ethnomusicology, Scientiﬁc Re -\nsearch Centre of Slovene Academy of Sciences and Arts.\nThe archive is especially suitable for our purpose, because\nit contains classiﬁcations of songs into variant types , tune\nfamilies and genres.\nBecause folk songs live largely through oral transmis-\nsion, there usually is no standard form of a song. As songs\nare passed through generations, they undergo an evolu-\ntionary process, parts change, they may be dropped and\nother parts may be added. Lyrics, as well as melodies get\nchanged in the process. Each performance of a folk song\nmay be unique and interpretations of the same song rep-\nresent song variants . All variants of a song belong to the\nsame variant type .\nIn this paper, we explore how measures extracted from\nfolk song melodies relate to folk song variants. Speciﬁ-\ncally, we explore whether we can derive a melodic simi-\nlarity measure that would correlate to variant types in the\nsense that it would measure songs belonging to the same\nvariant type as more similar, in contrast to songs from dif-\nferent variant types.\nThe use of music information retrieval tools in folk mu-\nsic research was very limited until recently; a good overvie w\ncan be found in the technical report of the Witchcraft projec t\n[6] as well as in [7, 8].\nOur research is focused on developing an algorithm that\ncalculates the similarity of two songs. In the following\nworks several different approaches of a calculating the sim -\nilarity measure are described. In [9] a method for melodic\nsimilarity of songs is presented; in [10], a method is de-\nscribed which uses each extracted statistical feature for\ntraining of separate self-organising map (SOM). All of the\nmaps are later on used for training of a Supermap, on which\nmelodies with similar features are located closer together ;\nin [11] a comparison of different computational approaches\n597Poster Session 4\nto rhythmic and melodic similarity is made to ﬁnd the fea-\ntures that characterise similarity of Dutch folk songs. A\nrhythmic similarity measure of folk songs is presented in\n[12]. Another similarity measure that uses pitch stability\namong a group of aligned folk songs is described in [13].\nWhich songs are similar, or how much they are alike, is\nnot a precise problem. Not even humans always agree on\nwhether two songs are similar or not, or which two songs\nare most alike. The study of how much experts agree on a\nmanual annotation method for melodic similarity and the\nstudy of melody feature sets is described in [14].\nIn our paper we are proposing a system that uses simple\nmelody-based features for classiﬁcation of songs into vari -\nant types. While most of the previously mentioned papers\ndescribe methods for calculating rhythmic or melodic sim-\nilarities in collections or ﬁnding features that are releva nt\nin calculations of such similarities, our goal is to create a\nretrieval system for melodies, that will help us classify ne w\nunknown songs into already deﬁned variant types.\n2. SIMILARITY MEASURE\nThe main hypotesis of our paper is: It is possible to clas-\nsify folk song melodies into correct variant types based on\nstatistical features of their melodies alone. To either ac-\ncept or reject our hypothesis, we ﬁrst have to answer the\nfollowing questions: What kind of data do we have at our\ndisposal and how much of it? Which features are we going\nto use and how to choose them? Which statistical methods\nshould we use and how to choose them?\nThe goal is to train a classiﬁer that will classify individ-\nual variants into variant types. For this we created pairs\nof songs from Ethnomuse archive. A positive example is a\npair of songs that are from the same variant type; a negative\nexample is a pair of songs from different variant types.\nWe selected 650 folk songs belonging to 40 different\nvariant types from the dataset. The scores for these melodie s\nare available in Sibelius format, which we converted to\nMIDI. The set was split into two subsets: a learning set\nof 600 and an independent test set of 50 songs. The learn-\ning set was again split into two subsets: an attribute se-\nlection learning set , and an attribute selection test set . For\nthe attribute selection learning set we only used songs from\nvariant types with more then 7 variants. From variant types\nwith more then 10 songs, we only used 10 randomly se-\nlected ones. The attribute selection test set was put to-\ngether from 100 randomly selected songs from the learn-\ning set. For the purpose of training the classiﬁer we created\npairs of all songs in each of the attribute selection sets. Fo r\nthese sets we calculated percentages of positive and nega-\ntive examples. The attribute selection learning set consis ts\nof 12.7% positive examples and 87.3% negative examples;\nattribute selection test set consists of 15,48% positive ex -\namples and 84.52% of negative examples.\nFor each melody, we calculated a set of 94 melody-\nbased features with the help of the MIDI Toolbox [15].\nWe analyzed whether these features can be used to com-\npare pairs of melodies and decide whether they belong to\nthe same variant type or not. Because the task involvespairs of melodies, we created pairs of songs, for which we\ncalculated compounded attributes as the quotient and abso-\nlute difference of individual features. All of the calculat ed\nattribute values were normalised and the SMO attribute se-\nlection method [16] used on the attribute selection sets to\nrank the attributes. We found the following attributes to\nbe useful for variant type classiﬁcation (details on the at-\ntributes can be found in [17]):\ncomplebm: the measure is an expectancy based model of\nmelodic complexity based on optimal combination\nof pitch and rhythm-related components calibrated\nin relation to the Essen Folksong Collection, where\nhigher value means higher complexity.\nentropy: relative entropy of note distribution in a matrix\nrepresentation of note events.\nmeteraccent: measure of phenomenal accent synchrony.\nMeter accent is deﬁned as:\nmeterAccent =mean(mh·ma·du)∗(−1)(1)\nwhere vector Metric hierarchy (mh) indicates the\nlocations of notes in the metric hierarchy (meter is\ncalculated as an autocorrelation-based estimate - for\nfurther information see [17]); Melodic accent (ma)\nassigns melodic accents according to the possible\nmelodic contours arising in 3-pitch windows. One\ncan say that the melodic accent will be greater in\nplaces where the pitch changes. Duration accent\n(du) is deﬁned in [19]\ngradus :degree of melodiousness (mean of Gradus suavi-\ntatis) was deﬁned by Euler [18]. Gradus suavitatis\nbases on prime factorisation of note frequency ratios\ndecreased by 1 and summed together with 1:\ngradus suavitatis = 1 +/summationdisplay\np1∈P(pi−1) (2)\nwhere Pis set of all prime factors of frequency ratio\nand note frequency ratios are acquired from nomina-\ntor and denominator matrices. Degree of melodious-\nness (gradus) is mean value of Gradus suavitatis for\nall note intervals:\ngradus =mean(/summationdisplay\nni∈N(ni)) (3)\nwhere Nis set of all note intervals.\ncompltrans: Simonton’s melodic originality score based\non 2nd order pitch- class distribution of classical mu-\nsic derived from a set of music themes.\nThe selected features were used to train a logistic re-\ngression (LR) model. For each pair of melodies, the model\noutputs values between 0 and 1 for each instance; the closer\nthe calculated value is to 1, the more probable it is that the\npair of melodies belongs to the same variant type and vice\nversa, the closer the value is to 0, the lower the chance that\n59810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nthe selected pair of songs is from same variant type. For\nthe calculated values we had to set the threshold, that de-\ntermines when the songs of a selected pair are from same\nvariant type, and when not. The threshold was set so that\nthe F-Measure reached the maximum on the attribute se-\nlection learning set . For later evaluation we have also cal-\nculated the F-Measure on the attribute selection test set .\nFor comparison we have also used the data to build the\nsame model with SVM Regression (SVM) as well as Mul-\ntilayer perceptron (MP). The Table 1 shows that there are\nonly small differences between different machine learning\nmodels and that all the models are better than random clas-\nsiﬁer (RC) in all measures.\nTable 1 . F-Measure, precision and recall values of differ-\nent models, for the attribute selection test set\nMethod F-Measure Precision Recall\nLR 0.2837 0.9888 0.1656\nSVM 0.3396 0.8750 0.2107\nMP 0.3237 0.8661 0.1990\nRC 0.2649 0.8503 0.1600\n3. EV ALUATION AND DISCUSSION\n3.1 Testing the model\nTo evaluate the logistic regression based similarity mea-\nsure on a realistic task, we set up a retrieval system that\ntakes an unknown melody as a query and returns an or-\ndered list of melodies that should belong to the same vari-\nant type as the query. The queries were chosen from the\nindependent test set and were compared to songs in the\nlearning set with the logistic regression classiﬁer trained\nas described previously; its output was used to rank the\nresults.\nTable 2 . Ranks of ﬁrst correct hits according to the pro-\nposed similarity measure.\nRank Number of correct hits\n1st 5\n2nd 10\n3rd or 4th 6\n5th - 20th 18\n21st - 30th 4\n31st or worse 7\nThe ranked list of hits contains 600 songs; the correct\nhitsare those belonging to the same variant type as the\nquery song. For our model, the majority of ﬁrst correct hits\nare ranked 30th or better (in 43 of 50 cases). The overall\nworst ﬁrst correct hit is on 422nd place. Table 2 shows\nwhere ﬁrst correct hits were ranked for the entire test set.\nFor our model 68% of ﬁrst correct hits lie within the top 10;\na random classiﬁer would reach 41%, so this is a signiﬁcant\nimprovement. The average rank of the ﬁrst correct hit is20.60th place, but if we exclude the most divergent results,\nthe average rank of the ﬁrst correct hit is 7.21th place for\nall but the worst 7 songs.\nFigure 1 . 11 point precision averages of test set items and\ntheir mean value.\nAnother measure frequently used for MIR system eval-\nuation is 11 point precision average. This measure is calcu-\nlated as the average precision at recall levels 0.0, 0.1, . . . ,\n0.9, 1.0. For our test set the calculated value of 11 point\nprecision average for songs from the independent test set is\n0.1544 . In Figure 1 the circles represent 11 point precision\naverage measure of each of the test set items. The dashed\nline indicates the mean value for all the test set items. Most\nof the worst cases (those under the mean line in Figure 1)\nare either from variant types with less then 7 variants or\nvariants from bigger variant types that derogate the most.\n3.2 Case study\nThe variant type with the most songs in our data set con-\ntains 163 songs. The best ﬁrst correct hit for a query song\nfrom this variant type is 2nd, while the worst ﬁrst correct\nhit is in the 31st place. 11 point precision average for this\nvariant type is 0.2096 , which is close to 11 point precision\nof the best query song - 0.2494 . Following is the compar-\nison of the best ﬁrst correct hit and worst ﬁrst correct hit\nexamples for this variant type.\n(a) query song\n(b) ﬁrst correct hit song (2nd)\nFigure 2 . Example of a good result (ﬁrst correct hit at 2nd\nplace) for the same variant type as in Figure 3.\nFigure 2 shows an example, where the ﬁrst correct hit\nwas on the 2nd place; the query and the correct hit are\nshown. The reason why this result is ranked so good (it\nwas ranked 2nd) is because not only complebm (the values,\n599Poster Session 4\n5.1332 of query and 5.2631 of ﬁrst hit song, are quite sim-\nilar), meteraccent (pitch in both, query and ﬁrst correct hit\nsong, is not monotonic) and gradus (both songs have quite\nhigh melodiousness) values are very similar with the val-\nues for query song, but also other two features ( entropy and\ncompltrans ) are very similar with values for query song;\nwhich is not true for the previous example.\nFigure 3 shows the worst ﬁrst correct hit example. The\nquery song, its ﬁrst and last correct hits and the ﬁrst hit\nsong on the ranked list returned by our system are given.\nThe main reason why the song in Figure 3(c) was ranked so\nlow, is because of the major differences in complebm (the\nquery song value is 5.0880 , the ﬁrst correct hit song value\nis4.8136 and the last correct hit song value is 3.9494 ),\nmeteraccent andgradus melodic features in comparison to\nthe query (Figure 3(a)); on the other hand complebm and\ncompltrans values of the ﬁrst hit song (Figure 3(d)), are\ncloser to the query song values, then the ﬁrst correct hit\nsong values.\n(a) query song\n(b) ﬁrst correct hit song (31st)\n(c) last correct hit song (592nd)\n(d) ﬁrst hit song (1st)\nFigure 3 . Example of a bad result (ﬁrst correct hit at 31st\nplace) for the variant type with the most examples.\n4. CONCLUSIONS AND FUTURE WORK\nAs we show, there is some correspondence between simple\nstatistical measures calculated on folk song melodies and\nthe classiﬁcation of folk songs into variant types. While\nresults are far from very good and such a basic approach\ncannot be used to build a fully automatic variant type clas-\nsiﬁcation system, the obtained similarity measure is good\nenough to create a retrieval system for melodies of an un-\nknown variant type that will give us list of a few (in our\ncase 10) variant types, that will contain the correct type\nwith high probability (in our case 68%). We also plan\nto combine the obtained melodic similarity measure withlyrics-based similarity measures and to use it for visualiz a-\ntion of folk song melodies in the Ethnomuse archive.\n5. ACKNOWLEDGMENT\nAuthors would like to thank anonymous reviewers for their\nconstructive comments, suggestions and for pointing out\nerrors; they helped make this article better.\nThe work was partly supported by the Slovenian Go-\nvernment-Founded R&D project EthnoCatalogue: creating\nsemantic descriptions of Slovene folk song and music.\n6. REFERENCES\n[1] Helmut Schaffrath. The Essen Folksong Collection. D.\nHuron (ed.), Stanford, CA, 1995. computer database.\n[2] Finnish Folk Tunes. University of Jyvskyl, 2004. URL:\nesavelmat.jyu.ﬁ.\n[3] The American Folk Song Collection, 2004. URL: ko-\ndaly.hnu.edu/home.cfm.\n[4] Australian Folk Songs, 1994. URL:\nhttp://folkstream.com/.\n[5] Grega Strle and Matija Marolt. Conceptualizing the\nEthnomuse: Application of CIDOC CRM and FRBR.\nProceedings of CIDOC2007 , 2007.\n[6] Peter van Kranenburg, J ¨org Garbers, Anja V olk, Frans\nWiering, Louis P. Grijp, and Remco C. Veltkamp. To-\nwards integration of music information retrieval and\nfolk song research. Technical Report UU-CS-2007-\n016, Department of Information and Computing Sci-\nences, Utrecht University, 2007.\n[7] Petri Toiviainen and Tuomas Eerola. Visualization in\ncomparative music research. In COMPSTAT 2006 -\nProceedings in Computational Statistics , pages 209–\n221, 2006.\n[8] Peter van Kranenburg, J ¨org Garbers, Anja V olk, Frans\nWiering, Louis P. Grijp, and Remco C. Veltkamp. To-\nwards integration of mir and folk song research. In Pro-\nceedings of the 8th International Conference on Mu-\nsic Information Retrieval (ISMIR 2007) , pages 505–\n508, Vienna, Austria, September 2007. ¨Osterreichische\nComputer Gesellschaft.\n[9] Rainer Typke. Music Retrieval based on Melodic Sim-\nilarity . PhD thesis, Utrecht University, Netherlands,\nFebruary 2007.\n[10] Petri Toiviainen and Tuomas Eerola. Method for com-\nparative analysis of folk music based on musical fea-\nture extraction and neural networks. In In III Interna-\ntional Conference on Cognitive Musicology , pages 41–\n45, 2001.\n[11] Anja V olk, J ¨org Garbers, Peter van Kranenburg, Frans\nWiering, Louis P. Grijp, and Remco C. Veltkamp.\nComparing Computational Approaches to Rhythmic\n60010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nand Melodic Similarity In Folksong Research. In Proc.\nMCM 2007 , 2007.\n[12] Anja V olk, J ¨org Garbers, Peter van Kranenburg, Frans\nWiering, Remco C. Veltkamp, and Louis P. Grijp.\nApplying rhythmic similarity based on inner met-\nric analysis to folksong research. In Proceedings of\nthe 8th International Conference on Music Informa-\ntion Retrieval (ISMIR 2007) , pages 293–296, Vienna,\nAustria, September 2007. ¨Osterreichische Computer\nGesellschaft.\n[13] J ¨org Garbers, Peter van Kranenburg, Anja V olk, Frans\nWiering, Remco C. Veltkamp, and Louis P. Grijp. Us-\ning pitch stability among a group of aligned query\nmelodies to retrieve unidentiﬁed variant melodies. In\nProceedings of the 8th International Conference on\nMusic Information Retrieval (ISMIR 2007) , pages 451–\n456, Vienna, Austria, September 2007. ¨Osterreichische\nComputer Gesellschaft.\n[14] Anja V olk, Peter van Kranenburg, J ¨org Garbers, Frans\nWiering, Remco C. Veltkamp, and Louis P. Grijp. A\nmanual annotation method for melodic similarity and\nthe study of melody feature set. Proceedings of the 9th\nInternational Conference on Music Information Re-\ntrieval (ISMIR 2008) , September 2008.\n[15] Tuomas Eerola and Petri Toiviainen. Mir in matlab:\nThe midi toolbox. In ISMIR , 2004.\n[16] Ian H. Witten and Eibe Frank. Data Mining: Practi-\ncal Machine Learning Tools and Techniques . Morgan\nKaufmann Series in Data Management Systems. Mor-\ngan Kaufmann, second edition, June 2005.\n[17] Tuomas Eerola and Petri Toiviainen. MIDI Toolbox:\nMATLAB Tools for Music Research . University of\nJyv¨askyl ¨a, Jyv ¨askyl ¨a, Finland, 2004.\n[18] Leonhard Euler. Tentamen novae theoriae musicae .\n1739.\n[19] Richard Parncutt. A perceptual model of pulse salience\nand metrical accent in musical rhythms. Music Percep-\ntion, 11(4):409–464, 1994.\n601"
    },
    {
        "title": "Evaluating and Analysing Dynamic Playlist Generation Heuristics Using Radio Logs and Fuzzy Set Theory.",
        "author": [
            "Klaas Bosteels",
            "Elias Pampalk",
            "Etienne E. Kerre"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417675",
        "url": "https://doi.org/10.5281/zenodo.1417675",
        "ee": "https://zenodo.org/records/1417675/files/BosteelsPK09.pdf",
        "abstract": "In this paper, we analyse and evaluate several heuristics for adding songs to a dynamically generated playlist. We explain how radio logs can be used for evaluating such heuristics, and show that formalizing the heuristics using fuzzy set theory simplifies the analysis. More concretely, we verify previous results by means of a large scale evaluation based on 1.26 million listening patterns extracted from radio logs, and explain why some heuristics perform better than others by analysing their formal definitions and conducting additional evaluations.",
        "zenodo_id": 1417675,
        "dblp_key": "conf/ismir/BosteelsPK09",
        "keywords": [
            "heuristics",
            "radio logs",
            "fuzzy set theory",
            "evaluating heuristics",
            "formal definitions",
            "additional evaluations",
            "listening patterns",
            "large scale evaluation",
            "previous results",
            "formalizing heuristics"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nEVALUATING AND ANALYSING DYNAMIC PLAYLIST GENERATION\nHEURISTICS USING RADIO LOGS AND FUZZY SET THEORY\nKlaas Bosteels\nGhent University, Gent, Belgium\nklaas.bosteels@ugent.beElias Pampalk\nLast.fm Ltd., London, UK\nelias@last.fmEtienne E. Kerre\nGhent University, Gent, Belgium\netienne.kerre@ugent.be\nABSTRACT\nIn this paper, we analyse and evaluate several heuristics\nfor adding songs to a dynamically generated playlist. We\nexplain how radio logs can be used for evaluating such\nheuristics, and show that formalizing the heuristics using\nfuzzy set theory simpliﬁes the analysis. More concretely,\nwe verify previous results by means of a large scale eval-\nuation based on 1.26 million listening patterns extracted\nfrom radio logs, and explain why some heuristics perform\nbetter than others by analysing their formal deﬁnitions and\nconducting additional evaluations.\n1. INTRODUCTION\nIn January 2009, Arbitron and Edison Research measured\nthe popularity of digital music platforms by means of a\nsurvey of 1,858 American people aged 12+ .1They esti-\nmated that 42 million Americans tune to online radio on a\nweekly basis, which is more than twice their number from\n2005, and claim that the number of 12+ year old Ameri-\ncans owning a digital music player increased from 14% in\n2005 to 42% in 2009. They also found that the vast ma-\njority of these people own an Apple iPod or iPhone. Ev-\nidently, the Apple products dominate their market, which\nis commonly attributed to their innovating design and user\ninterfaces. The recent “Genius” feature is a nice example\nof such innovation. Using this feature, users can automat-\nically create coherent playlists by selecting a seed song ,\ni.e., an example of a song of interest, and pressing a sin-\ngle button. Many of the popular online radio stations are\nsimilar in concept. The user supplies one or more seeds,\nand the system generates a corresponding list of tracks that\nis turned into a custom radio station. Hence, automatic\nplaylist generation can be seen as a technology that is, to\nsome extent, responsible for the recent growth established\nby certain digital music platforms, and its commercial im-\nportance is likely to increase further in the near future.\nThis paper is about simple heuristics for automatically\ngenerating playlists. More precisely, we will discuss sim-\n1http://www.arbitron.com/study/digital_radio_\nstudy.asp\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.ple rules of thumb for choosing the song to be played next,\ngiven a set of candidate songs. This set of candidates can\nconsist of all available tracks, but usually it is restricted to\na limited subset. In order to avoid repetition, for instance,\nthe set of candidates has to be restricted to the songs that\nhave not been played yet. A realistic scenario is to select\nthe candidates using some other method, effectively turn-\ning the heuristic into an enhancement rather than a playlist\ngeneration method on its own.\nA very simple way to improve upon random selection,\nis to repeatedly choose the candidate that is most similar to\na given seed song [1]. This playlist generation heuristic is\nsaid to be static because the song sequence is completely\ndetermined from the seed, without taking any additional\nuser input into account. Dynamic heuristics, on the other\nhand, rely on user feedback to dynamically improve the se-\nlection process [2]. For example, the aforementioned static\nheuristic can be made dynamic by letting it pick the song\nthat is most similar to any of the accepted songs, where\nthe set of accepted songs consists of the seed song as well\nas all tracks that were not skipped [3]. When there is no\ngiven seed, the set of accepted songs can initially be empty\nand the next track can be chosen at random until there is\nat least one accepted song. This latter heuristic could eas-\nily be added to any system that returns multiple candidate\nsongs for being played next.\nPutting it in one sentence, we discuss simple dynamic\nplaylist generation heuristics in this paper. In comparison\nwith alternative techniques, such heuristics are interesting\nbecause they (i) are simple and thus easy to compute and\nimplement, and (ii) can easily be added as an enhancement\nto many existing playlist generation systems.\n2. RELATED WORK\nDynamic playlist generation can be seen as a special case\nof the well-known relevance feedback paradigm from in-\nformation retrieval [4]. In this paradigm, the user is asked\nto give explicit feedback by labeling results as either rel-\nevant or irrelevant, which leads to additional information\nthat can be used by the system to reﬁne the search strat-\negy and generate a better list of results. Several rounds\nof feedback can be conducted, each bringing the results\ncloser to the user’s implicit target concept. Hence, dy-\nnamic playlist generation is basically relevance feedback\nwith the returned set of results restricted to one item. In\ncase of this paper, the feedback taken into account is also\nimplicit rather than explicit, but there is no reason to as-\n351Oral Session 4: Music Recommendation and Playlist Generation\nsume that dynamic playlist generation heuristics could not\nbe based on more explicit feedback like “thumbs up/down”\nbuttons instead of skipping behavior.\nOver the past few years, relevance feedback has received\nquite a lot of research attention. In particular, some rele-\nvance feedback techniques have already been applied to\nmusic information retrieval, including training a decision\ntree [5], a vector quantizer [6], and an SVM [7]. Since\nthe number of examples is very low when the returned\nset of results is restricted to one item [2], using these ma-\nchine learning techniques for dynamic playlist generation\nmight be problematic, however. For the custom-tailored\nheuristics described in this paper, this is less of a prob-\nlem. Moreover, their simplicity can be considered an addi-\ntional advantage from a computational and implementation\npoint of view. Furthermore, as we already mentioned in\nthe introductory section, the described heuristics can also\nbe thought of as a reﬁnement that can be added to a more\ncomplex relevance feedback system.\n3. FORMALIZATION\nThe deﬁnition of playlist generation heuristics can be for-\nmalized using fuzzy set theory [8]. In this section, we ex-\nplain this formalization in detail, since we rely on it exten-\nsively in the subsequent sections.\n3.1 Fuzzy Sets\nLetUdenote a universe, i.e., a (crisp) set of considered\nobjects. A fuzzy setFinUis aU→[0,1]mapping that\nassociates a degree of membership F(u)with each element\nufromU[9]. The higher F(u), the moreuis a member\nofF. In particular, ufully belongs to FwhenF(u) = 1 ,\nandF(u) = 0 implies that uis not at all an element of F.\nWe use the notation F(U)for the class of fuzzy sets in U,\nwhich can be regarded a superclass of P(U), the class of\ncrisp sets in U. A(binary) fuzzy relation LinUis a fuzzy\nset inU×U, i.e.,L∈F(U×U)[9].\nThe fuzzy set SimX, withXa crisp set in the (ﬁnite)\nuniverseCof songs to be explored, is the main stepping\nstone towards the fuzzy formalization. It is given by\nSimX(u) = max\nx∈XM(u,x) (1)\nfor allu∈U, whereUis the subset of Cconsisting of all\ncandidate songs. In this deﬁnition, Mis a fuzzy relation in\nCsuch that each relationship degree M(c,d), with (c,d)∈\nC2, corresponds to the degree to which cis similar to d.\nPutting it in words, SimXis a fuzzy set in Usuch that\nSimX(u)can be interpreted as the degree to which uis\nsimilar to any song in X.\nIn order to obtain a crisp set of tracks from a fuzzy song\nset, we rely on the following formal operator:\nX⌉F=/braceleftbigg\nx∈X|F(x) = max\ny∈XF(y)/bracerightbigg\n(2)\nfor allX∈P(C)andF∈F(C), i.e.,X⌉Fis the crisp set\nconsisting of the elements from Xwith the greatest mem-\nbership degree in F. Using this operator, we can formallydeﬁne the dynamic heuristic discussed in the introductory\nsection of this paper as U⌉SimA, withAthe set of all ac-\ncepted songs. In practice, the set U⌉SimAwill be a sin-\ngleton most of the time, but theoretically speaking it can\ncontain up to|U|elements. We can choose one element at\nrandom when|U⌉SimA|>1, however, since each song\nfrom the set can be considered equally suitable for being\nplayed next. In the remainder of this paper, we silently\nassume that this procedure is followed for all introduced\nheuristics, i.e., we will deﬁne the heuristics as crisp sets\nand assume that one element is chosen at random when\nthis set has several members.\n3.2 Operations on Fuzzy Sets\nThe set-theoretic operations complement, intersection, and\nunion can be generalized to fuzzy sets as follows:\n(coNF)(u) =N(F(u)) (3)\n(F∩TG)(u) =T(F(u),G(u)) (4)\n(F∪SG)(u) =S(F(u),G(u)) (5)\nfor eachu∈U, withF,G∈F(U),Nanegator ,Ta\nt-norm , andSat-conorm . We restrict the sheer number of\npossibilities by only considering the widely-used standard\nnegatorNSgiven byNS(x) = 1−xfor allx∈[0,1], the\nthree prototypical t-norms [10] given by\nTM(x,y) = min(x,y) (6)\nTP(x,y) =x·y (7)\nTL(x,y) = max(x+y−1,0) (8)\nfor allx,y∈[0,1], and their duals\nSM(x,y) = max(x,y) (9)\nSP(x,y) =x+y−x·y (10)\nSL(x,y) = min(x+y,1) (11)\nfor allx,y∈[0,1]. In the remainder, we will abbreviate\ncoNSbycosinceNSis the only negator we consider.\nFor this paper, however, we mainly need a generalized\nset-theoretic difference, which can be obtained by deﬁning\n(F\\IG)(u) =NS(I(F(u),G(u))) (12)\nfor everyufromU, withF,G∈F(U)andIanimpli-\ncator . We consider two ways of generating implicators in\nthis paper, namely, S-implicators andR-implicators . The\nS-implicator induced by a t-conorm Sand the standard\nnegatorNSis the [0,1]2→[0,1]mappingISdeﬁned as\nIS(x,y) =S(NS(x),y), for allx,y∈[0,1], and the R-\nimplicator induced by a t-norm Tis the [0,1]2→[0,1]\nmappingITgiven by, for all x,y∈[0,1],IT(x,y) =\nsup{γ∈[0,1]|T(x,γ)≤y}. For the above-mentioned\nprototypical t-norms and the corresponding t-conorms, this\nleads to the following implicators:\nISM(x,y) = max(1−x,y) (13)\nISP(x,y) = 1−x+x·y (14)\nISL(x,y) = min(1−x+y,1) (15)\n35210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nITM(x,y) =/braceleftBigg\n1ifx≤y\nyotherwise(16)\nITP(x,y) =/braceleftBigg\n1ifx≤y\ny\nxotherwise(17)\nITL(x,y) =ISL,NS(x,y) (18)\nfor allx,y∈[0,1].\n3.3 Formal Heuristics\nHaving the operations on fuzzy sets at our disposal, we\ncan incorporate the set Rof all rejected songs by replac-\ningSimAinU⌉SimAwith a set-theoretic expression in\nterms of both SimAandSimR. The heuristic deﬁned as\nU⌉(SimA\\ISimR), for instance, selects the songs that\nare similar to an accepted song but not similar to any re-\njected ones, as illustrated by Fig. 4(a). By taking into ac-\ncount the fact that (U⌉F)⌉F=U⌉Ffor eachF∈F(U),\nwe can easily deﬁne slightly more ﬁne-grained heuristics,\nhowever. Instead of replacing SimAinU⌉SimA, we can\nﬁrst rewrite U⌉SimAas(U⌉SimA)⌉SimAand then re-\nplace only the ﬁrst occurrence of SimA, which effectively\nleads to heuristics of the form (U⌉P)⌉SimA, wherePis a\nset-theoretic expression in SimAandSimR. We call this\nexpressionPthepreselection expression , since it imple-\nments a preselection step that precedes further ﬁltering by\nSimA. As values for P, we consider the set-theoretic ex-\npressions illustrated by Fig. 4(a), Fig. 4(b), Fig. 4(c), which\nleads to the following heuristics:\nHI\na= (U⌉(SimA\\ISimR))⌉SimA (19)\nHb=U⌉SimA (20)\nHI\nc= (U⌉co(SimR\\ISimA))⌉SimA (21)\nFor the value of the parameter IinHI\naandHI\nc, we will\nconsider the ﬁve different implicators discussed in the pre-\nvious subsection, namely, ISM,ISP,ISL,ITM, andITP.\nThe contour plots in Fig. 1 show how the implemented\npreselection strategy varies for the considered implicators.\nFor each preselection expression P, there exists a corre-\nsponding [0,1]2→[0,1]mappingpsuch thatP(u) =\np(SimA(u),SimR(u))for allu∈U. Table 1 lists these\nmappings for all considered (non-trivial) preselection ex-\npressions, and the plots in Fig. 1 each illustrate one of these\nmappings. Essentially, these plots provide a top view of\nthe three-dimensional plots for the [0,1]2→[0,1]map-\npings. More precisely, the lines connect points for which\nthe illustrated mapping yields the same value, leading to a\npartitioning of the [0,1]2square into different areas. The\ndarker the area, the higher the values returned by the map-\nping in this area. Hence, songs ufor which the point\n(SimA(u),SimR(u))is in a dark area are given preference\nby the preselection strategy in question.\nAll previously-introduced playlist generation heuristics\ncan be formalized in this way [8]. In particular, the well-\nperforming heuristic deﬁned as\nFor each candidate song, let dabe the distance to\nthe nearest accepted, and let dsbe the distance topreselection expression [0,1]2→[0,1]mapping\nSimA\\ISMSimR min(x,1−y)\nSimA\\ISPSimR x−x·y\nSimA\\ISLSimR max(x−y,0)\nSimA\\ITMSimR/braceleftBigg\n0 ifx≤y\n1−yotherwise\nSimA\\ITPSimR/braceleftBigg\n0 ifx≤y\n1−y\nxotherwise\nco(SimR\\ISMSimA) max(1−y,x)\nco(SimR\\ISPSimA) 1−y+y·x\nco(SimR\\ISLSimA) min(1−y+x,1)\nco(SimR\\ITMSimA)/braceleftBigg\n1ify≤x\nxotherwise\nco(SimR\\ITPSimA)/braceleftBigg\n1ify≤x\nx\nyotherwise\nTable 1 . Corresponding [0,1]2→[0,1]mappings for the\nconsidered (non-trivial) preselection expressions.\n(a)HISMa\n (b)HISPa\n (c)HISLa\n (d)HITPa\n (e)HITMa\n(f)HISMc\n (g)HISPc\n (h)HISLc\n (i)HITMc\n (j)HITPc\nFigure 1 . Contour plots illustrating the preselection strate-\ngies of the considered instances of HI\naandHI\nc. Every can-\ndidate songucorresponds to a point (SimA(u),SimR(u))\nin each of these plots, and the points in the darker areas are\ngiven preference by the strategy in question.\nthe nearest skipped. If da< ds, then add the candi-\ndate to the set S. From Splay the song with small-\nestda. IfSis empty, then play the candidate song\nwhich has the best (i.e. the lowest) da/dsratio.\nin [2], is equivalent to HITPc. In addition to being more\nconcise and precise, the formal deﬁnition of this heuris-\ntic was also obtained more systematically and is easier to\nanalyse, as we will demonstrate later on in this paper.\n4. BASIC EVALUATION\nThe evaluations described in [2] and [8] are all based on\nthe fairly simplistic assumption that a song is a good ad-\ndition to a playlist when it is from the same genre as the\nseed. For this paper, however, we evaluated the considered\nheuristics using patterns extracted from Last.fm radio logs.\nMore precisely, we looked for sequences of 22 tracks for\nwhich the last two tracks did not both get accepted or re-\njected, i.e., one of them got accepted while the other got\n353Oral Session 4: Music Recommendation and Playlist Generation\nrejected. Tracks were considered accepted when the user\nlistened to more than 50% of them. In order to make sure\nthat the extracted patterns represent genuine user interac-\ntions, we imposed two additional restrictions: (i) at least\n5 and at most 15 of the ﬁrst 20 tracks got accepted, and\n(ii) the last song of the sequence was not the last song of a\nlistening session. In this way, we avoid problems like, e.g.,\nthe user falling asleep or getting distracted while listening\nto the radio station, or the last song being considered a skip\nwhereas the user really just turned off the radio while this\nsong was playing.\nAll of the patterns used for our evaluation were extrac-\nted from log ﬁles produced by Last.fm “playlist” radio sta-\ntions, which basically shufﬂe randomly through user-gene-\nrated lists of tracks. Last.fm provides its users the abil-\nity to create and share playlists, and subscribers can listen\nto these playlists in random shufﬂe mode when they con-\ntain at least 45 playable tracks by 15 different artists. We\nconsidered 1,260,271 patterns extracted from log ﬁles gen-\nerated by such stations, involving 53,768 unique listeners\nand 516,261 different tracks from 70,306 artists.\nThe similarity values used for our evaluation were de-\nrived from tag data using the well-known cosine similarity\nmeasure [4], i.e., songs to which Last.fm users applied the\nsame tags were considered similar to each other. Since the\nvalues from [0,1]obtained in this way can directly be in-\nterpreted as membership degrees, we did not have to apply\nany normalization procedures in order to obtain the fuzzy\nrelationMon which the deﬁnition of SimXis based.\nFor each considered pattern, we made every heuristic\nchoose between the last two tracks based on the acceptance\nhistory for the 20 previous tracks, and counted how many\ntimes they picked the wrong one. More formally, each pat-\ntern corresponds to a (A,R,r,w )tuple, where AandR\nare the sets of accepted and rejected songs, respectively,\nandrandware the right and the wrong choice. The fail-\nure rate for a given heuristic is then obtained by putting\nU={r,w}for each pattern and counting how many times\nwis returned by the heuristic.\nFig. 2 shows the results of our basic evaluation. The\ncircles mark the failure rates, and the lines through them\nrepresent the 95% binomial conﬁdence intervals computed\nby approximating the binomial distribution with a normal\ndistribution. These results roughly conﬁrm the ﬁndings ob-\ntained in [8]. Again, HISLcandHITPcperform signiﬁcantly\nbetter than the other heuristics, although the difference be-\ntweenHITPcandHISPcis just barely signiﬁcant in this case.\nIt still remains unclear why exactly these two heuristics\nperform best, however, which is precisely the motivation\nfor the subsequent sections of this paper.\n5. INCONSISTENT USER PREFERENCES\nWith each pattern considered for our basic evaluation, we\ncan associate two pairs of the form (similarity with ac-\ncepted tracks, % listened), one for the right choice and an-\nother for the wrong one. Similarly, we can also associate\ntwo pairs of the form (similarity with rejected tracks, % lis-\ntened) with each pattern. Fig. 3 shows the distribution of\n43.5 44 44.5 45 45.5 46HITPcHITMcHISLcHISPcHISMcHbHITPaHITMaHISLaHISPaHISMa\nfailure rate (%)Figure 2 . Results of the basic evaluation. The circles mark\nthe failure rates, and the lines represent the 95% binomial\nconﬁdence intervals.\nsimilarity with accepted\n% listened\n020406080100\nsimilarity with rejected\nFigure 3 . Two-dimensional histograms for the (similar-\nity with accepted tracks, % listened) and (similarity with\nrejected tracks, % listened) pairs corresponding to the con-\nsidered patterns. Darker regions contain more pairs, and\nthe thick black lines were obtained using linear regression.\nthese pairs for the considered patterns. The two thick black\nlines in this ﬁgure are the linear regression lines, i.e., the\nbest-ﬁtting straight lines through all of the points in terms\nof least squares. As illustrated by these regression lines,\nusers apparently tend to avoid songs that are similar to the\nskipped tracks in favor of the ones similar to the tracks that\nwere not skipped, which is the main assumption behind\nthe dynamic heuristics discussed in this paper. However,\nthe regression lines are only slightly tilted, suggesting that\nthe user preferences are often driven by reasons unrelated\nto the (computed) similarity with the accepted or rejected\ntracks. We say such preferences are inconsistent , and dis-\ntinguish the resulting inconsistent skipping behavior into\ntwo categories: (i) an inconsistent accept occurs when an\naccepted song is either similar to a rejected track, or not\nsimilar to any of the accepted ones, and (ii) an inconsistent\nreject occurs when a rejected song is similar to an accepted\ntrack or not similar to any rejected tracks. In the context\nof a radio station, for instance, an eclectic user might not\nmind when a song is not similar to any of the already ac-\ncepted songs, leading to an inconsistent accept. On the\nother hand, the user might reject a particular track because\nshe happens to dislike the corresponding artist for certain\n(unmeasurable) reasons, even though the track is very sim-\nilar to the already accepted songs, resulting in an inconsis-\ntent reject which might in turn lead to inconsistent accepts,\nsince the user is likely to accept songs that are similar to\n35410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSimA SimRU(a)SimA\\SimR\nSimA SimRU (b)SimA\nSimA SimRU (c)co(SimR\\SimA)\nSimA SimRU (d)co(SimA)∪SimR\nSimA SimRU (e)SimA∪co(SimR)\nFigure 4 . The dark areas in these Venn diagrams depict the main set-theoretic expressions considered in this paper.\nselections non-selections\nHI\na\nSimA SimRU\nSimA SimRU\nHb\nSimA SimRU\nSimA SimRU\nHI\nc\nSimA SimRU\nSimA SimRU\nFigure 5 . The inconsistent selections and non-selections\narea for all considered heuristics.\nthe inconsistently rejected track.\nNow, by thinking of the fuzzy sets as if they were crisp\nsets, we can intuitively determine how well the preselec-\ntion expressions from the heuristics comply with inconsis-\ntent user preferences. A song ufromUselected by a crisp\npreselection expression Pcan lead to an inconsistent ac-\ncept when either u /∈SimAoru∈SimR. Hence, the\narea corresponding to potential inconsistent accepts for a\npreselection expression Pis the intersection of Pwith the\nset-theoretic expression shown by Fig. 4(d). We call this\narea the inconsistent selections area . Similarly, we can de-\nﬁne the inconsistent non-selections area as the intersec-\ntion of co(P)and the expression shown by Fig. 4(e). The\nlarger the inconsistent selections area, the better the prese-\nlection expression complies with inconsistent accepts, and\nthe larger the inconsistent non-selections area, the better it\ncomplies with inconsistent rejects.\nFig. 5 shows the inconsistent selections and non-selec-\ntions area for all considered heuristics. Judging from this\nﬁgure,HI\nashould perform best when inconsistent rejects\noccur more frequently than inconsistent accepts, HI\ncis ex-\npected to perform best when inconsistent accepts are more\ncommon, and HI\nbshould perform similarly under both cir-\ncumstances. In order to verify these theoretical insights,\nwe conducted some additional evaluations.\n6. ADDITIONAL EVALUATIONS\nBy disregarding some of the extracted patterns, we can\ncontrol the level of inconsistent accepts and rejects. As\nillustrated by Fig. 6(a), for example, the relative number\nof inconsistent accepts can be increased by ignoring all\npatterns for which either sA−lA>0.6or1−sR−\nlR>0.6holds, with (sA,lA)and(sR,lR)a (similarity\nwith accepted tracks, % listened) and a (similarity with re-jected tracks, % listened) pair, respectively, corresponding\nto the pattern in question. Disregarding patterns in this\nway actually removes inconsistent rejects, but this effec-\ntively leads to a higher percentage of inconsistent accepts\nin the obtained dataset. Increasing the level of inconsistent\nrejects can be done analogously. By considering several\ncut-off values, we generated 9 different datasets that grad-\nually move from a high level of inconsistent accepts to a\nhigh level of inconsistent rejects, as illustrated by Fig. 6.\nWe conducted the basic evaluation for every generated\ndataset, which led to the plots shown in Fig. 7. In accor-\ndance with Fig. 5, the dash-dotted line is below the dashed\none in the left part of each of these plots, whereas it is al-\nways above the dashed one in the right part. The solid line,\non the other hand, is roughly symmetrical along the dotted\nvertical divider, which also complies nicely with Fig. 5.\nAlthough their magnitudes vary a lot depending on the\nused value for the implicator I, the differences in perfor-\nmance are clearly visible in each subﬁgure, conﬁrming the\ninsights we obtained by analysing the formal deﬁnitions of\nthe heuristics.\nNow that we linked the performance of the heuristics to\ninconsistent user preferences, we can ﬁnally explain why\nthe failure rate for the best performing instance of HI\ncis\nsigniﬁcantly smaller than those for all instances of HI\nain\nFig. 2. The reason for this is simply that the full collec-\ntion of extracted listening patterns contains more incon-\nsistent accepts than inconsistent rejects, which can easily\nbe demonstrated by reducing the granularity of the two-\ndimensional histograms from Fig. 3 and summing up the\ncounts for certain bins. For instance, we can get a rough\nidea of the number of inconsistent accepts by considering\nmerely four bins and summing up the counts for the bins\nhighlighted in Fig. 8(a). Similarly, we can roughly deter-\nmine the number of inconsistent rejects by summing up the\ncounts for the bins highlighted in Fig. 8(b). The following\nnumbers were obtained in this way: 1,222,094 inconsistent\naccepts and 1,186,155 inconsistent rejects. Moreover, the\ndataset illustrated by Fig. 6(a) consists of 554,614 patterns,\nwhile the one corresponding to Fig. 6(e) is made up of only\n440,171 patterns. Hence, the original dataset indeed seems\nto contain more inconsistent accepts than inconsistent re-\njects. The difference is not that large, however, which ex-\nplains why there is only a very small gap between the per-\nformance of HISLcandHISPain Fig. 2.\nNote that Fig. 7 also illustrates that ISLcan be seen as a\nbalanced compromise between the extremes ISMandITM.\nFor the other implicators, the measured performance tends\nto vary a lot for different heuristics, but ISLrarely leads\nto signiﬁcantly worse performance than any of the other\nconsidered implicators. In Fig. 2 as well in as all empirical\n355Oral Session 4: Music Recommendation and Playlist Generation\n(a) dataset 1\n (b) dataset 3\n (c) dataset 5\n (d) dataset 7\n (e) dataset 9\nFigure 6 . Two-dimensional histograms that illustrate how the 9 generated datasets gradually move from a high level of\ninconsistent accepts to a high level of inconsistent rejects.\n2 4 6 820304050\n(a)ISM\n2 4 6 820304050(b)ISP\n2 4 6 820304050(c)ISL=ITL\n2 4 6 820304050(d)ITP\n2 4 6 820304050(e)ITM\nFigure 7 . Results of the additional evaluations for HI\na(- -),Hb(–), andHI\nc(-·-). The numbers along the horizontal axis\nare dataset identiﬁers, while the vertical axis shows failure rate percentages.\nresults described in [8], HISLaandHISLcperform at least\nas well as all other instances of HI\naandHI\nc, respectively.\n7. CONCLUSION AND FUTURE WORK\nThe mathematical apparatus from the theory of fuzzy sets\nproves to be very convenient for deﬁning dynamic playlist\ngeneration heuristics. Using the described fuzzy frame-\nwork, we obtained deﬁnitions that are not only systematic\nand both concise and precise, but also intuitively clear and\neasy to analyse. We relied on this latter beneﬁt to relate the\nperformance of the considered heuristics to inconsistent\nuser preferences. More precisely, we established that HI\na\nperforms best when inconsistent rejects occur more fre-\nquently than inconsistent accepts, that HI\nccan be expected\nto perform best when inconsistent accepts are more com-\nmon, and that HI\nbperforms similarly under both circum-\nstances. We clearly conﬁrmed these theoretical insights by\nmeans of a new methodology for evaluating playlist gener-\nation heuristics based on listening patterns extracted from\nradio logs, which allowed us to conduct accurate experi-\nments using massive amounts of data.\nSince we mainly focussed on comparing the heuristics\nwith each other in this paper, it still remains largely un-\nclear to what extent they can improve the performance of a\nparticular playlist generation system. Future work should\ntry to measure the performance impact of the considered\nheuristics on speciﬁc playlist generations systems, and com-\npare them with potential alternatives. In order to obtain a\nfairer comparison, the underlying fuzzy relation Mcould\nthen be based on a more advanced similarity measure than\nsimple tag-based cosine similarity.\n8. ACKNOWLEDGEMENTS\nThe ﬁrst author would like to thank the Fund for Scientiﬁc\nResearch–Flanders (FWO) for funding this research.\n(a) inconsistent accepts\n (b) inconsistent rejects\nFigure 8 . Categorization of certain bins from the coarse-\ngrained two-dimensional histograms.\n9. REFERENCES\n[1] B. Logan. Content-based playlist generation: Ex-\nploratory experiments. In Proc. ISMIR Intl. Conf. on\nMusic Info. Retrieval , 2002.\n[2] E. Pampalk, T. Pohle, and G. Widmer. Dynamic\nplaylist generation based on skipping behavior. Proc.\nISMIR Intl. Conf. on Music Info. Retrieval , 2005.\n[3] B. Logan. Music recommendation from song sets. In\nProc. ISMIR Intl. Conf. on Music Info. Retrieval , 2004.\n[4] H. Blanken, A. de Vries, H. Blok, and L. Feng, editors.\nMultimedia retrieval . Springer, 2007.\n[5] S. Pauws and B. Eggen. PATS: Realization and user\nevaluation of an automatic playlist generator. In Proc.\nISMIR Intl. Conf. on Music Info.Retrieval , 2002.\n[6] K. Hoashi, K. Matsumoto, and N. Inoue. Personaliza-\ntion of user proﬁles for content-based music retrieval\nbased on relevance feedback. In Proc. ACM Intl. Conf.\non Multimedia , 2003.\n[7] M. Mandel, G. Poliner, and D. Ellis. Support vector\nmachine active learning for music retrieval. Multime-\ndia Systems , 12:3–13, 2006.\n[8] K. Bosteels and E. Kerre. A fuzzy framework for deﬁn-\ning dynamic playlist generation heuristics. Fuzzy Sets\nand Systems . To appear.\n[9] L. Zadeh. Fuzzy sets. Information and Control , 8:338–\n353, 1965.\n[10] E. Klement, R. Mesiar, and E. Pap. Triangular norms .\nKluwer Academic Publishers, 2000.\n356"
    },
    {
        "title": "Integrating Musicology&apos;s Heterogeneous Data Sources for Better Exploration.",
        "author": [
            "David Bretherton",
            "Daniel A. Smith",
            "Monica M. C. Schraefel",
            "Richard Polfreman",
            "Mark Everist",
            "Jeanice Brooks",
            "Joe Lambert"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415168",
        "url": "https://doi.org/10.5281/zenodo.1415168",
        "ee": "https://zenodo.org/records/1415168/files/BrethertonSSPEBL09.pdf",
        "abstract": "Musicologists have to consult an extraordinarily heterogeneous body of primary and secondary sources during all stages of their research. Many of these sources are now available online, but the historical dispersal of material across libraries and archives has now been replaced by segregation of data and metadata into a plethora of online repositories. This segregation hinders the intelligent manipulation of metadata, and means that extracting large tranches of basic factual information or running multi-part search queries is still enormously and needlessly time consuming. To counter this barrier to research, the “musicSpace” project is experimenting with integrating access to many of musicology’s leading data sources via a modern faceted browsing interface that utilises Semantic Web and Web2.0 technologies such as RDF and AJAX. This will make previously intractable search queries tractable, enable musicologists to use their time more efficiently, and aid the discovery of potentially significant information that users did not think to look for. This paper outlines our work to date.",
        "zenodo_id": 1415168,
        "dblp_key": "conf/ismir/BrethertonSSPEBL09",
        "keywords": [
            "Musicologists",
            "heterogeneous sources",
            "online availability",
            "historical dispersal",
            "segregation",
            "metadata",
            "faceted browsing",
            "Semantic Web",
            "Web2.0 technologies",
            "modern faceted browsing interface"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nINTEGRATING MUSICOLOGY’S HETEROGENEOUS DATA \nSOURCES FOR BETTER EXPLORATION \nDavid Bretherton, Daniel Alexander Smith, mc schraefel,  \nRichard Polfreman, Mark Everist, Jeanice Brooks, and Joe Lambert \nUniversity of Southampton, Southampton, UK, SO17 1BJ \nD.Bretherton@soton.ac.uk; {ds, mc}@ecs.soton.ac.uk;  \n{R.Polfreman, M.Everist, L.J.Brooks}@soton.ac.uk; jl2@ecs.soton.ac.uk \nABSTRACT \nMusicologists have to consult an extraordinarily hetero-\ngeneous body of primary and secondary sources during all stages of their research . Many of these sources are \nnow available online, but the historical dispersal of mate-rial across libraries and arch ives has now been replaced \nby segregation of data and metadata into a plethora of online repositories. This segregation hinders the intelli-gent manipulation of metadata, and means that extracting large tranches of basic factual information or running multi-part search queries is still enormously and need-lessly time consuming. To counter this barrier to re-search, the “musicSpace” project  is experimenting with \nintegrating access to many of musicology’s leading data \nsources via a modern faceted  browsing interface that util-\nises Semantic Web and Web2.0 technologies such as RDF and AJAX. This will make previously intractable search queries tractable, enable  musicologists to use their \ntime more efficiently, and aid the discovery of potentially significant information that users did not think to look for. This paper outlines our work to date.  \n1. INTRODUCTION \nA significant barrier to the research endeavours of musi-\ncologists is the sheer volume of potentially relevant in-formation that has accumulated over centuries. Research-ers once faced the daunting prospect of manually scour-ing through seemingly endless primary and secondary sources in order to answer the basic whats, wheres and \nwhens of musicology, particularly when making lists of people or repertoire  according to specific criteria. Many \nof the sources needed to a ddress these queries are becom-\ning available online. Yet th e dramatic increase in the \nonline availability of data, the variety of data subjects, the growing number of data providers, and, moreover, the inability of current mainstream search tools to manipulate \nthe associated metadata in useful ways, means that ex-\ntracting large tranches of ba sic factual information (e.g. \nmanuscripts once owned by “a,” opera roles performed by “b”) or running multi-part  search queries (e.g. com-\nposers from place “c” that were  active during decade “d”) \nis still enormously and needlessly time consuming.  \nAccordingly, the “musicSpace” project \n<http://www.mspace.fm/projects/ musicspace> is exploit-\ning Semantic Web [1] and Web2.0 technologies to de-velop an experime ntal innovative search interface that \nintegrates access to some of musicology’s largest and most significant online data and metadata repositories, including the British Library Music Collections cata-logue, the British Library Sound Archive catalogue, Ce-cilia, Copac, Grove Music Online, Naxos Music Library, RILM, and RISM UK and Ireland. We anticipate that in-tegrating heterogeneous metadata sources into one ex-ploratory search user inte rface will allow our users to \nspend their research time more efficiently, make previ-ously intractable search queries tractable, and ultimately open up new avenues for musicological study.  \nmusicSpace is exploring and developing numerous \nmethods for enhancing and generating additional meta-\ndata from our data partners’ particularly heterogeneous \ndata sets, and a primary focus is the development of web-based UIs and the longitudinal analysis of their effects on musicological scholarship and human-computer interac-tion. This distinguishes our work from that of  previous notable projects concerned with music data source integration, such as Variations2 <http://variations2.indiana.edu> and EASAIER <http://www.easaier.org> [2 , 3]. The “mSpace” frame-\nwork and interaction layer of musicSpace has been de-signed and evaluated [4, 5] specifically to support multi-\nple browsing and exploratory search tactics that go be-yond common keyword search. Our user interface gives the provenance of all records, and is designed not only to help musicologists discover relevant resources, but also to enable them to go from mu sicSpace to those resources \nin their original context in a single click. Beyond these core features, there are numerous support services based on related usability research to assist with collecting,  \n \nPermission to make digital or hard copi es of all or part of this work fo r\npersonal or classroom use is grante d without fee provided that copies\nare not made or distributed for profit or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page. \n© 2009 International Society for Music Information Retrieval  \n27Oral Session 1-A: Knowledge on the Web\n  \n \n \nFigure 1 . The musicSpace interface in use.  \n \norganising, exporting, and sharing information relevant to a particular query. It should also be noted that as mu-sicSpace is a Web2.0 application, a web browser is all that is required to access the interface, a screenshot of \nwhich is given in Figure 1.  \nIn this paper we give an overview of our work so far \nand outline the findings of our initial trial of the music-Space browser interface. To begin, we review the motiva-tion for our approach to supporting musicological knowl-edge building. \n2. MOTIVATION: BARRIERS TO EFFICIENCY \n2.1 Database Heterogeneity \nThe digitisation of musicology’s central resources has \nrevolutionised the research process, yet dispersal of mate-rial across numerous libraries and archives has now been replaced by segregation of data  into a plethora of discrete \nand disparate online database resources. These are usu-ally segregated according to me dia type (text, image, au-\ndio, video), date of publication, subject, language, and/or copyright holder. Yet typical musicological research cuts across these artificial divisi ons, meaning that musicolo-\ngists are routinely forced to  consult an extraordinarily \nheterogeneous body of online data repositories. In short, a significant amount of valuable research time is ex-pended in establishing basic factual information, not   \n because the data is unavailable, but because a lack of da-tabase integration requires extensive manual collation of discovered data. This problem of heterogeneity is exacer-\nbated by the fact that search  interfaces to data providers’ \ncontent remain almost universally rooted in the now somewhat dated ‘textbox-based’ search paradigm. Not only does the current situation mean that users’ research \ntime is used inefficiently, but  it also means that large, \ncomplex data queries are essentially intractable.  \nThese barriers can be a major disadvantage at any \nstage of the research proce ss. For example, a musicolo-\ngist trying to mould an inchoate thought about Monte-verdi’s madrigals into a well-formed research question would need to execute the same keyword searches sev-\neral times each because there are several relevant data sources. Similarly, because of the segregation of data into \ndisparate, discreet databases and the limitations of cur-rently deployed search inte rfaces, real-world multi-part \nqueries such as “which scri bes have created manuscripts \nof Monteverdi’s works, and which other composers’ works have they inscribed?” or “which singers have re-corded the operas that Mo zart composed during the \n1780s, what other operatic roles have they taken, and where can I get hold of their recordings?” have to be broken down into their component parts, queried sepa-rately using multiple data sources, and finally collated, all of which can take hours or even days.  \nRecently, a number of academic publishers, including \nOxford University Press (with Oxford Music Online \n2810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \n<http://www.oxfordmusiconline.com>) and Alexander \nStreet Press (with Alexande r Street Press Music Online \n<http://muco.alexanderstreet.com>), have recognised the benefits of integrating their musicological data sources [6, 7]. However, because th eir portals only provide ac-\ncess to their own data repositories, and because their in-terfaces rely on existing te xtbox-based search technol-\nogy, their work only takes us partway towards overcom-ing the barriers to research highlighted above; there re-mains a pressing need for further integration of data sources and better interaction support for more diverse search paradigms.  \n2.2 “Intractable” Queries \nThe musicSpace team includes musicologists who spe-\ncialise in four pilot res earch areas: Monteverdi re-\ncordings, Schubert’s songs, nineteenth-century opera buffa, and twentieth-century electroacoustic music. At \nthe start of the project we asked our musicologists for ex-amples of queries that they considered intractable (or, \nmore specifically, not readily tractable) using the current \nsearch interfaces of our data providers, such that they had \nlargely given up on a particular line of enquiry, and which they hoped that musicSpace would be able to fa-cilitate. The list of queries suggested included:  \nA. Which scribes have created manuscripts of a \ncomposer’s works, and which other composers’ works have they inscribed?  \nB. Which performers have recorded Monteverdi’s \nmadrigals, and what else did they record in the \nsame years?  \nC. Which poets have had their poems set as songs \nby Schubert, which other song composers have also set them, and where can I get recordings of these settings?  \nD. Which singers have sung the role of Malatesta in \nDon Pasquale , and what else have they sung?  \nE. Which comic operas were composed in the nine-\nteenth century and premiered in the twentieth?  \nF. Which electroacoustic works were published \nwithin five years of their premier?  \nIt will be noted that all the above queries have multiple \nparts, and, therefore, if one were to use current search interfaces, one would have to break them down into their component queries and manually collate the results. There are several further obstacles to tractability. Queries B, C, D and F call (in particul ar) for several data sources \nto be consulted (for Queries B and D, for example, one would want to consult both the Naxos Music Library and the British Library Sound Archive catalogue), and so data source integration would clearly be beneficial in these cases. In addition, increased metadata granularity is a necessary prerequisite for the tractability of Queries A, C, D and F (for example, in Query A one would rely on metadata in RISM, yet although it is possible to use RISM’s interface to search by  “Person,” it is not possible \nto further restrict this  to  “Composer” or “Scribe”). Fi-\nnally, in addressing Queries C, E and F one would neces-sarily wish to consult the works lists in Grove Music \nOnline. However, because these works lists are not \nmarked up semantically, a system to generate relevant metadata from the raw data is needed (this particular is-sue is currently being addre ssed by musicSpace, and will \nbe reported on at a later date).  \n3. EXPERIMENTAL SOLUTIONS: APPARENT \nINTEGRATION \nThere is at least one seemingly obvious solution to the \nabove query dilemmas: enable integrated real-time query-ing over all the available metadata, and enable people to use that metadata to guide their queries. The associated \nissues for this solution also imply that all data that could be construed as useful, even if buried in the database re-cords, is extracted in some wa y, and that, similarly, there \nis an interaction approach that will enable this metadata to be explored effectively to formulate the kinds of rich compound queries described above.  \nTo this end, we have take n a dual approach to address-\ning this exploration problem: designing back-end services to integrate (and, where n ecessary, surface) available \n(meta)data for exploratory search; and providing a front-end interface to support rich exploratory search interac-\ntion. We discuss these components below.  \n3.1 Multi-Source Integration  \nDespite advances in the development of protocols for \nshareable metadata in the form of the Open Archives Ini-tiative <http://www.openarchives.org> [8], federated search [9], and, more recently, the application of Seman-tic Web technologies to the domain of music [10, 11], only a very small number of musicSpace’s data partners offer such systems for the harvesting of metadata. This is typically either because funds  are presently unavailable to \nmeet the costs of implementi ng such systems, or, in the \ncase of some data providers, because metadata is consid-ered to be as much of an intellectual property asset as \ndata content itself. Hence our data partners’ data sets are currently provided to us manually.  \nWe have thus taken a purpose-driven approach to uni-\nfying the metadata from our data partners, which is sup-plied adhering to a number of different schemas and seri-\nalisations (MARCXML, MODS XML, custom MARC, and source-specific XML). In order to unify these sources for the purposes of cross-source exploration, we have created static mappings from the schemas used by each data provider to a two-level hierarchy based on metadata type. The upper level of the hierarchy includes, for example, “Person” and “Score,” while the sub-level respectively adds granularity to “Composer” and “Manu-script Score” (among other possibilities). In some cases we were able to directly ma p a record field to our type \nhierarchy, while in other cases some light syntactic and/or semantic analysis was performed on the source data. For example, some sources denote a person with \n29Oral Session 1-A: Knowledge on the Web\n  \n \ntheir name, followed by their role in that record, e.g. “J. \nS. Bach (composer).” In this case we extract the name and role as two individual related facts to allow us to as-sociate “J. S. Bach” as “Com poser” in the record, rather \nthan simply “J. S. Bach (composer)” as “Person.” This pre-processing of the metadata adds granularity to the source data and allows richer filtering and exploration through the browsing interface. We developed a tool to \nmap the imported data to an RDF representation of our type hierarchy. By using RDF for the integrated set of data, we can make use of the many benefits of Semantic Web technologies, one of which is the facility to create multiple files of RDF at different times and using differ-ent tools, assert them into a single graph of a knowledge base, and query all of the asserted files as a whole.  \nOne of the challenges in aligning heterogeneous data \nsources is that of entity co-reference. It is rare that data providers share identifiers for entities, and as such, we have to perform co-reference mapping ourselves. For the musicological data we are aligning in musicSpace, a \nstraightforward string matching system is appropriate to match entities across sources; we use Alignment API [12], which uses Wordnet. To ensure greater confidence in these matches, we have developed a semi-automated system that enables musicol ogists to check the mappings \nand inform the system of an y changes that need correct-\ning. Whenever a mapping is automatically performed, our system adds the mapping to a gazetteer, documenting the two strings that were matched along with a small amount of contextual metadata from both records to aid understanding. The gazetteer is then ordered by confi-dence, so that a musicologist  – with reference to the Li-\nbrary of Congress Authorities website <http://authorities.loc.gov> – can check over the low-\nconfidence mappings carefully , update the gazetteer (ei-\nther to remove the mapping, alter it, or provide a re-placement), and inform the co -reference software of the \nchanges. By using this approach we can be sure that the data sources are aligned properly, and that any updates from our data partners will re-use the manually corrected gazetteers.  \nBecause of the legacy issues  that many of our data \npartners have to contend with, there are inevitably short-comings and inconsistencies in their database structures, schemas, and records. But by using gazetteers in the string matching process, adding contextual metadata, and increasing granularity as r ecords are imported, we are \nable to negate any such data quality issues. In addition, our approach means that we do not have to maintain cop-ies of our data partners’ databases for ourselves; rather, we provide a user interface se rvice that provides a single \npoint of entry to our data partners’ repositories.  \n3.2 User Interface  \nData sources integrated into  musicSpace are explored via \na customised version of the “mSpace” faceted browser [4, 5], which provides a s calable web-based faceted \nbrowsing interface for large-scale data sets and utilises the AJAX client-server query mechanism to improve re-sponse times. Faceted browsing is an alternative com-plementary search paradigm to keyword searching, the latter currently being the most commonly deployed form of large-scale data explora tion. The faceted interface cus-\ntomisation used by musicSpace presents columns that list \nattributes from a number of facets of the data, such as “Date,” “Musical Work,” “C omposer,” and “Genre,” al-\nlowing the user to make selec tions in these facets in order \nto filter down results. The interf ace is reactive, in that the \nlists of facets are updated every time a selection is made, so that subsequent choices are limited to those that would yield results.  \n \n \nFigure 2.  Scribes associated with the composer “Monte-\nverdi, Claudio.”   \n \n \n \nFigure 3.  Composers associated with the scribe “Immyns, \nJohn.”   \nThe faceted and reactive na ture of the interface en-\nables complex queries to be addressed. Let us consider the query “which scribes have created manuscripts of Monteverdi’s works, and which other composers’ works have they inscribed?” In Figure 2, the musicSpace inter-face is showing three f acets: “Composer,” “Copy-\nist/Scribe,” and “Manuscript Score.” The selection “Mon-teverdi, Claudio” in “Composer” has been made, as well as “Immyns, John” in “Copyist/Scribe,” and the interface has filtered the results in “Manuscript Score” to a single record that matches these selections: “Giovinetta pianta, La.” Following from this interaction, in Figure 3 the user has dragged the column “Copyist/Scribe” leftwards, so that the selection “Immyns, John” now filters on the “Composer” column, as well as the “Manuscript Score” column, so that the user can  see works by other compos-\ners that had John Immyns as the scribe. \n3010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \n3.3 Saving, Exporting, and Sharing Findings  \nEach interaction with the musicSpace interface generates \na specific URL that, when re-e ntered into a web browser \nat a later stage, will return users to exactly that same point in the data exploration process. Thus users can pause and resume their research at any time by using the bookmarking feature common to all web browsers, and, moreover, they can save, share, and disseminate their findings with colleagues, students, and the wider internet by using Web2.0 services such as del.icio.us, Facebook and StumbleUpon, all of whic h can be accessed by click-\ning the appropriate icon in the musicSpace interface. Ex-\nporting of findings via email is also supported. In addi-tion, musicSpace has the fac ility to allow users to access \nand export metadata as RDF (using the Music Ontology <http://musicontology.com> [11] as a data model), but licensing restrictions with our data partners currently pre-vent us from doing so for all data sets.  \n4. EVALUATION \nSince the mSpace UI has been  evaluated for exploratory \nsearch usability in a variety of contexts, our main focus in testing the musicSpace application is its impact on re-search: how well is it supporting the kinds of queries mu-sicologists want it to enable? And, likewise, what new kinds of research questions, as yet unanticipated, may it enable? Towards answering these questions, we have re-cently completed an early pilot study. We describe our findings below. While these ar e early stage tests, our in-\ntention in outlining our findi ngs here is to have knowl-\nedge of our approach and preliminary results available within the Music IR community in order to enhance en-gagement with the project.  \n4.1 First Phase  \nA version of the musicSpace interface was released inter-\nnally to a team of six musicologists for an initial period of testing and evaluation on 29 April 2009, and their feedback was very encourag ing. Although this initial re-\nlease did not integrate our full spread of data sources, testers nevertheless reported significant improvements with search speed and ease:  \n• “All the information showed up very quickly, \nand it was easy to find material. It was really good to have different kinds of material in the same place.”  \n• “[musicSpace offers] a sp eedier way to research \ncrossed search pathways.”  \n•  “Excellent interface – very simple to under-\nstand.”  \nTesters were also impressed with the way that music-\nSpace’s faceted interface allowed for browsing around a subject and for instantaneous paradigmatic shifts in search focus:  \n• “I would recommend musicSpace for its ability \nto manipulate queries in order to get results that you wouldn’t otherwise be able to get [without \nstarting over].”  \n• “I liked the ability to explore around a topic \nonce I’d identified something of interest.”  \n• “The ability to switch columns around and add \nnew columns was most useful.”  \nAside from these early hoped-for indications that mu-\nsicSpace will provide a quicker and more flexible way to explore a variety of musicological data sources, testers also reported that increased search data granularity (as \ncompared to that of our data  partners’ search interfaces) \nwas a substantial benefit. For example, a number of test-ers were pleased by musicSp ace’s facility to browse by \nopera character: \n• “[Without using musicSp ace] it would not be at \nall easy to do a character search. You would have to use printed reference books like Pipers  \nEnzyklopädie des Musiktheaters  [13], but even \nthis does not have an index of characters, so you’d have to look at the entry for each opera and draw up character lists by hand. You would also have to know what you were looking for before you started out!”  \n• “I used musicSpace to explore how many operas \nhave a character named Alceste. This informa-tion simply isn’t get-at-able using other search interfaces – you’d have to sort through the in-formation on your own.”  \nThere was similar enthusiasm for musicSpace’s ability \nto browse by scribe and the former owner of manuscripts.  \n4.2 Future Phases  \nOver the coming months there will be incremental re-\nleases of musicSpace, each expanding the data set, refin-ing our data mappings, and polishing the UI. This process will culminate in a broader pub lic release towards the end \nof 2009, which will enable us to assess its real-world ef-\nficacy as a research tool.  \n5. CONCLUSION \nEarly results from our testi ng of musicSpace’s ability to \nenable rapid and effective e xploratory search across het-\nerogeneous musicological sources are promising. Our testers clearly appreciated th e speed gains of integrating \ndata sources; in fact the only recurring negative com-ments from testers during our initial period of evaluation concerned their desire to see still more data repositories integrated into musicSpace. In  addition to data source in-\ntegration, both increased data granularity and the flexibil-ity of faceted browsing were found to be very  beneficial. \nThese three features enabled te sters to explore data in a \nway that had not previously been possible, and a number of intractable queries were indeed made tractable.  \nIn his keynote address to this conference in 2005, \nNicholas Cook predicted that “working with larger data sets will open up new areas of musicology” [14]. But if \n31Oral Session 1-A: Knowledge on the Web\n  \n \nCook’s prediction is to be realised, then increasing the \nsize and number of data sets that musicologists work with both demands and allows for better systems to integrate those data sets, and also for far more sophisticated sys-tems for manipulating data. To this end, our research demonstrates a potentially powerful approach for helping musicologists to deal inte lligently and productively with \nlarge and heterogeneous data sets. We believe that mu-sicSpace will allow musicologi sts to find the information \nthey need more easily, and to discover information that they did not think to look for. In so doing, it may also en-courage additional speculative – but potentially fruitful – searches, thus enabling the discovery of new knowledge.  \n6. ACKNOWLEDGEMENTS \nThe musicSpace project is funded by the Arts and Hu-\nmanities Research Council <http://www.ahrc.ac.uk>, the \nEngineering and Physical Sciences Research Council <http://www.epsrc.ac.uk>, and th e Joint Information Sys-\ntems Committee <http://www.jisc.ac.uk>. Our data  partners include the British Library  <http://www.bl.uk>, the British Library Sound  Archive <http://www.bl.uk/nsa>, Cecilia  <http://www.cecilia-uk.org>, Copac <http://copac.ac.uk>, Grove Music Online (OUP) <http://www.oxfordmusiconline.com>, Naxos Music Li-brary <http://www.naxosmusiclibrary.com>, RILM <http://www.rilm.org>, and RISM UK and Ireland <http://www.rism.org.uk>.  \n7. REFERENCES \n[1] T. Berners-Lee, J. Hendler, and O. Lassila: “The \nSemantic Web,” Scientific American , Vol. 284, No. \n5, pp. 34–43, 2001.  \n[2] J. W. Dunn, D. Byrd, M. Notess, J. Riley, and R. \nScherle: “Variations2: Retrieving and Using Music in an Academic Setting,” Communications of the \nACM , Vol. 49, No. 8, pp. 55–58, 2006. \n[3] C. Landone, J. Harrop, and J. Reiss: “Enabling \nAccess to Sound Archives through Integration, Enrichment and Retrieval: the EASAIER Project,” Proceedings of the Eighth International Conference \non Music Information Retrieval , pp. 159–160, 2007.  \n[4] mc schraefel, D. A. Smith, A. Owens, A. Russell, C. \nHarris, and M. L. Wilson: “The Evolving mSpace Platform: Leveraging the Semantic Web on the Trail of the Memex,” presented at Hypertext , Salzburg, 6–\n9 September 2005.  \n[5] mc schraefel, M. L. Wilson, A. Russell, and D. A. \nSmith: “mSpace: Improving Information Access to Multimedia Domains with Multimodal Exploratory Search,” Communications of the ACM , Vol. 49, No. \n4, pp. 47–49, 2006.  [6] L. Macy: “Letter from the Editor,” Oxford Music \nOnline , March 2008 <http://www.oxfordmusic \nonline.com/public/page/letter_08>.  \n[7] A. Hall: “Alexander Street Press: New \nDevelopments,” presented at the Academic Music \nLibrarians’ Seminar , Birmingham Conservatoire, 21 \nMay 2009.  \n[8] C. Lagoze, and H. Van de Sompel: “The Open \nArchives Initiative: Building a Low-Barrier Interoperability Framework,” Proceedings of the 1st \nACM/IEEE-CS Joint C onference on Digital \nLibraries , 2001, pp. 54–62.  \n[9] C. N. Cox, ed.: Federated Search: Solution or \nSetback for Online Library Services , Haworth \nInformation Press, Binghamton NY, 2007. \n[10] C. Lai, I. Fujinaga, D. Descheneau, M. Frishkopf, J. \nRiley, J. Hafner, and B. McMillan: “Metadata Infrastructure for Sound Recordings,” Proceedings \nof the Eighth International Conference on Music Information Retrieval , pp. 157–158, 2007.  \n[11] Y. Raimond, S. Abdallah, M. Sandler, and F. \nGiasson: “The Music Ontology,” Proceedings of the \n8th International Conference on Music Information Retrieval , 2007, pp. 417–422.  \n[12] J. Euzenat: “An API for Ontology Alignment,” \nProceedings of 3rd Inte rnational Semantic Web \nConference , pp 698–712, 2004.  \n[13] C. Dahlhaus, and S. Döhring, eds: Pipers \nEnzyklopädie des Musiktheaters: Oper, Operette, Musical, Ballet , 7 Vols, Piper, Munich, 1986–1997.  \n[14] N. Cook: “Towards the Complete Musicologist,” \nProceedings of the Sixth International Conference \non Music Information Retrieval , 2005.  \n32"
    },
    {
        "title": "An Efficient Multi-Resolution Spectral Transform for Music Analysis.",
        "author": [
            "Pablo Cancela",
            "Martín Rocamora",
            "Ernesto López"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416788",
        "url": "https://doi.org/10.5281/zenodo.1416788",
        "ee": "https://zenodo.org/records/1416788/files/CancelaRL09.pdf",
        "abstract": "In this paper we focus on multi-resolution spectral analysis algorithms for music signals based on the FFT. Two previously devised efficient algorithms (efficient constantQ transform [1] and multiresolution FFT [2]) are reviewed and compared with a new proposal based on the IIR filtering of the FFT. Apart from its simplicity, the proposed method shows to be a good compromise between design flexibility and reduced computational effort. Additionally, it was used as a part of an effective melody extraction algorithm.",
        "zenodo_id": 1416788,
        "dblp_key": "conf/ismir/CancelaRL09",
        "keywords": [
            "multi-resolution",
            "spectral analysis",
            "FFT",
            "IIR filtering",
            "music signals",
            "efficient algorithms",
            "design flexibility",
            "reduced computational effort",
            "melody extraction",
            "part of algorithm"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nANEFFICIENT MULTI-RESOLUTION SPECTRAL TRANSFORM\nFOR MUSICANALYSIS\nPabloCancela Mart ´ın Rocamora ErnestoL ´opez\nUniversidad de la Rep ´ublica, Instituto de Ingenier ´ıa El´ectrica,Montevideo, Uruguay\n{pcancela,rocamora,elopez }@fing.edu.uy\nABSTRACT\nIn this paper we focus on multi-resolution spectral anal-\nysis algorithms for music signals based on the FFT. Two\npreviously devised efﬁcient algorithms (efﬁcient constan t-\nQtransform[1]andmultiresolutionFFT[2])arereviewed\nand compared with a new proposal based on the IIR ﬁl-\ntering of the FFT. Apart from its simplicity, the proposed\nmethod shows to be a good compromise between design\nﬂexibility and reduced computational effort. Additionall y,\nit was used as a part of an effective melody extraction al-\ngorithm.\n1. INTRODUCTION\nMany automatic music analysis algorithms, such as those\nintended for melody extraction or multiple pitch estima-\ntion, rely on a spectral representation of the audio sig-\nnal, typically the discrete Short Time Fourier Transform\n(STFT).Akeyissuethatarisesisthecompromisebetween\ntimeandfrequencyresolution. Thefrequencycomponents\nof a Discrete Fourier Transform (DFT) are equally spaced\nand have a constant resolution. However, in polyphonic\nmusic a higher frequency resolution is needed in the low\nandmidfrequencieswherethereisahigherdensityofhar-\nmonics. On the other hand, frequency modulation gets\nstronger as the number of harmonic is increased, requir-\ning shorter windows for improved time resolution. Thus,\na multi resolution spectral representation is highly desir ed\nfor the analysis of music signals. In addition, computa-\ntional cost is a critical issue in real time or demanding ap-\nplications soefﬁcient algorithms are oftenneeded.\nInthiscontext several proposals have been made tocir-\ncumvent the conventional linear frequency and constant\nresolutionoftheDFT.Theconstant-Qtransform(CQT)[3]\nis based on a direct evaluation of the DFT but the chan-\nnel bandwidth ∆fkvaries proportionally to its center fre-\nquency fk,inordertokeepconstantitsqualityfactor Q=\nfk/∆fk(asinWavelets). Centerfrequenciesaredistributed\ngeometrically, to follow the equal tempered scale used in\nWestern music, in such a way that there are two frequency\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbear this noticeand thefull citation ontheﬁrst page.\n© 2009International Society forMusic InformationRetrieva l.components for each musical note(although higher values\nof Q provide a resolution beyond the semitone). Direct\nevaluation of the CQT is very time consuming, but fortu-\nnately an approximation can be computed efﬁciently tak-\ningadvantage of theFast Fourier Transform (FFT) [1].\nVarious approximations to a constant-Q spectral repre-\nsentation have also been proposed. The bounded-Q trans-\nform (BQT) [4] combines the FFT with a multirate ﬁlter-\nbank. Octaves are distributed geometrically, but within\neach octave, channels are equally spaced, hence the log\nrepresentation is approximated but with a different num-\nber of channels per octave. Note that the quartertone fre-\nquency distribution, in spite of being in accordance with\nWestern tuning, can be too scattered if instruments are not\nperfectly tuned, exhibit inharmonicity or are able to vary\ntheir pitch continuously (e.g. glissando or vibrato). Re-\ncently a new version of the BQT with improved channel\nselectivity was proposed in [5] by applying the FFT struc-\nture but with longer kernel ﬁlters, a technique called Fast\nFilter Bank. An approach similar to the BQT is followed\nin [6] as a front-end to detect melody and bass line in real\nrecordings. Also in the context of extracting the melody\nof polyphonic audio, different time-frequency resolution s\nare obtained in [2] by calculating the FFT with different\nwindow lengths. This is implemented by a very efﬁcient\nalgorithm, named the Multi-Resolution FFT (MR FFT),\nthat combines elementary transforms into a hierarchical\nscheme.\nInthispaperwefocusonmulti-resolutionspectralanal-\nysis algorithms for music signals based on the FFT. Two\npreviously devised efﬁcient algorithms that exhibit diffe r-\nentcharacteristicsarereviewed,namely,theefﬁcientCQT\n[1] and the MR FFT [2]. The former is more ﬂexible re-\ngarding Q design criteria and frequency channel distribu-\ntion while the latter is more efﬁcient at the expense of de-\nsignconstrains. Thesealgorithmsarecomparedwithanew\nproposal based on the Inﬁnite Impulse Response (IIR) ﬁl-\nteringoftheFFT(IIRCQT),thatinadditiontoitssimplic-\nity shows to be a good compromise between design ﬂexi-\nbilityand reduced computational effort.\n2. FIRQ TRANSFORM IMPLEMENTATIONS\n2.1 Efﬁcient constant Q transform\nAsstatedin[3]aCQTcanbecalculatedstraightforwardly\nbasedontheevaluationoftheDFTforthedesiredcompo-\n309Poster Session 2\nnents. Consider the kthspectral component of theDFT:\nX[k] =N−1/summationdisplay\nn=0w[n]x[n]e−j2πkn/N\nwhere w[n]is the temporal window function and x[n]is\nthe discrete time signal. In this case the quality factor for\na certain frequency fkequals k, since Qk=fk/∆f=\nfkN/f s=k. This corresponds to the number of peri-\nods in the time frame for that frequency. The digital fre-\nquencyis 2πk/Nandtheperiodinsamplesis N/k. Inthe\nCQT the length of the window function varies inversely\nwithfrequency(buttheshaperemainsthesame),sothat N\nbecomes N[k]andw[n]becomes w[n,k]. For a given fre-\nquency fk,N[k] =fs/∆fk=fsQk/fk. The digital fre-\nquency of the kth component is then given by 2πQ/N [k],\nthe period in samples is N[k]/Qand always Qcycles for\neach frequency are analyzed. The expression for the kth\nspectral component ofthe CQT isthen1,\nXcq[k] =1\nN[k]N[k]−1/summationdisplay\nn=0w[n,k]x[n]e−j2πQn/N [k].(1)\nDirect evaluation of equation (1) is time consuming, so\nan efﬁcient algorithm for its computation has been pro-\nposedin[1]. TheCQTcanbeexpressedasamatrixmulti-\nplication, Xcq=x·T∗,where xisthesignalrowvectorof\nlength N(N≥N[k]∀k)andT∗isthecomplexconjugate\nof the temporal kernel matrix Twhose elements T[n,k]\nare,\nT[n,k] =/braceleftbigg1\nN[k]w[n,k]e−j2πQn/N [k]ifn < N [k]\n0 otherwise\nComputational effort can be improved if the matrix multi-\nplication is carried out in the spectral domain. Using Par-\nseval’s relationfor theDFT, theCQT can be expressed as,\nXcq[k] =N−1/summationdisplay\nn=0x[n]T∗[n,k] =1\nNN−1/summationdisplay\nk′=0X[k′]K∗[k′,k](2)\nwhere X[k′]andK[k′,·]are the DFT of x[n]andT[n,·]\nrespectively. Spectral kernels arecomputed only once tak-\ning full advantage of the FFT. In the case of conjugate\nsymmetric temporal kernels, the spectral kernels are real\nand near zero over most of the spectrum. For this rea-\nson,ifonlythespectralkernelvaluesgreaterthanacertai n\nthreshold are retained, there are few products involved in\nthe evaluation of the CQT (almost negligible compared to\nthe computation of theFFT of x[n]).\nItisimportanttonoticethatalthoughtheoriginalderiva-\ntion of the CQT implies a geometrical distribution of fre-\nquency bins, it can be formulated using other spacing, for\ninstance a constant separation. In the following, linear\nspacing is used to put all the compared algorithms under\nan uniﬁed framework.\n1Anormalizationfactor 1/N[k]mustbeintroducedsincethenumber\nofterms varies with k.2.2 Multi-resolutionFFT\nA simple way to obtain multiple time-frequency resolu-\ntions is through the explicit calculation of the DFT us-\ning different frame lengths. In [2], an efﬁcient technique\nis proposed where the DFT using several frame lengths\nis computed by means of the combination of the DFT of\nsmall number of samples, called elementary transforms.\nThe idea arises from the observation that a transform of\nframe length Ncan be split into partial sums of Lterms\n(assuming N/L ∈N),\nX[k] =N−1/summationdisplay\nn=0x[n]e−j2πkn\nN=N\nL−1/summationdisplay\nc=0(c+1)L−1/summationdisplay\nn=cLx[n]e−j2πkn\nN.(3)\nEach inner sum in equation 3 corresponds to the DFT of\nlength Nofasequence xc[n],where xc[n]isanLsamples\nchunk of x[n], time-shiftedand zero padded,\nxc[n] =/braceleftbiggx[n], cL ≤n <(c+ 1)L\n0,otherwise.\nSo,itispossibletoobtainaDFTofaframeofsize Nfrom\nN/Lelementary transformsof framesize L, deﬁned as\nXl[k] =L−1/summationdisplay\nn=0x[n+lL]e−j2πkn/N, l= 0,...,N\nL−1.\nTo that end, it is enough to add the elementary transforms\nmodiﬁed with a linear phase shift to include the time shift\nofxc[n], as statedby theshiftingtheorem of theDFT,\nX[k] =N\nL−1/summationdisplay\nl=0Xl[k]e−j2πkl/N. (4)\nThis procedure can be generalized to compute the DFT of\nanyframeoflength M=rLbyadding relementarytrans-\nforms ( r= 1,...,N/L) in the equation 4, which results in\nN/Lpossiblesspectralrepresentationswithfrequencyres-\nolutions of fs/(rL).\nThecomputationofthemulti-resolutionspectrumfrom\na combination of elementary transforms requires the win-\ndowing process to be done by means of convolution prod-\nuct in the frequency domain. Temporal windows of the\nform\nw[n] =M\n2/summationdisplay\nm=0(−1)mamcos/parenleftbigg2π\nMmn/parenrightbigg\n(5)\nare suitable for this purpose because its spectrum has only\nfew non-zero samples. Due to the fact that windowing is\napplied over zero-padded transforms, it is convenient to\nconsider a periodic time window of the same length of the\nDFT to avoid the appearance of new non-zero samples of\nthe window spectrum. In this case, the spectrum of a win-\ndow of theform of equation 5resultsin\nW[k] =M\n2/summationdisplay\nm=0(−1)mam\n2/parenleftbigg\nδ/bracketleftbigg\nk−mN\nM/bracketrightbigg\n+δ/bracketleftbigg\nk+mN\nM/bracketrightbigg/parenrightbigg\n31010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 1. Zero-Pole diagram and IIR ﬁlters responses for\nthree different input sinusoids of frequencies f1= 0.11,\nf2= 0.30andf3= 0.86radians.\nForexample,inHannandHammingwindowsonly a0and\na1are not zero and so its DFT contains solely three non-\nzerosamples. Asacounterpart,therestrictionthat N/M =\nN/(rL)∈Nmustbeimposed,reducingthepossiblenum-\nber ofresolutions to log2(N/L) + 1.\n3. IIRQ TRANSFORM\n3.1 FIR/IIRFilterbank\nThe proposed methods deﬁne a Finite Impulse Response\n(FIR)ﬁlterbankwithdifferentimpulseresponsesfordiffe r-\nent frequencies. The result of applying one of these ﬁlters\ncan be regarded as multiplying the frame with a time win-\ndow,whichdeﬁnesthetime/frequencyresolution. Variable\nwindowing in time can also be achieved applying an IIR\nﬁlterbank in the frequency domain. Let us deﬁne the kth\nﬁlterasaﬁrstorderIIRﬁlterwithapole pk,andazero zk,\nas,\nYk[n] =X[n]−zkX[n−1] +pkYk[n−1](6)\nItsZtransform isgiven by,\nHfk(z) =z−zk\nz−pk.\nHere, Hfk(z)evaluated in the unit circle z=ejτrepre-\nsents its time response, with τ∈(−π,π]being the nor-\nmalized time within the frame. A different time window\nforeachfrequencybinisobtainedbyselectingthevalueof\nthekthbinas theoutput of the kthﬁlter.\nThe design of these ﬁlters involves ﬁnding the zero and\npole for each ksuch that wk(τ) = |Hfk(ejτ)|, where\nτ∈(−π,π]andwk(τ)isthedesiredwindowforthebin k.\nWhenaframeisanalyzed,itisdesirabletoavoiddisconti-\nnuitiesatitsends. Thiscanbeachievedbyplacingthezero\ninτ=π, that is zk=−1. If we are interested in a sym-\nmetric window, wk(τ) =wk(−τ), the pole must be real.\nConsidering a causal realization of the ﬁlter, pkmust be\ninside the unit circle to assure stability, thus pk∈(−1,1).\nFigure 1 shows the frequency and time responses for the\npoles depicted inthezero-pole diagram.\nThisIIRﬁlteringinfrequencywillalsodistortthephase,\nso a forward-backward ﬁltering should be used to obtain azero-phase ﬁlter response. Then, the set of possible win-\ndows that can be represented withthese values of pkis,\nwk(τ) =(1−pk)2\n4/bracketleftbiggA(τ)\nB(τ)/bracketrightbigg2\n=(1−pk)2(1 + cos τ)\n2(1 + p2\nk−2pkcosτ)(7)\nwhere A(τ)andB(τ)arethedistancestothezeroandthe\npole, as shown in Figure 1, and gk= (1 −pk)2/4is a\nnormalization factor2to have 0 dB gain at time τ= 0,\nthat is, wk(0) = 1.\nWhile this ﬁlter is linear and time invariant (in fact fre-\nquency invariant3) a different time window is desired for\neachfrequencycomponent. Computingtheresponseofthe\nwhole bank of ﬁlters for the entire spectrum sequence and\nthen choosing the response for only one bin is computa-\ntionally inefﬁcient. For this reason, a Linear Time Variant\n(LTV) system, that consists in a Time Varying (TV) IIR\nﬁlter, is proposed as a way to approximate the ﬁlterbank\nresponseatthefrequencybinsofinterest. Itwillnolonger\nbe possible to deﬁne the ﬁlter impulse response, as this\ncouldonlybedoneiftheﬁlterswereinvarianttofrequency\nshifts.\n3.2 LTV IIR System\nSelecting a different ﬁlter response of the ﬁlterbank for\neach frequency bin can be considered as applying an LTV\nsystemtotheDFT of aframe. The desiredresponse of the\nLTV for a given frequency bin is the impulse response of\nthecorrespondent ﬁlter.\nAny LTV system can be expressed in the matrix form,\nY=K.Xwhere Kis the linear transformation matrix\n(also referred as Greens matrix) and, in this case, Xis the\nDFT of the signal frame. A straightforward way to con-\nstructKforany LTV systemistosetits ithcolumn asthe\nresponsetoashifteddelta δ[n−i],whichisnamedSteady\nStateResponse (SSR).\nTheapproachfollowedinthisworkconsistsinapproxi-\nmatingtheLTVsystembyasingleTVIIRﬁlter,assuming\nthat the LTV system has a slow time varying behavior and\nthatitsSSRcanbeimplementedbyanIIRﬁlterbank. Then\nitisveriﬁedthattheapproximationissufﬁcientlygoodfor\nour purposes. In the case of variable windowing to ob-\ntainaconstantQ,theseassumptionshold,astimewindows\nfortwoconsecutivefrequencybinsareintendedtobevery\nsimilar,andtheLTVsystemcanbeimplementedbyanIIR\nﬁlterbank as seen before.\nA direct way of approximating the IIR ﬁlterbank is by\naﬁrstorderIIRoftheformofequation6,butinwhichthe\npole varies withfrequency ( p=p[n]),\nY[n] =X[n] +X[n−1] +p[n]Y[n−1].(8)\nWith an appropriate design, it reasonably matches the de-\nsired LTV IIR ﬁlterbank response, and its implementation\nhas low computational complexity.\n2This normalization factor can be calculated from the impulse r e-\nsponseevaluatedat n= 0,orbytheintegralofthetimewindowfunction.\n3Note that we will use the usual time domain ﬁltering terminology in\nspite ofthe factthat ﬁltering is performedinthe frequencyd omain.\n311Poster Session 2\n3.3 Time Varying IIRﬁlterdesign\nA question that arises is how to design the TV IIR ﬁlter\nin order to have a close response to that of the LTV IIR\nﬁlterbank. Several design criteria have been proposed in\nthe literature[7], that may depend on theproblem itself.\nThe TV IIR can also be represented by a matrix Kvin\na similar way as the LTV ﬁlterbank, so the design can be\ndone as in[7], byminimizing thenormalized mean square\nerror,E=||K−Kv||2/||K||2. In this work, the adopted\ndesign criteria is to impose the windows behavior in time\ninordertoobtainthedesiredconstantQ.Then,theerroris\nregarded as the difference between the desired Q and the\neffective obtained value. It becomes necessary to deﬁne\nan objective measure of Q. Usually the quality factor of a\npassband ﬁlter is deﬁned as the ratio between the center\nfrequency and the bandwidth at 3 dB gain drop. In our\ncase the ﬁltering is done in the frequency domain, so it is\nreasonable to measure Q in the time domain. Given that Q\nrepresents the number of cycles of an analyzed frequency\ncomponent in the frame, it makes sense to deﬁne Q as the\nnumberofcycleswithinthewindowwidthatacertaingain\ndrop, for example 3 dB. If τ′\nkis the time at this drop for\nfrequency fk,wk(τ′\nk) = 10−3\n20w(0)/definesw′\nk, then τ′\nk=\nQ/(2fk). This deﬁnition allows the comparison of Q for\nmethodswithdifferentwindowshapes. Notehowever,that\na similar measure of Q can be formulated in the frequency\ndomain.\nIn the proposed approach the ﬁrst step is to design an\nIIR ﬁlterbank that accomplishes the constant Q behavior.\nThen, a TV IIR ﬁlter is devised based on the poles of the\nﬁlterbank. Finally a ﬁne tuning is performed to improve\nthe steadiness of the Q value for the TV IIR ﬁlter. In the\nfollowing section, thisprocedure isdescribed indetail.\n3.3.1 Proposed design\nFollowing the deﬁnition of Q in time, the poles of the IIR\nﬁlterbankcanbecalculatedfromequation7asthesolution\nof asecond order polynomial: (2w′\nk−cos(τ′\nk)−1)p2\nk+\n(2+2cos( τ′\nk)−4w′\nkcos(τ′\nk))pk+2w′\nk−cos(τ′\nk)−1 = 0.\nThen, a simple and effective design of the TV IIR ﬁl-\nter consists in choosing for each frequency bin the corre-\nsponding pole of the IIR ﬁlterbank, that is p[n] =pk, with\nk=n. TheQfactorsobtainedwiththisapproachareclose\nto the desired constant value but with a slight linear drift.\nThisresultshowsthattheslowvariationoftheLTVsystem\nallowsanapproximationbyasingleTVIIRwithalittlede-\nviationthatcanbeeasilycompensatedbyaddingthesame\nslope to the desired Q value at each bin. Figure 3 shows\nthe Q curve for theoriginal and compensated designs.\nAnotherdesignconsiderationisthatforlowfrequencies\na constant Q would imply a longer window support than\ntheframetime. Itbecomesnecessarytolimitthetime τ′\nkto\namaximumtime τmax,suchthat 2τmaxissmallerthanthe\nframetime. Thislimitationof τ′\nktoamaximumvaluemust\nbe done in a smooth way. Let ¯τ′\nkbe a new variable that\nrepresents the result of saturating τ′\nk. The transition can\nbe implemented with a hyperbola whose asymptotes are\n¯τ′\nk=τ′\nkand¯τ′\nk=τmax,sothat (¯τ′\nk−τmax)(¯τ′\nk−τ′\nk) =δ,0 0.5 1 1.5 2 2.5 3 3.5−1−0.500.51\nNormalized frequency (radians)pPole design for different frequencies\n  \nSaturated pole\nIdeal pole\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7−1−0.500.51\nNormalized frequency (radians)pPole design detail for low frequencies \n  \nSaturated pole\nIdeal pole\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7SSR and TV IIR response to shifted deltas for low frequencies\nNormalized frequency (radians)Magnitude\n  \nTime Varying IIR\nSteady State IIR\n−3 −2 −1 0 1 2 3Low frequency time windows\nNormalized time (radians)Magnitude\nFigure 2. Detail of poles design. Pole locations for the\nideal and saturated design. Impulse responses at low fre-\nquencies for the TV IIR and the Steady State, along with\ncorresponding TV IIRtimewindows.\nwhere δisaconstantthatdeterminesthesmoothnessofthe\ntransition.\nTheselectionof τmax,affectsthebehaviorofthetrans-\nforminlowfrequencies. Choosingasmall τmaxcompared\nto the frame time gives poor frequency resolution. On the\ncontrary,if τmaxissettoavalueclosetotheframetime,a\nbetter resolution is expected, but some distortion appears .\nThis is because the time windows get close to a rectan-\ngular window for low frequencies. The spectrum of these\nwindowshasbigsidelobes,introducingGibbsoscillations\nin the representation. Additionally, as a time window for\nlow frequency approaches to a rectangular shape, its re-\nsponse to an impulse vanishes more slowly, so it becomes\nnecessary to calculate the response for some negative fre-\nquency bins, adding extra complexity. In practice it is\nreasonable to choose an intermediate value of τmax, e.g.\nτmax≈0.7π, such that only for very low frequencies the\ntransform exhibits non constant Q. Figure 2 shows details\nof thedescribed poles design.\n3.3.2 TVIIRﬁltering and zero-padding intime\nIt is common practice to work with a higher sampling fre-\nquencyofthespectrum,typicallyobtainedbyzero-padding\nintime. InthiscasetheTVIIRﬁlterdesignchanges,asthe\nsignal support becomes (−τ1,τ1]with0< τ1< π. Then,\nthediscontinuitytobeavoidedattheendsoftheframeap-\n31210th International Society for Music Information Retrieval Conference (ISMIR 2009)\np=design_poles (NFFT,Q) ;\nX=fft(fftshift (s) ) ;\nY'(1) = X(1) ;\nfori= 2:NFFT/2\nY'(n) =X(n−1) +X(n) +p(n)Y'(n−1);\nend\nY(n) =Y'(NFFT/2) ;\nfori=NFFT/2−1:−1:1\nY(n) =Y'(n+1) + Y'(n) +p(n)Y(n+1) ;\nend\nTable 1. Pseudocode of the TV IIR ﬁlter. First, the poles\nandnormalizationfactoraredesignedgiventhenumberof\nbins (NFFT) and the Q value. Then the FFT of the signal\nframesiscomputedaftercenteringthesignalattime0. Fi-\nnally the forward-backward TV IIR ﬁltering is performed\nfor that frame.\npearsat ±τ1,soacoupleofzerosat ±τ1havetobeplaced\ninstead of the zero at π. Window properties outside this\nsupport are irrelevant, as windowed data values are zero.\nThedesignofpoleshastotakeintoaccountthenewzeroes\nand the time re-scaling, but windows with similar proper-\nties areobtained.\n3.3.3 Implementation\nThe method implementation4is rather simple, as can be\nseeninthepseudocodeofTable1. Afunctiontodesignthe\npoles is called only once and then the forward-backward\nTVIIRﬁlteringisappliedtotheDFTofeachsignalframe.\nThe proposed IIR ﬁltering applies a window centered at\ntime0, so the signal frame has to be centered before the\ntransform. To avoid transients at the ends, the ﬁltering\nshould be done circularly using a few extra values of the\nspectrum as preﬁx and postﬁx. Their lengths can be cho-\nsensoastruncationerrorliesbelowacertainthreshold,fo r\ninstance 60 dB.\n4. METHODS COMPARISON\n4.1 Frequency scale\nDependingonthecontextofthemusicanalysisapplication\ndifferentfrequencygridsmaybepreferred. Tothisrespect ,\ntheefﬁcientCQTmethodcanbedesignedforanyarbitrary\nfrequency spacing. On the contrary, the MR FFT and the\nIIR CQT are constrained to a linear frequency scale be-\ncause they rely on the DFT. This spacing typically implies\nan oversampling at high frequencies to conform with the\nminimum spacing at low frequencies.\n4.2 Effective qualityfactor\nThe analyzed methods have different ﬂexibility to deﬁne\nan arbitrary Q at each frequency. The efﬁcient CQT offers\nthe freedom to set any possible Q for every bin. The MR\nFFT allows choosing the resolution for every bin from a\nreducedsetnotenablinganarbitraryQ.Ontheotherhand,\n4The complete codeis available at http://iie.fing.edu.uy/\n˜pcancela/iir-cqt .Figure 3. Comparison of the effective Q for a target value\nof 12.9 given the deﬁnition of 3.3. This value gives 34\ncycles withinthe window, as commonly used intheCQT.\nFigure 4. Windows comparison at frequencies f1,f2and\nf3for the different methods. At f1andf3the three meth-\nods have the same Q, while at f2the MR FFT can not\nachievethedesiredQ.Forthisreason,thetwonearestMR\nFFT windows are considered at f2. CQT and MR FFT are\ncomputed using a Hamming and Hann windows respec-\ntively.\nthe TV IIR ﬁlter allows any Q value for any frequency but\nwith the constraint that it evolves slowly with frequency.\nThis holds particularly well in the case of a constant Q\ntransform, so the IIR CQT can give any constant Q with\nafairlysimpledesign. Figure3showstheobtainedQwith\nthedifferentmethods. ItcanbeobservedthattheMRFFT\nhas abounded Q due tothe resolutionquantization.\n4.3 Windows properties\nThe spectral and temporal characteristics of windows at\nthree different frequencies are shown in Figure 4 for each\nmethod. At frequency f1, IIR CQT time window behaves\nlike a Hann window. For lower frequencies it exhibits a\nﬂatter shape to extend the range of constant Q (see Figure\n2). For higher frequencies, the main lobe of the obtained\nwindows has a steeper drop up to -50 dB compared to a\nconventional Hann or Hamming window. As a counter-\npart, time resolution is slightly diminished. Note that the\nselected drop value in the deﬁnition of Q sets the location\ninthiscompromise.\n313Poster Session 2\n4.4 Computational complexity\nThethreealgorithmsarecomparedbasedonthenumberof\nreal ﬂoating point operations performed in mean for each\nfrequencybin. AllofthemcomputetheDFTofanonwin-\ndowed frame, sothese operations arenot considered.\nThenumber ofoperations intheefﬁcient CQTdepends\non the length of the frequency kernels. This length varies\nwith Q and is different for different frequency bins. For\nthe Q and threshold values used in Figures 3 and 4 (QCQT\n=34,Q=12.9,th=0.0054), NFFT=2048andf s= 44100\nHz, the frequency kernel length varies from 1 to 57 coef-\nﬁcients, which implies a mean number of 27 real multi-\nplications and 27 real additions. This result depends on\nthe threshold and inversely on Q. The MR FFT takes ad-\nvantage of the hierarchical implementation of the FFT to\ncomputethetransform,sothewindowinginthefrequency\ndomain needs only 3 complex sums and 2 multiplications\nfor each bin. The total number of real ﬂoating point oper-\nations is then, 4 multiplications and 6 additions. The IIR\nCQT involves a forward and backward IIR ﬁltering with a\nvariablerealpoleandazero,followedbyarealnormaliza-\ntion(seeTable1forapseudocode). Asthefrequencycom-\nponents are complex values, the necessary number of real\noperations to compute each bin is 6 multiplications and 8\nadditions(plusanegligiblenumberofextraoperationsdue\ntothe circularlyﬁltering approximation).\n5. APPLICATIONS AND RESULTS\nFinally, two different examples of the spectral analysis of\npolyphonic music using the proposed IIR CQT method\nare shown in Figure 5 together with conventional spectro-\ngrams. As it is expected in a constant Q transform, it can\nbe noticed that singing voice partials with high frequency\nslope tend to blur in the spectrogram but are sharper in\nthe IIR CQT. This improved time resolution in high fre-\nquenciesalsocontributestodeﬁnemorepreciselythenote\nonsets, as can be seen in the second example (e.g. the\nbass note at the beginning). Moreover, in the low fre-\nquency band, where there is a higher density of compo-\nnents,theIIRCQTachievesabetterdiscrimination,dueto\nthefactthatitstimewindowsareﬂatterthantypicallyused\nwindows. At the same time, frequency resolution for the\nhigher partials of notes withasteady pitch isdeteriorated .\nThe proposed IIR CQT method was used as part of the\nspectral analysis front-end of a melody extraction algo-\nrithm submitted to the MIREX Audio Melody Extraction\nContest2008,performingbestonOverallAccuracy5. Al-\nthough the constant Q behavior of the spectral representa-\ntion is just a small component of the algorithm, the results\nmay indicate that theusage of the IIRCQT isappropriate.\n6. CONCLUSIONS\nIn this work a novel method for computing a constant Q\nspectral transform is proposed and compared with two ex-\n5The MIREX 2008 evaluation procedure and results are availab le\nathttp://www.music-ir.org/mirex/2008/index.php/\nAudio_Melody_Extraction .\nFigure 5. STFT and IIR CQT for two audio excerpts, one\nwith a leading singing voice and the other, instrumental\nmusic.\nisting techniques. It shows to be a good compromise be-\ntweentheﬂexibilityoftheefﬁcientCQTandthelowcom-\nputational cost of the MR FFT. Taking into account that it\nwas used in the spectral analysis of music with encourag-\ning results and that its implementation is rather simple, it\nseems to be a good spectral representation tool for audio\nsignal analysis algorithms.\n7. REFERENCES\n[1] J.C.BrownandM.S.Puckette,“Anefﬁcientalgorithm\nfor the calculation of a constant Q transform,” JASA,\nvol. 92, no. 5, pp. 2698–2701, 1992.\n[2] K.Dressler,“SinusoidalExtractionUsingandEfﬁcient\nImplementation of a Multi-Resolution FFT,” in Pro-\nceedings of theDAFx-06 , (Montreal, Canada), 2006.\n[3] J. C. Brown, “Calculation of a constant Q spectral\ntransform,” JASA, vol. 89, no. 1, pp. 425–434, 1991.\n[4] K. L. Kashima and B. Mont-Reynaud, “The bounded-\nQ approach to time-varying spectral analysis,” Tech.\nRep. STAN-M-28, Stanford University, 1985.\n[5] F. C. C. B. Diniz, I. Kothe, S. L. Netto, and L. W. P.\nBiscainho, “High-Selectivity Filter Banks for Spectral\nAnalysis of Music Signals,” EURASIP Journal on Ad-\nvances inSignal Processing , vol. 2007, 2007.\n[6] M. Goto, “A Real-time Music Scene Description\nSystem: Predominant-F0 Estimation for Detecting\nMelody and Bass Lines in Real-world Audio Signals,”\nSpeech Communication (ISCAJournal) , vol. 43, no. 4,\npp. 311–329, 2004.\n[7] J. S. Prater and C. M. Loefﬂer, “Analysis and design\nof periodically time-varying IIR ﬁlters, with applica-\ntions to transmultiplexing,” IEEE Transactions on Sig-\nnal Processing , vol. 40, no. 11, pp. 2715–2725, 1992.\n314"
    },
    {
        "title": "Using Source Separation to Improve Tempo Detection.",
        "author": [
            "Parag Chordia",
            "Alex Rae"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416536",
        "url": "https://doi.org/10.5281/zenodo.1416536",
        "ee": "https://zenodo.org/records/1416536/files/ChordiaR09.pdf",
        "abstract": "We describe a novel tempo estimation method based on decomposing musical audio into sources using principal latent component analysis (PLCA). The approach is motivated by the observation that in rhythmically complex music, some layers may be more rhythmically regular than the overall mix, thus facilitating tempo detection. Each excerpt was analyzed using PLCA and the resulting components were each tempo tracked using a standard autocorrelationbased algorithm. We describe several techniques for aggregating or choosing among the multiple estimates that result from this process to extract a global tempo estimate. The system was evaluated on the MIREX 2006 training database as well as a newly constructed database of rhythmically complex electronic music consisting of 27 examples (IDM DB). For these databases the algorithms improved accuracy by 10% (60% vs 50%) and 22.3% (48.2% vs. 25.9%) respectively. These preliminary results suggest that for some types of music, source-separation may lead to better tempo detection.",
        "zenodo_id": 1416536,
        "dblp_key": "conf/ismir/ChordiaR09",
        "keywords": [
            "principal latent component analysis",
            "tempo estimation",
            "rhythmically complex music",
            "tempo detection",
            "aggregating estimates",
            "tempo accuracy",
            "MIREX 2006 training database",
            "IDM DB",
            "electronic music",
            "system evaluation"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nUSING SOURCE SEPARATION TO IMPROVE TEMPO DETECTION\nParag Chordia\nDept. of Music, GTCMT\nGeorgia Institute of TechnologyAlex Rae\nDept. of Music, GTCMT\nGeorgia Tech Center for Music Technology\nABSTRACT\nWe describe a novel tempo estimation method based on\ndecomposing musical audio into sources using principal\nlatent component analysis (PLCA). The approach is moti-\nvated by the observation that in rhythmically complex mu-\nsic, some layers may be more rhythmically regular than the\noverall mix, thus facilitating tempo detection. Each excerpt\nwas analyzed using PLCA and the resulting components\nwere each tempo tracked using a standard autocorrelation-\nbased algorithm. We describe several techniques for ag-\ngregating or choosing among the multiple estimates that\nresult from this process to extract a global tempo estimate.\nThe system was evaluated on the MIREX 2006 training\ndatabase as well as a newly constructed database of rhyth-\nmically complex electronic music consisting of 27 exam-\nples (IDM DB). For these databases the algorithms im-\nproved accuracy by 10% (60% vs 50%) and 22.3% (48.2%\nvs. 25.9%) respectively. These preliminary results suggest\nthat for some types of music, source-separation may lead\nto better tempo detection.\n1. BACKGROUND AND MOTIVATION\nA working deﬁnition of tempo is the rate of the underlying\nrhythmic pulse of music determined by a human listener\ntapping along to the music, typically expressed in beats per\nminute (BPM). This may differ from a notated tempo, and\ndifferent listeners, or the same listener at different times,\noften entrain to different metrical levels, so that some tap-\nping rates may be half or double as fast as others. Further,\nin some types of music, the most natural way to tap along\nis asymmetric (e.g. tapping on the accented ﬁrst and third\nbeat in a fast group of ﬁve beats). For our purposes, these\ncomplexities are important to acknowledge at the outset as\nthey set natural bounds on performance and suggest appro-\npriate ways of judging accuracy.\nTempo estimation is a fundamental MIR task and under-\nlies almost all rhythmic descriptions of music. However,\nstate-of-the-art tempo detection is still highly variable in\nits accuracy, working well on most simple cases, but often\nperforming poorly or not at all on rhythmically complex\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.music [1]. The current work is motivated by two observa-\ntions: 1) rhythmically complex music may be constructed\nout of components or layers (e.g. musical parts or sources)\nthat are rhythmically simpler than the mix and thus easier\nto track; 2) in many types of music, humans track the beat\nor the tempo by hearing out a particular instrument or part.\nFor example, in many types of rhythmically complex elec-\ntronic music, a “click track” is present in the mix. More\ngenerally, in many musical genres a particular part plays\na time-keeping function: for example, in standard jazz the\nwalking bass line is the time keeper, in Indian music the\ntabla, in Afro-Cuban music the clave. Being able to hear\nout these time-keeping parts makes tempo tracking easier\nfor humans.\n2. RELATED WORK\nThe starting point of the current work is tempo detection\nthat looks for periodicities in the signal by taking the au-\ntocorrelation of the detection function (ACF). A good re-\nview of current algorithms can be found in McKinney et\nal. [2] as well as speciﬁc descriptions of autocorrelation-\nbased approach in Ellis [3] and Davies and Plumbley [4].\nRecent work has explored the extension of this basic ap-\nproach to tempo detection in a variety of ways. Wright\net al. [5] describe a system that searches for the rhythmic\npattern of the clave in Afro-Cuban music and show that\nsuch an approach out-performs techniques more reliant on\nisochronous events such as the Ellis and Dixon [6] algo-\nrithms. In their work, a matched ﬁlter is used to extract the\nclave from the mix. In this paper, we attempt to general-\nize the idea of ﬁnding the time-keeper in the mix in a way\nthat is less reliant on domain-speciﬁc knowledge. Seyer-\nlehner et al. [7] cast tempo estimation as a nearest neighbor\nproblem, representing instances using a smoothed autocor-\nrelation function (ACF). This approach suggests the idea\nof using not just the peak of the ACF, but including other\nfeatures to improve tempo detection. Xiao et al, [8] demon-\nstrate that using timbral features in addition to ACF-based\nfeatures can reduce double/half tempo errors and indicates\nthat even very crude uses of timbre can improve tempo esti-\nmation accuracy. Earlier work on tempo detection has also\nsought to improve accuracy by processing information in\nparticular frequency sub-bands [9, 10]. In some cases, this\nis akin to a crude source separation, for example, separat-\ning the bass drum from the rest of a song.\nProbabilistic latent component analysis (PLCA), a tech-\nnique for source-separation described in Section 3.2, has\nbeen used for unmixing as well as transcription [11, 12].\n183Oral Session 2: Tempo and Rhythm\nThe closely related technique of non-negative matrix fac-\ntorization (NMF) has been used to improve drum detec-\ntion [13, 14]. In these works, tracks were separated into\nsources that were then grouped into either tonal or percus-\nsive layers based on features of the components. This is\nrelevant to the current work because it demonstrates the\nidea of using source-separation as a pre-processing step to\nimprove performance on a standard MIR task. Addition-\nally, features of the components are used to classify them\ninto different groups, a technique used in this work to judge\nhow strong a pulse different components have.\n3. METHOD\n3.1 Overview\nAs stated, the technique described here builds on ACF-\nbased tempo detection. First, the track is separated into\ncomponents using a single-channel source separation method\n(PLCA). Next, the tempo of each component is estimated\non the separated audio. The component tempo estimates,\nalong with the windowed ACF that was used to calculate\nthe component tempo, are then used to ﬁnd a global tempo\nestimate for the excerpt. We discuss several attempts to\nsolve the problem of ﬁnding the best tempo estimate from\nthe components. Two basic strategies were employed: se-\nlecting the tempo of a component with the highest esti-\nmated rhythmic clarity, and clustering component tempo\nestimates and weighting each cluster by the rhythmic clar-\nity of each element in the cluster. Figure 1 shows a block\ndiagram of the system.\n3.2 Source Separation\nBlind source separation attempts to recover constituent el-\nements from a signal without any speciﬁc a priori knowl-\nedge of their characteristics. For audio, this corresponds to\n“unmixing,” the reconstruction of a clean signal of each of\na number of sounds that have been mixed together. Faith-\nful reconstruction of component elements has a wide array\nof potential applications; in the current work we are less\ninterested in mimicking the timbre of the original sources\nthan in capturing rhythmic characteristics that may be less\nevident in a full mix.\nWe approach this task using the non-shift-invariant ver-\nsion of Probabilistic Latent Component Analysis (PLCA)\n[11, 12]. The input to the PLCA is a spectrogram, com-\nputed using a 1024 sample Hann window with a hopsize\nof 256 samples and then normalized to be a valid probabil-\nity distribution. Latent variables representing the compo-\nnents are estimated using expectation maximization, and\nthe output consists of a magnitude spectrum and relative\ncontribution over time for each component; the number of\ndesired components must be speciﬁed by the user.\nAfter some experimentation, we set the number of com-\nponents to be extracted to eight. A more systematic evalua-\ntion of the optimal number components remains for future\nwork. The corresponding timbral and temporal proﬁles\nwere used to synthesize audio for each component using\nphase information from the original audio.3.3 Tempo Estimation\nThe tempo was estimated for each component using the\nEllis algorithm [3]. The algorithm constructs a detection\nfunction based on a 40-channel db-magnitude mel spectro-\ngram. First the signal is downsampled to 8 kHz, mixed to\nmono and divided into 32 ms frames with a 4 ms hopsize.\nThe ﬁrst-order difference is taken for each channel, and the\nsum of positive values across all channels is the value of the\ndetection function for that frame (spectral ﬂux). The auto-\ncorrelation of the detection function is calculated and then\nwindowed to bias it towards tempos close to 120 BPM. The\nwindowing effectively excludes tempos falling outside an\nacceptable range, and at the same time mimics the natu-\nral preference of humans to tap at rates between 90-120\nBPM [1]. The tempo estimate is simply the lag time corre-\nsponding the peak value of the windowed ACF, converted\nto BPM. Any peaks before the ﬁrst zero-crossing of the\nACF are disallowed to prevent spurious peaks near zero\nlag. A small modiﬁcation was made to the Ellis algorithm\nso that the top ten tempo candidates were returned rather\nthan a single best tempo estimate, deﬁned as the BPMs cor-\nresponding to the ten highest peaks in the windowed ACF.\nThese additional tempo estimates were used in the cluster-\ning method described below. For all other techniques, only\nthe best estimate for each component was used.\nEach component was tempo tracked in this way, result-\ning in ten candidate tempos for each component. This\nmeant that for a given track there were 80 tempo candi-\ndates (8 components ×10 estimates). The ACF value as-\nsociated with each candidate and the entire windowed ACF\nwere also stored, and these were used to help select the best\nglobal tempo estimate from the candidates.\n3.4 Tempo Selection\nBelow we describe several approaches to selecting a single\ntempo estimate from the candidates.\n3.4.1 Pulse-clarity\nInspired by the idea that certain components might accu-\nrately represent a relatively isochronous part of the track,\nthe ﬁrst approach focused on ﬁnding the best component\nfrom which to estimate the global tempo. That is, we at-\ntempted to ﬁnd the component with the clearest pulse, and\nthen choose the highest ranked tempo estimate for that one\ncomponent as the global tempo estimate.\nLartillot et al. [15] showed that several features of the\nACF are correlated with human judgments of pulse-clarity.\nIntuitively, the idea is that a relatively isochronous part\nwith clear onsets will lead to an ACF that has well-deﬁned\nand relatively large peaks. Following Lartillot et al., we\ncalculated the following features on the ACF: maximum,\nminimum, and kurtosis. Additionally we added entropy\nand sparseness [16] as features, with sparseness deﬁned\nas:\nsparseness(x) =√n−(/summationtext|xi|)//radicalbig/summationtextx2\ni√n−1(1)\n18410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nt\nt\ntttt\nt\ntPLCA\ncomponent component component component component\nT empo Tracking\nT empo Candidatest1t2t3t4t5t6t7t8\nClustering\ntt\ntttt\nt\ntt\ntt t tt\ntt\ntt\nt\nttt\ntt\ntttt\nttt t\ntt\ntt\ntt\ntt\nt\ntt t\nt\ntt\ntttt\ntt\nttt\nttt\nt\nt\nttt\nttt\nt\nt t\ntt\nGlobal T empo Estimate\nFigure 1 . Block diagram of tempo estimation algorithm\nBefore calculating these features the ACF was normal-\nized so that higher amplitude components would not domi-\nnate. For each component the ACF values were divided by\nthe sum of the absolute value of all ACF values. The max\nand min were simply the maximum and minimum ACF\nvalues after normalization, and we expected that larger ab-\nsolute values would correspond with greater pulse-clarity.\nKurtosis was used to measure the peakiness of the ACF,\ni.e. how well-deﬁned the ACF peaks were. Entropy and\nsparseness also assessed peakiness.\nEach feature was evaluated separately; Table 2 summa-\nrizes the performance of each feature (evaluation criteria\nare discussed in Section 4.2). It can be seen that the most\nobvious feature, the maximum ACF value, outperformed\nthe other measures on the IDM09 data, while they were\nabout equal on the MIREX06 data.\nIn order to make better use of pulse-clarity features,\nan attempt was made to apply them to a more system-\natic supervised machine-learning framework. For this, we\ntrained a multivariate Gaussian classiﬁer using a ten-fold\ncross validation scheme. In addition to the ACF features\nwe deﬁned a new set of features based on the ratios of\nthe candidate tempo estimates for each component. These\nfeatures were based on the idea that we would expect to\nsee harmonically related peaks in the ACF of rhythmically\nclear components. The ratio between every possible pair-\ning of the ten candidate tempos was computed, leading to/parenleftbig10\n2/parenrightbig\n, i.e. forty-ﬁve ratios per component. We then com-\nputed a histogram of these values in the range .45 to 2.05\nwith a bin width of .1, leading to 23 features. The targetswere binary, representing whether the component estimate\nmatched the ground truth. The tempo of an excerpt was\ncalculated by choosing the tempo associated with the com-\nponent that had the highest posterior probability, i.e. the\ngreatest likelihood of its tempo matching the ground truth\ngiven the ACF features. This approach worked well for the\nMIREX06 data but less so for the IDM09 data (Table 2).\nAt this point it is difﬁcult to say whether the IDM09 perfor-\nmance was due to an insufﬁciently large training database\nto accurately learn the multivariate distribution or if more\ndiscriminative features must be found.\n3.4.2 Clustering\nAnother method was attempted for determining the global\ntempo, based on the idea of taking a vote among the candi-\ndate tempos, possibly weighted by the corresponding nor-\nmalized ACF values. The basic intuition was that the true\ntempo should appear more frequently than spurious esti-\nmates among the candidate tempos. To implement this, we\nﬁrst partitioned the candidate tempos into clusters using a\nhierarchical cluster tree. However, a simpler approach that\ndid not attempt an exclusive partitioning performed bet-\nter. In the latter approach, the candidate tempos for all\ncomponents and their associated normalized ACF values\nwere merged into a single matrix. For each tempo candi-\ndate, a score was determined by summing the ACF values\nfor that tempo as well as for any tempos that were half or\ndouble, within a 5% tolerance. Of course such a method\nwill often lead to ties, which we resolved by choosing the\ntempo closest to 120 BPM. Because we chose to measure\n185Oral Session 2: Tempo and Rhythm\naccuracy accounting for half and double matches (see Sec-\ntion 4.2), this was not a major issue. The tempo candidate\nwith the highest score was chosen as the global tempo es-\ntimate. To see if ACF weighting was important we also\nperformed experiments ignoring ACF values and assign-\ning scores by simply counting the number of elements in\neach cluster. However, ACF weighting consistently im-\nproved performance and was chosen as the default. Ad-\nditionally, we experimented with multiplying each ACF\nvalue by the pulse-clarity estimate of the component based\non the heuristics described above. This did not affect re-\nsults and was therefore not included in the ﬁnal version.\n4. EVALUATION\n4.1 Databases\nEvaluation was performed on two databases. The main\ndatabase consisted of twenty-seven 30-second excerpts cho-\nsen from the IDM/glitch genre of electronic music (IDM09),\nwith an emphasis on tracks that we thought were rhythmi-\ncally complex and layered. For each excerpt, two inde-\npendent manual annotations were made.1For all excerpts\nthe human annotators agreed, with the exception of a few\nhalf/double conﬂicts. In those cases, we randomly selected\na single estimate. It should be noted that our accuracy mea-\nsure allowed for half/double errors.\nAdditionally, the twenty publicly available MIREX06\ntraining excerpts were used [2]. These consisted of a mix\nof genres and tempo ranges, and included annotation of\ntwo tempos representing the two highest peaks in a distri-\nbution of tempos calculated from listeners’ tapping times.\nFor our experiments we simply selected the ground truth\ntempo that was more commonly assigned.\n4.2 Accuracy measure\nWe deﬁned a match to be whenever the estimated tempo\nmatched the annotated tempo, or double or half the anno-\ntated tempo, within a ﬁve percent tolerance window. Eval-\nuation of tempo detection algorithms is somewhat depen-\ndent on the end-goal. We might reasonably hope that the\ntempo detection algorithm would correspond to judgments\nof human listeners. However, although there may be a fair\ndegree of reliability between judgments for simple rhythms,\nthere can be substantial disagreement about the appropri-\nate metrical level or even the tempo for more rhythmically\ncomplex music. Moreover, more experienced listeners of-\nten tap at a lower metrical level (i.e. slower tempo) than\nnovice listeners and in some cases novice listeners tap ir-\nregularly and are unable to clearly sense the tempo. Al-\nthough this may be trivially true for music with no clear\nrhythm, this can also occur for music where there is a\nhigh degree of reliability for experienced listeners. For\nretrieval tasks, such as selecting tracks with similar tem-\npos, it might be more appropriate to consider a match only\nwhen the metrical level of the main ground truth annota-\ntion is matched. On the other hand, for transcription or\n1The IDM09 database and the tempo annotations will be made pub-\nlicly available online.Baseline (Ellis) Clustering Change\nMIREX06 0.50 0.60 0.10\nIDM09 0.26 0.48 0.22\ncombined 0.36 0.53 0.17\nTable 1 . The primary results are summarized here for each\nof the databases as well as for the combined set. The base-\nline is the Ellis algorithm run on the unseparated excerpts.\nClustering refers to choosing the global estimate according\nto the procedure described in Section 3.4.2\nsynchronization tasks it is appropriate to consider matches\nat different metrical levels. Because our emphasis here was\non IDM, a genre that often contains metrical level ambigu-\nity, we decided that this latter deﬁnition of accuracy made\nthe most sense.\n4.3 Results\nTo get a sense of the upper-bound of performance for each\ntrack we checked to see if the true tempo was the pri-\nmary tempo estimate for any of the components, and also\nwhether the true tempo was present in any of the candi-\ndate tempos. Since subsequent steps attempt to ﬁlter these\nvalues, our pulse-clarity based technique can do no bet-\nter than this ﬁrst value, and the clustering method can do\nno better than the latter. The primary component tempo\nwas correct for 70.4% of excerpts from IDM09 and 75%\nof MIREX06. A match was found in a candidate tempo\nof one of the components 96.3% and 85% of the time for\nIDM09 and MIREX06 respectively. Of course it should\nbe noted while that we would expect this percentage to in-\ncrease as the number of candidate tempos per component\nincreases, the number of false positives will also tend to in-\ncrease. Nevertheless these data suggest a high performance\nceiling.\nTable 1 summarizes the main the results, while Table\n2 provides a more complete view of the performance of\nthe different algorithms described in the paper. The ﬁrst\ncolumn in both tables is the baseline performance, given\nby running the Ellis algorithm on the unseparated excerpt\nusing the deﬁnition of accuracy described above. Base-\nline accuracy for the MIREX06 data was 50% and 25.9%\nfor IDM09. The substantially lower baseline accuracy for\nIDM09 reﬂects the rhythmic complexity of these excerpts.\nIt can be seen that for the MIREX06, IDM09, and com-\nbined databases that the clustering algorithms improved\naccuracy by 10% (60% vs 50%) , 22.3% (48.2% vs. 25.9%)\nand 17% (53.2% vs. 36.2%), respectively. From the de-\ntailed results table we can see that the ML-based approach\nachieved a 10 percentage-point improvement on MIREX06\n(60% vs 50%), and a 3.7 percentage-point increase on IDM09.\nClustering using multiple candidates per component, as\nwell as the ML-based approach, had an accuracy of 60%\nfor MIREX06, a 10% improvement on the baseline. In\nthe case of IDM09 there was a substantial improvement of\n22.3% (48.2% vs. 25.9%). However in this case the ML-\nbased approach was only marginally better than baseline.\n18610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nBaseline Pulse Clustering ML\nEllis min max entropy kurtosis sparseness all features pulse-only\nMIREX06 0.50 0.35 0.40 0.40 0.40 0.40 0.60 0.60 0.60\nIDM09 0.26 0.15 0.33 0.26 0.26 0.30 0.48 0.27 0.26\ncombined 0.36 0.23 0.36 0.32 0.32 0.34 0.53 0.43 0.40\nTable 2 . Detailed accuracy results for each of the pulse-clarity measures described in Section 3.4.1 as well as for the\nmachine learning algorithm also described in Section 3.4.1. For the ML algorithm results are shown for all features, as well\nas with only the original pulse heuristic features.\nFor both data sets using pulse-clarity alone did not im-\nprove results, with the exception of the max ACF (33.3%\nvs. 25.9%) and sparseness (29.6% vs 25.9%) features for\nIDM09.\n5. DISCUSSION AND CONCLUSION\nFrom these data it seems that the clustering-based approach\nis the superior method, particularly when compared to us-\ning a single component as the basis for the global estimate.\nIt is possible, however, that this is simply an artifact of\nan inaccurate source separation step. Auditioning compo-\nnents reveals that many components are not true sources\nat all but parts of sources or several sources; source sep-\naration is still a delicate art. Nevertheless, many compo-\nnents do clearly correspond to parts and at times one can\nclearly hear time-keeping parts popping out. This noisi-\nness probably accounts for the fact that the clustering ap-\nproach, which retains more information about possible pe-\nriodicities by retaining multiple tempo estimates for each\ncomponent, is more robust. Although the current work did\nnot bear out the ML-based approach, we believe that sys-\ntematic incorporation of multidimensional rhythmic infor-\nmation will play an important part in future component-\nbased tempo detection algorithms.\nWe have shown that for these data, using source separa-\ntion in conjunction with clustering can substantially im-\nprove results, particularly for rhythmically complex and\nlayered material. We have also explored a variety of tech-\nniques for implementing the core idea of using source de-\ncomposition to improve tempo estimation. In particular,\nwe developed techniques for tempo estimation based on\npulse clarity scoring of components and clustering of com-\nponent tempo estimates. As source separation techniques\nimprove, it should be possible to more closely mimic the\nrhythmic perception of humans, which in many cases is\nbased on recognizing distinct parts that have a time-keeping\nfunction.\nWe expect that the approach described here will be most\nuseful for layered, rhythmically complex music that tends\nto have simpler sub-parts. For simpler music, on the other\nhand, the less dramatic results are unlikely to justify the\ncomputational cost of source separation. We expect that\nthis method will fail for music where the rhythm is emer-\ngent, i.e. only becomes apparent when several layers are\nplayed simultaneously.6. FUTURE WORK\nThere are many possible extensions to this work. Thus\nfar we have done little work to tune the source separa-\ntion step. For example, what is the optimum number of\ncomponents? It is likely that eventually this should be\nset adaptively based on the characteristics of the piece and\nthe likely number of sources. These, however, remain un-\nsolved problems, though the recent surge in research on\nsingle-channel source separation using PLCA and NMF\nis likely to dramatically improve our unmixing capabili-\nties. Additionally, we intend to pursue the ML-based ap-\nproach. In the long-run, it is likely that some combination\nof features can be found that will determine more reliably\nwhether a component tempo estimate is the correct global\nestimate. And, as always, only with the expansion of the\ntempo database, and additional benchmarking against mul-\ntiple systems, will we truly be able to assess the strengths\nand weaknesses of the techniques presented here.\n7. REFERENCES\n[1] Martin F. McKinney and Dirk Moelants? Ambiguity\nin tempo perception: What draws listeners to differ-\nent metrical levels? Music Perception , 24(2):155–166,\n2006.\n[2] M. F. Mckinney, D. Moelants, M. E. P. Davies, and\nA. Klapuri. Evaluation of audio beat tracking and mu-\nsic tempo extraction algorithms. Journal of New Music\nResearch , 36(1):1–16, 2007.\n[3] Daniel P. Ellis. Beat tracking by dynamic program-\nming. Journal of New Music Research , 36(1):51–60,\n2007.\n[4] M. E. P. Davies and M. D. Plumbley. Beat tracking with\na two state model. In Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing , 2005.\n[5] Matthew Wright, Andrew Schloss, and George Tzane-\ntakis. Analyzing afro-cuban rhythm using rotation-\naware clave template. In Proceedings of International\nConference on Music Information Retrieval , 2008.\n[6] Simon Dixon. Evaluation of the audio beat track-\ning system beatroot. Journal of New Music Research ,\n26(1):39–50, 2007.\n187Oral Session 2: Tempo and Rhythm\n[7] K. Seyerlehner, G. Widmer, and D. Schnitzer. From\nrhythm patterns to perceived tempo. In Proceedings\nof International Conference on Music Information Re-\ntrieval , 2007.\n[8] Linxing Xiao, Aibo Tian, Wen Li, and Jie Zhou. Us-\ning a statistic model to capture the association between\ntimbre and perceived tempo. In Proceedings of Inter-\nnational Conference on Music Information Retrieval ,\n2008.\n[9] Eric D. Scheirer. Tempo and beat analysis of acoustic\nmusical signals. The Journal of the Acoustical Society\nof America , 103(1):588–601, 1998.\n[10] Masataka Goto. An audio-based real-time beat tracking\nsystem for music with or without drum-sounds. Jour-\nnal of New Music Research , 30(2):159–171, 2001.\n[11] Paris Smaragdis, Bhiksha Raj, and Madhusudana\nShashanka. Supervised and semi-supervised separation\nof sounds from single-channel mixtures. In Proceed-\nings of the 7th International Conference on Indepen-\ndent Component Analysis and Signal Separation , Lon-\ndon, UK, September 07.\n[12] Paris Smaragdis and Bhiksha Raj. Shift-invariant prob-\nabilistic latent component analysis. Technical report,\n2007.\n[13] Christian Uhle, Christian Dittmar, and Thoma Sporer.\nExtraction of drum tracks from polyphonic audio us-\ning independent subspace analysis. In 4th International\nSymposium on Independent Component Analysis and\nBlind Signal Separation , 2003.\n[14] Marko Heln and Tuomas Virtanen. Separation of\ndrums from polyphonic music using non-negative ma-\ntrix factorization and support vector machine. In In:\nProc. EUSIPCO2005. (2005 , 2005.\n[15] Olivier Lartillot, Tuomas Eerola, Petri Toiviainen, and\nJose Fornari. Multi-feature modeling of pulse clarity:\nDesign, validation, and optimization. In Proceedings\nof International Conference on Music Information Re-\ntrieval , 2008.\n[16] Patrik Hoyer. Non-negative matrix factorization with\nsparseness constraints. Journal of Machine Learning\nResearch , 5:1457–1469, 2004.\n188"
    },
    {
        "title": "Exploring Social Music Behavior: An Investigation of Music Selection at Parties.",
        "author": [
            "Sally Jo Cunningham",
            "David M. Nichols"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416410",
        "url": "https://doi.org/10.5281/zenodo.1416410",
        "ee": "https://zenodo.org/records/1416410/files/CunninghamN09.pdf",
        "abstract": "This paper builds an understanding how music is currently listened to by small (fewer than 10 individuals) to medium-sized (10 to 40 individuals) gatherings of people—how songs are chosen for playing, how the music fits in with other activities of group members, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of participant observations and interviews focusing on the selection of songs to play at social gatherings. We suggest features for software to support music playing at parties.",
        "zenodo_id": 1416410,
        "dblp_key": "conf/ismir/CunninghamN09",
        "keywords": [
            "music selection",
            "social gatherings",
            "song choice",
            "hardware/software",
            "participant observations",
            "interviews",
            "features for software",
            "qualitative analysis",
            "current listening habits",
            "medium-sized groups"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   EXPLORING SOCIAL MUSIC BEHAVIOR:  AN INVESTIGATION OF MUSIC SELECTION AT PARTIES Sally Jo Cunningham         David M. Nichols Department of Computer Science University of Waikato Hamilton, New Zealand {sallyjo, dmn}@cs.waikato.ac.nz ABSTRACT This paper builds an understanding how music is cur-rently listened to by small (fewer than 10 individuals) to medium-sized (10 to 40 individuals) gatherings of peo-ple—how songs are chosen for playing, how the music fits in with other activities of group members, who sup-plies the music, the hardware/software that supports song selection and presentation. This fine-grained context em-erges from a qualitative analysis of a rich set of partici-pant observations and interviews focusing on the selec-tion of songs to play at social gatherings. We suggest fea-tures for software to support music playing at parties. 1. INTRODUCTION Our experience of music includes both individual and group settings. When music is heard in social situations [1] the question which follows is: who selects the music? In this paper we explore issues of social music selection in the context of small private gatherings such as parties. The portability of digital music, on devices such as iPods, enables people to easily bring their own music but the selection of music to be played is largely based on the social roles of the participants. Ethnographic methods are used to understand these settings and so inform the de-sign of systems for supporting shared music experiences. Section 2 outlines previous work on social music sys-tems. We then describe our methods and discuss the sup-port provided by media players. Section 5 outlines the collaborative nature of music selection and we conclude by comparing our results with existing systems.  2. EXISTING SOCIAL MUSIC SYSTEMS There is “little in the literature to suggest how to design new and unique tools that facilitate social music use within and between the different contexts in which people work, play, and otherwise live their lives” [2]. Rentfrow and Gosling [3] show that music plays an important part in many peoples’ lives. Music is a conversation topic, “individuals’ music preferences convey consistent and accurate messages about their personalities” and music-genre stereotypes are used when forming opinions of oth-ers [3]. Further, “synchronized music consumption among people in physical proximity, as it happens in clubs or during parties, can create a strong emotional connection, more than what an asynchronous download of music over distance could provide” [4]. However, North et al. [1] note that a “lack of ecological validity” constrains much of the research on the social and psychological impact of music in everyday life. North et al. also report that their “data indicate that the great majority of listening episodes occurred in the pres-ence of other people”. Several systems have been de-signed to enhance this shared experience of music. Several social music systems (e.g. SocialPlaylist [5], tunA [4], Push!Music [6]) use personal mobile technol-ogy, such as iPods, PDAs and mobile phones, reflecting the increasing portability of music collections [7]. Liu and Reimer [5] recommend that such systems provide smooth integration between personal and social modes, as inevitably users will occasionally prefer individual selec-tions. In tests of Push!Music the “sharing of music be-came a prompt for social interaction, but this happened only between users who already knew each other and were socializing face-to-face” [6]. Nettamo et al. [8] note that even though mobile music is widespread, music in the home is often played via computers and that the “home PC acted as music hub”. An alternative to these person-to-person mobile forms of shared music is to allow voting or collaborative recom-mendation in public spaces. Deployment of the Jukola system [9] allowed users to express their musical prefer-ences and this encouraged “debate, conversation and ne-gotiation around music.” [9]. Pering et al. [7] used inter-views and observations of music in shared spaces to iden-tify four key types of stakeholders:  providers, contribu-tors, proprietors, and listeners. Pering et al also note that the use of audio in shared spaces today may well soon be generalized to other media such as photos and video [7]. The MUSICtable system is designed for a further context of use, the “private social gathering” [10]: The user interface of the PC-based digital music player clearly does not support music selection by multiple people in a social situation. One manifesta- Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2009 International Society for Music Information Retrieval  \n747Oral Session 9-B: Sociology and Ethnomusicology\n   tion…is…“separate party syndrome,” wherein a small number of people tend to gather around the desktop computer … dominating the selection of music.  The PartyVote system [11] takes the voting concept from Jukola and applies it in small group situations such as parties. Both PartyVote and MUSICtable also include visualizations to provide awareness feedback to the vot-ers. The Smart Party [12] reflects the preferences of users by dividing a party into several rooms, where each room’s music adapts to the preferences of the people pre-sent, changing as guests move around the party. Bluemu-sic supports the same idea, personalization by being there, using Bluetooth from portable devices [13]. Many of these systems are designed without an obvious grounding in the detailed behavior of users with current music technology in social situations [2]. In this paper we investigate the use of music in these “private social gath-erings” through ethnographic methods. We also compare our findings with those from other social music settings and consider the interaction between the setting, the us-ers’ roles, the technology and the resulting experience. 3. DATA GATHERING Our research uses data collected in a third year university HCI course. The course focuses on qualitative and quan-titative techniques for gaining an understanding of user needs, goals, and preferences, and using these insights to inform the user requirements and initial prototyping stages of a user-centered software development effort.  This course adopts the ‘practical approach’ to incorporat-ing ethnography into software design, as advocated by Randall et al. [14]. Students work individually over the semester to design and prototype a system based around the given focus application, where their designs are in-formed by a series of ethnographic investigations into behavior associated with the application domain.   In 2008, the students explored the problem of designing a system to support groups of people in selecting and playing music. They began by performing participant observations of social gatherings that included music, with the observations focusing on how the music is cho-sen for playing, how the music fits in with the other ac-tivities being conducted, who supplies the music, and how/who changes the songs or alters the volume. The students then explored subjective social music experi-ences through interviews, both of themselves (‘auto-ethnographies’ [15]) and of a friend. These interviews explored aspects of a social gathering that made it more, or less, likely for attendees to participate in selecting the songs, and the social factors that made attendees feel more, or less, comfortable in selecting and playing mu-sic. The students also critiqued the usefulness of existing systems for collaborative music selection and playing in social situations. Thirty student investigators gathered ethnographic data (Table 1). The students were encouraged to construe ‘so-cial gathering’ very broadly, and so performed partici-pant observations in a variety of settings (including car trips, bars, café’s, private homes, and religious institu-tions) and with a range of size (from two friends in a dormitory room to a hundreds at a rave). For this paper we focus on small- (10 or fewer attendees) and medium-sized gatherings (from 10 to 40 attendees), that are not professionally organized or occur in commercial settings (e.g., informal parties in student flats, birthday parties, a Friday night get-together, a computer gaming session, etc.). Music might be the primary focus of the event (e.g., a gathering to listen to a friend’s new CDs) or be a part of the background (e.g., a quiet evening of conversa-tion). This focus often shifts—a party may begin with a meal accompanied by soft music, move to louder music and dancing, and cycle back and forth through the eve-ning. A set of 43 participant observations met these criteria:  29 small- and 14 medium-sized gatherings. The observa-tions lasted from a minimum of 15 minutes to a maxi-mum of 4 hours, with an average of a little over 2 hours (113 minutes). A total of 88 interviews provided deeper interpretations of the observation experiences. In the fol-lowing sections, the investigators are identified by a let-ter/number code (e.g., Participant K, Participant A2). Table 1. Characteristics of student investigators Male Female National Origin Count 34 6 NZ/Australia 14   China 9   Mid-East 5   Other 2 Grounded Theory methods [16] were used to analyze the student summaries of their participant observations, in-terviews, and system critiques. With Grounded Theory, the researchers attempt to generate theory from data, through an inductive analysis of the data. This present paper teases out the behaviors and social issues that in-fluence how songs are selected for playing at parties. 4. MEDIA AND MUSIC PLAYER SUPPORT  The participant observations reference a staggering array of music media and players. While the majority of social gatherings were supported by digital media and players, as was expected, two included cassette tapes and several included ‘old school’ music CD players with only the ru-dimentary play, pause, and skip controls. The limitations imposed by cassettes and basic CD players are signifi-cant: it is not possible to browse through a cassette or a CD on a simple player; the songs must be played in the original sequence; both contain a limited number of songs, and skipping past songs that are disliked or that do not fit the developing atmosphere of the party further re-\n74810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   duces the play time. Physically changing a cassette or CD introduces silences that may break the mood (though in practice it can be faster to physically swap out a CD than for an inexperienced user to wrestle with selecting songs using an unfamiliar piece of software).  An advantage of the cassette and simple CD player is that they can highlight musical knowledge or expertise in their owner. A carefully compiled party-themed mix cas-sette or mix CD can showcase the creator’s ability to es-tablish and sustain a mood—though at the cost of not be-ing able to fine-tune the songs or their sequence of play as the event unfolds. Portable MP3 players and computer–based music systems feature in the majority of gatherings reported in the par-ticipant observations, some quite elaborate (for example, [D] describes a setup in which ‘the music was on a com-puter in another room and being streamed to the X-box 360 via a wireless network. The TV and X-box are con-nected to an amplifier that powers the surround speakers and the sub-woofer’). Similar home computer based mu-sic setups are described in [8]. The MP3 players, laptops, and portable hard drives can support extensive music collections, but is there ever enough storage space? (‘Music was provided by me, but it was limited because I had a selection of songs on a 250GB hard drive …’  [U]).  An MP3 player is not ideal as the primary device for se-lecting and playing songs; the limited physical controls (in particular, the lack of a keyboard) and the small or non-existent display (‘the information you can see at one time is limited’ [X]) make it difficult to search or browse through a collection to select songs. The larger display of a laptop or desktop computer affords music organization software that includes a larger set of searching and browsing facilities, but these more busy interfaces can be confusing to unfamiliar users. Further, there is no stan-dard music organizer (Winamp, Windows Media Player, iTunes, and the command line MPlayer are mentioned in the participant observations and interviews), so the likeli-hood of a party-goer encountering an unfamiliar setup is high. As will be discussed in Section 5, fear of making mistakes while selecting music can deter people from participating in the selection of songs in a social situation. 5. COLLABORATIVE SELECTION OF MUSIC We describe patterns in the social setting and expecta-tions for playing, choosing, and changing music. 5.1 The Host, Guest of Honor, and Guests A social gathering generally includes at least one Host (who may provide the venue and initiate implicit or ex-plicit invitations, and who feels a sense of responsibility for creating an enjoyable occasion), and one or more Guests (attendees at the event). If music is a part of the event—and it commonly is—then the host can be ex-pected to provide the initial stock of songs for a party and the hardware/software needed to play them. Choosing appropriate music is a significant responsibility:  the set of songs played at an event and the order of play can have a dramatic impact on the atmosphere (Section N) of the event. Poor selection can have social repercussions:  for example, Participant D reports of an interviewee that, ‘…she likes to host parties and have friends over and if they thort [sic] she had crap music or played crap music then they would not come over any more.’ Before the party, the Host creates the initial party playlist.  This preparation may occur at the beginning of the party itself ([D]: ‘me and one of the others spent about 30 min on the computer in the other room creating a play list of songs to be listened to’), or begin well in advance ([C]: “downloading music for the party a few days before-hand’). Frequently the playlist is crafted specifically for the event, but a Host may also develop a generic, re-usable Party collection. Sometimes Guests contribute to the party collection, be-forehand or at the beginning of the party or as the party progresses--though the latter might be a bit of an insult to the host, as the unsolicited provision of supplemental mu-sic implies that the host’s selections are not suitable ([D], of interviewee: ‘if [the music at the party] ‘sucked’ then she would bring a CD or something so that she could change it.’).  The Host is more likely to invite Guests to contribute songs to a party if the Host is unsure of their musical tastes or if it is a formal or commemorative occa-sion (for example, a 21st birthday party). The Host retains responsibility, however for selecting the final party play-list from the pool of contributions. Some parties (eg, birthday) feature a special Guest.  The Guest of Honor may or may not also be the Host, but the Guest of Honor assumes a similar role. Where the Host’s song selections might be later altered by Guests (Section 5.2), the Guest of Honor’s usually are not—changes to a Guest of Honor’s playlist would constitute a more serious breach of party protocol.  For example, Participant G re-ports, ‘The music was selected by one person only, throughout the entire night. This person was the birthday girl. … The ipod was sitting on the stereo during the party, but it was made clear that nobody else was to ad-just the music, except for the birthday girl.’ 5.2 The Invitation The Host normally assumes initial control over the music selection; as the event progresses, the Host may maintain control throughout the occasion, or may pass control to others. Permission to alter the gathering’s playlist is passed through The Invitation: the Host explicitly or im-plicitly invites others to browse available collections and select songs. A Host might overtly encourage Guests to add, delete, or re-order songs on the playlist (‘As host I will always make a short play list, long enough to last un-\n749Oral Session 9-B: Sociology and Ethnomusicology\n   til most people arrive, then encourage anyone who com-ments on the music to change it.’ [X]), or might more subtly indicate that alterations are acceptable by leaving the control to the music playing device in an easily acces-sible spot. 5.3 Maintaining the Atmosphere While music is generally not the main focus for the gath-erings described in the participant observations, it was an integral part of the occasions. Awareness of the music naturally ebbs and flows. When conversation flags, Guests are more attentive to the songs playing (music ‘is there because during the breaks when people are not talk-ing, the atmosphere feels awfully quiet so music helps lighten the mood’ [K]). A song can spark new conversa-tion by serving as a reminder of earlier occasions (‘Par-ticipant A mentioned on more than one occasion a song’s significance in his life, e.g. “When I was at school we used to play this song on our stereos at lunchtime”’ [L]). Guests frequently show an interest in discussing unfamil-iar songs (eg, [L] reports that Guests would ‘occasionally ask questions or make comments about a certain song like, “who sings this song, I really like it” or “we should get this song”). An interest in learning more about new music is expected given that the music was selected in anticipation that it matches the Guests’ tastes. A common pattern is for a gathering to begin with quieter or less obtrusive music during an initial ‘socializing’ phase, move to ‘faster, louder and less organized’ [A] songs, and then end the evening with ‘chilling out’ music. A skilled Host monitors the Guests’ interest in the music and its affect on the gathering’s ambiance, and modifies the music when necessary to create or enhance the appro-priate atmosphere (‘During the night there were a number of situations where the music and the mood needed to change’ [I]).  5.4 Skipping, Sampling, Searching, and Browsing Changes to the party playlist are of two types:  deletion of undesirable songs and insertion of new songs. The usual case for deletion is to stop the current song that is playing and move to another song—skipping. The simplest strat-egy for choosing a replacement song is simply to skip se-quentially through the playlist, playing a few moments of each song (sampling) until an acceptable one is encoun-tered. The audio effect is less than ideal, given the abrupt ends of skipped songs, but one benefit for the user is that the interaction is selecting a new song involves simply clicking a single ‘next song’ button.  Skipping is also the strategy of choice for Guests who are unfamiliar with the unfamiliar with the searching/browsing facilities of the music software (‘…people will feel uncomfortable [if they] stand in front of the computer for a long time, while they are finding the music they want to listen [to]’ [T]), and/or Guests who do not want to expose their ignorance of the gathering’s preferred music genres:  ‘…when I get told [to] change the music I will simply skip enough songs until I find an improve-ment … For me this is mostly because I do not remember song names or even mainstream artists. If I have to chose music I have to resort to one of the limited number of artists I know or pick ran-domly.’ [X] Sampling may also occur outside the context of skipping, when a Guest or Host attempts to identify desirable songs from a pool of potential additions to the party playlist. If the individual cannot identify songs by the available metadata, then a ‘good’ sample is needed to decide whether to include a song on the playlist (where ‘good’ probably includes the chorus or other characteristic sec-tion of the song, rather than the beginning; ‘often they want to skip to the chorus of a song to find out if it is the song they actually want’ [G].  Searching and browsing to select songs are more rarely reported in the participant observations than skipping and sampling. Searching for a specific song requires some confidence that that song is actually accessible, and pos-sibly knowledge of its approximate location on the physi-cal device (‘Participant B requested that another song she knew to be on the laptop be played. Participant A then queued up the requested song (which resided in a differ-ent directory)’ [L]). Effective browsing (in the absence of sampling) require a deep familiarity with the specific songs in the collection: [X] reports of one successful browser that ‘he has a much broader knowledge [of mu-sic] …  he is able to scroll through large number of music titles and understand what many of them sound like based on the artist and title given and decide if they would be appropriate.’ 6. SUPPORTING SOCIAL MUSIC USE The specific music organization and selection software mentioned in the participant observations and interviews include Winamp, Windows Media Player, iTunes, and the command line MPlayer. Given that music is frequently an integral part of social, festive occasions, it is surprising that only iTunes avoids a clinical, ‘somewhat dark’ [X] appearance. It seems appropriate that interfaces should enhance the enjoyment and entertainment that people ex-perience when listening to music and interacting with music collections—interfaces should be attractive and playful, appropriate to an enjoyable social gathering. Existing music organization software was found to be adequate for supporting the Host in developing the initial party playlist—which is not surprising, given that the Host is usually interacting with his/her personal system and music collection. Difficulties arise when Guests or multiple Hosts contribute to the pre-party development of the initial party playlist (Section 5.1).  Songs arrive on a variety of media (flash drives, MP3 players, CDs, exter-nal hard drives, downloaded from the Web), and in trans-ferring them to the Host’s system it is easy to lose meta-data (artist, title, genre, etc.) or to discover that metadata values and schemes are incompatible (particularly genre). Better support is needed for creating a pool from multiple \n75010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   sources, and integrating them into a single (possibly ephemeral) collection. As contributions are pooled, event-specific information is easily lost (eg, who contributed a song, which party Guests it might appeal to, and so forth)—further stressing the Host as s/he makes the final decisions on which songs to eliminate. Additional support for merging contribu-tions into a draft playlist would be welcome—perhaps by encouraging contributing Guests to annotate groups of songs with a justification for its inclusion (for example, whether the songs have a strong beat for dancing, are suitable for chilling out at the end of the evening, etc.). An annotation facility is particularly important because inclusion criteria can be complex, idiosyncratic, and not well-matched to conventional metadata; eg, songs ‘that a lot of people would know so they would sing along’ [D], or songs suitable for both high school age and ‘old(er) guests’ [I].  It could be helpful to allow pre-party con-tributors to view the draft playlist and cast votes for the retention / deletion of songs, with the Host retaining ulti-mate responsibility for setting the initial playlist.  Creating the initial party playlist requires a great deal of insight into the musical tastes of the Guests and the an-ticipated atmosphere of the party, and a great deal of skill to match those to the available songs. It is difficult to see how this can be automated effectively. Smart Party [12], for example, automatically builds playlists from party at-tendees’ personal music devices—a strategy that initially appears reasonable.  But music in personal collections may not be suitable for public listening.  An earlier study reports that subsets of a personal collection may represent ‘guilty pleasures’ (music that does not fit the public per-sona of the collection owner) [17], and it could be awk-ward to have those songs appear in a public party playlist. More fundamentally, personal favorites may not be suit-able for a given social occasion:  ‘music in a group situa-tion that is good for that occasion can be vastly different than music I would usually listen to normally. … music that is listened to with other people and when you are less focused on it can be different to your usual tastes or that you will put up with it even if you don’t like it that much to not distance yourself from the group you are with.’ [R] Voting systems such as Jukola [9] and Party Vote [11] may finesse the potential social minefields of correctly interpreting when The Invitation has been given (Section 5.2), by providing an impartial mechanism for suggesting alterations to the music lineup without offending the Host (Section 5.1), and generally foiling ‘pushy people, some-one who just plays their own music and wont [sic] take any consideration for others’ [E]. These systems, like the old-fashioned jukeboxes they derive from, clearly give permission to choose music—that is their primary func-tion. [F] explicitly makes this connection, commenting that he particularly likes jukeboxes because ‘the fact that the whole system is set up just so party goers can select music made me feel totally comfortable with using it to select the music I wanted to hear.’ As social gatherings are an opportunity to be exposed to new music (Section 5.3), it should be easy for Guests to access further information about the songs that are played. At a minimum this should include an easy-to-read screen that features basic song metadata (artist, title), with access to more detailed records (eg, lyrics, genre, ‘maybe a little trivia associated with the song/artist’ [L]).  A few participant observations report Guests attempting to use the party’s music software to learn more about a song, but an inexpert user can too easily select a song to play when intending to view its metadata (‘This would cause people to get annoyed as they would be listening to a song and then someone would skip to a different one part way through [D]). Ideally there would be a clear dif-ferentiation between the interface elements that support playing music and those elements that support brows-ing/searching collections. The skipping strategy for moving past songs that are dis-liked or that are inappropriate to the gathering’s mood can itself disrupt the party’s atmosphere; it is annoying and disconcerting for the song to stop abruptly (‘People will have a notice when the song was changed in the middle…’  [T]). The availability of a crossfade effect to smooth song transition would not completely solve the problem, but would be an improvement. Sampling, or listening to brief portions of a song to make a play / no play decision (Section 5.4), could be made more efficient by allowing the user to skip to the chorus or other readily identifiable song extract (e.g., [18]). Alternatively, though aurally not as satisfactory, the system could support skimming through a song by increasing the speed of play (‘allows them to quickly listen to the feature of the song’ [P]). A common scenario for sampling involves using it to build up a sequence of songs to play. But for the music management software encountered in the participant ob-servations, it was difficult to ‘stack’ selections, so the person choosing plays samples until a single acceptable song is identifies; that plays, and then sampling begins again. If this occurs during the party, the mood can be significantly disrupted. Ideally, the user would be able ‘to (privately) listen to previews of songs, … to make in-formed decisions on the music they select’ [L], similar to the ‘previewing’ of tracks in professional DJ software. Effectively supporting music searching and browsing (Section 5.4) remains an open research problem.  Several student investigators suggested the inclusion of lyrics metadata would be the simplest and most straightforward way to support both direct search and browsing (by al-lowing the user to ‘skim’ a song without hearing the audio). Lyrics would also be helpful for gathering at-tendees who wished to sing along to the music. \n751Oral Session 9-B: Sociology and Ethnomusicology\n   The clearest directive that emerged from the participant observations was the importance of a simple, clean inter-face design, preferably with large, clearly labeled controls whose operation do not require fine motor movements. Interaction sequences should be brief and each step in a sequence should be clearly signaled, in large font (‘nor-mally the University student would absolutely drink beer while having a party’ [Y]).  7. CONCLUSIONS This study presents a rich picture of collaborative music selection among a large group of (primarily) university students in New Zealand. As such, the insights gained must be treated cautiously—a logical next step is to ‘tri-angulate’ through further studies involving participants with different backgrounds.  The environmental conditions revealed in the party ob-servations differ significantly from the austere, controlled environment of a usability laboratory—and so lab testing would be likely to miss significant issues. Testing of a collaborative music system should occur in authentic en-vironments and real social situations, to ensure that the interface is usable with, for example, limited lighting, a noisy setting, and intoxicated users. 8. REFERENCES [1] A.C. North, D.J. Hargreaves, J.J. Hargreaves: “Uses of Music in Everyday Life,” Music Perception, Vol. 22 No.1, pp. 41–77, 2004. [2] F. Bentley, C. Metcalf, G. Harboe: “Personal vs. commercial content: the similarities between consumer use of photos and music,” Procs of CHI’06, pp. 667-676, 2006. [3] P.J. Rentfrow, S.D. Gosling: “Message in a Ballad: The Role of Music Preferences in Interpersonal Perception,” Psychological Science Vol. 17 No. 3, pp. 236-242, 2006. [4] A. Bassoli, J.  Moore, S. Agamanolis.:  “tunA: socialising music sharing on the move,” In K. O'Hara and B. Brown (Eds), Consuming Music Together: Social and Collaborative Aspects of Music Consumption Technologies, Dordrecht: Springer, pp. 151-172, 2006. [5] K. Liu, R.A. Reimer: “Social playlist: enabling touch points and enriching ongoing relationships through collaborative mobile music listening,” Procs of MobileHCI '08, pp. 403-406, 2008. [6] M. Håkansson, M. Rost, L.E. Holmquist: “Gifts from friends and strangers: a study of mobile music sharing,” Procs of  ECSCW’07, pp. 311-330, 2007. [7] T. Pering, R. Want, L. Gardere, K. Vadas, E. Welbourne: “Musicology: Bringing Personal Music into Shared Spaces,” Procs of Fourth Annual International Conference on Mobile and Ubiquitous Systems (MobiQuitous 2007), pp.1-8, 2007. [8] E. Nettamo, M. Nirhamo, J. Häkkilä : “A cross-cultural study of mobile music: retrieval, management and consumption,” Proceedings of the 18th Australia Conference on Computer-Human Interaction (OZCHI '06), pp. 87-94,  2006. [9] K. O'Hara, M. Lipson, M. Jansen, A. Unger, H. Jeffries, P. Macer: “Distributing the Process of Music Choice in Public Spaces,” In K. O'Hara and B. Brown (Eds), Consuming Music Together: Social and Collaborative Aspects of Music Consumption Technologies, Dordrecht: Springer, pp. 87-109, 2006. [10] I. Stavness, J. Gluck, L. Vilhan, S. Fels: “The MUSICtable: A Map-based Ubiquitous System for Social Interaction with a Digital Music Collection,” Procs of International Conference on Entertainment Computing (ICEC05), pp. 291-302, 2005. [11] D. Sprague, F. Wu, M. Tory: “Music selection using the PartyVote democratic jukebox,” Procs of AVI '08, pp. 433-436, 2008. [12] K. Eustice, V. Ramakrishna, N. Nam, P. Reiher: “The Smart Party: A Personalized Location-Aware Multimedia Experience,” Procs of the 5th IEEE Consumer Communications and Networking Conference (CCNC 2008), pp.873-877, 2008. [13] H. Mahato, D. Kern, P. Holleis, A. Schmidt:  “Implicit personalization of public environments using Bluetooth,” Extended Abstracts of CHI '08, pp. 3093-3098. 2008. [14] Randall, D., Harper, R., Rouncefield, M.:  Fieldwork and Ethnography: A perspective from CSCW, Proceedings of EPIC 2005, pp. 81-99, 2005. [15] Cunningham, S.J., Jones, M.: “Autoethnography: a tool for practice and education,” Procs of the 6th New Zealand Int. Conf. on Computer-Human Interaction (CHINZ 2005), pp. 1-8, 2005. [16] Glaser, B., and Strauss, A.:  The Discovery of Grounded Theory: Strategies for Qualitative Research, Chicago, 1967. [17] Cunningham, S.J., Jones, M., and Jones, S.: “Organizing digital music for use: an examination of personal music collections”. Procs of the ISMIR04, Barcelona (October 2004), pp. 447-454, 2004. [18] Goto, M.: A chorus section detection method for musical audio signals and its application to a music listening station. IEEE Trans on Audio, Speech, and Language Processing, 14(5), pp. 1783-1794, 2006 \n752"
    },
    {
        "title": "Estimating the Error Distribution of a Single Tap Sequence without Ground Truth.",
        "author": [
            "Roger B. Dannenberg",
            "Larry A. Wasserman"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417265",
        "url": "https://doi.org/10.5281/zenodo.1417265",
        "ee": "https://zenodo.org/records/1417265/files/DannenbergW09.pdf",
        "abstract": "Detecting beats, estimating tempo, aligning scores to audio, and detecting onsets are all interesting problems in the field of music information retrieval. In much of this research, it is convenient to think of beats as occuring at precise time points. However, anyone who has attempted to label beats by hand soon realizes that precise annotation of music audio is not possible. A common method of beat annotation is simply to tap along with audio and record the tap times. This raises the question: How accurate are the taps? It may seem that an answer to this question would require knowledge of “true” beat times. However, tap times can be characterized as a random distribution around true beat times. Multiple independent taps can be used to estimate not only the location of the true beat time, but also the statistical distribution of measured tap times around the true beat time. Thus, without knowledge of true beat times, and without even requiring the existence of precise beat times, we can estimate the uncertainty of tap times. This characterization of tapping can be useful for estimating tempo variation and evaluating alternative annotation",
        "zenodo_id": 1417265,
        "dblp_key": "conf/ismir/DannenbergW09",
        "keywords": [
            "beats",
            "estimating tempo",
            "aligning scores to audio",
            "detecting onsets",
            "music information retrieval",
            "precise annotation",
            "tap times",
            "true beat times",
            "tempo variation",
            "alternative annotation"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nESTIMATING THE ERROR DISTRIBUTION OF A TAP SEQUENCE\nWITHOUT GROUND TRUTH\nRoger B. Dannenberg\nCarnegie Mellon University\nSchool of Computer ScienceLarry Wasserman\nCarnegie Mellon University\nDepartment of Statistics\nABSTRACT\nDetecting beats, estimating tempo, aligning scores to au-\ndio, and detecting onsets are all interesting problems in\nthe ﬁeld of music information retrieval. In much of this\nresearch, it is convenient to think of beats as occuring at\nprecise time points. However, anyone who has attempted\nto label beats by hand soon realizes that precise annotation\nof music audio is not possible. A common method of beat\nannotation is simply to tap along with audio and record the\ntap times. This raises the question: How accurate are the\ntaps? It may seem that an answer to this question would re-\nquire knowledge of “true” beat times. However, tap times\ncan be characterized as a random distribution around true\nbeat times. Multiple independent taps can be used to esti-\nmate not only the location of the true beat time, but also\nthe statistical distribution of measured tap times around\nthe true beat time. Thus, without knowledge of true beat\ntimes, and without even requiring the existence of precise\nbeat times, we can estimate the uncertainty of tap times.\nThis characterization of tapping can be useful for estimat-\ning tempo variation and evaluating alternative annotation\nmethods.\n1. INTRODUCTION\nTempo estimation and beat tracking are considered to be\nfundamental tasks of automatic music analysis and under-\nstanding. To evaluate machine performance in these sorts\nof tasks, it is useful to have audio annotated with beat\ntimes. We often assume that beat times are obvious and\neasily measured, usually through manual annotation. In\nsome sense this is a fair assumption. Humans are good\nat detecting beats, especially in popular music, and hu-\nman performance is generally better than machine perfor-\nmance. Most research simply accepts human-generated\ndata as correct.\nIn cases where the goal is simply to get close to “true”\nbeat times, or to estimate tempo (which can be a long-term\naverage), ignoring potential tapping errors might be rea-\nsonable. However, it is troubling to assume errors do not\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.matter without any way to test this assumption. Further-\nmore, there are some cases where automated methods can\ndeliver quite precise results. For example, onset detection\nand beat detection in piano music can rely on fast onsets\nto obtain precise times in the millisecond range automati-\ncally. It seems unlikely that humans can tap or otherwise\nannotate beat times with this degree of precision, so how\ncan we evaluate automatic labels?\nThe main goal of this work is to characterize the quality\nof human beat and tempo estimates in prerecorded audio\ndata. A simple approach to this problem is to synthesize\nmusic from known control data such as MIDI, using con-\ntrol timing as the “ground truth” for beat times. This ap-\nproach offers a clear connection to an underlying sequence\nof precise times, and after a human taps along with the mu-\nsic, some simple statistics can describe the distribution of\nactual tap times relative to the “true” beats. The problem\nhere is that “real” music seems more complicated: Musi-\ncians are somewhat independent, adding their own timing\nvariations, both intentional and unintentional. Musicians\nplay instruments with varying attack times and they some-\ntimes place their note onsets systematically earlier or later\nthan the “true beat” times. How can we know that tapping\nto carefully controlled synthesized music is indicative of\ntapping to music in general?\nWe present an alternative approach in which multiple\nindependent taps to beat-based music are used to estimate\na distribution around the underlying “true” beat time. We\nassume that a true but hidden beat time exists and that ob-\nserved tap times are clustered around these true times. In\naddition, we assume “all beats are the same” in the sense\nthat observed tap times for one beat have the same distri-\nbution as observed tap times around any other beat. (This\nassumption will be discussed later.)\nIt should be apparent that different forms of tapping\n(tapping with different kinds of audio feedback, tapping\nwith hands or feet, tapping by or while performing a mu-\nsical instrument) will have subtle implications for the po-\nsitioning and distribution of the tap times. Our techniques\nenable us to explore these differences but say nothing about\nwhether one is more correct than another. In other words,\nthere may be different implied “true” beat times for differ-\nent tapping conditions.\nIn addition to estimating the distribution of tap times\nfrom multiple independent taps, our technique can esti-\nmate the distribution of another source of tap times. For\nexample, we will show how a single set of foot tap times\n297Poster Session 2\ncaptured in a live performance can be used to estimate the\naccuracy of foot tapping, again without any ground truth.\nOur technique is interesting because it does not require any\nmanual time estimation using visual editing, ground truth,\nor acoustical analysis, yet it gives us the ability to describe\nany sequence of estimated beat times as a probabilistic dis-\ntribution around the underlying “true” beats.\nBy collecting data from real music audio examples, we\ncan get a sense not only of the location of beats but the\nuncertainty of these locations. Since studies of expressive\ntiming and tempo are normally based on beat time esti-\nmates, it is important to characterize uncertainty. In real-\ntime computer music performance, estimating tempo and\npredicting the time of the next beat is an important prob-\nlem. A good model of tapping and uncertainty can help to\nclarify the problem and analyze proposed solutions. There\nis also the potential to apply our model to the evaluation\nof automated beat tracking systems and to compare their\nperformance to human tapping. Finally, models of timing\nand tempo change can help to build better beat tracking\nsystems, which must reconcile prediction from past beat\nestimates using a steady-tempo hypothesis with new but\nuncertain beat estimates allowing the system to adapt to\ntempo change.\n2. RELATED WORK\nPrevious studies have looked directly at tapping and syn-\nchronization. Michon [1] studied synchronization to se-\nquences of clicks, and Mecca [2] studied human accom-\npanists and how they adapt to tempo change. Wright [3]\nstudied perceptual attack time, the perceived time or dis-\ntribution of times at which a tone is perceived to begin.\nDixon et al. [4] studied tapping to a short musical excerpt\nwith expressive timing. There is a substantial literature on\nthe perception of beats and rhythmic grouping [5]. The\nautomatic detection of beats and tempo also has a long his-\ntory of study [6,7]. The Mazurka project [8] has published\nbeat times estimated using acoustic data from expressive\nperformances.\nComputer accompaniment [9] is a popular topic in the\ncomputer music literature and this work is closely related\nto ours. Tempo change in computer accompaniment has\nbeen modeled using Bayesian belief networks [10]. Our\nstudy of beat estimation and tempo in fact addresses short-\ncomings of existing computer accompaniment systems. In\nparticular, computer accompaniment is usually based on\nscore following, which assumes that a score exists and that\nthere are audio signals to be matched to the score [9]. In re-\nality, popular music often involves improvisation and other\ndeviations from the score (if any), so the computer system\nmust be “aware” of beats, measures, and cues in order to\nperform effectively with live players [11].\nConducting is another means for synchronizing com-\nputers to live performers and another example of human\nindication of beats. Various conducting systems have been\ncreated using traditional conducting gestures as well as\nsimple tapping interfaces [12]. These studies are closely\nrelated to our work because any conducting system mustsense beat times and make predictions about the tempo\nand the next beat time. Our work extends previous work\nby measuring human performance in tapping along to mu-\nsic. The sequential drum [13] and radio drum [14] of Max\nMathews are also in the category of conducting systems.\nThese emphasize expressive timing and multidimensional\ngestural control.\nThe “virtual orchestra” concept [15, 16] is also related.\nVirtual orchestras have been created to accompany dance,\nopera, and musical theater. Most if not all of this work is\ncommercial and proprietary, so it is not known what tech-\nniques are used or how this work could be replicated, mak-\ning any comparative studies impractical. Certainly, a bet-\nter understanding of beat uncertainty and tempo estimation\ncould contribute to the performance of these systems.\n3. THE MODEL AND ASSUMPTIONS\nWe are interested in characterizing information obtained\nfrom tapping to music audio. In an ideal world, we would\nﬁrst label the audio with precise beat times. For example,\nwe might ask a subject to tap by hand many times along\nwith the music, measure the tap times, and compute the\nmean tap time /hatwideθ1,/hatwideθ2, . . .for each beat. Presumably, these\nmean tap times estimate and converge to a precise under-\nlying or “hidden” time θifor each beat. In this way, beat\ntimes can be estimated with arbitrary precision given suf-\nﬁcient data. Once beat times are estimated, we can study\nother tap sequences. For example, given a sequence of foot\ntap times Fiwe might like to estimate the distribution of\ntiming errors: ∆i=Fi−θi. If we ignore the difference\nbetween /hatwideθiandθi, it is simple to compute the mean and\nstandard deviation of ∆ior simply to plot a histogram to\ncharacterize the distribution.\nIt should be noted that the outcome (the distribution of\n∆i) is a distribution over timing errors throughout the en-\ntire piece, not a distribution for a particular beat. Timing\nerrors and the distributions of individual beats might be in-\nteresting things to study, but these are not considered by\nour model.\nUnfortunately, tapping along to music requires much\ntime, care, and concentration. We want to achieve the same\nresults without tapping along to music many times. In fact,\nif we make a few assumptions about ∆i, we only need to\ntap twice. Then, given a measured sequence of times Fi,\nwe can estimate the corresponding distribution ∆i.\nThe assumptions are that, ﬁrst, ∆iis normal. We will\nshow some evidence that ∆iobtained from actual tap data\nis in fact approximately normal. The second assumption is\nthat the sequence of true beat times θiis well deﬁned and\nthe same for all tap sequences. So for example, if we want\nto compare foot taps to hand taps, we need to assume that\nthe underlying “true” beats for each sequence are the same.\nAlternatively, if we want to measure the tap distribution of\nseveral subjects, we must assume they all share the same\ntrue beats.\nIn practice, we are seldom concerned about absolute\nshifts (subject A always perceives beats 10ms earlier than\nsubject B). But introducing a time offset to a collection of\n29810th International Society for Music Information Retrieval Conference (ISMIR 2009)\ntap times, say from subject A, generally increases the esti-\nmated variance of the tap times. If we believe that an offset\nmay reﬂect individual differences, sensors, or calibration\nproblems, then we can simply estimate and subtract off the\noffset. (Details will follow.) In that case the only assump-\ntion is that the “true” beat times for any two sequences of\ntaps are the same except for a constant (but unknown) time\noffset.\n4. ESTIMATING THE DISTRIBUTION\nTo estimate the distribution of actual tap times, let θ1, θ2, . . .\ndenote the true beat times. (These will remain unknown.)\nAlso, we will collect two sets of hand taps at times H1\ni, H2\ni.\nWe assume that these times are normally distributed around\nthe true beat times:\nH1\ni, H2\ni∼Normal( θi, σ2). (1)\nAn unbiased estimate of σ2is\n/hatwideσ2=1\n2nn/summationdisplay\ni=1(H1\ni−H2\ni)2. (2)\nThus, with only two sets of taps generated under the\nsame conditions, we can estimate the distribution of the\ntaps relative to the true beat times. It should be mentioned\nthatH1\niandH2\nimust correspond to the same true beat. If,\nfor example, one tap is missing from H1, then some differ-\nences (H1\ni−H2\ni)will be increased by one beat. In practice,\ntaps rarely differ by more than 150ms and beats are typi-\ncally separated by 500ms or more (taps can be every 2, 3,\nor 4 beats if the tempo is faster), so errors are simple to\nﬁnd and correct.\nWhat if H2has a constant offset relative to H1? Since\nwe assume the distribution around the true beat should be\nthe same for both sequences, the mean of their differences\n¯d:\n¯d=1\nnn/summationdisplay\ni=1(H1\ni−H2\ni) (3)\nshould be zero. We can “correct” any constant offset (esti-\nmated by ¯d) by replacing H2\niby(H2\ni+¯d).\nNow suppose we have another set of tap times generated\nby a different source, for example foot taps or taps from an-\nother subject. What is the distribution of these taps? Given\nH1\niandH2\ni, we only need one set of taps (one tap per beat)\nfrom the new source.\nLetFibe the new set of tap times, and let ∆i=Fi−θi.\nThe problem is to estimate the distribution of the ∆i’s. Let\nus begin by deﬁning\n/hatwide∆i=Fi−/hatwideθi (4)\nwhere/hatwideθiis an estimate of θi. For these estimates, we will\nuse\n/hatwideθi=H1\ni+H2\ni\n2. (5)\nFrom the assumption that Fiis normal, Fi∼N(θi, τ2), it\nfollows that\n/hatwide∆i∼N/parenleftbigg\n0, τ2+σ2\n2/parenrightbigg\n(6)Here, τ2is due to random variation in Fiandσ2\n2is due to\nuncertainty in our estimates of the true beat times. Now,\nif we let s2be the expected sample variance of the /hatwide∆i, we\nobtain\ns2=τ2+σ2\n2(7)\nand hence\nτ2=s2−σ2\n2(8)\nThus,\n∆i∼N/parenleftbigg\n0, s2−σ2\n2/parenrightbigg\n(9)\nWe already have an estimate of σ2, and we can estimate\ns2using the sample variance /hatwides2of/hatwide∆i. Substituting /hatwideσ2for\nσ2and/hatwides2fors2, we can estimate the distribution of ∆i\nand thus the accuracy of taps from the new source even\nwithout any ground truth for beat times. All we need are\ntwo additional sets of times obtained by tapping along with\nthe music.\n5. GENERALIZATION TO N SEQUENCES\nThis approach can be generalized to multiple tap sequences.\nFor example, taps from many different subjects might be\ncombined. Suppose that Ntap sequences, H1\ni, . . . , HN\ni\nare normally distributed with means θiand variance σ2.\nWe estimate means and variance as follows:\n/hatwideθi=1\nNN/summationdisplay\nj=1Hj\ni (10)\nand\n/hatwideσ2=1\nnn/summationdisplay\ni=1s2\ni (11)\nwhere\ns2\ni=1\nN−1N/summationdisplay\nj=1(Hj\ni−/hatwideθi)2. (12)\nDeﬁning /hatwide∆iagain as in (4), we generalize (6) to\n/hatwide∆i∼N(0, τ2+σ2\nN). (13)\nLetting S2be the expected sample variance of the /hatwide∆i,\n∆i∼N(0, τ2) =N/parenleftbigg\n0, S2−σ2\nN/parenrightbigg\n. (14)\nAgain, we can estimate S2using the sample variance /hatwideS2\nof/hatwide∆iand estimate the variance of ∆ias/hatwideS2−bσ2\nN.\n6. IS ∆INORMAL?\nOur analysis assumes that the distribution of ∆iis nor-\nmal. We collected some taps to music synthesized from\nMIDI with note onsets quantized to exact beat times and\nsmoothly varying but mostly constant tempo. Figure 1\nshows a histogram of differences between the 117 “true”\n299Poster Session 2\nbeats and hand-tapped (by one of the authors) beats, cor-\nresponding to ∆i. To characterize the error, we use the\nmean of the absolute difference (MAD) between tapped\nbeats and true beats after adjusting an absolute time offset\nto obtain a mean difference of zero. For this condition, the\nMAD is 16.13ms. Although the extreme values of +/-60ms\nseem quite large, the MAD value of 16.13ms compares fa-\nvorably to the typical value of 10ms cited as the just notice-\nable difference (JND) for timing deviation [17]. (Since our\ngoal is to describe a representation and its theory, we show\nonly a couple of typical examples from data collected from\na growing collection of songs, performers, and tappers.)\ntaps ! beats \n!60!40!2002040600246810\nFigure 1 .Histogram of deviations (in ms) of hand tap\ntimes from “true” (MIDI) beat times.\nUsing live acoustic music, two sets of hand tap times\nwere collected, and Figure 2 shows differences between\ncorresponding hand tap times. In this example, the music\nwas from a big-band jazz rehearsal. Again, the data is from\none of the authors, but it is typical of other data we have\ncollected. This differs from Figure 1 in that the time differ-\nences are between two tap times to acoustic music rather\nthan between a tap time and a known beat time in synthe-\nsized music. The standard deviation is 26ms. As with the\nMIDI-related data, the general shape of the histogram ap-\npears to be Gaussian, so the Normality assumption is at\nleast reasonable. A Shapiro-Wilks test of Normality on\ndata in Figures 1 and 2 yields values of W= 0.9829 (p-\nvalue = .6445), and W= 0.9882 (p-value = .2625), sug-\ngesting again that Normality is reasonable.\n7. EXAMPLE\nWe are interested in characterizing foot tapping as an in-\ndicator of beat times. For our data used in Figure 2, /hatwideσ=\n32.77ms (standard error 2.006ms).\nEven before collecting hand tap times, we recorded au-\ndio and foot tap times from a live performance. The foot\ntaps are sensed by a custom pedal that uses a force-sensitive\nresistor (FSR) to control the frequency of a low-power ana-\nlog oscillator [15]. The audio output from the pedal can be\nsent to one channel of a stereo recording in synchrony with\ndifference between hand taps \n!100!50050100150051015Figure 2 .Histogram of differences (in ms) between two\nsets of hand tap times to audio recording of a live perfor-\nmance.\nthe live music on the other channel. Later, the “foot pedal\nchannel” can be analyzed to detect foot taps with precise\nsynchronization to the music audio. We then used Sonic\nVisualizer [18] to record hand taps (twice) while listening\nto the music channel of the recording.\nFinally, using the analysis described in Section 4, we\nobtain a mean of 0 and a standard deviation of 37.2ms.\nThis number reﬂects a particular condition involving the\ntype of music, the other players involved, the interference\ntask of performing, and possibly individual differences.\nThus, we are not suggesting that one can give meaningful\nnumbers for the accuracy of hand-tapped or foot-tapped\nbeats in general, only that for any given situation, the taps\ncan be accurately and efﬁciently characterized without a\nground truth for beat times.\nThis example is interesting because it is impossible to\nobtain more than one set of foot taps or ground truth from\na live performance, yet our technique still provides an es-\ntimate of the foot tap error distribution.\n8. DISCUSSION\nBecause beats are hidden and perceptual, there are multi-\nple ways to characterize beats. Just as there are differences\nbetween pitch (a percept) and fundamental frequency (a\nphysical attribute), a distinction can be made between per-\nceived beat times and acoustic event times. Some research\nrelies on acoustic events to estimate beat times. While ob-\njective and often precise, these times are subject to various\ninﬂuences including random errors and physical character-\nistics of the instrument [19], so even acoustic times are the\nresult of human perception, cognition, and action. After\nall, performing within an ensemble requires a perception\nof the beat and precise timing, so it is not all that different\nfrom tapping.\nFurthermore, polyphony creates ambiguity because note\nonsets are often not synchronized. In fact, there is good ev-\nidence that note onsets are deliberately notplaced on “the\nbeat,” at least in some important cases [20]. Therefore, this\n30010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nwork attempts to identify and characterize perceptual beat\ntimes through tapping.\nEven this approach has limitations. As seen in our ex-\nample data, beat times are characterized as distributions\nrather than precise times, reﬂecting the limited information\navailable from a small number of tap sequences. Moreover,\nall distributions are assumed to have the same variance. In\nmusic with a steady beat, this seems to be a reasonable as-\nsumption. In music with expressive timing, rubato, etc.,\none would expect some beats to be more accurately tapped\nthan others. Learning from repeated listening can affect\ntapping times [4]. We suspect learning is a bigger factor in\nmusic with expressive timing where subjects might learn\nto anticipate timing variations. In music with a steadier\ntempo, any learning effect should be minimal.\nThe “meaning” of variance ( τ2) merits discussion. One\ninterpretation is that the perceived beat time is very pre-\ncise, but there are limitations in motor control that give\nrise to variation in tap times. Another interpretation is that\nthe perception of beat times is not consistent from one lis-\ntening to the next, resulting in different tap times. If dif-\nferent subjects tap, variance could arise from a difference\nbetween subjects. Ultimately, τ2models real data, so a\nmore detailed model may not be relevant. On the other\nhand, experiments might be able isolate and characterize\ndifferent inﬂuences on tap timing.\nUsing a limited amount of “ﬁeld recording” data, we\nobserved that foot tap timing can be approximated by a\nnormal (Gaussian) random distribution around the “true”\nbeat time. This is suggested by histograms as well as a\nShapiro-Wilks test of Normality. The observed variance is\nalmost certainly dependent upon the clarity of the beat, the\nsteadiness of tempo, the skill of the tapper, interference\ntasks including playing an instrument while tapping, and\nother factors. The good news is that the method is practical\nand inexpensive, and the method can be used to study all\nof these factors.\nMany studies in Computer Music, Music Information\nRetrieval, and Music Perception depend upon estimates of\nbeat times and tempo variation. The techniques described\nhere offer a principled way to go about characterizing the\nuncertainty of beat times obtained by tapping.\n9. APPLICATIONS AND FUTURE WORK\nThe goal of this paper is to describe a representation of\nbeat timing, the underlying estimation theory, and a practi-\ncal way to use this representation. Current work is examin-\ning data from many sources with the goal of understanding\nthe range of uncertainty ( τ2) observed under different con-\nditions, and perhaps factors that account for differences.\nAlso, experiments could study the degree to which tap time\nvariance results from perceptual uncertainty vs motor con-\ntrol.\nOne of our goals is to create music systems that perform\nwith live musicians using techniques based on work in Mu-\nsic Information Retrieval. Beat tracking, gesture sensing,\nanalysis of mood, and other aspects of a performance all\nprovide important input to an automated music performer.In the area of beats and tempo, the techniques presented\nhere are being used to analyze data from a variety of per-\nformances. For synchronization to live performers, the\ndata will help us to tune systems that accurately predict the\nnext beat time, allowing an artiﬁcial performer to play ac-\ncurately on the beat. Beat timing variation implies tempo\nchange. Modeling tap times probabilistically can help to\ndistinguish between random timing errors and true tempo\nchange. For example, preliminary analysis has shown that,\ndepending upon the amount of tempo variation in a piece\nof music, estimating tempo using the previous 6 to 18 beats\ngives the best prediction of the next beat time. This work is\nclosely related to beat tracking systems where smoothing\nover beats can help the system stay on track, but smooth-\ning over too many beats makes the system unable to follow\ntempo changes.\nAnother application is in the construction and evalua-\ntion of score-to-audio alignment systems. While scores\nhave precise beat times, audio recordings do not. By sub-\nstituting alignment times for foot tap times ( Fiin (4)), we\ncan measure score alignment quality without any ground\ntruth.\nAudio labeling is another application. We might like\nto compare beat labels based on audio features to percep-\ntual beat times. Since tap times might have a large vari-\nance, one is tempted to conclude that precise audio-based\nlabels are more reliable. With our techniques, this can be\ntested. Another issue with labeling is the reliability of\nhand-labeled audio using an audio editor. This is a very\ndifﬁcult task where one might expect to see individual dif-\nferences among human labelers. The lack of ground truth\nmakes it difﬁcult to evaluate different labelers. Our method\nmight be useful because it does not need the ground truth\nto provide an analysis.\nFinally, it is interesting to study tempo in the abstract.\nIn live performances we have tapped to, we have found\nsubstantial tempo changes (on the order of 10%) during\nsolos with a rhythm section where the tempo is nominally\nsteady. As with live synchronization, one must be care-\nful to avoid attributing tempo change to jitter in tap times,\nand a characterization of the tap time distribution helps to\nidentify true tempo changes.\n10. CONCLUSION\nOur work concerns the analysis of beat times in music with\na fairly steady beat. Our live data collection and analy-\nsis indicate that foot tap timing can be modeled well as\na Gaussian distribution around a “true” but unknown beat\ntime. We have introduced a new technique for estimating\ntapping accuracy that does not require the accurate iden-\ntiﬁcation of underlying beats. By comparing foot tap data\n(or data from other sources) to multiple hand taps on the\nsame music, we are able to estimate the standard deviation\nand thus characterize the uncertainty in the tapping data. A\nmajor strength of this approach is that a one-time, irrepro-\nducible sequence of taps such as from a live performance\ncan be analyzed in terms of accuracy without ground truth\nfor “true” beat times.\n301Poster Session 2\n11. ACKNOWLEDGEMENTS\nThe authors would like to acknowledge the contributions\nof Nathaniel Anozie to the initial exploration and analysis\nof this data, and Cosma Shalizi for helpful consultation and\ndiscussions. This work was supported in part by Microsoft\nResearch through the Computational Thinking Center at\nCarnegie Mellon.\n12. REFERENCES\n[1] J. A. Michon. Timing in Temporal Tracking . Van Gor-\ncum, Assen, NL, 1967.\n[2] M. Mecca. Tempo following behavior in musical ac-\ncompaniment. Carnegie Mellon University, Depart-\nment of Philosophy, Pittsburgh, PA, USA (Masters\nThesis), May 1993.\n[3] Matt Wright. Computer-Based Music Theory and\nAcoustics . PhD thesis, Stanford University, CA, USA,\nMarch 2008.\n[4] S. Dixon, W. Goebl, and E. Cambouropoulos. Percep-\ntual smoothness of tempo in expressively performed\nmusic. Music Perception , 23(3):195–21, 2006.\n[5] P. Fraisse. Rhythm and tempo. In D. Deutsch, editor,\nThe Psychology of Music , pages 149–80. Academic\nPress, New York, 1st edition edition, 1982.\n[6] F. Gouyon and S. Dixon. A review of automatic\nrhythm description systems. Computer Music Journal ,\n29(1):34–54, 2005.\n[7] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-\ntakis, C. Uhle, and P. Cano. An experimental compari-\nson of audio tempo induction algorithms. IEEE Trans-\nactions on Audio, Speech, and Language Processing ,\n14(5):1832–1844, September 2006.\n[8] Craig. Sapp. Comparative analysis of multiple musi-\ncal performances. In Proceedings of the 8th Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR’07) , pages 497–500, 2007.\n[9] R. B. Dannenberg and C. Raphael. Music score align-\nment and computer accompaniment. Communications\nof the ACM , 49(8):39–43, August 2006.\n[10] C. Raphael. Synthesizing musical accompaniments\nwith bayesian belief networks. Journal of New Music\nResearch , 30:59–67, 2000.\n[11] A. Robertson and M. D. Plumbley. B-keeper: A beat\ntracker for real time synchronisation within perfor-\nmance. In Proceedings of New Interfaces for Musical\nExpression (NIME 2007) , pages 234–237, 2007.\n[12] R. B. Dannenberg and K. Bookstein. Practical aspects\nof a midi conducting program. In ICMC Montreal 1991\nProceedings , pages 537–540, San Francisco, 1991. In-\nternational Computer Music Association.[13] M. V. Mathews and C. Abbot. The sequential drum.\nComputer Music Journal , 4(4):45–59, Winter 1980.\n[14] M. V. Mathews and W. A. Schloss. The radio drum\nas a synthesizer controller. In Proceedings of the 1989\nInternational Computer Music Conference , San Fran-\ncisco, 1989. Computer Music Association.\n[15] Roger B. Dannenberg. New interfaces for popular mu-\nsic performance. In NIME ’07: Proceedings of the 7th\nInternational Conference on New Interfaces for Musi-\ncal Expression , pages 130–135, New York, NY, USA,\n2007. ACM.\n[16] Gregory M. Lamb. Robo-music gives musicians the\njitters. The Christian Science Monitor , December 14,\n2006.\n[17] A. Friberg and J. Sundberg. Perception of just notice-\nable time displacement of a tone presented in a metrical\nsequence at different tempos. Technical report, STL-\nQPSR, Vol. 34, No. 2-3, pp. 49–56, 1993.\n[18] C. Cannam, C. Landone, M. Sandler, and J. P. Bello.\nThe sonic visualizer: A visualization platform for se-\nmantic descriptors from musical signals. In ISMIR\n2006, 7th International Conference on Music Informa-\ntion Retrieval , pages 324–327, 2006.\n[19] Patrik N. Juslin. Five facets of musical expression: A\npsychologist’s perspective on music performance. Psy-\nchology of Music , 31(3):273–302, 2003.\n[20] A. Friberg and A. Sundstrom. Swing ratios and ensem-\nble timing in jazz performance: Evidence for a com-\nmon rhythmic pattern. Music Perception , 19(3):333–\n349, 2002.\n302"
    },
    {
        "title": "A Comparison of Score-Level Fusion Rules for Onset Detection in Music Signals.",
        "author": [
            "Norberto Degara-Quintela",
            "Antonio S. Pena",
            "Soledad Torres-Guijarro"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415692",
        "url": "https://doi.org/10.5281/zenodo.1415692",
        "ee": "https://zenodo.org/records/1415692/files/Degara-QuintelaPT09.pdf",
        "abstract": "Finding automatically the starting time of audio events is a difficult process. A promising approach for onset detection lies in the combination of multiple algorithms. The goal of this paper is to compare score-level fusion rules that combine signal processing algorithms in a problem of automatic detection of onsets. Previous approaches usually combine detection functions by adding these functions in the time domain. The combination methods explored in this work fuse, at score-level, the peak score information (peak time and onset probability) in order to obtain a better estimate of the probability of having an onset given the probability estimates of multiple experts. Three state-ofthe-art spectral-based onset detection functions are used: a spectral flux detection function, a weighted phase deviation function, and a complex domain detection function. Both untrained and trained fusion rules will be compared using a standard data set of music excerpts.",
        "zenodo_id": 1415692,
        "dblp_key": "conf/ismir/Degara-QuintelaPT09",
        "keywords": [
            "automatic detection",
            "audio events",
            "score-level fusion",
            "signal processing",
            "onset detection",
            "multiple algorithms",
            "peak score information",
            "probability of having an onset",
            "onset probability",
            "onset detection functions"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nA COMPARISON OF SCORE-LEVEL FUSION RULES FOR ONSET\nDETECTION IN MUSIC SIGNALS\nNorberto Degara-Quintela, Antonio Pena\nDepartment of Signal Theory and Communications\nUniversity of Vigo, Spain\n{ndegara, apena }@gts.tsc.uvigo.esSoledad Torres-Guijarro\nLaboratorio Oﬁcial de Metrolox ´ıa\nde Galicia (LOMG), Spain\nstorres@lomg.net\nABSTRACT\nFinding automatically the starting time of audio events is\na difﬁcult process. A promising approach for onset detec-\ntion lies in the combination of multiple algorithms. The\ngoal of this paper is to compare score-level fusion rules\nthat combine signal processing algorithms in a problem of\nautomatic detection of onsets. Previous approaches usually\ncombine detection functions by adding these functions in\nthe time domain. The combination methods explored in\nthis work fuse, at score-level, the peak score information\n(peak time and onset probability) in order to obtain a bet-\nter estimate of the probability of having an onset given the\nprobability estimates of multiple experts. Three state-of-\nthe-art spectral-based onset detection functions are used:\na spectral ﬂux detection function, a weighted phase devi-\nation function, and a complex domain detection function.\nBoth untrained and trained fusion rules will be compared\nusing a standard data set of music excerpts.\n1. INTRODUCTION\nThe automatic detection of onsets is essential in many ap-\nplications, including a number of important music infor-\nmation retrieval (MIR) tasks. Onset detection is useful in\nthe analysis of the temporal structure of music as, for ex-\nample, beat tracking and tempo induction, but it is also\nimportant in other relevant tasks such as melody, bass-line\nand chord extraction.\nFinding automatically the starting time of audio events\nis a difﬁcult process and many onset detection methods ex-\nist [1–3]. However, the performance of current detection\nmethods is highly dependent on the nature of the signal\nas shown in [1]. The reason is that onset detection tech-\nniques assume an implicit nature or probability model for\nthe signal to be analyzed. Actually, several well known\nalgorithms can be described in terms of an implicit proba-\nbility model of the signal [4].\nFor this reason, it is not expected that a single method\nwill perform accurately for strongly nonstationary signals\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.and audio signals are intrinsically variable in nature. In-\nstead of designing a very complex algorithm, a promis-\ning development lies in the combination of multiple meth-\nods [5]. In fact, this is most likely the way human percep-\ntion seems to work [6], using different processing princi-\nples for the same purpose so when one of them fails per-\nhaps another succeeds.\nMethods that combine time-domain onset detection func-\ntions to provide with a more accurate detection have been\nproposed. However, most of the existing combination\nschemes use ad-hoc approaches that, for example, choose\na particular detection function between two different func-\ntions based on the type of onset [7] or a quality measure [8].\nRecently, onset detection systems based on machine\nlearning algorithms have been developed. In [9] two Gaus-\nsian Mixture Models are used to merge multiple audio fea-\ntures, but the combination of the individual detection func-\ntions is still done by a linear weighted sum of the time\ndomain functions. Other approaches merge the detection\nfunctions using a time-delay neural network [10,11].\nThe integration of tools and information is one of the\nsigniﬁcant challenges for the ﬁeld of MIR as discussed\nin [12] and fusion methods can potentially be used for this\npurpose. Fusion is an important research area that stud-\nies the combination of multiple sources of knowledge to\nobtain more reliable information [13,14].\nThis paper emphasizes the use of information fusion\nmethods to gather the efforts of MIR community which de-\nvelops multiple signal processing algorithms for the same\npurpose. In particular, we compare the use of untrained and\ntrained fusion rules to combine, at score-level, the peak\ninformation obtained from three spectral-based onset de-\ntection functions. Scores represent the estimated time in-\nstant and the probability of having an onset at that instant.\nHence, our multiple-expert approach aims to calculate a\nbetter estimate of that probability given the probability es-\ntimates (scores) of multiple experts, which is radically dif-\nferent to adding time-detection functions as previous ap-\nproaches do. This study is the ﬁrst work, to our knowl-\nedge, that focuses just on the combination of techniques by\nintroducing score-level fusion for onset detection, opening\na novel direction to address the problem of combining de-\ntection algorithms.\nSection 2 introduces the fusion approach to onset de-\ntection, describing the structure of the system and the de-\ntection functions extracted. Section 3 describes the dataset\n117Poster Session 1\nFigure 1 . The Multiple-expert paradigm. The system fuses\nthe peak information extracted from three detection func-\ntions: the spectral ﬂux measure (SF), the weighted phase\ndeviation (WPD) and the complex domain method (CD).\nand the evaluation measures used in the present work. Re-\nsults are presented in Section 4. And ﬁnally, Section 5\ncontains the conclusions and some ideas for future work.\n2. FUSION FOR ONSET DETECTION\nFusion is an important and widely studied area that focuses\non the issue of how to combine information to achieve an\nimproved performance. This multiple expert paradigm is\nbased on the combination of various diagnosis to exploit\nthe expertise of the different experts. Score-level fusion\ncombines the different opinions (probability estimates) of\nthe experts to obtain a better estimate of the appropriate a\nposteriori probability.\nFigure 1 shows the multiple expert fusion system that\ncombines the peak information obtained from three state-\nof-the-art onset detection algorithms. First, the spectrum of\nthe audio signal is calculated using the Short Time Fourier\nTransform (STFT). Then, three experts derive the detection\nfunctions using features extracted from the STFT. Finally,\nthe system combines the peak information obtained from\nthe detection functions using a fusion rule.\n2.1 Onset Detection Functions\nThe detection functions used for fusion in this work are the\nfollowing spectral-based reduction methods: the spectral\nﬂux measure (SF), the weighted phase deviation (WPD)\nand the complex domain method (CD) described in [2].\nAll these methods are based on a STFT scheme that\napplies a Hamming window h(n). Given an audio signal\nx(n)sampled at fs= 44 .1 kHz , the kth frequency bin of\nthe nth spectrum frame X(n, k)is given by:\nX(n, k)=m=N\n2−1/summationdisplay\nm=−N\n2x(nh+m)h(m) exp−j2πkm\nN(1)\nIn our experiments, the window size in samples is N=\n2048 (46 ms ) and the hop size h= 441 (10 ms ).The spectral ﬂux (SF) measures the distance between\nsuccessive short-time Fourier spectra:\nSF(n)=m=N\n2−1/summationdisplay\nm=−N\n2H(|X(n, k)|−| X(n−1,k)|)(2)\nwhere H(x)=x+|x|\n2is a half-wave rectiﬁer. This function\nis used to emphasize onsets rather than offsets since the\nsum is restricted to those frequencies where the spectral\ndifference is positive and an increase of energy exists.\nIn order to add phase information in this system of mul-\ntiple experts, the weighted phase deviation reduction\nmethod has also been considered. The rate of change of\nphase is an estimation of the instantaneous frequency and\nabrupt changes in the instantaneous frequency may suggest\na potential onset. The weighted phase deviation (WPD) re-\nduction method takes the mean of the absolute value of the\ninstantaneous frequency difference weighted by the mag-\nnitude of the spectra:\nWPD (n)=1\nNm=N\n2−1/summationdisplay\nm=−N\n2|X(n, k)||ϕ/prime/prime(n, k)| (3)\nwhere ϕ/prime/prime(n, k)is the second derivative of the 2π-\nunwrapped phase of the Fourier spectra X(n, k).\nFinally, the complex domain detection function consid-\ners jointly both magnitude and phase to search for tran-\nsients on the signal. The spectral component X(n, k)can\nbe predicted from the previous frame spectra magnitude\nand phase change:\n/hatwideX(n, k)=|X(n−1,k)|eϕ(n−1,k)+ϕ/prime(n−1,k)(4)\nThe complex domain (CD) detection function is deﬁned\nas the sum of the absolute deviations from the predicted\nspectral values /hatwideX(n, k),\nCD(n)=m=N\n2−1/summationdisplay\nm=−N\n2|X(n, k)−/hatwideX(n, k)| (5)\nNormalization is a key step in fusion, therefore each of\nthe detection functions is normalized to have a mean 0and\nstandard deviation of 1.\n2.2 The Multiple-expert Architecture\nIn this approach, where multiple algorithms are combined\nto accomplish the same goal and can potentially interact to\nadapt its behavior, the architecture is very important. In\nthis sense, blackboard modeling, an approach taken from\nartiﬁcial intelligent systems, has been successfully applied\nto other relevant applications such as computational audi-\ntory scene analysis [15] and polyphonic music transcrip-\ntion [16]. In a blackboard model, experts communicate\nusing a common database what allows to pursue multiple\nlines of analysis at the same time and to adapt the strategies\nto a particular problem context.\n11810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nThe multiple-expert approach described in this paper\nhas been developed within a blackboard-agent framework.\nAlthough the number of experts used in this paper is small,\nthe blackboard-agent framework will probably be useful\nwhen combining many more experts, by implementing top-\ndown processing where results coming from fusion are fed\nback into experts to improve individual results.\n2.3 Peak Selection\nPeaks are selected from the onset detection functions by\npeak-picking the local maxima. We apply the peak-picking\nalgorithm used in [2] to obtain the peak-score information\nused for fusion: the peak time and the estimated probabil-\nity of having an onset at that time.\nA peak at time t=nh\nfs(where nis the current sample,\nhthe hop length and fsthe sampling frequency) is chosen\nas a relevant peak if the peak is a local maximum and the\ndetection function is larger than a threshold above the local\nmean of the detection function f(n), this is:\nf(n)≥f(m)formsuch that n−w≤m≤n+w(6)\nf(n)−/summationtextn+w\nm=n−lwf(m)\nlw+w+1≥δ (7)\nwhere wis the size of the window used to ﬁnd local max-\nima, lis a weighting factor to calculate the mean over a\nlarger range before the peak (emphasizing onsets rather\nthan offsets) and δis the threshold.\nPeak scores are normalized by subtracting the calcu-\nlated local mean to the peak value of the detection function\nas given in equation (7).\nThe values of the peak-picking parameters have a large\nimpact on the results. Hence, we follow the approach cho-\nsen in [1] and [2] selecting the parameters that maximize\nthe F-measure, a performance measure deﬁned in Section 3.\n2.4 Fusion\nOnsets in the original signal are related to peaks in the\ndetection functions, therefore the normalized peak scores\nand times pairs are selected by using the mean-ﬁlter peak-\npicking algorithm described above. Peak scores and time\nstamps from the three experts are grouped in time frames\nof50msand50% overlap. If a given expert proposes\nseveral peaks within the merging frame, the peak with the\nhighest score is selected.\nLetF(l)={fsffpdfcd}andT(l)={tsftpdtcd}be,\nrespectively, the peak scores and time stamps for each ex-\npert in the grouping time frame l. The proposed system\nfuses this peak information using the rules described be-\nlow and classiﬁes the frame as an onset or non-onset frame.\nThe parameters of the fusion algorithms are chosen so as\nto maximize the performance of the fusion system.\nVoting is perhaps one of the oldest strategies for deci-\nsion making. The voting mechanism counts the number\nof expert scores that are higher than a given threshold and\na consensus pattern is applied. A grouping frame can beclassiﬁed as an onset-frame if any, the majority or all (una-\nnimity) the experts exceed the threshold.\nThe sum rule simply adds the normalized expert scores\nin the grouping frame to obtain a better estimate of the a\nposteriori onset probability. A frame is labeled as an onset-\nframe if the resulting sum score exceeds a threshold.\nTrained fusion strategies are also explored in this pa-\nper. In particular, we evaluate the performance of a K-\nNearest Neighbor (K-NN) rule and a Support Vector Ma-\nchine (SVM) with RBF kernel using cross-validation. The\nparameters of the RBF kernel are selected using a grid-\nsearch technique.\nGrouping peak information in overlapping time frames\ngenerates doubled detections, therefore the output of the\nfusion rule is post processed to remove doubled onsets.\n3. DATASET AND EVALUATION\nMETHODOLOGY\nThe evaluation of the proposed fusion approaches is per-\nformed using the annotated dataset used in [1]. The dataset\nis composed of excerpts of different musical styles classi-\nﬁed into the following categories: pitched non-percussive\n(PN), pitched percussive (PP), non-pitched percussive (NP)\nand complex mixtures (CM). This allows to test the algo-\nrithms on different classes of audio signals. There is a total\nof1060 onsets.\nThe majority of the literature reporting results on onset\ndetection shows a lack of proper statistical evaluation. Few\nworks report standard deviations to give an idea of the vari-\nability of the results and most of them rely on mean per-\nformances only. Fortunately, a proper statistical hypothesis\ntesting methodology has been adopted in MIREX 2008.\nHence, we decided to segment the original signals into\nhomogeneous folds to evaluate the accuracy of our system\nusing K-fold cross-validation. Cross-validation allows the\nstatistical evaluation of the performance measures, enabling\nthe estimation of conﬁdence intervals [17]. We used differ-\nent cross-validation ﬁles for each category, with no overlap\nbetween folds. The number of folds were 14 (CM), 12\n(NP), 12 (PN) and 14 (PP).\nFor the evaluation and comparison of onset detection al-\ngorithms three measures are usually considered: precision\n(P), recall (R) and F-measure (F). These evaluation mea-\nsures are deﬁned as:\nP=ncd\nncd+nfp(8)\nR=ncd\nncd+nfn(9)\nF=2PR\nP+R(10)\nwhere ncdis the number of correctly detected onsets, nfp\nis the number of false positives (detection of an onset when\nno ground truth onset exists) and nfnis the number of\nfalse negatives (missed detections). Due to the reliabil-\nity of hand-labeled annotations, a time tolerance of 50 ms\nis usually assumed. This means that an onset is consid-\nered to be correctly matched if the detected onset is within\n119Poster Session 1\n50 ms of the ground truth onset time. In addition, we do\nnot penalized merged onsets since we do not try to identify\nindividual notes.\nAs discussed in Section 2.3, peak-picking and fusion\nrule parameters are chosen so as to maximize the F-\nmeasure, which assigns the same signiﬁcance to false pos-\nitives and false negatives.\n4. RESULTS AND DISCUSSION\nTable 1 compares the results of the best individual experts\nand the proposed fusion rules on the different datasets. We\nchoose the best expert for comparison because fusion al-\nways performed better than the worst expert in our experi-\nments. In addition, we want to show that fusion can obtain\neven better results than the best expert and that fusion per-\nformance is not limited by the worst expert.\nTotal performance do not show enough information to\ncompare different approaches and a proper statistical anal-\nysis is essential to fully understand how the different meth-\nods perform. Hence, Table 1 shows mean values and the\n95% conﬁdence interval for the F-measure using cross-\nvalidation.\nAs it can be seen in this table, fusion rules are able to\nachieve better performance than the best of the experts. For\nthe PN and PP datasets, the relative increase in F-measure\nis important considering that the performance of the best\nof the experts is already high. Hence, the accuracy of the\nfusion algorithms is not limited by the worst of the experts\nand fusion achieves an improvement in performance by ex-\nploiting consensus diagnosis of the three experts.\nFor the NP and CM cases, the increase in performance\ngiven by the fusion rules is not signiﬁcant. In fact, the\nperformance is limited by the number of false negatives\nbecause there is a number of onsets that are not detected\nfor any of the the experts. To exploit the beneﬁts of fusion,\nexperts should be as diverse as possible meaning that on-\nset detection functions should be accurate and should not\nmake coincident errors.\nIt is noteworthy that fusion has reduced the F-measure\ndeviation in the PN and PP datasets but is still large for\nthe NP and CM datasets. A large deviation means that\nfusion obtains good results for some of the folds but the\nperformance is very low for other folds. In this sense, the\nperformance could potentially be increased if we were able\nto identify the quality of the detection functions and apply\ndifferent fusion strategies based on this quality measure.\nWe turn now to discuss the different approaches for fu-\nsion. Simple fusion rules obtain better results than trained\nfusion rules. The size of the test sets is small and both\nthe K-NN and SVM approaches suffer from overﬁtting. In\naddition, the SVM achieves better performance than the\nK-NN except for the CM case. Finally, the SVM achieves\nvery good results for the PP case, probably because the\nnumber of samples required to learn the task of identifying\nPP onsets is low.\nWe followed the statistical evaluation methodology pro-\nposed in [17] and we assumed a t-distribution for the sam-\nple mean estimator of the F-measure (the number of foldsfor cross-validation was less than 30). However, perfor-\nmance depends on various factors such as the set size, com-\nposition and the choice of the samples. Another interest-\ning accuracy measure would be the Weighted Error Rate\n(WER), widely used in biometrics. In this case, a speciﬁc\nmethod for the calculation of conﬁdence intervals for the\ntotal WER, not the mean, is already deﬁned in [18]. This\nmethod reduces the performance dependency of these fac-\ntors. The WER, a error measure widely used in biometrics,\nis deﬁned as:\nWER (R)=fn+Rfp\n1+R(11)\nwhere fnandfpare the false negatives and positives rates.\nThe parameter Rallows to balance the signiﬁcance of the\nfalse positives and false negatives in the error measure\nwhich could be of interest in some applications and useful\nto compare algorithms at different operating points. There-\nfore, the WER can be an appropriate measure for the statis-\ntical evaluation of music information retrieval experiments.\n5. CONCLUSIONS AND FUTURE WORK\nThe originality of this contribution is the introduction of\nscore-level fusion strategies for onset detection, looking at\nthe problem of combining onset information as a multiple-\nexpert fusion problem. Our approach aims to calculate\na better estimate of that probability given the probability\nestimates of multiple experts, which is radically different\nto adding time-detection functions as previous approaches\ndo. This study is the ﬁrst work, to our knowledge, that\nfocuses just on the combination of techniques by introduc-\ning score-level fusion for onset detection, opening a novel\ndirection to address the problem of combining detection\nalgorithms.\nThis paper compares untrained and trained fusion rules\non four sets of different music styles. Results show how\ninformation fusion rules can lead to a higher performance\nwhen combining multiple signal processing algorithms de-\nsigned for onset detection. However, the increase in perfor-\nmance seems to be not important if experts are not diverse.\nSimple fusion rules show better performance than trained\nrules due to, probably, the small number of samples avail-\nable for training.\nIn addition, a performance measure widely used in bio-\nmetrics has been proposed. The Weighted Error Rate al-\nlows to balance the signiﬁcance of the false positives and\nfalse negatives in the error measure and a speciﬁc method\nfor the calculation of the conﬁdence intervals of the total\nerror rate is already deﬁned.\nIn future work, we will include more experts to exploit\ndiversity in the information fusion process. In addition, the\ncross-validation analysis showed a high deviation of the F-\nmeasure for complex signals. This means that the perfor-\nmance of the experts is highly dependent on the conditions\nof the signal. To face this problem, we will explore quality-\nbased information fusion which basically weights scores\naccording to the quality of the expert’s detection functions.\n12010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nPN data PP data NP data CM data\nP R F P R F P R F P R F\nB.E. 93.898.195.7±5.197.498.597.8±1.799.594.596.7±5.589.489.6 88.8±6.7\nVot. 99.195.697.3±2.498.498.898.6±1.096.996.796.7±5.791.088.5 89.2±7.5\nSum 99.195.697.3±2.499.898.699.2±0.998.094.696.2±5.593.985.4 88.9±7.0\nKNN 91.496.493.5±4.395.798.096.7±1.594.292.693.2±8.088.282.984.3±10.4\nSVM 92.298.194.7±5.699.598.599.0±1.096.895.696.2±6.384.084.8 83.5±8.9\nTable 1 . Performance comparison of the score-fusion rules and the best individual expert (B.E.), showing precision (P),\nrecall (R) and F-measure (F), for the different data sets. The table shows the mean and 95% conﬁdence interval for the\nF-measure using K-fold cross-validation.\nWe will also intend to use a larger dataset to avoid over-\nﬁting in trained fusion rules. Finally, we will consider in-\nformation fusion in other relevant problems such as beat\ntracking and tempo induction.\n6. ACKNOWLEDGMENTS\nThanks to Juan Pablo Bello for kindly providing the dataset.\nThis work has been partially supported by the Spanish MEC,\nref. TEC2006-13883-C04-02, under the project AnClaS3\n“Sound source separation for acoustic measurements”.\n7. REFERENCES\n[1] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury,\nM. Davies, and M. B. Sandler, “A tutorial on onset de-\ntection in music signals,” Speech and Audio Process-\ning, IEEE Transactions on , vol. 13, no. 5, pp. 1035–\n1047, 2005.\n[2] S. Dixon, “Onset detection revisited,” in Proc. of the\nInt. Conf. on Digital Audio Effects (DAFx-06) , Mon-\ntreal, Quebec, Canada, Sept. 18–20, 2006, pp. 133–\n137.\n[3] N. Collins, “A comparison of sound onset detection al-\ngorithms with emphasis on psychoacoustically moti-\nvated detection functions,” in In AES Convention 118 ,\nno. 6363, 2005.\n[4] S. Abdallah and M. Plumbley, “Probability as meta-\ndata: Event detection in music using ICA as a condi-\ntional density model,” in ica03 , Nara, Japan, Apr. 2003,\npp. 233–238.\n[5] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzane-\ntakis, C. Uhle, and P. Cano, “An experimental com-\nparison of audio tempo induction algorithms,” Audio,\nSpeech, and Language Processing, IEEE Transactions\non, vol. 14, no. 5, pp. 1832–1844, 2006.\n[6] A. Bregman, “Psychological data and computational\nasa,” in Computational auditory scene analysis ,\nD. Rosenthal and H. Okuno, Eds. Mahwah, NJ, USA:\nLawrence Erlbaum Associates, Inc., 1998.\n[7] R. Zhou, M. Mattavelli, and G. Zoia, “Music onset de-\ntection based on resonator time frequency image,” Au-\ndio, Speech, and Language Processing, IEEE Transac-\ntions on , vol. 16, no. 8, pp. 1685–1695, 2008.[8] Y. S. Wan-Chi Lee and C.-C. J. Kuo, “Musical onset\ndetection with linear prediction and joint features,” in\n2007 MIREX contest results , 2007.\n[9] C.-C. Toh, B. Zhang, and Y. Wang, “Multiple-Feature\nFusion Based Onset Detection for Solo Singing Voice,”\ninProceedings of the 9th International Conference on\nMusic Information Retrieval (ISMIR’08) , Philadelphia,\nUSA, September 2008.\n[10] A. Lacoste and D. Eck, “A supervised classiﬁcation al-\ngorithm for note onset detection,” EURASIP J. Appl.\nSignal Process. , vol. 2007, no. 1, pp. 153–153, 2007.\n[11] N. Degara-Quintela, A. Pena, M. Sobreira-Seoane, and\nS. Torres-Guijarro, “A mixture-of-experts approach for\nnote onset detection,” in 126th AES Convention , Mu-\nnich, Germany, May 2009.\n[12] M. A. Casey, R. Veltkamp, M. Goto, M. Leman,\nC. Rhodes, and M. Slaney, “Content-based music in-\nformation retrieval: Current directions and future chal-\nlenges,” Proceedings of the IEEE , vol. 96, no. 4, pp.\n668–696, 2008.\n[13] L. I. Kuncheva, Combining Pattern Classiﬁers: Meth-\nods and Algorithms . Wiley-Interscience, July 2004.\n[14] J. Kittler, “Combining classiﬁers: A theoretical frame-\nwork,” Pattern Analysis & Applications , vol. 1, no. 1,\npp. 18–27, March 1998.\n[15] D. P. Ellis, “Prediction-driven computational auditory\nscene analysis,” Ph.D. dissertation, MIT Department of\nElectrical Engineering and Computer Science, 1996, i.\n[16] J. P. Bello, “Towards the automated analysis of sim-\nple polyphonic music: A knowledge-based approach,”\nPh.D. dissertation, King’s College London - Queen\nMary, University of London, 2003, i.\n[17] A. Flexer, “Statistical evaluation of music informa-\ntion retrieval experiments,” Journal of New Music Re-\nsearch , vol. 35, no. 2, pp. 113–120, June 2006.\n[18] N. Poh and S. Bengio, “Estimating the conﬁdence in-\nterval of expected performance curve in biometric au-\nthentication using joint bootstrap,” Tech. Rep., 2006.\n121"
    },
    {
        "title": "21st Century Electronica: MIR Techniques for Classification and Performance.",
        "author": [
            "Dimitri Diakopoulos",
            "Owen Vallis",
            "Jordan Hochenbaum",
            "Jim W. Murphy",
            "Ajay Kapur"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416278",
        "url": "https://doi.org/10.5281/zenodo.1416278",
        "ee": "https://zenodo.org/records/1416278/files/DiakopoulosVHMK09.pdf",
        "abstract": "The performance of electronica by Disc Jockys (DJs) presents a unique opportunity to develop interactions between performer and music. Through recent research in the MIR field, new tools for expanding DJ performance are emerging. The use of spectral, loudness, and temporal descriptors for the classification of electronica is explored. Our research also introduces the use of a multitouch interface to drive a performance-oriented DJ application utilizing the feature set. Furthermore, we present that a multi-touch surface provides an extensible and collaborative interface for browsing and manipulating MIRrelated data in real time.",
        "zenodo_id": 1416278,
        "dblp_key": "conf/ismir/DiakopoulosVHMK09",
        "keywords": [
            "electronica",
            "performer",
            "music",
            "spectral",
            "loudness",
            "temporal",
            "classification",
            "DJ performance",
            "multitouch interface",
            "performance-oriented DJ application"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   \n \n21ST CENTURY ELECTRONICA: MIR TECHNIQUES FOR \nCLASSIFICATION AND PERFORMANCE \nDimitri Diakopoulos, Owen Vallis, Jordan Hochenbaum, Jim Murphy, Ajay Kapur \n \nCalifornia Institute of the Arts \nValencia, CA USA \n{ddiakopoulos,jamesmurphy} \n@alum.calarts.edu New Zealand School of Music \nWellington, NZ \n{ajay.kapur,vallisowen,hochenjord}\n@nzsm.ac.nz \nABSTRACT  \nThe performance of electronica by Disc Jockys (DJs) \npresents a unique opportunity to develop interactions be-tween performer and music. Through recent research in the MIR field, new tools for expanding DJ performance are emerging. The use of spectral, loudness, and temporal descriptors for the classifi cation of electronica is ex-\nplored. Our research also introduces the use of a multi-touch interface to drive a pe rformance-oriented DJ appli-\ncation utilizing the feature set. Furthermore, we present that a multi-touch surface provi des an extensible and col-\nlaborative interface for browsing and manipulating MIR-related data in real time.  \nKeywords: Electronica, Electronic Dance Music, Genre \nClassification, User Interfaces, DJ, Multi-touch.   \n1. INTRODUCTION  \nElectronic dance music, often re ferred to as Electronica, is \nan overarching collection of genres that focus predomi-nately on rhythmic motifs & repeating loops. A task of the electronica DJ is to compile a set-list of music for per-formance. Additionally, DJs are always looking for ways to expand the interactivity of their performances through the use of new tools. The primary goal of this work is to give the modern, digital DJ access to a wider range of performance options using MIR techniques such as fea-ture extraction, genre classification, and clustering. Com-bined with advances in tabletop computing, these tech-niques have made it possible to add a layer of interactivity to automatic playlist generation.  \nIn the following section we detail related work on mu-\nsic features and electronic performance interfaces includ-\ning recent work in tabletop co mputing. In the remainder \nof the paper we discuss our feature extractors, genre clas-sification results and the inte rface that we developed to \nenable DJs to interact with those results to create set-lists. We conclude the paper with a discussion of future work in interactive MIR powered DJ applications and tabletop computing.   2. RELATED WORK \nOur work draws on a wide array of related research rang-\ning from musical descriptors and novel performance inter-faces to recent applications in tabletop computing. By \nsynthesizing these related but disparate areas of research, \nwe enable new performance experiences for individual \nand group DJs to create and modify set-lists in real time.  \nGenre classification can be  accomplished using a range \nof signal features and algorithms. For electronica in par-ticular, features and patterns such as rhythm, tempo, pe-riodicity, and even use of panning have been explored in the literature [1-3].  \nFor DJs specifically, the use of interfaces to retrieve \nmusically relevant material in performance has included query-by-beat-boxing [4], and query-by-humming [5]. Retrieval using both traditional and non-traditional in-struments and interfaces has b een explored by [6]. Other \nresearch in the academic ar ena for enabling DJ perform-\nance includes AudioPad [7], and Mixxx [8]. Although we take influence in these interf aces for retrieval, our work \nwishes to explore a browsing paradigm using similar creative interfaces.  \nIn the commercial sector, Stanton’s Final Scratch\n1 en-\nables DJs to use a physical controller to manipulate and mix digital music, while Native Instruments’ Traktor\n2 is a \nsoftware-only solution for DJ performance. Ableton’s \nflagship software, Live3, has been increasingly used to \nenable DJs to use their own pre-composed music in live \nperformance through the synchronized playback of dif-ferent audio loops, known as clips.    \nA multitude of literature on tabletop computing & in-\nterfaces exists.  The Reactable team was one of the first groups to directly apply both tangible and multi-touch \ninteraction to the performance of music [9], followed by others including the earlier referenced AudioPad,  which \nis also a tangible interface.  More recently , MarGrid, a UI \nfor the browsing of a digital music collection using Self-Organizing Maps has been examined using a tabletop interface [10]. The use of Se lf Organizing Maps (SOM) \nfor visualizing feature data has also been previously cov-ered by [11], [12]. In addition, although not performance-oriented, MusicSim  presents an interesting combination of \naudio analysis and music brow sing in an interactive com-\nputer-based interface [13]. \nOur aim here is to expand on these efforts by introduc-\ning the use of a multi-touch surface in a way that is both intuitive and collaborative. The use of Self Organizing \n                                                          \n \n1 http://www.stantondj.com/ \n2 http://www.native-instruments.com/ \n3 http://www.ableton.com/  Permission to make digital or hard copi es of all or part of this work fo r \npersonal or classroom use is granted without fee p rovided that copies\nare not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. \n© 2009 International Society for Music Information Retrieval \n \n465Poster Session 3\n   \n \n Maps represents a useful way of organizing features for \nvisualization, on-top of which many real-time interactive applications are possible.  \n3. DATA COLLECTION \nFor our experiments, six genres across the spectrum of \nelectronic music were selected  for their diverse character-\nistics and wide-spread popularity.  \nOne hundred 2 to 8 minute prototypical tracks were \nsliced at random into si ngle 30-second chunks for each \ngenre. Our dataset contains at least 20 distinct artists in each genre; tracks were not chosen on the perceived genre of the composing artist, but a human baseline analysis by the authors.  In total, there are 600 30-second clips, each in a stereo 44.1 kHz PCM-encoded file format. \nAll files were normalized before experimentation.  \n3.1 Genre Definitions \nMany subgenres fall beneath the umbrella term of elec-\ntronica—this paper examines six of the most broad & popular genres commonly played  by DJs: intelligent dance \nmusic (IDM), house, techno drum and bass (DnB), trance, and downtempo. A brief description of them is as follows:  \nIDM  distinguishes itself by its heavy use of complex \nmeter, sophisticated and often sporadic percussive ele-ments, and varying use of syncopation. IDM carries with it a rich harmonic and melodic palate borrowed from many genres. Tempos typically range from 150-180 BPM. Notable artists in the genre are Aphex Twin, Squarepusher, and Autechre. IDM may sometimes be referred to as Glitch music.  \nHouse  music makes use of the common ‘four-on-the-\nfloor’ rhythm pattern consisting of a steady kick drum on each downbeat in a 4/4 meter. Defining characteristics involve offbeat open hi-hat patterns and snare or claps on the two and four of every bar. Harmonic content and in-strumentation is often borrowed from Disco genres. Tem-pos usually range from 115 to 135 BPM. Daft Punk, Thomas Bangalter, and Alan Braxe are popular artists in the genre.  \nTechno  uses minimal melodic ornamentation, relying \nmore on bass riffs and polyrhythmic drums layered over a common four-on-the-flour kick drum. The rhythmic ele-ments in techno are often the defining features of the song, with percussive groove s and riffs taking precedence \nover more traditional melodic and harmonic structure. Significant artists include Derrick May, Richie Hawtin, and Robert Hood.  \nDnB makes heavy use of “break beat chopping,”—the \nre-sequencing of drum hits from other previously re-corded material. DnB is often composed above 160 BPM, with characteristic bass lines moving at half the tempo. Goldie and Pendulum are both well-known artists. \nTrance distinguishes itself by employing thick, com-\nplex harmonic components, leaving little room for the \ncomplex rhythmic structures found in other similar gen-res. Trance often makes use of arpeggios, drum rolls, and long crescendos of synthesizers. The genre is composed around 140 BPM. DJ Tiesto, Ferry Corsten, and Sasha \nare popular artists within the Trance genre. \nDowntempo employs lush harmonic textures and \ngroove-oriented percussion. Tempos are characteristically low, ranging from 60 to 90 BPM. Boards of Canada, Air, and Bonobo are well-known artists within the genre.  \n4. AUDIO ANALYSIS AND CLASSIFICATION \nAudio analysis was performed using the ChucK audio \nprogramming language [14]. Our results are based on a two-second (88200 sample) Ha nn window, resulting in 15 \n8-dimensional vectors for each audio clip. In addition to \nbeing written to disk for further analysis, the raw data was also sent over networked protocol (OSC\n1) into Process-\ning2, a visuals-oriented programming language.  The proc-\ness of visualizing the data using Processing is later pre-sented in Section 5. Before application development could \nbegin, a central concern was to uncover a feature-set that could accurately classify el ectronica. We follow with a \ndescription of the eight features used in our experiments.  \n4.1 Spectral Features \n• Centroid, the centre of mass of the spectrum;  \n• Flux, the change in sp ectral energy across suc-\ncessive frames;  \n• Rolloff, the frequency below which resides 85 \npercent of a spectrum's energy.  \n4.2 Loudness Features \n• RMS, the amplitude of a window; \n• Panning, a coefficient used to describe the \nweight of the signal in either the left or right channels [3]; \n• Panning Delta, change in the panning coefficient \nacross successive windows [3].   \n4.3 Temporal Features \n• Number of Bass Onsets, an integer representing \nthe number of peaks (‘Beats’) detected in a win-dow; \n• Average Inter-onset Time, a basic feature to de-\nscribe the periodicity of the beats across a win-dow. \n4.4 Classification \nFour separate classifiers were run on all six classes, and \nalso on a smaller set of four classes. All experiments were performed utilizing a 10-fold cross-validation method in \nthe Weka machine learning environment  [15].  \nA k-Nearest Neighbour classifier (IBk) gave the best \noverall result, resting at a 75.2% classification rate across the six classes (16.7% baseline accuracy). Table 1 shows the confusion matrix for this experiment.  \n                                                          \n \n1 http://opensoundcontrol.org/ \n2 http://www.processing.org/ \n46610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   \n \n As Table 1 illustrates, the k-NN classifier had trouble \ndistinguishing between IDM & DnB, and House & Techno. This is most likely attributed the sporadic per-cussive elements found in IDM & DnB, and very similar tempos found in House & Techno. Another experiment was run omitting IDM and House, resulting in a superior 87.0% classification rate. In the context of real-time per-\nformance and playlist generation, the omission was the result of IDM and House being considerable similar to genres already being classified. In favor of omitting any single pair of the confused genres, one of each was left out. The confusion matrix of this experiment is shown in Table 2.  \nOther classifiers used in testing were a C4 Decision-\nTree (J48), a backpropagation Artificial Neural Network (MultiLayerPerceptron), and a Support Vector Machine \n(SMO). More details on these classifiers can be found in [15, 16]. Table 3 lists the accu racy of the four different \nclassifiers using both the six-class and four-class datasets. \nThe exclusion of the two panning features and average \ninter-onset for the 6 and 4-class datasets using k-NN re-duced classification accuracy  by 7.70% and 4.85% re-\nspectively, indicating that both temporal and panning features moderately improved classification. Given the distinct tempos and production values between elec-tronica genres, higher-level features using both tempo and panning should be considered an  important facet of future \nclassification experiments.  \n \n Idm Tno Dnb Hse Trn Dtm \nIdm 0.60 0.02 0.16 0.07 0.02 0.12 \nTno 0.02 0.84 0.03 0.07 0.04 0.01 \nDnb 0.10 0.03 0.72 0.05 0.05 0.04 \nHse 0.04 0.05 0.06 0.78 0.04 0.03 \nTrn 0.01 0.04 0.05 0.05 0.82 0.02 \nDtm 0.13 0.01 0.06 0.05 0.02 0.74 \n \nTable 1  Confusion matrix, in percent, for the 6-class k-\nNN classifier  \n Techno Dnb Trance Dtempo \nTechno 0.90 0.05 0.05 0.01 \nDnb 0.04 0.84 0.06 0.06 \nTrance 0.05 0.06 0.87 0.02 \nDtempo 0.01 0.10 0.02 0.87 \n \nTable 2  Confusion matrix, in percent, for the 4-class k-\nNN classifier \n \n IBk J48 MLPercept. SMO \n6 Class 0.75 0.66 0.60 0.58 \n4 Class 0.87 0.82 0.81 0.79 \n \nTable 3  Accuracy, in percent, among the four classifiers \n     5. APPLICATIONS \n \nA large portion of our work consisted of prototyping \nand testing potentially useful tools for the DJ. By sorting our dataset through the use of Self Organizing Maps, DJs will be able to generate groupi ngs of musical material that \nimmediately work well together. This data organization will provide not only obvious s ong clusters, but also in-\nteresting musical associations that may otherwise be over-looked.  \n5.1 Bricktable \nA multi-touch surface called Bricktable [17] was chosen \nas the interface for visualizi ng and interacting with the \nSOMs. Multi-touch screens add a certain physicality to the data for the user, additionally supplying a modular soft-ware platform on which to expand the performance capa-bilities of these tools, especially between multiple poten-tial users.  \n5.2 Self Organizing Maps \nOur first application visualized the data in Processing us-\ning a SOM. The ability to effectively reduce dimensional-ity using a standard k-NN algorithm and the ease of visu-alization made a SOM an appealing choice to display the data as well as create a basic platform for playlist genera-tion. The use of SOMs for playlist generation has been previously researched extensively by M. Dittenbach et. al.  \nusing their PlaySOM system [18].  \nIndividual songs consist of 15 8-dimensional feature \nvectors. During our feature extraction stage, ChucK sends the features over Open Sound Control into Processing along with file name and path. The features are then or-dered in a hierarchal manner, and superimposed over an RGB vector. The color vectors are then used to visualize \nunique songs on a 2D map. Once the map is populated and sorted, users can access i ndividual songs by touching \na coloured circle. This will recall the filename and begin playing the song, allowing users to quickly compare neighbouring music.  \n \n \n \nFigure 1  The SOM being displayed on the Bricktable, \nwith the DJen  interface minimized \n467Poster Session 3\n   \n \n 5.3 The DJen  Performance Application \nAlthough the use of multi-touc h for selecting songs di-\nrectly on the SOM provides an engaging way for brows-ing a music collection, the application can be pushed fur-ther within the multi-touch paradigm. With this in mind, we present the DJen (‘D-Gen’) application, a tool to fa-\ncilitate automatic set-list generation by enabling effective navigation of large libraries of music.   \nA critical skill among successful  DJs is the ability to \nnavigate seamlessly between many different songs, some-times from varying genres. The key to this task is having the songs share a relationship in some way, usually through tempo. Via our SOM visualization, DJs already have access to musical groupings based off similarities, even if the genre is misclassified; however, DJen  allows \nDJs to gesture a path through this map creating a dynamic playlist that can be used as source material for a perform-ance. Due to the similar ities between neighbours on the \nmap, any arbitrary path will automatically generate a list of songs that share a str ong relationship. Additionally, \nmultiple DJs can create paths simultaneously, and DJen  \ncan interpolate a single path equidistant from all other paths. This will create a set list that represents the mean vectors between the original DJen  paths. Finally, paths \nmay be modified in real time for fine-tuning. Through this process we hope to enable the grouping of material in ways that a DJ may find inspiring.  This path-based sys-tem is reminiscent of research conducted by R. Gulik and F. Vignoli in [19].   \nFigure 2 demonstrates the DJen GUI with the two pri-\nmary UI elements shown: the playlist editor, and the ‘now-playing’ bar. Without a path set, a DJ can drag in-dividual circles into the playlist editor to create a set list. When a path is drawn, the editor is automatically popu-lated. If working collaboratively, another DJ may reshape the path and the playlist editor will automatically re-generate a set list.   \n \n \n \nFigure 2  The SOM with the DJen  GUI.  \n6. CONCLUSIONS & FUTURE WORK \nThrough the use of MIR tools we have shown strong po-\ntential for categorizing Electronica by genre. The k-NN algorithm provided an effective way of processing the sample data, making the DJen application possible through the use of a SOM. Finally, coupling this work \nwith a multi-touch surface opened new avenues for DJs to interact with their music.   \nThe DJen  application represents a motivating step to-\nward ‘intelligent’, MIR-powered tools for DJs. Its strength is revealed through th e interactive user interface \nand visualization techniques. In the future, more rhythmic features to categorize electr onica may be explored to cre-\nate a system whereby  DJen can perform automatic transi-\ntions between songs. This w ill allow the DJ to concen-\ntrate on other expressive areas such as sampling, looping, and effects processing.  \nThe continuous growth of multi-touch necessitates \ndevelopment of further applications to explore both single and multi-user performance paradigms. Although DJen \nmay be used by one or more users, extensive collabora-tion options may be enabled by allowing one DJ to over-see transitioning, while another manages multiple set-lists stemming from the original path  chosen across the map.  \nLooking into the future, we hope DJen  and other \nMIR-powered applications of its type will in the future enable any DJ to create expr essive performances for their \naudiences.   \n7. ACKNOWLEDGEMENTS \n \nWe would like to thank Nick Diakopoulos for his helpful \nideas and revisions. We would also like to thank George Tzanetakis for his inspiration and training.  \n \n8. REFERENCES \n \n1. Gouyon, F. and S. Dixon. Dance Music Classifi-\ncation: a tempo-based approach . in Proceedings \nof the 5th International Conference on Music In-formation Retrieval . 2004. Barcelona, Spain. \n2. Gouyon, F., et al. Evaluating rhythmic descrip-\ntors for musical genre classification . in Proceed-\nings of the 25th International AES Conference . \n2004. London, UK. \n3. Tzanetakis, G., R. Jones, and K. McNally. \nSte-\nreo Panning Features for Classifying Re-cording Production Style\n. in Proceedings of \nthe 8th International Conference on Music Information Retrieval\n. 2007. Vienna, Aus-\ntria. \n4. Kapur, A., R. McWalter, and G. Tzanetakis. \nQuery-by-Beat-Boxing: Mu sic Retrieval For The \nDJ. in Proceedings of the 5t h International Con-\nference on Music Information Retrieval . 2004. \nBarcelona, Spain. \n5. Birmingham, W., R. Dannenberg, and B. Pardo, \nQuery by humming with the VocalSearch system.  \nCommun. ACM, 2006. 49(8): p. 49-52. \n6. Kapur, A., R. McWalter, and G. Tzanetakis. \nNew Music Interfaces for Rhythm-Based Re-trieval . in Proceedings of the 6th International \nConference on Music Information Retrieval . \n2005. London, England. \n46810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   \n \n 7. Patten, J., B. Recht, and H. Ishii. Interaction \ntechniques for musical performance with table-top tangible interfaces . in Proceedings of the \n2006 ACM SIGCHI international conference on Advances in computer entertainment technology . \n2006. Hollywood, California. \n8. Andersen, T.H. Mixxx: Towards Novel DJ Inter-\nfaces . in Proceedings of the 2003 Conference on \nNew Interfaces for Musical Expression . 2003. \nMontreal, Canada. \n9. Jorda, S., et al. The reacTable . in Proceedings of \nthe International Computer Music Conference . \n2004. Barcelona, Spain. \n10. Hichner, S., J. Murdoch, and G. Tzanetakis. Mu-\nsic Browsing Using a Tabletop Display . in Pro-\nceedings of the 8th Inte rnational Conference on \nMusic Information Retrieval . 2007. Vienna, \nAustria. \n11. Pampalk, E., G. Widmer, and A. Chan, A new \napproach to hierarchical clustering and struc-turing of data with Self-Organizing Maps.  Intell. \nData Anal., 2004. 8(2): p. 131-149. \n12. Cooper, M., et al., Visualization in Audio-Based \nMusic Information Retrieval.  Comput. Music \nJournal, 2006. 30(2): p. 42-62. \n13. Chen, Y.-X. and A. Butz, Musicsim: integrating \naudio analysis and user feedback in an interac-tive music browsing ui , in Proceedings of the \n13th International Conference on Intelligent User Interfaces . 2009: Florida, USA. \n14. Fiebrink, R., G. Wang, and P. Cook. Support \nFor MIR Prototyping and Real-Time Applica-tions in the ChucK Programming Language . in \nProceedings of the 9th International Con-\nference on Music Information Retrieval . \n2008. Philadelphia, USA. \n15. Witten, I.H. and E. Frank, Data Mining: Practi-\ncal machine learning tools and techniques . 2nd \ned. 2005, San Francisco: Morgan Kaufmann. \n16. Duda, R.O., P.E. Hart, and D.G. Stork, Pattern \nClassification . 2nd ed. 2000, New York: John \nWiley & Sons. \n17. Hochenbaum, J. and O. Vallis. Bricktable: A \nMusical Tangible Multi-Touch Interface . in Pro-\nceedings of the Berlin Open . 2009. Germany. \n18. Dittenbach, M., R. Neumayer, and A. Rauber. \nPlaySOM: An Alternative Approach to Track Se-lection and Playlist Generation in Large Music Collections . in Proc. 1st Intl. Workshop on Au-\ndio-Visual Content and Information Visualiza-tion in Digital Libraries . 2005. Cortona, Italy. \n19. Guelik, R.V. and F. Vignoli. Visual Playlist \nGeneration on the Artist Map . in Proceedings of \nthe 6th International Conference on Music In-formation Retrieval . 2005. London, England. \n   \n469"
    },
    {
        "title": "Ten Years of ISMIR: Reflections on Challenges and Opportunities.",
        "author": [
            "J. Stephen Downie",
            "Donald Byrd",
            "Tim Crawford"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415170",
        "url": "https://doi.org/10.5281/zenodo.1415170",
        "ee": "https://zenodo.org/records/1415170/files/DownieBC09.pdf",
        "abstract": "The International Symposium on Music Information Retrieval (ISMIR) was born on 13 August 1999. This paper expresses the opinions of three of ISMIR’s founders as they reflect upon what has happened during its first decade. The paper provides the background context for the events that led to the establishment of ISMIR. We highlight the first ISMIR, held in Plymouth, MA in October of 2000, and use it to elucidate key trends that have influenced subsequent ISMIRs. Indicators of growth and success drawn from ISMIR publication data are presented. The role that the Music Information Retrieval Evaluation eXchange (MIREX) has played at ISMIR is examined. The factors contributing to ISMIR's growth and success are also enumerated. The paper concludes with a set of challenges and opportunities that the newly formed International Society for Music Information Retrieval should embrace to ensure the future vitality of the conference series and the ISMIR community.",
        "zenodo_id": 1415170,
        "dblp_key": "conf/ismir/DownieBC09",
        "keywords": [
            "International Symposium on Music Information Retrieval (ISMIR)",
            "established on 13 August 1999",
            "opinions of three founders",
            "reflect upon the first decade",
            "first ISMIR held in October 2000",
            "elucidates key trends",
            "growth and success indicators",
            "Music Information Retrieval Evaluation eXchange (MIREX)",
            "future vitality of the conference series",
            "ISMIR community"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \n \n                                                          TEN YEARS OF ISMIR:  \nREFLECTIONS ON CHALLENGES AND OPPORTUNITIES \nJ. Stephen Downie Donald Byrd Tim Crawford \nUniversity of Illinois  \nat Urbana-Champaign \njdownie@illinois.edu  Indiana University  \nat Bloomington \ndonbyrd@indiana.edu  Goldsmiths College \nUniversity of London \nt.crawford@gold.ac.uk\nABSTRACT \nThe International Symposium on Music Information Re-\ntrieval  (ISMIR) was born on 13 August 1999. This paper \nexpresses the opinions of three of ISMIR’s founders as \nthey reflect upon what has happened during its first dec-ade. The paper provides the background context for the events that led to the establishment of ISMIR. We high-light the first ISMIR, held in Plymouth, MA in October \nof 2000, and use it to elucidate key trends that have in-\nfluenced subsequent ISMIRs. Indicators of growth and \nsuccess drawn from ISMIR publication data are pre-\nsented. The role that the Music Information Retrieval Evaluation eXchange (MIREX) has played at ISMIR is \nexamined. The factors contributing to ISMIR's growth and success are also enumerated. The paper concludes with a set of challenges a nd opportunities that the newly \nformed International Society for Music Information Re-\ntrieval  should embrace to ensure th e future vitality of the \nconference series and the ISMIR community. \n1. ORIGINS OF ISMIR \nIn mid-August 1999, Byrd and Downie were at the Ra-disson Hotel Berkeley Marina conference center in Berkeley, California: Byrd for the ACM (Association for \nComputing Machinery) Di gital Library Conference \n(DL ’99), Downie for the ACM SIGIR conference, \nwhich immediately followed DL ’99. We had not met \nbefore, but our paths had been converging for some time, and in retrospect, it is hardly surprising that something special came out of our face- to-face encounter. Crawford \nwas in England at the time, but he and Byrd had been collaborating since the early 1990s. Crawford and Byrd had recently received word that their “Online Music \nRecognition and Searching” (OMRAS) project [1] would be jointly funded by the Joint Information Sys-tems Committee (JISC) of the UK and the National Science Foundation (NSF) of the USA. Steve Griffin, the project’s NSF program officer, had already sug-gested to Byrd and Crawford independently that a music-IR workshop be organized in conjunction with OMRAS. Furthermore, Crawford was organizing another work-shop on music IR, as part of the “Digital Resources for the Humanities” conference to  be held in London in Sep-\ntember 1999. Finally, Downie, with the assistance of David Huron (Ohio State University) and Craig Nevill-Manning (then of Rutgers University), had organized “The Exploratory Workshop on Music Information Re-\ntrieval” at SIGIR ’99.\n1 Before going to Berkeley, Down-\nie was already thinking of a larger-scale follow-up event \nas this was an explicit goal of his SIGIR workshop. One of the workshop presenters, Michael Fingerhut of IRCAM, would later play a pivotal role in the success of ISMIR through his establishment and maintenance of vital community resources (see Section 3.2). \nWith the encouragement of Bruce Croft (University of \nMassachusetts, Amherst)—a very well-known researcher \nin the text IR world, and Byrd’s boss at the time—\nDownie and Byrd decided on the spot to join forces to plan a larger-scale event instead of a workshop in the normal sense, and they came up with the name “Interna-tional Symposium on Music Information Retrieval.” \nMost of the above has been described in print before \n[2]. Previously unreported, however, are some informal meetings convened in Berkeley, which variously in-cluded Byrd, Downie, Nevill-Manning, David Bain-bridge (University of Waikato), Matthew Dovey (Univer-sity of Oxford), and Massimo Melucci (University of Pa-dua). It is interesting that Byrd’s notes of these meetings show a heavy emphasis on music in symbolic form over audio, and quite a bit of discussion of TREC\n2-like evalu-\nations of music-IR systems.\n1.1 What’s in a Name?: Evolution of “ISMIR” \nThe ISMIR acronym, decided upon during the August \n1999 meetings, was carefully crafted. First, both Byrd and Downie wanted to strongly encourage the participa-tion of researchers from around the world, so Interna-\ntional  was chosen without hesitation. Second, the word \nsymposium  has its roots in the Greek verb, sympotein ,  \nPermission to make digital or hard copi es of all or part of this work fo r\npersonal or classroom use is granted withou t fee provided that copies\nare not made or distributed for profit or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page. © 2009 International Society for Music Information Retrieval   \n1 See http://nema.lis.uiuc.e du/sigir99_mir _wshop.pdf.  \n2 The Text Retrieval Conference upon which MIREX is based.  \n13Keynote Talks\n  \n \nwhich means “to drink together.”1 As many know, \nDownie is particularly fond of symposia. Besides its so-\ncial connotations, the term Symposium  was agreed upon \nas it indicated a certain academic middle-ground between a workshop and a full-fledged conference. Before long, however, some participants noted that they were having difficulties obtaining travel funding to attend “a mere \nsymposium,” and in 2002 ISMIR became the “Interna-tional Conference  on Music Information Retrieval.” Over \nthe years, ISMIR organizers explored affiliation oppor-tunities with such organizations as the Association for Computing Machinery (ACM), the Institute of Electrical and Electronics Engineers (I EEE), and the International \nComputer Music Association (ICMA); none worked out. Undeterred, Ichiro Fujinaga of McGill University led the way to formally establishing ISMIR as an independent society. On 4 July 2008, the “International Society  for \nMusic Information Retrieval” was officially born. By the time ISMIR 2009 in Kobe concludes, the music-IR com-munity will have elected its first roster of ISMIR execu-tive officers and held its first Annual General Meeting.  \n2. ISMIR 2000 AT PLYMOUTH, MA:  \nLANDING OF THE MUSIC-IR PILGRIMS \nIn accordance with the events of 1999 described above, ISMIR 2000\n2 was held in Plymouth, Massachusetts (the \nsite of the Pilgrims’ 1620 arrival in the New World) from 23 to 25 October 2000. Byrd was general chair and Downie was program chair. The other organizing com-mittee members were Crawford, Croft, and Nevill-Manning. In addition, Jeremy Pickens, then a PhD stu-dent working on the OMRAS pr oject, became, by virtue \nof his good nature, the local organizer—i.e., audio-visual \nperson and general helper—during the conference. \nIn terms of statistics, 88 people attended ISMIR 2000: \nnot bad at all for a first conference in the field, and about twice the attendance at the first computer-music confe-rence (which Byrd had attended in 1974). Furthermore, attendance was already very international: 29 attendees (33%) came from 11 countries outside the United States. ISIMIR 2000 was very heavy on invited papers, of which there were nine. An additional 33 papers were submitted; 10 were accepted as papers, 16 as posters. \n2.1 ISMIR 2000: Highlights and Commentary • Marvin Minsky delivered the keynote address. His talk \nwas uniquely creative and poi nted out several connec-\ntions that are still relevant, e.g., to artificial intelligence, \nimprovisation vs. written-out music, and even to his in-\nstitution, MIT.  \n• Beth Logan gave one of the first papers formally ex-\namining the implications of using Mel Frequency Cep-stral Coefficients (MFCC) for music; this created a fair \namount of controversy. We wish we had a penny for \neach MFCC calculated since 2000!  \n• There were two papers on music digital library applica-\ntions: Jon Dunn spoke on the “Variations” system; Da-vid Bainbridge talked about the “New Zealand Digital Music Library.” Downie, as a library science professor, \nnotes with some sadness that the digital library theme has not gained much traction in subsequent ISMIRs.  \n• Byrd, Crawford, and Steve Larson led a “Lecture, Re-\ncital, Discussion, and Survey” session on music similari-ty. Centered on Mozart’s piano piece Variations on Ah! Vous dirai-je, maman (the melody English speakers call \n“Twinkle, Twinkle, Little Star”), Larson played the \npiece, and attendees filled out survey forms to say how similar they felt each of the selected variations was to the theme. This session led to our choosing three meas-\nures from the Mozart variations for the ISMIR logo (Figure 1). The “similarity problem” remains a huge challenge, not least because of the difficulty of establish-ing “ground-truth” in this subjective area.  \n \nFigure 1 . The Mozart-based official ISMIR logo.  \n \n• Mary Levering of the U.S. Patent and Trademark Of-\nfice talked about “Intellectual Property Rights in Musi-\ncal Works.” This is a problem that continues to plague many music-related activities, including music-IR re-search.  \n• George Tzanetakis and Perry Cook gave a paper on au-\ndio-IR tools. Tzanetakis’s MARSYAS is now one of the \nmost widely used music-IR toolkits. \n• Jonathan Foote gave a pa per on recognizing pieces of \norchestral music regardless of performance differences. Foote’s approach looked so lely at low-level (though \nlong-term) audio features. Numerous music-IR papers since 2000 ignore musical knowledge and instead em-ploy low-level features that seem to work; this paper fo-reshadows the trend. Many of the intellectual themes, challenges, and oppor-tunities that would resonate  throughout subsequent con-\nvenings of ISMIR were already evident in Plymouth. To illustrate this, a selection of ISMIR 2000 highlights with \neditorial comments  follows:     \n \n                                                          \n • There were papers on musicology applications, tran-\nscription from audio, retrieval from audio, Optical Mu-1 See http://en.wikipedia.org/wiki/Symposium.  \n2 See http://ismir2000.ismir.net/.  \n1410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nsic Recognition (OMR), language modeling, and XML \nrepresentation of music notation. All of these except lan-\nguage modeling have been the subjects of numerous ISMIR papers since.  \n• Eric Allamanche of Fraunhofer gave an informal demo \nof an audio fingerprinting application designed to identi-fy broadcast music in real time. Similar systems are now \nwidely available in the commercial sphere. \n• The creators of MusicXML and MEI each gave posters \non early versions of their representations.  Since 2000 \nMusicXML has become the most popular XML form for music content, but MEI has r ecently been the subject of \ndevelopment for specific applications, particularly in the musicological domain. \n• Suzanne Lodato of the Andrew W. Mellon Foundation \ntook an active role in the plenary planning and discus-sions sessions of the symposium. The Mellon Founda-\ntion would go on to provide critical funding to prepare \nfor, establish, and run MIREX. \nOf the 19 full papers presented, six were mostly or en-\ntirely on audio; nine mostly or entirely on music in sym-bolic form (including metadata); and four about equally on audio and symbolic music. As is obvious to anyone who has attended the last five or six ISMIRs, the predo-minance of symbolic music (reflecting the backgrounds of the original organizers) has not persisted; we will say more about this later.  \nDespite the inexperience of the organizers and the no-\nvelty of the subject area, ISMIR 2000 was universally \nregarded as a resounding success.  \n3. SUCCESS AND GROWTH OF ISMIR: 2000–2009 \nThe ongoing success and growth of ISMIR since 2000 is \nboth remarkable and encouraging. The vitality of the community is readily appare nt from even the most cur-\nsory examination of the sta tistics. For example, Table 1 \npresents the number of published items (both posters and papers), number of pages published, and the number of unique authors represented in the proceedings of each ISMIR from 2000 to 2009. The table shows a 251% in-crease in the number of published items per year, from 35 to 123. The number of pages went up even more: 155 to 729 is a 370% increase. \nThe number of unique authors represented also grew \ntremendously, by 363% from 2000 (63) to 2009 (292). \nOn average, 183 unique authors made contributions to \neach of the 10 ISMIRs under consideration. For us, the growth in the number of unique authors is the best statis-tic of the set, since it indicat es that ISMIR has attracted \nthe most important asset of any conference: active, en-\ngaged and publishing researchers. \nMailing list statistics also confirm ISMIR’s success. \nThe music-ir@ircam.fr  list, established in October 2000, \nis the ISMIR community’s primary communications me-chanism. This list, as of 22 August 2009, has 1190 regis-tered subscriptions. It has broadcast nearly 3000 messag-es for an average of 28 per month. These are strong num-bers for such a specialized research area as music IR.  \nYEAR LOCATION ITEMS PAGES UNIQUE \n AUTHORS \n2000 Plymouth, MA 35 155 63\n2001 Bloomington, IN 41 222 86\n2002 Paris, FR 57 300 117\n2003 Baltimore, MD 50 209 111\n2004 Barcelona, ES 105 582 214\n2005 London, UK 114 697 233\n2006 Victoria, BC 95 397 198\n2007 Vienna, AT 127 486 267\n2008 Philadelphia, PA 105 630 253\n2009 Kobe, JP 123 729 292\nTOTALS ---- 852 4407 ---- \nTable 1.  ISMIR  publication  and author data 2000–\n2009.  1\ntself as a permanent \nfi ure in time for ISMIR 2005 [7].\n2005 2006 2007 20083.1 The Audio Description Contest and MIREX \nAs mentioned in Section 1, the formal evaluation of mu-sic-IR systems has been part of the ISMIR “wish list” since its inception. However, notwithstanding strong community interest, it was surprisingly difficult to insti-tute a formal evaluation framework along the lines of TREC. There were many challenges to overcome, the greatest of which was the lack of high-quality test collec-tion and ground-truth data caused primarily by the very \nrestrictive intellectual property regimes governing music \n[4]. After a series of exploratory workshops led by Downie and funded by Mellon and NSF [5], the organiz-ers of ISMIR 2004 in Barcelona were able to put togeth-er the “Audio Description Contest” (ADC) [6]. Many valuable lessons were learned in the running of ADC and these were subsequently incorporated into MIREX. After receiving substantial long- term funding from both Mel-\nlon and NSF, MIREX established i\nxt  \n \nNumber of Task \n(and Subtask) “Sets”  10 13 12 18\nNumber of Individuals 82 50 73 84\nNumber of Countries 19 14 15 19\nNumber of Runs 86 92 122 169\nTa\nthe key descriptive data for MIREX between 2005 and \n                                                          ble 2.  MIREX descriptive data 2005–2008 [8]. \nLike the publication data examined previously, the \nMIREX data are quite encouraging. Table 2 summarizes \n \n1 2000–2008 data sourced from the Preface of the ISMIR 2008 proceed-\nings [3]. \n15Keynote Talks\n  \n \n2008. A fuller explication of the MIREX data can be \nfound in [4, 8]. \nThe number of task and subtask sets grew by 80% \nfrom 2005 (10) to 2008 (18). This growth can be attri-buted to growing interest in MIREX and the donation of new high-quality data sets from community members.  \nIn keeping with ISMIR’s international mission, the \nnumber of countries represented was a strong, but flat, 19 for both 2005 and 2008 with an average of 17 per year. Most of these numbers come from European countries \nwith Japan, China, and Taiwan also represented. Like-\nwise, the number of individual participants has not ap-preciably increased between 2005 (82) and 2008 (84). We do note the lack of growth in the country and partici-pant numbers as something that needs addressing. \nThe most heartening MIREX statistic concerns the \nnumber of individual runs performed: this went from 86 to 169, an increase of 96%. Note that the increase is greater than the incr eases in both participants and tasks: \nparticipants are more likely now to submit multiple varia-tions on their algorithms. This fact suggests to us that MIREX has been successful in  its message that MIREX \nexists as an exploratory mechanism designed to try out new ideas and not a “contest” to be won or lost.  \nIn total, MIREX has run 469 algorithms. It is interest-\ning to note the distribution of runs over areas of interest: \n• 129 (28%) can be categorized as “train-test” machine-\nlearning classification experiments (e.g., Audio Genre \nClassification, Audio Mood Classification, etc.). \n• 139 (30%) can be categorized as “search” experiments \n(e.g., Audio Cover Song Identification, Audio Music Similarity, etc.) \n• 201 (43%) can be categorized as “low-level” feature \nexperiments (e.g., Audio Onset Detection, Audio Beat Tracking, etc.) \nWe must also note that, of the 22 unique task sets run \nover 2005 to 2008, only three (14%) have dealt exclu-sively with symbolic music data (i.e., Symbolic Genre Classification, Symbolic Key Finding, and Symbolic Me-lodic Similarity). Not one of these symbolic tasks was run in 2008 and not one proposed for MIREX 2009. 16 task sets (73%) have been exclusively audio-based (e.g., Audio Tempo Extraction, Audio Key Finding, etc.), and three tasks have involved a combination of audio and symbolic data (i.e., Query-by-Singing/Humming, Query-by-Tapping, and Score Following). As these data show, \nMIREX has been quite successful in growing evaluation \nactivity in the audio domain, but not at all successful in helping the symbolic sub-community to flourish: this is perhaps MIREX’s most serious weakness.  \n3.2 Success and Growth Factors \nMany factors have contributed to the success and growth \nof ISMIR over the years. Thes e factors are both external and internal to ISMIR. Like many things in life, ISMIR has been successful through a combination of good tim-ing, thoughtfulness, and hard work. \nFrom the beginning, ISMIR’s timing was good; it has \nbenefitted from several important external opportunities and trends that developed in parallel. These develop-\nments have provided ISMIR with a larger body of re-searchers and research themes to draw upon than we could have anticipated, especially in the audio domain. We believe these external factors include: \n• The success of the audio compression research com-\nmunity in developing techniques specifically designed for, and tested against, music. It was this success and the subsequent acceptance of these approaches that afforded \nthe opportunity to create, share, and store large collec-tions of music audio. \n \n• The explosive growth in the availability of audio files, \nmostly MP3’s, via the Internet. This growth resulted to a great extent from the audi o-compression research de-\nscribed above, but in turn it created a demand for better search and retrieval mechanisms. Napster, for example, was established in 1999.\n \n• The work of such standards bodies as the MPEG-7 \ngroup that brought together important industry players with leading academic res earch groups. The MPEG-7 \nfirst working draft came out in December 1999.\n1 \n• The success of such search engines as Google, Yahoo, \netc., that encouraged resear chers to seek fame and for-\ntune in the music domain. The great “dot.com bubble” of 1998–2001 was contemporaneous with ISMIR’s early development.  \nThe internal factors that have contributed to ISMIR’s \nsuccess are founded upon the thoughtful actions, good-will, and hard work of community members acting either \nas individuals, in small groups, or collectively. These fac-\ntors include: \n• The establishment of the communication resources \nhoused at IRCAM. The music-ir@ircam.fr  mailing list, \nthe hosting of the conference websites, and the archiving \nof the collected ISMIR pro ceedings are resources with-\nout which ISMIR might not exist today. Each of these has contributed inestimably to the openness, continuity, and intellectual life of the ISMIR community. We ap-plaud Michael Fingerhut for his continued service. \n• The diversity of backgrounds and disciplines \nrepresented on the ISMIR Steering Committee (SC). The SC has worked hard over the years to ensure that the \nbroadest possible range of research interests is present at each ISMIR. Ichiro Fujinaga has been the SC’s coordi-\nnator for years, and he is especially commended for his ability to guide the SC through its deliberations. \n• The great fortune ISMIR has had in the quality of the \nchairs and program committee (PC) members for each \n                                                          \n \n1See http://www.chiariglione.org/mpe g/standards/mpeg-7/mpeg-7.htm. \n1610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nconference. We have nothing but praise for the ISMIR \nPC teams; each ISMIR has been organized and run with enthusiasm, integrity, and efficiency. \n• The implicit policy of inclusiveness that has pervaded \nthe conference programming ethos of each ISMIR. Un-like other technology-related conference series, ISMIR has not measured its intrinsic value through high rejec-tion rates. In fact, the ISMIR PCs are to be applauded for finding mechanisms like expanded poster presenta-\ntion opportunities to allow for the maximum level of \nparticipation yet maintaini ng academic research quality \nthrough strong peer-reviewing. We believe that it is pre-cisely this policy of inclusiveness that has allowed for the all-important growth in unique author participation noted in Section 3. The ISMIR community as a whole is also to be praised for its consistent efforts to make the peer-review process simultaneously as fair, open-minded, and rigorous as possible.  \n• The ongoing PC and general community support for \nADC and MIREX. This support has contributed to ISMIR by fostering a sense of common purpose and ex-ploration among researchers in many of ISMIR’s sub-fields. MIREX has also helped to set standards in many sub-fields for what constitutes proper evaluation. Finally, MIREX has provided an extra opportunity for participa-tion in ISMIR for those researchers whose work could not be included in the offici al proceedings. We must ac-\nknowledge here the extra-special efforts made by Kris \nWest, M. Cameron Jones, Andreas F. Ehmann, and Mert \nBay in making MIREX run well. \n4. CHALLENGES AND OPPORTUNITIES \nIn its first 10 years, ISMIR has grown into a vibrant and enthusiastic research community. We now need to turn our attention to making ISMIR’s next 10 years, its “teen” years, even more rewardi ng and successful. Like a tee-\nnager, ISMIR will undoubtedly stop growing in size at \nsome point; this is only natural. But if ISMIR—both as a conference series and as a society—is to have a success-ful “adulthood,” it will need to  address some challenges \nthat it has not fully engaged with before. It must recast \nthese challenges as opportunities and engage them with its growing maturity and its youthful vigor. Five of the most important challenges are: \n1. ISMIR needs to more activ ely encourage the participa-\ntion of potential users of music-IR systems. Notwith-\nstanding the laudable efforts made by the ISMIR Steering Committee, ISMIR has tended to focus much less on the potential users of music-IR technology than on its devel-\nopers. These users might include, for example, perform-ing musicians, film-makers, musicologists, music libra-\nrians, sound archivists, music educators, and music en-thusiasts of all types. The knowledge acquired by inte-racting with users like these can only improve the quality of the community’s research output. It will also go a long way to helping ISMIR research ers create truly useful mu-\nsic-IR systems. 2. ISMIR research projects mu st dig deeper into the mu-\nsic itself. Notwithstanding some recent—and hearten-ing—developments in such areas as, for example, chord detection, cover song detection, and structural analysis, etc., a large amount of ISMIR research effort, especially in timbre-based audio matching, has gone into attempts to \nenhance a few basic features and matching algorithms. \nHowever, it seems likely that there is a point beyond \nwhich improved matching performance using any single \nfeature cannot be achieved [9]. On the other hand, the incorporation of multiple features in what might be thought of as “hybrid” matching tends to be more suc-cessful. But such combining of features needs to be done in a way that is understood and principled, and much more research needs to be done in understanding what such combinations actually represent in musical terms . \nThe integration of symbolic music data to create hybrid audio + symbolic music-IR systems could help in this re-gard. \n3. Time has come for ISMIR to expand its musical hori-\nzons. The vast majority of ISMIR’s collective music-IR \nresearch has been conducted on Western popular musics of the late-20\nth and early-21st centuries. This is a serious \nproblem because there is an enormous amount of music in existence that is utterly different from these corpora. There is no reason to assume algorithms that work super-bly for the Beach Boys  will do anything useful with Tu-\nvan throat singing, musique concrète , or Indian Raga.  \n4. ISMIR must rebalance the portfolio of music informa-tion types with which it engages. Music information is \ninherently multifaceted. Each of its manifestations—\naudio, symbolic, and metadata—contributes different but equally important features to the experience of music. We celebrate the accomplishments of ISMIR’s audio re-searchers but, as noted before, research exploiting the symbolic aspects of music information has not thrived under ISMIR. We are thrilled to see, however, the grow-ing body of work that strives to unite social metadata and audio information. Rather than “pushing down” on the audio side of ISMIR research, we challenge ISMIR to make special efforts to “pull up” symbolic and metadata research to create a more productive, synergistic, and harmonious balance among the three.  \n5. ISMIR must encourage the development and deploy-\nment of full-featured, multifaceted, robust, and scalable music-IR systems with help ful user-interfaces. During \nISMIR’s first decade, we have seen a great deal of effort \nexpended on the development of the various sub-\ncomponents of music-IR systems. Unfortunately, we \nhave not yet seen much in the way of a successful inte-\ngration of these sub-systems into real-world-useable re-\n17Keynote Talks\n  \n \nsources. This state of affairs cannot be sustained for the \nnext decade as the community  needs these full-featured \nsystems to exist in order to inspire the development of the next generation of refinement s and improvements. In the \ntext IR world, and starting in the 1960s, such systems as “SMART,”\n1 “Managing Gigabytes,”2 and “Terrier,”3 \nhave fulfilled this important, if not imperative, role.  \n4.1 The Grand Challenge \nWe see our “complete system” challenge as “ The Grand \nChallenge” for ISMIR’s second decade. By embracing \nthis challenge, the preceding ones will necessarily have \nto be engaged. We do recognize, however, that meeting \nthis “Grand Challenge” will not be easy. We believe \nthere will be difficulties because academic researchers traditionally have obtained little academic credit for comprehensive system development. Future ISMIR pro-gram committees need to find a mechanism through which the developers of such systems can acquire full academic credit for accomplishm ents. One possibility is \nto have ISMIR create a rigorous set of peer-reviewing criteria specifically designed to handle this type of work. Along these lines, the demonstration of complete systems should receive the same stat us now afforded to paper \npresentations. Special awards should also be considered. \n5. CLOSING REMARKS \nAs we noted in the beginning of this paper, the founders of ISMIR, because of their backgrounds, had conceived of music IR as an intersection of music and symbolic IR techniques. As early as ISMIR 2000, it became readily apparent that this conception was much, much too limit-\ning. ISMIR research papers now cover a wide range of \nactivities and recent “Calls for Papers” have reflected this \nbroadening of scope explicitly. We now challenge the ISMIR community to consider whether the term “music IR” has outlived its usefulness. Is it possible that “infor-mation retrieval” is too narrow a concept to fully encap-sulate what ISMIR researchers actually do? Byrd has proposed several times making the “R” in “ISMIR” stand for “Research” instead of “Retrieval” which could better describe the breadth of the organization without losing ISMIR’s name recognition. A related idea is to refer to “music informatics” instead of “music information.”  \nWe will leave these questions open in the hope that \nthey will inspire so me healthy, self-reflective, debate \nabout the future of ISMIR. It will be through such reflec-\ntions that ISMIR will continue to be vibrant, energetic, and successful well past its second decade. \n                                                          \n \n1 See http://en.wikipedia.org/wiki/SMART_Information_Retrieval_System.  \n2 See http://www.cs.mu.oz.au/mg/. \n3 See http://ir.dcs.gla.ac.uk/terrier/. 6. ACKNOWLEDGEMENTS \nWe thank Jeremy Pickens, Mi chael Casey, Ichiro Fuji-\nnaga, Masataka Goto, Keiji Hirata, and the Kobe 2009 \nOrganizing Committee, for th eir assistance in writing \nthis paper. We also thank the NSF and the Mellon Foun-dation for their financial support of MIREX. We are es-pecially grateful to Steve Gr iffin of NSF for the financial \nsupport he arranged for ISMIR 2000 and 2001. \n7. REFERENCES \n[1] D. Byrd and T. Crawford. “Problems of Music \nInformation Retrieval in the Real World,” Information Processing and Management , Vol. 38, \nNo. 2, pp. 249–272, 2002.  \n[2] D. Byrd and M. Fingerhut. “The History of ISMIR -A Short Happy Tale.” D-Lib Magazine , Vol. 8, No. \n11, 2002. See http://www.dlib.org/dlib/november02/ 11inbrief.html#BYRD.  \n[3] D. P. W. Ellis, Y. Kim, J. P. Bello, and E. Chew. “Preface,” Proceedings of the International \nConference on Music Information Retrieval (ISMIR 2008) , pp. 9–11, 2008.  \n[4] J. S. Downie. “The Music Information Retrieval Evaluation Exchange (2005-2007): A Window into Music Information Retrieval Research,” Acoustical \nScience and Technology , Vol. 29, No. 4, pp. 247–\n255, 2008. See http://dx.doi.org/10.1250/ast.29.247.  \n[5] J. S. Downie. “The Scientific Evaluation of Music Information Retrieval Systems: Foundations and Future,” Computer Music Journal , Vol. 28, No. 3, \npp. 12–23, 2004.  \n[6] P. Cano, E. Gomez, F. Gouyon, P. Herrera, M. Koppenberger, B. Ong, X. Serra, S. Streich, and N. Wack. ISMIR 2004 Audio Description Contest. MTG Technical Report, MTG-TR-2006-02 , Music \nTechnology Group, Barcelona, 2004.  \n[7] J. S. Downie, K. West, A. F. Ehmman, and E. Vincent. “The 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005): Preliminary Overview,” Proceedings of the International Conference on Music Information Retrieval (ISMIR 2005), pp. 320–323, 2005.  \n[8] J. S. Downie, A. F. Ehmann, M. Bay, and M. C. Jones. “The Music Information Retrieval Evaluation eXchange: Some Observations and Insights,” \nAdvances in Music In formation Retrieval, Springer, \nNew York, in press.  \n[9] J.-J. Aucouturier and F. Pachet, “Improving Timbre \nSimilarity: How High is the Sky?” Journal of \nNegative Results in Speech and Audio Sciences , Vol. \n1, 2004. See http://www.csl.sony.fr/downloads/ papers/uploads/aucouturier-04b.pdf. \n18"
    },
    {
        "title": "Harmonically Informed Multi-Pitch Tracking.",
        "author": [
            "Zhiyao Duan",
            "Jinyu Han",
            "Bryan Pardo"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418045",
        "url": "https://doi.org/10.5281/zenodo.1418045",
        "ee": "https://zenodo.org/records/1418045/files/DuanHP09.pdf",
        "abstract": "This paper presents a novel system for multi-pitch tracking, i.e. estimate the pitch trajectory of each monophonic source in a mixture of harmonic sounds. The system consists of two stages: multi-pitch estimation and pitch trajectory formation. In the first stage, we propose a new approach based on modeling spectral peaks and non-peak regions to estimate pitches and polyphony in each single frame. In the second stage, we view the pitch trajectory formation problem as a constrained clustering problem of pitch estimates in all the frames. Constraints are imposed on some pairs of pitch estimates, according to time and frequency proximity. In clustering, harmonic structure is employed as the feature. The proposed system is tested on 10 recorded four-part J. S. Bach chorales. Both multi-pitch estimation and tracking results are very promising. In addition, for multi-pitch estimation, the proposed system is shown to outperform a state-of-the-art multi-pitch estimation approach.",
        "zenodo_id": 1418045,
        "dblp_key": "conf/ismir/DuanHP09",
        "keywords": [
            "multi-pitch tracking",
            "estimate pitch trajectory",
            "monophonic source",
            "mixture of harmonic sounds",
            "two stages",
            "multi-pitch estimation",
            "pitch trajectory formation",
            "spectral peaks",
            "non-peak regions",
            "harmonic structure"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nHARMONICALLY INFORMED MULTI-PITCH TRACKING\nZhiyao Duan\nNorthwestern University\nEvanston, IL, USA\nzhiyaoduan00@gmail.comJinyu Han\nNorthwestern University\nEvanston, IL, USA\njinyuhan@gmail.comBryan Pardo\nNorthwestern University\nEvanston, IL USA\npardo@northwestern.edu\nABSTRACT\nThis paper presents a novel system for multi-pitch track-\ning, i.e. estimate the pitch trajectory of each monophonicsource in a mixture of harmonic sounds. The system con-sists of two stages: multi-pitch estimation and pitch tra-jectory formation. In the ﬁrst stage, we propose a newapproach based on modeling spectral peaks and non-peakregions to estimate pitches and polyphony in each singleframe. In the second stage, we view the pitch trajectoryformation problem as a constrained clustering problem ofpitch estimates in all the frames. Constraints are imposed\non some pairs of pitch estimates, according to time andfrequency proximity. In clustering, harmonic structure is\nemployed as the feature. The proposed system is tested on10 recorded four-part J. S. Bach chorales. Both multi-pitchestimation and tracking results are very promising. In ad-dition, for multi-pitch estimation, the proposed system isshown to outperform a state-of-the-art multi-pitch estima-tion approach.\n1. INTRODUCTION\nMulti-pitch analysis is a fundamental research problems inmusic information retrieval (MIR). Pitch analysis resultscan provide helpful information for many other applica-tions, such as automatic music transcription, audio sourceseparation, content-based music search, etc. This task,however, remains challenging and existing methods do notmatch human ability in either accuracy or robustness.\nDue to the complexity of multi-pitch analysis, researchers\ntry to break it into different subproblems. Multi-pitch Es-\ntimation (MPE) [1, 2] usually refers to estimating pitches\nand the number of concurrent pitches (polyphony) in eachsingle time frame. Based on MPE, Note Formation [3, 4]\nforms notes using pitch estimates in adjacent frames.\nThese two subproblems are important, however, they do\nnot constitute the whole multi-pitch analysis problem. Ina piece of polyphonic music consisting of several soundsources (usually different instruments), estimating pitchesand ﬁnding the pitch trajectory for each underlying source,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.is a more advanced problem, which we call Multi-pitch\nTracking (MPT) .\nMulti-pitch tracking is similar to stream organization in\nAuditory Scene Analysis (ASA) [5], which is described as\na grouping process of distinct acoustic events into a sin-gle perceptual entity, i.e. a stream. In [5], Bregman de-scribes two main grouping processes in stream organiza-tion. Simultaneous grouping is to “integrate components\nthat occur at the same time in different parts of the spec-trum”. Sequential grouping is to “put together events that\nfollow one another in time”. It is noted that MPE involvesonly simultaneous grouping, where harmonics of a soundsource are integrated into a single pitch. Based on MPE,note formation also involves sequential grouping, whereproximate pitches are integrated in a small time scale intonotes. MPT also involves sequential grouping, but in amuch larger time scale, i.e. the whole music piece.\nMulti-pitch tracking is closely related to monaural source\nseparation, which in fact estimates more information, i.e.the timbre of each source. A number of source separationsystems [6–8] are built on multi-pitch estimation results.Other methods [9] usually train prior source models fromsolo excerpts. Those methods [10,11] not relying on pitchestimation results nor prior source models utilize the sameamount of information as MPT, but they are only tested onmixtures of two or three sounds.\nMulti-pitch tracking is a difﬁcult problem. Even for\nhumans, only highly trained musicians have the ability totrack concurrent pitch trajectories in listening. Therefore,some researchers proposed to only detect the main melodyline and bass line of a polyphonic music [12,13].\nTo our knowledge, few systems address the multi-pitch\ntracking problem. Kameoka et al. [14] proposed a multi-pitch analyzer based on harmonic temporal structured clus-tering that jointly estimates pitch, intensity, onset and dura-tion of each underlying source. Chang et al. [15] presenteda multi-pitch tracking system that and tracks pitches intonote contours using Hidden Markov Models. Althoughthese methods track concurrent pitch trajectories of mul-tiple sound sources, they are only evaluated in the multi-pitch estimation and note formation level [14, 15]. La-grange and Tzanetakis [16] proposed a sound source sepa-ration method that streams spectral peaks. It was not, how-ever, evaluated on pitch tracking performance [16].\nIn this paper, we propose a system to address the MPT\nproblem. Our system differs from previous systems in sev-eral ways. We start with multi-pitch estimation, where we\n10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n333Oral Session 3: Musical Instrument Recognition and Multipitch Detection\nFigure 1 . System overview.\npropose a new method that models spectral peaks and non-\npeak regions. Given pitch estimates for individual frames,we cast pitch trajectory formation as a constrained clus-tering problem, where each cluster corresponds to a trajec-tory. Harmonic structure is used as the feature of each pitchin clustering. Finally, note formation happens after pitchtrajectories are formed, instead of forming notes and thenplacing them in streams, as previous systems have done.\nFigure 1 shows the system structure. We address the\nMPT problem in two stages. The ﬁrst stage is multi-pitch\nestimation (Section 2), where pitches and polyphony in\neach frame are estimated, and then reﬁned using estimatesin neighboring frames. The second stage is pitch trajec-\ntory formation (Section 3). Initial pitch trajectories are\nformed by grouping pitch estimates across frames accord-ing to pitch height. Within each initial trajectory, pitch es-timates that are close in frequency and contiguous in timeare grouped to form notelets. Final pitch trajectories are\nobtained through constrained clustering of pitch estimates,\nwhere must-link constraints are imposed on pitch pairs ineach notelet and cannot-link constraints are imposed onpitch pairs of concurrent notelets. From the view of Audi-\ntory Scene Analysis, the ﬁrst stage is simultaneous group-ing and the second stage is sequential grouping.\n2. MULTI-PITCH ESTIMATION\n2.1 Multi-pitch Estimation In A Single Frame\nIn multi-pitch estimation, we break the music audio into 46\nms frames with a 10 ms hop size. We estimate polyphonyand pitches in each time frame, but do not group estimatesacross adjacent frames into notes or trajectories. We viewthis problem (given polyphony N) as a Maximum Likeli-\nhood parameter estimation problem in the frequency do-main. The parameters to be estimated are the set of pitchesθ={F\n1\n0,···,FN\n0}. The observation is the spectrum,\nwhich is represented as spectral peaks andnon-peak re-\ngions. Peaks are detected using the peak detection algo-rithm in [10]. The non-peak region is deﬁned as the setof frequencies falling more than a quarter-tone from anyobserved peak.\nFor harmonic sounds, peaks typically appear only near\ninteger multiples of F0s. We try to ﬁnd the set of F0s withharmonics that maximize the probability of the occurrenceof observed peaks, and minimize the probability that theyhave harmonics in non-peak regions.\nThus, the likelihood function can be deﬁned as follows:\nL(θ)=L\npeak(θ)·Lnon-peak region(θ) (1)\nTo modelLpeak(θ), each detected peak kis represented\nby its frequency fkand amplitude ak. We assume condi-\ntional independence between peaks, given a set of F0s. Inaddition, we consider the probability that a peak is normal\n(caused by some harmonic) or spurious (caused by other\nreasons). Then the peak likelihood is deﬁned as\nL\npeak(θ)=K/productdisplay\nk=1p(fk,ak|θ) (2)\n=K/productdisplay\nk=1/summationdisplay\nskp(fk,ak|sk,θ)P(sk|θ)(3)\nwhereKis the number of detected peaks; skis the binary\nvariable to indicate whether the k-th peak is normal (s k=\n0) or spurious ( sk=1).\nIn modeling p(fk,ak|sk=0,θ), we notice that a nor-\nmal peak may be generated by several F0s when their har-\nmonics overlap at the peak position. In this case, how-ever, the probability is conditioned on multiple F0s, which\nleads to a combinatorial problem in training we wish toavoid. Instead, we adopt the binary masking assumption[17], i.e. each peak is generated by only one F0, the onehaving the largest likelihood to generate the peak. Thusp(f\nk,ak|sk=0,θ)is approximated by maxF0∈θp(fk,ak|F0).\nFor each F0∈θ, a harmonic number hkis calculated as\nthe nearest harmonic position of F0fromfk. Then,\np(fk,ak|F0)=p(fk|F0)p(ak|fk,F0) (4)\n=p(dk|F0)p(ak|fk,hk) (5)\nwheredk=fk−F0−12log2hk, is the frequency de-\nviation of the k-th peak from its corresponding harmonic\nposition in the logarithmic frequency domain.\nIn modeling p(fk,ak|sk=1,θ),θcan be ignored, since\nspurious peaks are assumed to be unrelated to F0s:\np(fk,ak|sk=1,θ)=p(fk,ak|sk=1) (6)\nTo modelLnon-peak region(θ), it is noted that instead of\ntelling us where F0s or their ideal harmonics should be,the non-peak regions tell us where they should not be. Agood set of F0s would predict as few harmonics as pos-sible in the non-peak regions. Therefore, we deﬁne thenon-peak region likelihood in terms of the probability of\nOral Session 3: Musical Instrument Recognition and Multipitch Detection\n33410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFrom polyphonic (random chord) data\nP(sk|θ) approximated with P(sk), then learned as the proportion of spurious peaks\np(dk|F0) approximated with p(dk), then learned as a GMM from normal peaks\np(ak|fk,hk) learned non-parametrically from normal peaks\np(fk,ak|sk=1) learned as a 2-D single Gaussian from spurious peaks\nFrom monophonic (individual note) data\nP(eh=1|F0) learned non-parametrically from normal peaks\nTable 1 . Model parameters that are learned from the training data.\nnotdetecting any harmonic in the non-peak regions, given\nan assumed set of F0s. Again, with the conditional inde-\npendence assumption of non-peak regions, the likelihoodcan be deﬁned as:\nL\nnon-peak region(θ)=/productdisplay\nF0∈θ/productdisplay\nhF0∈Fnp\nh∈1..H1−P(eh=1|F0)(7)\nwhereNis the polyphony; ehis a binary variable that in-\ndicates whether the h-th harmonic of F0is detected;Fnp\nis the set of frequencies in the non-peak regions; His the\nlargest harmonic number we consider.\nA harmonic not being detected in the non-peak region\nis because the corresponding peak in the source signal wasweak or not present (e.g. even harmonics of clarinet). There-fore, this probability can be learned from monophonic train-ing data, i.e. the monophonic notes used to generate thepolyphonic training data.\nThe above probabilities are learned in monophonic or\npolyphonic training data, as described in Table 1. Themonophonic training data are monophonic notes selectedfrom the University of Iowa data set\n1. The polyphonic\ntraining data are randomly mixed chords of polyphony from2 to 6, generated by mixing the above mentioned mono-phonic notes. Spectral peaks and non-peak regions are de-tected and collected in all the frames of the training data.Ground-truth F0s are obtained by YIN [18], a single pitchdetection algorithm, on each note prior to mixing. The fre-quency deviation of each peak from the nearest harmonicof any ground-truth F0 is calculated. If the deviation is lessthan a quarter-tone, the peak is labeled normal, otherwisespurious. The “a quarter-tone” threshold is used here wasselected in accordance with the standard tolerance used inmeasuring correctness of F0 estimation.\nSo far, given a set of F0s θ, its likelihoodL(θ)can be\ncalculated in Eq. (1). The search space of the maximum\nlikelihood solution ˆθ, however, is very large. We constrain\nthis problem with a greedy search strategy. We start from\nan empty setˆθ\n0. In each iteration, we add an F0 estimate\ntoˆθnto get a new setˆθn+1that gets maximum likelihood\namong all the possible values of the newly added F0. This\niteration terminates when n=N, the given polyphony.\nIf the polyphony is not given, we need to decide when\nto terminate. Since L(ˆθn)increases with n, we propose a\nsimple threshold-based method. The minimum number ofF0s that achieves a likelihood greater than the threshold\nis\n1http://theremin.music.uiowa.edu/returned as the polyphony estimate:\nN=m i n\n1≤n≤Mn,\ns.t.Δ(n)≥T·Δ(M) (8)\nwhereΔ(n)=L(ˆθn)−L(ˆθ1)is the maximum increase\nof likelihood that could be achieved when the polyphony isset to be n.Mis the maximum allowed polyphony which\nis set to 9 in all experiments; Tis a learned threshold which\ni ss e tt o0 . 8 8(av alue chosen by a machine learner us-\ning the monophonic and polyphonic training data). This\npolyphony estimation method works well on a large dataset containing both music pieces and block musical chords\nwith polyphony from 1 to 6.\n2.2 Reﬁne Pitch Estimates Using Neighboring Frames\nThere are often insertion, deletion and substitution errors\nin the multi-pitch estimation of a single frame. We pro-pose a reﬁnement method using estimates in neighboringframes: For each frame t, we build a weighted histogram\nin the frequency domain, where each bin corresponds toa semitone in the pitch range. Then, a triangular weight-\ning function centered at tis imposed on a neighborhood\noft, whose radius is rframes. The reﬁned polyphony esti-\nmateNis calculated as the weighted average of polyphony\nestimates in all the frames in this neighborhood. The N\nbins with the highest histogram values are selected to re-\nconstruct reﬁned pitch estimates. For each of these bins, ifthere is an original pitch estimate in frame tthat falls inside\nthis bin, the original pitch estimate is used as the reﬁnedpitch estimate. Otherwise, the reﬁned pitch estimate is cal-culated as the weighted average frequency of all the pitchestimates in this neighborhood that fall inside this bin. In\nour system, the radius ris set to 9 frames. This method\nremoved a number of inconsistent estimation errors.\n3. PITCH TRAJECTORY FORMATION\nGiven pitch estimates in all frames, we view pitch trajec-\ntory formation as a constrained clustering problem, where\neach pitch trajectory corresponds to a cluster.\n3.1 Constrained Clustering\nConstrained clustering [19,20] is a class of semi-supervised\nlearning algorithms that make use of domain knowledgeduring clustering. Constraints can be imposed on differentlevels, where the instance level is the simplest and mostcommon one. In instance-level constraints, there are two\n10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n335Oral Session 3: Musical Instrument Recognition and Multipitch Detection\nbasic forms: must-link andcannot-link. A must-link con-\nstraint speciﬁes that two instances should be assigned to\nthe same cluster. A cannot-link constraint speciﬁes thattwo instances should not be assigned to the same cluster.\nFor our pitch trajectory formation problem, we adopt\nthese two constraints. A must-link is imposed between twosimilar pitches in adjacent frames, since they probably be-long to the same trajectory. A cannot-link is imposed be-tween two pitches within a frame, due to our assumptionthat sources are monophonic. We then formulate the clus-tering problem to minimize the intra-class distance J,a s\nthe K-means algorithm does:\nJ=\nK/summationdisplay\nk=1/summationdisplay\nxi∈Tk/bardblxi−ck/bardbl2(9)\nwhereKis the number of pitch trajectories; xiis a feature\nvector in trajectory Tkandckis the mean feature vector in\ntrajectory Tk;/bardbl·/bardbldenotes the Euclidean distance.\nWagstaff et al. [19] proposed a constrained K-means\nclustering algorithm, which iteratively changes the clusterlabels of all points, without violating any constraint. Forour multi-pitch tracking problem, however, this algorithmdoes not work. The reason is that almost every pitch es-\ntimate has must-links or cannot-links to other points, so it\nis almost impossible to change a pitch’s label without vio-lating any constraint. In addition, Davidson and Ravi [20]proved that ﬁnding a feasible solution, i.e. a label assign-ment without violating any constraint, of a clustering prob-lem containing cannot-link constraints is NP-complete.\nWe now propose an iterative greedy algorithm that ﬁnds\na low-cost (in terms of Eq. (9)) assignment of pitches totrajectories within a reasonable time.\n3.2 Initial Pitch Trajectory Formation\nAlthough the general problem is NP-complete, we can re-\nmove some constraints to make it tractable. A trivial ex-ample is to remove all constraints, where random label as-signment can be a solution.\nInstead of random assignment, we utilize the intrinsic\nstructure of polyphonic music to assign initial labels. Note\nthat pitch trajectories (e.g. melody and baseline) are roughly\nordered in pitch, although sometimes they interweave. Sincethere are at most Kpitches in each frame, we sort them\nfrom high to low and assign labels from 1toK.\nThen must-links are imposed on similar pitches that are\nin adjacent frames andhave the same initial trajectory la-\nbel. The maximal must-link difference between pitches inadjacent frames is set to 0.3 semitones (30 cents). Pitchesconnected by must-links form a short trajectory, which wecall a notelet, since it is supposed to be some part of a\nnote. Once notelets are formed, cannot-links are imposed\nbetween all pitches in two notelets that overlap more than30ms. We say that two such notelets are in a cannot-link\nrelation. We allow the 30ms overlap within a melodic line\nas it may be reverberation or ringing of a string. We choseconservative values for these parameters to ensure that theyare reasonable for common real-world scenarios.This results in an initial set of track assignments that\nis reasonably correct. By assigning trajectory labels on aframe-by-frame basis, assuming a ﬁxed small number oftrajectories, and only forming notelets from within sets ofadjacent pitch estimates of the same trajectory, we greatlyreduce the space searched. This provides a reasonable start-ing point and bypasses the NP-completeness problem.\nBefore showing how we improve on this initial solution\nby minimizing Eq. (9), we ﬁrst explain the feature we use.\n3.3 Harmonic Structure\nFeature vectors in Eq. (9) should have the property that\nthey are close in the same trajectory and far in different tra-jectories. Pitch height is not suitable because the underly-ing pitch estimates may show octave errors and the melodiclines overlap in range. Harmonic structure has been provento be a good choice [10,11] for music played by harmonicinstruments, which is deﬁned as the vector of relative am-plitudes of harmonics of a pitch. Harmonic structures ofthe same instrument are similar, even if their pitches andloudness are different. On the other hand, different instru-ments usually have very different harmonic structures [10].\nWe calculate harmonic structure as follows: First, har-\nmonics of each pitch are found from spectral peaks. Foroverlapping harmonics of different pitches, the peak like-lihood in Eq. (4) is used to distribute energy to each pitch.Harmonic structures are then normalized to have the sametotal energy. The ﬁrst ﬁfty harmonics are used here.\n3.4 Final Pitch Trajectory Formation\nWe start from the initial set of pitch trajectories created in\nSection 3.2, where pitches are assigned trajectory labelsbased on pitch height, and then placed into notelets basedon time and pitch proximity. All pitch estimates compris-ing a notelet share the same trajectory label. All noteletsthat share a label form a trajectory . We now consider re-\nassigning notelets to different trajectories to minimize thecost function in Eq.\n(9).\nFigure 2 . Illustration of a swap-set (the rounded rectan-\ngle) for a notelet (the bold solid line). Solid and dashedlines represent notelets in trajectory T\nkandTl, respec-\ntively. Cannot-link relations are indicated by arrows.\nSuppose we want to change the trajectory of a notelet n\nfromTktoTl. We cannot do this in isolation, since there\nmay be a notelet in Tlthat overlaps nand we assume mono-\nphonic pitch trajectories. We could simply swap the tra-jectories for two overlapping notelets. This, however may\nOral Session 3: Musical Instrument Recognition and Multipitch Detection\n33610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nForeach notelet n, with trajectory Tk\nJ0= cost of current trajectory assignments (Eq. (9))\nJbest=J0\nForeach trajectory Tl, such that Tl/negationslash=Tk\nFind the swap-set between TlandTkcontaining n\nJ= (Eq. (9)) if we swap every notelet in the swap-set\nIfJ<J best,Jbest=J,End\nEnd\nIfJbest<J0, Perform swap that produces Jbest,End\nEnd\nTable 2 . The pitch trajectory formation algorithm.\ncause a chain reaction, since the swap may cause new over-\nlaps within a trajectory. Instead, we select two trajectoriesand pick a notelet nfrom one of the trajectories. We then\nﬁnd all notelets in these two trajectories, T\nkandTlthat\nconnect to nvia a path of cannot-link relations (deﬁned in\nSection 3.2). We call this the swap-set, as illustrated in\nFigure 2. These are the notelets affected by a potential tra-jectory swap between two notelets. All notelets in a swapset are swapped together, rather than individually, and thenew trajectory are evaluated with the cost function in Eq.(9). Table 2 describes the process we use to swap trajec-tories for notelets until the trajectories of all notelets reachﬁxed points. In our experiment, this usually takes 3 to 4rounds (where a round is a traversal of all notelets).\n3.5 Note Formation\nAfter pitch trajectories are formed, we form notes in each\ntrajectory from the notelets. Two notelets are consideredto be in the same note if the time gap between them is lessthan 100ms and their frequency difference is less than 0.3semitone. Then the pitches in the gap are reconstructedusing the average frequency of the note. Notes of lengthless than 100ms are considered spurious and removed. The0.3 semitone parameter is the same as the one in imposingmust-links. The 100ms parameter is set without tuning toadapt to the tempo and note lengths of the test music.\n4. EXPERIMENT\nThe proposed system was tested on 10 real music perfor-mances, totaling 330 seconds of audio. Each performancewas of a four-part Bach chorale, performed by a quartetof instruments: violin (Track 1), clarinet (Track 2), tenor\nsaxophone (Track 3) and bassoon (Track 4). Each musi-\ncian’s part was recorded in isolation at 44.1 kHz, while themusician listened to the others through headphones. Audiomixtures were created by summing the four tracks. In test-ing, each piece was broken into 46 ms frames with centertimes spaced every 10 ms.\nThe ground-truth pitch trajectories of each testing piece\nwere estimated using YIN [18] with manual corrections onmonophonic sound tracks prior to mixture. The ground-truth notes (frequency, onset and offset) for each sourcewere obtained by segmenting its pitch trajectory manually.\nWe evaluate the proposed system at the frame-level. For\neach estimated pitch trajectory, a pitch estimate in a\nframe% Initial Final\nNo. Precision Recall Precision Recall\n1 66.9±3.2 66.9± 3.2 82.7±4.9 71.3±5.5\n2 52.3±6.5 52.1± 6.5 64.1±8.8 52.4±7.9\n3 61.0±8.5 59.3± 8.7 78.3±11.5 70.8±12.7\n4 81.8±5.0 65.3± 7.4 82.6±5.4 73.9± 5.8\nTable 3 . Frame-level evaluation results (Mean ±Std) for\neach pitch trajectory. Track No. from 1 to 4 correspondsto the 4 parts of the quartets. “Initial” refers to the initialpitch trajectory formation (Section 3.2); “Final” refers tothe ﬁnal pitch trajectory formation (Section\n3.4)\n% Precision Recall\nKlapuri06 [1] 87.2±2.0 66.2±3.4\nMulti-pitch estimation (MPE) 84.9±1.7 79.9±2.9\nMulti-pitch tracking (MPT) 88.6±1.7 77.0± 3.5\nTable 4. Frame-level evaluation results in the mixture in-stead of each trajectory.\nis called correct if it deviates less than a quarter-tone from\nthe pitch in the ground-truth pitch trajectory. Then preci-\nsion and recall are calculated for each pitch trajectory by\nPrecision=#cor\n#estRecall=#cor\n#ref(10)\nwhere #cor, #est and #ref are the number of correctly esti-mated, estimated and reference pitches, respectively.\nTable 3 presents the average frame-level evaluation re-\nsults on the 10 pieces. The ﬁnal tracking results are com-pared with the initial tracking results obtained from Sec-tion 3.2, which serves as a baseline obtained from multi-pitch estimation. For all four tracks, the proposed pitchtrajectory formation method signiﬁcantly improves eitherprecision or recall from the baseline method. For Track 3,this improvement is up to 17.3% in precision and 11.5%in recall. In addition, in both initial and ﬁnal tracking, re-sults of Track 1 and 4 are better than Track 2 and 3. Thisis in accordance with previous researchers’ observationsthat melody and baseline are easier to transcribe than othersource streams [12].\nTable 4 presents the frame-level evaluation results. We\ncompare our system with Klapuri06 [1], a state-of-the-artmulti-pitch estimation approach. We used Klapuri’s codeand suggested parameter settings. We can see that the bestprecision is obtained by MPT (Section 2+3), while the bestrecall is obtained by MPE (Section 2). Klapuri06 gets ahigh precision, which is indistinguishable from MPT, butits recall is much lower than both MPE and MPT. FromMPE to MPT, precision has a signiﬁcant improvement of3.7%, while the two recalls are indistinguishable, consid-ering the variances. It indicates that pitch trajectory forma-tion improves the multi-pitch estimation results.\nWe also evaluate our system at the note-level in the mix-\nture, as other researchers do [3,4,15]. It is evaluated in twoways. In the ﬁrst way (Onset), a note estimate is correct if\nits frequency (average over all pitch estimates in this note)\ndeviates less than a quarter-tone from a ground-truth note,and its onset time differs less than 50ms/100ms from the\n10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n337Oral Session 3: Musical Instrument Recognition and Multipitch Detection\n% Precision Recall A OR\nOnset(50ms) 49.1±4.8 58.0±4.8 76.7±2.8\nOnset(50ms)+Of f 34.9±5.3 41.2±5.8 87.8±1.8\nOnset(100ms) 65.5±4.7 77.4±3.1 73.7±3.1\nOnset(100ms)+Of f 46.0±5.5 54.3±5.5 85.1± 2.3\nTable 5. Note-level evaluation results in the mixture in-\nstead of each trajectory.\nground-truth onset time. The second way (Onset+Off) has\nthe same requirements for frequency and onset time, butalso requires that each offset time estimate deviate from the\ntrue offset time by less than 20% of the true note duration.\nPrecision and recall are calculated by Eq. (10), where #cor,#est and #ref are the number of correctly estimated, esti-mated, and reference notes, respectively. Average Overlap\nRatio (AOR) between correctly estimated notes and their\nground-truth notes is calculated as\nAOR=Average/parenleftbiggmin{offsets}−max{onsets\n}\nmax{offsets}−min{onsets}/parenrightbigg\n(11)\nGiven a correctly estimate note and its corresponding groundtruth note, “onsets” is the set of onset times for both the es-timated and the and the true note and ”offsets” is similarlydeﬁned. “Average” is over all correctly estimated notes.\nTable 5 shows the note-level evaluation results. In Onset\n(100ms), both precision and recall are promising, and AORvalue is high. This indicates that our system outputs goodnote estimates in both frequency and duration (offset sub-tracts onset). However, either reducing the onset thresh-old from 100ms to 50ms or adding the offset criterion de-creases precision and recall signiﬁcantly. This indicatesthat our system does not estimate the absolute onset/offsettimes precisely. This is not a surprise, since our currentsystem does not have an onset/offset detection module.\n5. CONCLUSION\nWee presented a novel system for multi-pitch tracking. Oursystem ﬁrst estimates pitches and polyphony in each timeframe. Then pitch trajectories are formed by constrainedclustering pitch estimates across frames. Our system achievedpromising results on ten real music pieces.\nCurrently, when clustering the pitches into trajectories,\nonly harmonic structure is used. Future work includes in-corporating musicological information into clustering.\nThis work was supported by National Science Founda-\ntion grant IIS-0643752.\n6. REFERENCES\n[1] A. Klapuri: “Multiple fundamental frequency estima-\ntion by summing harmonic amplitudes,” Proc. ISMIR,\npp. 216-221, 2006.\n[2] M. Davy, S. J. Godsill, and J. Idier: “Bayesian analysis\nof western tonal music,” J. Acoust. Soc. Am. , Vol. 119,\nNo. 4, pp. 2498-2517, 2006.\n[3] M. Ryyn ¨anen and A. Klapuri: “Polyphonic music tran-\nscription using note event modeling,” Proc. WASPAA ,\npp. 319-322, 2005.[4] G. E. Poliner and D. Ellis: “A Discriminative Model for\nPolyphonic Piano Transcription” EURASIP Journal on\nAdvances in Signal Processing, Vol. 2007, 2007.\n[5] A. S. Bregman: Auditory Scene Analysis, Cambridge,\nMA: MIT Press, 1990.\n[6] T. Virtanen and A. Klapuri: “Separation of harmonic\nsounds using multipitch analysis and iterative parame-ter estimation,” Proc. WASPAA , pp. 83-86, 2001.\n[7] M. Bay and J. W. Beauchamp: “Harmonic source sep-\naration using prestored spectra,” Proc. ICA, pp. 561-\n568, 2006.\n[8] J. Woodruff, Y. Li and D. Wang: “Resolving overlap-\nping harmonics for monaural musical sound separationusing pitch and common amplitude modulation,” Proc.\nISMIR, 2008.\n[9] E. Vincent: “Musical source separation using time-\nfrequency source priors,” IEEE Trans. Audio Speech\nLanguage Process. , Vol. 14, No. 1, pp. 91- 98, 2006.\n[10] Z. Duan, Y. Zhang, C. Zhang and Z. Shi: “Unsuper-\nvised single-channel music source separation by aver-age harmonic structure modeling,” IEEE Trans. Audio\nSpeech Language Process. , Vol. 16, No. 4, pp. 766-\n778, 2008.\n[11] M. Kim, S. Choi: “Monaural music source separation:\nNonnegativity, sparseness, and shift-invariance,” Proc.\nICA, pp. 617-624, 2006.\n[12] M. Goto: “A real-time music scene description system:\npredominant-F0 estimation for detecting melody and\nbass lines in real-world audio signal,” Speech Commu-\nnication, Vol. 43, pp. 311-329, 2004.\n[13] J. Eggink and G. J. Brown: “Extracting melody lines\nfrom comlex audio,” Proc. ISMIR , 2004.\n[14] H. Kameoka, T. Nishimoto, and S. Sagayama: “A mul-\ntipitch analyzer based on harmonic temporal structured\nclustering,” IEEE Trans. on Audio, Speech and Lan-\nguage Processing , Vol. 15, No. 3, pp. 982-994, 2007.\n[15] W.-C. Chang, A. W.Y. Su, C. Yeh, A. Roebel and X.\nRodet: “Multiple-F0 tracking based on a high-orderHMM model,” Proc. DAFx , 2008.\n[16] M. Lagrange and G. Tzanetakis: “Sound source\ntracking and formation using normalized cuts,” Proc.\nICASSP,\n2007.\n[17] ¨O. Yılmaz and S. Rickard: “Blind separation of speech\nmixtures via time-frequency masking,” IEEE Trans.\nSignal Process. , Vol. 52, No. 7, pp. 1830-1847, 2004.\n[18] A. de Cheveign ´e and H. Kawahara: “YIN, a funda-\nmental frequency estimator for speech and music,” J.\nAcoust. Soc. Am. , Vol. 111, pp. 1917-1930, 2002.\n[19] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl:\n“Constrained K-means clustering with background\nknowledge,” Proc. ICML , 2001.\n[20] I. Davidson and S. S. Ravi: “Clustering with con-\nstraints: feasibility issues and the k-means algorithm,”Proc. SDM , 2005.\nOral Session 3: Musical Instrument Recognition and Multipitch Detection\n338"
    },
    {
        "title": "Prediction of Multidimensional Emotional Ratings in Music from Audio Using Multivariate Regression Models.",
        "author": [
            "Tuomas Eerola",
            "Olivier Lartillot",
            "Petri Toiviainen"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416730",
        "url": "https://doi.org/10.5281/zenodo.1416730",
        "ee": "https://zenodo.org/records/1416730/files/EerolaLT09.pdf",
        "abstract": "Content-based prediction of musical emotions and moods has a large number of exciting applications in Music Information Retrieval. However, what should be predicted, and precisely how, remain a challenge in the field. We provide an empirical comparison of two common paradigms of emotion representation in music, opposing a multidimensional space to a set of basic emotions. New groundtruth data consisting of film soundtracks was used to assess the compatibility of these models. The findings suggest that the two are highly compatible and a quantitative mapping between the two is provided. Next we propose a model predicting perceived emotions based on a set of features extracted from the audio. The feature selection and transformation is given special emphasis and three separate data reduction techniques are compared (stepwise regression, principal component analysis, and partial least squares regression). Best linear models consisting of 25 predictors from the data reduction process were able to account for between 58 and 85% of the variance. In general, partial least squares models performed the best and the data transformation has a significant role in building linear models.",
        "zenodo_id": 1416730,
        "dblp_key": "conf/ismir/EerolaLT09",
        "keywords": [
            "Music Information Retrieval",
            "Emotion representation",
            "Film soundtracks",
            "Quantitative mapping",
            "Feature selection",
            "Data reduction techniques",
            "Partial least squares regression",
            "Linear models",
            "Data transformation",
            "Emotion prediction"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nPREDICTION OF MULTIDIMENSIONAL EMOTIONAL RATINGS IN\nMUSIC FROM AUDIO USING MULTIVARIATE REGRESSION MODELS\nTuomas Eerola, Olivier Lartillot, Petri Toiviainen\nFinnish Centre of Excellence in Interdisciplinary Music Research,\nUniversity of Jyv ¨askyl ¨a, Finland\nfirstname.lastname@jyu.fi\nABSTRACT\nContent-based prediction of musical emotions and moods\nhas a large number of exciting applications in Music In-\nformation Retrieval. However, what should be predicted,\nand precisely how, remain a challenge in the ﬁeld. We pro-\nvide an empirical comparison of two common paradigms\nof emotion representation in music, opposing a multidi-\nmensional space to a set of basic emotions. New ground-\ntruth data consisting of ﬁlm soundtracks was used to as-\nsess the compatibility of these models. The ﬁndings sug-\ngest that the two are highly compatible and a quantitative\nmapping between the two is provided. Next we propose a\nmodel predicting perceived emotions based on a set of fea-\ntures extracted from the audio. The feature selection and\ntransformation is given special emphasis and three sepa-\nrate data reduction techniques are compared (stepwise re-\ngression, principal component analysis, and partial least\nsquares regression). Best linear models consisting of 2-\n5 predictors from the data reduction process were able to\naccount for between 58 and 85% of the variance. In gen-\neral, partial least squares models performed the best and\nthe data transformation has a signiﬁcant role in building\nlinear models.\n1. INTRODUCTION\nEmotional impact of music is one of the most important\nreasons for listening to music. A reliable content-based\nprediction of emotions in music would be a highly useful\napplication of MIR, as suggested by the promising proto-\ntypes recently been put forward. It seems however that an\nimprovement of the study would require a precise clariﬁ-\ncation of the concept under study, which is difﬁcult due to\nthe inherent fuzziness of the topic. Previous research de-\nﬁned mood as “sound and feel” of music (AllMusicGuide),\nof “feeling inspired by the music pieces” (Last.fm) [1].\nSuch broad opening of the study to a large realm of seman-\ntic expression, although interesting by itself, makes how-\never the problem particularly difﬁcult to tackle. Dealing\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.with the concept of emotion instead, which is rooted on a\nlarge background of scientiﬁc research, would enable on\nthe contrary a better controlled study of research.\nHowever, robust and generalizable prediction of emo-\ntions has been difﬁcult for several reasons, namely due to\ntheir conceptual elusiveness, their highly contextual de-\npendencies on situation, context and musical style, and\nthe limitations of the computational approaches utilised to\ndate, which emphasize mainly on low-level acoustic fea-\ntures. The conceptual elusiveness of emotions is apparent\nin both the multitude of theoretical approaches taken, as\nwell as the high individual variability in the subjective self-\nreports of emotional experiences. During the past decade,\nbasic emotion model, dimensional models, and domain-\nspeciﬁc emotion models have all received support in stud-\nies of music and emotion [2]. However, it still remains\nto be clariﬁed whether models and theories designed for\neveryday emotions – such as the basic emotion model –\ncan also be applied in an aesthetic context such as music.\nIt has been argued, for example, that a few primary basic\nemotions seem inadequate to describe the richness of the\nemotional effects of music [3].\nCurrent computational efforts of modelling polyphonic\ntimbre seem to have reached what Aucouturier has called a\n‘glass-ceiling’ effect, probably due to their strict reliance\non low-level audio features. This ceiling appears to be\naround 50-60% of the variance explained [4]. Out of these\nthree shortcomings, we aim to provide advances in two of\nthem, namely by carrying simultaneous conceptual com-\nparison of basic emotions and the circumplex model, and\nby performing the selection of relevant audio and musical\nfeatures by means of multivariate methods.\n2. BACKGROUND\n2.1 Mood, emotion, and affect terms\nMood ontologies structure emotional adjectives and labels\ninto a set of various mood clusters. Following purely the-\noretical studies [5, 6], more systematic approaches attempt\nto automatically infer the set of clusters based on analysis\nof large set of mood labels that are further reduced with the\nhelp of statistical tools: agglomerative hierarchical cluster-\ning of 179 AMG mood labels [1], consensus among a set of\ncandidate labels used in literature [7, 8] collected through\npsychological experiments [9, 10], etc.\n621Poster Session 4\nRepresentation of emotion in a dimensional affective\nspace has gained support among researchers in music and\nemotion [2]. Instead of claiming that independent neural\nsystem exists for every basic emotion, the two-dimensional\ncircumplex model [7] proposes that all affective states arise\nfrom two independent neurophysiological systems: one re-\nlated to valence (a pleasure-displeasure continuum) and\nthe other to activity (activation-deactivation). In contrast,\nThayer [11] suggested that the two underlying dimensions\nof affect were two separate arousal dimensions: energetic\narousal andtense arousal. However, the two-dimensional\nmodels have been criticized for their lack of differentiation\nwhen it comes to emotions that are close neighbours in the\nvalence-activation space, such as anger and fear. It has also\nbeen discovered, that the two-dimensional model is not\nable to account for all the variance in music-mediated emo-\ntions [12] and three-dimensional variant containing valence,\nenergy arousal and tension arousal has given better empir-\nical results [13].\n2.2 Ground truth collection\nExtensive work has been carried out for the collection of\nground truth related to mood ontology [10, 14]. Concern-\ning the dimensional paradigm, Kim et al [15] have col-\nlected dynamic ratings expressed on the valence-activity\nspace from thousands of songs drawn randomly from the\nuspop2002 database via a customized online game.\n2.3 Mood and emotion prediction\nPrevious computational works attempt to predict mood clus-\nters [16, 17] and emotion categories [18, 19]. Lu, Liu, and\nZhang [20] studied mood detection and tracking using a\nvariety of acoustic features related to intensity, timbre, and\nrhythm. Their classiﬁer used Gaussian Mixture Models\n(GMMs) for Thayer’s four principal mood quadrants in the\nvalence-activity representation. The system was trained\nusing a set of 800 classical music clips, each 20 seconds\nin duration, hand labeled to one of the 4 quadrants. Their\nsystem achieved an accuracy of 85% when trained on 75%\nof the clips and tested on the remaining 25%.\nWe believe that linear models are more useful than clas-\nsiﬁcations for understanding emotion in music. Indeed,\nmusic is often emotionally ambiguous and listeners are not\nparticularly certain of the emotion categories if given com-\nplex examples. Valence and activity mapping has been\npreviously done [21, 22], but selecting the optimal set of\nfeatures is more challenging, due to statistical constraints\nimposed by linear models.\n3. NEW GROUND-TRUTH SET: SOUNDTRACKS\nIn the present work, both discrete and dimensional mod-\nels of emotions are simultaneously investigated in order to\nclarify their mutual relationship and applicability to mu-\nsic and emotions. The three-dimensional model is used to\ncollect data regarding the dimensional approach as it en-\ncompasses both lower dimensional models. In order to\n1\n2\n3\n4\n5\n62\n3\n4\n5\n6\n71234567 \nValence\nActivity TensionAnger\nFear\nHappy\nSad\nTenderFigure 1. Average ratings of the three dimensions and ba-\nsic emotions for the 360 soundtrack excerpts.\nobtain a large sample of unknown yet emotionally stim-\nulating musical examples, a selection of ﬁlm soundtracks\nwas used. Soundtracks are composed for the purpose of\nconveying powerful emotional cues, and may serve as a\nrelatively ‘neutral’ musical material in terms of music pref-\nerences and familiarity. A three-part selection process was\nutilized. First, 12 experts chose 360 excerpts representing\nHappy, Sad, Tender, Scary and Angry emotions as well as\ndifferent quadrants in the 3D affect space.\n3.1 Evaluation\nThe expert panel (music students with extensive musical\nbackground) rated the examples, using both basic emotion\nconcepts and dimensional ratings, on Likert scales (cf. Fig-\nure 1). Then a sampling of the 360 excerpts using both\nconceptual frameworks was carried out.\n•For the basic emotion examples, the excerpts were\ncategorized and ranked according to the basic emo-\ntion concept that received highest rating. From these\nranked lists, the top ﬁve examples and ﬁve mod-\nerately high examples were chosen for each basic\nemotion (happiness, sadness, tenderness, anger and\nfear), yielding 50 basic emotion examples ([5 top +\n5 moderate] ×5 categories).\n•For the dimensional model, each dimension was sam-\npled at 4 percentiles along its axis whilst the other\ntwo dimensions were kept constant, resulting in 60\naudio examples that cover the affect space.\nThis set of 110 examples will be called Soundtrack110 set\nhereafter. The mean duration of the excerpts was 15.3 sec-\nonds (SD 1.9 s).\nIn the next phase, 116 university students aged 18-42\nyears rated the Soundtrack110 set using both 3D set and\n62210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n3D 2D\nR2(β) R2(β)\nHappiness .89 (V .93,A.79,T-.35) .89 (V .85,A.49)\nSadness .63 (V -.20,A-.84,T-.22).63 (V -.05,A-.69)\nTenderness .77 (V .33,A-.45,T-.58).74 (V .50,A-.51)\nFear .87 (V -.83,A.07,T.63) .87 (V -.90,A.24)\nAnger .64 (V -.52,A.32,T.35) .68 (V -.55,A.35\nMean .76 .76\nTable 1. Ridge regression summary of dimensional mod-\nels explaining basic emotion model categories. For in-\nstance, 89% of the variance (R2) of Happiness can be ex-\nplained with Valence (V) and Activity (A), with respective\nlinear coefﬁcient (β ) .85 and .49.\nbasic emotions (on Likert scales). For the ensuing anal-\nyses, the means of the ratings across the participants were\nused as high consensus existed (Cronbach α>.99 for each\nconcept).\n3.2 Basic emotions vs. dimensional ratings\nAs could be seen from the Figure 1, at least two emotion\ndimensions correlated heavily. In numerical terms, tension\nand valence correlate highly (r =−.83) and activity and\ntension in moderate way (r =.57), while valence and ac-\ntivity do not exhibit such a relation (r =−.08). The high\ncorrelation has implications in the task of constructing re-\ngression models for predicting categorical ratings based on\nthe dimensional rating data, because multicollinear vari-\nables are problematic for standard versions of the regres-\nsion. Hence we employed ridge regression since this tech-\nnique is less inﬂuenced by collinearity due to the inclusion\nof constant variance parameter. This enables to attenuate\nthe inﬂuence of collinearity in the calculation of the least\nsquares optimization in regression. Ridge regression was\nused to predict the dimensional ratings from the categori-\ncal ratings and vice versa. The results – displayed in Ta-\nbles 1 and 2 – demonstrate that the basic emotion model\ncan more accurately explain the results obtained with the\nthree-dimensional model than contrariwise. Nevertheless,\nthe difference is not large (17%, the difference between the\nmean R2from the Tables 1 and 2) and this high degree of\noverlap between the conceptual frameworks suggests that\nthe conceptual frameworks are highly compatible.\nTo further examine the validity of the three-dimensional\nmodel, its underlying coefﬁcients of determination were\nalso compared with the 2-dimensional circumplex model\n[7]. The results suggest that these two-dimensional mod-\nels can explain the results obtained with the basic emo-\ntion model virtually as accurately as the three-dimensional\nmodel, with the exception of anger and tenderness (minor\ndifferences in R2values, see Table 1). It is worth point-\ning out that sadness was explained equally modestly (R2\n= .63), in comparison to other emotion categories, by all\nthe dimensional models. This may reﬂect the participants’\ndifﬁculty to rate the valence of sad music, for sadness in\nmusic is rarely perceived to represent an unpleasant emo-Basic emotion model\nR2(β)\nValence .97 (H .35, S-.11, T.20, F-.50, A-.14)\nActivity .88 (H .47, S-.32, T-.42, F-.05, A.36)\nTension .93 (H -.29, S-.23, T-.55, F.18, A.12)\nMean .93\nTable 2. Ridge regression summary of dimensional mod-\nels explained by basic emotion model categories: Happi-\nness (H), Sadness (S), Tension (T), Fear (F) and Anger (A).\naudioframe\npredictorsextraction summary(mean, std)\nemotion ratings(activity, valence, tension)statisticalmappingtransformations\nFigure 2. General design of the methodology.\ntion. Despite this irregularity, these analyses suggest fairly\nhigh mutual correspondence between the two conceptual\nframeworks and stimulus sets.\n4. AUDIO AND MUSIC FEATURE EXTRACTION\nAND TRANSFORMATION\nThe methodology proposed in this study is summarised in\nFigure 2: The Soundtrack110 collection has been analysed\nusing MIRtoolbox [23], and a set of features has been se-\nlected, explained below. We assume that a theoretical se-\nlection of features combined with a suitable data reduction\ntechniques will result to the most parsimonious model. In\naddition, the features may require transformation to linear-\nity before statistical mapping, described in the ﬁnal sec-\ntion.\n4.1 Theoretical selection of features\nFirst, a theoretical selection is made based on the tradi-\ntional categories of musical elements (rhythm, timbre, pitch,\nform, etc.) and by representing these categories by a few,\nnon-redundant (non-correlating) features, in total 29. A\nsynthetic description of the complete feature extraction pro-\ncess is given in Figure 3.\n4.1.1 Timbre\nBased on a spectrogram with a frame length of .046 s and\nhalf overlapping, three timbral descriptions are computed:\ncentroid, spread and entropy, the latter predicting the pres-\nence of strong peaks. The mean correlation between fea-\ntures, computed using the sountrack110 set, is r=.10.\n4.1.2 Harmony\nThe peaks conﬁguration in the spectrogram enables to es-\ntimate a measure of roughness [24]. The entropy of each\nspectrum, collapsed into one single octave, indicates the\npresence of important chroma components. Or more pre-\ncisely, the spectrum is turned into a chromagram, wrapped\ninto one octave, and tonal information is computed – such\n623Poster Session 4\nspread \nroughness tonalanalysis\nkey clarity\nmajorness\nenvelope extractionframe\ntempo\ncentroid\nentropy \nnovelty\nﬂuctuationautocorrelation\npulse clarityaudioframespectrum collapsing\nentropy \nlow energy rate\nrms\nevent density\nattack estimation\nattack slope\nattack timechromagram\narticulationrhythmregistertimbre\ncentroid\ndeviation\nautocorrelationpitch estimation\nsalient pitch harmonystructure\nharmonic change dynamicsFigure 3. Flowchart of predictor extraction.\nas key clarity or harmonic change [25] – based on tonal\nproﬁle [26, 27]. We also designed a new measure of ma-\njorness, related to the difference of amplitude, observed on\nthe tonal proﬁle, between the best major score and the best\nminor score. For this dimension we obtain a within-feature\ncorrelation of r=.04\n4.1.3 Register\nBroad description of the localisation of pitch energy is per-\nformed through an estimation of the centroid and deviation\nof the unwrapped chromagram, and also in parallel a statis-\ntic description of pitch component based on advanced pitch\nextraction method [28]. r=.27\n4.1.4 Rhythm\nRhythmic periodicity is estimated both from a spectral anal-\nysis of each band of the spectrogram, leading to a ﬂuctu-\nation pattern [29], and based on the assessment of auto-\ncorrelation in the amplitude envelope extracted from the\naudio. The clarity of the pulsation can also be assessed\nthrough an observation of the global characteristic of the\nautocorrelation function [30]. r=.03\n4.1.5 Articulation\nOnsets indicated by peaks picked from the amplitude enve-\nlope leads to the estimation of the relative amount of event\ndensity. For each successive onset, the slope and tempo-\nral duration of the corresponding attack phase is also esti-\nmated.r=−.23\n4.1.6 Structure\nThe multidimensional structure of the pieces of music is\nestimated through the computation of novelty curves [31]\nbased on various functions already computed such as thespectrogram, the autocorrelation function and the chroma-\ngram.r=.85\nAs a whole, the features represent the categories in a\nnon-redundant way, as within-feature correlation is lower\nthan .30, except for structural features.\n4.2 Statistical selection of features\nThe second selection is based on statistical selection of rel-\nevant features, in which we compare Multiple Linear Re-\ngression (MLR) with a stepwise selection principle, Prin-\ncipal Component Analysis (PCA) followed by a selection\nof an optimal number of components, and Partial Least\nSquares Regression (PLS). Linear mapping via regression\nis known to be problematic as the predictors-to-cases ratio\nshould be 1:10 or larger (we have 29 features, we would\nneed at least 290 observations or more). Moreover, high\nnumber of predictors will probably be highly collinear, which\nis problematic for the establishment of a linear modeling of\nthe data. Principal component analysis will eliminate the\nproblem of collinearity, as the components are orthogonal\nand enables to use a low number of predictors (PCA com-\nponents) in the regression. However, this data reduction\nmethod is not sensitive to the covariance between the fea-\ntures and the predicted data and thus may discard important\nfeatures. The third technique, PLS regression [32], car-\nries out simultaneous data reduction and maximization of\ncovariance between features and predicted data, thus pre-\nserving any interesting correlational pattern between them.\nThe output from the PLS is similar to PCA, individual, or-\nthogonal components. To select the optimal number of fea-\ntures, Bayesian Information Criterion (BIC) was used.\n62410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nPrediction rate (R2)\nModel Valence Activity Tension\nMLR .64 .75 .67\nPCA .42 .74 .51\nPLS .70 .77 .71\nMLR λ.66 .74 .69\nPCA λ .51 .73 .63\nPLSλ .72 .85 .79\nTable 3. Prediction rates of the different models for cir-\ncumplex model of emotions. λdenotes Box-Cox trans-\nformed variables.\nPrediction rate (R2)\nModel Angry Scary Happy Sad Tender\nMLR .46 .55 .46 .38 .38\nPCA .66 .67 .60 .59 .54\nPLS .66 .62 .61 .61 .50\nMLR λ.56 .55 .63 .54 .45\nPCA λ .56 .47 .53 .52 .45\nPLSλ .70 .74 .68 .69 .58\nTable 4. Prediction rates for the 5 basic emotions.\n4.3 Data Transformation\nTo apply linear least-squares models, the distribution of\nthe data should be approximately normal. Each feature\nwas tested for normality (Lilliefors p<.001) and each non-\nnormally distributed feature was transformed by means of\nBox-Cox power transform [33] by testing λvalues between\n-2 and 2 in .1 increments and taking the one that yielded the\nmaximal normality. Finally, all features were normalized.\n5. RESULTS AND DISCUSSION\nTable 3 displays the prediction rate of linear regression\nmodels using ﬁrst 5 components in stepwise linear regres-\nsion (MLR), and ﬁrst 5 PCA components, and 2 ﬁrst com-\nponents from PLS, with or without data transformations\n(λ). 5-fold cross-validation (80% for training, 20% for\nprediction) was used in all cases to avoid overﬁtting. In\ngeneral, about 70 % of the variance in participants rat-\nings could be predicted with features extracted from the\naudio. Data transformation has an important contribution\nto the models. MLR provides fairly successful model but\nit is problematic due to the serious over optimization step-\nwise regression does when using 29 predictors to explain\n110 observations. PCA with 5 components has less power\nto predict the ratings but is nevertheless fairly adequate\nmodel. It suffers especially from the skewness and lack\nof normalization of the data. Finally, PLS (normalized)\nprovides the highest prediction rate with only two com-\nponents. The model adequacy is largely similar for basic\nemotions, displayed in Table 4.\nThe resulting predictive models vary depending on the\nchosen mapping method. Table 5 shows for instance the\nimportant features contributing to the perception of the cat-Anger Tenderness\nFeature β Feature β\nFluctuation peaks -.14 RMS variance -.44\nKey clarity -.07 Key clarity .08\nRoughness .05 Majorness -.08\nSp. centroid variance -.04 Sp. centroid -.05\nTonal novelty .004 Tonal novelty -.01\nTable 5. Components and standardized beta weights of the\nMLR λmodel for two chosen basic emotions.\negories of anger and tenderness, as predicted by the MLR\nmethod. The predictive models given by the PCA and PLS\nmethods are less easy to represent clearly, are their under-\nlying dimensions are formed by a high number of audio\nand musical features.\nWhen mapping the dimensional ratings onto each of\nthe ﬁve basic emotions, the regression models could ex-\nplain 63 to 89 percent of the variance. No signiﬁcant im-\nprovement was observed with the 3D model over the 2D\nmodel, with the exception of anger, for which adding the\nthird dimension increased the variance explained by ﬁve\nper cent. When mapping basic emotions onto the emotion\ndimensions, even higher proportions of variance could be\nexplained by the models, these ranged from 88 to 97 per\ncent. These results suggest that there is a high mutual cor-\nrespondence between the two emotion spaces.\nUsing a ﬁve-fold cross-validation, about 70% of the vari-\nance in the participants ratings could be explained by the\nPLS models. The highest proportion of variance explained\n(85%) was obtained when predicting activity with the PLS\nmodel using transformed features. We examined the effect\nof the Box-Cox transform on the predictive power of the\nregression models. In most cases this transform improved\nthe models signiﬁcantly. This observation suggests that the\ndistributions of the extracted features are a crucial factor in\nthe performance of such predictive models.\nThe emotion prediction model has been written in Mat-\nlaband has been integrated into the new version (1.3) of\nMIRtoolbox [23].\n6. REFERENCES\n[1] X. Hu and J. S. Downie. Exploring mood metadata:\nrelationships with genre, artist and usage metadata. In\nProceedings of the International Symposium on Music\nInformation Retrieval, 2007.\n[2] P. N. Juslin and J. A. Sloboda. Music and Emotion:\nTheory and Research. OUP, Oxford, UK, 2001.\n[3] M. Zentner, D. Grandjean, and K. Scherer. Emotions\nevoked by the sound of music: Characterization, clas-\nsiﬁcation, and measurement. Emotion, 8(4):494–521,\n2008.\n[4] J.-J. Aucouturier and F. Pachet. Improving timbre sim-\nilarity: How high is the sky? Journal of Negative Re-\nsults in Speech and Audio Sciences, 1(1), 2004.\n625Poster Session 4\n[5] K. Hevner. Experimental studies of the elements of ex-\npression in music. American Journal of Psychology,\n(48):246–268, 1936.\n[6] P. R. Farnsworth. The social psychology of music. The\nDryden Press, 1958.\n[7] J. A. Russell. A circumplex model of affect. Journal of\nPersonality and Social Psychology, 39(6):1161–1178,\n1980.\n[8] M. Leman, V . Vermeulen, L. De V oogdt, D. Moelants,\nand M. Lesaffre. Prediction of musical affect using\na combination of acoustic structural cues. Journal of\nNew Music Research, 34(1):39–67, 2005.\n[9] J. Skowronek, M. McKinney, and S. van de Par.\nGround-truth for automatic music mood classiﬁcation.\nInProceedings of the International Symposium on Mu-\nsic Information Retrieval, pages 295–296, 2006.\n[10] J. Skowronek, M. McKinney, and S. van de Par. A\ndemostrator for automatic music mood estimation. In\nProceedings of the International Symposium on Music\nInformation Retrieval, 2007.\n[11] R. E. Thayer. The Biopsychology of Mood and Arousal.\nOxford University Press, New York, USA, 1989.\n[12] E. Bigand, S. Vieillard, F. Madurell, J. Marozeau, and\nA. Dacquet. Multidimensional scaling of emotional re-\nsponses to music: The effect of musical expertise and\nof the duration of the excerpts. Cognition & Emotion,\n19(8), 2005.\n[13] U. Schimmack and R. Reisenzein. Experiencing ac-\ntivation: Energetic arousal and tense arousal are not\nmixtures of valence and activation. Emotion, 2(4):412–\n417, 2002.\n[14] X. Hu, M. Bay, and J. S. Downie. Creating a simpli-\nﬁed music mood classiﬁcation ground-truth set. In Pro-\nceedings of the International Symposium on Music In-\nformation Retrieval, pages 309–310, 2007.\n[15] Y . E. Kim, E. Schmidt, and L. Emelle. Moodswing: A\ncollaborative game for music mood label collection. In\nProceedings of the International Symposium on Music\nInformation Retrieval, pages 231–236, 2008.\n[16] T. Li and O. M. Ogihara. Detecting emotion in music.\nInProceedings of the International Symposium on Mu-\nsic Information Retrieval, 2003.\n[17] X. Hu, J. S. Downie, C. Laurier, M. Bay, and A. F.\nEhmann. The 2007 mirex audio mood classiﬁcation\ntask: Lessons learned. In Proceedings of the Inter-\nnational Symposium on Music Information Retrieval,\npages 462–467, 2008.\n[18] D. Yang and W. Lee. Disambiguating music emotion\nusing software agents. In Proceedings of the Inter-\nnational Symposium on Music Information Retrieval,\n2004.[19] Y . Feng, Y . Zhuang, and Y . Pan. Popular music retrieval\nby detecting mood. In Proceedings of the 26th annual\ninternational ACM SIGIR conference, Toronto, 2003.\n[20] L. Lu, D. Liu, and H.-J. Zhang. Automatic mood\ndetection and tracking of music audio signals. IEEE\nTrans.on Audio, Speech, and Language Processing,\n14(1):5–18, 2006.\n[21] K.F. MacDorman. Automatic emotion prediction of\nsong excerpts: Index construction, algorithm design,\nand empirical comparison. Journal of New Music Re-\nsearch, 36(4):281–299, 2007.\n[22] Y .H. Yang, Y .C. Lin, Y .F. Su, and H.H. Chen. Music\nemotion classiﬁcation: A regression approach. In Proc.\nIEEE Int. Conf. Multimedia and Expo , pages 208–211,\n2007.\n[23] O. Lartillot and P. Toiviainen. MIR in matlab (II): A\ntoolbox for musical feature extraction from audio. In\nProceedings of 5th International Conference on Music\nInformation Retrieval, 2007.\n[24] W. A. Sethares. , Timbre, Spectrum, Scale. Springer-\nVerlag, 1998.\n[25] M. B. Sandler C. A. Harte. Detecting harmonic change\nin musical audio. In Proceedings of Audio and Music\nComputing for Multimedia Workshop, 2006.\n[26] C. L. Krumhansl. Cognitive foundations of musical\npitch. OUP, Oxford, UK, 1990.\n[27] E. Gomez. Tonal description of music audio signal.\nPhD thesis, Universitat Pompeu Fabra, Barcelona,\n2006.\n[28] T. Tolonen and M. Karjalainen. A computationally efﬁ-\ncient multipitch analysis model. IEEE Transactions on\nSpeech and Audio Processing, 8-6:708–716, 2000.\n[29] E. Pampalk, A. Rauber, and D. Merkl. Content-based\norganization and visualization of music archives. In\nProceedings of the 10th ACM International Conference\non Multimedia, pages 570–579.\n[30] O. Lartillot, T. Eerola, P. Toiviainen, and J. Fornari.\nMulti-feature modeling of pulse clarity: Design, vali-\ndation, and optimization. In Proceedings of the Inter-\nnational Symposium on Music Information Retrieval,\n2008.\n[31] M. Cooper J. Foote. Media segmentation using self-\nsimilarity decomposition. In Proceedings of SPIE Stor-\nage and Retrieval for Multimedia Databases, volume\n5021, pages 167–175, 2003.\n[32] S Wold, M. Sjostrom, and L. Eriksson. Pls-regression:\na basic tool of chemometrics. Chemometrics and Intel-\nligent Laboratory Systems, 58:109–130, 2001.\n[33] G. E. P. Box and D. R. Cox. An analysis of transforma-\ntions. Journal of the Royal Statistical Society, Series B\n26:211–246, 1964.\n626"
    },
    {
        "title": "Auditory Spectral Summarisation for Audio Signals with Musical Applications.",
        "author": [
            "Sam Ferguson",
            "Densil Cabrera"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417393",
        "url": "https://doi.org/10.5281/zenodo.1417393",
        "ee": "https://zenodo.org/records/1417393/files/FergusonC09.pdf",
        "abstract": "Methods for spectral analysis of audio signals and their graphical display are widespread. However, assessing music and audio in the visual domain involves a number of challenges in the translation between auditory images into mental or symbolically represented concepts. This paper presents a spectral analysis method that exists entirely in the auditory domain, and results in an auditory presentation of a spectrum. It aims to strip a segment of audio signal of its temporal content, resulting in a quasi-stationary signal that possesses a similar spectrum to the original signal. The method is extended and applied for the purpose of music summarisation.",
        "zenodo_id": 1417393,
        "dblp_key": "conf/ismir/FergusonC09",
        "keywords": [
            "Spectral analysis",
            "Audio signals",
            "Graphical display",
            "Translation challenges",
            "Visual domain",
            "Auditory images",
            "Mental concepts",
            "Music summarisation",
            "Auditory presentation",
            "Quasi-stationary signal"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAUDITORY SPECTRAL SUMMARISATION FOR AUDIO SIGNALS WITH\nMUSICAL APPLICATIONS\nSam Ferguson\nFaculty of Design, Architecture and Building\nUniversity of Technology, Sydney\nsamuel.ferguson@uts.edu.auDensil Cabrera\nFaculty of Architecture, Design and Planning\nThe University of Sydney\nd.cabrera@arch.usyd.edu.au\nABSTRACT\nMethods for spectral analysis of audio signals and their\ngraphical display are widespread. However, assessing mu-\nsic and audio in the visual domain involves a number of\nchallenges in the translation between auditory images into\nmental or symbolically represented concepts. This paper\npresents a spectral analysis method that exists entirely in\nthe auditory domain, and results in an auditory presenta-\ntion of a spectrum. It aims to strip a segment of audio sig-\nnal of its temporal content, resulting in a quasi-stationary\nsignal that possesses a similar spectrum to the original sig-\nnal. The method is extended and applied for the purpose\nof music summarisation.\n1. INTRODUCTION\nGraphical display is the predominant approach to convey-\ning musical sound analysis information to people, includ-\ning via spectrograms, spectra, waveform graphics and mu-\nsical manuscript. While the visual system is dominant in\nmany information transfer contexts, soniﬁcation (the repre-\nsentation of information through non-speech sound) offers\nmany (often complementary) possibilities for information\ntransfer [1]. As audio and music are data that are experi-\nenced primarily in the auditory domain, soniﬁcation would\nappear to be an appropriate method for analysis and repre-\nsentation of audio data, as it sidesteps the translation pro-\ncess from the auditory domain to the visual domain that is\ninherent in using visual representations.\nA variety of simple techniques for soniﬁcation of sound\nin the context of audio education have been proposed by\nCabrera and Ferguson [2, 3], and Ferguson has developed\ntechniques for techniques for statistical soniﬁcations of au-\ndio in his Ph.D thesis [4]. These soniﬁcation methods pro-\nvide auditory analogues to common statistical visual dis-\nplays (such as cumulative distribution functions and box\nplots), but with much richer information than visual charts.\nOne of the solutions proposed in the thesis is a method of\ndisplaying spectral data, which is the focus of this paper.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.Spectral analysis is one of the most fundamental, pow-\nerful, and widely used methods for the investigation of\naudio. This paper discusses an approach to spectral dis-\nplay that does not use Fourier analysis, and exists com-\npletely within the auditory domain. Instead of a Fourier or\nrelated transform implemented through signal processing,\nthe method uses the spectral analysis of the human audi-\ntory system. While almost all listening could be thought\nof as involving auditory spectral processing, in listening\nthat is focused on spectral features, temporal features are\ndistractions that should be removed. Such features include\nrhythm, prosody, language, and more generally, the time\nstructure of the sound being analysed. Put simply, the tech-\nnique blurs temporally ﬂuctuating audio signals to create\nquasi-stationary signals with almost identical spectra en-\nvelopes to the original signals, but without any semblance\nto the original time-dependent ﬂuctuation. This technique\nis rooted in the theories of Gabor [5, 6] and granular syn-\nthesis [7], and has been strongly inﬂuenced by the recent\nadvances in concatenative synthesis by Schwarz [8, 9].\nInformation visualisation literature has focused on meth-\nods for presenting data in ways that present large overviews\nof data, but allow a user to ‘zoom and ﬁlter’ the representa-\ntion to ﬁnd information that is important [10]. Fry’s Com-\nputational Information Design outlines a method for devel-\noping interactive information representation systems [11].\nA soniﬁcation method that would improve on visual meth-\nods may; use the original audio as the sound material for\nthe analytic representation; ﬁlter the content of that audio\nin some way; maintain context and meaning of the audio;\nand draw relationships and present pertinent contrasts.\nSchenkerian analysis of musical works is well-known\nand features in many undergraduate music curricula [12].\nThis graphical analysis method based on musical manu-\nscripts allowed Schenker to reveal the various layers of a\ncomposition. The spectral soniﬁcation method described\nin this paper has the potential to be used in a similar man-\nner allowing a scaling of perspective from large to small\nscale structures depending on the periods analysed.\n2. SPECTRAL SUMMARISATION ALGORITHM\nThe core technique this paper presents is a method for\nspectrally summarising a larger audio signal. A represen-\ntation that can convey the spectrum using audio without\nfrequency domain signal processing can be built using a\n567Oral Session 7: Harmonic & Melodic Similarity and Summarization\nsimple digital algorithm based on fragmenting the time sig-\nnal into potentially overlapping windows, and recombining\nthem in a way that (i) maintains a roughly constant power\nspectral envelope that matches the signal’s long term spec-\ntrum; and (ii) removes distracting (non-spectral) features.\nThis can be achieved by concatenating short windows (or\ngrains) of audio by averaging a large proportion, but not\nall, of the original signal windows. A number of unique\nbut spectrally similar windows need to be concatenated to-\ngether, since if a single audio grain is repeated the resulting\nsound will be dominated by amplitude modulation related\nto the repetition rate. A systematic explanation of a pro-\ncess to create and concatenate unique but spectrally similar\ngrains is as follows:\n1. For a user-selected window length wnsamples, ran-\ndomly select a window length wnrfrom the range of\nvalues between wn−wn\n2andwn+wn\n2.\n2. Randomly select windows of length wnrfrom the\nsignal to be averaged to create a set {w1, w2, ...w m}\nofmunique windows.\n3. Sum this set of windows, and divide by the square\nroot of the number of windows ({w1+w2,...+wm}√m) to\nproduce a single frame of wnrsamples duration.\n4. Repeat steps 1, 2 and 3 (re-randomising each time)\nuntil enough unique but spectrally similar audio frames\nare produced to build a stationary sound of a chosen\nduration.\n5. Concatenate these audio frames, using overlapping\nand adding with a custom window function. Ramps\ntaken from either side of a Hanning window func-\ntion are applied only to the overlapping proportion\n(typically only 5-10% of wn) to maintain a constant\nsum between concatenation boundaries (see Figure\n3).\nThis method is simple, but it is successful at creating a\nquasi-stationary sound with a spectral proﬁle that matches\nthe original ﬁle, while keeping the time variance to a min-\nimum.\n2.1 Validation, Tradeoffs and Limitations\nTo validate the appropriateness of the averaging process we\nundertook a comparison of spectra created by this spectral\naveraging method against the spectrum of the unmodiﬁed\nsample. A distinction worth mentioning is that through\nmixing we are amplitude (pressure) averaging, rather than\npower (pressure squared) averaging. Summing a large num-\nber of randomly selected signals as described above may\nbe considered to be an operation on incoherent signals,\nwhich is why the square root of the number of windows\nis used in the denominator of the algorithms third step.\nHence, the power spectrum of the soniﬁcation approxi-\nmates the power spectrum of the original wave. While\nthere is some potential for a substantial discrepancy be-\ntween the power of the resulting spectrum and a true long\nterm power spectrum, tests have shown that discrepancies\nare not severe for realistic signals if the averaging method\nuses a window size larger than 1024 samples.\nThere is a signiﬁcant smearing of energy when using\n0 2000 4000 6000 8000 10000−120−100−80−60−40−200\nFrequency (Hz)Level (dB re FS)\n4096 s512 s256 s128 s\n2048 s1024 s\nOriginal \nSignalFigure 1 . A 2kHz sine wave is spectrally averaged using\na variety of window sizes, and compared with the original\nsine wave signal. The larger window sizes result in less\nspectral smearing.\n16 31.5 63 125 250 500 1000 2000 4000 8000 1600050556065\nFrequency (Hz)Level (dB)\n  \nPinkNoise\n4096 s\n1024 s\n256 s\nFigure 2 . A pink noise recording is compared against\nspectral representations of pink noise using various win-\ndow lengths - longer window length result in less spectral\ndeviation.\nshorter window lengths. We demonstrated this by com-\nparing a spectrally averaged sine tone at 2kHz, using a\nrange of window lengths, to the original signal. Figure\n1 demonstrates this effect. Generally, window lengths of\n1024 samples or greater decrease smearing and increase\nspectral representation quality signiﬁcantly. Subjectively,\nshort window lengths tend towards extremely noise-like\nsignals bearing little resemblance to the tonal spectra ex-\npected.\nThe window length used in the spectral summarisation\nalgorithm has a small effect on the low frequency range\nof the spectrum reproduced. To investigate this we com-\npared a spectrum of a sample of pink noise (with a 48000\nHz sample rate) against three spectral summarisations, one\nusing a 4096 point window, one with a 1024 point win-\ndow, and one with a 256 point window. The length of the\nwindow determines the frequency below which the spectral\nrepresentation begins to attenuate – at 4096 points there is\nlittle effect, but at 256 points it starts to become more sig-\nniﬁcant and further reductions in window length result in\nthe low cutoff frequency increasing proportionally. The\ncutoff frequency ( fc) is apparently based on the largest\nwave period that can be represented by a speciﬁc window\nlength ( wn), summarised by the relationship:\n56810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n0 500 1000 1500 2000 250000.51SampleWindow Overlap\n0 500 1000 1500 2000 250000.51SampleAddition of WindowsFigure 3 . A custom window is designed to maintain a con-\nstant power sum between concatenated windows of audio.\nfc=fs\nwn(1)\nThis investigation tends to support the use of window\nsizes of at least 1024 samples and upwards for this spec-\ntral averaging technique. Larger window sizes will tend to\nallow more temporal ﬂuctuation, depending on the tempo-\nral ﬂuctuation present in the original signal, so there is a\ntradeoff present, however window sizes smaller than 1024\nsamples seem likely to signiﬁcantly alter the spectrum to a\ndegree where it is unrecognisable and non-representative.\nThese parameters are likely to be experienced interactively,\nand therefore there is probably a subjective element to a\nuser’s selection for the most appropriate window size.\nThe windowing and overlapping at the concatenation\nstage must not introduce either discontinuities (clicks) or\namplitude modulation, and thus we have designed a cus-\ntom window shape that incorporates a large plateau (simi-\nlar to a Tukey window) as well as a Hanning window func-\ntion’s ramps at either end. The proportion of the window\ndevoted to the ramp is determined by the number of over-\nlapping samples, and the resulting window shape main-\ntains a constant sum at the window boundaries (see Figure\n3). Furthermore, the randomisation around a central win-\ndow length, ameliorates amplitude modulation effects that\nmay arise out of periodic selection window length.\n3. HARMONY ANALYSIS APPLICATIONS\nHarmony analysis typically requires a familiarity with read-\ning musical manuscripts, a difﬁcult skill that is analogous\nto learning a new language. This, of course, places an\nimmediate barrier to those users without these skills, but\nit also presumes a level of expertise in cross-modal per-\nception in those users who possess skills in this language.\nA user who is presented with a harmony analysis on a\nmanuscript is expected to ascertain the auditory meaning\nof the symbols and their consequences within the musi-\ncal structure. This is not necessarily straightforward, and\nmany users will ‘interact’ with the manuscript by using a\npiano to play back the pitches and compare their signif-\nicance, while other users develop skills in producing au-ditory images of the various pitches. Methods that bridge\nthe gap between symbolic representations of pitch relation-\nships and auditory pitches are possible alternative solutions\nfor these issues. Generally, the idea of this exploration is\nto make the patterns within music clearer than they are in\na typical musical recording, so that users may understand\nharmonic patterns at multiple structural levels, and in intu-\nitive manners.\nThe problem of producing a soniﬁcation of the harmony\nwithin the audio recording is therefore one of ﬁltering the\naudio recording to contain less information, with an em-\nphasis on that information which would be included in a\nharmonic representation. Such information may include\nmusical elements like the fundamental frequency of the\nbass notes, and other notes presented either loudly or for\ncomparatively long periods, while avoiding short decora-\ntive notes, or quick scalar passages. It would seek to re-\nmove, generally speaking, the temporal presentation of the\nnotes, as well as their amplitude envelopes, resulting in a\nstationary sound with each important pitch presented si-\nmultaneously to build a chordal sound, accentuating the\nharmonic contribution each note makes, and diminishing\neach note’s individual quality.\nA structural representation would also need to describe\nhow each section of the music relates to each other. The\nform of the piece is a crucial element in musicology, but\nit can be difﬁcult to understand music at the formal scale\nfrom reading a musical manuscript, or from listening to\nan audio recording. Snyder [13] describes three levels of\nmusical memory: the early processing level – which deals\nwith characteristics of single notes, the short-term memory\nlevel – which holds musical phrases and rhythmic pattern,\nand the long-term memory level – which deals with for-\nmal sections, movements or entire pieces. Snyder also de-\nscribes how long-term or formal memory deals with sec-\ntions of music that are too long to be understood in the\npresent, and their order needs to be consciously recon-\nstructed as they do not automatically retain their time-order.\nSimpliﬁcations or shorter versions of the musical sam-\nple can be used to describe the form of the piece in an\namount of time that can be held within short-term memory.\nBy presenting an auditory representation that is shorter than\nthe original audio recording, but is proportional to the orig-\ninal, form can be more appropriately presented. Elements\nlike key changes and voice ledaing become more obvious,\nas the ear can compare the short term memory of the old\nkey to the new.\n3.1 Voice Leading and Chord Patterns\nThe algorithm for building an averaged chord progression\nis as follows:\n1. Get time information to use for time value bound-\naries – this may be based on extracted symbolic mu-\nsical information, rhythmic information, timeseries\ndescriptor peaks or various other time markup meth-\nods.\n2. Find the ﬁrst time boundary and the second time\nboundary to be averaged across and ﬁnd the audio\n569Oral Session 7: Harmonic & Melodic Similarity and Summarization\nTime (seconds)  Frequency (Hz)\n0 2505001000150020002500\n  \n010002000\n25Figure 4 . Comparing the soniﬁcation (top) and the origi-\nnal audio (bottom) we can see that the soniﬁcation attempts\nto blur the spectral components from each bar into a sta-\ntionary sound. This sound is changed at every bar-line,\napproximately once every 3 seconds. The graphic only\ndemonstrates the ﬁrst 25 seconds of the piece.\ndata in between the two times.\n3. Apply the averaging method to this time interval to\ncreate the same duration of averaged audio, or per-\nhaps a duration altered by a constant factor.\n4. Place the resulting audio data at the corresponding\nsample numbers of the output audio, using appropri-\nate overlapping.\n5. Repeat the process after stepping forward to the sec-\nond and third boundary, and continuing to step for-\nward until the entire recording has been averaged\nand the output soniﬁcation built.\nA simple method for ﬁnding time boundaries with which\nto segment the chordal structure is to extract the beats and\nassume that chord changes will be synchronised with beats,\nor more likely with bars. Depending on the meter of the\npiece (3/4, 4/4 or 6/8 commonly) we use particular beats\nas time boundaries, and in the following examples we have\nmanually set the meter based on listening to the music, but\nadvanced beat tracking algorithms may correctly estimate\nit as well. We also need to set an anacrusis value, that\ndescribes whether the piece begins on the ﬁrst beat of the\nbar. Beat tracking is well-researched, and we use Dixon’s\nBeatroot algorithm [14].\n3.2 Harmonic Pattern Examples\nWe will attempt to use this algorithm to represent the long-\nterm structure of some pieces of music.\nOne piece that is deﬁned primarily by its chordal con-\ntent (as opposed to its melodic or rhythmic) is Bach’s Pre-\nlude No 1. from ‘the Well-tempered Clavier’. A Schenke-\nrian analysis has also been published for this piece [12]. By\napplying the algorithm to the audio we produce a soniﬁca-\ntion that is presented in Figure 4. The soniﬁcation created\nis not a completely stationary sound, like a set of tones, nor\nis it a sound that has discernible starting or ending notes.\nIt demonstrates characteristics of the timbre of the instru-\nment, but primarily it presents the notes that have sounded.\nThe quality of the sound is similar to the sound that would\n  \n01000200030004000\nTime (seconds)Frequency (Hz)\n  \n200100020003000400020Figure 5 . A second chord soniﬁcation example that\ndemonstrates the structure of a Beatles song, Norwegian\nWood . The upper graph is a spectrogram of the original au-\ndio, and the lower is the spectrally averaged soniﬁcation.\nThe graphic shows only the ﬁrst 20 seconds.\nbe produced if the pianist stopped at the end of the bar and\nheld down all the keys played in the bar.\nThe averaging across the bar is particularly appropriate\nfor this particular piece, due to the manner in which Bach\npresents a single chord per bar. For other situations this\nmay result in chords blurring into other chords, resulting\nin strong dissonance. Despite this, there are a large ma-\njority of pieces where this simple scheme would be sufﬁ-\ncient. The remainder may be dealt with using more sophis-\nticated methods, that employ harmonic and rhythm based\npre-processing to carefully avoid averaging across chord\nchanges incorrectly.\nOne purpose of the blurring of the audio is to be able\nto place one bar’s harmonic content temporally adjacent to\nthe next’s. This should allow each harmonic change to be\nunderstood in terms of the notes within each chord, and to\nwhich notes they each move. An example of a pattern that\nmight be uncovered through this process is the bassline in\nthis prelude. While these notes are strongly sounded at the\nbeginning of the bar, they decay by the end of the bar, and\nother higher notes are dominant by this stage. The blurring\napplied places each of these sounds adjacent to each other,\nyielding a legato bassline.\nThe other useful process possible by using the blurring\nof the audio is that the speed of the example can be arbitrar-\nily altered. The blurred audio has no temporal content, so a\nbar’s worth of sound may be presented over 3 seconds or in\nhalf a second. By setting an arbitrary compression factor\nfor the duration, we can proportionally change the duration\nof the piece while maintaining the formal structure. This\ncan be used to alter a 3 minute piece, whose structure can\nbe ‘remembered’, into a 20 second piece, whose structure\ncan be ‘heard’.\nIn the structural soniﬁcation of Norwegian Wood (Fig-\nure 5), we hear a clear structure of descending melodic\nnotes that deﬁne the chord structure. The structure is a lot\nsimpler than that of the Prelude, and each formal section\ncan be clearly heard.\n57010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n0 20 40 60 80 100 1202468x 10−5Chord Correlation01CmajorPrelude\n0 20 40 60 80 100 12070758085\nTime (seconds)SPL A −weighted fast (dB)Figure 6 . There is a 15dB SPL range over the time period\nof the musical example. High levels can be heard to cor-\nrespond to the chords which have the least correlation and\nare the most dissonant.\n3.3 Chordal Patterns and Context\nWhile we wish to maintain the temporal order of the var-\nious chords, due to their importance to the overall direc-\ntion and purpose of a piece of music, it may be interest-\ning to annotate the soniﬁcation in terms of the values of\nother parameters. A simple parameter to investigate is the\nsound pressure level (SPL). From listening to the soniﬁca-\ntion we can hear that often tonic chords are quieter com-\npared with the chords that lead into them. A comparison\nof SPL against chord estimates shows the decrease in SPL\nthat accompanies every return to the tonic (see Figure 6).\nWe may wish to accentuate this further. By normalising the\nlevel of audio from of each section, and then mapping the\nSPL extracted from the audio to an expanded gain func-\ntion that is then re-applied to each section, we can experi-\nence the structural implications of the performer’s use of\ndynamics more clearly.\nWhile this is a straightforward example, the use of sound\npressure level as the mapping target is arbitrary, and many\nother such targets exist. Another candidate parameter to\nbase a gain function soniﬁcation on is the harmonic dis-\ntance the chord is from the tonic. One can attempt to ap-\nproach this from a chord recognition perspective, but in\nthis case we will use the chroma pattern only and will com-\npare it against the ﬁrst (tonic) chord.\nThe virtual pitch algorithm of Terhardt takes a template\nmatching approach to ﬁnding pitches in the sound [15]. It\napplies a peak picking algorithm to ﬁnd the points in the\nspectrum that are peaks. These are then applied to a suc-\ncessive template matching algorithm that attempts to place\nthe peaks under a pitch template. The pattern of pitches\nacross the audible range can be constrained to create a\nchroma pattern, representing the strength of each of the\n12 pitches within an octave, regardless of pitch-height.\nUsing this method we calculate an average chroma pat-\ntern for each bar, and then multiply and sum those chroma\npattern vectors to create a number representing the corre-\nlation between each bar and the ﬁrst chord. A high number\nrepresents a high number of similar notes, or low chord\ndistance, while a low number represents a large amount\nof difference. These values are clearly a useful target for\nmapping to a gain function. With larger chord distances\nassociated to greater SPL, and smaller chord distances as-\n0 20 40 60 80 100 120123456789x 10−5\nTime (seconds)Chord CorrelationChord Distance − 01CmajorPrelude.Figure 7 . The correlation between various chords seems\nto follow a predictable pattern. The gain function soniﬁ-\ncation makes that pattern more apparent by exaggerating\nit, and highlighting low correlation values. Low values of\ncorrelation are analogous to high values of chord distance.\nsociated to low SPL, the effect should be similar to typical\nmusical approaches.\nAn alternative approach to expanding parameters such\nas sound pressure level is to use a parameter as the basis of\ntemporallycutting and reorganising the harmonic units or\nbars. Any measurable parameter that can be derived from a\nsteady-state spectrum could be used for this purpose. The\ntime periods (in this case bars) that are used to average\nfrom in the algorithm described in section 3.1 are then as-\nsociated with a median parameter value taken for their time\nperiod. The median parameter value determines the re-\nordering, and then the audio units are rearranged based on\nthe new order. If the chord correlation values mentioned\nabove are used to order the bars in ascending order, then\nthe soniﬁcation progresses from chords that are generally\ndissimilar to the tonic, to chords that are more consonant\n– giving an overall impression of a long cadence. During\nthis process particular chords can be assessed within the\noverall scheme of chords.\n3.4 Beyond Chords\nAnalysis of sections of music larger than individual chords\nor bars is easily implemented using this method. The long\nterm spectra of entire pieces can be soniﬁed into a short\nsound, so that, for example, the average spectra of each of\nBachs Preludes and Fugues (one in each major and minor\nkey) can be quickly compared by ear. The overall spectrum\nof the entire collection of preludes could then, for example,\nbe soniﬁed so as to be compared by ear to those of other\nsimilar collections of preludes or etudes played on piano\n(such as those of Chopin or Listz).\n4. CONCLUSIONS\nWe have presented an algorithm for spectral summarisa-\ntion, and have applied it to the problem of music summari-\nsation. This method is based on concatenative and gran-\nular synthesis and aims to strip musical audio of its ﬁne\ntemporal content while maintaining the spectral shape and\nenergy. We have described methods for applying this ba-\nsic spectral summarisation technique to the analysis of har-\nmony and voice leading, and for using it to compare chords\nagainst the tonic chord of a musical piece.\n571Oral Session 7: Harmonic & Melodic Similarity and Summarization\n4.1 Applications, Limitations and Future Work\nThese techniques are useful for assessing musical sam-\nples in a semi-automated manner - in a way that hope-\nfully falls somewhere between listening to the entire au-\ndio ﬁle, and an abstract information retrieval algorithm. In\nthis way it may be possible to apply this method in the de-\nsign or checking of music information retrieval algorithms.\nThis representation method may also be useful in educa-\ntion contexts, for the assessment of spectra, and to intro-\nduce ideas of structure and tonality. Its application to au-\nditory browsing, for instance of digital archives of musical\nrecordings, is also worth consideration.\nShort signals highlight a limitation of this method – a\ncertain amount of audio data is required to reliably build\nan average window from. For short signals the use of\nlarge windows is also difﬁcult, leading to the tradeoff be-\ntween spectral smearing and window length described in\n2.1. This method is likely to be experienced in an inter-\nactive context, due to its reliance on computer technology.\nInvestigation into good ways to provide interactive user ac-\ncess to this algorithm is likely to greatly improve its use-\nfulness. Lastly, the suppression of the audio’s temporal\ninformation throws away a lot of temporal qualities that\nare fundamental in musical practice. Modiﬁcations of this\nmethod that seek to systematically explore aspects of mu-\nsic apart from only the spectral and harmonic qualities are\nworth careful consideration.\nIt is easy to forget how powerful auditory analysis can\nbe when visual and textual presentation of data are over-\nwhelmingly common. Soniﬁcation of audio is more than\na tautology, and extends beyond the trivial case of merely\nplaying the original audio recording. This paper examines\none simple technique for the soniﬁcation of sound record-\nings which focuses on spectral features. One of the attrac-\ntive features of this technique is that it does not employ any\nspectral analysis using digital signal processing instead the\nspectrum analysis is achieved in the ear, and the purpose\nof the technique is to prepare the audio so as to provide a\nsound that focuses attention on spectral features. In other\nwork we have examined other spectrum soniﬁcation tech-\nniques that do use Fourier transforms, such as exaggerat-\ning spectral features through auto-convolution (raising the\nspectrum to an integer power) [2].\nApplications of this technique extend beyond conven-\ntional harmony-based music, and beyond music. Broadly\nspeaking, it is applicable to audio recordings that have med-\nium or long term spectral features of interest (including\nharmony, timbre) that might be difﬁcult to clearly discern\nwithout the removal of temporal structure and/or the com-\npression of duration.\n4.2 Acknowledgements\nThis research was supported by an Australian Postgraduate\nAward and a Departmental Supplementary Scholarship.5. REFERENCES\n[1] G. Kramer, B. N. Walker, Terri Bonebright, P. R. Cook,\nJ. H. Flowers, Nadine Miner, and John G. Neuhoff.\nSoniﬁcation report: Status of the ﬁeld and research\nagenda. Technical report, NSF, 1997.\n[2] D. Cabrera, & S. Ferguson. Auditory Display of Audio.\nIn120th Audio Engineering Society Convention , Paris,\nFrance, 2006.\n[3] D. Cabrera & S. Ferguson. Soniﬁcation of sound: Tools\nfor teaching acoustics and audio, In 13th International\nConference on Auditory Display , Montreal, Canada,\n2007.\n[4] S. Ferguson. Exploratory Sound Analysis: Statistical\nSoniﬁcations for the Investigation of Sound . Ph.D The-\nsis, University of Sydney, 2009.\n[5] D. Gabor. Theory of communication. J. Inst. Elec.\nEng., 93:429–457, 1946.\n[6] D. Gabor. Acoustical quanta and the theory of hearing.\nNature , 159:591–594, 1947.\n[7] C. Roads. Microsound . MIT Press, Cambridge, 2001.\n[8] D. Schwarz. Data-driven Concatenative Sound Synthe-\nsis. Ph.D Thesis, University of Paris 6 Pierre et Marie\nCurie, 2004.\n[9] D. Schwarz, R. Cahen, and S. Britton. Principles\nand applications of interactive corpus-based concate-\nnative synthesis. In Journ ´ees d’Informatique Musicale\n(JIM’08) , Albi, 2008.\n[10] B. Shneiderman. The eyes have it: a task by data\ntype taxonomy for information visualizations. In IEEE\nSymposium on Visual Languages , Boulder, CO, USA,\n1996.\n[11] B. Fry. Computational Information Design . Ph.D The-\nsis, MIT, Cambridge, MA, 2004.\n[12] H. Schenker and F. Salzer. Five graphic music analyses\n(Funf Urlinie-Tafeln) . Dover Publications, New York,\n1969.\n[13] B. Snyder. Music and Memory: An Introduction . MIT\nPress, Cambridge, MA, 2001.\n[14] S. Dixon. Automatic extraction of tempo and beat from\nexpressive performances. J. New Music Res. , 30(1),\n2001.\n[15] E. Terhardt, G. Stoll, and M. Seewan. Algorithm for\nextraction of pitch and pitch salience from complex\ntonal signals. J. Acoust. Soc. Am. , 71(3):679–688,\n1982.\n572"
    },
    {
        "title": "Accelerating Query-by-Humming on GPU.",
        "author": [
            "Pascal Ferraro",
            "Pierre Hanna",
            "Laurent Imbert",
            "Thomas Izard"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415798",
        "url": "https://doi.org/10.5281/zenodo.1415798",
        "ee": "https://zenodo.org/records/1415798/files/FerraroHII09.pdf",
        "abstract": "Searching for similarities in large musical databases has become a common procedure. Local alignment methods, based on dynamic programming, explore all the possible matchings between two musical pieces; and as a result return the optimal local alignment. Unfortunately these very powerful methods have a very high computational cost. The exponential growth of musical databases makes exact alignment algorithm unrealistic for searching similarities. Alternatives have been proposed in bioinformatics either by using heuristics or by developing faster implementation of exact algorithm. The main motivation of this work is to exploit the huge computational power of commonly available graphic cards to develop high performance solutions for Query-by-Humming applications. In this paper, we present a fast implementation of a local alignment method, which allows to retrieve a hummed query in a database of MIDI files, with good accuracy, in a time up to 160 times faster than other comparable systems.",
        "zenodo_id": 1415798,
        "dblp_key": "conf/ismir/FerraroHII09",
        "keywords": [
            "exponential growth",
            "local alignment methods",
            "dynamic programming",
            "computational cost",
            "exact alignment algorithm",
            "bioinformatics",
            "heuristics",
            "fast implementation",
            "high performance solutions",
            "Query-by-Humming applications"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nACCELERATING QUERY-BY-HUMMING ON GPU\nPascal Ferraro\nLaBRI - U. Bordeaux 1, France\nPIMS/CNRS - U. Calgary, Canada\nferraro@cpsc.ucalgary.caPierre Hanna\nLaBRI - U. Bordeaux 1,\nFrance\npierre.hanna@labri.fr\nLaurent Imbert\nLirmm - CNRS, France\nPIMS/CNRS - U. Calgary, Canada\nlaurent.imbert@lirmm.frThomas Izard\nLirmm - U. Montpellier 2,\nFrance\nthomas.izard@lirmm.fr\nABSTRACT\nSearching for similarities in large musical databases has\nbecome a common procedure. Local alignment methods,\nbased on dynamic programming, explore all the possible\nmatchings between two musical pieces; and as a result re-\nturn the optimal local alignment. Unfortunately these very\npowerful methods have a very high computational cost.\nThe exponential growth of musical databases makes exact\nalignment algorithm unrealistic for searching similarities.\nAlternatives have been proposed in bioinformatics either\nby using heuristics or by developing faster implementation\nof exact algorithm. The main motivation of this work is to\nexploit the huge computational power of commonly avail-\nable graphic cards to develop high performance solutions\nfor Query-by-Humming applications. In this paper, we\npresent a fast implementation of a local alignment method,\nwhich allows to retrieve a hummed query in a database of\nMIDI ﬁles, with good accuracy, in a time up to 160 times\nfaster than other comparable systems.\n1. INTRODUCTION\nOne of the main goal of music retrieval systems is to ﬁnd\nmusical pieces in large databases given a description or\nan example. These systems compute a numeric score on\nhow well a query matches each piece of the database and\nrank the music pieces according to this score. Computing\nsuch a degree of resemblance between two pieces of mu-\nsic is a difﬁcult problem. Three families of methodologies\nhave been proposed [1]. Approaches based on index terms\ngenerally consider N-grams techniques [2,3], which count\nthe number of common distinct terms between the query\nand a potential answer. Geometric algorithms [4–6] con-\nsider geometric representations of music and compute dis-\ntances between objects. Techniques based on string match-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.ing [7] are generally more accurate as they can take into\naccount errors in the query or in the pieces of music of\nthe database. This property is of major importance in the\ncontext of music retrieval systems since audio analysis al-\nways induces approximations. Moreover, some music re-\ntrieval application require speciﬁc robustness. Query by\nhumming (QbH), a music retrieval system where the in-\nput query is a user-hummed melody, is a very good ex-\nample. Since the sung query can be transposed, played\nfaster or slower, without degrading the melody, retrieval\nsystems have to be both transposition and tempo invariant.\nEdit distance algorithms, mainly developed in the context\nof DNA sequence recognition, have been adapted in the\ncontext of music similarity [8]. These algorithms, based\non the dynamic programming principle, are generalisations\nof a local sequence alignment method proposed by Smith\nand Waterman [9] in the early 80’s. Applications relying\non local alignment are numerous and include cover detec-\ntion [10], melody retrieval [8], Query-by-Humming [11],\nQuery-by-Tapping [12], structural analysis, comparison of\nchord progressions [13], etc. Local alignment approaches\nusually provide very accurate results as shown at the recent\neditions of the Music Information Retrieval Evaluation eX-\nchange (MIREX) [14].\nAlignment algorithms are powerful and optimal: they\nalways ﬁnd the best alignment. However, they are also\nvery time consuming. This drawback considerably limits\ntheir use for musical applications. For biological applica-\ntions, heuristics such as BLAST and FASTA can be used\nto speed-up local sequence alignment while allowing for\nmultiple regions of local similarity. These heuristics are\nvaluable, but they may fail to report hits or may report false\npositives. In order to get more accurate results faster im-\nplementations of exact alignment algorithms are therefore\nof primary importance.\nGraphics Processing Units (GPUs) have recently received\nlots of attention thanks to their extensive computing re-\nsources. Not only are the latest generations of GPUs very\npowerful graphic engines, they can also be used for Gen-\neral Purpose computation (GPGPU) [15]. With the re-\ncent evolutions of GPUs’ architecture into a uniﬁed, highly\n279Poster Session 2\nparallel programmable processor, and the development of\nprogramming tools and high-level programming languages\nsuch as NVIDIA’s CUDA, GPUs have become a very at-\ntractive, low-cost alternative to the traditional micropro-\ncessors for computationally demanding applications that\ncan be expressed as data-parallel computations, i.e. the\nsame program is executed on many data elements in paral-\nlel. This type of parallelism is well suited to the problem\nof QbH on very large scale music databases, although it\nalso brings new challenges regarding memory operations\nand computational resource allocations. In this paper, we\npresent an implementation of a variant of Smith-Waterman\nbased on local transpositions which illustrates the advan-\ntages of recent graphic cards as computation platforms.\nIn Section 2 we present the general concepts of sequence\nalignment and a variant based on local transpositions well\nsuited to musical applications. Our parallel implementa-\ntion on GPU is detailed in Section 3. Several tests and\ncomparisons are presented in Section 4.\n2. ALIGNING TWO MUSIC\nIn this section, we brieﬂy present the QbH system experi-\nmented. Following Mongeau and Sankoff [8], any mono-\nphonic piece can be represented by a sequence of notes,\neach given as a pair (pitch, length) . Several alphabets and\nsets of numbers have been proposed to represent pitches\nand durations [3]. In the following, we are using the in-\nterval relative representation, i.e.the number of semitones\nbetween two successive notes reduced modulo 12. In the\ncontext of QbH applications, this representation presents\nthe huge advantage to be transposition invariant.\n2.1 General Sequence Alignment\nSequence alignment algorithms are widely used to com-\npare strings. They evaluate the similarity between two\nstringstandqgiven on an alphabet A, and of respective\nsizes|t|and|q|. Formally an alignment between tandq\nis a stringzon the alphabet of pairs of letters, more pre-\ncisely on (A∪{/epsilon1})×(A∪{/epsilon1}), whose projection on the\nﬁrst component is tand the projection on the second com-\nponent isq. The letter /epsilon1does not belong to the alphabet\nA. It is often substituted by the symbol “-” and is called a\ngap. An aligned pair of zof type (a,b)witha,b∈Ade-\nnotes the substitution of the letter aby the letter b. A pair\nof type (a,-)denotes a deletion of the letter a. Finally, an\naligned pair of type (-,b)denotes the insertion of the letter\nb. A scoreσ(ti,qj)is assigned to each pair (ti,qj)of the\nalignment. The score Sof an alignment is then deﬁned as\nthe sum of the costs of its aligned pairs. Computational\napproaches to sequence alignment generally fall into two\ncategories: global alignments andlocal alignments . Cal-\nculating a global alignment is a form of global optimiza-\ntion that forces the alignment to span the entire length of\nall query sequences. By contrast, local alignments identify\nregions of similarity within long sequences that are often\nwidely divergent overall. In Query-by-Humming applica-\ntions, since the query is generally much shorter than thereference, one favours local alignment methods.\nBoth alignment techniques are based on dynamic pro-\ngramming [9,16,17]. Given two strings tandq, alignment\nalgorithms compute a (|t|+ 1)×(|q|+ 1) matrixTsuch\nthat:\nT[i,j] =S(t[0...i],q[0...j]),\nwhereS(t[0...i],q[0...j])is the optimal score between\nthe subsequences of tandqending respectively in position\n0≤i≤|t|and0≤j≤|q|. Dynamic programming al-\ngorithms can compute the optimal alignment (either global\nor local) and the corresponding score in time O(|t|×|q|)\nand memoryO(min{|t|,|q|})(see [9] for details).\n2.2 Local Transposition\nQueries produced by human beings can, not only be totally\ntransposed, but can also be composed of several parts that\nare independently transposed. For example, if the original\nmusical piece is composed of different harmonic voices,\nthe user may sing different successive parts with differ-\nent keys. In the same way, pieces of popular music are\nsometimes composed of different choruses sung based on\ndifferent tonic. A sung query may imitate these character-\nistics. Moreover, errors in singing or humming may occur,\nespecially for users that are not trained to perfectly control\ntheir voice like professional singers. From a musical point\nof view, sudden tonal changes are disturbing. However, if\nthese changes last during a long period, they may not dis-\nturb listeners. Figure 1 shows an example of query having\ntwo local transpositions.\nFigure 1 . Example of a monophonic query not transposed\n(top) and a monophonic query with two local transposi-\ntions (bottom).\nThe two pieces in Figure 1 sound very similar, although\nthe two resulting sequences are very different. This prob-\nlem has been addressed in [18] by deﬁning a local trans-\nposition algorithm. It requires to compute multiple score\nmatrices simultaneously, one for each possible transposi-\ntion value. The time complexity is O(∆×|q|×|t|), where\n∆is the number of local transposition allowed during the\ncomparison (for practical applications, ∆is set up to 12).\nOur experiments, presented in section 4, show that the lo-\ncal transposition algorithm provides a much better result.\n2.3 Pitch/Duration Scoring Scheme\nThe quality of an alignment-based algorithm heavily de-\npends on the scoring function. Results may differ signiﬁ-\ncantly whether one uses a basic scoring scheme or a more\nsophisticated scoring function [7]. For our experiments,\nwe use the scoring schemes introduced in [8] and [7], where\nthe score between two notes depends on the pitch, the du-\nration and the consonance of both notes. For example,\nthe ﬁfth (7 semitones) and the third major or minor (3 or\n28010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n4 semitones) are the most consonant intervals in Western\nmusic [19]. The score function between two notes is then\ndeﬁned as a linear combination of a function σpon pitches\n(its values are coded into a matrix) and a function σdon\ndurations as:\nσ(a,b) =α·σp(a,b) +β·σd(a,b).\nThe cost associated to a gap only depends on the note du-\nration. Finally a penalty (a negative score) is also applied\nto each local transposition.\n3. PARALLEL IMPLEMENTATION\n3.1 GPU Architecture\nBoth AMD and NVIDIA build architectures with uniﬁed,\nmassively parallel programmable units, which allow pro-\ngrammers to target that programmable unit directly instead\nof dividing work across multiple hardware units. More\nprecisely, a GPU contains many streaming multiproces-\nsors (MPs) each containing several elements including sev-\neral cores, also called streaming processors (SPs), and var-\nious types of on-chip shared memories and registers. The\nMPs also share some constant memory areas with very\nfast access and a global uncached large memory with rel-\natively low throughput and long latency. For example, the\nNVIDIA GeForce 9800 GX2 used for our experiments (see\nSection 4) is a dual GPU engine with 256 cores (128 per\nGPU) running at 1.5 GHz. These cores are regrouped into\n2×16MPs which share a global memory of 1GB with\na 512-bit interface width providing a throughput of 128\nGB/sec (64 GB/sec per GPU).\nThe MPs creates, manages, and executes concurrent threads\nin hardware with zero scheduling overhead. To manage\nhundred of threads running several different programs, the\nmultiprocessor employs an architecture called SIMT (single-\ninstruction multiple-thread), which resembles SIMD (single-\ninstruction multiple-data) vector organizations, i.e., single\ninstruction controls multiple processing elements. Unlike\nSIMD vector machines, SIMT enables programmers to write\nthread-level parallel code for independent, scalar threads,\nas well as data-parallel code for coordinated threads.\n3.2 GPU Computing with CUDA\nOur implementation uses CUDA, a general purpose par-\nallel computing framework developed and distributed by\nNVIDIA for use with their recent GPUs1. CUDA can be\nseen as an extension of C that allows developers to de-\nﬁne C functions, called kernels to be executed Ntimes\nin parallel by Ndifferent CUDA threads . CUDA threads\nmay access data from multiple memory spaces during their\nexecution. CUDA’s programming model assumes that the\nCUDA threads execute on a separate device , whereas the\nrest of the program runs on a CPU. In other words, the\n1CUDA was introduced in November 2006 along with the G80 se-\nries. CUDA can be downloaded for free from http://www.nvidia.\ncom/object/cuda_home.html . A list of CUDA-enabled prod-\nuct is available at http://www.nvidia.com/object/cuda_\nlearn_products.html .GPU operates as a coprocessor to the host running the C\nprogram. Both the host and device maintain their own\nmemory areas, allowing for concurrent programming be-\ntween the CPU and the GPU(s). CUDA kernels must be\ncompiled into binary code using nvcc , a C compiler for\nCUDA. Note that nvcc supports C++ programming for\nhost functions but kernels must be written in C, possibly\nwith templates. nvcc also supports device emulation.\n3.3 CUDA implementation of QbH\nThe process of evaluating how well each piece of music in\na database match a query (sung or hummed in the case of\nQbH), and rank the music pieces according to this score\ncan be parallelized at different levels. As explained ear-\nlier, our implementation uses a variant of Smith-Waterman.\nAlthough it is possible to do so [20], our choice was not\nto parallelize the implementation of Smith-Waterman it-\nself, as this approach could only provide signiﬁcant im-\nprovements for extremely large sequences. In contrast, our\nCUDA implementation optimizes the arithmetic intensity\n(the ratio of arithmetic operations to memory operations)\nby computing in parallel all the scores of a query with ev-\nery piece of music in the database. If the database contains\nNpieces of music, our program virtually launches Nker-\nnels executing the Smith-Waterman algorithm in parallel.\nThe main challenges are therefore to optimize the resource\nallocations and the memory operations.\nAfter the query has been converted from its original\nformat (typically a wave audio ﬁle), we store it in a spe-\ncial memory area called the texture memory, which allows\nfor very fast read/write operations. The texture memory is\nshared among all threads (Fig. 2). The database usually\ncontain too many pieces of music to be stored in any of the\ncached, fast memories (texture, constant, shared memory).\nTherefore, all the pieces of music are stored in the global\nmemory. On a GPU, the global memory is not cached, so\nit is extremely important to follow the right access pattern\nto get maximum memory bandwidth. Throughput of mem-\nory operations is 8 operations per clock cycle, plus 400 to\n600 clock cycles of memory latency. Under some size and\nalignment conditions, the device is capable of reading data\nfrom global memory in a single load instruction. More-\nover, the memory bandwidth can be used most efﬁciently\nwhen the simultaneous memory access by all the active\nthreads can be coalesced into a single memory transaction.\n(For more details, see [21].)\nIn order to satisfy all these constraints, the pieces of\nmusic of the database are store in an array of float2 , a\nCUDA structure containing two 32-bit ﬂoats, which stores\nthe pitch and duration of each note. Although the size and\nalignment properties are fulﬁlled by this data type, storing\nthe pieces of music sequentially, one after the other, would\nbe very inefﬁcient since simultaneous reading by all the\nthreads in a single transaction would be impossible. In-\nstead, if the database contain Npieces of music, we orga-\nnize the data in memory as a one dimensional array, such\nthat its ﬁrstNentries correspond to the ﬁrst note (pitch, du-\nration) of each piece; then, the next Nentries correspond\n281Poster Session 2\nto the second note of each piece, etc.\nEach thread performs its computations on its own ma-\ntrix, more exactly on its ∆ = 12 transposition matrices. In\norder to minimize the amount of required memory, we only\nstore the current row of each matrix. Moreover, to optimize\nmemory alignment, allocation is based on the query’s ﬁxed\nsize rather than the pieces of music’s variable sizes. Fi-\nnally, in order to allow simultaneous read/write operations\nby the active threads, the matrices are not stored at con-\nsecutive addresses but rather using the same strategy as the\ndatabase. Fig. 2 describes the device architectures.\nFigure 2 . nVidia GPUs architecture. Each multi-processor\nexecutes both the conversion of queries from audio ﬁles to\na vector of notes (stored in the texture memory) and the\ncomparison between the query and each reference (stored\nin the device memory). Each processor store its interme-\ndiate Smith-Waterman matrices (only one row) in its own\nshared memory space. Constants costs and intermediate\nvalues are respectively stored in constant and shared mem-\nories.\n4. TESTS AND RESULTS\n4.1 Benchmark\nOur experiments are based on the query data corpus pro-\nposed for the QbH tasks at the MIREX 2007 and 2008\nand three different noise databases. Roger Jang’s corpus\nis composed of 2797 queries, along with 48 ground-truth\nMIDI ﬁles2, with the particularity that all queries start at\nthe beginning of the references. The ﬁrst database (called\nDB1) consists of the 48 ground-truth MIDIs and a sub-\n2http://www.cs.nthu.edu.tw/ ˜jangset of 2000 MIDI noise ﬁles from the Essen Collection3.\nThe whole Essen Collection, made of 5982 ﬁles together\nwith the 48 ground-truth MIDIs ﬁles, is called DB2. Fi-\nnally, since the ground-truth MIDIs are rather short while\nEssen collection mainly consists of long data ﬁles, we also\nconsider a third database, called DB3, proposed during the\nMIREX 2005, which is a subset of the RISM A/II (Interna-\ntional inventory of musical sources) collection, composed\nof 17433 short excerpt of real world compositions.\nWe have tested our implementation on three different\nplatforms. Their characteristics are given in Table 1. For\nOS CPU GPU\nW1 NVidia Tesla\nWorksta-\ntion running\nLinux, CUDA\nv2.13GHz Intel\nCore 2 DuoNVidia\nGeForce\n9800 GX2, 512\nMB memory\nW2 Mac Pro\nrunning Mac\nOS X 10.5,\nCUDA v2.2Two 2.8GHz\nIntel Xeon\nQuad CoreNVidia\nGeForce\n8800 GT, 512\nMB Memory\nL1 MacBook Pro\nrunning Mac\nOS X 10.5,\nCUDA v2.22.53 GHz\nIntel Core 2\nDuoNVidia\nGeForce\n9400 M, shared\nmemory with\nCPU\nTable 1 . Characteristics of our three platforms\neach algorithm we have measured the time on both the\nCPU alone and the CPU together with the GPU used as\na parallel coprocessor.\nRegarding the algorithms, we have implemented the orig-\ninal Smith-Waterman algorithm (SW) and our extension\nbased on local transposition alignment (LT). Since the queries\nare known to be at the beginning of the references, we have\nimplemented variants of the above algorithms that only\ncompare the query with the beginning of each MIDI ﬁles\n(in Table 2 we only report timings for size of the query plus\n10 notes). These variants are respectively called SW10 and\nLT10.\nWe evaluate the quality of our music retrieval system\nusing two measures. The Mean Reciprocal Rank (MRR),\ni.e.the average of the reciprocal ranks of the ﬁrst correct\nanswer, computed for a sample of Nqueries as\nMRR =1\nNN/summationdisplay\ni=11\nri,\nwhereriis the rank of the ﬁrst correct answer for the ith\nquery. And the top- Xratio which reports the proportion of\nqueries for which ri≤X.\n4.2 Smith-Waterman vs Local Transposition\nAlignment\nWe ﬁrst evaluate the quality, given in terms of MRR and\ntop-5 ratio, of SW and LT on the three databases. For\n3http://www.esac-data.org/\n28210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nDB1, SW reaches a MRR of 0.274 and a top-5 ratio of\n30.3%, while LT reaches a MRR of 0.684 and a top-5 ratio\nof 75.4%. The MRR increases when the size of the query is\ntaken into account during the comparison: SW10 reaches\na MRR of 0.295 and a top-5 ratio of 32.3%, while LT10\nreaches a MRR of 0.732 and a top-5 ratio of 79.4%. We ob-\nserve the same behaviour for DB2 and DB3. Fig. 3 shows\nthat the MRRs obtained for databases of different sizes re-\nmains roughly the same. The local transposition alignment\nmethod provides better results than Smith-Waterman.\nFor a suggestive comparison, the method submitted by\nWu and Li, which performed best at MIREX 2008 reaches\na MRR of 0.9 on a well choosen subset of 2000 MIDI ﬁles\nfrom the Essen collection. However, since we could not\nﬁnd the original database used for the MIREX 2008 com-\npetition, our DB1 consists of 2000 randomly chosen MIDI\nﬁles from the Essen collection. It therefore contains several\ncopies of the same noise ﬁles, which automatically impacts\nthe quality of our results.\n0\n 5000\n 10000\n 15000 20000\nDatabase size0.0\n0.2\n0.4\n0.6\n0.8\n1.0MRR\nSmith Waterman 10\nSmith Waterman\nLocal Transposition 10\nLocal Transposition\nFigure 3 . MRRs obtained for SW, SW10, LT, LT10 on our\nthree databases\n4.3 CPU vs GPU\nAlthough LT provides very good results in terms of qual-\nity, it is very time consuming. On our fastest CPU (W2),\na Mac Pro equipped with two 2.8 GHz Intel Xeon Quad\nCore processors, the analysis on DB1 takes more than 326\nminutes with LT10 ( ∼7 sec. per query) and more than 595\nminutes with LT (∼13 sec. per query). This computation\ntime even reaches 1282 minutes for LT10 ( ∼27.5 sec. per\nquery) on the largest database DB3, which contains more\nthan 17000 MIDI ﬁles.\nAs shown in Table 2, our CUDA implementations pro-\nvide impressive speed-ups. The local transposition algo-\nrithms (LT and LT10) which gives the highest MRRs and\ntop-Xratios, are up to 162 times faster than their CPU\ncounterpart. This is achieved for DB3 on our Mac Pro con-\nﬁguration W2; the analysis of more than 17000 MIDI ﬁles\nusing LT10 was completed in 473 seconds ( ∼0.16 s per\nquery). Note also that the local transposition algorithm isperfectly adapted to parallel implementations as it is only\nslightly slower than SW.\nIt is important to remark that our CUDA implementa-\ntion leads to signiﬁcant improvements even on our lightest\nconﬁguration (L1), a laptop not designed to perform heavy\ngraphic computations and which embeds a cheap, on-chip\ngraphic card (NVidia 9400 M). The analysis of DB1 only\ntakes 470 seconds, i.e.∼0.16 s per query. During the\nMIREX 2008, the fastest implementation for the analy-\nsis of a database similar to DB1 was performed in 1699\nseconds on a AMD Athlon XP 2600+ running at 1.9GHz.\nOur fastest implementation, running on W1, completed the\nanalysis of the 2797 queries in only 301 seconds, that is al-\nmost 6 times faster.\nConﬁg. SW10 SW LT10 LT\nDB1L1 7:33 7:33 7:50 40:54\nW1 4:12 5:05 5:01 20:16\nW2 6:00 6:00 6:01 14:42\nDB2L1 7:53 7:51 15:10 79:45\nW1 6:55 6:54 6:10 25:46\nW2 6:18 6:18 6:16 20:40\nDB3L1 9:20 9:19 41:31 44:48\nW1 5:15 6:05 10:09 25:27\nW2 7:25 7:27 7:53 21:15\nTable 2 . Timings of the different algorithms on various\nGPUs and databases in mm:ss\n5. CONCLUSIONS\nLocal transposition alignment algorithms are very power-\nful. Using QbH as an experimental application, our variant\nof Smith-Waterman lead to very good results. We believe\nthat this type of algorithm would give even better results\nfor other music retrieval systems, such as cover detection,\nwhere the query is signiﬁcantly larger and contains fewer\nerrors than a sung or hummed query. Our implementation\ntakes advantage of the immense computing resources of-\nfered by the most recent graphic cards. These low-cost de-\nvices regroup hundreds of cores that can operate in parallel\nand sufﬁcient memory to store large musical databases. A\ngreat care must be taken when programming memory op-\nerations as a bad allocation strategy can have a signiﬁcant\nimpact on the computation time. At this time, we have not\nyet optimized the pre-processing phase of the system. In\nparticular, the analysis and conversion of the queries (wave\naudio ﬁles) is running exclusively on the CPU and takes\nbetween 75-90% of the overall computation time. Our\nnext task will be to implement this stage on GPU using\nthe CUDA CUFFT library. We anticipate signiﬁcant im-\nprovements in terms of speed.\n6. ACKNOWLEGMENT\nThis work has been partially sponsored by the French ANR\nSIMBALS (JC07-188930) and ANR Brasero (ANR-06-BLAN-\n0045) projects.\n283Poster Session 2\n7. REFERENCES\n[1] N. Orio. Music retrieval: A tutorial and review. Foun-\ndations and Trends in Information Retrieval , 1(1):1–\n90, 2006.\n[2] S. Doraisamy and S. R ¨uger. Robust polyphonic music\nretrieval with N-grams. Journal of Intelligent Informa-\ntion Systems , 21(1):53–70, 2003.\n[3] A. L. Uitdenbogerd. Music Information Retrieval Tech-\nnology . PhD thesis, RMIT University, Melbourne, Vic-\ntoria, Australia, July 2002.\n[4] E. Ukkonen, K. Lemstr ¨om, and V . M ¨akinen. Geomet-\nric algorithms for transposition invariant content-based\nmusic retrieval. In Proceedings of the 4th International\nConference on Music Information Retrieval, ISMIR\n2003 , pages 193–199, 2003.\n[5] R. Typke, R. C. Veltkamp, and F. Wiering. Search-\ning notated polyphonic music using transportation dis-\ntances. In Proceedings of the ACM Multimedia Confer-\nence, pages 128–135, 2004.\n[6] R. Typke and A. Walczak-Typke. A tunneling-vantage\nindexing method for non-metrics. In Proceedings of\nthe 9th International Conference on Music Information\nRetrieval, ISMIR 2008 , pages 351–352, 2008.\n[7] P. Hanna, P. Ferraro, and M. Robine. On optimizing the\nediting algorithms for evaluating similarity between\nmonophonic musical sequences. Journal of New Mu-\nsic Research , 36(4):267–279, 2007.\n[8] M. Mongeau and D. Sankoff. Comparison of musical\nsequences. Computers and the Humanities , 24(3):161–\n175, 1990.\n[9] T. F. Smith and M. S. Waterman. Identiﬁcation of com-\nmon molecular subsequences. Journal of Molecular\nBiology , 147(1):195–197, 1981.\n[10] J. Serr `a, E. G ´omez, P. Herrera, and X. Serra.\nChroma binary similarity and local alignment applied\nto cover song identiﬁcation. IEEE Transactions on Au-\ndio, Speech and Language Processing , 16:1138–1151,\n2008.\n[11] R. B. Dannenberg, W. P. Birmingham, B. Pardo, N. Hu,\nC. Meek, and G. Tzanetakis. A comparative evaluation\nof search techniques for query-by-humming using the\nMUSART testbed. Journal of the American Society for\nInformation Science and Technology , 58(5):687–701,\n2007.\n[12] P. Hanna and M. Robine. Query by tapping system\nbased on alignment algorithm. In Proceedings of the\nIEEE International Conference on Acoustics, Speech,\nand Signal Processing, ICASSP , 2009. (to appear).\n[13] J. P. Bello. Audio-based cover song retrieval using ap-\nproximate chord sequences: Testing shifts, gaps, swapsand beats. In Proceedings of the 8th International Con-\nference on Music Information Retrieval, ISMIR 2007 ,\npages 239–244, September 2007.\n[14] J. S. Downie, M. Bay, A. F. Ehmann, and M. C. Jones.\nAudio cover song identiﬁcation: MIREX 2006-2007\nresults and analyses. In Proceedings of the 9th Inter-\nnational Conference on Music Information Retrieval,\nISMIR 2008 , pages 51–56, 2008.\n[15] J. D. Owens, M. Houston, D. Luebke, S. Green, J. E.\nStone, and J. C. Phillips. GPU computing. Proceedings\nof the IEEE , 96(5):879–899, May 2008.\n[16] S. Needleman and C. Wunsch. A general method ap-\nplicable to the search for similarities in the amino acid\nsequences of two proteins. Journal of Molecular Biol-\nogy, 48:443–453, 1970.\n[17] D. Gusﬁeld. Algorithms on Strings, Trees and Se-\nquences – Computer Science and Computational Bi-\nology . Cambridge University Press, Cambridge, 1997.\n[18] J. Allali, P. Ferraro, P. Hanna, and C. Iliopoulos. Lo-\ncal transpositions in alignment of polyphonic musi-\ncal sequences. In String Processing and Information\nRetrieval Symposium, 14th International Symposium,\nSPIRE 2007, Proceedings , volume 4726 of Lecture\nNotes in Computer Science , pages 26–38. Springer,\nOctober 2007.\n[19] F. J. Horwood. The Basis of Music . Gordon V . Thomp-\nson Limited, Toronto, Canada, 1944.\n[20] Y . Liu, W. Huang, J. Johnson, and S. Vaidya. GPU ac-\ncelerated Smith-Waterman. In Computational Science,\nICCS 2006 , volume 3994 of Lecture Notes in Com-\nputer Science , pages 188–195. Springer, 2006.\n[21] NVIDIA CUDA. Programming Guide , April 2009.\nVersion 2.2. Available at http://www.nvidia.\ncom/object/cuda_home.html .\n284"
    },
    {
        "title": "Sheet Music-Audio Identification.",
        "author": [
            "Christian Fremerey",
            "Michael Clausen",
            "Sebastian Ewert",
            "Meinard Müller"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416742",
        "url": "https://doi.org/10.5281/zenodo.1416742",
        "ee": "https://zenodo.org/records/1416742/files/FremereyCEM09.pdf",
        "abstract": "In this paper, we introduce and discuss the task of sheet music-audio identification. Given a query consisting of a sequence of bars from a sheet music representation, the task is to find corresponding sections within an audio interpretation of the same piece. Two approaches are proposed: a semi-automatic approach using synchronization and a fully automatic approach using matching techniques. A workflow is described that allows for evaluating the matching approach using the results of the more reliable synchronization approach. This workflow makes it possible to handle even complex queries from orchestral scores. Furthermore, we present an evaluation procedure, where we investigate several matching parameters and tempo estimation strategies. Our experiments have been conducted on a dataset comprising pieces of various instrumentations and complexity. 1 INTRODUCTION When listening to an audio recording of a piece of music, an obvious problem is to decide, which bar of a corresponding sheet music representation is currently played. For technical reasons, we tackle this problem from the viewpoint of sheet music-audio identification: Given a sequence of bars from the sheet music as a query, the task is to find all temporal sections in the audio recording, where this bar sequence from the query is played. One application of this task is to find out, whether there are differences between the default bar sequence following the instructions in the sheet music and what is actually played in the audio interpretation. In case there are differences, sheet music-audio identification may also be used to automatically determine the bar sequence that is played in the interpretation, and to identify special parts like cadenzas that have no counterpart in the sheet music. If the bar sequence played in the audio interpretation is known in advance, sheet music-audio identification can We gratefully acknowledge support from the German Research Foundation DFG. The work presented in this paper was supported by the PROBADO project (http://www.probado.de/, grant INST 11925/1-1) and the ARMADA project (grant CL 64/6-1). Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c⃝2009 International Society for Music Information Retrieval. be solved by first performing sheet music-audio synchronization and then using the synchronization results to identify the temporal sections in the audio that correspond to a given query sequence of bars. In case the correct bar sequence is not known, a more direct approach must be taken. Here, sheet music-audio matching as performed in [1] seems to be a reasonable strategy. In the literature, alignment, identification and retrieval has been a popular field of research for the single-domain cases of either audio or symbolic data, see [2] and the references therein. For the cross-domain case, a lot of effort has been put into the task of off-line and on-line alignment of score data and audio data [3–6]. Here, the assumption is made that the bar sequence of the score is already known. The idea of using cross-domain synchronization results as ground truth or training data for more complicated music information retrieval tasks has already been formulated for the application of automatic transcription of pop music [7]. First important steps towards cross-domain matching and identification of polyphonic musical works have been conducted by the groups of Pickens and Orio [4, 8]. Using either audio transcription techniques [8] or a statistical model for the production of audio data from polyphonic score data [4] a complete audio track (song or movement) is used as a query to find the corresponding work in the score domain. First experiments for approaching the task of cross-domain work identification by querying arbitrary segments of score data have been conducted by Syoto et al. [9] as well as in our previous work [1]. None of the above approaches explicitly handles differences in bar sequence structure or repeats between the score and audio data, even though this is a common and practically relevant issue in real-world digital music libraries. The paper is structured as follows. Section 2 specifies the task of sheet-music audio identification in more detail and discusses some difficulties and pitfalls. Our two approaches to sheet music-audio identification are presented in Section 3, one using synchronization and the other using matching. Section 4 explains how MIDI events for comparison with the audio data are created from the sheet music data. The synchronization and matching procedures are outlined in Sections 5 and 6. Section 7 describes an evaluation procedure for the matching approach using the more reliable results of the synchronization approach as a ground truth. Experimental results using our test dataset are discussed in Section 8 before the paper concludes with an outlook on future work in Section 9. 645 Poster Session 4 2 SHEET MUSIC-AUDIO IDENTIFICATION In the following, we assume that we are given one scanned sheet music representation and one audio interpretation of the same piece of music. We assign a unique label (p, b) to each bar written in the sheet music, where p is the page number and b is the bar number on the page. Furthermore, B denotes the set of all bar labels of the piece. Sheet music may contain jump directives like repeat signs, alternative endings, dacapos or segnos. Following these directives as they are written in the sheet music, one obtains a sequence δ = (δ1, . . . , δn), δi ∈B, indicating the default sequence of bars that is to be played when performing the piece. In practice, however, the given audio recording does not always follow this sequence δ. Performers might, for example, choose to ignore or add repeats, or even introduce shortcuts. This leads to a possibly different sequence π = (π1, . . . , πd), πi ∈B ∪{↑}, which we call performance sequence. Here, we use the label ↑to mark sections that are not written in the sheet music, e.g., cadenzas. Given the performance sequence π, the audio recording can be segmented into time intervals I1, . . . , Id such that time interval Ii corresponds to the section in the audio data where bar πi is played (or something that is not written in the score in case πi =↑). Given a query sequence of bars Q = (q0, . . . , qm), Q a substring of δ, the task of sheet music-audio identification is to find all time intervals T in the audio data where the query sequence of bars is played. More formally, H(Q) := {T | ∃j : Q = (πj, πj+1, . . . , πj+m) ∧T = Ij ∪Ij+1 ∪. . . ∪Ij+m} denotes the set of hits w.r.t. Q. Note that in case of repeats that are notated as repeat signs, there can be more than one hit for a given query. Also note that besides the time intervals T there might be other time intervals in the audio data where the same musical content is played, but that belong to a different sequence of bars in the sheet music. We denote this kind of time intervals as pseudo-hits. 3 TWO APPROACHES Given a scanned sheet music representation and an audio recording of the same piece of music, in a first step we use optical music recognition (OMR) software to extract information about musical symbols like staffs, bars and notes from the sheet music scans. Note that the obtained symbolic score data usually suffers from recognition errors. For simplicity, we here assume that the set of bar labels B and the default sequence δ are correctly obtained from the OMR output. Given a query Q = (q0, . . . , qm), which is a substring of δ, we want to find the set of hits H(Q) as specified in Section 2. We now describe two approaches with different preconditions. For the first approach, we assume that the performance sequence π = (π1, . . . , πd), πi ∈B ∪{↑}, is known. In this case, we are left with the calculation of the corresponding time intervals I1, . . . , Id. This can be done by using sheet music-audio synchronization. The set of hits H(Q) can then be computed by finding occurrences of the query sequence in the performance sequence. In the second approach, the performance sequence π is unknown. In this case, a reasonable strategy is to use sheet music-audio matching to search for sections in the audio recording with a similar musical content compared to the query sequence of bars. These sections may be considered as an approximation of the set of hits H(Q). However, one should be aware of the fact that this method cannot distinguish correct hits from pseudo-hits, and is therefore expected to deliver false positives. In the following, we will refer to such false positives as content-induced confusion. Such confusion is also expected to be introduced by query sequences that differ only slightly, either in musical content or by a very small number of bars at the beginning or end of the sequence. This issue becomes particularly relevant, since the presence of OMR errors prohibits using too strict settings for rating similarity in the matching. Due to the additional information π that is given in the first approach, this approach works much more robust and reliable than the second approach. The required performance sequence π can be created with little effort by manually editing an automatically generated list of jump directives acquired from the available default sequence δ. Therefore, we consider this approach semi-automatic. On the contrary, the second approach is fully automatic, but the results are less reliable. In the optimum case, only content-induced confusion would occur. In practice, however, extra confusion is likely to be introduced by shortcomings of the matching procedure. The idea followed in this paper is to use the more reliable results of the semi-automatic first approach to create ground truth results for evaluating the less reliable fully automatic second approach. Using this method, we compare different settings of the matching procedure used in the second approach to learn which one works best for the task of sheet music-audio identification. 4 DATA PREPARATION To compare sheet music data with audio data, we first create MIDI note events from the OMR results. However, OMR results often suffer from non-recognized or misclassified symbols. Especially in orchestral scores with many parts, erroneous or missing clefs and key signatures lead to wrong note pitches when creating MIDI events. Furthermore, orchestral scores can comprise parts for transposing instruments, i.e., the notated pitch is different from the sounding pitch. Such transposition information is not output by current OMR software, but it is essential for creating correctly pitched MIDI events. To be able to handle even complex orchestral scores, a so-called staff signature text file is generated from each page and is manually corrected. The staff signature file contains information about the clef, the key signature and the transposition at the beginning of each staff that is found on the page, see Figure",
        "zenodo_id": 1416742,
        "dblp_key": "conf/ismir/FremereyCEM09",
        "keywords": [
            "sheet music-audio identification",
            "task of finding corresponding sections",
            "query consisting of a sequence of bars",
            "audio interpretation of the same piece",
            "two approaches proposed",
            "semi-automatic approach using synchronization",
            "fully automatic approach using matching techniques",
            "workflow for evaluating matching approach",
            "evaluation procedure",
            "several matching parameters and tempo estimation strategies"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSHEET MUSIC-AUDIO IDENTIFICATION\nChristian Fremerey, Michael Clausen, Sebastian Ewert\nBonn University, Computer Science III\nBonn, Germany\n{fremerey,clausen,ewerts }@cs.uni-bonn.deMeinard M ¨uller\nSaarland University and MPI Informatik\nSaarbr ¨ucken, Germany\nmeinard@mpi-inf.mpg.de\nABSTRACT\nIn this paper, we introduce and discuss the task of sheet\nmusic-audio identiﬁcation. Given a query consisting of a\nsequence of bars from a sheet music representation, the\ntask is to ﬁnd corresponding sections within an audio inter-\npretation of the same piece. Two approaches are proposed:\na semi-automatic approach using synchronization and a\nfully automatic approach using matching techniques. A\nworkﬂow is described that allows for evaluating the match-\ning approach using the results of the more reliable syn-\nchronization approach. This workﬂow makes it possible to\nhandle even complex queries from orchestral scores. Fur-\nthermore, we present an evaluation procedure, where we\ninvestigate several matching parameters and tempo estima-\ntion strategies. Our experiments have been conducted on a\ndataset comprising pieces of various instrumentations and\ncomplexity.\n1 INTRODUCTION\nWhen listening to an audio recording of a piece of mu-\nsic, an obvious problem is to decide, which bar of a corre-\nsponding sheet music representation is currently played.\nFor technical reasons, we tackle this problem from the\nviewpoint of sheet music-audio identiﬁcation : Given a se-\nquence of bars from the sheet music as a query, the task is\nto ﬁnd all temporal sections in the audio recording, where\nthis bar sequence from the query is played.\nOne application of this task is to ﬁnd out, whether there\nare differences between the default bar sequence follow-\ning the instructions in the sheet music and what is actually\nplayed in the audio interpretation. In case there are differ -\nences, sheet music-audio identiﬁcation may also be used to\nautomatically determine the bar sequence that is played in\nthe interpretation, and to identify special parts like cade n-\nzas that have no counterpart in the sheet music.\nIf the bar sequence played in the audio interpretation\nis known in advance, sheet music-audio identiﬁcation can\nWe gratefully acknowledge support from the German Research\nFoundation DFG. The work presented in this paper was support ed by the\nPROBADO project (http://www.probado.de/, grant INST 1192 5/1-1) and\nthe ARMADA project (grant CL 64/6-1).\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .be solved by ﬁrst performing sheet music-audio synchro-\nnization and then using the synchronization results to iden -\ntify the temporal sections in the audio that correspond to\na given query sequence of bars. In case the correct bar\nsequence is not known, a more direct approach must be\ntaken. Here, sheet music-audio matching as performed\nin [1] seems to be a reasonable strategy.\nIn the literature, alignment, identiﬁcation and retrieval\nhas been a popular ﬁeld of research for the single-domain\ncases of either audio or symbolic data, see [2] and the ref-\nerences therein. For the cross-domain case, a lot of effort\nhas been put into the task of off-line and on-line alignment\nof score data and audio data [3–6]. Here, the assumption is\nmade that the bar sequence of the score is already known.\nThe idea of using cross-domain synchronization results as\nground truth or training data for more complicated music\ninformation retrieval tasks has already been formulated fo r\nthe application of automatic transcription of pop music [7] .\nFirst important steps towards cross-domain matching\nand identiﬁcation of polyphonic musical works have been\nconducted by the groups of Pickens and Orio [4, 8]. Us-\ning either audio transcription techniques [8] or a statisti cal\nmodel for the production of audio data from polyphonic\nscore data [4] a complete audio track (song or movement)\nis used as a query to ﬁnd the corresponding work in the\nscore domain. First experiments for approaching the task\nof cross-domain work identiﬁcation by querying arbitrary\nsegments of score data have been conducted by Syoto et\nal. [9] as well as in our previous work [1]. None of the\nabove approaches explicitly handles differences in bar se-\nquence structure or repeats between the score and audio\ndata, even though this is a common and practically rele-\nvant issue in real-world digital music libraries.\nThe paper is structured as follows. Section 2 speciﬁes\nthe task of sheet-music audio identiﬁcation in more detail\nand discusses some difﬁculties and pitfalls. Our two ap-\nproaches to sheet music-audio identiﬁcation are presented\nin Section 3, one using synchronization and the other us-\ning matching. Section 4 explains how MIDI events for\ncomparison with the audio data are created from the sheet\nmusic data. The synchronization and matching procedures\nare outlined in Sections 5 and 6. Section 7 describes an\nevaluation procedure for the matching approach using the\nmore reliable results of the synchronization approach as a\nground truth. Experimental results using our test dataset\nare discussed in Section 8 before the paper concludes with\nan outlook on future work in Section 9.\n645Poster Session 4\n2 SHEET MUSIC-AUDIO IDENTIFICATION\nIn the following, we assume that we are given one scanned\nsheet music representation and one audio interpretation of\nthe same piece of music. We assign a unique label (p,b)\nto each bar written in the sheet music, where pis the page\nnumber and bis the bar number on the page. Furthermore,\nBdenotes the set of all bar labels of the piece. Sheet mu-\nsic may contain jump directives like repeat signs, alterna-\ntive endings, dacapos or segnos. Following these direc-\ntives as they are written in the sheet music, one obtains a\nsequence δ= (δ1,... ,δ n), δi∈B, indicating the default\nsequence of bars that is to be played when performing the\npiece. In practice, however, the given audio recording does\nnot always follow this sequence δ. Performers might, for\nexample, choose to ignore or add repeats, or even intro-\nduce shortcuts. This leads to a possibly different sequence\nπ= (π1,... ,π d), πi∈B∪ {↑} , which we call perfor-\nmance sequence . Here, we use the label ↑to mark sec-\ntions that are not written in the sheet music, e.g., caden-\nzas. Given the performance sequence π, the audio record-\ning can be segmented into time intervals I1,... ,I dsuch\nthat time interval Iicorresponds to the section in the au-\ndio data where bar πiis played (or something that is not\nwritten in the score in case πi=↑).\nGiven a query sequence of bars Q= (q0,... ,q m), Qa\nsubstring of δ, the task of sheet music-audio identiﬁcation\nis to ﬁnd all time intervals Tin the audio data where the\nquery sequence of bars is played. More formally,\nH(Q) := {T| ∃j:Q= (πj,πj+1,... ,π j+m)\n∧T=Ij∪Ij+1∪...∪Ij+m}\ndenotes the set of hitsw.r.t.Q. Note that in case of repeats\nthat are notated as repeat signs, there can be more than\none hit for a given query. Also note that besides the time\nintervals Tthere might be other time intervals in the audio\ndata where the same musical content is played, but that\nbelong to a different sequence of bars in the sheet music.\nWe denote this kind of time intervals as pseudo-hits .\n3 TWO APPROACHES\nGiven a scanned sheet music representation and an audio\nrecording of the same piece of music, in a ﬁrst step we use\noptical music recognition (OMR) software to extract infor-\nmation about musical symbols like staffs, bars and notes\nfrom the sheet music scans. Note that the obtained sym-\nbolic score data usually suffers from recognition errors.\nFor simplicity, we here assume that the set of bar labels\nBand the default sequence δare correctly obtained from\nthe OMR output. Given a query Q= (q0,... ,q m), which\nis a substring of δ, we want to ﬁnd the set of hits H(Q)as\nspeciﬁed in Section 2. We now describe two approaches\nwith different preconditions.\nFor the ﬁrst approach, we assume that the performance\nsequence π= (π1,... ,π d), πi∈B∪ {↑},is known. In\nthis case, we are left with the calculation of the correspond -\ning time intervals I1,... ,I d. This can be done by using\nsheet music-audio synchronization. The set of hits H(Q)\ncan then be computed by ﬁnding occurrences of the query\nsequence in the performance sequence.In the second approach, the performance sequence πis\nunknown. In this case, a reasonable strategy is to use sheet\nmusic-audio matching to search for sections in the audio\nrecording with a similar musical content compared to the\nquery sequence of bars. These sections may be considered\nas an approximation of the set of hits H(Q). However,\none should be aware of the fact that this method cannot\ndistinguish correct hits from pseudo-hits, and is therefor e\nexpected to deliver false positives. In the following, we\nwill refer to such false positives as content-induced confu-\nsion. Such confusion is also expected to be introduced by\nquery sequences that differ only slightly, either in musica l\ncontent or by a very small number of bars at the beginning\nor end of the sequence. This issue becomes particularly\nrelevant, since the presence of OMR errors prohibits using\ntoo strict settings for rating similarity in the matching.\nDue to the additional information πthat is given in the\nﬁrst approach, this approach works much more robust and\nreliable than the second approach. The required perfor-\nmance sequence πcan be created with little effort by man-\nually editing an automatically generated list of jump di-\nrectives acquired from the available default sequence δ.\nTherefore, we consider this approach semi-automatic. On\nthe contrary, the second approach is fully automatic, but\nthe results are less reliable. In the optimum case, only\ncontent-induced confusion would occur. In practice, how-\never, extra confusion is likely to be introduced by short-\ncomings of the matching procedure.\nThe idea followed in this paper is to use the more reli-\nable results of the semi-automatic ﬁrst approach to create\nground truth results for evaluating the less reliable fully\nautomatic second approach. Using this method, we com-\npare different settings of the matching procedure used in\nthe second approach to learn which one works best for the\ntask of sheet music-audio identiﬁcation.\n4 DATA PREPARATION\nTo compare sheet music data with audio data, we ﬁrst cre-\nate MIDI note events from the OMR results. However,\nOMR results often suffer from non-recognized or misclas-\nsiﬁed symbols. Especially in orchestral scores with many\nparts, erroneous or missing clefs and key signatures lead\nto wrong note pitches when creating MIDI events. Fur-\nthermore, orchestral scores can comprise parts for trans-\nposing instruments, i.e., the notated pitch is different fr om\nthe sounding pitch. Such transposition information is not\noutput by current OMR software, but it is essential for cre-\nating correctly pitched MIDI events. To be able to handle\neven complex orchestral scores, a so-called staff signature\ntext ﬁle is generated from each page and is manually cor-\nrected. The staff signature ﬁle contains information about\nthe clef, the key signature and the transposition at the be-\nginning of each staff that is found on the page, see Figure\n1. It also identiﬁes which staffs belong to the same grand\nstaff. The information from the staff signature ﬁles is used\nto correct errors in the OMR output and to add the missing\ninformation about transposing instruments.\nThere are several choices to be made regarding onset\ntimes and tempo, when creating the MIDI events from the\nOMR results. Since in the OMR output, notes or beams\n64610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 1 . Staff signature annotation for an example grand staff take n from a score of the “Symphony to Dante’s Divina\nCommedia S109 - Inferno” by Franz Liszt. Positive key signat ure values count the number of sharps, negative values count\nthe number of ﬂats. Transposition values are speciﬁed as the amount of semitones the pitch has to be modiﬁed with to\nsound correctly.\nare often missed out, the accumulated note durations are\nnot a good estimator for note onset times. This is espe-\ncially the case for scores with multiple staffs and possi-\nbly multiple voices per staff, where the voice onset times\nmight drift apart. Instead we use the horizontal position\nof notes within each measure as an estimator for the on-\nset time. Even though this does not deliver onset times\nthat perfectly match the musical meter, this method is very\nrobust against surrounding errors and effectively inhibit s\nvoices from drifting apart.\nAnother parameter that is required to convert sheet mu-\nsic data to MIDI events is the tempo. This parameter is\nusually not output by OMR systems. If the performance\nsequence πis known in advance, the mean tempo can be\ncalculated from the duration of the audio track. When π\nis not known, one might either use a ﬁxed tempo or try to\nestimate a tempo based on the musical content. Note that\nthe actual tempo used in audio interpretations can easily\nvary from 40to220beats per minute (quarter notes per\nminute). We will investigate the effects of different tempo\nestimation strategies in our experiments in Section 8.\nBoth the MIDI data and the audio data are converted\nto sequences of normalized chroma-based features. Each\nfeature is a 12-dimensional vector encoding the local en-\nergy distribution among the 12traditional pitch classes of\nWestern classical music commonly labeled C, C♯, D,...,B.\n5 SYNCHRONIZATION\nAfter transforming both the MIDI data as well as the au-\ndio data into sequences of normalized chroma vectors, we\nuse dynamic time warping (DTW) to synchronize the two\nsequences. Here, the main idea is to build up a cross-\nsimilarity matrix by computing the pairwise distance be-\ntween each score chroma vector and each audio chromavector. In our implementation, we simply use the in-\nner vector product for the comparison. An optimum-cost\nalignment path is determined from this matrix via dynamic\nprogramming. To speed up this computationally expensive\nprocedure, we use an efﬁcient multiscale version of DTW.\n6 MATCHING PROCEDURE\nThe task of the matching procedure is to ﬁnd sections in the\naudio interpretation that are considered similar to a given\nquery of score data. In this paper, we use a variant of the\nsubsequence dynamic time warping algorithm for this task.\nFor details we refer to the literature [2]. As in the case of\nsynchronization, both the audio data and the score data are\nﬁrst converted to feature sequences. Each feature vector\nfrom the score query is compared to each feature vector\nfrom the audio database by means of a suitable local cost\nmeasure . The results of this comparison are stored in a\ncost matrix, see Figure 2. Finding candidate matches from\nthis cost matrix means ﬁnding paths connecting the bot-\ntom row and the top row of the matrix. In particular, we\nare interested in paths pwhere the sum of the local cost of\nthe matrix cells covered by the path is as small as possible.\nSuch paths are calculated using dynamic programming by\niteratively advancing from the bottom left towards the top\nright using a constrained set of allowed step directions en-\nsuring that a path never runs backwards in time. For each\nmatrix cell, the minimum cost of any valid path leading to\nthat cell is saved in a so-called accumulated cost matrix .\nMatches are then identiﬁed by ﬁnding minima in the top\nrow of the accumulated cost matrix.\nGiven a query bar sequence Q, the match-\ning procedure outputs a set of matches M(Q) =\n{(p1,c1),... ,(pN,cN)}, where piis a path connecting\nthe top and bottom rows and ci∈R≥0is the cost of\n647Poster Session 4\nFigure 2 . Illustration of the subsequence DTW cost matrix\nfor a score query with a length of two measures accounting\nfor 11 seconds of MIDI data (Beethoven Sonata 3, Opus 2\nNo 3, Adagio, measures 16–17). An excerpt of 27 seconds\nof audio data including one correct match is displayed. The\noptimum-cost path pfor the correct match is rendered as a\nsequence of squares connected by lines.\nthe path pi. The results are ranked with respect to the\npath cost. The choice of allowed step directions can be\nvaried and associated step weights can be introduced to\nfavor certain directions and behaviors. Several settings f or\nstep directions and step weights will be discussed in our\nexperiments in Section 8.\n7 EV ALUATION PROCEDURE\nSheet music-audio matching depends on a multitude of pa-\nrameters and settings used in the steps of creating MIDI\nevents, creating feature sequences, and performing the\nmatching procedure. In this work, we are interested in ﬁnd-\ning out which parameters work best for the task of sheet\nmusic-audio identiﬁcation. We do this by evaluating and\ncomparing several parameter sets on a test dataset consist-\ning of a collection of musical tracks , with each track being\nrepresented by one sheet music representation and one au-\ndio interpretation.\nIn the evaluation, we perform the matching procedure\non a set of test queries. For each test query Q, we then eval-\nuate the matching results M(Q)using a set of ground truth\nhitsH(Q)and a suitable confusion measure. To calculate\nthe confusion measure, we ﬁrst identify which matches\noutput by the matching procedure correspond to ground\ntruth hits. Let T= [t0,t1]∈H(Q)be the ground truth hit\nand(p,c)∈M(Q)be a match whose path pcorresponds\nto the time interval T′= [t′\n0,t′\n1]in the audio. The match\n(p,c)is then considered to correspond to the ground truth\nhitT, if both the durations and the locations roughly coin-\ncide. More precisely, with ∆ :=t1−t0and∆′:=t′\n1−t′\n0\nwe require that\n|∆′−∆|<0.2∆ and |t′\n1−t1|<0.2∆.\nIn the following, we call a match that corresponds to\na ground truth hit a correct match and a match that does\nnot correspond to a ground truth hit an incorrect match .\nLetM(Q) = {(p1,c1),... ,(pN,cN)}be the set of all\nmatches for a query Q, and let C⊆[1 :N]be the set of\nindices of correct matches and I⊆[1 :N]be the set of\nindices of incorrect matches. The confusion measure we\nFigure 3 . Scape plot for Beethoven’s Piano Sonata no.7\nop.10 no.3 Rondo (Allegro) using the confusion measure\nΓH,M.\nuse in this paper is a binary-valued function ΓH,M that on\ninputQtakes the value 1if at least one ground truth hit\ninM(Q)has no corresponding match or if there is an in-\ncorrect match with lower cost than the highest-cost correct\nmatch, and 0otherwise:\nΓH,M(Q) :=\n\n1missed ground truth hit\n1 min i∈Ici<max i∈Cci\n0otherwise.\nIn other words, ΓH,M(Q) = 0 if all ground truth hits are\nfound and are ranked higher than any incorrect match. In\ncase of ΓH,M(Q) = 1 we also speak of confusion .\nUsing the results of sheet music-audio synchronization\nthat have been calculated in a preprocessing step, a set of\nground truth hits can be calculated for any input query se-\nquence of bars Qthat is a substring of δ. This allows us to\ntest each track using a grid of queries that covers not only\nthe whole track but also a wide range of query lengths.\nThe results can be nicely visualized in a so called scape\nplot[10]. Figure 3 shows a scape plot using the confusion\nmeasure ΓH,M. Time runs from left to right. The lowest\nrow shows the results for the shortest query length. The\nquery length successively increases when moving upwards\nin the plot. The darker shaded areas indicate confusion.\nFrom Figure 3, one can see that longer queries lead to\nless confusion and better separability of correct and in-\ncorrect matches. The plot also reveals where in the track\nand up to what query lengths the confusion happens. To\nnot only be able to visually compare parameters for each\nindividual track, but to also enable comparisons for the\nwhole dataset, we summarize the results of all queries in\none number per track by simply averaging over the com-\nplete grid of queries. Subsequently, we calculate the av-\nerage over all tracks to end up with a single number for\neach set of parameters. If one parameter set works better\nthan another parameter set, this fact should manifest in a\nlower average ΓH,M value. Note that one should not com-\npare absolute values of the confusion measure for differ-\nent tracks or datasets, because the absolute values depend\non too many uncontrolled factors like the content-induced\nconfusion, the tempo of the audio interpretation, and the\ncontent-dependent “uniqueness” of bars. Therefore, we\nkeep datasets ﬁxed, when studying the effects of using\n64810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nComposer Work Instrumentation #Pages #Tracks Duration\nBeethoven Piano Sonatas 1–15 Piano 278 54 5h 01min\nLiszt“A Symphony to Dante’s Divina\nCommedia”Symphonic Orchestra 145 2 44min\nMendelssohn Concert in E minor, Op.64 Violin and Orchestra 55 3 26min\nMozart String Quartetts 1–13 String Quartett 190 46 2h 46min\nSchubert“Die sch ¨one M ¨ullerin”, “Winter-\nreise” and “Schwanensang”Singer and Piano 257 58 3h 04min\nTable 1 . Information and statistics on the test dataset used for eva luation.\nFigure 4 .ΓH,M values averaged over the complete dataset\nfor every combination of 5tempo estimation strategies and\n4step direction and cost settings. Lower values are better.\ndifferent parameters by comparing the confusion measure\nvalues.\n8 EXPERIMENTS AND RESULTS\nUsing the procedures described in the previous sections,\nthere are many aspects whose effect on sheet music-audio\nidentiﬁcation should be investigated. Due to space lim-\nitation, we restrict ourselves to investigating the effect s\nof different tempo estimation strategies in combination\nwith different step settings and cost settings in the sub-\nsequence DTW. In particular, we test ﬁve tempo estima-\ntion strategies: fixedXXXbpm : Fixed tempo of XXX\nbeats per minute, with XXX taking the values 50,100\nand200.fixedAudio : Fixed mean tempo of the\ncorresponding audio interpretation (estimated via man-\nually annotated πand the duration of the audio ﬁle).\nadaptiveMax100bpm : The tempo is determined indi-\nvidually for each bar by taking into account the number\nof different onset times within the bar. The tempo is cho-\nsen such that the duration of the bar is 200ms times the\nnumber of different onset times. This leads to bars with\nruns of short-duration notes being slowed down compared\nto bars with long notes. Additionally, a maximum tempo\nof100bpm is used to limit the difference between slow andFigure 5 . Tempo distribution of the test dataset being\nweighted the same way as the results in Figure 4\nfast bars.\nWe use four different step and cost settings for\nthe subsequence DTW. classic : Step vectors\n(1,0),(0,1),(1,1)and cost weights 1,1,1.focussed :\nStep vectors (2,1),(1,2),(1,1)and cost weights 2,1,1.\noffset : Same asclassic , but with an additional cost\noffset of 1which is added to each cell of the local cost\nmatrix.normalized : The same as classic , but with\nan additional modiﬁcation at the stage of calculating the\naccumulated cost matrix. At each matrix cell, the cost\nbeing compared for making the decision about which\nstep vector leading to this cell delivers the minimum\naccumulated cost are normalized by the accumulated path\nlength up to this cell. This normalization prevents short\npaths being preferred over long paths, even if the short\npaths have a higher average cost.\nThe dataset used for testing consists of 5sheet music\nbooks covering a range of instrumentations and complex-\nities, see Table 1. One audio inpterpretation per track is\nincluded. For each track in the dataset, we calculate the\nΓH,M value for a grid of queries similar to the one used to\ncreate the scape plot in Figure 3. We start with a query\nlength of 5bars and use a hop size of 5bars to move\nthroughout the track. The query length is successively in-\ncreased by 5bars up to a maximum query length of 40\nbars.\nFigure 4 shows the results for testing all 20combina-\ntions of settings on the test dataset. The ΓH,M values il-\nlustrated in the ﬁgure are average values calculated by ﬁrst\ntaking the average over all tracks within each scorebook,\nand then taking the average over all scorebooks. This way,\neach of the ﬁve different types of instrumentation and com-\nplexity gets the same weight. Since we are measuring ef-\nfects that depend on the tempo, we also need to look at the\n649Poster Session 4\ndistribution of tempi of the tracks in the test dataset. Figu re\n5 shows the distribution of tempi being weighted the same\nway as the results in Figure 4 and conﬁrms that there is no\nbias towards slower or higher tempi that might distort our\nresults.\nFrom the results in Figure 4 we can see that both the\ntempo estimation strategy and the tested step direction and\ncost settings clearly have an effect on the average amount\nof confusion. The best overall results are achieved by the\nsettingfocussed when using the mean tempo of the\naudio interpretation. This was expected, since this set-\nting is more focussed towards the diagonal direction and,\ntherefore, beneﬁts the most from the fact that the tempo\nis known. However, in cases where the difference be-\ntween the estimated tempo and the actual tempo of the\ninterpretation becomes too large, the lack of ﬂexibility\nleads to confusion, as can be seen for the tempo strategies\nfixed50bpm andfixed200bpm .\nIn the cases, where the tempo of the audio interpretation\nis assumed to be unknown, the best results are achieved by\nthe setting classic using thefixed50bpm tempo es-\ntimation strategy. Both settings classic andoffset\nwork best when the estimated tempo is low. A possible ex-\nplanation for this effect is that the accumulating cost lead\nto a preference of short paths. Shorter paths contain less\nsteps and therefore accumulate less cost. When looking at\nthe cost matrix depicted in Figure 2, one may think of the\noptimum-accumulated-cost paths tending to make short-\ncuts towards the top of the cost matrix instead of following\nthe lane of minimum local cost. This effect leads to ad-\nditional confusion when the estimated tempo of the sheet\nmusic data is high compared to the actual tempo of the au-\ndio interpretation.\nThe setting normalized delivers better results than\ntheclassic andoffset settings for every tempo es-\ntimation strategy except for the fixed50bpm . For that\nstrategy, however, it clearly falls behind and leads to even\nworse results than in the fixed100bpm case. A possible\nexplanation is that, in contrast to the settings classic\nandoffset , the setting normalized does not prefer\nshorter paths over longer paths. This seems to be an ad-\nvantage when the estimated tempo is not too low, but in\nthefixed50bpm case, the lack of a driving force towards\nkeeping the path connecting the bottom and top rows short\ncauses paths to become much more sensitive to noise and\nlocal dissimilarities.\nTheadaptiveMax100 yields only a tiny improve-\nment over the fixed100bpm estimation. The reason for\nthat probably is that the difference between the two strate-\ngies usually affects only the slower pieces. A test run using\nonly the slower pieces might lead to a bigger advantage for\nthe adaptive strategy.\n9 CONCLUSIONS\nWe introduced and discussed the task of sheet music-audio\nidentiﬁcation, which is identifying sections of an audio\nrecording where a given query sequence of bars from the\nsheet music is played. Two approaches to solving the task\nhave been described, a semi-automatic approach using syn-\nchronization and a fully automatic approach using match-ing techniques. We proposed a workﬂow that allows for\nevaluating the matching approach by using results from\nthe more reliable synchronization approach. This work-\nﬂow includes contributions that make it possible to per-\nform synchronization and matching even for complex or-\nchestral scores. We introduced the idea of using scape plots\nto visualize results of matching or retrieval tasks that are\nperformed on a grid of test queries covering a complete\ntrack of music over a wide range of query lengths. Finally,\nwe performed an evaluation using a subsequence DTW\nbased matching technique for the task of sheet music-audio\nidentiﬁcation. Results were presented and discussed for\ndifferent sets of settings and tempo estimation strategies .\nIn our future work, we would like to investigate more\naspects of sheet music-audio identiﬁcation to answer ques-\ntions like the following: Which features work best? What\nis the optimum feature resolution? Can the results be im-\nproved by using a harmonic model on the MIDI events cre-\nated from the sheet music? What inﬂuence do OMR errors\nhave on the results? Besides comparing the amount of con-\nfusion, we are also interested in comparing the temporal\naccuracy of matches.\n10 ACKNOWLEDGEMENTS\nWe would like to express our thanks to the Bavarian State\nLibrary in Munich for their cooperation and for providing\nthe sheet music scans.\n11 REFERENCES\n[1] C. Fremerey, M. M ¨uller, F. Kurth, and M. Clausen: “Auto-\nmatic Mapping of Scanned Sheet Music to Audio Record-\nings,” Proc. ISMIR, Philadelphia, USA , pp. 413–418, 2008.\n[2] M. M ¨uller: Information Retrieval for Music and Motion ,\nSpringer, 2007.\n[3] F. Soulez, X. Rodet, and D. Schwarz: “Improving Poly-\nphonic and Poly-instrumental Music to Score Alignment,”\nProc. ISMIR, Baltimore, USA , pp. 143–148, 2003.\n[4] N. Orio: “Alignment of Performances with Scores Aimed at\nContent-Based Music Access and Retrieval,” Proc. ECDL,\nRome, Italy , pp. 479–492, 2002.\n[5] C. Raphael: “Aligning Music Audio with Symbolic Scores\nUsing a Hybrid Graphical Model,” Machine Learning ,\nV ol. 65 No. 2–3 pp. 389–409, 2006.\n[6] R.B. Dannenberg and C. Raphael: “Music Score Align-\nment and Computer Accompaniment,” Communications of\nthe ACM , V ol. 49 No. 8 pp. 38–43, 2006.\n[7] R.J. Turetsky and D.P.W. Ellis: “Ground-Truth Transcrip-\ntions of Real Music from Force-Aligned MIDI Syntheses,”\nProc. ISMIR, Baltimore, USA , pp. 135–141, 2004.\n[8] J. Pickens, J.P. Bello, G. Monti, T. Crawford, M. Dovey, and\nM. Sandler: “Polyphonic Score Retrieval Using Polyphonic\nAudio Queries: A Harmonic Modeling Approach,” Proc. IS-\nMIR, Paris, France , pp. 140–149, 2002.\n[9] I.S.H. Suyoto, A.L. Uitdenbogerd, and F. Scholer: “Search-\ning Musical Audio Using Symbolic Queries,” IEEE Transac-\ntions on Audio, Speech, and Language Processing , V ol. 16\nNo. 2 pp. 372–381, 2008.\n[10] C. Sapp: “Comparative Analysis of Multiple Musical Per-\nformances,” Proc. ISMIR, Philadelphia, USA , pp. 497–500,\n2008.\n650"
    },
    {
        "title": "Scalability, Generality and Temporal Aspects in Automatic Recognition of Predominant Musical Instruments in Polyphonic Music.",
        "author": [
            "Ferdinand Fuhrmann",
            "Martín Haro",
            "Perfecto Herrera"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416394",
        "url": "https://doi.org/10.5281/zenodo.1416394",
        "ee": "https://zenodo.org/records/1416394/files/FuhrmannHH09.pdf",
        "abstract": "In this paper we present an approach towards the classification of pitched and unpitched instruments in polyphonic audio. In particular, the presented study accounts for three aspects currently lacking in literature: model scalability to polyphonic data, model generalisation in respect to the number of instruments, and incorporation of perceptual information. Therefore, our goal is a unifying recognition framework which enables the extraction of the main instruments’ information. The applied methodology consists of training classifiers with audio descriptors, using extensive datasets to model the instruments sufficiently. All data consist of real world music, including categories of 11 pitched and 3 percussive instruments. We designed our descriptors by temporal integration of the raw feature values, which are directly extracted from the polyphonic data. Moreover, to evaluate the applicability of modelling temporal aspects in polyphonic audio, we studied the performance of different encodings of the temporal information. Along with accuracies of 63% and 78% for the pitched and percussive classification task, results show both the importance of temporal encoding as well as strong limitations of modelling it accurately.",
        "zenodo_id": 1416394,
        "dblp_key": "conf/ismir/FuhrmannHH09",
        "keywords": [
            "polyphonic audio",
            "pitched instruments",
            "unpitched instruments",
            "model scalability",
            "model generalization",
            "perceptual information",
            "unifying recognition framework",
            "audio descriptors",
            "temporal integration",
            "real world music"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSCALABILITY, GENERALITY AND TEMPORAL ASPECTS IN\nAUTOMATIC RECOGNITION OF PREDOMINANT MUSICAL\nINSTRUMENTS IN POLYPHONIC MUSIC\nFerdinand Fuhrmann, Mart ´ın Haro, Perfecto Herrera\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n{ferdinand.fuhrmann,martin.haro,perfecto.herrera }@upf.edu\nABSTRACT\nIn this paper we present an approach towards the classiﬁ-\ncation of pitched and unpitched instruments in polyphonic\naudio. In particular, the presented study accounts for three\naspects currently lacking in literature: model scalability\nto polyphonic data, model generalisation in respect to the\nnumber of instruments, and incorporation of perceptual in-\nformation. Therefore, our goal is a unifying recognition\nframework which enables the extraction of the main in-\nstruments’ information. The applied methodology consists\nof training classiﬁers with audio descriptors, using exten-\nsive datasets to model the instruments sufﬁciently. All\ndata consist of real world music, including categories of\n11 pitched and 3 percussive instruments. We designed our\ndescriptors by temporal integration of the raw feature val-\nues, which are directly extracted from the polyphonic data.\nMoreover, to evaluate the applicability of modelling tem-\nporal aspects in polyphonic audio, we studied the perfor-\nmance of different encodings of the temporal information.\nAlong with accuracies of 63% and 78% for the pitched and\npercussive classiﬁcation task, results show both the impor-\ntance of temporal encoding as well as strong limitations of\nmodelling it accurately.\n1. INTRODUCTION\nInstrument recognition is one of the big problems of cur-\nrent research in music information retrieval (MIR). Auto-\nmatic indexing and retrieval of audio data are basic con-\ncepts to efﬁciently administrate and navigate through big\ndatasets. Providing the information about the instrumen-\ntation of audio tracks via an automatic recognition sys-\ntem can highly facilitate these operations. Besides, such\na system provides higher-level musical information, which\nhelps to narrow the well-known semantic gap [1].\nComputational recognition of musical instruments makes\nuse of the intrinsic properties of, and differences between,\neach of the target categories. In the case of pitched in-\nstruments, where the sound is mostly composed of quasi-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.harmonic components, these are the amplitudes and fre-\nquency positions of the components and their evolution\nin time. The time-varying spectral envelope, an eminent\nfeature for pitched instrument recognition [2], can be es-\ntimated out of them. For percussive instruments, proper-\nties such as attack and decay time, or frequency coverage,\nare properties which allow to distinguish between them\n[3]. While these speciﬁc characteristics can be determined\nwithout big problems in the case of a monophonic record-\ning, the problem gets harder in polyphonic audio. Since\nthe co-occurrence of multiple sound sources is producing\noverlapping frequency components, information extracted\nfrom the raw audio is often ambiguous and only partially\nuseful for discriminating between several musical instru-\nments. Without any preprocessing based on source sepa-\nration, which is still not mature enough, models derived\nfrom simpliﬁed scenarios seem to imply strong limitations\nfor the use on polyphonic audio. However, we hypoth-\nesize that, by providing a well suited dataset, a coarse –\nbut MIR useful – modelling of predominant instruments\ndirectly from polyphonic audio is possible.\nBelow follows a short review of the current state of the\nart in computational musical instrument recognition. In\nSec. 3 we substantiate our work and provide details about\nthe general concepts we used for tackling the problem.\nSec. 4 gives insights in the used data, the developed al-\ngorithms, and shows the experimental results. In the sub-\nsequent discussion we point out capacities as well as lim-\nitations of the chosen techniques and, ﬁnally, Sec. 6 con-\ncludes this article.\n2. RELATED WORK\nIn current literature there exists a great unbalance between\nthe amount of studies dealing with recognition of pitched\ninstruments from polyphonic data and the amount of pub-\nlications studying the monophonic case. Since the latter is\nnot addressed in this paper, we refer to [4] for a compre-\nhensive overview. Regarding the scarce publications ad-\ndressing the more complex scenario, Kitahara et al. [5] pre-\nsented a method to eliminate unreliable feature data caused\nby source inference for instrument recognition in artiﬁ-\ncial polyphonic mixtures. Linear discriminative analysis\n(LDA) was used to enhance features which discriminate\nbest the ﬁve categories. The features were extracted from\nthe harmonic structures of the corresponding instruments\n321Oral Session 3: Musical Instrument Recognition and Multipitch Detection\nand used to train multivariate gaussian prototypes. Addi-\ntional post-processing was applied by integrating the frame-\nwise a-posteriori probabilities and incorporating higher-\nlevel musical knowledge to get the ﬁnal classiﬁcation. In\na more recent work, Every [6] manually annotated songs\nfrom a commercially available collection according to per-\nceptually dominant instruments along with their correspond-\ning pitches. A great set of audio descriptors was extracted\nfrom the raw audio in an unsupervised system, where the\nfeature vectors were clustered and the resulting accuracies\nwere measured. A strategy for tackling the problem at hand\nfrom a complete different direction was presented by Essid\net al. [7]. Unlike trying to isolate the instruments present\nin the mixture, the whole audio was classiﬁed consider-\ning the more frequent combinations of them. Therefore,\na suitable taxonomy was automatically generated by clus-\ntering a training corpus. Statistical models were built for\neach of the derived categories and used to classify unseen\ninstances.\nRegarding the recognition of percussive events in poly-\nphonic music, work has focused on transcription of most\ncommon drum kit sounds (i.e. Bass Drum, Snare Drum\nand Hi-Hat sounds). For an excellent overview of studies\non drum transcription up to 2006 see [3]. In a more recent\nwork Paulus and Klapuri [8] evaluated a system based on\nHidden Markov Models (HMM). In addition to standard\nspectral features, temporal features were derived from sub-\nband envelopes using 100 ms windows. Slight improve-\nments in transcription accuracy were reported by incorpo-\nrating this temporal information. Gillet and Richard [9]\nused source separation as preprocessing to obtain a drum-\nenhanced signal. A set of features was computed from\nboth original and “enhanced” signals. Classiﬁcation was\nderived from previously trained support vector machines\n(SVM) on an experimental database consisting of 28 songs\nfrom the ENST database (see Sec. 4.1 for an overview of\nthis database).\nExamining the literature review above we detect three\nmain gaps in which we substantiate our present work. More\nprecisely, we miss the aspect of polyphonic scalability , i.e.\na detailed research about the application of current meth-\nods for instrument recognition to highly polyphonic audio.\nSecond, there does not exist, to our knowledge, a study ac-\ncounting for instrument generality , i.e. presenting a consis-\ntent methodology incorporating multiple instruments from\ndifferent musical styles for tackling this problem. Finally,\nwe see a clear need for incorporating temporal character-\nistics within the recognition process when working with\nstatistical models, as this information is known to be im-\nportant but often neglected.\n3. CONCEPTUAL OVERVIEW\nThe present study is thought as a ﬁrst step towards assess-\ning the aforementioned gaps. We propose a methodology\nusing statistical recognition techniques to build indepen-\ndent classiﬁcation systems for 11 pitched and 3 unpitched\ninstruments. Ground truth obtained from mostly manually\ncreated collections is used for training the models, gatheredonly from real world music. Additionally, we evaluate the\nimportance and modelling accuracy of temporal aspects by\ncomparing systems using different encodings of temporal\ninformation. What follows is a more detailed description\nof the concepts addressed within this work.\nPolyphonic scalability. In this paper we are taking an\napproach of learning the time-frequency characteristics of\nmusical instruments directly from polyphonic data. Our\naim is to label a given audio excerpt with the name(s) of\nthe most salient instrument(s). There exist some evidence\nthat the performance of a recognition system is improved\nwhen the polyphonic context is incorporated into the train-\ning process [10]. As we focus on the application of a recog-\nnition algorithm on commercially available music, we in-\ntroduce the least simpliﬁed conditions and work directly\nwith real world data, all containing predominant instru-\nments plus accompaniment.\nInstrument generality. We included the recognition of\npitched as well as unpitched (percussive) instruments in\nour study. As they imply obvious differences in their sound\ncharacteristics, both groups have to be treated in a slightly\ndifferent way for computational processing. Percussive in-\nstruments produce a high energetic, impulsive sound and\ncarry the main information in a relative short time interval\n(typically between 100 and 200 ms), whereas pitched in-\nstruments tend to have a quasi-harmonic and continuous\ntone ranging from very short to medium long durations\n(several seconds). Moreover, percussive sounds produce\na spectrum in which their energy is scattered among the\nfrequency bins, whereas pitched instruments have a fre-\nquency representation with peaks at quasi-integer multi-\nples of their fundamental. Furthermore, a clear pitch de-\npendency of the spectrum can be observed with pitched in-\nstruments unlike the more ﬁxed spectral patterns of drum\nsounds.\nPitched Instruments: We consider an instrument to be\n“pitched” if it is able to produce a continuous, quasi-\nharmonic sound. Ten pitched instruments (Cello, Clar-\ninet, Flute, acoustic and electric Guitar, Hammond Organ,\nPiano, Saxophone, Trumpet and Violin) are used in this\nstudy, being a good representation for most of the possible\ninstrumentations in real world music of Western culture.\nWe also include the human singing voice as an extra cat-\negory in the corpus, as it can bee seen as frequently used\npitched instrument in pop and rock music.\nUnpitched Instruments: Due to the importance of the\ndrum kit in Western popular music we decided to con-\ncentrate our research efforts on this particular set of per-\ncussive instruments. Likewise, because of the number of\navailable instances and the musical relevance of each in-\nstrument within the drum kit, we work with the following,\nmost common in literature, instrument classes: Bass Drum\n(BD), Snare Drum (SD) and Hi-Hat (HH).\nTemporal characteristics. We incorporate temporal in-\nformation in our statistical modelling process. According\nto experimental ﬁndings in literature, the human auditory\nsystem uses temporal aspects as an important cue for the\nrecognition of musical instruments [2], but this informa-\n32210th International Society for Music Information Retrieval Conference (ISMIR 2009)\ntion is often neglected in related studies. Together with\nquasi static properties of the sound, its evolution in time\nshapes the basics of human timbre perception. Therefore\nwe directly compare different encodings of the temporal\ninformation in our statistical models. Doing that, we do\nnot only examine the applicability of these aspects to com-\nputational musical instrument recognition. We also study\nhow far they can be modelled directly from the polyphonic\naudio.\nIn [11] the modelling of temporal aspects was already\nanalyzed by comparing the performance of a monophonic\nto a polyphonic similarity task. Timbre similarity was eval-\nuated by both static systems, ignoring temporal aspects,\nand algorithms incorporating temporal behaviour. The used\ndata consisted of isolated sound samples for the mono-\nphonic similarity task, and one song from The Beatles, seg-\nmented into its individual notes, for the polyphonic case.\nThe authors concluded their work by stating that the frame-\nbased analysis of polyphonic audio is not suited for mod-\nelling any temporal related properties. Moreover, as the\nperformance of the dynamic systems was superior for the\nmonophonic analysis and the same amount inferior for the\npolyphonic scenario, they identiﬁed the polyphony itself\nto be the root of all evil. However, we try to tackle this\nproblem of polyphony by using big, diverse datasets and\nshow that there still remain temporal aspects which can be\nmodelled, if not for similarity retrieval, at least for sound\nsource recognition.\n4. METHOD\n4.1 Data\nA key concept for a successful modelling of musical in-\nstruments from polyphonic audio is the quality and the\nrepresentativeness of the used data. We used two public\navailable datasets to form the corpus for the recognition of\npercussive instruments, and developed our own collection\nfor the pitched instrument identiﬁcation task. Therefore\nwe manually gathered sound samples from the three con-\nsidered super-genres of Western music (jazz, classical and\npop/rock), all extracted from commercial available record-\nings.\nThe objective for the creation of the dataset for the\npitched instruments was to assemble excerpts of poly-\nphonic audio in which the target instrument is playing con-\ntinuously and is easily audible for a human listener. Each\naudio excerpt was then labelled with its predominating in-\nstrument (double-checked by two human experts), thus as-\nsigning more than one instrument to an audio excerpt was\nnot allowed. After all, a corpus containing about 2,500\naudio ﬁles was created, each one taken from a different\nrecording. We tried to equally distribute the data among\nthe three above-mentioned super-genres in order to cover\nmost musical styles and combinations of instruments.\nIn the case of percussive instruments we used two pub-\nlicly available collections with proper annotations of per-\ncussive events, namely the ENST-Drums database [12] and\nthe MAMI database [13]. The ﬁrst one is the largest pub-licly available drum database which provides “wet” and\n“dry” (see [12] for detailed information) drum tracks, as\nwell as the respective accompaniment tracks. We decided\nto work with the “wet” drums and their accompaniment.\nFrom the obtained collection of 64 songs we randomly\nselected 30 second excerpts of every song and its labels.\nThe MAMI database is a collection of 52 annotated mu-\nsic fragments extracted from commercial audio recordings.\nWe managed to gather 48 songs and aligned them with the\nprovided annotations. Finally, we mixed the ENST and the\nMAMI databases in order to have a representative database\nfor training purposes. Thus we obtained a large set of poly-\nphonic music excerpts adding up a total of 112 songs la-\nbelled with three, possibly concurrent, tags.\n4.2 Algorithm Processing\nOur approach towards assessing the information encoded\nby the different instruments and developing suitable mod-\nels is based on classical pattern recognition techniques.\nFirst, we extract segments from the audio ﬁle containing\nthe target instrument. For the drum recognition algorithm\nwe generate excerpts based on onset detection: either we\ntake a segment starting from the onset and lasting for 150\nms or, if the next onset falls within the following 150 ms,\nwe take the inter-onset-interval. In the end, we include ev-\nery so-generated excerpt in the dataset. For the pitched in-\nstruments we randomly extract a maximum of four 2.5 s\nlong segments from each audio ﬁle. This grants a big\namount of variability in the polyphonic background, which\naccompanies the main instrument. The length of 2.5 s\nwas empirically determined and showed superior perfor-\nmance over shorter durations, whereas no signiﬁcant im-\nprovement could be observed by using longer excerpts.\nThese segments are then framed with a ﬁxed framesize\nof 46 ms and hopsize of 12 ms using a Blackman-Harris\nwindowing function and audio features are extracted for\nevery frame. We use a big amount of spectral, cepstral, and\ntonal features, all of them are well known audio descrip-\ntors and will not be discussed here. For a comprehensive\noverview of standard audio features we refer the interested\nreader to [14].\nThe frame-wise extraction results in a time series of fea-\nture vectors, consisting of the raw feature values. This two\ndimensional representation (features versus frames) is fur-\nther processed by describing the evolution in time of each\naudio feature. We compute standard statistical measures\nlike mean, variance from both the actual and the delta val-\nues, as well as more speciﬁc quantities accounting for the\ntemporal information. The full set of the applied functions\ntogether with a short description is listed in Table 1. Fi-\nnally, we derive one vector with a dimension of 2,023 rep-\nresenting the audio content of the extracted segment.\nTo decrease the complexity of the problem we perform\nfeature selection on our data. For our experiments we\nsearch for the best subset of descriptors in the feature space,\ntaking their correlation with the respective classes and their\nintercorrelation inside the subset into account [16]. We\napply a 10 fold cross-validated feature selection to return\n323Oral Session 3: Musical Instrument Recognition and Multipitch Detection\nname description\nmean mean of the values\nvar variance of the values\ndmean mean of the delta values\ndvar variance of the delta values\nmax-norm-pos location of the maximum\nmin-norm-pos location of the minimum\nattack slope of the attack\ndecay slope of the decay\nslope overall slope\nt-centroid temporal centroid of the values\nt-skewness temporal skewness of the values\nt-kurtosis temporal kurtosis of the values\nTable 1 . Applied functions to describe temporal aspects of\nthe raw feature values. See [15] for details on their imple-\nmentation.\nFigure 1 . Block diagram of the training and recognition\nprocess. Black arrows indicate the training process while\nwhite ones show the prediction cycle. Note the decoupled\nmodules of extraction ,temporal integration andselection\nin the feature processing stage.\nboth a discriminative and compact set of descriptors. This\nprocedure reduces the dimensionality of the vectors by a\nfactor of 20, which signiﬁcantly lowers the computation\ntime of the following steps.\nThe feature vectors are then used to train SVMs, pow-\nerful classiﬁers for complex classiﬁcation tasks. As the\nSVM is a binary classiﬁer by deﬁnition, different strategies\nfor combining the data and training the SVMs were tested\nto apply them to the multi-class problem. For our drum\nrecognition system we utilized a balanced one–versus–all\nschema, where one SVM discriminates between the target\ncategory and an artiﬁcial one, consisting of a mixture of\nthe remaining classes. Hence, each classiﬁer determines\nthe presence or absence of the respective class. In the\ncase of pitched instruments we use a balanced one–versus–\none algorithm with pair-wise coupling (PWC) [17], which\nperformed superior than the one–versus–all approach in\npreceding experiments. Here, the ﬁnal decision about the\nclass membership is made by combining the output prob-\nabilities of all binary SVMs. The so generated models are\nthen used to predict the labels of new data, represented as\nfeature vectors. Fig. 1 shows an overview of the presented\nalgorithm with a detailed view on the feature processing\nstage.\nFigure 2 . Relative occurrences of different feature cate-\ngories in the ﬁnal feature selection, applied to the full set\nof descriptors. The categories are derived in respect to the\nacoustic facets the features represent. See text for a de-\ntailed description.\n4.3 Experiments and Results\nFirst we evaluated the application of our features in the\ncontext of the pitched and the unpitched classiﬁcation tasks.\nWe grouped all selected descriptors in respect to the acous-\ntic facets they represent: 8 categories were derived to eval-\nuate the relative importance of the raw features when ex-\ntracted from polyphonic data. In particular, the categories\nincluded ton(HPCPs, pitch salience), bar(Barkband en-\nergies), cep(MFCCs), lpc(LPCs), har(tristimuli, inhar-\nmonicity, odd2even), sp1(the four spectral moments), sp2\n(crest, rolloff,...), and pow (RMS, 3-bandenergies). Their\nrelative occurrences are shown in Fig. 2. Furthermore, to\nassess the importance of temporal information encoded in\nthe selected descriptors, we again grouped all of them into\nthree new subsets. According to their modelling of the\ntemporal information we derived the categories µ/σ2(only\nthe average and deviation of the values), ∆(coarse encod-\ning of the temporal characteristics in the delta descriptors),\nand time (detailed modelling of temporal aspects). Fig. 3\nshows the results.\nFor the ﬁnal evaluation of the recognition systems we\nsplit our data into two sets of 90 and 10% of their sizes.\n10 fold cross validation was performed on the 90% dataset\nwhile the remaining 10% were used as an independent hold-\nout test set. Performance was measured by the resulting\nclassiﬁcation accuracy. Furthermore, to evaluate the ef-\nfectiveness of the descriptors derived by the temporal in-\ntegration of the raw feature values, we compared the per-\nformance of 3 different feature subsets. The ﬁrst subset\nconsisted of the full set of features, the second contained\nthe average and the variance of both the actual and the delta\nfeature values and subset 3 only included the mean and the\nvariance of the raw values. Hence, we look at different\nencodings of the temporal information and their applica-\ntion for the recognition process. For all three groups we\nperformed the above described feature selection procedure\n32410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 3 . Relative occurrences of different descriptor cate-\ngories in the ﬁnal feature selection, applied to the full set of\ndescriptors. The categories represent increasing encodings\nof the temporal information.\ndata full set µ/σ2+∆ µ/σ2\nPitched 63.1 / 50.3% 63.4 / 50.3% 61.1 / 47.2%\nDrums 77.8 / 78.1% 77.6 / 82.6% 74.7 / 78%\nTable 2 . Performance comparison of different feature sub-\nsets with decreasing incorporation of temporal informa-\ntion. Accuracy of 10 fold cross validation (left values) and\nthe hold-out set (right values) is shown (values for drums\nrepresent averaged individual accuracies).\nbefore classiﬁcation. The resulting accuracies can be seen\nin Table 2. Additionally, Fig. 4 provides details about the\nsystem performance in correctly identifying the individual\nclasses on subset 2.\nA binomial test [18, p. 37] revealed a signiﬁcant dif-\nference between the cross-validated accuracies in the ﬁrst\ntwo columns of Table 2 and those from the third column,\nfor both pitched and unpitched instruments (p-value of the\nnull hypothesis≤10−3). Obviously, no statistical signif-\nicance was found by comparing the ﬁrst two columns for\nboth recognition algorithms.\n5. DISCUSSION\nThe above presented results show the capacities of the cho-\nsen approach as well as some clear limitations. By using a\nbig, well suited dataset covering all relevant musical styles\nand a selected set of audio descriptors, the algorithm is able\nto learn the time-frequency characteristics of different mu-\nsical instruments even from polyphonic data to a certain\ndegree. This indicates that, although the target instrument\nis partly masked by various accompanying sounds, there\nstill exist information in the audio data which is correlated\nwith the main instrument. Moreover, our selected audio\nfeatures can be used for extracting this intrinsic informa-\ntion from complex mixtures.\nLooking at the results of the grouping experiment pre-\nFigure 4 . System performance in correctly identifying in-\ndividual instrument categories. F-measures of the 10 Fold\nCross Validation on the µ/σ2+ ∆ feature subset.\nsented in Fig. 2 we can infer that both Barkband ener-\ngies and cepstral features play a major role in discriminat-\ning between instrument classes. In particular, in the drum\nrecognition the Barkbands form about 60% of all selected\ndescriptors. This conﬁrms that the spectral energy distri-\nbution is an important characteristic of percussive instru-\nments. In the case of pitched instruments, the number of\ncepstral and spectral descriptors selected indicates the im-\nportance of the spectral envelope for recognition.\nHowever, apart from the imbalance of categories (11 vs.\n3), the performance differences between the pitched and\npercussive recognition indicate that the former task is more\ncomplex than the latter. This can also be derived from the\nfact that the characteristics of pitched instruments are more\ndifﬁcult to capture with the current audio features. As the\nrequired information is carried in a few frequency bins, a\nlot of noise due to overlapping components is incorporated\ninto the feature values. Furthermore, percussive sounds\ngenerally carry more energy at the same time scale and\ntherefore exhibit a more robust feature extraction.\nExamining the performance of the individual instru-\nments (Fig. 4), we can observe that the Snare Drum per-\nforms worse in respect to Bass Drum and Hi-Hat. As the\nlatter ones only cover the very low and high frequency re-\ngions respectively, the Snare Drum has to compete with\nseveral other instruments in the same region, which de-\ngrades the systems’ performance in correctly labelling\nSnare Drum sounds. Regarding pitched instruments, the\nweakness of the saxophone can be explained by its variety\ninside the class (e.g. Bass, Baritone, Tenor, and Alto), in\ncontrary to other classes (e.g. hammond organ). Interest-\ningly, the singing voice performs best among the pitched\ninstruments, which was not expected. As an additional\nsupport of these observations, it is worth mentioning that\nsimilar results were obtained by the hold-out test set.\nNevertheless, compared to the human ability to recog-\nnize sounds, which is still the measure of all things, the\nresults clearly indicate an inferior performance of our ap-\n325Oral Session 3: Musical Instrument Recognition and Multipitch Detection\nproach. First, the evaluation of the detailed temporal mod-\nelling of our audio features shows that, when extracted\nfrom polyphony, the resulting descriptors are not very dis-\ncriminative between different instruments. We could not\nobserve any improvement in performance when they were\nincorporated for the drum recognition task, even if a major-\nity of the selected descriptors are describing ﬁne temporal\ncharacteristics (see Fig. 3). Moreover, hardly any of these\ndescriptors are selected for the pitched model. This im-\nplies that in the context of polyphony a detailed modelling\nbecomes impossible for both short (percussive) and longer\ntime-scale analysis (pitched), and that the remaining tem-\nporal aspects are best encoded in the coarse delta descrip-\ntors. That all strengthens the fact that temporal information\nis important for recognition but also shows the problems\nof modelling it accurately. These outcomes partly conform\nwith the results presented in [11] by identifying ﬁne tempo-\nral modelling of polyphonic audio as very fragile descrip-\ntors but proving the more coarse delta coefﬁcients to be\npowerful, even when extracted from polyphonic data. Sec-\nondly, even if there would be some headroom for improve-\nments, the algorithm will never be able to solve certain,\ndead-easy for humans, recognition tasks. Therefore we\nclearly see the need for different approaches in this area,\nstarting from new audio representations to new algorithms\nfor polyphonic processing. Enhanced signal processing as\na front end system coupled with a complete probabilistic\narchitecture (both bottom-up and top-down) could help to\ndiscover new paths, where an explicit source separation is\nnot needed. Moreover, the integration of different knowl-\nedge sources could increase performance, as one solution\nmight not always be applicable to all problems at hand.\n6. CONCLUSIONS\nIn this paper we addressed three open gaps in automatic\nrecognition of instruments from polyphonic audio. First\nwe showed that by providing extensive, well designed data-\nsets, statistical models are scalable to commercially avail-\nable polyphonic music. Second, to account for instrument\ngenerality , we presented a consistent methodology for the\nrecognition of 11 pitched and 3 percussive instruments in\nthe main western genres classical, jazz and pop/rock. Fi-\nnally, we examined the importance and modelling accuracy\noftemporal characteristics in combination with statistical\nmodels. Thereby we showed that modelling the temporal\nbehaviour of raw audio features improves recognition per-\nformance, even though a detailed modelling is not possible.\nResults showed an average classiﬁcation accuracy of 63%\nand 78% for the pitched and percussive recognition task,\nrespectively. Although no complete system was presented,\nthe developed algorithms could be easily incorporated into\na robust recognition tool, able to index unseen data or label\nquery songs according to the instrumentation.\nACKNOWLEDGEMENTS\nThe authors want to thank Joan Serr `a and Emilia G ´omez\nfor their help in improving the quality and style of thispublication. This research has been partially funded by the\nEU-IP project PHAROS1, IST-2006-045035.\n7. REFERENCES\n[1] X. Serra, R. Bresin, and A. Camurri, “Sound and music com-\nputing: Challenges and strategies,” Journal of New Music Re-\nsearch , vol. 36, no. 3, pp. 185–190, 2007.\n[2] S. McAdams, S. Winsberg, S. Donnadieu, G. DeSoete, and\nJ. Krimphoff, “Perceptual scaling of synthesized musical tim-\nbres: common dimensions, speciﬁcities, and latent subject\nclasses,” Psychological Research , vol. 58, no. 3, pp. 177–192,\n1995.\n[3] D. FitzGerald and J. Paulus, “Unpitched percussion transcrip-\ntion,” in Signal Processing Methods for Music Transcription ,\npp. 131–162, Springer, 2006.\n[4] P. Herrera, A. Klapuri, and M. Davy, “Automatic classi-\nﬁcation of pitched musical instrument sounds,” in Signal\nProcessing Methods for Music Transcription , pp. 163–200,\nSpringer, 2006.\n[5] T. Kitahara, M. Goto, K. Komatani, T. Ogata, and\nH. Okuno, “Instrument identiﬁcation in polyphonic music:\nFeature weighting to minimize inﬂuence of sound over-\nlaps,” EURASIP Journal on Advances in Signal Processing ,\nvol. 2007, pp. 1–16, 2007.\n[6] M. Every, “Discriminating between pitched sources in music\naudio,” IEEE Trans. on Audio, Speech, and Language Pro-\ncessing , vol. 16, no. 2, pp. 267–277, 2008.\n[7] S. Essid, G. Richard, and B. David, “Instrument recognition\nin polyphonic music based on automatic taxonomies,” IEEE\nTrans. on Audio, Speech, and Language Processing , vol. 14,\nno. 1, pp. 68–80, 2006.\n[8] J. Paulus and A. Klapuri, “Combining temporal and spectral\nfeatures in hmm-based drum transcription,” in Proc. of IS-\nMIR, 2007.\n[9] O. Gillet and G. Richard, “Transcription and separation of\ndrum signals from polyphonic music,” IEEE Trans. on Audio,\nSpeech, and Language Processing , vol. 16, no. 3, pp. 529–\n540, 2008.\n[10] D. Little and B. Pardo, “Learning musical instruments from\nmixtures of audio with weak labels,” in Proc. of ISMIR , 2008.\n[11] J. Aucouturier and F. Pachet, “The inﬂuence of polyphony on\nthe dynamical modelling of musical timbre,” Pattern Recog-\nnition Letters , pp. 654–661, 2007.\n[12] O. Gillet and G. Richard, “ENST-drums: an extensive audio-\nvisual database for drum,” in Proc. of ISMIR , 2006.\n[13] K. Tanghe, M. Lesaffre, S. Degroeve, M. Leman, B. D. Baets,\nand J. Martens, “Collecting ground truth annotations for drum\ndetection in polyphonic music,” in Proc. of ISMIR , 2005.\n[14] G. Peeters, “A large set of audio features for sound descrip-\ntion (similarity and classiﬁcation) in the CUIDADO project,”\ntech. rep., IRCAM, 2004.\n[15] M. Haro, “Detecting and describing percussive events in\npolyphonic music,” Master’s thesis, Universitat Pompeu\nFabra, Spain, 2008.\n[16] M. A. Hall, “Correlation-based feature selection for discrete\nand numeric class machine learning,” in Proc. of Int. Conf. on\nMachine Learning , pp. 359–366, 2000.\n[17] T. Hastie and R. Tibshirani, “Classiﬁcation by pairwise cou-\npling,” Annals of Statistics , pp. 451–471, 1998.\n[18] P. H. Kvam and B. Vidakovic, Nonparametric Statistics with\nApplications to Science and Engineering . Wiley, 2007.\n1http://www.pharos-audiovisual-search.eu\n326"
    },
    {
        "title": "Using XML-Formatted Scores in Real-Time Applications.",
        "author": [
            "Joachim Ganseman",
            "Paul Scheunders",
            "Wim D&apos;haes"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417893",
        "url": "https://doi.org/10.5281/zenodo.1417893",
        "ee": "https://zenodo.org/records/1417893/files/GansemanSD09.pdf",
        "abstract": "In this paper we present fast and scalable methods to access relevant data from music scores stored in an XML based notation format, with the explicit goal of using scores in real-time audio processing frameworks. Quick and easy access is important when accessing or traversing a score, for instance for real-time playback. Any time complexity improvement in these contexts is valuable, while memory constraints are usually less important. We show that with some well chosen design choices and precomputation of the necessary data, runtime time complexity of several key score manipulation operations can be reduced to a level that allows use in a real-time context.",
        "zenodo_id": 1417893,
        "dblp_key": "conf/ismir/GansemanSD09",
        "keywords": [
            "XML based notation format",
            "real-time audio processing frameworks",
            "access relevant data",
            "scalable methods",
            "runtime time complexity",
            "precomputation of data",
            "key score manipulation operations",
            "time complexity improvement",
            "memory constraints",
            "use in real-time context"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nUSING XML-FORMATTED SCORES IN REAL-TIME APPLICATIONS\nJoachim Ganseman, Paul Scheunders\nIBBT - Visionlab\nDept. of Physics, University of Antwerp\nUniversiteitsplein 1, building N\nB-2610 Wilrijk (Antwerp), Belgium\n{joachim.ganseman, paul.scheunders }@ua.ac.beWim D’haes\nMu Technologies NV\nSingelbeekstraat 121\nB-3500 Hasselt, Belgium\nwim.dhaes@mu-technologies.com\nABSTRACT\nIn this paper we present fast and scalable methods to access\nrelevant data from music scores stored in an XML based\nnotation format, with the explicit goal of using scores in\nreal-time audio processing frameworks. Quick and easy\naccess is important when accessing or traversing a score,\nfor instance for real-time playback. Any time complexity\nimprovement in these contexts is valuable, while memory\nconstraints are usually less important. We show that with\nsome well chosen design choices and precomputation of\nthe necessary data, runtime time complexity of several key\nscore manipulation operations can be reduced to a level\nthat allows use in a real-time context.\n1. INTRODUCTION\nIn real-time audio processing software, the use of music\nscores is not commonplace. To ﬁll that gap, we started the\nconstruction of a small C++ software library for handling\nMusicXML ﬁles, specially tailored for use in real-time au-\ndio processing software frameworks and streaming appli-\ncations. Since a score is for many people a well-known\nway to represent music, we consider this important func-\ntionality that has been strangely absent in real-time audio\nframeworks until now.\nBeing able to use XML-encoded digital music scores\nnatively in real-time environments has clear advantages over\nthe alternatives that are often used now, like MIDI [1] or\nthe development of an own ASCII-based format. The abil-\nity to use the countless mature software tools that are avail-\nable for XML parsing and processing is the main reason to\nprefer XML-based formats over others. Nowadays most\nscore ﬁle formats encode very detailed information, and\nXML formats can be easily extended or stripped to add or\nremove information, without needing to adapt the parser,\nwhich is much more difﬁcult with binary or plain text ﬁle\nformats. Also most upcoming web developments are cen-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.tered around XML-based standards (the semantic web etc.).\nFor all of the aforementioned reasons, we will only con-\nsider XML-based ﬁle formats here.\nMusicXML [2, 3] is the most widely used XML-based\nﬁle format for scores at the moment, but others exist. MEI\n[4, 5] is a mature alternative and provides valuable func-\ntionality to encode versioning and history tracking in doc-\numents. The WEDELMUSIC format [6] was developed as\nall-round multimedia format in an academic context and\nseems not to be under active development anymore, but its\nlegacy can more recently be found in MPEG SMR [7] and\nIEEE P1599/MX [8], that are both striving to include score\ninformation in a broader multimedia context.\nWe widen the scope of this paper to all of the aforemen-\ntioned formats, as they are all XML-based and use similar\nhierarchies to encode scores. The principles outlined in\nthis paper here thus hold for any of these formats. Also the\nprogramming language is of lesser importance: in any ma-\njor object-oriented programming language, the argumenta-\ntion for the design decisions will hold.\n2. PROBLEM STATEMENT AND\nREQUIREMENTS\nIn real-time audio processing, audio data is processed frame\nby frame, the necessary operations need to be performed\nwithin a certain time, and then the results are written to an\noutput buffer. The frames are usually kept small to min-\nimize delays. This leads to very strict time constraints,\nas also the operating system’s scheduler will lay claim to\nsome time for other processes or interrupts.\nA music score is layered on several levels (voices, in-\nstrument parts, chords), but these layers are often very much\ninterlinked (like voice crossing). This makes it unfeasible\nto ﬁnd an ideal single XML hierarchy to represent a score.\nThe result is that notes, voices and/or parts that are active\nat the same time can be found encoded in very different\nplaces in the ﬁle. If you need to access all notes occuring\nat a single moment, you may need to access data at tens to\nhundreds of different positions in the score ﬁle. Processing\ntime is very limited, and user interactivity or display of the\n663Poster Session 4\nscore require also quick access to random positions in the\nscore. This makes it becomes quickly undesirable to do the\nnecessary data structure traversals in real-time, based only\non the existing XML hierarchy.\nXML-based score data formats tend to produce really\nlarge data structures: common uncompressed score ﬁles\ncontain easily up to 250KB of text for a single A4 size page\nof piano solo music. When parsed into memory, this results\nin a relatively large XML tree. Since we envisioned use in\nreal-time environments, we want to absolutely minimize\nany calculation time that is needed ’on-line’ (during pro-\ncessing). There is no time to traverse an entire XML-tree to\nﬁnd the data that we need to access, as is commonplace in\nthe visitor design pattern [9] as used in libmusicxml [10].\nBecause data that is ’scheduled’ to occur at the same\ntime, can be heavily dispersed throughout the ﬁle, a SAX-\nbased approach to XML-parsing becomes difﬁcult, and a\nDOM model is easier to handle. This made us decide to\nwrite our own data structure for the score, ﬁrmly based on\nthe existing hierarchy of the format but with a few extra\nadditions in functionality and precalculation of data. In the\nfollowing sections of this paper, we will elaborate on the\nadditions that we had to make to keep run-time calcula-\ntion load as low as possible. To be able to parse XML ﬁles\nencoded in UTF-16, routines provided by the Unicode con-\nsortium can be used for the conversion of UTF-16 to UTF-\n8 data [11].\nWe set out to write a software library that could be used\nfrom real-time audio plug-in frameworks, as there are VST\n[12], AudioUnits [13] or RTAS [14]. In the end, we want to\nbe able to use and manipulate a music score in a sequencer\nthe same way we can use and manipulate an audio track.\nWe need:\n•quick access to all data.\n•an easy method for timewise browsing through a score.\n•easy extendability.\n•cross-platform operation, documentation, testing ...\nOn top of that, in practice we need to adhere to sev-\neral guidelines for real-time programming, amongst which\nsome important ones are [15] :\n•not allocating or deallocating memory\n•avoiding denormalized ﬂoating point numbers\n3. IT’S ABOUT TIME\n3.1 Timestamping\nMusic scores are generally structured as follows: scores\ncontain multiple parts or instruments, each of which con-\nsists of a series of measures that contain the notes. A scoreor a part can be subdivided into several sections, or within\na measure, multiple voices may be separately encoded. We\nleave intermediate levels like these out here for clarity.\nCoda, segno and other repeat signs inﬂuence at what ab-\nsolute time certain notes need to be played. This makes\nthat music scores are rarely written to ﬁle in a way that is\nlinear in time. The standard MIDI ﬁle format (SMF) comes\nclose, but was never meant to be used for score encoding.\nMusicXML stores timing information different than other\nformats: it only stores the note order and note length, and\nnot the absolute position in the score at which the note\noccurs. This system was derived from the MuseData for-\nmat [16]. In a real-time environment, we need absolute\ntimestamps in order to know at any given time where we\nare in the score. These timestamps thus need to be cal-\nculated if they are not present. The quarter note as a unit\nof absolute time is the most convenient choice. This is\nportable across scores, whichever time signature they have,\nand across recordings, whichever tempo they are played in.\nIn MusicXML, in order to know at what absolute time\nin the score a speciﬁc note occurs, one needs to add up the\nlength of all previous measures, and the previous notes in\nthe same measure. This is a too intensive computation in\nreal-time, therefore we need to precompute any absolute\ntime values that we need in our application. The easiest\nway to do this is to keep track of a global absolute time\nvalue during parsing, and store a timestamp in every ele-\nment that we encounter.\nHaving timestamps in the data structure has the follow-\ning effect on run-time operations, with n the number of\nelements that need to have such a timestamp:\n•worst-case time complexity to compute absolute time\nvalues in real-time decreases from O(n) to O(1),\n•when a change occurs in the score, these values need\nto be updated throughout the score, introducing a\npenalty of O(n).\nThese ﬁgures assume that the notes in a measure are\nalready sorted based on time, and that the timestamp of\nprevious elements can be used to update the timestamp of\nlater ones. Sorting on time is easy to accomplish by cre-\nating or overloading a comparison operator and running\na generic sorting algorithm after parsing. In general, all\ntime-modifying elements in the ﬁle format need to be pro-\ncessed during parsing to calculate timestamps for all notes.\nWe found it handy to also store the time at which a certain\ntimed element ends.\nWe need to add here that the increased complexity when\nchanges occur to the score (measures or notes added, deleted\nor moved), rarely outweighs the beneﬁts of having a times-\ntamp on all elements for the applications that we envision.\nFast access to useful data is the most important for us, and\n66410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nin real-time applications, especially when user interactiv-\nity is in play, score access operations tend to occur thou-\nsands of times more often than score manipulation opera-\ntions. If large-scale content manipulation of scores needs\nto be done very often, tools like XQuery are more ﬁt for\nthe job [17, 18].\n3.2 Some notes on tempo and repetitions\nIn order to be able to accomodate easily for tempo changes\nor other elements that affect playback (rallentando, accelerando\netc.), we tilted this performance timing information out of\nthe score, and transferred it to a separate datastructure. An\nelegant solution is the use of a warping function, map-\nping playback time (in seconds) to score time (in quarter\nnotes). The ﬁrst derivative of this function is the equiva-\nlent of the local tempo at a certain time. Smooth increases\nor decreases in tempo can be modeled using splines. Cu-\nbic Hermite splines are a good choice since those can be\ncalculated based only on two points and the tempo at these\npoints. When constraints are applied to keep the function\nstrict monotonously increasing, an inverse of that function\nexists which could eventually be used to encode informa-\ntion about performance, like lyricism.\nAlso affecting playback are structural elements, as there\nare repeat, coda, segno, ... Since these are usually limited\nin number and smooth transitions are not applicable here,\nthis data can be stored easiest in a simple table, storing the\ntimestamp values of all sections. When repeats should be\nskipped, one can just adapt this table, eliminating the need\nto do processing on the entire score data structure.\nGoing from playback time to score time ( illustrated in\nﬁg. 1 ) then comes down to deciding with what time in\nquarter notes this corresponds, through the previously de-\nﬁned warping function. If sections are repeated or skipped,\noffsets to this time need to be added or subtracted, accord-\ning to the information in the structure table. That way,\nwe come to a corresponding timestamp value in the score\nstructure itself, which can be used to access the necessary\ndata.\nNote that in this way, changes in tempo or performance\ninformation do not require updating the calculated times-\ntamps in the score, which would be a rather costly oper-\nation as mentioned previously. As long as the score data\nitself remains unchanged, data related to performance and\noverall structure (repeats etc.) are kept outside the score\nitself and are quickly and easily accessible and modiﬁable.\nThis is useful in applications needing some form of audio-\nto-score alignment [19].\n4. STRUCTURE AND DESIGN\n4.1 Indexing collections\nA score contains a number of parts, a part contains a num-\nber of measures, a measure contains a number of notes: it\nFigure 1 . Flowchart: from real time to score timestamps\nis clear that collections are an important feature in scores.\nThese collections often need to be accessible in multiple\nways. For fast direct access, a vector-like construction\nis ideal: accessing an element is then done in constant\ntime. But a standard XML parser will store elements in\na tree structure. In this tree, notes can be interleaved with\nother data (like harmony indications, certain dynamics el-\nements), and are not necessarily ordered on timestamp.\nTo gain fast access to the most important data, we im-\nplement indices in the data structure, sorted on the criteria\nwe wish to use for retrieval. We obtain fast access to notes\nby, during parsing, storing them in a map keyed on the\ntimestamp that it has been given. When we need to search\nfor a certain note occurring at a certain time, we can easily\nretrieve it (a map generally uses a binary search), and it-\nerating over all notes in order can still be done in constant\ntime using the best ﬁt iterators.\nUsing sorted indexes to access certain structures intro-\nduces:\n•searching for a certain element can be done in O(log(n))\ninstead of O(n).\n•accessing a speciﬁc element takes O(log(n)) for maps\nand O(1) for arrays.\n•when a change occurs, the indexes might need to be\nupdated. The cost depends on the operation being\n665Poster Session 4\nperformed and the data structure used for the index.\nInsertion or deletion on a map takes only O(log(n)),\nbut if resorting an array is needed, the penalty is O(n\nlog(n)).\nImplementing collections with these properties can be\ndone by designing proper templates for them - [20] is an\nexcellent resource on this. Templates are speciﬁcally meant\nto deﬁne operations and algorithms independent of type,\nand can thus be used for any type of element, while still\nmaking specializations based on type possible. Operator\noverloading is a useful programming trick to add additional\naccessing functionality.\n4.2 Cursors and Listeners\nFor browsing through a score, an iterator system is most\nelegant. Most programmers are very familiar with this\nkind of interface. Preferably, a score iterator for real-time\nuse corresponds to a position marker on a sequencer track,\nwe’ll therefore call them cursors. The cursor needs ac-\ncess to the tempo and structure information from the score.\nMultiple cursors on a single score are an asset, but to avoid\ndiscrepancies when multiple cursors are used, only one\nstructure table and tempo function should exist for each\nscore. The cursor’s internal logic is then responsible for\ntranslating the time information from the sequencer to the\nrelevant position in the score.\nTo keep track of the position in the score, a cursor keeps\ntrack of:\n•its current position in quarter notes (timestamp)\n•for each part, the current measure (this allows for\nmultimetric music)\n•for that measure, the next note that needs to be played.\nIn real-time software, a cursor is moved forward or back-\nward by very small increments. Using the adapted internal\nscore structure, moving the cursor forward comes down to:\n•calculate the target timestamp of the cursor.\n•for each part in the score, repeat the following un-\ntil the next note’s timestamp is scheduled after the\ntarget timestamp:\n–if the next note’s timestamp is before the target\ntimestamp, go to the next note\n–if there are no more notes in the current mea-\nsure, go to the next measure\nThis could be even more simpliﬁed improved upon by\ncollecting all notes of a score together and abstracting away\nthe different measures and parts. But in practice, we found\nthat we often needed to know at a certain moment which\nFigure 2 . Simpliﬁed component diagram of the overall\ndesign\nmeasure that a currently played note was in, and to what\npart it belonged. We consider it easier to keep track of this\ninformation in the cursor and request it from there, then to\nignore it there but later have to obtain it through the score\nanyway. The number of measure crossings and the number\nof parts is also usually limited compared to the number of\nnotes.\nAs mentioned earlier, intermediate levels in the score\nhierarchy, like voices or sections, can exist. Each sup-\nplementary level may add a nested loop to the aforemen-\ntioned cursor moving algorithm, so it is beneﬁcial to keep\nthe number of different levels low. The trade-off between\nremoving intermediate levels, keeping the original struc-\nture and accessing its information, and performance ben-\neﬁts or losses in different scenario’s is difﬁcult to make,\nand in the end the performance is highly dependent on the\nscores used: if a large amount of nested loops is kept, a\nsingle voice melody score will likely still be iterated over\nvery fast, while traversing an orchestral score will go much\nslower. On the other hand, by eliminating too many loops\nwe risk to need to introduce a large amount of intermedi-\nate variables in the cursor to keep track of all the necessary\ninformation, and updating and testing against these also\ntakes time.\nA cursor interface can be easily combined with the im-\nplementation of an observer pattern [9]. That way, when\nthe cursor passes a note, it can notify another part of the\nsoftware and trigger an event. In the observer pattern, one\nor more listeners can be attached to a cursor. The cursor\nthen only needs to notify all of its listeners that an event\nwas encountered. This system can be further generalized\nto enable notiﬁcations to be triggered at whatever element\nthat is encountered in a score, and have them pass data for\nthe listeners.\nA simpliﬁed component diagram of the overall design\nof the score handling library is shown in ﬁg. 2 .\n66610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n5. IMPLEMENTATION\nThe library that we developed is part of a larger project,\nimplemented as VST plugin [12]. It is meant to serve as\nprototyping platform for applications that use both audio\nand scores in real-time hosts. A screenshot is shown in ﬁg.\n3.\nAs a practical example, if we want simple playback, a\ncursor is put on the score at the beginning of the score,\nand incremented in small steps corresponding to the length\nof the audio data in the processing function of the plugin.\nWhen notes are encountered, an event is sent to a spe-\nciﬁc listener, that will take the note and some other nec-\nessary information to generate MIDI events out of it. The\nstart events are sent back to the host, while the stop events\nare stored in a scheduler to be used when they are neces-\nsary. If another event should happen when a note, measure,\ncrescendo, or whatever element in the score is encountered,\na developer would only need to write his/her own listener,\nconnect it to the cursor, and conﬁgure the cursor in such a\nway that it reacts to the element needed.\nTo display the score, we use exactly the same setup,\nonly now the cursor is not moved over a timeline, but over\na frame on the screen. A cursor is set on the position cor-\nresponding with the left viewport boundary as deﬁned by\nthe GUI’s zoom data, scroll data (scrollbars) and window\nplane. When the screen is redrawn, the cursor is moved\nto the right viewport boundary. A listener is notiﬁed at\neach note that is encountered, which draws the note onto\nthe window when necessary. There can be several thou-\nsands of these events triggered to draw a single score when\nzoomed out. Nevertheless we experienced that zooming\nand scrolling go ﬂuently using this design, even if they\nforce several redraws of the screen each second, each time\ngenerating a large ﬂow of events.\nWhile ﬁg. 3 only shows a piano roll representation of\nthe notes in the score, the cursor system combined with\nan implementation of the observer pattern, allows to create\nother visualizations as well. We might add dynamics in-\nformation, incorporate or leave out information about the\ntempo, or leave the notes out and only show rests - the\nlist goes on, and any custom visualization can be created\nbased on the same system that is used for MIDI playback\nor setting parameters in a plugin. In our work we haven’t\ngone that far though, and we will focus in the near future\non the development of real-time plugins using scores for\ne.g. audio-to-score matching, rather than on visualization.\n6. CONCLUSION\nThe use of music scores is not yet commonplace in many\nreal-time applications - usually a MIDI representation is\nused as substitute. In this paper we have presented our ef-\nforts to create a library to enable the use of music scores\nﬁle formats natively in such environments. We have sin-\ngled out the design desicions that were taken in order to\nFigure 3 . GUI of a prototype application, showing a basic\npiano roll representation of Joplin’s Elite Syncopations.\ncome to a performant library. These considerations are ap-\nplicable over the boundaries of ﬁle formats and computer\nlanguages.\nIn the applications that we envision, the beneﬁts of fast\ninformation retrieval from the score and score browsing\noutweigh the slightly increased complexity on rarely used\noperations and the precomputation needed. The design\nconsiderations presented herein ensure that the computa-\ntional load during processing is kept to a minimum.\n7. ACKNOWLEDGMENTS\nThis work was funded by a specialization grant from the\nInstitute for the Promotion of Innovation through Science\nand Technology in Flanders (IWT-Vlaanderen).\n8. REFERENCES\n[1] MIDI Manufacturers Association, Complete MIDI 1.0\nDetailed Speciﬁcation, 2nd ed. , 2001\n[2] Recordare LLC: “MusicXML deﬁnition, version 2.0,”\nAvailable at http://www.recordare.com/xml.html\n[3] M. Good: “Lessons from the Adoption of MusicXML\nas an Interchange Standard,” Proc. XML 2006 Confer-\nence, 2006.\n[4] P. Roland: “The music encoding initiative (MEI),”\nProc. 1st International Conference on Musical Appli-\ncations Using XML , pp. 55–59, 2002.\n[5] P. Roland and J.S. Downie “Recent Developments in\nthe Music Encoding Initiative Project: Enhancing Dig-\nital Musicology and Scholarship,” Proc. 19th Joint In-\nternational Conference of the Association for Comput-\ners and the Humanities and the Association for Lit-\nerary and Linguistic Computing (Digital Humanities\n2007) , pp. 186–189, 2007.\n[6] P. Bellini and P. Nesi: “WEDELMUSIC format: an\nXML music notation format for emerging applica-\ntions,” Proc. 1st International Conference on Web De-\n667Poster Session 4\nlivering of Music (WEDELMUSIC 2001) , pp. 79–86,\n2001.\n[7] P. Bellini, P. Nesi and G. Zoia: “Symbolic music repre-\nsentation in MPEG,” IEEE Multimedia , V ol. 12, No. 4,\npp. 42–49, 2005.\n[8] D. Baggi and G. Haus: “The Concept of Interactive\nMusic: the New Standard IEEE P1599 / MX,” Proc.\n2nd International Conference on Semantic and Digital\nMedia Technologies, (SAMT 2007) , pp. 185–195, 2007\n[9] E. Gamma, R. Helm, R. Johnson and J. Vlissides: De-\nsign Patterns: Elements of Reusable Object-Oriented\nSoftware , Addison-Wesley, Boston, 1994\n[10] GRAME Computer Music Research\nLab: “libmusicxml”, Available at\nhttp://libmusicxml.sourceforge.net/\n[11] Unicode Inc.: “Conversions between UTF32, UTF-\n16, and UTF-8”, Available at http://www.unicode.org\n/Public/PROGRAMS/CVTUTF/\n[12] Steinberg Media Technologies GmbH: “Virtual Studio\nTechnology,” Available at http://www.steinberg.net\n[13] Apple Inc.: “Audio Unit Programming\nGuide” Available at http://developer.apple.com\n/documentation/MusicAudio/Conceptual\n/AudioUnitProgrammingGuide\n/AudioUnitProgrammingGuide.pdf , 2007\n[14] Avid Technology, Inc: “Real Time Audio Suite” Avail-\nable at http://www.digidesign.com\n[15] L. de Soras: “Denormal numbers in ﬂoating\npoint signal processing applications” Available at\nhttp://ldesoras.free.fr/ , 2005\n[16] E. Selfridge-Field: Beyond MIDI: the handbook of mu-\nsical codes , MIT Press, Cambridge, MA, 1997\n[17] World Wide Web Consortium (W3C): “XQuery\n1.0: An XML Query Language - W3C Rec-\nommendation 23 January 2007” Available at\nhttp://www.w3.org/TR/xquery/\n[18] J. Ganseman, P. Scheunders and W. D’haes: “Using\nXQuery on MusicXML Databases for Musicological\nAnalysis” Proc. 9th International Conference on Mu-\nsic Information Retrieval (ISMIR) , Philadelphia, PA,\nUSA, 2008.\n[19] H. Heijink, P. Desain and L. Windsor: “Make Me a\nMatch: an evaluation of different approaches to Score-\nPerformance Matching,” Computer Music Journal ,\nV ol. 24, No. 1, pp. 43–56, 2000.\n[20] D. Vandevoorde and N.M. Josuttis: C++ Templates:\nthe complete guide , Addison-Wesley, Boston, 2002\n668"
    },
    {
        "title": "Body Movement in Music Information Retrieval.",
        "author": [
            "Rolf Inge Godøy",
            "Alexander Refsum Jensenius"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416668",
        "url": "https://doi.org/10.5281/zenodo.1416668",
        "ee": "https://zenodo.org/records/1416668/files/GodoyJ09.pdf",
        "abstract": "We can see many and strong links between music and human body movement in musical performance, in dance, and in the variety of movements that people make in listening situations. There is evidence that sensations of human body movement are integral to music as such, and that sensations of movement are efficient carriers of information about style, genre, expression, and emotions. The challenge now in MIR is to develop means for the extraction and representation of movement-inducing cues from musical sound, as well as to develop possibilities for using body movement as input to search and navigation interfaces in MIR.",
        "zenodo_id": 1416668,
        "dblp_key": "conf/ismir/GodoyJ09",
        "keywords": [
            "music",
            "human body movement",
            "dance",
            "listening situations",
            "MIR",
            "movement-inducing cues",
            "body movement",
            "search and navigation interfaces",
            "MIR",
            "information"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nBODY MOVEMENT IN MUSIC INFORMATION RETRIEVAL\nRolf Inge Godøy\nFourMs\nDepartment of Musicology\nUniversity of Oslo\nr.i.godoy@imv.uio.noAlexander Refsum Jensenius\nFourMs\nDepartment of Musicology\nUniversity of Oslo\na.r.jensenius@imv.uio.no\nABSTRACT\nWe can see many and strong links between music and hu-\nman body movement in musical performance, in dance,\nand in the variety of movements that people make in lis-\ntening situations. There is evidence that sensations of hu-\nman body movement are integral to music as such, and\nthat sensations of movement are efﬁcient carriers of infor-\nmation about style, genre, expression, and emotions. The\nchallenge now in MIR is to develop means for the extrac-\ntion and representation of movement-inducing cues from\nmusical sound, as well as to develop possibilities for using\nbody movement as input to search and navigation inter-\nfaces in MIR.\n1. INTRODUCTION\nThere are strong links between music and body movement:\nPerformers produce sound through movements, and listen-\ners very often move to music, as can be seen in dance\nand innumerable everyday listening situations. The links\nbetween music and body movement have been discussed\nsince antiquity, but it is mostly in the last decade that we\nhave seen more systematic research efforts on this topic\nwithin ﬁelds such as music technology, music performance,\nand music cognition [1–3]. Despite this rapidly growing\nresearch in various music-related ﬁelds, the idea of body\nmovement as an integral and ubiquitous part of both per-\nformance and perception of music seems so far not to have\nhad many consequences for music analysis, music theory,\nand music information retrieval. Based on a quick survey\nof papers from recent ISMIR conferences as well as on\nthe overview in [4], the papers that directly or indirectly\nare concerned with body movement seem limited to a few\non query by humming and tapping, as well as some on\nbeat tracking and tempo induction. Also, a cross-check on\nGoogle Scholar showed that out of 4670 hits on MIR, 3730\nincluded “audio”, 1990 “MIDI”, while only 21 included\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.“body movement”.1It seems fair then to conclude that\nbody movement has not been an important topic in MIR\ncontexts.\nBased on our own and various international colleagues’\nwork of the past decade, we believe that body movement\nis not just something that incidentally co-occurs with mu-\nsic, but that body movement is integral to music as a phe-\nnomenon. We would go so far as to claim that our expe-\nrience of music is based on the combination of sound and\nmovement sensations, hence that music is a fundamentally\nembodied phenomenon [5,6]. With such an understanding\nof music, it also becomes clear that sensations of music-\nrelated body movements are in fact highly salient features\nof music, and should be considered alongside various sonic\nfeatures, e.g. pitch, melody, harmony, and timbre. Explor-\ning music-related body movement then becomes an urgent\ntask also in relation to MIR, and in this paper we shall try\nto give an overview of the kinds of body movement that\ncould be of interest in MIR and how they can be studied.\nFinally, we shall present some suggestions for how body\nmovements could be used in interfaces for the search and\nretrieval of music information.\n2. MUSIC-RELATED MOVEMENT\nIt seems that listeners associate different kinds of body\nmovement with the music they hear, or merely imagine.\nHere it can be useful to start by making the general distinc-\ntion between sound-producing andsound-accompanying\nmovements. Although this distinction may not always be\nso clear-cut, sound-producing movements are those that\ncontribute to the production of musical sound, and sound-\naccompanying movements are those that are made in re-\nsponse to the sound being heard [3].\nSound-producing movements may further be divided into\nexcitatory movements such as hitting, bowing, blowing,\nandmodulatory movements such as those for making a vi-\nbrato or various timbral nuances. Associated with sound-\nproducing movements we also have various types of sound-\nfacilitating ,expressive , and communicative movements, mean-\ning movements that are not strictly speaking sound-producing\nbut still play an important role in music performance. Sound-\naccompanying movements, on the other hand, are all kinds\nof movements that people may make to music such as in\n1Search conducted 21 April 2009 using Google Scholar in English,\nand with a syntax of “Music Information Retrieval” + “. . . ”.\n45Oral Session 1-B: Performance Recognition\ndancing, marching/walking, swaying, and gesticulating.\nIn practice, we may often see these different movement\ntypes occur together: it is possible to make movements that\npartly reﬂect the sound-production, partly are more inde-\npendent of the sound-production, e.g. mimicking a solo\ndrum passage with the hands at the same time as swaying\nthe whole body to the meter of the music. We may also\nsee performers making movements that are partly neces-\nsary for producing sound, and partly more theatrical for the\nbeneﬁt of the audience, e.g. lifting the hand high up before\nstriking a chord on a guitar. This means that music-related\nmovements may be multi-functional in that they serve sev-\neral different purposes at the same time.\nWe believe that musical sound itself also conveys salient\nmovement images that are related to listeners’ sensations\nof effort (tense, relaxed, fast, slow, etc.) as well as to\nkinematics or geometry of musical instruments (register,\nup/down, position, posture, etc. in relation to instruments).\nStudies of so-called ‘air-instrument’ performance such as\n‘air guitar’, ‘air drums’, and ‘air piano’ suggest that even\nlisteners with little or no formal musical training are able\nto have images of sound-producing movements that re-\nproduce both the effort and the kinematics of the imag-\nined sound-production actions, i.e. they manage to follow\nthe spatiotemporal unfolding of instrumental performance\nquite well as if they were actually playing the music them-\nselves [7].\nAs for various kinds of sound-accompanying movement\nafforded by musical sound, a study of ‘free dance’ to mu-\nsic2shows that professional dancers tend to agree when it\ncomes to the sensation of effort or energy in dance move-\nments, although there are variations in the kinematics (ge-\nometry) of the movements [8, 9]. Furthermore, studies\nof ‘sound-tracing’ show that listeners with variable lev-\nels of musical training (ranging from none to professional\nlevel training) also seem to spontaneously associate var-\nious shapes with the musical sound that they hear [10].\nIn these studies, listeners were asked to draw on a digi-\ntal tablet the shape they associated with a sound fragment\nimmediately after they had heard the fragment. Figure 1\nshows the sound-tracings of 9 participants to a sound taken\nfrom the contemporary western music repertoire. This sound\nconsists of a high-pitched attack on a triangle, followed by\na downward glissando on strings, and ending up with a\ndrum roll [11]. The excerpt is rather unconventional with\nregards to melodic, harmonic, and timbral features, but as\nwe can see from the images of the sound-tracings, there\nstill seems to be some level of consensus between the nine\nlisteners as to the movement shape that was afforded by the\nsound.\n3. GLOBAL-LOCAL\nIt does not seem farfetched to suggest that listeners’ music-\nrelated movements often match well the overall motion\nand emotion features of the musical sound, e.g. calm mu-\nsic tends to induce calm movements, agitated music tends\n2The only instruction given was to make spontaneous movement to\nthe musical excerpts upon ﬁrst hearing.\nFigure 1 . Sound-tracings by nine listeners of the sound\nfragment built up of an initial triangle attack, a downward\nglide in the strings and a ﬁnal drum roll (spectrogram at\nthe bottom) [11].\nto induce agitated movements, accentuated music tends to\ninduce jerky movements, etc. The details of the move-\nments may vary, however, something that may be seen both\nfrom qualitative annotations [8], as well as from quantita-\ntive data. An example of the latter may be seen in how\nthequantity of motion seems to correlate quite well with\nthe dynamics of the waveform of the sound [7]. Similarly,\nmotiongrams3are useful for displaying movement from\nvideo material. Figure 2 shows an example of how a mo-\ntiongram of the hand movements of a pianist can be used\ntogether with the spectrogram of the resultant sound to\nstudy relationships between movement features and sonic\nfeatures in a 20 seconds excerpt from the last movement of\nBeethoven’s Tempest Sonata.\nVisual representations such as motiongrams and spec-\ntrograms make it possible to move between global and more\nlocal perspectives, i.e. facilitates the correlation of music-\nrelated movement at different timescales with correspond-\ning sonic features at different timescales. Here it could\nbe useful to identify three different timescale levels when\nstudying sound and movement in music:\nSub-chunk level: the level of perceiving continuous sound\n(pitch, timbre, and intensity) and movement (loca-\ntion, force, etc.).\nChunk level: sound fragments and actions that are per-\nceived holistically and that may allow for the percep-\ntion of rhythmical, textural, and melodic patterns, as\nwell as tonal/modal and harmonic features, and im-\nportantly, also expressive features.\n3A motiongram is a visual representation of movement in a video,\ncreated by spatially reducing frame-differenced video images, see [9] for\ndetails\n4610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 2 . Motiongram of hand movement (top) and spectrogram (bottom) of the corresponding sound in a 20 seconds\nexcerpt (ﬁrst 30 measures) from the last movement of Beethoven’s Tempest Sonata performed by Franc ¸ois-Ren ´e Duchable\n[12]. Notice the correlation between hand movements and the sound, as well as the sway in the upper body.\nSupra-chunk level: several chunks are concatenated into\nlarger-scale entities such as whole sections, tunes,\nmovements, and even whole works.\nWe believe that the chunk-level, in the range of approxi-\nmately 0.5 to 5 seconds, may be seen as the most important\nfor identiﬁcation of musical style, mode of performance, as\nwell as emotive features. As suggested by Pierre Schaef-\nfer’s work on sonic objects several decades ago [13,14] and\nrecently by work on more traditional western music [15],\nthe chunk level seems to be more important than larger\nscale levels in music. Interestingly, and probably not ac-\ncidentally, the temporal size of basic action units ﬁts well\nwith that of sonic objects, as well as with various other\nconstraints on attention and memory, see [16] for a sum-\nmary.\nFrom what emerges of the sound-movement correspon-\ndences mentioned above, we think it is plausible to think\nofgestural-sonic objects in music [17]. This means multi-\nmodal units that combine sound and movement so that in\naddition to various sonic features we also have movement\nfeatures such as proprioceptive, haptic, and visual images\nof trajectories and postures. This also means that there\nare movement-related schemata and constraints at work in\ngestural-sonic objects, i.e. various biomechanical and neu-\nrocognitive constraints such as limits to speed of move-\nment, need for rests, etc., as well as the phenomena of\nphase transition and of coarticulation . Phase transitions\nmean that the speed of movement will lead to differentgroupings, e.g. speeding up will at some tempo threshold\nlead to fusion of pulses into a higher order pulse, slowing\ndown will at some tempo threshold lead to ﬁssion of pulses\ninto subdivision pulses. Coarticulation means that other-\nwise distinct sounds and movements will be hierarchically\nsubsumed and contextually smeared so as to produce new\nemergent sensations, e.g. otherwise singular tone-events\nand movements fuse into superordinate phrases and move-\nment shapes. Coarticulation seems to be one of the most\nimportant elements in the formation of chunks, and fur-\nthermore, concerns both the generation and the perception\nof musical sound [16].\nGestural-sonic images may be ﬂexible, both with re-\nspect to resolution or acuity of detail, and with respect to\ngenerality by the principle of so-called motor equivalence .\nMotor equivalence means that motor images of singular\nactions may be generalized so as to encompass different\nversions of the action, allowing transfers and at the same\ntime preserve basic cognitive schemata across variations.\nAn example this is how the general category of ‘hitting’\nis applicable to all percussion instrument actions, with or\nwithout mallets, as well as to all keyboard and struck string\ninstruments.\n4. TYPOMORPHOLOGY OF GESTURAL-SONIC\nOBJECTS\nWith chunk-level gestural-sonic objects as the basic local\nfocus, we can differentiate various types as well as var-\n47Oral Session 1-B: Performance Recognition\nious features of such objects. Following the pioneering\nwork of Pierre Schaeffer [13,14], we can proceed in a top-\ndown manner starting with depicting the global features\nof sonic objects and proceed on to successively ﬁner dif-\nferentiations of features. The main principle for Schaef-\nfer was the subjective images of sonic objects, and where\nestablishing correlations between these subjective images\nand the acoustic substrate of the sonic objects was seen\nas a long-term goal. It is also important to keep in mind\nthat the ambition of Schaeffer was a universally applicable\ntheory, equally valid for sonic objects in electroacoustic,\ninstrumental, or vocal music, and applicable across differ-\nent genres and musical cultures. Hence, such an approach\ncould be seen as very much in accordance with a more\nopen-ended, universal approach to MIR.\nFor a start, Schaeffer suggested three main classes of\nsounds based on their mode of production:\nImpulsive: sounds that have a percussion like quality with\na sudden onset followed by a decay, i.e. a discontin-\nuous transfer of energy such as in hitting or kicking.\nSustained: a continuous transfer of energy so that the sound\nwould be more or less stable throughout its duration\nsuch as in bowing, stroking, or blowing.\nIterative: sounds produced by a rapid series of impulses\nsuch as in a drum roll or in a tremolo.\nIt is the energy envelope of the sound that reﬂects the\nunderlying assumed mode of sound-production, hence, that\nthese sonic object types are transducers of movement infor-\nmation. This movement information can also be applied\nto pitch-related information with the following three main\ntypes:\nPitched: a more or less clearly perceptible and stable pitch\nthroughout the duration of the sonic object.\nNon-pitched: inharmonic or variably noise-dominated sounds\nwith ambiguous or unclear pitch.\nVariable: sensation of pitch that varies throughout the sonic\nobjects, e.g. by glissando or vibrato.\nSchaeffer combined these three pitch-related types with\nthe three dynamic envelope types mentioned above into a\n3 x 3 matrix of basic sonic objects in what he called the ty-\npology . The typology of sonic objects was a ﬁrst and rough\ncategorization to be followed by a more detailed depiction\nof features in what was called the morphology of the sonic\nobjects. The morphology is basically concerned with the\n‘internal’ features of the sonic objects such as its various\npitch-related, dynamic, and/or timbral evolutions and ﬂuc-\ntuations in the course of time. Two of the most prominent\nfeatures of the morphology are the following:\nGrain: fast ﬂuctuations within the sound such as in the\n‘grainy’ sound of a deep bassoon tone or in a ﬂute\nﬂatterzunge.Motion: slower ﬂuctuations within the sound such as in\nslow ostinato or other textural movements.4\nThese features can be thought of as dimensions of sonic\nobjects, and may also be further differentiated, e.g. the\nspeed and amplitude of the grain ﬂuctuations may be thought\nof as sub-dimensions, and variations in speed and ampli-\ntude may be thought of as further sub-dimensions to these\ndimensions. The exploration of thresholds for different\nfeature values in relation to sound categories is then made\npossible, something that is useful for trying to determine\ncategorical thresholds for salient features of sonic objects,\nhence for sonic features in general in a MIR context.\nThe typology and the morphology of sonic objects can\nbe combined into an analytic system that for short is called\nthetypomorphology of sonic objects. The general strategy\nhere is then that of ﬁrst attaching metaphorical labels to\nperceptually relevant (or salient) features of the musical\nsound, and then proceeding to differentiate various sub-\nfeatures.\nIn summary, we believe that most (if not all) features\nof musical sound may be correlated to some kind of body\nmovement. This is actually the main point of motor the-\nory and embodied cognition, namely that we perceive by\ncorrelating whatever we hear (or see) to mental images of\nmovement [6, 7].\n5. SUGGESTIONS FOR IMPLEMENTATIONS\nGiven the abovementioned documentation of links between\nsound and body movement, the challenge now is to inte-\ngrate our knowledge of such sound–movement links in au-\ndio analysis so that this can be useful in a MIR context.\nSeveral of the features mentioned above can readily be\nfound in audio using traditional analysis techniques. For\nexample, the typological features can be correlated to the\namplitude envelope of a sound signal and/or to the pitch\ncontour or ﬂuctuations in the spectral centroid. Details in\nthe morphology, on the other hand, require more studies\nto be effectively implemented in a machine-based system.\nWhile it could be possible to implement this based on anal-\nysis of the sound alone, we believe that it may be worth-\nwhile to also look at the movement of performers as well\nas listeners when they experience music.\nAs an example, consider the sensation of an undulating\nor even circular motion that we would assume many lis-\nteners would experience in the example illustrated with the\nmotiongram in Figure 2. Although we may ﬁnd consider-\nable variation in the style of playing this piece, one source\nof such an undulating motion could be found in the sound-\nproducing actions of the pianist. To an expert musician it\nmight be natural or even obvious to predict from the score\nthat pianists would tend to make this kind of undulating\nmovements, yet it is an element that we believe could be\ncaptured and included in MIR as a feature of the music.\nFigure 3 shows a graph of the movements of the wrists\nand elbows of a pianist performing the ﬁrst 8 measures\n(with the upbeat ﬁgure) of the same piece as in Figure 2.\n4‘Motion’ is sometimes also rendered as ‘gait’ or ‘allure’ in English.\n4810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nThe graph is based on recordings with an infrared motion\ncapture system and shows the markers’ displacement along\nthe keyboard (i.e. the horizontal plane). This is of course\na crude simpliﬁcation of the richness of the performance,\nyet we believe it does convey the salient feature of the un-\ndulating motion of this piece.\n!\n!\n!\nFigure 3 . Trajectories of the wrists and elbows of a pianist\nperforming the ﬁrst 8 measures (and the upbeat measure)\nof the same Beethoven example as in Figure 2. The marked\nonset points are recorded from MIDI output from the digi-\ntal piano used in the study.\nMoving towards the analysis of body movement in a\nMIR context necessitates techniques to represent, store and\nnavigate such movement data. We are here thinking about\nrepresentations of data in many different forms, e.g.:\n•Continuous data from various types of motion cap-\nture systems.\n•Graphical representations of movement, both static\nand animated.\n•Analyzed movement and gesture data in a structured\nand symbolic form.\n•Various verbal movement metaphors.\nAlthough there exist formats and standards that handle\nthese types of data in other ﬁelds than music, we believe\nit is necessary to develop solutions that are speciﬁc to mu-\nsical applications [18]. One of the most important parts\nhere is to handle synchronisation between movement data,\naudio, video, MIDI, etc. We are not aware of any solu-\ntions that handle this issue in its full complexity, so for that\nreason we are currently developing the Gesture Descrip-\ntion Interchange Format5(GDIF) as a system for stream-\ning and storing motion capture data [19]. Equally impor-\ntant here is to work out a set of movement descriptors, and\nsound–movement descriptors, that are useful in a MIR con-\ntext.\nAlso, considering that a substantial amount of music is\nreadily available as audiovisual material (e.g. music videos\nof various kinds), this could be exploited if there were\nmore readily available methods for analyzing both audio\nand video, and most importantly, for analyzing the rela-\ntionships between features extracted from audio and video.\n5http://www.gdif.orgThis could then take into account the cross-modal interac-\ntions happening in our perception of audiovisual material,\nas documented in e.g. [20].\nFinally, including an embodied perspective in MIR re-\nsearch could also open for new applications of search and\nretrieval of music through body movement. Using various\ntypes of motion capture techniques, ranging from camera-\nbased to sensor-based systems, users could explore a large\nmusic collection through body movement. While this could\ncertainly be done in low-dimensional features spaces, we\nbelieve that systems that manage to connect complex body\nmovements to complex sound features will open for new\nand exciting ways of exploring the multidimensionality of\nmusical sound, e.g. as implemented in software for con-\ncatenative synthesis [21]. Considering the positive results\nof the studies of air-performance and sound-tracing as men-\ntioned above, this is something that both novices and ex-\nperts should be able to do without a too high learning thresh-\nold.\nIt could be useful to regard music-related body move-\nment as a link between otherwise separate elements in west-\nern musical thought: the acoustic signal, symbolic nota-\ntion, and higher level aesthetic and semiotic signiﬁcations\nof music. This is because music-related body movement\nmay encompass all these elements at once: On one side\nthe continuous body movement relates to the continuous\nacoustic signal, with sound-producing movements incor-\nporating the tone events of notational symbols, and with\nvarious types of expressive features in the movement touch-\ning on aesthetic and semiotic elements. On the other side,\nmusic-related body movement contain valuable informa-\ntion of the musical experience that is not present in the\naudio itself, but which is often available in video material\naccompanying the sound.\n6. CONCLUSIONS\nAlthough we still have a long way to go in exploring music-\nrelated body movement and its relationship to musical sound,\nit seems that we already have reasonable grounds for claim-\ning that sensations of body movement are essential in mu-\nsical experience. Actually, we would even claim that sen-\nsations of body movement are one of the most salient fea-\ntures of musical style and genre, and could for this reason\nalone be an important element in the development of MIR.\nWhen we rather optimistically believe that music-related\nbody movement has great (and mostly untapped) poten-\ntial for MIR, we are also acutely aware of great challenges\nhere, challenges that may be summarized as follows:\n•Development of signal processing methods for ex-\ntracting movement-inducing cues from audio.\n•Development of video processing methods for ex-\ntracting features of music-related body movement.\n•Development of taxonomies and formats for han-\ndling such multimodal features in MIR systems.\n49Oral Session 1-B: Performance Recognition\n•Development of solutions for using body movement\nin searching, retrieval, and navigation in audio or au-\ndiovisual music ﬁles.\nOn the way to this, we need to continue working on\nwhat movement sensations listeners have to music, painstak-\ningly building up our knowledge of subjective movement\nsensations and correlating these with lower-level signal-\nbased features of musical sound.\n7. REFERENCES\n[1] M. M. Wanderley and M. Battier, eds., Trends in Ges-\ntural Control of Music [CD-ROM] . Paris: IRCAM –\nCentre Pompidou, 2000.\n[2] A. Gritten and E. King, eds., Music and Gesture .\nHampshire: Ashgate, 2006.\n[3] R. I. Godøy and M. Leman, Musical Gestures: Sound,\nMovement, and Meaning . New York: Routledge, 2009\n(in press).\n[4] J. S. Downie, “The music information retrieval eval-\nuation exchange (2005–2007): A window into music\ninformation retrieval research,” Acoust. Sci. & Tech ,\nvol. 29, no. 4, pp. 247–255, 2009.\n[5] R. I. Godøy, “Motor-mimetic music cognition,”\nLeonardo , vol. 36, pp. 317–319, August 2003.\n[6] M. Leman, Embodied Music Cognition and Mediation\nTechnology . Cambridge, MA: The MIT Press, 2007.\n[7] R. I. Godøy, E. Haga, and A. R. Jensenius, “Play-\ning ‘air instruments’: Mimicry of sound-producing\ngestures by novices and experts,” in Gesture in\nHuman-Computer Interaction and Simulation, GW\n2005 (S. Gibet, N. Courty, and J.-F. Kamp, eds.),\nvol. LNAI 3881, pp. 256–267, Berlin: Springer-Verlag,\n2006.\n[8] E. Haga, Correspondences between music and body\nmovement . PhD thesis, University of Oslo, 2008.\n[9] A. R. Jensenius, Action–Sound : Developing Meth-\nods and Tools to Study Music-Related Body Movement .\nPhD thesis, University of Oslo, 2007.\n[10] R. I. Godøy, E. Haga, and A. R. Jensenius, “Explor-\ning music-related gestures by sound-tracing - a prelim-\ninary study,” in Proceedings of the COST287-ConGAS\n2nd International Symposium on Gesture Interfaces for\nMultimedia Systems (K. Ng, ed.), (Leeds), pp. 27–33,\n2006.\n[11] P. Schaeffer, “Sound fragment from cd3, track 13, 20”-\n29” (no original source indicated) in [14],” in Solf`ege\nde l’objet sonore , Paris: (with sound examples by G.\nReibel & B. Ferreyra), INA/GRM, 1998, ﬁrst pub-\nlished in 1967.[12] F.-R. Duchable, “Beethoven Concertos pour piano 1\nand 3. A la decouverte des Concertos. Franc ¸ois-Ren ´e\nDuchable, piano, John Nelson, conductor, Ensemble\nOrchestral de Paris.” [DVD] Harmonia Mundi, 2003.\n[13] P. Schaeffer, Trait ´e des objets musicaux . Paris: Edi-\ntions du Seuil, 1966.\n[14] P. Schaeffer, Solf`ege de l’objet sonore . Paris: (with\nsound examples by G. Reibel & B. Ferreyra),\nINA/GRM, 1998, ﬁrst published in 1967.\n[15] Z. Eitan and R. Granot, “Growing oranges on mozart’s\napple tree: ‘inner form’ and aesthetic judgment,” Music\nPerception , vol. 25, no. 5, pp. 397–417, 2008.\n[16] R. I. Godøy, “Reﬂections on chunking,” in Systematic\nand Comparative Musicology: Concepts, Methods,\nFindings. Hamburger Jahrbuch f ¨ur Musikwissenschaft\n(A. Schneider, ed.), vol. 24, pp. 117–132, Vienna: Pe-\nter Lang, 2008.\n[17] R. I. Godøy, “Gestural-sonorous objects: embodied ex-\ntensions of Schaeffer’s conceptual apparatus,” Organ-\nised Sound , vol. 11, no. 2, pp. 149–157, 2006.\n[18] A. R. Jensenius, A. Camurri, N. Castagne, E. Maestre,\nJ. Malloch, D. McGilvray, D. Schwarz, and M. Wright,\n“Panel: the need of formats for streaming and storing\nmusic-related movement and gesture data,” in Proceed-\nings of the 2007 International Computer Music Confer-\nence, (Copenhagen, Denmark), pp. 13–16, 2007.\n[19] A. R. Jensenius, K. Nymoen, and R. I. Godøy, “A\nmultilayered GDIF-based setup for studying coarticu-\nlation in the movements of musicians,” in Proceedings\nof the 2008 International Computer Music Conference ,\n(Belfast), pp. 743–746, 2008.\n[20] B. Vines, C. Krumhansl, M. Wanderley, and D. Levitin,\n“Cross-modal interactions in the perception of musical\nperformance,” Cognition , vol. 101, pp. 80–113, 2005.\n[21] D. Schwarz, G. Beller, B. Verbrugghe, and S. Britton,\n“Real-time corpus-based concatenative synthesis with\nCatart,” in Proceedings of the 9th Int. Conference on\nDigital Audio Effects (DAFx-06) , (Montreal), 2006.\n50"
    },
    {
        "title": "Music and Geography: Content Description of Musical Audio from Different Parts of the World.",
        "author": [
            "Emilia Gómez",
            "Martín Haro",
            "Perfecto Herrera"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416010",
        "url": "https://doi.org/10.5281/zenodo.1416010",
        "ee": "https://zenodo.org/records/1416010/files/GomezHH09.pdf",
        "abstract": "This paper analyses how audio features related to different musical facets can be useful for the comparative analysis and classification of music from diverse parts of the world. The music collection under study gathers around 6,000 pieces, including traditional music from different geographical zones and countries, as well as a varied set of Western musical styles. We achieve promising results when trying to automatically distinguish music from Western and non-Western traditions. A 86.68% of accuracy is obtained using only 23 audio features, which are representative of distinct musical facets (timbre, tonality, rhythm), indicating their complementarity for music description. We also analyze the relative performance of the different facets and the capability of various descriptors to identify certain types of music. We finally present some results on the relationship between geographical location and musical features in terms of extracted descriptors. All the reported outcomes demonstrate that automatic description of audio signals together with data mining techniques provide means to characterize huge music collections from different traditions, complementing ethnomusicological manual analysis and providing a link between music and geography.",
        "zenodo_id": 1416010,
        "dblp_key": "conf/ismir/GomezHH09",
        "keywords": [
            "audio features",
            "music analysis",
            "diverse parts of the world",
            "music collection",
            "traditional music",
            "Western musical styles",
            "automatic classification",
            "geographical zones",
            "ethnomusicological manual analysis",
            "data mining techniques"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMUSIC AND GEOGRAPHY:\nCONTENT DESCRIPTION OF MUSICAL AUDIO\nFROM DIFFERENT PARTS OF THE WORLD\nEmilia G ´omez, Mart ´ın Haro, Perfecto Herrera\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n{emilia.gomez,martin.haro,perfecto.herrera }@upf.edu\nABSTRACT\nThis paper analyses how audio features related to different\nmusical facets can be useful for the comparative analy-\nsis and classiﬁcation of music from diverse parts of the\nworld. The music collection under study gathers around\n6,000 pieces, including traditional music from different\ngeographical zones and countries, as well as a varied set\nof Western musical styles. We achieve promising results\nwhen trying to automatically distinguish music from West-\nern and non-Western traditions. A 86.68% of accuracy is\nobtained using only 23 audio features, which are represen-\ntative of distinct musical facets (timbre, tonality, rhythm),\nindicating their complementarity for music description. We\nalso analyze the relative performance of the different facets\nand the capability of various descriptors to identify certain\ntypes of music. We ﬁnally present some results on the\nrelationship between geographical location and musical\nfeatures in terms of extracted descriptors. All the reported\noutcomes demonstrate that automatic description of au-\ndio signals together with data mining techniques provide\nmeans to characterize huge music collections from differ-\nent traditions, complementing ethnomusicological manual\nanalysis and providing a link between music and geogra-\nphy.\n1. INTRODUCTION\nMost of existing Music Information Retrieval (MIR) tech-\nnologies and systems focus on mainstream popular music\nfrom the so-called ”Western tradition”. The term Western\nis generally employed to denote most of the cultures of\nEuropean origin and most of their descendants. The un-\navailability of scores for most musical traditions makes\nnecessary to work with audio recordings, and some re-\ncent works have studied if the available techniques and\ndescriptors for audio content description are suitable when\nanalyzing music from different traditions [12].\nWe provide in [3] an initial contribution in this direc-\ntion, with the goal of analyzing the descriptive power of\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.tonal features to discriminate Western vs non-Western mu-\nsic material. These tonal features are derived from chroma\nrepresentations, computed using an interval resolution of\n10 bins per semitone and representative of the employed\ntuning system and gamut. We found that tonal descriptors\nwere able to distinguish these two classes with an 80%\naccuracy using different classiﬁers and an independent set\nfor testing. The music collection was made of 1,500 pieces\nfrom different areas of the world. In a similar way, Liu et\nal. have recently performed a study on the classiﬁcation,\nby means of Support Vector Machines, of a music collec-\ntion of 1,300 pieces containing Western classical music,\nChinese and Japanese traditional music, Indian classical\nmusic and Arabic and African folk music [7]. The best\nresult (84.06%) was obtained using timbre features, and\nthe results for standard chroma features was very low. This\nmight indicate that one semitone resolution is not accurate\nenough to represent non-equal tempered scales and gamuts\nfound in various cultures.\nThe goals of this paper can be summarized as follows:\nﬁrst, to analyze the contribution of the different facets of\nmusic description (timbre, rhythm, tonality) for the au-\ntomatic classiﬁcation of Western vs non-Western music;\nsecond, to evaluate the validity of the different features to\ncharacterize certain types of music; and third, to inves-\ntigate the relationship between extracted descriptors and\ngeographical location of the analyzed pieces (latitude and\nlongitude). In order to do that, we have gathered a music\ncollection covering traditional music from different geo-\ngraphical zones and countries as well as a varied set of\nWestern musical styles. Up to our knowledge, the relation-\nship between geography and extracted descriptors has not\nbeen addressed in any previous existing piece of literature,\nand the present study provides an attempt in this direction.\n2. METHODOLOGY\n2.1 Music collection\nFor this study, we gathered a music collection comprising\n5,905 pieces from different musical traditions and styles.\nThey were manually divided into Western and non-Western\ncategories and labelled according to the musical genre and\ngeographical location (area and country).\nFor non-Western music, we gathered a total of 3,185\naudio recordings distributed by geographical region, as\n753Oral Session 9-B: Sociology and Ethnomusicology\nFigure 1 . Distribution of the music collection.\ndeﬁned by UNESCO1. They were distributed among the\ndifferent countries and labelled according to the country of\norigin and geographical region. We deﬁned the categories\nPaciﬁc, Greenland, Central Asia, Asia, Arab States and\nAfrica . These samples contain representative recordings\nof traditional music from different countries, discarding\nthose having some Western inﬂuence (e.g. equal-tempered\ninstruments). They were extracted from CD collections\nused for ethnomusicological studies (ﬁeld recordings and\ncompilations of traditional music).\nWe also considered 2,720 recordings from Western mu-\nsic assigned by UNESCO to the region Europe and North\nAmerica . A set of this data was gathered from commercial\nCDs and is scattered across different musical genres (alter-\nnative, blues, classical, country, disco, electronica, folk,\nfunk, hip-hop, jazz, metal, pop, reggae, rock and soul).\nA subset of the ”Western” collection that was chosen has\nbeen widely used within the MIR community [6, 10, 11].\nWe also added a collection of traditional music from West-\nern countries (Europe and American folk). This data was\nlabelled according to country of origin and musical genre.\nFigure 1 shows the class distribution of the music col-\nlection under study. For Western music, we have distin-\nguished between three main classes: classical, traditional\nmusic and a general class called modern that groups the\nremaining musical genres. For non-Western material, we\nhave grouped the different countries into the mentioned\ncategories. As it can be seen in the ﬁgure, classes are\nnot equally distributed. One reason for that is the vari-\nability of pieces available to our analysis, which made\nit very hard to ﬁnd the same number of excerpts for all\nthe considered countries (e.g. we only found around 10\npieces for countries such as Vanuatu, Oman, Zimbabwe or\nTanzania while the number of pieces for traditional music\nof European countries had to be restricted to 90 excerpts\nper country). On the other hand, geographical regions\n1http://portal.unesco.org/geographydiffer on the number of countries and musical traditions.\nFor instance, there were few recordings from Greenland\ncompared to the different styles present in Asia (includ-\ning Indian music for instance). We will minimize the\nimpact that this might have in the classiﬁcation problem\nby balancing the distribution of Western vs non-Western\nmaterial.\nFor this study we analyzed the ﬁrst 30 seconds of each\nmusical piece, and we discarded few non representative\nparts containing silences or ambiguous introductions (the\nmusic on these introductions was not related to the overall\ncontent of the piece).\n2.2 Feature extraction\nA main goal of this study is to provide a multi-faceted de-\nscription of the music collection and compare the relative\nperformance of different musical facets (tonal, timbre and\nrhythm) for comparative analysis of music from around the\nworld. In order to do that, extracted audio features are\nrelated to these different facets:\nTonality : tonal features are related to the pitch class\ndistribution of a piece, its pitch range or tessitura and the\nemployed scale and tuning system. The features in this\ngroup include the tuning frequency , which estimates the\nfrequency used to tune a musical piece if we consider\nan equal-tempered scale. This feature is expected to be\nclose to zero for pieces tuned in this temperament. High-\nresolution pitch class distributions are also obtained as the\nHarmonic Pitch Class Proﬁle (HPCP), computed with a\nresolution of 10 bins per semitone and averaged for the\nanalyzed segment. We also obtain a ”transposed” version\nof the HPCP that we call the THPCP, by ring shifting\nthe HPCP vector according to the position of the maxi-\nmum value. Some tonal features are then derived from\nthem ( equal-tempered deviation, non-tempered energy ra-\ntioanddiatonic strength ). We ﬁnally consider a dissonance\nmeasure and a descriptor called octave centroid , which\nis obtained from a multi-octave fundamental frequency\nrepresentation and corresponds to the geometry centre of\nthe played pitches. We compute this description on a\nframe basis and then obtain the average and variance for\nthe considered segment. This set of features was used in a\nprevious study [3].\nTimbre : we gather here a standard set of timbre fea-\ntures including loudness, spectral ﬂux, spectral ﬂatness,\nroughness, MFCCs and energy computation in bark bands.\nThese features are computed in a frame basis, and we then\nobtain statistical measures such as maximum and mini-\nmum value, mean and variance. Timbre features are ob-\ntained as explained in [9].\nRhythm : in terms of rhythmic features, we consider\ndifferent attributes such as the estimated global tempo for\nthe analyze excerpt as well as some features obtained from\nInter-Onset Interval (IOI) histograms (peak positions and\nvalues) and onset rate (number of onsets per second). The\nalgorithm for rhythmic feature computation is based on the\nsystem described in [2].\n75410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nDrum : this group is composed by a set of song-level\npercussion descriptors computed from the output of a tran-\nscription system that detects drum kit events (i.e. bass\ndrum, snare drum and hi-hat) [5]. Other instruments sound-\ning like them are probably detected and considered as\nbeing them. These song-level descriptors include: the\nratio between the number of detected events per instrument\nand the total number of onsets (e.g. bass drum/total), the\nratio between the number of instances among instruments\n(e.g. bass drum/hi-hat), the number of detected events\nper minute (e.g. hi-hat/min) and the peak values of the\nhistogram of the inter-instrument intervals.\n2.3 Classiﬁcation algorithms\nWe have approached several classiﬁcation methods but,\nfor the sake of summarization, we only present the results\nobtained for Support Vector Machines (SVM), considered\nas one of the best-performing learning algorithms currently\navailable. We have employed the data mining software\nRapidMiner [8]2, which implements SVM using LibSVM\n[1].\nWe have used a grid search facility available in Rapid-\nMiner to ﬁnd the following optimal values for the kernel\nfunction: linear ( u/prime·v), polynomial ( (γ·u/prime·v+coef 0)degree)\nand radial basis function ( e−γ·|u−v|2).coef 0has been set\nto its default value ( coef 0= 0) and a grid search has been\nrun to ﬁnd the optimal values of kernel type, γ,degree and\nC, which corresponds to the cost parameter that controls\nthe trade off between allowing training errors and forcing\nrigid margins. A soft margin then permits some misclas-\nsiﬁcations. Increasing the value of Cincreases the cost\nof misclassifying points and forces the creation of a more\naccurate model that may not generalize well3. We have\nadopted an evaluation procedure based on 10-fold cross-\nvalidation over equally-distributed classes.\n3. RESULTS\n3.1 Distribution of features\nIn order to have a preliminary idea of the usefulness of\nthe different features for comparative analysis, we provide\nhere some analysis of the feature distributions. Figure 2\nshows the distribution of the tuning descriptor for Western\nand non-Western music. As expected, the distribution of\ntuning deviation with respect to 440 Hz is centered on 0\ncents for Western music and equal distributed between -\n50 and 50 cents for non-Western pieces. We also ﬁnd\nsome differences in other tonal descriptors such as equal-\ntempered deviation, representing the deviation from an\nequal-tempered scale, which also appears to be lower for\nWestern than for non-Western music (see Figure 3).\nWe can also analyze the feature distribution for the\ndifferent geographical areas and musical genres. One ex-\nample is shown in Figure 4, where we represent the dis-\ntribution of the drum/total descriptor, for the different ge-\nographical areas and musical styles. This descriptor rep-\n2http://rapid-i.com\n3http://www.dtreg.com/svm.htm\nFigure 2 . Distribution of tuning frequency (normalized\nvalue) for non-Western (top) and Western music (bottom).\nFigure 3 . Distribution of equal tempered deviation (nor-\nmalized value) for non-Western (top) and Western music\n(bottom).\nresents the presence of drum sounds (or other instruments\nwith similar sound) in the analyzed piece. As expected,\nwe observe that the values for this feature are high for the\nclass modern (including musical genres such as jazz, pop\nand rock) and African music (with a signiﬁcant presence of\npercussive instruments), and are low for classical music.\n3.2 Western vs non-Western classiﬁcation\nOur goal here is to have a classiﬁer that automatically\nassigns the label ”Western” or ”non-Western” to any audio\nﬁle that is input and analyzed with the mentioned features.\nWe are aware of the limitations of the concept of Western\nas opposed to non-Western, as this is a ﬁrst step towards\nthe deﬁnition and formalization of stylistic features proper\nto different kinds of music.\nFor the Western vs non-Western categories, the achieved\n755Oral Session 9-B: Sociology and Ethnomusicology\nFigure 4 . Distribution of the drum/total descriptor for the\ndifferent geographical areas and musical genres.\nclassiﬁcation results for the different feature sets are sum-\nmarized in Table 1. The second row indicates the accuracy\nof timbre features after applying an attribute evaluation\nmethod for feature selection, correlation-based feature se-\nlection (CFS) [4]. This algorithm selects a near-optimal\nsubset of features that have minimal correlation between\nthem, and maximal correlation with the to-be-predicted\nclasses. This procedure was performed 10 times by means\nof a 10-fold cross-validation procedure, and only the tim-\nbre features that were selected more than 8 times were\nconsidered. They include descriptors based on spectral\nMFCCs, Bark-band energy, spectral ﬂux and roughness.\nThe last row indicates the accuracy after applying the\nsame feature selection method to the whole feature set.\nThe feature-selection procedure was performed 10 times\nas well by means of a 10-fold cross-validation procedure,\nand only the features that were selected 10 times were\nconsidered. The selected features include a combination\nof tonal (tuning frequency, deviations from equal tempered\nscale and relative intensity of the fourth and ﬁfth degree of\na diatonic scale), timbre (features derived from MFCCs,\nenergy in bark bands and spectral ﬂux) and drum features\n(number of detected hit-hat and bass-drum per minute).\nAs a general conclusion, we observe that the highest\nclassiﬁcation accuracy, 88.53%, is obtained using timbre\nfeatures. Nevertheless, the number of features for this set\nis very high (176 descriptors). Using a feature selection\nprocedure, the set can be reduced to 25 timbre features\nwith a 83.36% of accuracy. As the non-Western collec-\ntion contains many ﬁeld recordings, we think that timbre\ndescriptors may be related to recording quality instead\nof musical properties of the pieces under study. In this\nregard, we observe that 81.23% of accuracy is obtained\nby using 41 tonal features, and, using the feature selection\nprocedure described above, 23 features from the different\nsets yield to a global accuracy of 86.88%. This descriptor\nset should be considered robust to recording quality.\nWe can also see that the performance for rhythmic and\nFigure 5 . Classiﬁcation errors (%) for timbre descriptors.\nFigure 6 . Classiﬁcation errors (%) for tonal descriptors.\ndrum features is very low, indicating that these descrip-\ntor sets are incomplete to discriminate Western from non-\nWestern material. This was expected because Western\nmusic includes pieces without drums (e.g. classical and\ntraditional music) and without a steady rhythm. Looking\nat the F-measure for Western and non-Western classes, we\ndo not ﬁnd signiﬁcant differences for timbre, tonal and\ndrum features. Nevertheless, we observe that the value of\nF-measure for Western music is more than 10% lower than\nfor non-Western music when using rhythmic features. In\ngeneral, the low performance of rhythmic descriptors may\nsuggest that the implemented features only represent peri-\nodicity and tempo of music with a steady rhythm, but can\nbe insufﬁcient to capture more subtle rhythmical aspects of\nclassical and traditional music.\n3.3 Classiﬁcation accuracy for different musical\ngenres and traditions\nFigures 5 to 8 present the percentage of classiﬁcation errors\nper each class for the different feature sets. We observe\n75610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSet Nb Kernel function parameters\n(type, degree, cost, gamma) Accuracy (%) F-measure W F-measure non-W\nTimbre 176 polynomial, 3, C=8.87, gamma=0.4 88.53 0.8856 0.8850\nTimbre (CFS) 25 polynomial, 3, C=8.87, gamma=0.4 83.36 0.8345 0.8327\nTonal 41 linear, C= 2.14 81.23 0.8152 0.8095\nRhythm 23 linear, C= 2.14 62.02 0.5520 0.6704\nDrum 17 radial basis function, C= 0.0, gamma= 0.4 69.83 0.7036 0.6929\nSelection (CFS) 23 radial basis function, C= 0.0, gamma= 0.4 86.88 0.8600 0.8765\nTable 1 . Accuracy using SVM classiﬁer and a grid search procedure.\nFigure 7 . Classiﬁcation errors (%) for rhythmic descrip-\ntors.\nthat traditional music is the most misclassiﬁed Western\nclass for all the feature sets. The reason for that may be\nthat traditional pieces are closer to non-Western material\nwith respect, for instance, to instrumentation (e.g. a small\nnumber of instruments, similar recording conditions) or\ntonality (e.g. high degree of ornamentation or scales with\ndeviations from equal tuning).\nOn the other hand, we observe that more than 60% of\ntheclassical excerpts are not correctly classiﬁed when us-\ning rhythmic descriptors, and only 25% when using drum\ndescriptor. We can think that these feature sets may con-\nsider Western music as having a constant rhythm and with\na high presence of drum sounds, and that classical pieces\ndiffer from this assumption. We also observe that arabic\nmusic is sometimes labeled as Western when using tonal\nfeatures, as found in [3].\n3.4 Geographical location and feature distance\nWe can also analyze the correspondence between audio\nfeatures and geographical location by studying the geo-\ngraphical distribution of feature values. In order to do that,\nwe have computed a set of statistics over the audio features\nfor the considered countries. Figure 9 shows an example\nof the geographical distribution of the equal-tempered de-\nviation feature. We observe that low values are found in\nEurope, United States and Australia, while higher values\nFigure 8 . Classiﬁcation errors (%) for drum descriptors.\nare found in African and Asian music.\nWe have then studied the correlation of audio features\naverages and the average latitude and longitude coordi-\nnates for each country. In the Pearson correlation com-\nputation, the Bonferroni method was used to adjust the\nobserved signiﬁcance level for the fact that multiple com-\nparisons were made. The following average descriptors\nshowed a low (i.e., 0.3<|r|<0.45) but signiﬁcant\ncorrelation with the geographical coordinates:\nLatitude : 3rd peak of the Inter-Onset Interval histogram,\ntransposed chroma features (2nd, 3rd, 5th, 7th and 9th\nequal-tempered positions), chroma features (7th, 10th and\n12th equal-tempered positions), equal tempered deviation\nand non-tempered energy ratio.\nLongitude : 4th peak of the Inter-Onset Interval his-\ntogram and number of onsets per second.\nFrom the previous list, it is worth to note that latitude\nis mostly associated to tonal features, while longitude is\nmore associated to rhythmic descriptors. We can also\nbuild a regression model for latitude using only the above-\nmentioned 13 signiﬁcant descriptors, yielding a correlation\nvalue of 0.59. Regarding longitude, the correlation is 0.31.\nThese initial observations need to be carefully re-assessed\nin the context of theories that might relate geographical\nand climate variables to constraints on musical instrument\nconstruction or on music-centred social activities.\n757Oral Session 9-B: Sociology and Ethnomusicology\nFigure 9 . Geographical distribution of the equal tempered\ndeviation descriptor (normalized value).\n4. CONCLUSIONS AND FUTURE WORK\nWe have presented an empirical approach to the compar-\native analysis of music audio recordings based on tonal,\ntimbre and rhythmic features using a music collection from\nvarious parts of the world. We tried to automatically dis-\ntinguish music from Western and non-Western traditions\nby means of automatic audio feature extraction and classi-\nﬁcation. An accuracy of 86.68% was obtained for a music\ncollection of around 6,000 pieces, using only 23 features\nfrom different musical facets. This conﬁrms that tim-\nbre and rhythmic descriptors complement high-resolution\ntonal features for the characterization of music from var-\nious cultures. Furthermore, each feature set helped to\ndiscriminate certain types of music (e.g. drum features\nwere suitable to identify pieces from the modern category).\nAs a future work, we would like to extent this analysis\nto more speciﬁc music collections and complement our\ncurrent description from an ethnomusicology perspective.\nIn this regard, we will attempt the clustering of the pieces\nin terms of musical culture. Ideally, we should then be able\nto deﬁne and formalize stylistic features proper to different\ntraditions, and approach genres not just geographically\nbut as a set of traits. We think that this will help to\nreﬁne our descriptors and similarity measures accordingly.\nWe also plan to complement the current collection with\nmusic from the UNESCO region Latin America and the\nCaribbean and explore inﬂuences and ”frontier music”\nwith this procedure.\nAs a ﬁnal consideration, we conclude that existing MIR\ntechniques are of great interest for the comparative study\nof all existing music traditions in the world, and audio\ndescription tools have a great potential to assist in eth-\nnomusicological research. We hope that the present work\ncontributes to the understanding of our musical heritage by\nmeans of computational modeling.\n5. ACKNOWLEDGEMENTS\nThe authors are grateful to Georges Tzanetakis for shar-\ning his genre collection. They also want to thank theircolleagues at the Music Technology Group (UPF) and the\nanonymous reviewers for their helpful comments and sug-\ngestions. This research has been partially funded by the\nEU-IP project PHAROS6 (IST-2006-045035) and the e-\nContent Plus project V ARIAZIONI7 (ECP-2005-CULT-\n038264).\n6. REFERENCES\n[1] Chih-Chung, C. and Chih-Jen, L. “LIBSVM : a library\nfor support vector machines”, 2001. Software available\nat http://www.csie.ntu.edu.tw/ cjlin/libsvm\n[2] Dixon, S. ”Automatic extraction of tempo and beat\nfrom expressive performances”. Journal of New Music\nResearch , 30, pp. 39-58, 2001.\n[3] Gomez, E. and Herrera, P. “Comparative analysis of\nmusic recordings from Western and non-Western tradi-\ntions by automatic tonal feature extraction”, Journal of\nEmpirical Musicology , 3(3), pp. 140-156, 2008.\n[4] Hall, M.A. “Correlation-based feature selection for\ndiscrete and numeric class machine learning”, 7th\nInternational Conference on Machine Learning , pp.\n359-366, San Francisco, CA, USA, 2000.\n[5] Haro, M. and Herrera, P. “From low-level to song-level\npercussion descriptors of polyphonic music”, Inter-\nnational Conference on Music Information Retrieval ,\nKobe, Japan, 2009.\n[6] Holzapfel, A. and Stylianou, Y . “A statistical approach\nto musical genre classiﬁcation using non-negative ma-\ntrix factorization”, IEEE International Conference on\nAcoustics, Speech and Signal Processing , 2, pp. 15-20,\nHonolulu, Hawai, USA, 2007.\n[7] Liu, Y ., Xiang, Q., Wang, Y . and Cai, L. “Cultural\nstyle based music classiﬁcation of audio signals” IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing , Taipei Taiwan, April 2009.\n[8] Mierswa, I.,Wurst, M., Klinkenberg, R., Scholz, M.\nand Euler, T. “YALE: Rapid prototyping for complex\ndata mining tasks”, 12th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining\n(KDD-06), pp. 935-940, Philadelphia, USA, 2006.\n[9] Peeters, G. “A large set of audio features for sound de-\nscription (similarity and classiﬁcation) in the cuidado\nproject”, Technical report, CUIDADO I.S.T. project,\nIRCAM, 2004.\n[10] Rentfrow, P. J. and Godsling, S. D. “The do re mi’s of\neveryday life: The structure and personality correlates\nof music preferences”, Journal of Personality and\nSocial Psychology , 84(6), pp. 1236-1256, 2003.\n[11] Tzanetakis, G., Essl, G. and Cook, P.. “Automatic\nmusical genre classiﬁcation of audio signals”, Inter-\nnational Symposium on Music Information Retrieval ,\nBloomington, Indiana, USA, 2001.\n[12] Tzanetakis, G., Kapur, A., Schloss, W. and Wright,\nM. “Computational ethnomusicology”, Journal of In-\nterdisciplinary Music Studies , 1(2), pp.1-24, 2007.\n758"
    },
    {
        "title": "A Web-based Approach to Determine the Origin of an Artist.",
        "author": [
            "Sten Govaerts",
            "Erik Duval"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415974",
        "url": "https://doi.org/10.5281/zenodo.1415974",
        "ee": "https://zenodo.org/records/1415974/files/GovaertsD09.pdf",
        "abstract": "One can define the origin of an artist as the geographical location where he started his career. The origin is an important metadata element, because it can help to specify subgenres, be an indicator of regional popularity and improve recommendations. In this paper, we present six",
        "zenodo_id": 1415974,
        "dblp_key": "conf/ismir/GovaertsD09",
        "keywords": [
            "Artist Origin",
            "Geographical Metadata",
            "Web-Based Classification",
            "Last.fm",
            "Freebase",
            "Biography Analysis",
            "Metadata Automation",
            "Hybrid Methods",
            "Music Recommendation",
            "Mashup Applications"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n \n \nA WEB-BASED APPROACH TO DETERMINE  \nTHE ORIGIN OF AN ARTIST \n   Sten Govaerts Erik Duval \nK.U. Leuven \nDepartment of Computer Science \nCelestijnenlaan 200A, B-3001 Heverlee, Belgium \n{sten.govaerts, erik.duval}@cs.kuleuven.be  \nABSTRACT \nOne can define the origin of an artist as the geographical \nlocation where he started his career. The origin is an im-\nportant metadata element, b ecause it can help to specify \nsubgenres, be an indicator of regional popularity and im-\nprove recommendations. In this paper, we present six \nmethods to determine the origin, based on Web data \nsources: one extracts data from Last.fm, two query Free-base and three analyze biographies. We evaluate the dif-\nferent methods with 11275 artists. Circa 55% of the art-\nists can be classified using biographies. The best Freebase method can classify 26% and the Last.fm based method \n7%. When comparing on accuracy, the Last.fm and Free-\nbase methods perform similarly with around 90% accu-racy. For the biography-based methods we achieve 71%. To improve coverage, a final, hybrid method achieves \n77% accuracy and 60% coverage. The accuracy of the \ncontinent classification is 87%. As a showcase for our classifier, we developed a mashup application that dis-\nplays, among others, informati on about the origin of art-\nists from radio station playlists on a map. \n1. INTRODUCTION \nIn a previous project  [1], we developed a music player for \nhotels, restaurants and pubs. Peculiar to our approach is \nthat a user can describe the music he wants by referring to a situation, rather than by defining the usual search crite-ria on artist, title, etc. This system uses almost 40 meta-data fields, manually annotated by music experts of Aristo Music (http://www.aristomusic.com), which is a very time-consuming and expe nsive labor. Currently, the \nAristo Music database contains around 58000 songs. The time-consuming metadata annotation process is difficult to scale: in the case of Aristo Music for instance, it limits the ability to penetrate new markets. In order to assist the \nexperts, we already achieved some success in automating \nthe annotation process for some metadata fields [2]. \nThis paper reports on ongoing work in the MuziK pro-\nject that focuses on automatically generating the metadata fields that are most costly to do manually and most rele-\nvant for end users. For this purpose, we rely on a variety \nof approaches: digital signal processing [3], web-based \nclassification [4] and data analysis [5]. \nThis paper focuses on one of the parameters: the origin \nof an artist, defined here as the geographical location \nwhere an artist started his musical career. This can be \nhard to determine sometimes. It  can be seen as the coun-\ntry of the artist’s first success, where he lived most of his \nlife or where some of the group members live. For exam-\nple Georg Friedrich Händel was born in Germany, but went to England where his career really took off. \n1.1 Relevance \nAlthough the origin of an artist can thus be quite fuzzy, it \nis a useful piece of metadata in many cases.  \n• Some subgenres are based on geographical location of \nthe artist, for example Britpop and Viking Metal.  \n• It can also be a good indicator of the popularity of an \nartist in a region, as most artists are often most popu-lar in their country of origin. An artist popularity visu-alization based on Last.fm data shows this by comparing two countries, http://hublog.hubmed.org/archives/001085.html. There are of course exceptions with an international carreer, like the Spice Girls. \n• Recommendations can also improve by using the art-\nist’s origin. They can be tuned to the location of the listener. Some musically very similar songs can be \ngood or bad recommendations, depending on the re-\ngion (and dialect): for example, a small stage art genre in Belgium and the Netherlands (called “Kleinkunst”) is mostly expressed in regional dialects and if the rec-ommendation for a song from Antwerp is a song originating from Amsterdam it could break the at-mosphere, although they are musically very similar. \nThe data in Figure 1, collected during an internal time \nmanagement evaluation, shows the average time in sec-\nonds needed by a music expert to annotate different meta-\ndata fields. Origin is the 6th most expensive element, and rather close to the top. Provi ding the origin often requires \nmanual lookup work, hence making it very expensive to \nannotate. Annotating continent takes much less because it is derived from origin. Target  region is used for localized \nmusic distribution and is done in batch, making it faster. \nOne can determine the origin in a more or less precise way. Sometimes, the country is not specific enough due to differences in regional music styles or linguistic, \n \nPermission to make digital or hard copies of all or part of this work for  \npersonal or classroom use is granted without fee provided that copies  \nare not made or distributed for profit or commercial advantage and that  \ncopies bear this notice and the full citation on the first page. © 2009 International Society for Music Information Retrieval  \n261Poster Session 2\n \n \nlinguistic, cultural or religious  differences within a coun-\ntry, as in for instance East coast vs. West coast rap. We rely on two levels: country and continent.  \n1.2 Related work \nToday, specialized search engines enable searching for \npersons, which often requires linking different sources to a person. Researchers are using knowledge management \nsystems to find experts [6]. Numerous examples can be \nfound online: NNDB (http ://www.nndb.com/), Pipl \n(http://pipl.com/), and Spoc k (http://www.spock.com/). \nThey often offer information on birthplace and residence \nand some even recognize music artists. Except for \nCelma’s demo [7], we did not find any related work spe-cialized towards classifying the origin of music artists. \nOrigin is annotated by the experts based on either their \npersonal knowledge of the artist or by looking up the ori-gin information. Therefore, we try to automate this meta-\ndata field by extracting information from web data \nsources. In this paper, we present our approach to classify an artist’s geographical origin. First, the web data sources and algorithms will be explained and evaluated in section \n3. Afterwards, we discuss the concept and design of our \nmashup tool, which locates artists of radio station play-lists as a demonstration of the technique, and present the \nconclusion and possible further work. \n2. FINDING THE ORIGIN \nDetermining the origin of the artist will be very hard by \nusing content-based techniques (e.g. by analyzing the \nsignal). The best option is to analyze other data sources. \nIn our quest to classify the origin we looked at a wide \nplethora of music related web resources. Most of these \nresources are in plain text, fo r example sites with reviews \nlike Amazon.com, which makes it hard to extract geo-graphical locations, but results can be achieved with for \nexample named entity recognition [8]. One of the main \nproblems is the use of the names of the inhabitants or ad-jectives of geographical loca tions, e.g. German vs. Ger-\nmany. These kinds of words are called demonyms and \nare not recognized as geogr aphical descriptors by most named entity recognizers, e.g. Open Calais \n(http://www.opencalais.com/). Luckily, more structured data sources are available: for instance, most Wikipedia artist pages contain a box with background information. \nMoreover, there are some online databases available for \nquerying, e.g. Freebase (http://www.freebase.com). Tags are also a rich source of in formation and often contain \ngeographical data. If an artist is really more popular in \nhis country of origin (not proven), listening counts of an artist per country might also be an interesting source. \nOur approach relies on several methods with different \ndata sources, because no single data source covers all art-ists. In the remainder of this section, we will present our methods to determine the origin of an artist. Section 2.1 \ncovers a screen scraping technique based on Last.fm \n(http://last.fm). Section 2.2 presents two approaches that rely on Freebase, and section 2.3 details a method to ana-\nlyze an artist biogra phy with demonyms. \n2.1 Origin determination with Last.fm \nLast.fm is a well-known mu sic recommender system and \na music community website with over 30 million users, \nmaking it a great resource of metadata for MIR, such as \nbiographies and tags [9]. For some artists, Last.fm con-tains the origin and sometimes their different where-\nabouts over time, for example Radiohead is located in \nAbingdon, Oxfordshire, UK since 1986, according to \nhttp://www.last.fm/music/Radiohead. \nWe scrape the Last.fm artist page with Dapper \n(http://www.dapper.net/open/), which basically creates a web service out of unstructured data. Sometimes the data on Last.fm is incomplete: e.g. New York, without a coun-\ntry. To fill in these gaps, we use the Google Maps API \n(http://maps.google.com) for geo-coding to retrieve the ISO 3166-1 country code, used to identify the origin. \n2.2 Origin determination with Freebase \nFreebase [10] is a large collaborative semantic database, \ncontaining structured data, ha rvested from different re-\nsources, e.g. Wikipedia.org and MusicBrainz.org. Free-\nbase allows querying through a REST-ful web service, \nusing ontologies to describe the semantics and data inter-linking. Different classes (/music/group_membership, \n/music/artist, /music/mus ical_group/member) describe \nmusic artists and within these,  others (place_of_birth, na-\ntionality, origin, places_lived) describe geo-locations. We \nrun a long complex query covering all the artist and  geo-\nlocation classes of Freebase, which is used in 2 methods: \n• Based on the freebase origin class (freebase-\norigin):  the origin class is geo-coded with the Google \nMaps API to get the country code, which is the result. \n• \nMost frequent location (freebase-most_freq):  takes \nthe nationality, birthplace and places where the artist \nlived and geocodes them all. The most occurring \ncountry code in all the locations is selected. Then out of all the locations with this selected country code, the most occurring city is selected. The location of that \nFigure 1.  The average number of seconds to manually  \nannotate a metadata field. \n26210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n \n \ncity and country code is the result of the method. To \nconclude, this method returns the most frequent loca-tion of nationalities, birthplaces and residences of the group members. \n2.3 Origin determination with biographies \nBiographies often provide a lot of geographical informa-\ntion and thus can be a great source of information on the \norigin. As mentioned before, authors often describe geo-locations with demonyms, for example \"Anders Tren-\ntemøller is a Danish electronic musician…\" (from \nhttp://www.last.fm/music/Trentemøller). We looked for \nnatural language stemmers to transform demonyms, but none were found. One might be able to extend a stemmer \nwith rules to cover all exceptions. We use a list of coun-\ntries and their demonyms from Wikipedia, which also contains Anglo-Saxon cities and the states of the USA (http://en.wikipedia.org/wiki/Nationality). We noticed \nthat the origin or residence of an artist is often mentioned \nin the first sentences of th e biography. We implemented 3 \nvariations that exploit this characteristic. The biography \nis split into natural language sentences and for every sen-\ntence the occurring demonyms and locations are noted. \n• Highest occurrence (bio-most_freq):  The result is \nthe demonym or geographical location that occurs most often. \n• Favor first occurrences (bio-favour_1st):  For every \nencountered country code (cc) a list of sentence num-\nbers s in which cc occurred is kept. Say, s\ntot is the total \nnumber of sentences in the biography, then following formula is calculated for every country code cc: \nRcc=(stot+1)\u0002si\nstot+1i=0length (s)\n\u0001  \nThe result is the country code with the highest R cc. \n• Weaker favoring first occurrences (bio-\nweak_favour_1st):  this method is equal to the previ-\nous, but another weighting function is applied for every country code cc: \nRcc=(stot+2)\u0002si\nstot+1i=0length (s)\n\u0001  \nAgain, the result is R cc. This method tries to spread \nimportance a bit more over the sentences. \n3. EVALUATION \nThe approaches from section 2 are evaluated against a \nground truth data set provided by Aristo Music. First, we \ndescribe the data set normalization, then the results are discussed and a combination of all methods is presented. \n3.1 The data set \nAs ground truth, we use the origin and continent field \nfrom the Aristo Music database. They group metadata per song, although the origin is an artist property. For some \nartists, different songs indicate different origins, due to \nerrors in the database. In those cases, we consider the ori-gin that occurs most often. From the data set, we removed \n10 artists that are not annotated with origin, as well as artists with an origin value like \"Mixed\" and \"Others\": these are used when for instance a group of artists col-\nlaborate, e.g. \"George Michae l & Aretha Franklin\". These \nartists are removed from the ground truth – we identify them through connectors like \"and\", \"feat.\" and \"vs.\". The \nCaribbean contains the Caribbean Sea and its islands and \ncannot be mapped to a single country code. Artists anno-tated with “Caribbean” are t hus also removed from the \nground truth based. The overall result is that 25% are re-\nmoved: we keep 11275 artists (Table 1). \nTo classify continents, the origin is mapped to a conti-\nnent lookup table. There are di fferent ways to define con-\ntinents, based on geography or political treaties. We use a \nWikipedia table (http://en.wikipedia.org/wiki/List_of_-countries_by_continent_(data_file)). Table 1 also shows \nthe distribution of artists over continents. Our data set re-\nflects mainstream music taste in Europe, with a strong representation of North America and Europe. \nAs input for the methods, the artist page and biography \nis retrieved for all artists with the Last.fm API. All ori-\ngins are geocoded to obtain the country code. \n3.2 The results \nSection 3.2.1 examines how well all approaches cover the \ndata set. Section 3.2.2 discusses the accuracy of the re-sults per method. Finally, we present a new method that \nincreases the coverage and improves overall performance. \n3.2.1 Coverage \nThe coverage is defined as the number of artists of the \nground truth that have been determined. Figure 2 shows \nthe percentage of artists for which an origin was found \nper method. For 59%, an origin can be found by at least one of the methods. The large difference in coverage is \nthe main reason why we use multiple data sources.  \nOnly a small percentage of Last.fm artist pages (7%) \ncontain the origin. Freebase covers a bit more than 26%. Surprisingly, only 5% less artis ts have an explicit origin \nclass in the Freebase ontology. For the three biography-\nbased methods, the coverage is obviously the same (56%) and about double that of Freebas e. As only 63% of artists \nTable 1.  The number of artists in the data set and for\n \neach continent in the cleaned data set. \n263Poster Session 2\n \n \nhave a biography, this means that almost all of the re-\ntrieved biographies contain ge ographical information. For \n62% of the artists, an origin could be retrieved with at least one of the methods. The coverage of the combina-\ntion method will be discussed later. \nAs discussed, the best me thod can only annotate 56% \nof the artists. One of the reasons is that Aristo Music is a \nBelgian company with a database that includes many lo-\ncal artists for whom neither biographies nor freebase en-tries are available. There is also a rich tradition of mardi \ngras music in Belgium, made by artists well known in a \nvery small region (sometimes only a village or town) for a short period of the year. Their online presence is zero. There are also possible differ ences in artist name writing \nbetween Aristo Music and Last.fm. The latter often offers \nalternative pages for the different writings; most of these pages contain no biography or a very condensed one, dis-\nabling the biography-based methods. \n3.2.2 Accuracy \nThe combination method makes use of the results of the \nother methods. Therefore the same data set cannot be \nused to evaluate all methods. A data set, called combo \ndata set, contains 3000 randomly selected artists from the full data set to evaluate the combination method. The \n8275 artists left are used to evaluate the Last.fm, Freebase \nand biography methods and is called the LFB data set. The coverage of the LFB and combo data set is almost \nequal to Figure 2 (up to about 1% difference). \nThe accuracy is defined as the number of correct clas-\nsifications divided by the total number of artists covered by a method. The accuracy of the Last.fm, Freebase and \nbiography-based methods for the continent and origin \nclassification on the LFB data set is shown in Table 2. As expected, the continent classification is performing better, \nbecause small errors in the origin will not impact on the \ncontinent classification: for instance, a misclassification \nof a French artist as a German one will still result in a correct continent (Europe). The Last.fm screen scraping \napproach has the highest accuracy overall and the Free-\nbase methods perform equally well and are close second. Their 95% confidence intervals overlap, so the best per-\nformer cannot be concluded, nor from performed t-tests. There is quite a drop in accuracy when using the biogra-\nphy as a source. Continent cla ssification with biographies \nperforms much better than origin. When comparing the \nresults of the 3 different biography-based methods, it is clear that our assumption that the origin appears in the first sentences is valid, becau se the method favoring the \nfirst sentences most strongly,  bio-favour_1st, is the best. \nAll methods have their issues. We noticed that some of \nthem occur due to errors in  geocoding of locations with \nthe same name, e.g. Birmi ngham in USA and UK, or a \nmix-up between small geographical entities and their big neighboring countries, e.g. Je rsey and England, Luxem-\nburg and Belgium. Another pr oblem occurs when differ-\nent artists have the same name; in that case, they have to be identified on song level. Of course, there are also method dependent errors: for example, sometimes the \nLast.fm origin is given as a demonym instead of a coun-\ntry, e.g. “American” and “French”, and the geocoding will resolve this to anothe r country, respectively to \nAmericana, Brazil and Watts burg, USA. The freebase-\norigin method often takes the country of birth as the ori-gin of the artist, e.g. A kon was born in Senegal, but \nmoved as a child to the USA. In the case of freebase-\nmost_freq, it occurs that one band member dominates the \nlocations, because Freebase contains many more geo-graphical data on that one member as the rest. This is the \ncase with Ry Cooder of Buen a Vista Social Club: he \noriginates in the USA, while this is a Cuban band. The Last.fm biography can contain multiple biographies from \ndifferent artists with the same name. This of course con-\nfuses the biography-based methods. \nIn Figure 3, we can see the accuracy for every method \nfor every continent. It is clea r that the continent classifi-\ncation works better for Europe, North America and Oce-\nania. This can be due to the strong representation of these continents in the ground truth or a stronger representation \non the web (see Table 1). To be able to see a consistent \ntrend, we need more data for the smaller continents. We noticed that the misclassifications of the biography-based methods for Asia can often be traced back to Japan, be-\ncause of Japanese album releases. Some of the Belgian \nand Dutch artists are classified  with former colonies. This \nhappens when the biography cont ains the place of birth or \nperformance locations. Bio-favour_1st outperforms the \ntwo other biography methods in  Africa, Asia and South-\nFigure 2.  The percentage of annotated artists by the dif-\nferent methods. Table 2.  The accuracy and 95% confidence intervals for  \nall methods for the classification of continent and origin  \non the LFB data set. \n26410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n \n \nAmerica. Due to the high num ber of countries it is im-\npossible to this on country level. \n3.2.3 Improve by combination (combo_method) \nWe will now introduce a new method that maximizes \ncoverage and improves performance beyond bio-\nfavour_1st. The idea of the new method is to use all ori-gins found by all methods to improve the coverage and \nselect the origins smar tly to improve accuracy. \nThe result is selected in an order based on the accuracy \nin Table 2. Since the 95% confidence intervals of the Last.fm and Freebase methods overlap, we don’t know \nwhich one is significantly better. We used the highest ac-\ncuracy and the spread of the confidence intervals to order. If a location of lastfm-origin is available, this is the result, \nbecause it has the highest accuracy. Otherwise freebase-\nmost_freq is selected, because the confidence interval of freebase_most_freq lies encapsulated in that of freebase-\norigin, then freebase-origin and as last option bio-\nfavour_1st. We can still improve this slightly by using the continent accuracy in Figure 3. Freebase-origin performs better in Africa, Asia and South-America. If the origin \ndetermined by freebase-origin is from these continents, \nthis is selected before freebase-most_freq. \nObviously, the coverage of the new method (59%) for \nthe complete data set equals the percentage found by all \nmethods (Figure 2). The accu racy of the combo-method \nfor classifying origin on the combo data set is shown in Table 3. The method with the highest coverage previ-\nously, bio-favour_1st, improves from 71% to 77% and \nfor continent to 86%. The lower accuracy, compared to the Freebase and Last.fm methods, is caused by half of \nthe results, which are from bio-favour_1st. The combo-\nmethod performs 14% less than the best method, free-base-most_freq, but covers more than double. \nTable 4 shows the recall, precision and F-measure of \nthe 10 countries with the mo st artists classified by \ncombo_method. Overall the pr ecision is higher than the \nrecall. A higher precision is preferred for this task, be-\ncause the main task of the classifier is to classify new art-\nists. Belgium, the Netherlands and Germany have a rather low recall value. Currently, these countries are the main markets of Aristo Music, so there are comparatively more \nglobally lesser-known artists in the database for these \ncountries than others and thus less data available on the web. The lower precision for Canada is probably caused \nby misclassifications with US  and mistakes by the biog-\nraphy analysis: for example, if the bio mentions “French speaking”, then the artist may be classified with France. \n4. MASHUP \nAs a showcase for the classifier, a mashup application \nwas developed, that visualizes playlists from radio sta-tions and locates the artists on a map, based on the origin \nclassifier. We also link the da ta to YouTube videos, biog-\nraphies and the Last.fm account. \nThe playlists are retrieved from Last.fm accounts, \nTwitter or scraped from radio station websites with Dap-\nper. Yahoo! Pipes retrieves the data from these sources and adds the biography, pi ctures, track duration and \nLast.fm URL from the Last.fm API. Then we apply the \norigin classifier, implemente d as a REST web service on \nGoogle App Engine. It requires the artist, the biography and the Last.fm artist page URL; e.g. \nhttp://artistlocator.appspot.com/?artist=Morrissey&bio=T\nest+bio&last_fm_url=http://www.last.fm/music/Morrissey. Figure 4, shows a screenshot of the mashup applica-\ntion, http://www.cs.kuleuven.be/~sten/lastonamfm/. \nDuring its 24 days online, the mash-up attracted 220 \nindividual visitors; almost 30% of them return. Most visi-\ntors come from Belgium, USA, France and UK. On 8 \nMay, it was “Mashup of the Day” on Programmable Web \n(http://www.programmableweb.com/). A Belgian na-Figure 3.  The accuracy of every method for every conti-\nnent on the LFB data set. \nTable 4.  Precision, recall and F-measure of the top coun-\ntries categorized by combo_method. Table 3.  The accuracy and 95% confidence intervals for  \nall methods on the combo data set. \n265Poster Session 2\n \n \ntional radio station discussed it on air. Another radio sta-\ntion intends to use it to analyze playlists of competitors. \n5. CONCLUSION AND FUTURE WORK \nOur aim was not to build a very complex classifier, but to \nsee how far we could get usi ng simple techniques. Real \nlife systems can often benefit from this, e.g. short computation time. This rather simple approach leads to \ndecent performance. From 6 di fferent methods to classify \nthe origin, the 4 best performing are combined in the final classifier to maximize coverage to 59%. The resulting \norigins are selected from the classifier with the highest \naccuracy. This results in a final accuracy of 77% for ori-gin classification and 86% for continent classification. \n The classifier can probably benefit from applying data \nmining techniques. For exampl e the biographies can be \nanalyzed for words that co-o ccur for artists of the same \ncountry. Something similar could be trained on the \nLast.fm tags of artists with the same origin to extract \ngeographical information from  tags. Another idea is to \nuse named entity recognition, like Open Calais, to extract \nbirthplaces and other geographi cal facts. This could com-\nplement the demonym analysis  to get more detailed in-\nformation, for example city names. \n One important way to improve the classifier is by in-\ncreasing the coverage. The identification of an artist on-\nline has to be improved to get the correct biography. This can be done with online identification services, e.g. Mu-\nsicBrainz.  More data sources  can also enable further im-\nprovement. People search engines sometimes show the birthplace and could thus be leveraged. Additional data sources, such as Belgian rock/pop catalogs, could be ex-\nploited to cover local artists. An obstacle might the intel-\nlectual properties of such collections. The web can be used as a whole to extract information from by for exam-\nple crawling music related site s, using search engines for \nclassification [4] or using links from semantic search en-gines, like http://sig.ma.    \nIn any case, we believe that our classifier is accurate \nenough to automatically annotate the origin of an artist for applications like that of Aristo Music. This can re-\nmove the need for costly manual effort and enable scal-able automation for an important metadata element. \n6. ACKNOWLEDGEMENT \nWe gratefully acknowledge the support of IWT \nVlaanderen on the project: \"MuziK: Muziek op maat van de klant\" under grant IWT 080226 and Aristo Music NV \nfor the evaluation data, metadata annotation time man-\nagement study and their musical expertise. \n7. REFERENCES \n[1] N. Corthaut, S. Govaerts , E. Duval. “Moody Tunes: \nThe Rockanango Project”, Proc. of the 7th Int. Conf. on Music Inf. Retrieval, 2006, pp. 308-313.   \n[2] S. Govaerts, N. Cortha ut, E. Duval. “Mood-ex-\nMachina: Towards Automation of Moody Tunes”, \nProc. of the 8th Int. Conf. on Music Inf. Retrieval, 2007, pp. 347-350. \n[3] G. Tzanetakis, P. Cook. “Musical genre \nclassification of audio signals”, IEEE Trans. on Speech & Audio Proc., Jul. 2002, 10(5), 293-302. \n[4] M. Schedl, T. Pohle, P. Knees, G. Widmer. \n“Assigning and visualizing music genres by web-\nbased co-occurrence analyis”, Proc. of the 7th Int. Conf. on Music Inf. Retrieval, 2006. \n[5] J. Bergstra, A. Lacoste, D. Eck. “Predicting genre \nlabels for artists using FreeDB”, Proc. of the 7th Int. Conf. on Music Inf. Retrieval, 2006.  \n[6] I. Becerra-Fernandez. “The role of artificial \nintelligence technologies in the implementation of People-Finder knowledge management systems”, Knowl.-Based Syst., 2000,  13(5), pp. 315-320. \n[7] Ò. Celma, M. Nunes, “GeoMuzik: A geographic \ninterface for large music collections”, Proc. of the 9th Int. Conf. on Music Inf. Retrieval, 2008. \n[8] B. Pouliquen, R. Steinberger , C. Ignat, T.De Groeve. \n\"Geographical information recognition and visualization in texts written in various languages\", SAC '04, 2004, pp. 1051-1058.  \n[9] \nPaul Lamere. “Social tagging and music information \nretrieval”, Journ. New Music Res., 37(2), 101–114.  \n[10] K. Bollacker, C. Evans, et al. “Freebase: a \ncollaboratively created graph database for \nstructuring human knowledge”, 2008, ACM \nSIGMOD, pp. 1247-1250. \n[11] M. Meire, X. Ochoa, E.Duval, “SAmgI: Automatic \nmetadata generation v2.0”, 2007, Proc. of EdMedia07, AACE, pp. 1195-1204.  \nFigure 4.  A screenshot of the mashup application. \n266"
    },
    {
        "title": "The ISMIR Cloud: A Decade of ISMIR Conferences at Your Fingertips.",
        "author": [
            "Maarten Grachten",
            "Markus Schedl",
            "Tim Pohle",
            "Gerhard Widmer"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416434",
        "url": "https://doi.org/10.5281/zenodo.1416434",
        "ee": "https://zenodo.org/records/1416434/files/GrachtenSPW09.pdf",
        "abstract": "In this paper, we analyze the proceedings of the past International Symposia on Music Information Retrieval (ISMIR). We extract meaningful term sets from the accepted submissions and apply term weighting and Web-based filtering techniques to distill information about the topics covered by the papers. This enables us to visualize and interpret the change of hot ISMIR topics in the course of time. Furthermore, the performed analysis allows for assessing the cumulative ISMIR proceedings by semantic content (rather than by literal text search). To illustrate this, we introduce two prototype applications that are publicly accessible online 1 . The first allows the user to search for ISMIR publications by selecting subsets of ISMIR topics. The second provides interactive visual access to the joint content of ISMIR publications in the form of a tag cloud – the ISMIR Cloud.",
        "zenodo_id": 1416434,
        "dblp_key": "conf/ismir/GrachtenSPW09",
        "keywords": [
            "International Symposia on Music Information Retrieval",
            "Proceedings",
            "Term sets",
            "Term weighting",
            "Web-based filtering",
            "Hot ISMIR topics",
            "Semantic content",
            "Prototype applications",
            "ISMIR Cloud",
            "Search by subsets"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTHE ISMIR CLOUD:\nA DECADE OF ISMIR CONFERENCES AT YOUR FINGERTIPS\nMaarten Grachten Markus Schedl Tim Pohle Gerhard Widmer\nDepartment of Computational Perception\nJohannes Kepler University\nLinz, Austria\nmusic@jku.at\nhttp://www.cp.jku.at\nABSTRACT\nIn this paper, we analyze the proceedings of the past In-\nternational Symposia on Music Information Retrieval (IS-\nMIR). We extract meaningful term sets from the accepted\nsubmissions and apply term weighting and Web-based ﬁl-\ntering techniques to distill information about the topics cov-\nered by the papers. This enables us to visualize and inter-\npret the change of hot ISMIR topics in the course of time.\nFurthermore, the performed analysis allows for assessing\nthe cumulative ISMIR proceedings by semantic content\n(rather than by literal text search). To illustrate this, we\nintroduce two prototype applications that are publicly ac-\ncessible online1. The ﬁrst allows the user to search for\nISMIR publications by selecting subsets of ISMIR topics.\nThe second provides interactive visual access to the joint\ncontent of ISMIR publications in the form of a tag cloud –\ntheISMIR Cloud .\n1. INTRODUCTION AND MOTIVATION\nMusic information retrieval and extraction has been a fast\ngrowing ﬁeld of research during the past decade. Cer-\ntainly the most important forum for this multidisciplinary\nﬁeld is the International Symposium on Music Informa-\ntion Retrieval (ISMIR) [11]. In 2009, ISMIR celebrates\nits 10thanniversary. Thus, we think it is time to look back\nand investigate which general topics and research problems\nwere most important in MIR during the past decade. To\nthis end, we analyzed the digital ISMIR proceedings [11]\navailable online. Not only have we captured the princi-\npal topics reﬂected by previous, accepted ISMIR papers\nby means of text-based content extraction and analysis, but\nwe have also investigated how these topics changed over\ntime. Since MIR is a highly dynamic ﬁeld of research, we\n1http://www.cp.jku.at/projects/ISMIR-cloud/\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.gained interesting insights, which will be detailed in the\nfollowing. We further visualized the corpus of ISMIR doc-\numents via clusters of topics described by sets of terms.\nTo this end, we employed two approaches based on Non-\nNegative Matrix Factorization (NMF) and Principal Com-\nponents Analysis (PCA). Two prototype applications are\nprovided as a proof-of-concept. The ﬁrst is a Web appli-\ncation for browsing the cumulative online ISMIR proceed-\nings theme-wise. The second is an ofﬂine OpenGL appli-\ncation that visualizes the ISMIR Cloud in three dimensions,\nand allows for real-time interaction such as spatial naviga-\ntion and text-based search for tags.\nThe remainder of the paper is organized as follows. Sec-\ntion 2. gives an overview of related work on text informa-\ntion extraction and retrieval, topic-based clustering, and vi-\nsualization. Section 3. describes the features we extracted\nfrom the ISMIR corpus. In Section 4., we present our ap-\nproaches to visualize and browse the papers. Finally, Sec-\ntion 5. draws conclusions and points out directions for fu-\nture work.\n2. RELATED WORK\nRelated work mainly falls into the two ﬁelds of text mining ,\nmore precisely, text-based information extraction/retrieval\nandclustering and visualizing high-dimensional data . In\nline with the dedication of this paper to the MIR commu-\nnity, we will focus on work carried out in the context of\nmusic information research.\nIn the context of MIR, extracting terms from texts, more\nprecisely, from Web documents, in order to tag a music\nartist has ﬁrst been addressed in [28], where Whitman and\nLawrence extract different term sets (e.g., noun phrases\nand adjectives) from artist-related Web pages. Based on\nterm occurrences, individual term proﬁles are created for\neach artist. The authors then use the overlap between the\nterm proﬁles of two artists as an estimate for their similar-\nity. A quite similar approach is presented in [13]. Knees et\nal. however do not use speciﬁc term sets, but create a term\nlist directly from the retrieved Web pages. Subsequently,\na term selection technique is applied to ﬁlter out less im-\nportant terms. Hereafter, the TF ·IDF measure, e.g., [31],\nis used to weight the remaining words and subsequently\n63Poster Session 1\ncreate a weighted term proﬁle for each artist. Knees et\nal. propose their approach for artist-to-genre classiﬁcation\nand similarity measurement.\nA text-based music retrieval system that builds upon meth-\nods for term extraction from Web pages, term weighting,\naudio feature extraction, and similarity measurement is pre-\nsented in [15]. In this paper, the authors relate audio fea-\ntures extracted from a given music collection with terms\nextracted from Web pages that contain part of the meta-\ndata present in the music collection. These terms are then\nweighted using an adapted version of the TF ·IDF measure\nand joined with the audio features to build a feature vector\nfor each track, which serves as a track descriptor. This\napproach allows for searching music collections via de-\nscriptive natural language terms, e.g., by issuing queries\nlike “guitar riff” or “metal band with front woman”. Other\nwork related to MIR that makes use of text mining tech-\nniques includes [10], where a POS tagger is used to search\nlast.fm [17] tags for adjectives that describe the mood of\na song. In [3] the machine learning algorithm AdaBoost\nis used to learn relations between acoustic features and\nlast.fm tags.\nAs for general work on text-based information extraction\nand retrieval, different methods for term selection and term\nweighting have been analyzed with respect to their perfor-\nmance in text categorization, cf. [2, 16, 30], in text-based\nretrieval, cf. [24], and in clustering, cf. [6]. A comprehen-\nsive evaluation of term weighting techniques and similarity\nmeasures for information retrieval purposes is presented in\n[31]. In their extensive evaluation of various formulations\nof TF, IDF, and similarity measures, Zobel and Moffat con-\nclude that no single combination outperforms the others\nconsistently. In fact, the performance of any combination\nwas found to be highly dependent on the domain and query\nset it had been applied to. Text-based IE from the Web usu-\nally relies on identifying or learning speciﬁc patterns that\ncontain the information to be extracted. Already in [7],\nthe use of static rules to determine hyponyms in text cor-\npora was proposed. [4] presents a system that complements\ngeneric text patterns with domain-speciﬁc rules found by\npattern extraction via search engines and subsequent se-\nlection of high-quality extraction rules. [1] proposes an ap-\nproach that solely relies on Google’s page counts for spe-\nciﬁc patterns to determine instances of a given concept.\nAs for clustering and visualizing high-dimensional fea-\nture data in the context of MIR, in [21] Non-Negative Ma-\ntrix Factorization (NMF) [18] was employed to determine\nclusters of concepts based on tags describing music artists,\nwhich were extracted from last.fm . Using NMF on features\ngained from a term weighting approach in order to cluster\ndocuments was already proposed in [29].\nAnother data projection and visualization technique is Prin-\ncipal Components Analysis (PCA) [8, 12]. PCA consists\nin a a linear projection of high-dimensional data onto a\nsmall set of orthogonal dimensions with minimal loss in\nvariance. The relative distances between data points in the\nhigh-dimensional space are preserved as good as possible\nin the low-dimensional projection. Reducing the dimen-sionality of the feature space to two or three thus allows\nfor the visualization of possible low-dimensional structure\nin the original high-dimensional data space.\nA precedent of interactive visualization of scientiﬁc in-\nformation ﬂows (such as citation patterns across disciplines\nand journals, and temporal evolution of citation indices) is\nprovided by [22,27]. A notable difference of this approach\nis that the information being visualized is obtained from\nbibliometric data (journal citation reports) rather than data\nobtained through automatic content extraction from publi-\ncations.\n3. DATA ACQUISITION AND FEATURE\nEXTRACTION\nText-based information extraction and retrieval commonly\nrelies on the bag of words model, which can be traced\nback at least to [19]. According to this model, a document\nis represented as an unordered set of its words, ignoring\nstructure and grammar rules. Words can be generalized to\nterms, where a term may be a single word or a sequence\nofnwords ( n-grams ), or correspond to some grammatical\nstructure, e.g., a noun phrase. Using such a bag of words\nrepresentation, each term tdescribing a particular docu-\nmentdis commonly assigned a weight wt,dthat estimates\nthe importance of tind. Each document can then be de-\nscribed by a feature vector that aggregates the single term\nweights. When considering a whole corpus of documents,\neach document can be thought of as a representation of its\nfeature vector in a feature space orvector space whose di-\nmensions correspond to the particular term weights. This\nso-called vector space model is a fundamental model in in-\nformation retrieval and was originally described in [25].\nFor the term weighting function wt,d, in modern informa-\ntion retrieval, typically some variant of TF ·IDF scores is\nused. The TF term gives more weight to terms that appear\nmany times in a document, whereas the IDF term ensures\nthat less weight is given to terms that appear in many docu-\nments. More details on term weighting via TF ·IDF can be\nfound in [32]. The TF ·IDF function assigns a weight wt,d\nto a particular term tand document d. Calculating wt,d\nfor all terms remaining after having performed term selec-\ntion on the terms extracted from the corpus thus yields a\nrepresentation of das a term weight vector in the feature\nspace.\nFollowing these basic principles of text-based informa-\ntion retrieval, we performed feature extraction as follows.\nFirst, we retrieved the PDF ﬁles of the accepted ISMIR\nsubmissions from the online repository [11]. This yielded\neffectively 719 documents. Subsequently, we converted\nthe PDF ﬁles to standard text ﬁles. To this end, the GNU/-\nLinux tools pdftotext from xpdf-utils andiconv from libc6\nwere used. Minor problems encountered in the transcrip-\ntion process, such as occasional truncation of words, were\naddressed by preﬁx/sufﬁx ﬁltering as detailed later. Next,\nwe employed the part-of-speech (POS) tagger Geniatag-\nger[26] to extract all noun phrases from the corpus since\nwe believe that these are most important to describe the\ncontent of ISMIR papers. As the output of the POS tagger\n6410th International Society for Music Information Retrieval Conference (ISMIR 2009)\ncontained a lot of noise, we subsequently applied some ad-\nhoc ﬁlters: We discarded all terms containing non-alphabetic\ncharacters and retained only trigrams, bigrams, and uni-\ngrams. This yielded approximately 70,000 terms.\nSince we aimed at emphasizing terms important to MIR,\nwe performed term selection via Web-based ﬁltering of\nterms that tend to be important in a general context. Such\nterms thus tend to be rather unimportant in the context\nof MIR and also not very discriminative for the corpus\nof ISMIR papers. For this purpose, we queried the Web\nsearch engine exalead [5] for the extracted n-grams as ex-\nact phrase and retrieved the returned page-count-values.\nWe then discarded all terms whose TF in the ISMIR cor-\npus was lower than their page-count-value. To alleviate the\nproblem of truncated words after PDF-to-text-conversion,\nwe removed any n-gram vthat was a preﬁx/sufﬁx of an-\nothern-gram w(of equal n) and whose minimal TF among\nthe single words occurring in vwas lower than the minimal\nTF among the single words occurring in w, assuming that\ntruncated words typically have a low TF.\nThis approach ﬁnally yielded a term list of approximately\n12,500 terms. We extracted, for each of these terms, its\nabsolute TF count per ISMIR document and its global DF\ncount in the corpus. Weighting each TF value with the\n(logarithmic) IDF obtained from the ISMIR corpus accord-\ning to Formula 1 provided a TF ·IDF vector representation\nof each ISMIR document. In Equation 1, nis the total\nnumber of documents in the corpus, t ft,dis the number of\noccurrences of term tind, andd ftis the number of docu-\nments in the whole corpus in which toccurs at least once.\nConcatenating the TF ·IDF representation of all documents\nyields a term-document matrix.\nwt,d=/braceleftbiggt ft,d·logn\nd ftift ft,d>0\n0 otherwise(1)\n4. VISUALIZING ISMIR\nDetermining and illustrating the most important concepts\ntackled by ISMIR papers over time, we used Non-Negative\nMatrix Factorization (NMF) and Principal Components Anal-\nysis (PCA) as elaborated in the following.\n4.1 Finding Concepts by Non-Negative Matrix Factor-\nization\nTopic detection on the TF ·IDF vectors, calculated as de-\nscribed in the previous section, was performed as proposed,\nfor example, in [9, 18, 21, 29]. For NMF calculation, the\ncost function is the square of the Euclidean distance, and\nupdate takes place by the standard multiplicative update\nrules. Initialization is done randomly. NMF aims to ﬁnd\nan approximate decomposition (into matrices WandH)\nwith non-negativity constraints, cf. Equation 2, where Vis\nthen×mmatrix of the 12,500-dimensional TF ·IDF vec-\ntors and m= 719 documents.\nV≈WH (2)Matrix Wis interpreted as containing the amount each of\nthenterms is associated with each of rconcepts, and H\nas containing the amount each document is associated with\neach of the rconcepts.\n4.2 Changes of ISMIR Topics over Time\nThe association between concepts and documents that re-\nsults from NMF over the term-document matrix, allows us\nto make an association between concepts and years (by\nsumming the activation of concepts in the documents of\neach year). Figure 1 shows the evolution of r= 22 con-\ncepts over time, with the overall height representing the\nnumber of relevant publications in the year. The legend\nshows the three top-weighted terms for each concept. The\nconcepts have been ordered vertically according to their\ngrowth/decline over time, the most growing concepts be-\ning on top. To this end, we performed linear regression\nover the development of concept weights during the con-\nsidered time span.\nSeveral interesting observations can be made. Firstly,\nnote that the concepts seem to be of different categories.\nWhereas some concepts clearly represent topics (e.g., genre\nclassiﬁcation, onset detection, rhythm description, or ﬁn-\ngerprinting), others seem to represent methods that can be\nused to solve different types of problems (such as matrix\nfactorization or dynamic time-warping).\nSecondly, even if the presence of most concepts is rather\nstable over the years, there are some notable changes over\ntime. Some of the changes that the analysis reveals are not\nvery surprising, such as the fact that semantic audio an-\nnotation performed via collaborative tagging (such as em-\nployed by last.fm) or Web mining, was virtually absent as\na research topic in the ﬁrst ISMIR conferences. By 2008,\nit has gained the largest share. Other changes are less obvi-\nous. For example, the share of query-by-humming/singing\nin ISMIR 2002 papers was considerably higher than it was\nin later years. Furthermore, genre classiﬁcation seems to\nhave boomed brieﬂy at ISMIR 2005. This might be related\nto the MIREX 2005 genre classiﬁcation contest.\n4.3 A Web-Interface to Concept-based search of IS-\nMIR publications\nThe concepts found by NMF can also be used to create an\ninterface for searching ISMIR papers associated with par-\nticular concepts. The simplest form of such an interface\nis a selection screen that lets the user select one or more\nof the rconcepts of interest. The user selection is trans-\nformed into a vector of length r, with all entries set to zero\nexcept these that correspond to selected concepts, which\nare set to one. This vector is used as a query vector, and\ncompared to each of the documents’ concept vectors by\ncosine similarity [23]. The outcome is then presented to\nthe user as a list of suggested documents ranked accord-\ning to their similarity to the query vector. More elaborate\napproaches would include search reﬁnement (e.g., by rel-\nevance feedback), or query term expansion based on the\n65Poster Session 1\nFigure 1 . Evolution of the main ISMIR topics over the years.\nconcept vectors. We have implemented a small prototype\napplication for document retrieval as a web service.\n4.4ISMIRviewer : Navigating the ISMIR Cloud\nTo visualize the semantic content of of the joint ISMIR\npublications, we pursue the idea of the tag cloud . In this\nsubsection, we describe how we construct the tag cloud\ncontaining the terms extracted from the documents, and\npresent the ISMIRviewer , an application for interactively\nnavigating this tag cloud.\nThe term-document matrix that contains the IDF-weighted\nterm frequency of each term in each of the 719 documents\ncan be seen as specifying each term as a point in a 719-\ndimensional Euclidean space. In general, the more fre-\nquently two terms occur in the same subset of documents,\nthe closer they will be in this space. This high-dimensional\nspace, however, cannot be used directly for visualizing the\nrelationships among terms. Moreover, if multiple docu-\nments contain the same terms with similar frequencies, there\nwill be redundancies in the corresponding dimensions. In\nthis case, the dimensionality of the feature space can be\nreduced without losing information about the distance be-\ntween the terms and their relative location. PCA is a tech-\nnique for such a dimensionality reduction, in which the\ndata is projected on a set of orthogonal dimensions that\nhave been rotated to maximize the variance along each di-\nmension. The dimensions are ordered according to the data\nvariance they hold. In this way, a subset of dimensions\nof any size can be chosen with maximal data variance.\nThe principal components are obtained by computing the\neigenvalues of the covariance matrix of the data, cf. [12].Since we aim at providing a spatial visualization, the\nnumber of dimensions is obviously limited to three. How-\never, we found that projecting the data from 719 to three\ndimensions directly was not useful in this case as the ﬁrst\nthree principal components accounted only for 12% of the\nvariance in the data (90% being reached when using 347\ndimensions). When visualized, the terms are very con-\ndensed in space, where the variance is highly dominated\nby a few common terms like “music” and “audio”. Using\nthe logarithms of term frequencies alleviated this problem\nslightly, but not satisfyingly.\nInstead, we have opted for a two-stage approach to data\nreduction. The ﬁrst stage applies NMF, as described in\nsubsection 4.1. It yields a small set of basis vectors, which\nare formally activation patterns over documents, and tend\nto represent musically meaningful concepts. Each term t\nhas an activation value for each concept c, denoting how\nrelevant tis toc. Experimentation with NMF using dif-\nferent numbers of concepts shows that an NMF reduction\nto twenty concepts include most recognizable subﬁelds of\nMIR without introducing many unrecognizable concepts2.\nGiven these twenty concepts, terms are ﬁltered to include\nonly the 100 most activated terms for each of the concepts.\nAs a second stage, we perform dimensionality reduction\nto three dimensions through PCA on the subset of terms\nand the activations over the twenty concepts that were ob-\ntained in the ﬁrst stage. The resulting space is less densely\npopulated and the terms it contains tend to be more MIR-\nrelevant.\nFor interactive inspection of the constructed tag cloud,\n2As judged informally by the authors\n6610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 2 . Screenshot of the ISMIRviewer showing the\nonset-detection neighborhood of the ISMIR Cloud.\nwe have developed the ISMIRviewer , that allows the user\nto freely rotate the space and zoom in on regions using the\nmouse. Furthermore, subsets of the cloud can be selected\nby text search. As the user types, the matching tags light\nup. For each matching tag, neighboring tags are displayed,\nwhile remote and non-matching tags are dimmed. For the\ngiven selection of tags, ﬁve publications are shown in the\ncorner of the screen that have been determined to be the\nmost relevant for that term. Instead of determining doc-\nument relevance through TF ·IDF, the term is mapped to\nthe documents via the concepts found by NMF. This effec-\ntively realizes a document search by query expansion .\nIn this way, the user can search for MIR-related top-\nics, methods, or author names, and obtain relevant pub-\nlications. Figure 2 shows a screenshot of the application.\nThe displayed terms are the result of searching for the term\nonset-detection . The neighborhood of the search term (small\nblack font) contains related concepts, e.g., period ,bpm,\ntechniques used (e.g. autocorrelation ), and authors who\nhave published on onset detection, such as Klapuri and\nDixon.\n5. CONCLUSIONS\nIn this paper, we analyzed the proceedings of the past IS-\nMIR conferences, extracted terms from the documents, and\nemployed text and Web mining techniques to distill a set\nofn-grams we believe to be important to describe the ﬁeld\nof music information retrieval. Using a TF ·IDF weighting\nfunction, we described each document by means of its term\nweights. We then applied clustering techniques to reveal\nthe most important concepts covered by the ISMIR papers.\nFurthermore, a year-wise analysis of the publications re-\nvealed interesting changes of topics addressed in ISMIR\nover the years. For example, in ISMIR 2002, query-by-\nhumming was a major topic, that has received considerably\nless attention in later years. Furthermore, genre classiﬁca-\ntion had a particularly large share in ISMIR 2005.\nMoreover, we presented two prototype applications that\nprovide access to the semantic content of the past ISMIRpublications. The ﬁrst one is a Web-based retrieval system\nto search the corpus of ISMIR proceedings via the con-\ncepts found by NMF. The second one, which we call IS-\nMIRViewer , provides an interactive tag cloud visualization\nto reveal the relationships between MIR related terms. It\nemploys a focus and context technique to show subsets of\nthe tag cloud in response to user-entered text queries, and\nprovides the ISMIR publications that are most relevant to\nthe text queries.\nThe applications are presented as a proof-of-concept,\ntheir user-interfaces leave room for improvement. Fur-\nther work to be done includes investigating other cluster-\ning techniques, e.g., Aligned Self-Organizing Maps [20] or\nMusic Description Maps [14].\n6. ACKNOWLEDGMENTS\nThis work was supported by the Austrian FWF under grants\nL511 and P19349.\n7. REFERENCES\n[1] P. Cimiano, S. Handschuh, and S. Staab. Towards the\nSelf-Annotating Web. In Proceedings of the 13th Inter-\nnational Conference on World Wide Web (WWW 2004) ,\npages 462–471, New York, NY , USA, 2004. ACM\nPress.\n[2] F. Debole and F. Sebastiani. Supervised Term Weight-\ning for Automated Text Categorization. In Proceedings\nthe 18th ACM Symposium on Applied Computing (SAC\n2003) , pages 784–788, Melbourne, FL, USA, March\n9–12 2003. ACM.\n[3] D. Eck, T. Bertin-Mahieux, and P. Lamere. Autotag-\nging Music Using Supervised Machine Learning. In\nProceedings of the 8th International Conference on\nMusic Information Retrieval (ISMIR 2007) , Vienna,\nAustria, September 23–27 2007.\n[4] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu,\nT. Shaked, S. Soderland, D. S. Weld, and A. Yates.\nMethods for Domain-Independent Information Extrac-\ntion from the Web: An Experimental Comparison. In\nProceedings of the 19th National Conference on Ar-\ntiﬁcial Intelligence (AAAI 2004) , pages 391–398, San\nJose, CA, USA, 2004.\n[5] Exalead: Redeﬁning information access for the enter-\nprise and the web:. http://www.exalead.com ,\n2009. (access: April 2009).\n[6] V . Fresno, R. Mart ´ınez, and S. Montalvo. Improving\nweb page clustering through selecting appropiate term\nweighting functions. In Proceedings of the 1st IEEE\nInternational Conference on Digital Information Man-\nagement (ICDIM 2006) , pages 511–518, Bangalore,\nIndia, December 6–8 2006.\n[7] M. A. Hearst. Automatic Acquisition of Hyponyms\nfrom Large Text Corpora. In Proceedings of the 14th\n67Poster Session 1\nConference on Computational Linguistics – Vol. 2 ,\npages 539–545, Nantes, France, August 1992.\n[8] H. Hotelling. Analysis of a Complex of Statistical Vari-\nables Into Principal Components. Journal of Educa-\ntional Psychology , 24:417–441 and 498–520, 1933.\n[9] P. Hoyer. Non-negative matrix factorization with\nsparseness constraints. Journal of Machine Learning\nResearch , 5:1457–1469, 2004.\n[10] X. Hu, M. Bay, and J. S. Downie. Creating a Simpliﬁed\nMusic Mood Classiﬁcation Ground-Truth Set. In Pro-\nceedings of the 8th International Conference on Music\nInformation Retrieval (ISMIR 2007) , Vienna, Austria,\nSeptember 23–27 2007.\n[11] International society for music information retrieval,\nismir: Conferences, publications and related ac-\ntivities. http://www.ismir.net , 2009. (ac-\ncess: May 2009).\n[12] I. T. Jolliffe. Principial Component Analysis . Springer,\nNew York, NY , USA, 1986.\n[13] P. Knees, E. Pampalk, and G. Widmer. Artist Classi-\nﬁcation with Web-based Data. In Proceedings of the\n5th International Symposium on Music Information\nRetrieval (ISMIR 2004) , pages 517–524, Barcelona,\nSpain, October 10–14 2004.\n[14] P. Knees, T. Pohle, M. Schedl, and G. Widmer. Auto-\nmatically Describing Music on a Map. In Proceedings\nof 1st Workshop on Learning the Semantics of Audio\nSignals (LSAS 2006) , Athens, Greece, December 6–8\n2006.\n[15] P. Knees, T. Pohle, M. Schedl, and G. Widmer. A Mu-\nsic Search Engine Built upon Audio-based and Web-\nbased Similarity Measures. In Proceedings of the 30th\nAnnual International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval (SI-\nGIR 2007) , Amsterdam, the Netherlands, July 23–27\n2007.\n[16] M. Lan, C.-L. Tan, H.-B. Low, and S.-Y . Sung. A\nComprehensive Comparative Study on Term Weight-\ning Schemes for Text Categorization with Support Vec-\ntor Machines. In Special Interest Tracks and Posters of\nthe 14th International Conference on World Wide Web\n(WWW 2005) , pages 1032–1033, Chiba, Japan, May\n10–14 2005. ACM Press.\n[17] Last.fm - listen to internet radio and the largest mu-\nsic catalogue online. http://last.fm , 2008. (ac-\ncess: April 2009).\n[18] D. D. Lee and H. S. Seung. Learning the Parts of Ob-\njects by Non-negative Matrix Factorization. Nature ,\n401(6755):788–791, 1999.\n[19] H. P. Luhn. A Statistical Approach to Mechanized En-\ncoding and Searching of Literary Information. IBM\nJournal , pages 309–317, October 1957.[20] E. Pampalk. Aligned Self-Organizing Maps. In Pro-\nceedings of the Workshop on Self-Organizing Maps\n(WSOM 2003) , pages 185–190, Kitakyushu, Japan,\nSeptember 11–14 2003. Kyushu Institute of Technol-\nogy.\n[21] T. Pohle, P. Knees, M. Schedl, and G. Widmer.\nBuilding an Interactive Next-Generation Artist Rec-\nommender Based on Automatically Derived High-\nLevel Concepts. In Proceedings of the 5th Interna-\ntional Workshop on Content-Based Multimedia Index-\ning (CBMI’07) , Bordeaux, France, June 25–27 2007.\n[22] M. Rosvall and C. T. Bergstrom. Maps of information\nﬂow reveal community structure in complex networks.\nInProceedings of the National Academy of Sciences\nUSA, volume 105, pages 1118–1123, 2007.\n[23] G. Salton. The Use of Citations as an Aid to Auto-\nmatic Content Analysis. Technical Report ISR-2, Sec-\ntion III, Harvard Computation Laboratory, Cambridge,\nMA, USA, 1962.\n[24] G. Salton and C. Buckley. Term-weighting Approaches\nin Automatic Text Retrieval. Information Processing\nand Management , 24(5):513–523, 1988.\n[25] G. Salton, A. Wong, and C. S. Yang. A Vector Space\nModel for Automatic Indexing. Communications of the\nACM , 18(11):613–620, 1975.\n[26] Y . Tsuruoka and J. Tsujii. Bidirectional inference with\nthe easiest-ﬁrst strategy for tagging sequence data. In\nProceedings of HLT/EMNLP , pages 467–474, 2005.\n[27] well-formed.eigenfactor.org : Visualizing informa-\ntion ﬂow in science:. http://well-formed.\neigenfactor.org , 2009. (access: May 2009).\n[28] B. Whitman and S. Lawrence. Inferring Descriptions\nand Similarity for Music from Community Meta-\ndata. In Proceedings of the 2002 International Com-\nputer Music Conference (ICMC 2002) , pages 591–598,\nG¨oteborg, Sweden, September 16–21 2002.\n[29] W. Xu, X. Liu, and Y . Gong. Document Clustering\nBased on Non-negative Matrix Factorization. In Pro-\nceedings of the 26th Annual International ACM SIGIR\nConference on Research and Development in Informa-\ntion Retrieval (SIGIR 2003) , pages 267–273, Toronto,\nCanada, July 28–August 1 2003. ACM Press.\n[30] Y . Yang and J. O. Pedersen. A comparative study\non feature selection in text categorization. In D. H.\nFisher, editor, Proceedings of ICML-97, 14th Interna-\ntional Conference on Machine Learning , pages 412–\n420, Nashville, USA, 1997. Morgan Kaufman.\n[31] J. Zobel and A. Moffat. Exploring the Similarity Space.\nACM SIGIR Forum , 32(1):18–34, 1998.\n[32] J. Zobel and A. Moffat. Inverted Files for Text Search\nEngines. ACM Computing Surveys , 38:1–56, 2006.\n68"
    },
    {
        "title": "Who Is Who in the End? Recognizing Pianists by Their Final Ritardandi.",
        "author": [
            "Maarten Grachten",
            "Gerhard Widmer"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417109",
        "url": "https://doi.org/10.5281/zenodo.1417109",
        "ee": "https://zenodo.org/records/1417109/files/GrachtenW09.pdf",
        "abstract": "The performance of music usually involves a great deal of interpretation by the musician. In classical music, final ritardandi are emblematic for the expressive aspect of music performance. In this paper we investigate to what degree individual performance style has an effect on the form of final ritardandi. To this end we look at interonset-interval deviations from a performance norm. We define a criterion for filtering out deviations that are likely to be due to measurement error. Using a machine-learning classifier, we evaluate an automatic pairwise pianist identification task as an initial assessment of the suitability of the filtered data for characterizing the individual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, pianists can often be identified with accuracy significantly above baseline.",
        "zenodo_id": 1417109,
        "dblp_key": "conf/ismir/GrachtenW09",
        "keywords": [
            "interpretation",
            "musician",
            "interpretation",
            "classical music",
            "emblematic",
            "expressive aspect",
            "final ritardandi",
            "performance style",
            "individual playing style",
            "machine-learning classifier"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nWHO IS WHO IN THE END? RECOGNIZING PIANISTS BY THEIR FINAL\nRITARDANDI\nMaarten Grachten\nDept. of Computational Perception\nJohannes Kepler University, Linz, Austria\nmaarten.grachten@jku.atGerhard Widmer\nAustrian Research Institute for\nArtiﬁcial Intelligence, Vienna, Austria\nDept. of Computational Perception\nJohannes Kepler University, Linz, Austria\ngerhard.widmer@jku.at\nABSTRACT\nThe performance of music usually involves a great deal of\ninterpretation by the musician. In classical music, ﬁnal ri-\ntardandi are emblematic for the expressive aspect of music\nperformance. In this paper we investigate to what degree\nindividual performance style has an effect on the form of\nﬁnal ritardandi. To this end we look at interonset-interval\ndeviations from a performance norm. We deﬁne a criterion\nfor ﬁltering out deviations that are likely to be due to mea-\nsurement error. Using a machine-learning classiﬁer, we\nevaluate an automatic pairwise pianist identiﬁcation task\nas an initial assessment of the suitability of the ﬁltered data\nfor characterizing the individual playing style of pianists.\nThe results indicate that in spite of an extremely reduced\ndata representation, pianists can often be identiﬁed with\naccuracy signiﬁcantly above baseline.\n1. INTRODUCTION AND RELATED WORK\nThe performance of music usually involves a great deal\nof interpretation by the musician. This is particularly true\nof piano music from the romantic period, where perfor-\nmances are characterized by large ﬂuctuations of tempo\nand dynamics. The expressive interpretation of the music\nby the musician is crucial for listeners to understand emo-\ntional and structural aspects of the music (such as voice\nand phrase structure) [1–3]. In addition to these functional\naspects of expressive music performance, there is undeni-\nably an aspect of personal style. Skilled musicians tend\nto develop an individual way of performing, by means of\nwhich they give the music a unique aesthetic quality (a no-\ntable example of this is the legendary pianist Glenn Gould).\nAlthough the main focus in music performance research\nhas been on functional aspects of expression, some stud-\nies also deal with individual performance style. Through\nanalysis of listeners ratings on performances, Repp char-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.acterized pianists in terms of factors that were mapped to\nadjective pairs [4]. In [5], a principal component analysis\nof timing curves revealed a small set of signiﬁcant com-\nponents that seem to represent performance strategies that\nperformers combine in their performances. Furthermore, a\nmachine learning approach to performer identiﬁcation has\nbeen proposed by Stamatatos and Widmer [6], where per-\nformers are characterized by a set of features relating to\nscore-related patterns in timing, dynamics and articulation.\nSaunders et al. [7] represent patterns in timing and dynam-\nics jointly as strings of characters, and use string-kernel\nclassiﬁers to identify performers.\nIt is generally acknowledged in music performance re-\nsearch that, although widely used, the mechanical perfor-\nmance (implying constant tempo throughout a piece or mu-\nsical part) is not an adequate performance norm for study-\ning expressive timing, as it is not the way we generally\nbelieve the music should sound. As an alternative, models\nof expressive timing could be used, as argued in [8]. How-\never, only few models exist that model expressive timing\nin general [9, 10]. Because of the complexity and hetero-\ngeneity of expressive timing, most models only describe\nspeciﬁc phenomena, such as the timing of grace notes [11],\nor the ﬁnal ritardando [12, 13].\nThis paper addresses systematic differences in the per-\nformance of ﬁnal ritardandi by different pianists. In a pre-\nvious study [14] on the performance of ﬁnal ritardandi, a\nkinetic model [13] was ﬁtted to a set of performances. Al-\nthough in some cases systematic differences were found\nbetween pianists, in general the model parameters (describ-\ning the curvature and depth of the ritardando) tend to reﬂect\nprimarily aspects of the piece, rather than the individual\nstyle of the pianist. Given this result, a possible approach\nto study performer-speciﬁc timing in ritardandi would be\nby subtracting the ﬁtted model from the modeled timing\ndata and looking performer-speciﬁc patterns in the residu-\nals. A problem with this approach is that the kinetic model\nis arguably too simple, since it models tempo as a function\nof score time only, and is ignorant of any structural as-\npects of the music, which also have an effect of the tempo\ncurve [15]. As a result of this, residuals in the data with\nrespect to the ﬁtted model are likely to contain patterns re-\nlated to piece-speciﬁc aspects like rhythmic grouping.\n51Oral Session 1-B: Performance Recognition\nIn this study, in order to minimize the amount of piece-\nspeciﬁc information present in the residuals, we compute\nthe average performance per piece and subtract it from\neach performance of that piece. In addition to this, we\nﬁlter the residual data based on an estimation of its sig-\nniﬁcance. This estimation is obtained from an analysis of\ndata annotation divergences for a subset of the data. The\nresulting data contain the deviations from the common way\nof playing the ritardandi that are unlikely to be due to mea-\nsurement errors.\nOur long-term goal is to develop a thorough and sensi-\nble way of interpreting deviations of performance data with\nrespect to some performance norm , be it either a model, or\nas in this study, a norm derived from the data. To obtain\na ﬁrst impression of the potential of characterizing artists\nby this method of analyzing the data, we deﬁned a pair-\nwise pianist identiﬁcation task (as in [6]). Using a data\nset consisting of performances of ritardandi in Chopin’s\nNocturnes by a number of famous pianists, we show that\npianists can be identiﬁed based on regularities in the way\nthey deviate from the performance norm.\nIn section 2, we describe the acquisition and content of\nthe data set. Section 3 documents the data processing pro-\ncedure. Results of the pianist classiﬁcation task are pre-\nsented and discussed in section 4, and conclusions and fu-\nture work in section 5.\n2. DATA\nThe data used here consists in measurements of timing\ndata of musical performances taken from commercial CD\nrecordings of Chopin’s Nocturnes. The contents of the\ndata set are speciﬁed in table 1. We have chosen Chopin’s\nNocturnes since they exemplify classical piano music from\nthe romantic period, a genre which is characterized by the\nprominent role of expressive interpretation in terms of tempo\nand dynamics. Furthermore, the music is part of a well-\nknown repertoire, performed by many pianists, facilitating\nlarge scale studies.\nTempo in music is usually estimated from the interon-\nset intervals of successive events. A problematic aspect of\nthis is that when a musical passage contains few events, the\nobtained tempo information is sparse, and possibly unreli-\nable, thus not very suitable for studying tempo. Therefore,\nthrough inspection of the score, we selected those Noc-\nturnes whose ﬁnal passages have a relatively high note den-\nsity, and are more or less homogeneous in terms of rhythm.\nIn two cases (Op. 9 nr. 3 and Op. 48 nr. 1), the ﬁnal pas-\nsage consists of two clearly separated parts, both of which\nare performed individually with a ritardando. These ritar-\ndandi are treated separately (see table 1). In one case (Op.\n27 nr. 1), the best-suited passage is at the end of the ﬁrst\npart, rather than at the end (so strictly speaking, it is not a\nﬁnal ritardando).\nThe data were obtained in a semi-automated manner,\nusing a software tool [16] for automatic transcription of\nthe audio recordings. From the transcriptions generated in\nthis way, the segments corresponding to the ﬁnal ritardandi\nwere extracted and corrected manually by the authors, us-ingSonic Visualizer , a software tool for audio annotation\nand analysis [17].\n3. METHOD\nAs mentioned in section 1, the expressive timing data is\nexpected to have a strong component that is determined\nby piece-speciﬁc aspects like rhythmical structure and har-\nmony. In order to focus on pianist-speciﬁc aspects of tim-\ning, it is helpful to remove this component. In this section,\nwe ﬁrst describe how the IOI data are represented. We then\npropose a ﬁlter on the data based on an estimate of the mea-\nsurement error of IOI values. Finally, we describe a pianist\nidentiﬁcation task as an assessment of the suitability of the\nﬁltered data for characterizing the individual playing style\nof pianists.\n3.1 Calculation of deviations from the performance\nnorm\nThe performance norm used here is the average perfor-\nmance per piece. That is, for a piece k, LetMbe the\nnumber of pianists, and Nkbe the number of measured IOI\ntimes in piece k. We use vk,ito denote the vector of the Nk\nIOI values of pianist iin piecek. Correspondingly, uk,iis\nthe IOI vector of pianist ifor piecek, centered around zero\n(¯vk,ibeing the mean of all IOI’s in vk,i):\nuk,i=vk,i−¯vk,i (1)\nThe performance norm akfor piecekis deﬁned as the\naverage over pianists per IOI value:\nak(j) =1\nMM/summationdisplay\ni=1uk,i(j) (2)\nwhere ak(j)is thej-th IOI value of the average perfor-\nmance of piece k.\nFigure 1 shows the performance norms obtained in this\nway. Note that most performance norms show a two stage\nritardando, in which a gradual slowing down is followed\nby a stronger decrease in tempo, a general trend that is also\nobserved in [12]. The plots show furthermore that in addi-\ntion to a global slowing down, ﬁner grained timing struc-\nture is present in some pieces.\n3.2 Estimation of measurement error\nAn inherent problem of empirical data analysis is the pres-\nence of measurement errors. As described above, the tim-\ning data from which the tempo curves are generated is ob-\ntained by measurement of beat times from audio ﬁles. The\ndata is manually corrected, but even manually the exact\ntime of some note onsets is hard to identify, especially\nwhen the pianist plays very softly while using the sus-\ntain pedal. Therefore, it is relevant to investigate to which\ndegree different beat time annotations of the same perfor-\nmance differ from each other. This gives us an idea of the\nsize of the measurement error, and allows us to distinguish\nsigniﬁcant deviations from the performance norm from the\nnon-signiﬁcant deviations.\n5210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nPianist Year Op.9 nr.3 rit1 Op.9 nr.3 rit2 Op.15 nr.1 Op.15 nr.2 Op.27 nr.1 Op.27 nr.2 Op.48 nr.1 rit1 Op.48 nr.1 rit2\nArgerich 1965 X\nArrau 1978 X X X X X X X X\nAshkenazy 1985 X X X X X X X X\nBarenboim 1981 X X X X X X X X\nBiret 1991 X X X X X X X X\nEngerer 1993 X X X X X X X X\nFalvai 1997 X X X X X X X X\nHarasiewicz 1961 X X X X X X X X\nHewitt 2003 X X X X X X X X\nHorowitz 1957 X X\nKissin 1993 X X\nKollar 2007 X X X X X X X\nLeonskaja 1992 X X X X X X X X\nMaisenberg 1995 X\nMertanen 2001 X X X X X X\nMertanen 2002 X X\nMertanen 2003 X X\nOhlsson 1979 X X X X X X X X\nPerahia 1994 X\nPires 1996 X X X X X X X X\nPollini 2005 X X X X X X X X\nRichter 1968 X\nRubinstein 1937 X X X X X X X X\nRubinstein 1965 X X X X X X X X\nTsong 1978 X X X X X X X X\nVasary 1966 X X X X X X X\nWoodward 2006 X X X X X X X X\nd´Ascoli 2005 X X X X X X X X\nTable 1 . Performances used in this study. The symbol “X” denotes the presence of the corresponding combination of\npianist/piece in the data set. The additions “rit1” and “rit2” refer to two distinct ritardandi within the same piece\nop9_3_rit1\nop9_3_rit2\nop15_1\nop15_2\nop27_1\nop27_2\nop48_1_rit1\nop48_1_rit2\nFigure 1 . The average performance per ritardando. Both\nscore time (horizontal axis) and tempo (vertical axis) are\nnormalized\nTo this end, a subset of the data containing seven perfor-\nmances of various performers and different pieces has beenannotated twice, by two different persons.1This set in to-\ntal contains 304 time points to be measured. For each beat\na pair of annotated beat times was available after annota-\ntion by both annotators, from which the absolute pairwise\ndifferences were calculated.\nFigure 2 shows a scatter plot of absolute pairwise dif-\nferences of measured IOI time versus the beat duration.2\nNote that beat durations have been calculated from note in-\nteronset times that were sometimes at a substantially faster\npace than the beat. Hence, a beat duration of, say, 14 sec-\nonds does not imply that two measured points are actually\n14 seconds apart. It can be observed from the plot that at\nslower tempos, there is more agreement between annota-\ntors about the onset times of notes. This is likely to be\neither because the slower parts tend to be played in a more\narticulate way, or simply because of the lower note density,\nwhich makes it easier to determine note onsets precisely.\nThe line in ﬁgure 2 shows the function that we use as\na criterion to either accept or reject a particular IOI data\npoint for further analysis. More speciﬁcally, the function\nspeciﬁes how far a data point must be away from the per-\nformance norm in order to be considered as a signiﬁcant\ndeviation. Conversely, we consider deviations of points\ncloser to the norm too likely caused by measurement er-\nrors. The criterion is rather simple, and deﬁnes .2 seconds\nas an absolute minimum for deviations, with an increas-\ning threshold for measurements at higher tempos (shorter\nbeat durations), to accommodate for the increasing mea-\nsurement differences observed in the data. The constants\nin the the function have been chosen manually, ensuring\n1Because of the size of the data set, and the effort that manual correc-\ntion implies, it was not feasible to annotate the complete data set multiple\ntimes\n2bybeat we mean score unit duration, rather than a perceived pulse\n53Oral Session 1-B: Performance Recognition\n 0 0.1 0.2 0.3 0.4 0.5 0.6\n 0 2 4 6 8 10 12 14Differences in annotated beat time (seconds)\nBeat duration (seconds)y(x) = .09+e-2.5x+1Figure 2 . Scatter plot of absolute beat time annotation dif-\nferences versus beat duration, between two annotators\nthat a substantial amount of the measurement deviations\nin the scatter plot ( >95%) are excluded by the criterion.\nThis approach can admittedly be improved. Ideally, taking\ninto account the signiﬁcance of deviations from the perfor-\nmance norm should be done by a weighting of data points\nthat is inversely proportional to the likelihood of being due\nto measurement errors.\nWith the current criterion we ﬁlter the data by keeping\nonly those data points that satisfy the inequality:\nuk,i(j)>0.09 + exp [ −2.5(ak(j) +¯vk,i) + 1.0] (3)\nThe set of data points after ﬁltering is displayed for two\npianists in ﬁgure 3. The left plot shows the signiﬁcant\ndeviations from the performance norm over all ritardandi\nperformed by Falvai. The right plot shows those of Leon-\nskaja. In order to compare the ritardandi from different\npieces (with differing length and different number of mea-\nsured IOI’s), time has been normalized per piece. Note that\na large part of Falvai’s IOI deviations has been ﬁltered out\nbased on their size. This means that Falvai’s ritardandi are\nare mostly in agreement with the performance norm. In-\nterestingly, the endings of Falvai’s ritardandi deviate in a\nvery consistent way by being slightly faster than the norm\nuntil the last few notes, which tend to be delayed more than\nnormal. Leonskaja’s IOI deviations are more diverse and\nappear to be more piece dependent. A more in-depth inves-\ntigation seems worthwhile here, but is beyond the scope of\nthis article.\n3.3 Evaluation of the data: automatic identiﬁcation of\npianists\nIn order to verify whether the residual timing data after\nsubtracting the norm and ﬁltering with the measurement\nerror criterion in general carry information about the per-\nforming pianist, we have designed a small experiment. In\nthis experiment we summarize the residual timing data by\nfour attributes and apply a multilayer perceptron [18] (astandard machine learning algorithm, as available in the\nWeka toolbox for data mining and machine learning) to\nperform binary classiﬁcation for all pairs of pianists in the\ndata set.3The training instances (ritardandi of a particular\npiece performed by a particular pianist) containing varying\nnumbers of IOI deviation values, each associated with a\nnormalized score time value, describing where the IOI de-\nviation occurs in the ritardando (0 denoting the beginning\nof the ritardando, and 1 the end). In order to use these data\nfor automatic classiﬁcation, they must be converted to data\ninstances with a ﬁxed number of attribute-value pairs. We\nchoose an extremely simple approach, in which we repre-\nsent a set of IOI deviation / score time pairs by the mean\nand standard deviation of the IOI values and the mean and\nstandard deviations of the normalized time values. Thus,\nwe effectively model the data by describing the size and\nlocation of the area where IOI deviation values tend to oc-\ncur in the plots of ﬁgure 3.\n4. RESULTS AND DISCUSSION\nThe pairwise pianist classiﬁcation task is executed as fol-\nlows: for each possible pair of pianists, the ritardandi of\nboth pianists are pooled to form the data set for evaluating\nthe classiﬁer. The training set in most cases contains 16\ninstances, one for each of the eight pieces, for each of the\ntwo pianists. The pianists from whom less than 6 perfor-\nmances were contained in the data set were not included in\nthe test. The data set was used to evaluate the multilayer\nperceptron using 10-fold cross-validation. This was done\nfor all 171 combinations of 19 pianists. The results are\ncompared to a baseline algorithm that predicts the mode of\nthe target concept, the pianist, in the training data.\nThe classiﬁcation results on the test data are summa-\nrized in tables 2 and 3. Table 2 shows the proportion of\npairwise identiﬁcation tasks where the multilayer percep-\ntron classiﬁed above, at, and, below baseline classiﬁcation,\nrespectively. The top row presents the results for the con-\ndition where the IOI deviation data has been ﬁltered us-\ning the measurement error criterion, as explained in sub-\nsection 3.2. The bottom row correspond to the condition\nwhere no such ﬁltering was applied.\nThe measurement error ﬁltering clearly leads to an im-\nprovement of classiﬁcation accuracy. With ﬁltering, the\npercentage of pianist identiﬁcation tasks that are executed\nwith an accuracy that is signiﬁcantly ( α=.05) above base-\nline accuracy, is 32%. Although this percentage does not\nseem very high, it must be considered that the amount of\ninformation available to the classiﬁer is very small. Firstly,\nthe ritardandi are only short fragments of the complete per-\nformances. Secondly, the training sets within a 10-fold\ncross-validation never contain more than seven ritardandi\nof a single pianist. Lastly, the IOI deviation information\navailable has been summarized very coarsely, by a mean\nand standard deviation of the values in the time and IOI\ndimension. This result implies that larger deviations from\n3For some pianists less than six performances were available; Those\npianists have not been included in the experiment.\n5410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n-20-10 0 10 20 30\n 0  0.2  0.4  0.6  0.8  1Deviation from performance norm (BPM)\nNormalized ritardando timeFalvai\nop15_1\nop15_2\nop27_1op27_2\nop9_3_rit1\nop9_3_rit2op48_1_rit2\nop48_1_rit2\n-20-10 0 10 20 30\n 0  0.2  0.4  0.6  0.8  1Deviation from performance norm (BPM)\nNormalized ritardando timeLeonskaja\nop15_1\nop15_2\nop27_1op27_2\nop9_3_rit1\nop9_3_rit2op48_1_rit2\nop48_1_rit2Figure 3 . Deviations from the performance norm after applying the measurement error criterion; Left: Falvai; Right:\nLeonskaja\nthe performance norm by individual pianists are at least to\nsome degree pianist speciﬁc, and not just piece speciﬁc.\nWe wish to emphasize that by no means we claim that\nthe speciﬁc form of the measurement error criterion we\nproposed in subsection 3.2 is crucial for the success of of\npianist identiﬁcation. Other ﬁltering criteria might work\nequally well or better. Note however that there is a trade\noff between avoiding the disturbing effect of measurement\nerrors on the one hand, and a reduction of available data\non the other. A more elegant approach to canceling the\neffect of measurement errors would be to use a weighting\ncriterion rather than a ﬁltering criterion.\nWithout ﬁltering, accuracy is even signiﬁcantly below\nthe baseline in 19% of the cases. The fact that under this\ncondition accuracy does not often surpass the baseline is\nnot surprising, since the unﬁltered data contains all avail-\nable IOI deviation values, equally distributed over time. A\nconsequence of this that mean and standard deviation of\nthe normalized times associated to the IOI data are con-\nstant. This reduces the available information so much that\nit is unrealistic to expect above baseline accuracy. That the\nprediction accuracy is signiﬁcantly below baseline is more\nsurprising. Given that the performance norm is subtracted\nfrom the original timing data per piece, a strong interfer-\nence of the piece with the pianist identiﬁcation is not to be\nexpected. A possible explanation for this result could be\nthat there are multiple distinct performance strategies. Ob-\nviously, the average performance as a performance norm\nis not adequate for this situation, where multiple perfor-\nmance norms are present. If two pianists choose a similar\nstrategy, their residual IOI values after subtracting the av-\nerage performance may still be more similar to each other\nthan to their own IOI values in a different piece.\nTable 3 shows the average identiﬁcation accuracy over\nall identiﬁcation-tasks that involve a speciﬁc pianist. High\naccuracy could indicate that a pianist plays both consis-\ntently, and distinctively. By playing consistently we mean\nthat particular IOI deviations tend to occur at the similarProcedure <baseline baseline >baseline\nwith ﬁltering 0 (0%) 116 (68%) 55 (32%)\nwithout ﬁltering 33 (19%) 131 (76%) 7 (4%)\nTable 2 . Number of 10-fold cross-validated pairwise pi-\nanist classiﬁcation tasks with results over, at, and below\nbaseline results, respectively ( α=.05)\npositions in the ritardando, as observed in the case of Fal-\nvai, in ﬁgure 3 (see also [19] for a discussion of performer\nconsistency). Playing distinctively means that no other pi-\nanist has similar IOI deviations at similar positions. Con-\nversely, a low identiﬁcation accuracy could point to either\na varied way of performing ritardandi of different pieces,\nor playing ritardandi in particular pieces in a way that is\nsimilar to the way (some) other pianists play them, or both.\n5. CONCLUSIONS AND FUTURE WORK\nRitardandi in musical performances are good examples of\nthe expressive interpretation of the score by the pianist.\nWe have investigated the possibility of automatically iden-\ntifying pianists by the way they perform ritardandi. More\nspeciﬁcally, we have reported an initial experiment in which\nwe use IOI deviations from a performance norm (the aver-\nage performance) to distinguish pairs of pianists. Further-\nmore, we have introduced a simple ﬁltering criterion that\nis intended to remove parts of the data that are likely to be\ndue to measurement errors. Although more sophisticated\nmethods for dealing with measurement error can certainly\nbe developed, the ﬁltering method improved the accuracy\nof pianist identiﬁcation substantially.\nContinued work should include the development of a\nmore gradual way to deal with the signiﬁcance of IOI devi-\nations, rather than an all-or-nothing ﬁltering method. Also,\nbetter models of expressive timing and tempo are needed\nto serve as a performance norm. In this work we have em-\nployed the average performance as a substitute norm, but it\n55Oral Session 1-B: Performance Recognition\nPianist avg. % correct\nLeonskaja 65.31\nPollini 64.83\nVasary 63.50\nOhlsson 62.28\nMertanen 62.06\nBarenboim 61.69\nFalvai 57.42\nEngerer 54.33\nHewitt 53.50\nWoodward 53.47\nBiret 51.47\nPires 51.03\nTsong 50.17\nHarasiewicz 49.78\nKollar 49.33\nd´Ascoli 48.06\nAshkenazy 47.69\nRubinstein 45.83\nArrau 43.53\nTable 3 . Average identiﬁcation accuracy per pianist on test\ndata\nis obvious that a norm should be independent of the data.\n6. ACKNOWLEDGMENTS\nWe wish to thank Werner Goebl and Bernhard Niedermayer\nfor their help in the acquisition of the timing data from\nthe audio recordings. This work is funded by the Aus-\ntrian National Research Fund (FWF) under project number\nP19349-N15.\n7. REFERENCES\n[1] E. F. Clarke. Generative principles in music. In J.A.\nSloboda, editor, Generative Processes in Music: The\nPsychology of Performance, Improvisation, and Com-\nposition . Oxford University Press, 1988.\n[2] P. Juslin and J. Sloboda, editors. Music and Emotion:\nTheory and Research . Oxford University Press, 2001.\n[3] C. Palmer. Music performance. Annual Review of Psy-\nchology , 48:115–138, 1997.\n[4] B. H. Repp. Patterns of expressive timing in perfor-\nmances of a Beethoven minuet by nineteen famous pi-\nanists. Journal of the Acoustical Society of America ,\n88:622–641, 1990.\n[5] B. H. Repp. Diversity and commonality in music per-\nformance - An analysis of timing microstructure in\nSchumann’s “Tr ¨aumerei”. Journal of the Acoustical\nSociety of America , 92(5):2546–2568, 1992.\n[6] E. Stamatatos and G. Widmer. Automatic identiﬁcation\nof music performers with learning ensembles. Artiﬁcial\nIntelligence , 165(1):37–56, 2005.\n[7] C. Saunders, D. Hardoon, J. Shawe-Taylor, and\nG. Widmer. Using string kernels to identify famous\nperformers from their playing style. Intelligent Data\nAnalysis , 12(4):425–450, 2008.[8] W. L. Windsor and E. F. Clarke. Expressive timing and\ndynamics in real and artiﬁcial musical performances:\nusing an algorithm as an analytical tool. Music Percep-\ntion, 15(2):127–152, 1997.\n[9] N.P. Todd. A computational model of rubato. Contem-\nporary Music Review , 3 (1), 1989.\n[10] A. Friberg. Generative rules for music performance: A\nformal description of a rule system. Computer Music\nJournal , 15 (2):56–71, 1991.\n[11] R. Timmers, R.and Ashley, P. Desain, H. Honing,\nand L. Windsor. Timing of ornaments in the theme of\nBeethoven’s Paisiello Variations: Empirical data and a\nmodel. Music Perception , 20(1):3–33, 2002.\n[12] J. Sundberg and V . Verrillo. On the anatomy of the re-\ntard: A study of timing in music. Journal of the Acous-\ntical Society of America , 68(3):772–779, 1980.\n[13] A. Friberg and J. Sundberg. Does music performance\nallude to locomotion? a model of ﬁnal ritardandi de-\nrived from measurements of stopping runners. Jour-\nnal of the Acoustical Society of America , 105(3):1469–\n1484, 1999.\n[14] M. Grachten and G. Widmer. The kinematic rubato\nmodel as a means of studying ﬁnal ritards across pieces\nand pianists. In Proceedings of the 6th Sound and Mu-\nsic Computing Conference , 2009.\n[15] H. Honing. Is there a perception-based alternative to\nkinematic models of tempo rubato? Music Perception ,\n23(1):79–85, 2005.\n[16] B. Niedermayer. Non-negative matrix division for the\nautomatic transcription of polyphonic music. In Pro-\nceedings of the 9th International Conference on Music\nInformation Retrieval . ISMIR, 2008.\n[17] C.L. Cannam, M. Sandler, and J.P. Bello. The sonic vi-\nsualiser: A visualisation platform for semantic descrip-\ntors from musical signals. In Proceedings of the 7th\nInternational Conference on Music Information Re-\ntrieval . ISMIR, 2006.\n[18] D. E. Rumelhart and J. L. McClelland, editors. Parallel\nDistributed Processing , volume 1. MIT Press, 1986.\n[19] S. T. Madsen and G. Widmer. Exploring pianist per-\nformance styles with evolutionary string matching. In-\nternational Journal on Artiﬁcial Intelligence Tools ,\n15(4):495–513, 2006. Special Issue on Artiﬁcial Intel-\nligence in Music and Art.\n56"
    },
    {
        "title": "A Mid-Level Representation for Capturing Dominant Tempo and Pulse Information in Music Recordings.",
        "author": [
            "Peter Grosche",
            "Meinard Müller"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416016",
        "url": "https://doi.org/10.5281/zenodo.1416016",
        "ee": "https://zenodo.org/records/1416016/files/GroscheM09.pdf",
        "abstract": "Automated beat tracking and tempo estimation from music recordings become challenging tasks in the case of nonpercussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation which captures predominant local pulse information. To this end, we first derive a tempogram by performing a local spectral analysis on a previously extracted, possibly very noisy onset representation. From this, we derive for each time position the predominant tempo as well as a sinusoidal kernel that best explains the local periodic nature of the onset representation. Then, our main idea is to accumulate the local kernels over time yielding a single function that reveals the predominant local pulse (PLP). We show that this function constitutes a robust mid-level representation from which one can derive musically meaningful tempo and beat information for non-percussive music even in the presence of significant tempo fluctuations. Furthermore, our representation allows for incorporating prior knowledge on the expected tempo range to exhibit information on different pulse levels.",
        "zenodo_id": 1416016,
        "dblp_key": "conf/ismir/GroscheM09",
        "keywords": [
            "beat tracking",
            "tempo estimation",
            "nonpercussive music",
            "soft note onsets",
            "time-varying tempo",
            "mid-level representation",
            "tempogram",
            "local spectral analysis",
            "temporal kernel",
            "local pulse information"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nA MID-LEVEL REPRESENTATION FOR CAPTURING DOMINANT\nTEMPO AND PULSE INFORMATION IN MUSIC RECORDINGS\nPeter Grosche and Meinard M ¨uller\nSaarland University and MPI Informatik, Saarbr ¨ucken, Germany\n{pgrosche,meinard}@mpi-inf.mpg.de\nABSTRACT\nAutomated beat tracking and tempo estimation from music\nrecordings become challenging tasks in the case of non-\npercussive music with soft note onsets and time-varying\ntempo. In this paper, we introduce a novel mid-level rep-\nresentation which captures predominant local pulse infor-\nmation. To this end, we ﬁrst derive a tempogram by per-\nforming a local spectral analysis on a previously extracted ,\npossibly very noisy onset representation. From this, we de-\nrive for each time position the predominant tempo as well\nas a sinusoidal kernel that best explains the local periodic\nnature of the onset representation. Then, our main idea is\nto accumulate the local kernels over time yielding a single\nfunction that reveals the predominant local pulse (PLP).\nWe show that this function constitutes a robust mid-level\nrepresentation from which one can derive musically mean-\ningful tempo and beat information for non-percussive mu-\nsic even in the presence of signiﬁcant tempo ﬂuctuations.\nFurthermore, our representation allows for incorporating\nprior knowledge on the expected tempo range to exhibit\ninformation on different pulse levels.\n1. INTRODUCTION\nThe automated extraction of tempo and beat information\nfrom audio recordings has been a central task in music\ninformation retrieval. To accomplish this task, most ap-\nproaches proceed in two steps. In the ﬁrst step, positions\nof note onsets in the music signal are estimated. Here, one\ntypically relies on the fact that note onsets often go along\nwith a sudden change of the signal’s energy and spectrum,\nwhich particularly holds for instruments such as the piano,\nguitar, or percussive instruments. This property allows fo r\nderiving so-called novelty curves , the peaks of which yield\ngood indicators for note onset candidates [1, 15]. In the\nsecond step, the novelty curves are analyzed with respect\nto reoccurring or quasiperiodic patterns. Here, generally\nspoken, one can roughly distinguish between three differ-\nent methods. The autocorrelation method allows for de-\ntecting periodic self-similarities by comparing a novelty\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .curve with time-shifted copies [5, 12]. Another widely\nused method is based on a bank of comb ﬁlter resonators,\nwhere a novelty curve is compared with templates consist-\ning of equally spaced spikes or pulses representing various\nfrequencies and phases [10, 14]. Similarly, one can use\na short-time Fourier transform to derive a time-frequency\nrepresentation of the novelty curve [12]. Here, the novelty\ncurve is compared with templates consisting of sinusoidal\nkernels each representing a speciﬁc frequency. Each of the\nmethods reveals periodicity properties of the underlying\nnovelty curve, from which one can estimate the tempo or\nbeat structure. The intensities of the estimated periodici ty,\ntempo, or beat properties typically change over time and\nare often visualized by means of spectrogram-like repre-\nsentations referred to as tempogram [3],rhythmogram [9],\norbeat spectrogram [6].\nRelying on previously extracted note onset indicators,\ntempo and beat tracking tasks become much harder for\nnon-percussive music, where one often has to deal with\nsoft onsets or blurred note transitions. This results in rat her\nnoisy novelty curves, exhibiting many spurious peaks. As\na consequence, more reﬁned methods have to be used for\ncomputing the novelty curves, e. g., by analyzing the sig-\nnal’s spectral content, pitch, or phase [1, 8, 15]. Even more\nchallenging becomes the detection of locally periodic pat-\nterns in the case that the music recording reveals signif-\nicant tempo changes, which typically occur in expressive\nperformances of classical music as a result of ritardandi,\naccelerandi, fermatas, and so on [4]. Finally, the extrac-\ntion problem is complicated by the fact that the notions of\ntempo and beat are ill-deﬁned and highly subjective due\nto the complex hierarchical structure of rhythm [2]. For\nexample, there are various levels that are presumed to con-\ntribute to the human perception of tempo and beat. Most of\nthe previous work focuses on determining musical pulses\non the tactus (the foot tapping rate or beat [10]) or mea-\nsure level, but only few approaches exist for analyzing the\nsignal on the ﬁner tatum level [13]. Here, a tatum ortem-\nporal atom refers to the fastest repetition rate of musically\nmeaningful accents occurring in the signal.\nIn this paper, we introduce a novel mid-level represen-\ntation that unfolds predominant local pulse (PLP) infor-\nmation from music signals even for non-percussive mu-\nsic with soft note onsets and changing tempo. Avoiding\nthe explicit determination of note onsets, we derive a tem-\npogram by performing a local spectral analysis on a pos-\nsibly very noisy novelty curve. From this, we estimate for\n189Oral Session 2: Tempo and Rhythm\neach time position a sinusoidal kernel that best explains th e\nlocal periodic nature of the novelty curve. Since there may\nbe a number of outliers among these kernels, one usually\nobtains unstable information when looking at these ker-\nnels in a one-by-one fashion. Our idea is to accumulate\nall these kernels over time to obtain a mid-level represen-\ntation, which we refer to as predominant local pulse (PLP)\ncurve. As it turns out, PLP curves are robust to outliers and\nreveal musically meaningful periodicity information even\nin the case of poor onset information. Note that it is not\nthe objective of our mid-level representation to directly r e-\nveal musically meaningful high-level information such as\ntempo, beat level, or exact onset positions. Instead, our\nrepresentation constitutes a ﬂexible tool for revealing lo -\ncally predominant information, which may then be used\nfor tasks such as beat tracking, tempo and meter estima-\ntion, or music synchronization [10, 11, 14]. In particular,\nour representation allows for incorporating prior knowl-\nedge, e. g., on the expected tempo range, to exhibit infor-\nmation on different pulse levels. In the following sections ,\nwe give various examples to illustrate our concept.\nThe remainder of this paper is organized as follows. In\nSect. 2, we review the concept of novelty curves while in-\ntroducing a variant used in the subsequent sections. Sect. 3\nconstitutes the main contribution of this paper, where we\nintroduce the tempogram and the PLP mid-level represen-\ntation. Examples and experiments are described in Sect. 4\nand prospects of future work are sketched in Sect. 5.\n2. NOVELTY CURVE\nCombining various ideas from [1, 10, 15], we now exem-\nplarily describe an approach for computing novelty curves\nthat indicate note onset candidates. Note that the particul ar\ndesign of the novelty curve is not in the focus of this pa-\nper. Our mid-level representation as introduced in Sect. 3\nis designed to work even for noisy novelty curves with\na poor pulse structure. Naturally, the overall result may\nbe improved by employing more reﬁned novelty curves\nas suggested in [15]. Given a music recording, a short-\ntime Fourier transform is used to obtain a spectrogram\nX= (X(k,t))k,twithk∈[1 :K] := {1,2,... ,K }\nandt∈[1 :T]. Here, Kdenotes the number of Fourier\ncoefﬁcients, Tdenotes the number of frames, and X(k,t)\ndenotes the kthFourier coefﬁcient for time frame t. In\nour implementation, each time parameter tcorresponds to\n23milliseconds of the audio. Next, we apply a logarithm\nto the magnitude spectrogram |X|of the signal yielding\nY:= log(1 + C· |X|)for a suitable constant C > 1,\nsee [10]. Such a compression step not only accounts for\nthe logarithmic sensation of sound intensity but also allow s\nfor adjusting the dynamic range of the signal to enhance\nthe clarity of weaker transients, especially in the high-\nfrequency regions. In our experiments, we use the value\nC= 1000 . To obtain a novelty curve, we basically com-\npute the discrete derivative of the compressed spectrum Y.\nMore precisely, we sum up only positive intensity changes\nto emphasize onsets while discarding offsets to obtain the\n0 1 2 3 4 5 6 7 8 9 1000.511.520 1 2 3 4 5 6 7 8 9 100\n  \n0 1 2 3 4 5 6 7 8 9 1050100150200250300350400450500\n24681012x 10−3 0 1 2 3 4 5 6 7 8 9 1000.511.5\n  \n0 1 2 3 4 5 6 7 8 9 10150200250300\n0 1 2 3 4 5 6 7 8 9 1002.55(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\nFigure 1: Excerpt of Shostakovich’s second Waltz from Jazz\nSuite No. 2. The audio recording is a temporally warped or-\nchestral version conducted by Yablonsky with a linear tempo in-\ncrease ( 216−265BPM). (a)Piano-reduced score of measures\n13−24.(b)Ground truth onsets. (c)Novelty curve ∆with local\nmean. (d)Novelty curve ¯∆.(e)Magnitude tempogram |T |for\nKS = 4 sec .(f)Estimated tempo τt.(g)PLP curve Γ.\nnovelty function ∆ : [1 : T−1]→R:\n∆(t) :=/summationtextK\nk=1|Y(k,t+ 1)−Y(k,t)|≥0 (1)\nfort∈[1 :T−1], where |x|≥0:=xfor a non-negative\nreal number xand|x|≥0:= 0 for a negative real number\nx. Fig. 1c shows the resulting curve for a music record-\ning of an excerpt of Shostakovich’s second Waltz from the\nJazz Suite No. 2. To obtain our ﬁnal novelty function ¯∆,\nwe subtract the local average and only keep the positive\npart (half-wave rectiﬁcation), see Fig. 1d. In our imple-\nmentation, we actually use a higher-order smoothed differ-\nentiator. Furthermore, we process the spectrum in a band-\nwise fashion [14] using 5bands. The resulting 5novelty\ncurves are weighted and summed up to yield the ﬁnal nov-\nelty function. For details, we refer to the quoted literatur e.\n3. TEMPOGRAM AND PLP CURVE\nWe now analyze the novelty curve with respect to local\nperiodic patterns. Note that the novelty curve as intro-\nduced above typically reveals the note onset candidates\nin form of impulse-like spikes. Due to extraction errors\nand local tempo variations, the spikes may be noisy and\n19010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nirregularly spaced over time. Dealing with spiky novelty\ncurves, autocorrelation methods [5] as well as comb ﬁl-\nter techniques [14] encounter difﬁculties in capturing the\nquasiperiodic information. This is due to the fact that spik y\nstructures are hard to identify by means of spiky analysis\nfunctions in the presence of irregularities. In such cases,\nsmoothly spread analysis functions such as sinusoids are\nmuch better suited to detect locally distorted quasiperiod ic\npatterns. Therefore, similar to [12], we use a short-time\nFourier transform to analyze the novelty curves. More pre-\ncisely, let ¯∆be the novelty curve as described in Sect. 2.\nTo avoid boundary problems, we assume that ¯∆is deﬁned\nonZby setting ¯∆(t) := 0 fort∈Z\\[1 :T−1]. Further-\nmore, we ﬁx a window function W:Z→Rcentered at\nt= 0with support [−N:N]. In our experiments, we use\na Hann window of size 2N+ 1. Then, for a frequency pa-\nrameter ω∈R≥0, the complex Fourier coefﬁcient F(t,ω)\nis deﬁned by\nF(t,ω) =/summationtext\nn∈Z¯∆(n)·W(n−t)·e−2πiωn.(2)\nNote that the frequency ωcorresponds to the period 1/ω.\nIn the context of beat tracking, we rather think of tempo\nmeasured in beats per minutes (BPM) than of frequency\nmeasured in Hertz (Hz). Therefore, we use a tempo pa-\nrameter τsatisfying the equation τ= 60·ω.\nSimilar to a spectrogram, we deﬁne a tempogram which\ncan be seen as a two-dimensional time-pulse representa-\ntion indicating the strength of the local pulse over time.\nHere, intuitively, a pulse can be thought of a periodic se-\nquence of accents, spikes or impulses. We specify the peri-\nodicity of a pulse in terms of a tempo value (in BPM). The\nsemantic level of a pulse is not speciﬁed and may refer to\nthe tatum, the tactus, or measure level. Now, let Θ⊂R>0\nbe a ﬁnite set of tempo parameters. In our experiments, we\nmostly use the set Θ = [30 : 500] , covering the (integer)\nmusical tempi between 30and500BPM. Here, the bounds\nare motivated by the assumption that only events showing\na temporal separation between 120milliseconds and 2sec-\nonds contribute to the perception of rhythm [2]. Then, the\ntempogram is a function T: [1 :T]×Θ→Cdeﬁned by\nT(t,τ) =F(t,τ/60). (3)\nFor an example, we refer to Fig. 1e, which shows the mag-\nnitude tempogram |T |for our Shostakovich example. Note\nthat the complex-valued tempogram contains magnitude as\nwell as phase information. We now make use of both,\nthe magnitudes and the phases given by T, to derive a\nmid-level representation that captures the predominant lo-\ncal pulse (PLP) of accents in the underlying music signal.\nHere, the term predominant pulse refers to the pulse that is\nmost noticeable in the novelty curve in terms of intensity.\nFurthermore, our representation is local in the sense that it\nyields the predominant pulse for each time position, thus\nmaking local tempo information explicit, see also Fig. 1f.\nAlso, the semantic level of the pulse may change over time,\nsee Fig. 4a. This will be discussed in detail in Sect. 4.\nTo compute our mid-level representation, we determine\nfor each time position t∈[1 :T]the tempo parameter2 3 4 5 6 7 8−10123\n2 3 4 5 6 7 8−10123\n2 3 4 5 6 7 8−10123\n2 3 4 5 6 7 8−3−2−10123(a)\n(b)\nFigure 2: (a) Optimal sinusoidal kernel κtfor various time pa-\nrameters tusing a kernel size of 4seconds for the novelty curve\nshown in Fig. 1d. (b)Accumulation of all kernels. From this, the\nPLP curve Γ(see Fig. 1f) is obtained by half-wave rectiﬁcation.\nτt∈Θthat maximizes the magnitude of T(t,τ):\nτt:= argmaxτ∈Θ|T(t,τ)|. (4)\nThe corresponding phase ϕtis deﬁned by [11]:\nϕt:=1\n2πarccos/parenleftbiggRe(T(t,τt))\n|T(t,τt)|/parenrightbigg\n. (5)\nUsing τtandϕt, the optimal sinusoidal kernel κt:Z→R\nfort∈[1 :T]is deﬁned as the windowed sinusoid\nκt(n) :=W(n−t)cos(2 π(τt/60·n−ϕt)) (6)\nforn∈Z. Fig. 2a shows various optimal sinusoidal ker-\nnels for our Shostakovich example. Intuitively, the sinu-\nsoidκtbest explains the local periodic nature of the nov-\nelty curve at time position twith respect to the set Θ. The\nperiod 60/τtcorresponds to the predominant periodicity of\nthe novelty curve and the phase information ϕttakes care\nof accurately aligning the maxima of κtand the peaks of\nthe novelty curve. The properties of the kernels κtdepend\nnot only on the quality of the novelty curve, but also on the\nwindow size 2N+1ofWand the set of frequencies Θ. In-\ncreasing the parameter Nyields more robust estimates for\nτtat the cost of temporal ﬂexibility. In our experiments,\nwe chose a window length of 4to12seconds. In the fol-\nlowing, this duration is referred to as kernel size (KS).\nThe estimation of optimal sinusoidal kernels for nov-\nelty curves with a strongly corrupted pulse structure is sti ll\nproblematic. This particularly holds in the case of small\nkernel sizes. To make the periodicity estimation more ro-\nbust, our idea is to accumulate these kernels over all time\npositions to form a single function instead of looking at the\nkernels in a one-by-one fashion. More precisely, we deﬁne\na function Γ : [1 : T]→R≥0as follows:\nΓ(n) =/summationtext\nt∈[1:T]|κt(n)|≥0 (7)\nforn∈[1 :T], see Fig. 2b. The resulting function is our\nmid-level representation referred to as PLP curve . Fig. 1g\nshows the PLP curve for our Shostakovich example. As it\nturns out, such PLP curves are robust to outliers and reveal\nmusically meaningful periodicity information even when\nstarting with relatively poor onset information.\n191Oral Session 2: Tempo and Rhythm\n0 2 4 6 8 10 12 14 16 1800.51\n0 2 4 6 8 10 12 14 16 1802.55\n0 2 4 6 8 10 12 14 16 180\n  \n0 2 4 6 8 10 12 14 16 18100200300400500\n  \n0 2 4 6 8 10 12 14 16 18100200300400500\n  \n0 2 4 6 8 10 12 14 16 18100200300400500\n  \n0 2 4 6 8 10 12 14 16 18100150200250300350\n  \n0 2 4 6 8 10 12 14 16 18100150200250300350\n0 2 4 6 8 10 12 14 16 18100150200250300350(a) (b) (c)26 29 33\nFigure 3: Excerpt of an orchestral version conducted by Ormandy of Brahms ’s Hungarian Dance No. 5. The score shows measures 26\nto38in a piano reduced version. (a)Novelty curve ¯∆, tempogram derived from ¯∆, and estimated tempo. (b)PLP curve Γ, tempogram\nderived from Γ, and estimated tempo. (c)Ground-truth pulses, tempogram derived from these pulses, and estim ated tempo. KS = 4 sec .\n4. DISCUSSION AND EXPERIMENTS\nIn this section, we discuss various properties of our PLP\nconcept and sketch a number of application scenarios by\nmeans of some representative real-world examples. We\nthen give a quantitative evaluation on strongly distorted\naudio material to indicate the potential of PLP curves for\naccurately capturing local tempo information.\nFirst, we continue the discussion of our Shostakovich\nexample. Fig. 1a shows a piano-reduced score of the mea-\nsures13−24. The audio recording (an orchestral version\nconducted by Yablonsky) has been temporally warped to\npossess a linearly increasing tempo starting with 216BPM\nand ending at 265BPM at the quarter note level. Firstly,\nnote that the quarter note level has been identiﬁed to be\nthe predominant pulse throughout the excerpt, see Fig. 1e.\nBased on this pulse level, the tempo has been correctly\nidentiﬁed as indicated by Fig. 1f. Secondly, ﬁrst beats\nin the 3/4 Waltz are played by non-percussive instruments\nleading to relatively soft and blurred onsets, whereas the\nsecond and third beats are played by percussive instru-\nments. This results in some hardly visible peaks in the\nnovelty curve shown in Fig. 1d. However, the beats on\nthe quarter note level are perfectly disclosed by the PLP\ncurve Γshown in Fig. 1d. In this sense, a PLP curve can\nbe regarded as a periodicity enhancement of the original\nnovelty curve, indicating musically meaningful pulse on-\nset positions. Here, the musical motivation is that the peri -\nodic structure of musical events plays a crucial role in the\nsensation of note changes. In particular, weak note onsets\nmay only be perceptible within a rhythmic context.\nAs a second example, we consider Brahm’s Hungarian\nDance No. 5. Fig. 3 shows a piano reduced version of mea-\nsures26−38, whereas the audio recording is an orches-\ntral version conducted by Ormandi. This excerpt is very\nchallenging because of several abrupt changes in tempo.\nAdditionally, the novelty curve is rather noisy because ofmany weak note onsets played by strings. Fig. 3a shows\nthe extracted novelty curve, the tempogram, and the ex-\ntracted tempo. Despite of poor note onset information, the\ntempogram correctly captures the predominant eighth note\npulse and the tempo for most time positions. A manual\ninspection reveals that the excerpt starts with a tempo of\n180BPM (measures 26−28, seconds 0−4), then abruptly\nchanges to 280BPM (measures 29−32, seconds 4−6),\nand continues with 150BPM (measures 33−38, seconds\n6−18). Due to the corrupted novelty curve and the rather\ndiffuse tempogram, the extraction of the predominant sinu-\nsoidal kernels is problematic. However, accumulating all\nthese kernels smooths out many of the extraction errors.\nThe peaks of the resulting PLP curve Γ(Fig. 3b) correctly\nindicate the musically relevant eighth note pulse position s\nin the novelty curve. At this point, we emphasize that all\nof the sinusoidal kernels have the same unit amplitude in-\ndependent of the onset strengths. Actually, the amplitude\nofΓindicates the conﬁdence in the periodicity estimation.\nConsistent kernel estimations produce constructive inter -\nferences in the accumulation resulting in high values of\nΓ. Contrary, outliers or inconsistencies in the kernel es-\ntimations cause destructive interferences in the accumula -\ntion resulting in lower values of Γ. This effect is visible\nin the PLP curve shown in Fig. 3b, where the amplitude\ndecreases in the region of the sudden tempo change. As\nnoted above, PLP curves can be regarded as a periodicity\nenhancement of the original novelty curve. Based on this\nobservation, we compute a second tempogram now based\non the PLP instead of the original novelty curve. Com-\nparing the resulting tempogram (Fig. 3b) with the origi-\nnal tempogram (Fig. 3a), one can note a signiﬁcant clean-\ning effect, where only the tempo information of the dom-\ninant pulse (and its harmonics) is maintained. This exam-\nple shows how our PLP concept can be used in an iterative\nframework to stabilize local tempo estimations. Finally,\nFig. 3c shows the manually generated ground truth onsets\n19210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n0 2 4 6 8 10 12 14 16 18 20100200300400500\n  \n0 2 4 6 8 10 12 14 16 18 20406080100120140160180\n  \n0 2 4 6 8 10 12 14 16 18 20140160180200220240260280\n  \n0 2 4 6 8 10 12 14 16 18 20350400450500\n0 2 4 6 8 10 12 14 16 18 2002.55\n0 2 4 6 8 10 12 14 16 18 2002.55\n0 2 4 6 8 10 12 14 16 18 2002.55\n0 2 4 6 8 10 12 14 16 18 2002.55(a) (b) (c) (d)1 3 7\nFigure 4: Beginning of the Piano Etude Op. 100 No. 2 by Burgm ¨uller. Tempograms and PLP curves ( KS = 4 sec ) are shown for various\nsetsΘspecifying the used tempo range (given in BPM). (a)Θ = [30 : 500] (full tempo range). (b)Θ = [40 : 180] (quarter note tempo\nrange). (c)Θ = [140 : 280] (eighth note tempo range). (d)Θ = [350 : 500] (sixteenth note tempo range).\nas well as the resulting tempogram (using the onsets as ide-\nalized novelty curve). Comparing the three tempograms of\nFig. 3 again indicates the robustness of PLP curves to noisy\ninput data and outliers.\nIn our ﬁnal example, we look at the beginning of the\nPiano Etude Op. 100 No. 2 by Burgm ¨uller, see Fig. 4. The\naudio recording includes the repetition and is played in a\nrather constant tempo. However, the predominant pulse\nlevel changes several times within the excerpt. The piece\nbegins with four quarter note chords (measures 1−2), then\nthere are some dominating sixteenth note motives (mea-\nsures3−6) followed by an eighth note pulse (measures\n7−10). The change of the predominant pulse level is\ncaptured by the PLP curve as shown by Fig. 4a. We\nnow indicate how our PLP concept allows for incorpo-\nrating prior knowledge on the expected tempo range to\nexhibit information on different pulse levels. Here, the\nidea is to constrain the set Θof tempo parameters in the\nmaximization (4) of Sect. 3. For example, using a con-\nstrained set Θ = [40 : 180] instead of the original set\nΘ = [30 : 500] , one obtains the tempogram and PLP\ncurve shown in Fig. 4b. In this case, the PLP curve cor-\nrectly reveals the quarter note pulse positions as well as\nthe quarter note tempo of 100BPM. Similarly, using the\nsetΘ = [140 : 280] (Θ = [350 : 500] ) reveals the eighth\n(sixteenth) note pulse positions and the corresponding tem -\npos, see Fig. 4c (Fig. 4d). In other words, in the case there\nis a dominant pulse of (possibly varying) tempo within the\nspeciﬁed tempo range Θ, the PLP curve yields a good pulse\ntracking on the corresponding pulse level.\nIn view of a quantitative evaluation of the PLP concept,\nwe conducted a systematic experiment in the context of\ntempo estimation. To this end, we used a representative\nset of ten pieces from the RWC music database [7] con-\nsisting of ﬁve classical pieces, three jazz, and two popular\npieces, see Table 1 (ﬁrst column). The pieces have differ-\nent instrumentations containing percussive as well as non-\npercussive passages of high rhythmic complexity. In this\nexperiment, we investigated to what extend our PLP con-\ncept is capable of capturing local tempo deviations. Using\nthe MIDI ﬁles supplied by [7], we manually determined\nthe pulse level that dominates the piece. Then, for each\nMIDI ﬁle, we set the tempo to a constant value with regardto the respective dominant pulse level,1see Table 1 (sec-\nond and third columns). The resulting MIDI ﬁles are re-\nferred to as original MIDIs . We then temporally distorted\nthe MIDI ﬁles by simulating strong local tempo changes\nsuch as ritardandi, accelerandi, and fermatas. To this end,\nwe divided the original MIDIs into 20-seconds segments\nand then alternately applied to each segment a continuous\nspeed up or slow down (referred to as warping procedure )\nso that the resulting tempo of the dominant pulse ﬂuctu-\nates between +30% and−30% of the original tempo. The\nresulting MIDI ﬁles are referred to as distorted MIDIs . Fi-\nnally, audio ﬁles were generated from the original and dis-\ntorted MIDIs using a high-quality synthesizer.\nTo evaluate the tempo extraction capability of our PLP\nconcept, we proceed as follows. Given an original MIDI,\nletτdenote the tempo and let Θbe the set of integer tempo\nparameters covering the tempo range of ±40% of the orig-\ninal tempo τ. This coarse tempo range reﬂects the prior\nknowledge of the respective pulse level (in this experiment ,\nwe do not want to deal with tempo octave confusions) and\ncomprises the tempo values of the distorted MIDI. Based\nonΘ, we compute for each time position tthe maximizing\ntempo parameter τt∈Θas deﬁned in (4) of Sect. 3 for\nthe original MIDI using various kernel sizes. We consider\nthe local tempo estimate τtcorrect, if it falls within a 2%\ndeviation of the original tempo τ. The left part of Table 1\nshows the percentage of correctly estimated local tempi for\neach piece. Note that, even having a constant tempo, there\nare time positions with incorrect tempo estimates. Here,\none reason is that for certain passages the pulse level or\nthe onset information is not suited or simply not sufﬁcient\nfor yielding good local tempo estimations, e. g., caused by\nmusical rests or local rhythmic offsets. For example, for\nthe piece C022 (Brahms’s Hungarian Dance No. 5), the\ntempo estimation is correct for 74.5%of the time param-\neters when using a kernel size ( KS) of4 sec . Assuming a\nconstant tempo, it is not surprising that the tempo estima-\ntion stabilizes when using a longer kernel. In case of C022,\nthe percentage increases to 85.4%forKS = 12 sec .\n1In this experiment, we make the simplistic assumption that the pr e-\ndominant pulse does not change throughout the piece. Actuall y, this is not\ntrue for most pieces such as C003 (Beethoven’s Fifth), C022 (B rahms’s\nHungarian Dance No. 5), or J001 (Nakamura’s Jive).\n193Oral Session 2: Tempo and Rhythm\noriginal MIDI distorted MIDI\nPiece Tempo Level 4 6 8 12 4 6 8 12\nC003 360 1/16 74.5 81.6 83.7 85.4 73.9 81.1 83.3 86.2\nC015 320 1/16 71.4 78.5 82.5 89.2 61.8 67.3 71.2 76.0\nC022 240 1/8 95.9 100.0 100.0 100.0 95.0 98.1 99.4 89.2\nC025 240 1/16 99.6 100.0 100.0 100.0 99.6 100.0 100.0 96.2\nC044 180 1/8 95.7 100.0 100.0 100.0 82.6 85.4 77.4 59.8\nJ001 300 1/16 43.1 54.0 60.6 67.4 37.8 48.4 52.7 52.7\nJ038 360 1/12 98.6 99.7 100.0 100.0 99.2 99.8 100.0 96.7\nJ041 315 1/12 97.4 98.4 99.2 99.7 95.8 96.6 97.1 95.5\nP031 260 1/8 92.2 93.0 93.6 94.7 92.7 93.7 93.9 93.5\nP093 180 1/8 97.4 100.0 100.0 100.0 96.4 100.0 100.0 100.0\naverage: 86.6 90.5 92.0 93.6 83.5 87.1 87.5 84.6\naverage (after iteration): 89.2 92.0 93.0 95.2 86.0 88.8 88.5 83.1\nTable 1: Percentage of correctly estimated local tempi for the\nexperiment based on original MIDI ﬁles (constant tempo) and\ndistorted MIDI ﬁles for kernel sizes KS = 4 ,6,8,12 sec .\nAnyway, the tempo estimates for the original MIDIs\nwith constant tempo only serve as reference values for\nthe second part of our experiment. Using the distorted\nMIDIs, we again compute the maximizing tempo param-\neterτt∈Θfor each time position. Now, these values\nare compared to the time-dependent distorted tempo val-\nues that can be determined from the warping procedure.\nAnalogous to the left part, the right part of Table 1 shows\nthe percentage of correctly estimated local tempi for the\ndistorted case. The crucial point is that even when using\nstrongly distorted MIDIs, the quality of the tempo estima-\ntions only slightly decreases. For C022, the tempo estima-\ntion is correct for 73.9%of the time parameters when using\na kernel size of 4 sec (compared to 74.5%in the original\ncase). Averaging over all pieces, the percentage decreases\nfrom86.6%(original MIDIs) to 83.5%(distorted MIDIs),\nforKS = 4 sec . This clearly demonstrates that our concept\nallows for capturing even signiﬁcant tempo changes. As\nmentioned above, using longer kernels naturally stabilize s\nthe tempo estimation in the case of constant tempo. This,\nhowever, does not hold when having music with constantly\nchanging tempo. For example, looking at the results for the\ndistorted MIDI of C044 (Rimski-Korsakov, The Flight of\nthe Bumble Bee), we can note a drop from 82.6%(4 sec\nkernel) to 59.8%(12 sec kernel).\nFurthermore, we investigated the iterative approach al-\nready sketched for the Brahms example, see Fig 3b. Here,\nwe use the PLP curve as basis for computing a second\ntempogram from which the tempo estimation is derived.\nAs indicated by the last line of Table 1, this iteration in-\ndeed yields an improvement for the tempo estimation for\nthe original as well as the distorted MIDI ﬁles. For exam-\nple, in the distorted case with KS = 4 sec the estimation\nrate raises from 83.5%(tempogram based on ¯∆) to86.0%\n(tempogram based on Γ).\n5. CONCLUSIONS\nIn this paper, we introduced a novel concept for extracting\nthe predominant local pulse even from music with weak\nnon-percussive note onsets and strongly ﬂuctuating tempo.\nWe indicated and discussed various application scenarios\nranging from pulse tracking, periodicity enhancement of\nnovelty curves, and tempo tracking, where our mid-level\nrepresentation yields robust estimations. Furthermore, o urrepresentation allows for incorporating prior knowledge o n\nthe expected tempo range to adjust to different pulse lev-\nels. In the future, we will use our PLP concept for sup-\nporting higher-level music tasks such as music synchro-\nnization, tempo and meter estimation, onset detection, as\nwell as rhythm-based audio segmentation. In particular\nthe sketched iterative approach, as ﬁrst experiments show,\nconstitutes a powerful concept for such applications.\nAcknowledgements: The research is funded by the\nCluster of Excellence on Multimodal Computing and In-\nteraction at Saarland University.\n6. REFERENCES\n[1] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M. Davies,\nand M. B. Sandler: “A Tutorial on Onset Detection in Mu-\nsic Signals,” IEEE Trans. on Speech and Audio Processing ,\nV ol. 13(5), 1035–1047, 2005.\n[2] J. Bilmes: “A Model for Musical Rhythm,” in Proc. ICMC ,\nSan Francisco, USA, 1992.\n[3] A. T. Cemgil, B. Kappen, P. Desain, and H. Honing: “On\nTempo Tracking: Tempogram Representation and Kalman\nFiltering,” Journal of New Music Research , V ol. 28(4), 259–\n273, 2001.\n[4] S. Dixon: “Automatic Extraction of Tempo and Beat from\nExpressive Performances,” Journal of New Music Research ,\nV ol. 30(1), 39–58, 2001.\n[5] D. P. W. Ellis: “Beat Tracking by Dynamic Programming,”\nJournal of New Music Research , V ol. 36(1), 51–60, 2007.\n[6] J. Foote and S. Uchihashi: “The Beat Spectrum: A New Ap-\nproach to Rhythm Analysis,” in Proc. ICME , Los Alamitos,\nUSA, 2001.\n[7] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka: “RWC\nMusic Database: Popular, Classical and Jazz Music\nDatabases,” in Proc. ISMIR , Paris, France, 2002.\n[8] A. Holzapfel and Y . Stylianou: “Beat Tracking Using Group\nDelay Based Onset Detection,” in Proc. ISMIR , Philadelphia,\nUSA, 2008.\n[9] K. Jensen, J. Xu, and M. Zachariasen: “Rhythm-Based Seg-\nmentation of Popular Chinese Music,” in Proc. ISMIR , Lon-\ndon, UK, 2005.\n[10] A. P. Klapuri, A. J. Eronen, and J. Astola: “Analysis of the\nmeter of acoustic musical signals,” IEEE Trans. on Audio,\nSpeech and Language Processing , V ol. 14(1), 342–355, 2006.\n[11] M. M ¨uller: “Information Retrieval for Music and Motion,”\nSpringer, 2007.\n[12] G. Peeters: “Template-based estimation of time-varying\ntempo,” EURASIP Journal on Advances in Signal Process-\ning, V ol. 2007, 158–171, 2007.\n[13] J. Sepp ¨anen: “Tatum grid analysis of musical signals,” in\nProc. IEEE WASPAA , New Paltz, USA, 2001.\n[14] E. D. Scheirer: “Tempo and beat analysis of acoustical mu-\nsical signals,” Journal of the Acoustical Society of America ,\nV ol. 103(1), 588–601, 1998.\n[15] R. Zhou, M. Mattavelli, and G. Zoia: “Music Onset Detection\nBased on Resonator Time Frequency Image,” IEEE Trans. on\nAudio, Speech, and Language Processing , V ol. 16(8), 1685–\n1695, 2008.\n194"
    },
    {
        "title": "Fingering Watermarking in Symbolic Digital Scores.",
        "author": [
            "David Gross-Amblard",
            "Philippe Rigaux",
            "Lylia Abrouk",
            "Nadine Cullot"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416454",
        "url": "https://doi.org/10.5281/zenodo.1416454",
        "ee": "https://zenodo.org/records/1416454/files/Gross-AmblardRAC09.pdf",
        "abstract": "We propose a new watermarking method that hides the writer’s identity into symbolic musical scores featuring fingering annotations. These annotations constitute a valuable part of the symbolic representation, yet they can be slightly modified without altering the quality of the musical information. The method applies a controlled distortion of the existing fingerings so that unauthorized copies can be identified. The proposed watermarking method is robust against attacks like random fingering alterations and score cropping, and its detection does not require the original fingering, but only the suspect one. The method is general and applies to various fingering contexts and instruments. Keywords. Watermarking, fingering",
        "zenodo_id": 1416454,
        "dblp_key": "conf/ismir/Gross-AmblardRAC09",
        "keywords": [
            "watermarking",
            "symbolic musical scores",
            "fingering annotations",
            "controlled distortion",
            "unauthorized copies",
            "score cropping",
            "robust",
            "detection",
            "original fingering",
            "suspect one"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFINGERING WATERMARKINGINSYMBOLIC DIGITAL SCORES\nDavid Gross-Amblard\nLe2i-CNRS Lab.\nUniversit´e de Bourgogne\nFrance\nfirst.last@u-bourgogne.frPhilippeRigaux\nLamsade-CNRS Lab.\nUniversit´e de Dauphine\nParis IX,France\nfirst.last@dauphine.frLyliaAbrouk NadineCullot\nLe2i-CNRS Lab.\nUniversit´e de Bourgogne\nFrance\nfirst.last@u-bourgogne.fr\nABSTRACT\nWe propose a new watermarking method that hides the\nwriter’sidentityintosymbolicmusicalscoresfeaturingﬁ n-\ngering annotations. These annotations constitute a valu-\nable part of the symbolic representation, yet they can be\nslightly modiﬁed without altering the quality of the musi-\ncalinformation. Themethodappliesacontrolleddistortio n\nof the existing ﬁngerings so that unauthorized copies can\nbeidentiﬁed. Theproposedwatermarkingmethodisrobust\nagainst attacks like random ﬁngering alterations and score\ncropping, and its detection does not require the original\nﬁngering, but only the suspect one. The method is general\nand applies tovarious ﬁngering contexts and instruments.\nKeywords. Watermarking, ﬁngering\n1. INTRODUCTION\nInthisworkweconsidersymbolicmusicalscoresthatcon-\ntainﬁngering annotations . Such ﬁngerings ease the score\ninterpretation forthe novice player, and can guide the pro-\nfessional player. Producing high quality ﬁngerings is a\ncomplex and costly task for the score writer. Up to now,\nit mainly remains an hand-made task, although several au-\ntomaticﬁngeringmethodshavebeenproposedrecently[1–\n3].\nThe score writer’s investment is threaten by the devel-\nopment of musical scores in digital form. Any buyer of\nsuchscorescanobtainaperfectcopyoftheﬁlesandresell\nillegalcopies. Watermarkingisaknowntooltoprotectthe\nintellectual property of digital content, and it can be envi -\nsioned for musical scores as well. This would enable the\ndistribution and sharing of score ﬁles marked by the copy-\nrightoftheirowner(s),justlikescoresheetsarenowadays ,\nbutwiththenumerous advantages associatedwiththedig-\nital format.\nSeveralmethodshavebeenproposedtohidetheowner’s\nidentity into score images, by changing pixels [4], staff\nthickness [5] or symbols shape [6,7]. These approaches\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbear this noticeand thefull citation ontheﬁrst page.\nc/circlecopyrt2009International Society forMusic InformationRetrieval .are well ﬁtted for protecting score images, but are not rel-\nevant for data exchange in a symbolic format like Mu-\nsicXML [8]. Given the high cost of producing a symbolic\ndigital score, writers may demand a robust mechanism to\nembed their copyright mark in the music symbolic repre-\nsentation. Thiscopyrightmarkmustbepreservedthrough-\nout the operations that can be applied to the digital repre-\nsentation(e.g.,transposition). Itshouldnotdependonsi de\naspectssuchasgraphicaloutputdetails(e.g.,thethickne ss\nof staff lines) which can easily be replaced or even elim-\ninated without harm, as they are not part of the symbolic\nrepresentation. Finally, the watermark should not alter th e\nmusic content. In order to satisfy these requirements, our\napproach consists in watermarking the existing scores an-\nnotations. Inthepresentpaperweapplythisideatoﬁnger-\ningannotations. Uptoourknowledge,thisistheﬁrstwork\nonwatermarking themusic semantics itself.\nThe key idea of the method, given a musical score and\na hand-made high quality ﬁngering, is to choose several\nshort secret fragments of the score. Given a score frag-\nment, we replace the existing ﬁngering with another ﬁn-\ngering,chosensecretlyamongseveralcomputer-madeﬁn-\ngeringsofcomparablequality. Allsecretchoicesaremade\nusing a cryptographic pseudo-random number generator,\nseeded by a summary of the musical structure and with a\nsecret key known only by the legitimate owner. The re-\nsulting ﬁngering will be published with the musical score.\nFinally, given a suspect score, the correspondence of the\nsuspectﬁngeringwithoursecretchoicesonoursecretfrag-\nments acts as the proof of ownership. Our method applies\nto any ﬁngering scenario, as soon as a quality metric of\nﬁngerings is available along with an automatic ﬁngering\nmethodforsmallfragments(suchasinpianoorguitarmu-\nsicfor example).\nItshouldbeclearthatweprotectthecombinationofthe\nscore and its ﬁngering, and not the score itself. We also\nsuppose that the attacker cannot afford to alter the score\nsigniﬁcantly, as this would result in an unsellable score\n(nevertheless we moderate thisassertioninSection 3).\nOutline. The paper is organized as follows. In Section 2\nwe introduce our general model for ﬁngering and water-\nmarking. Section 3 presents our watermarking and detec-\ntion algorithms. Section 4 discusses several issues on the\nrobustness of the watermark against natural score manip-\nulation or malevolent attacks. Experiments assessing our\n141Poster Session 1\nmethodarepresentedinSection5. Section6brieﬂycovers\nthe relatedwork and Section 7concludes.\n2. FINGERING AND WATERMARKING\n2.1 Fingering\nThe method proposed in this paper applies to any ﬁnger-\ning context, but for the sake of simplicity we will focus\non right-hand piano ﬁngering for melodic inputs. Given\na score in symbolic notation, we abstract it as a sequence\ns= (n1,... ,n N)ofNconsecutive notes. A ﬁngering\nf(ni)for a note niis an integer in {1,2,... ,5}, where\nnumber 1 to 5 represents a right-hand ﬁnger, respecting\nthe usual conventions. For example, f(A) = 2means that\nnote A willbe played by theforeﬁnger.\nThewatermarkingmethodusesanestimateofthequal-\nityofaﬁngering,thatisrelatedtotheplayerinnerfeeling s.\nWesupposetheexistenceofacostfunction cost(f,s)that\nprovides the cost of ﬁngering ffor the score s: the higher\nthe cost output by this function, the lower the quality of\nthe provided ﬁngering (such functions exist for several in-\nstruments like piano [1]). We will explicit such a function\nin the experiments of Section 5, but our method applies to\nanycostfunction. Wealsooftenusethecostofafragment\nwof the score s,that we denote cost(f,w,s ).\nTheﬁrststaffofFigure1presentsanoriginalscorefrag-\nment with ﬁngering annotations built by the score writer.\nFingeringannotationsappearabovethescore. Annotations\nbelow the score are presented here only for the purpose of\nexplanation,butarenotpublishedbythescorewriter. They\nshowthecumulativecostofplayingthescorewiththecor-\nrespondingﬁngering(forexample,playingthewholescore\ncosts 50 according tothechosen cost function).\nOriginal:\n/noteheads.s23...\n/noteheads.s22\n31/noteheads.s24\n/noteheads.s25/rests.0/clefs.G/timesig.C44/noteheads.s22\n/noteheads.s23\n20/noteheads.s21\n/noteheads.s25/noteheads.s21\n/noteheads.s23\n/noteheads.s24\n24/noteheads.s22\n/noteheads.s225\n/noteheads.s25\n43/noteheads.s21\n/clefs.G /noteheads.s22\n505 3\n/noteheads.s21\n/noteheads.s22\n39/noteheads.s2 /noteheads.s25\n/noteheads.s2 /noteheads.s24\n/noteheads.s22\n/noteheads.s23\nWatermarked:\n/noteheads.s24\n/noteheads.s25\n/noteheads.s22\n/noteheads.s23/timesig.C44/clefs.G5/noteheads.s22\nM+01\n/noteheads.s2\n244\n/noteheads.s2/noteheads.s21\nM+2/noteheads.s2 /noteheads.s23\n20/noteheads.s21 ...\n/noteheads.s23/rests.0\n/noteheads.s23\n/noteheads.s2 /noteheads.s251\n/noteheads.s2\n54/noteheads.s24 5 22\n/noteheads.s2 /clefs.G5 2\n/noteheads.s25\n/noteheads.s2\nM+23\n/noteheads.s21\n/noteheads.s2/noteheads.s21\n/noteheads.s2\n41\nFigure 1. Different ﬁngerings of the same score, with cu-\nmulative costs\n2.2 Watermarking protocols\nA watermarking protocol is a pair of algorithms (W,D),\nwhere WandDare respectively the marker and detector\nalgorithms (see Figure 2). Given an original score sanda high quality ﬁngering f, the score writer will watermark\nit by obtaining a speciﬁc ﬁngering fM=W(s,f,K), de-\npending on a secret numerical key K. The watermarked\nscore(s,fM)is sold to users. If a suspect copy (s∗,f∗)\nis discovered, the detector Dapplied on (s∗,f∗)using the\nsecretkey Kshouldoutput guiltyiff∗wasobtainedfrom\nfM, andnotguilty iff∗is a ﬁngering obtained indepen-\ndently from fM. A watermarking protocol is said to be\nblindif the original ﬁngering is not needed at detection\ntime,whichmaybeusefulaswriter’sﬁngeringsmaynotbe\naccessible easily or archived properly. The suspect ﬁnger-\ningmayhavebeenalsoattacked/distortedbeforereselling ,\nin order to erase the watermark. A watermarking protocol\nis said to be robustif it can still detect reasonably altered\nﬁngerings. Finally, respecting usual conventions, marker\nanddetectoralgorithmsarepublic,andtheirsecurityreli es\nonly onthe secret key.\nsecret key\nsecret keyscore publisher’s side\nvery good fingering\nproof\nof ownershipusers side\n(s,f)\nresellingillegalattack\n(s*,f*)marker\ndetectororiginal score\n& good fingering\n(s,f  )Moriginal score &lawful user\nmalevolent user\nFigure 2. Protecting scoreand ﬁngering by watermarking\n3. FINGERING WATERMARKING\n3.1 Watermarking algorithm\nAlgorithm 1 gives the pseudo-code of the marker. This\nalgorithmscansagivenscore sbyconsideringonlyawin-\ndow of kconsecutive notes (line 4 and 5). For each win-\ndow, we ﬁrst decide if it constitutes a good candidate for\nwatermarking (line 6 and 7). This choice is secret and is\nbased on the window content, the secret key Kand a wa-\ntermarking period γknown only by the score writer (this\nwillbe explained inthe next section).\nIf a given window wis considered for watermarking,\nwe focus on its ﬁrst note ni. We try to replace the original\nﬁngering f(ni)for this note by another one, f′(ni), also\nchosen secretly between the 5 possible ﬁngerings for our\npiano example (line9).\nWecomparethecostofthisnewﬁngering cost(f′,w,s)\non window wwith the cost of the original ﬁngering\ncost(f,w,s )onw(line 10). If the new cost exceeds the\npreviousonebyalimit ε,wecancelthismodiﬁcation(line\n12). If the new ﬁngering has a reasonable cost, we keep it\nfor publication. Parameter ε, chosen by the score writer,\ncontrols the allowed amount of alteration that results from\nthe watermarking process, and guarantees to produce ﬁn-\ngerings withagood quality.\nThe second staff on Figure 1 demonstrates the process.\nFor example, the 9thnote (E) is considered for watermark-\n14210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAlgorithm 1 : Watermarking\nInput: ascore sofNnotesn1,... ,n N, ahigh\nquality ﬁngering ffors,a secret key K, a\nwindow size k,a qualitythreshold ε, a\nperiod γ.\nOutput: a watermarked ﬁngering f′.\nbegin1\n// copy ftof′2\nf′:=f 3\nfori= 1toN−k+ 1do 4\nw=ni.ni+1... n i+k−1//reference window 5\nseed PRNG Gwithsignature (w).K 6\nif(G.nextInt () mod γ= 0)then 7\n//trytowatermark theﬁrstnote 8\nf′(ni) :=G.nextInt () mod 5 9\nif(|cost(f′,w,s)−cost(f,w,s )|> ε)then 10\n//revert changes 11\nf′(ni) :=f(ni) 12\nend 13\nend14\nend15\nreturn f′16\nend17\ning. Its original ﬁngering (ﬁnger 2) has been replaced by\na new ﬁngering (ﬁnger 1). This yields an extra cost of 2,\nwhichisconsideredreasonableforthisexample. Theover-\nall watermarking process yields a total extra cost of 4 on\nthe scoreﬁngering.\n3.2 Randomness\nWe now explain how random choices are made. Given a\nwindow w, we compute its musical signature based on its\ncore music content ( signature() function, line 6). The sig-\nnatureisindependentfromannotationsandornamentsthat\nare pointless for our algorithm. It is robust against na ¨ıve\ntransposition attacks as it transposes the score into a com-\nmon key (but of course, ﬁngering costs are computed ac-\ncording to the original score). It is also invariant against\nscore rewriting replacing a note or group of notes by an\nequivalent encoding (forexample, replacingahalfnoteby\ntwo tied quarters). In this paper, the signature is the con-\ncatenation of transposed note pitches, where consecutive\nequal pitches are suppressed. For example, the signature\nofABAABC isABABC(seen as a number), and time is not\ntaken intoaccount.\nWe concatenate this signature with the secret key K(a\nnumber), known only by the score writer. Then (line 6),\nwe seed a cryptographic pseudo-random number genera-\ntor (PRNG) with this number (as in [9]). This generator is\nused for all subsequent choices and has interesting prop-\nerties. First, if it is seeded with the same value, the pro-\nduced numbers are deterministic. Hence, if we know the\nsecretkey,wewillbeabletoreproducethepseudo-random\nchoices made at watermarking time. Second, if the se-\ncretkeyisunknown,thegeneratoroutputslookcompletelyrandomandcannotbereproduced. Henceanattacker,un-\naware of thesecret key, isﬁghting against randomness.\n3.3 Detection algorithm\nAlgorithm 2 : Detection\nInput: asuspect score sofNnotesn1,... ,n Nwith\nitsﬁngering f∗, asecret key K, awindow size\nk,aquality threshold ε, aperiod γ, asecurity\nparameter δ.\nOutput:guiltyornotguilty .\nbegin1\n//copy f∗tof′2\nf′:=f∗3\ntotal:= 0,match := 0 4\nfori:= 1toN−k+ 1do 5\nw=ni.ni+1... n i+k−1// reference window 6\nseed PRNG Gwithsignature (w).K 7\nif(G.nextInt () mod γ= 0)then 8\n//check thiswindow 9\n//compute awaited value 10\nf′(ni) :=G.nextInt () mod 5 11\nif(|cost(f′,w,s)−cost(f∗,w,s)| ≤ε)then 12\n//probably watermarked position 13\ntotal++ 14\nif(f′(ni) =f∗(ni))then 15\nmatch++ 16\nend 17\nend 18\nelse 19\nf′(ni) :=f∗(ni)//revert changes 20\nend 21\nend22\nend23\nif(match/total >1\n5+threshold (N,δ))then 24\nreturn guilty 25\nelse26\nreturn notguilty 27\nend28\nend29\nThedetectionalgorithm(seeAlgorithm2forthepseudo-\ncode) proceeds like the marker algorithm. Using the same\nwindow size, watermarking period and secret key used at\nwatermarking time, we seed the generator with each win-\ndow signatureand thesecretkey (line7). Hence, thesame\nrandomchoicesmadeatwatermarkingtimearereproduced.\nThus we can locate exactly those windows selected at wa-\ntermarkingtime(line8). Then, sincethedetectordoesnot\nhave the watermarked ﬁngering for comparison (blind de-\ntector), we have to assess that this position has really been\nused for watermarking. For that, we replace the ﬁngering\nof the ﬁrst note by the awaited one, using the random gen-\nerator(line11). Wethencomputethecostofthisﬁngering.\nIf it exceeds the error limit ε, we discard this window and\nrestore the initial ﬁngering (line 20). If error limit is re-\nspected, this position is probably a watermark (line 14).\n143Poster Session 1\nWethencomparetheawaitedﬁngeringwiththefoundone\n(line15). Forthewholescore,wemaintaintheratioofthe\nnumber of matching ﬁngerings with the number of win-\ndowsconsideredfordetection. Ifthisratioexceedsagiven\nthreshold (line 24), we consider the score as suspect (the\nthreshold value isdiscussedbelow).\n4. DISCUSSION\nInthissectionwediscussseveralclassicalissuesrelated to\nwatermarking algorithms.\nImpactonquality. SincethePRNGoutputsrandomnum-\nbers with uniform distribution, the probability for a win-\ndowwto be considered for watermarking is 1/γ. The im-\npact of watermarking this window can not be higher than\nε. Hence, for a Nnotes score, the mean overall alteration\nisat most ε⌊N−k⌋/γ.\nWindowsize. Asthewindowsize kincreases,theamount\nofrandomnessinjectedintotherandomgeneratorextends.\nIf we consider reasonable scores whose notes span 2 oc-\ntaves,thereisupto 14kpotentialﬁngeringsfor kconsecu-\ntive notes. We chose k= 5in our experiments, leading to\nhalf-a-milliondistinct window signatures.\nFalse positives probability and threshold function. A\nfalse-positivedetectionoccurswhenthedetectorconside rs\na random score as guilty. Clearly, this probability must\nbe negligible. Let δbe this acceptable probability, say\nδ= 10−10. Let us consider a random score. The prob-\nability of a given window to be selected by the detector\nis1/γ. For piano ﬁngering, the probability of a ﬁnger-\ning to correspond – by chance – to the watermarked one\nis1/5(as there is 5 different possible ﬁngerings). Hence\nthe average number of total matches on a random score is\n⌊N−k⌋/5γ. BytheHoeffdingbound[10],theprobability\nthat the detector ratiomatch\ntotalon a random score deviates\nfrom theprevious average issuch that\nP[|match\ntotal−1\n5|> threshold (N,δ)]< e−2N\nγthreshold (N,δ)2.\nHence,choosing threshold (N,δ) =/radicalBig\nγ\nNln1\nδguarantees\nafalsepositiveratesmallerthan δ. Forexample,onascore\nof 10,000 notes with a watermarking period γ= 10and\nδ= 10−10,the recommended threshold is0.22.\nAvailable bandwidth. Robustness and signiﬁcance are\nproportional to the amount of watermark bits that can be\nhidden. In popular guitar pieces (e.g., guitar scores and\ntablatures for beginners), a signiﬁcant number of water-\nmark positions are available. But music for expert players\nmaycontainonlyafewﬁngeringannotations. Ifthisnum-\nber is not sufﬁcient to reach the security limit, or if the\nmusical corpus is made of small pieces only, a natural ex-\ntension is to consider the watermarking of an entire piece\ncollection(collectedinaCDforexample). Thewatermarkisspread on thecollection, and since the detection method\nuses only a ﬁnite-size sliding window, the order of pieces\nwithin the collection is pointless at detection time. The\nmethod is also robust enough to recover the watermark on\nasubset/superset of scores.\nAttacks. An attacker suspecting the occurrence of a wa-\ntermarkmaytrytoevadedetectionbyseveralmeans. First,\ntheattackercanaddeasy-to-correcterrorsintheﬁngering .\nTo be successful, the attacker will have to add such errors\nall along the piece, in order to erase sufﬁcient watermark\npositions. Hencetheoverallﬁngeringisfulloferrors. Sec -\nond, the attacker can leave the ﬁngering unchanged, but\nadd errors on the score itself, in order to break synchro-\nnization with the ﬁngering. If errors are simply equiva-\nlent notes rewritings, the signature method will probably\nrecover the correct ones. If the error is large, it will break\nonewatermarkposition. Again,errorsmustspanthewhole\nscore to be efﬁcient, which is unreasonable (due to lack of\nspace,weomitthemathematicalproofofthesestatements.\nThey aresimilartothefalse-positive analysis).\nAnotherapproachfortheattackeristoreﬁngerthescore.\nA complete rewriting represents a signiﬁcant amount of\nwork, so why would this attacker bother buying a ﬁngered\nscore in the ﬁrst place ? On the contrary, a small reﬁn-\ngering acts as a random attack, as the attacker has no idea\nwhere toperform thisﬁngering.\nFinally, the malevolent user can attack the score struc-\nture. Brute-force transposition is not sufﬁcient, as we nor -\nmalize the score in a speciﬁc key for detection. A ﬁrst\ntechnique is to resell only subscores (excerpts). This can\noccur even for a normal buyer using the score. However,\nas long as a signiﬁcant fraction of the piece is present, the\nwatermark can be detected (this fraction is typically 30%\nin the database watermarking literature [9]). If less than\n1/3 of the piece is stolen, the loss of property is harmless.\nIf an attacker mixes a watermarked collection with a huge\nnumber of unwatermarked pieces, theargument issimilar.\nAlasttechniqueistofoldorunfoldthescoreaccording\nto repetition symbols. This attack can be counterfeited by\ndiscardingrepeatedpartsinthe signature ()function,both\nforwatermarking and detection.\n5. EXPERIMENTS\n5.1 Data, cost function, parameters\nOurexperimentsarebasedon50Chopinpianopiecesfrom\ntheKernScoresrepository[11],foratotalofaround10,000\nnotes. OriginalﬁngeringswerefoundwithaDijkstraalgo-\nrithm using a ﬁngering cost function close to [1] and [2]\n(our method supposes hand-made high quality ﬁngerings,\nbut this approach is sufﬁcient to measure the watermark-\ning impact on quality). These models encompass the cost\nof playing a note with a given hand position (vertical cost\ncostv(f,n)), and the cost of the transition between one\nhandpositiontothenextone(horizontalcost costh(fi,ni→\nfi+1,ni+1)). These costs are constant values that agree\nwiththehumanhandphysicalpossibilities(theprecisedef -\n14410th International Society for Music Information Retrieval Conference (ISMIR 2009)\ninition of these costs in not relevant for the present paper,\nwe refer the reader to [2] for in-depth explanations.) The\ncost(f,n)of a ﬁngering fis the sum of its horizontal and\nvertical costs,i.e.,\ncost(f,n) =N/summationdisplay\ni=1costv(fi,ni)+costh(fi,ni→fi+1,ni+1).\nWe used window size k= 5, error tolerance ε= 10\nand detection threshold 0.8(vertical and horizontal costs\nfor one note or transitionspan between 0and +14).\n5.2 Experiments\nFigure3showstheimpactofthewatermarkingmethodfor\nvariousvaluesofwatermarkingperiod γ. Clearly,aperiod\nsmaller than 5 yields a huge distortion, and greater values\ntendtowardaconstanterrorwithrespecttotheoriginalﬁn-\ngering. Figure4and5studytheimpactofarandomattack\nthat tries to erase the watermark as follows: a note ﬁn-\ngeringischosenwithprobability 1/γa,andchangedintoa\nrandomﬁngeringuptoacostimpactof10. Figure4shows\nthe attack impact on the watermarked ﬁngering quality for\nvarious values of γa. It appears that the attack impact is\nlarger than the watermark impact on the ﬁngering cost:\nchoosing γa<5leadstoﬁngeringswithpoor(unsellable)\nquality. Figure 5 shows the attack impact on the detec-\ntor ratio. Choosing a detection threshold of 0.8guarantees\nthatallsuspectﬁngeringsarecorrectlydetected,expectf or\nthosewithattack γasmallerthan6. Hence,Figure4and5\narguethatanyattacktrickingthedetectoralsodestroysth e\nﬁngering quality. Finally, Figure 6 shows that using a ran-\ndom secret key does not yield false positive detection (the\ncorrect key ispresented at index 50).\nFigure 3. Impact of watermarking on ﬁngering cost\n6. RELATED WORK\nHidinginformation(forvariouspurposes)inmusicalscore s\nis an old story. A study of music score watermarking was\nperformed during the WEDELMUSIC project. A good\nsurvey[12]recallstheseapproaches. Inthevisualdomain,Figure 4. Impact ofattack onﬁngering cost\nFigure 5. Impact of attack on detector’s output\nFigure 6. Detector output for random secret keys (correct\none at 50)\n145Poster Session 1\nclassical but adapted image watermarking techniques can\nbe applied on the image of a musical score. The water-\nmark can be hidden by altering grayscales, or the binary\nrepresentation of images, or the pixels themselves. In the\nmusical notation (but still into the score image), one can\nalter the staff thickness, the vertical or horizontal dista nce\nbetween notes or groups of notes, notes orientation, thick-\nness [5] or shape [6,7]. Little is known on information\nhiding intothe musicsemantics, where our workstands.\nOur method shares some similarities with database wa-\ntermarkingmethods: watermarkingofrelationaldatabases\nof numerical values [9], numerical data streams [13] and\nXMLstreams[14]. AllthesemethodsusethesamePRNG\ntechnique, and [13,14] also use a ﬁnite window to scan\na numerical or textual stream. The main difference is that\nourmethodhastocontrolanon-localcostondataandmay\nrequire rollbacks.\n7. CONCLUSION\nOn-line distribution of musical scores is a promising area.\nAmong other advantages, it could offer instant access to\nmusic collections, a wide diffusion of rare musical pieces,\nandcomputer-basedservicestobrowse,recommend,search\nand analyze music. However, producing music scores is a\ncostlyprocessandtheprotectionofscorewritersagainsti l-\nlegalcopiesisaprerequisiteforon-linecollectiontoeme rge.\nInthepresentpaper,weproposeawatermarkingalgorithm\nbasedontheideathattheownersignatureshouldbebased\nonthemusicalcontent(whichcanhardlybemodiﬁed)and\nhidden in a valuable annotation of this content – namely,\nﬁngerings. We propose a simple algorithm and show that\nitresultsinaneffectiveprotection. Althoughcurrentlyl im-\nited to ﬁngerings, we believe that our approach can be ex-\ntended to music annotations in general, for instance lyrics\nin vocal music. We are currently investigating this larger\ncontext.\n8. ACKNOWLEDGEMENT\nThis work is partially supported by the French ANR Con-\ntent and Interaction NeumaProject [15].\n9. REFERENCES\n[1] Melanie Hart, Robert Bosch, and Elbert Tsai. Find-\ning optimal piano ﬁngerings. The UMAP Journal ,\n2(21):167–177, 2000.\n[2] Alia Al Kasimi, Eric Nichols, and Christopher\nRaphael. A simple algorithm for automatic generation\nof polyphonic piano ﬁngerings. In International Soci-\nety for Music Information Retrieval Conference (IS-\nMIR),pages 355–356, 2007.\n[3] Yuichiro Yonebayashi, Hirokazu Kameoka, and\nShigekiSagayama.Automaticdecisionofpianoﬁnger-\ning based on a hidden Markov model. In Manuela M.\nVeloso, editor, International Joint Conference on Arti-\nﬁcial Intelligence (IJCAI) ,pages 2915–2921, 2007.[4] Wolfgang Funk and Martin Schmucker. High capacity\ninformation hiding in music scores. In Paolo Nesi, ed-\nitor,First International Conference on WEB Deliver-\ning of Music, proceedings of Wedelmusic 2001 , num-\nber 5020 in IEEE Computer Society Technical Com-\nmittee on Computer Generated Music, pages 12–19,\nLosAlamitos,California,USA,2001.IEEEComputer\nSociety.\n[5] Martin Schmucker. Capacity improvement of a blind\nsymbolic music score watermarking technique. In\nWah Ping Wong, editor, Security and Watermarking of\nMultimedia Contents IV. , number 4675 in SPIE Pro-\nceedings, pages 206–213, Washington, 2002.\n[6] Martin Schmucker and Hongning Yan. Music score\nwatermarking by clef modiﬁcations. In Edward J.\nDelp,editor, SecurityandWatermarkingofMultimedia\nContents V. , number 5020 in SPIE Proceedings, pages\n403–412, Bellingham, 2003.\n[7] Martin Schmucker, Christoph Busch, and Anoop Pant.\nDigital watermarking for the protection of music\nscores. In Wah Ping Wong, editor, Security and Wa-\ntermarking of Multimedia Contents III , number 4314\ninSPIE Proceedings, pages 85–95, Washington, 2001.\n[8] Recordare. MusicXML document type deﬁnition.\nhttp://www.recordare.com/xml.html .\n[9] RakeshAgrawal,PeterJ.Haas,andJerryKiernan.Wa-\ntermarking relational data: framework, algorithms and\nanalysis.VLDB J., 12(2):157–169, 2003.\n[10] Wassily Hoeffding. Probability inequalities for sums\nofboundedrandomvariables. JournaloftheAmerican\nStatistical Association , 58(301):13–30, March1963.\n[11] Craig Stuart Sapp. Online database of scores in the\nHumdrumﬁleformat.In InternationalSocietyforMu-\nsic Information Retrieval Conference (ISMIR) , pages\n664–665, 2005.\n[12] M. Monsignori, Paolo Nesi, and Marius B. Spinu. Wa-\ntermarking music sheets. In PCM ’01: Proceedings\nof the Second IEEE Paciﬁc Rim Conference on Mul-\ntimedia,pages 646–653, London, UK,2001. Springer-\nVerlag.\n[13] Radu Sion, Mikhail J. Atallah, and Sunil Prabhakar.\nRights protection for discrete numeric streams. IEEE\nTrans. Knowl. Data Eng. (TKDE) , 18(5):699–714,\n2006.\n[14] Julien Lafaye and David Gross-Amblard. XML\nstreamswatermarking.In IFIPWG11.3WorkingCon-\nference on Data and Applications Security (DBSEC) ,\n2006.\n[15] Neuma project: network-enabled and user-friendly\nmusicanalysis tools,2008.\nhttp://neuma.irpmf-cnrs.fr .\n146"
    },
    {
        "title": "Improving Rhythmic Similarity Computation by Beat Histogram Transformations.",
        "author": [
            "Matthias Gruhne",
            "Christian Dittmar",
            "Daniel Gärtner"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417635",
        "url": "https://doi.org/10.5281/zenodo.1417635",
        "ee": "https://zenodo.org/records/1417635/files/GruhneDG09.pdf",
        "abstract": "Rhythmic descriptors are often utilized for semantic music classification, such as genre recognition or tempo detection. Several algorithms dealing with the extraction of rhythmic information from music signals were proposed in literature. Most of them derive a so-called beat histogram by auto-correlating a representation of the temporal envelope of the music signal. To circumvent the problem of tempo dependency, post-processing via higher-order statistics has been reported. Tests concluded, that these statistics are still tempo dependent to a certain extent. This paper describes a method, which transforms the original auto-correlated envelope into a tempo-independent rhythmic feature vector by multiplying the lag-axis with a stretch factor. This factor is computed with a new correlation technique which works in the logarithmic domain. The proposed method is evaluated for rhythmic similarity, consisting of two tasks: One test with manually created rhythms as proof of concept and another test using a large realworld music archive.",
        "zenodo_id": 1417635,
        "dblp_key": "conf/ismir/GruhneDG09",
        "keywords": [
            "rhythmic descriptors",
            "semantic music classification",
            "genre recognition",
            "tempo detection",
            "beat histogram",
            "auto-correlation",
            "tempo dependency",
            "post-processing",
            "higher-order statistics",
            "tempo independent"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nIMPROVING RHYTHMIC SIMILARITY COMPUTATION BY BEAT\nHISTOGRAM TRANSFORMATIONS\nMatthias Gruhne\nBach Technology AS\nghe@bachtechnology.comChristian Dittmar\nFraunhofer IDMT\ndmr@idmt.fhg.deDaniel Gaertner\nFraunhofer IDMT\ngtr@idmt.fhg.de\nABSTRACT\nRhythmic descriptors are often utilized for semantic mu-\nsic classiﬁcation, such as genre recognition or tempo de-tection. Several algorithms dealing with the extraction ofrhythmic information from music signals were proposed inliterature. Most of them derive a so-called beat histogramby auto-correlating a representation of the temporal enve-lope of the music signal. To circumvent the problem oftempo dependency, post-processing via higher-order statis-tics has been reported. Tests concluded, that these statis-tics are still tempo dependent to a certain extent. Thispaper describes a method, which transforms the originalauto-correlated envelope into a tempo-independent rhyth-mic feature vector by multiplying the lag-axis with a stretchfactor. This factor is computed with a new correlation tech-nique which works in the logarithmic domain. The pro-posed method is evaluated for rhythmic similarity, consist-ing of two tasks: One test with manually created rhythmsas proof of concept and another test using a large real-world music archive.\n1. INTRODUCTION\nDuring the last years the need of new search and retrieval\nmethods for digital music increased signiﬁcantly due to the\nalmost unlimited amount of digital music on users harddisks and in online stores. An important pre-requisite for\nthese search methods is the semantic classiﬁcation, which\nrequires suitable low- and mid-level features. The ma-\njor goal of many researchers is the computation of mid-\nlevel representations from audio signals, which are des-\ntined to capture the rhythmic gist from the music. A hugeamount of work has been done in this ﬁeld so far by devel-oping techniques like beat histogram, inter-onset-intervalhistogram or rhythmic mid-level features, e.g., [1], [2], [3],\n[4], [5]. In general, the beat histogram technique very of-\nten used as feature basis for semantic classiﬁcation. Thishistogram is computed by taking the audio spectrum en-\nvelope signal, which is differentiated and half/full-wave\nrectiﬁed. As a ﬁnal step an auto-correlation function is ap-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.plied, which estimates the periodicities within the modiﬁed\nenvelope. The resulting feature vector is only limited us-\nable for pattern recognition. Two similar rhythms are eas-ily comparable with the beat histogram as feature, if their\ntempi are equal. A different tempo leads to a compression\nor expansion of the lag-axis, as depicted in Figure 1. This\nmodiﬁcation has a disadvantageous effect when perform-\ning a comparison of beat histograms via Euclidean distance\nmeasure. This issue has been raised by Foote [6]. A num-ber of approaches tried to come up with solutions for thatchallenge. Paulus [7] presented a method, which could beconsidered reasonable for comparing beat histogram vec-tors containing different tempi by applying a dynamic timewarping technique. A similar approach has been also pro-\nposed by Holzapfel [8]. These techniques require special-ized classiﬁers and the beat histogram cannot be used as\nfeature in conjunction with other low-level features. In or-\nder to solve that problem, Tzanetakis [1], Gouyon [2], and\nBurred [3] computed descriptive statistics, such as mean,\nvariance, and kurtosis on the beat histogram. These statis-tics were used as feature vector for classiﬁcation. To a cer-tain degree, these are also tempo-dependent. This papersuggests a new post-processing method which performs atransformation of the beat histogram into the logarithmiclag domain. The transformation into the logarithmic do-main has not been described for rhythm features, but forharmonic and chroma features in [9] and [10]. This trans-formation transfers the multiplicative factor of the tempo\nchanges into an additive offset. Hence, the transformed\nrhythmic feature vector contains a tempo independent partlocated on the right-hand side of the vector. An approachfor detection of this tempo independent rhythmic informa-\ntion is presented. A number of different features were ex-tracted and evaluated for the task of rhythmic similarity.\nThe remainder of this paper will be organized as fol-\nlows: Section 2 introduces the proposed algorithm, Sec-\ntion 3 describes the evaluation and discusses the results.Section 4 concludes and indicates further directions in thisarea.\n2. PROPOSED APPROACH\nIn this work, the beat histogram is extracted from MPEG-\n7 AudioSpectrumEnvelope (ASE) features [11]. Different\nvariants of the basic feature extraction algorithm have beenreported in literature. Tzanetakis’ [1] work was based ona wavelet transform, Scheirer [12] used a ﬁlter bank. Nev-\n177Oral Session 2: Tempo and Rhythm\nertheless, both authors extracted an envelope signal from\nnon-linearly spaced frequency bands, as is the case withASE. In the proposed implementation, the different ASE\nbands are smoothed in time. Subsequently, the bands are\nweighted by enhancing the lower and higher frequency ba-\nnds and decreasing the center frequencies. All bands areaccumulated, differentiated in time, and full-wave-rectiﬁed.\nThis results into a so-called detection function, containing\nthe the most salient rhythmic information of the music sig-nal. The detection function is subdivided into snippets ofN successive frames. The auto-correlation inside such aframe yields the beat histogram, also called rhythmic mid-level feature, beat spectrum, etc.\nThe beat histogram may be used in a different num-\nber of applications, such as beat tracking or tempo detec-\ntion. As already mentioned in Chapter 1, this vector shouldnot be directly utilized for classiﬁcation. If two similar\nrhythms are played in different rhythms and there beat his-\ntograms are compared, the vectors would look similar, but\none would be a more stretched or compressed (in terms ofthe lag-axis) version from the other. Hence, a direct com-parison of these vectors using common distance measures(e.g., Euclidean distance) results in large distances. Thus,it is state of the art to compute descriptive statistics fromthe beat histogram and use these measures as features forclassiﬁcation. Unfortunately, these statistics are also proneto tempo changes.\nIn order to create a tempo independent beat histogram,\nFoote [6] proposed to stretch or compress the original vec-tor based on the tempo of the rhythm. The compression ofthe beat histogram can be considered as multiplication of a\ntime-stretching factor fwith the argument τof the under-\nlying pattern signal c(τ\n/prime). This pattern signal can be the\nmentioned auto-correlation signal. The observed feature\nvector can therefore be described with c(τ)=c(τ/prime∗f).I n\norder to obtain the tempo invariant beat histogram c(τ/prime),\nthe stretch factor fneeds to be known, but its automatic\ncomputation might be unreliable. One option for solvingthis issue is to use a logarithm function. By applying thelogarithm on an arbitrary function, multiplicative terms are\ntransformed to additive terms. Transferring this theorem to\nthe lag-axis of the beat histogram c(τ)leads to the equation\n(1):\nc(log(τ\n/prime∗f)) =c(log(f)+l o g ( τ/prime)) (1)\nFor the logarithmic processing step, a new argument is\nestimated by (2):\nτlog =log(τ)∗max (τ)\nlog(max (τ))(2)\nResampling the original beat histogram c(τ)in such a\nway, that the values in τare available on places of τlog\nresults in a new beat histogram feature with logarithmized\nlag-axis (Figure 2 d).\nSince τlogconsists of non-integer values, the practical\nimplementation of this variable requires an interpolation.For this task, a bicubic interpolation method as describedin [13] has been applied.0 50 100 150 200 25000.51\nRhythmic GridEnergy\n0 50 100 150 200 25000.51\nLogarithmized Rhythmic Grid\n0 100 200 300 400 50000.51\nBeat HistogramEnergy\n0 100 200 300 400 50000.51\nLogarithmized Beat Histograma) b)\nc) d)\nFigure 2 . This ﬁgures shows an example beat histogram\n(c) and a rhythmic grid (a) and their logarithmic counter-\nparts (d,a, respectively).\nFigure 2 c,d shows an example beat histogram and its\ntransformation into the log-lag domain.\nBy inspecting a large number of such logarithmized vec-\ntors it can be observed, that all vectors consist of a largedecaying slope towards a ﬁrst local minimum, whose ab-solute position depends on the tempo of the music. Thatslope represents the ﬁrst maximum lobe of the auto-correlationfunction. Due to the fact, that a time-varying signal is al-\nways most similar to itself for small lags, the ﬁrst lobe\nis always the highest and does not carry any signiﬁcantrhythmic information. However, the successive minimum\nappears to be the point from where on the logarithmized\nbeat histogram shows similar tempo-independent charac-\nteristics if the rhythm is similar. These characteristics aresimilar, but they are moved further right or further left, de-pending on the tempo. The goal is to ﬁnd the starting pointof these tempo-independent characteristics and to use thetempo-independent excerpt of the feature vector for classi-ﬁcation. In the original beat histogram the ﬁrst local min-imum (or maximum) could be used as starting point forstretching or compressing the vector in order to receive a\ntempo-independent version. Unfortunately, this procedureis only applicable on a minority of rhythms, since often the\nﬁrst local minimum is misleading and the stretched vectorresults in octave errors. In the log-lag domain the resultwould be similar, if only the ﬁrst minimum is used. The\nproposal in this publication is to ﬁnd the point more re-liably by taking the evolution of the vector into account.\nTherefore, the authors use an artiﬁcial rhythmic grid fea-turing eight successive Gaussian pulses as depicted in Fig-ure 2 a. The Gaussian pulses are computed as described inthe following Matlab code snippet (Code 1) with the block-sizeblksize as functional parameter and tmp\nacf as result\nvector.\nThis rhythmic grid is transformed into the logarithmic\ndomain with the same method as described above. In order\nto ﬁnd the tempo-independent characteristics of the loga-rithmized beat histogram, both vectors, the logarithmizedrhythmic grid and the logarithmized beat histogram are\n17810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n50 100 150 200 25000.20.40.60.81Magnitude\n50 100 150 200 25000.20.40.60.81\n50 100 150 200 25000.20.40.60.81\nFigure 1 . These ﬁgures depict a beat histogram excerpt for the same rhythm with tempos of 90 Bpm (left), 110 Bpm\n(middle), 130 Bpm (right).\nCode 1 Example Matlab code for the creation of Gaussian\npulses\nmu = [29:29:blksize]; sd = 2;tmp_acf = zeros(1,blksize); lobe=[];\nfor k = 1:length(mu)t_exp=-0.5 *(((1:blksize)-mu(k))/sd).ˆ2;\nlobe(k,:) = exp(t_exp)/(sd *sqrt(2*pi));\nlobe(k,:) = lobe(k,:)/max(lobe(k,:));tmp_acf = tmp_acf + lobe(k,:);end\ncross-correlated. Best results could be achieved by onlyevaluating only the ﬁrst slope (histogram points 200-300 in2 d). The maximum of the correlation function equals thepoint in the vector, where the tempo-independent charac-teristic starts. A faster tempo results in a shift of the tempo-independent part to the left, and thus additional peaks ap-pearing at the right border. In order to process almost iden-tical beat histograms, regardless of the tempo, the length ofthe tempo independent characteristics has to be suitably re-stricted. This tempo independent vector could be theoreti-\ncally used as feature vector for rhythmic similarity. Due to\nthe interpolation for the logarithmic processing, small vari-ations lead sometimes to a small movement either to theright or to the left side of the axis. These small variationsaffect the rhythmic similarity negatively. In order to reducethis effect, statistical measures as proposed by the otherauthors have been applied in the tests for this paper. The\nfollowing statistics as described by Tzanetakis [1], Gou-\nyon [4], and Burred [3] were computed from the tempoindependent vector. All statistics from these authors were\nappended and formed the ﬁnal feature vector for the exper-iments:\n•Tzanetakis: Relative amplitude (divided by the sum\nof amplitudes) of the ﬁrst, and second histogram peak;\nration of the amplitude of the second peak dividedby the amplitude of the ﬁrst peak; period of the ﬁrst,second peak in bpm; overall sum of the histogram\n•Gouyon: Mean of magnitude distribution; geometric\nmean of magnitude distribution; total energy; cen-troid; ﬂatness; skewness; high-frequency content\n•Burred: Mean; standard deviation; mean of the deri-\nvative; standard deviation of the derivative; skew-\nness; kurtosis and entropy.\nSince some statistics from Gouyon and Burred partly\noverlapped the ﬁnal feature size consisted of 18 dimen-\nsions. For the practical implementation, excerpts of 500\nASE frames were chosen, which corresponds to 5 secondsin music, given a low-level hop-size of 10 milliseconds.This size constitutes a trade-off between the length of atleast two repeating patterns and the ability to track abrupttempo changes sometimes encountered in real-world mu-sic. A correlation size of 5 seconds has been also used inprevious approaches (e.g., [14]). Since the test songs con-tain more than ﬁve seconds of audio content, one of such afeature vector is computed every 0.5 seconds. In order tocompute the Gaussian pulses, a default standard deviationof 2 has been chosen and and only eight successive pulseswere used in the evaluation. Another standard deviationcould also be chosen, which increases/decreases the width\nof the pulses.\nFor the tests in this paper, the following 4 feature vec-\ntors were created:\n•Statistics of original beat histogram: The beat his-\ntogram has been extracted as described in this paper.Based on that histogram, a feature vector contain-ing all statistics by Tzanetakis [1], Gouyon [4], and\nBurred [3] as described above was extracted.\n•Statistics of logarithmized beat histogram: The statis-\ntics by Tzanetakis, Gouyon, and Burred were com-\nputed from the logarithmized beat histogram tech-nique as described above.\n•Statistics of beat histogram with stretch factor: Based\non the logarithmized beat histogram, a point has been\nestimated, where the tempo-independent rhythmic\ncharacteristic begins. This point has been transformed\ninto the non-logarithmic domain and a stretch fac-tor (as proposed by Foote) has been computed. The\noriginal beat histogram has been stretched by the\n179Oral Session 2: Tempo and Rhythm\nstretch factor and the statistics from Tzanetakis, Gou-\nyon, and Burred were computed from that vector.\n•Beat histogram with stretch factor: The original beat\nhistogram has been stretched as suggested by Foote\nwith the stretch factor derived from the logarithmic\npost-processing.\n3. EV ALUATION\n3.1 Evaluation ProcedureIn order to test the logarithmic post-processing of the beat\nhistogram, two different evaluation strategies were imple-mented. The ﬁrst test evaluated a number of manually cre-ated rhythms in order to prove the theoretic improvementof the results. The second test evaluates rhythmic similar-ity based on beat histograms with a large real-world musicset.\n3.1.1 Tests based on manually created rhythms\nThe ﬁrst test scenario examined the tempo dependence of\nthe described feature sets based on different rhythms. Anumber of 18 different base rhythms were established, whichcan be divided into 9 rhythm genres, e.g., electro, drum’n’baseor hip hop. The rhythms were played without any addi-tional instruments in order to test the tempo dependence ofonly the base rhythms. Each of these rhythms was playedin six different tempo variations ranging from 90 Bpm to190 Bpm in 20 Bpm steps. Each base rhythm was repeated\na number of times, whereby the duration of one single\nrhythm pattern was less than 5 seconds. A total of 108rhythms were collected and the low-level ASE features as\nwell all four versions of the described mid-level features\nwere extracted. Since the window length of the described\nmid-level features consisted of 5 seconds, the base rhythm\nof every rhythm class is contained in every frame of thefeature matrix. Therefore, an arbitrary frame from the fea-ture matrix can be chosen for comparison. In the evalua-tion for this paper, the second consecutive vector was usedas mid-level feature. Prior to the classiﬁcation, a mean anda variance normalization step over all data was applied.\nA simple k-nearest neighbor classiﬁer with Euclidean dis-\ntance was set up using the features and the rhythm classinformation as ground-truth. kfor the k-nearest neigh-\nbor classiﬁer has been chosen to be one. Subsequently,\nall features were consecutively used as query to the classi-ﬁer, whereby it has been ensured, that the query item wasnot contained in the reference set. The evaluation methodreturned the distance and the closest class to each of the108 rhythms. The average accuracy has been estimated perclass. The minimum, maximum and average of the over-\nall test set has been estimated by using the class-dependentaccuracy. Based on the results of this simple classiﬁer abase-line assumption can be made about the accuracy ofthe tempo independent rhythmic classiﬁcation. One might\nraise concerns that the comparison of base rhythms is not\nvery practice relevant, since popular music contains addi-tional polyphonic properties in the signal, which may in-terfere with the beat histogram. In order to prevent this”distortion” it has been shown, e.g. in [15], that drum tran-\nscription algorithms as preprocessing steps have a positiveeffect on beat histogram.\n3.1.2 Tests based on a large test set\nTo evaluate the performance on real world data instead of\nthe rather artiﬁcial data, a diverse set of 753 songs from60 different genres and sub-genres was compiled. Rhyth-\nmic similarity measures are hard to evaluate by using real\nworld data. An option for testing rhythmic similarity mea-\nsures can be based on the assumption, that songs from the\nsame genre have similar rhythms, while songs from dif-ferent genres have different rhythms. But similar rhythms\nmight be also available across genres and the results would\nnot directly predicate rhythmic similarity. To cope withthat, another approach was chosen. A rhythm similarityground truth was manually created for the used dataset.\nFirst, for each song, a representative rhythm pattern was\nannotated by hand, then a similarity matrix from all pairsof rhythms was calculated.\nRepresentative rhythm pattern: For each song, one rep-\nresentative rhythm pattern was manually annotated. Fivedifferent classes of rhythmical events were differentiated:base drum, snare drum, hi hats, further percussive events,and non-percussive events. A quantization could be freelychosen, but in general, events have been quantized onto1/16 bar length in case of a 4/4 bar and 1/12 bar length incase of a 3/4 bar. Similarity between patterns: The dis-tance between two characteristic patterns was calculatedby performing the following steps. First, both of the pat-terns have been stretched onto the same length. Then, allthe simultaneous occurrences of an event of a certain classin both patterns were summed up. Finally, the resultingvalue was normalized by the length of the pattern. For eachof the mentioned percussion classes, the 753x753 distancematrix was computed. Afterwards, the mean distance ma-\ntrix was estimated by equally weighting all distances of the\ndistance matrices from each percussion class.\nAlso, for each song in the database the features de-\nscribed above were extracted whereby the mean value forall feature frames of a song was calculated. Using Eu-\nclidean distance, the 5 closest songs to each song excluding\nthe query itself were determined. The list of the 5 closest\nsongs to the query song Care denoted L\nC. Incorporating\nboth the ground-truth rhythm similarity matrix and the listof the 5 closest songs for each of the 753 queries, the dif-ferent feature sets were compared using the following pro-cedure: For each query song C, a list T\nCof all the other\nsongs, was generated. This list was sorted in ascending or-der of the distances derived from the manually annotatedrhythm patterns. Then, for each song cinL\nCthe number\nof songs in TChave been counted, which were closer to C\nthanc. By averaging over these numbers, a value ris cal-\nculated. This value describes the mean number of songs\ninTCthat are closer to the query song than the retrieved\nsongs. In order to obtain a statement about the accuracyof the system in such a way, that higher numbers refer tobetter results, a score has been computed by S\ni=|S−1|.\n18010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n01020304050607080\nStatistics Beat\nHistogramStatistics Log\nBeat\nHistogramStatistics\nStretched Beat\nHistogramStretched Beat\nHistogramMean Accuracy\nFigure 3 . Average accuracy for rhythmic classiﬁcation of\nthe ﬁrst test based on different feature vectors in percent.\nMean Min Max\nStat. Original Beat Hist. 25.93 0.00 83.33\nStat. Logarithm. Beat Hist. 57.41 0.00 100.00\nStat. Stretched Beat Hist. 51.85 16.67 66.67\nStretched Beat Hist. 66.67 33.33 100.00\nTable 1 . Accuracy measures (ﬁrst test) for rhythm classi-\nﬁcation based on different feature vectors in percent.\nThis score is referred to the term similarity index. For sig-\nniﬁcance purposes a random score has been established bygenerating a random result list for each of the 753 songs.This result list has been evaluated in a similar procedure asthe described mid-level features.\nOther rhythmic similarity measures were described in\nliterature by Hofman-Engl [16] and Toussiant [17]. Thesemeasures are established when it comes to the compari-son of actual rhythmic descriptions. In this paper featuresbased on rhythms are to be compared. Therefore, thesemethodologies could not be applied.\n3.2 Results and Discussion\n3.2.1 Test based on manually created rhythmsThe following table (Table 3.2.1) shows the results for the\nﬁrst test containing the manually created rhythms. This\ntable shows minimum, maximum and mean accuracy. In\norder to get a quick overview about the results in general,\nthe mean is also plotted in Figure 3.\nThe state of the art methodology by computing statis-\ntics over the beat histogram achieves an average accuracyof approx. 26%. This is based on the fact, that the statis-tic measures are by far not tempo independent. Better re-sults could be obtained by the logarithmic post-processingstep. The statistics computed on the logarithmized beathistogram and over the stretched beat histogram performedreasonably well with 57.4% and 51.9%, respectively. Thebest results could be obtained by the stretched beat his-togram with the stretch factor computed from the logarith-mized beat histogram. This methodology leads to an aver-age accuracy of 66.7%. An intuitive guess would be, thatidentical rhythms in different tempos should always returnan accuracy of 100%. In practice, the results look differ-0,60,610,620,630,640,650,660,670,680,690,7\nRandom Statistics\nBeat\nHistogramStatistics\nLog Beat\nHistogramStatistics\nStretched\nBeat\nHistogramStretched\nBeat\nHistogramSimilarity Index\nFigure 4 . Similarity index (second test) expressing the\nrhythmic similarity for different feature vectors.\nent due to windowing effects. The minimum accuracy of\nthe algorithms ranges from 0% to 33.3%. This is based on\nthe fact, that the separability between some of the 18 base\nrhythms is strongly restricted. The highest accuracy is ob-tained by the stretched beat histogram also in case of theminimum. This might imply that postprocessed beat his-\ntogram performs better as feature than the statistics over\npostprocessed beat histograms. A similar statement canbe also made by evaluating the maxima of the four fea-\nture vectors. These tests prove, that the tempo independent\nversion of the beat histogram (stretched beat histogram)outperforms the statistics over the beat histogram.\n3.2.2 Test based on a large real-world music set\nThe following ﬁgure (Figure 4) shows the accuracy for\nthe test with real-world music. Additionally, these num-bers are depicted in Table 3.2.2. The similarities betweenmanually annotated base rhythms and the beat histogramfeatures are expressed by a similarity index. The higherthe index is, the better is the similarity between the man-ually annotated rhythms and the automatically extractedrhythms. The ﬁgure shows, that a random generation ofsimilarities results with a similarity index of 0.632. Mostof the observed feature vectors obtained a similarity in-\ndex around 0.65, including the statistics over the beat his-\ntogram, the statistics over the logarithmized beat histogramand the stretched version of the beat histogram. The statis-tics computed from the stretched beat histogram outper-form all other results by a similarity index of 0.03.\nThe ﬁrst test, which was based on the manually cre-\nated rhythms, showed the best results on the stretched beathistogram. In this second test, these results cannot be val-idated in every case. This may be based on the fact thatthe point in the logarithmic domain, which separates the\ntempo dependent and tempo independent parts is inaccu-rate in a few cases. These inaccuracies have inﬂuence on\nthe stretched beat histogram and may result in octave er-\nrors, which affect the rhythmic similarity. However, com-\nputing the descriptive statistics over the resulting vectors\nimproves the results. These statistics seem to neglect theslight deviations signiﬁcantly. This test on real world datamight be not optimal, since rhythms in real songs might\n181Oral Session 2: Tempo and Rhythm\nFeature Name Similarity Index\nRandom 0.632\nStat. Original Beat Hist. 0.658\nStat. Logarithm. Beat Hist. 0.650\nStat. Stretched Beat Hist. 0.687\nStretched Beat Hist. 0.648\nTable 2 . Similarity index of the second test expressing the\nrhythmic similarity for different feature vectors.\nchange and the evaluation was performed on one repre-\nsentative rhythm of the song. But this methodology gives\na rough indication of the performance of the logarithmic\nprocessing.\n4. CONCLUSIONS AND FUTURE WORK\nThe rhythmic information from music is captured by the\ncommonly used beat histogram. This paper presented apost-processing technique for the beat histogram, which is\nbased on logarithmic re-sampling of the lag axis and cross-\ncorrelation with an artiﬁcial rhythmic grid. This techniqueseems to improve the applicability of the beat histogramtechnique as feature for music information retrieval tasks.The practical tests on a large music archive were based ona mean feature vector per song. In order to be more accu-rate, future tests should perform a rhythmic segmentationand analyze the segments individually. The logarithmicprocessing methodology as described in this paper maybe also beneﬁcial for beat tracking and tempo detection.Future tests will provide an evaluation, if the tempo esti-mation results can be improved when using the proposedalgorithm.\n5. ACKNOWLEDGMENT\nThis work has been partly supported by the PHAROS Inte-\ngrated Project (IST-2005-2.6.3), funded under the EC IST6th Framework Program. Additionally, this project has\nbeen funded by the MetaMoses project (nr. 183217) from\nthe Norwegian research council.\n6. REFERENCES\n[1] G. Tzanetakis and P . Cook. Musical genre classiﬁca-\ntion of audio signals. IEEE Transactions on Speech,\nAudio, and Language Processing , 10(5):293–302,\n2002.\n[2] F. Gouyon and S. Dixon. A review of automatic rhythm\ndescription systems. Computer Music Journal , 29(1),\n2005.\n[3] J. Burred and A. Lerch. A hierarchical approach to au-\ntomatic musical genre classiﬁcation. In Proceedings of\nthe 6th International Conference on Digital Audio Ef-fects (DAFx-03) , 2003.\n[4] F. Gouyon, S. Dixon, E. Pampalk, and G. Widmer.\nEvaluating rhythmic descriptors for musical genre clas-siﬁcation. In Proceedings of the 25th AES International\nConference , 2004.\n[5] S. Dixon, F. Gouyon, and G. Widmer. Towards charac-\nterisation of music via rhythmic patterns. In Proceed-\nings of the 25th AES International Conference , 2004.\n[6] J. Foote and S. Uchihashi. The beat spectrum: A\nnew approach to rhythm analysis. In Proceedings of\nthe International Conference on Multimedia and Expo(ICME) , 2001.\n[7] J. Paulus and A. Klapuri. Measuring the similarity of\nrhythmic patterns. In Proceedings of the 3rd Interna-\ntional Symposium on Music Information Retrieval (IS-MIR) , 2002.\n[8] A. Holzapfel and Y . Stylianou. A scale transform based\nmethod for rhythmic similarity of music. In Proceed-\nings of the IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP) , 2009.\n[9] S. Saito, H. Kameoka, T. Nishimoto, and S. Sagayama.\nSpecmurt analysis of multi-pitch music signals withadaptive estimation of common harmonic structure. InProceedings of the 6th International Conference onMusic Information Retrieval , 2005.\n[10] J. Jensen, M. Christensen, D.P .W. Ellis, and S. Jensen.\nA tempo-insensitive distance measure for cover songidentiﬁcation based on chroma features. In Proceed-\nings of the IEEE International Conference on Audio,\nAcoustics, and Signal Processing (ICASSP) , 2008.\n[11] M. Casey. Mpeg-7 sound recognition. IEEE Transac-\ntion on Circuits and Systems Video Technology, specialissue on MPEG-7 , 11:737–747, 2001.\n[12] E. Scheirer. Tempo and beat analysis of acoustic musi-\ncal signals. Journal of the Acoustical Society of Amer-\nica, 103(1):588–601, 1998.\n[13] William H. Press, Saul A. Teukolsky, William T. V et-\nterling, and Brian P . Flannery. Numerical Recipes in C .\nCambridge University Press, 1992.\n[14] S. Dixon, E. Pampalk, and G. Widmer. Classiﬁcation of\ndance music by periodicity patterns. In Proceedings of\nthe 4th International Symposium on Music Information\nRetrieval (ISMIR) , 2003.\n[15] M. Gruhne and C. Dittmar. Improving rhythmic pattern\nfeatures based on logarithmic preprocessing. In Pro-\nceedings of the 126th Audio Engineering Society (AES)\nConvention , 2009.\n[16] L. Hofmann-Engl. Rhythmic similarity: A theoretical\nand empirical approach. In Proceedings of the Sev-\nenth International Conference on Music Perception\nand Cognition , 2002.\n[17] G.T. Toussaint. A comparison of rhythmic similarity\nmeasures. In Proceedings of the 5th International Con-\nference on Music Information Retrieval , 2004.\n182"
    },
    {
        "title": "Modeling Harmonic Similarity Using a Generative Grammar of Tonal Harmony.",
        "author": [
            "W. Bas de Haas",
            "Martin Rohrmeier",
            "Remco C. Veltkamp",
            "Frans Wiering"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416212",
        "url": "https://doi.org/10.5281/zenodo.1416212",
        "ee": "https://zenodo.org/records/1416212/files/HaasRVW09.pdf",
        "abstract": "In this paper we investigate a new approach to the similarity of tonal harmony. We create a fully functional remodeling of an earlier version of Rohrmeier’s grammar of harmony. With this grammar an automatic harmonic analysis of a sequence of symbolic chord labels is obtained in the form of a parse tree. The harmonic similarity is determined by finding and examining the largest labeled common embeddable subtree (LLCES) of two parse trees. For the calculation of the LLCES a new O(min(n, m)nm) time algorithm is presented, where n and m are the sizes of the trees. For the analysis of the LLCES we propose six distance measures that exploit several structural characteristics of the Combined LLCES. We demonstrate in a retrieval experiment that at least one of these new methods significantly outperforms a baseline string matching approach and thereby show that using additional musical knowledge from music cognitive and music theoretic models actually helps improving retrieval performance.",
        "zenodo_id": 1416212,
        "dblp_key": "conf/ismir/HaasRVW09",
        "keywords": [
            "harmonic analysis",
            "parse tree",
            "remodeling",
            "LLCES",
            "distance measures",
            "structural characteristics",
            "retrieval experiment",
            "baseline string matching approach",
            "music cognitive models",
            "music theoretic models"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMODELING HARMONIC SIMILARITY USING A GENERATIVE\nGRAMMAR OF TONAL HARMONY\nW. Bas de Haas\nUtrecht University\nBas.deHaas@cs.uu.nlMartin Rohrmeier\nUniversity of Cambridge\nmr397@cam.ac.ukRemco C. Veltkamp\nUtrecht University\nRemco.Veltkamp@cs.uu.nlFrans Wiering\nUtrecht University\nFrans.Wiering@cs.uu.nl\nABSTRACT\nIn this paper we investigate a new approach to the simi-\nlarity of tonal harmony. We create a fully functional re-\nmodeling of an earlier version of Rohrmeier’s grammar of\nharmony. With this grammar an automatic harmonic anal-\nysis of a sequence of symbolic chord labels is obtained\nin the form of a parse tree. The harmonic similarity is\ndetermined by ﬁnding and examining the largest labeled\ncommon embeddable subtree (LLCES) of two parse trees.\nFor the calculation of the LLCES a new O(min(n, m)nm)\ntime algorithm is presented, where nandmare the sizes\nof the trees. For the analysis of the LLCES we propose\nsix distance measures that exploit several structural char -\nacteristics of the Combined LLCES. We demonstrate in a\nretrieval experiment that at least one of these new meth-\nods signiﬁcantly outperforms a baseline string matching\napproach and thereby show that using additional musical\nknowledge from music cognitive and music theoretic mod-\nels actually helps improving retrieval performance.\n1. INTRODUCTION\nHarmonic Similarity is a relatively new research topic with -\nin Music Information Retrieval (MIR) that is concerned\nwith determining the similarity of the chord sequences in\nsongs and enables users to search for songs on the basis\nof their harmony. Retrieval based on harmony offers obvi-\nous beneﬁts: it allows for ﬁnding cover songs (especially\nwhen melodies vary), songs of a certain family (like Blues\nor Rhythm Changes), or variations over standard basses in\ninstrumental baroque music, to name a few. So far, very\nfew measures of harmonic similarity have been proposed.\nDe Haas et al. [1] developed a distance measure based on\nLerdahl’s Tonal Pitch Space [2].\nWhen researching MIR, it is important to realize that\nonly part of the information needed for good similarity\njudgment can be found in the musical data. Musically\nschooled as well as unschooled listeners have extensive\nknowledge about music [3,4] and one important task of a\nMIR researcher is to select or develop the appropriate mu-\nsic cognitive and music theoretical models that provide the\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage an d that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieva l.knowledge needed for making good similarity judgments.\nWe strongly believe that such a model is necessary, and\nthat systems without such additional musical knowledge\nare incapable of capturing a large number of important\nmusical features. In this study we report a new method\nof harmonic similarity matching that applies a remodeling\nof Rohrmeier’s [5] phrase-structure grammar for tonal har-\nmony as underlying cognitive and music theoretical model.\nIn analogy to linguistics, various hierarchical models\nof musical structure have been proposed since the 1980s\nand have been brought up recently in cognitive and com-\nputational discussions [6–8]. In this context, Rohrmeier’ s\ngenerative grammar of diatonic harmony [5] transfers no-\ntions about the hierarchical organization of tonal music\n[6,7] to the area of harmony. It is based on the assump-\ntion that, within a sequence of harmonies, different chords\nhave different degrees of stability and dependency, based\non their position within the hierarchical structure. In a\nchord sequence several chords may be replaced, inserted\nor omitted in such a way that the harmonic structure re-\nmains intact, whereas the changes of structurally impor-\ntant anchor chords may result in large structural modiﬁ-\ncations of the entire dependency structure of the harmony\nsequence. These dependency features and relationships re-\nsemble constituent structure and dependencies in linguis-\ntic syntax and can be modeled with a grammatical formal-\nism [9].\nThese variable relationships between chords and their\nstructural roles motivate the rationale not to base our har-\nmony matching methods on sequence matching methods–\nwhich assume the equal importance of all chords in a se-\nquence–but on a hierarchical formalization that incorpo-\nrates the differences in structural function. Figure 1, dis -\nplaying two versions of the jazz standard Take the ‘A’ train ,\nillustrates this idea. Even though both sequences appear to\nbe substantially different when compared element by ele-\nment, an analysis of their formal dependencies reveals that\nboth derive from a common harmonic pattern that is repre-\nsented by the parse trees and ﬁts human intuition.\nWe present a fully functional remodeling of Rohrmeier’s\ngrammar [5], which parses sequences of symbolic chord\nlabels and returns parse trees like the ones in Figure 1, in\nsection 3. A parse tree is more than a harmonic analysis\nalone, since it contains all the structural relations of the\nharmonies used in a song, and is therefore very suitable for\ndetermining harmonic similarity. We compare parse trees\nby ﬁnding and examining the combined Largest Labeled\n549Oral Session 7: Harmonic & Melodic Similarity and Summarization\nPiece\nP\nPCP\nt\nI\nC69d\nV\nV1\nG13ii\nDm9V/V\nV/V1\nII7\nD7b9ii/V\nvi\nAm9P\nPCP\nt\nI\nCMaj7d\nV\nG7b9P\nHCP\nd\nd1\nd2\nV\nV1\nG13ii\nDm9V/V\nII7\nD7s1\nIV2\nF69s\nIV\nIV1\nFMaj7V/IV\nI7\nC7t\nI\nC69P\nHCP\nd\nV\nV1\nG13ii\nDm9V/V\nV/V1\nII7\nD7b9ii/V\nvi\nAm9t\nI\nCMaj7P\nHCP\nd\nd1\nV\nG13s\nii\nDm9t\nI\nC69P\nHCP\nd\nV\nV1\nG13ii\nDm9V/V\nV/V1\nII7\nD7b9ii/V\nvi\nAm9t\nI\nCMaj7P\nHCP\nd\nd1\nV\nG13s\nii\nDm9t\nI\nC69P\nHCP\nd\nV\nV1\nG13ii\nDm9V/V\nV/V1\nII7\nD7b9ii/V\nvi\nAm9t\nI\nCMaj7\nPiece\nP\nPCP\nt\nI\nCMaj7d\nd1\nV\nG7s\nii\nDm7P\nPCP\nt\nI\nCMaj7d\nV\nV1\nG7ii\nDm7V/V\nII7\nD7P\nPCP\nt\nI\nCMaj7d\nV\nVsub\nbII7\nDb7P\nHCP\nd\nd1\nV\nV1\nG7ii\nDm7V/V\nII7\nD7s\nIV\nFMaj7t\nI\nCMaj7P\nHCP\nd\nV\nV1\nG7ii\nDm7V/V\nII7\nD7Rept\nt1\nI1\nCMaj71t\nI\nCMaj7P\nHCP\nd\nV\nV1\nG7ii\nDm7V/V\nII7\nD7t\nI\nCMaj7\nFigure 1 . Two parse trees of different versions of the same jazz stand ardTake the ‘A’ train . The leafs of the tree represent\nthe actual chords of the sequence.\nCommon Embeddable Subtree (LLCES). The LLCES is\nthe tree that can be included in both parse trees while main-\ntaining the labels and the ancestor relations. In section 4. 2\nwe present a new algorithm that ﬁnds the LLCES. Using\nthe LLCES we deﬁne a series of similarity measures for\ntonal harmony.\nContribution : First, we present a remodeling of a for-\nmal grammar for tonal harmony and propose solutions for\nsome of the typical problems of its application. Second, we\npresent a new O(min(n, m)nm)time algorithm that calcu-\nlates the LLCES, where nandmare the sizes of the trees.\nThird, six LLCES based distance measures are deﬁned to\ncompare the parse trees. Last, the retrieval performance\nof these distance measures is experimentally veriﬁed on a\ndataset of 72 symbolic chord sequences of jazz standards.\n2. RELATED WORK\nIn the last century, numerous formal theoretical approache s\nto western tonal music have been proposed. Formaliz-\ning Schenker’s theory [6], Lerdahl and Jackendoff [7] pro-\nposed a generative theory that organized western tonal mu-\nsic by recursive hierarchical dependency relationships be -\ntween musical elements in terms of time-span reduction\nand prolongation structure. They formalized the interac-\ntion between these structures with metrical and grouping\nstructure in terms of constraint based preference rules. Si m-\nilarly, there is some theoretical evidence that tonal har-\nmony is organized in a comparable, hierarchical way. Early\nattempts by Kostka and Payne ( [10] ch. 13) and Baroni\n[11] suggest that harmony is organized in hierarchical lay-\ners. Current theoretical approaches [5,12–15] suggest tha t\nthe structure of harmony sequences exceeds the simplic-\nity of a straightforward chord transition table (or ﬁnite-\nstate grammar [9]), like Piston’s table of root progression s\n[16], and may be modeled by hierarchical, context-free\nor phrase-structure grammars [9]. Pachet [17] proposes\na set of rewrite rules for jazz harmony similar to Steed-\nman’s grammar [12]. He shows that these rules could be\nlearned form chord sequence data in an automated fashion.Rohrmeier [5] gave an encompassing account how tonal\nharmonic relationships may be formalized using a genera-\ntive context-free grammar with variable binding.\n3. A GRAMMAR FOR TONAL HARMONY\nThe generative formalism proposed by Rohrmeier [5] ex-\npands on earlier approaches in a number of ways. Steed-\nman’s approach [12,13] is merely concerned with Blues\nprogressions and, featuring only seven context-sensitive\nrules (with variations), omits a number of theoretically im -\nportant features to extend to a broader domain. Rohrmeier’s\napproach extends on these ideas and gives an overarching\naccount of tonal harmony and tonal-phrase structure inde-\npendently of a speciﬁc style or musical form. In addition, it\nproposes to incorporate the structural distinctions betwe en\nform, theoretical harmonic function [18], scale degree pro -\nlongation [6,7] and surface feature realization into diffe r-\nent levels of the syntactic derivation. The present study\nproposes a remodeling of the grammar without modulation\nand with limited local tonicization and scale adaptation in\norder to reduce the complexity for the implementation of\na ﬁrst-stage working system. The current remodeling was\noptimized for jazz, but the aim is to develop a set of core\nrules that explain basic harmony structures which can be\naugmented with style speciﬁc rules.\nThe grammar incorporates four levels: a phrase level,\nfunctional level, scale-degree level and surface level. Th e\nphrase level divides a piece into phrases, the functional\nlevel speciﬁes the functional role a certain scale-degree h as\nwithin a phrase. The scale-degree captures the relation be-\ntween the chord and the key and the surface level expresses\nthe actual chord with all its possible additions, inversion s,\netc.\nBelow, the main rules of the grammar are listed in or-\nder to give an outline of the architecture of the grammar.\nA piece always consists of one or more phrases ( P). On\nthis phrase level the grammar distinguishes two types of\nphrases: phrases which end on a perfect cadence ( PCP )\nand phrases which end with a half-cadence ( HCP ). Per-\n55010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nfect cadence phrases are distinguished by ending with a\ntonic function ( t) upon which all subordinate harmonic el-\nements are dependent, whereas half-cadence phrases force\na phrase to end with a dominant function ( d) which results\nin a tonicization of, or a perfect or imperfect cadence on\nthe dominant.\n1.Piece →P+\n2.P→PCP\n3.P→HCP\n4.PCP →d t+|d d t+|t d t\n5.HCP →t+d\nAt the functional level, the grammar encapsulates core\nrelationships between the three main harmonic functions:\ntonic ( t), dominant ( d) and subdominant ( s).\n6.d→s d\n7.t→tpg\nThese functional rules can be applied recursively, but ﬁ-\nnally translate into scale-degrees. Rule 9 deals with certa in\nforms of parallels ( tpg).\n8.t→I\n9.tpg→vi|iii\n10.d→V|vii0\n11.s→ii|IV\nThe functional level also incorporates a number of addi-\ntional prolongational rules that allow for the derivation o f\nmore distant harmonic structures such as the preparatory\nuse of iii and tritone substitutions. Rule 12 incorporates\na feature speciﬁcally added for modeling the prototypical\nII-V-I sequences in jazz harmony that are less frequent in\nother styles.\n12.x→V(x)x|ii(x) V(x)xfor any scale degree x\n13. IV →iii IV\n14. V( x)→bII(x) for any scale degree x\nAt the surface level scale degree symbols are translated\ninto the actual surface chord. These translation steps are\nstraightforward when the key is known beforehand. For\ninstance, a VI symbol in the key of C minor would trans-\nlate into an A ♭-chord. In addition, elaborations of chords\nare added at this level of description: a surface realizatio n\nof a VI chord may result in a A ♭6 chord. Some of these\nsurface elaborations of chords are tied to their structural\nfunctions (strong typing), e.g. an Em7 ♭5 chord label indi-\ncates a subdominant function ii in a ii-V-I sequence, or a\nD7 chord label indicates a dominant function (except in\nblues contexts where minor sevenths loose their functional\nconnotation).3.1 Implementation and Parsing\nThere are some additional rules that have been implemented,\nbut are not described here. Among these are rules for typ-\nical voice-leading and prolongational structures and some\ntypical borrowings from the parallel key. Since we have\nnot incorporated modulation yet, it is necessary to label\nthese phenomena to be able to explain the remainder of\nthe piece. Furthermore, there are rules that deal with typi-\ncal well-known diminished chord transitions in various de-\nscending and ascending forms.\nThe grammar as speciﬁed above is not strictly a con-\ntext free grammar, because rule 12 and rule 14 use a vari-\nable binding. However, by expanding a rule for every el-\nement xthat it holds, a set of context free rules can be\ncreated that yields the exact same outcome. Having a con-\ntext free grammar, a free Java implementation [19] of an\nEarley Parser [20] is used to parse the chord sequences in\nO(n3)time, where nis the number of chords.\nContext free grammars often create multiple ambiguous\nparse trees. To select the optimal parse tree out of the set\nof parse trees, we provided the rules with weights (set by\nhand) and deﬁned the total weight of a parse tree as the\nproduct of the weights of the rules used in its construction.\nBecause of this, some rules have less chance to be used in\nthe ﬁnal derivation. This allows to select the best tree from\nthe ambiguous parse trees. The complete grammar as well\nas the lead-sheets of the examples in Fig 1 are available\nonline1.\n4. COMMON HIERARCHICAL STRUCTURES\nIn this section we present six distance measures for the\nparse trees generated by the grammar discussed in the pre-\nvious section. For the comparison of parse trees, we pro-\npose an approach based on the problem of tree inclusion,\nwhich is elaborately dealt with in [21]. Given the parse\ntrees of two songs, the general idea is to ﬁnd the collec-\ntion of largest labeled common embeddable subtrees (LL-\nCESs) for every combination of phrases. The LLCES is the\nlargest tree that is included in both parse trees. This means\nthat there exists a one-to-one mapping from the nodes of\nthe LLCES to the nodes with the same label in both parse\ntrees that preserves ancestor relations, but not necessari ly\nparent relations. When processing harmony parse trees,\nthis is a natural thing to do because typically a chord pro-\ngression is augmented by adding a structure to the left\nbranch and repeating the right branch, e.g. when a Dm is\nprepared by an A7 chord. Hence, if both trees are similar,\nthe LLCES reﬂect the structure of the parse trees it is gen-\nerated from, and if both trees are dissimilar, the resulting\nLLCES will be much smaller and less grammatical. In the\nnext sections we explain the calculation of the LLCES, and\nhow we use it to deﬁne six distance measures.\n4.1 Preliminaries\nA rooted tree is a structure denoted with P= (V, E,P root),\nwhere Vis a ﬁnite set of nodes, Eis the set of edges con-\n1http://give-lab.cs.uu.nl/music/\n551Oral Session 7: Harmonic & Melodic Similarity and Summarization\nB\nF A\nG\nX HYE\nA\nY XC\nA\nN\nY XML K\nFigure 2 . An example of a rooted by A that can be embed-\nded into two larger trees rooted by B and C.\nnecting nodes in V, andProotis the root of the tree P. The\nnodes of the parse trees generated by the grammar of sec-\ntion 3 are all labeled and the label of node vis denoted with\nlabel(v). The subtree of Prooted at node vis denoted with\nP[v]andchildren (v)denotes the subset of Vwith nodes\nthat have vas parent. Similarly desc(v), denotes the dece-\ndents of v, i.e. the subset of Vthat is contained in P[v]\nand have vas ancestor. Furthermore we use a few addi-\ntional functions, po(v)denotes the post order number that\nis assigned to a node vin a postorder traversal. depth(P)\ndenotes the depth of a tree, i.e. the number of nodes in the\nlongest path from leaf to the root. Finally, the degree (P)\nis the degree of a tree, i.e. the maximum number of chil-\ndren.\nWe say that a tree P= (V, E, P root)isincluded in a\ntreeT= (W, F, T root)if there exists an embedding of P\nintoT. An embedding is an injective function f, mapping\neach node in Pto a node in T, that preserves labels and\nancestorship. Figure 2 shows an example of an included\ntree. Note that a left-to-right ordering of the descendants is\nnot required. Formally, this means that for all nodes uand\nvinPit is required that:\n1.f(u) =f(v)if and only if u=v,\n2.label(u) =label(f(v)),\n3.uis an ancestor of vinPif and only if f(u)is an\nancestor of f(v)inT.\n4.2 Largest Labeled Common Embeddable Subtree\nWe are not aware of an algorithm that calculates the largest\ncommon embeddable subtree for labeled trees. Gupta and\nNishimura [22] have developed a O(n2.5logn)time al-\ngorithm for ﬁnding this tree for two unlabeled trees. The\nalgorithm we present here calculates the largest common\nembeddable subtree for the labeled case and expands on\nthe general tree matching ideas as described in [21], ch. 3.\nAlgorithm 1 calculates the LLCES of two trees P=\n(V, E, P root)andT= (W, F, T root). To store the nodes\nof the subtrees of the LLCES the algorithm uses a table\nMsuch that M[po(w)]stores the subtrees that can be em-\nbedded into both PandT[w]. The algorithm builds the\nLLCES up from the leaves by traversing the nodes of Tand\nPin postorder. When a node vwith an identical label as w\nis encountered (line 5), the algorithm creates a new node x\nwith the same label as v. In case wis a leaf, xis stored in\nM(lines 8–9). In case wis an internal node, we look up\nthe subtrees in Mthat match the children of w. Because\nthe tree is processed in postorder these nodes were previ-\nously stored in Mand can be retrieved from M[po(w′)]\nfor each child w′. If a previously stored subtree rooted byAlgorithm 1 Largest Labeled Common Embeddable Subtree\n1:procedure LLCES( P,T)\n2: M←∅\n3: for all w∈Win postorder do\n4: for all v∈Vin postorder do\n5: iflabel(v) =label(w)then\n6: x←new node\n7: label(x)←label(v)\n8: ifchildren (w) =∅then\n9: addxtoM[po(w)]\n10: else\n11: for all w′∈children (w)do\n12: for all x′∈M[po(w′)]do\n13: ifx′∈desc(v)then\n14: add(x, x′)toM[po(w)]\n15: else\n16: addx′toM[po(w)]\n17: end if\n18: end for\n19: end for\n20: addxtoM[po(w)]\n21: end if\n22: end if\n23: end for\n24: ifM[po(w)] =∅then\n25: for all w′∈children (w)do\n26: addM[po(w′)]toM[po(w)]\n27: end for\n28: end if\n29: end for\n30: return M[po(Troot)]\n31:end procedure\nx′is a descendant of v, this subtree becomes a child of the\nnew node x, by adding a new edge (x, x′)toM[po(w)]\n(lines 10–15). Otherwise, if x′is not a descendant of v,\nx′is stored in M[po(w)](line 16). After all, a common\nancestor can show up in a next iteration. Finally, the new\nsubtree xis stored in Mas well (line 20). If the label of\nwdoes not match any of the labels of the nodes in P, the\nsubtrees stored in Mfor all children w′ofware added\ntoM[po(w)](lines 24–28). This process continues until\nall nodes of Thave been matched against all nodes of P\nand ﬁnally M[po(Troot)], the LLCES of PandT, is re-\nturned. A drawback of our algorithm is that it is incapable\nof dealing with duplicate labels. Therefore we number the\nduplicate labels that descent the same phrase.\nThe running time of the algorithm is dominated by the\nlines 3-23. For each of the O(nm)combinations of wand\nv(lines 3, 4) a constant time test is performed. Because the\nlabels are unique, only min(n, m)times each of the O(n)\nnodes in the subtrees that has been stored in M[po(w)]so\nfar (line 12), is checked against each of the O(m)descen-\ndants of node v(line 13). This results in a time complexity\nfor the whole algorithm of O(min(n, m)nm).\n4.3 Distance Measures\nWe base the distance measures on the LLCES, but we do\nnot calculate the LLCES of two parse trees directly for two\nreasons. First, as we can see in Figure 1, there are quite\nsome duplicate labels in the parse trees which our algo-\nrithm cannot handle. Second, if a parse tree of a song con-\ntains a repetition of a phrase that the matched song does\nnot have, the repeated phrase cannot be matched. To solve\n55210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nPiece\nP\nt\nId\nd1\nVs\niiP\nt\nI\nCMaj7d\nV\nV1ii V/V\nII7P\nPCP\nt\nI\nCMaj7d\nVP\nHCP\nd\nd1\nV\nV1ii V/V\nII7\nD7s\nIV\nFMaj7t\nIP\nHCP\nd\nV\nV1ii V/V\nII7t\nI\nCMaj7P\nHCP\nd\nV\nV1ii V/V\nII7t\nI\nCMaj7\nFigure 3 . The Combined LLCES for every combination of phrases of the p arse trees of Take the ‘A’ Train as in Fig. 1.\nthese two problems we compare parse trees in a phrase-\nwise fashion. For every phrase in the target tree Twe\ncalculate the LLCES for every phrase of the pattern parse\ntreePand pick the largest LLCES to create a Combined\nLLCES (see Fig. 3). The duplicate labels are re-labeled per\nphrase too in preorder (see the superscripts in Fig. 1 and\n3). Because the number of duplicate labels per phrase is\nsmall, the labellings will be nearly identical if two trees\nhave a similar harmonic structure. Note that if the two\nparse trees TandPhave a different number of phrases, the\nstructure of the Combined LLCES will differ if Pis used\nas a target tree, because for every phase in the Ta LLCES\nis constructed. This makes every Combined LLCES based\nmeasure is asymmetrical.\nWe propose three distance measures for sequences of\nsymbolic chord labels based on the Combined LLCES:\n1.Relative Combined LLCES Size Distance (Rel) : By\ndividing the number of nodes in the target tree T\nby the number of nodes in the Combined LLCES a\ndistance measure between 0 and 1 is obtained that is\nnormalized by the size of T.\n2.Grammar Violation Distance (Viol) : if two trees are\nnot similar, the combined LLCES will contain con-\nnections between nodes that cannot be explained by\nthe grammar. By dividing the number of nodes in the\ntarget tree T(which are grammatical by deﬁnition)\nby the number of grammatical nodes in the Com-\nbined LLCES we obtain a distance measure between\n1 and 0 that is normalized by the size of T.\n3.Average Depth Distance (Dep) : if trees are very sim-\nilar, the level of complexity in the harmonic structure\nin the Combined LLCES will be comparable to the\nlevel of complexity target tree T. By dividing the\naverage leaf depth of Tby the average leaf depth of\nthe Combined LLCES, we obtain a distance measure\nbetween 1 and 0, that is normalized by the size of T.\nOne can observe in Figure 1 that, having the actual parse\ntree structure, the actual chord labels are not of much im-\nportance anymore. Given two similar sequences, it is rather\narbitrary whether the chords labels match or not: the struc-\nture of the harmony determines the similarity. Therefore\nwe can remove each leaf node describing a surface chord\nfrom the Combined LLCES and target trees. The structure\nof the phrase, functional and scale-degree level remains un -changed. As a consequence, this yields three additional\nharmonic distance measures that are concerned with the\nstructure of the harmony only. Other Combined LLCES\ndistance measures can be thought of.\n5. EXPERIMENT\nWe have evaluated the six LLCES based distance mea-\nsures described in the previous section in an experiment.\nWe assembled a dataset of 72 symbolic chord label se-\nquences extracted from user-generated Band-in-a-Box ﬁles\nthat were collected on the Internet. Band-in-a-Box is a\nsoftware package that generates accompaniment given a\ncertain chord sequence provided by the user. This dataset\nconsists of 51 different pieces of which 16 pieces contain\ntwo or three versions, forming 16 song classes. These\npieces are all jazz standards from before the 1970’s and\ncan all be found in the Real Book [23] or similar sources.\nAll parse trees of these pieces are available online2. The\ntask is to retrieve the other versions of a song class, given\na certain query song from that class. All songs containing\nmore than one version are used as a query and the rank-\nings are analyzed by calculating the mean average preci-\nsion (MAP). To place the results in perspective, we cal-\nculate the edit distance [24] between all chord sequences,\nrepresented as a string of chord labels, as a baseline mea-\nsure.\nThe results are presented in Table 1. It seems that all\nCombined LLCES based methods perform better than the\nbaseline edit distance, but only the difference between the\nViol distance measure without chord symbol nodes scores\nsigniﬁcantly better than the baseline edit distance ( p < .01,\ntwo-tailed T-test). The results show therefore that the num -\nber of grammatical connections in the Combined LLCES\nis a good indicator for harmonic similarity. The lack of\nsigniﬁcance of the other measures might be explained by\nthe limited size of the relatively small dataset. However,\nthe experiment does show that a matching method that ana-\nlyzes the structure of the harmony outperforms a sequence-\nbased method that does not use any musical knowledge.\n6. CONCLUDING REMARKS\nThis paper introduced a new approach to harmonic simi-\nlarity. We showed that a grammar of tonal harmony can\n2http://give-lab.cs.uu.nl/music/\n553Oral Session 7: Harmonic & Melodic Similarity and Summarization\nChord Symbols No Chord Symbols\nDistance: Rel Viol Dep Rel Viol Dep Edit\nMAP: 0,79 0,81 0,72 0,81 0,86 0,73 0.67\nTable 1 . The MAP of the six Combined LLCES based\nsimilarity measures and a baseline edit distance.\nbe adapted in such a way that is usable for matching har-\nmony sequences. However, there are some open issues.\nAt the moment we cannot calculate distance measures to\npieces that do not parse and for every grammar there are\nalways pieces imaginable that do not parse. A solution to\nthis problem can be found in partial matching. Often only\none or two chords cannot be explained by the grammar. By\nremoving these chords and parse the left and the right side\nseparately, it is possible to obtain a parse tree that can be\nused for matching.\nA property of context free grammars is that sequences\ncan have multiple ambiguous parse trees. Using the gram-\nmar presented here, many chord sequences are intrinsically\nambiguous and have multiple derivations. One solution\nmight be to incorporate intrinsically ambiguous parse tree s\nin the creation of the Combined LLCES. Nevertheless, it\nis important to keep the number of unwanted ambiguous\nparse trees as low as possible. By making the grammar\nstrongly typed and adding weights to rules, we controlled\nthe number and the selection of parse trees. Still, the gram-\nmar as presented here features several problems with re-\nspect to the parsing of phrase boundaries, which consti-\ntutes a main source of ambiguities (as in Fig. 1). A set of\nadditional preference rules will be designed for future ver -\nsions of the model to rule out unlikely phrase-boundaries.\nThese will be based on metrical information which is not\nyet incorporated in the present model. Yet another way of\nimproving the expressive power of the grammar and limit-\ning the number of ambiguous parse trees at the same time,\nis to start parsing with a very strict grammar and, only af-\nter a rejection of the chord sequence, to add more loosely\ntyped rules that can explain the more exotic harmonic phe-\nnomena.\nThe research presented here demonstrates how a gram-\nmar of harmony may characterize harmonic similarity in a\nmusical way. This will have a large impact on the quality\nof the representation, analysis and retrieval of tonal musi c.\nThis research also provides a case study that demonstrates\nthe importance of cognitive and theoretic models of mu-\nsic in the design of appropriate methods for MIR tasks that\nhave been neglected so far because of their inherent musi-\ncal complexity.\n7. ACKNOWLEDGMENTS\nThis work was supported by the Dutch ICES/KIS III Bsik\nproject MultimediaN and in part by Microsoft Research\nthrough the European PhD Scholarship Programme.\n8. REFERENCES\n[1] W.B. de Haas, R.C. Veltkamp, and F. Wiering. Tonal Pitch\nStep Distance: A Similarity Measure for Chord Progressions .InProceedings of the 9th International Conference on Music\nInformation Retrieval , pages 51–56, 2008.\n[2] F. Lerdahl. Tonal Pitch Space . Oxford University Press, 2001.\n[3] I. Deli` ege, M. M´ elen, D. Stammers, and I. Cross. Musica l\nSchemata in Real Time Listening to a Piece of Music. Music\nPerception , 14(2):117–160, 1996.\n[4] E. Bigand. More About the Musical Expertise of Musically\nUntrained Listeners. Annals of the New York Academy of Sci-\nences , 999:304–312, 2003.\n[5] M. Rohrmeier. A Generative Grammar Approach to Dia-\ntonic Harmonic Structure. In Anagnostopoulou Georgaki,\nKouroupetroglou, editor, Proceedings of the 4th Sound and\nMusic Computing Conference , pages 97–100, 2007.\n[6] H. Schenker. Der Freie Satz. Neue musikalische Theorien und\nPhantasien, 1935.\n[7] F. Lerdahl and R. Jackendoff. A Generative Theory of Tonal\nMusic . MIT press, 1983.\n[8] A.D. Patel. Language, Music, Syntax and the Brain. Nature\nNeuroscience , 6:674–681, 2003.\n[9] N. Chomsky. Syntactic Structures . Mouton, 1957.\n[10] S. Kostka and D. Payne. Tonal Harmony with an Introduction\nto 20th-century Music . McGraw-Hill, 1984.\n[11] M. Baroni, S. Maguire, and W. Drabkin. The Concept of Mu-\nsical Grammar. Music Analysis , 2(2):175–208, 1983.\n[12] M. J. Steedman. A Generative Grammar for Jazz Chord Se-\nquences. Music Perception , 2(1):52–77, 1984.\n[13] M. J. Steedman. The Blues and the Abstract truth: Music\nand Mental Models , chapter 15, pages 305 – 318. Psychol-\nogy Press, 1996.\n[14] M. Chemillier. Toward a Formal Study of Jazz Chord Se-\nquences Generated by Steedmans Grammar. Soft Computing-\nA Fusion of Foundations, Methodologies and Applications ,\n8(9):617–622, 2004.\n[15] S. Tojo, Y . Oka, and M. Nishida. Analysis of Chord Progre s-\nsion by HPSG. In Proceedings of the 24th IASTED interna-\ntional conference on Artiﬁcial intelligence and applicati ons,\npages 305–310. ACTA Press Anaheim, CA, USA, 2006.\n[16] W. Piston. Harmony . Norton, W. W. & Company, New York,\n1948.\n[17] F. Pachet. Surprising Harmonies. International Journal of\nComputing Anticipatory Systems , 4, 1999.\n[18] H. Riemann. Vereinfachte Harmonielehre; oder, die Lehre\nvon den tonalen Funktionen der Akkorde . Augener, 1893.\n[19] S. Martin. Pep is an Earley Parser. http://www.ling.oh io-\nstate.edu/˜scott/, 2007.\n[20] J. Earley. An Efﬁcient Context-free Parsing Algorithm .Com-\nmunications of the ACM , 13(2):94–102, 1970.\n[21] P. Kilpel¨ ainen. Tree Matching Problems with Applications to\nStructured Text Databases . PhD thesis, Departement of Com-\nputer Science, University of Helsinki, November 1992.\n[22] A. Gupta and N. Nishimura. Finding Largest Subtrees and\nSmallest Supertrees. Algorithmica , 21(2):183–210, 1998.\n[23] Various Authors. The Real Book . Hal Leonard Corporation,\n6th edition, 2004.\n[24] V . I. Levenshtein. Binary Codes Capable of Correcting D ele-\ntions, Insertions, and Reversals. Cybernetics and Control\nTheory , 10(8):707–710, 1966.\n554"
    },
    {
        "title": "Interactive Gttm Analyzer.",
        "author": [
            "Masatoshi Hamanaka",
            "Satoshi Tojo"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415094",
        "url": "https://doi.org/10.5281/zenodo.1415094",
        "ee": "https://zenodo.org/records/1415094/files/HamanakaT09.pdf",
        "abstract": "We describe an interactive analyzer for the generative theory of tonal music (GTTM). Generally, a piece of music has more than one interpretation, and dealing with such ambiguity is one of the major problems when constructing a music analysis system. To solve this problem, we propose an interactive GTTM analyzer, called an automatic time-span tree analyzer (ATTA), with a GTTM manual editor. The ATTA has adjustable parameters that enable the analyzer to generate multiple analysis results. As the ATTA cannot output all the analysis results that correspond to all the interpretations of a piece of music, we designed a GTTM manual editor, which generates all the analysis results. Experimental results showed that our interactive GTTM analyzer outperformed the GTTM manual editor without an ATTA. Since we hope to contribute to the research of music analysis, we publicize our interactive GTTM analyzer and a dataset of three hundred pairs of a score and analysis results by musicologist on our website http://music.iit.tsukuba.ac.jp/hamanaka/gttm.htm, which is the largest database of analyzed results from the GTTM to date.",
        "zenodo_id": 1415094,
        "dblp_key": "conf/ismir/HamanakaT09",
        "keywords": [
            "interactive analyzer",
            "generative theory of tonal music",
            "ambiguous music interpretation",
            "music analysis system",
            "adjustable parameters",
            "multiple analysis results",
            "GTTM manual editor",
            "experimental results",
            "research contribution",
            "dataset"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nINTERACTIVE GTTM ANALYZER  \nMasatoshi Hamanak a Satoshi Tojo  \nUniversity of Tsukuba  \n \nhamanaka@iit.tsukuba.ac.jp  Japan Advanced Institute of  \nScience and Technology  \ntojo@jaist.ac.jp  \nABSTRACT  \nWe describe an interactive analyzer for the generative \ntheory of t onal music (GTTM). Generally, a piece of  mu-\nsic has more than one interpretation, and dealing with \nsuch ambiguity is one of the major problems when co n-\nstructing a music analysis sy stem. To solve this problem, \nwe propose an interactive GTTM analyzer, called an au-\ntomatic time -span tree an alyzer (ATTA), with a GTTM \nmanual editor. The ATTA has adjustable par ameters that \nenable the analyzer to generate multiple ana lysis results. \nAs the ATTA cannot output all the analysis results that \ncorrespond to all the interpr etations of a piece of music, \nwe designed a GTTM manual editor, which gen erates all \nthe analysis results. Experimental r esults showed that our \ninteractive GTTM analyzer outperformed the GTTM m a-\nnual editor without an ATTA. Since  we hope to contr i-\nbute to the  research of music analysis, we publicize our \ninteractive GTTM analyzer and a dataset of three hundred \npairs of a score and analysis results by musicologist  on \nour website http://music.iit.tsukuba.ac.jp/hamanaka/gttm.htm , \nwhich is the largest database of a nalyzed results from the \nGTTM to date.  \n1. INTRODUCTION  \nWe have been constructing a music analyzer based on \nthe g enerative theory of tonal music (GTTM) [1]. The \nGTTM is composed of four modules, each of which a s-\nsigns a separate structural description to a list ener’s u n-\nderstanding of a piece of music. These four modules \noutput a grouping structure, a metrical structure, a time -\nspan tree, and a prolongational tree. The main adva n-\ntage of implementing the GTTM on a computer is to ac-\nquire tree structures called time -span and pr olongational \ntrees from the surface structure of a piece of music. The \ntime-span and prolongational trees provide melody \nmorphing , which  generates an intermediate melody b e-\ntween two melodies with a systematic order [2]. It can \nalso be used for performance rendering [3 -5] and repr o-\nducing music [6]  and provides a su mmarization of the \nmusic . This summarization can be used as a represent a-tion of a search, result ing in music retrieval sy stems [7].  \nIn computer implementation of music theory [1, 8 -\n10], we have to consider two types of ambiguity in m u-\nsic analysis . One involves human understanding of mu-\nsic, and the other concerns the representation of music \ntheory. The former tolerates our subjective interpret a-\ntion, while the latter is caused by the incom pleteness of \nformal theory , and the GTTM is not an exception . \nTherefore, due to the former’s amb iguity, we assume \nthere is more than one correct result.   \nIn our previous work, we proposed the exGTTM \n(machine -executable  extension of GTTM) and co n-\nstructed a n automatic time -span tree analyzer (ATTA) \nto avoid the latter type of ambiguity , introducing as \nmany parameters as possible  [11, 12]. Whenever we \nfind a correct result that the exGTTM cannot generate, \nwe add new parameters with proper values to improve \nthe result. \nHowever, the ATTA has been clumsy for the first \ntype of ambiguity. Even an identical melody can be \nplayed in different ways to represent different feelings  \nsince the  ATTA cannot output the different analysis r e-\nsults in the same melod y repetition.  To solve this pro b-\nlem, we developed a GTTM manual editor that manua l-\nly alternate s the analysis results of the ATTA, according  \nto the user's interpretations of a piece of music.  \nHowever, t he ATTA still exhibits  problems concer n-\ning the latter type of ambigu ity. For example, the \nGTTM consists of feed -back operations  from hig her- to \nlower -level  in the tree structure ; however, no d etailed \ndescri ption and only a few examples are given. To solve \nthis problem, we developed a GTTM process editor , \nwhich enables seam less change of the automatic anal y-\nsis process with an ATTA and the manual edit process \nwith a GTTM manual editor. Therefore, a user can a c-\nquire the target analysis results by iterating the automa t-\nic and manual processes interactively and easily r eflect \nhis or her interpretations on a piece of m usic. \nThis paper is organized as follow s. We present an \noverview of our interactive GTTM analyzer, which co n-\nsists of the ATTA, GTTM manual editor, and GTTM \nprocess editor in Section 2, propose a m anual editing \nmethod of the GTTM manual editor in Section 3, pr o-\npose a process editing method of the GTTM process \neditor in Section 4, and present exp erimental results and \nconclusion s in Sections 5 and 6 , respectively . Finally, \nwe provide in the appendix the data format of the  ana-\nlyzing results of the GTTM, which we publicize along \nwith those of the i nteractive GTTM analyzer.   \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee prov ided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2009 International Society for Music Information Retrieval  \n291Poster Session 2\n  \n \n2. INTERACTIVE GTTM ANA LYZER  \nFigure 1 is a screenshot of the viewer of our intera ctive \nGTTM analyzer. There is a sequence of notes displayed \nin a piano roll  format. Below the notes is a grouping \nstructure, which is graphically presented as several levels \nof arcs. The grouping structure is intended to formalize \nthe intuitive belief that tonal music is organized into \ngroups that are in turn composed of subgroup s. Below \nthe grouping structure is a metrical structure. The metri c-\nal structure describes the rhythmical hierarchy of the \npiece by ident ifying the position of strong beats at the \nlevels of a quarter note, half note, one measure, two \nmeasures, four measures , and so on. Strong beats are ill u-\nstrated as several levels of bars. Above the notes, there is \na time -span tree. The time -span tree is a binary tree, \nwhich is a hierarchical structure describing the relative \nstructural importance of notes that differentiat e the esse n-\ntial parts of the melody from the o rnamentation. Below \nthe time -span tree is a prolongational tree, a binary tree \nthat expresses the structure of tension and relaxation in a \npiece of music.  \nFigure 2 is an overview of our interactive GTTM an a-\nlyzer, consisting of an ATTA,  a GTTM manual editor, \nand a GTTM process editor. The ATTA  consists of  a \ngrouping structure, a metrical structure, and time -span \ntree analyzer s. We have been developing a prolongatio n-\nal tree analyzer. Hamanaka  et al. explain the  details of \nthe ATTA [11].  \nThe GTTM manual editor  consists of  grouping, m e-\ntrical, time -span, prolongational, and Tonal Pitch Space \neditors. The Tonal Pitch Space  [12] is a music theory for \nchord progression composed by Lerdhal, who is one of \nthe author s of the GTTM. Although  the GTTM includes \nrules that require  the analysis  results of chord progre ssion, the ATTA uses such rules by adopt ing the r esults of the \nTonal Pitch Space.  \nThe analyzing process with the ATTA and GTTM m a-\nnual editor is complicated, and so metimes a user is co n-\nfused as to what he or she should do in the next process , as \nthere are three analyzing processes in the ATTA and five \nediting processes in the GTTM manual editor. A user may \niterate the ATTA  and manual edit process es multiple  times. \nTo solve this problem, we propose a GTTM process editor, \nwhich presents candidates for the next process of analysis, \nand a user  only needs to change the process , just by selec t-\ning the next process.  \nWe use an XML format for all the input and output d a-\nta stru ctures of our interactive GTTM analyzer. Each an a-\nlyzer and editor of our analyzer works independently, but \nthey are integrated with the XML -based data stru cture. \n3. GTTM MANUAL EDITOR  \nIn some cases, the ATTA may produce a preferable result , \nwhich reflects the  user’s interpretation, but in other case it \nmay not. When  a user wants to change the analysis result \naccording to his  or her interpretation, he or she can use \nthe GTTM manual editor. We describe the method for \nediting and constructing a musical structure of the GTTM \nusing the GTTM m anual editor.  \nFigure  1. Screenshot of interactive GTTM analy zer. Grouping  \nStructure \nMetrical   \nStructure Prolongational  \nTree \nTime -span  \nTree Figure  2. Overview of interactive GTTM analyzer.  \nGrouping structure analyzer\nMetrical structure analyzer\nTime -span tree analyzer\nProlongational tree analyzerGrouping structure editor\nMetrical structure editor\nTime -span tree editor\nProlongational tree editor\nTonal Pitch Space editorATTA GTTM manual editor GTTM process editor\n29210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \n3.1 Grouping structure editor  \nFigure 3 is a screenshot of our interactive GTTM an alyzer \nin editing a grouping structure. The color of the target  \ngroup and all its subgroups turn red after selecti on with a \nmouse. Then we can open a popup menu by right clic king \nthe mouse. There are four operations in the popup menu , \ndivide this group and create subgroup , divide this group , \ndelete , and delete descendant . \n To change a position of a grouping boundary, a user \nfirst delete th e groups which adjoin the boundary then d i-\nvide the upper level (global level) group and create new \nsubgroups where he or she wants to create a boundary. By \nleft clicking a grouping boundary, the user see s the rules \nthat are applied  to the boundary and he o r she can add or \ndelete these rules.  \n3.2 Metrical structure editor  \nAlthough  the metrical structure analyzer in the ATTA \nperforms fairly well [11], a user may want  to slightly edit \nthe me trical structure . In which case, he or she applies the \nmetrical structure editor , and  change s the strength le vel of \na beat by dra gging a bar up or down. At the same time , he \nor she see s the rules that are a pplied to the bar and can \nadd or delete these rules.  \nWhile editing beat strength, a user may break hierarchi c-\nal metrical str uctures. In other words, the results of the m e-\ntrical structure editor sometimes do not hold for the metri c-\nal preference rules. This problem can be solved using the \nGTTM process editor , which we discuss  in Se ction 4.  \n3.3 Time -span tree ed itor \nIn the time -span t ree, each branch has a head repr e-\nsented by a square in the time -span tree editor, and a user \ncan move the head by dra gging another branch. Figure 4 \nis a screenshot of dragging a head. The light blue branch \nis the former position, and the dark blue branch i s the la t-\nter position. A user can select a type for each head by \nopening the popup menu among th ose four types,  ordi-\nnary, fusion , transformation , and cadential retention .   \n3.4 Prolong ational tree editor  \nThe process  of the prolongational tree editor is the sam e \nas th at for the time-span tree. The prolongational tree is \nconstructed by reconnecting the heads based on the time -\nspan tree. There are head connection  constraints  of the prolongational tree. When a head connection of a prolo n-\ngational tree is ill  -formed , the GTTM process editor a u-\ntomatically opens the popup menu and displays  cand i-\ndates for a sol ution. \n3.5 Tonal Pitch Space ed itor \nThe reason we include a Tonal Pitch Space editor in our \ninteractive GTTM analyzer is that the editor provides qua n-\ntitative ground s for the prolongational tree to be hierarchi c-\nal. Therefore, analyzing the Tonal Pitch Space with the pr o-\nlongational tree improves analyzing perfo rmance.  \n4. GTTM PROCESS EDITOR  \nThere are two types of rules in the GTTM , which are  \nwell-formedness  and preference . Well -formedness rules \nare necessary conditions for the  rules  assignment of a \nstructure as well as the restrictions on the structure. \nWhen more than one structure satisfies the well -\nformedness rules, the preference rules indicate the sup e-\nriority of one st ructure over another.   \nIn the GTTM, the analysis sequence proceeds  from the \ngrouping structure, secondly to the metrical structure, next to \nthe time-span tree, and finally to the prolongational tree. \nHowever, the GTTM contains  feedback links from hig her- to \nlower -level structures. For example, grouping preference \nrule 7 (GPR7) (time -span and prolongational stability) pr e-\nfers a grouping structure that results in a more stable time -\nspan and/or prolongation reduction. Therefore, to analyze \nwith feedback link r ules, we need to perform several analy z-\ning processes by trial and error. The GTTM process editor \nhelps in this repetition  by performing three functions , data \ninputting, history recording, and process co ntrolling.  \n4.1 Data  inputting  \nData inputting helps  with the input of analysis results, \nwhich are prepared by another user or analyzer. For e x-\nample, we do not have an automatic analyzer for the Ton-\nal Pitch Space in our interactive GTTM analyzer; howe v-\ner, attempts have been made to implement the Tonal Pitch \nSpace, so we can use those results [13].  \nWe can add new rules to the ATTA using data inpu t-\nting. For example, grouping preference rule 6 (GPR6) is a \nrule for parallelism in a grouping structure; however, the \nGTTM does not define the decision criteria for co nstruing \nFigure  4. Screensho t of when dragging head.  \nFigure  3. Screenshot of grouping structure editor.  \n293Poster Session 2\n  \n \nwhether two or more segments are parallel or not. Ther e-\nfore, many implementations of GPR6 would be possible , \nalthough we propose only one of them . By ad ding a new \nrule to the ATTA, we can control a new adjustable par a-\nmeter for  the new rule, GPR6+, which  is the new impl e-\nmentation of GPR6.  \n4.2 History recording  \nHistory recording records the operation of analysis, and a \nuser can return to the previou s phase of analysis. History \nrecording enables the copying and pasting  of several o p-\nerations of analysis while  editing parallel phrases.  \nIn the GTTM, there are few descriptions of the reasoning \nand working algorithms needed to compute the analysis \nresults, especially for the time -span and prolongational \ntrees. By using history recording, we look forward to sto r-\ning the analysis knowledge , which  improves automatic \nanalysis.  \n4.3 Process controll ing \nProcess controlling enables seamless change of the anal y-\nsis process by using the ATTA and the manual edit \nprocess by using the GTTM manual editor , representing \ncandidates for the  next process of analysis. The represe n-\ntation method differs depending on the number of cand i-\ndates for the next process.  \n4.3.1 One candidate  \nWhen there is only one candidate process, the process -\ncontrolling function automatically executes the process. \nFor examp le, when a user edits the strongest beat in Fi g-\nure 5a in the 2nd level, the hierarchical metrical stru cture \nis broken because in level 3 of Figure 5b there are three \nweak continuous beats, and the metrical well -formedness \nrule 2 (MWFW2) does not hold. MWFR 2 requires that \nstrong beats are spaced either two of three beats apart  at \neach metrical level . The process editor automatically al-\nternately produces strong and weak beats in level 3 (Fig-\nure 5c). If there is a higher metrical structure than level 3, \nthe me trical analyzer of the ATTA a utomatically analyzes \nafter level 3 and constructs a hierarchical metrical stru c-\nture reflecting the user’s intention. \n4.3.2 A few candidates  \nWhen there ar e a few candidates, the process  controlling \nfunction automatically opens the po pup menu and shows \nthe candidates. For example, if there is a gro uping stru c-ture, as shown Figure 6a , and a user deletes a group at the \nupper left (Figure 6b) , the grouping structure of Fi gure 6b \nis broken since  grouping well -formedness rule 3 \n(GWFR3) does  not hold. GWFR3 requires constraints  \nthat a group may contain smaller groups. To solve this \nproblem, there are only two processes:  \n- Delete all the groups at the same level of the d eleted \ngroup (Figure 6c).  \n- Extend the grouping boundary of the left end o f the \nright group of the deleted group to the left end of th at \ndeleted group (Figure 6d).  \nThe next process can be executed by selecting one of the \ntwo processes displayed  in the popup menu.  \n \n4.3.3 Many candidates  \nWhen there are many candidates, the process -controlling \nfunction selects and shows the top-ten candidates from the \nhistory recording. The candidates are ordered depending \non the similarity of the history. For example, after editing \na time -span tree with the time -span tree edi tor, executing \na grouping analyzer or metrical analyzer in the ATTA is \nranked in the upper levels because there are rules for \nfeedback link such as GPR7 or metrical preference rule 9 \n(MPR 9). GPR7 (time -span and prolongational st ability) is \na link from the time-span and prolongational trees to the \ngrouping structure, and MPR9 (time -span i nteraction) is a \nlink from the time -span tree to the metrical structure.  \nWe have not  implement ed the original ATTA on \nGPR7 and MPR9. In this paper, we omit the details of th e \nimplementation of these rules due to  space  limitations.  \n5. EXPERIMENTAL RESULTS  \nWe asked a musicolog ist expert to manually an alyze the \nscore data faithfully with regard to the GTTM using our \ninteractive GTTM analyzer. The musicologist collected \nthree hundre d 8-bar-long, monophonic, classical music \npieces including notes, rests, slurs, accents, and articul a-\ntions entered manually with music notation software \ncalled Finale  [14]. The musicologist needed ten to twenty \nminutes for analyzing a piece. Three other ex perts \ncrosschecked these results.  \nWe measured the operating time for acquiring the ta r-\nget analysis results of our interactive GTTM analyzer and \ncompared it with that of the GTTM manual editor wit hout \nan ATTA. For the target analysis, we used one hundred \npieces from the three hu ndred pairs of scores and correct \ndata of grouping structure, metrical structure, and time -\nspan tree. We did not use the prolo ngational tree in this Figure  6. Two types of solutions for broken grouping \nstructure. \n(a) Original structure (b) Structure broken\nby user editing(c) Solution 1\n(d) Solution 2\n: Deleted group\nFigure  5. Automatically correct broken metrical stru cture. \n(a) Original structure (b) Structure broken \nby user editing(c) Automatically \nsolve using \nprocess controller  Strongest \nbeetUser editinglevel\n1234\nThree weak continuous beat\n29410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nmeasurement since  its analyzer is still under  develo pment . \nAs a result, our interacti ve GTTM an alyzer outperformed \nthe GTTM manual editor without an ATTA (Table 1).   \n \nMelodies  Interactive GTTM an a-\nlyzer  GTTM manual \neditor \n1 Grande Valse Bri llante  326 sec  624 sec  \n2. Moments Mus icaux  541 sec  791 sec.  \n3. Turkish  March  724 sec  1026 sec  \n4. Anitras Tanz  621 sec  915 sec.  \n5. Valse du Petit Chien  876sec.  1246 sec.  \n : : \nTotal (100 mel odies)  575 sec.  891 sec.  \nTable 1. Operation time of interactive GTTM analyzer \nand GTTM manual editor.  \n6. CONCLUSION  \nWe developed a music ana lyzer called  the interactive \nGTTM analyzer, which derives the grouping stru cture, \nmetrical structure, time -span tree, and prolongational tree \nof the GTTM. The analyzer also derives analysis results \nof chord progression based on the Tonal Pitch Space. The \nanalyzer consists of an automatic GTTM an alyzer, called \nan ATTA, a GTTM manual editor, and a GTTM process \neditor. By using the process editor, a user can seamlessly \nchange the analysis process of the ATTA and that of the \nmanual editor. The experimental resu lts show that our i n-\nteractive GTTM analyzer outperformed the GTTM m a-\nnual editor without an ATTA.  \nSince the original grouping rules  of GTTM are based \non monophonic melodies, we have implemented our  sys-\ntem faithfully observing the theory. In the future, ho wever, \nwe plan to include harmonic analysis to compl ement the \noriginal  theory and to target homophonic music.  \n7. REFERENCES  \n[1] Lerdahl, F., and R. Jackendoff. A Generative Theory \nof Tonal Music . MIT Press, Cambridge, \nMassachusetts, 1983.  \n[2] Hamanaka , M., Hirata , K., and Tojo , S.: ''Melody \nmorphing method based on GTTM ', Proceedings of \nthe 200 8 International Computer Music conference  \n(ICMC200 8), pp. 155-158, 200 8. \n[3] Todd, N. ''A Model of Expressive Timing in Tonal \nMusic''. Musical Perception , 3:1, 33 -58, 1985.  \n[4] Widmer, G.  ''Understanding and Learning Musical \nExpression'', Proceedings of 1993 International \nComputer Music Conference  (ICMC1993) , pp. 268 -\n275, 1993.  \n[5] Hirata, K., and Hiraga, R. ''Ha -Hi-Hun plays \nChopin’s Etude'', Working Notes of IJCAI -03 \nWorkshop on Methods for Automatic Music \nPerformance and their Applications in a Public  Rendering Contest , pp. 72 -73, 2003.  \n[6] Hirata, K., and Matsuda, S. ''Annotated Music for \nRetrieval, Reproduction, and Sharing'', Proceedings \nof 2004 International Computer Music Conference  \n(ICMC2 004), pp. 584 -587, 2004.  \n[7] Hirata, K., and Matsuda, S. ''Interactive Music \nSummarization based on Generative Theory of \nTonal Music''. Journal of New Music Research , 32:2, \n165-177, 2003.  \n[8] Cooper, G., and Meyer, L. B. The Rhythmic \nStructure of Music . The Univer sity of Chicago Press, \n1960.   \n[9] Narmour, E. The Analysis and Cognition of Basic \nMelodic Structure . The University of Chicago Press, \n1990.  \n[10] Temperley, D. The Congnition of Basic Musical \nStructures . MIT press, Cambridge, 2001.  \n[11] Hamanaka , M., Hirata , K., and Tojo , S. \n''Implementing ‘A Generative Theory of Tonal \nMusic’'', Journal of New Music Research , 35:4, 249 -\n277, 2006.  \n[12] Lerdahl, F . Tonal Pitch Space , Oxford University \nPress , 2001 . \n[13] Sakamoto, S., and Tojo, S.: '' Harmony Analysis of \nMusic in Tonal Pitch Space '', Information \nProcessing Society of Japan SIG Technical Report, \nVol. 200 9, May 2009 (in Japanese) . \n[14] PG Music Inc .: Finale, Available online at: \nhttp://www.pgmusic.com/ finale.htm,  2009.  \n[15] Recordare, LLC.: MusicXML 2.0 Tutorial,  \nAvailable online at \nhttp://www.rec ordare.com/xml/musicxml -\ntutorial.pdf , 2009.  \n[16] Recordare, LLC.: Dolet 4 for Finale,  Available \nonline at \nhttp://www.recordare.com/finale/index.html, 2009.  \n[17] W3C. XML Pointer Language (XPointer). \nhttp://www.w3.org/TR/xptr/, 2002.  \n[18] W3C. XML Linking Language (XLink ) Version 1.0. \nhttp://www.w3.org/TR/xlink/, 2001.  \nAPPENDIX: PUBLICLY A ND DATA FORMAT  \nWe publicize our interactive GTTM analyzer and \ndatabase of three hundred pairs of scores and correct data \nat the following URL.  \nhttp:// music.iit.tsukuba.ac.jp /hamanaka /gttm.htm \nWe believe that the exhibition of this kind of resource \nis important for the music information -researching \ncommunity. The interactive GTTM analyzer is the first \napplication for acquiring time -span trees and \n295Poster Session 2\n  \n \n<ts timespan=\"3.0\" leftend=\"0.0\" rightend=\"3.0\">  \n  <head> <note id=\"P1 -1-4\" /> </head>  \n  <primary>  \n    <ts timespan=\"1.0\" leften d=\"2.0\" rightend=\"3.0\">  \n      <head> <note id=\"P1 -1-4\" /> </head>  \n    </ts>  \n  </primary>  \n  <secondary>  \n    <ts timespan=\"2.0\" leftend=\"0.0\" rightend=\"2.0\">  \n      <head> <note id=\"P1 -1-1\" /> </head>  \n      <primary>  \n        <ts timespan=\"1.0\" leftend=\"0.0\" r ightend=\"1.0\">  \n          <head> <note id=\"P1 -1-1\" /> </head>  \n        </ts>  \n      </primary>  \n      <secondary>  \n        <ts timespan=\"1.0\" leftend=\"1.0\" rightend=\"2.0\">  \n          <head> <note id=\"P1 -1-2\" /> </head>  \n          <primary>  \n            <ts timespa n=\"0.5\" leftend=\"1.0\" righ tend=\"1.5\">  \n              <head> <note id=\"P1 -1-2\" /> </head>  \n            </ts>  \n          </primary>  \n          <secondary>  \n            <ts timespan=\"0.5\" leftend=\"1.5\" righ tend=\"2.0\">  \n              <head> <note id=\"P1 -1-3\" /> </he ad> \n            </ts>  \n          </secondary>  \n        </ts>  \n      </secondary>  \n    </ts>  \n  </secondary>  \n</ts>  -<group>  \n -<group>  \n+<note id=\"P 1-1-1\"/> \n+<note id=\"P1 -1-2\"/> \n+<note id=\"P1 -1-3\"/> \n+<note id=\"P1 -1-4\"/> \n </group>  \n <applied  rule= ”2a”/> \n<applied  rule= ”3a”/> \n<applied  rule= ”6”/> \n -<group>  \n             \n \n </group>  \n</group>  <metric dot=\"6\" at=\"0.0\">  \n<applied level=\"0.5\" rule=\"3\"/>  \n<applied level=\"0.5\" rule=\"5c\"/>  \n<applied level=\"1.0\" rule=\"3\"/>  \n<applied level=\"1.0\" rule=\"5c\"/>  \n<applied level=\"3.0\" rule=\"1\"/>  \n<applied level=\"3.0\" rule=\"3\"/>  \n<applied level=\"3.0\" rule=\"5c\"/>  \n<applied level=\"6.0\" rule=\"3\"/>  \n+<note id=\"P1 -1-1\"/> \n</metric>  \n<metric dot=\"1\" at= \"0.5\"/>  \n<metric dot=\"2\" at=\"1.0\">  \n<applied level=\"0.5\" rule=\"3\"/>  \n<applied level=\"1.0\" rule=\"3\"/>  \n+<note id=\"P1 -1-2\"/> \n</metric>  \n<metric dot=\"1\" at=\"1.5\">  \n<applied level=\"0.5\" rule=\"3\"/>  \n+<note id=\"P1 -1-3\"/> \n</metric>  \n<metric dot=\"2\" at=\"2.0\">  \n<applied lev el=\"0.5\" rule=\"3\"/>  \n<applied level=\"1.0\" rule=\"3\"/>  \n+<note id=\"P1 -1-4\"/> \n</metric>  \n<metric dot=\"1\" at=\"2.5\"/>  (a) Gro upingXML  (b) Metr icalXML  (c) Time -spanXML  \n3,5c \n3,5c \n1, 3,5c  \n3 3 \n3 3 3 \n3 2a \n3a \n6 \nFigure  7. GroupingXML, MetricalXML, and Time -spanXML  prolongational trees. We hope to benchmark  the analyzer \nto other systems, which will be constructed.  \nWe use the XML as the  import and export format \nsince  the XML format is extremely qualified to express \nhierarchical musical structures.  \nMusicXML  \nAs a primary input format, we chose MusicXML [1 5] \nbecause it provides a common ‘interlingua’ for music \nnotation, analysis, retrieval, and other applications. For \nexporting MusicXML from finale we use a plug -in called \nDolet  [16]. \nGroupingXML  \nWe designed Grouping.XML as an import and export \nformat for hierarc hical grouping structures. The \nGroupingXML has group , note, and applied  elements. All \nnote elements are inside hierarchical group  elements. The \napplied  elements are located between the end of a group \ntag and the start of the next group tag, which is where the \ngrouping preference rules (GPRs) are applied. Figure 7 a \nshows a simple example of GroupingXML.  \nMetricalXML  \nWe designed MetricalXML as an import and export \nformat for metrical structures. MetricalXML has metric  \nelements, which require a dot attribute, an at attribute, \nand applied  elements. The dot attribute indicates the \nstrength of each beat.  The at attribute indicates the time \nfrom the start of the piece. The applied  element requires a \nlevel  attribute and a rule attribute.  In the metrical \nstructure analysis, metrical preference rules (MPRs) are applied to each hierarchy of a dot. The  level  attribute \nindicates the interval of dots. If there is an onset of a note \nat the beat position, the note element is inserted before the \nend of the metric element (F igure 7b) \nTime -spanXML, ProlongationalXML  \nThe Time -spanXML has ts, primary , and secondary \nelements. The ts element has a time-span  attribute, a \nleftend  attribute, and a rightend  attribute. Therefore, the ts \nelement indicates the length and position of the time-span \nin a piece of music. In the  ts element, there is a head  \nelement , which  requires a note element indicating the \nmost salient note in the time -span tree. If there is more \nthan one note in the time -span, we can divide the time -\nspan in two parts. One includes the head , and  the other \ndoes not. The primary  element in the ts element has a \nnext-level ts element that corresponds to the time -span, \nwhich includes the upper level head. The secondary  \nelement in the ts element has a next -level ts element that \ncorresponds to the time -span, which does not include the \nupper level head (Figure 7c).  \nWe do not expla in ProlongationalXML because its \nstructure is similar to that of the time -span tree.  \nTonal Pitch Spac eXML  \nThe Tonal Pitch SpaceXML has region  elements. Ins ide \nthe region  elements there are chord  elements , and  inside \nthe chord element there are note elements.  \nNote that note elements in GroupingXML, \nMetricalXML, Time -spanXML, ProlongationalXML, and \nTonal Pitch Space -XML are connected to note elements \nin Music XML using Xpointer [1 7] and Xlink [1 8]. \n296"
    },
    {
        "title": "Automatic Identification of Instrument Classes in Polyphonic and Poly-Instrument Audio.",
        "author": [
            "Philippe Hamel",
            "Sean Wood",
            "Douglas Eck"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415092",
        "url": "https://doi.org/10.5281/zenodo.1415092",
        "ee": "https://zenodo.org/records/1415092/files/HamelWE09.pdf",
        "abstract": "We present and compare several models for automatic identification of instrument classes in polyphonic and poly-instrument audio. The goal is to be able to identify which categories of instrument (Strings, Woodwind, Guitar, Piano, etc.) are present in a given audio example. We use a machine learning approach to solve this task. We constructed a system to generate a large database of musically relevant poly-instrument audio. Our database is generated from hundreds of instruments classified in 7 categories. Musical audio examples are generated by mixing multi-track MIDI files with thousands of instrument combinations. We compare three different classifiers : a Support Vector Machine (SVM), a Multilayer Perceptron (MLP) and a Deep Belief Network (DBN). We show that the DBN tends to outperform both the SVM and the MLP in most cases.",
        "zenodo_id": 1415092,
        "dblp_key": "conf/ismir/HamelWE09",
        "keywords": [
            "automatic identification",
            "instrument classes",
            "polyphonic audio",
            "poly-instrument audio",
            "machine learning approach",
            "large database",
            "musically relevant",
            "poly-instrument audio",
            "classifier comparison",
            "Deep Belief Network"
        ],
        "content": "AUTOMATIC IDENTIFICATION OF INSTRUMENT CLASSES IN\nPOLYPHONIC AND POLY-INSTRUMENT AUDIO\nPhilippe Hamel, Sean Wood and Douglas Eck\nD´epartement d’informatique et de recherche op ´erationnelle, Universit ´e de Montr ´eal\nCIRMMT\nfhamelphi,woodsean,eckdoug g@iro.umontreal.ca\nABSTRACT\nWe present and compare several models for automa-\ntic identiﬁcation of instrument classes in polyphonic and\npoly-instrument audio. The goal is to be able to identify\nwhich categories of instrument (Strings, Woodwind, Gui-\ntar, Piano, etc.) are present in a given audio example. We\nuse a machine learning approach to solve this task. We\nconstructed a system to generate a large database of mu-\nsically relevant poly-instrument audio. Our database is ge-\nnerated from hundreds of instruments classiﬁed in 7 cate-\ngories. Musical audio examples are generated by mixing\nmulti-track MIDI ﬁles with thousands of instrument com-\nbinations. We compare three different classiﬁers : a Sup-\nport Vector Machine (SVM), a Multilayer Perceptron (MLP)\nand a Deep Belief Network (DBN). We show that the DBN\ntends to outperform both the SVM and the MLP in most\ncases.\n1. INTRODUCTION\nThanks in part to the vast amount of music available on-\nline, much research has been done on the automatic extrac-\ntion of descriptors for music audio, such as genre, artist,\nmood and instrumentation. Because the majority of this re-\nsearch has focused on commercial recorded music, where\nground truth is lacking, relatively little work has been done\nin identifying which instruments are playing in music au-\ndio. Solving this problem would give rise to better descrip-\ntion of commercial audio collections. It could also form a\npart of a system able to synthesize music with timbres that\nmatch the instruments found in a particular audio ﬁle (e.g.\n“generate music that sound like this Sex Pistols mp3”).\nSuch a system may be useful in applications such as user-\ncontent-guided video game music generation.\nIn this paper, our focus is on constructing a model able\nto determine which classes of musical instrument are present\nin a given musical audio example, without access to any\ninformation other than the audio itself. In order to obtain\nsufﬁcient labeled training examples for good generaliza-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc\r2009 International Society for Music Information Retrieval.tion, we generated our own database of audio. Our goal\nwas to have enough variability in the set of instruments so\nas to allow us to generalize to instruments not used in the\ntraining set. An overview of our system is illustrated in Fi-\ngure 1.\nFigure 1 . Overview of our automatic instrument class re-\ncognizer model\nWe compared three different classiﬁers to solve this task :\na Support Vector Machine (SVM), a Multilayer Perceptron\n(MLP) and a Deep Belief Network (DBN).\nThe main contribution of this paper is the introduction\nof the DBN model to the instrument recognition task. Until\nrecently, deep neural networks (i.e. networks having many\nhidden layers) were not used in practice because they are\nhard to train using random initialization and gradient des-\ncent alone. Recent developments have made training such\nnetworks possible [3, 15]. DBNs have since shown poten-\ntial in many ﬁelds such as image and speech recognition.Deep networks aim at learning higher-level features at\neach layer from the features of the layer below. Learning\nsuch high-level features allows a model to construct an abs-\ntract representation of the inputs. This is similar to how the\nhuman brain transforms raw sensory inputs to abstract fea-\ntures. An in-depth description and justiﬁcation of the use\nof deep architectures for learning can be found in [2].\nThe paper is organized as follows : In Section 2, we\ndescribe previous research in the domain of instrument re-\ncognition. In Section 3 we describe the system used to ge-\nnerate our audio database. In Section 4, we discuss the fea-\ntures extracted from the audio. In Section 5, we describe\nin detail the three classiﬁcation models we employed. We\nthen discuss our results in Section 6.\n2. PREVIOUS WORK\nThe problem of automatic instrument classiﬁcation has\nbeen tackled from several different angles in the past de-\ncades. Psycho-acoustic studies have been conducted to build\n“timbre spaces” in which the distance between two sounds\nrepresents their degree of similarity [24, 30]. From the to-\npology of these spaces, we can outline some important fea-\ntures of the sounds that are important when studying timbre\n(e.g. spectral centroid, spectral ﬂux, etc.). A lot of work on\ninstrument recognition has been done on isolated instru-\nment sounds and monophonic audio [1,8–11,17,22,23,27].\nAn overview of previous approaches to automatic instru-\nment classiﬁcation is described in [13]. Recent work deals\nwith more complex and musically relevant sounds such as\nduets and polyphonies [6,7,16,18–20]. There has also been\nmuch work done on the related task of predicting genre-\nand instrument-related tags from audio [4, 21, 26] for mu-\nsic recommentation.\nIn a polyphonic context, notes are not easily separable.\nPitch tracking and source separation techniques can be use-\nful to address this problem, but these are still unsolved pro-\nblems in polyphonic audio that attracts a lot of research\nactivity. The problem of instrument recognition becomes\neven more complex when we consider multi-instrument\naudio. Many different machine learning models have been\ntried to solve this task. In [6], missing feature theory and\na Gaussian-mixture model (GMM) classiﬁer was used to\nidentify instruments in monophonic and polyphonic audio.\nIn [20], a process using linear discriminant analysis (LDA)\nfor instrument recognition for solo and duet performances\nis presented. A Support Vector Machine (SVM) and a hie-\nrarchical classiﬁcation scheme are used on polyphonic mu-\nsic in [7]. [16] presents a classiﬁcation model using LDA\nand feature weighting using polyphonic audio and musi-\ncal context information. In [18], instrument recognition in\npolyphonic audio is done by applying a post-process on\na mid-level harmonic atoms representation. [19] compares\nthe performance of three classiﬁers : SVMs, Extra Trees\nand K-Nearest Neighbors.\nUnfortunately, the relative performances of these dif-\nferent approaches are difﬁcult to measure. Since each re-\nsearch team uses a different test database and a different\nclassiﬁcation taxonomy, it would be unfair to compare thereported classiﬁcation accuracies.We take a small step to-\nwards addressing this by publishing our entire instrument\ndatabase. See [12].\nMost previous work on instrument recognition in poly-\nphonic audio has focused on recognizing speciﬁc instru-\nments from a small set of instruments. These models do\nnot attempt to deal with instruments they have never heard\nbefore. In this paper we address this limitation by intro-\nducing a model capable of recognizing classes of instru-\nments instead of speciﬁc ones. We argue that this behavior\nis better suited to large, rapidly-evolving commercial audio\ndatabases.\n3. DATABASE GENERATION\nTo solve a difﬁcult task such as instrument class recog-\nnition in poly-instrument audio, we require a large data-\nbase with a lot of variability in the data. To address this\nchallenge, we constructed our own database with a wide\nrange of sampled and synthesized instruments. Using MIDI\nﬁles to control our instruments, we were able to easily ge-\nnerate musically plausible examples with a wide range of\nvelocities, harmonization, and note lengths. We believe this\nwill help our classiﬁer to better generalize.\n3.1 Instrument bank\nWe used the instrument sounds from the commercial\nsampler “Kontakt 3” from “Native Instruments” to gene-\nrate our database. The advantage of using a sampler ins-\ntead of banks of isolated sounds or recorded performances\nis that we can generate musically relevant audio ﬁles from\nany MIDI ﬁle. We selected 172 different physical instru-\nments. For 23 of these physical instruments, we treated dif-\nferent dynamics as individual instruments for a total of 320\ninstruments. We separated our instruments into 7 classes :\nPiano, Guitar, Bass, Organ, Woodwind, Brass and Strings.\nEach class contained from 9 to 94 instruments. To test for\ngeneralization we divided our instrument bank into three\nindependent sets : 50% of the instruments were placed in a\ntraining set, 20% in a validation set and the remaining 30%\nin a test set.\n3.2 Audio Generation\nWe built a system to automatically generate a large quan-\ntity of audio ﬁles using MIDI ﬁles and a bank of instru-\nment samples. We generated two audio corpuses using solo\ninstrument and poly-instrument MIDI ﬁles. We composed\nand pre-mixed six to seven 30-second midi ﬁles per expe-\nriment. The ﬁrst corpus was obtained by generating audio\nfrom the solo instrument MIDI ﬁles, while the second was\ngenerated with multi-track MIDI ﬁles. We separated the\nMIDI ﬁles into individual tracks, each ﬁle having between\ntwo and six tracks. For each track, we randomly selected an\ninstrument with a compatible range. By “compatible ran-\nge”, we mean that the chosen instrument has a sample set\nwith a wide enough range (e.g. C4–C6) to actually play\nall of the notes in the ﬁle. We then mixed the tracks, ma-\nking sure that all instruments in a mix were from the samedata set (train, valid or test). We generated audio from each\nMIDI ﬁle with hundreds of different instruments mixes.\nExamples of our generated audio and corresponding mo-\ndel predictions are available at the website :\nhttp://www.iro.umontreal.ca/ ˜gamme/ismir_\n2009/\n4. FEATURE EXTRACTION\nThe selection of features is a crucial aspect of any ins-\ntrument classiﬁer. One of the most widely used features for\ntimbre analysis is the Mel-Frequency Cepstral Coefﬁcients\n(MFCCs). We used the 20 ﬁrst MFCCs as well as their\nﬁrst and second derivatives (dMFCCs and ddMFCCs). The\nMFCCs were calculated on 32 ms windows with a window\nstep size of 10 ms.\nWe also used a set of spectral features : centroid, spread,\nskewness, kurtosis, decrease, slope, ﬂux and roll-off. The\nmathematical deﬁnitions of these features are described in\n[25].\nWe divided the audio ﬁles into 1 second frames, and cal-\nculated the mean and the standard deviation of each feature\nfor each frame, yielding two values for each feature. In to-\ntal, the feature vectors contain 136 values : 40 MFCCS, 40\ndMFCCs, 40 ddMFCCs and 16 spectral features.\n5. MODELS\nWe tested three different classiﬁers : a Mulitlayer Per-\nceptron (MLP), a Support Vector Machine (SVM) and a\nDeep Belief Network (DBN).\n5.1 Multilayer Perceptron\nThe ﬁrst model is a single hidden layer feed-forward\nneural network, also know as Multilayer Perceptron. An\nadvantage of such models is that they are very fast to use,\nonce trained, making them good candidates for a real-time\napplication. We used the neural network implementation\nfrom the publicly available PLearn library [28].We used a\ntanh activation function for the hidden layer, and a logis-\ntic sigmoid function for the output layer. We used cross-\nentropy as the cost function to optimize. To avoid over-\nﬁtting, we used an L2 norm regularization on the weights\nas well as an early stopping condition. We also used conju-\ngate gradient descent to accelerate training.\n5.2 Support Vector Machine\nWe also tested a Support Vector Machine (SVM) with\na radial basis kernel. SVMs are widely used large margin\nclassiﬁers. The implementation of a SVM is quite com-\nplex, but publicly available ready-to-use libraries make them\nrather simple to use [5]. SVMs have been used for the\ntask of instrument recognition with a good degree of suc-\ncess [19]. SVMs have the advantage of having fewer hyper-\nparameters to optimize than neural networks. We used cross-\nvalidation to optimize the hyper-parameters.5.3 Deep Belief Network\nA deep network is constructed by superposing many\nlayers of neurons. It is essentially an MLP with many hid-\nden layers. The main difference comes from the initiali-\nzation of the weights of the connections between neurons.\nIn the single hidden layer case, a random initialization is\ngenerally sufﬁcient for the gradient descent to work. Ho-\nwever, with random initialization on many hidden layers,\nthe solutions obtained appear to correspond to poor solu-\ntions that perform worse than the solutions obtained for\nnetworks with 1 or 2 hidden layers [2, 3]. To circumvent\nthis problem, the DBN learning procedure consists of a\ngreedy layer-wise unsupervised pre-training phase, follo-\nwed by a supervised gradient descent ﬁne-tuning phase.\nThe pre-training phase conﬁgures the network such that it\nmay efﬁciently represent the input data. The pre-training\nphase is typically done with layers of Restricted Boltz-\nmann Machines (RBMs) [15] or autoencoders [14, 29]. In\nthis work, we used RBMs. RBMs are constituted of two\nlayers of neurons : a visible layer and a hidden layer. Each\nneuron is connected to every neuron of the other layer, but\nhave no connection with neurons of the same layer. The\nRBMs have a simple and fast learning algorithm that ba-\nsically try to minimize the reconstruction error using an\nalgorithm called contrastive divergence [15]. We can stack\nmany RBMs on top of each other, where the visible layer\nof the top RBMs is the hidden unit of the RBM below, to\nobtain a DBN. The pre-training phase then consists of trai-\nning each RBM sequentially, starting from the input layer\nup to the output layer. Once this is completed, the model\nis further “ﬁne tuned” for a speciﬁc supervised learning\ntask. This ﬁne tuning is done using the same gradient des-\ncent learning asn an MLP : given a cost function to opti-\nmize, the gradient is propagated through the network, and\nweights are updated accordingly. As for our MLP model,\nwe used a cross-entropy cost function. One problem with\nDBNs is the large number of hyper-parameters : number\nof layers, number of units per layer, pre-training learning\nrate, gradient descent learning rate, weight regularization\nconstant, number of pre-training epochs. This makes the\nhyper-parameter search tedious.\n6. EXPERIMENTS\n6.1 Experimental Setup\nAs in [19], we used weak labels as targets for training,\ni.e. targets for every frame in a given song are the same.\nIf a song contains a string instrument and a guitar, every\nframe of that song will be labelled as containing ‘strings’\nand ‘guitar’, even though there is no guarantee that there is\na string instrument and a guitar in every frame.\nThe task of instrument class recognition in poly-instrument\naudio is a multi-label classiﬁcation task, i.e. each instru-\nment class may be present or not, and the classiﬁer is una-\nware of how many classes are present.\nIn order to compare the three different models, we used\nthe F-Score as a performance measure. The F-Score is a\nmeasure that balances precision and recall. The precisionand recall are deﬁned as\nPrecision =tp\ntp+fp;Recall =tp\ntp+fn(1)\nwhere tp,fpandfnare the number of ‘true positives’,\n‘false positives’ and ‘false negatives’ examples. A true po-\nsitive is a positive example that was correctly labeled as po-\nsitive by the model. A false positive is a negative example\nthat was mislabeled as positive. A false negative is a posi-\ntive example that was mislabeled as negative. The F-Score\n(F) is deﬁned as the harmonic mean of the precision and\nthe recall\nF=2(precision\u0003recall )\nprecision +recall(2)\nThis can be simpliﬁed to\nF=2tp\n2tp+fp+fn: (3)\nTo obtain F-Scores for each instrument, we calculated\nthe F-Scores independently as for 7 independent classiﬁca-\ntion tasks. In order to get a global F-score that represents\nthe overall performance of the models, we took the sum of\ntp,fpandfnover all the instruments.\nThe neural networks (MLP, DBN) output a probability\n2[0;1]for each instrument, representing the network’s be-\nlief that the instrument is present in the given input frame.\nIf the probability for a given instrument is higher than a gi-\nven threshold, we classify this class as being present. Lo-\nwering the threshold improves the recall, but lowers the\nprecision, while increasing the threshold has the opposite\neffect. To label a whole song, we take the mean of the pro-\nbabilities from each frame and apply a threshold to decide\nwhether or not each instrument class is present. We opti-\nmized the threshold to maximize the global F-score.\nThe output of our SVM model is binary (0 or 1) for each\nclass. We used a similar technique as the neural network to\nlabel a song, except that we have binary votes instead of\nprobabilities.\n6.2 Results and Discussion\n6.2.1 Feature sets\nTo conﬁrm that the features we extracted from the au-\ndio were useful for training our models, we compared the\nresults of training with subsets of our feature sets on the\nsolo instrument audio corpus. The mean F-score for each\nsubset using our three models are shown in Table 1. We see\na tendency that using more features helps the SVM and the\nDBN, but the MLP doesn’t show improvement with the full\nset of features compared to using only 20 MFCCs. Another\nresult that is remarkable is that the DBN performs surpri-\nsingly well compared to the two other models with only the\nspectral features as inputs. For the following experiments,\nwe will always use our full set of features.\n6.2.2 Solo instrument audio\nOur ﬁrst audio corpus contains solo performances from\nall the instruments. For this experiment, we generated a to-\ntal of 2735 song examples generated from 7 different MIDISVM MLP DBN\nSpectral Features (16) 0.51 0.74 0.81\n12 MFCCs (72) 0.75 0.85 0.85\n20 MFCCs (120) 0.81 0.86 0.87\nAll Features (136) 0.84 0.84 0.88\nTable 1 . Global F-score for different features subsets (fea-\ntures vector length in parenthesis)\nﬁles. We used 1984 of these for training and validation, for\na total of 62434 1-second frames. The results are shown in\nTable 2.\nSVM MLP DBN %\nBass 0.88 0.88 0.88 13.85%\nBrass 0.87 0.88 0.91 22.37%\nGuitar 0.0 0.0 0.21 2.13%\nOrgan 0.96 0.89 0.96 7.46%\nPiano 0.45 0.43 0.57 6.39%\nStrings 0.94 0.95 0.97 9.59%\nWoodwind 0.82 0.85 0.89 29.83%\nGlobal 0.84 0.84 0.88\nTable 2 . F-score for solo instrument audio. The results that\nclearly outperforms the other models are highlighted in\nbold. The percentage of positive examples in the training\nset for each instrument is shown in the rightmost column\nWe see that the DBN tends to perform better than both\nthe SVM and the MLP in this experiment. Moreover, the\nDBN seems to perform signiﬁcantly better when the quan-\ntity of positive training example is smaller. Note that both\nthe SVM and the MLP were unable to recognize the guitar\ninstrument class. This is probably related to the fact that\nonly a small fraction of the data set contained positive gui-\ntar examples.\nThe DBN that gave the best validation F-score had 5\nlayers of 50 units each. Only 3 epochs of pre-training over\nthe training set were necessary to achieve the best genera-\nlization performance. The best MLP model had 40 hidden\nunits.\n6.2.3 Poly-instrument audio\nOur second audio corpus is constructed from mixes of\ninstruments. Each song is generated from one of 6 MIDI\nﬁles containing between 2 and 6 tracks, and thus each example\ncontains from 1 to 6 classes (many instruments from the\nsame class are allowed). The data set is constituted of 3654\ntraining and validation examples divided in 186532 frames.\nResults are shown in Table 3.\nAgain, in this experiment, the DBN seems to perform\nslightly better than the SVM and the MLP. In three cases\n(brass, guitar and woodwind), the performance difference\nwas important. The DBN with the best generalization per-\nformance in this experiment had 4 layers of 100 units andSVM MLP DBN %\nBass 0.86 0.83 0.85 50.00%\nBrass 0.38 0.45 0.63 25.90%\nGuitar 0.05 0.15 0.28 11.94%\nOrgan 0.84 0.84 0.85 62.99%\nPiano 0.83 0.80 0.83 64.44%\nStrings 0.37 0.37 0.36 18.82%\nWoodwind 0.31 0.41 0.52 31.81%\nGlobal 0.72 0.72 0.74\nTable 3 . F-score for poly-instrument audio. The results\nthat clearly outperforms the other models are highlighted\nin bold. The percentage of positive examples in the training\nset for each instrument is shown in the rightmost column\nrequired 4 epochs of pre-training. The best MLP was construc-\nted with 60 hidden units.\n6.3 Discussion\nIn all 3 experiments, the DBN generally performed bet-\nter than the 2 other models, although the difference is not\nalways important. The DBN tends to perform better espe-\ncially in cases where the quantity of positive examples is\nsmall. This could indicate that the DBN was able to learn\nhigher-level features to discriminate instrument classes. In\nother words, it was able to use what it learned from other\ninstrument classes to discriminate instruments that were\nless frequent.\nAlthough the results seem to show that the DBN perfor-\nmed better than the SVM and MLP, we cannot draw any\nhard conclusion with these results because of the similarity\nof the results and the lack of conﬁdence intervals. The F-\nScore may not be the best measure to get such conﬁdence\nintervals. However, these results clearly show that DBNs\ncan be useful for the task of instrument recognition. These\nresults also motivate more experiments to conﬁrm the ten-\ndency shown. In future work, these experiments should be\nrun using cross-fold testing and measuring the classiﬁca-\ntion error in order to obtain a reliable conﬁdence measure.\nWhen generating our labeled examples, we tried to stay\nas close to real music as possible. The MIDI format is good\nto reproduce some features of real music such as harmoni-\nzation and timing. However, it is harder to represent mu-\nsical features such as expressiveness and instrument dyna-\nmics variations in MIDI. Also, our system used a rather\nsimple ﬁxed mixing of the instruments in a given song,\nwhich gave rise to small variability in the relative volume\nof the instruments. The limited number of midi ﬁles we\nused is also a limitation of our model. In future work, we\nwould like to add more variability to the music generation\nby using more songs and by diversifying the mixing bet-\nween instruments.\nAnother aspect that could improve the performance of\nthe three models would be to learn an independent decision\nthreshold for each instrument class. We used only one deci-\nsion threshold that was optimized on the validation set glo-bal F-Score. This may be related to the fact that the SVM\nand the MLP were unable to recognize the guitar class in\nthe solo instrument experiment.\n7. CONCLUSION AND FUTURE WORK\nIn this work, we have introduced the DBN model for\ninstrument recognition. We have shown that DBNs per-\nform at least as well as SVMs and MLPs for this task. We\nhave also shown that the DBN tends to outperform these\nmodels when the feature set is limited, and when the num-\nber of positive examples for a class is limited. These results\nmotivate the application of deep networks in music infor-\nmation retrieval tasks.\nAs seen in Section 4, adding more relevant features seems\nto improve the performance of the classiﬁers. In future\nwork, it would be interesting to consider extracting a wi-\nder variety of features from the audio. In this study, we\navoided harmonic features that rely on the identiﬁcation of\na single fundamental frequencey for a frame of audio be-\ncause this is ill-deﬁned in the polyphonic case. In future\nwork, it would be interesting to test if extracting simple\nharmonic features (e.g. odd to even harmonics ratio) from\nmixed instruments using an estimate of the most salient\nfrequency could help for this task. We suppose that there is\nuseful information in such features.\nWe also plan to add more variability to our data set by\nadding reverb and background noise to our audio examples.\nWe hypothesize that this would add robustness to our trai-\nned models.\nFinally, it would be interesting to test our model on real\nmusic. This is something that we plan for the near future.\nTo test our model on commercial music, we would need to\ntrain a wider range of instruments, such as drums, distorted\nguitars, vocals, etc.\n8. ACKNOWLEDGMENTS\nPhilippe Hamel was supported ﬁnancially by FQRNT.\nDouglas Eck and Sean Wood are ﬁnanced by NSERC Dis-\ncovery Grants. Special thanks to Nathanael Lecaude, Pas-\ncal Lamblin, Simon Lemieux, Olivier Delalleau, and all the\npeople at the GAMME and LISA labs for helpful discus-\nsions.\n9. REFERENCES\n[1] G. Agostini, M. Longari, and E. Pollastri. Musical ins-\ntrument timbres classiﬁcation with spectral features.\nEURASIP J. Appl. Signal Process. , 2003 :5–14, 2003.\n[2] Y Bengio. Learning deep architectures for AI. Founda-\ntions and Trends in Machine Learning , to appear, 2009.\n[3] Y . Bengio, P. Lamblin, D. Popovici, and H. Larochelle.\nGreedy layer-wise training of deep networks. In Bern-\nhard Sch ¨olkopf, John Platt, and Thomas Hoffman, edi-\ntors, Advances in Neural Information Processing Sys-\ntems 19 , pages 153–160. MIT Press, 2007.\n[4] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere.\nAutotagger : A model for predicting social tags fromacoustic features on large music databases. Journal of\nNew Music Research” , 37(2) :115–135, 2008.\n[5] C.-C. Chang and C.-J. Lin. LIBSVM : a library for sup-\nport vector machines , 2001.\n[6] J. Eggink and G.J. Brown. Application of missing fea-\nture theory to the recognition of musical instruments in\npolyphonic audio. In International Symposium on Mu-\nsic Information Retrieval (ISMIR ’03) , 2003.\n[7] S. Essid, G. Richard, and B. David. Instrument recog-\nnition in polyphonic music based on automatic taxono-\nmies. IEEE Transactions On Audio, Speech And Lan-\nguage Processing , 14 :68–80, 2006.\n[8] S. Essid, G. Richard, and B. David. Musical instrument\nrecognition by pairwise classiﬁcation strategies. IEEE\nTransactions On Audio, Speech And Language Proces-\nsing, 14 :1401–1412, 2006.\n[9] A. Fraser and I. Fujinaga. Toward realtime recogni-\ntion of acoustic musical instruments. In Proceedings of\nthe International Computer Music Conference. 175–7. ,\n1999.\n[10] I. Fujinaga. Machine recognition of timbre using\nsteady-state tone of acoustic musical instruments. In\nProceedings of the International Computer Music\nConference. 207-10. , 1998.\n[11] I. Fujinaga and K. MacMillan. Realtime recognition of\norchestral instruments. In Proceedings of the Interna-\ntional Computer Music Conference. 141–3. , 2000.\n[12] P. Hamel, S. Wood, S. Lemieux, and D. Eck. The\nGAMME Poly-Instrument Audio Database, 2009.\nhttp://www.iro.umontreal.ca/ ˜gamme/\ninstrument_data/ .\n[13] P. Herrera, A. Klapuri, and M. Davy. Signal Processing\nMethods for Music Transcription , chapter Automatic\nClassiﬁcation of Pitched Musical Instrument Sounds,\npages 163–200. Springer US, 2006.\n[14] G. Hinton and R. Salakhutdinov. Reducing the di-\nmensionality of data with neural networks. Science ,\n313 :504–507, 2006.\n[15] G. E. Hinton, S. Osindero, and Y . Teh. A fast learning\nalgorithm for deep belief nets. Neural Computation ,\n2006.\n[16] T. Kitahara, M. Goto, K. Komatani, T. Ogata, and H. G.\nOkuno. Instrument identiﬁcation in polyphonic music :\nfeature weighting to minimize inﬂuence of sound over-\nlaps. EURASIP J. Appl. Signal Process. , 2007(1) :155–\n155, 2007.\n[17] A. G. Krishna and T. V . Sreenivas. Music instru-\nment recognition : from isolated notes to solo phrases.\nInin Proceedings of IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP\n’04), volume 4, pages 265–268, Montreal, Quebec, Ca-\nnada, May 2004.\n[18] P. Leveau, D. Sodoyer, and Daudet L. Automatic ins-\ntrument recognition in a polyphonic mixture using\nsparse representations. In ISMIR , 2007.[19] D. Little and B. Pardo. Learning musical instruments\nfrom mixtures of audio with weak labels. In Procee-\ndings of the 9th International Conference on Music In-\nformation Retrieval (ISMIR 2008) , 2008.\n[20] A. Livshin and X. Rodet. Musical instrument identi-\nﬁcation in continuous recordings. In Proc. of the 7th\nInt. Conference on Digital Audio Effects (DAFX-04),\nNaples, Italy , 2004.\n[21] M. I. Mandel and D. P.W. Ellis. Song-level features\nand support vector machines for music classiﬁcation.\nInProceedings of the 6th International Conference on\nMusic Information Retrieval (ISMIR 2005) , pages 594–\n599, 2005.\n[22] J. Marques and P. J. Moreno. A study of musical ins-\ntrument classiﬁcation using gaussian mixture models\nand support vec- tor machines. Crl technical report\nseries crl/4, Cambridge Research Laboratory, Cam-\nbridge, Mass, USA, 1999.\n[23] K Martin. Sound-source recognition : A theory and\ncomputational model . PhD thesis, MIT, 1999.\n[24] S. McAdams. Psychological constraints on form-\nbearing dimensions in music. Contemporary Music Re-\nview, 1989.\n[25] G. Peeters. A large set of audio features for sound des-\ncription (similarity and classiﬁcation) in the cuidado\nproject. Technical report, IRCAM, 2004.\n[26] D. Turnbull, L. Barrington, D. Torres, and G. Lan-\nckriet. Semantic annotation and retrieval of music and\nsound effects. IEEE Transactions on Audio, Speech\nAnd Language Processing , 16(2) :467–476, 2008.\n[27] E. Vincent and X. Rodet. Instrument identiﬁcation in\nsolo and ensemble music using independent subspace\nanalysis. In ISMIR , 2004.\n[28] P. Vincent, Y . Bengio, N. Chapados, et al. Plearn.\nhttp://plearn.berlios.de/ .\n[29] P. Vincent, H. Larochelle, Y . Bengio, and P.-A. Man-\nzagol. Extracting and composing robust features with\ndenoising autoencoders. In ICML ’08 : Proceedings\nof the 25th international conference on Machine lear-\nning, pages 1096–1103, New York, NY , USA, 2008.\nACM.\n[30] D. L. Wessel. Timbre space as a musical control struc-\nture. Computer Music Journal , V ol 3 No 2, 1979."
    },
    {
        "title": "SMERS: Music Emotion Recognition Using Support Vector Regression.",
        "author": [
            "Byeong-jun Han",
            "Seungmin Rho",
            "Roger B. Dannenberg",
            "Eenjun Hwang"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415674",
        "url": "https://doi.org/10.5281/zenodo.1415674",
        "ee": "https://zenodo.org/records/1415674/files/HanRDH09.pdf",
        "abstract": "Music emotion plays an important role in music retrieval, mood detection and other music-related applications. Many issues for music emotion recognition have been addressed by different disciplines such as physiology, psychology, cognitive science and musicology. We present a support vector regression (SVR) based music emotion recognition system. The recognition process consists of three steps: (i) seven distinct features are extracted from music; (ii) those features are mapped into eleven emotion categories on Thayer’s two-dimensional emotion model; (iii) two regression functions are trained using SVR and then arousal and valence values are predicted. We have tested our SVR-based emotion classifier in both Cartesian and polar coordinate system empirically. The result indicates the SVR classifier in the polar representation produces satisfactory result which reaches 94.55% accuracy superior to the SVR (in Cartesian) and other machine learning classification algorithms such as SVM and GMM.",
        "zenodo_id": 1415674,
        "dblp_key": "conf/ismir/HanRDH09",
        "keywords": [
            "music emotion recognition",
            "support vector regression",
            "music retrieval",
            "mood detection",
            "Thayer’s two-dimensional emotion model",
            "arousal",
            "valence",
            "machine learning classification algorithms",
            "SVM",
            "GMM"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n.SMERS: MUSIC EMOTION RECOGNITION\nUSING SUPPORT VECTOR REGRESSION\nByeong-jun Han, Seungmin Rho Roger B. Dannenberg Eenjun Hw ang \nSchool of Electrical Engineering \nKorea University \n{hbj1147, sm rho}@korea.ac.kr School of Com puter Science \nCarnegie Mellon University \nrbd@cs.cm u.edu School of Electrical Engr.\nKorea University \nehwang04@korea.ac.kr\nABSTRACT\nMusic emotion plays an important role in music retrieval, \nmood detection and other music-related applications. \nMany issues for music emotion recognition have been \naddressed by different disciplines such as physiology, \npsychology, cognitive science and musicology. We \npresent a support vector regression (SVR) based music \nemotion recognition system . The recognition process \nconsists of three steps: (i) seven distinct features are ex-\ntracted from  music; (ii) those features are mapped into \neleven emotion categories on Thayer’s two-dim ensional \nemotion  model; (iii) two regression  functions are traine d \nusing SVR and then arousal and valence values are pre-\ndicted. We have tested our SVR-based emotion classifier \nin both Cartesian and polar coordinate system  empirically. \nThe result indicates the SVR classifier in the polar repre-\nsentation produces satisfactory result which reaches \n94.55% accuracy superior to the SVR (in Cartesian) and \nother machine learning classi fication algorithm s such as \nSVM and GMM.  \n1.INTRODUCTION\nWith the recent advances in the field of music inform a-\ntion retrieval, there is an emerging interest in (autom ati-\ncally) analyzing and understanding the emotional content \nof music. Due to the diversity and richness of music con-\ntent, many researchers have been pursuing a multitude of \nresearch topics in this field, ranging from  computer \nscience, digital signal processing, mathem atics, and sta-\ntistics applied to musicology and psychology. Many \ncomputer scientists [1][2] have focused on music retriev-\nal by using musical meta-data (such as title, genre or \nmood) as well as low-level feature analysis (such as pitch, \ntempo or rhythm ), while music psychologists [3][4] have \nbeen interested in studying how music communicates \nemotion.  \nCurrently, there is no standard method to measure and \nanalyze emotion in music. However, a psychological \nmodel of emotion has found increasing use in computa-\ntional studies. Thayer’s two-dim ensional emotion mod-el[5] offers a simple but quite effective model for plac-\ning emotion in a two-dim ensional space. In the model, \nthe amount of arousal and valence is measured along the \nvertical and horizontal axis, respectively\nThe goal of this paper is to develop a music emotion \nrecognition system  for predicting the arousal and valence \nof a song based on audio content. First, we analyzed sev-\nen different musical features (such as pitch, tempo, loud-\nness, tonality, key, rhythm  and harm onics) and mapped \nthem  into eleven categories of emotion: angry, bored, \ncalm , excited, happy, nervous , peaceful, pleased, relaxed, \nsad and sleepy. This categorization is based on Juslin’s \ntheory [3] along with Thayer’s emotion model [5]. Se-\ncondly, we adopt support vector regression (SVR) [6] as \na classifier to train two regression functions for predict-\ning arousal and valence values based on the low-level \nfeatures, such as pitch, rhythm and tempo, extracted from  \nmusic. In addition, we compared our SVR-based method \nwith other classification algorithm s such as GMM (Gaus-\nsian Mixture Model) and SVM (Support Vector Ma-\nchine) to evaluate the perform ance.  \nIn the following section, we present a brief overview \non the current state-of-the-art music recognition system s, \nand emotion models. In Section 3, we illustrate a musical \nfeature extraction schem e and give an overview of our \nproposed system . Section 4 describes our proposed SVR-\nbased music emotion recognition method. Experim ental \nresults are given in Section  5. In the last section, we con-\nclude the paper with som e observations and future work. \n2.RELATED WORK \nMany researchers have explored models of emotions and \nfactors that give rise to the perception of emotion in mu-\nsic. Many other researchers investigate the problem  of \nautom atically recognizing em otion in m usic. \n2.1Music and Emotion \nTraditional mood and emotion research in music has fo-\ncused on finding psychological and physiological factors \nthat influence emotion recognition and classification. \nDuring the 1980s, several emotion models were pro-\nposed, which were largely based on the dimensional ap-\nproach for em otion rating.  Perm ission to make digital or hard copies of all or part of this work for\npersonal or classroom  use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. \n© 2009 International Society for Music Inform ation Retrieval  \n651Poster Session 4\n  \n \nFigure 2.  System diagram of the SMERS \nThe dimensional approach focuses on identifying \nemotions based on their location on a small number of \ndimensions such as valence and activity. Russell’s [7] \ncircumflex model has had a si gnificant effect on emotion \nresearch. This model defines a two-dimensional, circular \nstructure involving the dime nsions of activation and va-\nlence. Within this structure, emotions that are across the circle from one another, such as sadness and happiness, correlate inversely. Thayer [5] suggested a two-dimensional emotion model that is si mple but powerful in \norganizing different emoti on responses: stress and energy. \nThe dimension of stress is called valence while the di-mension of energy is called arousal.  \nAs shown in Figure 1, the two-dimensional emotion \nplane can be divided into four quadrants with eleven emotion adjectives placed over them. We use eleven types based on Juslin’s theory and Thayer’s emotion model.  \nDuring the last decade, many researchers have investi-\ngated the influence of music factors like loudness and to-nality on the perceived emotional expression [3][5]. They analyzed those factors using diverse techniques, some of \nwhich are involved in measuri ng psychological and phy-\nsiological correlati on between the state of particular mus-\nical factor and emotion evoca tion. According to the [3], \nJuslin and Sloboda investigated the utilization of acoustic cues in the communication of music emotions by perfor-mers and listeners and measured the correlation between \nemotional expressions (such as anger, sadness and happi-\nness) and acoustic cues (such as tempo, spectrum and ar-\nticulation).  \n2.2 Music Emotion Recognition  \nAutomatic emotion detection and recognition in speech \nand music is growing rapidly with the technological ad-\nvances of digital signal processing and various effective feature extraction methods. Emotion recognition can play an important role in many other potential applica-\ntions such as music entertainment and human-computer interaction systems.  \nOne of the first studies of em otion detection in music \nis presented by Feng et al.  [8]. Their work, based on Computational Media Aest hetics (CMA), analyzes two \ndimensions of tempo and artic ulation which are mapped \ninto four categories of moods: happiness, anger, sadness and fear. Lie et al. [4] developed a hierarchical frame-work for extracting music emotion automatically from acoustic music data. They used music intensity to represent the energy dimensi on of Thayer model, and \ntimbre and rhythm for the stress dimension.  \nFEELTRACE [9] is software that is designed to let \nobservers track the emotional content of stimuli (such as words, faces, music, and video) as they perceive it and taking full account of gradation and variation over time. Yang et al. [10] developed a mu sic emotion recognition \n(MER) system from a continuous perspective and \nrepresented each song as a point in the emotion plane.  They also proposed a novel arousa l/valence computation \nmethod based on regression theory.  \n3. IMPLEMENTATION\nIn this paper, we implemented a music recognition sys-\ntem, called SMERS (SVR-based Music Emotion Recog-nition System). The system diagram is shown in Figure 2 \nand the details are described as follows. \n3.1 System Description \nThe SMERS mainly consists of three steps: (i) Feature \nextraction: Seven distinct musical features are extracted \nand analyzed (Details are described in the Section 3.3); (ii) Mapping: Extracted features are mapped into eleven emotion categories on Thay er’s two-dimensional emo-\ntion model; (iii) Training: The system uses extracted fea-tures as input vectors to train the SVR. We use two dis-\ntinct SVR functions in a pol ar coordinate system: one is \nfor distance  from origin (0, 0) to  the emotion in a Thay-\ner-like coordinate system, and the other is for angle . Us-\ning these two trained SVRs, the system predicts each song’s emotion. Based on empirical  test results, the polar \ncoordinate system is a better representation than the ob-vious Cartesian coordinates. (M ore details about training \nprocedure in both Cartesian  and polar coordinate sys-\ntems are presented in Section 4.1). \n \nFigure 1.  Modified Thayer’s 2-di mensional emotion \nmodel \n65210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \n3.2 Dataset\nThe music dataset for traini ng the SMERS is made up of \n165 western pop songs. We collected the 15 songs in each of eleven categories of emotion from the large mu-sic database, All Music Guide [11], which provides 180 \nemotional categories for classifying entire songs. To \nbuild classifiers we used Support Vector Regression \n(SVR) and our implementation is based on the LIBSVM \nlibrary [12], which gives almost full functionalities for \nSVR training.  \n3.3 Musical Features\nIn this paper, we consider various musical features in-\ncluding scale, intensity, rhythm, and harmonics and use \nthem as an input vector in the emotion recognition system.  \n3.3.1 Scale \nScale is an overall rule of tonic formation of music. In \nour study, we defined scale as a set of key, mode, and to-\nnality. For accurate scale features, we first analyzed the chromagram for representing the frequencies in musical \nscales. After that, we applied the key profile matrix by \nKrumhansl [13]. The following equations show the \nprocess of combining chromagram and key characteriza-tion:  \nMatrix KeyProfileC/g152/g32 Tonality\n/g11/g12 )( max Idx Tonality Key /g32 (1)\nKeyIndex (2)\n, where vector C has 12 elements and represents the \nsummed chromagram analyzed for each acoustic frame. KeyProfileMatrix is a key profile matrix, which is com-\nposed of 12-by-24 elements. KeyIndex indexes KeyProfi-\nleMatrix, where KeyIndex=1,2,…,24. After the inner product of C and KeyProfileMatrix in Equation (1), we obtain a tonality score for each key. Finally, we can ob-\ntain the most appropriate key  by picking the key having \nmaximum tonality in Equation (2). \n3.3.2 Average Energy (AE) \nAverage energy (AE) of the overall wave sequence is \nwidely adopted to measure the loudness of music. Also, standard deviation (/g305) of AE measures the regularity of loudness. Those are defined as: \n/g11/g12 /g11/g12 /g11 /g12 /g11/g12 /g11/g12 /g11 /g12/g11/g12/g166 /g166\n/g32 /g32/g16 /g32 /g32N\ntN\nttxxNx txNx\n02\n02AE1AE ,1AE /g86 (3)\n, where x is an input discrete signal, t is the time in sam-\nples, and N is the length of x in samples. \n3.3.3 Rhythm \nRhythm, which is composed of rhythmic features such as \ntempo and beat, is one of the most important elements in music. Beat is a fundamental rhythmic element of music. \nTempo is usually defined as the beats per a minute \n(BPM) which is used to repr esent the global rhythmic feature of music. Tempo and regularity of beats can be \nmeasured in various ways. For beat tracking and tempo analysis, we used the algorithm by Ellis et al. [14]. The features we use are overall tempo (in beats per minute) and the standard deviation of beat  intervals, which indi-\ncates tempo regularity. \n3.3.4 Harmonics \nHarmonics can be observed in musical tones. In mono-\nphonic music, harmonics are easily observed in the spec-\ntrogram. However, it is hard to find harmonics in poly-phony, because many instruments and voices are per-formed at once. To solve this problem, a method to com-pute harmonic dist ribution yields \n/g11/g12 /g11/g12 /g11/g12 /g11/g12 /g166\n/g32/g32M\nkkf f f\n1X, X min HS\n (4)\nHere, M denotes the maximum number of harmonics \nconsidered, f is the fundamental frequency, and X is the \nshort-time Fourier transform (STFT) of the source signal. In the equation, the min function is used in such a way that only the strong fundamental and strong harmonics result in a large value for HS. In our implementation, we measured average of each frequency using (4) and then computed their standard deviati on to define the harmonic \nfeature. \n4. EMOTION RECOGNITION \n4.1 Training Process \nThere are some essential conditions needed for effective \nemotion recognition. Firstly, the regression function should be trained as perfectly close to ground-truth as it can. If the trained regression function cannot generate proper Arousal/Valence (AV) values for a music emotion adjective, the separation policy also cannot act in a prop-er way. Secondly, a proper musi c emotion separation pol-\nicy on the AV plane should be presented. It acts like a decoder or quantizer of AV values. If the separation poli-cy does not reflect the natu ral mapping between emotion \nadjectives and AV values, system might have to learn more complex mapping from features to the AV values. \nOur music emotion separation policy in the AV plane \nis shown in Figure 3. In case of Cartesian representation, \nthe emotion of a song can be represented by ( a, v), where \na denoting arousal and v denoting valence and their \nranges are aù[-1,1] and vù[-1,1], respectively. There \nare also 5 separating lines: v=v\n(+), v=v(-), v=0, a=a(+), and \na=a(-). These lines separate the AV plane in 11 areas. As \nshown in Figure 3, each area has a center point, which is drawn as a black dot. These dots are used as the ground-truth data for training SVRs.  On the other hand, the blank \ndots are outputs of the SVR-based on feature vectors ex-\ntracted from songs. \n653Poster Session 4\n  \n \nFor training our emotion classifier, we need two dis-\ntinct SVR functions. One is for training an arousal value \nand the other is for a valence value. The training is per-\nformed by the musical feat ures of songs as input and the \ncenter values of each music emotion as the desired output. Our test verifies whether or not the outputs (arousal and \nvalence values) of trained regression functions are within the range of the proper music emotion in AV plane.  \nUsing Cartesian coordinate s, we found that some emo-\ntions such as “Peaceful” and “Bored” are misclassified \ninto the “Calm” emotion category in the center of the AV \nplane. We decided to train using polar coordinates as the \ndesired output to see if that would produce better results.  \nAssume that Emotion\nc and Emotion p represent an emo-\ntion in Cartesian and polar coordinate systems, respec-\ntively. We can calculate the distance  and angle  values of \neach emotion and transfer the coordinate system from Cartesian to polar using the following equa ions: t\n/g11/g12\n/g11/g12\n/g11/g12\n/g176/g176\n/g175/g176/g176\n/g174/g173\n/g184/g184\n/g185/g183\n/g168/g168\n/g169/g167/g32/g14 /g32/g32/g32\nCCC CPC\nValenceArousalAngleValence Arousal Distance\ntsAngle Distance EmotionValence Arousal Emotion\narctan..,,\n21\n2 2 \n(5)\n/g11/g12\n/g11/g12 /g175/g174/g173\n/g152 /g32/g152 /g32\nAngle Distance ValenceAngle Distance Arousal\nPP\nsincos  (6)\n4.2 Classification Methods \n4.2.1 Support Vector Regression (SVR)-based Training \nThe basic idea of regression is to determine a function \nthat accurately approximates target values using input values. SVR [6] is an appli cation of SVM to find the \nmapping function between i nput and output. There are \ntwo major training strategies of SVR. One is /g304-SVR, \nwhich employs /g304-insensitive loss fun ction to solve the \nquadratic optimization problem. However, /g304-SVR has the \nfollowing limitations: /g304 should be set before training the \nSVR model. Also, it is hard to  anticipate the range of /g304 in \nmost problems. The other strategy, named /g547-SVR [15], solves the limitations of /g304-SVR by limiting the task of \nfinding /g304 to the quadratic optimization problem. \nFor the training sets {( x\n1, y1), (x2, y2), …, ( xn, yn)} with \nxiùRn, yiùR, and i=1, 2, …, n. The relation between the \ninput xi and output yi can be mapped by an optimal re-\ngression function f(x) by SVR training. As the result of \ntraining, the difference betw een trained function output \nfrom input and ground-truth of input should be lower \nthan the error /g304. Assuming linearity, f can be represented \nas the following hyperplane: f(x) = /g550·/g301(x) + b, where \n/g550ùRn, bùR, and /g301 denotes a nonlinear transformation \nfrom Rn to a high-dimensional space. \nOur goal is to find the value /g550 and b. The values of x \ncan be determined by solving following quadratic opti-mization problem: \n/g11/g12\n/g11/g12 /g11 /g12 /g11/g12\n/g175/g174/g173\n/g143 /g116/g14/g100/g14 /g41/g152/g16/g100 /g14/g16/g14/g14 /g14/g166\n/g32\n]1,0(,0s.t.21min\n1* 2\nn eb x yC\ni i i in\nii i\n/g91/g72 /g90 /g91/g72/g81/g72/g91/g91 /g90  \n(7)\n, where C is a constant value. With some data points /g302i \nand /g302i*, we can write /g550 to , so that f \ncan be rewritten as: /g11/g12 /g11/g12 /g166/g32/g41/g16 /g32n\ni i i i x1*/g68 /g68 /g90\n/g11/g12 /g11/g12 /g11/g12 /g11 /g12 /g11/g12 /g11/g12 bxxk bx x xfn\nii i in\nii i i /g14 /g16 /g32/g14 /g41/g152 /g41 /g16 /g32 /g166 /g166\n/g32 /g32 1*\n1*, /g68 /g68 /g68 /g68 (8)\n, where k is known as the kernel function. On the other \nhand, (7) can be solved by transforming to the Lagrange function and getting its multipliers, /g302\ni and /g302i*, as indi-\ncated in [16]. These are called support vectors and mea-\nningful when they are nonzero values. Also we can get \noptimal b and /g304 by the Kuhn-Tucker condition. In our \nsystem, we employed Radial  Basis Function (RBF) as a \nkernel function instead of usi ng linear or polynomial \nfunctions due to its flexibility.   \n4.2.2 Support Vector Machine (SVM)-based Training \nFor emotion classification, we used multi-class SVM. \nSince SVM classifies only one class at a time, we trained 11 SVMs to classify each emotion separately. This set of classifiers receives input feature vectors extracted from \nmusic. Each classifier generates a probability that the \nmusic has a specific emotion.  The highest probability \nvalue determines the final selection of a single emotion \nlabel for the music. \n4.2.3  Gaussian Mixture Model  (GMM)-based Training \nAll musical features are m odeled using Gaussian Mixture \nModels (GMMs). We use 7 Gaussian models for arousal and valence sets. Each GMM is trained using the Expec-tation Maximization (EM) algorithm. The step of GMM-based classification is as follo ws: first of all, 3 and 4 \nGMMs were trained for labeling arousal and valence, re-spectively. Next, the two GMMs sets produce two classi-fications for arousal and valence, respectively. For exam-\nple, the GMMs set for arousal la beling could classify A is \nFigure 3.  Music emotion separation policy in AV \nplane (in both Cartesi an and polar representation) \n65410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nlower than -1/3, between -1/3 and 1/3, or higher than 1/3. \nIn final step, music emotion is determined by combining \nthe results from two GMMs sets. \n5. EXPERIMENTS AND RESULTS \nIn this section, we evaluate the effectiveness of our emo-\ntion recognition system in terms of accuracy. Coefficients \nfor SVR, SVM and GMM and kernels are very critical to performance. In our experiment, we tried to find the op-\ntimal classification parameters empirically. We also con-\nsidered the /g542-fold cross-validation method in order to \nprevent the over-fitting problem. We tested /g542-fold cross-\nvalidations using different /g542 values.  \nThe best SVR training paramet ers and optimum values \nin both Cartesian and polar representation are shown in \nTable 1 and 2, respectively. We searched for optimal val-ues of all parameters (except “# of folds in cross valida-\ntion”) in steps of about 7%. Moreover, cross validations were carried out 54 times for each step.  \nIn order to evaluate Cartesi an coordinate system-based \nclassification methods, we em ployed three types of clas-\nsifiers: SVMs with one-to-one training policy, SVR, and \nGMM. First of all, in SVMs-based classification, one-to-one training policy was em ployed, since SVM does not \nsupport multi-classification basically. In SVR-based clas-\nsification, we trained two regression functions to \nrepresent arousal and valence respectively. Finally, \nGMM was trained following the procedure in Section 4.2.3. On the other hand, in polar coordinate system-\nbased classification, two SVRs, which represent distance  \nand angle  respectively, were trained. \n5.1 Confusion Matrix \nConfusion matrices of each coordinate system combined \nwith each classifier are presented in Figure 4. As shown \nin Figure 4, the errors of both SVMs and SVR in Carte-\nsian coordinate system were comparably higher than both \nGMM in Cartesian coordinate system and SVR in the po-\nlar coordinate system. \nThe result of SVMs in the Ca rtesian coordinate system, \npresented in Figure 4(a), was good on specific music emotions such as angry, bored, and peaceful. However, most other diagonal elements had poor results.  \nThe change from multi SVMs to SVR increased the \nperformance as shown in Figure 4(b). On average, 9.5 Table 1.  SVR training paramet ers and obtained opti-\nmums in Cartesian representation\nName of parameters Range Optimum\nNu (/g547) 2-5 ~ 2-0.1 2-1.7 \nGamma of RBF (g) 2-20 ~ 2-0.1 2-8.3 \nCost (C) 1 ~ 215 27.4 \n \nTable 2.  SVR training paramet ers and obtained opti-\nmums in polar representation\nName of parameters Distance Angle \nNu (/g547) 2-8 2-8 \nGamma of RBF (g) 2-10 2-4 \nCost (C) 28 26 \nmean squared error 0.02498 0.09834 \n \n(a) (b) (c) \n \n  Table 3.  Classification result\nClassifiers Coordinate \nSystem Accuracy \nSVMs Cartesian 32.73% \nSVR Cartesian 63.03% \nGMM Cartesian 91.52% \nSVR polar 94.55% \nGMM polar 92.73% \n(d) (e)  \nFigure 4.  Confusion matrices: Cartesian coordi nate system with (a) SVMs (b) SVR \n(c) GMM, and polar coordinate system with (d) GMM and (e) SVR.  \n655Poster Session 4\n  \n \nsongs were correctly classified, but still some emotions \nhad errors. It can be seen that 12.73% of songs (21 songs) were misclassified into  calm in Figure 4(b). This \nindicates that the calm probl em should be solved first. \nThe result in Figure 4(c) and (d) is better than Figure \n4(a) and (b). Most diagonal el ements were well classified. \nIn the case of GMM in the Cartesian coordinate system, 12.8 songs on average were classified correctly. However, \nthere is still a concentration of misclassification in some \nemotions such as angry (4 songs), sad (5 songs), and sleepy (2 songs). However, SVR in  the polar coordinate \nsystem showed that the imbalanced classifications were \nsignificantly reduced: the average number of correct clas-\nsification was 14.2 songs, and al so, misclassification was \nconcentrated only in rel axed (2 songs) and sleepy (3 \nsongs). \n5.2 Accuracy\nThe results are shown in Table 3. In the experiments \nbased on Cartesian coordinate systems, maximum accu-racy was 91.52% (151 of 165 samples). By changing coordinate system into polar, the accuracy was increased to 94.55% (156 of 165 samples) using SVR and 92.73% (153 of 165 samples) using GMM. \n6. CONCLUSIONS AND FUTURE WORK \nIn this paper, automatic emotion recognition of music has \nbeen evaluated using various machine learning classifica-\ntion algorithms such as SVM, SVR and GMM. In our \nexperiment, it is shown that the SVR-based classification \nin the polar coordinate system remarkably improved the accuracy of the emotion recognition from 63.03% to 94.55%. However, the GMM classification with polar coordinates only improved from 91.52% to 92.73%. \nFor further research, more perceptual features should \nbe considered and other classifi cation algorithms such as \nfuzzy and kNN (k-Nearest Neighbor). We also plan to compare the result of machine learning (ML)-based emo-tion recognition with human performed arousal/valence data. \n7. REFERENCES\n[1] W. Birmingham, R. Dannenberg and B. Pardo: “An \nIntroduction to Query by Humming with the Vocal Search System,” Communications of the ACM , Vol. \n49 (8), pp. 49-52, 2006. \n[2] S. Rho, B. Han, E. Hwang and M. Kim: \n“MUSEMBLE: A Novel Music Retrieval System \nwith Automatic Voice Query Transcription and Reformulation,” Journal of Systems and Software \n(Elsevier) , Vol. 81(7), pp. 1065-1080, 2008.  \n[3] P.N. Juslin and J.A. Sloboda: “Music and Emotion: \nTheory and research,” Oxford Univ. Press , 2001. [4] L. Lie, D. Liu and Hong-Jiang Zhang: “Automatic \nMood Detection and Tracki ng of Music Audio \nSignals,” IEEE Trans. on ASLP , Vol. 14(1), 2006. \n[5] R. E. Thayer: “The Bi opsychology of Mood and \nArousal,” New York: Oxford University Press , 1989. \n[6] Smola, Alex J., et al.: “A tutorial on support vector \nregression,” Statistics and Computing , Vol.14, \npp.199-222, 2004. \n[7] J. A. Russell: “A Circumplex Model of Affect,” \nJournal of Personality and Social Psychology , Vol. \n39, 1980. \n[8] Y. Feng, Y. Zhuang, Y. Pan : “Music information \nretrieval by detecting m ood via computational media \naesthetics,” Proc. of IEEE/WIC Intl. Conf., Web \nIntelligence , pp. 235-241, 2003. \n[9] E. Cowie, et al. : “’FEELTRACE’: An instrument \nfor recording perceived emotion in real time,” Proc. \nof Speech Emotion , pp. 19–24, 2000.  \n[10] Y.H. Yang, et al.: “A regression approach to music \nemotion recognition,” IEEE Trans. on ASLP , Vol. \n16 (2), pp. 448–457, 2008.  \n[11] “The All Music Guide,” Available: \nhttp://www.allmusic.com. \n[12] Chih-Chung. Chang, and Lin, Chih-Jen: “LIBSVM: \na library for support vector machines,” 2001.  Available: http://www.csie.ntu.edu.tw/ ~cjlin/libsvm. \n[13] C. Krumhansl: “Cognitive foundations of musical \npitch,” Oxford University Press , 1990. \n[14] D. Ellis, P.W. Poliner, E. Graham: “Identifying \n‘Cover Songs’ with chroma features and dynamic \nprogramming beat tracking,” IEEE Conf. on ICASSP , \nVol. 4, 1429-1432, 2007. \n[15] B. Schölkopf, et. al .: “New support vector \nalgorithms,” Neural Computation , Vol.12, 2000. \n[16] A. Smola, T. Freiß, B. Schölkopf: “Semiparametric \nsupport vector and linear programming machines,” Nuero COLT TR , NC2-TR-1998-024, 1998. \n[17] Hsu, Chih-Wei., and Lin, Chih-Jen: “A comparison \nof methods for multiclass support vector machines,” \nIEEE Trans . on Neural Networks , Vol.13(2), \npp.415-425, 2002. \n \n656"
    },
    {
        "title": "Interfaces for Document Representation in Digital Music Libraries.",
        "author": [
            "Andrew Hankinson",
            "Laurent Pugin",
            "Ichiro Fujinaga"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414780",
        "url": "https://doi.org/10.5281/zenodo.1414780",
        "ee": "https://zenodo.org/records/1414780/files/HankinsonPF09.pdf",
        "abstract": "Musical documents, that is, documents whose primary content is printed music, introduce interesting design challenges for presentation in an online environment. Considerations for the unique properties of printed msic, as well as users’ expected levels of comfort with these materials, present opportunities for developing a viewer specifically tailored to displaying musical documents. This paper outlines five design considerations for a music document viewer, drawing examples from existing digital music libraries. We then present our work towards incorporating these considerations in a new digital music library system currently under development.",
        "zenodo_id": 1414780,
        "dblp_key": "conf/ismir/HankinsonPF09",
        "keywords": [
            "Musical documents",
            "printed music",
            "design challenges",
            "online environment",
            "unique properties",
            "printed music",
            "users comfort",
            "digital music libraries",
            "new digital music library system",
            "five design considerations"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   INTERFACES FOR DOCUMENT REPRESENTATION IN DIGITAL MUSIC LIBRARIES Andrew Hankinson1 Laurent Pugin2 Ichiro Fujinaga1 1 Schulich School of Music of McGill University, Montréal (Quebec) Canada 2 RISM Switzerland, Bern, Switzerland {ahankinson,laurent,ich}@music.mcgill.ca ABSTRACT Musical documents, that is, documents whose primary content is printed music, introduce interesting design challenges for presentation in an online environment. Considerations for the unique properties of printed msic, as well as users’ expected levels of comfort with these materials, present opportunities for developing a viewer specifically tailored to displaying musical documents. This paper outlines five design considerations for a music document viewer, drawing examples from existing digital music libraries. We then present our work towards incorporating these considerations in a new digital music library system currently under development. 1. INTRODUCTION In 2008, the Swiss working group for the Répertoire International des Sources Musicales (RISM) project began work towards digitizing its national music collection. These digitized scores would be incorporated into an online catalogue of works, and would allow users of this system to view these items online. One crucial element for the success of this project was the implementation of software that presented these documents using musically consistent techniques. The presentation of printed musical materials in an online environment poses interesting design challenges. Music and text documents have superficial similarities—they are written or printed on paper and bound in books—but they also differ significantly in their intended use, complexity of notation and stylistic considerations for presentation on the page. When displaying music documents in an online environment these differences should be taken into account. Beyond putting the scanned content online, there needs to be consideration for how to show the material to users. As we will demonstrate in our literature review, the presentation of content can have a significant impact on a user’s ability to navigate and comprehend the content itself. Next, we will propose five design considerations, formulated as requirements for implementation in a document viewer for a digital music library. Accompanying these design considerations we will show specific examples of how these have been implemented in existing digital library systems. Finally, we conclude with a brief discussion of our implementation of these design considerations, as well as commentary on possible directions for future work. 2. BACKGROUND In his 1984 dissertation, Byrd [1] describes printed music—specifically, conventional music notation (CMN) —as a “modified coordinate system” that encapsulates semantic, syntactic, and graphic complexity occurring in four dimensions (pitch, time, loudness, and timbre). Accompanying the complexity of the music itself are practical considerations that play an integral role in the interpretation of the materials, such as page layouts, line justifications, and convenient page-turns. While these considerations are not part of the musical content, they are part of the total information content of the score. Put another way, while there is no one correct way to present the printed music, there are many wrong ways to present it that can lead to mis-interpretation of the music itself. The dimensionality and complexity of printed music, Byrd states, exceeds the complexity of printed text and is central to understanding the problems that exist with computerized analysis of these materials.   The delivery of information in online environments is an area of research that has received quite a bit of attention. In particular, Thong et al. [2] show that “[in] the context of digital libraries, it not only matters what we put on the screen, but how.” They continue: “[the] way that information is arranged on the screen can influence the users’ interaction with digital libraries beyond the effect of the information content.”  Additional research has conclusively identified the affective relationship between the aesthetic perception of the materials and its effect on cognition and learning.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2009 International Society for Music Information Retrieval  \n39Oral Session 1-A: Knowledge on the Web\n   Kurosu and Kashimura [3] found that “[users] may be strongly affected by the aesthetic aspect of the interface even when they try to evaluate the interface in its functional aspects.” Building on these findings, Tractinsky et al. [4] performed a study of automated teller machines and found “strong correlation between users’ perception of an interface aesthetics and their perception of the usability of the entire system.” They postulated that factors of aesthetics and usability can play a significant role in the overall satisfaction derived from an interface.  These studies’ results are congruent with other work in the affective nature of human-computer interaction. When suggesting that “attractive things work better,” Norman [5] (building on studies by Ashby et al. [6]) suggests that aesthetic interfaces can lead to a greater overall satisfaction in an interaction, which in turn can have significant effects on understanding the content. Increases in tension or anxiety, caused by unpleasant experiences with a system can negatively affect cognition of the material, leading not just to an unpleasant interaction, but also a decrease in the users’ ability to understand the material itself.  While most usability research for digital libraries has focused specifically on textual materials, there has been work done on the evaluation of digital music library interfaces. Byrd and Crawford [7] touch on the topic of user interfaces for music information retrieval, simply stating that they are “hard.” Byrd and Isaacson [8] address problems of music representation in a digital music library; however, they deal specifically with issues of notation layout, and not with interactions with digitized print materials.  The VARIATIONS project at Indiana University has conducted a number of usability studies on their system. Fuhman et al. [9] observed that non-musically trained users of their system took longer to complete musically-oriented tasks than musically-trained users, and gave a lower overall subjective rating to interfaces designed for displaying musical content. One possible explanation for this might be that musically-trained users have learned specific techniques for interacting with musical materials that are not shared by users who are unfamiliar with this content.  Finally, the SyncPlayer software [10] has received quite a bit of attention as a system that provides an easy-to-use interface for navigating score and audio representations of music. While this software presents an interesting interface for viewing and navigating complex scores, it was not included because it has not been used in a large-scale, public digital music library implementation. 3. DESIGN CONSIDERATIONS As part of the design process for the document viewer, we identified five key considerations for designing an interface specifically for displaying printed music. These were formulated to encapsulate both the musical considerations of the documents, as well as some behavioural considerations of our target audience—musicologists and music researchers. For each consideration, we examined a number of existing systems used for displaying digital documents, musical or otherwise. By looking at these systems we were able to understand the current state of the art for displaying musical items, as well as discover interesting techniques to incorporate into our own implementation.  For a list of all the systems mentioned here, please refer to Table 1. \n3.1 Preserve Document Integrity  One of the most common methods for presenting pages in a digital library is as a series of images on separate web pages, with navigation elements such as ‘next’ and ‘previous’ links, drop-down menus or hyperlinked page numbers as the primary means of navigating through the item. This method of document display suggests an ‘image gallery’ metaphor, rather than representing the American Memory Project-Sheet:  Music from the Civil War Era http://memory.loc.gov/ammem/cwmhtml/cwmhome.html British Library “Turning the Pages” Project http://portico.bl.uk/onlinegallery/ttp/ttpbooks.html Chopin Early Editions http://chopin.lib.uchicago.edu Digital Image Archive of Medieval Music http://www.diamm.ac.uk Google Books http://books.google.com Inventions of Note Sheet Music Collection http://libraries.mit.edu/music/sheetmusic Juilliard Manuscript Collection http://www.juilliardmanuscriptcollection.org Lester S. Levy Sheet Music Collection http://levysheetmusic.mse.jhu.edu Neue Mozart Ausgabe http://dme.mozarteum.at  Schubert Manusckripte http://www.univie.ac.at/wwtf/schubert Sibley Music Library http://urresearch.rochester.edu/handle/1802/291 VARIATIONS Score Prototype http://www.dlib.indiana.edu/variations/scores World Digital Library http://wdl.org Table 1. Digital Music Libraries Examined \n4010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n      \nFigure 1. Google Books interface. The inner frame scrolls, with item metadata presented in the sidebar.   Figure 2. Side-by-side presentation of items in NMA. Musical material is presented on the left, and a critical report is on the right. cohesive original document as a single entity. To preserve this cohesiveness, one of our design goals was to implement a display metaphor that preserved the original document integrity. Google Books, the Neue Mozart Ausgabe (NMA), and the VARIATIONS prototype viewer provide interesting examples of this functionality. These systems present the items as a single, scrollable entity embedded within a frame on the web page. This allows users to scroll very quickly through the item without having to click ‘next’ and ‘back’ links and wait for the page to reload. A different technique was employed by the University of Illinois collection and the British Library’s “Turning the Pages” project. These presented their documents using a book metaphor where users could use the mouse to ‘turn’ the pages. As a navigation system this was largely a novelty and presented some usability challenges for turning one page or many pages simultaneously. However, these systems excelled at presenting an accurate picture of the original page and book layout, an especially important consideration for musical materials. 3.2 Allow Side-by-side Comparison of Items Musical documents can be divided into multiple physical items, with each item containing a portion of the complete musical work. Choir part books and orchestral instrument parts are common examples, but this can also extend to opera scores and libretti, early and later editions of a work, theory treatises and criticisms, adaptations, reductions, or various other modifications. It is not uncommon for scholars to need to consult multiple volumes for a single score. Two systems, the Digital Image Archive of Medieval Music (DIAMM) and the NMA, had the facility for displaying multiple items, but neither of them allowed multiple musical items to be displayed simultaneously. DIAMM displayed corresponding scans from entries in a printed RISM catalogue that was digitized, while the NMA displayed scans from a published critical report on that piece of music (see Figure 2).  3.3 Provide Multiple Page Resolutions When studying older manuscripts or printed works, the ability to view small details on a page, such as faint pencil markings or smudged note heads, can provide valuable information to the scholar. High-resolution images provides users with the ability to ‘zoom in’ on these markings, while lower resolution ones would allow them to move quickly through an entire document without having to navigate large pages. Most of the systems examined provide more than one size of image. Typically, they would provide three page image sizes: a ‘thumbnail’ view for quick selection and browsing, a ‘browser-safe’ view for fitting in a browser, and a ‘high resolution’ view for downloading, printing, or further detailed study.  In two cases, DIAMM and the World Digital Library (WDL) provided methods for smoothly zooming in and out from the page images. In the case of DIAMM, they used an Adobe Flash-based viewer called “Zoomify,” typically used for viewing high-resolution landscape photographs, while the WDL used a technology developed by Microsoft for their Photosynth viewer (see Figure 3). 3.4. Optimize Page Loading Showing multiple high-resolution document pages presents significant challenges for network and browsing speeds. Furthermore, we know that our target user base often works in environments such as small libraries, monasteries, churches, or in rural locations, where bandwidth can be at a premium. To address these issues, one of our design goals was to only display the  pages and areas of the page that the user was currently viewing. This would preclude the need to download an entire document of high-resolution page images if they \n41Oral Session 1-A: Knowledge on the Web\n   \nonly wished to consult a single page. As mentioned previously, Google Books uses an optimization technique that loads page images on demand. The Illinois Flip Book (beta) system seem to offer this as well, but at high zoom levels it required the user to download the whole high-resolution page image, slowing down the interaction. The viewing system for the Schubert Manuskripte library used a segmentation system to display single high-resolution scans of a single page. Each page image was broken into smaller image tiles that could be downloaded in parallel, theoretically speeding up the interaction. However, it seemed to use real-time image manipulation (e.g., re-sizing and rotating) on the server side, meaning that any speed optimization gained in parallel download was lost while the user waited for the server to recalculate the image. 3.5. Present Item and Metadata Simultaneously The catalogue record of a document often contains more information than is immediately available in the item itself or can serve to correct erroneous or outdated information on the item. For example, some compositions have been commonly attributed to the wrong composer, or their catalogue of works may have updated numbers. Although this seems like a small interface consideration, many implementations we examined would open images in the current or new window, replacing or obscuring the metadata and causing users to constantly flip between two browser windows or use the ‘back’ and ‘forward’ browser buttons to switch between item and item record. The reasons for this separation are varied. Some systems, e.g., Harvard, DIAMM, the University of Illinois, and Juilliard, used document presentation software separate from its catalogue to display the actual item. Other systems, such as the American Memory Project and the Levy Sheet Music Collection, separated the catalogue records and the navigation of the pages in the item on different web pages. Still others, such as the Sibley Music Library and the Inventions of Note collection, simply provided their items as PDFs to download. The Chopin collection offered a “tab” for switching between the score and the bibliographic interface, (Figures 4 and 5) but switching between the two did not maintain the users’ position in the score, reverting them to the view of the title page. Google Books and the VARIATIONS prototype feature a sidebar with some cataloguing information present, but the full catalogue entry was on a separate page. \nFigure 4. Bibliographic Description Tab in the Chopin Early Editions. \nFigure 5. View Score tab in the Chopin Early Editions. 4. CURRENT WORK While each design consideration we studied in our research can be found in several systems that we evaluated, our goal was to provide a system that would implement all of them. This viewer uses a number of technologies adopted from our examination of the existing solutions. Figure 6 shows a screenshot of our document viewer. The unified document display methods found in the Google Books and VARIATIONS systems has been adopted. It has been enhanced to allow users to scroll both vertically and horizontally through an item, based \nFigure 3. Zooming in on a manuscript in the World Digital Library \n4210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   on the page orientation of the item (vertical scrolling for items in portrait orientation, horizontal for items in landscape orientation).  For musical works with many physical items, panels in the viewer allow users to view one to four items simultaneously. These panels are synchronized so that the location in a score is maintained across all panels as the user scrolls through one panel. The synchronization is currently limited to movement or section indexing that has to be provided by hand by the cataloger. In order to provide multiple page resolution while keeping page loading optimized at high and very high resolutions (600 dpi or higher), the system uses a tiling mechanism that separate the images into small tiles, enabling it to serve only the displayed part of the document. By restricting the download to only the tiles that are needed by the user, we avoid the need to download the entire high-resolution image to view only a specific portion of an image. When combined with the unified document approach, this means that users can very quickly scroll through a document and zoom in on a specific page or set of pages without having to download the entire item. \nFigure 6. The Swiss RISM digital music document viewer. Three separate documents are displayed in panels in the middle of the page and can be scrolled vertically or horizontally. Document metadata appears in the lower-left panel. Finally, the simultaneous presentation of metadata was incorporated into the interface by employing a sidebar similar to the VARIATIONS prototype. This panel can be hidden and shown dynamically, allowing users to concentrate on viewing the item but giving them easy access to the full catalogue record without having to navigate to another page.  4.1. Technical aspects Ruby-on-Rails and MySQL provide the data storage on the server side. The client interface uses the ExtJS Javascript Framework [11].  For multiple page resolutions and optimized page loading, the system uses the IIP Image Server [12] tiling system. The image server separates large, high-resolution images into separate 256×256 pixel tiles and serves them on-demand. Javascript Object Notation (JSON) is used as a communication language between the database, tile server, and user interface. Client and server communication is performed asynchronously. From a user’s perspective, this means that there are very few page refreshes and performance approaches that of a native application instead a website.  To create new documents in this system, the images representing the page images are placed in a ZIP file and uploaded through the interface. These are then unzipped on the server side and processed using the VIPS image processing software [13] to create a pyramid TIFF [14] file that contains a sequence of images at increasingly coarse resolutions, representing zoom levels for these images (see Figure 7).  \nFigure 7. Pyramid TIFFs contain multiple resolutions of an image in a single file. When a user requests a document through their web browser, the interface translates this into a request for the images and zoom level of the pages currently visible in the viewer. The images are then served to the client as separate tiles and re-assembled as a page image in the interface. This is repeated for each page so users can scroll through an entire document at very high resolutions without waiting for the whole document to download.  The software and display techniques presented here will be incorporated into a modular digital library system currently under development. Each component of this system uses open-source software, and we will be releasing this system under an open-source MIT license, freely available for implementation in existing digital library systems. \n43Oral Session 1-A: Knowledge on the Web\n   5. FUTURE WORK The design considerations proposed here have not been verified by user testing. As part of our ongoing work on this software, we plan to study the impact of these design considerations on our target audience in real-world usage situations.  As part of the ongoing Swiss RISM project, the viewer interface will be integrated into the catalogue of Swiss musical works available online [15] as the documents are digitized. 6. CONCLUSION This paper introduces our work towards a viewer for digital music documents, taking into account the unique properties of printed music and the expectations of users who use these systems. We also expect that having an architecture designed specifically for music documents will be of great benefit in the long run as it should facilitate the integration of other information research technologies specific to music, such as content-based synchronization or online optical music recognition. 7. ACKNOWLEDGEMENTS This work was funded by the Swiss National Science Foundation and the Swiss Academy of Humanities and Social Sciences. REFERENCES [1] Byrd, D. 1984. Music notation by computer. PhD diss., Indiana University. [2] Thong, J., W. Hong, and K.Y. Tam. 2002. Understanding user acceptance of digital libraries: What are the roles of interface characteristics, organizational context, and individual differences? International Journal of Human-Computer Studies 57 (3): 215–42. [3] Kurosu, M., and K. Kashimura. 1995. Apparent usability vs. inherent usability: Experimental analysis on the determinants of the apparent usability. In Proceedings of the Conference on Human Factors in Computing Systems, 292–93. [4] Tractinsky, N., A. Katz, and D. Ikar. 2002. What is beautiful is usable. Interacting with Computers 13 (2): 127–45. [5] Norman, D. 2002. Emotion & design: Attractive things work better. Interactions 9 (4): 36–42. [6] Ashby, F., A. Isen, and U. Turken. 1999. A neuropsychological theory of positive affect and its influence on cognition. Psychological Review 106 (3): 529–50. [7] Byrd, D., and T. Crawford. 2002. Problems of music information retrieval in the real world. Information Processing and Management 38 (2): 249–72. [8] Byrd, D., and E. Isaacson. 2003. Music representation in a digital music library. In Proceedings of the Joint Conference on Digital Libraries, 234–36. [9] Fuhrman, M., D. Gauthier, and A. Dillon. 2001. Usability test of Variations and DML prototypes. http://variations2.indiana.edu/pdf/VariationsTest.pdf [Accessed 20 May 2009]. [10] Fremerey, C. 2006. SyncPlayer: A framework for content-based music navigation. Diplomarbeit, Univ. of Bonn. [11] ExtJS Framework. http://extjs.com/ [Accessed 9 August 2009]. [12] Pillay, R., and D. Pitzalis. 2008. IIP Image Server. http://iipimage.sourceforge.net [Accessed 20 May 2009]. [13] Cupitt, J., and K. Martinez. 1996. VIPS: An image processing system for large images. In Proceedings of the Society of Photo-Optical Instrumentation Engineers (SPIE) 2663 (19): 19–28. [14] Library of Congress. 2009. TIFF, Pyramid. http://www.digitalpreservation.gov/formats/fdd/fdd000237.shtml [Accessed 22 May 2009]. [15] RISM Switzerland Online Catalogue. http://www.rism-ch.org/ [Accessed 9 August 2009]. \n44"
    },
    {
        "title": "From Low-Level to Song-Level Percussion Descriptors of Polyphonic Music.",
        "author": [
            "Martín Haro",
            "Perfecto Herrera"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417201",
        "url": "https://doi.org/10.5281/zenodo.1417201",
        "ee": "https://zenodo.org/records/1417201/files/HaroH09.pdf",
        "abstract": "We address here the automatic description of percussive events in real-world polyphonic music. By taking a pattern recognition approach we evaluate more than 2,450 objectlevel features. Three binary instrument-wise support vector machines (SVM) are built from a training set of more that 100 songs and 10 genres. Then, we use these binary models to build a drum transcription system achieving comparable results with state of the art algorithms. Finally, we present 17 song-level percussion descriptors computed from the imperfect output of the transcription algorithm. We evaluate the usefulness of the proposed descriptors in music information retrieval (MIR) tasks like genre classification, danceability estimation and Western vs. nonWestern music discrimination. We conclude that the presented song-level percussion descriptors provide complementary information to “classic” descriptors, that can help in the previously mentioned MIR tasks.",
        "zenodo_id": 1417201,
        "dblp_key": "conf/ismir/HaroH09",
        "keywords": [
            "percussive events",
            "polyphonic music",
            "pattern recognition",
            "object-level features",
            "binary support vector machines",
            "drum transcription",
            "song-level descriptors",
            "music information retrieval",
            "genre classification",
            "danceability estimation"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFROM LOW-LEVEL TO SONG-LEVEL PERCUSSION\nDESCRIPTORS OF POLYPHONIC MUSIC\nMart ´ın Haro, Perfecto Herrera\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n{martin.haro,perfecto.herrera }@upf.edu\nABSTRACT\nWe address here the automatic description of percussive\nevents in real-world polyphonic music. By taking a pattern\nrecognition approach we evaluate more than 2,450 object-\nlevel features. Three binary instrument-wise support vec-\ntor machines (SVM) are built from a training set of more\nthat 100 songs and 10 genres. Then, we use these bi-\nnary models to build a drum transcription system achieving\ncomparable results with state of the art algorithms. Finally,\nwe present 17 song-level percussion descriptors computed\nfrom the imperfect output of the transcription algorithm.\nWe evaluate the usefulness of the proposed descriptors in\nmusic information retrieval (MIR) tasks like genre clas-\nsiﬁcation, danceability estimation and Western vs. non-\nWestern music discrimination. We conclude that the pre-\nsented song-level percussion descriptors provide comple-\nmentary information to “classic” descriptors, that can help\nin the previously mentioned MIR tasks.\n1. INTRODUCTION\nDuring the last decade the interest in the transcription of\npercussive instruments has grown and most of the work has\nfocused on the problem of drum1transcription [1]. The\naim of such systems is to obtain, from an audio signal, a\nrepresentation of the type of percussion instrument played\n(instrument recognition), and when it has been played (tem-\nporal location).\nThe transcription of isolated or polyphonic drum sounds\n(i.e. without concurrent pitched sounds) can be considered\na practically solved problem (e.g. see [2]). However, the\nautomatic transcription of percussive events in polyphonic\nmusic is an open problem where there is still a lot of room\nfor improvement.\nInstead of focusing on a full transcription system, we\nconsider that, when MIR of polyphonic music is addressed,\nan automatic music “description” approach should be taken.\n1The word “drum” refers to a standard Rock/Pop drum kit found in\nWestern music.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.The main idea behind such an approach is to obtain “predi-\ncates” or labels that apply to a given music excerpt and usu-\nally this information goes beyond traditional music scores.\nIn this paper we present and evaluate several song-level\npercussion descriptors extracted from the output of an im-\nperfect transcription system. The aim of these descrip-\ntors is to semantically describe general characteristics of\nwithin-song drum events such as drum-instrument degree\nof presence, drum-instrument relationships (i.e. inter-ins-\ntrument ratios) and most-frequent inter-instrument inter-\nvals. Finally, we explore the usefulness of the proposed\ndescriptors for some MIR tasks such as genre classiﬁca-\ntion, danceability estimation and Western vs. non-Western\nmusic classiﬁcation.\nThe paper is organized as follows: An overview on per-\ncussion transcription of polyphonic music is presented in\nsection 2. In section 3 “full” and “relaxed” transcription\nsystems are described. Next, song-level percussion de-\nscriptors are proposed and evaluated within several MIR\ntasks (section 4). Finally, section 5 presents some conclu-\nsions.\n2. RELATED WORK\nMost of the works on transcription of percussive events in\npolyphonic music have focused on transcription of drum\nkit sounds, specially on bass drum (BD), snare drum (SD)\nand hi-hat (HH) sounds. In Table 1 a summary of the most\nrelevant works on drum transcription in polyphonic mu-\nsic is presented. It is worth to notice here that the pre-\nsented results can not be directly compared since they were\nnot evaluated on the same dataset. Sandvold et al. [3]\nused a combination of general and localized sound mod-\nels. The correct classiﬁed instances obtained from the gen-\neral model (C4.5 with AdaBoost) were manually parsed\nto a localized training set. Yoshii et al. [4] achieved the\nbest results in the MIREX 20052audio drum detection\ncontest by using matching template spectrograms. The\nmain idea here was to obtain a template spectrogram rep-\nresentation of a particular percussion instrument from a\nlarge training database of sounds. Thus, when analyzing a\nsong, a template-adaptation algorithm was applied on ev-\nery onset. A distance measure for template matching was\nused to try to minimize the spectral overlapping of other\nsounds. Dittmar’s [5] system was also evaluated within\n2See MIREX web site: http://www.music-ir.org/evaluation/mirex-\nresults/audio-drum/index.html for more information.\n243Poster Session 2\nAuthors # songs Approach Main Algorithms Overall BD SD HH\nSandvold et al. [3] 25 Patt-Rec local. model 0.924 0.951 0.931 –\nYoshii et al. [4] 50 Sp. Temp Temp. match. 0.670 0.728 0.702 0.574\nDittmar [5] 50 S. Sep+Sp. Temp NN ICA 0.588 0.606 0.581 0.585\nTanghe et al. [6] 50 Patt-Rec SVM 0.615 0.688 0.555 0.601\nGillet and Richard [7] 20 Patt-Rec SVM+local. model 0.840 0.824 0.842 –\nPaulus and Klapuri [8] 45 Patt-Rec HMM 0.697 0.795 0.655 0.660\nGillet and Richard [9] 28 S. Sep+Patt-Rec SVM 0.678 0.695 0.583 0.755\nTable 1 .Summary of drum transcription systems in polyphonic music. Almost all works classify bass drum (BD), snare drum (SD) and\nhi-hat (HH) sounds, except for Gillet and Richard [7] (where only BD and SD were detected) and Sandvold et al. [3] (where BD, SD and\nCymbal were computed).“Approach” considers Pattern Recognition (Patt-Rec), Source Separation (S. Sep), and Spectral Template (Sp.\nTemp). F-measures results (except for Sandvold et al. where accuracy was measured). Since different datasets were used, results can not\nbe directly compared.\nthe MIREX contest. It combined source separation (Non-\nNegative ICA) with template matching algorithms. Tanghe\net al. [6], another MIREX participant, presented an onset-\nbased classiﬁcation system using N-binary SVM as recog-\nnition algorithm (being Nthe number of instruments to\ndetect). Gillet and Richard [7] also used N-binary SVM\nas classiﬁcation algorithm. This system performed a band-\nwise harmonic/noise decomposition as pre-processing step\nto enhance the presence of unpitched instruments. A lo-\ncalized adapted model like the one presented in [3] was\nalso evaluated. Paulus and Klapuri [8] presented an evalu-\nation using Hidden Markov Models with a combination of\nspectral features and temporal descriptors calculated from\nlong narrow-band frames. In a recent work by Gillet and\nRichard [9] a combination of source-separation and pattern-\nrecognition algorithms was proposed. Two set of features\nwere computed, one from the original audio signal and the\nother from a “drum enhanced” track obtained by source\nseparation. These feature vectors were then classiﬁed by\nthree binary SVM.\nAs can be seen from the literature review, there is still a\nlot of room for improvement in the problem of drum tran-\nscription in polyphonic music. But, instead of pursuing\na perfect transcription system, we decided to explore the\npotential of song-level percussion descriptors to describe\nreal-world music. In order to achieve that we implemented\na simple drum transcription system. This system could be\nused later as a baseline for more complex implementations\n(e.g. based on source separation or harmonic/noise decom-\nposition). Thus, we chose to follow a standard pattern-\nrecognition approach, trained on a large set of sounds and\naudio features. The potential of this kind of basic system\nto describe percussive events in polyphonic music was not\npreviously assessed.\n3. TRANSCRIPTION EXPERIMENTS\n3.1 Datasets\nWe used three song collections with proper annotations of\npercussive events. Two of them are publicly available and\nwere used in previous studies on drum transcription.\nENST-Drums database [10]: This is the largest pub-\nlicly available drum database. Since we wanted to detectdrum events in “real” music, we decided to mix the pro-\nvided “wet” drums and their accompaniment tracks with-\nout further changes on amplitude (-6dB drum level). From\nthe obtained collection of 64 songs we randomly selected\n30 seconds excerpts of each song and their labels.\nMAMI database [11]: This database is a collection of\n52 music fragments (30 seconds length) extracted from\ncommercial CDs. We managed to gather 48 songs and\naligned them with the provided annotations. This database\nwas one of the three databases used in the MIREX 2005\naudio drum detection contest.\nIn-house database [12]: This is a database of 30 anno-\ntated music excerpts (20 seconds length), extracted from\ncommercial CDs. Since HH events were not speciﬁcally\nannotated, we only used the BD and SD labels.\nDue to the number of instances and the musical impor-\ntance of each instrument within the drum kit we decided to\nwork with the following instruments classes: BD, SD and\nHH (including open and closed HH sounds). Finally, we\nobtained a large set of polyphonic music excerpts adding\nup a total of 142 songs labeled with three, possibly con-\ncurrent, tags. The ﬁnal number of instances per instrument\nwas: 6,407 for BD, 5,655 for SD and 8,400 for HH.\n3.2 Descriptors\nFirst, we computed a set of frame-level descriptors (frame-\nsize of 46 ms, hop-size of 12 ms) namely: temporal de-\nscriptors (zero-crossing rate and lpc coefﬁcients), spectral\ndescriptors (e.g. centroid, complexity, crest, decrease, dis-\nsonance, energy, ﬂatness, ﬂux, kurtosis, pitch, rms, rolloff,\nskewness, spread, strong-peak), perceptual descriptors\n(MFCCs, Bark-bands and Bark-bands kurtosis, skewness\nand spread) and tonal descriptors (Harmonic Pitch Class\nProﬁle). See [13] and [14, p. 20] for an overview on these\ndescriptors.\nAfter this ﬁrst step we computed a set of object-level\ndescriptors3from the time series of each frame-level de-\nscriptor (about 12 frames per object). The computed object-\nlevel descriptors were:\n3The term “object” is considered here as: every sound event starting\nfrom an onset and ﬁnishing 150 ms after (or in the next onset if this new\nonset falls within the 150 ms interval).\n24410th International Society for Music Information Retrieval Conference (ISMIR 2009)\na) Amplitude-related object descriptors: mean, variance,\nminimum, maximum, skewness and kurtosis.\nb) Time-related object descriptors: temporal skewness,\ntemporal kurtosis, temporal centroid, max. and min. nor-\nmalized position (normalized temporal position of the max-\nimum, or minimum, value of the time series), slope (arct-\nangent of the slope of the linear regression of the data), at-\ntack and decay (slope descriptor from initial, or end, point\nto the maximum point) and amplitude-normalized attack\nand decay.\nAt the end of this process we obtained about 2,400 de-\nscriptors for every sound object. See [14, p. 46] for a de-\ntailed explanation on the computed descriptors.\n3.3 Model training\nSince we were working with three drum categories that\ncan occur at the same time, we decided to train N-binary\n(SVM) classiﬁers instead of one model with 2Npossible\nclasses. In this context we have each trained model in\ncharge of detecting the presence or absence of one partic-\nular instrument (e.g. SD or not-SD).\nIn order to have a more representative database for train-\ning purposes we mixed the ENST and the MAMI databases.\nTaking into account that the ﬁnal system has to label pre-\ndetected onsets we decided to train our models with la-\nbeled onsets. Thus, we performed an onset detection (by\nusing an implementation of the onset algorithm proposed\nby Brossier in [15]) and we assigned the corresponding la-\nbels to every detected onset. Finally we split the database\nleaving 90% for training and reserving 10% to be used as\nindependent test set. We called these databases 90%MIX\nand 10%MIX. The In-House database was also reserved as\nsecond independent testing set.\nTo build the SVM models we ﬁrst used the correla-\ntion based feature selection (CFS) [16] algorithm in 10-\nfold cross-validation (CV) to identify the most informa-\ntive object-level descriptors from the 90%MIX database.\nWe chose only those descriptors selected in all CVs (i.e.\n10 times) obtaining 56 relevant descriptors for BD (e.g.\nlow Bark-bands, MFCCs, spectral-energy low and spec-\ntral ﬂux), 77 for SD (e.g. mid Bark-bands, temporal lpc,\nMFCCs and spectral ﬂatness ) and 38 for HH (e.g. high\nBark-bands, temporal lpc, MFCCs, spectral spread and\nspectral ﬂatness). Then, we trained the SVM models with\nthe selected descriptors of the 90%MIX database and eval-\nuated their performance using 10-fold CV . We also ap-\nplied these models to the testing sets (i.e. 10%MIX and\nIn-House). The classiﬁcation results for every labeled in-\nstance and every model (after a grid search of SVM pa-\nrameters) can be seen in Table 2. We obtained averaged\nF-measure results of 0.806 and 0.782 for the training and\ntesting sets respectively.\n3.4 Full transcription\nSince up to this step we had worked only with labeled\nonsets, the next step was to evaluate the learned models\nagainst all the ground truth labels in the datasets. In order\nto do that we implemented a complete drum transcriptionInstrument Model 90%MIX In-House 10%MIX\nbass drum 0.834 0.812 0.835\nsnare 0.778 0.687 0.773\nhi-hat 0.806 — 0.802\nTable 2 .F-measure classiﬁcation results after grid search of\nSVM parameters. Models were trained with 90%MIX database.\nResults were evaluated using 10-fold CV on each dataset.\nsystem. The three previously described databases were an-\nalyzed (ENST, MAMI and In-house) adding up a total of\n142 songs (20 to 30 seconds length).\nThe experiment set-up for evaluating the transcription\ncapabilities of our system was as follows: a) Perform an\nonset detection on the audio excerpts (we used the same\nonset detector as in the model training step). b) Compute\nthe descriptors used by each model on every onset plus\n150 ms (or until the next onset). c) Apply the models to\nevery set of descriptors to obtain the predicted labels. d)\nEvaluate the predicted results against the ground truth an-\nnotations (as in the MIREX 2005 contest, a range of ±30\nms from the true times was allowed). After evaluating all\n142 song excerpts, we obtained an overall result of 0.659\n(F-measure) and per instrument F-measure results of 0.699\nfor BD, 0.652 for SD and 0.626 for HH. If we compare our\nsystem with the fully automatic systems described in sec-\ntion 2 (i.e. [4, 6, 8, 9]) we can see that our system obtained\nnear state-of-the-art drum transcription performance with a\nquite simple pattern recognition algorithm. Nevertheless,\nthese performances are still far from reliable transcriptions.\n3.5 Relaxed Transcription\nTaking into account that state-of-the-art algorithms are still\nfar from yielding perfect transcriptions and that our ﬁnal\ngoal was to derive song-level percussion descriptors, we\ndecided to evaluate the capacity of our transcription sys-\ntem to estimate the total number of drum events in a song\n(e.g. how many BD, SD or HH strikes a particular song\nhas). These descriptors could be used to characterize a\nsong as having, for example, a lot of SD, no HH, etc.,\nhence they contribute to bridge the semantic gap [17]. In\nthis experiment we considered as a “correct” decision the\ntotal number of instrument instances (e.g. HH events) in\nthe whole audio ﬁle discarding time-information4. Using\nthe same datasets as in the full transcription experiments\nwe obtained, as expected, better classiﬁcation performance\n(F-measure) for all classes (BD = 0.822, HH = 0.794 and\nSD = 0.698). The overall performance of this “relaxed”\ntranscription system was 0.771 (F-measure). These results\nencouraged us to investigate if useful song-wise percussion\ndescriptors could be computed.\n4We deﬁne correct transcription (CTR) as the arg min( TR, GT ), be-\ningTR=transcription and GT=ground truth labels per instrument.\nThen, we compute P=CTR/TR andR=CTR/GT and ﬁnally\nF= 2PR/(P+R).\n245Poster Session 2\n4. SONG-LEVEL PERCUSSION DESCRIPTORS\n4.1 Computed Descriptors\nIn [12] and [18] two percussion-related descriptors were\npresented and evaluated with promising results namely:\nPercussion Index (a ratio between the total number of de-\ntected percussion events and the number of detected on-\nsets) and Percussion Proﬁle (the relative amount of BD,\nSD, cymbals, and non-percussion events normalized by the\ntotal number of onsets). Following this idea of percussion\nrelated descriptors we decided to compute and evaluate the\nfollowing song-level percussion descriptors (some of these\ndescriptors appeared as suggested future work in [12] but,\nup to our knowledge, they have not been implemented nor\nevaluated yet).\nComputed song-level percussion descriptors:\n•Percussion Proﬁle: The ratio between the number\nof detected percussion events and the number of de-\ntected onsets [18]. Computed for BD, SD, HH and\ndrum (D)5(e.g. BD/total, SD/total).\n•Inter-Instrument Ratio: The ratio among all per-\ncussive instrument events namely: BD/SD, BD/HH\nand SD/HH.\n•Instrument Per Minute: The number of detected\nevents per minute for BD, SD, HH and D.\n•Inter-Instrument Interval (iii): The ﬁrst and sec-\nond peak values of the histogram of the differences\nbetween successive events. Thus, we computed: ﬁrst\nand second-iii-peak for BD, SD and HH.\nAt the end of this process we obtained 17 song-level per-\ncussion descriptors for each song.\n4.2 Evaluation\nTo investigate the correlation between the proposed per-\ncussion descriptors and the ground truth values we com-\nputed the percussion descriptors both from the ground truth\nlabels (labeled onsets) and from the output of our tran-\nscription system. Then, we built a fractional ranking6for\neach descriptor (for every song) and computed the Pear-\nson’s correlation coefﬁcient between both rankings. The\ncorrelation results showed large correlation values (i.e. >\n0.5) for 12 out of 17 proposed descriptors (only: BD/SD,\nsecond-iii-peak for the three instruments and ﬁrst-iii-peak\nfor HH presented correlation values below 0.5). These\nhighly correlated values between our descriptors and de-\nscriptors computed from the ground truth labels were spe-\ncially strong (i.e. >0.7) for D/total, D/min, HH/total and\nHH/min. These results suggest that the proposed descrip-\ntors have potential to describe the percussive content of a\nsong.\n5In this context “drum” means the number of detected onsets labeled\nby the system as BD, SD or HH.\n6If the ordered vector to rank is A,B,C,D and B is equal to C (i.e. tie)\nthe fractional ranking assigns the same mean position value in both cases,\ni.e. 1,2.5,2.5,4.\nFigure 1 .Genre classiﬁcation results per genre and descriptor\nset. F-measure after 10-fold CV .\nNext, we evaluated the usefulness of the percussion de-\nscriptors as features in several MIR tasks such as genre\nand sub-genre classiﬁcation, danceability7and Western\nvs. non-Western music estimation. We decided to set-up\na general methodology for evaluating the song-level per-\ncussion descriptors on every selected MIR task. Firstly,\nwe computed, for each song in the dataset, the mean value\nof a set of “standard” descriptors to be used as baseline\nfor the evaluation. Secondly, we computed the proposed\nsong-level percussion descriptors on the same database.\nThirdly, we selected a classiﬁcation algorithm and we de-\ntermined the “best” classiﬁcation values for the “standard”,\n“percussion” and “standard + percussion” descriptor sets.\nFinally, we evaluated the classiﬁcation results by compar-\ning F-measures and performing a Binomial test [20, p. 37]\nwith 5% signiﬁcance level (i.e. α= 0.05). This Binomial\ntest determines if the difference between correctly classi-\nﬁed songs for each descriptor set is statistically signiﬁcant\nor not. It is worth to notice that none of the songs used\nfor training the SVM models were used in these evaluation\nexperiments.\n4.2.1 Genre\nFor genre classiﬁcation we used an in-house database of\n30 seconds excerpts extracted from 350 songs equally dis-\ntributed among 7 genres: classic, dance, hip-hop, jazz, pop,\nr&b and rock. The computed “standard” descriptors were:\nBark-bands, Bark-bands kurtosis, Bark-bands skewness,\nBark-bands spread, spectral centroid, spectral crest, spec-\ntral decrease, spectral dissonance, spectral energy, spec-\ntral energy-band high, spectral energy-band low, spectral\nenergy-band middle high, spectral energy-band middle low,\nspectral ﬂatness, spectral ﬂux, spectral hfc, spectral kurto-\nsis, MFCCs, spectral skewness, spectral spread and tempo-\nral zero-crossing rate. We called “timbral” descriptors this\nset of 60 features. We used multi-class SVM as classiﬁca-\ntion algorithm.\nThe genre classiﬁcation results can be seen in Figure 1.\nFrom these results it is interesting to notice that by using\nthe percussion descriptors only, good discrimination rates\n7The easiness with which one can dance to a musical track [19].\n24610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nGenre T P T+P\nambient 0.531 0.433 0.588\ndrum’n bass 0.475 0.619 0.576\nhouse 0.200 0.500 0.427\ntechno 0.369 0.269 0.380\ntrance 0.438 0.566 0.427\nAverage 0.403 0.478 0.480\nTable 3 .Electronic sub-genre classiﬁcation. T = timbral, P\n= percussion and T+P = timbral+percussion. C4.5 classiﬁcation\nalgorithm. Results in F-measure after 10-fold CV .\ncan be achieved for classic ( F= 0.818), rock, dance and\nhip-hop ( F≈0.600) genres. The overall classiﬁcation\nfor the percussion-only data set was about 12 percentage\npoints (pp) below “timbral” descriptors. When combining\n“timbral” and “percussion” descriptors a small improve-\nment in the overall result was observed (+2.1 pp). It is\nworth to notice that big improvements were produced in\ndance (+12.7 pp) and pop (+15 pp) results, whereas results\nfor rock and r&b decreased 5.6 and 4.4 pp respectively.\nThe Binomial test showed no statistically signiﬁcant dif-\nference between “timbral + percussion” and “timbral” de-\nscriptors ( p= 0.1932 ), but both sets evidenced signiﬁ-\ncant differences with the “percussion” descriptor set ( p <\n0.0001 in both cases).\n4.2.2 Electronic\nFor electronic sub-genre classiﬁcation we performed our\nexperiments on an in-house database of 270 songs (30 sec-\nonds length each) equally distributed among the following\ngenres: ambient, drum’n bass, house, techno and trance.\nThe computed descriptors were the same as in the genre\nexperiment. Given that sub-genre and genre classiﬁcation\ncould be considered as very related tasks, we decided to try\na different algorithm to gain some insight on the descrip-\ntors. Therefore, we used in this case the C4.5 decision tree\nalgorithm for classiﬁcation, since its output can be easily\nsummarized into interpretable trees of descriptors.\nResults for electronic sub-genre classiﬁcation are de-\npicted in Table 3. In this experiment we observed that clas-\nsiﬁcation results obtained by the “percussion” set outper-\nformed “timbral” descriptors by 7.5 pp. The combination\nof “timbral” and “percussion” descriptors showed no sig-\nniﬁcant difference with results from percussion-only de-\nscriptors in the overall classiﬁcation result (although this\ncombination seems to output more balanced classiﬁcation\nrates among categories). In both “percussive” and “tim-\nbral + percussive” models the D/total and ﬁrst and second\niii-peak BD were the most informative descriptors.\nThe signiﬁcance test corroborates the conclusions ex-\ntracted from the F-measure results where “percussion” de-\nscriptors performed signiﬁcatively better than “timbral” de-\nscriptors ( p= 0.0028 ), “timbral + percussion” performed\nbetter than “timbral” ( p= 0.0058 ) and no statistical dif-\nference between “percussion” and “percussion + timbral”\ndescriptor sets was appreciated ( p= 0.4273 ).\nFigure 2 .Danceability classiﬁcation results after 10-fold CV .\n4.2.3 Danceability\nFor danceability tests we used an in-house database of 374\nsong excerpts of 30 seconds equally distributed into three\nclasses (i.e. non-dance., mid-dance. and high-dance.). We\ncomputed the same descriptors as in the genre experiment\nplus an estimation on the beats per minute (bpm) of the\nsong8, we called this descriptor set as “timbral + bpm”.\nAs in the genre experiments we decided to use the SVM\nalgorithm (multi-class).\nResults for Danceability tests are shown in Figure 2.\nFrom these results we can conclude that “percussion” de-\nscriptors performed better than both “timbral + bpm” and\n“timbral + bpm + percussion”. Percussion-only descrip-\ntors outperformed by 8.9 pp and 7.4 pp “timbral + bpm”\nand “timbral + bpm + percussion” respectively, obtaining\nbetter results in all three categories. It is interesting to no-\ntice that percussion descriptors also outperformed obtained\nresults by [19] which achieved an accuracy of 61.78% in\nclassifying 225 songs into the same three categories by us-\ning a different and more complex approach.\nThe Binomial test on danceability results showed that\n“percussion” descriptors provided signiﬁcatively better per-\nformance than the other two sets ( p= 0.0025 for “timbral\n+ bpm” and p= 0.0074 for “timbral + bpm + percus-\nsion”). The test also showed no statistical difference be-\ntween “timbral + bpm” and “timbral + bpm + percussion”\ndescriptor sets ( p= 0.3793 ).\n4.2.4 Western vs. non-Western music classiﬁcation\nFor Western vs. non-Western experiments we used an in-\nhouse database of 139 Western songs from 16 genres in-\ncluding classical, jazz, rock, pop, religious and hip-hop,\nand 139 non-Western songs including songs from Africa,\nArab States, Asia and the Paciﬁc. The computed descrip-\ntors and classiﬁcation algorithm were the same as in genre\nexperiments.\nThe results for these experiments are shown in Table\n4. Here we observed an almost linear increment in the\nclassiﬁcation rates starting by “timbral” descriptors with\n8Since bpm is an important descriptor for danceability estimation we\nincluded it into the “standard” set. Otherwise, it would be too easy for\nour descriptors to outperform.\n247Poster Session 2\nClass T P T+P\nWestern 0,817 0,803 0,856\nnon-Western 0,747 0,828 0,833\nAverage 0,782 0,816 0,844\nTable 4 .Western vs. non-Western music classiﬁcation. T =\ntimbral, P = percussion and T+P = timbral+percussion. SVM\nclassiﬁcation algorithm, F-measure results after 10-fold CV .\nF= 0.782followed by “percussion” descriptors with F=\n0.816(+3.4 pp) and “timbral + percussion” with F= 0.844\n(+2.8 pp from “percussion”). It seems clear that adding\npercussion descriptors helped in the process of Western\nvs. non-Western song discrimination. Is is also interesting\nto notice that classiﬁcation results for non-Western music\nwere much better when percussion descriptors were used\n(more than 8 pp above “timbral”).\nThe signiﬁcance test showed no statistically signiﬁcant\ndifference between “percussion + timbral” and “percus-\nsion” descriptors ( p= 0.1212 ) and between “percussion”\nand “timbral” descriptors ( p= 0.1349 ). The test also\ndepicted statistical difference between “percussion + tim-\nbral” and “timbral” descriptors ( p= 0.0096 ). See [21]\nfor an in-depth study on Western vs. non-Western music\nclassiﬁcation.\n5. CONCLUSIONS\nWithin the present work we have conducted several exper-\niments in order to detect and describe percussive events in\npolyphonic music. Firstly, we built, by combining three\ndatabases, a large set of percussion-labeled songs. Sec-\nondly, we evaluated the capacity of an automatic drum\ntranscription system, based on object-level features and\nthree binary SVM models, to transcribe percussion events\nin polyphonic music. From the transcription results we\nextrapolated that our relatively simple algorithm can be\nplaced among the top ranked ones, even though all these\nsystems leave a lot of room for improvement. After per-\nforming “relaxed” transcription experiments we observed\nthat our system can detect the total number of drum events\nin a song with an overall F-measure of 0.771. Finally,\nwe presented 17 song-level percussion descriptors and we\nevaluated their usefulness among several MIR tasks. These\npreliminary results suggest that song-level percussion (i.e.\n“semantic”) descriptors, even though they are based on im-\nperfect transcriptions, can help in MIR tasks such as genre\nand sub-genre classiﬁcation, danceability and Western vs.\nnon-Western music estimation. It also seems clear that\nsong-level percussion descriptors offer useful information\nthat complements the one provided by classic “spectral”\nand “timbral” descriptors. This new information could also\nbe exploited in music similarity tasks.\n6. REFERENCES\n[1] D. Fitzgerald and J. Paulus. Unpitched Percussion Transcrip-\ntion, chapter 5, pages 131–162. Signal Processing Methods\nfor Music Transcription. Springer, 2006.[2] P. Herrera, A. Dehamel, and F. Gouyon. Automatic labeling\nof unpitched percussion sounds. In Proc. of AES 114th , Am-\nsterdam, The Netherlands, 2003.\n[3] V . Sandvold, F. Gouyon, and P. Herrera. Percussion classiﬁ-\ncation in polyphonic audio recordings using localized sound\nmodels. In Proc. of ISMIR , pages 537–540, Barcelona, Spain,\n2004.\n[4] K. Yoshii, M. Goto, and H. G. Okuno. Adamast: A drum\nsound recognizer based on adaptation and matching of spec-\ntrogram. In MIREX , London, UK, 2005.\n[5] C. Dittmar. Drum detection from polyphonic audio via de-\ntailed analysis of the time frequency domain. In MIREX , Lon-\ndon, UK, 2005.\n[6] K. Tanghe, S. Degroeve, and B. De Baets. An algorithm for\ndetecting and labeling drum events in polyphonic music. In\nMIREX , London, UK, 2005.\n[7] O. Gillet and G. Richard. Drum track transcription of poly-\nphonic music using noise subspace projection. In Proc. of IS-\nMIR, pages 92–99, London, UK, 2005.\n[8] J. Paulus and A. Klapuri. Combining temporal and spectral\nfeatures in hmm-based drum transcription. In Proc. of ISMIR ,\npages 225–228, Vienna, Austria, 2007.\n[9] O. Gillet and G. Richard. Transcription and separation of\ndrum signals from polyphonic music. IEEE Transactions on\nAudio, Speech, and Language Processing , 16(3):529–540,\n2008.\n[10] O. Gillet and G. Richard. ENST-Drums: an extensive audio-\nvisual database for drum. In Proc. of ISMIR , pages 156–159,\nVictoria, Canada, 2006.\n[11] K. Tanghe, M. Lesaffre, S. Degroeve, M. Leman, B. De\nBaets, and J. Martens. Collecting ground truth annotations\nfor drum detection in polyphonic music. In Proc. of ISMIR ,\npages 50–57, London, UK, 2005.\n[12] V . Sandvold. Percussion descriptors. A semantic approach\nto music information retrieval. Master’s thesis, University of\nOslo, 2004.\n[13] G. Peeters. A large set of audio features for sound description\n(similarity and classiﬁcation) in the cuidado project. Techni-\ncal report, CUIDADO project, 2004.\n[14] M. Haro. Detecting and describing percussive events in poly-\nphonic music. Master’s thesis, Universitat Pompeu Fabra,\n2008.\n[15] P. Brossier. Automatic annotation of musical audio for inter-\nactive systems . PhD thesis, Queen Mary University of Lon-\ndon, 2006.\n[16] M. A. Hall. Correlation-based feature selection for discrete\nand numeric class machine learning. In Proc. 17th Interna-\ntional Conf. on Machine Learning , pages 359–366, San Fran-\ncisco, USA, 2000.\n[17] O. Celma, P. Herrera, and X. Serra. Bridging the music se-\nmantic gap. In Workshop on Mastering the Gap: From Infor-\nmation Extraction to Semantic Representation , volume 187,\nBudva, Montenegro, 2006. CEUR.\n[18] P. Herrera, V . Sandvold, and F. Gouyon. Percussion-related\nsemantic descriptors of music audio ﬁles. In Proc. of AES\n25th, London, UK, 2004.\n[19] S. Streich and P. Herrera. Detrended ﬂuctuation analysis of\nmusic signals danceability estimation and further semantic\ncharacterization. In Proc. AES 118th , Barcelona, Spain, 2005.\n[20] P. H. Kvam and B. Vidakovic. Nonparametric Statistics with\nApplications to Science and Engineering . Wiley, 2007.\n[21] E. G ´omez, M. Haro, and P. Herrera. Music and geography:\nContent description of musical audio from different parts of\nthe world. In Proc. of ISMIR , Kobe, Japan, 2009.\n248"
    },
    {
        "title": "Musical Instrument Recognition in Polyphonic Audio Using Source-Filter Model for Sound Separation.",
        "author": [
            "Toni Heittola",
            "Anssi Klapuri",
            "Tuomas Virtanen"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417377",
        "url": "https://doi.org/10.5281/zenodo.1417377",
        "ee": "https://zenodo.org/records/1417377/files/HeittolaKV09.pdf",
        "abstract": "This paper proposes a novel approach to musical instrument recognition in polyphonic audio signals by using a source-filter model and an augmented non-negative matrix factorization algorithm for sound separation. The mixture signal is decomposed into a sum of spectral bases modeled as a product of excitations and filters. The excitations are restricted to harmonic spectra and their fundamental frequencies are estimated in advance using a multipitch estimator, whereas the filters are restricted to have smooth frequency responses by modeling them as a sum of elementary functions on the Mel-frequency scale. The pitch and timbre information are used in organizing individual notes into sound sources. In the recognition, Mel-frequency cepstral coefficients are used to represent the coarse shape of the power spectrum of sound sources and Gaussian mixture models are used to model instrument-conditional densities of the extracted features. The method is evaluated with polyphonic signals, randomly generated from 19 instrument classes. The recognition rate for signals having six note polyphony reaches 59%.",
        "zenodo_id": 1417377,
        "dblp_key": "conf/ismir/HeittolaKV09",
        "keywords": [
            "novel approach",
            "source-filter model",
            "sound separation",
            "polyphonic audio signals",
            "excitations",
            "filters",
            "pitch and timbre information",
            "Mel-frequency cepstral coefficients",
            "Gaussian mixture models",
            "recognition rate"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMUSICAL INSTRUMENT RECOGNITION IN POLYPHONIC AUDIO\nUSING SOURCE-FILTER MODEL FOR SOUND SEPARATION\nToni Heittola, Anssi Klapuri and Tuomas Virtanen\nDeparment of Signal Processing, Tampere University of Technology\ntoni.heittola@tut.fi ,klap@cs.tut.fi ,tuomas.virtanen@tut.fi\nABSTRACT\nThis paper proposes a novel approach to musical instru-\nment recognition in polyphonic audio signals by using a\nsource-ﬁlter model and an augmented non-negative matrix\nfactorization algorithm for sound separation. The mixture\nsignal is decomposed into a sum of spectral bases modeled\nas a product of excitations and ﬁlters. The excitations are\nrestricted to harmonic spectra and their fundamental fre-\nquencies are estimated in advance using a multipitch esti-\nmator, whereas the ﬁlters are restricted to have smooth fre-\nquency responses by modeling them as a sum of elemen-\ntary functions on the Mel-frequency scale. The pitch and\ntimbre information are used in organizing individual notes\ninto sound sources. In the recognition, Mel-frequency cep-\nstral coefﬁcients are used to represent the coarse shape of\nthe power spectrum of sound sources and Gaussian mix-\nture models are used to model instrument-conditional den-\nsities of the extracted features. The method is evaluated\nwith polyphonic signals, randomly generated from 19 in-\nstrument classes. The recognition rate for signals having\nsix note polyphony reaches 59%.\n1. INTRODUCTION\nThe majority of research on the automatic recognition of\nmusical instruments until now has been made on isolated\nnotes or on excerpts from solo performances. A compre-\nhensive review of proposed approaches on isolated note\nbased recognition can be found in [1]. In recent years,\nthere has been increasing research interest on more de-\nmanding and realistic multi-instrumental polyphonic au-\ndio. Most of the proposed techniques extract acoustic fea-\ntures directly from the signal, avoiding the source separa-\ntion [2, 3].\nIn polyphonic mixtures consisting of multiple instru-\nments, the interference of simultaneously occurring sounds\nis likely to limit the recognition performance. The inter-\nference can be reduced by ﬁrst separating the mixture into\nsignals consisting of individual sound sources. In addition\nto the analysis of mixtures of sounds, sound source sepa-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.ration has applications in audio manipulation and object-\nbased coding.\nMany sound source separation algorithms aim at sepa-\nrating the most prominent harmonic sound from the mix-\nture. Usually they ﬁrst track the pitch of the target sound\nand then use the harmonic structure and sinusoidal model-\ning in the separation. A separation system based on this ap-\nproach has been found to improve the accuracy of a singer\nidentiﬁcation in background music [4, 5]. Sinusoidal com-\nponents can also be grouped based on grouping cues such\nas common onset times, and the recognition can be done\nusing the amplitudes of the grouped sinusoidal partials [6].\nInstrument-speciﬁc harmonic models trained\nusing instrument-speciﬁc material can achieve separation\nand recognition simultaneously [7].\nRecently, many separation algorithms have been pro-\nposed which are based on matrix factorization of the mix-\nture spectrogram. The methods approximate the magni-\ntudext(k)of the mixture spectrum in frame tand at fre-\nquencykas a weighted sum of basis functions as\nˆxt(k) =M/summationdisplay\nm=1gm,tbm(k), (1)\nwheregm,tis the gain of basis function min framet, and\nbm(k),m= 1,...,M are the bases. This means that\nthe signal is represented as a sum of components having\na ﬁxed spectrum and a time-varying gain. The decomposi-\ntion can be done, e.g., using independent component anal-\nysis (ICA) or non-negative matrix factorization (NMF), the\nlatter usually leading to a better separation quality [8]. The\nadvantage of the methods is their ability to learn the spec-\ntral characteristics of each source from a mixture, enabling\nseparation of sources which overlap in time and frequency.\nInstrument recognition systems based on the decomposi-\ntion obtained with ICA have extracted the features from\nthe estimated spectral basis vectors [9] or from the recon-\nstructed time-domain signals [10].\nA shortcoming of the basic spectrogram decompositions\nis that each pitch of each instrument has to be represented\nwith a unique basis functions. This requires a large amount\nof basis functions, making the separation and classiﬁca-\ntion difﬁcult. Virtanen and Klapuri [11] proposed to model\neach spectral basis vector as a product of an excitation and\na ﬁlter. The excitation models the time-varying pitch pro-\nduced by a vibrating element such as a string, which can\nbe shared between instruments, whereas the ﬁlter models\n327Oral Session 3: Musical Instrument Recognition and Multipitch Detection\nFigure 1 . System overview.\nthe unique resonant structure of each instrument. To im-\nprove the performance of the method, FitzGerald proposed\nto model the excitations of different fundamental frequen-\ncies explicitly as a sum of sinusoids at the harmonic fre-\nquencies [12]. Badeau et al. [13] used also a harmonic\nmodel for the excitation, but they modeled the ﬁlter us-\ning a moving-average model, resulting in a smooth fre-\nquency response. Vincent et al. [14] modeled the spec-\ntral basis vectors as a weighted sum of harmonically con-\nstrained spectra having a limited frequency support. In this\nmodel the weights of each frequency band parametrized\nthe rough spectral shape of the instrument.\nIn this paper, we present a novel approach to sound sep-\naration by using source-ﬁlter model in the context of mu-\nsical instrument recognition. The mixture signal is decom-\nposed into a sum of spectral bases modeled as a product of\nexcitations and ﬁlters. The excitations are restricted to har-\nmonic spectra and their fundamental frequencies are esti-\nmated in advance using a multiple pitch estimator, whereas\nthe ﬁlters are restricted to have smooth frequency responses\nby modeling them as a sum of elementary functions on\nMel-frequency scale. The pitch and timbre information\nare used in organizing individual notes into sound sources\n(“streaming”). Separated streams are recognized with a\nGaussian mixture model (GMM) classiﬁer. The system\nis evaluated with randomly mixed polyphonic signals us-\ning the Real World Computing (RWC) database [15] and\nsounds from 19 different instruments.\n2. METHOD\nAn overview of the system is shown in Figure 1. Multi-\npitch estimation is ﬁrst employed to estimate the pitches\nin each analysis frame. The estimated pitches are used\nin the streaming algorithm to form temporally continuous\nstreams of notes. Signals corresponding to individual\nsources are estimated using NMF for source-ﬁlter model.\nFeatures are extracted from the signals and they are clas-\nsiﬁed using a GMM classiﬁer. These processing steps are\nexplained in detail in the following.\n01234567891000.51\nFrequency (kHz)Amplitude\n8001200160000.51\nFrequency (kHz)AmplitudeFigure 2 . An example excitation spectrum en,t(k)corre-\nsponding to pitch value 800 Hz. The entire spectrum is\nshown on the left and a closer view of a small portion of it\non the right.\n2.1 Signal model\nIn the proposed signal model, each basis bm(k)in (1) is\nexpressed as a product of an excitation spectrum en,t(k)\nand a ﬁlterhi(k). This leads to the model\nˆxt(k) =N/summationdisplay\nn=1I/summationdisplay\ni=1gn,i,ten,t(k)hi(k) (2)\nfor the magnitude xt(k)of the discrete Fourier transform\nxt(k) of Hamming-\nwindowed signal in frame t. The excitations en,t(k)are as-\nsumed to correspond to the pitch values of individual notes\nn= 1,...,N at timest= 1,...,T , and the ﬁlters hi(k)\nare assumed to correspond to the spectral shapes of instru-\nmentsi= 1,...,I . We model only magnitude spectra of\nthe excitations and ﬁlters, and therefore they restricted to\nnon-negative real values. All combinations of excitations\nand ﬁlters are allowed, since we do not know in advance\nwhich instrument has produced which note. A polyphonic\nsignal consists of several excitation and ﬁlter combinations\noccurring simultaneously or in sequence.\nThe excitations en,t(k)are generated based on pitch\nvalues obtained from a multipitch estimator. For simplic-\nity, we assume that the number of notes (pitch values) Nis\nthe same in all frames t. The multipitch estimator ﬁnds the\npitchesFt(n),n= 1,...,N in each frame t, and based on\nthese, the corresponding excitation spectra en,t(k)are cal-\nculated which consist of sinusoidal components with unity\namplitudes at integer multiples of the corresponding pitch,\nFt(n). Figure 2 shows the excitation spectrum correspond-\ning to pitch 800 Hz. Variation in amplitude appears in the\nﬁgure since the partial frequencies do not fall exactly on\nspectral bins.\nThe ﬁlterhi(k)is further represented as a linear combi-\nnation of ﬁxed elementary responses:\nhi(k) =J/summationdisplay\nj=1ci,jaj(k) (3)\nwhere we chose the elementary responses aj(k)to con-\nsist of triangular bandpass magnitude responses, uniformly\ndistributed on the Mel-frequency scale fMel= 2595 log10(1+\nfHz/700) . The bases are illustrated in Fig. 3.\n32810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n01234567891000.51\nFrequency (kHz)MagnitudeFigure 3 . The elementary responses used to represent\nthe ﬁltershi(k). Triangular responses are uniformly dis-\ntributed on the Mel-frequency scale.\nSubstituting (3) to (2) gives the ﬁnal signal model\nˆxt(k) =N/summationdisplay\nn=1I/summationdisplay\ni=1gn,i,ten,t(k)J/summationdisplay\nj=1ci,jaj(k) (4)\nIn this model, en,t(k)are obtained as described above,\naj(k)are ﬁxed in advance, and therefore only gn,i,t and\nci,jremain to be estimated using the proposed augmented\nNMF algorithm. The coefﬁcients ci,jdetermine the spec-\ntral shape (ﬁlter) of instrument i,i= 1,...,I , and the\ngainsgn,i,t determine the amount of contribution from in-\nstrumentito notenat timet. Note that all instruments\nare allowed to play the same note simultaneously. Further\nconstraints to associate each excitation with only one ﬁlter\n(instrument) are described in Sec. 2.3.\nThe amount of parameters to estimate is much smaller\nin the proposed model (4) than in the traditional model (1).\nThis is because the traditional model practically requires a\nseparate basis spectrum for each pitch/instrument combi-\nnation. In the proposed model, the different notes coming\nfrom instrument iare represented by a single basis function\n(ﬁlter)hi(k), multiplied by the excitation spectra en,t(k)\nto produce different pitch values. The smaller amount of\nparameters improves the reliability of the estimation. Fur-\nthermore, in the traditional model, the bases bm(k)have to\nbe clustered to their respective sources after the estimation,\nwhereas in the proposed model this takes place automati-\ncally.\n2.2 Estimating the excitation spectra en,t(k)\nThe multipitch estimator proposed by Klapuri in [16] is\nused to estimate the note pitches Ft(n),n= 1,...,N in\neach analysis frame t. Figure 4 illustrates the output of\nthe multipitch estimator for a polyphonic signal consist-\ning of four simultaneous sounds. Based on the pitch value\nFt(n), the corresponding excitation en,t(k)is constructed\nwhich consists of Hamming-windowed sinusoidal compo-\nnents at integer multiples of the pitch value Ft(n). These\n“harmonic combs” extend over the entire frequency range\nconsidered and have a unity magnitude for all the harmon-\nics. An example excitation spectrum is shown in Figure 2.\n2.3 Streaming algorithm to link excitations with ﬁlters\nIn the described model, all combinations of excitations and\nﬁlters are allowed. In other words, all instruments (ﬁlters)\n00.511.522.533.54406080100\nTime (s)Pitch (MIDI)Figure 4 . Output of the multipitch estimator for a mixture\nsignal consisting of four simultaneous sounds.\nican play all the detected notes (excitations) nsimultane-\nously. In a realistic situation, however, it is more likely that\neach note is played by one instrument and only occasion-\nally two or more instruments play the same note.\nParametergn,i,tcontrols the salience of each excitation\nand ﬁlter combination in each frame. Robustness of the\nparameter estimation can be improved if the excitations\nen,t(k)can be tentatively organized into “streams”, where\na stream consists of the successive notes (excitations) com-\ning from a same instrument (ﬁlter). Here stream icorre-\nsponds to the instrument iand we assume that the number\nof simultaneous notes Nis equal to the number of ﬁlters\nI. The output of the streaming is a label sequence /lscriptt(n),\nwhere/lscriptt(n) =iindicates that the note (excitation) nat\ntimetcomes from instrument i. Even though the stream\nformation algorithm described here is imperfect, it is very\nhelpful in initializing the augmented NMF algorithm that\nwill be described in Sec. 2.4.\nLet us introduce a state variable qtthat corresponds to\na certain stream-labelling of the excitations en,t(k),n=\n1,...,N at timet. The number of different labellings (and\nstates) is equal to I!, that is, the number of different permu-\ntations of numbers 1,...,I . For convenience, the different\npermutations of numbers 1,...,I are stored as columns in\na matrix [U]n,qof size (N×I!).\nA candidate solution to the streaming problem can be\nrepresented as a sequence of states Q= (q1q2...qT).\nThe goodness of a candidate state sequence Qis deﬁned\nso that it is proportional to the cumulative frame-to-frame\nvariation of acoustic features extracted within each stream.\nTwo types of acoustic feature vectors zt(n)were investi-\ngated: pitch ( zt(n)≡Ft(n)) and Mel-frequency cepstral\ncoefﬁcients (MFCCs) calculated from a spectrum that was\nconstructed by picking only the spectral components corre-\nsponding to the harmonic partials of excitation nfrom the\nmixture spectrum xt(k). More exactly, the goodness Γof\na candidate solution Qgiven the features zt(n)is deﬁned\nby\nΓ(Q|{zt(n)}1:T,1:N) =γ(q1)T/productdisplay\nt=2γ(qt|qt−1)(5)\nwhere the frame-to-frame feature similarity is calculated\n329Oral Session 3: Musical Instrument Recognition and Multipitch Detection\nby\nγ(qt|qt−1) =−N/summationdisplay\nn=1/vextenddouble/vextenddoublezt([U]n,q t)−zt−1([U]n,q t−1)/vextenddouble/vextenddouble.\n(6)\nThe above goodness measure basically assumes that F0s of\nconsecutive sounds coming from the same instrument usu-\nally have only small variations, or using MFCC features,\nthat the spectral shapes of consecutive sounds from a same\ninstrument are similar. Initial goodness values γ(q1)are\ndeﬁned to be zero for all other states except for q1= 1for\nwhich we set γ(q1= 1) = 1 . This removes the ambiguity\nrelated to the ordering of different streams.\nThe most likely sequence Q= (q1q2...qT)given the\nobserved features zt(n)is a search problem\nˆQ= arg max\nQΓ(Q|{zt(n)}1:T,1:N) (7)\nwhich can be straightforwardly solved using the Viterbi al-\ngorithm. The output of the streaming (associating each ex-\ncitation with only one ﬁlter) is not ﬁxed rigidly, but is used\nin initializing the NMF parameter estimation algorithm, as\nwill be described below.\n2.4 NMF algorithm for parameter estimation\nThe spectrahi(k)can be viewed as the magnitude responses\nof the ﬁlters, and therefore it is natural to restrict them to\nbe entrywise non-negative. This is achieved using non-\nnegative coefﬁcients ci,j. Furthermore, the model can be\nrestricted to be purely additive by limiting the gains gn,i,t\nto be non-negative. NMF estimates the bases and their\ngains by minimizing the reconstruction error between the\nobserved magnitude spectrogram xt(k)and the model ˆxt(k)\nwhile restricting the parameters to non-negative values.\nCommonly used measures for the reconstruction error\nare the Euclidean distance, and divergence d, deﬁned as\nd(x,ˆx) =/summationdisplay\nk,txt(k) logxt(k)\nˆxt(k)−xt(k) + ˆxt(k) (8)\nThe divergence is always non-negative, and zero only when\nxt(k) = ˆxt(k)for allkandt. An algorithm that minimizes\nthe divergence for the traditional signal model (1) has been\nproposed by Lee and Seung [17]. In their algorithm, the\nparameters are initialized to random non-negative values,\nand updated by applying multiplicative update rules itera-\ntively. Each update decreases the value of the divergence.\nWe propose an augmented NMF algorithm for estimat-\ning the parameters of the model (4). Multiplicative updates\nwhich minimize the divergence (8) are given by\nci,j←ci,j/summationtext\nn,t,krt(k)gn,i,ten,t(k)aj(k)/summationtext\nn,t,kgn,i,ten,t(k)aj(k)(9)\ngn,i,t←gn,i,t/summationtext\nj,krt(k)en,t(k)ci,jaj(k)/summationtext\nj,ken,t(k)ci,jaj(k)(10)\nwherert(k) =xt(k)\nˆxt(k)is evaluated using (4) before each up-\ndate. The overall estimation algorithm is given as follows:1. Estimate excitations en,t(k)using multipitch esti-\nmator and the procedure explained in Sec.2.2. Ini-\ntialize the gains gn,i,t and the ﬁlter coefﬁcients ci,j\nwith absolute values of Gaussian noise.\n2. Update the ﬁlter coefﬁcients ci,jusing (9).\n3. Update the gains gn,i,tusing (10)\n4. Repeat steps 2-3 until the changes in parameters are\nsufﬁciently small.\nIn our experiments we observed that the divergence (8) is\nnon-increasing under each update. If streaming is used,\ninitialgn,i,t are multiplied with small factor 0.001 for n\nthat do not belong to stream i. Using a small non-zero\nvalue favours the streamed excitations to be associated with\nthe ﬁlteri, but this does not exclude the possibility of that\nthe NMF algorithm will “correct” the streaming when the\ngains are updated during the algorithm. The streaming\nbased on F0 values or MFCC values is far from perfect\nbut yet improves the robustness of the parameter estima-\ntion with the NMF algorithm.\n2.5 Reconstruction of instrument-wise spectrograms\nSpectrograms corresponding to a certain instrument ican\nbe reconstructed by using (4) and limiting the sum over i\nto one value only:\nˆxi,t(k) =N/summationdisplay\nn=1gn,i,ten,t(k)J/summationdisplay\nj=1ci,jaj(k) (11)\nSpectrogram of instrument iis reconstructed as\nyi,t(k) =ˆxi,t(k)\nˆxt(k)xt(k) (12)\nwhere the denominator is calculated using (4) and sum-\nming over all i.\nTime-domain signals are generated by using phases of\nthe mixture signal and inverse discrete Fourier transform.\n2.6 Classiﬁcation\nMel-frequency cepstral coefﬁcients (MFCC) are used to\nrepresent the coarse shape of the power spectrum of the\nseparated instrument-wise signals. MFCCs are calculated\nfrom the outputs of a 40-channel ﬁlterbank which occupies\nthe band from 30Hz to half the sampling rate. In addition\nto the static coefﬁcients, their ﬁrst time derivatives approx-\nimated with a three-point ﬁrst-order polynomial ﬁt are used\nto describe the dynamic properties of the cepstrum.\nGaussian mixture models are used to model instrument-\nconditional densities of the features. The parameters for\nthe GMM are estimated with Expectation Maximization\n(EM) algorithm from the training material. Amount of\nGaussian distributions in the mixture model was ﬁxed to 32\nfor each class. In order to prevent acoustic mismatch be-\ntween the training material and the testing material, models\nare trained with the separated signals. In the training stage,\nthe perfect streaming is used with a prior knowledge about\n33010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nthe sources in the signals. In the classiﬁcation stage, like-\nlihoods of the features are accumulated over the signal for\nthe instrument classes, energy weighting the likelihoods\nwith the RMS energy in the corresponding\nframe, and the classiﬁcation is performed with maximum-\nlikelihood classiﬁer.\n3. EXPERIMENTS\nThe proposed algorithm is evaluated in the musical instru-\nment recognition task with generated polyphonic signals.\nThe streaming algorithm is evaluated using both the pitch\nand the timbre information. “no separation” denotes a sys-\ntem where instrument recognition is done without sepa-\nration directly from the mixture signal. “no streaming”\ndenotes a system where the NMF is initialized (step 1 in\nSection 2.4) with random values gn,i,t. “streaming (given\nF0s)” denotes system where time-varying pitches of sources\nwere given in advance. “streaming (est. F0s)” denotes\nsystem where the pitches were estimated with the multi-\npitch estimator and used in automatic streaming. “stream-\ning (timbre)” uses timbre information in streaming and the\nF0s used for estimating the timbre were given in advance.\nPrior information of polyphony is used in all systems.\n3.1 Acoustic Data\nPolyphonic signals are generated by linearly mixing sam-\nples of isolated notes from the RWC musical instrument\nsound database. Nineteen instrument classes are selected\nfor the evaluations (accordion, bassoon, clarinet, contra-\nbass, electric bass, electric guitar, electric piano, ﬂute, gui-\ntar, harmonica, horn, oboe, piano piccolo, recorder, sax-\nophone, trombone, trumpet, tuba) and the instrument in-\nstances are randomized either into training (70%) or testing\n(30%) set. The polyphonic signals are generated from these\nsets, 500 cases for the training and 100 cases for the test-\ning.\nFour-second polyphonic signals are generated by ran-\ndomly selecting instrument instances and generating ran-\ndom note sequences for them. For each instrument, the\nﬁrst note in a note sequence is taken randomly from the\nuniform distribution speciﬁed by the available notes in the\nRWC database for the instrument instance. The next notes\nin the sequence are taken from a normal distribution having\na previous note as the mean and the standard deviation σ\n(being 6 semitones if not mentioned otherwise). Unisonal\nnotes are excluded from the note sequence. The notes are\nrandomly truncated to have length between 100 ms and one\nsecond. Signals from each instrument are mixed with equal\nmean-square levels. Examples of test signals are available\natwww.cs.tut.fi/~heittolt/ismir09/ .\n3.2 Evaluation Results\nThe separation quality was measured by comparing the\nseparated signals with the reference ones. The signal-to-\nnoise ratio (SNR) of a separated signal is estimated asPolyphony\n2 3 4 5 6\nno\nstreaming4.9 2.6 2.1 1.5 1.2\nstreaming\n(est. F0s)7.2 4.0 2.5 1.7 1.2\nstreaming\n(timbre)7.6 4.4 3.3 2.4 1.9\nTable 1 . Average signal-to-noise rations (in dB) for differ-\nent system conﬁgurations.\nPolyphony\n1 2 3 4 5 6\nno\nseparation62.0 18.7 12.1 13.7 24.4 29.5\nno\nstreaming62.0 49.5 42.1 42.6 39.3 42.7\nstreaming\n(given F0s)62.0 59.0 58.0 57.9 57.8 56.0\nstreaming\n(est. F0s)61.0 60.2 53.5 56.7 55.2 53.8\nstreaming\n(timbre)62.0 57.6 51.9 57.0 55.9 59.1\nTable 2 . F-measures (%) for different system conﬁgura-\ntions.\nSNR = 10 log10Σts(t)2\nΣt(s(t)−ˆs(t))2, (13)\nwheres(t)is the reference signal and ˆs(t)is the separated\nsignal. The average signal-to-noise ratios obtained for dif-\nferent system conﬁgurations are given in Table 1.\nIn instrument recognition, balanced F-measure is used\nas metric in the evaluations. The recall Ris calculated as\nthe ratio of correctly recognized instrument labels to sum\nof the correctly recognized instrument labels and unrecog-\nnized instrument labels. The precision Pis calculated as\nthe ratio of correctly recognized instrument labels to all in-\nstrument labels produced by the system. The F-measure is\ncalculated from these two values as F= 2RP/(R+P).\nThe evaluation results for different system conﬁgura-\ntions are given in Table 2. The system without separation\nuses the prior knowledge about the polyphony of the sig-\nnal to ﬁnd same amount of instruments directly from the\nmixture signal. This increases the random guess rate as\nthe polyphony increases. The proposed approach using\nseparation as a pre-processing gives rather steady perfor-\nmance regardless of the polyphony and gives reasonable\nperformance already without the streaming algorithm. The\nstreaming algorithm improves the results evenly, giving\n10-15% increase in performance. The pitch and the timbre\ninformation based streaming gives same level of accuracy,\nthough the pitch information seems to give slightly more\nrobust performance. The estimated fundamental frequen-\ncies work almost as well as the given frequencies. The\nevaluation results for different types of polyphonic signals\n331Oral Session 3: Musical Instrument Recognition and Multipitch Detection\nPolyphony\nσ 1 2 3 4 5 6\n3 51.0 59.9 53.5 57.3 57.6 54.6\n6 62.0 57.6 51.9 57.0 55.9 59.1\n12 72.0 63.1 53.7 57.5 55.4 57.8\nTable 3 . F-measures (%) for different polyphonic signal\nconditions with the timbre based streaming.\nare given in Table 3. The proposed system gives quite con-\nsistent results with all levels of the polyphony and when\nvarying the standard deviation σfrom 3 to 12 semitones.\nThe slight variations in some cases are due to the random-\nization of used instruments for different polyphony levels.\n4. CONCLUSIONS\nIn this paper, we proposed a source-ﬁlter model for sound\nseparation and used it as a preprocessing step for musical\ninstrument recognition in polyphonic music. The experi-\nmental results with the generated polyphonic signals were\npromising. The method gives good results when classify-\ning into 19 instrument classes and with the high polyphony\nsignals, implying a robust separation even with more com-\nplex signals. When recognizing the instrument from a se-\nquence of several notes, it seems that the remaining slight\nseparation artefacts average out to quite neutral noise,\nwhereas the information related to the target instrument is\nconsistent and leads to a robust recognition. Even when the\nF0s are estimated automatically, they provide sufﬁciently\naccurate information to get reasonable results.1\n5. REFERENCES\n[1] P. Herrera-Boyer, A. Klapuri, and M. Davy. Automatic\nclassiﬁcation of pitched musical instrument sounds. In\nSignal Processing Methods for Music Transcription ,\npages 163–200. Springer, 2006.\n[2] T. Kitahara, M. Goto, K. Komatani, T. Ogata, and\nH. Okuno. Musical instrument recognizer \"instrogram\"\nand its application to music retrieval based on instru-\nmentation similarity. International Symposium on Mul-\ntimedia , pages 265–274, 2006.\n[3] S. Essid, G. Richard, and David.B. Instrument recog-\nnition in polyphonic music based on automatic tax-\nonomies. IEEE Transactions on Audio, Speech & Lan-\nguage Processing , 14(1):68–80, 2006.\n[4] H. Fujihara, T. Kitahara, M. Goto, K. Komatani,\nT. Ogata, and H. G. Okuno. Singer identiﬁcation based\non accompaniment sound reduction and reliable frame\nselection. In Proc. ISMIR 2005 , pages 329–336, 2005.\n[5] A Mesaros, T. Virtanen, and A. Klapuri. Singer iden-\ntiﬁcation and polyphonic music using vocal separa-\ntion and pattern recognition methods. In Proc. ISMIR ,\npages 375–378, 2007.\n1This work was ﬁnancially supported by the Academy of Finland.[6] J. J. Burred, A. Röbel, and T. Sikora. Polyphonic musi-\ncal instrument recognition based on a dynamic model\nof the spectral envelope. In Proc. of the IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing , pages 173–176, 2009.\n[7] P. Leveau, E. Vincent, G. Richard, and L. Daudet.\nInstrument-speciﬁc harmonic atoms for mid-level\nmusic representation. IEEE Transactions on Audio,\nSpeech & Language Processing , 16(1):116–128, 2008.\n[8] T. Virtanen. Monaural sound source separation by non-\nnegative matrix factorization with temporal continuity\nand sparseness criteria. IEEE Trans. Audio, Speech and\nLanguage Processing , 15(3):1066–1074, 2007.\n[9] P. Jincahitra. Polyphonic instrument identiﬁcation us-\ning independent subspace analysis. In Proceeding of\nthe IEEE International Conference on Multimedia and\nExpo , pages 1211–1214. IEEE, 2004.\n[10] P. S. Lampropoulou, A. Lampropoulos, and\nG. Tsihrintzis. Musical instrument category dis-\ncrimination using wavelet-based source separation. In\nNew Directions in Intelligent Interactive Multimedia ,\npages 127–136. 2008.\n[11] T. Virtanen and A. Klapuri. Analysis of polyphonic au-\ndio using source-ﬁlter model and non-negative matrix\nfactorization. In Advances in Models for Acoustic Pro-\ncessing, Neural Information Processing Systems Work-\nshop , 2006.\n[12] D. FitzGerald, M. Cranitch, and E. Coyle. Extended\nnonnegative tensor factorisation models for musi-\ncal source separation. Computational Intelligence and\nNeuroscience , 2008.\n[13] R. Badeau, V . Emiya, and B. David. Expectation-\nmaximization algorithm for multi-pitch estimation and\nseparation of overlapping harmonic spectra. In Pro-\nceedings of the IEEE International Conference on\nAcoustics, Speech and Signal Processing , 2009.\n[14] E. Vincent, N. Bertin, and R. Badeau. Harmonic and\ninharmonic nonnegative matrix factorization for poly-\nphonic pitch transcription. In Proceedings of the IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing , pages 109–112, 2008.\n[15] M. Goto, T. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Music genre database and mu-\nsical instrument sound database. In Proc. ISMIR , pages\n229–230, 2003.\n[16] A. Klapuri. Multiple fundamental frequency estima-\ntion by summing harmonic amplitudes. In Proc. IS-\nMIR, pages 216–221, 2006.\n[17] D. D. Lee and H. S. Seung. Learning the parts of\nobjects by non-negative matrix factorization. Nature ,\n401:788–791, October 1999.\n332"
    },
    {
        "title": "Global Feature Versus Event Models for Folk Song Classification.",
        "author": [
            "Ruben Hillewaere",
            "Bernard Manderick",
            "Darrell Conklin"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417829",
        "url": "https://doi.org/10.5281/zenodo.1417829",
        "ee": "https://zenodo.org/records/1417829/files/HillewaereMC09.pdf",
        "abstract": "Music classification has been widely investigated in the past few years using a variety of machine learning approaches. In this study, a corpus of 3367 folk songs, divided into six geographic regions, has been created and is used to evaluate two popular yet contrasting methods for symbolic melody classification. For the task of folk song classification, a global feature approach, which summarizes a melody as a feature vector, is outperformed by an event tained on the folk song corpus was achieved with an ensemble of event models. These results indicate that the event model should be the default model of choice for folk song classification.",
        "zenodo_id": 1417829,
        "dblp_key": "conf/ismir/HillewaereMC09",
        "keywords": [
            "Music classification",
            "machine learning approaches",
            "corpus of 3367 folk songs",
            "six geographic regions",
            "two popular yet contrasting methods",
            "symbolic melody classification",
            "global feature approach",
            "ensemble of event models",
            "default model of choice",
            "folk song classification"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nGLOBAL FEATURE VERSUS EVENTMODELSFOR FOLKSONG\nCLASSIFICATION\nRuben Hillewaere andBernard Manderick\nComputationalModelingLab\nDepartmentofComputing\nVrijeUniversiteitBrussel\nBrussels, Belgium\n{rhillewa,bmanderi }@vub.ac.beDarrell Conklin\nMusicInformatics Research Group\nDepartmentofComputing\nCityUniversityLondon\nUnitedKingdom\nconklin@city.ac.uk\nABSTRACT\nMusic classiﬁcation has been widely investigated in the\npastfewyearsusingavarietyofmachinelearningapproach-\nes. In this study, a corpus of 3367 folk songs, divided\ninto six geographic regions, has been created and is used\nto evaluate two popular yet contrasting methods for sym-\nbolic melodyclassiﬁcation. For the task of folksong clas-\nsiﬁcation, a global feature approach, which summarizes a\nmelody as a feature vector, is outperformed by an event\nmodel of abstract event features. The best accuracy ob-\ntained on the folk song corpus was achieved with an en-\nsemble of event models. These results indicate that the\neventmodelshouldbethedefaultmodelofchoiceforfolk\nsongclassiﬁcation.\n1. INTRODUCTION\nComputational folk music analysis is gaining increasing\ninterest in recent years, revitalized by the developingﬁel d\nof computational ethnomusicologyand also by interest in\nnon-Western musics, the availability of advanced music\ndata mining methods capable of dealing with very large\ndatasets,andtheexistenceofexpandingfolksongcorpora\non the internet. A mechanical process that can accurately\nlocate new folksongsinto geographicalregions,proposed\nas early as the 1950s [1], can now be developed. In this\nwork we describe and investigate two very different ma-\nchine learning methods for folk song classiﬁcation. Folk\nsongs from six different European regions will be used,\nandtheclassiﬁcationtaskistoassignunseensongstotheir\ncorrectregions.\nThemorepreciseobjectiveofthisstudyistodetermine\nwhetherglobalfeature modelsare outperformedby meth-\nods based on event features for the task of folk song clas-\nsiﬁcation. Aglobalfeatureencapsulatesinformationabou t\na whole piece into a single value: numeric, nominal, or\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2009 International Society for MusicInformation Retrieva l.pitch 7475777470798182\nmelodic interval ⊥+1+2-3-4+9+2+1\nmelodic contour ⊥uudduuu\nduration ratio ⊥11141/416\naverage pitch 76.5\nrel. freq. M3 0.143\nHuron contour ascending\nFigure 1. Excerpt of the English folk tune “Harding’s\nFolly’sHornpipe”,illustratingthe contrastbetweenglob al\nfeatures(lowerthree)andeventfeatures(upperfour).\nBoolean. Using global features, pieces can be simply re-\nexpressed as feature vectors and a wide range of standard\nmachine learning algorithms can then be applied [2,3].\nEvent features, on the other hand, do not summarize a\npiece into a single value, but rather view a piece as a se-\nquenceofevents,eacheventwithitsownfeatures. Astan-\ndard technique for working with sequential symbolic mu-\nsic data expressed as event features is the n-gram model,\nwhichisparticularlywell-knownforlanguagemodeling,a\nword in language being roughly analogous to an event in\nmusic.\nFigure1 illustrates a short melodic fragment, the ﬁrst\nmeasuresofanEnglishtunecalled“Harding’sFolly’sHorn-\npipe”, expressed using both event features “pitch”, “me-\nlodic interval”, “melodic contour”, “duration ratio”, and a\nfew global features“average pitch”, “rel. freq. M3” (rela-\ntivefrequencyofmajorthirds),and“Huroncontour”.\nDespitemanydifferentproposalsofglobalfeaturesets,\nandstudiescomparingafewfeaturesetstooneanother[4],\ntherehasnotyetbeenarigorousandsystematicstudycom-\nparing the relative efﬁcacy of global features versus n-\ngram models for music classiﬁcation. In this paper, we\nstudy four different global feature sets for the task of folk\nsongclassiﬁcation. Allfeaturesetsareusedforwell-know n\nclassiﬁcation techniques such as naive Bayes, logistic re-\ngression, SVM, k-nearest neighbours and decision trees.\nTheseresultswill becomparedwithbothasimple n-gram\n729Oral Session 9-A: Folk Songs\nOrigin #pieces avgnotes/piece\nEngland 990(29.4%) 93\nFrance 393(11.7%) 71\nIreland 798(23.7%) 105\nScotland 445(13.2%) 119\nS.E.Europe 123(3.7%) 118\nScandinavia 618(18.3%) 94\nTotal 3367\nTable 1. TheEuropa-6 collection: the number of pieces\nandtheaveragenumberofnotesperpieceineachregion.\nevent model of linked interval/duration and its extension,\nthemultipleviewpointmodel[5].\nBasedonthefactthateventmodelstakeintoaccountse-\nquentialstructure,thehypothesisofthisstudyisthateve nt\nmodels will outperform global feature models on the task\nof folk song classiﬁcation. The remainder of this paper\ndescribes the methods and results employed in the explo-\nrationofthishypothesis.\n2. METHODS\nInthissectionwedescribetheglobalfeatureapproachand\nthe event models, and we detail the monophonic data set\nusedfortraining.\n2.1 Experimentaldataset\nTo explore the performance of these models, we compare\ntheir relative efﬁciency in terms of classiﬁcation accura-\nciesonaverylargecorpusoffolksongs,whichwecallthe\nEuropa-6 collection. Thisisacollectionoffolksongsfrom\n6 countries/regions of Europe: see Table 1 for the classes\nand the piece counts in each class. The classiﬁcation task\nistoassignunseenfolksongstotheircorrectregionofori-\ngin. Initially, 3724 pieces were selected by Li et al. [6]\nout of a collection of 14,000 folk songs transcribed in the\nABCformat. Thecollectionwasprunedto3367piecesby\nﬁlteringoutduplicateﬁles. Thiswasdonebyclusteringall\npieces into groups containing identical Jesser feature vec -\ntors (Section 2.2). If a group contained pieces spanning\ndifferent regions (e.g., England and Ireland), all pieces i n\nthe group were discarded due to this ambiguity in annota-\ntion, otherwise just one piece of the group was retained.\nFurthermore, we retained only the highest note of double\nstops present in some instrumental folk songs. To focus\non core melodies rather than performanceelaboration, we\nremoved all grace notes, trills, staccato, and ignored re-\npeated section indications. Time and key signatures were\nretained. Sincemostofthesepieceshavenotempoindica-\ntion,alltempoindicationsthatwerepresentwereremoved.\nFinally,by meansof abc2midiwe generateda cleanquan-\ntizedMIDIcorpus,andremovedalldynamic(velocity)in-\ndications generated by the style interpretation mechanism\nofabc2midi.2.2 Globalfeaturemodels\nThere have been many proposals of global feature sets.\nVolk et al. [4] provide an evaluation of several global fea-\nture sets for the task of comparing folk songs for melodic\nsimilarity,andseveralmoresetscanbefoundinthelitera-\nture. Inourexperiments,we chosefour:\n•Theﬁrstisthe Alicantesetof28globalfeatures,pro-\nposedby Ponce de L´ eonand I˜ nesta, appliedto clas-\nsiﬁcation of 110 MIDI tunes in jazz/classical/pop\ngenres[2]. Fromthisset,were-implementedacom-\npactsubset: thetop12selectedby[2]: Table1.\n•The second is the Fantastic set: 92 features com-\nputedbytheprogramcalledFeatureANalysisTech-\nnologyAccessingSTatistics(InaCorpus),currently\ndeveloped by M¨ ullensiefen [7] (v0.9, downloaded\nfrom[8]). Forthis study,we onlyincludethe global\nfeaturesbasedonasinglemelody,whichreducesthe\nsetto37features. Inadditiontosomebasicdescrip-\ntive statistics based on pitch and duration, this set\nalsoincludesafewentropy-basedfeatures,andsome\ncontourfeaturesderivedfromtheworkofSteinbeck\n[9] and Huron [10]. Moreover, the set of 37 fea-\nturesincludesomestatisticsofso-called m-typesthat\ntake into account some local sequential note order.\nBy default, Fantastic segments the scores and com-\nputes the features on the created phrases, but we in-\nsteadreportresultswithoutsegmentationsincethese\nachievedgloballybetterresults.\n•TheJesserset contains40 pitch and duration statis-\ntics [11]. The pitch-based features are simple rel-\native interval counts, like “amajsecond” (ascending\nmajor second). Similar features are present for all\nascending and descending intervals in the range of\nthe octave. Almost all features were implemented,\nonly the feature “numlines” was not applicable for\nfolksongclassiﬁcationbasedonmelody.\n•The last set is the McKayset of 101global features,\ndeveloped for the classiﬁcation of orchestrated (in-\nstrumentation and dynamics) MIDI ﬁles [3]. Im-\nportantly, these features were used in the winning\n2005 MIREX symbolic genre classiﬁcation exper-\niment which used orchestrated ﬁles for evaluation.\nThefeatureswerecomputedwithMcKay’ssoftware\npackage jSymbolic (version 12.2.0) from [12]. A\nfew attributes “harmonicity of two strongest rhyth-\nmicpulses”and“strengthratiooftwostrongestrhyth-\nmic pulses” were removed due to runtime and nu-\nmericalerrorscausedbytheircomputationusingthe\njSymbolic tool. For the analysis of the Europa-6\ncorpus, many features such as those based on in-\nstrumentation, dynamics, polyphonic texture, glis-\nsando had the same value for every piece and were\nremoved. TheﬁnalMcKayset containsa totalof62\nfeatures.\nThe four global feature sets above are summarized in\nTable2. In this table, we also indicate for each feature\n73010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nGlobalfeatureset #features pitch duration\nAlicante 12 7 2\nFantastic 37 21 15\nJesser 39 31 6\nMcKay 62 35 5\nTable 2. Global featuresets used in our experiments. The\nlast two columnsshow the numberof featuresthat are de-\nrivedfrompitchandduration.\nset how many features are derived from pitch or duration.\nA feature is derived from pitch (duration) when at least\nonepitch(duration)valueisinspectedforthefeaturecom-\nputation. Most features are derived from pitch or dura-\ntion, spanningfromverybasic featureslike “variabilityo f\nnote duration” to more abstract ones, such as “tonalness”\nor “interval distribution normality”. Examples of feature s\nthat are not derived from pitch or duration are descrip-\ntors such as “has meter changes” and “average time be-\ntween attacks”, or more speciﬁc ones like “polyrhythms”\nor “strength of strongest rhythmicpulse”. In the Fantastic\nsetsomefeaturesarebasedonbothpitchandduration,like\nthe step contourand interpolationcontour features. These\nfour global feature sets were also chosen as they do not\nshow much overlap in semantic content, aside from some\nvery basic features such as “number of notes” and “pitch\nrange”.\nA global feature set summarizes a piece as a feature\nvector, which can be viewed as a data point in a feature\nspace. The classiﬁcation task can thuseasily be addressed\nwithstandardmachinelearningtechniques,forwhichtool-\nboxes are available. The underlying idea is to assess the\ndiscriminative power of a global feature set by looking at\nitsperformanceintermsofclassiﬁcation accuracies.\n2.3 Eventmodels\nIncontrasttoglobalfeaturemodels,eventmodelstakeinto\naccount the sequential structure of the melody. A type of\nevent model commonlyused for statistical language mod-\neling is the n-gram model [13]. In an n-gram model for\nmusic, the probability of a piece eℓ= [e1, . . . , e ℓ]is ob-\ntained bycomputingthe joint probabilityof the individual\neventsinthepiece:\np(eℓ) =ℓ/productdisplay\ni=1p(ei|ei−1), (1)\nwithsuitablerestrictionsonthecontext ei−1,forexample,\nfor a trigram model ei−1is restricted to [ei−2, ei−1]. The\nconditional event probabilities p(ei|ei−1)are estimated\nby the n-gram counts of the training data. In addition,\nMethod C smoothing [13] is used to handle the zero fre-\nquency problem. To use n-gram models for melody clas-\nsiﬁcation, for each class a separate model is built and the\npredicted class of a piece is the class whose model gener-\natesthepiecewith thehighestprobability.\nPresented with sparse data, n-gram models for musiccannotmodel the pitch or durationdirectly,hencethe mu-\nsiceventsmustﬁrstbeclusteredintomoreabstractequiva-\nlenceclassesbyapplyingfunctionscalled viewpoints [14].\nExamples of viewpoints are “melodic interval” or “dura-\ntion contour”, which obviously lead to event features that\nare less sparse than the concrete music events in the cor-\npus. Particularly useful are linkedviewpoints, which cap-\nture correlation between abstract classes, for example, a\nlinked viewpoint of melodic interval and duration, mean-\ning we represent everyevent as a pair of its melodic inter-\nvalandduration.\nForaviewpoint τ, theeventsequence eℓistransformed\ntotheabstractfeaturesequence /hatwideτ(eℓ) = [τ(e1), . . ., τ (eℓ)],\nand Equation 1 can be adapted as follows, resulting in the\nviewpointmodel :\np(eℓ) =ℓ/productdisplay\ni=1p(τ(ei)|/hatwideτ(ei−1))×p(ei|τ(ei)).(2)\nThe ﬁrst factor in (2) is the probabilityof the abstract fea-\nture using an n-gram model, and the second factor is the\nprobabilityoftheconcreteeventgiventheabstractfeatur e,\nwhichismodeledbya uniformdistribution.\nAn extension is the multiple viewpoint model , an en-\nsemble of viewpoints used in aggregation to compute the\nprobability of a sequence. Multiple viewpoints have been\nusedwithsuccessinsymbolicmusicprocessingtaskssuch\nas melody prediction and generation [5] and melody seg-\nmentation [15]. To combine the predictions of kview-\npoints τ1, . . ., τ k,onestraightforwardwayistousethege-\nometricmeanofthecomponentviewpointpredictions:\np(eℓ) =1\nZ×/parenleftBiggk/productdisplay\ni=1pτi(eℓ)/parenrightBigg1/k\n(3)\nwhere Zisanormalizationfactor. IntheResultssection,a\nmultiple viewpoint model will be contrasted with both an\nn-grammodelanda globalfeaturemodel.\n3. RESULTS\nIn this section we report on the experimental results on\ntheEuropa-6 collection. For the evaluation of the global\nfeature sets, we used a standard machine learning tool-\nboxcalledWeka(version3.6.1)[16]whichisdocumented\nin [17]. Weka containsmanydifferentalgorithmsforclas-\nsiﬁcation: weusedthewell-knownclassiﬁersNaiveBayes,\nk-nearest neighbours, decision trees (J48), Support Vec-\ntor Machines (SVM) and Logistic regression. The feature\nsetswerecomparedbycomputingclassiﬁcationaccuracies\nfor each of these classiﬁers, which were all obtained by\n10-fold cross validation. Default Weka conﬁguration pa-\nrameterswere used for all classiﬁers. Results are reported\nin Table3. To measure the difﬁculty of this classiﬁcation\ntask, note that always predicting the most frequent class\n(England)will achieveanaccuracyofonly29.4%. Forthe\nmethod of k-nearest neighbourswe explored many values\nofk, andk= 20seemed to yield the best results in gen-\neral. We observe that the McKay and the Jesser features\n731Oral Session 9-A: Folk Songs\nFeatureset Alicante Fantastic JesserMcKay\n#features 12 37 39 62\nNaiveBayes 45.3 52.5 47.1 53.8\nDecisiontree 48.2 47.3 58.8 58.4\nSVM 51.0 57.6 63.4 66.7\nkNN(k=20) 52.7 51.3 61.9 60.7\nLogisticregr. 51.9 46.5 63.8 67.8\nTable3. Classiﬁcationaccuraciesoftheglobalfeaturesetsonthe Europa-6 collection,obtainedby10-foldcrossvalidation.\nFeatureselectionmethod Joinedset CfsSubsetEval ClassifSubsetEval PCA\n(BestFirst) (NaiveBayes) (top13)\n#features 150 41 20 13\nNaiveBayes 55.7 60.0 63.9 61.6\nDecisiontree 59.1 62.7 59.1 53.5\nSVM 69.7 68.3 61.6 64.3\nkNN(k=20) 65.9 66.3 61.9 64.3\nLogisticregr. N/A 69.5 49.4 63.8\nTable 4. Classiﬁcation accuracies of the sets created by various fe ature selection methods, obtained by 10-fold cross\nvalidation.\nobtain the highest accuracies, and the best overallresult i s\nobtained with logistic regression on the McKay features,\nwith anaccuracyof67.8%.\nBefore comparing these results to those obtained with\nthe event models, we explored whether there might be a\ncompact optimal global feature set for the task of classi-\nﬁcation of folk tunes. Therefore, we have created a ﬁfth\nglobal feature set containing all global features from the\nAlicante,Fantastic, Jesser andMcKaysets. On thisjoined\nset,weperformedvariousfeatureselectionmethods,eithe r\nby evaluating attribute subsets on their predictive value,\nlike correlation-basedfeature selection or classiﬁer sub set\nevaluators,orbyusingsingle-attributeevaluators,byme a-\nsuring the information gain of each attribute or by apply-\ningprincipalcomponentanalysis. Severalsearchstrategi es\nwereexploredforthesubsetevaluators,andforthesingle-\nattribute evaluators we searched for the optimal number\nof top features to include. The best results obtained are\ndetailed in Table4. The maximum result of 69.7% is ob-\ntainedonthefulljoinedsetwithaSupportVectorMachine.\nThis conﬁrms the known strength of an SVM classiﬁer\nwhen dealing with high dimensional feature vectors. The\noverall best performing compact subset was found with a\ncorrelation-based feature set evaluator and a greedy hill-\nclimbingsearch(BestFirst) asdescribedin [18],obtainin g\n69.5% with a multi-class classiﬁer using logistic regres-\nsion. Itwasnotpossibletocomputetheaccuracywiththat\nclassiﬁer onthefulljoinedset, asthecomputationwastoo\nheavy.\nTo evaluate the event models on the Europa-6 , we im-\nplemented a 10-foldcross validationscheme. With a pen-\ntagrammodelofalinkedviewpointofmelodicintervaland\nduration, the obtained classiﬁcation accuracy is 72.7%,\nsigniﬁcantly higher than even the best of the global fea-turemodels. Forthemultipleviewpointmodelweusedan\nensemble of four viewpoints, the same collection as used\nby[5]:\n•alinkedviewpointofmelodicinterval,andpitchclass\ninterval from the reference pitch class of the piece.\nThelatteriscalculatedassumingthemajormode;\n•alinkedviewpointofmelodicintervalandinter-onset\ninterval(timedifferencebetweentwosuccessiveon-\nsets,whichwillbedifferentthananevent’sduration\ninthepresenceofrests);\n•asimpleviewpointthatreturnsthepitchofanevent;\n•a linked viewpoint of pitch class interval from the\nreference pitch and ﬁrst metric level. The latter is a\nBooleanviewpointwhichis trueif an eventis at the\nbeginningofa bar.\nIn the multiple viewpoint mode, each of the above has\nits own pentagram model, and the component viewpoints\nare combined as described in Equation (3). As expected,\nthismultipleviewpointmodelachievesasigniﬁcantlyhigh -\neraccuracythanthelinkedviewpointof 76.0%. Thisisthe\nbest resultwe haveyetobtainedonthe Europa-6 corpus.\n4. CONCLUSIONS ANDFUTUREWORK\nInthispaper,we presentedaﬁrst systematicstudyforfolk\nsongclassiﬁcation,comparingtwowell-knownmethodsto\napproachthistask,namelyglobalfeaturemodelsandevent\nmodels. The hypothesisof the event model was validated,\nsince the results show that four established global feature\nsetsusingstandardclassiﬁerswereoutperformedbyavery\nsimple n-grammodelofalinkedviewpoint,andevenmore\n73210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nbythemultipleviewpointmodel. We haveexploredmeth-\nodstoﬁndanoptimalglobalfeaturesetbyjoiningthefour\nsets and byperformingattributeselection, andwe observe\na slight improvement, but the event models still achieve\nhigherclassiﬁcationaccuracies. Webelievetheeventmod-\nelsperformbetter,preciselybecausetheyretainsequenti al\ninformation that global features do not take into account.\nInordertoidentifyfolksongregions,oneneedstocapture\nthe inner structure of a musical phrase. The event model\nshould thus be the default model of choice for folk song\nclassiﬁcation.\nTherearestillsomeothertypesofmodelsthatwewould\nliketoconsiderinourfuturework,suchasmethodswhere\nonefocusesonthedevelopmentofagoodsimilarityordis-\ntancemeasurebetweenpieces[19]. Anothertypeofmodel\nthat has not been thoroughly explored for classiﬁcation is\na pattern model, where one uses a collection of musical\npatterns and classiﬁes a piece according to the patterns it\ncontains [20]. Another question we want to address is if\nthese results still hold for polyphonic music. Global fea-\nture models can easily be expanded to polyphony. Event\nmodels, however, are harder to extrapolate to polyphony,\nbecausewethenhavetodealwiththeproblemofﬁndinga\nsuitableharmonicrepresentationandsegmentation.\n5. ACKNOWLEDGEMENTS\nThisresearchispartiallyfundedbyMessiaenWeerspiegeld .\nSpecial thanks are due to Xiao Li for correspondenceson\nher original ABC corpus, to Daniel M¨ ullensiefen for clar-\niﬁcations of the Fantastic feature set, to Mathieu Berg-\neron for assistance with the abc2midicode,and to Kerstin\nNeubarth, Stijn Meganck and H´ el` ene Viratelle for helpful\ncommentsona draftofthisarticle.\n6. REFERENCES\n[1] A. Lomax: “Folk Song Style: Notes on a Systematic\nApproach to the Study of Folk Song,” Journal of the\nInternational Folk Music Council , Vol. 8, pp. 48-50,\n1956.\n[2] P. J. Ponce de L´ eon and J. M. I˜ nesta: “Statistical de-\nscriptionmodelsformelodyanalysisandcharacteriza-\ntion,”Proceedingsofthe2004InternationalComputer\nMusicConference ,pp.149–156,2004.\n[3] C. McKay and I. Fujinaga: “Automatic genre classiﬁ-\ncationusinglargehigh-levelmusicalfeaturesets,” Pro-\nceedingsof the InternationalConference on Music In-\nformationRetrieval ,pp.525–530,2004.\n[4] A. Volk, P. van Kranenburg, J. Garbers, F. Wiering,\nR.C. Veltkamp and L.P. Grijp: “A manual annotation\nmethodformelodicsimilarityandthestudyofmelody\nfeaturesets,” Proceedingsofthe9thInternationalCon-\nference on Music Information Retrieval , pp. 101–106,\n2008.[5] D. Conklinand I. H. Witten: “Multiple viewpoint sys-\ntems for music prediction,” Journal of New Music Re-\nsearch,Vol. 24,No.1,pp.51–73,1995.\n[6] X.Li,G.LiandJ.Bilmes: “Afactoredlanguagemodel\nof quantized pitch and duration,” International Com-\nputerMusicConference ,2006.\n[7] D.M¨ ullensiefen: FANTASTIC:FeatureANalysisTech-\nnology Accessing STatistics (In a Corpus): Technical\nReportv0.9 ,2009.\n[8]http://doc.gold.ac.uk/isms/mmm\n[9] W. Steinbeck: Struktur und ¨Ahnlichkeit: Methoden\nautomatisierter Melodieanalyse , B¨ arenreiter, Kassel,\n1982.\n[10] D. Huron: “The melodic arch in western folksongs,”\nComputinginMusicology,10 ,pp.3–23,1996.\n[11] B. Jesser: Interaktive Melodieanalyse , Peter Lang,\nBern,1991.\n[12]http://jmir.sourceforge.net/\njSymbolic.html\n[13] D. Jurafsky and J. Martin: Speech and Language Pro-\ncessing. Prentice-Hall,EnglewoodCliffs,NJ, 2000.\n[14] D. Conklin: “Melodic analysis with segment classes,”\nMachineLearning ,Vol.65,pp.349–360,2006.\n[15] M.T.Pearce,D.M¨ ullensiefenandG.A.Wiggins: “An\ninformation-dynamicmodelofmelodicsegmentation,”\npresented at the International Workshop on Machine\nLearning and Music. University of Helsinki, Finland.\nJuly,2008.\n[16]http://www.cs.waikato.ac.nz/ml/weka/\n[17] I.H.WittenandE.Frank: DataMining: practicalma-\nchinelearningtoolsandtechniques ,2ndedition,Mor-\nganKaufmann,2005.\n[18] M. A. Hall: Correlation-based Feature Subset Selec-\ntion for Machine Learning , PhD Thesis, Department\nof Computer Science, University of Waikato, Hamil-\nton,New Zealand,1998.\n[19] M. Li and R. Sleep: “Melody classiﬁcation using a\nsimilarity metric based on Kolmogorov complexity,”\nSoundandMusic ComputingConference ,Paris,2004.\n[20] C.R. Lin, N.H. Liu, Y.H. Wu and A.L.P. Chen: “Mu-\nsic classiﬁcation using signiﬁcant repeating patterns,”\nLecture notes in Computer Science , volume 2973,\nSpringer,pp.506–518,2004.\n733"
    },
    {
        "title": "Automatic Detection of Internal and Imperfect Rhymes in Rap Lyrics.",
        "author": [
            "Hussein Hirjee",
            "Daniel G. Brown 0001"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416160",
        "url": "https://doi.org/10.5281/zenodo.1416160",
        "ee": "https://zenodo.org/records/1416160/files/HirjeeB09.pdf",
        "abstract": "Imperfect and internal rhymes are two important features in rap music often ignored in the music information retrieval community. We develop a method of scoring potential rhymes using a probabilistic model based on phoneme frequencies in rap lyrics. We use this scoring scheme to automatically identify internal and line-final rhymes in song lyrics and demonstrate the performance of this method compared to rules-based models. Higher level rhyme features are produced and used to compare rhyming styles in song lyrics from different genres, and for different rap artists.",
        "zenodo_id": 1416160,
        "dblp_key": "conf/ismir/HirjeeB09",
        "keywords": [
            "imperfect",
            "internal",
            "rhymes",
            "music",
            "information",
            "retrieval",
            "community",
            "probabilistic",
            "model",
            "phoneme"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAUTOMATIC DETECTION OF INTERNAL AND IMPERFECT RHYMES\nIN RAP LYRICS\nHussein Hirjee Daniel G. Brown\nUniversity of Waterloo\nCheriton School of Computer Science\n{hahirjee, browndg }@uwaterloo.ca\nABSTRACT\nImperfect and internal rhymes are two important fea-\ntures in rap music often ignored in the music information\nretrieval community. We develop a method of scoring\npotential rhymes using a probabilistic model based on\nphoneme frequencies in rap lyrics. We use this scoring\nscheme to automatically identify internal and line-ﬁnal\nrhymes in song lyrics and demonstrate the performance\nof this method compared to rules-based models. Higher\nlevel rhyme features are produced and used to compare\nrhyming styles in song lyrics from different genres, and\nfor different rap artists.\n1. INTRODUCTION\nSong lyrics have received relatively little attention in mu-\nsic information retrieval, but can provide data about song\nstyle or content that is missing from raw audio ﬁles or user-\ninput tags. Recent work focusing on lyrics [1–3] involves\nusing lyric text to extract song topic, theme, or mood in-\nformation; the pattern and sound of the words themselves\nis usually ignored.\nThese sound features are central to rap music, providing\ninformation about vocal delivery and rhyme scheme. This\ndata can be characteristic of different rappers, as MCs often\nboast of the uniqueness and superiority of their rhyming\nstyle. Lyric rhymes have previously been studied as an\naid in characterizing different musical genres [4], but this\nprior work ignores two stylistic features of rap lyrics: im-\nperfect rhymes, where syllable end sounds are similar but\nnot identical, and internal rhyme, which occurs in the mid-\ndle of lines.\nTo study these features, we have developed a system\nfor automatic detection of rap music rhymes. We train a\nprobabilistic scoring model of rhymes using a corpus of\nrap lyrics known to be rhyming, using ideas derived from\nbioinformatics. We then use this model to ﬁnd and catego-\nrize various rhymes in different song lyrics, and assess the\nmodel’s success. Finally, we calculate high-level statistical\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.rhyme scheme features to attempt to quantitatively model\nand compare rhyming styles between artists and genres.\nOur work allows the automated study of new features in\nrap music, and may be extensible to other genres of song\nlyrics or for poetry analysis.\n2. BACKGROUND\nHip hop music is characterized by lyrics with intermit-\ntent rhymes being rhythmically chanted (rapped) to an ac-\ncompanying beat. In “Old School” rap (dating from the\nlate 1970s to mid 1980s), lyrics typically followed a sim-\nple pattern and contained a single rhyme falling on the\nfourth beat of each bar [5]. Contemporary rap features\nmore varied delivery and many complex rhyme stylistic el-\nements that are often overlooked. Key among these are\nrhymes that are imperfect, extended, or internal. Holtman\n[6] provides a good overview of the abundance of imper-\nfect rhyme in rap lyrics. A normal rhyme involves two syl-\nlables that share the same nucleus (vowel) and coda (end-\ning consonants). Two syllables form an imperfect rhyme if\none of these two parts does not correspond exactly. How-\never, these types of rhymes are not just composed of vow-\nels and consonants being paired randomly; there is a con-\nstraint to the amount of dissimilarity in these rhymes, de-\ntermined by the shared articulatory features of matching\nphonemes.\nIn Holtman’s hierarchy, the most similar consonants are\nnasals, fricatives, and plosives differing only in place of\narticulation, as in the line-ending /m/ and /n/ phonemes in:\nEntertain and tear you out of your frame\nLeave you in a puddle of blood, then let it rain. [7]\n(Rhyming syllables in quoted lyrics are displayed with\nthe same font style.) Less similar consonant pairs include\nthose with the same place of articulation, but differing in\nvoice or continuancy, such as the /k/ and /g/ pair in:\nBring a bullet-proof vest, nothin’ to ricochet\nReady to aim at the brain, now what the trigger say ? [7]\nV owels are most similar when differing only in height or\n“length” (advanced tongue root), such as the penultimate\nvowels in:\nI’m the alpha, with no omega\nBeginning without the, end so play the . [7]\nHoltman’s work is largely taxonomic and describes\nknown rhymes, rather than discovering them. Hence, we\n711Oral Session 8: Lyrics\nused a statistical model of phonetic similarity based on\nfrequencies in actual rap lyrics. However, the patterns we\nautomatically discovered largely validate her taxonomy.\nRap music often features triplet or longer rhymes with\nunstressed syllables following the initial stressed pair,\nwhich may span multiple words (mosaic rhymes). Longer\nrhymes can also include more than one pair of stressed\nsyllables:\nMaybe my sense of h´umor gets ´ınto you\nBut girl, they can make a per f´ume from the sc ´ent of you .\n[8]\n(Here the accents mark the syllables with primary stress.)\nFinally, contemporary rap music features dazzlingly com-\nplex internal rhyme. Alim [9] analyzes Pharoahe Monch’s\n1999 album Internal Affairs [10] as a case study, and iden-\ntiﬁes chain rhymes, compound rhymes, and bridge rhymes.\nChain rhymes are consecutive words or phrases in which\neach rhymes with the previous, as in:\nNew York City gritty committee pity the fool that\nActshitty in the midst of the calm the witty , [10]\nwhere “city”, “gritty”, “committee”, and “pity” participate\nin a chain. Compound rhymes are formed when two pairs\nof line internal rhymes overlap within a single line. A good\nexample of this is given in “Ofﬁcial”:\nYo, I stick around like hockey, now what the puck\nCooler than fuck , ma neuver like Van couver Canucks ,\n[10]\nwhere “maneuver” and “Vancouver” are found between\n“fuck” and “Canucks.” Bridge rhymes are internal rhymes\nspanning two lines:\nHow I made it yousalivated over my calibrated\nRAPSthatvalidated my ghetto credi bility\nStill I be PACK in agilities unseen\nForreal-a my killin a bilities unclean facilities . [10]\nHere, we call pairs in which both members are internal\n(such as “agilities” / “abilities”) bridge rhymes, and those\nwhere the ﬁrst word or phrase is line-ﬁnal (such as “cali-\nbrated” / “validated”), link rhymes.\n3. FINDING RHYMES AUTOMATICALLY: A\nPROBABILISTIC MODEL\nWe modeled our rhyme detection program after local align-\nment protein homology detection algorithms using BLO-\nSUM (BLOcks of amino acid SUbstition Matrix) [11]. In\nthis framework, pairs of proteins are modeled as sequences\nof symbols generated either randomly or based on shared\nancestry (homology). Pairs of matched amino acids re-\nceive a log-odds score in the BLOSUM matrix M: a pos-\nitive score indicates the pair more likely co-occurs due to\nhomology, and a negative score indicates the pair is more\nlikely to co-occur due to chance. Scores are in log-odds:\nM[i, j] = log2(Pr[i, j|H]/Pr[i, j|R]), where H is a model\nof related proteins (obtained by counting the frequency\nwith which we see symbols iandjmatched to each other\nin proteins known to be homologous) and R is the fre-\nquency of the symbols iandjin random proteins (obtainedfrom frequency counts over all proteins). If a pair of pro-\ntein sequences contains regions in which the amino acids\nalign to give high scores, the pair is considered to be ho-\nmologous.\nIn our work, song lyrics are transcribed into sets of se-\nquences of syllables, with each sequence corresponding to\na line of text. Similar to Kawahara’s [12] treatment of con-\nsonants in Japanese rap lyrics, probabilistic methods are\nused to calculate similarity scores for any given pair of syl-\nlables. Phonemes which match with each other in rhyming\nphrases more often than expected by chance receive pos-\nitive scores, while those which match less often than ex-\npected receive negative scores. Regions with syllables that,\nwhen matched to each other, have total score surpassing a\nthreshold are identiﬁed as rhymes.\n4. RHYMING SYLLABLES\nTo generate models of rhyming and randomly co-occurring\nsyllables in rap lyrics, we needed a data set of known\nrhymes. Our training corpus includes the lyrics of 31\ninﬂuential albums from the “Golden Age” of rap (1984-\n1994), chosen because they received the highest rating\nfrom The Source, the top-selling US rap music magazine\nof the time, plus nine additional albums by inﬂuential\nartists from the time period (Run-DMC, LL Cool J, The\nBeastie Boys, Public Enemy, Eric B. and Rakim). We\ndownloaded lyrics from the Web and manually corrected\nthem to ﬁx typos and ensure that pairs of consecutive lines\nended with matching rhymes, yielding 27,956 lines of\nlyrics (13,978 rhymed pairs), approximately 700 lines per\nalbum.\nWe ﬁrst transcribe plaintext lyrics into sequences of\nphonemes using a wrapper we built around the Carnegie\nMellon University (CMU) Pronouncing Dictionary [13],\nwhich gives phonemes and stress markings for words in\nNorth American English. We augmented the dictionary\nwith slang terms and common elements of hip-hop vernac-\nular (e.g., the “-in” ending in “runnin’ ”, or the “-a” ending\nin “brotha” or “killa”), and reduced the stress assigned\nto common one-syllable words of minor signiﬁcance in\nrhyme (“a”, “I”, etc.). To handle words not found in\nthe augmented dictionary, we added the Naval Research\nLaboratory’s text-to-phoneme rules [14].\n5. SCORING POTENTIAL RHYMES\nTo generate a log-odds scoring matrix for rhyming syl-\nlables, we need models for random syllables and for\nrhymes. For any pair of syllables iandj, the random\nmodel, Pr[i, j|Random], gives the likelihood of iandj\nbeing matched together by chance while the rhyme model,\nPr[i, j|Rhyme], gives the likelihood of iandjbeing paired\nin a true rhyme. As in BLOSUM [11], the log-odds score\nis calculated as ln(Pr[ i, j|Rhyme ]/Pr[i, j|Random ]). To\navoid overﬁtting, we reduce each syllable to its vowel\n(nucleus), end consonants (coda), and stress—the relevant\nfeatures for determining rhyme. We approximate the coda\nby taking the ﬁrst half (rounded up) of the consonants\n71210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nbetween adjacent pairs of vowels. Both models are trained\nusing the occurrence frequencies of phonemes in the\ntraining data.\nIn the random model, the likelihood of vowel amatch-\ning with vowel bis calculated by taking the product of the\nfrequencies of aandb. The likelihoods for consonants and\nvarying stress are calculated in the same manner. For the\nrhyming model, the likelihood of vowels aandbbeing\nmatched is calculated by taking the number of times aand\nbare seen matching in known rhymes, and dividing by the\ntotal number of matched vowel pairs in known rhymes.\nThen the log-odds score for the vowels is calculated as\nvowelScore (a, b) = ln(Pr[ a, b|Rhyme ]/Pr[a, b|Random ]).\nThe likelihood for consonants is more complicated\nsince we must also consider unmatched consonants when\naligning syllable codas of differing size. We use an iterated\napproach to solve these problems. In the ﬁrst pass over\nthe training data, we produce initial vowel and consonant\nscoring matrices by calculating the statistics above. We\nconsider rhymes in paired lines to be all syllables follow-\ning the ﬁnal primary-stressed syllable, after Holtman [6].\nIn the second pass, we identify the start of rhymes by\nmoving backwards from the end of the line while initial\nscores for stressed syllables are positive. We perform\nglobal alignment [15] on matched codas to determine\nfrequencies for consonants pairing with other consonants,\nand being unmatched at the start or end of the coda. This\ndistinction is useful since some consonants (such as /l/ and\n/r/) are more likely to be unmatched at the beginning of\nclusters, and others (often coronals, such as /d/ and /z/)\nare more likely to be unmatched at the ends of clusters. A\nsimple example of this is found in the repeated occurrences\nof “alarmed” rhyming with “bomb” in Public Enemy’s\n“Louder Than A Bomb.” [16]\nUsing these frequency statistics, we produce the\nrhyming model and log-odds scores for consonants and\nstress in the same way as for vowels. Finally, we normal-\nize the consonant score by dividing by the length of the\ncoda to avoid the problem of syllables with long codas\nhaving the consonant score dominate. Intuitively, “win”\nand “gin” rhyme as well as “splints” and “mints.” Since\nall the constituent scores are log-odds, they can be added\ntogether to form a combined probabilistic log score. The\nﬁnal score for two given syllables is the sum of the vowel\nscore, normalized consonant score, and stress score.\nTables 1 and 2 show the pairwise scoring matrices. The\nsymbols “ *” and “* ” indicate scores for unmatched con-\nsonants at the beginning and end of codas, respectively.\nHigh scores for pairs like (/m/,/n/) and (/k/,/p/) largely val-\nidate Holtman’s hierarchy [6].\n6. RHYME DETECTION ALGORITHM\nWith our probabilistic scoring method for matched sylla-\nbles in place, we need a procedure to identify internal and\nend rhymes. Our technique is a variant on local align-\nment [15]; for each syllable, we identify its closest pre-\nceding rhyming syllable, and longest preceding rhyming\nphrase within the current and previous lines. For example,AA AE AH AO AW AY EH ER EY IH IY OW OY UH UW\nAA 2.3 -3.3 -0.8 1.6 -1.7 -2.7 -7.2 -0.6 -3.9 -4.8 -3.9 -1.0 -1.7 -3.3 -3.9\nAE 2.1 -1.5 -6.6 -1.9 -3.3 -1.5 -3.4 -1.8 -2.0 -4.3 -4.6 -4.5 -3.7 -6.7\nAH 2.2 -1.2 -1.4 -1.4 -0.6 -0.2 -1.7 -0.3 -3.0 -1.0 -0.6 -0.9 -1.5\nAO 3.1 -1.0 -3.8 -6.5 -1.1 -3.9 -4.2 -6.3 -0.3 -0.4 1.1 -3.3\nAW 3.8 -0.3 -6.0 -4.2 -5.7 -6.0 -5.7 -2.0 -2.9 -4.5 -1.4\nAY 2.5 -4.2 -1.1 -7.0 -1.8 -3.2 -4.3 -1.1 -5.7 -6.4\nEH 1.9 -1.2 -1.5 0.2 -2.1 -7.0 -4.5 -6.1 -4.3\nER 3.9 -5.6 -1.5 -5.5 -1.6 -2.7 -1.3 -2.6\nEY 2.5 -3.4 -2.7 -4.4 -4.3 -5.8 -6.5\nIH 2.0 -0.9 -7.1 0.2 -2.2 -3.7\nIY 2.4 -4.4 -4.2 -5.8 -6.4\nOW 2.8 -4.0 -2.5 -1.5\nOY 4.9 0.1 -3.7\nUH 2.6 -0.5\nUW 3.1\nTable 1 . Scoring Matrix for V owels\ngiven the line\nUnob tainable to the brain it’sunexplain able what the\nverse’ll do [10]\nfrom Pharoahe Monch’s “Right Here,” the middle “ain”\nsyllables all rhyme, while the whole of “unexplainable”\nalso rhymes with “unobtainable.”\nFor every pair of consecutive lines in a set of lyrics,\nwe ﬁrst construct a two-dimensional matrix of the score\nfor every pair of syllables. Entries in this matrix (corre-\nsponding to pairs of syllables in the lines) are selected as\n“anchors” if they have score above a threshold and con-\ntain a stressed syllable or are line-ﬁnal. From these anchor\npositions, rhymes are extended forward, ensuring that the\nlength-normalized score is above a syllable threshold. In\naddition to the iterative extension, a “jump”-type exten-\nsion is also allowed, in which one or two syllables can be\nskipped over if the following syllable pair is an anchor type\nwith score above a higher threshold. This was included\nsince longer polysyllabic mosaic rhymes often contain one\nor two syllables that do not rhyme in the midst of three\nor four that do. A good example of this can be found in\nFabolous’ “Can’t Deny It”:\nIkeep spittin’, them clips copped on those calicos\nKeep shittin’, with ziplocks of that Cali ’dro [8]\nwhere the two lines rhyme in their entirety, with the excep-\ntion of “them”/“with” and “those”/“that.”\nWe ﬁltered the set of rhymes to remove one-syllable\nrhymes including unstressed syllables, as these tended to\nbe noise. After a set of rhymes was identiﬁed, we removed\nduplicates and consolidated consecutive and overlapping\nrhymes together.\n7. V ALIDATING THE METHOD\nOur ﬁrst test veriﬁes that our probabilistic score for sylla-\nble rhyming is better at identifying perfect and imperfect\nrhymes than rules-based phonetic similarity measures. We\ndid a 10-fold cross validation where we chose 36 albums\nfrom the training data, trained a rhyme model for those al-\nbums, and used it to score the known rhyming lines from\nthe other four albums (true positives) as well as randomly\nselected lines from those four albums (presumed to be true\n713Oral Session 8: Lyrics\nB CH D DH F G JH K L M N NG P R S SH T TH V Z ZH * *\nB 4.3 -4.8 1.1 0.4 -5.5 1.9 1.9 -6.9 -0.3 -0.5 -1.6 -5.5 0.1 -0.9 -1.6 -4.6 -1.0 -4.3 2.3 0.3 -2.5 -0.6 -1.5\nCH 4.2 -1.6 -4.9 -0.3 0.3 0.4 1.5 -6.8 -6.6 -2.8 -5.5 1.1 -6.7 0.3 0.6 0.9 1.4 -6.1 -2.0 -2.5 -6.0 -2.6\nD 2.3 -7.0 -7.6 0.1 0.2 -3.1 -1.7 -2.2 -2.2 -3.0 -1.8 -0.9 -9.0 -2.1 0.2 0.0 -0.2 0.0 -4.6 -0.2 1.2\nDH 3.5 -5.6 -5.1 -4.2 -0.4 -0.2 -2.0 -7.5 -5.6 -6.2 -1.4 -7.0 -4.8 -0.3 1.3 2.8 1.1 -2.6 -6.0 -3.4\nF 3.4 -1.2 -4.9 -0.3 -1.5 -1.3 -3.5 -1.6 1.1 -2.7 1.1 1.2 -0.9 4.0 0.6 -7.3 -3.2 -1.4 -2.9\nG 4.2 1.9 0.0 -0.2 -1.0 -1.9 -5.7 -0.6 -0.8 -2.5 -4.9 -1.1 -4.5 0.3 -0.3 -2.7 -0.9 -2.8\nJH 5.2 -6.3 -1.5 0.1 -0.5 -4.8 -0.2 -0.3 -0.6 0.6 -1.1 -3.6 1.4 1.0 4.1 -5.3 0.5\nK 2.6 -2.9 -2.1 -2.6 -1.3 1.7 -2.1 -0.7 -0.6 0.9 0.5 -1.8 -3.1 -4.7 -1.0 -1.8\nL 2.8 -1.8 -1.8 -2.8 -8.1 -0.5 -2.9 -6.6 -2.9 -6.3 -1.3 -1.6 -4.5 0.4 -1.0\nM 2.7 1.8 0.7 -3.2 -1.2 -2.9 -1.1 -2.5 0.4 -0.6 -3.7 -4.2 -0.8 -1.7\nN 2.2 1.2 -2.5 -1.0 -2.3 -0.7 -1.5 -0.6 -1.5 -2.1 -5.1 -0.4 -2.3\nNG 4.1 -6.8 -2.7 -2.3 -5.3 -3.5 -5.0 -2.1 -2.0 -3.2 0.2 -3.9\nP 3.3 -2.0 -1.1 -0.7 1.1 0.9 -0.6 -7.9 -3.8 -0.7 -0.8\nR 2.8 -2.3 -0.8 -1.2 -6.1 -2.1 -2.2 -4.3 1.7 -0.7\nS 2.6 2.4 -1.0 1.0 -2.4 0.5 0.0 0.6 0.6\nSH 5.2 -0.6 -4.1 -1.3 -0.2 3.6 -5.8 -7.7\nT 1.7 1.6 -0.9 -9.2 -5.2 0.0 0.7\nTH 4.4 0.5 -6.1 -2.0 -5.4 -0.6\nV 2.9 -0.4 1.6 -1.2 -1.7\nZ 2.6 3.0 -1.3 1.1\nZH 6.8 -3.7 -5.6\nTable 2 . Scoring Matrix for Consonants\nnegatives). We developed implementations of the mini-\nmal mismatch of articulatory features and Kondrak align-\nment [17] metrics to compare the performance of these\nscoring measures, which are based on the physical process\nof the human voice. We show receiver operator character-\nistic (ROC) curves comparing the true positive rate to false\npositive rate when varying the score threshold for each of\nthe three methods in Figure 1. The probabilistic method\nsigniﬁcantly outperforms both simpler rules-based meth-\nods.\nFigure 1 . ROC curves for the three different scoring meth-\nods, comparing percentage of actual rhymes found by algo-\nrithm on the y-axis with percentage of unrelated syllables\ndetected as rhyming on the x-axis\nNext, we considered false positives and negatives for\ndetected end rhymes, using the score threshold of 1.5\n(meaning matched syllables are at least e1.5times more\nlikely to rhyme than expected by chance). Out of 1000\npairs of unrelated random lines from our training data,\n79 syllables were marked as parts of end rhymes (“false\npositives”) by our procedure. Of these, 22 were in facttrue rhymes, with scores higher than 3.0. 30 were near-\nrhymes; that is, that they could be found (though less\nfrequently) as line ﬁnal rhymes in actual lyrics. Usu-\nally scoring above 2.0, they included matches such as\n“stiff”/“ﬁt”, “pen”/“thing”, and “cling”/“smothering”,\nwith more than one articulatory difference or different\nstress. 14 matched end syllables (often sufﬁxes), typically\nwith high scores (greater than 3.0). Examples such as\n“weak er”/“drumm er” and “tapp in’”/“posit ion”, may have\nexact matches, but are not relevant rhymes due to their\nlack of stress. The remaining 13 moderately high scoring\n(between 1.5 and 2.5) pairs featured either high consonant\nscores (like “bust”/“test”) or high vowel scores due to\nmatching rare vowel sounds (“box”/“wrong”).\nFrom a set of 1000 matched pairs of lines, we used the\niterative method (moving backwards from the end of the\nline while scores for stressed syllables are positive) to see\nwhich true rhymes would be missed. Pairs with all such\nmatches scoring less than 1.5 were marked and treated as\nfalse negatives. Out of 132 such syllables, the largest group\n(48) were moderately low scoring (between -1.0 and 1.5)\npairs participating in polysyllabic and mosaic rhymes. A\ngood example of this is “battery”/“battle me” in Eric B.\nand Rakim’s “No Omega” [7]; many of these were ﬂanked\nby high scoring pairs, and would be included in rhymes us-\ning the jump extension described in the above section. 35\nwere very low scoring pairs (less than 0.0) which were ei-\nther caused by words having been transcribed improperly\nor the lack of a true rhyme in the lyrics. 22 were caused\nby the rhyme start being extended too far back and start-\ning with a low positive scoring pair. Again, this would\nnot cause problems in our actual detection algorithm since,\nin that case, rhymes are extended forward from stressed\nanchors. 17 were caused by differences between the ac-\ntual pronunciation and the dictionary’s pronunciation (“po-\nems” treated as one syllable, or “battles” speciﬁcally being\npronounced to rhyme with “shadows”). Finally, 10 were\ncaused by deliberate mismatch in syllable stress.\nThe probabilistic model is quite good at ﬁnding both\nperfect and imperfect rhymes. Quite few syllable pairs\n71410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n(less than 15 in the 1000 line pairs) scored highly without\nbeing perceivably rhyming, and most low scoring “true”\nrhyme pairs take part in complex mosaic and polysyllabic\nrhymes.\nFinally, we used our model on a set of manually an-\nnotated rap lyrics, to measure the ability of the program\nto ﬁnd both internal and line-ﬁnal rhymes. We used ﬁve\nsongs of varying style: the Beastie Boys’ “Intergalac-\ntic”, a Grammy-winning song in the old-school style;\nPharoahe Monch’s “The Truth” (featuring Common and\nTalib Kweli) and “Right Here”, which were annotated by\nAlim [9] and feature high rhyme density and a compli-\ncated scheme; Jay-Z and Eminem’s “Renegade”, which\nfeatures very high rhyme density; and Fabolous’ “Trade\nIt All (Part 2)”, a song speciﬁcally mentioned by Alim\nfor its prevalence of long (ﬁve or six syllable) rhymes.\nWe show the ROC curves for this test set in Figure 2; the\nbest overall performance is for speciﬁcity and sensitivity\njust above 60%. Most “false positive” are rhymes that\nwere not annotated due to lack of rhythmic importance\nor accidental omission. False negatives included several\nwhere the performer created a rhyme from words that do\nnot appear to rhyme as text, and some longer rhymes that\nwere cut off prematurely due to too many non-rhyming\nsyllables within them and lower scoring syllable pairs\nsurrounding them. Finally, some rhymes were missed due\nto intervening rhymes being found between the rhyming\nparts, particularly when the threshold for rhymes is set\nlow. This is especially evident in the ROC curves at lower\ncut-off thresholds, where true positive rates peak around\n80% and begin to decline as the threshold is lowered.\n8. EXPERIMENTS\nWe used our procedure to examine a variety of features\nabout the rhymes in several sets of lyrics. We computed\nthe number of syllables per line, the number of rhymes\nper line, the number of rhymes per syllable, average end\nrhyme scores, and proportion of rhymes having two, three,\nfour, or more syllables. We also counted all of the complex\nrhyming features (bridge, link, chain and internal rhymes)\nper line.\nWe hypothesized that these features would show dif-\nferences between genres of popular music, and calculated\nthem for four sets of data: the top 10 songs from Bill-\nboard Magazine’s 2008 year-end Hot Rap Singles chart;\nthe top 20 songs from the 2008 year-end Hot Modern Rock\nSongs chart; the ﬁrst 400 lines of Milton’s “Paradise Lost”\n[18], as a similar-sized sample of non-rhyming verse; and\nthe top 10 songs from the 1998 year-end Hot Rap Singles\nchart. To compare the verses most of all, the song lyrics\nwere modiﬁed to remove intro/outro text, repeated lines,\nand additional choruses. Our results are in Table 3. High\nend rhyme scores are indicative of song lyrics in general\n(relative to unrhymed verse); rap has higher rhyme density,\ninternal rhyme, link rhymes, and bridge rhymes. Interest-\ningly, blank verse and rock lyrics have similar amounts of\nrhyming per line, but rock lyrics have more rhymes per syl-\nlable. The data from 1998 and 2008 rap songs suggest thatin their rhyming pattern, there has not been much shift in\nstyle.\nRap ’08 Rap ’98 Rock Blank\nNumber of Lines 476 613 502 400\nNumber of Syllables 4646 6492 4053 4146\nSyllables per Line 9.76 10.59 8.07 10.37\nNumber of Rhymes 794 1118 476 393\nRhymes per Line 1.67 1.82 0.95 0.98\nRhymes per Syllable 0.17 0.17 0.12 0.09\nRhyme Density 0.28 0.27 0.19 0.12\nAverage End Score 5.28 5.21 4.36 2.49\nper Syllable 3.75 3.67 4.01 2.28\nDoubles per Rhyme 0.23 0.29 0.15 0.18\nTriples per Rhyme 0.08 0.06 0.04 0.03\nQuads per Rhyme 0.02 0.03 0.05 0.00\nLongs per Rhyme 0.03 0.02 0.04 0.01\nInternals per Line 0.62 0.60 0.27 0.28\nLinks per Line 0.20 0.28 0.13 0.16\nBridges per Line 0.43 0.48 0.28 0.40\nChaining per Line 0.32 0.18 0.15 0.07\nTable 3 . Rhyme Features for Different Genres\nWe also hypothesized that features of individual rap-\npers might also be informative, so we produced these\nstatistics for albums by nine famous MCs from a diverse\nrange of styles and eras: Run-DMC, Rakim, Notorious\nB.I.G., 2Pac, Jay-Z, Fabolous, Eminem, 50 Cent, and Lil’\nWayne. Features were calculated for segments of at least\n40 lines to produce means and standard deviations of the\nstatistics for each album. The results indicate that many\nof these features can be characteristic of different artists’\nstyles. For example, Run-DMC’s (1984) old-school style\nhas lower rhyme density and less internal rhyme with an\naverage of 1.5 rhymes per line and only 6% of rhymes\nbeing longer than 2 syllables; while Rakim (1987), known\nfor his more complex style, is detected as using more\ninternal rhymes (0.63 per line to Run-DMC’s 0.48) and\nmore rhymes longer than 2 syllables (12%). Rival rappers\nNotorious B.I.G. (1994) and Tupac Shakur (1995) display\nfairly similar style characteristics: 28% of their rhymes\nare 2 syllables long, 6% are three syllables, and 3% are\nlonger. However, Biggie’s lines are signiﬁcantly shorter in\nlength, with, on average, 10.8 syllables to 2Pac’s 11.6.\nArtists from the early 2000s like Jay-Z (2001), Eminem\n(2000), and especially Fabolous (2001) favour longer\nrhymes, with 15%, 17%, and 30% respectively of their\nrhymes being longer than 2 syllables. They also have the\nmost rhyme density overall, with 2.2, 2.3, and 1.9 rhymes\nper line respectively. Jay-Z and Eminem tend to use\nmore internal rhyme as well, having 0.8 internal rhymes\nper line–about 25% higher than the average among other\nMCs. Although he portrays a “thug” persona, 50 Cent\n(2003) uses the most syllables per line (12.1), while Lil’\nWayne (2008) has the fewest (10.2). However, he manages\nhigh rhyme density (0.3 rhymed syllables for each syllable\nused) with relatively few (only 1.8) rhymes per line. In\ngeneral, we ﬁnd that automatic rhyme detection can yield\ncharacteristic data about performers and genres.\n715Oral Session 8: Lyrics\nFigure 2 . Rhyme Detection Syllable ROC Curves for Different Songs. The y-axis indicates the percentage of true rhymes\nidentiﬁed by the algorithm, while the x-axis shows the percentage of automatically identiﬁed rhymes not considered to be\ntrue rhymes.\n9. CONCLUSION\nUsing a probabilistic scoring model, we were able to iden-\ntify both perfect and imperfect rhymes with a higher level\nof accuracy then simpler rules-based methods. The heuris-\ntic rhyme detection methods achieved moderate success at\nﬁnding both internal and line-ﬁnal rhymes in song lyrics.\nMore importantly, statistical features of these rhymes did\ncorrespond to real world characterizations of rhyme style,\nand many of these features are quite consistent within in-\ndividual artists’ lyrics and varied between different artists.\nThis leads to the possibility that automatically calculated\nrhyme statistics can be used to make meaningful catego-\nrizations and recommendations based on rhyme style.\n10. REFERENCES\n[1]B. Wei, C. Zhang, M. Ogihara: “Keyword Generation for\nLyrics,” Proceedings of the International Conference on Mu-\nsic Information Retrieval , 2007.\n[2]F. Kleedorfer, P. Knees, T. Pohle: “Oh Oh Oh Whoah! To-\nwards Automatic Topic Detection in Song Lyrics,” Proceed-\nings of the International Conference on Music Information\nRetrieval , pp. 287–292, 2008.\n[3]H. Fujihara, M. Goto, J. Ogata: “Hyperlinking Lyrics: A\nMethod for Creating Hyperlinks Between Phrases in Song\nLyrics,” Proceedings of the International Conference on Mu-\nsic Information Retrieval , pp. 281–286, 2008.\n[4]R. Mayer, R. Neumayer, A. Rauber: “Rhyme and Style Fea-\ntures for Musical Genre Classiﬁcation by Song Lyrics,” Pro-\nceedings of the International Conference on Music Informa-\ntion Retrieval , pp. 337–342, 2008.\n[5]Adam Bradley: Book of Rhymes: The Poetics of Hip Hop ,\nBasic Civitas Books, 2009.\n[6]Astrid Holtman: A Generative Theory of Rhyme: An Opti-\nmality Approach , PhD dissertation, Utrecht Institute of Lin-\nguistics, 1996.\n[7]Eric B. and Rakim: Let the Rhythm Hit ’Em , MCA Records,\n1990.[8]Fabolous: Ghetto Fabolous , Elektra Records, 2001.\n[9]H. Samy Alim: “On Some Serious Next Millennium Rap\nIshhh: Pharoahe Monch, Hip Hop Poetics, and the Internal\nRhymes of Internal Affairs,” Journal of English Linguistics ,\nV ol. 31, No. 1, pp. 60–84, 2003.\n[10] Pharoahe Monch: Internal Affairs , Rawkus Records, 1999.\n[11] S. Henikoff and J.G. Henikoff “Amino Acid Substitution\nMatrices from Protein Blocks” Proceedings of the National\nAcademy of Sciences of the United States of America , V ol. 89\nNo. 22 pp. 10915-10919, 1992.\n[12] Shigeto Kawahara: “Half rhymes in Japanese rap lyrics and\nknowledge of similarity,” Journal of East Asian Linguistics ,\nV ol. 16, No. 2, pp. 113–144, 2007.\n[13] Kevin Lenzo: The CMU Pronouncing Dictionary ,\nhttp://www.speech.cs.cmu.edu/cgi-bin/\ncmudict,2007 .\n[14] H.S. Elovitz, R.W. Johnson, A. McHugh, J.E. Shore\n“Automatic translation of English text to phonetics\nby means of letter-to-sound rules,” Interim Report\nNaval Research Lab . Washington, DC., 1976 http:\n//www.speech.cs.cmu.edu/comp.speech/\nSection5/Synth/text.phoneme.3.html .\n[15] R. Durbin, S. Eddy, A. Krogh, G. Mitchison Biological Se-\nquence Analysis: Probabilistic Models of Proteins and Nu-\ncleic Acids , Cambridge University Press, 1999.\n[16] Public Enemy: It Takes a Nation of Millions to Hold Us Back ,\nDef Jam Recordings, 1988.\n[17] Grzegorz Kondrak: “A New Algorithm for the Alignment\nof Phonetic Sequences,” Proceedings of the First Meeting of\nthe North American Chapter of the Association for Compu-\ntational Linguistics , pp. 288–295, 2000.\n[18] John Milton: Paradise Lost , Samuel Simmons, 1667.\n716"
    },
    {
        "title": "Usability Evaluation of Visualization Interfaces for Content-based Music Retrieval Systems.",
        "author": [
            "Keiichiro Hoashi",
            "Shuhei Hamawaki",
            "Hiromi Ishizaki",
            "Yasuhiro Takishima",
            "Jiro Katto"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414834",
        "url": "https://doi.org/10.5281/zenodo.1414834",
        "ee": "https://zenodo.org/records/1414834/files/HoashiHITK09.pdf",
        "abstract": "This research presents a formal user evaluation of a typical visualization method for content-based music information retrieval (MIR) systems, and also proposes a novel interface to improve MIR usability. Numerous interfaces to visualize content-based MIR systems have been proposed, but reports on user evaluations of such proposed GUIs are scarce. This research aims to evaluate the effectiveness of a typical 2-D visualization method for content-based MIR systems, by conducting comparative user evaluations against the traditional list-based format to present MIR results to the user. Based on the observations of the experimental results, we next propose a 3-D visualization system, which features a function to specify sub-regions of the feature space based on genre classification results, and a function which allows users to select features that are assigned to the axes of the 3-D space. Evaluation of this GUI conclude that the functions of the 3-D system can significantly improve both the efficiency and usability of MIR systems.",
        "zenodo_id": 1414834,
        "dblp_key": "conf/ismir/HoashiHITK09",
        "keywords": [
            "user evaluation",
            "typical visualization method",
            "content-based music information retrieval",
            "novel interface",
            "visualize content-based MIR systems",
            "conducted comparative user evaluations",
            "traditional list-based format",
            "proposed 3-D visualization system",
            "functions of the 3-D system",
            "significantly improve both efficiency and usability"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nUSABILITY EVALU ATION OFVISU ALIZA TION INTERF ACES FOR\nCONTENT -BASED MUSIC RETRIEV ALSYSTEMS\nKeiichir oHoashi †Shuhei Hamawaki ‡HiromiIshizaki †Yasuhir oTakishima †JiroKatto ‡\n†KDDI R&D Laboratories Inc.\n{hoashi, ishizaki, takisima }@kddilabs.jp\n‡Graduate School ofScience andEngineering, Waseda University\n{hama waki,katto }@katto.comm.w aseda.ac.jp\nABSTRA CT\nThis research presents aformal user evaluation ofatypical\nvisualization method forcontent-based music information\nretrie val(MIR) systems, andalso proposes anovelinter-\nfacetoimpro veMIR usability .Numerous interf aces tovi-\nsualize content-based MIR systems havebeen proposed,\nbutreports onuser evaluations ofsuch proposed GUIs are\nscarce. This research aims toevaluate theeffectiveness\nofatypical 2-D visualization method forcontent-based\nMIR systems, byconducting comparati veuser evaluations\nagainst thetraditional list-based format topresent MIR re-\nsults totheuser.Based ontheobserv ations oftheexper-\nimental results, wenextpropose a3-D visualization sys-\ntem, which features afunction tospecify sub-re gions of\nthefeature space based ongenre classi\u0002cation results, and\nafunction which allowsusers toselect features thatareas-\nsigned totheaxesofthe3-Dspace. Evaluation ofthisGUI\nconclude thatthefunctions ofthe3-Dsystem cansignif-\nicantly impro veboth theef\u0002cienc yandusability ofMIR\nsystems.\n1.INTR ODUCTION\nThe popularity ofonline music distrib ution services have\nprovided anopportunity forusers toaccess tomillions of\nsongs. Furthermore, therapid spread ofportable devices\nwith largestorage, e.g.,theiPod and3Gmobile phones,\nhasalsoenabled common users tocarry around music col-\nlections, which may consist ofthousands ofsongs. These\ndevelopments haveprompted theneed foreffectivemusic\ninformation retrie val(MIR) technologies, inorder toease\ntheuser burden to\u0002ndsongs which theywanttolisten to.\nItisobvious that, foranyMIR system, theusability of\nitsinterf aceisessential fortheuser toef\u0002ciently search\nforsongs which match their preferences. However,while\nvarious GUIs forMIR systems havebeen proposed, reports\nonuser evaluations ofsuch GUIs arescarce, hence, the\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies are\nnotmade ordistrib uted forpro\u0002t orcommercial advantage andthatcopies\nbear thisnotice andthefullcitation onthe\u0002rstpage.\nc/circlecopyrt2009 International Society forMusic Information Retrie val.effectiveness and/or problems ofvisualizing MIR systems\nareyettobeformally clari\u0002ed.\nThe objecti veofthisresearch istoevaluate theeffec-\ntiveness ofMIR visualization, andderivewhat functions\narenecessary toimpro veitsusability .Inorder toaccom-\nplish thisobjecti ve,we\u0002rst conduct acomparati veuser\nexperiment between atypical 2-D visualization MIR in-\nterface, against thetraditional list-based format. Through\ntheanalysis ofthisexperiment, weverify ifvisualization\nactually contrib utestoimpro vetheef\u0002cienc yofMIR, and\nalso derivepotential problems ofvisualization ingeneral.\nBased ontheknowledge obtained from thisanalysis, we\nnextpropose anextended 3-D interf acewith severalnew\nfunctions, which aimtoresolv etheproblems thathavebe-\ncome apparent from theresults oftheprior experiment.\nComparati veexperiments with theprevious GUI indicate\nthattheadditional functions contrib utetoimpro vetheef\u0002-\ncienc yandentertainability ofMIR systems.\n2.RELA TED WORK\nOneoftheinitial research efforts tovisualize content-based\nMIR istheIslands ofMusic application, developed byPam-\npalk [1]. This application utilizes self-or ganized maps to\nplot songs onatwo-dimensional feature space, andex-\npresses populated clusters inthefeature space byillustrat-\ningislands ofmusic. Furthermore, Lamere etal.have\ndeveloped avisualization application called SearchInside\ntheMusic [2],which calculates audio-based similarity be-\ntween songs inthemusic collection, andlocates highly\nsimilar songs adjacently ina3-Dfeature space.\nRecently ,there havebeen numerous reports tocollect\nmeta-information ofsongs and/or artists from theWeb,and\ndisplay thecollected information ontheuser interf aceof\nMIR systems, tosupport theusers' music searching pro-\ncess. MusicRainbow [3]isanapplication designed todis-\ncoverartists, which maps similar artists onacircular rain-\nbow.Theartist similarity iscalculated based onacoustic\nanalysis oftheartists' songs. Labels oftheartists areap-\nplied byanalyzing Webinformation retrie vedbyusing the\nname oftheartists asthesearch query .Other applications\ntovisualize music with web-based metadata include Mu-\nsicSun [4],anextended application ofMusicRainbow ,as\nwell astheworkpresented in[5,6],etc.\nAsclear from theabovedescriptions ofexisting work,\n207Poster Session 2\nmanyproposals ofuser interf aces forMIR systems have\nbeen made inrecent years. However,thorough user eval-\nuations ofsuch interf aces arescarce. Therefore, itisnot\napparent thatthevarious functions, user interf acedesigns,\netc.which havebeen proposed inprevious work,actually\ncontrib utetoimpro vetheoverall usability ofMIR systems.\nConsidering these problems, wesetthemotivation of\nourresearch to\u0002rstevaluate theeffectiveness oftypical vi-\nsualization interf aces, bycomparison with thetraditional\nlist-based format tooutput MIR results. Furthermore, through\ntheanalysis ofuser logs oftheexperiment, wewillclarify\ntheadvantages anddrawbacks ofvisualization, andutilize\nthisanalysis tofurther impro veusability ofMIR visualiza-\ntioninterf aces.\n3.EVALU ATION OF2-DINTERF ACE\nOur \u0002rst experiment istoevaluate aprototype 2-D inter-\nfaceforcontent-based MIR systems, bycomparison with\nthetraditional list-based interf ace. Forthefollowing ex-\nperiment, 16subjects (allJapanese students incomputer\nscience) haveparticipated toworkonanexperiment task\nusing theMIR systems. Details ofthesystems, andthe\nevaluation experiment areasfollows.\n3.1Systems\n3.1.1 Featur eextraction\nForboth MIR systems used intheexperiment, thesongs\ninthemusic collection arevectorized, based ontheTreeQ\nalgorithm developed byFoote [7]. Fortheinitial training\ndata ofTreeQ, weusethesongs andsub-genre metadata\noftheRWC Genre Database [8].MFCCs areextracted as\nthefeatures used fortheTreeQ method. Inorder toop-\ntimize thefeature space tosuitthecharacteristics ofthe\nexperimental music collection, were-construct thefeature\nspace based onthealgorithm proposed byHoashi etal\n[9].The\u0002nal song vectors aregenerated based onthere-\nconstructed feature space.\n3.1.2 2-Dinterface\nBased onthesong vectors generated bytheprevious pro-\ncess, wehavedeveloped aprototype 2-Dinterf aceforour\nMIR system. Inorder toplotthesong vectors tothe2-D\nspace, thevectors arecompressed totwodimensions, by\nconducting principal component analysis (PCA), andex-\ntracting the\u0002rsttwocomponents ofthePCA results.\nAscreenshot ofthissystem isillustrated inFigure 1.\nThe2-Dinterf aceconsists oftwomajor components: the\nmacr ofeatur espace viewer,which displays theentire uni-\nverse ofthemusic feature space, andthelocal featur e\nspace viewer,which displays aclose-up viewofthearea\nwhere theuser isinterested in.Inthissystem, users can\n\u0002rstselect their area ofinterest, byclicking onthemacro\nfeature space viewer.Next,users canlisten tosongs inthe\nselected area, byclicking ontheplots displayed inthelocal\nfeature space viewer.\nmacro feature \nspace viewermacro feature \nspace viewerlocal feature \nspace viewerlocal feature \nspace viewersong \nplotssong \nplotssong \nplotssong \nplots\nFigur e1.Screenshot of2-Dinterf ace\nList-based \nMIR resultsList-based \nMIR results\nSearch \nbuttonSearch \nbutton\nFigur e2.Screenshot oflist-based interf ace\n3.1.3 List-based interface\nForcomparison with the2-D interf ace, wehavealso de-\nveloped alist-based MIR system, which outputs theMIR\nresults inalist-based format. This interf aceresembles the\nlistformat topresent search results fortypical Websearch\nengines. Ascreenshot ofthissystem isshowninFigure 2.\nInthissystem, arandom listofsongs inthemusic col-\nlection isinitially presented, with thetitleandartist infor -\nmation hidden totheuser.Users aretosearch forthetarget\nsongs, by\u0002rstlistening tothesongs inthisinitial listtose-\nlectaquery ,andclicking ontheSearch button toexecute\nMIR. Thevector similarity between thequery song andall\nother songs arecalculated, andthesongs with high sim-\nilarity arepresented inthelist,sorted accordingly tothe\nsimilarity tothequery song. Users canlisten tothesongs\ninthelistandcontinue searching, byrepeating theMIR\nprocedure forthesongs listed intheMIR results.\n3.2Experiment task\nThetask giventothesubjects oftheexperiment istouse\nthetwoprevious MIR systems, tosearch forsongs per-\nformed byspeci\u0002c artists. Foreach experiment, asetoftar-\ngetsongs, which areperformed byapre-speci\u0002ed Japanese\nartist, areadded tothebase collection, which consists of\n723Korean popsongs (hereafter referred toasK-pop songs).\nNaturally ,theK-pop songs areunfamiliar tothesubjects.\nThe number oftargetsongs ranges from 10to14songs,\ndepending onthetargetartist.\nPrior toeach experiment, thesubjects areprovided with\nasetofsample songs, which areperformed bythetarget\n20810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nartist, butaredifferent from thetargetsongs added tothe\nbase collection. Based ontheimpression oflistening to\nthesample songs, thesubjects then usetheMIR system to\nsearch forthetargetsongs. Asingle task session \u0002nishes,\nwhen thesubject hassuccessfully disco vered oneofthe\ntargetsongs. Each experiment consists ofthree sessions.\nForeach session, adifferent artist isassigned asthetarget.\nEach subject performs thisexperiment, forboth the2-D\nandlist-based systems.\n3.3Evaluation measur es\nComparati veevaluation ofthetwosystems areconducted\nbythefollowing objecti veandsubjecti vemeasures.\n3.3.1 Objective evaluation measur es\nForobjecti veevaluation, wemeasure theef\u0002cienc yofthe\nMIR experiment task, based onthefollowing twomea-\nsures:\n\u000fOperation time (OTime):Time required tocom-\nplete experiment task.\n\u000fNumber ofsong plays (PlayNum) :Number ofsongs\nplayed tocomplete task.\n3.3.2 Subjective evaluation measur es\nForsubjecti veevaluation, each subject isaskedtoapply\na\u0002ve-rank edrating tothesystems, forthefollowing ele-\nments (1:Bad \u00185:Good):\n\u000fOperability (Oper) :Evaluation oftheusability of\ntheinterf ace\n\u000fAccuracy (Acc) :Evaluation oftheaccurac yofMIR\nresults\n\u000fExplicitness (Expl) :Easiness tograsp relationship\nbetween songs intheMIR results\n\u000fEnjoyability (Enj) :Evaluation ofoverall entertain-\nability ofsystem\n3.4Results\n3.4.1 Comparison ofevaluation measur eresults\nInorder todirectly compare thetwosystems, wesumma-\nrizetheevaluation results byanelection-lik eapproach. For\neach subject, theevaluation valueforeach measure iscom-\npared, andavote forthesubject iscast tothesystem\nwith thehigher score. Fortheobjecti vemeasures (OTime,\nPlayNum ),thevoteiscasttothesystem with thesmaller\nvalue, since theef\u0002cienc yofasuperior system should re-\nsultinlowerOTimeandPlayNum .\nTheresulting number ofvotesofallevaluation measure\nforthetwosystems, areillustrated inFigure 3.Note that,\nthevotes ofthesubjects who could notcomplete theex-\nperiment task areomitted from theresults oftheobjecti ve\nmeasures (OTime,PlayNum ).\nComparison ofthenumber ofvotes fortheobjecti ve\nmeasures inFigure 3,showthat thenumber ofsubjects01234567891011121314\nOTime PlayNum Oper Acc Expl EnjNumber of votesList-based\n2-D\n(Equal)\nFigur e3.Number ofvotesfor2-Dandlist-based systems\nwho havecompleted theexperiment task more ef\u0002ciently\nwith the2-D interf aceisapproximately twice ashigh as\nthelist-based interf ace. This result indicates thatthe2-D\ninterf acecontrib utestoimpro veMIR ef\u0002cienc y.However,\nthevoteresults forOper showthat, subjecti vely,theop-\nerability ofthetwosystems donotdifferasmuch asthe\nobjecti veevaluation results suggest. Areason forthisre-\nsultisassumed tobethat, manyofthesubjects aregen-\nerally used tothetraditional list-based interf ace,while the\n2-D interf acerequires time togetacquainted with. Fur-\ntheranalyses todiscuss thisresult willbepresented inthe\nfollowing section.\nTheresults fortheother subjecti veevaluation measures\nshowthatthe2-D interf acehasbeen more well-accepted\nthan thelist-based interf ace. Thereason whythe2-Dsys-\ntemhasrecei vedmore votesforExpl isobvious, since the\nlist-based interf acecanonly present thesimilarity between\nthequery songs andother songs intheMIR results, while\nthe2-D interf acenotonly displays thesimilarity tothe\nquery ,butalso therelati veposition between other songs\nthatareplotted intheinterf ace. Aninteresting observ ation\nisthattheaccurac y(Acc)ofthe2-Dsystem hasrecei ved\nmore votes, despite thefactthatthefeature extraction and\nvectorization methods ofthesystems arethesame. This re-\nsultindicates thatthevisualization ofthefeature space not\nonly impro vesef\u0002cienc yofMIR, butalso applies abetter\nimpression about theaccurac yofMIR results.\n3.4.2 Analysis ofuser logs\nForfurther analysis ofthe2-D system, weanalyze the\nuser logs oftheexperiment. Figures 4and5illustrate the\nPlayNum ofallsubjects, forthethree targetsongs (here-\nafter referred toasJ-pop f1,2,3 g,respecti vely). Forthe\n\u0002rsttargetsong (J-pop1 ),PlayNum isgenerally lowerfor\nthelist-based system, compared tothatofthe2-Dsystem.\nThis indicates thattheinitial ef\u0002cienc yofMIR ishigher\nforthelist-based system, which isassumed tobethereason\nfortheclose voting results forOper inFigure 3.However,\nastheexperiment proceeds tothesecond andthird target\nsongs, adecreasing trend ofPlayNum canbeobserv edfor\nthe2-Dsystem, while nosuch trend isapparent forthelist-\nbased system. These observ ations showthatusers areable\ntosearch formusic with high ef\u0002cienc ybyusing the2-D\nsystem, assoon astheygetacquainted with itsinterf ace.\nNext,weanalyze thesearch logs of2-Dsystem users,\n209Poster Session 2\n050100150200250300350400450\nJ-pop1 J-pop2 J-pop3PlayNum per target song\nFigur e4.PlayNum pertargetsong (2-D system)\n050100150200250300350400450\nJ-pop1 J-pop2 J-pop3PlayNum per target song\nFigur e5.PlayNum pertargetsong (list-based system)\ntoidentify drawbacks ofthe2-D system. Figure 6illus-\ntrates thedistrib ution oftheK-pop andtargetsong plots\ninthe2-D feature space, andFigure 7showsthesearch\npath ofatypical subject when using the2-Dinterf ace. The\nsearch path forthe\u0002rsttargetsong J-pop1 showsthatthe\nsubject \u0002rsttries tograsp thecharacteristics ofthefeature\nspace, bylistening tosongs thatareplotted invarious lo-\ncations. Inthenextsession, i.e.,thesearch forJ-pop2 ,the\nlogshowsthatthesubject initially focuses ontheedges of\nthefeature space, where theacoustic features ofthesongs\nareassumed tobecharacteristic from theothers, andgrad-\nually movestowards thecenter area. Finally ,inthethird\nsession, itisclear thatthesubject isabletodisco verthetar-\ngetsongs (J-pop3 )with ease, assumably based onhis/her\nexperience from theprevious sessions.\nOverall, theaboveexperimental results provide proof\nthatthevisualization interf aceoftheprototype 2-Dsystem\ncontrib utes toimpro vetheef\u0002cienc yandusability ofthe\nMIR process. However,analysis ofuser logs also show\nthatamajor problem ofthevisualization interf aceisthat\nusers arerequired toexperience thesystem suf\u0002ciently ,in\norder tograsp itscharacteristics. Another interpretation of\nthisresult may bethat, manyusers haveexperienced dif\u0002-\nculty tocomprehend thefeatures, i.e.,thetimbral features\noftheMFCC-based TreeQ vectors, thatareused forthe\nvisualization ofthesongs intheMIR system.\n4.3-DMIR SYSTEM INTERF ACE\nInorder toresolv etheproblems thathavebecome appar -\nentfrom theprevious experiment, wenextdevelop anex-\ntended visualization interf aceforcontent-based MIR. This\nsystem features a3-Dvisualization interf acewith thefol-\nlowing additional functions: (1)Selection ofsubfeature\nspaces based ongenre classi\u0002cation, and(2)User selec-\ntion offeatures which de\u0002ne theaxesinthe3-D feature\nspace. Ascreenshot ofthe3-Dsystem isillustrated inFig-First principal componentSecond principal component\nK-pop\nJ-pop1\nJ-pop2\nJ-pop3\nFigur e6.Song plots inthe2-Dinterf ace\nFirst principal componentSecond principal component\nJ-pop1\nJ-pop2\nJ-pop3\nFigur e7.Search path oftypical user of2-Dinterf ace\nure8.The objecti veofthe\u0002rst function istoprovide an\nintuitional waytochoose speci\u0002c areas within thevisual-\nization feature space, inorder toreduce thetime required to\ngrasp itscharacteristics. Thesecond function aims toim-\nprovetheunderstandability ofthevisualization interf ace,\nbyallowing theusers toselect features which theyprefer\ntouseforMIR. Details ofthesystem arepresented inthe\nfollowing sections.\n4.1Selecting subspaces based ongenr eclassi\u0002cation\nAmajor problem ofvisualization, asclari\u0002ed from thepre-\nvious experiment, istheexperience time required forthe\nuser togetacquainted tothevisualization interf ace. In\norder toreduce thisacquaintance time, wehaveadded a\nfunction tothe3-Dsystem, which enables theusertoselect\nsub-spaces inthemacro feature space viewer,byutiliz-\ninggenre classi\u0002cation results.\nThe sub-spaces aregenerated byconducting k-means\nclustering onallvectors ofthesongs included inthemu-\nsiccollection, plus thesong vectors oftheRWC Genre\nDatabase [8],which were used astheinitial training data\ntogenerate thetree-based vector quantizer .Next,labels\nareapplied totheclusters, byreferring tothesub-genre\nmetadata oftheRWC songs thatareclassi\u0002ed toeach of\ntheclusters. Thenumber ofclusters kwassettok=7,\nbased onpreliminary experiments. The labels applied to\ntheclusters arelisted inTable 1.\nEach sub-space isrepresented asasphere inthemacro\nfeature space viewer ofthe3-D system. The labels for\neach sub-space arelisted belowtheviewer.Users candrag\nintheviewertorotate thefeature space, andclick onthe\nsphere which best represents their area ofinterest. This\n21010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nmacro feature \nspace viewermacro feature \nspace viewerlocal feature \nspace viewerlocal feature \nspace viewerfeature \nselectorfeature \nselector\nFigur e8.Screenshot of3-Dinterf ace\nCluster ID Labels\nC1 blues, bossano va,enka, \u0003amenco, reggae\nC2 baroque, bigband, classic, folk, india\nC3 house, rhythm blues, samba, techno\nC4 heavymetal, pops, rock\nC5 ballad, gospel, traditional\nC6 funk, rap\nC7 chanson\nTable 1.Sub-space labels of3-Dsystem\nfunction enables users toselect areas inthefeature space\nintuiti vely,thus isexpected toresolv ethetry-and-error ap-\nproach necessary togetacquainted with theprevious 2-D\nsystem.\n4.2Selection offeatur esforvisualization\nByclicking onasub-space sphere inthemacro feature\nspace viewer,users canviewtheplots ofthesongs which\nbelong intheselected sub-space (cluster), onthelocal fea-\nturespace viewer.Asillustrated inFigure 8,thesong plots\naredisplayed ina3-Dfeature space.\nAnother problem ofvisualization istheunfamiliarity of\nthefeatures used tovisualize music. However,itisalso\nclear thattheappropriateness offeatures differamong in-\ndividual users. Forexample, users with musical experience\nmay consider keysorchords asadequate features forMIR,\nwhile casual music listeners may notbeable todiscrimi-\nnate such high-le velfeatures ofmusic.\nInorder toresolv ethisproblem, wehaveadded the\nfeatur eselector function, which allowstheuser tode\u0002ne\nthefeatures tobeused inthe3-D feature space. Byus-\ningthefeature selector ,system users canselect afeature\nwhich corresponds tothex,y,zaxesinthefeature space.\nThelistoffeatures from which users canselect from, are\nwritten inTable 2,along with thetools used toextract\nthefeatures: TreeQ [10],MIRtoolbox [11], jAudio [12],\nandCOFV iewer[13].Note thattheTimbre features are\nequivalent tothePCA results oftheTreeQ vectors utilized\nintheprevious 2-Dsystem.Feature Description Tool\nTimbre(1,2,3) PCA results (\u0002rst 3compo-\nnents) ofTreeQ vectorsTreeQ\nMan-W oman Categorization score of\nMale/Female vocal TreeQ\nclassi\u0002erTreeQ\nTempo Average tempo MIRtoolbox\nDynamics Overall powerofsong jAudio\nKey Basic keyofsong COFV iewer\nTransK ey Transition ratio ofkeys COFV iewer\nMode Degree ofmajor/minor MIRtoolbox\nStat(1,2,3) PCA results (\u0002rst 3compo-\nnents) ofjAudio featuresjAudio\nTable 2.Listoffeatures in3-Dsystem\n5.EVALU ATION OF3-DINTERF ACE\nWehaveconducted auser experiment toevaluate the3-\nDsystem. The objecti veofthisexperiment istoexam-\ninewhether theproblems ofthe2-Dsystem havebeen re-\nsolvedbytheadditional features ofthe3-D system. The\ntaskoftheexperiment, andthesubjects arethesame asthe\nexperiment described inSection 3.Themeasures used for\nevaluation arealsothesame astheprevious experiment.\n5.1Results anddiscussions\nFirst, wecompare theusability ofthe2-Dand3-Dinter-\nfaces, bycounting thevotes ofthesubjects forallevalu-\nation measures, similar totheevaluation inSection 3.4.1.\nFigure 9illustrates thenumber ofvotes forthe2-D and\n3-Dsystems.\nThe voting results fortheobjecti vemeasures, OTime\nandPlayNum ,indicate thatthe3-Dsystem hascontrib uted\ntoimpro vetheef\u0002cienc yofMIR. Furthermore, thevotes\nforthesubjecti vemeasures showthat thesubjects have\nconsidered the3-Dsystem tobesuperior than the2-Dsys-\ntem, forallevaluation measures.\nAsmentioned inSection 3.4.2, aproblem ofthe2-D\nsystem isthetime required togetused totheinterf ace.\nInorder toexamine ifthe3-D system hasresolv edthis\n211Poster Session 2\n012345678910111213141516\nOTime PlayNum Oper Acc Expl EnjNumber of votes2-D\n3-D\n(Equal)\nFigur e9.Number ofvotesfor2-Dand3-Dsystems\n050100150200250300350400450\nJ-pop1 J-pop2 J-pop3PlayNum per target song\nFigur e10.PlayNum pertargetsong (3-D system)\nproblem, weanalyze thePlayNum ofthe3-D system for\nthethree targetsongs, similar totheanalysis presented in\nFigure 4.This result isillustrated inFigure 10.\nComparison oftheresults inFigures 4and10showthat,\nthePlayNum ofthe\u0002rsttargetsong J-pop1 islowerforthe\n3-Dsystem. This result showsthatusers haverequired less\ntime togetacquainted with theinterf aceofthe3-Dsystem.\nNext,inorder toevaluate theusability ofthefeature\nselection function, weaskedthesubjects toselect which\nfeatures theyconsidered useful fortheexperiment task, af-\ntertheexperiment (subjects areallowed toselect multiple\nfeatures inthisquestionnaire). The number ofselections\nforeach feature islisted inTable 3.\nTheresults inTable 3showthatsystem users havese-\nlected features other than thetimbre features thatwere used\ninthe2-Dsystem. Furthermore, inthisquestionnaire, 11\nofthe16subjects haveselected more than onefeature as\nuseful. These results indicate thatusers haveproacti vely\nselected various features during thetask, meaning thatusers\nhavefavorably accepted thefeature selection function.\n6.CONCLUSION\nInthisresearch, wehaveevaluated theeffectiveness ofvi-\nsualization methods forcontent-based MIR, byconducting\ncomparati veuser experiments ofvarious MIR interf aces.\nThe\u0002rstexperiment, which isbased onaGUI with atyp-\nical2-Dvisualization approach, hasprovedthatvisualiza-\ntion contrib utes toimpro veoverall usability compared to\nthelist-based interf ace, butalso indicate problems forin-\nexperienced users tocomprehend thecharacteristics ofthe\nvisualized feature space. Evaluation oftheextended GUI,\nwhich isadded newfunctions toresolv etheprevious prob-\nlems, makesclear thattheadditional functions further con-\ntributetoimpro vetheusability ofMIR systems. ThroughFeature Noofselections\nMan-W oman 14\nTempo 8\nTimbre 4\nDynamics 4\nKey 1\nTotal 31\nTable 3.Number ofselections perfeature in3-Dsystem\nthisresearch, weconsider ourselv estohavecontrib uted\ntoclarify general user requirements forthevisualization\nofMIR systems, andalso haveproposed asuccessful ap-\nproach tosatisfy such needs.\n7.REFERENCES\n[1]E.Pampalk: Islands ofMusic: Analysis, Organization\nandVisualization ofMusic Archi ves,Master' sthesis,\nVienna University ofTechnology ,2001.\n[2]P.Lamere, D.Eck: Using 3DVisualizations toExplore\nandDisco verMusic, Proc. ofISMIR 2007, 2007.\n[3]E.Pampalk, M.Goto: MusicRainbo w:ANewUser\nInterf acetoDisco verArtists Using Audio-based Simi-\nlarity andWeb-based Labeling, Proc. ofISMIR 2006,\n2006.\n[4]E.Pampalk, M.Goto: MusicSun: ANewApproach to\nArtist Recommendation, Proc. ofISMIR 2007, 2007.\n[5]P.Knees, M.Schedl, T.Pohle, G.Widmer: AnIn-\nnovativeThree-dimensional User Interf aceforExplor -\ningMusic Collections Enriched with Meta-information\nfrom theWeb,Proc. ofACMMultimedia 2006, 2006.\n[6]M.Torrens, P.Hertzog, J.-L. Arcos: Visualizing and\nExploring Personal Music Libraries, Proc. ofISMIR\n2004, 2004.\n[7]J.Foote: Content-based Retrie valofMusic andAudio,\nProc. ofSPIE, Vol.3229, pp.138-147, 1997.\n[8]M.Goto, H.Hashiguchi, T.Nishimura, R.Oka: RWC\nMusic Database: Music Genre Database and Musi-\ncalInstrument Sound Database, Proc. ofISMIR 2003,\n2003.\n[9]K.Hoashi, K.Matsumoto, F.Sugaya, H.Ishizaki,\nJ.Katto: Feature Space Modi\u0002cation forContent-\nbased Music Retrie valbased onUser Preferences,\nProc. ofICASSP 2006, Vol.V,pp.517-520, 2006.\n[10] TreeQ softw are,http://treeq.sourcefor ge.net/\n[11] O.Lartillot: MIRtoolbox, http://www .jyu.\u0002/hum/\nlaitokset/musiikki/en/research/coe/materials/mirtoolbox\n[12] D.McEnnis, C.McKay ,I.Fujinag a,P.Depalle: jAu-\ndio: AFeature Extraction Library ,Proc. ofISMIR\n2005, 2005.\n[13] T.Inoshita, J.Katto: KeyEstimation Using Circle of\nFifth, Proc. ofMMM 2009, 2009.\n212"
    },
    {
        "title": "Easy As CBA: A Simple Probabilistic Model for Tagging Music.",
        "author": [
            "Matthew D. Hoffman",
            "David M. Blei",
            "Perry R. Cook"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417347",
        "url": "https://doi.org/10.5281/zenodo.1417347",
        "ee": "https://zenodo.org/records/1417347/files/HoffmanBC09.pdf",
        "abstract": "Many songs in large music databases are not labeled with semantic tags that could help users sort out the songs they want to listen to from those they do not. If the words that apply to a song can be predicted from audio, then those predictions can be used both to automatically annotate a song with tags, allowing users to get a sense of what qualities characterize a song at a glance. Automatic tag prediction can also drive retrieval by allowing users to search for the songs most strongly characterized by a particular word. We present a probabilistic model that learns to predict the probability that a word applies to a song from audio. Our model is simple to implement, fast to train, predicts tags for new songs quickly, and achieves state-of-the-art performance on annotation and retrieval tasks.",
        "zenodo_id": 1417347,
        "dblp_key": "conf/ismir/HoffmanBC09",
        "keywords": [
            "automatic",
            "annotation",
            "semantic",
            "tags",
            "prediction",
            "audio",
            "retrieve",
            "search",
            "words",
            "qualities"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nEASY AS CBA: A SIMPLE PROBABILISTIC MODEL FOR TAGGING\nMUSIC\nMatthew D. Hoffman\nDept. of Computer Science\nPrinceton University\nmdhoffma at cs.princeton.eduDavid M. Blei\nDept. of Computer Science\nPrinceton University\nblei at cs.princeton.eduPerry R. Cook\nDept. of Computer Science\nDept. of Music\nPrinceton University\nprc at cs.princeton.edu\nABSTRACT\nMany songs in large music databases are not labeled with\nsemantic tags that could help users sort out the songs they\nwant to listen to from those they do not. If the words that\napply to a song can be predicted from audio, then those\npredictions can be used both to automatically annotate a\nsong with tags, allowing users to get a sense of what qual-\nities characterize a song at a glance. Automatic tag predic-\ntion can also drive retrieval by allowing users to search for\nthe songs most strongly characterized by a particular word.\nWe present a probabilistic model that learns to predict the\nprobability that a word applies to a song from audio. Our\nmodel is simple to implement, fast to train, predicts tags\nfor new songs quickly, and achieves state-of-the-art per-\nformance on annotation and retrieval tasks.\n1. INTRODUCTION\nIt has been said that talking about music is like dancing\nabout architecture, but people nonetheless use words to de-\nscribe music. In this paper we will present a simple system\nthat addresses tag prediction from audio—the problem of\npredicting what words people would be likely to use to de-\nscribe a song.\nTwo direct applications of tag prediction are semantic\nannotation and retrieval. If we have an estimate of the\nprobability that a tag applies to a song, then we can say\nwhat words in our vocabulary of tags best describe a given\nsong (automatically annotating it) and what songs in our\ndatabase a given word best describes (allowing us to re-\ntrieve songs from a text query).\nWe present the Codeword Bernoulli Average (CBA)\nmodel, a probabilistic model that attempts to predict the\nprobability that a tag applies to a song based on a vector-\nquantized (VQ) representation of that song’s audio. Our\nCBA-based approach to tag prediction\n•Is easy to implement using a simple EM algorithm.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.•Is fast to train.\n•Makes predictions efﬁciently on unseen data.\n•Performs as well as or better than state-of-the-art ap-\nproaches.\n2. DATA REPRESENTATION\n2.1 The CAL500 data set\nWe train and test our method on the CAL500 dataset [1,\n2]. CAL500 is a corpus of 500 tracks of Western popular\nmusic, each of which has been manually annotated by at\nleast three human labelers. We used the “hard” annotations\nprovided with CAL500, which give a binary value yjw∈\n{0,1}for all songs jand tagswindicating whether tag w\napplies to song j.\nCAL500 is distributed with a set of 10,000 39-dimensional\nMel-Frequency Cepstral Coefﬁcient Delta (MFCC-Delta)\nfeature vectors for each song. Each Delta-MFCC vector\nsummarizes the timbral evolution of three successive 23ms\nwindows of a song. CAL500 provides these feature vec-\ntors in a random order, so no temporal information beyond\na 69ms timescale is available.\nOur goals are to use these features to predict which tags\napply to a given song and which songs are characterized by\na given tag. The ﬁrst task yields an automatic annotation\nsystem, the second yields a semantic retrieval system.\n2.2 A vector-quantized representation\nRather than work directly with the MFCC-Delta feature\nrepresentation, we ﬁrst vector quantize all of the feature\nvectors in the corpus, ignoring for the moment what feature\nvectors came from what songs. We:\n1. Normalize the feature vectors so that they have mean\n0 and standard deviation 1 in each dimension.\n2. Run the k-means algorithm [3] on a subset of ran-\ndomly selected feature vectors to ﬁnd a set of K\ncluster centroids.\n3. For each normalized feature vector fjiin songj, as-\nsign that feature vector to the cluster kjiwith the\nsmallest squared Euclidean distance to fji.\n369Oral Session 5: Tags\nThis vector quantization procedure allows us to represent\neach songjas a vectornjof counts of a discrete set of\ncodewords:\nnjk=Nj/summationdisplay\ni=11(kji=k) (1)\nwherenjkis the number of feature vectors assigned to\ncodewordk,Njis the total number of feature vectors in\nsongj, and 1(a=b)is a function returning 1 if a=band\n0 ifa/negationslash=b.\nThis discrete “bag-of-codewords” representation is less\nrich than the original continuous feature vector representa-\ntion. However, it is effective. Such VQ codebook represen-\ntations have produced state-of-the-art performance in im-\nage annotation and retrieval systems [4], as well as in sys-\ntems for estimating timbral similarity between songs [5,6].\n3. THE CODEWORD BERNOULLI A VERAGE\nMODEL\nIn order to predict what tags will apply to a song and what\nsongs are characterized by a tag, we developed the Code-\nword Bernoulli Average model (CBA). CBA models the\nconditional probability of a tag wappearing in a song j\nconditioned on the empirical distribution njof codewords\nextracted from that song. One we have estimated CBA’s\nhidden parameters from our training data, we will be able\nto quickly estimate this conditional probability for new\nsongs.\n3.1 Related work\nOne class of approaches treats audio tag prediction as a\nset of binary classiﬁcation problems to which variants of\nstandard classiﬁers such as the Support Vector Machine\n(SVM) [7,8] or AdaBoost [9] can be applied. Once a set of\nclassiﬁers has been trained, the classiﬁers attempt to pre-\ndict whether or not each tag applies to previously unseen\nsongs. These predictions come with conﬁdence scores that\ncan be used to rank songs by relevance to a given tag (for\nretrieval), or tags by relevance to a given song (for anno-\ntation). Classiﬁers like SVMs or AdaBoost focus on bi-\nnary classiﬁcation accuracy rather than directly optimiz-\ning the continuous conﬁdence scores that are used for re-\ntrieval tasks, which might lead to suboptimal results for\nthose tasks.\nAnother approach is to ﬁt a generative probabilistic\nmodel such as a Gaussian Mixture Model (GMM) for each\ntag to the audio feature data for all of the songs manifest-\ning that tag [2]. The posterior likelihood p(tag|audio )of\nthe feature data for a new song being generated from the\nmodel for a particular tag is then used to estimate the rel-\nevance of that tag to that song (and vice versa). Although\nthis model tells us how to generate the audio feature data\nfor a song conditioned on a single tag, it does not deﬁne\na generative process for songs with multiple tags, and so\nheuristics are necessary to estimate the posterior likelihood\nof a set of tags.\nRather than assuming that the audio for a song depends\non the tags associated with that song, we will assume that\nnjkβkwzjwyjwKJKWFigure 1 . Graphical model representation of CBA. Shaded\nnodes represent observed variables, unshaded nodes repre-\nsent hidden variables. A directed edge from node ato node\nbdenotes that the variable bdepends on the value of vari-\nablea. Plates (boxes) denote replication by the value in\nthe lower right of the plate. Jis the number of songs, Kis\nthe number of codewords, and Wis the number of unique\ntags.\nthe tags depend on the audio data. This will yield a proba-\nbilistic model with a discriminative ﬂavor, and a more co-\nherent generative process than that in [2].\n3.2 Generative process\nCBA assumes a collection of binary random variables y,\nwithyjw∈{0,1}determining whether or not tag wap-\nplies to song j. These variables are generated in two steps.\nFirst, a codeword zjw∈{1,...,K}is selected with prob-\nability proportional to the number of times njkthat that\ncodeword appears in song j’s feature data:\np(zjw=k|nj,Nj) =njk\nNj(2)\nThen a value for yjwis chosen from a Bernoulli distribu-\ntion with parameter βkw:\np(yjw= 1|zjw,β) =βzjww (3)\np(yjw= 0|zjw,β) = 1−βzjww\nThe full joint distribution over zandyconditioned on\nthe observed counts of codewords nis:\np(z,y|n) =/productdisplay\nw/productdisplay\njnjzjw\nNjβzjww (4)\nThe random variables in CBA and their dependencies\nare summarized in ﬁgure 1.\n3.3 Inference using expectation-maximization\nWe ﬁt CBA with maximum-likelihood (ML) estimation.\nOur goal is to estimate a set of values for our Bernoulli\nparametersβthat will maximize the likelihood p(y|n,β)\nof the observed tags yconditioned on the VQ codeword\ncountsnand the parameters β. Analytic ML estimates\nforβare not available because of the latent variables z.\nWe use the Expectation-Maximization (EM) algorithm, a\nwidely used coordinate ascent algorithm for maximum-\nlikelihood estimation in the presence of latent variables\n[10].\nEach iteration of EM operates in two steps. In the ex-\npectation (“E”) step, we compute the posterior of the latent\n37010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nvariableszgiven our current estimates for the parameters\nβ. We deﬁne a set of expectation variables hjwkcorre-\nsponding to the posterior p(zjw=k|n,y,β):\nhjwk =p(zjw=k|n,y,β) (5)\n=p(yjw|zjw=k,β)p(zjw=k|n)\np(yjw|n,β)(6)\n=\n\nnjkβkwPK\ni=1njiβiwifyjw= 1\nnjk(1−βkw)PK\ni=1nji(1−βiw)ifyjw= 0(7)\nIn the maximization (“M”) step, we ﬁnd maximum-\nlikelihood estimates of the parameters βgiven the ex-\npected posterior sufﬁcient statistics:\nβkw←E[yjw|zjw=k,h] (8)\n=/summationtext\njp(zjw=k|h)yjw/summationtext\njp(zjw=k|h)(9)\n=/summationtext\njhjwkyjw/summationtext\njhjwk(10)\nBy iterating between computing h(using equation 7)\nand updatingβ(using equation 10), we ﬁnd a set of values\nforβunder which our training data become more likely.\n3.4 Generalizing to new songs\nOnce we have inferred a set of Bernoulli parameters β\nfrom our training dataset, we can use them to infer the\nprobability that a tag wwill apply to a previously unseen\nsongjbased on the counts njof codewords for that song:\np(yjw|nj,β) =/summationdisplay\nkp(zjw=k|nj)p(yjw|zjw=k)\np(yjw= 1|nj,β) =1\nNj/summationdisplay\nknjkβkw (11)\nAs a shorthand, we will refer to our inferred value of\np(yjw= 1|nj,β)assjw.\nOnce we have inferred sjwfor all of our songs and\ntags, we can use these inferred probabilities both to re-\ntrieve the songs with the highest probability of having a\nparticular tag and to annotate each song with a subset of\nour vocabulary of tags. In a retrieval system, we return the\nsongs in descending order of sjw. To do automatic tag-\nging, we could annotate each song with the Mmost likely\ntags for that song. However, this may lead to our annotat-\ning many songs with common, uninformative tags such as\n“Not Bizarre/Weird” and a lack of diversity in our annota-\ntions. To compensate for this, we use a simple heuristic:\nwe introduce a “diversity factor” dand discount each sjw\nbydtimes the mean of the estimated probabilities s·w. A\nhigher value of dwill make less common tags more likely\nto appear in annotations, which may lead to less accurate\nbut more informative annotations. The diversity factor d\nhas no impact on retrieval.\nThe cost of computing each sjwusing equation 11 is\nlinear in the number of codewords K, and the cost of vec-\ntor quantizing new songs’ feature data using the previouslycomputed centroids obtained using k-means is linear in the\nnumber of features, the number of codewords K, and the\nlength of the song. For practical values of K, the total cost\nof estimating the probability that a tag applies to a song is\ncomparable to the cost of feature extraction. Our approach\ncan therefore tag new songs efﬁciently, an important fea-\nture for large commercial music databases.\n4. EVALUATION\nWe evaluated our model’s performance on an annotation\ntask and a retrieval task using the CAL500 data set. We\ncompare our results on these tasks with two other sets\nof published results for these tasks on this corpus: those\nobtained by Turnbull et al. using mixture hierarchies\nestimation to learn the parameters to a set of mixture-\nof-Gaussians models [2], and those obtained by Bertin-\nMahieux et al. using a discriminative approach based on\nthe AdaBoost algorithm [9]. In the 2008 MIREX audio tag\nclassiﬁcation task, the approach in [2] was ranked either\nﬁrst or second according to all metrics measuring annota-\ntion or retrieval performance [11].\n4.1 Annotation task\nTo evaluate our model’s ability to automatically tag unla-\nbeled songs, we measured its average per-word precision\nand recall on held-out data using tenfold cross-validation.\nFirst, we vector quantized our data using k-means. We\ntested VQ codebook sizes from K= 5toK= 2500 . After\nﬁnding a set of Kcentroids using k-means on a randomly\nchosen subset of 125,000 of the Delta-MFCC vectors (250\nfeature vectors per song), we labeled each Delta-MFCC\nvector in each song with the index of the cluster centroid\nwhose squared Euclidean distance to that vector was small-\nest. Each song jwas then represented as a K-dimensional\nvectornj, withnjkgiving the number of times label kap-\npeared in song j, as described in equation 1.\nWe ran a tenfold cross-validation experiment modeled\nafter the experiments in [2]. We split our data into 10 dis-\njoint 50-song test sets at random, and for each test set\n1. We iterated the EM algorithm described in section\n3.3 on the remaining 450 songs to estimate the pa-\nrametersβ. We stopped iterating once the negative\nlog-likelihood of the training labels conditioned on\nβandndecreased by less than 0.1% per iteration.\n2. Using equation 11, for each tag wand each song j\nin the test set we estimated p(yjw|nj,β), the prob-\nability of song jbeing characterized by tag wcon-\nditioned onβand the vector quantized feature data\nnj.\n3. We subtracted d= 1.25times the average condi-\ntional probability of tag wfrom our estimate\nofp(yjw|nj,β)for each song jto get a score sjw\nfor each song.\n4. We annotated each song jwith the ten tags with the\nhighest scores sjw.\n371Oral Session 5: Tags\nModel Precision Recall F-Score AP AROC\nUpperBnd 0.712 (0.007) 0.375 (0.006) 0.491 1 1\nRandom 0.144 (0.004) 0.064 (0.002) 0.089 0.231 (0.004) 0.503 (0.004)\nMixHier 0.265 (0.007) 0.158 (0.006) 0.198 0.390 (0.004) 0.710 (0.004)\nAutotag (MFCC) 0.281 0.131 0.179 0.305 0.678\nAutotag (afeats exp.) 0.312 0.153 0.205 0.385 0.674\nCBAK= 5 0.198 (0.006) 0.107 (0.005) 0.139 0.328 (0.009) 0.707 (0.007)\nCBAK= 10 0.214 (0.006) 0.111 (0.006) 0.146 0.336 (0.007) 0.715 (0.007)\nCBAK= 25 0.247 (0.007) 0.134 (0.007) 0.174 0.352 (0.008) 0.734 (0.008)\nCBAK= 50 0.257 (0.009) 0.145 (0.007) 0.185 0.366 (0.009) 0.746 (0.008)\nCBAK= 100 0.263 (0.007) 0.149 (0.004) 0.190 0.372 (0.007) 0.748 (0.008)\nCBAK= 250 0.279 (0.007) 0.153 (0.005) 0.198 0.385 (0.007) 0.760 (0.007)\nCBAK= 500 0.286 (0.005) 0.162 (0.004) 0.207 0.390 (0.008) 0.759 (0.007)\nCBAK= 1000 0.283 (0.008) 0.161 (0.006) 0.205 0.393 (0.008) 0.764 (0.006)\nCBAK= 2500 0.282 (0.006) 0.162 (0.004) 0.206 0.394 (0.008) 0.765 (0.007)\nTable 1 . Summary of the performance of CBA (with a variety of VQ codebook sizes K), a mixture-of-Gaussians model\n(MixHier), and an AdaBoost-based model (Autotag) on an annotation task (evaluated using precision, recall, and F-score)\nand a retrieval task (evaluated using average precision (AP) and area under the receiver-operator curve (AROC)). Autotag\n(MFCC) used the same Delta-MFCC feature vectors and training set size of 450 songs as CBA and MixHier. Autotag\n(afeats exp.) used a larger set of features and a larger set of training songs. UpperBnd uses the optimal labeling for each\nevaluation metric, and shows the upper limit on what any system can achieve. Random is a baseline that annotates and\nranks songs randomly.\nTo evaluate our system’s annotation performance, we\ncomputed the average per-word precision, recall, and F-\nscore. Per-word recall is deﬁned as the average fraction\nof songs actually labeled wthat our model annotates with\nlabelw. Per-word precision is deﬁned as the average frac-\ntion of songs that our model annotates with label wthat are\nactually labeled w. F-score is the harmonic mean of pre-\ncision and recall, and is one metric of overall annotation\nperformance.\nFollowing [2], when our model does not annotate any\nsongs with a label wwe set the precision for that word to\nbe the empirical probability that a word in the dataset is\nlabeledw. This is the expected per-word precision for w\nif we annotate all songs randomly. If no songs in a test set\nare labeledw, then per-word precision and recall for ware\nundeﬁned, so we ignore these words in our evaluation.\n4.2 Retrieval task\nTo evaluate our system’s retrieval performance, for each\ntagwwe ranked each song jin the test set by the prob-\nability our model estimated of tag wapplying to song j.\nWe evaluated the average precision (AP) and area under\nthe receiver-operator curve (AROC) for each ranking. AP\nis deﬁned as the average of the precisions at each possi-\nble level of recall, and AROC is deﬁned as the area under\na curve plotting the percentage of true positives returned\nagainst the percentage of false positives returned. As in the\nannotation task, if no songs in a test set are labeled wthen\nAP and AROC are undeﬁned for that label, and we exclude\nit from our evaluation for that fold of cross-validation.4.3 Annotation and retrieval results\nTable 1 and ﬁgure 2 compare our CBA model’s average\nperformance under the ﬁve metrics described above with\nother published results on the same dataset. MixHier is\nTurnbull et al.’s system based on a mixture-of-Gaussians\nmodel [2], Autotag (MFCC) is Bertin-Mahieux’s AdaBoost-\nbased system using the same Delta-MFCC feature vec-\ntors as our model, and Autotag (afeats exp.) is Bertin-\nMahieux’s system trained using additional features and\ntraining data [9]. Random is a random baseline that re-\ntrieves songs in a random order and annotates songs ran-\ndomly based on tags’ empirical frequencies. UpperBnd\nshows the best performance possible under each metric.\nRandom and UpperBnd were computed by Turnbull et al.,\nand give a sense of the possible range for each metric.\nWe tested our model using a variety of codebook sizes\nKfrom 5 to 2500. Cross-validation performance improves\nas the codebook size increases until K= 500 , at which\npoint it levels off. Our model’s performance does not de-\npend strongly on ﬁne tuning K, at least within a range of\n500≤K≤2500 .\nWhen using a codebook size of at least 500, our CBA\nmodel does at least as well as MixHier and Autotag under\nevery metric except precision. Autotag gets signiﬁcantly\nhigher precision than CBA when it uses additional training\ndata and features, but not when it uses the same features\nand training set as CBA.\nTables 2 and 3 give examples of annotations and re-\ntrieval results given by our model during cross-validation.\n37210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nCBAMixHierAutotag (MFCC)Autotag (afeats exp.)Random5102050100200500100020000.100.150.200.25\nCodebook Size KF-score\n5102050100200500100020000.250.300.350.40\nCodebook Size KMean Avg. Prec.\n5102050100200500100020000.500.550.600.650.700.750.80\nCodebook Size KMean AROCFigure 2 . Visual comparison of the performance of several models evaluated using F-score, mean average precision, and\narea under receiver-operator curve (AROC).\n4.4 Computational cost\nWe measured how long it took to estimate the parame-\nters to CBA and to generalize to new songs. All experi-\nments were conducted on one core of a server with a 2.2\nGHz AMD Opteron 275 CPU and 16 GB of RAM running\nCentOS Linux.\nUsing a MATLAB implementation of the EM algorithm\ndescribed in 3.3, it took 84.6 seconds to estimate CBA’s pa-\nrameters from 450 training songs vector-quantized using a\n500-cluster codebook. In experiments with other codebook\nsizesKthe training time scaled linearly with K. Once\nβhad been estimated, it took less than a tenth of a mil-\nlisecond to predict the probabilities of 174 labels for a new\nsong.\nWe found that the vector-quantization process was the\nmost expensive part of training and applying CBA. Finding\na set of 500 cluster centroids from 125,000 39-dimensional\nDelta-MFCC vectors using a C++ implementation of k-\nmeans took 479 seconds, and ﬁnding the closest of 500\ncluster centroids to the 10,000 feature vectors in a song\ntook 0.454 seconds. Both of these ﬁgures scaled linearly\nwith the size of the VQ codebook in other experiments.\n5. DISCUSSION AND FUTURE WORK\nWe introduced the Codeword Bernoulli Average model,\nwhich predicts the probability that a tag will apply to a\nsong based on counts of vector-quantized feature data ex-\ntracted from that song. Our model is simple to implement,\nfast to train, generalizes to new songs efﬁciently, and yields\nstate-of-the-art performance on annotation and semantic\nretrieval tasks.\nWe plan to explore several extensions to this model in\nthe future. In place of the somewhat ad hoc diversity fac-\ntor, one could use a weighting similar to TF-IDF to choose\ninformative words for annotations. The vector quantiza-\ntion preprocessing stage could be replaced with a mixed-\nmembership mixture-of-Gaussians model that could be ﬁt\nsimultaneously with β. Also, we hope to explore princi-\npled ways of incorporating song-level feature data describ-\ning information not captured by MFCCs, such as rhythm.Query Top 5 Retrieved Songs\nJohn Lennon—Imagine\nShira Kammen—Music of Waters\nTender/Soft Crosby Stills and Nash—Guinnevere\nJewel—Enter From the East\nYakshi—Chandra\nTim Rayborn—Yedi Tekrar\nSolace—Laz 7 8\nHip Hop Eminem—My Fault\nSir Mix-a-Lot—Baby Got Back\n2-Pac—Trapped\nRobert Johnson—Sweet Home Chicago\nShira Kammen—Music of Waters\nPiano Miles Davis—Blue in Green\nGuns n’ Roses—November Rain\nCharlie Parker—Ornithology\nTim Rayborn—Yedi Tekrar\nMonoide—Golden Key\nExercising Introspekt—TBD\nBelief Systems—Skunk Werks\nSolace—Laz 7 8\nNova Express—I’m Alive\nRocket City Riot—Mine Tonite\nScreaming Seismic Anamoly—Wreckinball\nPizzle—What’s Wrong With My Footman\nJackalopes—Rotgut\nTable 3 . Examples of semantic retrieval from the CAL500\ndata set. The left column shows a query word, and the right\ncolumn shows the ﬁve songs in the dataset judged by our\nsystem to best match that word.\n6. REFERENCES\n[1] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Towards musical query-by-semantic-description\nusing the CAL500 data set. In Proc. ACM SIGIR , pages\n439–446, 2007.\n[2] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Semantic annotation and retrieval of music and\n373Oral Session 5: Tags\nGive it Away Fly Me to the Moon Blue Monday Becoming\nthe Red Hot Chili Peppers Frank Sinatra New Order Pantera\nUsage—At a party Calming/Soothing Very Danceable NOT—Calming/Soothing\nHeavy Beat NOT—Fast Tempo Usage—At a party NOT—Tender/Soft\nDrum Machine NOT—High Energy Heavy Beat NOT—Laid-back/Mellow\nRapping Laid-back/Mellow Arousing/Awakening Bass\nVery Danceable Tender/Soft Fast Tempo Genre—Alternative\nGenre—Hip Hop/Rap NOT—Arousing/Awakening Drum Machine Exciting/Thrilling\nGenre (Best)—Hip Hop/Rap Usage—Going to sleep Texture Synthesized Electric Guitar (distorted)\nTexture Synthesized Usage—Romancing Sequencer Genre—Rock\nArousing/Awakening NOT—Powerful/Strong Genre—Hip Hop/Rap Texture Electric\nExciting/Thrilling Sad Synthesizer High Energy\nTable 2 . Examples of semantic annotation from the CAL500 data set. The two columns show the top 10 words associated\nby our model with the songs Give it Away, Fly Me to the Moon, Blue Monday, andBecoming .\nsound effects. IEEE Transactions on Audio Speech and\nLanguage Processing , 16(2), 2008.\n[3] J.B. MacQueen. Some methods for classiﬁcation and\nanalysis of multivariate observations. In Proc. Fifth\nBerkeley Symp. on Math. Statist. and Prob., Vol. 1 ,\n1966.\n[4] C. Wang, D. Blei, and L. Fei-Fei. Simultaneous im-\nage classiﬁcation and annotation. In Proc. IEEE CVPR ,\n2009.\n[5] M. Hoffman, D. Blei, and P. Cook. Content-based\nmusical similarity computation using the hierarchical\nDirichlet process. In Proc. International Conference on\nMusic Information Retrieval , 2008.\n[6] K. Seyerlehner, A. Linz, G. Widmer, and P. Knees.\nFrame level audio similarity—a codebook approach. In\nProc. of the 11th Int. Conference on Digital Audio Ef-\nfects (DAFx08), Espoo, Finland, September , 2008.\n[7] M. Mandel and D. Ellis. LabROSA’s\naudio classiﬁcation submissions, mirex\n2008 website. http://www.music-\nir.org/mirex/2008/abs/AA AGATMM CCmandel.pdf.\n[8] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vla-\nhavas. Multilabel classiﬁcation of music into emotions.\nInProceedings of the 9th International Conference on\nMusic Information Retrieval (ISMIR) , 2008.\n[9] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere.\nAutotagger: a model for predicting social tags from\nacoustic features on large music databases. Journal of\nNew Music Research , 37(2):115–135, 2008.\n[10] A. Dempster, N. Laird, and D. Rubin. Maximum likeli-\nhood from incomplete data via the EM algorithm. Jour-\nnal of the Royal Statistical Society, Series B , 39:1–38,\n1977.\n[11] Mirex 2008 website. http://www.music-\nir.org/mirex/2008/index.php/Main Page.\n374"
    },
    {
        "title": "Rhythmic Similarity in Traditional Turkish Music.",
        "author": [
            "Andre Holzapfel",
            "Yannis Stylianou"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417861",
        "url": "https://doi.org/10.5281/zenodo.1417861",
        "ee": "https://zenodo.org/records/1417861/files/HolzapfelS09.pdf",
        "abstract": "In this paper, the problem of automatically assigning a piece of traditional Turkish music into a class of rhythm referred to as usul is addressed. For this, an approach for rhythmic similarity measurement based on scale transforms has been evaluated on a set of MIDI data. Because this task is related to time signature estimation, the accuracy of the proposed method is evaluated and compared with a state of the art time signature estimation approach. The results indicate that the proposed method can be successfully applied to audio signals of Turkish music and that it captures relevant properties of the individual usul.",
        "zenodo_id": 1417861,
        "dblp_key": "conf/ismir/HolzapfelS09",
        "keywords": [
            "automatic assignment",
            "traditional Turkish music",
            "rhythm usul",
            "rhythmic similarity measurement",
            "scale transforms",
            "MIDI data",
            "time signature estimation",
            "accuracy evaluation",
            "state of the art",
            "audio signals"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nRHYTHMICSIMILARITYINTRADITIONALTURKISH MUSIC\nAndre Holzapfel\nInstituteofComputerScience, FORTH and\nUniversityofCrete\nhannover@csd.uoc.grYannisStylianou\nInstituteofComputerScience, FORTH and\nUniversityofCrete\nyannis@csd.uoc.gr\nABSTRACT\nInthispaper,theproblemofautomaticallyassigningapiec e\noftraditionalTurkishmusicintoaclassofrhythmreferred\nto asusulis addressed. For this, an approach for rhyth-\nmic similaritymeasurementbasedonscale transformshas\nbeen evaluated on a set of MIDI data. Because this task\nis related to time signature estimation, the accuracy of the\nproposed method is evaluated and compared with a state\nof the art time signature estimation approach. The results\nindicate that the proposed method can be successfully ap-\npliedtoaudiosignalsofTurkishmusicandthatit captures\nrelevantpropertiesoftheindividual usul.\n1. INTRODUCTION\nTraditionalmusicofTurkeyhasabigcommunityoflisten-\ners,andthemusicisstronglyrelatedtothemusicofneigh-\nboringregions.Forexample,inGreeceandArabiancoun-\ntriesmusicmelodiesoftraditionalmusicareoftenbasedon\nsimilar modal systems as in Turkey. Concerning rhythm,\nthere is a correspondence in classes of rhythm found in\nArabic music ( iqa’) and in Turkey ( usul), and dances en-\ncountered in Turkey have inﬂuenced rhythms played in\nGreekRembetiko music. Thus, automatic retrieval of this\ninformation not only enables a better understanding of an\nimportantcultural heritage but may also be of major com-\nmercial interest. Methods for this type of retrieval can be\nassignedto the branchof computationalethnomusicology,\nas introduced in [20]. Only recently, ﬁrst research results\non theclassiﬁcation ofTurkishmusic intomelodicclasses\nwere presented [7]. The retrieval of rhythmic information\nfromtraditionalTurkishmusichasnotbeenaddressedyet.\nIn this paper, classiﬁcation of samples of Turkish music\ninto rhythmic classes is proposed. These classes are re-\nferred to as usul[16]. A data set containing samples of\nsongscomposedin sixdifferent usulhasbeencompiledto\nconduct experiments.As it will be shown in the later Sec-\ntions,inthecontextofthisdataset theclassiﬁcationinto a\nspeciﬁc rhythmic class is related to the recognition of the\ntime signatureinWestern music.\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2009 International Society for MusicInformation Retrieva l.In [18], an approach was presented to estimate the time\nsignature of a piece of music based on symbolic descrip-\ntions (MIDI). This approach uses autocorrelation coefﬁ-\ncients (ACF) derivedfrom the annotatedonsets. In [21],a\ntimesignatureestimationsystemforaudiosignalswaspro-\nposedandevaluatedonasetofpercussivemusic.Thesys-\ntem estimates the tatum [2] of the signal using inter-onset\nintervals(IOI)andinparallel,ACFarecomputedfromthe\namplitude envelope of the signal. Beat and bar length are\nchosenfromthe peaksof theACF, takinginto accountthe\nestimatedtatum.In[8],thedeterminationofmusicalmeter\nwasreducedtoaclassiﬁcationintoeitherbinaryorternary\nmeter. Beat indexesare extractedin a semi-automaticway\nand then ACF on a chosen set of features are used to de-\ncide on the meter type. Using audio signals, the general\nproblem of rhythmic similarity was addressed previously\nin [10] [1] in the context of traditional music, in both cas-\nes byapplyingDynamicTime Warpingtechniques.In [4],\nrhythmicpatternswerecomputedfromsamplesofWestern\nballroomdances.\nIn[14]asystemwasproposedfortheautomaticestimation\nof the musical meter,i.e., the estimation of the position of\ntatum,beatandbarsinthesignal.Theestimationofbarpo-\nsitionsin3\n4timesignaturesismentionedtobeerror-prone.\nCompound time signatures such as9\n8are not mentioned\nand to the best of our knowledge no reliable method has\nbeenpresentedtoestimatethe meterin suchsignals.\nOn the other hand, compound or complex time signatures\nare commonlyencounteredin traditionalmusic of Turkey.\nThetimesignaturescantakevariousforms,asitwillbede-\ntailed in Section 2. Furthermore, the goal of the approach\npresentedinthispaperisnotonlythecorrectestimationof\na time signature, but a descriptionof the rhythmic proper-\nties of a class, because usulcannot be only distinguished\nbytimesignatureinallcases.In[11],audiosamplesoftra-\nditionaldanceswerecompared:ACFwerecomputedfrom\nonset strength signals (OSS) and these ACF were trans-\nformedinto the scale domainby using the scale transform\n[3].Thisresultsindescriptorsthatdonotvaryduetotempo\nchanges.Thus,the scale transformmagnitudes(STM)can\nbe used to compare the rhythmic content of audio using\nsimple point to point distance measures without the need\nofmeterestimation.Theapproachin[11]wasshowntobe\nsuperior to the DTW based approachpresented in [10]. In\nthispaper,itwillbecombinedwiththeapproachpresented\nin [18] and applied to a set of MIDI data. MIDI data was\nchosen as a ﬁrst step to approachthe problemof automat-\n99Poster Session 1\nTE TEK TEK DUM DUM KE89\nFigure1.Symbolicdescriptionofthe usulAksak\nic rhythmdescriptioninTurkishmusic.Thisapproachcan\neasily be applied to audio signals by replacing the type of\nOSS,hasnoneedofmeterestimationandisrobusttolarge\ntempodeviations.\nSection 2 introduces the basic concepts of the analyzed\ntype of music and describes the data set. Section 3 intro-\nducesthedescriptorsbasedonscaletransformandpropos-\nes a method of comparison. Section 4 gives experimental\nresultsandSection6 concludesthepaper.\n2. DATASET\nCompositions in Turkish traditional music follow certain\nschemesregardingtheirmelodicandrhythmiccontent.Me-\nlodies are characterized by a modal system referred to as\nmakam, and it deﬁnesa melodictextureconsisting of spe-\nciﬁc tonal segments, progressions, directionality, tempo -\nral stops, tonal centers and cadences [13]. The rhythmic\nschemes encountered in traditional Turkish music are re-\nferred to as usul. Anusulis a rhythmic pattern of certain\nlength that deﬁnes a sequence of strong and weak intona-\ntions.AnexampleisshowninFigure1:the usulAksak has\nalengthofninebeats.Thenotesontheupperlinelabelled\nd¨umhave the strongest intonation while the notes on the\nlowlinedenoteweakintonations.Thenotedurationsinthe\nsequence shown in Figure 1 can be described as the string\nxoxxxoxox , where xsymbolizes the start of a note and\nometric unit without note [19]. Note that this representa-\ntion is a further simpliﬁcation of the one shown in Figure\n1, because no differentiation of the intonation strength is\ncontained. However these representations can be used for\nestimatingthesimilaritybetweenrhythmsofsamelengths\nbycomputinga chronotonicdistance,asdetailedin[19].\nUnlike in [19],the length of the usulvaries. According\nto H. Sadeddin Arel (1880-1955), the usulcan be divid-\ned into minor and major usul. Minorusulhave a length of\nup to 15 time units, while the major usulhave up to 124\ntime units. As denoted in [16], minor usul are related to\nsmall musical forms, while larger musical forms employ\nthe major usul in most cases. Musical forms that are usu-\nally composed in major usul are, e.g., PresrevandBeˆste.\nTwoexamplesofsmallmusicalformsare SarkıandT¨urk¨u.\nThelatterarefolksongsofunknowncomposers,whilethe\nformer are short songs based usually on four lines of text\nwith known composer. Both forms have in common that\na song follows a certain minor usuland a certain makam,\nand both forms are vocal music. The most popular songs\nin Turkishmusicarecomposedinthese forms.Becauseof\nthat,alongwitha systemfortherecognitionofthe makam\nas presented in [7], an approach for the recognition of theusulrepresents an essential element in automatic retrieval\nof information from this music. Apart from that, the re-\nlation between melody and usulhas not been investigated\nandanautomaticapproachliketheonepresentedherecan\ngivevaluableinsightintotherelationbetweenmelodyand\nusul.\nThedatasetusedinthispaperconsistsofTurkishsongsin\nthe forms of SarkıandT¨urk¨u. They are following six dif-\nferenttypesofrhythmicschemeshavinglengthsfrom3up\nto10:Aksak(9\n8),Curcuna(10\n8),D¨uyek(8\n8),Semai(3\n4),So-\nfyan(4\n4),andT¨urkAksa˘gi(5\n8).Thesoftware mus2okur [13]\nhas been used to obtain a data set consisting of 288 songs\ndistributedalongthesixclassesasshowninthesecondline\nof Table 1. Each sample consists of a MIDI descriptionof\nthe song melody, in most cases also a MIDI voice with\na percussive accompanimentis contained.This percussive\naccompanimenthasbeenleftout,inordertobeabletofo-\ncus on the rhythmic properties of the melody. Due to the\ncharacter of this music, there exists no chord accompani-\nment.\nAs allusulin the data set have different length,the recog-\nnition of the usulcan be reduced to a recognition of its\nlength. This is closely related to the task of time signa-\nture recognition and motivates the experimental setup de-\nscribed in the following Sections. The lower two lines in\nTable 1 depict the mean values of the tempi in bpm(beats\nperminute)andthestandarddeviationofthetempi,respec-\ntively. It is apparent that there are large overlaps between\nthetempodistributionsofthe usul.Thus,asystemfor usul\nlength estimation for a given audio signal has to be robust\nto thetempodeviationsandoverlaps.\nTable 1. Data set: number of songs, mean and standard\ndeviationoftempiin bpm\nCLASS AKSCURDUYSEMSOFTUR\nNSongs 645747226038\nMEAN 98.598.370.7131.981.373.1\nSTD 27.913.512.626.316.722.3\n3. TIME SIGNATUREESTIMATION\n3.1 RhythmDescription\n3.1.1 Tempo-invariantACF\nIn order to describe and compare the content of the sam-\nples, an autocorrelationbasedmethodas presentedin [18]\nhasbeencombinedwithamethodusedforestimatingrhyth-\nmic similarity presented in [11]. The onset times are read\nfrom the MIDI ﬁles and each onset is assigned a weight.\nIn [18],different methodsto set the weights were evaluat-\ned, and in this paper the three most successfull weighting\nschemes have been applied: the weight of an onset can ei-\nther be related to the note duration as proposed in [15],\nto characteristics of the melody [17], or all onsets are as-\nsignedthesameweight.Thebestweightingschemewillbe\n10010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n0 1 2 3 400.20.40.60.81\nLAG / sr(m)\nFigure 2. Autocorrelations ruderived from two samples\nofusulaksak\ndetermined in Section 4. In the method presented in [18],\nan onset strength signal (OSS) is generated at a sampling\nfrequencyrelatedtotheeighthnoteofthepiece.ThisOSS\nhas an impulse of height accordingto the assigned weight\natthepositionsrelatedtotheonsettime.FromanOSS o(n)\nanACF r(m)canbederived\nr(m) =/summationtext\nno(n)o(n−m)/summationtext\nno(n)2(1)\nNote that the autocorrelations are not affected by tempo\ndifferences,whentheOSSarecomputedatasamplingfre-\nquencythatchangeswiththetempo(eighthnote).Because\nof this, changing the tempo will result in constant ACF,\nwhichwill bedenotedas rc.\n3.1.2 Tempo-variantACF\nAs mentioned in [18], beat tracking is a necessary step\nwhen applying the above described approach to audio. It\nis necessary to correctly estimate all metric levels in or-\nder to determine the eighth note pulse of the piece. When\ndealing with compound rhythms of different type as they\narecontainedinthedatasetandcommonlyencounteredin\nthe music ofTurkeyandthe wholeeasternMediterranean,\nnomethodhasbeenpresentedyettoperformthistask.For\nthat reason, the MIDI data contained in the data set as de-\nscribed in Section 2 is used to compute OSS using a con-\nstantsamplingfrequencyof fs= 50Hz.FromtheOSSau-\ntocorrelationsare derived.Fortwo pieceshavingthe same\ntime signature but different tempi, their autocorrelation s\nwill differbyan unknownscaling factor,as can be seen in\nFigure 2. This is particularly critical for the type of music\nexamined in this paper due to the large tempo deviations\nas detailed in Section 2. In order to overcome this scaling\nproblem,typicallythe beattrackingwouldbenecessaryin\norderto estimate the tempodifferencebetweenthe pieces.\nHowever,in thispaperthe usageofthemethodintroduced\nin[11]isproposedtoavoidtheintractableproblemofbeat\ntracking in the presence of complex and compound time\nsignatures. Due to the unknown scaling factor depicted in\nFigure 2, a simple point-to-pointdistance measure cannot\nbe applied when comparing these autocorrelations, which\nduetotheunknownscalingwillbedenotedas ru.Inorder\nto solve this problem, a scale transform has been applied1  50 100 15000.050.10.150.20.25\ncSTM\nFigure3. TwoSTM derivedfromthetwo aksakexamples\nshowninFigure2\nto theautocorrelationsequence ru(t) :\nR(c) =1\n2π/integraldisplay∞\n0ru(t)e(−jc−1/2) lntdt(2)\nThescaletransformhasthepropertythatforasignal ru(t)\nand its time scaled version√aru(at), with a >0being\nthe scaling factor, the two computedscale transformmag-\nnitudes will be the same. This can be seen in Figure 3,\nwhere the two scaled autocorrelationsfrom Figure 2 have\nbeen transformed to scale space. Due to the scale invari-\nance property they are aligned and can be directly com-\npared.\nThus, in this paperOSS will be computedfrom the MIDI\nﬁles using a constant sampling frequency of fs= 50Hz.\nThen,scaletransformmagnitudes(STM)arecomputedfrom\nthe autocorrelations ruusing the discrete scale transform\nalgorithm proposed in [22]. This results in a STM vector\nthat describesthe rhythmiccontent of the signal, the scale\nresolution was found to be of minor importance and has\nbeensetto ∆c= 0.5.Theaccuracyinthetaskoftimesig-\nnature recognition when using either scaling free autocor-\nrelations rcortheSTMderivedfrom ruwillbecompared.\nThe results will indicate if by using a scale transform, the\nunsolvedproblemofmeterestimationincomplextimesig-\nnaturescan be avoided and the usullength could be deter-\nminedbyusingthismethod.\n3.2 RhythmDissimilarity\nIn orderto determinethe time signatureof a piece the fol-\nlowing approach will be applied: All pairwise dissimilari-\ntiesbetweensongsarecomputedusingeitherthescale-free\nACFrcor the STM vectors, by using a cosine distance as\nproposed in [6] [9]. This results in dissimilarity matrices ,\nhavingvaluesclosetozerowhenevertwopiecesarefound\nto be similar regarding their rhythmic content. In order to\ndeterminetheaccuracyoftheproposedrhythmicsimilarity\nmeasure,the accuraciesof a modiﬁed k-Nearest Neighbor\n(kNN)classiﬁcationwillbedetermined.Forthis,eachsin-\ngle song will be used as a query for that a classiﬁcation\ninto one of the available classes is desired. This classiﬁ-\ncation is performed by applying the modiﬁed kNN to the\ndissimilarity matrix. As shown in [10], a locally weighted\nkNNwasfoundtoimproveaccuraciesonsimilardata,and\n101Poster Session 1\nDURATION MELODY FLAT\n80.2% 68.1% 72.9%\nTable2.Timesignaturerecognitionaccuracieswhenusing\nscale free rcrepresentation\ntherefore it has been used in the experiments. It assigns a\nweight wi= 1−(di/dk+1)to the i-th training sample,\nwhere dk+1is the distance of the k+ 1-nearest neighbor\nto the test sample. Thus, training samples more far away\nfromthe test samplecontributeless toitsclassiﬁcation.\nAnusulcan be expressed in a simpliﬁed way as a string,\nas for example the string xoxxxoxox forAksak. In Sec-\ntion 4, for some usultheir string representations will be\nused to estimate their similarity using a method proposed\nin[19]:Fromthestringrepresentationschronotonicchain s\ncan be computed, by breaking down the rhythm into its\nsmallest time unit on the x-axisand assigning to each el-\nement a height on the y-axisaccording to the beat-to-beat\ninterval. This results in the chronotonic chain [211221] in\ncase ofAksak. As proposed in [19], in order to compare\ntwo such chronotonic chains, then a discrete form of the\nKolmogorovVariationalDistance(DKVD)canbeapplied.\nGiven two chronotonic chains gandfof same length L,\nthisdistancecanbecomputedas\nK=L/summationdisplay\ni=1|f[i]−g[i]| (3)\nandisequaltothe 1−normdistancebetweenthechains.\nThus, by depicting an usulpair as two strings of same\nlength, their rhythmic similarity can be estimated. In this\npaper, this method will be applied to pairs of usulfor that\nsamples frequently were confused in the time signature\nrecognition.\n4. EXPERIMENTS\n4.1 Scale-freeACF\nThree different weightingschemes have been evaluated in\nthe experiments: the duration accent as proposed in [15],\nthe melodic accent [17], and the ﬂat accent (i.e., using the\nsameaccentweightforallonsets).Usingthe rcautocorre-\nlations computed using these three accents in the classiﬁ-\ncationapproachasdescribedinSection3.2,resultedinthe\nbest accuracies for the duration accent, as documented in\nTable 2. This contradicts with the ﬁndings in [18], where\nthe melodic and ﬂat accents were found to be preferable.\nFurthermore,using a selected rangeof autocorrelationco-\nefﬁcientscouldnotfurtherimproveresultsonthisdataset ,\nwhilein[18]usingthecoefﬁcientsoflongerlagsandleav-\ning out the coefﬁcients of short lags was found superior.\nThis must be assigned to the differencesbetween the data\nsets.\nIn Table 3 the confusion matrix for the best classiﬁca-\ntion in Table 2 is shown. The biggest confusion happens\nbetween the8\n8time signature usuland the4\n4usul(D¨uyekPredicted\n9/810/88/83/44/45/8\nNotated9/86201010\n10/80500016\n8/814240180\n3/40002020\n4/420120460\n5/80900029\nTable 3.Confusionmatrixfor rcusingdurationaccent\nSymbolicDescription\nD¨uyek: xxoxxoxo Curcuna: xoxxoxoxox\nSofyan: xoooxoxo T¨urk Aksa˘gi: xoooxoooxo\nChronotonicChains\nD¨uyek: 12212222 Curcuna: 2212222221\nSofyan: 44442222 T¨urk Aksa˘gi: 4444444422\nNormalizedDKVDbetw.ChronotonicChains\n10/8=1.25 18/10=1.8\nTable 4. Computing chronotonic distances between con-\nfusedusul\nandSofyan, respectively). The pieces in the8\n8-usulcould\nbe equivalently annotated in a8\n4time signature by chang-\ningtheirdegree,referredtoas mertebe,tofour.Thesecond\nbiggestconfusionhappensbetween CurcunaandT¨urkAk-\nsa˘gi. The time signatures are related by a factor of two as\nwell (10\n8and5\n8). These types of errors have been denoted\nas typical as well in [18]. Still, the confusion between be-\ntweenD¨uyekandSofyanis larger. This can be attributed\nto the different degree of similarity of the usul, which can\nbe estimated using the approach proposed in [19]: In Ta-\nble4,thesymbolicdescriptionsforthetwoconfused usul-\npairs are depicted as vectors of same length. From these\ndescriptionsthechronotonicchainshavebeenderivedthat\nare depicted in Table 4. Note that Sofyanwould be typi-\ncally denoted as [211]as its smallest beat-to-beat interval\nis a fourth note. In order to get chains of equal length,\nthe eighth note has been chosen as smallest unit. Com-\nputing the KolmogorovVariational Distances between the\nchronotonic chains, and normalizing by the length of the\nvectors it can be seen that the usul D¨uyekandSofyanare\nmore similar than the other pair. This is reﬂected in the\nhigherconfusioninTable3.Thus,itcanbeconcludedthat\nthe appliedautocorrelationmethodis not onlysuitable for\ndeterminingtimesignatures,butcanaswellcapturerhyth-\nmic similaritiescontainedinthepiece.\n4.2 ScaleTransformMagnitudes\nThe results presented in Section 4.1 have been obtained\nusing the known note values that have been read from the\nMIDIﬁles.Asdiscussedabove,whenaudiosignalshaveto\nbe examinedinstead of MIDI, this knowledgecan only be\nobtained by means of beat tracking, which is an unsolved\n10210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 4. Result of the parameter grid search using the\nSTM descriptors\nPredicted\n9/810/88/83/44/45/8\nNotated9/85133133\n10/80522003\n8/811302112\n3/43031510\n4/40281481\n5/82430128\nTable 5. Confusionmatrixfor STM at C= 140and max-\nimumlagof 14s\ntask for the time signatures obtained in the data set. Thus,\ntheSTMrepresentasolutiontoavoidbeattracking,andin\nthisSectiontheinﬂuenceofitsapplicationontheresultin g\naccuracieswill bedocumented.\nThe parameters to set when using the STM are the maxi-\nmumlagconsideredintheautocorrelation ruandthenum-\nberofscalecoefﬁcients Cthatistobeusedwhencomput-\ningthecosinedistance.\nThe inﬂuenceof these parametershasbeenevaluatedin a\ngrid search. The resulting accuracies are depicted in Fig-\nure 4. It can be seen that by increasing the maximum lag\nsize and the maximum scale coefﬁcient the accuracies are\nimproveduntil a level of about 77%is reached. The high-\nest accuracy achieved at some points on the dotted line in\nFigure 4 is 77.8%, for example at C= 140and at a max-\nimum lag of 14s(marked in Figure 4). Choosing a point\nwithsmallmaximumlagleadstofastercomputationofthe\nscale transform, and choosinga small value of Cmeans a\nmorecompactSTMdescription.\nThe related confusion matrix is shown in Table 5 and\ncomparing it with the confusion matrix shown in Table\n3 reveals very similar structure. The decrease in accuracy\nseems to be caused by some misclassiﬁcation that cannot\nbe justiﬁed by a similarity of the usul, as for example the\n9\n8-timesignature,whichfortheSTMdescriptorisrandom-\nlymisclassiﬁed.Thusitappearsthattransformingautocor -\nrelations to scale domain in the proposed way introduces\nsome noise to the rhythm descriptors. However, the per-formance is only 2.4%lower than for using the scale-free\nautocorrelations ( 77.8%instead of 80.2%). Hence, by in-\ncludingscaletransformthecurrentlyinfeasiblestepofbe at\ntracking in this kind of meters is avoided and time signa-\nture estimation is made feasible, when presented with ar-\nbitrarytypesof musicsignalshavinga compoundor com-\nplexmeter.\n5. FUTUREWORK:TOWARDSAUDIOSIGNALS\nAs mentionedin[18],in orderforthe abovedescribedap-\nproach to work on audio instead of MIDI three algorith-\nmic steps have to be added: onset detection, pitch estima-\ntion and beat tracking. The ﬁrst step appears to be nec-\nessary, because the onset locations are not known as it is\nthe case for MIDI. The pitch estimation is necessary only\nwhen the weights in the OSS are desired to be inﬂuenced\nby the pitch properties of the melody. On audio data, this\ncan be approached using a fundamental frequency based\nOSS as proposed in [12], otherwise this step can be left\nout and an OSS as described in [5] can be used instead.\nThe most error-prone step when dealing with audio is the\nbeat tracking: it is necessary to correctly estimate all met -\nriclevelsinordertodeterminetheeighthnotepulseofthe\npiece,whenthemethodasdescribedinSection3.1.1isde-\nsired to be applied. Fortunately,the results using the STM\nasdescribedinSection3.1.2avoidsthisstepofbeattrack-\ning. Thus, time signatures and rhythmic properties can be\ncaptured by computing an OSS from an audio signal, and\ncomputing ACF and STM as described above. In order to\nevaluate the accuracy of the approach on audio data, a set\nof audio recordingssimilar to the MIDI data set will have\nto becompiled.\n6. CONCLUSIONS\nInthispapertheapplicationofscaletransformforthereco g-\nnition of time signatures is proposed. Using a data set of\nMIDI data with high class intern tempo deviations it is\nshown that this method achieves almost the same accu-\nracy as a method that assumes that the metric levels of\nthe piece are known. Thus, this method can be applied\nto the time signature recognition of audio signals by esti-\nmating an OSS suitable for the character of the signal and\nthencomputingtheSTMdescriptorsasproposed.Thisrep-\nresents a signiﬁcant achievement because the estimation\nof the metric levels in music signals having compound or\ncomplexmetersisnotasolvedproblem.Theproposedap-\nproach is computationally simple because the scale trans-\nformcanbeperformedusingFFTalgorithms.Furthermore,\ntheproposeddescriptorsseemtocaptureareasonableamoun t\nofinformationabouttherhythmicpropertiesofthe usul,as\ncould be seen in the relation between symbolic similarity\nand the confusion. As the rhythmic properties of Turkish\nmusic have neverbeen studied using computationalmeth-\nods, this indicates an interesting direction for future stu d-\nies. Next steps of these studieshave to be the usage of au-\ndiosignalsandtheexaminationof usulofsame length.\n103Poster Session 1\n7. REFERENCES\n[1] Iasonas Antonopoulos, Angelos Pikrakis, Sergios\nTheodoridis,OlmoCornelis,DirkMoelants,andMarc\nLeman. Music retrieval by rhythmicsimilarity applied\non greek and african traditional music. In Proc. of IS-\nMIR - InternationalConference on Music Information\nRetrieval,Vienna,Austria,2007.\n[2] JeffreyA.Bilmes. TimingisoftheEssence .PhDthesis,\nMasterThesis,MassachusettsInstituteOfTechnology,\n1993.\n[3] L.Cohen.Thescalerepresentation. IEEETransactions\nonSignalProcessing ,41(12):3275–3292,1993.\n[4] Simon Dixon, Fabien Gouyon, and Gerhard Widmer.\nTowards characterisation of music via rhythmic pat-\nterns. InProc. of ISMIR- InternationalConferenceon\nMusicInformationRetrieval ,2004.\n[5] DanielP.W. Ellis. Beat trackingbydynamicprogram-\nming.Journal of New Music Research , 36(1):51–60,\n2007.\n[6] JonathanFoote,MatthewD.Cooper,andUnjungNam.\nAudio retrieval by rhythmic similarity. In Proc. of IS-\nMIR - InternationalConference on Music Information\nRetrieval,pages265–266,2002.\n[7] Ali C. Gedik and Baris Bozkurt. Automatic classiﬁca-\ntion of turkish traditionalart music recordingsby ariel\ntheory.In Proc.ofCIM08,4thConferenceonInterdis-\nciplinaryMusicology ,Thessaloniki,Greece,2008.\n[8] Fabian Gouyon and Perfecto Herrera. Determination\nof the meter of musical audio signals: Seeking recur-\nrences in beat segment descriptors. In 114th Conven-\ntionoftheAudioEngineeringSociety ,2003.\n[9] Andre Holzapfel and Yannis Stylianou. Musical genre\nclassiﬁcation using non-negative matrix factorization\nbased features. IEEE Transactions on Audio, Speech\nandLanguageProcesing ,16(2):424–434,2008.\n[10] AndreHolzapfelandYannisStylianou.Rhythmicsim-\nilarityofmusicbasedondynamicperiodicitywarping.\nInProceedings of the IEEE International Conference\nonAcoustics,Speech,andSignalProcessing.ICASSP ,\npages2217–2220,2008.\n[11] Andre Holzapfel and Yannis Stylianou. A scale trans-\nform based method for rhythmic similarity of music.\nInProceedings of the IEEE International Conference\nonAcoustics,Speech,andSignalProcessing.ICASSP ,\npages317–320,2009.\n[12] Andre Holzapfel, Yannis Stylianou, Ali C. Gedik, and\nBarı¸s Bozkurt. Three dimensions of pitched instru-\nmentonsetdetection. AcceptedforpublicationinIEEE\nTrans. on Audio, Speech and Language Procesing ,\n2009.[13] M. K. Karaosmano˘ glu, S¨ uleyman Metin Yılmaz,\n¨Omer T¨ oren, Secgi Ceran, Utku Uzmen, G¨ ulhan Ci-\nhan, and Emre Bas ¸aran. Mus2okur . Data-Soft Ltd.,\nhttp://www.musiki.org/,Turkey,2008.\n[14] A. P. Klapuri, A. J. Eronen, and J. T. Astola. Analysis\nof the meter of acoustic musical signals. IEEE Trans-\nactions on Acoustics Speech and Signal Processing ,\n14(1):342–355,2006.\n[15] R. Parncutt. A perceptual model of pulse salience and\nmetrical accent in musical rhythms. Music Perception ,\n11(4):409–464,1994.\n[16] TolgaBektas ¸.Relationshipsbetweenprosodicandmu-\nsical meters in the beste form of classical turkish mu-\nsic.AsianMusic , 36(1),Winter/Spring2005.\n[17] M. T. Thomassen. Melodic accent: Experimentsand a\ntentative model. Journal of the Acoustical Society of\nAmerica,71:1596–1605,1982.\n[18] Petri Toiviainen and Tuomas Eerola. Autocorrelation\nin meter induction: The role of accent structure. Jour-\nnaloftheAcousticalSocietyofAmerica ,119(2):1164–\n1170,2006.\n[19] GodfriedT.Toussaint.Acomparisonofrhythmicsimi-\nlaritymeasures.In Proc.ofISMIR-InternationalCon-\nferenceonMusicInformationRetrieval ,2004.\n[20] George Tzanetakis, Ajay Kapur, Andrew Schloss,\nand Matthew Wright. Computational ethnomusicolo-\ngy.Journal of interdisciplinary music studies , 1(2):1–\n24,2007.\n[21] Christian Uhle and Juergen Herre. Estimation of tem-\npo,microtimeandtimesignaturefrompercussivemu-\nsic. InProc. of the Int. Conference on Digital Audio\nEffects(DAFx) ,2003.\n[22] W.J. Williams and E.J. Zalubas. Helicopter transmis-\nsionfaultdetectionviatime-frequency,scaleandspec-\ntral methods. Mechanical systems and signal process-\ning,14(4):545–559,July 2000.\n104"
    },
    {
        "title": "Singing Pitch Extraction from Monaural Polyphonic Songs by Contextual Audio Modeling and Singing Harmonic Enhancement.",
        "author": [
            "Chao-Ling Hsu",
            "Liang-Yu Chen",
            "Jyh-Shing Roger Jang",
            "Hsing-Ji Li"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418009",
        "url": "https://doi.org/10.5281/zenodo.1418009",
        "ee": "https://zenodo.org/records/1418009/files/HsuCJL09.pdf",
        "abstract": "This paper proposes a novel approach to extract the pitches of singing voices from monaural polyphonic songs. The hidden Markov model (HMM) is adopted to model the transition between adjacent singing pitches in time, and the relationships between melody and its chord, which is implicitly represented by features extracted from the spectrum. Moreover, another set of features which represents the energy distribution of the enhanced singing harmonic structure is proposed by applying a normalized sub-harmonic summation technique. By using these two feature sets with complementary characteristics, a 2stream HMM is constructed for singing pitch extraction. Quantitative evaluation shows that the proposed system outperforms the compared approaches for singing pitch extraction from polyphonic songs.",
        "zenodo_id": 1418009,
        "dblp_key": "conf/ismir/HsuCJL09",
        "keywords": [
            "hidden Markov model (HMM)",
            "transition between adjacent singing pitches",
            "melody and its chord",
            "features extracted from the spectrum",
            "normalized sub-harmonic summation technique",
            "enhanced singing harmonic structure",
            "2stream HMM",
            "singing pitch extraction",
            "quantitative evaluation",
            "compared approaches"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nSINGING PITCH EXTRACTION FROM MONAURAL \nPOLYPHONIC SONGS BY CONTEXTUAL AUDIO \nMODELING AND SINGING HARMONIC ENHANCEMENT \nChao-Ling Hsu1 Liang-Yu Chen1 Jyh-Shing Roger Jang1 Hsing-Ji Li2 \n1MediaTek-NTHU Joint Lab \nDept. of Computer Science, \nNational Tsing Hua Univ., Taiwan  \n{leon, davidson833, jang} @mirlab.org 2Innovative Digitech-Enabled Applications & \nServices Institute (IDEAS)  \nInstitute for Information Industry, Taiwan \nlihsingji@iii.org.tw \nABSTRACT \nThis paper proposes a novel approach to extract the \npitches of singing voices from monaural polyphonic songs. The hidden Markov model (HMM) is adopted to model the transition between adjacent singing pitches in \ntime, and the relationships between melody and its chord, which is implicitly represented by features extracted from the spectrum. Moreover, another set of features which represents the energy distribution of the enhanced singing harmonic structure is proposed by applying a normalized sub-harmonic summation technique. By using these two feature sets with compleme ntary characteristics, a 2-\nstream HMM is constructed for singing pitch extraction. Quantitative evaluation shows that the proposed system \noutperforms the compared approaches for singing pitch extraction from polyphonic songs. \n1. INTRODUCTION \nMelody, usually represented by the pitch contour of a \nlead vocal in a song, is considered as one of the most im-portant elements of a song. It is broadly used in various applications, including singing voice separation, music retrieval, and musical genre classification. \nSince Goto [1] proposed the first melody extraction \nsystem by employing a parametric model trained by sta-tistical methods in 1999, more and more work has been proposed in the literature [2 -8]. Because harmonic struc-\ntures of a singing voice are ve ry noticeable in spectro-\ngram even in a polyphonic song, they are commonly used as cues for extracting the singing melody [1][4-6]. How-ever, they neglect the contex tual information of music. \nRyynänen et al. [8] used both acoustic and musicolog-\nical models to generate hidden Markov models (HMMs) for a singing melody transcription system. The musico-logical models determine the transition probabilities be-tween the adjacent notes. Li et  al. [2] also utilized an \nHMM where the transition probability was estimated from the labeled training data. However, they only consi-\ndered the transition between adjacent notes; the concur-\nrent pitches generated by other musical instruments, such as chords, were not considered. \nWhile the concurrent pitches are usually the obstacles \nin singing pitch extraction, we try to utilize them as the cues to extract the melody. Ge nerally speaking, melody is \ncomposed of a series of notes and is decorated by chords. The chords here represent the concurrent pitches accom-panying the melody. These notes and chords progress ac-cording to some underlying music rules to make the song euphonious. Therefore, we use an HMM to learn these rules from actual song data by observing their spectro-grams. Note that we do not identify the chords explicitly. Instead, we use the energy di stribution of each semitone \nto train the contextual audio model. In addition, in order to utilize the harmonics information as cues to extract the singing pitches, we also model the energy distribution of harmonics by using the proposed normalized sub-harmonic summation (NSHS) to enhance the harmonic structures of the sound sources especially for those of the singing voices. By synergizing these two techniques, the accuracy of singing pitch extr action is improved signifi-\ncantly. \nThe rest of this paper is organized as follows. Section \n2 describes the proposed system in detail. The experi-mental results are presented in section 3, and section 4 concludes this work with possible future directions. \n2. SYSTEM DESCRIPTION \nFig. 1 shows the overview of the proposed system. Two \nstreams of features are ex tracted from the spectrogram \nand the NSHS map, respectively, of the input polyphonic song. A 2-stream HMM is then employed to decode the input songs into the most likely unbroken pitch vectors. On the other hand, the MFCCs (Mel-frequency cepstral coefficients) are extracted to perform the voiced/non-voiced detection. Lastly, the singing pitch vectors are produced by integrating the results of these two processes. The following subsections explain these blocks in detail. \n2.1 Features Extraction from a Spectrum \nThis block extracts two types of features, including \nMFCCs and ESI (Energy at Semitones of Interests). \n \nPermission to make digital or hard copi es of all or part of this work fo r\npersonal or classroom use is grante d without fee provided that copies\nare not made or distributed for profit or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page. \n© 2009 International Society for Music Information Retrieval  \n201Poster Session 2\n  \n \nMFCCs are the features for a 2-state HMM for \nvoiced/non-voiced detection. ESI is the 1st-stream fea-ture for a 2-stream HMM fo r pitch extraction. Since \nmost of the songs nowadays follow the twelve-tone equal temperament, it is intuitive to employ semitone scale to model the relations between melody and chords. For each integer semitone of interests within the \nrange\n[]72,40 , we identify its maximum energy as an \nelement of the feature vector. Take semitone 69 for ex-\nample, the search range in semitone is [] 5.69,5.68 , cor-\nresponding to a frequency bin of [] 89.452,47.427  in \nterms of Hertz. Then we find the maximum power spec-\ntrum within this range as th e feature associated with se-\nmitone 69. Since there are 33 elements within semitone \nof interests, the length of the feature vector of ESI is also 33. \nMore specifically, the ESI computed from a spectrum \nin the time frame \nt can be obtained as follows: \n \n()()fP mvtf fffffft\nm m\nmm m\nm2 21 1max )(\n−+<≤−−+ −= ,                 (1)                         \n \nwhere ()∗tP  is the power spectrum calculated from short \ntime Fourier transform (STFT), 1 ,..,1,0− = M m , M is \nthe total number of semitones that are taken into account, \nand mf is the frequency of mth semitone in the selected \npitch range. \nNote that we also need to  record the maximizing fre-\nquency within each frequency bin in order to reconstruct the most likely pitch contours. \n2.2 HMM-based Voiced/Non-voiced Detection \nThis block employs a continuous 2-state HMM to decode \nthe mixture input into voiced and non-voiced segments, similar to the one proposed by Fujihara et al. [9]. Note \nthat the “voiced” here indi cates the voiced singing voice, \nand “non-voiced” indicates the unvoiced singing voice and music accompaniments. Given the MFCC feature \nvectors \n},,,{0 ⋅⋅⋅⋅⋅⋅=tx x X  of the input mixtures, the \nproblem is to find the most probable sequence of \nvoiced/non-voiced states, },,,{ˆ0⋅⋅⋅⋅⋅⋅=ts s S :  \n{}\n⎪⎭⎪⎬⎫\n⎪⎩⎪⎨⎧\n= ∏ −\ntt t t t tSsspsxpsp sxpsp S )|()|()( )|()( maxargˆ1 0 0 0, (2)  \nwhere )|(sxp  is the output likelihood of a state s, \n)|(1−ttssp  is the state transition probability from state \n1−ts to ts, and )(tsp  is the prior of the state ts. Note \nthat )|(1−ttssp  and )(tsp  can be obtained from the ac-\ntual song data with manual annotations. \n2.3 Features Extraction from NSHS \nThis block extracts the 2nd-stream feature vector which \nrepresents the energy distributions of the enhanced har-monic structures of singing voices. The harmonic struc-tures can be enhanced by sub-harmonic summation (SHS) proposed by Hermes [10]: \n() ( )∑\n==N\nntn t nfPh fH\n1,                             (3) \nwhere ()fHt  is the sub-harmonic summation value of  \nthe frequency f at time frame t, ()∗tP  is the power \nspectrum calculated from STFT, n is the index of har-\nmonic components, N is the number of the harmonic \ncomponents in consideration, and nh is the weight indi-\ncating the contribution of the nth harmonic component. \nUsually we set 1−=n\nnh h , where 1≤h . In order to fur-\nther enhance the harmonics  of singing voices, we propose \nthe use of normalized SHS (NSHS)  defined as follows: \n()()\n∑∑\n===\nff\nN\nnnN\nntn\nt\nhnfPh\nfH\n11 ˆ  ,                             (4) \nwhere the number of harmonic components fN depend \non the frequency under consideration: \n⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛=fffloor Ns\nf5.0,                              (5) \nwith sfbeing the sampling rate. The reason of the mod-\nification is based on the obs ervation that most of the \nenergy in a song in located at the low frequency bins, and the energy of the harmonic structures of the singing voice \nseems to decay slower than that of instruments [2]. Therefore, when more harmonic components are consi-dered, energy of the vocal s ounds is further strengthened. \nAlthough some percussive instruments (e.g. cymbals) 0 50 100 150 200 250 300 350 400405060\nTime FramesSemitonesSpectrum computation NSHS computation\nSinging pitch vectors2nd-stream 1st-stream MFCC\nunbroken pitch vectors V/N decisionsPolyphonic songs\nMFCC \nExtractionESI extraction \nfrom a spectrumESI extraction \nfrom NSHSspectrum NSHS spectrumFeature extraction from a spectrum Feature extraction from NSHS\nHMM-based \nvoiced/non-voiced detection2-stream HMM-based\npitch extraction\n \nFigure 1 . System overview \n \n20210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \npresent high energy at hi gher frequency bins, their non-\nharmonic nature does not affect the NSHS much. \nFigure 2 illustrates the energy  distributions of a tradi-\ntional spectrogram, original SHS map, and the proposed NSHS map. By comparing the spectrogram in 2(b) and the SHS map in 2(c), it is obvious that most of the energy of accompaniments in the spectrogram is attenuated in the SHS map. However, the energy in the lower frequen-cy bins remains high. The proposed NSHS map shown in 2(d) further attenuates the low -frequency energy and en-\nhances the sub-harmonic struc ture of the singing voice. \nAs a result, after the enhan cement by the NSHS map, the \npitch of the singing voice can be extracted much easier. \nBased on the proposed NSHS, we can extract a 33-\nelement feature vector of ESI for each given frame, as explained in Section 2.1. The feature vector is sent to the \n2-stream HMM for pitch extraction. \n2.4 2-Stream HMM-based Pitch Extraction \nWe employ a 2-stream HMM to model the relationship \nbetween the adjacent melody pitches and their corres-ponding audio context. Given th e 1st-stream ESI feature \nvectors \n},,,{0⋅⋅⋅⋅⋅⋅=tv v V  from spectrogram and the 2nd-\nstream ESI feature vectors },,,{0⋅⋅⋅⋅⋅⋅=tc c C  from NSHS \nmap, our goal is to find the most likely sequence of pitch \nstates, },,,{ˆ0 ⋅⋅⋅⋅⋅⋅=tr r R :  {}\n⎪⎭⎪⎬⎫\n⎪⎩⎪⎨⎧\n= ∏ −\nttt ttt tRrrprcvprp rcvprp R )|()|,()( )|,()( maxargˆ1 0 00 0,\n    (6) \nwhere )|(1−ttrrp  is the state transition probability from \npitch state 1−tr to tr, )(trp  is the prior of the pitch state \ntr, and )|,( rcvp  is the joint output likelihood of the \npitch state r defined as: \n)|()|( )|,( rcprvp rcvpc v= ,                  (7) \nwhere )|(rvpv  and )|(rcpc  are the state likelihoods of \nfeature vectors vand c, respectively, given the state r. \nThis is a typical multi-stream HMM which is broadly \nused in speech processing [11]. The state likelihoods (or conditional observation likelihoods), transition probabili-ties, and priors of eq. (6) and (7) can all be obtained from \nthe actual song data with manually annotated pitch con-tours. \nFigure 3 shows the benefits of applying a 2-stream \nHMM instead of using a single-stream feature from either the spectrum or the NSHS. Each of the plots is a state-frame likelihood table where the vertical axis indicates the pitch state of each semitone  and horizontal axis indi-\ncates time frames. The likelihood is computed for each state and time frame. All likelihood in the same time frame is normalized to zero mean and unity variance for better visualization. The ideal  singing pitches are overlaid \nas solid lines. Figure 3(a) shows \n)|(rvpv  of each state \nwhich utilizes audio context as cues to extract the singing \npitches. Figure 3(b) shows )|(rcpc  of each state which \nindicates the likelihood that an enhanced singing harmon-\nic structure is presented, a nd Figure 3(c) shows the joint \nlikelihood )|()|( rcprvpc v . Figure 3(d) and (e) illustrate \nthe overall maximum likelihoods (up to a given frame \ntime and pitch state) of single-stream HMMs using fea-ture vectors \nV and C, respectively. More specifically, \nthe value of each point in the figure represents the max-\nimized accumulated likelihood of the previous pitch states sequence including the transition probabilities.  Again, for better visualization, each column in these two tables is normalized to have zero mean and unity variance. The joint likelihood using the 2-stream HMM is shown in Figure 3(f). It can be observe d that the likelihood of the \nsinging pitch states at around 1.2 and 5.6 seconds are low \nin Figure 3(d), but they are recovered in Figure 3(f) by combining with the likelihood in Figure 3(e). In addition, the likelihood of the states that are not corresponding to the singing pitches between 1. 5 and 5.3 seconds in Figure \n3(e) are diminished in Figure  3(f) as well. Furthermore, \nboth single-stream HMMs e xhibit high likelihood for the \nsinging pitch states. Therefore, after combining the like-lihood using the 2-stream HMM, the likelihood of the singing pitch states are higher th an that of the other states, \nand the pitches of singing voices can thus be extracted more accurately. 1 2 3 4 5 6−1  01(a)Amplitude\n(b)Frequency (Hz)\n1 2 3 4 5 6100200300400500\n(c)Frequency (Hz)\n1 2 3 4 5 6100200300400500\n(d)Frequency (Hz)\n1 2 3 4 5 6100200300400500\n1 2 3 4 5 6100200300400500(e)\nTime (sec)Frequency (Hz)\n \nFigure 2 . The energy distributions of a sample clip \nAmy_4_05 in MIR-1K dataset at 0 dB SNR. The distri-butions are computed within the frequency range [80.06, 538.58], or [39.5, 72.5] in terms of semitones. (a) The waveform of the mixture. (b) The spectrogram. (c) The SHS map. (d) The proposed NSHS map. (e) The manual-ly labeled pitch vector of the singing voice.   \n203Poster Session 2\n  \n \n3. EVALUATION \nTwo datasets were used to evaluate the proposed ap-\nproach. The first one, MIR-1K1, is a publicly available \ndataset proposed in our previous  work [12]. It contains \n1000 song clips recorded at 16 kHz sample rate with 16-bit resolution. The duration of each clip ranges from 4 to 13 seconds, and the total length of the dataset is 133 mi-nutes. These clips were extracted from 110 karaoke songs which contain a mixed track and a music accompaniment track. These songs were selected (from 5000 Chinese pop songs) and sung by our colleagues in the lab, consisting of 8 females and 11 males. Most of the singers are ama-teurs with no professional training. The music accompa-niment and the singing voice we re recorded at the left \nand right channels, respectiv ely. The second dataset, \ncalled commercial set for short , contains 178 song clips \n                                                          \n \n1 The MIR-1K dataset is available at \nhttp://unvoicedsoundseparati on.googlepages.com/mir-1k  \n from commercial CDs, and the total length of the dataset \nis about 25 minutes. The ground truth of the voiced/non-\nvoiced segments and pitch values of the singing voices were first estimated from the pure singing voice and then manually adjusted for these two datasets.  \nAll songs are mixed at 0 dB SNR, indicates that the \nenergy of the music accompaniment is equal to the sing-ing voice. Note that the SNRs for commercial pop songs are usually larger than zero,  indicating that our experi-\nments were set to deal with more adversary scenarios than the general cases. \n3.1 Evaluation for Voiced/Non-voiced detection \nThe evaluation was performed via two-fold cross valida-\ntion with the MIR-1K dataset. The dataset was divided into two subsets of similar sizes (487 vs. 513, recorded by disjoint subjects). In addition, the commercial set was also evaluated by using all MIR-1K for training. The rea-son for not using the commercial set for training the voiced/non-voiced model is because its size is too small. \n39-dimensional MFCCs (12 cepst ral coefficients plus \na log energy, together with their first and second deriva-\ntives) were extracted from each frame. The MFCCs were computed from STFT with a half-overlapped 40-ms Hamming window. Cepstral mean subtraction (CMS) was used to reduce channel effects. \nTwo 32-component GMMs were trained for voiced \nframes and non-voiced frames, respectively. All GMMs had diagonal covariance matrices. Parameters of the GMMs were initialized via k-means clustering algorithm and were iteratively adjusted via expectation-maximization (EM) algorithm with 30 iterations. Each of the GMMs was considered as a state in a fully connected \n2-state HMM, where the transition probabilities and the weight of each GMMs we re obtained through frame \ncounts of the labeled dataset.  For a given input song mix-\nture, Viterbi algorithm was used to decode the mixture into voiced and non-voiced segments. \nTable 1 shows the performance of voiced/non-voiced \ndetection. The precision is the percentage of the frames that are correctly classified as voiced over the frames that \nare classified as voiced. The recall is the percentage of the frames that are correctly classified as voiced over all the voiced frames. The effects of the results will be dis-cussed in the following subsections.  \n3.2 Evaluation for Singing Pitch Extraction \nThe MIR-1K dataset was divided into two subsets in the \nsame way as subsection 3.1 for two-fold cross validation, \nand the commercial set was evaluated by using all MIR-1K for training. The spectrum of each frame was com- MIR-1K Commercial \nset \nPrecision 87.48 % 91.14 % \nRecall 86.03 % 91.78 % \nOverall accuracy 81.52 % 87.12 % \n \nTable 1.  Performance of voiced/non-voiced detection \n \nSemitoneState Likelihood Comparison\n(a) Contextual Audio Model Likelihood\n1 2 3 4 5 640455055606570\nSemitone(b) Enhanced  Harmonic Model Likelihood\n1 2 3 4 5 640455055606570\nTime (sec)Semitone(c) Joint Likelihood\n1 2 3 4 5 640455055606570\nSemitoneHMM Likelihood Comparison\n(d) Contextual Audio Model Likelihood\n1 2 3 4 5 640455055606570\nSemitone(e) Enhanced Harmonic Model Likelihood\n1 2 3 4 5 640455055606570\nTime (sec)Semitone(f) Joint Likelihood\n1 2 3 4 5 640455055606570\nFigure 3 . The state likelihood and HMM likelihood \ncomparison for the clip Amy_4_05 in MIR-1K dataset at 0 dB SNR. (a) to (c) and (d) to (f) show the likelih-ood of contextual audio model, the likelihood of en-hanced harmonic model, and the join likelihood of state likelihood and HMM likelihood, respectively. The solid line indicates the manually labeled pitch vector of the singing voice. \n20410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nputed from STFT with a half-overlapped 40-ms window \nand zero padding to142. In addition, the pitch range for \ncomputing ESI (for both spect ra and NSHS) was [40-0.5, \n72+0.5] in semitones or [80.06, 538.58] in Hertz, which is similar to the common singing frequency range used in \n[2]. The compression factor \nh for computing NSHS was \nset to 0.99. At last, a 33-dimentional feature vector tv \nfrom spectra and a 33-dimentional feature vector tc from \nNSHS were extracted for each frame. \nTwo diagonal 8-component GMMs,  VΓ and CΓ, \nwere trained for each of the 33 semitone models by using \nfeature vectors tv and tc, respectively. Parameters of the \nGMMs were initialized via k-means clustering algorithm \nand were iteratively adjusted via EM algorithm with 30 \niterations. Each of the mC V ),(ΓΓ  pairs (with ]32,0[=m )  \nwas considered as a state in  an HMM, where the transi-\ntion probabilities and the prior of each GMM were ob-tained through frame counts of the labeled dataset. For a given input mixture, Viterbi algorithm was used to de-\ncode the mixture into a sequence of pitch states \nRˆ. By \ntracking the maximizing freque ncy (which generates ESI \nat each semitone) for each pitch state, we can then recon-struct the optimum pitch contour. \nIn order to evaluate the propos ed method, eight other \napproaches were used for comparison. For simplicity, we use SPEC and NSHS to indicate ESI that were extracted from a spectrum or a NSHS, respectively. In addition, HMM, DP, and MAX are used to indicate different schemes for extracting the singi ng pitches. More specifi-\ncally, HMM represents the proposed HMM approach; DP represents the approach of dynamic programming over spectrum/NSHS directly (to be detailed next); MAX is simply maximum-picking over spectrum/NSHS. \nThe goal of the DP method is to find a path \n[]1 0 ,,,,−⋅⋅⋅⋅⋅⋅=n i f f f f  that maximizes the score function: \n() ( )∑∑−\n=−\n=−−×− =1\n01\n11 , sn\ntn\ntt t tt ff fY f core θ θ ,          (8) \nwhere ()ttfY  is a feature vector  extracted from spec-\ntrum/NSHS at the frame t and frequency tf. The first \nterm in the score function is the sum of energy of the \npitches along the path, while the second term controls the smoothness of the path with the use of a penalty term \nθ \n(which is set to 2 in this study). If θ is larger, then the \ncomputed path are smoother. In particular, the MAX ap-\nproach sets θ to be zero so that maximizing the above \nobjective function is equivalent to maximum-picking of \nthe features from spectrum/NSHS of each frame. \nThe DP method employs dynamic programming to \nfind the maximum of the score function, where the opti-\nmum-valued function ),(mtD  is defined as the maximum \nscore starting from frame 1 to t, with mft=: \n \n[]{ }, ),1( max)( ),(\n32,0mk k tD mY mtD\nkt −×−− +=\n∈θ         (9) where nis the number of frames, ]1,1[−=n t , and \n]32,0[=m . The initial conditions are )( ),0(0mY m D= , \nand the optimum score is equal to\n[]),1( max\n32,0m nD\nm−\n∈. \nMoreover, the “Dressler” appr oach indicates a melody \nextraction method proposed by Dressler [4] which ranked first from 2005 to 2006 in the MIREX task of audio me-lody extraction. We obtained the software from her for comparison purpose. The “Cao” approach indicates the method proposed by Cao et al. [5], which was re-implemented by us for comparison. \nFigure 4 shows the performance comparison for the \nsinging pitch extraction. Figure 4(a) shows the raw pitch accuracy with ideal voiced/ non-voiced detection, where \nthe correct rate is computed over the frames that were la-beled as voiced in the reference files. Figure 4(b) shows the raw pitch accuracy with  automatically detected \nvoiced/non-voiced segments. Figure 4(c) shows the over-all accuracy where all frames are taken into account for computing the correct rate. In other words, Figure 4(a) shows the performance of the singing pitch extraction alone, assuming ideal voiced/non-voiced detection. On the other hand, the Figure 4(b) and (c) shows the perfor-mance in a practical situati on where the results are af-\nfected by the errors of voiced/non-voiced detection. Since Dressler’s and Cao’s method perform singing voice detection implicitly, their performance is only shown for the cases of raw pitch and overall accuracy in Figure 4(b) and (c). Note that Dressler’s method was designed not MIR−1K Commercial Set5060708090Correct Rate (%)Performance Comparison of Singing Pitch Extraction\n(a) Raw Pitch Accuracy with Ideal Voiced/Non−voiced Detection\nSPEC−MAX\nSPEC−DP\nSPEC−HMM\nNSHS−MAX\nNSHS−DP\nNSHS−HMM\nProposed\nMIR−1K Commercial Set5060708090Correct Rate (%)(b) Raw Pitch Accuracy\nSPEC−MAX\nSPEC−DP\nSPEC−HMM\nNSHS−MAX\nNSHS−DP\nNSHS−HMM\nProposed\nDressler\nCao\nMIR−1K Commercial Set5060708090Correct Rate (%)(c) Overall Accuracy\nSPEC−MAX\nSPEC−DP\nSPEC−HMM\nNSHS−MAX\nNSHS−DP\nNSHS−HMM\nProposed\nDressler\nCao\n \nFigure 4 . Performance comparison for singing pitch ex-\ntraction. (a) Raw pitch accuracy with ideal voiced/non-voiced detection. (b) Raw pitch accuracy. (c) Overall accuracy.  \n205Poster Session 2\n  \n \nonly to extract the melody from vocal songs, but also \nfrom non-vocal music which contains no singing voice, so the performance may not be as good as the other ap-proaches that solely designed for vocal songs. \nThe proposed system achieved 71.10% and 80.24% \noverall accuracy in MIR-1K and commercial set, respec-\ntively. Experiments show that performance is significant-ly improved by applying the proposed HMM and by the NSHS in both datasets. Two points are worth noting. Firstly, while NSHS enhances the harmonic structures of both the singing voices and chords, the energy enhance-ment of chords is relativel y weaker. Therefore, the im-\nprovement of using HMM over the MAX and DP ap-proaches is much larger by using spectrum-based ESI than NSHS-based. This shows that the chord information \nembedded in spectrum-based ESI does help for extracting the singing pitches. Secondly, when spectrum-based ESI are replaced by NSHS-based ESI, the performance of MAX and DP is improved significantly. It shows that the NSHS does help for reducing the interference of non-singing pitches. By taking the advantages of both ap-proaches, the proposed method therefore performs signif-icantly better than the compared approaches. \n4. CONCLUSIONS \nIn this paper, we propose a new singing pitch extraction \nsystem by employing a 2-stream HMM to model the rela-tion between adjacent notes and between melody and chords. By modeling the energy distribution in spectro-gram and in the proposed NSHS map, the performance is significantly improved. Besides, the improvement of the performance is quite similar in different datasets which confirms the robustness of the proposed approach. \nThe proposed NSHS only applies a simple weight \nfunction for harmonic components; the performance can be further improved by optimizing it with the training scheme proposed by Klapuri [13]. In addition, the raw \npitch accuracy with ideal voi ced/non-voiced detection of \nthe proposed system is much higher than that of the over-all accuracy (6~8%). Th erefore it is also one of our future \ndirections to improve the voiced/non-voiced detector by not only using MFCCs but also considering the voice vi-brato information as proposed by Regnier et al. [14].  \nIt is worth noting that the evaluation is performed by \nusing our dataset, MIR-1K, which contains more song clips than that used in MI REX (less than 20 minutes, and \nonly 7 minutes of them are publicly available). It allows researchers to evaluate and compare their systems with \nothers easily by using the more comprehensive dataset. \n5. ACKNOWLEDGEMENT \nThis study is conducted under the \"III Innovative and \nProspective Technologies Project \" of the Institute for In-\nformation Industry which is subsidized by the Ministry of Economy Affairs, the Republic of China. The authors would also like to thank K. Dressler for providing her pitch tracking software.  6. REFERENCES \n[1] M. Goto, “A Real-Time Music Scene Description \nSystem: Predominant-F0 Estimation for Detecting Melody and Bass Lines in Real-World Audio Signals,” \nSpeech Communication , vol. 43, no. 4, pp. \n311–329, 2004. \n[2] Y. Li and D. L. Wang, “Detecting Pitch of Singing \nVoice in Polyphonic Audio,” IEEE ICASSP , pp. \n17–20, 2005. \n[3] G. E. Poliner and D. P. W. Ellis, “A Classification \nApproach to Melody Transcription,”  6th ISMIR , pp. \n161-166, 2005. \n[4] K. Dressler, “An Auditory Streaming Approach on \nMelody Extraction,” Extended abstract for 7th \nISMIR , 2006. \n[5] C. Cao, M. Li, J. Liu and Y. Yan, “Singing Melody \nExtraction in Polyphonic Music by Harmonic Tracking,” \n8th ISMIR , 2007. \n[6] V. Rao and P. Rao, “Melody Extraction Using \nHarmonic Matching,” Extended abstract for 9th \nISMIR,  2008. \n[7] J.-L. Durrieu, G. Richard and B. David, “An \nIterative Approach to M onaural Musical Mixture \nDe-soloing,” IEEE ICASSP , pp. 105-108, 2009. \n[8] M. Ryynänen and A. Klapuri, \"Transcription of the \nSinging Melody in Polyphonic Music,\" 7th ISMIR , \npp. 222-227, 2006. \n[9] H. Fujihara, M. Goto, J. Ogata, K. Komatani, T. \nOgata, and H. G. Okuno, “Automatic Synchronization between Lyrics and Music CD Recordings Based on Viterbi Alignment of Segregated Vocal Signals,” \nISM, pp. 257–264, 2006. \n[10] D. J. Hermes, “Measurement of Pitch by \nSubharmonic Summation,” Journal of Acoustic \nSociety of America , vol.83, pp. 257-264, 1988. \n[11] S. J. Young, G. Evermann, M. J. F. Gales, T. Hain, \nD. Kershaw, X. Liu, G. Moore, J. J. Odell, D. Ollason, D. Povery, V. Valtchev, and P. C. Woodland: \nThe HTK Book (for HTK version 3.4) , \nCambridge University, 2006. \n[12] C. L. Hsu and J. S. Jang, “On the Improvement of \nSinging Voice Separation for Monaural Recordings Using the MIR-1K Dataset,” \nIEEE Trans. Audio, \nSpeech, and Language Processing , accepted. \n[13] A. Klapuri, \"Multiple Fundamental Frequency \nEstimation by Summing Harmonic Amplitudes,\" \n7th ISMIR , 2006. \n[14] L. Regnier and G. Peeters, “Singing Voice \nDetection in Music Tracks using Direct Voice Vibrato Detection,” \nIEEE ICASSP , pp. 1685-1688, \n2009. \n206"
    },
    {
        "title": "Lyric-based Song Emotion Detection with Affective Lexicon and Fuzzy Clustering Method.",
        "author": [
            "Yajie Hu",
            "Xiaoou Chen",
            "Deshun Yang"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417983",
        "url": "https://doi.org/10.5281/zenodo.1417983",
        "ee": "https://zenodo.org/records/1417983/files/HuCY09.pdf",
        "abstract": "A method is proposed for detecting the emotions of Chinese song lyrics based on an affective lexicon. The lexicon is composed of words translated from ANEW and words selected by other means. For each lyric sentence, emotion units, each based on an emotion word in the lexicon, are found out, and the influences of modifiers and tenses on emotion units are taken into consideration. The emotion of a sentence is calculated from its emotion units. To figure out the prominent emotions of a lyric, a fuzzy clustering method is used to group the lyric’s sentences according to their emotions. The emotion of a cluster is worked out from that of its sentences considering the individual weight of each sentence. Clusters are weighted according to the weights and confidences of their sentences and singing speeds of sentences are considered as the adjustment of the weights of clusters. Finally, the emotion of the cluster with the highest weight is selected from the prominent emotions as the main emotion of the lyric. The performance of our approach is evaluated through an experiment of emotion classification of 500 Chinese song lyrics.",
        "zenodo_id": 1417983,
        "dblp_key": "conf/ismir/HuCY09",
        "keywords": [
            "emotion detection",
            "Chinese song lyrics",
            "affective lexicon",
            "emotion units",
            "modifiers",
            "tenses",
            "fuzzy clustering",
            "clustering method",
            "prominent emotions",
            "main emotion"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nLYRIC-BASED SONG EMOTION DETECTION\nWITH AFFECTIVE LEXICON AND FUZZY CLUSTERING METHOD\nYajie Hu, Xiaoou Chen and Deshun Yang\nPeking University\nInstitute of Computer Science & Technology\n{huyajie,chenxiaoou,yangdeshun }@icst.pku.edu.cn\nABSTRACT\nA method is proposed for detecting the emotions of Chi-\nnese song lyrics based on an affective lexicon. The lexicon\nis composed of words translated from ANEW and words\nselected by other means. For each lyric sentence, emo-\ntion units, each based on an emotion word in the lexicon,\nare found out, and the inﬂuences of modiﬁers and tenses\non emotion units are taken into consideration. The emo-\ntion of a sentence is calculated from its emotion units. To\nﬁgure out the prominent emotions of a lyric, a fuzzy clus-\ntering method is used to group the lyric’s sentences accord-\ning to their emotions. The emotion of a cluster is worked\nout from that of its sentences considering the individual\nweight of each sentence. Clusters are weighted accord-\ning to the weights and conﬁdences of their sentences and\nsinging speeds of sentences are considered as the adjust-\nment of the weights of clusters. Finally, the emotion of the\ncluster with the highest weight is selected from the promi-\nnent emotions as the main emotion of the lyric. The perfor-\nmance of our approach is evaluated through an experiment\nof emotion classiﬁcation of 500 Chinese song lyrics.\n1. INTRODUCTION\nIn order to organize and search large song collections by\nemotions, we need automatic methods for detecting the\nemotions of songs. Especially, they should work in small\ndevices such as iPod and PDA. At present, much, if not\nmost, research work on song emotion detection was con-\ncentrated on the audio signals of songs. For example, a\nnumber of algorithms [2,7,9] that classify songs from their\nacoustic properties were developed.\nThe lyric of a song, which will be heard and understood\nby listeners, plays an important part in determining the\nemotion of the song. Therefore, detecting the emotions of\nthe lyric effectively contributes to detecting the emotions\nof the song. However, there is now comparatively less\nresearch done on methods for detecting the emotions of\nsongs based on lyrics. There has been indeed a very large\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.\nValence Arousal \n(more energetic)\n(more positive)Exhilarated \nExcited \nHappy \nPleasure Anxious \nAngry \nTerrified \nDisgusted \nSad \nDespairing \nDepressed \nBored Relaxed \nSerene \nTranquil \nClam +V+A -V+A\n-V-A +V-AFigure 1 . Russell’s model of mood\nliterature already out there on emotion analysis or opinion\nanalysis of text. But, nearly all of them [1, 3, 6] use a one-\ndimensional model of emotions, such as positive-negative,\nwhich is not ﬁne enough to represent lyric emotions which\nneed more dimensions. Lyrics are much smaller in size\nthan other kinds of text, such as Weblogs and reviews, and\nthis makes it hard to detect lyrics’ emotions. Being more\nchallenging, lyrics are often abstract and in lyrics, emo-\ntions are expressed implicitly.\nWe propose an approach to detecting the emotions of\nlyrics based on an affective lexicon. The lexicon is orig-\ninated from a translated version of ANEW and then ex-\ntended. According to the lexicon, emotion units(EUs) [13]\nof a sentence are extracted and the emotion of the sentence\nis calculated from those EUs.\nA lyric generally consists of several sentences and those\nsentences usually expresses more than one emotions. In\norder to ﬁgure out all the prominent emotions of a lyric,\nwe use a fuzzy clustering method on the sentences of the\nlyric. The method is robust enough to sustain the noises\ninduced in previous processing steps.\nIn our approach, Russell’s model of mood [11] is adopted,\nas shown in Figure 1, in which emotions are represented\nby two dimensions, valence andarousal . The lyric ﬁles\nwe use are in LRC format1which have time tags in them\nand we got the LRC ﬁles from the Web. The framework of\nour approach is illustrated in Figure 2. It consists of three\n1http://en.wikipedia.org/wiki/LRC_(file_\nformat)\n123Poster Session 1\nFigure 2 . The framework of the proposed approach\nmain steps: (i) building the affective lexicon (ANCW); (ii)\ndetecting the emotion of a sentence; (iii) integrating the\nemotions of all sentences.\nThe rest of this paper is organized as follows. In Section\n2, the method for building an affective lexicon is presented.\nSection 3 describes the method for detecting the emotions\nof sentences. The approach to integrating the emotions of\nsentences is described in Section 4. Experiments and dis-\ncussion are presented in Section 5. Finally, we conclude\nour work in Section 6.\n2. BUILDING THE AFFECTIVE LEXICON\n2.1 Translating the Words in ANEW\nFor analyzing the emotion of Chinese song lyrics, an af-\nfective lexicon called ANCW (Affective Norms for Chi-\nnese Words) is built from the Bradley’s ANEW [4]. The\nANEW list was constructed during psycholinguistic exper-\niments and contains 1,031 words of all four open classes.\nAs described in it, humans assigned scores to each word\naccording to dimensions such as pleasure ,arousal , and\ndominance . The emotional words in ANEW were trans-\nlated into Chinese and these constitute the basis of ANCW.\n10 people took part in the translation work. Each of them\nwas asked to translate all the words in ANEW into Chi-\nnese words that he/she thought to be unambiguous and\nused often in lyrics. The Chinese word that was chosen\nby the largest number of translators for an ANEW word\nwas picked and added into ANCW. A word may have more\nthan one part of speech(POS), namely performs different\nfunctions in different context, and each may have a differ-\nent emotion. Therefore, the part of speech of an ANCW\nword must be indicated. The words, the emotions of which\nin English culture are different from that in Chinese cul-\nture, are simply excluded from ANCW. To see if ANCW\nis consistent with ANEW, we use Meyers’s method [10] to\nextend ANCW based on a corpus of People’s Daily and the\nextended ANCW includes 18819 words. Meyers extends\nANEW to a word list including 73157 words. The distri-\nbutions of the emotion classes of the words in the extended\nANCW is illustrated in Figure 3. We ﬁnd that the emo-\ntion class distribution of the words in the extended ANCW\nis similar to the distribution of the words in the extended\nANEW. This proves that ANCW is consistent with ANEW\nand is reasonable.\nFigure 3 . Distributions of the words in the extended\nANEW and ANCW\nTable 1 . The origins of the words in ANCW\nOrigin Translated Synonyms Added by\nfrom ANEW lyrics corpus\n# of words 985 2995 71\n2.2 Extending ANEW\nHowever, the words translated from ANEW are not sufﬁ-\ncient for the purpose of detecting emotions of lyrics so it\nis necessary to extend ANCW. We extend ANCW in two\nways. In one way, with each word in ANCW as a seed,\nwe ﬁnd out all of its synonyms in TONG YI CI CI LIN2.\nThen, only synonyms with the same part of speech as that\nof their seed are added to ANCW. In the other way, we\nextract all constructions of apposition and coordination in\na corpus of lyrics(containing 18000 Chinese lyrics) by an\noff-the-shelf natural language processing tool [8]. If either\nword in such a construction is in ANCW, its counterpart\nis added to ANCW. The origins of the words in ANCW\nis shown in Table 1 and valence-arousal distribution of\nthe words in ANCW is illustrated in Figure 4. To indi-\ncate whether a word in ANCW is a translated word from\nANEW or a later added word, we attach an origin property\nto each word. Therefore, terms in the affect lexicon have\nthe following form: <word,origin ,POS,valence ,arousal >\n3. DETECTING THE EMOTION OF A SENTENCE\nFirst, word segmentation, POS annotation and NE recog-\nnition are performed for lyrics, with the help of the NLP\ntool. After stop words removed, the remaining words of\na sentence are examined to see if they appear in ANCW,\nand each of the words that do appear in ANCW constitutes\nan EU. If there is an adverb that modiﬁes or negates an\nemotion word, it is included in the corresponding EU as a\nmodiﬁer. We recognize the modiﬁers of EUs by using the\nNLP tool. The emotion of an EU is determined as follows:\nvu=vWord (u)·mModifier (u),v (1)\nau=aWord (u)·mModifier (u),a (2)\n2The lexicon of synonyms is manually built and includes 77,343 terms\n12410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 4 . Valence-arousal distribution of the words in\nANCW\nWhere vuandaudenote the valence and arousal value\nof EU urespectively, vWord (u)andaWord (u)denote the\nvalence and arousal value of the EU’s emotion word re-\nspectively, mModifier (u),vandmModifier (u),adenote mod-\nifying factors to represent the effect of the EU’s modiﬁer\non the EU’s valence and arousal respectively. vWord (u)\nandaWord (u), the valence and arousal value of the emo-\ntion word are obtained through looking up in ANCW. Sen-\ntences that have not any emotion unit are discarded.\nWe have collected 276 individual modiﬁer words, which\ncover all the occurrences in the Chinese lyric corpus we\nuse, and a table of modiﬁers has been set up. According to\nthe polarities and degrees to which modiﬁers inﬂuence the\nemotions of EUs, we assign each modiﬁer a modifying fac-\ntor on valence and a modifying factor on arousal. The val-\nues of the modifying factors are in the range of [−1.5,1.5].\nFor a negative modiﬁer adverb, mModifier (u),vis set to a\nvalue in [−1.5,0]and for a positive modiﬁer adverb, it is\nset to a value in [0,1.5].\nTenses inﬂuence the emotions of sentences. Some sen-\ntences literally depict a happy life or tell romantic stories in\none’s memory but, actually, the lyric implies the feeling of\nmissing the happiness or romances of past days. Similarly,\nthe sentence with future tense sometimes gives the sense\nof expectation. Therefore, when we calculate the emotions\nof sentences, the inﬂuence of particular tenses are consid-\nered. We use Cheng’s method [5] to recognize tenses of\nsentences and sentences are classiﬁed into three categories\nnamely, past,current andfuture , according to their tenses.\nA sentence may have more than one EUs. Because the\nEUs of a sentence always have similar or even identical\nemotions, they can be uniﬁed into one in a simple way, as\nfollows:\nvs=/summationtext\nu∈Usvu\n|Us|·fTense (s),v (3)Table 2 . Adjustment of wuandruof unit u\nIncrease when Decrease when\nwuuis after uis before\nadversative words; adversative words;\nuis after uis before\nprogressive words; progressive words.\nuis in title.\nru None. The emotion word’s\norigin is extended;\nThe sentence is\nadjusted by tense.\nas=/summationtext\nu∈Usau\n|Us|·fTense (s),a (4)\nwhere vsandasdenote the valence and arousal of sentence\nsrespectively, Usdenotes the set of EUs of the sentence,\nvuandaudenote the valence and arousal of EU u(u∈Us)\nrespectively, and fTense (s),vandfTense (s),aare modifying\nfactors to represent the effect of the tense of the sentence on\nvalence and arousal respectively. The values of the modi-\nfying factors representing the effects of tenses on emotions\nare in the range of [−1.0,1.0].\nThere are cases where two sentences(clauses) joined by\nan adversative or progressive word form an adversative or\nprogressive relation. The following are two examples:\nAdversative relation:\nYou are carefree\nBut I am at a loss what to do\nProgressive relation:\nNot only I miss you\nBut also I love you\nAdversative and progressive relations in lyrics tend to\nremarkably affect the strength of involved EUs in deter-\nmining the emotions of lyrics. Speciﬁcally, an emotion\nunit following an adversative word in a lyric inﬂuences the\nemotion of the lyric more signiﬁcantly than a unit before\nan adversative word does. For example, the EUs in the\nsentence before but is given less weight, while the EUs\nof the sentence after the adversative word is given more\nweight. Similarly, in a progressive relation, the emotion\nunit after the progressive word is thought to be more im-\nportant. So, a weight property is introduced for an EU to\nrepresent its strength of inﬂuence on lyric emotions. The\ninitial value of weight of an EU is set to 1. A conﬁdence\nproperty is also attached to an EU. If the emotion word of\nan EU is a later added word in ANCW, its conﬁdence will\nbe decreased. Also, if the emotion of a sentence is adjusted\ndue to a particular tense, the conﬁdence of its EUs will be\ndecreased. The initial value of conﬁdence of an EU is set\nto 0. The details of how to adjust the values of the weight\nand conﬁdence of an EU are shown in Table 2. Accord-\ningly, properties weight andconﬁdence are also introduced\nfor a sentence, which are calculated from that of its EUs in\na simple way as follows:\n125Poster Session 1\nws=/summationdisplay\nu∈Uswu (5)\nrs=/summationdisplay\nu∈Usru (6)\nwhere wsandrsdenote the weight and conﬁdence of\nsentence srespectively, and wuandrudenote the weight\nand conﬁdence of EU urespectively. wsandrsare used\nto determine the main emotion of a lyric in the following\nprocessing.\n4. INTEGRATING THE EMOTIONS OF ALL\nSENTENCES\n4.1 Challenges\n1.Reduce the effect of errors in sentence emotions on\nthe result of the emotions of lyrics.\n2.Recognize all the emotions of a lyric on the condi-\ntion that the lyric has more than one emotion.\n3.Select one emotion as the main emotion, if needed,\nor give a probability to each of the emotions.\n4.2 Methodology\nIn recent years, spectral clustering based on graph parti-\ntion theories decomposes a document corpus into a num-\nber of disjoint clusters which are optimal in terms of some\npredeﬁned criteria functions. If the sentences of a lyric\nare considered as documents and the lyric is regarded as\nthe document set, the document clustering technology can\nconquer the above three challenges. We deﬁne an emotion\nvector space model, where each sentence of a lyric is con-\nsidered as a node with two dimensions that represent the\nvalence and arousal of an emotion respectively. We choose\nWu’s fuzzy clustering method [12] because it can cluster\nthe sentences without the need to specify the number of\nclusters, which meets our demands. Wu’s fuzzy cluster-\ning method includes three steps: building a fuzzy similar-\nity matrix, generating a maximal tree using Prim algorithm\nand cutting tree’s edges whose weight is lower than a given\nthreshold.\nA song usually repeat some sentences. Sometimes the\nrepeated sentences are placed in one line, with each sen-\ntence having its own time tag. In other cases, each repeated\nsentence occupies one line and the line has one time tag. If\nthe repeated sentences are placed in more than one lines,\nthese sentences are bound to form a cluster in the later\nclustering processing. If the emotions of those repeated\nsentences were not recognized correctly, subsequent pro-\ncessing will be ruined deﬁnitely. Hence, before sentences\nare clustered, lyrics should be compressed so as to place\nthe iterative sentences in one line, with each sentence hav-\ning its own time tag.\nHaving examined hundreds of lyrics, we ﬁnd that sen-\ntences in a lyric always fall into several groups. The sen-\ntences of a group have similar emotions which can be uni-\nFigure 5 . Distribution of speed, V and A\nﬁed to a prominent emotion of the lyric. Therefore, the\nisolated sentences are mostly noises and will be removed.\nThere are a dozen of means to measure the similarity be-\ntween two nodes in vector space. After experiment those\nmeans, we select the following means to measure the sim-\nilarity of the sentences’ emotions i, j.\nSim ij= 1−σ(|vi−vj|+|ai−aj|) (7)\nwhere vi,vj,ai, and ajdenote the valence and arousal\nof sentences iandjrespectively, and σis set to 0.3.\nThe center of a survived cluster is calculated as the weighted\nmean of emotions of all members of the cluster. The weighted\nmean is deﬁned as follows:\nvc=/summationtext\ns∈Scvs·ws\n|Sc|(8)\nac=/summationtext\ns∈Scas·ws\n|Sc|(9)\nwhere Scdenotes the set of sentences in cluster c,vcand\nacdenote the valence and arousal respectively of cluster c,\nandvs,asandwsdenote the valence, arousal and weight\nrespectively of sentence s(s∈Sc).\nThe weight of cluster cis calculated as follows:\nwc=/summationdisplay\ns∈Sc(α·ws+β·Loop (s))\n−γ·rs+ 1(10)\nwhere Loop (s)denotes the number of times sentence\ns(s∈Sc) repeats, α, β andγare set to 2, 1, 1, respec-\ntively. These constant parameters are adjusted through ex-\nperimentation and the set of values resulting in the highest\nF-measure was chosen.\nLyrics we got have time tags and we use these tags to\ncompute the singing speed of sentences in lyrics, which is\ndeﬁned in milliseconds per word. Although, singing speed\nis not the only determinant of the emotions of lyrics, there\nis correlation between the singing speed of a song and its\nemotions, as shown in Figured 5. Hence, we use singing\nspeeds of sentences to re-weight each clustering center.\nHaving analyzed the singing speeds and emotions of the\nsongs in the corpus, we think that Gaussian Model is suit-\nable for expressing the degrees to which different singing\nspeeds inﬂuence emotions. The re-weighting is considered\nas follows:\n12610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTable 3 . The distribution of the songs corpus\nClass +V ,+A +V ,-A -V ,-A -V ,+A\n# of lyrics 264 8 174 54\nw/prime\nc=wc+M√\n2πσe−(Speed (c)−µv)2\n2σ2 +M√\n2πσe−(Speed (c)−µa)2\n2σ2\n(11)\nM= max( wc|c∈Lyric ) (12)\nwhere the µvandµaare the offset of vanda, respec-\ntively. The meaning of σis the variance of the speed of\nlyrics. Lyric is the set of emotion clusters of a lyric. Speed (c)\nis the average speed of sentences in cluster c. Finally, the\nclustering center with the highest weight is considered the\nmain emotion. If there is a need for the possibility of sev-\neral emotions, the possibility is computed as follows:\np(c) =w/prime\nc/summationtext\nc∈Lyricw/primec(13)\n5. EXPERIMENTS\nOur ultimate goal is to compute the valence and arousal\nvalue of lyrics, not to do classiﬁcation. We do classiﬁcation\nfor broad classes for the purpose of evaluating our emotion\ndetecting method and comparing the performance of our\nmethod with that of other classiﬁcation methods proposed\nin the literatures, many of which were for the same broad\nclasses.\n5.1 Data Sets\nTo evaluate the performance of our approach, we collected\n981 Chinese songs from the classiﬁed catalogue accord-\ning to emotion in www.koook.com . These songs are up-\nloaded by netizens and their genres include pop, rock &\nroll and rap. These songs were labeled by 7 people whose\nages are from 23 to 48. Two of them are professors and ﬁve\nare postgraduate students, all native Chinese. Each judge\nwas asked to give only one label to a song. The songs that\nare labeled by at least 6 judges to the same class are re-\nmained. We use these songs’ lyrics as the corpus. The\ndistribution of the corpus in four classes is shown in Table\n3. Although the number of songs in +V-A class is small, it\nis not surprising. This phenomenon conforms to the distri-\nbution in reality.\n5.2 Results\nTo demonstrate how our approach improves the emotion\nclassiﬁcation of lyrics in comparison to existing methods,\nwe implemented a emotion classiﬁcation method based on\nlyrics with emotion lexicon: Lyricator [10]. Lyricator uses\nANEW to extend the emotion lexicon by natural language\ncorpus with a co-occurrence method. Using the extended\nemotion lexicon, Lyricator computes the emotion of eachTable 4 . Evaluation results of Lyricator and our work\nClass Lyricator Our work\n+V+A Precision 0.5707 0.7098\nRecall 0.7956 0.6856\nF-measure 0.6646 0.6975\n+V-A Precision 0.0089 0.0545\nRecall 0.1250 0.7500\nF-measure 0.0167 0.1017\n-V+A Precision 0.6875 0.6552\nRecall 0.0632 0.3276\nF-measure 0.1158 0.4368\n-V-A Precision 0.0000 0.3125\nRecall 0.0000 0.2778\nF-measure 0.0000 0.2941\nsentence of a lyric and the sentence emotion is the mean of\nemotion values of the emotion words contained in the sen-\ntence. The emotion of a lyric is weighted mean of values\nof the emotions of sentences. The weight is deﬁned as the\nloop of sentences in the lyric.\nTo process Chinese lyrics, we translate the lexicon used\nin Lyricator and implement Lyricator’s method. What’s\nmore, the parameters are adjusted to gain its best perfor-\nmance. Under the same test corpus that has been men-\ntioned above, we compare Lyricator with our system. Ta-\nble 4 shows the evaluation results between Lyricator and\nour work in the same songs corpus. The precision for a\nclass is the number of lyrics correctly labeled the class di-\nvided by the total number of lyrics labeled as belonging\nto the class. The Recall is deﬁned as the number of true\npositive divided by the total number of lyrics that actually\nbelong to the positive class. The small number of lyrics\nin +V-A leads to the low precision for this class. Because\nwe have used the wealth of NLP factors and fuzzy cluster-\ning method, our method’s performance is better than the\nprevious work.\n5.3 Discussion\nAn analysis of the recognition results reveals the following\nﬁndings:\n1.Errors made by the NLP tool are especially salient\nbecause lyrics are very different from ordinary texts\nin word selection and arrangement. It is challenging\nfor the NLP tool to do word segmentation, POS and\nNE recognition well. For example,\nHope desperation and helpless to fly\naway\nthe NLP tool considered terms ”desperation” and ”help-\nless” as verbs while they are actually norms. With-\nout word lemmatization, recognizing POS of words\nin Chinese is much harder than in English. What’s\nmore, it will lead to errors in subsequent processing.\n2.Some errors were due to complex and unusual sen-\ntence structures, which make it hard for our rather\nsimple method to recognize emotion units correctly.\n127Poster Session 1\nFor example, the subject of a sentence is usually\nomitted due to the limitation of length of lyrics.\n3.It seems that lyrics usually don’t express much about\narousal dimension of emotion. Experimental results\nshow confusion rate between +A and -A is higher\nthan that between +V and-V , suggesting that lyrics\ndon’t express much about arousal dimension.\n4.The emotions of some lyrics were not explicitly ex-\npressed, and therefore deduced by human listeners\nbased on his or her knowledge and imagination.\nThe following sentences come from a typical lyric, the\nemotions of which are not recognized correctly:\nDo you love me? Maybe you love me.\nHanging your head, you are in silence.\nThose sentences form the chorus of CherryBoom’s Do\nYou Love Me and they express intensive emotions. Al-\nthough it is easy for human listeners to tell the emotions, it\nis quite difﬁcult for a computer to detect the emotions only\nliterally from the words of the lyric.\n6. CONCLUSION\nIn this paper, we propose an approach to detecting emo-\ntions of songs based on lyrics. The approach analyzes the\nemotion of lyrics with an emotion lexicon, called ANCW.\nIn order to obtain the emotion of a lyric from that of its\nsentences, we applied a fuzzy clustering technique which\ncan reduce the effect of errors introduced in the process of\nanalyzing emotions of sentences. Finally, we use the mean\nsinging speed of sentences to re-weight the emotion results\nof clusters. The experimental result is encouraging.\nAlthough this paper handles Chinese lyrics, we also im-\nplement an English version of emotion analysis system\nusing English lexicon because our method is not speciﬁ-\ncally designed for Chinese environment. What’s more, the\nmethod is unsupervised and training is not needed. Conse-\nquently, it takes about two seconds3to process a lyric and\nis apt to apply in small devices.\n7. REFERENCES\n[1]N. Archak, A. Ghose, and P. Ipeirotis. Show me the\nmoney! deriving the pricing power of product features\nby mining consumer reviews. In Proceedings of The\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining , 2007.\n[2]D. Bainbridge, S. J. Cunningham, and J. S. Downie.\nAnalysis of queries to a wizard-of-oz mir system:\nChallenging assumptions about what people really\nwant. In Proceedings of The 4th International Confer-\nence on Music Information Retrieval , pages 221–222,\n2003.\n[3]M. Bansal, C. Cardie, and L. Lee. The power of neg-\native thinking: Exploiting label disagreement in the\n3CPU: 400MHz; Memory: 128MB; OS: Windows Mobile 6.0min-cut classiﬁcation framework. In Proceedings of\nThe International Conference on Computational Lin-\nguistics , 2008.\n[4]M. M. Bradley and P. J. Lang. Affective norms for\nenglish words (anew): Stimuli, instruction manual\nand affective ratings. Technical report, The Center for\nResearch in Psychophysiology, University of Florida,\n1999.\n[5]J. Cheng, X. Dai, J. Chen, and Q. Wang. Processing\nof tense and aspect in chinese-english machine transla-\ntion. Application Research of Computer , V ol 3:79–80,\n2004.\n[6]P. Chesley, B. Vincent, L. Xu, and R. Srihari. Us-\ning verbs and adjectives to automatically classify blog\nsentiment. In AAAI Symposium on Computational Ap-\nproaches to Analysing Weblogs , page 27C29, 2006.\n[7]P. Knees, T. Pohle, M. Schedl, and G. Widmer. A\nmusic search engine built upon audio-based and web-\nbased similarity measures. In Proceedings of The 30th\nAnnual International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval ,\npages 23–27, 2007.\n[8]J. Lang, T. Liu, H. Zhang, and S. Li. Ltp: Language\ntechnology platform. The 3rd Student Workshop of\nComputational Linguistic , pages 64–68, 2006.\n[9]B. Logan, D. P.W. Ellis, and A. Berenzweig. Toward\nevaluation techniques for music similarity. In Proceed-\nings of The 4th International Conference on Music In-\nformation Retrieval , pages 81–85, 2003.\n[10] Owen C. Meyers. A mood-based music classiﬁcation\nand exploration system. Master’s thesis, Massachusetts\nInstitute of Technology, 2007.\n[11] Russell and James A. A circumplex model of affect.\nJournal of Personality and Social Psychology , V ol\n39(6):1161–1178, 1980.\n[12] Z. Shi. Knowledge Discovery . Tsinghua University\nPress, 2002.\n[13] Y . Xia, L. Wang, K. Wong, and M. Xu. Sentiment vec-\ntor space model for lyric-based song sentiment classiﬁ-\ncation. In Proceedings of ACL-08: HLT, Short Papers ,\npages 133–136, 2008.\n128"
    },
    {
        "title": "Lyric Text Mining in Music Mood Classification.",
        "author": [
            "Xiao Hu 0001",
            "J. Stephen Downie",
            "Andreas F. Ehmann"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416790",
        "url": "https://doi.org/10.5281/zenodo.1416790",
        "ee": "https://zenodo.org/records/1416790/files/HuDE09.pdf",
        "abstract": "This research examines the role lyric text can play in improving audio music mood classification. A new method is proposed to build a large ground truth set of 5,585 songs and 18 mood categories based on social tags so as to reflect a realistic, user-centered perspective. A relatively complete set of lyric features and representation models were investigated. The best performing lyric feature set was also compared to a leading audio-based system. In combining lyric and audio sources, hybrid feature sets built with three different feature selection methods were also examined. The results show patterns at odds with findings in previous studies: audio features do not always outperform lyrics features, and combining lyrics and audio features can improve performance in many mood categories, but not all of them.",
        "zenodo_id": 1416790,
        "dblp_key": "conf/ismir/HuDE09",
        "keywords": [
            "lyric text",
            "audio music mood classification",
            "ground truth set",
            "social tags",
            "lyric features",
            "representation models",
            "hybrid feature sets",
            "feature selection methods",
            "patterns",
            "previous studies"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nLYRIC TEXT MINING IN MUSIC MOOD CLASSIFICATION \nXiao Hu J. Stephen Downie Andreas F. Ehmann \nInternational Music Information Retrieval Systems Evaluation Laboratory \nUniversity of Illinois at Urbana-Champaign \nxiaohu@illinois.edu jdownie@illinois.edu aehmann@illinois.edu\nABSTRACT \nThis research examines the role lyric text can play in im-\nproving audio music mood classification. A new method \nis proposed to build a large ground truth set of 5,585 \nsongs and 18 mood categories based on social tags so as to reflect a realistic, user-centered perspective. A relative-\nly complete set of lyric features and representation mod-\nels were investigated. The best performing lyric feature set was also compared to a leading audio-based system. \nIn combining lyric and audio sources, hybrid feature sets \nbuilt with three different feature selection methods were also examined. The results show patterns at odds with \nfindings in previous studies:  audio features do not always \noutperform lyrics features, and combining lyrics and au-dio features can improve performance in many mood cat-\negories, but not all of them.  \n1. INTRODUCTION\nThere is a growing interest in developing and evaluating \nMusic Information Retrieval (MIR) systems that can pro-\nvide automated access to the mood dimension of music. \nTwenty-two systems have been evaluated between 2007 \nand 2008\n1 in the Audio Mood Classification (AMC) task \nof the Music Information Retrieval Evaluation eXchange \n(MIREX). However, during these evaluations, several \nimportant issues have emerged an d resolving these issues \nwill greatly facilitate further progress on this topic.  \n1.1 Difficulties Creating Ground Truth Data \nDue to the inherent subj ectivity of music perception, \nthere are no generally accepted standard mood categories. \nMusic psychologists have cr eated many different mood \nmodels but these have been criticized for missing the so-cial context of music listening [1]. Some MIR researchers \nhave exploited professionally assigned mood labels (e.g. \nAMG, MoodLogic\n2) [2,3], but none of these taxonomies \nhas gained general acceptance. Professionally created la-\nbels have been criticized for not capturing the users’ \nperspectives on mood.  \n                                                          \n1http://www.music-ir.org/mirex/2007/index.php/AMC\n2http://en.wikipedia.org/wiki/MoodLogicTo date, the AMC dataset is the only ground truth set \nthat has been used to evaluate mood classification sys-tems developed by multiple labs. However, this dataset \ncontains only 600 30 sec. song c lips. In fact, reported ex-\nperiments are seldom evaluated against datasets of more than 1,000 music pieces. The subjective nature of music \nmakes it very difficult to achieve cross assessor agree-\nments on music mood labels. A post-hoc analysis of the 2007 AMC task revealed discrepancies among human \njudgments on about 30% of the audio excerpts [4]. To \novercome the limitation, one could recruit more assessors to assess more candidate tracks. Unfortunately this would \nrequire too much human labor to be realistic for most \nprojects. Thus, it is clear that a scalable and efficient me-thod is sorely needed for building ground truth sets for \nmusic mood classification expe rimentation and evalua-\ntion. \n1.2  Need for Multimodal Mood Classification \nThe seminal work of Aucouturier and Pachet [5] revealed \na “glass ceiling” in spectral-ba sed MIR, due to the fact \nthat many high-level (e.g., semantic) music features \nsimply are not discernable using spectral-only techniques. Thus, researchers started to supp lement audio with lyrics \nand have reported improvements in such tasks as genre \nclassification and artist identification [6,[7]. However, very few studies have combined audio and text for music \nmood classification [8], and th eir limitations (see below) \ncall for more studies to investigate whether and how lyr-ics might help improve cl assification performance.  \n1.3 Related Work \nHu et al. [11] derived a set of three primitive mood cate-\ngories using social tags on last.fm. They collected social \ntags of single adjective words on a publicly available au-\ndio dataset, USPOP [12], and manually selected 19 mood \nrelated terms of the highest  popularity which then re-\nduced to three latent mood categories using multi-\ndimensional scaling. This set  was not adopted by others \nbecause three categories were seen as a domain oversim-\nplification. \nYang and Lee [8] performed early work on supple-\nmenting audio mood classificati on with lyric text analy-\nsis. They combined a lyric bag-of-words (BOW) ap-proach with 182 psychologi cal features proposed in the \nGeneral Inquirer [13] to disa mbiguate categories that au-\ndio-based classifiers found confusing and the overall classification accuracy was im proved by 2.1%. However, \ntheir dataset was too small (145 songs) to draw any relia-\nble conclusions. Laurier et al. [9] also combined audio \nPermission to make digital or hard copies of all or part of this work fo r\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page. \n© 2009 International Society for Music Information Retrieval  \n411Poster Session 3\nand BOW lyric features. They conducted binary classifi-\ncation experiments on 1,000 songs in four categories and \nexperimental results showed that audio + lyrics combined features improved classification accuracies in all four cat-\negories. Yang et al. [10] eval uated both unigram and bi-\ngram BOW lyric features as well as three methods for fusing lyric and audio sources on 1,240 songs in four cat-\negories.  In these studies, the set of four mood categories \nwas most likely oversimplified, the datasets were rela-tively small and the lyric text features, namely BOW in \ntf-idf representation, were very limited.  \nIn this paper, we describe a novel method of building a \nlarge-scale ground truth dataset with 5,585 songs in 18 \nmood categories. We then repo rt experiments on a rela-\ntively complete set of lyric text features, including func-\ntion words, POS features and the effect of stemming. Fi-\nnally, we examine the impact of lyric features on music mood classification by comparing  1) lyric features; 2) au-\ndio features; 3) hybrid (lyric  + audio features without fea-\nture selection; and, 4) hybri d features generated by three \nfeature selection methods.   \n2. BUILDING A GROUND TRUTH SET \n2.1 Data Collection \nWe began with an in-house collection of about 21,000 \naudio tracks. Social tags on these songs were then col-\nlected from last.fm. 12,066 of the pieces had at least one last.fm tag. Simultaneously, song lyrics were gathered \nfrom online lyrics databases. Lyricwiki.org was the major \nresource because of its broad coverage and standardized format. To ensure data quality, our crawlers used song \ntitle, artist and album information to identify the correct \nlyrics. In total, 8,839 songs had both tags and lyrics. A \nlanguage identification program\n3 was then run against the \nlyrics, and 55 songs were identified  and manually con-\nfirmed as non-English, leaving lyrics for 8,784 songs. \nTable 1 presents the composition of the collection. \nCollection Avg. length \n(sec.) Unique Have tags Have Eng-\nlish Lyrics \nUSPOP 253.6 8,271 7,301 6,948\nUSCRAP 243.5 2,553 456 237\nAmerican music 183.2 5,049 2,209 790\nMetal music 311.8 105 105 104\nBeatles 163.8 163 162 161\nMagnatune  253.9 4,204 1,261 19\nAssorted pop  233.8 600 572 525\nTotal (Avg.)  234.8 20,945 12,066 8,784\nTable 1.  Descriptions and statistics of the collection. \n2.2 Identifying Mood Categories \nSocial tag data are noisy. We employed a linguistic re-\nsource, WordNet-Affect [14], to filter out  junk tags and \ntags with little or no affective meanings. WordNet-Affect \nis an extension of Word Net where affective labels are as-\nsigned to concepts represen ting emotions, moods, or \nemotional responses. There were 1,586 unique words in \nthe latest version of WordNet-Affect and 348 of them ex-\nactly matched the 61,849 unique tags collected from \n                                                          \n3 http://search.cpan.org/search%3fmodule=Lingua::Identlast.fm. However, these 348 words were not all mood re-\nlated in the music domain. We turned to human expertise \nto clean up these words. Two human experts were con-sulted for this project. Bo th are MIR researchers with a \nmusic background and na tive English speakers. They first \nidentified and removed tags with music meanings that did not involve an affective aspect (e.g., “trance” and “beat”). \nSecond, judgmental tags such as “bad”, “poor”, “good” \nand “great” were removed. Third, some words have am-biguous meanings and there was not enough information \nto determine the intentions of the users when they applied \nthe tags. For example, does “love” mean the song is about \nlove or the user loves the song? To ensure the quality of \nthe labels, these ambiguous words were removed. 186 words remained and 4,197 songs were tagged with at \nleast one of the words. \nNot all the 186 words represent distinguishable mean-\nings. In fact, many of them are synonyms and should be \ngrouped together [3]. WordNet  is a natural resource for \nsynonym identification, because it organizes words into synsets . Words in a synset are synonyms from the linguis-\ntic point of view. WordNet-A ffect goes one step further \nby linking each non-noun synset (verb, adjective and ad-verb) with the noun synset from which it is derived. For \ninstance, the synset of “sorrowful” is marked as derived \nfrom the synset of “sorrow”. Hence, for the 186 words, those belonging to and being derived from the same syn-\nset in WordNet-Affect were grouped together. As a re-\nsult, the tags were merged into 49 groups. \nSeveral tag groups were further merged if they were \ndeemed musically similar by the experts. For instance, \nthe group of (“cheer up”, “cheerful”) was merged with \n(“jolly”, “rejoice”); (“melancholic”, “melancholy”) was \nmerged with (“sad”, “sadness”). Th is resulted in 34 tag \ngroups, each representing a m ood category for this data-\nset. Using the linguistic resources allowed this process to \nproceed quickly and minimized the workload of the hu-man experts.  \nFor the classification experiments, each category \nshould have enough samples to build classification mod-els. Thus, categories with fewer than 20 songs were \ndropped resulting in 18 mood categories containing 135 \ntags. These categories and their member tags were then validated for reasonableness by a number of native Eng-\nlish speakers. Table 2 lists the categories, a subset of their \nmember tags and number of songs in each category (after the filtering step described below)\n4.\n2.3 Selecting the Songs \nA song was not selected for a category if its title or artist \ncontained the same terms within that category. For exam-\nple, all but six songs tagged with “disturbed” were songs \nby the artist “Disturbed.” In this case, the taggers may \nsimply have used the tag to restate the artist instead of \ndescribing the mood of the song. In order to ensure enough data for lyric-base d experiments, we only se-\nlected those songs with lyrics whose word count was \ngreater than 100 (after unfolding repetitions as explained \n                                                          \n4Due to space limit, the complete tag list can be found at \nhttp://www.music-ir.org/archive/figs/18moodcat.htm\n41210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nin Section 3.2). After these filtering criteria were applied, \nwe were left with 2,829 unique songs.  \n  Multi-label classification is relatively new in MIR, \nbut in the mood dimensi on, it is more realistic than sin-\ngle-label classification: A music piece may be “happy \nand calm” or “aggressive and depressed,” etc. This is evi-dent in our dataset as we have many songs that are mem-\nbers of more than one mood category. Table 3 shows the \ndistribution of songs belonging to multiple categories. We adopted a binary classification approach for each of \nthe 18 mood categories, and the 2,829 songs formed the \npositive example set. \nCate gories # of ta gs #of son gs\ncalm, comfort, quiet, serene, mellow, chill ou t,… 25 1,394\nsad, sadness, unha ppy, melancholic, melanchol y 8 916\nhappy, happiness, ha ppy songs, happy m usic, … 6 472\nromantic, romantic music 2 447\nupbeat, gleeful, hi gh spirits, zest, enthusiastic, … 8 321\ndepressed, blue, dark, de pressive, drear y, 11 288\nanger, an gry, choleric, fur y, outra ged, ra ge, … 7 156\ngrief, heartbreak, mournful, sorrow, sorr y, … 14 112\ndream y 18 5\ncheerful, cheer u p, festive, jolly,jovial, merr y, … 13 76\nbroodin g, contem plative, meditative, reflective, … 8 69\naggression, a ggressive 2 53\nconfident, encoura ging,  encoura gement, o ptimism 5 43\nangst, anxiet y, anxious, jumpy, nervous, an gsty 63 6\nearnest, heartfel t 23 4\ndesire, ho pe, ho peful, mood: ho peful 4 28\npessimism, c ynical, pessimistic, weltschmerz,… 5 27\nexcitement, excitin g, exhilaratin g, thrill, ardor,… 8 20\nTOTAL 135 4,578\nTable 2.  Mood categories and song distributions. \n# of  cate gories 1 2 3 4 5 6\n # of son gs 1,625 788 305 91 17 2\nTable 3.  Distribution of songs with multiple labels.  \nIn a binary classification task, each category needs \nnegative samples as well. To create our negative sample set for a given category, we chose songs that were not \ntagged with any of the terms found within that category \nbut are heavily tagged with many other terms. Since there \nwere plenty of negative samples for each category, we \nrandomly selected songs tagged with at least 15 other \nterms including mood terms in other categories. Hence, some negative samples of one category are positive sam-\nples of another category. In order to make samples of var-\nious categories as diverse as pos sible, we set a constraint \nthat no negative samples were members of more than one \ncategory. Similar to positive samples, all negative sam-\nples have at least 100 words in their unfolded lyric tran-scripts. We balanced equally the positive and negative set \nsizes for each category. Our final dataset comprised 5,585 \nunique songs.  3. EXPERIMENTS \n3.1 Evaluation Measures and Classifiers \nThis study uses classification accuracy as the perfor-\nmance measure. For each category, accuracy was aver-\naged over a 10-fold cross validation. For each feature set, \nthe accuracies across categories were averaged in a macro manner, giving equal importance to all categories regard-\nless of the size of the categories. To determine if perfor-\nmances differed significantly, we chose the non-parametric Friedman’s ANOVA test because the accura-\ncy data are rarely no rmally distributed.  \nSupport Vector Machines (SVM) were chosen as our \nclassifier because of their strong performances in text ca-\ntegorization and MIR tasks. We used the LIBSVM [15] implementation of SVM and chose a linear kernel as trial \nruns with polynomial kernels did not yield better results. \nParameters were tuned using the grid search tool in LIBSVM, and the default parameters performed best for \nmost cases. Thus, the default parameters were used for all \nthe experiments.   \n3.2 Lyric Preprocessing \nLyric text has unique structures and characteristics requir-\ning special preprocessing techniques. First, most lyrics consist of such sections as intro ,interlude ,verse ,pre-\nchorus ,chorus  and outro , many with annotations on these \nsegments. Second, repe titions of words and sections are \nextremely common. However, very few available lyric \ntexts were found as verbatim transcripts. Instead, repeti-\ntions were annotated as instructions like [repeat chorus \n2x],(x5), etc. Third, many lyrics contain notes about the \nsong (e.g., “written by” ), instrumentation (e.g., “(SOLO \nPIANO),”  and/or the performing artists. In building a \npreprocessing program that  took these characteristics into \nconsideration, we manually iden tified about 50 repetition \npatterns and 25 annotation patterns. The program con-\nverted repetition instructions into the actual repeated \nsegments for the indicated number of times while recog-nizing and removing other annotations. \n3.3 Lyrics Features \nLyrics are a very rich resourc e and many types of textual \nfeatures can be extracted from them. This work compares \nsome of the feature types most commonly used in related \ntext classification tasks. \n3.3.1 Bag-of-Words (BOW) \nBag-of-words (BOW) are collections of unordered words. \nEach word is assigned a value that can represent, among others, the frequency of the wo rd, tf-idf weight, norma-\nlized frequency or a Boolean value indicating presence or \nabsence. Among these variations, tf-idf weighting is the most widely used in text analysis and MIR, but some stu-\ndies in text sentiment analysis also reported other repre-\nsentations outperformed tf-idf weighting [16]. These four \nrepresentations were comp ared in our experiments. \nSelecting the set of words to comprise the BOW set is \nan important consideration. Stemming is a process of \nmerging words with the same  morphological roots, and \n413Poster Session 3\nhas shown mixed effects in text  classification. Thus, we \nexperimented with both options. We used the Snowball \nstemmer5 supplemented with irregular nouns and verbs6\nas this stemmer cannot handle irregular words. Function \nwords (see below) were remove d for both the stemming \nand not stemming cases. \n3.3.2 Part-of-Speech (POS) \nPart-of-Speech (POS) is a popular feature type in text \nsentiment analysis [17] and text style analysis [18]. Other MIR studies on lyrics have also used POS features [6,19]. \nWe used the Stanford POS tagger\n7 which tags each word \nwith one of 36 unique POS tags. \n3.3.3 Function Words \nFunction words (e.g. the,a, etc.) carry little meaning. \nHowever, function words have been shown to be effec-\ntive in text style analysis [18]. To evaluate the usefulness \nof function words in mood classification, the same list of 435 function words found in [18] were used as an inde-\npendent feature set. \n3.4 Audio Processing and Features \nStudies in other MIR tasks have generally found lyrics \nalone are not as informative as audio [6,7]. To find out \nwhether this is true in musi c mood classification, our best \nperforming lyrics feature set was compared to Marsyas\n8,\nthe best performing audio system evaluated in the \nMIREX 2007 AMC task. Mar syas uses 63 spectral fea-\ntures: means and variances of Spectral Centroid, Rolloff, \nFlux, Mel-Frequency Cepstral Coefficients, etc. It also \nuses LIBSVM with a linear kernel for classification. Every audio track in th e dataset was converted to \n44.1KHz stereo .wav files and fed into Marsyas. The ex-\ntracted spectral features were subsequently processed by \nSVM classifiers.  \n3.5 Hybrid Features and Feature Selection \nPrevious MIR studies suggest that combining lyric and \naudio features improves classification performance. Thus, \nwe concatenated our best perform ing lyrics features and \nthe spectral features to see whether and how much the \nhybrid features could improve classification accuracies.  \nIn text categorization w ith BOW features, the dimen-\nsionality of document vectors is usually high. Thus, fea-\nture selection is often used for the sake of good generali-\nzability and efficient computation.  In this study, we com-\npared three methods in selecting the most salient lyric \nfeatures: \n1. Select features with high F-scores. F-score measures \nthe discrimination power of  a feature between two sets \n                                                          \n5http://snowball.tartarus.org/\n6 The irregular verb list was obtained from \nhttp://www.englishpage.com/irregularverbs/irregularverbs.html, and the \nirregular noun list was obtained from http://www.esldesk.com/esl-\nquizzes/irregular-nouns/irregular-nouns.htm\n7http://nlp.stanford.edu/software/tagger.shtml \n8http://www.music-ir.org/mirex/2007/abs/AI_CC_GC_MC_AS_\ntzanetakis.pdf[20]. The higher a feature’s F-score is, the more likely it \nis to be discriminative. F-score is a generic feature reduc-\ntion technique independent of  classification task and me-\nthod. \n2. Select features using lan guage model differences \n(LMD) proposed in [9], where th e top 100 terms with \nlargest LMD were combined with audio features and \nshowed improved classification accuracies. We wish to \nfind out if this method works in this study with more cat-egories.\n3. Select features based on the SVM itself. A trained \ndecision function in a linear SVM contains weights for \neach feature indicating the relevance of the feature to the \nclassifier. [16] has shown that trimming off features with lower weights improved SVM performance in literature \nsentimentalism classification. This study investigates if it \nworks for music mood classification. \n4. RESULTS\n4.1 Best Lyrics Features \nTable 4 shows the average accuracies across all 18 cate-\ngories for the considered lyrics features and representa-\ntions.  \nRepresentation Boolean term fre-\nquency (tf) norma-\nlized tf tf-idf \nweighting\nBOW-Stemming 0.5748 0.5819 0.5796 0.6043\nBOW-Not Stemming 0.5817 0.5829 0.5840 0.5923\nPOS 0.5277 0.5768 0.5691 0.5571\nFunction Words 0.5653 0.5733 0.5692 0.5723\nTable 4.  Average accuracies for lyric features. \nThe best text feature type is BOW with stemming and \ntf-idf weighting (BSTI). The difference between stem-ming options is not significant at p < 0.05 . The four re-\npresentations of BOW features do not differ significantly \nin average performances.        For POS features, the Boo lean representation is not as \ngood as others. This is not unexpected because presuma-\nbly, most lyrics would cont ain most POS types. In gener-\nal, POS features and function words are not as good as \nBOW features. This confirms the heuristic that content \nwords are more useful for mood classification. \n4.2 Combining Audio and All Text Features  \nThree feature sets were compared: spectral features, \nBSTI, and direct concatenation of both. Their accuracies \nare shown as part of Table 5. Although their difference is \nnot significant (at p < 0.05 ) on average, BSTI was signif-\nicantly better than spectral features in these five catego-\nries: romantic ,grief ,aggression ,angst , and exciting . This \nobservation is different from findings in [9] where lyrics \nfeatures alone did not outperfo rmed audio features in any \ncategory.\n   The accuracies in individual categories are shown in Figure 1 where categories are ordered by decreasing \nnumber of samples.   \n41410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 1 . Accuracies of three systems in all categories. \nAs shown in Figure 1, syst em performances on differ-\nent categories vary greatly, and no feature set performs \nbest for all categories. It appears that spectral features are better for larger sized categories, while lyric features are \nbetter for middle sized categories. Data sparseness may \nbe less an issue for text features because the samples were chosen to have a certain length of lyrics. From a \nsemantic point of view, the categories where spectral fea-\ntures are significantly better than text features (“upbeat”, “happy” and “calm”)  may have typical auditory charac-\nteristics that can be captured by audio spectral features. \nOn the other hand, there may be certain lyrics words that connect well to the semantics  of some categories like \n“grief”, “romantic” and “anger.” Thus, lyrics features \npossibly work better because of this connection. For ex-ample, the following stemmed words are ranked high for \nthese categories by all of the three aforementioned feature \nselection methods: \ngrief: singl, scare, confus, heart, cri, sorry, lone, oooh,… \nromantic: endless, love, promis, ador, whisper, lady,… \nanger: fuck, man, dead, thumb, girl, bitch, kill,… \nIt is also clear from Figure 1 that the performances of \nthe combined feature set closely follow the trend of the \nlyrics-only features. This is pr obably inevitable given the \nfact that there are several orders of magnitude more lyric features than spectral features in the combined set. This \nalso demonstrates the necessity of feature selection.  \nWe note that there is a general trend in terms of aver-\nage accuracy decreasing with smaller sample sizes, some-\ntimes even achieving lower than baseline (50%) perfor-\nmance. These cases also show the highest variance in terms of accuracies across folds. This is a somewhat ex-\npected result as the length s of the feature vectors far out-\nweigh the number of traini ng instances. Therefore, it is \ndifficult to make broad generalizations about these ex-\ntremely sparsely represented mood categories.     \n4.3 Combining Audio and Selected Text Features \nUsing each of the three feature selection methods, we se-\nlected the top nBSTI features and combined them with \nthe 63 spectral features. We first varied n from 63 to 500 \n(63, 100, 200,…, 500) fo r all categories. Since the num-\nber of features varies per category, we also varied n based \non the number of features available in each category, \nfrom 10% to 90%. The resu lts show that the best n varies \nacross the three feature selection methods. Table 5 shows accuracies of the feature sets with the best average per-\nformances among each feature selection method. \nCategory Spec + \nF-score\nn=80%Spec + \nLMD\nn=63Spec + \nSVM    \nn=70%Spec + \nBSTIBSTI Spectral\ncalm 0.6112 0.6664 0.6054 0.6176 0.5674 0.6635\nsad 0.6496 0.6976 0.6573 0.6524 0.6295 0.6796\nhappy 0.5965 0.6147 0.5784 0.5922 0.5455 0.6168\nromantic 0.7014 0.7124 0.7127 0.7104 0.6959 0.6407\nupbeat 0.6232 0.6075 0.6013 0.6107 0.5920 0.6389\ndepressed 0.6318 0.6448 0.6613 0.6475 0.6183 0.5741\nanger 0.6692 0.6827 0.6721 0.6787 0.6754 0.6194\ngrief 0.6477 0.6386 0.6511 0.6511 0.6610 0.5314\ndreamy 0.6396 0.6326 0.6354 0.6083 0.6118 0.5771\ncheerful 0.5661 0.5732 0.5929 0.5866 0.5598 0.5330\nbrooding 0.5583 0.5071 0.5726 0.5440 0.5571 0.5452\naggression 0.6683 0.5667 0.6300 0.6500 0.6400 0.5167\nconfident 0.6417 0.7208 0.5050 0.5100 0.5200 0.5175\nangst 0.4750 0.5875 0.6292 0.6292 0.6125 0.4833\nearnest 0.5667 0.6250 0.5833 0.5708 0.5750 0.5958\ndesire 0.5083 0.4250 0.5417 0.5250 0.5583 0.6417\npessimism 0.6833 0.5333 0.6667 0.6667 0.6583 0.5917\nexciting 0.5750 0.4250 0.5750 0.5000 0.6000 0.3250\nAVERAGE 0.6118 0.6033 0.6151 0.6084 0.6043 0.5717\nTable 5.  Accuracies of feature sets for individual categories. \n(bold font denotes the best for that category, italic indicates \nsignificant difference from spectral features at p <0.05 .) \nThe results show that not all categories can be im-\nproved by combining lyric features with spectral features. \nAudio-only and lyric-only features outperform all com-\nbined feature sets in five of the 18 categor ies. Each of the \ncombined feature sets outperforms lyric and audio fea-\ntures in at most nine categories. This is different from \nfindings in previous studies [8,9] where combined fea-tures were best for all experimented categories.  \nIn particular, the language model difference method \nwith 63 lyric features (Spec + LMD n = 63) shows an in-teresting pattern: it improves accuracies in six of the 12 \ncategories where lyric features outperform spectral fea-\ntures and three of the six categories where spectral fea-tures beat lyric features. This indicates that, with the same \ndimensionality, lyrics and audio have indeed a similar \nimpact on combined features.  \nIn combining lyrics and audi o features, feature selec-\ntion often yields better results because many text features \nare either redundant or noisy. In terms of average accura-\ncies, features selected by SVM models work slightly bet-\nter for SVM classifiers than the other two feature selec-tion methods. However, it is interesting to see that (Spec \n+ LMD n = 63) outperforms lyric and audio features in \nnine categories which are the most among all combined feature sets. It also outperfo rms all others in five mood \ncategories and achieves significan tly better results than \nspectral features in three other mood categories. Similar patterns are observed for the F-score method with 63 lyric \nfeatures. This suggests that in hybrid feature sets, lyric \nfeatures can be and should be aggressively reduced. \n5. CONCLUSIONS AND FUTURE WORK \nThis paper investigates the usefulness of text features in \nmusic mood classification on 18 mood categories derived \nfrom user tags. Compared to Part-of-Speech and function \n415Poster Session 3\nwords, Bag-of-Words are still th e most useful feature \ntype. However, there is no significant difference between \nthe choice of stemming or not stemming, or among the four text representations (e.g.  tf-idf, Boolean, etc) on av-\nerage accuracies across all categories. \nOur comparisons of lyric, audio and combined features \ndiscover patterns at odds with previous studies. In partic-\nular lyric features alone can outperform audio features in \ncategories where samples are more sparse or when se-mantic meanings taken from lyrics tie well to the mood \ncategory. Also, combining lyrics and audio features im-\nproves performances on most, but not all, categories. Ex-\nperiments on three different feature selection methods \ndemonstrated that too many text features are indeed re-dundant or noisy and combining audio with the most sa-\nlient text features may lead to higher accuracies for most \nmood categories.  \nFuture work includes inve stigation of other text fea-\ntures, such as text statistics a nd affective words provided \nby domain lexicons. It would  also be interesting to take a \nclose look at individual categ ories and find out why lyrics \nfeatures do or do not help. Moreover, more sophisticated \nfeature and model combination techniques besides naïve feature vector concatenation are worth investigating.   \n6. ACKNOWLEDGEMENT \nWe thank the Andrew W. Mellon Foundation for their \nfinancial support.  \n7. REFERENCES \n[1] P. N. Juslin and P. Lau kka: “Expression, Perception, \nand Induction of Musical Emotions: A Review and \nA Questionnaire Study of Everyday Listening,” \nJournal of New Music Research , Vol. 33, No. 3, pp. \n217-238, 2004. \n[2] M. Mandel, G. Poliner, and D. Ellis: “Support Vec-\ntor Machine Active Learn ing for Music Retrieval,” \nMultimedia Systems , Vol. 12, No. 1, pp. 3-13, 2006. \n[3] X. Hu and J. S. Downie: “Exploring Mood Metada-\nta: Relationships with Genre, Artist and Usage Me-tadata,” Proceedings of the International Conference \non Music Info rmation Retrieval , 2007. \n[ 4 ] X .  H u ,  J .  S .  D o w n i e ,  C .  L a u r i e r ,  M .  B a y ,  a n d  A .  \nEhmann: “The 2007 MIREX Audio Music Classifi-\ncation Task: Lessons Learned,” Proceedings of the \nInternational Conference on Music Information Re-\ntrieval , 2008. \n[5] J-J. Aucouturier and F. Pachet: “Improving Timbre \nSimilarity: How High Is the Sky?” Journal of Nega-\ntive. Results in Speech and Audio Sciences,  Vol.1, \nNo.1, 2004. \n[6] T. Li and M. Ogihara: “Semi-Supervised Learning \nfrom Different Information Sources,” Knowledge \nand Information Systems , Vol.7, No.3, pp.289-309, \n2004 [7] R. Neumayer and A. Ra uber: “Integration of Text \nand Audio Features for Genre  Classification in Mu-\nsic Information Retrieval,” Proceedings of the Euro-\npean Conference on Information Retrieval, pp.724 – \n727, 2007. \n[8] D. Yang, and W. Lee: “Disambiguating music Emo-\ntion Using Software Agents,” In Proceedings of the \n5th International Conference on Music Information \nRetrieval (ISMIR'04), 2004.  \n[9] C. Laurier, J. Grivolla and P. Herrera: “Multimodal \nMusic Mood Classification Using Audio and \nLyrics,” Proceedings of the International \nConference on Machine Learning and Applications, \n2008. \n[10] Y.-H. Yang, Y.-C. Lin, H.-T. Cheng, I.-B. Liao, Y.-\nC. Ho, and H. H. Chen: “Toward multi-modal music \nemotion classification”, Proce edings of  Pacific Rim \nConference on Multimedia (PCM’08), 2008. \n[11] X. Hu, M. Bay, and J. S. Downie: “Creating a Sim-\nplified Music Mood Classification Groundtruth Set,” \nProceedings of the 8th International Conference on \nMusic Information Retrieval, 2007. \n[12] D. Ellis, A. Berenzweig, and B. Whitman: “The \nUSPOP2002  Pop Music Data Set,” Retrieved from \nhttp://labrosa.ee.columbi a.edu/projects/musicsim/us\npop2002.html , 2003. \n[13] Stone, P. J. General Inquirer: a Computer Approach \nto Content Analysis.  Cambridge: M.I.T. Press, 1966. \n[14] C. Strapparava and A. Valitutti: “WordNet-Affect: \nan Affective Extension of WordNet,” Proceedings \nof the International Conference on Language Re-\nsources and Evaluation,  pp. 1083-1086, 2004. \n[15] C. Chang and C. Lin: LIBSVM: a library for support \nvector machines , 2001. Software available at \nhttp://www.csie.ntu.edu.t w/~cjlin/libsvm\n[16] B. Yu: “An Evaluation of Text Classification \nMethods for Literary Study,” Literary and Linguistic \nComputing  Vol. 23, No. 3, pp. 327-343, 2008. \n[17] B. Pang and L. Lee: “Opi nion Mining and Sentiment \nAnalysis,” Foundations and Trends in Information \nRetrieval,  Vol.2 No.1-2, pp. 1–135, 2008 \n[18] S. Argamon, M. Saric and S.  S. Stein: “Style Mining \nof Electronic Messages for Multiple Authorship Discrimination: First Results,” Proceedings of the \nACM International Conference on Knowledge Dis-\ncovery and Data Mining, pp. 475-480, 2003. \n[19] R. Mayer, R. Neumayer, and A. Rauber: “Rhyme \nand Style Features for Musical Genre Categorisation \nby Song Lyrics,” Proceedings of the International \nConference on Music Information Retrieval, 2008. \n[20] Y.-W. Chen and C.-J. Lin: “Combining SVMs with \nVarious Feature Selection Strategies,” In Feature \nExtraction, Foundations and Applications , Springer, \n2006. \n416"
    },
    {
        "title": "A Probabilistic Topic Model for Unsupervised Learning of Musical Key-Profiles.",
        "author": [
            "Diane Hu",
            "Lawrence K. Saul"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415160",
        "url": "https://doi.org/10.5281/zenodo.1415160",
        "ee": "https://zenodo.org/records/1415160/files/HuS09.pdf",
        "abstract": "We describe a probabilistic model for learning musical keyprofiles from symbolic files of polyphonic, classical music. Our model is based on Latent Dirichlet Allocation (LDA), a statistical approach for discovering hidden topics in large corpora of text. In our adaptation of LDA, symbolic music files play the role of text documents, groups of musical notes play the role of words, and musical keyprofiles play the role of topics. The topics are discovered as significant, recurring distributions over twelve neutral pitch-classes. Though discovered automatically, these distributions closely resemble the traditional key-profiles used to indicate the stability and importance of neutral pitchclasses in the major and minor keys of western music. Unlike earlier approaches based on human judgement, our model learns key-profiles in an unsupervised manner, inferring them automatically from a large musical corpus that contains no key annotations. We show how these learned key-profiles can be used to determine the key of a musical piece and track its harmonic modulations. We also show how the model’s inferences can be used to compare musical pieces based on their harmonic structure.",
        "zenodo_id": 1415160,
        "dblp_key": "conf/ismir/HuS09",
        "keywords": [
            "probabilistic",
            "model",
            "learning",
            "musical",
            "keyprofiles",
            "symbolic",
            "files",
            "polyphonic",
            "classical",
            "music"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nA PROBABILISTIC TOPIC MODEL FOR\nUNSUPERVISED LEARNING OF MUSICAL KEY-PROFILES\nDiane J. Hu and Lawrence K. Saul\nDepartment of Computer Science and Engineering\nUniversity of California, San Diego\n{dhu,saul }@cs.ucsd.edu\nABSTRACT\nWe describe a probabilistic model for learning musical key-\nproﬁles from symbolic ﬁles of polyphonic, classical mu-\nsic. Our model is based on Latent Dirichlet Allocation\n(LDA), a statistical approach for discovering hidden topics\nin large corpora of text. In our adaptation of LDA, sym-\nbolic music ﬁles play the role of text documents, groups\nof musical notes play the role of words, and musical key-\nproﬁles play the role of topics. The topics are discovered\nas signiﬁcant, recurring distributions over twelve neutral\npitch-classes. Though discovered automatically, these dis-\ntributions closely resemble the traditional key-proﬁles used\nto indicate the stability and importance of neutral pitch-\nclasses in the major and minor keys of western music. Un-\nlike earlier approaches based on human judgement, our\nmodel learns key-proﬁles in an unsupervised manner, in-\nferring them automatically from a large musical corpus that\ncontains no key annotations. We show how these learned\nkey-proﬁles can be used to determine the key of a musical\npiece and track its harmonic modulations. We also show\nhow the model’s inferences can be used to compare musi-\ncal pieces based on their harmonic structure.\n1. INTRODUCTION\nMusical composition can be studied as both an artistic and\ntheoretical endeavor. Though music can express a vast\nrange of human emotions, ideas, and stories, composers\ngenerally work within a theoretical framework that is highly\nstructured and organized. In western tonal music, two im-\nportant concepts in this framework are the keyand the tonic .\nThe key of a musical piece identiﬁes the principal set of\npitches that the composer uses to build its melodies and\nharmonies. The key also deﬁnes the tonic, or the most sta-\nble pitch, and its relationship to all of the other pitches in\nthe key’s pitch set. Though each musical piece is charac-\nterized by one overall key, the key can be shifted within a\npiece by a compositional technique known as modulation .\nNotwithstanding the inﬁnite number of variations possible\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.\nFigure 1 . C major (left) and C minor (right) key-\nproﬁles proposed by Krumhansl-Kessler (KK), used in the\nKrumhansl-Schmukler (KS) key-ﬁnding algorithm.\nin music, most pieces can be analyzed in these terms.\nMusical pieces are most commonly studied by analyz-\ning their melodies and harmonies. In any such analysis,\nthe ﬁrst step is to determine the key. While the key is in\nprinciple determined by elements of music theory, individ-\nual pieces and passages can exhibit complex variations on\nthese elements. In practice, considerable expertise is re-\nquired to resolve ambiguities.\nMany researchers have proposed rule-based systems for\nautomatic key-ﬁnding in symbolic music [2,10,12]. In par-\nticular, Krumhansl and Schmuckler (KS) [8] introduced a\nmodel based on “key-proﬁles”. A key-proﬁle is a twelve-\ndimensional vector in which each element indicates the\nstability of a neutral pitch-class relative to the given key.\nThere are 24 key-proﬁles in total, one for each major and\nminor key. Using these key proﬁles, KS proposed a sim-\nple method to determine the key of a musical piece or\nshorter passages within a piece: ﬁrst, accumulate a twelve-\ndimensional vector whose elements store the total duration\nof each pitch-class in a song; second, compute the key-\nproﬁle that has the highest correlation with this vector. The\nKS model uses key-proﬁles derived from probe tone stud-\nies conducted by Krumhansl and Kessler (KK) [9]. Fig-\nure 1 shows the KK key proﬁles for C major and C minor;\nproﬁles for other keys are obtained by transposition. In re-\ncent work [14, 15], these key-proﬁles have been modiﬁed\nto achieve better performance in automatic key-ﬁnding.\nIn this paper, we show how to learn musical key-proﬁles\nautomatically from the statistics of large music collections.\nUnlike previous studies, we take a purely data-driven ap-\nproach that does not depend on extensive prior knowledge\nof music or supervision by domain experts. Based on a\nmodel of unsupervised learning, our approach bypasses the\n441Poster Session 3\nneed for manually key-annotated musical pieces, a pro-\ncess that is both expensive and prone to error. As an ad-\nditional beneﬁt, it can also discover correlations in the data\nof which the designers of rule-based approaches are un-\naware. Since we do not rely on prior knowledge, our model\ncan also be applied in a straightforward way to other, non-\nwestern genres of music with different tonal systems.\nOur approach is based on Latent Dirichlet Allocation\n(LDA) [1], a popular probabilistic model for discovering\nlatent semantic topics in large collections of text docu-\nments. In LDA, each document is described as a mixture\nof topics, and each topic is characterized by its own par-\nticular distribution over words. LDA for text is based on\nthe premise that documents about similar topics contain\nsimilar words. Beyond document modeling, LDA has also\nbeen adapted to settings such as image segmentation [5],\npart-of-speech tagging [6], and collaborative ﬁltering [11].\nOur variant of LDA for unsupervised learning of key-\nproﬁles is based on the premise that musical pieces in the\nsame key use similar sets of pitches. Roughly speaking,\nour model treats each song as a “document” and the notes\nin each beat or half-measure as a “word”. The goal of\nlearning is to infer harmonic “topics” from the sets of pitches\nthat commonly co-occur in musical pieces. These har-\nmonic topics, which we interpret as key-proﬁles, are ex-\npressed as distributions over the twelve neutral pitch-classes.\nWe show how to use these key-proﬁles for automatic\nkey-ﬁnding and similarity ranking of musical pieces. We\nnote, however, that our use of key-proﬁles differs from that\nof the KS model. For key-ﬁnding, the KS model con-\nsists of two steps: 1) derive key-proﬁles and 2) predict\nkeys using key-proﬁles. In our model, these steps are nat-\nurally integrated by the Expectation-Maximization (EM)\nalgorithm [3]. We do not need further heuristics to make\nkey-ﬁnding predictions from our key-proﬁles as the EM\nalgorithm yields the former along with the latter.\n2. MODEL\nThis section describes our probabilistic topic model, ﬁrst\ndeveloping the form of its joint distribution, then sketching\nout the problems of inference and parameter estimation.\nWe use the following terminology and notation throughout\nthe rest of the paper:\n1. A note u∈ {A,A/sharp,B, . . . , G/sharp}is the most basic\nunit of data. It is an element from the set of neu-\ntral pitch-classes. For easy reference, we map these\npitch-classes to integer note values 0 through 11. We\nrefer to V=12 as the vocabulary size of our model.\n2. A segment is a basic unit of time in a song (e.g.,\na measure). We denote the notes in the nth seg-\nment by un={un1, . . . , u nL}, where un/lscriptis the /lscriptth\nnote in the segment. Discarding the ordering of the\nnotes, we can also describe each segment simply by\nthe number of time each note occurs. We use xnto\ndenote the V-dimensional vector whose jth element\nxj\nncounts the number of times that the jth note ap-\npears in the nth segment.3. A song sis a sequence of notes in Nsegments:\ns={u1, . . . , uN}. Discarding the ordering of notes\nwithin segments, we can also describe a song by the\nsequence of count vectors X= (x1, . . . , x N).\n4. A music corpus is a collection of Msongs denoted\nbyS={s1, . . . , s M}.\n5. A topic zis a probability distribution over the vo-\ncabulary of V= 12 pitch-classes. Topics model\nparticular groups of notes that frequently occur to-\ngether within individual segments. In practice, these\ngroupings should contain the principle set of pitches\nfor a particular musical key. Thus, we interpret each\ntopic’s distribution over twelve pitch-classes as the\nkey-proﬁle for a musical key. We imagine that each\nsegment in a song has its own topic (or key), and we\nusez= (z1, z2, . . . , z N)to denote the sequence of\ntopics across all segments. In western tonal music,\nprior knowledge suggests to look for K= 24 topics\ncorresponding to the major and minor scales in each\npitch-class. Section 2.3 describes how we identity\nthe topics with these traditional key-proﬁles.\nWith this terminology, we can describe our probabilistic\nmodel for songs in a musical corpus. Note that we do not\nattempt to model the order of note sequences within a seg-\nment or the order of segments within a song. Just as LDA\nfor topic modeling in text treats each document as a “bag\nof words”, our probabilistic model treats each song as a\n“bag of segments” and each segment as a “bag of notes”.\n2.1 Generative process\nOur approach for automatic key-proﬁling in music is based\non the generative model of LDA for discovering topics in\ntext. However, instead of predicting words in documents,\nwe predict notes in songs. Our model imagines a sim-\nple, stochastic procedure in which observed notes and key-\nproﬁles are generated as random variables. In addition,\nwe model the key-proﬁles as latent variables whose values\nmust be inferred by conditioning on observed notes and\nusing Bayes rule.\nWe begin by describing the process for generating a\nsong in the corpus. First, we draw a topic weight vector\nthat determines which topics (or keys) are likely to ap-\npear in the song. The topic weight vector is modeled as\na Dirichlet random variable. Next, for each segment of\nthe song, we sample from the topic weight vector to deter-\nmine the key (e.g., A minor) of that segment. Finally, we\nrepeatedly draw notes from the key-proﬁle until we have\ngenerated all the notes in the segment. More formally, we\ncan describe this generative process as follows:\n1. For each song in the corpus, choose a K-dimensional\ntopic weight vector θfrom the Dirichlet distribution:\np(θ|α) =Γ(/summationtext\niαi)/producttext\niΓ(αi)/productdisplay\niθαi−1. (1)\n44210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nNote that αis aK-dimensional corpus-level param-\neter that determines which topics are likely to co-\noccur in individual songs. The topic weight vector\nsatisﬁes θi≥0and/summationtext\nkθk= 1.\n2. For each segment indexed by n∈ {1, . . . , N }in a\nsong, choose the topic zn∈ {1,2, . . . , K }from the\nmultinomial distribution p(zn=k|θ) =θk.\n3. For each note indexed by /lscript∈ {1, . . . , L }in the nth\nmeasure, choose a pitch-class from the multinomial\ndistribution p(un/lscript=i|zn=j, β) =βij. The βpa-\nrameter is a V×Kmatrix that encodes each topic as\na distribution over V=12 neutral pitch-classes. Sec-\ntion 2.3 describes how we identify these distribu-\ntions as key-proﬁles for particular musical keys.\nThis generative process speciﬁes the joint distribution over\nobserved and latent variables for each song in the corpus.\nIn particular, the joint distribution is given by:\np(θ,z, s|α, β) =p(θ|α)N/productdisplay\nn=1p(zn|θ)Ln/productdisplay\nl=1p(unl|zn, β).(2)\nFigure 2(a) depicts the graphical model for the joint distri-\nbution over all songs in the corpus. As in LDA [1], we use\nplate notation to represent independently, identically dis-\ntributed random variables within the model. Whereas LDA\nfor text describes each document as a “bag of words”, we\nmodel each song as a “bag of segments”, and each segment\nas a “bag of notes”. As a result, the graphical model in Fig-\nure 2(a) contains an additional plate beyond the graphical\nmodel of LDA for text.\n2.2 Inference and learning\nThe model in eq. (2) is fully speciﬁed by the Dirichlet pa-\nrameter αand the musical key-proﬁles β. Suppose that\nthese parameters are known. Then we can use probabilis-\ntic inference to analyze songs in terms of their observed\nnotes. In particular, we can infer the main key-proﬁle for\neach song as a whole, or for individual segments. Infer-\nences are made by computing the posterior distribution\np(θ,z|s, α, β ) =p(θ,z, s|α, β)\np(s|α, β)(3)\nfollowing Bayes rule. The denominator in eq. (3) is the\nmarginal distribution, or likelihood, of a song:\np(s|α, β)=/integraldisplay\np(θ|α)N/productdisplay\nn=1K/summationdisplay\nzn=1p(zn|θ)Ln/productdisplay\nl=1p(unl|zn, β)dθ.\n(4)\nThe problem of learning in our model is to choose the\nparameters αandβthat maximize the log-likelihood of\nall songs in the corpus, L(α, β) =/summationtext\nmlogp(sm|α, β).\nLearning is unsupervised because we require no training\nset with key annotations or labels.\nIn latent variable models such as ours, the simplest ap-\nproach to learning is maximum likelihood estimation using\nthe Expectation-Maximization (EM) algorithm [3]. The\nFigure 2 . (a) Graphical representation of our model and\n(b) the variational approximation for the posterior distribu-\ntion in eq. (3). See Appendix A for details.\nEM algorithm iteratively updates parameters by comput-\ning expected values of the latent variables under the pos-\nterior distribution in eq. (3). In our case, the algorithm\niteratively alternates between an E-step, which represents\neach song in the corpus as a random mixture of 24 key-\nproﬁles, and an M-step, which re-estimates the weights of\nthe pitch classes for each key-proﬁle. Unfortunately, these\nexpected values cannot be analytically computed; there-\nfore, we must resort to a strategy for approximate prob-\nabilistic inference. We have developed a variational ap-\nproximation for our model based on [7] that substitutes a\ntractable distribution for the intractable one in eq. (3). Ap-\npendix A describes the problems of inference and learning\nin this approximation in more detail.\n2.3 Identifying Topics as Keys\nRecall from section 2.1 that the estimated parameter βex-\npresses each topic as a distribution over V= 12 neutral\npitch-classes. While this distribution can itself be viewed\nas a key-proﬁle, an additional assumption is required to\nlearn topics that can be identiﬁed with particular musical\nkeys. Speciﬁcally, we assume that key-proﬁles for differ-\nent keys are related by simple transposition: e.g., the pro-\nﬁle for C /sharpis obtained by transposing the proﬁle for C up by\none half-step. This assumption is the full extent to which\nour approach incorporates prior knowledge of music.\nThe above assumption adds a simple constraint to our\nlearning procedure: instead of learning V×Kindependent\nelements in the βmatrix, we tie diagonal elements across\ndifferent keys of the same mode (major or minor). En-\nforcing this constraint, we ﬁnd that the topic distributions\nlearned by the EM algorithm (see section 3) can be un-\nambiguously identiﬁed with the K= 24 major and minor\nmodes of classical western music. For example, one topic\ndistribution places its highest seven weights on the pitches\nC, D, E, F, G, A, and B; since these are precisely the notes\nof the C major scale, we can unambiguously identify this\ntopic distribution with the key-proﬁle for C major.\n3. RESULTS\nWe estimated our model from a collection of 235 MIDI\nﬁles compiled from http://www.classicalmusicmidipage.com.\n443Poster Session 3\nFigure 3 . The C major and C minor key-proﬁles learned\nby our model, as encoded by the βmatrix.\nThe collection included works by Bach, Vivaldi, Mozart,\nBeethoven, Chopin, and Rachmaninoff. These composers\nwere chosen to span the baroque through romantic periods\nof western, classical music.\nWe experimented with different segment lengths and\ndifferent ways of compiling note counts. Though mea-\nsures deﬁne natural segments for music, we also exper-\nimented with half-measures and quarter-beats. All these\nchoices led to similar musical key-proﬁles. We also exper-\nimented with two ways of compiling note counts within\nsegments. The ﬁrst method sets the counts proportional to\nthe cumulative duration of notes across the segment; the\nsecond method sets the counts proportional to the number\nof distinct times each note is struck. We found that the\nsecond method worked best for key-ﬁnding, and we report\nresults for this method below.\n3.1 Learning Key-Proﬁles\nRecall that each column of the estimated βmatrix encodes\na musical key as a distribution over V= 12 neutral pitch-\nclasses. Fig. 3 shows the two columns that we identiﬁed\nas belonging to the keys of C major and C minor. These\nkey-proﬁles have the same general shape as those of KK,\nthough the weights for each pitch-class are not directly\ncomparable. (In our model, these weights denote actual\nprobabilities.) Note that in both major and minor modes,\nthe largest weight occurs on the tonic (C), while the second\nand third largest weights occur on the remaining degrees of\nthe triad (G, E for C major; G, E /flatfor C minor). Our key-\nproﬁles differ only in the relatively larger weight given to\nthe minor 7th (B /flat) of C major and major 7th (B) of C mi-\nnor. Otherwise, the remaining degrees of the diatonic scale\n(D, F, A for C major; D, F, A /flatfor C minor) are given larger\nweights than the remaining chromatics. Proﬁles for other\nkeys can be found by transposing.\n3.2 Symbolic Key-Finding\nFrom the posterior distribution in eq. (3), we can infer hid-\nden variables θandzthat identify dominant keys in whole\nsongs or segments within a song. In particular, we can\nidentify the overall key of a song from the largest weight\nof the topic vector θthat maximizes eq. (3). Likewise, we\ncan identify the key of particular segments from the most\nprobable values of the topic latent variables zn.\nWe ﬁrst show results at the song-level, using our model\nto determine the overall key of the 235 musical pieces in\nC minorEb MajorF minorC minorF minorC minorEb MajorAb MajorC minorF minorC minorEb MajorG minorC minorG MajorC minorEb MajorF minorC minorBb MajorAb MajorEb MajorC minorAb MajorC minorEb MajorAb MajorAb MajorEb MajorF minorC minorEb MajorAb MajorEb MajorC minorAb MajorEb MajorC minorAb MajorEb MajorC minorF minorEb MajorC minorEb MajorAb MajorC minorEb MajorC minorAb MajorEb MajorC minorAb MajorEb MajorC minorF minorEb MajorC minorAb MajorEb MajorC minorF minorEb MajorAb MajorC minorEb MajorC minor1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12Figure 4 . Key judgments for the ﬁrst 12 measures of\nBach’s Prelude in C minor, WTC-II. Annotations for each\nmeasure show the top three keys (and relative strengths)\nchosen for each measure. The top set of three annotations\nare judgments from our LDA-based model; the bottom set\nof three are from human expert judgments [8].\n44410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSong Length All 20 beats 8 beats 4 beats\nLDA 86% 77% 74% 67%\nKS 80% 71% 67% 66%\nTable 1 . Key-ﬁnding accuracy of our LDA model and the\nKS model on 235 classical music pieces. Song length in-\ndicates how much of each piece was included for analysis.\nFigure 5 . Songs represented as distributions over key-\nproﬁles. The ﬁrst set of bars shows keys used in the query\nsong; the remaining sets of bars show keys used in the three\nsongs of the corpus judged to be most similar. Note how\nall songs modulate between the keys of E /flatM, A /flatM, C m,\nand F m.\nour corpus. We tested our model against a publicly avail-\nable implementation of the KS model [4] that uses normal-\nized KK key-proﬁles and weighted note durations. Table 1\ncompares the results when various lengths of each piece\nare included for analysis. In this experiment, we found\nthat our model performed better across all song lengths.\nWe also compared our model to three other publicly\navailable key-ﬁnding algorithms [13]. We were only able\nto run these algorithms on a subset of 107 pieces in our\ncorpus, so for these comparisons we only report results\non this subset. These other algorithms used key-proﬁles\nfrom another implementation of the KS model [8] and from\nempirical analyses of key-annotated music [14, 15]. Ana-\nlyzing whole songs, these other algorithms achieved ac-\ncuracies between 62%–67%. Interestingly, though these\nmodels obtained their key-proﬁles using rule-based or su-\npervised methods, our unsupervised model yielded signiﬁ-\ncantly better results, identifying the correct key for 79% of\nthe songs in this subset of the corpus.\nNext, we show results from our model at the segment\nlevel. Fig. 4 shows how our model analyzes the ﬁrst twelve\nmeasures of Bach’s Prelude in C minor from Book II of the\nWell-Tempered Clavier (WTC-II). Results are compared to\nannotations by a music theory expert [8]. We see that the\ntop choice of key from our model differs from the expert\njudgment in only two measures (5 and 6).\n3.3 Measuring Harmonic Similarity\nTo track key modulations within a piece, we examine its\nK= 24 topic weights. These weights indicate the propor-tion of time that the song spends in each key. They also\nprovide a low-dimensional description of each song’s har-\nmonic structure. We used a symmetrized Kullback-Leibler\n(KL) divergence to compute a measure of dissimilarity be-\ntween songs based on their topic weights. Fig. 5 shows\nseveral songs as distributions over key-proﬁles. (Note that\nprevious graphs showed key-proﬁles as distributions over\npitches.) The ﬁrst set of bars show the topic weights for\nthe same Bach Prelude analyzed in the previous section;\nthe remaining sets of bars show the topic weights for the\nthree songs in the corpus judged to be most similar (as\nmeasured by the symmetrized KL divergence). From the\ntopic weight vectors, we see that all songs modulate pri-\nmarily between the keys of E /flatM, A /flatM, C m, and F m.\n4. CONCLUSION\nIn this paper, we have described a probabilistic model for\nthe unsupervised learning of musical key-proﬁles. Un-\nlike previous work, our approach does not require key-\nannotated music or make use of expert domain knowledge.\nExtending LDA from text to music, our model discovers la-\ntent topics that can be readily identiﬁed as the K=24 pri-\nmary key-proﬁles of western classical music. Our model\ncan also be used to analyze songs in interesting ways: to\ndetermine the overall key, to track harmonic modulations,\nand to provide a low-dimensional descriptor for similarity-\nbased ranking. Finally, though the learning in our model is\nunsupervised, experimental results show that it works very\nwell compared to existing methods.\n5. REFERENCES\n[1] D. M. Blei, A. Y . Ng, M.I. Jordan: “Latent Dirich-\nlet allocation,” Journal of Machine Learning Research ,\n3:993-1022, 2003.\n[2] E. Chew: “The Spiral Array: An Algorithm for Deter-\nmining Key Boundaries,” Proc. of the Second Int. Conf.\non Music and Artiﬁcial Intelligence , 18-31, 2002.\n[3] A. Dempster, N. Laird, D. Rubin: “Maximum like-\nlihood from incomplete data via the EM algorithm,”\nJournal of the Royal Statistical Society, Series B ,\n39(1):1-38, 1977.\n[4] T. Eerola, P. Toiviainen: “MIDI Toolbox: MAT-\nLAB Tools for Music Research,” University\nof Jyv ¨askyl ¨a, Jyv ¨askyl ¨a, Finland , Available:\nhttp://www.jyu.ﬁ/musica/miditoolbox/, 2004.\n[5] L. Fei-Fei, P. Perona: “A Bayesian hierarchical model\nfor learning natural scene categories,” CVPR , 524-531,\n2005.\n[6] T. Grifﬁths, M. Steyvers, D. Blei, J. Tenenbaum: “In-\ntegrating topics and syntax,” In L. Saul, Y . Weiss, and\nL. Bottou, editors, NIPS , 537-544, 2005.\n[7] M. I. Jordan, Z. Ghahramani, T. Jaakkola, L. Saul: “In-\ntroduction to variational methods for graphical mod-\nels,” Machine Learning , 37:183-233, 1999.\n445Poster Session 3\n[8] C. Krumhansl: Cognitive Foundations of Musical\nPitch , Oxford University Press, Oxford, 1990.\n[9] C. Krumhansl, E. J. Kessler: “Tracing the dynamic\nchanges in perceived tonal organization in a spatial\nrepresentation of musical keys,” Psychological Review ,\n89:334-68, 1982.\n[10] H. C. Longuet-Higgins, M. J. Steedman: “On interpret-\ning Bach,” Machine Intelligence , 6:221-41, 1971.\n[11] B. Marlin: “Modeling user rating proﬁles for col-\nlaborative ﬁltering,” In S. Thrun and L. Saul and B.\nSch¨olkopf, editors, NIPS , 2003.\n[12] D. Rizo: “Tree model of symbolic music for tonality\nguessing,” Proc. of the Int. Conf. on Artiﬁcial Intelli-\ngence and Applications , 299-304, 2006.\n[13] D. Sleator, D. Temperley: “The Melisma Music An-\nalyzer,” Available: http://www.link.cs.cmu.edu/music-\nanalysis/, 2001.\n[14] D. Temperley: The Cognition of Basic Musical Struc-\nture, MIT Press, 2001.\n[15] D. Temperley: “A Bayesian approach to key-ﬁnding,”\nLecture Notes in Computer Science , 2445:195-206,\n2002.\nA. VARIATIONAL APPROXIMATION\nThis appendix describes our variational approximation for\ninference and learning mentioned in section 2. It is similar\nto the approximation originally developed for LDA [1].\nA.1 Variational Inference\nThe variational approximation for our model substitutes a\ntractable distribution for the intractable posterior distribu-\ntion that appears in eq. (3). At a high level, the approxima-\ntion consists of two steps. First, we constrain the tractable\ndistribution to belong to a parameterized family of distri-\nbutions whose statistics are easy to compute. Next, we\nattempt to select the particular member of this family that\nbest approximates the true posterior distribution.\nFigure 2(b) illustrates the graphical model for the ap-\nproximating family of tractable distributions. The tractable\nmodel q(θ,z|γ, φ)drops edges that make the original model\nintractable. It has the simple, factorial form:\nq(θ,z|γ, φ) =q(θ|γ)N/productdisplay\nn=1q(zn|φn) (5)\nWe assume that the distribution q(θ|γ)is Dirichlet with\nvariational parameter γ, while the distributions q(zn|φn)\nare multinomial with variational parameters φn. For each\nsong, we seek a factorial distribution of the form in eq. (5)\nto approximate the true posterior distribution in eq. (3). Or\nmore speciﬁcally, for each song sm, we seek the varia-\ntional parameters γmandφmsuch that q(θ,z|γm, φm)best\nmatches p(θ,z|sm, α, β ).Though it is intractable to compute the statistics of the\ntrue posterior distribution p(θ,z|α, β)in eq. (3), it is pos-\nsible to compute the Kullback-Leibler (KL) divergence\nKL(q, p) =/summationdisplay\nz/integraldisplay\ndθq(θ,z|γ, φ) logq(θ,z|γ, φ)\np(θ,z|s, α, β )(6)\nup to a constant term that does not depend on γandφ.\nNote that the KL divergence measures the quality of the\nvariational approximation. Thus, the best approximation is\nobtained by minimizing the KL divergence in eq. (6) with\nrespect to the variational parameters γandφn. To derive\nupdate rules for these parameters, we simply differentiate\nthe KL divergence and set its partial derivatives equal to\nzero. The update rule for γmis analogous to the one in the\nLDA model for text documents [1]. The update rule for the\nmultinomial parameters φniis given by:\nφni∝V/productdisplay\nj=1βxj\nn\nijexp[Ψ( γi)], (7)\nwhere Ψ(·)denotes the digamma function and xj\nndenotes\nthe count of the jth pitch class in the nth segment of the\nsong. We omit the details of this derivation, but refer the\nreader to the original work on LDA [1] for more detail.\nA.2 Variational Learning\nThe variational approximation in eq. (5) can also be used\nto derive a lower bound on the log-likelihood logp(s|α, β)\nof a song s. Summing these lower bounds over all songs\nin the corpus, we obtain a lower bound /lscript(α, β, γ, φ )on the\ntotal log-likelihood L(α, β) =/summationtext\nmlogp(sm|α, β). Note\nthat the bound /lscript(α, β, γ, φ )≤ L (α, β)depends on the\nmodel parameters αandβas well as the variational pa-\nrameters γandφacross all songs in the corpus.\nThe variational EM algorithm for our model estimates\nthe parameters αandβto maximize this lower bound. It\nalternates between two steps:\n1. (E-step) Fix the current model parameters αandβ,\ncompute variational parameters {γm, φm}for each\nsong smby minimizing the KL divergence in eq. (6).\n2. (M-step) Fix the current variational parameters γand\nφacross all songs from the E-step, maximize the\nlower bound /lscript(α, β, γ, φ )with respect to αandβ.\nThese two steps are repeated until the lower bound on the\nlog likelihood converges to a desired accuracy. The up-\ndates for αandβin the M-step are straightforward to de-\nrive. The update rule for βis given by:\nβij∝M/summationdisplay\nm=1N/summationdisplay\nn=1φi\nmnxj\nmn. (8)\nWhile the count xj\nmnin eq. (8) may be greater than one,\nthis update is otherwise identical to its counterpart in the\nLDA model for text documents. The update rule for αalso\nhas the same form.\n446"
    },
    {
        "title": "Full-Automatic DJ Mixing System with Optimal Tempo Adjustment based on Measurement Function of User Discomfort.",
        "author": [
            "Hiromi Ishizaki",
            "Keiichiro Hoashi",
            "Yasuhiro Takishima"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418231",
        "url": "https://doi.org/10.5281/zenodo.1418231",
        "ee": "https://zenodo.org/records/1418231/files/IshizakiHT09.pdf",
        "abstract": "This paper proposes an automatic DJ mixing method that can automate the processes of real world DJs and describes a prototype for a fully automatic DJ mix-like playing system. Our goal is to achieve a fully automatic DJ mixing system that can preserve overall user comfort level during DJ mixing. In this paper, we assume that the difference between the original and adjusted songs is the main cause of user discomfort in the mixed song. In order to preserve user comfort, we define the measurement function of user discomfort based on the results of a subjective experiment. Furthermore, this paper proposes a unique tempo adjustment technique called “optimal tempo adjustment”, which is robust for any combination of tempi of songs to be mixed. In the subjective experiment, the proposed method obtained higher averages of user ratings on three evaluation items compared to the conventional method. These results indicate that our system is able to preserve user comfort.",
        "zenodo_id": 1418231,
        "dblp_key": "conf/ismir/IshizakiHT09",
        "keywords": [
            "automatic DJ mixing",
            "real world DJs",
            "fully automatic DJ mixing system",
            "preserving user comfort",
            "tempo adjustment technique",
            "subjective experiment",
            "conventional method",
            "evaluation items",
            "robust for any combination",
            "overall user comfort level"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFULL-AUTOMATICDJ MIXINGSYSTEM WITH OPTIMAL TEMPO\nADJUSTMENTBASED ONMEASUREMENT FUNCTIONOF USER\nDISCOMFORT\nHiromiIshizaki\nKDDI R&D Laboratories Inc.\nishizaki@kddilabs.jpKeiichiroHoashi\nKDDI R&D Laboratories Inc.\nhoashi@kddilabs.jpYasuhiroTakishima\nKDDI R&D Laboratories Inc.\ntakisima@kddilabs.jp\nABSTRACT\nThis paper proposes an automatic DJ mixing method that\ncan automate the processes of real world DJs and describes\na prototype for a fully automatic DJ mix-like playing sys-\ntem. Our goal is to achieve a fully automatic DJ mixing\nsystem that can preserve overall user comfort level during\nDJ mixing.\nIn this paper, we assume that the difference between the\noriginal and adjusted songs is the main cause of user dis-\ncomfort in the mixed song. In order to preserve user com-\nfort, we deﬁne the measurement function of user discom-\nfort based on the results of a subjective experiment. Fur-\nthermore, this paper proposes a unique tempo adjustment\ntechnique called “optimal tempo adjustment”, which is ro-\nbust for any combination of tempi of songs to be mixed. In\nthe subjective experiment, the proposed method obtained\nhigher averages of user ratings on three evaluation items\ncompared to the conventional method. These results indi-\ncate that our system is able to preserve user comfort.\n1. INTRODUCTION\nDue to the development of various audio compression meth-\nods, many online music distribution services have provided\nthe opportunity for users to listen to songs from huge mu-\nsic collections. Furthermore, the increasing popularity o f\nportable music players has enabled users to carry around\nthousands of songs. However, the variety of methods for\nthe common user to enjoy listening to the songs in their\ncollection is basically limited to “shufﬂe” play, which sim -\nply plays songs in the collection (and/or playlists) in ran-\ndom order. In order to extract a set of songs that match user\npreferences from large-scaled music collections , there ar e\nmany useful techniques such as [1–3]. These techniques\ncan provide users a set of songs as playlists, from which\nusers select and play songs. In order to provide users new\nexperience, it is important to play the songs in an entertain -\ning way. For instance, Basu proposed a method which can\nblend two songs smoothly to create different aspects of the\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .songs [4].\nIn the real world, DJs (disk jockey), i.e., people who se-\nlect and play music in clubs and discos, are able to main-\ntain the excitement of the audience by continuously play-\ning songs with the utilization of various DJ techniques: se-\nlections of songs, beat adjustment, etc.. One fundamental\nDJ technique is to gradually switch from one song to the\nother, while adjusting the beats of the songs. This tech-\nnique enables the DJ to switch songs smoothly without\ndisturbing the listener. A similar method should be ef-\nfective in providing an entertaining music experience for\ncommon music listeners. However, such music playing\nrequires skilled techniques and/or specialized equipment ,\nwhich are both difﬁcult for casual users to utilize.\nIn this research, we propose an automatic DJ mixing\nmethod that can automate real world DJ processes and de-\nscribe a prototype for a fully automatic system. The ob-\njective of this research is to develop an automatic music\nplaying system that can play a variety of different songs\nconsecutively in an entertaining way without causing the\nusers any discomfort. Speciﬁcally, we deﬁne the measure-\nment function of user discomfort based on the results of a\nsubjective experiment. Furthermore, we propose an opti-\nmal tempo adjustment technique that is robust for any com-\nbinations of the tempi of songs to be mixed.\n2. CONVENTIONAL PLAYING METHOD\nAs mentioned in the previous section, DJs effectively uti-\nlize the cross-fade playing ( CFP ) technique to maintain\nthe entertain level of the music they play. Naive CFP ,\ni.e., cross-fading two songs without any tempo/beat ad-\njustment, is a simple and effective approach in avoiding\nsilence between songs, and can be easily implemented in\nany music playing application. This method is effective in\navoiding silence between songs, which may be distracting\nto listeners who prefer that the music play continuously.\nHowever, especially in situations where the tempi of the\ntwo songs to be cross-faded are signiﬁcantly different (Fig -\nure 1-(a)), naive CFP may result in a negative listening\nexperience, since the beats of the two songs occur asyn-\nchronously. Therefore, it is necessary for DJs to conduct\nCFP while adjusting the tempo and beat of one song to\nthe other. The adjustment of tempo can be done by simple\nsignal expansion (in cases where the song is to be played\nslower than the original) or contraction [5].\n135Poster Session 1\nvolume level\nsong A song Bcause of discomfort: beat\n(a) Cross-fade playing\nsong A\nsong A’tempo adjustment with f\nsong Bincrease of tempo adjustment: beat\n(b) naive DJ mixing\nFigure 1 . Conceptual illustrations of cross-fade playing\nand naive DJ mixing.\n3. PROBLEMS\nHowever, there are two problems in realizing such DJ tech-\nniques automatically.\nOne problem is the degradation in the acoustic qual-\nity of music, which may occur in the tempo adjustment\nprocess, especially in conditions where the tempi of the\ntwo target songs are signiﬁcantly different (Figure 1-(b)) .\nSuch quality degradation may cause discomfort for listen-\ners. Furthermore, the double or half tempo error is com-\nmon for any existing automatic tempo extraction algorithm,\nas mentioned in [9]. Although a highly accurate tempo and\nbeat extraction method is obviously essential for the imple -\nmentation of a fully automatic DJ mix playing system, it is\nunrealistic to expect any system to achieve 100% accurate\nbeat extraction. If the fully automatic DJ mix playing sys-\ntem adjusts the tempo based on tempo extraction results\nwith double/half errors, the resulting factors of tempo ad-\njustment will be two times the actual requirement. It is\nobvious that such excessive tempo adjustment is a cause\nof acoustic quality degradation, and ultimately, discomfo rt\nfor music listeners. Furthermore, in the cases of adjust-\nment of the song/songs that result in double/half tempo\nerrors, strong beats and weak beats are adjusted to each\nother, which causes user discomfort.\nThe other problem is that there is no previous work on\nthe effective measure of tempo adjustment to preserve the\ncomfort level of users. It is not clear that users feel dis-\ncomfort with regard to the degree of tempo adjustment or\nthe manner in which the tempo was adjusted for the songs\nto be mixed. Actually, it is essential to deﬁne some kind\nof measure in order to achieve the fully automatic DJ mix-\ning system. Additionally, it is important to investigate th e\nthreshold and the applicable range of tempo adjustment for\nsongs to be mixed in order to achieve a comfortable DJ\nmixing system.\n4. DEFINITION OFMEASUREMENT FUNCTION\nIn this section, we conducted a subjective experiment to\ndeﬁne the measurement function of user discomfort. The\nobjective of this experiment is to deﬁne the measurementfunction of user discomfort to determine the level of user\ndiscomfort given the tempo adjustment ratio.\nIn this experiment, we assume that the difference be-\ntween the original and adjusted songs is the main cause of\nuser discomfort. We investigate the correlation between\nuser discomfort and tempo adjustment factors with actual\ntempo adjusted songs using time-scaling algorithms. De-\ntails of this experiment are presented as follows.\n4.1 Experimental method\nThe methodology of this experiment, namely, details on\nthe method of generating the sample audio and the subjec-\ntive measure, are explained. In this experiment, we gen-\nerate the actual songs for which the tempo will change.\nSubjects listen to these songs and input the time when they\nfeel discomfort.\nThe experimental data set consists of 18 popular songs\nselected from the RWC music database [11]. For each of\nthe selected songs, tempo changes are applied to the song\nexcerpts. The adjusted tempo is obtained by multiplication\nof the original tempo of song and the factor of tempo ad-\njustment f,f >1means the speedup factor and f <1\nmeans the slowdown factor. The speedup and slowdown\nfactors for tempo changes are set from 1.00to2.00and\n1.00to0.30, respectively. For each experiment, the song\nis played in its original tempo for the ﬁrst 15 seconds. Af-\nter this initial period, the tempo of the song is repetitivel y\nincreased (in the case of speedup) or decreased (in the case\nof slowdown) by a scale of 0.05, for every three seconds,\nuntil the tempo change factor reaches its maximal/minimal\nvalue. This range is decided empirically enough to investi-\ngate the correlation.\nIn the tempo adjustment, we have changed the time scale\nof the songs, while maintaining the original pitch. As tools\nof tempo adjustment, we use the two time-scaling algo-\nrithms: the audio processing library SoundTouch Library1\nand the SOLA [10] time-scaling algorithm. SoundTouch\nis a high quality means to change tempo, SOLA is a low\nquality means. A total of 72 excerpts are generated for this\nexperiment (44.1 kHz, 16-bit, WA V).\nIn this experiment, the 96 subjects are divided into two\ngroups. Each group listens to half of the excerpts (36 ex-\ncerpts per group). In the listening task, the subject is to\nsubmit the time when they feel discomfort to the tempo\nchange of the song. The submission results are accumu-\nlated to analyze the effects of tempo change factors.\n4.2 Results\nTable 1 shows the averages of tempo adjustment factors\nthat subjects feel discomfort to the song associated with\neach time-scaling algorithm. In this table, there are diffe r-\nences between speedup and slowdown factors where the\nsubjects feel discomfort. These results show that the sub-\njects are more sensitive to effect of slowdown as opposed\nto speedup. Furthermore, the averages of tempo adjust-\nment factors for SoundTouch andSOLA are approximately\n1SoundTouch Library: http://www.surina.net/soundtouch/\n13610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n0.5 1 1.5 205101520\nTempo adjustment factor  fRelative frequency (%)  \nSOLA\nSoundTouch\nFigure2 . Histogram of user discomfort to factors of tempo\nadjustment.\nTable 1 . Averages of tempo adjustment factors\nmethod speedup slowdown\nSoundTouch 1.227 0.852\nSOLA 1.226 0.852\nequal to each other. These results indicate that user dis-\ncomfort depends on tempo adjustment factors rather than\nthe method.\nFigure 2 shows the histogram of user discomfort and\nfactors of tempo adjustment with each time-scaling algo-\nrithm. In this ﬁgure, the factors at the peaks of each al-\ngorithm are 1.10(speedup) and 0.90(slowdown). The\npercentages of subjects that feel discomfort inside these\nfactors of each algorithm are 15.42% (SOLA ) and11.31%\n(SoundTouch ). In the area near the original tempo, there\nare differences between the algorithms. SoundTouch is bet-\nter able to preserve the comfort level of subjects under the\ncondition in which the factor satisﬁes 0.90< f < 1.10\nthan SOLA .\n4.3 Deﬁnition from theresult\nIn order to deﬁne the measurement function based on the\nresults in the previous section, we assume that the differ-\nence between the original and adjusted songs is the main\ncause of user discomfort. On the basis of this assump-\ntion and previous results, we deﬁne the level of discomfort\n(Ldc) expressed by the following equation:\nLdc(f) =\n\na(f−1) f >1\n0 f= 1\nb(1/f−1)f <1(1)\nIn Eq.(1), parameters aandbare to be weighted because\nthe level of user discomfort is different between the ad-\njustment from the speedup factor and from the slowdown\nfactor as described in the previous section. Hence we ex-\ntract the weighted parameters aandbasa= 0.765and\nb= 1.000, these are extracted to make the score computed\nby speedup and slowdown factors equal when the factors\nare given as those written in Table 1. These weighted\nparameters are assumed to be effective in preserving the\nusers’ level of comfort in the song-to-song ( StS) transi-\ntion of DJ mixing. For example, Eq.(1) is able to decide\nwhich factor is appropriate (speedup or slowdown) in thedatabaseTempo and Beat Extraction\nMusic Information Retrieval\nOTAC computation\nTempo and Beat Adjustment\nCross-fade processingInput the query song\nOutput the mixed soundPre-processing\nReal-time processing\nFigure 3 . An overview of prototype of fully automatic DJ\nmixing system.\nDJ mixing. Additionally, we extract the stricter and aver-\nage applicable ranges from the factors at the peaks (men-\ntioned in Section 4.2) and the averages shown in Table 1.\nSpeciﬁcally, we extract the 0.90< f < 1.10as the stricter\napplicable range and 0.852< f < 1.227as the average\napplicable range.\n5. SYSTEM\nIn this section, we describe the prototype of the fully auto-\nmatic DJ mixing system, which can solve the problems of\ntempo/beat adjustment, described in Section 3. By apply-\ning the score of the measurement function, which is com-\nputed based on the tempi of the target songs, our system\nis designed to be able to preserve the overall level of user\ncomfort during the transition between songs.\nFig. 3 shows the overview of the prototype for the fully\nautomatic DJ mixing system. This system mainly consists\nof ﬁve processes: tempo and beat extraction, music infor-\nmation retrieval ( MIR), optimal tempo adjustment coefﬁ-\ncients computation, tempo and beat adjustment, and cross-\nfade playing. In this system, we propose a unique tempo\nand beat adjustment method, which is able to deal with\ndouble or half tempo errors in the tempo and beat extrac-\ntion technique: optimal tempo adjustment is able to com-\npute the optimal factors of tempo adjustment to minimize\nthe amount of tempo adjustment by dealing with tempo\noctave relationships. Details of the main processes of the\nsystem are described as follows.\n5.1 Tempo and beat extraction\nIn this section, we describe the method of automating the\nDJ processes: tempo and beat extraction. As concerns the\ntempo and beat extraction process, there are many research\nefforts in tempo and beat extraction techniques, such as\n[6–8]. Although these techniques have the common prob-\nlem of double/half error, there are practical mean to ex-\ntract the tempo and beat automatically. Such methods can\nbe useful to automate the tempo and beat extraction in DJ\nmixing processes. In our proposal, we apply BeatRoot2as\nthe method of extracting the beat in the pre-process to the\ndatabase.\n2http://www.elec.qmul.ac.uk/people/simond/beatroot/\n137Poster Session 1\ntempo adjustment \nto target temposong A \nsong B’song A’\nsong Bt: beat\nminimize the amount of tempo adjustment\nFigure 4 . Conceptual image of dual tempo adjustment\n5.2 Musicinformation retrieval\nIn this section, we describe the method of automating the\nDJ processes of selecting the songs to be mixed. As men-\ntioned in Section 1, there are many research efforts in mu-\nsic information retrieval/recommendation. Although thes e\nare not specialized to DJ mix playing, these have achieved\nhighly accurate retrieval/recommendations. Hence these\nare practical ways of substituting and selecting the song\nmanually. In this system, we apply the content-based MIR\ntechnique [2], which can retrieve songs from the database\nby means of content-based similarity to the users’ query.\n5.3 Proposed DJmixing\n5.3.1 Optimal tempo adjustment coefﬁcient computation\nIn order to automatically generate a smooth StStransition,\nwe propose a unique tempo adjustment technique. Our\nproposal computes the optimal tempo adjustment coefﬁ-\ncients, hereafter described as OTAC , which expresses the\nfactors of tempo adjustment for the songs to be consecu-\ntively played, thus is capable of automatically generating\nsmooth StStransitions for any given combination of songs.\nNamely, two OTAC s are computed and optimized for each\nsong in the combination. As previously mentioned, the\nnaive tempo adjustment approach may result in user dis-\ncomfort, especially under conditions where the tempo of\nsong A(TA) and song B(TB) are signiﬁcantly different,\nwhich causes the tempo adjustment factor to be extremely\nhigh.\nIn order to solve this problem, the proposed method\nconsiders the individual position of beats in the two songs\nto compute the OTAC s, which will hereafter be denoted as\nfopt. Figure 4 shows the conceptual image of proposed DJ\nmixing. We focus on the position of beats in the two songs,\nand it is clear that the beats of the two songs can match the\nsmaller factors of tempo adjustment compared to naive DJ\nmixing. The proposed method computes OTAC s by utiliz-\ning the double/half characteristics to reduce the score for\nuser discomfort.\nThe following describes the computational procedure\nforOTAC s, which expresses the factors of optimal tempo\nadjustment of the two target songs. In this procedure, we\nreduce the amount of tempo adjustment and user discom-\nfort in a StStransition by dual tempo adjustment, for exam-\nple, song Awith a 5% speedup factor and song Bwith a 5%\nslowdown factor, instead of song Awith a 10% speedupsmooth tempo change to smooth tempo change to tgtT\nATBTBPM\ntgtTBTcross-fade range\ntsong A song B\nFigure 5 . Shifts of tempi of target songs in StStransition.\nfactor and song Buntouched. In the following explanation,\nsong Ais deﬁned as the target song to compute OTAC s.\nFirst, a candidate set of adjusted TAis computed using\nthe following Equation:\nT′\nA= 2C×TA (2)\nwhere C={−2,−1,0,1,2}. From the set of T′\nA, we\nselect the result which is closest to TB. This is equivalent\nto determining Copt= argmin( |T′\nA−TB|).\nNext, parameter boptis computed with the following\nEquation:\nbopt= 2Copt×TA (3)\nIn Eq.(3), multiple values of boptcan be computed in cer-\ntain combinations of TAandTB. In such cases, the value\nbopt, which results in a smaller |Copt|, is selected. For ex-\nample, given tempo combination as (TA,TB) = (50 ,75),\npossible solutions of Eq.(3) are bopt= 50,100. In this\ncase,bopt= 50 is selected as the ﬁnal parameter.\nThe target tempo Ttgt, which the adjustment of the tempi\nof songs AandBwill match, is computed with the follow-\ning equation:\nTtgt=(a−b)Tlow+/radicalbig\n(a−b)2T2\nlow+ 4abThighTlow\n2a(4)\nwhere Thigh denotes the tempo of the song with a higher\ntempo, and Tlowdenotes the lower in boptandTB.Ttgtis\ndesigned to divided the score based on Eq.(1) equally be-\ntween the two songs, i.e.,Ttgtis computed in order to sat-\nisfy that the Ldcof speedup and slowdown is equal. Figure\n5 shows the shifts in the tempi of target songs in the transi-\ntion, which is the case where the tempo of song Ais lower\nthan song B. These shifts are optimized for reducing the\nscore of user discomfort based on Eq.(1).\nFinally, the OTAC sfoptA,foptB are computed based on\nbopt.\nfoptA=Ttgt\nbopt, foptB=Ttgt\nTB(5)\nThe proposed method is capable of computing the fac-\ntors of optimal tempo adjustment for any combination of\ntwo songs. For instance, where the tempi of songs Aand\nBare 60 and 120 BPM, the result of the computed OTAC s\nisfoptA=foptB= 1, which is equal to the ideal rate\nfor preserving the overall acoustic quality of the DJ mix\nresult. It is also notable that the proposed method is ca-\npable of applying the DJ mix regardless of the existence\n13810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nof double/half tempo estimation errors, since the effect of\nsuch errors is disregarded during the OTAC computational\nprocedure.\n5.3.2 Beat adjustment and cross-fade playing\nNext, we explain the procedure to generate the StStransi-\ntion of the mixed sound. This procedure is necessary to re-\nduce the discomfortness of the mixed sound, which assume\nto occur when the strong beats of a song are adjusted to the\nweak beat of the other song during the cross-fade range.\nIn this procedure, we utilize the power of the beats in the\ncross-fade sections, to avoid the mismatching of strong and\nweak beats in the two songs to be mixed.\nIn order to generate the StStransition that matches the\nstrong beats precisely, our method computes the score for\nthe cross-correlation of the beats of target songs within th e\nrange of the cross-fade. When the powers of beats within\nthe range of the cross-fade of songs A,Bare described as\nPow AandPow B. The following describes the power of\nn-th beat as Pow A(n)andPow B(n). The score between\nthe songs A,Bis described as Equation (6):\nscore(τ) =/summationtextτ\nk=1(Pow A(NA−k+ 1)Pow B(k))\nτ(6)\nwhere τdenotes the number of beats within the range of\nthe cross-fade and NAdenotes the number of beats of song\nAas the former song in the mixed sound. Speciﬁcally, the\nbeats of song Aare matched to the beats of song Bwhen\nτmax= argmaxτ(score(τ))is satisﬁed. Pow s are com-\nputed by the power located near the beat ( ±50ms). The\npowers of the spectrogram are computed by the FFT of the\naudio signal low-pass ﬁltered (20th order FIR, cutoff freq.\n1500Hz). Finally, cross-fade is applied to the overlapped\nrange based on the highest score computed by τmax.\n6. EXPERIMENT\nIn this section, we will describe the experiment to sub-\njectively evaluate our system and the proposed DJ mixing\nmethod. The objective of this experiment is to evaluate the\neffectiveness of the proposal.\nIn order to conduct this evaluation, two sets of DJ mixed\nsounds are generated; one by naive DJ mixing, and the\nother by the proposed method. The experiment is evalu-\nated in a subjective manner. Namely, subjects of the exper-\niment are to listen to the mixed sounds and provide prefer-\nence ratings for each sample. Details of the experiment are\ndescribed as follows.\n6.1 Data\nExperimental data consist of 1434 songs, which are col-\nlected from Jamendo3, a web site which distributes music\nlicensed by Creative Commons. The source audio used\nfor the experiments is extracted from the songs in the data\n3http://www.jamendo.com/comfort rhythm entertainability12345Average of user ratings\n  \nproposed method\nnaive DJ mixing\nFigure6 . Average of user ratings in proposal and naive DJ\nmixing.\ncollection. The length of each source is 30 seconds in-\ncluding the chorus. Note that, for all source audio, meta-\ninformation, such as the position of each beat, and tempo\n(BPM) are applied by BeatRoot .\n6.2 Experimental method\n6.2.1 DJ mixed sound generation\nThe mixed sound ﬁles are generated by applying one of the\npreviously described methods using ﬁve selected source\naudio extracted by MIR system [2] as the target songs. In\ntotal, six mixed sounds are generated by naive DJ mixing\nand the proposal, respectively. For the methods that utiliz e\ntempo adjustment, we have added interval periods to grad-\nually change the tempi from/to the original to/from the tar-\nget tempo, as shown in Fig.5. This interval period, which\nis ﬁxed as 5 seconds for all mixed songs, is inserted in or-\nder to avoid abrupt changes in tempo, which is obviously\nuncomfortable. The period in which CFP is conducted be-\ngins immediately after the 5 second interval. For tempo\nadjustment, we use the SoundTouch Library .\n6.2.2 Subjects and evaluation measures\nA total of 27 subjects participated in the experiment. Each\nsubject listened to all of the generated DJ mixed sounds\nand were asked to provide subjective ratings in ﬁve ranks\nfor all sounds. In total, 165 ratings were collected on naive\nDJ mixing and the proposed method, respectively. Evalu-\nation measures consist of the following three items: “ com-\nfort”: the level of listener comfort during StStransition (1:\ndiscomfort – 5: comfort), “ rhythm ”: the smoothness of the\nrhythm through the sound (1: bad – 5: good), and “ en-\ntertainability ”: the overall preference rating (1: bad – 5:\ngood).\n6.3 Results\nAverage of user ratings in proposed method and naive DJ\nmixing are shown in Figure 6. It is clear from this ﬁg-\nure that the proposed method was given a higher rating for\nall evaluation items compared to the conventional method,\nproving the overall effectiveness of the proposed method.\nAccording to the result of paired t-test, there are statisti cally-\nsigniﬁcant differences ( p <0.001).\n139Poster Session 1\n0.5 1 1.5 22015105051015202530\nTempo adjustment factor  fRelative frequency (%)\n  \nnaive DJ mixing\nproposed method\nFigure 7 . Histograms of the relative frequency of factors\ninStStransitions of proposal and naive DJ mixing.\nFigure 7 shows the histograms of the relative frequency\nof factors in each of the StStransitions in each mixed sound.\nIn this ﬁgure, stricter and average applicable ranges de-\nscribed in Section 4.2 are plotted as solid and dashed lines.\nIt is clear from this ﬁgure that the proposed method can\nkeep factors near the original tempo compared to naive DJ\nmixing in a transition. The proposed method is able to\ndeal with the difference in tempi between the former and\nlatter songs. Furthermore, it is notable that the proposed\nmethod can almost satisfy the stricter applicable range and\nperfectly satisfy the average applicable range. Speciﬁcal ly,\nthe percentage of factors inside the stricter range of the\nproposed method is 50.00% and inside the average range\nis100.00%.\nFor further analysis, we investigated the averages of\nuser ratings for each mixed sound. There were some cases\nthat although Ldcof the proposed method were lower than\nnaive DJ mixing, the score of user ratings was lower than\nnaive DJ mixing. These cases tended to be adjusted strong\nbeats and weak beats. In this case, user ratings of the pro-\nposed method about the evaluation item RHis lower than\nthat of naive DJ mixing, which is able to adjusted appro-\npriately. Furthermore, the correlation between CFandRH\nhas a strong positive-correlation to each other. These re-\nsults indicate that appropriate beat adjustment is one of th e\nimportant factors. Generation of a smooth StStransition in\nthe aspect of RHis essential to achieving a high quality DJ\nmixing method.\n7. CONCLUSIONS\nIn this paper, we proposed an automatic DJ mixing method\nwith optimal tempo adjustment with a function to measure\nuser discomfort, described a prototype for a fully auto-\nmatic DJ mixing system. The measurement function is de-\nﬁned by a subjective experiment, and our proposed method\nis designed to optimize the score of the function. In or-\nder to generate a smooth song-to-song transition, this pa-\nper proposes an optimal tempo adjustment based on the\ncomputation of optimal tempo adjustment coefﬁcient. Fur-\nthermore, the proposed DJ mixing method is designed to\npreserve user comfort. The proposed DJ mixing is ca-pable of generating a smooth song-to-song transition for\nany given combination of songs that includes double or\nhalf tempo errors. The advantages of the proposed method\nwere proved by comparing the subjective evaluations of\nthe samples generated by the proposed and conventional\nmethods.\nHowever, it is also obvious that tempo is just one of\nmany elements in music that affect user preferences. For\nexample, some combinations of source songs were unac-\nceptable to subjects in the experiments, regardless of the\nDJ mixing method implemented to generate the sample\naudio. Therefore, we plan to further pursue research to de-\nvelop a way to effectively apply the measurement function\nand a fully automatic music playing method, including the\nextraction and utilization of features other than tempo and\nbeat position.\n8. REFERENCES\n[1] S. Pauws and B. Eggen: “PATS: Realization and user\nevaluation of an automatic playlist generator,” Proc. IS-\nMIR 2002 , pp. 222-230, 2002.\n[2] K. Hoashi, et al. : “Personalization of User Proﬁles For\nContent-based Music Retrieval Based on Relevance\nFeedback,” Proc. ACM Multimedia 2003 , pp.110-119,\n2003.\n[3] K. Yoshii, et al. : “ Improving Efﬁciency and Scal-\nability of Model-based Music Recommender System\nBased on Incremental Training,” Proc. of ISMIR ,\npp.89-94, Vienna, Sep. 2007.\n[4] S. Basu: “Mixing with Mozart,” Proc. of ICMC 2004\n[5] A. Inoue, et al. : “Playback and Distribution Meth-\nods for Digital Audio Players” IPSJ SIG Notes 2006(9)\npp.133-138 ( in japanese )\n[6] M. Alonso, et al. : “ Tempo and beat estimation of\nmusical signals,” Proc. ISMIR 2004 , pp.158-163, 2004.\n[7] S. Dixon: “Automatic extraction of tempo and beat\nfrom expressive performances,” J.New Music Res. ,\nV ol.30, No.1, pp.39-58, 2001.\n[8] E. Scheirer: “Tempo and beat analysis of acoustic mu-\nsical signals,” J. Acoust. Soc. Amer. , V ol.103, No.1,\npp.588-601, 1998.\n[9] F. Gouyon, et al. : “An experimental comparison of\naudio tempo induction algorithms,” IEEE Trans. Au-\ndio, Speech, and Lang. Process. , IEEE Transactions\non. Sept., pp.1832-1844, 2006.\n[10] S. Roucos and A. M. Wilgus: “High quality time-scale\nmodiﬁcation for speech,” IEEE ICASSP , pp.493-496,\n1985.\n[11] M. Goto, et al. : “RWC Music Database: Popular,\nClassical, and Jazz Music Databases,” Proc. of ISMIR\n2002 , pp.287-288, October 2002.\n140"
    },
    {
        "title": "Tonal-Atonal Classification of Music Audio Using Diffusion Maps.",
        "author": [
            "Özgür Izmirli"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416936",
        "url": "https://doi.org/10.5281/zenodo.1416936",
        "ee": "https://zenodo.org/records/1416936/files/Izmirli09.pdf",
        "abstract": "In this paper we look at the problem of classifying music audio as tonal or atonal by learning a low-dimensional structure representing tonal relationships among keys. We use a training set composed of tonal pieces which includes all major and minor keys. A kernel eigenmap based method is used for structure learning and discovery. Specifically, a Diffusion Maps (DM) framework is used and its parameter tuning is discussed. Since these",
        "zenodo_id": 1416936,
        "dblp_key": "conf/ismir/Izmirli09",
        "keywords": [
            "classification",
            "tonal",
            "atonal",
            "learning",
            "low-dimensional",
            "structure",
            "tonal relationships",
            "keys",
            "training set",
            "tonal pieces"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nTONAL-ATONAL CLASSIFICATION OF MUSIC AUDIO \nUSING DIFFUSION MAPS \n  Özgür İzmirli   \n  \n  Center for Arts and Technology \nComputer Science Department \nConnecticut College \noizm@conncoll.edu \n \n     \nABSTRACT \nIn this paper we look at the problem of classifying music \naudio as tonal or atonal by learning a low-dimensional structure representing tonal relationships among keys. We use a training set composed of tonal pieces which in-cludes all major and minor keys. A kernel eigenmap based method is used for structure learning and discov-ery. Specifically, a Diffusion Maps (DM) framework is \nused and its parameter tuning is discussed. Since these \nmethods do not scale well with increasing data size, it \nbecomes infeasible to use these methods in online appli-\ncations. In order to facilitate on-line classification an out-of-sample extension to the DM framework is given. The learned structure of tonal relationships is presented and a simple scheme for classifica tion of tonal-atonal pieces is \nproposed. Evaluation results show that the method is able to perform at an accuracy above 90% with the current data set.  \n1. INTRODUCTION \nAudio key estimation is an important aspect of MIR. It \ninforms many other tasks including music analysis, seg-mentation, cover song detection, modulation tracking, local key finding and chord recognition. In order to esti-mate the key, most key finding models use a similarity metric between predetermined reference features and the analyzed features from the audio. All of these approaches assume that the fragment of the piece being analyzed \ncontains tonal music and furthermore that musical con-\ntent is in a single key. These models generally lack me-chanisms to detect music that is not tonal and hence would make best-guess estimates regardless of the tonal quality of the input. One importa nt question, which is the \ntopic of this paper, is how to determine whether a piece belongs to the tonal idiom: whether there are clear and unambiguous tonal implications or not.   \nIn this work, we explore the utility of dimensionality \nreduction, manifold learning and structure discovery in the context of tonal versus atonal music audio classifica-tion. We investigate the possibility of learning a low-dimensional structure representing tonal relationships among pieces. We report on expe riments that utilize Dif-\nfusion Maps to perform dimensionality reduction and feature extraction from high-dimensional spectral data. We use a set of audio recordings representative of all 24 keys as the reference training set and test the model with tonal and atonal audio fragments to evaluate its perfor-mance.  \nThe structure of the paper is as follows: The next sec-\ntion makes reference to related work and explains the concept of tonalness. Section 3 describes kernel methods and DM in particular. This section also discusses the tun-ing of the width parameter of DM. Section 4 outlines the main outcomes and describes the evaluation method. Sec-\ntion 5 concludes the paper.   \n2. RELATED WORK \nTemperley describes a probabilistic framework on sym-\nbolic data for measuring tonal implication, tonal ambigu-ity and tonalness for pitch-class sets [1]. According to his definition, tonal implication is the key implied by the pitch-class set being used. Ambiguity refers to whether a pitch set implies a single key or several keys. Tonalness is the degree to which a set is characteristic of common-\npractice tonality. In this sense, our work relates directly to the concept of tonalness. Our assumption is that a piece that conforms to pitc h distributions of common \npractice tonality will have certain spectral properties that distinguish it from other types of pitch distributions such as those found in twelve-t one music or polytonality. \nThese spectral properties, or so called spectral signa-tures, have native representations in a high-dimensional space and therefore need to be mapped to low-\ndimensional features to be useful - not only for classifi-\ncation purposes but also for visualization and geometric-al interpretations. The remainder of the paper discusses a method to classify music audio based on the degree of tonalness.  \nIn her thesis, Gómez applied her key finding method \nto an atonal piece by Schoenbe rg [2]. She observed that \nthe correlations of her Harmonic Pitch Class Profile (HPCP) with the major and minor profiles, that are de-rived from Krumhansl's work, remained low throughout the piece, indicating ambiguity. \nIzmirli reported on the performance of a template \nbased key finding algorithm using a low-dimensional representation obtained through dimensionality reduction [3]. He graphed the performance of his method as a \n687Poster Session 4\n  \n \nfunction of the number of dimensions and noted that 2 \nand 3 dimensions produced acceptable accuracy for the particular model he was using.  \nPurwins briefly discusses poly-tone analysis and ton-\nal ambiguity in relation to Pitch Class Profiles that he uses in his key finding algorithm [4]. \n3. DIMENSIONALITY REDUCTION, MANIFOLD \nLEARNING AND STRUCTURE DISCOVERY \n3.1 Method \nIn general, given a set of training data we would like to \ninfer some parameterization of it such that new data can be efficiently compared to the training data. The parame-\nterization can then be used for many different purposes including classification. In the following we present a method that performs dimens ionality reduction on a train-\ning set of tonal audio in order to find a representative structure. The resulting low dimensional representation is then used to determine whether new input data resembles the training data or not; more specifically, if it is tonal or atonal. This section describes the method of dimensional-ity reduction used and a scalable extension for new data. \n3.2 Kernel Methods \nIn contrast to the standard linear methods such as Prin-\ncipal Component Analysis (PCA) and Multidimensional Scaling (MDS) for dimensi onality reduction, nonlinear \nmethods are better suited to preserving local geometry. \nThis is due to the fact that they attempt to approximate \nmanifolds in the high-dimens ional space by considering \nconnectivity between neighboring points as opposed to capturing the global nature of the data. Nonlinear me-thods include Isometric Feature Mapping (ISOMAP), Kernel PCA and a class of kernel eigenmap methods in-cluding Laplacian Eigenmaps, Locally Linear Embedding (LLE), Hessian Eigenmaps (Hessian LLE) and Local Tangent Space Alignment (LTSA). In [5] Coifman and Lafon show that the kernel eigenmap methods are special cases of a general framework based on diffusion processes. Here, we follow a formulation for dimensio-\nnality reduction, manifold learning and data parametriza-tion based on DM [5]. The major advantages of this ap-proach over PCA and MDS are that it is nonlinear and preserves local structures. Kernel eigenmap methods rely on the idea that eigenvector s of a transition matrix \nrepresenting the distances between points in the input \nspace can be interpreted as coordinates on the data set. \n3.3 Diffusion Maps \nThe concept of diffusion maps stems from dynamical \nsystems and it is based on a Markov random walk on the graph of the data. The proximity of the data points is modeled as diffusion distances  according to the affinity \nbetween neighboring points. DM preserves local geome-try present in the high-dimensional input while perform-ing dimensionality reduction.  \nAssume the data set containing k elements is given by \nX={x\n0, x1, x2, ...,x k-1} with xi element of Rm. A pairwise \nsimilarity matrix L  is calculated using a Gaussian kernel \nwith parameter ε : \n22\n) , (ε\nεj ix x\nj i j ie x x w L− −= =                       (1) \nFurthermore, a diagonal normalization matrix is defined to make the sum of the rows of L equal 1: \n∑=\njj i i iL D                              (2) \nThe normalized graph Laplacian is then given by the \nMarkov matrix L D M1−= . In order to find a mapping, \nΦ, from Rm to Rn, where m > n, an eigen-decomposition \nof M is performed. The eigenvectors and eigenvalues can \nbe found by solving the equivalent generalized eigenva-\nlue problem φλφ D L= . When ε in Eq. 1 is large enough, \nM is fully connected and has a unique eigenvalue of 1. \nFrom the remaining k-1 eigenvalues, n of the largest \n0≥ ≥n ... 1>2 1≥ ≥λλλ  and their corresponding eigen-\nvectors nφφφ ,...2,1 can be retained to map input samples \nfrom the high-dimensional space onto the lower dimen-\nsional feature space. The mapping is given by  \n)] ( ),... ( ), ( [ :2 2 1 1 i i i xn n i φλφλφλ→Φ                       (3) \nwhere index i in ) (inφ represents the i'th element of the \neigenvector. \n3.4 Determining ε \nThe width parameter, ε, controlling the Gaussian in Eq. 1 \nhas an effect on the locality of the structure captured. For \nexample, a relatively small ε  will capture the local struc-\nture better. However, if ε  is too small then matrix L  will \nhave many small elements and hence, low connectivity, \nwhich will prevent it from capturing the desired structure. \nAn unnecessarily large value on the other hand will cause the method to overlook the local structure. Although the value of this parameter is data dependent, fortunately, its choice can be automated. \n Several approaches have been proposed to determine \nthe optimal value of ε. The average of the distances be-\ntween nearest neighbors in the data set are used in [6]. Another method is to adjust the parameter until every point has a significant connection to at least one neigh-bor. We follow the approach used in [7]. The method consists in searching for a point on the linear segment of \nthe log-log graph of  \n) (εT and ε, where \n  ∑∑=\nijj ix x w T ) , ( ) (ε ε                          (4) \n68810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nThe graph contains two asymptotes, ) ( limεεT∞ → and \n) ( lim0εεT→ which are connected by an approximately \nlinear line. We choose ε corresponding to the midpoint \nbetween the asymptotes in this graph. \n3.5 Scalability and Out-of-Sample Extensions \nKernel methods described in the previous section have \nbeen successfully applied to dimensionality reduction and \nmanifold learning. They are, however, computationally \nexpensive and do not scale well to large data sets. They \nalso do not directly accommoda te new data and in that \nsense are limited to their training set requiring a new run \nevery time new data is to be added. \nOut-of-sample extensions are approximations that \nutilize the original eigen-decomposition to compute the mapping of new samples that do not belong to the origi-nal data set. In [8] the authors discuss how to compute out-of-sample extensions for various kernel methods. We employ the Nystrӧ m extension to find the mapping of the \nnew data point as follows: \n∑∑−\n=−\n=−=1\n01\n01~\n) , ( ) ( ) , (k\nik\npp new j i new j j x x w i x x wε ε φ λ φ        (5) \nOnce  is calculated, it is substituted for the \ncorresponding eigenvectors in Eq. 3 to obtain the position \nin the lower dimensional feature space. ) .. 1 (~\nn jj=φ\nCalculation of the Nystr ӧm extension is computation-\nally light. The denominator of  Eq. 5 can be precalculated \nand the numerator is just a scaled sum of k vectors.  \n4. LEARNING TONAL STRUCTURE \n4.1 Geometric Models of Pitch and Key \nMany geometric models of pitch and key space have been \nproposed that originate from music theory and cognitive \nscience. These include structur es such as a circle, torus, \nhelix and double helix (See for example [9] and [10]). \nFurthermore, most of these geometric structures are cyc-\nlic at one if not at multiple levels. In its simplest form, we \nknow that key arrangements of the 12 major keys moving in fifths forms a circle. Similarly minor keys follow the same pattern. Obviously, this is based on the assumption that the music is performed in  an equal tempered system.  \nIn [11] it has been demonstrated that this or another \ncyclic structure can be captu red from the audio of musi-\ncal instruments playing di atonic scales. In this 2-\ndimensional space, points that represent key centers are organized in such a way that if we draw lines between the \nclosely related keys the resulting arrangement forms a closed loop visiting each key center once.  \n4.2 Learning Structure from Audio Data \nIn this work, we explore the utility of structure discovery \nin the context of tonal versus atonal music audio. We ob-tain a chroma representation similar to [12] from the Hanning windowed short-time Fourier Transform. A 12-\nelement chroma vector is obtained by summing the semi-\ntone frequency ranges of the amplitude spectrum accord-ing to pitch-class equivalence. That is, the semitone fre-quency range around the fundamental frequency of a note, the range around its octave and its second octave etc. all map to a single bin in the chroma vector.  \nInitially, we employed the method outlined in Section \n3 to test if it was able to learn a low-dimensional struc-ture using only recordings of tonal music. The training data, X, comprised of chroma vectors calculated from ini-\ntial fragments of 289 pieces  containing compositions \nmainly from the common practice period. Each point in the data set, x\ni, represents the average of 30 seconds of \nmusic taken from the beginning of each piece. This dura-tion was determined experimentally and can be chosen to be shorter without significantly effecting the algorithm's output. Note that the training is unsupervised and al-though the key labels are known from the titles of pieces they are not part of the input. The key distribution of the data set, although not completely uniform, is such that the lowest number of pieces in the same key is 9. For a col-\nlection of this size, a completely even distribution would \nrequire 12 pieces for each of the 24 keys. Although it \nwould have been possible to e ither trim all pieces to the \nsame number or add more pieces to bring the key totals to \nthe same level, the current distribution was kept to ob-serve the sensitivity of the DM algorithm to the density of samples on the manifold. It should be mentioned that sampling density is a main concern for many manifold learning algorithms and may need special attention if the spatial distribution is unbalanced. \n \n \nFigure 1. The input data set consisting of tonal pieces \nmapped to the first two dimensions. A circular structure resembling the circle-of-fifths is captured for the chro-ma representation (left) and for the spectral representa-tion (right). \n \nThe left plot in Figure 1 shows the mapping Φwith \n2=n  in response to the input data set, X, based on the \nchroma representation as described above. The out-of-\nsample extension is not used for this part. A circular structure is clearly visible in the figure which means it \nwas able to capture some kind of circularity. Then again, \nthis highly resembles the circ le-of-fifths pattern. We veri-\nfied the order of keys by analyzing their key labels to make sure the neighboring clusters were in a fifths rela-tionship. There was considerable scatter within classes \n689Poster Session 4\n  \n \nthat belong to the same key. There was also significant \noverlap between classes, yet,  the circle-of-fifths pattern \nwas evident. The output for th e spectral representation is \nshown in the right plot in Figure 1. These vectors are the same spectral vectors used to  calculate the chroma repre-\nsentation. The reason for inclusion of the spectral vectors is to see if DM is able to obtain a mapping on par with or better than the traditional chroma representation. It should be noted that the uneven density of points does \nmanifest itself in both plots without loss of generality of \nthe result. \nTo further demonstrate the circle-of-fifths pattern we \nused chroma templates obtained from the audio of mono-\nphonic instrument sounds playing major scales. Each of the 12 templates consists of a single scale over multiple octaves. The details of the construction of the templates can be found in [13] and [14]. The templates were mapped using the out-of-sample method with respect to the tonal training data, X, described above. The results are shown in Figure 2. Here, each template represents an ideal key position in the feat ure space and the projection \nserves as a demonstration of  the circle-of-fifths relation-\nship among the 12 major keys. A similar order has also been observed for minor keys. \n \n \nFigure 2. Mapping of audio templates to the first two \ndimensions. The labeled points representing the major templates are superimposed on the chroma based repre-sentation in Figure 1 (left). \n4.3 Training and Test Data for Evaluation \nStarting from the observation that a data set containing \npieces in all 24 keys results in the constellations shown in Figure 1, we turn to testing the DM model with tonal and atonal data using the out-of-sample extension described above. For this part we a dded 25 complete atonal pieces \ncomposed by Boulez, Schoenberg and Webern. Both the tonal and atonal pieces were  segmented into 10-second \nfragments. There are 599 atonal fragments and 925 tonal fragments in the data set. Each fragment is represented as a point, x\ni, found by dividing the spectral or chroma vec-\ntors by their L 2 norm, and an associated tonal/atonal label \nserving as ground truth for evaluation purposes. The fre-quency ranges of interest for both representations are 55 - 2000 Hz. The training data set was constructed as fol-lows: 60% of the tonal points were randomly chosen and were used to train the DM model. The remaining 40% were added to the test set accompanied by an equal num-ber of points randomly chosen from the atonal set. After calculating the original mapping using 60% of the tonal \npoints, the out-of-sample calculations were performed on \nthe test set. Figure 3 shows the mapping of the test results onto the first two dimensions. These results are overlaid with the training points to show the nature of generaliza-tion the extension brings.  \n \n \nFigure 3. Training and test da ta mapped to the first two \ndimensions: chroma based inputs (top) and spectrum \nbased inputs (bottom). Tonal training data are shown \nwith dots ( .), the tonal test data are shown with circles \n(○) and the atonal test data ar e shown with pluses (+). \n4.4 The Tonal-Atonal Classifier \nAs can be easily observed from Figure 3, the tonal train-\ning points and the tonal test points tend to appear at posi-tions closer to the outer circ ular pattern whereas the aton-\nal test points tend to appear near the center. Therefore, \nwe simply choose to use the Euclidean norm of a point in the feature space to quantify its tonalness as defined in \nSection 2. For the 2-dimensional case, the performance of \nthe classifier is given by the peak classification accuracy \nin which a circle acts as the class boundary. It should be \nnoted that although we treat the problem as a two-class \nclassification task in this paper, in fact, the calculated to-nalness is a continuous entity and is indeed correlated with the degree of the musical fragment's tonal implica-tion. The distances in the f eature space can be used to \n69010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nquantify the degree of tonalness. A study of the tonalness \nof transpositional type pitch class sets can be found in [15].  \n4.5 Results \nAn average accuracy was calculated by running the \nabove classification 10 times. The chroma based classifi-\ncation resulted in an averag e accuracy of 91.2% and the \nspectrum based classification resulted in 90.4% accuracy. \nAs an alternative feature we ran a classification task \nbased on the variance of the chroma and spectrum vec-\ntors ( x\ni) to see how they compared  with the presented me-\nthod. The intuition was that the chroma vector corres-\nponding to tonal pieces would have more variance com-\npared to atonal pieces because it would exhibit a strong interleaved response across bins of the vector. i.e. say, for C major, one would expect the bins corresponding to the white keys to be strong and those of the black keys to be weak.  On the other hand, atonal pieces would have a \nmore uniform spread across the bins. The chroma va-riance feature performed at 84.9% accuracy. The same reasoning does not really apply to the spectrum vectors because they are fairly spar se compared to the chroma \nvectors but nevertheless we tested the feature and ob-tained 64.4% accuracy; very low as expected.  \n5. CONCLUSION \nIn this paper we have di scussed a method based on Dif-\nfusion Maps to perform tonal-atonal classification of mu-\nsic audio. Initially, we learn a low-dimensional structure representing pitch distributions that pertain to the tonal idiom. We then extend the learned mapping to new points and test the performance of the method. The learned cyc-lic structure is demonstrated through a display of the pro-jected circular constellation of the training points and the projection of major scale templates representing ideal key locations in relation to this constellation. The use of the learned cyclic structure in quantifying tonalness is also discussed. Finally, results are presented for the tonal-atonal classification task for chroma representations as \nwell as raw spectral representations. The results are en-couraging and promising. Future work involves exploring more general mechanisms for calculating the structure similarity between training and test structures, and find-ing optimal training sets for faster and more efficient op-eration. \n6. REFERENCES \n[1] D. Temperley: \"The Tonal Properties of Pitch-\nClass Sets: Tonal Implication, Tonal Ambiguity, and Tonalness,\" Eleanor Selfridge-Field and Wal-ter Hewlett, eds. Computing in Musicology , Tonal \nTheory for the Digital Age , Vol. 15, 24-38, 2008. \n[2] E. Gómez: “Tonal Description of Music Audio Signals,” Ph.D. Dissertation , Pompeu Fabra Uni-\nversity, Barcelona, 2006. [3] Ö. İzmirli: “Audio Key Finding Using Low-\nDimensional Spaces,” Proceedings of the Interna-\ntional Conference on Music Information Re-\ntrieval , Victoria, Canada, 2006. \n[4] H. Purwins: \"Profiles of Pitch Classes Circularity \nof Relative Pitch and Key – Experiments, Models, Computational Music Analysis, and Perspectives,\" Ph.D. Thesis , Berlin University of Technology, \n2005. \n[5] R. R. Coifman and S. Lafon: \"Diffusion Maps,\" Applied and Computational  Harmonic Analysis , \n21, pp. 5–30, July, 2006. \n[6] S. Lafon: \"Diffusion Maps and Geometric Har-monics,\" Ph.D. Thesis , Yale University, New Ha-\nven, USA, 2004.  \n[7] A. Singer, R. Erban, I. Kevrekidis and R. Coif-man: \"Detecting Intrinsic Slow Variables in Sto-chastic Dynamical System s by Anisotropic Diffu-\nsion Maps,\" Proceedings of the National Academy of Sciences  (PNAS) 2009.  \n[8] Y. Bengio, J.-F. Paiement, and P. Vincent: \"Out-\nof-sample extensions for LLE, Isomap, MDS, Ei-\ngenmaps and Spectral Clustering,\" Advances in \nNeural Information Processing Systems, 16 , 2004. \n[9] F. Lerdahl: Tonal Pitch Space . New York: Oxford \nUniversity Press, 2001. \n[10] H. Purwins, B. Blankertz, K. Obermayer: \n\"Toroidal Models in Tonal Theory and Pitch-Class Analysis,\" Eleanor Selfridge-Field and Walter Hewlett, eds. Computing in Musicology , \nTonal Theory for the Digital Age , Vol. 15, 73-98, \n2008. \n[11] Ö. İzmirli: \"Cyclic Distance Patterns Among \nSpectra of Diatonic Sets: The Case of Instrument \nSounds with Major and Minor Scales,\" Eleanor Selfridge-Field and Walter Hewlett, eds. Computing in Musicology , Tonal Theory for the \nDigital Age , Vol. 15, 11-23, 2008. \n[12] T. Fujishima: \"Realtime Chord Recognition of Musical Sound: A System Using Common Lisp Music,\" Proceedings of the International \nComputer Music Conference (ICMC), Beijing, \nChina, 1999. \n[13] Ö. İzmirli: \"Template Based Key Finding From \nAudio,” Proceedings of the International \nComputer Music Conference  (ICMC), Barcelona, \nSpain, 2005. \n[14] Ö. İzmirli: “An Algorithm for Audio Key \nFinding,” 2005 Music Information Retrieval \nEvaluation eXchange (MIREX) Audio Key-Finding Contest , www.music-ir.org/evaluation/ \nmirex-results/articles/key_audio/izmirli.pdf, 2005. \n[15] Ö. İzmirli: \"Estimating the Tonalness of \nTranspositional Type Pitch-Class Sets Using Learned Tonal Key Spaces,\" Proceedings of \nMathematics and Computation in Music , New \nHaven, USA, 2009. \n691"
    },
    {
        "title": "An Ecosystem for Transparent Music Similarity in an Open World.",
        "author": [
            "Kurt Jacobson",
            "Yves Raimond",
            "Mark B. Sandler"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417151",
        "url": "https://doi.org/10.5281/zenodo.1417151",
        "ee": "https://zenodo.org/records/1417151/files/JacobsonRS09.pdf",
        "abstract": "There exist many methods for deriving music similarity associations and additional variations are likely to be seen in the future. In this work we introduce the Similarity Ontology for describing associations between items. Using a combination of RDF/OWL and N3, our ontology allows for transparency and provenance tracking in a distributed and open system. We describe a similarity ecosystem where agents assert and aggregate similarity statements on the Web of Data allowing a client application to make queries for recommendation, playlisting, or other tasks. In this ecosystem any number of similarity derivation methods can exist side-by-side, specifying similarity relationships as well as the processes used to derive these statements. The data consumer can then select which similarity statements to trust based on knowledge of the similarity derivation processes or a list of trusted assertion agents.",
        "zenodo_id": 1417151,
        "dblp_key": "conf/ismir/JacobsonRS09",
        "keywords": [
            "methods",
            "music similarity",
            "Similarity Ontology",
            "RDF/OWL",
            "N3",
            "Web of Data",
            "agent assertion",
            "aggregation",
            "recommendation",
            "playlisting"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAN ECOSYSTEM FOR TRANSPARENT MUSIC SIMILARITY IN AN\nOPEN WORLD\nKurt Jacobson\nCentre for Digital Music\nQueen Mary\nUniversity of London, UK\nkurtjx@gmail.comYves Raimond\nBBC\nLondon, UK\nyves.raimond@bbc.co.ukMark Sandler\nCentre for Digital Music\nQueen Mary\nUniversity of London, UK\nABSTRACT\nThere exist many methods for deriving music similarity as-\nsociations and additional variations are likely to be seen\nin the future. In this work we introduce the Similarity\nOntology for describing associations between items. Us-\ning a combination of RDF/OWL and N3, our ontology al-\nlows for transparency and provenance tracking in a dis-\ntributed and open system. We describe a similarity ecosys-\ntem where agents assert and aggregate similarity statements\non the Web of Data allowing a client application to make\nqueries for recommendation, playlisting, or other tasks. In\nthis ecosystem any number of similarity derivation meth-\nods can exist side-by-side, specifying similarity relation-\nships as well as the processes used to derive these state-\nments. The data consumer can then select which similarity\nstatements to trust based on knowledge of the similarity\nderivation processes or a list of trusted assertion agents.\n1. INTRODUCTION\nThe process of music recommendation in a general sense\ninvolves drawing associations between music-related items\n- i.e. artist aissimilar to artist bso recommend bif the user\nexpresses interest in artist a. We believe that similarity is\nthe underlying “currency” for recommendation. This real-\nization drives our interest in developing a formal model for\nsimilarity.\nSimilarity is a difﬁcult concept. The exact nature of\nsimilarity has been discussed extensively in cognition [26,\n28], philosophy [22, 14], and computer science [27, 17].\nIn the ﬁeld of music information retrieval we have been\nless concerned with the nature of similarity and more con-\ncerned with ﬁnding ways of calculating it [18, 20, 5]. This\npragmatic approach has led to a wealth of methods for de-\nriving music similarity statements from audio analysis and\ncontextual metadata.\nBut if we want to develop a generalized model for mu-\nsic similarity, it becomes more complicated. As Wittgen-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.stein puts it in his seminal work Philosophical Investiga-\ntions “Some things share a complicated network of simi-\nlarities overlapping and criss-crossing: sometimes overall\nsimilarities, sometimes similarities of detail.” Music would\ndeﬁnitely be such a thing. Discussing a pair of songs, we\ncan have a dizzying array of similarity options: the au-\ndio could have timbral similarity, rhythmic similarity, or\nmelodic similarity; the contexts of the songs could make\nthem similar in terms of lyrical content, cultural meaning,\nor shared listenership; or an authoritative source such as a\nmusic critic or website could judge the songs to be simi-\nlar without providing any additional justiﬁcation. Further\ncomplicating matters, similarity is subjective - what one\nindividual or agent considers similar another may not.\nBecause similarity can be so nebulous and contentious\nwe purpose a model for expressing similarity that foregoes\nhierarchical classiﬁcations and instead focuses on prove-\nnance and transparency. Instead of focusing on how a par-\nticular similarity statement is related to another similarity\nstatement, we focus on who made the similarity statement\nandwhy.\nOur approach is based on the Resource Description Frame-\nwork (RDF) [4, 9] and the Web Ontology Language [3].\nWhile these technologies provide an impressive amount of\nexpressiveness and form the foundation of the Semantic\nWeb, we augment their expressiveness with N3 [7]. The\nfacilities for quoting formulae provided by N3 allows us\nto use the N3-Tr framework [23] for deﬁning similarity\nderivation workﬂows.\nIn Section 2 we develop our model in the form of a Web\nontology, brieﬂy discussing some of the supporting tech-\nnologies and previous work. In Section 3 we describe our\nvision of a similarity ecosystem where a number of agents\naggregate and publish similarity statements in the Web of\nData while music applications query these statements for\nrecommendation or playlist generation. In Section 4 we\nprovide a cursory evaluation of our ontology. In Section 5\nwe review some related work and ﬁnally provide some\nconclusions and directions for future work in Section 6.\n2. AN ONTOLOGY FOR SIMILARITY\nBecause of its decentralized nature, wide deployment base,\nand robust technological underpinnings we use the RDF/OWL\nframework [4, 3, 9] for deﬁning our Similarity Ontology.\n33Oral Session 1-A: Knowledge on the Web\nThis allows us to use the concepts, practices, and resources\nofLinked Data [8]. In the Linked Data paradigm, every\nresource and concept is given a Unique Resource Identi-\nﬁer (URI). These URIs can be dereferenced using HTTP\nto provide additional information and links to other rele-\nvant URIs.\n2.1 Previous Ontologies\nRDF [4] allows us to express information in the form of\ntriples : subject, predicate, object statements. Generally\nthe subject will be an instance of a class concept while the\npredicate will be an instance of a property . The object will\nalso be an instance of a class concept but not necessarily\nthe same class as the subject. Classes and properties are\ndeﬁned in an ontology document using the Web Ontology\nLanguage (OWL) [3] or the RDF Schema (RDFS) [9] or a\ncombination of both. These technologies together enable\nwhat is commonly referred to as the Semantic Web or Web\nof Data.\nThese concepts have been successfully applied to the\ndomain of music with the Music Ontology [24, 23]. The\nMusic Ontology allows us to express a wide variety of\nmusic-related information as structured data in a decen-\ntralized fashion. It has been adopted by the Linked Data\ncommunity and is used extensively throughout the Web of\nData as a means of describing tracks, artists, performances,\nand related data.\nThe Music Ontology provides a basic facility for deal-\ning with music similarity. The mo:similar_to prop-\nerty allows one to assert a similarity relationship between\ntwo items. However, this property relation does not pro-\nvide any further information - How was the similarity de-\nrived? Who derived it? How similar are the two items?\n2.2 Association as a Concept\nInstead of treating similarity or, to use a broader term,\nassociation as a property , we treat association as a class\nconcept . This allows us to reify the association in or-\nder to provide additional information about it. We in-\ntroduce the class sim:Association and a sub-class\nsim:Similarity as the key concepts in our ontology.\nA simple similarity example is presented in the following\nlisting1:\n:track01 a mo:Track .\n:track02 a mo:Track .\n:me a foaf:Person .\n:mySimilarity a sim:Similarity ;\nsim:element :track01 ;\nsim:element :track02 ;\nsim:weight \"0.90\" ;\nfoaf:maker :me .\nWe introduce the namespace sim to refer to our Similar-\nity Ontology. First we deﬁne two tracks using the cor-\n1We use N3 [6] in all our code listings. Each block corresponds to\na set of statements (subject, predicate, object) about one subject. Web\nidentiﬁers are either between angle brackets or in a preﬁx:name notation\n(with the namespaces deﬁned at the end of the paper). Universally quan-\ntiﬁed variables start with ?. Existentially quantiﬁed variables start with\n:. Curly brackets denote a literal resource corresponding to a particular\nRDF graph. The keyword acorrespond to the identiﬁer rdf:type . The\nkeyword =>correspond to the identiﬁer log:implies .responding Music Ontology concept mo:Track . The\nidentiﬁers of these tracks can give entry points to addi-\ntional information in other data sets (i.e. linking to db-\npedia.org2URIs or Musicbrainz3identiﬁers). We deﬁne\n:mySimilarity to actually make the similarity state-\nment. The sim:element property is used to refer to the\ntracks involved in this similarity and the foaf:maker\nproperty refers to the agent which asserted this similarity.\nAlso note we can assign a numerical weight value to the\nsimilarity using the sim:weight property.\nNow we have a method for asserting a similarity state-\nment and reifying that statement to some extent. However,\nin the above example we only know who is making the\nsimilarity statement, we do not know how orwhy.\n2.3 Provenance and Transparency\nWe introduce the sim:AssocationMethod concept to\nidentify the process used to derive a similarity statement.\nThis enables some interesting functionality when consum-\ning the associations data - a consumer application can elect\nto include only similarity statements that are tied to a par-\nticular sim:AssocationMethod . This is discussed fur-\nther in section 3.1. For now let us consider the following\nN3 listing:\n:timbreSimilarityStatement\na sim:Similarity ;\nsim:element :track01 ;\nsim:element :track02 ;\nsim:weight \"0.9\" ;\nsim:method :timbreBasedSimilarity .\n:timbreBasedSimilarity\na sim:AssociationMethod ;\nfoaf:maker :me ;\nsim:description :algorithm .\n:algorithm = {\n{ { ?signal1 mo:published_as ?track01 .\n?signal1 sig:mfcc ?mfcc1 .\n?mfcc1 sig:gaussian ?model1 }\nctr:cc\n{ ?signal2 mo:published_as ?track02 .\n?signal2 sig:mfcc ?mfcc2 .\n?mfcc2 sig:gaussian ?model2 } .\n(?model1 ?model2) sig:emd ?div .\n?div math:lessThan 0.2 } =>\n{ _:timbreSimilarityStatement\na sim:Similarity ;\nsim:element ?track01 ;\nsim:element ?track02 }\n}\nHere:timbreBasedSimilarity is the entity that de-\nscribes our process for deriving similarity statements. Note\nthat this entity is only described by three triples - its class\ntype, a property for the creator and the description.\nN3 extends the semantics and syntax of RDF in a use-\nful and intuitive way. It allows for the existence of RDF\ngraphs (a set of triple statements) as quoted formulæ. We\ncan then make statements about the entire RDF graph pro-\nviding metadata about that graph. In this way N3 is sim-\nilar to Named Graphs [10], the main difference being that\nN3 considers RDF graphs as literals (their identity is their\nvalue), whereas Named Graphs consider graphs as entities\nnamed by a web identiﬁer.\n2http://dbpedia.org\n3http://musicbrainz.org/\n3410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nIn the above example, when we follow the sim:description\nproperty we see an RDF graph :algorithm denoted by\nthe{and}characters. This RDF graph provides a dis-\nclosure of the algorithm used in the similarity derivation\nprocess. In this case, MFCCs are extracted and Gaussian\nmixture models are created concurrently for the two sig-\nnals, and an earth mover’s distance is calculated between\nmodels. Depending on that distance, we output a similar-\nity statement. If more details are needed about a particular\ncomputational step, e.g. if we want to gather more infor-\nmation about the MFCC extraction step, we can look-up\nthe corresponding web identiﬁer, in this case sig:mfcc .\nThe algorithm is speciﬁed using the N3-Tr framework\nwhich uses transaction logic and N3 to describe signal pro-\ncessing workﬂows. Additional details on N3-Tr are avail-\nable in [23].\nHere, the N3-Tr formulæ describe the workﬂow sup-\nporting the similarity statement. We could forego the use\nof the sim:AssociationMethod concept and use the\nlog:supports built-in predicate4in the N3 framework.\nHowever, as we will discuss in section 3.1, binding similar-\nity workﬂows to the sim:AssociationMethod con-\ncept allows us to make simple, useful queries (i.e.“show\nme all similarity derivation methods available in the sys-\ntem”).\nFinally, note that we bind the foaf:maker property\nto the association method rather than directly to the asso-\nciation itself. As in the above example we can make our\nassociation method transparent, or we can provide a min-\nimum amount of information when dealing with a “black\nbox” similarity derivation processes. In either case it is\na matter of best practice to create an association method,\neven if we do not desire full transparency because this al-\nlows data consumers to make simple queries.\nAs indicated in Figure 2.3, our framework also supports\nthe grounding of similarity statements directly through the\nproperty sim:grounding . This property associates a\nsimilarity statement with the instantiated N3-Tr formulæ\nwhich enabled its derivation. In the above example, we\nwould link our timbre similarity statement directly to a spe-\nciﬁc workﬂow with references to the calculated values at\neach step.\n3. A SIMILARITY ECOSYSTEM\nThe data model provided by the Similarity Ontology al-\nlows for lots of ﬂexibility in specifying similarity state-\nments. This ﬂexibility is balanced by the built-in mecha-\nnisms for provenance tracking. By following the method\nproperty in a similarity statement we know who made the\nstatement and why. When consuming similarity data, we\nselect statements by deciding which agents and algorithms\nto trust. While it is entirely possible to make a similarity\nstatement within this framework completely anonymously,\nsuch statements are likely to be ignored by data consumers.\nInstead the statements from trusted agents or transparent\nalgorithmic processes are likely to be selected by data con-\n4seehttp://www.w3.org/DesignIssues/N3Logic\nFigure 1 . Using the Similarity Ontology. As additional\nproperties are bound to our association and association\nmethod statements, we achieve greater transparency.\nsumers. In a music recommendation application, this al-\nlows for more transparent recommendations - providing\nthe end user with the source or process used to make the\nrecommendation. Intuition as well as recommender system\nresearch suggest users are more likely to trust transparent\nrecommendation processes [11].\nBeyond the speciﬁcation of the Similarity Ontology, we\nenvision a broader ecosystem where autonomous, semi-\nautonomous, and human agents operate in parallel, making\nsimilarity statements about music tracks and artists while\nproviding provenance and justiﬁcation for these statements.\nA simple diagram illustrating how this ecosystem might be\nstructured is provided in Figure 2.\nAn enabled client music application publishes the end\nuser’s listening habits to the Web of Data. Similarity agents\noperate on the Web of Data and publish their own mu-\nsic similarity statements - perhaps consuming the listening\nhabits of end users as well as other data. These statements\nrefer to speciﬁc URIs for each track and artist. Similarly,\nthe client music application links the content in the user’s\npersonal collection to URIs using methods such as those\ndetailed in [25]. This avoids ambiguity - we can be sure\nthat the similarity statements are referring to the speciﬁc\nresource in which we are interested. The similarity state-\nments made by various agents are aggregated into one or\nmore data stores for querying. The client music applica-\ntion, perhaps responding to a user request, can query the\ndata store for similarity statements from trusted agents in-\nvolving the target resource (i.e a track or artist). The query\nreturns similarity information that can be used for content\nrecommendations or playlist generation.\n3.1 Similarity Queries\nQueries in this similarity ecosystem would be made using\nthe SPARQL query language [1]. The SPARQL speciﬁca-\ntion is a W3C recommendation and the preferred method\nfor querying RDF graphs. As mentioned before, the de-\nsign of the Similarity Ontology allows for the construction\nof simple queries to retrieve similarity information. The\nfollowing query retrieves artists similar to a target artist as\n35Oral Session 1-A: Knowledge on the Web\nFigure 2 . The music similarity ecosystem. Similarity agents operate on structured data to create similarity statements. Such\nstatements are aggregated in a data store and queried by a client music application to provide recommendations, playlists,\nand other functionality.\nstated by a speciﬁc trusted method:\nSELECT ?artists WHERE {\n?statement sim:method <http://trusted.method/uri> .\n?statement sim:element <http://target.artist/uri> .\n?statement sim:element ?artists . }\nNotice we only have to include a triple pattern for our tar-\nget resource, a triple pattern for our trusted agent, and a\ntriple pattern to select the similar artists. Of course this is\na very simple example and in real-world applications we\ninclude additional optional patterns and conjunctions for a\nmore expressive query.\nIn an initialization step, an application could query avail-\nable data sources to determine exactly what association\nmethods and asserting agents are available. The applica-\ntion would use the following query:\nSELECT DISTINCT ?method WHERE{\n?method a sim:AssociationMethod . }\nThe application could then ﬁlter through the results and,\nperhaps with some input from the end-user, decide which\nsimilarity agents to trust.\n3.2 Similarity and Recommendation\nWhile we hold that similarity is the basis of recommenda-\ntion, we also acknowledge that similarity and recommen-\ndation are not identical. By no means does the ecosystem\nproposed here solve the problems of recommender sys-\ntems - rather it provides a new distributed cross-domain\nplatform on which future recommender systems might be\nbuilt.\nWhile an item-to-item recommendation system ﬁts\nquite naturally into this similarity ecosystem, we can also\nimagine a collaborative ﬁltering-style user-item recom-\nmendation system. Each user in the system is treated\nas an sim:AssocationMethod instance. Each user’s\nmethod makes a set of statements asserting that the tracksfound in that user’s personal collection are similar to each\nother. Then an additional sim:AssocationMethod\ninstance is used to match users with each other based on\nthe contents of their respective music libraries. Finally, for\na given user, the recommendations for that user are an ag-\ngregation of the similarity statements derived from the as-\nsociation methods bound to the most similar users.\nAlso note that the similarity ecosystem fosters hybrid\nrecommendation approaches. Because the similarity state-\nments are made using common semantics and syntax, we\ncan easily combine and compare these statements to derive\nrecommendations or new similarity statements.\n4. ONTOLOGY EVALUATION\nWhile our Similarity Ontology is very ﬂexible and poten-\ntially very expressive, there is one import limit to its ex-\npressiveness - there is no mechanism for expressing dis-\nsimilarity . This is an intentional design decision that fol-\nlows from the open world assumption - we cannot know\nallinstantiations of similarity, and what we consider dis-\nsimilar, another agent may consider similar.\nAs a cursory evaluation of our Similarity Ontology we\npresent several real-world similarity scenarios and show\nhow our ontology can accommodate these examples.\n4.1 Directed Similarity\nAs often noted in psychology and cognition [28], similarity\nis not always symmetric. For example in the domain of\nmusic we may wish to express an inﬂuence relationship\nor we may simply have a similarity derivation algorithm\nthat is non-symmetric. This leads to a directed similarity\nrelationship. To accommodate such scenarios we introduce\nsim:subject andsim:object as sub-properties of\nthesim:element property. This allows us to specify a\ndirected similarity statement where the subject is similar\n3610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nto the object, accepting that the reverse is not necessarily\ntrue.\n4.2 Contextual Similarity\nBecause music is a complex construct deeply ingrained in\nculture and society, we often want to make music similar-\nity statements that relate to the context of musical works\nrather than the content of the musical works themselves.\nLet us consider an example from popular rap music. In the\nmid to late 1980s a series of songs were released disputing\nthe place of origin of the musical genre hip hop launching\na multi-faceted feud that became colloquially referred to\nasThe Bridge Wars5. By simply creating an association\nmethod that asserts similarities between artists and tracks\nrelated to this feud we can accommodate this scenario.\n4.3 Personal Associations\nThe emotional affect of music can be highly personal. A\nset of associations between music artists or tracks might\nbe unique for one particular individual. Consider the fol-\nlowing statement, “When a ﬁrst year student at college, I\ndated a girl who listened to Bob Marley and David Bowie”\n- while this association between David Bowie and Bob\nMarley might hold weight for the narrator, it is likely that\nfew other individuals would share this association. How-\never, the narrator, for any number of reasons, may wish\nto express this association anyway. This is entirely possi-\nble in our ontological framework. The narrator can sim-\nply create an sim:AssociationMethod that asserts\nsimilarity statements based on the musical taste of his ex-\ngirlfriend.\n5. RELATED WORK\nSemantic Web technologies have been applied to music\nrecommendation in previous works [12, 21] although, to\nthe best knowledge of the authors, the present work is the\nﬁrst effort to develop a comprehensive framework for ex-\npressing music similarity on the Web of Data.\nThe Sim-Dl framework provides a basis for deriving\nsimilarities from semantic information within a description\nlogic paradigm, although no formal syntax for expressing\nsimilarity results is provided [15]. Similarly, the iSPARQL\nframework extends SPARQL to include customized simi-\nlarity functions [16] but fails to provide a formal method\nof expressing the resulting similarities.\nAlthough the N3-Tr framework provides a clean and ex-\ntensible syntax for describing similarity derivation work-\nﬂows, alternative frameworks can be used as well. The\nProof Markup Language provides a ﬂexible means for jus-\ntifying the results of a Semantic Web query [13].\nThe vast body of work on music similarity and music\nrecommendation [18, 5, 20, 11] provides a set of templates\nfor designing music similarity agents that might operate in\nour purposed ecosystem.\n5http://en.wikipedia.org/wiki/The_Bridge_WarsKnowledge management systems for music-related data\nsuch as Pachet’s work [19] and more speciﬁcally the ontol-\nogy engineering of Raimond [24, 23] and Abdallah et. al\n[2] provide the basis for the similarity ecosystem. Without\nthe Music Ontology framework for describing music meta-\ndata and the technology and infrastructure provided by the\nLinked Data community - including Muscibrainz URIs for\nsongs and artists and data publishing guidelines - the Sim-\nilarity Ontology would be unusable.\n6. CONCLUSIONS AND FUTURE WORK\nWe have presented an ontological framework for describ-\ning similarity statements on the Web of Data. This on-\ntology is extremely ﬂexible and capable of expressing a\nsimilarity between any set of resources. This expressive-\nness is balanced by transparency and provenance, allowing\nthe data consumer to decide what similarity statements to\ntrust. We have shown hows this framework could exist as\nthe foundation for a broader music similarity ecosystem\nwhere autonomous, semi-autonomous, and human agents\npublish a wealth of similarity statements which are com-\nbined, consumed, and re-used based on provenance, trust,\nand application appropriateness.\nWe have suggested how similarity algorithms can be\nmade transparent. We have adopted the N3-Tr syntax for\ndescribing similarity derivation workﬂows. In future work\nwe plan to extend this syntax and the supporting ontolo-\ngies to better enable the publication of similarity derivation\nworkﬂows. Furthermore we hope to develop a series of\nrecommendations for best practice when publishing such\nworkﬂows to maximize their usefulness and query-ability.\nWe also plan to adopt a method of digitally signing sim-\nilarity statements in our ecosystem using terms available in\nthe WOT RDF vocabulary6. This would allow agents to\nsign similarity statements using Public Key Cryptography\nto avoid “spam” similarity statements.\nWhile our Similarity Ontology was designed with mu-\nsic similarity in mind, it is by no means limited to the do-\nmain of music. As we have shown, the framework is both\nﬂexible and extensible. We leave it to future work to ex-\nplore how this framework might be applied in different do-\nmains and across domains.\n7. NAMESPACES\nThe following namespaces are used throughout this work:\n@prefix mo: <http://purl.org/ontology/mo/>.\n@prefix sim: <http://purl.org/ontology/similarity/>.\n@prefix foaf: <http://xmlns.com/foaf/0.1/>.\n@prefix math: <http://www.w3.org/2000/10/swap/math#>.\n@prefix log: <http://www.w3.org/2000/10/swap/log#>.\n@prefix sig: <http://purl.org/ontology/signal/>.\n@prefix ctr: <http://purl.org/ontology/ctr/>.\n8. REFERENCES\n[1] SPARQL query language for RDF, W3C recommenda-\ntion, 2008.\n6http://xmlns.com/wot/0.1/\n37Oral Session 1-A: Knowledge on the Web\n[2] Samer Abdallah, Yves Raimond, and Mark Sandler.\nAn ontology-based approach to information manage-\nment for music analysis systems. In 120th Audio Engi-\nneering Society Convention , 2006.\n[3] Sean Bechhofer, Frank van Harmelen, Jim Hendler,\nIan Horrocks, Deborah McGuinness, Peter Patel-\nSchneijder, and Lynn Andrea Stein. OWL Web On-\ntology Language Reference. Recommendation, World\nWide Web Consortium (W3C), February10 2004. See\nhttp://www.w3.org/TR/owl-ref/ .\n[4] D. Beckett. RDF/XML Syntax Speciﬁcation (Re-\nvised). Recommendation, World Wide Web Consor-\ntium (W3C), 2004. Internet: http://www.w3.\norg/TR/rdf-syntax/ .\n[5] A. Berenzweig, B. Logan, D. P. W. Ellis, and B. P. W.\nWhitman. A large-scale evaluation of acoustic and sub-\njective music-similarity measures. Computer Music J. ,\n28(2):63–76, 2004.\n[6] Tim Berners-Lee. Notation 3, 1998. See\nhttp://www.w3.org/DesignIssues/\nNotation3.html .\n[7] Tim Berners-Lee, Dan Connolly, Lalana Kagal, Yosi\nScharf, and Jim Hendler. N3logic: A logical frame-\nwork for the world wide web. Theory and Practice of\nLogic Programming , 2007.\n[8] Chris Bizer, Richard Cyganiak, and Tom Heath. How\nto publish linked data on the web.\n[9] Dan Brickley and Ramanatgan V . Guha. Rdf vo-\ncabulary description language 1.0: Rdf schema.\nW3c recommendation, W3C, February 2004.\nhttp://www.w3.org/TR/2004/REC-rdf-schema-\n20040210/.\n[10] Jeremy J. Carroll, Christian Bizer, Pat Hayes, and\nPatrick Stickler. Named graphs, provenance and trust.\nInWWW ’05: Proceedings of the 14th international\nconference on World Wide Web , pages 613–622, New\nYork, NY , USA, 2005. ACM.\n[11] O. Celma. Music Recommendation and Discovery in\nthe Long Tail . PhD thesis, Universitat Pompeu Fabra,\nBarcelona, Spain, 2008.\n[12] `Oscar Celma, Miquel Ram ´ırez, and Perfecto Herrera-\nBoyer. Foaﬁng the music: A music recommendation\nsystem based on rss feeds and user preferences. Pro-\nceedings of the 6th ISMIR, 2005.\n[13] Paulo Pinheiro da Silva, Deborah L. McGuinness, and\nRichard Fikes. A proof markup language for semantic\nweb services. Inf. Syst. , 31(4):381–395, 2006.\n[14] Keith J. Holyoak. The Cambridge Handbook of Think-\ning and Reasoning . Cambridge University Press, Cam-\nbridge, UK, April 2005.[15] Krzysztof Janowicz. Sim-dl: Towards a semantic sim-\nilarity measurement theory for the description logic\nin geographic information retrieval. On the Move to\nMeaningful Internet Systems 2006: OTM 2006 Work-\nshops , pages 1681–1692, 2006.\n[16] Christoph Kiefer, Abraham Bernstein, and Markus\nStocker. The fundamentals of isparql - a virtual triple\napproach for similarity-based semantic web tasks. In\nKarl Aberer and Key-Sun Choi, editors, Proceedings\nof the 6th International Semantic Web Conference ,\nLNCS, pages 295–308, Berlin, 2007. Springer.\n[17] Dekang Lin. An information-theoretic deﬁnition of\nsimilarity. In ICML ’98: Proceedings of the Fifteenth\nInternational Conference on Machine Learning , pages\n296–304, San Francisco, CA, USA, 1998. Morgan\nKaufmann Publishers Inc.\n[18] B. Logan and A. Salomon. A music similarity function\nbased on signal analysis. Multimedia and Expo ICME ,\npages 745–748, 2001.\n[19] Francois Pachet. Knowledge management and musical\nmetadata. Encyclopedia of Knowledge Management ,\n2005.\n[20] E. Pampalk. Computational Models of Music Similar-\nity and their Application in Music Information Re-\ntrival . PhD thesis, Technischen Universit ¨at Wien, May\n2006.\n[21] Alexandre Passant and Yves Raimond. Combining so-\ncial music and semantic web for music-related recom-\nmender systems. In Semantic Web Workshop , 2008.\n[22] W. V . Quine. Ontological relativity and other essays .\nColumbia University Press, New York, NY , USA,\n1969.\n[23] Yves Raimond. A distributed music information sys-\ntem. PhD thesis, Queen Mary University of London,\n2009.\n[24] Yves Raimond, Samer Abdallah, Mark Sandler, and\nFr´ed´erick Giasson. The music ontology. Proceedings\nof the 8th ISMIR, 2007.\n[25] Yves Raimond, Christopher Sutton, and Mark Sandler.\nAutomatic interlinking of music datasets on the seman-\ntic web, 2008.\n[26] Roger N. Shepard. Geometrical approximations to the\nstructure of musical pitch. Physchological Review ,\n89:305–333, 1982.\n[27] Joshua B. Tenenbaum. Learning the structure of simi-\nlarity. In G. Tesauro, D. S. Touretzky, and T. K. Leen,\neditors, Advances in Neural Information Processing\nSystems 8 . MIT Press, Cambridge, MA, USA, 1996.\n[28] Amos Tversky and Itamar Gati. Similarity, separabil-\nity, and the triangle inequality. Physchological Review ,\n89:123–154, 1982.\n38"
    },
    {
        "title": "An Efficient Signal-Matching Approach to Melody Indexing and Search Using Continuous Pitch Contours and Wavelets.",
        "author": [
            "Woojay Jeon",
            "Changxue Ma",
            "Yan Ming Cheng"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415054",
        "url": "https://doi.org/10.5281/zenodo.1415054",
        "ee": "https://zenodo.org/records/1415054/files/JeonMC09.pdf",
        "abstract": "We describe a method of indexing and efficiently searching music melodies based on their continuous dominant fundamental frequency (f0) contours without obtaining notelevel transcriptions. Each f0 contour is encoded by a redundant set of wavelet coefficients that represent its shape in level-normalized form at various locations and time scales. This allows a query melody to be exhaustively compared with variable-length portions of a target melody at arbitrary locations while accounting for differences in key and tempo. The method is applied in a Query-by-Humming (QBH) system where users may search a database of recorded pop songs by humming or singing an arbitrary part of the melody of an intended song. The system has fast retrieval times because the wavelet coefficients can be effectively indexed in a binary tree and a vector distance measure instead of dynamic programming is used for comparisons. Using automatic pitch extraction to obtain all f0 contours from acoustic data, the method demonstrates practical performance in an experiment with an existing monophonic data set and in a preliminary experiment with real-world polyphonic music.",
        "zenodo_id": 1415054,
        "dblp_key": "conf/ismir/JeonMC09",
        "keywords": [
            "indexing",
            "efficiently searching",
            "music melodies",
            "continuous dominant fundamental frequency",
            "wavelet coefficients",
            "level-normalized form",
            "query melody",
            "target melody",
            "arbitrary locations",
            "key and tempo"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAN EFFICIENT SIGNAL-MATCHING APPROACH\nTO MELODY INDEXING AND SEARCH\nUSING CONTINUOUS PITCH CONTOURS AND WA VELETS\nWoojay Jeon, Changxue Ma, and Yan Ming Cheng\nApplied Research and Technology Center\nMotorola, Inc.\nSchaumburg, IL, U.S.A.\n{woojay, Changxue.Ma, fyc002 }@motorola.com\nABSTRACT\nWe describe a method of indexing and efﬁciently searching\nmusic melodies based on their continuous dominant fun-\ndamental frequency (f0) contours without obtaining note-\nlevel transcriptions. Each f0 contour is encoded by a re-\ndundant set of wavelet coefﬁcients that represent its shape\nin level-normalized form at various locations and time sc-\nales. This allows a query melody to be exhaustively com-\npared with variable-length portions of a target melody at\narbitrary locations while accounting for differences in ke y\nand tempo. The method is applied in a Query-by-Humming\n(QBH) system where users may search a database of record-\ned pop songs by humming or singing an arbitrary part of\nthe melody of an intended song. The system has fast re-\ntrieval times because the wavelet coefﬁcients can be ef-\nfectively indexed in a binary tree and a vector distance\nmeasure instead of dynamic programming is used for com-\nparisons. Using automatic pitch extraction to obtain all\nf0 contours from acoustic data, the method demonstrates\npractical performance in an experiment with an existing\nmonophonic data set and in a preliminary experiment with\nreal-world polyphonic music.\n1. INTRODUCTION\nIt has been suggested in the past that using “continuous”\n(or frame-based) pitch contours may result in more robust\nmatches of music melodies [1] compared to using sym-\nbolic string representations (usually note transcription s).\nBoth methods require reliable extraction of the dominant\npitch contour from both query and target for matches to be\nsuccessful, but the latter approach requires an extra tran-\nscription stage of converting the continuous contours to\nsymbolic strings, which can exacerbate the effect of pitch\ntracking errors because it makes hard decisions on note\nboundaries and quantization levels. However, the former\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .approach also has the major drawback of high computa-\ntional complexity, especially when applying string match-\ning techniques to handle differences in tempo and key as\nwell as the well-known insertion, deletion, and substitu-\ntion errors. Piecewise approximations of the contours have\nbeen used for greater efﬁciency [2], but this still requires\nquery and target melodies to have roughly similar tempi.\nAnother problem in melody search is the length and\nlocation of queries within their target songs. Query-by-\nHumming(QBH) applications often limit queries to spe-\nciﬁc music phrases or hooks, hence simplifying the search\nspace, but in other melody search scenarios, the query may\nbe a completely random portion of a song, e.g. a brieﬂy au-\ndible segment of a tune in a TV commercial that the viewer\nwishes to identify.\nIn this study, we present a method that tries to address\nboth issues – the computational complexity when using\ncontinuous pitch contours andallowing the search of par-\ntial melodies at arbitrary locations – by using redundant\nwavelet transformations to index and match pitch contours.\nThe method avoids edit-distance comparisons and instead\nuses distance measures between ﬁxed-dimension vectors\nwhile explicitly resolving tempo and key differences from\nthe very beginning of the search process. This is done by\ndividing target melodies into overlapping, level-normali zed\nsegments over a range of lengths and using wavelets to\nefﬁciently represent the segments and match them with\nqueries. The wavelet coefﬁcients are stored in vectors that\nare in turn indexed in a binary K-D Tree [3] for fast search.\nAlthough rhythmic inconsistencies within queries are ig-\nnored for computational efﬁciency, the results show that in\npractice we can achieve reasonable performance. Search-\ning continuous pitch contours at arbitrary locations was\ntried in the past [4], but computation-intensive dynamic\nprogramming was used for the matching.\nWhile we agree that symbolic melody descriptions are\nthe future for robust melody-matching, with reliable music\nmodeling and transcription methods pending we believe it\nworthwhile to explore the use of continuous pitch contours\nin a somewhat traditional, signal-matching framework that\nis fast enough for practical use.\nIn addition, it is hard to tell from QBH experiments\nusing MIDI target data how well the same system would\n681Poster Session 4\nperform on arbitrary polyphonic music for which the tran-\nscriptions are unavailable and must be extracted automat-\nically and crudely. Assuming perfect note transcriptions\ncould lead to QBH methods that are overly sensitive to\nthe integrity of the transcription and turn out to have littl e\nvalue in such real-world scenarios. Therefore, in this stud y\nwe also conduct a preliminary QBH experiment on “real-\nworld” data, i.e., commercial recordings of polyphonic mu-\nsic from which dominant pitch contours are obtained using\nan automatic f0tracking method.\nWavelets [5] have a rich history of diverse applications\nin the areas of signal coding and matching. In particu-\nlar, they have been used in the past to match whole image\ncontours [6] with robustness to afﬁne transformations, and\nalso to encode f0contours for speaker identiﬁcation [7].\nIn the former case, the wavelet coefﬁcients were used to\nmatch whole contours, while in the latter, to encode the\nf0contour using compact dyadic wavelet coefﬁcients. In\nour study, to match f0contours for the purpose of melody\nmatching, we employ “redundant” sets of wavelets deﬁned\non non-integer scale and time indices to encode segments\nof varying locations and time scales.\nNote that throughout this paper, we conveniently as-\nsume that “main melody” and “dominant pitch contour”\nboth mean “dominant f0contour,” although strictly speak-\ning, all three concepts have subtle differences.\n2. INDEXING VIA REDUNDANT WA VELETS\n2.1 Brief Overview of Wavelets and Notation\n0 0.5 1−101\nt\nFigure 1 . The Haar wavelet, ψ(t)\nIt is well known that a real, continuous-time signal x(t)\nmay be decomposed into a linear combination of a set of\nwavelets that form an orthonormal basis of a Hilbert Space\n[5]. First, we deﬁne a wavelet as\nψm,n(t) = 2−m/2ψ/parenleftbig\n2−mt−n/parenrightbig\n(1)\nform,n∈ R (real numbers) where mis a dilation factor, n\nis a displacement factor, and ψ(t)is some mother wavelet\nfunction. In this paper, we use the Haar Wavelet in Fig.1.\nIt is easy to see that the support of (1), then, is\nt∈[n2m,(n+ 1) 2m) (2)\nThe corresponding wavelet coefﬁcient of a signal x(t)is\n/an}bracketle{tx(t),ψm,n(t)/an}bracketri}ht=/integraldisplay+∞\n−∞x(t)ψm,n(t)dt (3)\nIt is well known that when m,n are integersj,k∈ Z (inte-\ngers), {ψj,k}form an orthonormal basis and x(t)is a linear2222\n4444\n6666\n8888\n10101010\n12121212\n14141414\n16161616\ntttt\nTTTTf0\nj=0\nm=log2T\nn=k=0\nj=−1\nm=log2T−1\nn=k=0,1\nj=−2\nm=log2T−2\nn=k=0,1,2,3\nFigure 2 . Example query pitch contour q(t)with support\n[0,T)and “dyadic-equivalent” wavelets ψm,nthat corre-\nspond to some of the dyadic wavelets ψj,kofq(Tt). The\nvertical dotted line indicates T. The wavelet amplitudes in\nthe ﬁgure are not plotted to scale.\ncombination of the resulting “dyadic” wavelet coefﬁcients :\nx(t) =/summationdisplay\nj,k∈Z/an}bracketle{tx(t),ψj,k(t)/an}bracketri}htψj,k(t) (4)\nSince signals are often represented by a compact set of co-\nefﬁcients, we can efﬁciently compare real signals using\n/integraldisplay+∞\n−∞{x(t)−y(t)}2dt=/summationdisplay\nj,k∈Z(/an}bracketle{tx,ψj,k/an}bracketri}ht − /an}bracketle{ty,ψj,k/an}bracketri}ht)2\n(5)\nThroughout this paper, we always assume m,n∈ R and\nj,k∈ Z.\n2.2 Application of Wavelets to Pitch Contour\nMatching\nAssume some query f0contourq(t), shown in Fig. 2.\nAlso assume a pitch contour p(t)of a target song, shown\nin Fig. 3 representing the “dominant” f0in a piece of poly-\nphonic music. The query contour closely resembles a por-\ntion of the target contour, and our goal is to locate this\nsegment. Given two contour segments representing iden-\ntical melody, there are two different types of scaling that\nmust be considered before attempting to directly compare\nthem. The ﬁrst one is in frequency, resulting from differ-\nence in musical key, which will cause one contour to be a\nscaled version of the other in the linear frequency domain.\nIn the log-frequency domain, it will be a linear translation .\nThe second scaling is in the time domain, resulting from\ndifference in tempo. Notice that the two example melodies\nare sung at different speeds. The query is about 17 seconds\nlong, while the matching segment in the target is about 12\nseconds long. Both of these issues prevent us from directly\ncomparingp(t)andq(t), and they will now be addressed.\n2.2.1 Key Normalization\nFirst, assume some signal x(t)deﬁned arbitrarily on [0,1)\nand 0 elsewhere. Since ψj,0= 2−j/2in[0,1)whenj >0,\nwe have\n/an}bracketle{tx,ψj,0/an}bracketri}ht= 2−j/2Sx(j >0), Sx/defines/integraldisplay1\n0x(t)dt (6)\n68210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAlso note that /an}bracketle{tx,ψj,k/an}bracketri}ht= 0in[1,0)forj >0andk/ne}ationslash= 0.\nFrom these relations it follows that the wavelet expansion\nofx(t)can be decomposed as follows:\nx(t) =/summationdisplay\nj≤0,k∈Z/an}bracketle{tx,ψj,k/an}bracketri}htψj,k+/summationdisplay\nj>0,k=0/an}bracketle{tx,ψj,k/an}bracketri}htψj,k\n+/summationdisplay\nj>0,k/negationslash=0/an}bracketle{tx,ψj,k/an}bracketri}htψj,k\n=xN(t) +Sx/summationdisplay\nj>0,k=02−j+ 0 =xN(t) +Sx(7)\nwhere we have deﬁned\nxN(t)/defines/summationdisplay\nj≤0,k∈Z/an}bracketle{tx,ψj,k/an}bracketri}htψj,k (8)\nFrom the orthogonality property of the wavelets, and the\nfact thatx(t)is 0 outside of [0,1), note that\n/an}bracketle{txN,ψj,k/an}bracketri}ht=/braceleftbigg/an}bracketle{tx,ψj,k/an}bracketri}ht(j,k)∈ W\n0 all otherj,k(9)\nwhere we deﬁne the set Wof tuplets (j,k)that correspond\nto the dyadic wavelets in [0,1):\nW=/braceleftbig\n(j,k) :j≤0,0≤k≤2−j−1,j∈ Z,k∈ Z/bracerightbig\n(10)\nNow, assume another signal y(t) =x(t) +cin[1,0)and\n0elsewhere. Since Sy=Sx+c, we can see from (7)\nthatyN(t) =xN(t). Hence, for any arbitrary x(t)and\ny(t)on[0,1), we can obtain “level-normalized” signals\nxN(t)andyN(t)that are independent of constant bias. In\nour case, “level” is in fact “key” when x(t)andy(t)are\nlog-frequency pitch contours, since key shifts will result in\nconstant biases. To compute their mean squared distance\nin a “level(key)-normalized” way, we use, instead of (5),\n/integraldisplay+∞\n−∞{xN(t)−yN(t)}2dt=/summationdisplay\nj,k∈W(/an}bracketle{tx,ψ/an}bracketri}ht − /an}bracketle{ty,ψ/an}bracketri}ht)2\n(11)\n2.2.2 Time and Key Normalization of the Query\nAssume that the query signal q(t)is deﬁned arbitrarily in\n[0,T)and 0 elsewhere. The ﬁrst step is to time-scale it\ninto a “time-normalized” signal q′(t)deﬁned on [0,1)and\n0 elsewhere:\nq′(t)/definesq(Tt) (12)\nUsing (3) and (1), it is easy to see that\n/an}bracketle{tq′(t),ψj,k(t)/an}bracketri}ht=T−1/2/an}bracketle{tq(t),ψm,n(t)/an}bracketri}ht\nm=j+ log2T, n=k(j,k∈ Z) (13)\nFig. 2 shows ψm,nfor(j,k)∈ W whenj= 0,−1,and\n−2, which corresponds to m= log2T,−1 + log2T, and\n−2 + log2T, respectively. The wavelets {ψm,n}could be\nregarded as the “dyadic-equivalent” wavelets of q(t)– the\nwavelets applied to q(t)that are equivalent to the dyadic\nwavelets applied to its time-normalized version q′(t).4812162024283236\nttttttttttf0\nm=12\n3\nn=1\n8w\nu=0,v=0\nm=11\n3\nn=1\n8w\nu=1,v=0\nm=10\n3\nn=1\n8w\nu=2,v=0\nm=9\n3\nn=1\n4w\nu=0,v=1\nm=8\n3\nn=1\n4w\nu=1,v=1\nm=7\n3\nn=1\n4w\nu=2,v=1\nm=6\n3\nn=1\n2w\nu=0,v=2\nm=5\n3\nn=1\n2w\nu=1,v=2\nm=4\n3\nn=1\n2w\nu=2,v=2Part of target resembling queryEnd of analysis range\nFigure 3 . Example target pitch contour p(t)and a re-\ndundant set of wavelets with design parameters D= 3,\nM= 12 ,V= 2, andE= 3 encoding the contour at dif-\nferent locations over a range of time scales. The bold bro-\nken line shows the segment resembling the query in Fig.2,\nand the bold lines show the “dyadic-equivalent” wavelets\nthat encode this segment\nNow, if we only compute those wavelet coefﬁcients for\n(j,k)∈ W , we can obtained the key-normalized ,time-\nnormalized signalq′\nN(t). From (9) and (13), we have\n/an}bracketle{tq′\nN,ψj,k/an}bracketri}ht=\n\nT−1/2/an}bracketle{tq,ψm,n/an}bracketri}ht(j,k)∈W\nm=j+log2T,n=k\n0 all otherj,k\n(14)\n2.2.3 Normalization and Redundant Encoding of Targets\nFor the target pitch contour p(t), we do a redundant wavelet\nanalysis so that we can search multiple, overlapping sec-\ntions of varying time scales in p(t). Some sort of regular-\nity must be imposed on the scale factors and analysis in-\ntervals so that the coefﬁcients can be used efﬁciently. Note\nthat there can be many ways to do this, and here we are\nproposing one such method. While we present a general\n683Poster Session 4\nformulation of our design, the easiest way to understand\nthis section is by studying the speciﬁc example in Fig.3.\nWe compute a “redundant” set of wavelet coefﬁcients\n{/an}bracketle{tp,ψm,n/an}bracketri}ht:u,v,w }, where we set\nm=M−u\nD−v, u= 0,1,· · ·,D−1, v= 0,1,· · ·,V\n(15)\nThe constant Drepresents the amount of resolution in the\ntime scales over which the redundant analysis is done. M >\nDrepresents some upper limit in m,uis a time scale fac-\ntor,vis a nonzero integer, and V <M\nDrepresents some\nlower limit in m. For eachm, the possible values of nare\nn=1\n2E−vw, w= 0,1,· · · (16)\nE > V represents the amount of time resolution. Fig. 3\nshows the wavelets with D= 3,M= 12 ,V= 2 and\nE= 3.\nNow, consider the part of p(t)in\nt∈[n02m0,(n0+ 1) 2m0) (17)\nwhich is exactly the support of ψm0,n0by (2). We also\nconstrainm0andn0to conform to (15) and (16):\n\n\nm0=M−u0\nD−v00≤u0≤D−1,0≤v0≤V,\nu0,v0∈ Z\nn0=1\n2Ew0w0≥0,w0∈ Z\n(18)\nThe time-normalized version of this portion of p(t), as-\nsuming zero elsewhere, is\np′(t) =/braceleftbiggp(2m0(t+n0))t∈[0,1)\n0 elsewhere(19)\nIt is easy to see that the corresponding key-normalized,\ntime-normalized signal p′\nN(t)will have wavelet coefﬁcients\n/an}bracketle{tp′\nN,ψj,k/an}bracketri}ht=\n\n2−m0/2/an}bracketle{tp,ψm′,n′/an}bracketri}ht(j,k)∈W\nm′=m0+j\nn′=k+2−jn0\n0 all otherj,k\n(20)\nNow, from (15), (16), and (18) one can see that all coef-\nﬁcients /an}bracketle{tp,ψm′,n′/an}bracketri}htrequired above can always be found in\nthe set of wavelets {/an}bracketle{tp,ψm,n/an}bracketri}ht:u,v,w }up to scale level\nj=v0−V. One can also notice that many wavelet coef-\nﬁcients can be “reused” in the sense that they contribute to\nmore than one contour segment. In the example in Fig.3,\nwe see {ψm′,n′}forj= 0,−1,−2withm0=11\n3and\nn0=14\n8, which encode the section of p(t)that pertains to\nthe queryq(t)in Fig.2.\nUsing the wavelet coefﬁcients in (14) and (20), we can\ncompute the distance between the key- and time-normalized\nqueryq′\nN(t)and target segment p′\nN(t)using (11). The dis-\ntance will be an approximation, since we cannot take the\ncoefﬁcients over the entire set Wbut over a ﬁnite number\nof scale levels that provides sufﬁcient accuracy (e.g., j= 0\ntoj=−4for a 7s pitch contour sampled at 10ms).To account for variations in tempo, we compare seg-\nments over a range of values of m0. Note ﬁrst that if\nthe query and target had the same tempo, we should have\nm0= log2T, which would produce portions of p(t)with\nlengthTin (17), to obtain the most accurate match. Now,\nif we allow the query’s tempo to be as slow as half the tar-\nget’s tempo and as fast as twice the target’s tempo, we can\nletm0vary within the range\n−1 + log2T <m 0<1 + log2T (21)\nwhich results in around 2Ddifferent values of m0accord-\ning to the system design.\n2.2.4 Two-Stage Search of Arbitrary Target Locations\nQuery coefficients \nBinary search \nover K -D tree \nCandidate list \nLinear rescoring \nusing full set of coefficients \nFinal ranked list j=0\nj = -2 j = -1 j = -3\nTarget \nDB \nFigure 4 . Schematic overview of two-stage search. In this\nexample, 7 wavelet coefﬁcients are indexed by a K-D Tree,\nand 15 coefﬁcients are used for the linear rescoring.\nThe variable n0in (19) controls the location of the tar-\nget segment compared with the query. The resolution of\nthe wavelet locations can be controlled to ﬁnd a good com-\npromise between speed and accuracy. For efﬁcient com-\nparison of a query with a large number of targets, the pos-\nsible dyadic-equivalent coefﬁcients embedded in every tar -\nget (i.e., the coefﬁcients in (20)) can be indexed as co-\nordinates in a binary K-D Tree [3] with a ﬁxed number\nof dimensions. Each leaf in the tree coarsely represents\na melodic fragment in the target database. At the ﬁrst\nstage of the search, the query coefﬁcients are appropri-\nately scaled to form a search sphere that is used to ﬁnd tree\nleaves that are spatially close to the query, resulting in a l ist\nof candidate melody fragments. At the second stage, a lin-\near search is conducted over the candidates using a larger\nnumber of coefﬁcients to more accurately compute (11),\nwhich is then used to rank the results as shown in Fig.4.\nIn practice, no more than 31 wavelet coefﬁcients ( j=\n0toj=−4inW) are usually sufﬁcient to represent a\nmelody segment with length 7s sampled every 10ms. In\nsuch a case, the ﬁrst 7 coefﬁcients ( j= 0 toj=−2\ninW) can be indexed in the K-D tree, while the full 31\ncoefﬁcients are used in the linear rescoring stage.\nIn terms of computational complexity, if dynamic pro-\ngramming(DP) were used to search for a query of length\nLq[frames] in a target of length Lt[frames], scores would\nhave to be calculated for LqLtcoordinates (assuming no\n68410th International Society for Music Information Retrieval Conference (ISMIR 2009)\npruning). If a linear search were used with the proposed\nmethod, we would need to compute only kLt(k << L q)\nvector distances, where kis essentially a constant since the\nnumber of wavelet coefﬁcients (as in Fig.3) increases lin-\nearly withLt. The addition of a K-D tree further reduces\nthis number drastically, making the computational gains of\nthe proposed method even more apparent.\n3. EXPERIMENT\n3.1 Pitch Contour Extraction\nTime (s)Frequency (Hz)\n24 25 26 27 2810020040080016003200\n(a) Log-frequency spectrum\n24 25 26 27 28100150200300400600800\nTime (s)Frequency (Hz)\nSuddenly      I’m    ...  man I   ...   to be                           th.. −dow  ...  ov−   er me\n(b) Dominant f0contour\nFigure 5 . (a) Log magnitude of log-frequency spectrum\n(dark is high) from 82.4 Hz to 3.84 kHz of a segment of\nYesterday byThe Beatles . Frequency components of both\nvoice and instrumental accompaniment are clearly visi-\nble. (b) Pitch contour of segment with hand-marked note\nboundaries (broken vertical lines) and corresponding lyri cs\nin select locations (full lyrics are “Suddenly, I’m not half\nthe man I used to be, there’s a shadow hanging over me”)\nA simple method based on known techniques was used\nto obtain dominant f0contours from music recordings. The\nConstant-Q Transform [8] of each music signal was taken\nto obtain spectral components on a log-frequency scale.\nFig.5(a) shows the spectrogram for a segment of Yesterday\nbyThe Beatles . Next, we assigned scores for each (t,f)\non the time-frequency plane by computing weighted sums\nof the spectral components at harmonics of f[9]. After\nlimiting the range of the dominant pitch via some heuris-\ntics, we applied dynamic programming on the t−fplane\nof scores to obtain a continuous pitch contour [10] that\nmaximizes the sum of pitch scores along its path. Fig.5(b)\nshows the pitch contour obtained for the Yesterday exam-ple. While the overall structure of the contour reﬂects the\nvocal melody of this part of the song, we can notice that in\nthe non-vocal sections between “Suddenly” and “I’m” and\nbetween “to be” and “there’s”, the dynamic programming\npicked up the pitch of the strings in the background. In-\nﬂections in vocal pitch inevitably produced during singing ,\nand other minor deviations from what is probably the “true”\nmusic score are also reﬂected in the continuous contour.\nHowever, we made no attempt to identify and compensate\nfor any such deviations or discriminate between vocal and\nnon-vocal sections, and directly used the whole pitch con-\ntour from every target in the database in our experiments.\n3.2 QBH Test\n0 10 20 30 40657075808590\n(a) MIREX 2006 test set0 10 20 30 40707580859095\n(b) “Real-world” test set\nFigure 6 . Search performance for (a) MIREX 2006 test set\nand (b) “real-world” test set. The vertical axis represents\nthe inclusion rate(%), and the horizontal axis is the number\nof search results.\nTwo experiments were conducted: the ﬁrst on mono-\nphonic music to validate our method with existing QBH\ntasks, and the second on polyphonic music to make a pre-\nliminary assessment of its use in real-world scenarios. For\nboth experiments, the dominant f0contour was automati-\ncally extracted from query and target data using the afore-\nmentioned method. Contours were sampled every 10ms.\nFor the ﬁrst experiment, we used the MIREX 2006 QB-\nSH test set (see description in [2]). All target data in this s et\nare monophonic MIDI data, so we ﬁrst converted them to\nWA V format. Each song in the database was 29.9s long on\naverage (17 hours total for the database of 2,048 songs).\nFig.6(a) shows the inclusion rate for varying number of\nsearch results, i.e., the rate at which the correct melody\nwas ranked within the top nof all returned results. For\nn= 20 , the inclusion rate was 84.9%, which is signif-\nicantly lower than the state-of-the-art [2], 96.4%. Note,\nhowever, that the latter system constrained the queries to\noccur at only the beginning of music phrases. Since al-\nmost all queries in the MIREX 2006 test set start at the\nbeginning of their targets, such a data set would greatly\nfavor systems with such constraints. Our proposed sys-\ntem, on the other hand, made no assumptions on starting\nlocations and exhaustively searched over all possible loca -\ntions, limited only by the wavelet parameters. Also, tempo\nvariation is taken into account from the very beginning of\nthe search, not just at the latter ﬁne search stage. Hence,\n685Poster Session 4\nthe search space was larger, which resulted in more room\nfor confusion. At the same time, the search time for each\nquery was usually less than one second on a 3.2GHz pro-\ncessor depending on system parameters.\nFor the second experiment, we used a “real-world” data-\nbase consisting of 613 acoustic recordings of songs with\ninstrumental accompaniment, totaling around 37 hours of\naudio (average 3.6 minutes per song). 155 of the songs\nwere from the RWC Music Database [11], and the rest\nwere commercially-distributed pop songs. A preliminary\nset of queries were obtained from six non-professional sing -\ners – three male, and three female. Each person was asked\nto sing several easy and well-known songs including “Ha-\nppy Birthday,” “The Alphabet Song,” and “Are You Sleep-\ning, Brother John?” from which query segments at random\nlocations were extracted. Each query was 5 ∼12 seconds\nlong, and there were a total 50 queries. We informally\nveriﬁed that the songs in the target database correspond-\ning to these queries had reasonably clear dominant f0’s,\nbut there were still noticeable errors in the f0extraction\ndue to instrumental accompaniment, like in the example in\nFig. 5(b). Fig. 6(b) shows the inclusion rate for a varying\nnumber of search results. The inclusion rate was 86% for\nn= 5and 88% for n= 20 , which seems similar to that of\nanother state-of-the-art system [12] that also allows quer ies\nto begin at random locations but uses a MIDI database. We\nare cautious in directly comparing the performance of the\ntwo systems, however, because they differ in experimental\nsetup. Nevertheless, our results are promising because we\nused a database of polyphonic recordings instead of MIDI\ndata. Larger data sets and larger numbers of queries will\nhave to be used in the future to more rigorously assess real-\nworld performance.\n4. CONCLUSION AND FUTURE WORK\nWe have proposed an efﬁcient method of indexing and mat-\nching music melodies based on their continuous pitch con-\ntours while allowing partial matches at arbitrary location s\nusing redundant wavelet transformations. By directly com-\nparing continuous pitch contours instead of their note tran -\nscriptions as in most existing methods, we avoid the com-\npounding of transcription errors. On the other hand, our\nmethod is also computationally efﬁcient because it uses\nthe mean squared sum between ﬁxed vectors instead of dy-\nnamic programming, while at the same time being able to\nadjust for differences in tempo and key. Experiments were\nconducted on both existing monophonic MIDI databases\nand preliminarily on real-world recordings with instrumen -\ntal accompaniment to show that the system can be prac-\ntically applied, even when using a simple mean squared\ndistance measure between key- and time-normalized con-\ntour segments. While the system still depends on reliable\ndominant pitch extraction, minor pitch tracking errors did\nnot hurt performance because the overall pitch and rhythm\nstructure of contours was compared. One trade-off for the\nsystem’s efﬁciency is that it does not explicitly account fo r\nrhythmic variations within queries as do techniques based\non string-matching. Much work is being done in the MIRcommunity toward model-based symbolic representations\nthat allow a more modular framework for indexing and\nsearch, such as via HMMs, and we plan to leverage the\ninsights gained in our work to this end.\n5. ACKNOWLEDGMENTS\nThanks to Lei Wang for kindly sharing the data for the\nMIREX 2006 QBH experiment used in [2].\n6. REFERENCES\n[1] D. Mazzoni and R. B. Dannenberg. Melody matching\ndirectly from audio. In Proc. ISMIR , 2001.\n[2] L. Wang, S. Huang, S. Hu, J. Liang, and B. Xu. Im-\nproving searching speed and accuracy of query by\nhumming system based on three methods: Feature fu-\nsion, candidates set reduction and multiple similar-\nity measurement rescoring. In Proc. INTERSPEECH ,\npages 2024–2027, 2008.\n[3] J. L. Bentley. Multidimensional binary search trees\nused for associative searching. Comm. ACM , 18, 1975.\n[4] L. Guo, X. He, Y . Zhang, and Y . Lu. Content-based re-\ntrieval of polyphonic music objects using pitch contour.\nInIEEE Int. Conf. Acoust., Speech. Signal Processing ,\npages 2205–2208, 2008.\n[5] I. Daubechies. Ten Lectures on Wavelets . SIAM: Soci-\nety for Industrial and Applied Mathematics, 1992.\n[6] Q. M. Tieng and W. W. Boles. Complex daubechies\nwavelet based afﬁne invariant representation for object\nrecognition. In IEEE ICIP , pages 198–202, 1994.\n[7] F. Farahani, P.G. Georgiou, and S.S. Narayanan.\nSpeaker identiﬁcation using supra-segmental pitch pat-\ntern dynamics. In IEEE Int. Conf. Acoust., Speech. Sig-\nnal Processing , 2004.\n[8] J. C. Brown and M. S. Puckette. An efﬁcient algo-\nrithm for the calculation of a constant Q transform.\nIEEE Trans. Audio, Speech, and Language Processing ,\n92:2698–2701, 1992.\n[9] D. J. Hermes. Measurement of pitch by subharmonic\nsummation. J. Acoust. Soc. Am. , 83(1):257–264, 1988.\n[10] B. Secrest and G. Doddington. An integrated pitch\ntracking algorithm for speech systems. In IEEE Int.\nConf. Acoust., Speech. Signal Processing , April 1983.\n[11] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.\nRWC music database: Popular, classical, and jazz mu-\nsic databases. In Proc. ISMIR , pages 287–288, 2002.\n[12] E. Unal, E. Chew, P.G. Georgiou, and S.S. Narayanan.\nChallenging uncertainty in query by humming sys-\ntems: A ﬁngerprinting approach. IEEE Trans. ASLP ,\n16, 2008.\n686"
    },
    {
        "title": "Motive Identification in 22 Folksong Corpora Using Dynamic Time Warping and Self Organizing Maps.",
        "author": [
            "Zoltán Juhász"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416216",
        "url": "https://doi.org/10.5281/zenodo.1416216",
        "ee": "https://zenodo.org/records/1416216/files/Juhasz09.pdf",
        "abstract": "A system for automatic motive identification of large folksong corpora is described in this article. The method is based on a dynamic time warping algorithm determining inherent repeating elements of the melodies and a self-organizing map that learns the most typical motive contours. Using this system, the typical motive collections of 22 cultures in Eurasia have been determined, and another great common self organising map has been trained by the unified collection of the national/areal motive collections. The analysis of the overlaps of the national-areal excitations on the common map allowed us to draw a graph of connections, which shows two main distinct groups, according to the geographical distribution.",
        "zenodo_id": 1416216,
        "dblp_key": "conf/ismir/Juhasz09",
        "keywords": [
            "automatic motive identification",
            "large folksong corpora",
            "dynamic time warping algorithm",
            "melodies",
            "self-organizing map",
            "motive collections",
            "Eurasia",
            "cultures",
            "unified collection",
            "national/areal motive collections"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   MOTIVE IDENTIFICATION IN 22 FOLKSONG CORPORA USING DYNAMIC TIME WARPING AND SELF ORGANIZING MAPS  Zoltán Juhász   Research Institute for Technical Physics and Materials Science. P.O.B 49. Budapest H-1525.  juhasz@mfa.kfki.   ABSTRACT A system for automatic motive identification of large folksong corpora is described in this article. The method is based on a dynamic time warping algorithm determin-ing inherent repeating elements of the melodies and a self-organizing map that learns the most typical motive contours. Using this system, the typical motive collec-tions of 22 cultures in Eurasia have been determined, and another great common self organising map has been trained by the unified collection of the national/areal mo-tive collections. The analysis of the overlaps of the national-areal excitations on the common map allowed us to draw a graph of connections, which shows two main distinct groups, according to the geographical distribu-tion.   1. INTRODUCTION In order to study interethnic and historical relations, Bartók and Kodály compared different layers of Hungar-ian folk music to those of other nations living in the neighborhood of Hungarians. Later, they extended the study to Anatolian, Mari and Chuvash folk music [1-2]. These exciting results raise the question, whether it is possible to describe a whole and clear system of musical contacts in Eurasia by a systematic comparison of a suffi-cient number of national or regional cultures.   A further question, raised by the classical results men-tioned above, refers just to the method of the analysis. The aim of these classical works was to find parallelism of entire melody structures. The similarity of whole mel-ody contours seems to be really a sufficient condition to find genetic musical relations [1-3]. However, the question rises: do less rigorous requirements also exist? Instead of comparing the complete melody structures, our aim was to find and analyse the smallest independent me-lodic units. It is well known that folksongs can usually be divided into certain phrases on the basis of musical and textual regularities. In a previous work, we have shown some results comparing individual phrases, as well as whole melodies of 6 European cultures [4].    The idea of a motive identification algorithm can be de-rived from the recognition that phrases are not necessarily the smallest intelligible units in folk music. We want to find the most frequently appearing motive types in a well defined melody corpus, with the assumption that each motive type may have several variants. However, the rep-etition inside a melody can also be considered as an indi-cation of a motive. Therefore, we suppose two possible detections of the motives. In addition to the “culture-defined” motive identification, based on the frequent ap-pearance in different songs, we also suppose the existence of the “melody-defined” identification which is based on the repetitions inside the melodies.   The central problem of algorithmic melody pattern identi-fication is the musical relevance of the results [5]. The most frequently applied melody segmentation techniques can be divided into two main groups. In the first group, segmenting is based on pre-defined and data-independent rules [6-8]. Using such rules, the so-called Local Boun-dary Detection Model (LBDM) determines a boundary strength value between each couple of notes, and deter-mines the segment boundaries at the maximal strength values [6,9]. Due to the requirement of pre-defined rules, such methods are not available for the sake of a learning system. The second group of segmenting techniques is based on a learning process to determine the regularities of a given melody corpus. Such regularities can be char-acterised by the frequencies or conditional probabilities of the motives [10-12]. The so called Markov technique operating with conditional probabilities has already been applied to folk songs [13-14]. A further data-based self learning method for segmenting a large corpus of folk-songs has been also described, which determines the con-ditional entropy of the motives and defines an average entropy increment value for a given segmentation [15]. A method based on knowledge representation has been el-aborated for identifying recurrent melody parts in large folksong corpora [16].   The learning unit of the system described in this paper is a self organising map (SOM), trained by the contour functions of the motives [17-18]. The motive identifica-tion in a given melody is accomplished in two steps. Firstly we determine the repeating elements of the melody by an algorithm based on dynamic time warping (DTW). After that, the remaining melody parts are ana-lysed using a self organizing map, which learns and iden-tifies the most frequently appearing patterns as “culture-defined” motives.  Our current possibilities allowed us to set up 22 folksong corpora, each of them consisting of 600-2400 melodies, representing Hungarian, Slovak, Moravian, Chinese, \n171Poster Session 1\n   Mongolian, Kyrgyz, Mari-Chuvash-Tatar, Karachay-Balkar, Anatolian Turkish, Azeri, Sicilian, Spanish, Ru-manian, Bulgarian, Polish - Cassubian, Finnish, Norwe-gian, German, Luxembourgian-Lotharingian, French, Dutch and Irish-Scottish-English  musical traditions. In order to make an unbiased and general analysis, these nearly 40 000 melodies were transposed to the common final tone G automatically in the analysis.  2. DETERMINATION  OF MOTIVES DEFINED BY REPETITION WITHIN MELODIES  To search for essentially identical, but not completely uniform motives inside melodies, we developed an algor-ithm based on dynamic time warping technique [17]. The operation of the algorithm is illustrated in Figures 1 and 2.  In the first step, the contour vectors   of the melodies are generated in the way demonstrated in Figure 1. The time duration of the kth melody is divided into small units according to the rhythmic value of 1/16, and the pitch values belonging to these subsequent small time intervals are stored in a multidimensional vector .    Figure 1. Generation of the contour vector .  The original aim of a DTW process is to determine a non-negative scalar number characterising the difference of two vectors. In order to calculate this DTW-distance be-tween melody contours  and , the matrix  is gener-ated containing the deviations of the nth and mth pitch samples of the vectors   and :    ,                (1) where  and are the dimensions of  and  re-spectively. Figure 2 shows an example of the above cal-culation for the contour vectors demonstrated by the dia-grams on the left side and the bottom of the matrix.   The zero elements of the matrix  marked by bold italic characters indicate local warping curves assigning similar parts of the two vectors to each other. Our algorithm is based right on this recognition: instead of determining the total DTW distance of the vectors, we search for such lo-cal warping paths in matrix . To do this, the partial time warping distances  are calculated, according to the dynamic time warping process:    Figure 2. Generation of the partial deviation matrix , and the path of 0 elements indicating the relation between corresponding motives.                  (2)  The original DTW algorithm produces the final distance at the end of the above recursive calculation as . The local warping paths can be determined using the dimensional matrix . Since the elements of the matrix  cannot be defined for negative indices, the al-gorithm starts with the values of ,  and the initial values of   are  and .   The overall similarity of the vectors can be characterised by the summed length of the similar sub-sequences com-pared to the sum of the total length of the vec-tors. Thus, our technique can characterise the similarity of two different contour vectors by a scalar number ranging between 0 and 1. This similarity measure ignores the order of the motives, in contrast to the origi-nal DTW and the Euclidean distances. Therefore it is able to detect the relationship even if the successions of the characteristic melody parts are different in the compared melodies.   Example 1 shows two couples of melodies arising from different cultures, with a significant amount of similar parts found by the above described method. For instance, the first, second and fourth phrases of the Hungarian song in the first example are practically identical to the second and fourth phrases of the corresponding Appalachian melody, and the third phrase of the Appalachian song ap-pears as a dominant part of the corresponding Hungarian phrase, too. Due to these local correspondences, the melodies are found to be similar, in spite of the difference \n17210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   between the domed, as well as descending character of the two melodies.   The above technique can be applied also to identical vec-tors (i.e.). In such cases, the trivial result that the whole melody is identical to itself is indicated by the zero elements of the diagonal, but the partial warping paths marked by zero matrix elements indicate the similar sub-sequences. Therefore, our technique is also able to find similar parts within one given melody (see Figure 3).   \n  Figure 3.  Application of the DTW technique to search for repeated parts in a melody.   The technique can be generalized for not exactly identical pitch values, too, by the extension of the search for paths of small elements in matrix . Some results of the method are shown in Example 2.    \n  Example 1.  Common motives of related melodies arising from different cultures.  3. THE COMPLEX MOTIVE IDENTIFICATION ALGORITHM  In addition to the melody-based motive identification, we also need a technique for the culture-defined identifica-tion which was defined as the determination of those melody parts which frequently appear in a whole national/areal database. While the melody-based tech-nique needs the analysis of one given melody, the cul-ture-based identification requires a self learning process \n  Example 2. Melody-defined and culture-defined motives in 4 folksongs.  analysing the whole database simultaneously. In order to solve this problem, i.e. to identify the most frequent melody parts automatically, we developed a system based on a self organising map, as it is shown in Figure 4.  \n  Figure 4. The complex motive identification system.  The input to the algorithm is a melody selected randomly from the database. At the beginning of the process, the  dimensional motive type contour vectors assigned to the lattice points of the SOM,  are filled by random numbers. The choice of  proved to be sufficient for our database.   The processing is done by the following steps:   1. In the first step, all melody-defined motives of a melody are determined, using the melody-based identifi-cation algorithm.  2. All possible motives of the remaining parts of the melody are determined. The time duration of each pos-sible motive is divided into  parts, and the pitch values belonging to the subsequent time intervals are stored in a vector of dimensionality . This operation has been discussed in reference to melody contour generation (see Figure 1), but it is worth mentioning here an important \n173Poster Session 1\n   difference: When generating motive contour vectors, the vector dimension  is a pre-defined constant, while it is variable for melody vector generation, because the sam-pling time unit is pre-defined in this latter case.   3. The optimal motives are identified on the basis of the current estimates of the most typical motive types as-signed to the lattice points of the SOM. Let  denote the contour vector belonging to the kth possible motive, and the current estimate of the motive contour type belonging to the lattice point with the coordinates. The motive contour vector  is assigned to the most similar motive contour type vector of the SOM:   ,                                                       (3)    where the similarity measure  is the Euclidean distance between the  and .                                   (4)  Finally, the culture-based motives are defined using the following constraints:  - The distance of the motive and the corresponding mo-tive type must be less than a critical value. - The culture-defined motives are defined as the longest melody parts satisfying the above requirement. - The culture-defined motives should not overlap with melody-defined motives. Melody-defined motives have priority.  4. The SOM is trained with the resulting set of culture-defined and melody-defined motives, using the well known algorithm. Each  vector determines a “winner” motive type contour on the SOM according to Equation 3, and the winner vectors  are modified towards the corresponding motive contour (denoting a winner posi-tion by  on the SOM). The motive type vectors lo-cated in the surroundings of a winner are also modified, while the radius defining the surroundings decreases dur-ing the training steps [17].   The input data vectors are usually invariable during the training process of self organising maps. In our system, however, they are variable, because the optimal culture-based motive identification depends on the current state of the motive type vectors  (see Equations 3 and 4). Since  are modified during the learning process, the optimal segmentation itself depends on the current state of the SOM. In other words, there exists a feedback be-tween the segmentation and the learning algorithm, thus, our system converges to an optimal training- and feature vector set in parallel. The results of many independent training processes verified that all of the characteristic motive contour types have been learned consistently and independently of the starting conditions of the SOM-s.  4. ANALYSYS OF THE CULTURAL CONTACTS AMONG 22 CORPORA  Let suppose that we can create a whole collection of mo-tive contour types, containing all the significant contours that appear in any of the 22 cultures. It is obvious that the national/areal sets of motive types can be considered as different subsets of this great common collection, there-fore the study of musical connection between different cultures can be determined by the analysis of the intersec-tions of these subsets.   Being in possession of the size of the great common mo-tive contour type collection (N), the sizes of its two na-tional/areal subsets (A and B), as well as the size of their intersection (X), the measure of the relationship between these cultures can be expressed by a probability as fol-lows.   As a first step we compute the probability of the event that a random choice of two subsets with sizes A and B from the set of size N results in an intersection of size x, as .                                          (5)  Using this probability density function, the probability of the event that the size of the intersection is less than X, is expressed as  (6).                                                                 A high value of this probability indicates that the number of common contour types in the two corpora is much higher than the expected value in case of random correla-tions. Consequently the similarity, manifested by such high intersection of two corpora, cannot be a product of occasional coincidences of independent musical evolutions. It can be stated in such cases of similarity that the common musical characteristics implicate a historical or present, immediate or intermediate cultural interaction, that is, the established relationship is necessarily determi-nistic.  To construct the above mentioned sets, we first had to deduce the characteristic motive contour type collections for eash of the 22, by training 22 SOM-s of size 20*20 lattice points separately. After determining the 22 na-tional/areal motive contour type collections, a new large self organizing map of size 30*30 was trained by the \n17410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   united set of them, in order to determine the set of all possible motive contour types appearing anywhere in the 22 cultures.   This common SOM allows us to classify all motive types of a given national/areal collection on it using Equations 3 and 4. We call this process “excitation of the common map by a culture”. The values A, B and X can be deter-mined for any selected two cultures by counting up the lattice points excited in the great common SOM. With these quantities, the calculation of the probability  can be carried out using Equations 5 and 6, knowing that N is equal to the total number of the common contour types. It is worth mentioning here that this calculation avoids the problems arising from the different sizes of the corpora, since the expected intersection decreases with decreasing subset sizes A and B.   The graph of the system of closest relationships is sum-marized in Figure 5, where a connection line indicates a high probability () of deterministic contact between the nodes of musical cultures. The Figure shows two main sub-graphs containing an “Eastern” - Mongo-lian, Chinese, Volga,  Hungarian, Slovak, Moravian, Spanish, Kyrgyz, Romanian, Bulgarian, Azeri, Sicilian, Turkish and Karachay-Balkar, as well as a “Western” - Finnish, Norwegian, Irish-Scottish-English, French, German, Dutch, Luxembourgish and Cassubian – group of nodes. There are some interconnections between these two large sets due to the close connections of the Hungar-ian – Slovak – Finnish – (Irish-Scottish-English), and the Moravian - Norwegian corpora. Besides these close con-tacts of the Carpathian Basin to the Scandinavian and Irish-Scottish-English cultures, the Irish-Scottish-English and Norwegian corpora have certain further Eastern con-tacts to the Volga-region and Kyrgyzstan. Anyhow, the connection of the two main subsystems indicates a spe-cial role of the above mentioned cultures inside their main groups and also in the whole system.   The structure of the graph indicates certain smaller groups inside the great “Eastern” system. The majority of the motives belonging to the large pattern excited by the Mongolian, Chinese and Volga group on the common SOM move in the highest regions of the melodies – they start or end at the octave or higher notes (See the Mongo-lian motive contour type in Figure 4). The visible over-laps of the patterns of the Hungarian, Slovak, Karachay-Balkar, Turkish and Sicilian excitations with the above mentioned triad are based mainly on the above mentioned motives in the highest region of the melodies.   The patterns of the Irish-Scottish-English, Finnish and Norwegian excitations also indicate an important role of such motives, resulting in the deterministic contacts of these cultures to the Carpathian Basin and the Volga-region. However, this “Eastern” part of the common mo-tive type map empties in the further Western patterns.  The French and Dutch contour examples show that the most common Western motive types move in the lowest \n  Figure 5. The graph of deterministic relations of 22 musi-cal cultures in Eurasia.   ranges of the melodies, starting or ending at a fourth or fifth below the ending note.   The cloud of the high motives also disappears gradually along the branch of the Spanish – Kyrgyz – Romanian – Bulgarian – Azeri excitations, while the pattern on the left side of the motive type map becomes more and more emphasized. The Azeri motive example illustrates that the motive types belonging to this part of the map are of low ambit, ranging between the fourth, third or the sec-ond. The Sicilian, Turkish and  Karachay-Balkar excit-ations show that these cultures also frequently apply such motive types, (beneath the above mentioned group of mo-tives in high), indicating deterministic cultural contacts between the two branches. However, the group of these low-ambit motives practically misses in the Mongolian-Chinese-Volga branch, and it is also rather rare in the Hungarian, Slovak and Moravian melodies. Therefore, these cultures have no direct connections to the Spanish-Kyrgyz-Romanian-Bulgarian-Azeri branch.   SUMMARY \n175Poster Session 1\n   The very clear connections between the patterns of the different national/regional excitations on the common motive type map allowed us to analyze the musical struc-tures of different cultures as different manifestations of a common motive set, and led to the conclusion that the main contacts between the cultures can be explained by the dominance/lack of a few motive types. This analysis clarified that “Eastern” cultures prefer motives in high regions of the melody, generally moving between the oc-tave and the fifth as well as fourth, while the “Western” melodies prefer motives connecting the tonic to a fifth or a fourth beyond the tonic. The combined analysis of the contact probabilities and the overlaps of the national/areal patterns indicated several distinguishable branches among the Eastern cultures. The Mongolian-Chinese-Volga branch highly prefers motives in high, while the Sicilian-Turkish-Karachay branch evaluates a balance between these high motives and those of an explicitly low ambit. The close contacts of Hungarian, Slovak and Moravian cultures to these two distinguishable branches are based mainly on the high motive types. At the same time, the high motive types gradually disappear in the Spanish-Kyrgyz-Romanian-Bulgarian-Azeri branch, while the dominance of motives of low ambit connects them to the Sicilian-Turkish-Karachay branch.   Not forgetting the simplifications made during the appli-cation of our technique, we can state that the motive analysis allowed us to draw a rather perspicuous picture of the cross-cultural connections of different folksong cultures. We hope that these results may demonstrate the feasibility of an extended research of “musical linguis-tics”, and suggest an efficient and quantitative tool for “melody mining”, using artificial intelligence and other mathematical tools.   Acknowledgement. The Author is grateful to Ewa Dah-lig-Turek, Damien Sagrillo, Louis Grijp, Hans-Hinrich Thedens, János Sipos and Gergely Agócs for their altru-istic help to extend the database with Cassubian, Luxem-bourgian, Lotharingian, Dutch, Norwegian, Anatolian, Kyrgyz, Mongolian, Azeri and Karachay-Balkar collec-tions.  References  [1] B. Bartók: “On Collecting Folk Songs in Turkey”  Tempo, New Ser., No. 13, Bartok Number (Autumn, 1949), pp. 15-19+38  [2] Z. Kodály.: “Folk Music of Hungary”. Budapest, Corvina.   [3] D. Huron: “The melodic arch in Western folksongs”. Computing in Musicology, Vol. 10, pp. 3-23.  [4] Z. Juhász: “A systematic comparison of different European folk music traditions using self-organising maps”. Journal of New Music Research 2006, Vol. 35, No. 2, pp 95-112.  [5] O. Lartillot and P. Toiviainen. (2007). \"Motivic matching strategies for automated pattern extraction\", Musicae Scientiae, Discussion Forum 4A, pp. 281-314.  [6] E. Cambouropoulos : « Musical Parallelism and Me-lodic Segmentation”, Proceedings XII Colloquium on Musical Informatics, Gorizia, Italy   [7] D. Halperin: “A Segmentation Algorithm and its Ap-plication to Medieval Monophonic Music”.  Musiko-metrika 2 (1990)  [8] J. Singer: “Creating a nested melodic representation: competition and cooperation among bottom-up and top-down Gestalt principles”. ISMIR 2004  [9] E. Cambouropoulos: “A Formal Theory for the Dis-covery of Local Boundaries in a Melodic Surface”. Pro-ceedings of the Troisiémes Journées d’Informatique Mu-sicale (JIM-96), Caen, France  [10] E. Charniak: “Tree-bank Grammars”, Pproceedings AAAI-96 Menlo Park, Ca  [11] E. Charniak: “A Maximum-Entropy-Inspired Parser”. Proceedings ANLP-NAACL’2000, Seattle, Washington   [12] S. Seneff: “TINA: A Natural Language System for Spoken Language Applications”. Computational Lin-guistics 18(1), 61-86.  [13] R. Bod: “Memory-Based Models of Melodic Analy-sis: Challenging the Gestalt Principles”. Journal of New Music Research 31 (2002)  27-37.  [14] R. Bod: “Probabilistic Grammars for Music Proceed-ings” BNAIC 2001, Amsterdam  [15] Z. Juhász: “Segmentation of Hungarian Folk Songs Using an Entropy-Based Learning System”. Journal of New Music Research 33 (2004) No 1, (pp 5-15).  [16] D. Conklin, Ch. Anagnostopoulou: “Segmental Pat-tern Discovery in Music”, Informs Journal on Comput-ing, Vol. 18, No. 3, Summer 2006, pp. 285-293 DOI: 10.1287/ijoc.1040.0122  [17] T. Kohonen: „Self-organising Maps“. Ber-lin:Springer-Verlag  [18] P. Toiviainen: “Symbolic AI Versus Connectionism in Music Research”. In E. Mirinda (Ed.), Readings in Music and Artificial Intelligence. Amsterdam: Harwood Academic Publishers (2000). \n176"
    },
    {
        "title": "SongExplorer: A Tabletop Application for Exploring Large Collections of Songs.",
        "author": [
            "Carles Fernandes Julià",
            "Sergi Jordà"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418177",
        "url": "https://doi.org/10.5281/zenodo.1418177",
        "ee": "https://zenodo.org/records/1418177/files/JuliaJ09.pdf",
        "abstract": "This paper presents SongExplorer, a system for the exploration of large music collections on tabletop interfaces. SongExplorer addresses the problem of finding new interesting songs on large music databases, from an interaction design perspective. Using high level descriptors of musical songs, SongExplorer creates a coherent 2D map based on similarity, in which neighboring songs tend to be more similar. All songs are represented as throbbing circles that highlight their more relevant high-level properties, and the resulting music map is browseable and zoomable by the users who can use their fingers as well as specially designed tangible pucks, for helping them to find interesting music, independently of their previous knowledge of the collection. SongExplorer also offers basic player capabilities, allowing the users to organize the songs they have just discovered into playlists which can be manipulated as well as played and displayed. In this paper, the system hardware, software and interaction design are explained, and the usability tests carried are presented. Finally, conclusions and future work are discussed.",
        "zenodo_id": 1418177,
        "dblp_key": "conf/ismir/JuliaJ09",
        "keywords": [
            "SongExplorer",
            "large music collections",
            "tabletop interfaces",
            "finding new interesting songs",
            "interaction design",
            "musical songs",
            "coherent 2D map",
            "high-level descriptors",
            "throbbing circles",
            "browseable and zoomable"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSONGEXPLORER: A TABLETOP APPLICATION FOR EXPLORING\nLARGE COLLECTIONS OF SONGS\nCarles F. Juli `a, Sergi Jord `a\nMusic Technology Group\nUniversitat Pompeu Fabra, Barcelona, Spain\n{carles.fernandez, sergi.jorda }@upf.edu\nABSTRACT\nThis paper presents SongExplorer, a system for the ex-\nploration of large music collections on tabletop interfaces.\nSongExplorer addresses the problem of ﬁnding new inter-\nesting songs on large music databases, from an interaction\ndesign perspective. Using high level descriptors of musi-\ncal songs, SongExplorer creates a coherent 2D map based\non similarity, in which neighboring songs tend to be more\nsimilar. All songs are represented as throbbing circles that\nhighlight their more relevant high-level properties, and the\nresulting music map is browseable and zoomable by the\nusers who can use their ﬁngers as well as specially de-\nsigned tangible pucks, for helping them to ﬁnd interesting\nmusic, independently of their previous knowledge of the\ncollection. SongExplorer also offers basic player capabili-\nties, allowing the users to organize the songs they have just\ndiscovered into playlists which can be manipulated as well\nas played and displayed. In this paper, the system hard-\nware, software and interaction design are explained, and\nthe usability tests carried are presented. Finally, conclu-\nsions and future work are discussed.\n1. INTRODUCTION\nSince the popularization of the Internet and broadband con-\nnections, the amount of music which we are exposed to,\nhas been increasing permanently. Nowadays, many web-\nsites do offer very large collections of music to the user,\neither free of charge (e.g. Magnatune1, Jamendo2) or on\na fee-paying basis (e.g. iTunes3, The Orchard4). Such a\nnumber of available and still undiscovered music records\nand songs seems too difﬁcult to manage in a sorting and\nsearching-by-keyword way. In order to solve this problem\nand help users to discover new music, many online mu-\nsic recommendation services have been created (e.g. Pan-\n1http://www.margatune.com\n2http://www.jamendo.com\n3http://www.apple.com/itunes/\n4http://www.theorchard.com\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.dora5, Last.fm6). One of the main drawbacks of most\ncurrent music recommenders, independently of the recom-\nmendation mechanisms and algorithms they employ (user\nproﬁling, experts-based knowledge, statistical models, etc.),\nis that they apply information ﬁltering techniques to the en-\ntire collections, in order to extract and display only a subset\nof songs that the system believes the user could enjoy. By\ndoing it this way, the user loses the opportunity to discover\nmany new songs which are not presented by the system,\nwhatever the cause may be.\nTo solve this problem, we propose to construct maps\nof the entire collections of songs and allow users to ex-\nplore them in novel ways. Maps are widely used to explore\nspaces and also concepts. Although most commonly used\nto depict geography, maps may represent any space, real\nor imagined, without regard to context or scale. We use\nconceptual maps to discuss ideas, we organize data in 2D\nspaces in order to understand it, and we can get our bear-\nings using topographical maps. SongExplorer’s maps are\nconstructed using MIR techniques that provide the high-\nlevel descriptors needed successfully organizing the data;\nthey do not ﬁlter or hide any content, thus showing the\ncomplete collection while highlighting some of the songs’\ncharacteristics.\nTherefore, SongExplorer provides intuitive and fast ways\nfor promoting the direct exploration of these maps. In\nthe last years, several successful projects have shown that\ntangible, tabletop and multitouch interfaces exhibit useful\nproperties for advanced control in general (such as con-\ntinuous, real-time interaction with multidimensional data,\nand support for complex, skilled, expressive and explo-\nrative interaction) [4] and for the exploration of bidimen-\nsional spaces in particular [2]. Following this trend, Song-\nExplorer allow users to interact with the maps directly with\ntheir hands, touching the surface with their ﬁngers and ma-\nnipulating physical tokens on top of it. In the following\nsection we will comment some of the most relevant previ-\nous works, related to the two main aspects of our project,\ni.e. (i) the visualization of musical data, and (ii) the direct\nmanipulation of this or any other type of data, in a tabletop\ninteraction context.\n5http://www.pandora.com\n6http://www.last.fm\n675Poster Session 4\n2. RELATED WORK\n2.1 Visualization of music collections\nIn the ﬁeld of visualization, there is an extensive bibliog-\nraphy on the representation of auditory data. In the partic-\nular case we are focusing on, that of the visual organiza-\ntion of musical data, solutions often consist in extracting\nfeature descriptors from data ﬁles, and creating a multidi-\nmensional feature space that will be projected into a 2D\nsurface, using dimensionality reduction techniques.\nA very well known example of this method is the work\nIslands of Music by Pampalk [13], which uses a landscape\nmetaphor to present a large collection of musical ﬁles. In\nthis work, Pampalk uses a Self Organizing Map (SOM) [9]\nto create a relief map in which the accumulation of songs\nare presented as the elevation of the terrain over the sea.\nThe islands created as a result of this process roughly cor-\nrespond to musical genres.\nA later attempt to combine different visualizations on\na single map was also created by Pampalk et al [14]. By\nusing different parameters to organize the SOM, they cre-\nated several views of the collection, later interpolating the\ndifferent solutions for creating a smooth combination of\nsituations with which to explore new information.\nBeyond the 2D views, an interesting work on music\ncollections visualization, which distributes the songs on a\nspherical surface, thus avoiding any edge or discontinuity,\nis described by Leitich and Topf [11].\nIn the aforementioned examples, a topological metaphor\nis taken in advantage to enable users exploring big collec-\ntions of data. A different and original visualization ap-\nproach is chosen in Musicream [1], an interesting example\nof exploratory search in music databases, using the search\nby example paradigm. In Musicream, songs are repre-\nsented using colored circles, which fall down from the top\nof the screen. When selected, these songs show their title\non their center, and they can be later used to ”ﬁsh” similar\nones.\n2.2 Tangible tabletop interaction\nIn the domain of Tabletop and Tangible User Interfaces\n(TUI) there is also a growing interest in working with mu-\nsical applications. As a matter of fact, from the Audiopad\n[15] to the Reactable [5], music performance and creation\nhas arguably become the most popular and successful ap-\nplication ﬁeld in the entire lifetime of this interaction par-\nadigm. This is, according to Jord `a [4], because of the spe-\nciﬁc affordances of this type of interfaces: support of col-\nlaboration and sharing of control; continuous, real-time in-\nteraction with multidimensional data; and support of com-\nplex, expressive and explorative interaction. In this sense,\nand although less proliﬁc than the applications strictly con-\nceived for musical performance, some interesting works\nhave also been developed to interact with large music col-\nlections.\nMusictable [16] takes a visualization approach similar\nto the one chosen in Palmpalk’s Islands of Music, to create\na two dimensional map that, when projected on a table, is\nFigure 1 . reactiVision framework schema.\nused to make collaborative decisions to generate playlists.\nAnother adaptation into the tabletop domain is the work\nfrom Hitchner et al [3], which uses a SOM to build the\nmap and also creates a low resolution mosaic that is shown\nto the user. The users can redistribute the songs on this\nmosaic and thus changing the whole distribution of SOM\naccording to the user’s desires.\nWe believe this paper also represents a real contribu-\ntion to the tangible/tabletop user interface community. As\nnoted before, it has been proposed very recently [4] that\nthey can be especially adequate for complex and non-task\noriented types of interaction, which could include real-time\nperformance, as well as explorative search. The topic ad-\ndressed by this paper (N-Dimensional navigation in 2-D)\nhas never been addressed before within tabletop interfaces.\n3. HARDWARE\nSongExplorer is a tabletop application, i.e. a computer ap-\nplication meant to run on a tangible and multitouch sur-\nface, designed for the exploration and discovery of new\nmusic. In this section we discuss its main hardware com-\nponents.\nAs schematized in Fig.1, the system is made of a translu-\ncent plastic surface, some infrared lamps for diffused illu-\nmination, an infrared camera for the detection of the user\ninteraction, and a projector for the projection of the visual\nfeedback on the table surface. The surface is round, as in\nthe Reactable case, for encouraging collaboration [5].\nThe tracking software is based on reacTIVision [6], an\nopen-source framework for the recognition of ﬁngers and\nobjects tagged with ﬁducials. The images showing the\nﬁducial markers that are stuck into the physical pucks, and\nthe ﬁngers that are in contact with the translucent surface,\nare captured by the infrared camera and processed by reac-\nTIVision. For each video frame, this software component\nsends the corresponding data (which includes the positions\nand IDs of the identiﬁed objects and ﬁngers) to SongEx-\nplorer, using the TUIO protocol [7]. SongExplorer subse-\nquently identiﬁes the gestures and the actions performed\non the table surface, and proceeds with the appropriate\n67610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nresponses, ﬁnally generating the output image that is dis-\nplayed by the projector on the translucent surface.\n4. SOFTWARE\nThis section describes the main components of the Song-\nExplorer software: feature extraction, visualization and in-\nteraction.\n4.1 Feature Extraction\nSongExplorer uses all the songs included in the Magnatune\nonline database, which comprises a total of 6666 songs\nweighting more than 26 GB. Being Creative Commons-\nlicensed, this library is used in many research projects.\nThese songs are processed by an in-house music anno-\ntation library developed at the Music Technology Group\n(MTG) [10], and the results are transformed to binary ﬁles\nthat can be loaded by the system using the Boost7C++\nlibrary.\n4.2 Visualization\nFrom the whole set of available annotated features gener-\nated by the annotation library, we are curently using the\nmost high-level properties together with the BPM:\n•Beats Per Minute (BPM)\n•Happy probability\n•Sad probability\n•Party probability\n•Acoustic probability\n•Aggressive probability\n•Relaxed probability\nAll these high level features are independent, and even the\nmoods, which try to cover all the basic emotions, do not\ndepend on each other (i.e. a song could be both sad and\nhappy) [10]. The emotional features can, in fact, be con-\nsidered binary, with their values indicating the probability\nof this feature being true.\nWith this data, a multidimensional feature space (of 7\ndimensions) is constructed, in which each song is a sin-\ngle data point with its position deﬁned by these 7 features,\nall of them being normalized between 0 and 1. From this\nmultidimensional data we construct a 2D space which pre-\nserves its topology, and we present it to the user, who will\nthen be able to explore it.\nSimilarly to other visualization works, a SOM is used\nto distribute the data on the 2D space. Our implementa-\ntion of the Kohonen network uses a circular, hexagonally-\nconnected neuron grid, in order to ﬁt the shape of the inter-\nactive surface. As opposed to the original implementation\nof SOM [9], a restriction was added to prevent more than\none song falling into a single neuron, so that every rep-\nresentation in the 2D space should be visible and equally\ndistant from its direct neighbors, as shown in Fig. 2.\nIn the visualization plane, every song is represented by\na colored circle, throbbing at the song’s BPM. Since there\n7http://www.boost.org\nFigure 2 . Detail of the hexagonal structure of the grid.\nseems to be a strong agreement about the usefulness of art-\nwork to recognize albums or songs [11, 12], depending on\nthe zoom factor, the actual artwork may be shown in the\ncenter of each song.\nAdditionally, colors are used to highlight the different\nproperties of the songs. The coupling {feature→color}\nwas deﬁned with an online survey where 25 people had\nto pair the high-level tags to colors. The color candidates\nwere 7 basic colors with maximum saturation and light-\nness: red, blue, green, brown, cyan, yellow and magenta.\nSubjects were only able to choose the best color represen-\ntation for each tag. The results were: aggressive-red (with\nan agreement of 100%), relaxed-cyan (43.5%), acoustic-\nbrown (52%), happy-yellow (39%), party-magenta (48%)\nand sad-blue (56.5%).\nFor every song, its corresponding property value is mapped\ninto the saturation of the related color (0 meaning no sat-\nuration thus resulting on a grey color, 1 corresponding to\nfull saturation), while the lightness is kept to the maximum\nand the hue is obviously linked to the emotional feature se-\nFigure 3 . Colors highlighting high-level properties: sad,\nparty, happy, relaxed, aggressive and acoustic (Best seen\nin color, colors modiﬁed for B/W printing).\n677Poster Session 4\nFigure 4 . The tangibles of SongExplorer: playlist naviga-\ntor,color changer ,magnifying glass andnavigation menu\nlected, as described in the previous color pairings (Fig. 3\nshows the effect of different highlights on the songs). An\noption to see colors representing genres is also provided,\nalthough in that case the pairing between genres and col-\nors is done randomly.\n4.3 Interaction\nFrom a users perspective, SongExplorer is a table that shows\ndynamic images on its surface, which can be manipulated\nin several ways, using both the ﬁngers as well as some spe-\ncial pucks we will call tangibles, and which will be de-\nscribed later.\n4.3.1 Multitouch interaction\nBasic ﬁnger interaction includes single and multiple ﬁnger\ngestures, and the use of one or two hands. The simplest\ngesture, selecting and clicking, is implemented by touch-\ning a virtual object shown on the table surface, with a sin-\ngle ﬁnger and for more than 0.4 seconds. In order to distin-\nguish them from the selection action, other ﬁnger gestures\ninvolve the use of two simultaneous ﬁngers for each hand.\nThat way, using only one hand, users can translate the map\nand navigate through it, while the use of both hands al-\nlows rotating and zooming the map (see Fig. 5). It should\nbe noted that most of these gestures have become de-facto\nstandards in multitouch and tabletop interaction [8].\n4.3.2 Tangible interaction with pucks\nAdditionally, SongExplorer tangibles also include 4 trans-\nparent Plexiglas objects of about 50cm2 each, each one\nwith a different shape and a different icon that suggests its\nfunctionality, as described in Table 1. These pucks, which\ncan be kept on the table frame outside the interactive zone\n(see Fig. 4), become active and illuminated when they get\nin contact with the interactive surface. As indicated below,\nsome (like the color changer or the navigator) will apply to\nthe whole map, while others (such as the magnifying glass)\napply to the selected song.\n•The color changer puck allows selecting and high-\nlighting one of the different emotional properties of the\nwhole song space. For example, changing the map to\nred allows us to see the whole map according to its\naggressive property, with fully red dots or circles cor-\nresponding to the more aggressive songs, and grey dots\nto the least aggressive ones. Apart from helping to ﬁnd\nsongs based on a given property, the resulting visual\nFigure 5 . Virtual Map movement (up) and zooming\n(down)\ncues also help to memorize and recognize the explored\nzones of the map.\n•When placed on top o a song, the magnifying glass\npuck allows seeing textual information on this particu-\nlar song, such as the song title, the album, the authors\nname, as well as the artwork.\n•Thenavigation puck displays a navigation menu, which\nallows the user to perform actions related to the move-\nment and zooming of the map, such as returning to the\ninitial view, or zooming and centering on the currently\nplaying song.\n•The playlist navigator puck allows the creation and\nmanagement of the playlist, as described below.\n4.3.3 Managing playlist and playing back songs\nSongExplorer has the ability of creating and managing playlists.\nPlaylist are graphically represented on the surface as a con-\nstellation, in which the stars (i.e. the corresponding songs\nit contains) are connected by lines establishing their play-\ning order (see Fig. 6). Most stars show a white stroke,\nexcept for the one that is currently playing (red), and the\none the playlist navigator is focusing on (green).\nPlaylists allow several actions using both the ﬁngers and\nthe playlist navigator puck. When clicking on a song, this\nis automatically added to the playlist. Users can start play-\ning a song by clicking on any star of the playlist. Similarly,\ncrossing out a star removes the corresponding song from\nthe list. A song will stop playing either when it reaches its\nend, when the song is deleted from the playlist or when an-\nother song is selected for playing, and a playlist will keep\nplaying until its end, unless it is stopped with the playlist\nnavigator puck. This object allows several additional ac-\ntions to be taken on the playlist, such as navigating through\nits songs and showing information about the focused song\nin the same way the magnifying glass does.\n5. EV ALUATION\nSome user tests have been undertaken in order to evaluate\nthe system, focusing on the interface design. The evalu-\nation consisted in three areas: subjective experience, ade-\nquacy of the visualization and the organization, and inter-\naction.\n67810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSymbol Name Description\nplaylist navigator Permits to run over the songs on the playlist\ncolor changer Allows to highlight features of the songs\nmagnifying glass Shows information about songs\nnavigation menu Provides a way to return to known situations\nTable 1 . Tangibles used in SongExplorer\nFigure 6 . Playlist and Playlist navigator\n5.1 Experiment Procedure\nTo carry out the tests, an interactive table with SongEx-\nplorer up and running was provided. The system was al-\nways on an initial state at the beginning. One subject at\na time was doing the test. First of all, a little explanation\nabout the purpose, visualization and interaction was given.\nThen the subject was asked to ﬁnd something interesting\nin the music collection. No time limit was imposed, and\nthe subject was observed along the process. At the end of\nthe activity, the subject was told to ﬁll a questionnaire, on\nwhich she had to rate, using a Likert scale of 10 levels8,\nthe several aspects of each area. They could also write sug-\ngestions at the end of the test.\n5.2 Results\nAfter doing the tests the results were quite positive (see\nTable 2). Regarding the personal experience with SongEx-\nplorer, the subjects enjoyed the experience, discovered new\nand interesting music, felt comfortable, and found it useful\n810: Totally agree, 0: Totally disagree.µ1/2IQR\nEnjoyed the experience 8 1\nDiscovered new music 8 1\nFelt comfortable 8 1.5\nFound it useful 9 0.5\nFound colors correct 8 1.5\nFound categories suitable 7 1\nFound graphics adequate 9 1.5\nTable 2 . Evaluation Results. µ1/2: Median,IQR : In-\nterquartile range.\nto ﬁnd interesting music. So the overall experience seemed\nto be good; we have to notice the low deviation, indicating\nthat there was an agreement about these opinions.\nFocusing on the visualization process, there was also a\ncommon opinion about the suitability of the colors used.\nThis is not a surprise, as they were extracted from an on-\nline poll (details on subsection 4.2). The categories (for-\nmerly the high-level properties from the annotation soft-\nware) were suitable, according to the subjects, for the pur-\npose of describing music. The graphics were also evalu-\nated (meaning the adequacy of icons, the metaphor song-\ncircle, the panels...) and also appreciated.\nFor the evaluation of the interaction, this paper will not\nenter into details, because of its extension, but the results\nwere also quite positive. The level of understanding of ev-\nery gesture and tangible of SongExplorer was tested, as\nwell as their difﬁculty of use and usefulness. The only no-\nticeable result was that there seemed to be an inverse cor-\nrelation between previous experiences with tabletops and\nthe perceived difﬁculty of ﬁnger gestures.\nFinally there was a general demand for more music-\nplayer capabilities like pause or a progress bar for jumping\nto the middle of the song. The option of bookmarking and\nstoring playlist was also desired.\n6. CONCLUSIONS AND FURTHER WORK\nWe have presented SongExplorer, a new system for large\nmusic collections exploration, based on similarity and high\nlevel property highlighting that can allow users to ﬁnd in-\nteresting new music.\n679Poster Session 4\nThe user tests have shown that this system can be a good\ntool for discovering new, valuable music to the users. And\nthis forces us to think about its possible real world applica-\ntions. As long as this type of interfaces are uncommon,\nit is not intended for personal use because of its physi-\ncal nature (size) and its hardware requirements. But other\nuses than the personal one can be imagined. For instance,\nsome researchers from the annotation software communi-\ncated their desire to use SongExplorer to test the reliability\nof its annotation systems. Using the virtual map they can\neasily search for inconsistencies. This can be extended to\nother annotation software systems.\nAs another real world user case, it would be useful, as\na way of promoting music in stores, to have this system\navailable to their customers. An additional feature could\nbe created allowing users to highlight their favorite music\nso they can then ﬁnd similar music near to the ones they\nlike, to optionally buy the records afterwards.\nIn the future versions of SongExplorer, we want to give\nit the ability of storing playlists, give the user the option of\nrating songs, adding common player-like capabilities like\njumping to the middle of a song, searching songs using ac-\ntual records (using identiﬁers on the CD cases) and proba-\nbly more features.\nVideo:\nhttp://www.vimeo.com/4796964\n7. ACKNOWLEDGEMENTS\nWe thank the MTG team at Universitat Pompeu Fabra for\nits excellent work on music mood classiﬁcation that made\nthis project possible. Also to the Reactable project for pro-\nviding access to its hardware and software platform that\nis the base of SongExplorer. Finally, we want to thank\nthe open source community for providing the software li-\nbraries and free content (Magnatune) on witch this appli-\ncation relies.\n8. REFERENCES\n[1] M. Goto and T. Goto. Musicream: New music play-\nback interface for streaming, sticking, sorting, and re-\ncalling musical pieces. In ISMIR 2005: Proceedings of\nthe 6th International Conference on Music Information\nRetrieval , 2005.\n[2] J.Y . Han. Multi-touch interaction wall. In Interna-\ntional Conference on Computer Graphics and Interac-\ntive Techniques . ACM New York, NY , USA, 2006.\n[3] S. Hitchner, J. Murdoch, and Tzanetakis G. Music\nbrowsing using a tabletop display.\n[4] S. Jord `a. On stage: the reactable and other musical tan-\ngibles go real. International Journal of Arts and Tech-\nnology , 1(3):268–287, 2008.\n[5] S. Jorda, M. Kaltenbrunner, G. Geiger, and R. Bencina.\nThe reactable*. In Proceedings of the InternationalComputer Music Conference (ICMC 2005), Barcelona,\nSpain , pages 579–582, 2005.\n[6] M. Kaltenbrunner and R. Bencina. reacTIVision: a\ncomputer-vision framework for table-based tangible\ninteraction. In Proceedings of the 1st international con-\nference on Tangible and embedded interaction , pages\n69–74. ACM New York, NY , USA, 2007.\n[7] M. Kaltenbrunner, T. Bovermann, R. Bencina, and\nE. Costanza. TUIO: A protocol for table-top tangible\nuser interfaces. Proc. of the The 6th Int’l Workshop on\nGesture in Human-Computer Interaction and Simula-\ntion, 2005.\n[8] J. Kim, J. Park, H.K. Kim, and C. Lee. Hci (human\ncomputer interaction) using multi-touch tabletop dis-\nplay. In IEEE Paciﬁc Rim Conference on Communica-\ntions, Computers and Signal Processing, 2007. PacRim\n2007 , pages 391–394, 2007.\n[9] T. Kohonen. Self-Organizing Maps . Springer, 2001.\n[10] C. Laurier, O. Meyers, J. Serr `a, M. Blech, and P. Her-\nrera. Music mood annotator design and integration.\nChania, Crete, Greece, 03/06/2009 2009.\n[11] S. Leitich and M. Topf. Globe of Music: Music Li-\nbrary Visualization Using GEOSOM. In Proceedings\nof the 8th International Conference on Music Informa-\ntion Retrieval (ISMIR 2007) .\n[12] A. Pabst and R. Walk. Augmenting a rugged standard\nDJ turntable with a tangible interface for music brows-\ning and playback manipulation. In Intelligent Environ-\nments, 2007. IE 07. 3rd IET International Conference\non, pages 533–535, 2007.\n[13] E. Pampalk. Islands of Music Analysis, Organization,\nand Visualization of Music Archives. Journal of the\nAustrian Soc. for Artiﬁcial Intelligence , 22(4):20–23,\n2003.\n[14] E. Pampalk, S. Dixon, and G. Widmer. Exploring mu-\nsic collections by browsing different views. Computer\nMusic Journal , 28(2):49–62, 2004.\n[15] J. Patten, B. Recht, and H. Ishii. Audiopad: A tag-\nbased interface for musical performance. In Proceed-\nings of the 2002 conference on New interfaces for musi-\ncal expression , pages 1–6. National University of Sin-\ngapore Singapore, Singapore, 2002.\n[16] I. Stavness, J. Gluck, L. Vilhan, and S. Fels. The MU-\nSICtable: A Map-Based Ubiquitous System for Social\nInteraction with a Digital Music Collection. Lecture\nNotes In Computer Science , 3711:291, 2005.\n680"
    },
    {
        "title": "Automatic Identification for Singing Style based on Sung Melodic Contour Characterized in Phase Plane.",
        "author": [
            "Tatsuya Kako",
            "Yasunori Ohishi",
            "Hirokazu Kameoka",
            "Kunio Kashino",
            "Kazuya Takeda"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417415",
        "url": "https://doi.org/10.5281/zenodo.1417415",
        "ee": "https://zenodo.org/records/1417415/files/KakoOKKT09.pdf",
        "abstract": "A stochastic representation of singing styles is proposed. The dynamic property of melodic contour, i.e., fundamental frequency (F0) sequence, is assumed to be the main cue for singing styles because it can characterize such typical ornamentations as vibrato . F0 signal trajectories in the phase plane are used as the basic representation. By fitting Gaussian mixture models to the observed F0 trajectories in the phase plane, a parametric representation is obtained by a set of GMM parameters. The effectiveness of our proposed method is confirmed through experimental evaluation where 94.1% accuracy for singer-class discrimination was obtained.",
        "zenodo_id": 1417415,
        "dblp_key": "conf/ismir/KakoOKKT09",
        "keywords": [
            "stochastic representation",
            "melodic contour",
            "fundamental frequency (F0)",
            "phase plane",
            "Gaussian mixture models",
            "singer-class discrimination",
            "experimental evaluation",
            "parametric representation",
            "vibrato",
            "typical ornamentations"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAUTOMATICIDENTIFICATIONFOR SINGING STYLE BASEDON SUNG\nMELODICCONTOURCHARACTERIZEDIN PHASE PLANE\nTatsuyaKako†,YasunoriOhishi‡, HirokazuKameoka‡, KunioKashino‡,Kazuya Takeda†\n†Graduate School of Information Science, NagoyaUniversity\n‡NTTCommunication Science Laboratories, NTT Corporation\nkako@sp.m.is.nagoya-u.ac.jp, ohishi@cs.brl.ntt.co.jp, kameoka@eye.brl.ntt.co.jp\nkunio@eye.brl.ntt.co.jp, kazuya.takeda@nagoya-u.jp\nABSTRACT\nA stochastic representation of singing styles is pro-\nposed. Thedynamicpropertyofmelodiccontour,i.e.,fun-\ndamental frequency ( F0) sequence, is assumed to be the\nmaincueforsingingstylesbecauseitcancharacterizesuch\ntypical ornamentations as vibrato.F0signal trajectories\nin the phase plane are used as the basic representation. By\nﬁtting Gaussian mixture models to the observed F0trajec-\ntoriesinthephaseplane,aparametricrepresentationisob-\ntained by a set of GMM parameters. The effectiveness of\nour proposed method is conﬁrmed through experimental\nevaluationwhere94.1%accuracyforsinger-classdiscrim-\ninationwasobtained.\n1. INTRODUCTION\nAlthough no ﬁrm deﬁnition has yet been established for\n“singingstyle”inmusicalinformationprocessingresearch,\nseveral studies have reported the relationship between\nsinging styles and such signal features as singing formant\n[1,2] and singing ornamentations. Various research ef-\nfortshavebeenmadetocharacterizeornamentationsbythe\nacousticalpropertyofthesungmelody,i.e., vibrato[3–11],\novershoot [12], and ﬁne ﬂuctuation [13]. The importance\nofsuchmelodicfeaturesforperceivingsingerindividuality\nwas also reported in [14] based on psycho-acoustic exper-\niments. They concluded that the average spectrum and the\ndynamical property of the F0sequence affect the percep-\ntionoftheindividuality. Thosestudiessuggestthatsinging\nstyleisrelatedtothelocaldynamicsofasungmelodythat\ndoes not contain any musical information. Therefore, in\nthis study, we focus on the local dynamics of the F0se-\nquence, i.e., the melodic contour, as a cue of singing style\nand propose a parametric representation as a model for\nsingingstyles.\nOn the other hand, very few application systems have\nbeenreportedthatusethelocaldynamicsofasungmelody.\n[15] reported a singer recognition experiment using vi-\nbrato. [16] reported a method for evaluating singing skill\nthroughthespectrumanalysisofthe F0contour. Although\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandthatcopies\nbearthis notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.thesestudiestrytousethelocaldynamicsofmelodiccon-\ntour as a cue for ornamentation, no systematic method\nhas been proposed for characterizing singing styles. A\nlag system model for typical ornamentations was reported\nin[14,17–19];however,variationofsingingstyleswasnot\ndiscussed.\nIn this paper, we propose a stochastic phase plane as\na graphical representation of singing styles and show its\neffectiveness for singing style discrimination. One merit\nof this representation to characterize singing style is that\nsince neither an explicit detection function for ornamen-\ntation like vibratonor estimation of the target note is re-\nquired, it is robustto sung melodies.\nIn a previous paper [20], we applied this graphical rep-\nresentationofthe F0contourinthephaseplanetoaquery-\nby-hammingsystemandneutralizedthelocaldynamicsof\ntheF0sequence so that only musical information was uti-\nlized for the query. In contrast, in this study, we use the\nlocal dynamics of the F0sequence for modeling singing\nstyles and disregard the musical information because mu-\nsical information and singing style are in a dual relation.\nIn this paper, we also evaluate the proposed represen-\ntation through a singer-class discrimination experiment in\nwhich we show that our proposed model can extract the\ndynamic properties of sung melodies shared by a group of\nsingers.\nIn the next section, we propose stochastic phase plane\n(SPP)asastochasticrepresentationofthemelodiccontour\nand show how singing ornamentations are modeled by the\nproposed SPP. In Section 3, we experimentally show the\neffectiveness of our proposed method through singer class\ndiscrimination experiments. Section 4 discusses the ob-\ntained results and concludes this paper.\n2. STOCHASTICREPRESENTATIONOF THE\nDYNAMICALPROPERTYOF MELODIC\nCONTOUR\n2.1F0signal in the Phase Plane\nSuch ornamental expressions in singing as vibratoare\ncharacterized by the dynamical property of their F0sig-\nnal. Sincethe F0signalisacontrolledoutputofthehuman\nspeechproductionsystem,itsbasicdynamicalcharacteris-\nticscanberelatedtoadifferentialequation. Therefore,we\ncanusethephaseplane,whichisthejointplotofavariable\nand its time derivative, i.e., (x,˙x), to depict its dynamical\nproperty.\n393Poster Session 3\n∆F0 Classical (female) F0 trajectory\n0 10 20 30 450050005500\nF0 [cent] \nTime [sec] \nClassical (female) F0 - ∆F0 phase plane\n50 \n0\n-50\n4500 5000 5500\nF0 [cent] \nClassical (female) F0 - ∆∆ F0 phase plane\n∆∆ F0 \n4500 5000 5500\nF0 [cent] 40 \n20 \n0\n-20\n-40Figure 1. Melodic contour (top) and corresponding phase\nplanesforF0-∆F0(middle)and F0-∆∆F0(bottom)\nAlthoughthesignalsequenceisnotgivenasanexplicit\nfunction of time, F0(t), but as a sequence of numbers,\n{F0(n)}n=1,···,N, we can estimate the time derivative us-\ningthedelta-coefﬁcientgivenby\n∆F0(n) =K/summationdisplay\nk=−Kk·F0(n+k)\nK/summationdisplay\nk=−Kk2, (1)\nwhere 2Kisthewindowlengthforcalculatingthedynam-\nics. Changingthewindowlengthextractsdifferentaspects\nofthe signal property.\nAn example of such a plot for a given melodic contour\nis shown in Fig. 1. Here, the F0signal (top), the phase\nplane(middle),andthesecondorderphaseplane,whichis\ngivenbythejointplotof F0and∆∆F0(bottom),areplot-\nted. The singing ornamentations are depicted as the local\nbehavior of the trajectory around the centroids that com-\nmonly represent target musical notes. Vibratoin singing,\nfor example, is shown as circular trajectories centered at\ntarget notes. In the second order plane, the trajectories ap-\npear as lines with a slope of -45 degrees. This shows that\ntherelationship between F0and∆∆F0is givenas\n∆∆F0=−F0. (2)\nHence, the sinusoidal component is imposed in the given\nsignal. Over/under-shootstothetargetnotearerepresented\nasspiral patterns around the note.\n2.2 Stochastic representationof Phase Plane\nOnceasingingstyleisrepresentedasaphaseplanetrajec-\ntory, parameterizing the representation becomes an issue\nfor further engineering applications. Since the F0signal\nisnotdeterministic,i.e.,itvariesacrosssingingbehaviors,\na stochastic model must be deﬁned for the parameteriza-\ntion. Byﬁttingaparametricprobabilitydensityfunctionto\nthetrajectoriesinthephaseplane,wecanbuildastochastic\nPop (female) F0 - ∆F0 phase plane\n4500 5000 5500\nF0 [cent] ∆F0 40 \n20 \n0\n-20\n-40\n5000 5500F0 [cent] ∆F0 40 \n20 \n-20\n-400\n4500Figure 2. Gaussian mixture model ﬁtted to F0contour in\nphase plane\nphaseplane(SPP)anduseitforcharacterizingthemelodic\ncontour. A common featureof the trajectories in thephase\nplane is that most of their segments are distributed around\nthe target note, and therefore the distribution’s histogram\nismultimodal,buteachmodecanberepresentedbyasim-\nple symmetric 2d or 3d-pdf. Therefore, Gaussian mixture\nmodel (GMM),\nM/summationdisplay\nm=1λmN(f0(n);µm,Σm), (3)\nwhere\nf0(n) = [F0(n),∆F0(n),∆∆F0(n)]T,(4)\nis adopted for the modeling. N(·)is a Gaussian distribu-\ntion, and\nΘ ={λm,µm,Σm}m=1,···,M, (5)\nare parameters of the model, each of which represents the\nrelativefrequency,themeanvector,andthecovariancema-\ntrix of each Gaussian.\nA GMM trained for F0contours in the phase plane is\ndepicted in Fig. 2. A smooth surface is trained through\nmodel ﬁtting. The horizontal deviations of each Gaussian\nrepresent the stability of the melodic contour around the\ntargetnote,buttheverticaldeviationsrepresentthe vibrato\ndepth. Inthismanner,singingstylescanbemodeledbyset\nof parameters Θof the stochastic phase plane.\n2.3 Examples of Stochastic Phase Plane\nIn Fig. 3, the F0signals of three female singers are plot-\nted: professional classical, professional pop, and an ama-\nteur. A deep vibratois observed as a large vertical devia-\ntion in the Gaussians in the professional classical singer’s\nplot. On the other hand, the amateur’s plot is character-\nizedbylargehorizontaldeviations. Althoughdeep vibrato\nis not observed in the plot for the professional pop singer,\nits smaller horizontal deviation shows that she accurately\nsang the melody.\n39410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n∆F0 \nClassical (female)\nF0 [cent] 40 \n20 \n0\n-20\n-400 200 400 600 800 1000 1200\nPop (female)\n∆F0 \nF0 [cent] 20 \n0\n-20\n-400 200 400 600 800 1000 120040 \nAmateur (female)\n∆F0 \nF0 [cent] 20 \n0\n-20\n-400 200 400 600 800 1000 120040 Figure 3. Stochastic phase plane models for professional\nclassical (top), professional pop (middle), and amateur\n(bottom)\nTable1. Signalanalysisconditionsfor F0estimation. Har-\nmonical PSD pattern matching [21] is used with these pa-\nrameters.\nSignalsampling freq. 16 kHz\nF0estimation windowlength 64 ms\nWindowfunction Hanning window\nWindowshift 10 ms\nF0contour smoothing 50 ms MA ﬁlter\n∆coefﬁcientcalculation K= 2\nThestochasticrepresentationsofthesecondorderphase\nplane are also shown in Fig. 4. Strong negative correla-\ntions between F0and∆∆F0can be found only in the plot\nfortheprofessionalclassicalsingerthatalsoindicatesdeep\nvibratoin the singing style.\n3. EXPERIMENTALEVALUATION\nThe effectiveness of using SPP to discriminate different\nsingingstyles is evaluatedexperimentally.\n3.1 Experimental set up\nThe following singing signals of six singers were used:\none of each gender in the categories of professional clas-\nsical, professional pop, and amateur. With/without musi-\ncalaccompaniment,eachsubjectsangsongswithJapanese\nlyrics and hummed. The songs were “ Twinkle, Twinkle,\nLittle Star ”, and “Ode to Joy ” and ﬁve etudes. A total of\n102song signals wasrecorded.\nTheF0contour was estimated using [21]. The signal\nprocessing conditions for calculating F0,∆F0, and the\n∆∆F0contoursare listed in Table1.\nSincetheabsolutepitchofthesongsignalsdifferacross\nsingers, we normalized them so that only the singing style\nof each singer is used in the experiment. Normalization\nClassical (female)\nF0 [cent] \nPop (female)\nF0 [cent] \nAmateur (female)\nF0 [cent] 0 200 800 1000 1200\n0 800 1000 1200\n200 400 600 800 1000 1200600 400\n200 600 400\n0∆∆ F0 \n∆∆ F0 \n∆∆ F0 10 \n-100\n10 \n-100\n10 \n-100Figure 4. 2nd order stochastic phase plane models for\nprofessionalclassical(top),professionalpop(middle),and\namateur (bottom)\nwas done in the procedure below. First, the F0frequency\nin[Hz]isconvertedto [cent] by\n1200×log2F0\n440×23/12−5[cent].(6)\nThenthelocaldeviationsfromthetemperedclavierarecal-\nculated by the residue operation mod (·):\nmod (F0+ 50,100). (7)\nObviously, after this conversion, the F0value is limited to\n(0,100)in[cent].\n3.2 Discrimination Experiment\nThe discrimination of three singer classes, i.e., profes-\nsional classical, professional pop, and amateur, was per-\nformed based on the maximum a posteriori probability\n(MAP) decision:\nˆs= arg max\ns[p(s|{F0,∆F0,∆∆F0})]\n= arg max\ns/bracketleftBigg\n1\nNN/summationdisplay\nn=1logp(f0(n)|Θs) + logp(s)/bracketrightBigg\n(8)\nwheresis the singer-class id and Θsis the model param-\neters of the sthsinger-class. We used “ Twinkle-Twinkle,\nLittle Star ” and ﬁve etudes sung by singers from each\nsingerclassfortrainingand“ OdetoJoy ”sungbythesame\nsingers for testing. Therefore the results are independent\nfrom sung melodies but closed in singers. Nis the length\nof the signal in the samples. Since we assumed an equal\na prioriprobability for singer-class distribution p(s), the\nabove MAP decision is equivalent to the Maximum Like-\nlihood decision.\n3.3 Results\nFig. 5 shows the accuracy of the singer-class discrimi-\nnation. The best is attained for a 13-second input sig-\n395Poster Session 3\nSignal length [sec]Discrimination accuracy [%] 95 \n90 \n85 \n80 5.0 7.0 9.0 11.0 13.0 15.0M = 8\nM = 16\nM = 32Figure5. Accuracyin discriminating three singer classes\nF0 , ∆F0 , ∆∆ F0 F0 , ∆F0 F0Discrimination accuracy [%] 100\n80 \n60 \n40 \n20 \n0\nFigure 6. Comparing accuracy in discriminating singer\nclasses\nnal. The accuracy increases with the length of the test\nsignal and 94.1% is attained with an 8-mixture GMM for\nsinger-class models, when a 13-second signal is available\nfor the test input. No signiﬁcant improvement in accuracy\nwas found for the longer test input because more song-\ndependentinformationcontaminatedthetestsignal. Fig. 6\ncompares the accuracy of singer-class discriminations us-\ningthethreesetsoffeatures: F0only,(F0,∆F0),and(F0,\n∆F0,∆∆F0). As shown in the ﬁgure, by combining F0\nand∆F0,thediscriminationerrorratebecomeshalfofthe\nerrorwhenonlyusing F0. Combiningsecondorderderiva-\ntive∆∆F0furtherreducestheerrorbutnotasmuchasthe\ncaseof ∆F0. Theseresultsshowthattheproposedstochas-\ntic representation of the phase plane effectively character-\nizesthe singing styles of the three singer classes.\n4. DISCUSSION\nOur proposed method for representing and parameterizing\ntheF0contour effectively discriminates the three typical\nsingerclasses,i.e.,professionalclassicalandpop,andam-\nateurs. To conﬁrm that the method models the singing\nstyles(andnotsingerindividuality),wecomparedourpro-\nposed representation with MFCC under two conditions.\nAs a closed condition, we trained three MFCC-GMMs\nusing “Twinkle-Twinkle, Little Star ” and ﬁve etudes sung\nby six (male and female professional classic, professional\npop, and amateur) singers and used “ Ode to Joy ” sung by\nMFCC F0 , ∆F0 , ∆∆ F0Discrimination accuracy [%] 100\n80 \n60 \n40 \n20 MFCC, ∆MFCCClosed condition Open conditionFigure7. ComparingproposedrepresentationwithMFCC\nunder twoconditions\nthe same singers for testing. On the other hand, as an\nopen condition, we evaluated the MFCC-GMMs through\na singer independent manner where singer-class models\n(GMMs) were trained by female singer data and tested by\nmale singer data. As shown in Fig. 7, the performances\nof the MFCC-GMM and the proposed method are almost\nidentical (95.0%) in the closed condition. However, in the\nnew (unseen) singer experiment, the result of the MFCC-\nGMM system signiﬁcantly degraded to 33.3%, but the\nproposed method attained 87.9% accuracy. These results\nsuggest that the MFCC-GMM system does not model the\nsinging style but discriminates singer individuality. How-\never, since SPP-GMM can correctly classify even an un-\nseen singer’s data, our proposed representation models the\nF0dynamic characteristics common within a singer class\nbetter than singer individuality.\n5. SUMMARY\nInthispaper,weproposedamodelforsingingstylesbased\non the stochastic graphical representation of the local dy-\nnamicalpropertyofthe F0sequence. Sincevarioussinging\nornamentations are related to signal production systems\ndescribed by differential equations, phase plane is a rea-\nsonable space for depicting singing styles. Furthermore,\nthe Gaussian mixture model effectively parameterizes the\ngraphical representation; therefore, more than 90% accu-\nracy can be achieved in discriminating the three classes of\nsingers.\nSincethescaleoftheexperimentswassmall,increasing\nthe number of singers and singer classes is critical future\nwork. Evaluatingtherobustnessoftheproposedmethodto\nnoisyF0sequences estimated under such realistic singing\nconditionsas“karaoke”isalsoaninevitablestepforbuild-\ning real-worldapplication systems.\n6. REFERENCES\n[1]J. Sundberg, The Science of the Singing . Northern\nIllinois UniversityPress, 1987.\n[2]J.Sundberg,“Singingandtimbre,” Musicroomacous-\ntics,vol.17, pp. 57–81, 1977.\n39610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n[3]C. E. Seashore, “A musical ornament, the vibrato,” in\nProc.PsychologyofMusic . McGraw-HillBookCom-\npany,1938, pp. 33–52.\n[4]J. Large and S. Iwata, “Aerodynamic study of vi-\nbrato and voluntary ”straight tone” pairs in singing,”\nJ.Acoust. Soc. Am. ,vol.49, no. 1A, p. 137, 1971.\n[5]H. B. Rothman and A. A. Arroyo, “Acoustic variabil-\nity in vibrato and its perceptual signiﬁcance,” J. Voice,\nvol.1, no. 2, pp. 123–141, 1987.\n[6]D. Myers and J. Michel, “Vibrato and pitch transi-\ntions,”J.Voice,vol.1, no. 2, pp. 157–161, 1987.\n[7]J. Hakes, T. Shipp, and E. T. Doherty, “Acoustic char-\nacteristics of vocal oscillations: Vibrato, exaggerated\nvibrato,trill,andtrillo,” J.Voice,vol.1,no.4,pp.326–\n331,1988.\n[8]C. D’Alessandro and M. Castellengo, “The pitch of\nshort-duration vibrato tones,” J. Acoust. Soc. Am. ,\nvol.95, no. 3, pp. 1617–1630, 1994.\n[9]D. Gerhard, “Pitch track target deviation in natural\nsinging,”in Proc.ISMIR , 2005, pp. 514–519.\n[10]K. Kojima, M. Yanagida, and I. Nakayama, “Variabil-\nity of vibrato -a comparative study between japanese\ntraditional singing and bel canto-,” in Proc. Speech\nProsody, 2004, pp. 151–154.\n[11]I. Nakayama, “Comparative studies on vocal expres-\nsions in japanese traditional and western classical-\nstyle singing, using a common verse,” in Proc. ICA ,\n2004,pp. 1295–1296.\n[12]G. de Krom and G. Bloothooft, “Timing and accuracy\noffundamentalfrequencychangesinsinging,”in Proc.\nICPhS, 1995, pp. 206–209.\n[13]M. Akagi and H. Kitakaze, “Perception of synthesized\nsinging voices with ﬁne ﬂuctuations in their funda-\nmentalfrequencycontours,”in Proc.ICSLP ,2000,pp.\n458–461.\n[14]T.Saitou,M.Goto,M.Unoki,andM.Akagi,“Speech-\nTo-Singing synthesis: Converting speaking voices to\nsinging voices by controlling acoustic features unique\nto singing voices,” in Proc. WASPAA , 2007, pp. 215–\n218.\n[15]T. L. Nwe and H. Li, “Exploring vibrato-motivated\nacoustic features for singer identiﬁcation,” IEEE\nTransactionsonAudio,Speech,andLanguageprocess-\ning,pp. 519–530, 2007.\n[16]T. Nakano, M. Goto, and Y. Hiraga, “An automatic\nsinging skill evaluation method for unknown melodies\nusing pitch interval accuracy and vibrato features,” in\nProc.Interspeech , 2006, pp. 1706–1709.\n[17]H. Mori, W. Odagiri, and H. Kasuya, “F0 dynamics in\nsinging: Evidence from the data of a baritone singer,”\nIEICE Trans. Inf. and Syst. , vol. E87-D, no. 5, pp.\n1086–1092,2004.[18]N.Minematsu,B.Matsuoka,andK.Hirose,“Prosodic\nmodeling of nagauta singing and its evaluation,” in\nProc.SpeechProsody , 2004, pp. 487–490.\n[19]L. Reqnier and G. Peeters, “Singing voice detection in\nmusic tracks using direct voice vibrato,” in Proc. IC-\nCASP,2009, pp. 1658–1688.\n[20]Y. Ohishi, M. Goto, K. Itou, and K. Takeda., “A\nstochastic representation of the dynamics of sung\nmelody,”in Proc.ISMIR ,2007, pp. 371–372.\n[21]M. Goto,K. Itou, and S.Hayamizu, “Areal-timeﬁlled\npause detection system for spontaneous speech recog-\nnition,”in Proc.Eurospeech , 1999, pp. 227–230.\n397"
    },
    {
        "title": "Tag-Aware Spectral Clustering of Music Items.",
        "author": [
            "Ioannis Karydis",
            "Alexandros Nanopoulos",
            "Hans-Henning Gabriel",
            "Myra Spiliopoulou"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417483",
        "url": "https://doi.org/10.5281/zenodo.1417483",
        "ee": "https://zenodo.org/records/1417483/files/KarydisNGS09.pdf",
        "abstract": "Social tagging is an increasingly popular phenomenon with substantial impact on Music Information Retrieval (MIR). Tags express the personal perspectives of the user on the music items (such as songs, artists, or albums) they tagged. These personal perspectives should be taken into account in MIR tasks that assess the similarity between music items. In this paper, we propose an novel approach for clustering music items represented in social tagging systems. Its characteristic is that it determines similarity between items by preserving the 3-way relationships among the inherent dimensions of the data, i.e., users, items, and tags. Conversely to existing approaches that use reductions to 2way relationships (between items-users or items-tags), this characteristic allows the proposed algorithm to consider the personal perspectives of tags and to improve the clustering quality. Due to the complexity of social tagging data, we focus on spectral clustering that has been proven effective in addressing complex data. However, existing spectral clustering algorithms work with 2-way relationships. To overcome this problem, we develop a novel data-modeling scheme and a tag-aware spectral clustering procedure that uses tensors (high-dimensional arrays) to store the multigraph structures that capture the personalised aspects of similarity. Experimental results with data from Last.fm indicate the superiority of the proposed method in terms of clustering quality over conventional spectral clustering approaches that consider only 2-way relationships.",
        "zenodo_id": 1417483,
        "dblp_key": "conf/ismir/KarydisNGS09",
        "keywords": [
            "social tagging",
            "Music Information Retrieval (MIR)",
            "personal perspectives",
            "clustering music items",
            "3-way relationships",
            "inherent dimensions",
            "users",
            "items",
            "tags",
            "spectral clustering"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTAG-AWARE SPECTRAL CLUSTERING OF MUSIC ITEMS\nIoannis KarydisiAlexandros NanopoulosiiHans-Henning GabrieliiiMyra Spiliopoulouiii\niDepartment of Informatics, Ionian University, Greece\niiInstitute of Informatics, Hildesheim University, Germany\niiiFaculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany\nkarydis@ionio.gr, nanopoulos@ismll.de, {hgabriel, myra }@iti.cs.uni-magdeburg.de\nABSTRACT\nSocial tagging is an increasingly popular phenomenon with\nsubstantial impact on Music Information Retrieval (MIR).Tags express the personal perspectives of the user on themusic items (such as songs, artists, or albums) they tagged.These personal perspectives should be taken into accountin MIR tasks that assess the similarity between music items.\nIn this paper, we propose an novel approach for cluster-\ning music items represented in social tagging systems. Itscharacteristic is that it determines similarity between itemsby preserving the 3-way relationships among the inherentdimensions of the data, i.e., users, items, and tags. Con-versely to existing approaches that use reductions to 2-\nway relationships (between items-users or items-tags), this\ncharacteristic allows the proposed algorithm to consider\nthe personal perspectives of tags and to improve the clus-tering quality. Due to the complexity of social tagging data,\nwe focus on spectral clustering that has been proven effec-\ntive in addressing complex data. However, existing spectralclustering algorithms work with 2-way relationships. Toovercome this problem, we develop a novel data-modelingscheme and a tag-aware spectral clustering procedure thatuses tensors (high-dimensional arrays) to store the multi-graph structures that capture the personalised aspects ofsimilarity. Experimental results with data from Last.fm in-dicate the superiority of the proposed method in terms of\nclustering quality over conventional spectral clustering ap-\nproaches that consider only 2-way relationships.\n1. INTRODUCTION\nMusic Information Retrieval (MIR) is highly interdisci-\nplinary a ﬁeld that, due to the nature of music, requires\nan increased amount of contextual information for most of\nits processes [1]. One popular method that supplies this\nTHE SECOND AUTHOR GRATEFULLY ACKNOWLEDGES\nTHE PARTIAL CO-FUNDING OF HIS WORK THROUGH\nTHE EUROPEAN COMMISSION FP7 PROJECT MYMEDIA\n(WWW.MYMEDIAPROJECT.ORG) UNDER THE GRANT\nAGREEMENT NO. 215006.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.contextual information is the practice of social-tagging .\nSocial tags are shared, free-text keywords that web userscan assign to music items, such as artists, albums, songs,playlists, genres, etc. The popularity of music tagging restswith the easy and effective organisation it produces, in con-trast to the obscure and ambiguous hierarchical classiﬁca-tion (in terms of genre, mood, etc). Social tagging assiststhe retrieval of items and social expression of taste [2].Therefore, tags over music items reﬂect conveniently thepersonalised opinion of users for these items.\nSocial tagging attracts increasing attention and MIR sys-\ntems like Last.fm [3] and MyStrands [4] contain a bodyof collected data in which data mining is challenging and\npromising. One of the most essential data mining tasks is\nthe clustering of music data to assist their organisation,\nthe creation of playlists and the model-based music rec-ommendation. However, several existing MIR approachesconsider clustering of data based solely on features ex-tracted directly from the audio. In contrast, the proposedapproach is based on user-generated content, in the form oftags, in order to include contextual information that wouldbe otherwise non-extractable from the content of items.\nData from social-tagging systems have 3 inherent di-\nmensions: the users, the music items, and the tags. More-\nover, they contain 3-way relationships of the form items–\nusers–tags between these dimensions. Thus, there is a clear\ndifference between just knowing that a tag has been applied\nto an item regardless by which users, and knowing the spe-ciﬁc users that applied this tag to the item. The reason is\nthat in the latter case the tag expresses the personalised per-spective of the speciﬁc users on the item. Clustering of mu-sic items with existing algorithms requires the suppression\nof the 3 dimensions and the reduction of their 3-way rela-\ntionships into 2-way of the form items–users or items–tags.This is because most existing clustering algorithms model\nthe data in 2-dimensional arrays whose rows correspond\nto items and columns to features. Thus, clustering can beperformed over items-users or items-tags arrays, but notwithout breaking the original 3-way relationships betweenitems-users-tags. However, such approach may incur loss\nof valuable information contained in the 3-way relation-\nships.\nTo address the complexity of data from social tagging\nsystems, we focus on the popular family of spectral cluster-\ning algorithms. This type of clustering algorithms work ona similarity graph that connects every item to its knearest-\n159Poster Session 1\nInput:\nSocialCompute/g3\nthe/g3k/g882NN/g3Compute/g3\nthe/g3Decompose/g3\nthe/g3Run/g3\nconventional\nclusteringinOutput/g3Social/g3\nDatamulti/g882\ngraphLaplacian\nTensorLaplacian\nTensorclustering /g3in/g3\neigenvector/g3\nspaceClusters\nFigure 1 . The steps followed by the proposed approach.\nneighbors ( k-NN) and map each item to a feature space de-\nﬁned by eigenvectors of the similarity graph. Spectral clus-tering algorithms have been proven effective in address-ing complex data [5]. However, existing spectral cluster-ing algorithms cannot be used directly for data from social\ntagging systems without suppressing the 3 dimensions in\norder to consider only either items-users or items-tags re-lationships. The reason is that existing spectral clusteringalgorithms form the k-NN similarity graph based on the\nsingle value of similarity between each pair of items.\nTo overcome the problems of existing approaches and\navoid breaking the original 3-way relationships existing insocial-tagged data, we propose the extension of spectral\nclustering in order to become tag-aware and directly han-\ndle all present dimensions. Our technical contributions to-\nwards this objective are the following: (i) We provide the\ninsight that multiple similarity values between each pair\nof items should be used to account for the fact that whenall 3 dimensions are considered, then similarity between\ntwo items depends both on the users who tagged them and\nthe tags they assigned, a fact that leads to several similar-ity values between them. (ii) To support multiple similarity\nvalues, we extend the modeling based on k-NN similarity\ngraphs by using k-NN similarity multigraphs, which allow\nthe existence of multiple edges between two nodes. (iii)We extend existing spectral clustering algorithms to con-sider the k-NN similarity multigraphs by extracting infor-\nmation about eigenvectors from tensors (i.e., multidimen-\nsional arrays). (iv) We perform experiments with real data\ncrawled from Last.fm and compare the proposed method\nagainst conventional spectral clustering that suppresses the\noriginal data and consider only 2-way relationships (eitheritems-users or items-tags) in terms of quality of the ﬁnal\nclustering.\nThe rest of this article is organised as follows. Section 2\nreviews related work. Section 3 presents an overview of theproposed approach, whereas Section 4 describes the pro-\nposed data modeling and Section 5 the proposed clustering\nalgorithm. Experimental results are detailed in Section 6.\nFinally, Section 7 concludes the article.\n2. RELATED WORK\nClustering tagged music data, as well as their visualisa-\ntion, has also been the focus of the research of Lehwark et\nal. [6]. In the interest of discovering new music based on\nthe semantic organisation provided by tags on music data,they propose the use of the Emergent-Self-Organising-Map(ESOM) for the clustering of tagged data. Additionally,\nthey also utilise U-Map in order to provide a visually ap-pealing user interface and an intuitive way of exploringnew content. Differently from this approach, we apply spec-tral clustering in contrast to ESOM while our focus is on\nmultiple pairwise similarities in contrast to visualisation of\nthe produced clusters.\nLevy et al. [7], investigate the performance of mod-\nels for varying latent dimensions examining the alterationof low-dimensional semantic representations discrimina-tive capability in searching music collections. This approachis different than the one presented in our work, as we fo-cus on multiple pairwise similarities on the music data forthe purpose of clustering the music items, in contrast to [7]where different models are tested in order to uncover emer-gent semantics from social tags for music.\nThe clustering of music data has received extensive at-\ntention from the MIR community. Most research aims in\ngenre classiﬁcation (readers are suggested [8] for a detailed\nsurvey of the area) as the classiﬁcation emerging is based\non objective similarity measures from the data, thus avoid-\ning the constraints possed by ﬁxed taxonomies, which may\nbe difﬁcult to deﬁne as well as suffer from ambiguities and\ninconsistencies. Using a set of extracted features from thecontent of the music data, and a similarity measure for the\ncomparison of the data, clustering algorithms organise mu-sic data in clusters of similar objects.\nSymeonidis et al. [9] proposed dimensionality reduction\nusing higher order SVD for the purposes of personalised\nmusic recommendation. That is, given a user and a tag,\ntheir purpose is to predict how likely is the user to label\na speciﬁc music item with this tag. However, converselyfrom [9] we use tensor factorisation for extracting spec-tral information and performing spectral clustering, not forpredicting recommendations.\n3. OVERVIEW OF PROPOSED APPROACH\nThis section outlines the proposed approach. The steps that\nwill be described in the following are depicted (for refer-ence) in Figure 1.\nExisting (non tag-aware) spectral clustering algorithms\n[5] ﬁrst compute the k-NN similarity graph, which con-\nnects every item with its k-NN. Next, the Laplacian graph\nof the k-NN similarity graph is used instead, because of\nthe beneﬁts it offers, i.e., it is always positive-semideﬁnite\n(allowing its eigenvector decomposition) and the numberof times 0 appears as its eigenvalue is the number of con-\n16010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nnected components in the k-NN similarity graph. Due to\nthese convenient properties, if cclusters are required to\nbe found, spectral clustering algorithms proceed by com-puting the ceigenvectors that correspond to the csmall-\nest eigenvalues, and represent each original item as a c-\ndimensional vector whose coordinates are the correspond-\ning values within the ceigenvectors. With this representa-\ntion, they cluster the c-dimensional vectors using simple\nalgorithms, like k-means or hierarchical agglomerative.\nAs described in Introduction, differently from conven-\ntional spectral clustering algorithms, our proposed approachconsiders multiple similarity values between each pair ofitems. In particular, let Ube the set of all users. For a given\ntagt, let U\n1⊆Ube the set of users that tagged an item i1\nwith t, whereas U2⊆Ube the set of users that tagged\nan item i2with ttoo. We can deﬁne a similarity value be-\ntween i1and i2as follows. We form two vectors v1and\nv2, both with |U|elements that are set to 1 at positions that\ncorrespond to the users contained in U1and U2, respec-\ntively, whereas all rest positions are set to 0. Therefore, thesimilarity between i\n1and i2is given by the cosine measure\nbetween the two vectors v1and v2. Since the above process\ncan be repeated for all tags, the result is several similarity\nvalues between each pair of items i1and i2. The set of all\nmultiple similarity values are tag-aware and reﬂect the per-sonalised aspect of similarity perceived by the users (e.g.,two users may tag the same item but using entirely differ-ent tags).\nTo account for the various similarity values between\neach pair of items, we extend (Section 4) the k-NN similar-\nity graph to a k-NN multidigraph that is the union of mul-\ntiple simple k-NN graphs, one for each distinct tag. The\nadjacency matrix of a k-NN multidigraph forms a tensor,\ni.e., a multidimensional array. In order to attain the afore-\nmentioned advantages of the Laplacian graph, we propose\na method (Section 5.1) to extent towards the constructionof the Laplacian multidigraph, whose adjacency matrix isagain represented as a tensor. To map each item to a fea-ture space comprised from spectral information extractedfrom the Laplacian tensor, we describe (Section 5.2) howto use tensor factorisation that extends SVD to multidimen-\nsional arrays. Finally, based on the computed features, we\ndescribe (Section 5.3) how the clustering is performed. To\nhelp comprehension, we use the data from the following\nexample.\nExample 1 (Data representation). We assume 3 users that\nassign tags to 4 music items (henceforth ‘items’ for sim-\nplicity) from a tag-set with 3 tags. Each assignment com-prises a triple of the form (user, item, tag). The 9 triples\nof the example are given in Table 1, whereas we addition-\nally denote (in the ﬁrst column) the ID of the triple. The\ncorresponding view of the data as tripartite graph is de-\npicted in Figure 2. In this ﬁgure, the numbered edges cor-respond to the triple IDs in Figure 2a. For instance, the ﬁrst\ntriple (ID = 1) is: Alice tagged Elvis as Classic. In Fig-\nure 2 this corresponds to the path consisting of all edges\nlabelled as 1. To avoid cluttering the ﬁgure, parallel edges(i.e., edges between the same two nodes) with different la-bels are depicted as one with different labels separated by\ncomma. In this example, we assume that Elvis and Beatlesform one cluster, whereas Mozart and Bach form a sec-ond cluster. This follows by observing in Figure 2 that,although users tag items from both clusters, they assign\ndifferent tags to the ﬁrst cluster than the second. There-\nfore, the relationships between users-items alone are not\nable to determine a clustering structure among the items.In contrast, when considering the multi-way relationshipsbetween users-items-tags, we are able to better detect theclustering of items. Although this simple example high-\nlights the advantage of preserving the multi-way relation-\nships compared to considering only item-user relationships,\nour experimental results show the advantages compared to\nthe consideration of only item-tag relationships, as well. /square\nIDUser Item Tag\n1Alice Elvis Classic\n2Bob Beatles Classic\n3Bob Elvis Classic\n4Bob Mozart Symphonic\n5JoeMozart Symphonic\n6Joe Bach Symphonic\n7Alice Mozart Orchestral\n8JoeMozart Orchestral\n9Joe Bach Orchestral\nTable 1 . Example of input data\nAlice1\n1,33ClassicElvis\n22\n45SymphonicBeatles\n44,5/g3\n6\n787BobSymphonic\nMozart\n6,97,85,8\n9Joe\nBachOrchestral\nFigure 2 . Illustration of the tripartite graph.\n4. DATA MODELLING\nIn this section, we describe the modelling of multiple sim-\nilarity values with a k-nearest-neighbor multidigraph. A\nmultidigraph is a directed graph permitted to have multi-\nple directed edges (henceforth, simply called edges), i.e.,edges with the same source and target nodes.\nA tripartite graph (like in the example of Figure 2b) can\nbe partitioned according to the tags. For each tag t,w e\n161Poster Session 1\nget the corresponding underlying subgraph Bt, by keeping\nusers and items that participate in triples with this tag.\nEach bipartite subgraph is represented with its adjacency\nmatrix Bt(1≤t≤| T|), whose size is |I|×| U|; that\nis, its rows correspond to items and its columns to users.(Henceforth, wherever there is no ambiguity, we use inter-changeably the same symbol for a graph and its adjacencymatrix.) Each element B\nt(i, u )is equal to 1, if there is an\nedge between the item iand user u, or 0 otherwise. There-\nfore, from each adjacency matrix Btwe can compute for\nevery pair of items i, j(1≤i, j≤|I|), a similarity mea-\nsure according to the values in their corresponding rowsB\nt(i,:)and Bt(j,:). Following the widely used approach\nfor 2 dimensional matrixes (like document-term in infor-mation retrieval or user-item in CF), we consider the cosinesimilarity measure between every pair of items.\nHaving deﬁned a similarity measure, from each sub-\ngraph B\nt(1≤t≤|T|), we can compute the correspond-\ning k-nearest neighbor ( k-NN) graph, Nt, which is a la-\nbelled and directed graph (digraph). The nodes of each Nt\ncorrespond to the items. There is an directed edge between\nitems iand j(1≤i, j≤|I|), if jis among the knearest\nneighbors of i. Each edge is labelled with the correspond-\ning similarity value.\nConsidering all k-NN digraphs together, we form the k-\nNN labelled multidigraph, N, that summarises all multiple\nsimilarities. The nodes of Ncorrespond to the items. The\nlabelled edges of Nis a multiset resulting from the union\nof the labelled edges of all Ntfor 1≤t≤|T|.\nExample 2 ( k-NN multidigraph). For the data in Figure 2,\nthe resulting k-NN multidigraph N, for k=1, is depicted\nin Figure 3a. The multiple edges between the nodes of\nNdenote the different similarities between the items, ac-\ncording to the different tags. To assist notation, we assume\nthat T1denotes the ﬁrst tag, i.e., Classic, T2the second,\ni.e., Symphonic, and T3the third, i.e., Orchestral. In Fig-\nure 3a, the edges representing similarities according to tagT\ni(1≤i≤3) are annotated with Tiand then follows the\ncorresponding similarity value.1Notice that Ncorrectly\ncaptures the clustering structure: edges exist only betweenitems of the same cluster, i.e., between Elvis and Beatlesfor the ﬁrst cluster and between Mozart and Bach for the\nsecond. Conversely, in Figure 3b, which depicts the k-NN\ndigraph when only user-item relationships are considered,the separation of clusters is not clear. /square\n5. THE PROPOSED CLUSTERING ALGORITHM\n5.1 Constructing the Laplacian Tensor\nFor each k-NN digraph N\nt(1≤t≤|T|)o fN, compute\nDtas a diagonal matrix the diagonal elements of which are\ndeﬁned as follows:\n1In this small example, to avoid numerical problems, we assign similarity equal\nto 0 when at least one item has no edge at all in the corresponding bipartite\ngraphs. Moreover, to avoid cluttering the graph, only the non-zero similarities\nare depicted.T1:/g30.71 0.71Elvis Elvis Bt l Beatles\nT0710.82Elvis Elvis Beatles Beatles\nT2:/g30.71\nT:0710.58Mozart Mozart Bach Bach\nT2:/g30.71\n(a) (b)\nFigure 3 . The k-NN multidigraph for the running example.\nDt(i, i)=|I|/summationdisplay\nj=1Nt(i, j) (1)\nThe Laplacian matrix, Lt, of each Ntis computed as\nfollows [10]:\nLt=1 I−D−1/2\nt NtD−1/2\nt (2)\nwhere 1Iis the identity matrix.\nThe Laplacian tensor of Nis therefore deﬁned as L∈\nR|I|×| I|×| T|, whose elements are given as follows:\nL(i, j, t )= Lt(i, j) (3)\nThus, each matrix Lt, for 1≤t≤|T|, comprises a frontal\nslice in L.\nThe Laplacian tensor Lhas 3 modes : the ﬁrst mode cor-\nresponds to the items, the second mode to the neighboring\nitems, and the third mode to the tags. To perform spectral\nclustering, we are interested in extracting the spectrum of\nLfor the ﬁrst mode. This procedure is explained in the\nsection to follow.\n5.2 Factorising the Laplacian Tensor\nIn this subsection, we summarise the factorisation of the\nLaplacian tensor using Tucker decomposition [11], which\nis the high-order analogue of the Singular Value Decompo-\nsition (SVD) for tensors. The factorisation of the Laplaciantensor will produce the required spectrum of its ﬁrst (cor-\nresponding to items) mode.\nFirst, we deﬁne the n-mode product T×\nnMbetween\na general N-order tensor T∈RI1×...×INand a matrix\nM∈RJn×In. The result is an (I1×I2×...×In−1×\nJn×In+1×...×IN)-tensor, whose entries are deﬁned\nas follows (elements are denoted through their subscript\nindexes):\n(T× nM)i1i2...i n−1jnin+1...i N=\n/summationdisplay\ninTi1i2...i n−1inin+1...i NMjnin(4)\nSince Lis a 3-order tensor, we henceforth focus only on\n1-mode, 2-mode and 3-mode products.\nThe Tucker decomposition of the 3-order tensor Lcan\nbe written as follows [12]:\n16210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nL≈C× 1P1×2P2×3P3 (5)\nThe P1∈R|I|×| I|,P2∈R|I|×| I|,P3∈R|T|×| T|are\ncalled the mode-1 (items), mode-2 (neighboring items),and mode-3 (tags) projection matrixes, respectively. The3 projection matrixes contain the orthonormal vectors foreach mode, called the mode-1, mode-2 and mode-3 sin-gular vectors, respectively. Cis called the core tensor and\nhas the property of all orthogonality. Nevertheless, unlikeSVD for matrixes, Cis not diagonal. Recently, several al-\ngorithms have bee proposed to efﬁciently compute the com-ponents of the Tucker decomposition. Due to lack of space,\nmore details about the algorithms and their complexity can\nbe found in a recent survey on tensor factorisation [11].\nHaving already performed the Tucker decomposition of\nthe Laplacian tensor L, we are interested in the mode-1\nsingular vectors that are stored in P\n1. A frequently fol-\nlowed approach in spectral clustering, when cclusters are\nrequired, is to select the ceigenvectors associated to the c\nsmallest eigenvalues [5]. Similarly, we select the cmode-1\nsingular vectors in P1associated to the smallest singular\nvalues in the core tensor C.\n5.3 Performing the Final Clustering\nTo ﬁnd cclusters of items using the cmode-1 singular\nvectors that were computed and selected during the fac-torisation of the Laplacian tensor, we apply the followingsteps: (1) Normalise the cselected mode-1 singular vectors\nto have norm equal to 1. (2) Form a matrix X∈R\n|I|×k,\nwhose columns are the normalised cselected mode-1 sin-\ngular vectors. (3) Associate each item ito a point xiwhose\ncoordinates are the contents of the i-th row of X. (4) Choose\na distance metric for the (xi)i=1 ,...,|I|points. (5) Cluster\nthe points (xi)i=1 ,...,|I|into cclusters using a conventional\nclustering algorithm, according to the chosen distance met-ric. (6) Assign each item to the cluster of its associatedpoint.\nDue to the properties of the Laplacian tensor, in prac-\ntice, the points in Xcan be easily clustered (Step 5) us-\ning simple conventional algorithms, like the K-Means orthe hierarchical agglomerative algorithms. In the sequel\nwe consider hierarchical agglomerative algorithms for this\npurpose based on Euclidean distance (Step 4).\nTherefore, the proposed approach can better detect the\nclustering as it fully exploits all users-items-tags relation-\nships. This is veriﬁed with the experimental results in thefollowing section.\n6. PERFORMANCE EVALUATION\n6.1 Experimental setting\nIn our experiments we tested the proposed method, de-\nnoted as Tag-aware Spectral Clustering (TSC). For com-\nparison purposes we tested two baseline Spectral Cluster-\ning methods, denoted as SC(U) and SC(T), that apply spec-\ntral clustering on a 2-dimensional item-user and item-tagmatrix, respectively. In the former matrix an element isset to 1 when the corresponding item has been tagged at\nleast once by the corresponding user (otherwise set to 0),whereas in the second matrix, when the corresponding itemhas been assigned at least once the corresponding tag (oth-erwise set to 0). All methods have been implemented in\nMatlab using the same components. Tensor factorisation\nwas computed using the Tensor toolbox\n2.\nWe used a real data set crawled from Last.fm (June\n2008) by using Last.fm web services. The music items cor-respond to song titles. There are 64,025 triplets in the formuser–tag–song. These triplets correspond 732 users, 2,527tags and 991 songs.\nSocial-tagging data present problems like tag polysemy\nand sparsity. To address them, we applied the widely used\ntechnique of Latent Semantic Indexing (LSI) and reducedthe number of dimensions in the modes of users and tags,\nby maintaining a percentage of them. This reduction wasperformed by modelling the original triples as a 3-modetensor and applying Tucker decomposition [11]. The item\nmode is left unchanged, whereas the number of maintained\nusers and tags after this process is expressed as a percent-\nage (default value 30%) of the original number of users andtags (for simplicity we use the same percentage for both).Both SC(U) and SC(T) also utilise this technique by main-taining the same percentage for users or tags.\nTo form the k-NN similarity graphs and multidigraphs,\nwe used the cosine distance, which is commonly applied\nfor 0-1 sparse data like in our case. We tested several val-\nues of kand found that all examined methods are not sen-\nsitive in this parameter (default value k=1 0 ). For the\nﬁfth step of the spectral clustering algorithm, we examined\nthe Unweighted Pair Group Method with Arithmetic mean\n(UPGMA) hierarchical agglomerative clustering algorithmover the Euclidean distance (in the spectral feature space).Following the approach of conventional spectral clusteringalgorithms [5], we considered the number of clusters as auser-deﬁned parameter. The quality of the ﬁnal clustering\nresult is measured with the popular Silhouette coefﬁcient\n(the higher the better) that expresses both the coherencywithin clusters and the separation between clusters. For an\nitem that is mapped to a vector xin the spectral feature\nspace and is assigned to cluster C, its silhouette coefﬁcient\ns(x)is calculated as follows: a\nxis the mean distance of\nxfrom all other vectors in C, whereas bxis the minimum\nmean distance from vectors in all other clusters except C.\nThen, s(x)=( bx−ax)/max( ax,bx). The overall silhou-\nette coefﬁcient is the mean of all s(x)for each x.3\n6.2 Experimental results\nWe experimentally compare TSC against SC(U) and SC(T).\nThe mean Silhouette coefﬁcients for varying number ofclusters is depicted in Figure 4. Due to its ability to con-sider 3-way relationships, TSC clearly outperforms the twobaseline methods, which suppress the 3-way relationships\n2http://csmr.ca.sandia.gov/ ∼tgkolda/TensorToolbox/\n3For all compared method the silhouette coefﬁcients are computed\nbased on the Euclidean distance in the resulting feature space.\n163Poster Session 1\ninto 2-way, thus loosing information that is valuable for the\nclustering.\n246810121416182000.250.50.751\nnum of clustersSilhouette coeff.TSC SC(U) SC(T)\nFigure 4 . Results for varying number of clusters.\nWe also tested the sensitivity of the result against the\npercentage of maintained users/tags after the application ofLSI (described in Section 6.1). Figure 5 depicts the result-ing Silhouette coefﬁcients for varying values of this per-centage (the number of clusters is set to 10). When the per-centage of maintained users/tags is severely low, the qual-\nity of TSC is reduced, as the resulting information is not\nadequate to capture the clustering structure. When the per-\ncentage is high, quality is again reduced, as the problems\nin the original data (polysemy, sparsity, noise) cannot be\naddressed. Therefore, in accordance to most applications\nof LSI, the best performance is attained with percentagesthat are in between the two extremes. In all cases, TSC\ncompares favorably against TS(U) and TS(T).\n10 20 30 40 50 60 7000.250.50.751\nperc. users/tagsSilhouette coeff.TSC SC(U) SC(T)\nFigure 5 . Results for varying perc. maintained users/tags.\n7. CONCLUSIONS\nWe proposed a novel, tag-aware clustering algorithm for\nmusic data from social tagging systems. The advantage of\nthe proposed algorithm over conventional clustering algo-\nrithms is that it preserves all 3 dimensions in the data and\nthe 3-way relationships among them. The 3-way relation-ships of the form items–users–tags between these dimen-\nsions offers a clear advantage between just knowing that a\ntag has been applied to an item regardless by which users,\nand knowing the speciﬁc users that applied this tag to theitem. To attain its advantages, the proposed algorithm usestensors to store the underlying data model represented with\nmultigraph structures, and extracts spectral features fromthem using tensor factorisation. Experimental results withreal data showed that the proposed method yields cluster-ing with better quality compared to conventional spectral\nclustering methods that suppress the dimensions and con-\nsider only 2-way relationships.\n8. REFERENCES\n[1] D. Byrd. Organization and searching of musical infor-\nmation, course syllabus, 2008.\n[2] A. Morgan and M. Naaman. Why we tag: motivations\nfor annotation in mobile and online media. In CHI ’07:\nProceedings of the SIGCHI conference on Human fac-\ntors in computing systems , pages 971–980, 2007.\n[3] Last.fm. Listen to free music with internet radio and\nthe largest music catalogue online, 2009.\n[4] MyStrands. Social recommendation and discovery,\n2009.\n[5] U. von Luxburg. A tutorial on spectral clustering. Tech-\nnical report, (No. TR-149) Max Planck Institute for Bi-\nological Cybernetics, 2006.\n[6] P. Lehwark, S. Risi, and A. Ultsch. Data Analysis, Ma-\nchine Learning and Applications , chapter Visualization\nand Clustering of Tagged Music Data, pages 673–680.Springer Berlin Heidelberg, 2008.\n[7] M. Levy and M. Sandler. Learning latent semantic\nmodels for music from social tags. Journal of New Mu-\nsic Research , 37(2):137–150, 2008.\n[8] N. Scaringella, G. Zoia, and D. Mlynek. Automatic\ngenre classiﬁcation of music content: A survey. IEEE\nSignal Processing Magazine , 23(2):133–141, 2006.\n[9] P. Symeonidis, M. Ruxanda, A. Nanopoulos, and\nY. Manolopoulos. Ternary semantic analysis of socialtags for personalized music recommendation. In Proc.\nInternational Conference on Music Information Re-trieval (ISMIR08) , 2008.\n[10] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clus-\ntering: Analysis and an algorithm. In Proceedings of\nthe Advances in Neural Information Processing Sys-tems (NIPS’01) , pages 849–856, 2001.\n[11] T. G. Kolda and B. W. Bader. Tensor decompositions\nand applications. SIAM Review , 51(3), September 2009\n(to appear).\n[12] L. de Lathauwer, B. de Moor, and J. Vandewalle. A\nmultilinear singular value decomposition. SIAM Jour-\nnal of Matrix Analysis and Applications , 21(4):1253–\n1278, 2000.\n164"
    },
    {
        "title": "RhythMiXearch: Searching for Unknown Music by Mixing Known Music.",
        "author": [
            "Makoto P. Kato"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416542",
        "url": "https://doi.org/10.5281/zenodo.1416542",
        "ee": "https://zenodo.org/records/1416542/files/Kato09.pdf",
        "abstract": "We present a novel method for searching for unknown music. RhythMiXearch is a music search system we developed that can accept two music inputs and mix those inputs to search for music that could reasonably be a result of the mixture. This approach expands the ability of Query-by-Example and allows greater flexibility for users in finding unknown music. Each music piece stored by our system is characterized by text data written by users, i.e., review data. We used Latent Dirichlet Allocation (LDA) to capture semantics from the reviews that were then used to characterize the music by Hevner’s eight impression categories. RhythMiXearch mixes two music inputs in accordance with a probabilistic mixture model and finds music that is the most likely product of the mixture. Our experimental results indicate that the proposed method is comparable to human in searching for music by multiple examples.",
        "zenodo_id": 1416542,
        "dblp_key": "conf/ismir/Kato09",
        "keywords": [
            "novel",
            "method",
            "music",
            "search",
            "unknown",
            "music",
            "system",
            "Query-by-Example",
            "flexibility",
            "review_data"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nRHYTHMIXEARCH: SEARCHING FOR UNKNOWN MUSIC BY MIXING\nKNOWN MUSIC\nMakoto P. Kato\nDepartment of Social Informatics, Graduate School of Informatics\nKyoto University, Kyoto, Japan\nkato@dl.kuis.kyoto-u.ac.jp\nABSTRACT\nWe present a novel method for searching for unknown\nmusic. RhythMiXearch is a music search system we de-\nveloped that can accept two music inputs and mix those\ninputs to search for music that could reasonably be a re-\nsult of the mixture. This approach expands the ability of\nQuery-by-Example and allows greater ﬂexibility for users\nin ﬁnding unknown music. Each music piece stored by our\nsystem is characterized by text data written by users, i.e.,\nreview data. We used Latent Dirichlet Allocation (LDA) to\ncapture semantics from the reviews that were then used to\ncharacterize the music by Hevner’s eight impression cate-\ngories. RhythMiXearch mixes two music inputs in accor-\ndance with a probabilistic mixture model and ﬁnds music\nthat is the most likely product of the mixture. Our experi-\nmental results indicate that the proposed method is compa-\nrable to human in searching for music by multiple exam-\nples.\n1. INTRODUCTION\nMuch music content has become available, and music anal-\nysis and retrieval systems have recently been rapidly devel-\noping. To make ﬁnding music easy, many prototype sys-\ntems for searching for music pieces by using content-based\nIR techniques have been proposed [17] [6]. They enable\nusers to ﬁnd music by inputting an audio ﬁle as a query,\ncalled Query-by-Example (QBE), in particular inputting by\nhumming, i.e., Query-by-Humming (QBH) [4]. Based on\nthe input audio signals, QBE systems retrieve music by\ncalculating the similarity between the queried music piece\nand stored music and then return the results in the order of\nsimilarity to the query. Searching by example is helpful for\nobtaining new music similar to music that you have or that\nyou have heard.\nHowever, these content-based IR methods are not able\nto meet the speciﬁc needs of users wanting to ﬁnd music\nthey have never heard. A common situation is that you\nwant to ﬁnd a certain piece of music which you imagine\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.in your mind, but have neither the keywords related to it,\nmusic similar to it, nor the ability to sing it. In addition,\ncontent-based approaches rank at the top only music simi-\nlar to what you know well, so you cannot ﬁnd music very\ndifferent from yours; the opportunity to discover new mu-\nsic is lost. This is caused by the lack of ﬂexibility in in-\nputting queries. As the amount of digital music content in-\ncreases, ﬁnding the precise music you want requires higher\nexpressiveness of queries.\nWe present a novel approach to searching for unknown\nmusic. RhythMiXearch is a music search system we devel-\noped that can accept two or more music inputs. By mixing\nthe input music, it searches for music that could reason-\nably be a result of the mixture. This approach expands the\nability of Query-by-Example and allows greater ﬂexibility\nfor users in ﬁnding unknown music. For example, intu-\nitively, RhythMiXearch can introduce music similar to The\nBeatles’ Let It Be +Coldplay’s Viva la Vida to you.\nStored music in RhythMiXearch is characterized on the\nbasis of users’ impressions. We retrieved review data on\nAmazon.com , analyzed the text data by using Latent Dirich-\nlet Allocation (LDA) [2], and determined the impressions\nthat users received from the music.\nThere is a strong reason that users’ impressions were\nused as a feature of the music and were extracted from the\nreview data rather than the features of the music itself be-\ning used. Consider music that users do not know but want\nto ﬁnd. The mood or impression the music will give users\nis more important than the timbre [1] or rhythm [5] [12] it\nhas. Users are likely not able to imagine the details of the\nwanted music, such as the timbre and rhythm; they only\nfeel the sense of the music they want, such as the mood\nand impression. In addition, in our approach to mixing in-\nput music, picturing mixtures of timbre and rhythm would\nbe difﬁcult, and for users, the result may not be what is\nexpected or wanted. For detecting the impression given by\nmusic pieces, seeing review text written by humans about\nthe music would be more effective than analyzing the mu-\nsic itself. Mood detection by signal analysis has been pro-\nposed [15] [14]. However, the ﬁnal feeling we get from lis-\ntening to music is a product of knowing the title and artist,\nlistening to the melody, understanding the lyrics, and so\non; simply analyzing the timbre and rhythm of a piece is\nnot enough for estimating what listeners will feel. In con-\ntrast, reviews are provided by music listeners, so analyzing\nreview text rather than the music itself would be helpful for\n477Poster Session 3\ndetermining the impression given by the music.\nInput music is combined based on the features of the\nmusic represented by the estimated impression, and our\nsystem ranks its stored music pieces by their likelihood of\nreasonably being a result of the combined input music. Sit-\nuations in which multiple examples could be used include\nthe following: searching for music that has all the features\nof multiple music inputs, and searching with multiple in-\nputs of your favorite music. For these situations, we devel-\noped a method to combine two music inputs in one query.\nWe named the multiple input query Query-by-Mixture-of-\nExamples .\n2. RELATED WORK\nCharacterizing music by using text data has been reported\nrecently. Knees et al. used Web documents to develop a\nsystem that searches for music pieces through natural lan-\nguage queries [11] and also presented a method to combine\nsignal-centered features with document-centered ones [9].\nThey characterized music pieces by using a conventional\nIR approach, which is the Vector Space Model with tf-idf\nmethod. In addition to searching for music, artist classi-\nﬁcation [10] was done by the same text-based approach\nwith the SVM. Pohle et al. [13] describe artists by com-\nmon topics or aspects extracted from Web documents. A\nbrowser application they presented enables users to formu-\nlate a query to search for desired artists by simply adjusting\nslider positions.\nTurnbull et al. [16] focused on natural language queries\nsuch as “ female lead vocals ” , called Query-by-Semantic-\nDescription (QBSD). In their approach, the Computer Au-\ndition Lab 500-Song (CAL500) data set was used to learn a\nword-level distribution over an audio feature space. QBSD\ncan search for music pieces unfamiliar to users, which is\nthe same aim as ours. Terms used as queries to illustrate\nmusic, however, are limited with regard to amount and can-\nnot capture subtle nuances to search for wanted music.\nMusic Mosaics [18] is a concept for creating a new\nquery by concatenating short segments of other music pieces.\nIt applies the signal analysis technique to characterize mu-\nsic and represents pieces of the music by thumbnails. Query-\ning with multiple music pieces in music mosaics is quite\nsimilar to our method, but as mentioned above, making a\nquery by assembling pieces of signal information to ﬁnd\nunfamiliar music is difﬁcult.\nSimilar to our approach, MusicSense [3] is a music rec-\nommendation system for users reading Web documents such\nas Weblogs. It adopted a generative model called Emo-\ntional Allocation Modeling to detect emotions of docu-\nments and music with the text. In this model, a collection\nof terms is considered as generated over a mixture of emo-\ntions, like the LDA approach.\n3. METHODOLOGY\nAt ﬁrst, we propose a framework of our approach. Then,\nwe explain song characterization with reviews by using\nReviews on\nMusic PiecesHevner’s 8 Categories \nlofty\nlightexciting tendersad martial\nhappy quiet\nMixing Input \nMusic PiecesDetecting Music Impression by \nUsing Latent Dirichlet Allocation\nMusic DatabaseUser\nInput Multi Example\nRanking Music3.1.\n2.Figure 1 . Framework of our approach.\n1\nspiritual\n8 lofty 2\nvigorous awe-inspiring pathetic\nrobust dignified doleful\nemphatic sacred sad\nmartial solemn mournful\nponderous sober tragic\n7 majestic serious melancholy 3\nexhilarated exalting frustrated dreamy\nsoaring depressing yielding\ntriumphant gloomy tender\ndramatic heavy sentimental\npassionate dark longing\nsensational yearning\nagitated pleading\nexciting 6 4 plaintive\nimpetuous merry lyrical\nrestless joyous 5 leisurely\ngay humorous satisfying\nhappy playful serene\ncheerful whimsical tranquil\nbright fanciful quiet\nquaint soothing\nsprightly\ndelicate\nlight\ngraceful\nFigure 2 . 8 sets of impression words proposed by Hevner.\nAdjacent sets are similar impressions, and opposite ones\nare counter-impressions.\nLDA, probabilistic mixture model for combining input mu-\nsic pieces, and ranking music pieces by the similarity.\n3.1 Framework of Our Approach\nThe framework of our approach is shown in Fig. 1. It\nconsists of three steps: (1) detecting impressions of music\npieces by using LDA from music reviews, (2) mixing in-\nput music pieces on the basis of the impressions, and (3)\nranking stored music pieces by their likelihood of being\nthe result of the mixture.\nFor extracting impressions from music reviews, we used\na generative model named LDA, in which it is assumed that\nterms in a document are generated by a mixture of topics,\ni.e., multinomial distributions over topics. The assump-\ntion enables us to conjecture the fundamental meanings of\ndocuments, and the meanings are represented by the topic\ndistribution for each document.\nThe sets of impression words for music proposed by\nHevner [8] are shown in Fig. 2. The impression words are\nused to ﬁnd which impression a review gives in the genera-\n47810th International Society for Music Information Retrieval Conference (ISMIR 2009)\ntive model , in intuitive terms, by calculating the similarity\nbetween reviews and the impression words, where we re-\ngard the sets of impression words as documents. We obtain\nthe probability that each distribution over topics for a doc-\nument would generate a set of impression words if only\nHevner’s sets of impression words were provided.\nGiven multiple music inputs, we mix on the basis of the\nimpression probability. Different mixture models are pro-\nposed for different situations. Finally, the results from the\nstored music pieces are returned, ranked by the similarity\nto the mixture of multiple examples. One easy method is\nthe similarity-based ranking between stored music and the\nvirtual music created as a result of a mixture. We apply\nthis method to our system and introduce a prototype sys-\ntem based on the framework.\n3.2 Characterizing Songs by Reviews\nFirst, we introduce a method to characterize songs by an-\nalyzing text review data with LDA. In the LDA analysis,\nterms in a document are assumed to be generated from a\ntopic and topics allocated to words are chosen from multi-\nnomial distributions for the documents. Each multinomial\ndistribution is selected from the Dirichlet distribution, which\nis often adopted as a prior distribution for a multinomial\ndistribution.\nThe LDA generative process consists of choosing pa-\nrameters for each document was follows.\n1. Choose θ∼Dirichlet( α).\n2. For each ithword wiin document w,\n(a) choose a topic zi∼multinomial( θ), and\n(b) choose a word wifromp(wi|zi, β), a multino-\nmial distribution conditioned on the topic zi,\nwhere αandβare hyper-parameters for a corpus that was\nassumed to be previously ﬁxed in this paper, θis deter-\nmined for a document, and wiandzifor a word.\nThe probability over the ithword for a multinomial dis-\ntribution θis given by\np(wi|θ, β) =∑\nzip(wi|zi, β)p(zi|θ). (1)\nThe probability p(zi|θ)characterizes a document by the\ntopics, which have lower dimensions Kthan the words.\nEach topic is represented by the word-occurrence p(wi|zi, β).\nWith multiplication of all the Nwords in a document\nwand integration over θ, the occurrence distribution of a\ndocument wis computed as\np(w|α, β) =∫\np(θ|α)(N∏\ni=1∑\nzip(wi|zi, β)p(zi|θ))\ndθ.\n(2)\nTaking the product of all the documents in a corpus, we\nobtain the occurrence probability of the corpus. We use the\nGibbs sampling technique [7] to estimate the parameters\nfor the probability of the corpus and obtain the approxi-\nmate distribution p(wi|zi, β)and the parameter θ, which is\nallocated to each document.\nθ z w\nθ z wReviews\nSets of Impression W ords)|(whpαβFigure 3 . Graphical model representation of Latent\nDirichlet Allocation and of detecting impressions given by\nmusic reviews. The upper outer rectangle represents re-\nviews, and the inner rectangle represents the chosen topics\nand words in a review. The bottom outer rectangle repre-\nsents sets of impression words. We estimate impressions of\nmusic by calculating the probability p(h|w)that a multi-\nnomial distribution for a review wgenerates a set of im-\npression words h.\nAfter analyzing a corpus, we calculate the probability\nthat a topic distribution for a document would generate a\nset of impression words. The distribution is denoted by\np(h|w), where his a variable for Hevner’s sets of impres-\nsion words Hand is one of the sets. A graphical model rep-\nresentation of LDA and of detecting impressions given by\ndocuments is shown in Fig. 3. Through Bayes’ theorem,\np(h|w)is represented by only the product p(w|h)p(h):\np(h|w) =p(w|h)p(h)∑\nhp(w|h)p(h), (3)\nwhere parameter βis omitted and p(h)is assumed to be\nthe same for all hinH.\nThe probability p(w|h)is divided by the latent param-\neters or the topics:\np(w|h) =N∏\ni=1∑\nzip(wi|zi)p(zi|θh). (4)\nθhis a parameter of a multinomial distribution for a set of\nimpression words h, which is estimated regarding the set\nas a document.\nFinally, summing up over all documents for a music\npiece, i.e., reviews, we obtain the probability p(h|m)that\na music piece mgenerates an impression h:\np(h|m) =∑\nw∈Dmp(h|w)p(w|m), (5)\nwhere Dmis a collection of reviews for music piece mand\nwe assume the same distribution for p(w|m), i.e., 1/|Dm|.\nThe probability p(h|m)can be explained as an impression\nrepresented by a set of words hat the probability p(h|m)\nobtained by a user listening to music m.\n479Poster Session 3\nThe Beatles\nLet It BeColdplay\nViva la Vida\nFigure 4 . The left chart represents Let It Be byThe Bea-\ntles. The right chart represents Viva la Vida byColdplay .\nThe numbers correspond to those in Fig. 2.\nExamples are shown in Fig. 4. The reviews for the two\npieces of music were downloaded from Amazon.com , and\nthe probability p(h|m)was visualized by Google Chart\nAPI1.\nThere are two reasons we put the topic distributions into\neight impression categories. First, to measure the similar-\nity between music pieces effectively, we should select the\nmost suitable topic, i.e., give weight to topics that strongly\nrepresent the music features and reduce the weight of those\nthat do not relate to the features. This is because all the top-\nics do not necessarily represent features of the music, e.g.,\na topic may simply indicate that a music piece is expen-\nsive. Second, to convey to users why the speciﬁc results\nwere returned, the music must be visualized in some way.\nThis is important particularly in a situation when a user\nwants to ﬁnd unknown music.\n3.3 Probabilistic Mixture Model\nIn the previous subsection, we characterized music pieces\nbyp(h|m), which is the probability that the music mgives\nan impression hrepresented by some adjectives. On the\nbasis of this probability, two music pieces input by users\nare combined and a new probability for the result of the\nmixture is generated. A basic method is to compute the\naverage of two given distributions p(h|mx)andp(h|my),\ni.e.,{p(h|mx) +p(h|my)}/2. However, this is likely to\nprovide a ﬂattened distribution whose probabilities are sim-\nilar. An ordinary average operation has a potential prob-\nlem: a remarkable feature on the distribution may be ig-\nnored in the result of the combination. Thus, we propose\ntwo mixing operations for two input distributions that can\nbe used in different situations.\n3.3.1 Feature-preserved Mixture\nTo combine two music pieces while preserving their fea-\ntures, we suppose the following probabilistic process.\n1. Choose one of two input music pieces at a 1/2prob-\nability.\n2. Repeat two impression extractions from the chosen\nmusic until the extracted impressions converge.\n1http://code.google.com/intl/en/apis/chart/3. Adopt the concurrent impression as the result of the\nmixture for the two music pieces.\nThe process is given by the following equation:\np(h|mz) =1\n2{p(h|mx)2\n∑\nhp(h|mx)2+p(h|my)2\n∑\nhp(h|my)2}\n,(6)\nwhere p(h|mx)andp(h|my)are the distributions over the\nimpressions for input music mxandmy, respectively, and\np(h|mz)is that for virtual music mzassumed to be the\nresult of the mixture.\nThe operation to adopt the concurrent impression en-\nhances the outstanding probability in each distribution. This\nmethod to combine two music pieces is suitable for a situ-\nation where users want music that has the remarkable fea-\ntures of both pieces.\n3.3.2 Product Mixture\nThe second approach to mix two music pieces effectively\nis to accentuate the features common to both music inputs.\nThis is achieved by the formula\np(h|mz) =p(h|mx)p(h|my)∑\nhp(h|mx)p(h|my). (7)\nThis operation corresponds to the following process.\n1. Repeat extractions of the impression from each mu-\nsic piece until the extracted impressions converge.\n2. Adopt the concurrent impression as the result of the\nmixture for the two music pieces.\nThis method is suitable for a situation where users want\nmusic that has a remarkable feature common to input mu-\nsicmxandmy. It can be applied for recommending music\nby using multiple music pieces listened to by users as a\nquery.\n3.4 Ranking by Similarity between Music Pieces\nThe virtual music resulting from the combination of two\nmusic inputs is characterized by a distribution p(h|mz),\nand the music in a system is ranked by closest similarity\nand returned as a search result. Here, deﬁning the similar-\nity between two music pieces is necessary.\nGenerally, the Kullback-Leibler divergence DKL(p||q)\nis used for the similarity of probabilistic distributions pand\nq. This function is not symmetric, thus we take the average\nof the two versions and deﬁne the similarity between two\nmusic pieces mxandmy, letting p=p(h|mx)andq=\np(h|my):\nSim(mx, my) = exp[\n−1\n2{DKL(p||q) +DKL(q||p)}]\n.\n(8)\nGiven the distribution p(h|mz)for a virtual music piece,\neach music piece m∈Min a system is returned on the ba-\nsis of the similarity Sim(mz, m).\n48010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nNumber of impression Average percentage of songs\nneighbors in same genre\n1 0.579\n5 0.523\n10 0.488\n20 0.454\n50 0.407\n100 0.359\nAll 0.152\nTable 1 . Average percentage of most similar songs in same\ngenre\n4. IMPLEMENTATION\nWe collected music pieces and reviews from Amazon.com\nwith Amazon Web Services2, querying by artist names\nthat are listed in CAL500 [16]. We obtained 86,050 pieces,\nfor which 879,666 reviews were written; the average num-\nber of reviews per artist was about 10.2. The obtained re-\nviews were analyzed by GibbsLDA++3, which is an im-\nplementation of Gibbs sampling for LDA. As parameters\nin LDA, we ﬁxed the number of topics K= 100 and\nhyper-parameters α= 50/Kandβ= 0.1. We then con-\nducted 1000 iterations of Gibbs sampling for the parameter\nestimation.\n5. EVALUATION\n5.1 Evaluation of Characterization\nBefore evaluating our system, the performance of charac-\nterization by impressions must be clariﬁed. We evaluated\nour method in accordance with the objective evaluation by\nAucouturier et al. [1]. We calculated the correlation be-\ntween impression and genre similarity by using the songs\nin our system. Because Amazon.com has multiple labels\non songs, only 356 songs that had only 1 label and more\nthan 20 reviews were used in our evaluation , and the top\n11 genres used in our experiment were R&B, country, rap\nand hip-hop, classic rock, classical, jazz, blues, pop, alter-\nnative rock, world music, and soundtracks.\nThe results can be seen in Table 1 and Fig. 5. In Ta-\nble 1, the closest ksongs for a song were retrieved, the\npercentage of the same genre was calculated, and the av-\nerage was taken for all the songs. There was a low corre-\nlation between impression and genres. As indicated in the\nstudy on timbre and genre [1], this approach cannot mea-\nsure the performance correctly because two songs in the\nsame genre do not always give similar impressions. How-\never, comparing the results with those of timbre similarity,\nwe could show the effectiveness of review-centered char-\nacterization.\nA similarity matrix for each genre is shown in Fig. 5.\nEach cell represents the average of the similarity between\nsongs in two genres. We could see a difference between\nsongs in the same genre and different genres except in the\nalternative rock genre.\n2http://aws.amazon.com/\n3http://gibbslda.sourceforge.net/\nRap&Hip-Hop 0.879 0.824 0.841 0.838 0.836 0.531 0.678 0.776 0.811 0.614 0.827 \nCountry 0.824 0.871 0.821 0.806 0.827 0.505 0.719 0.791 0.827 0.555 0.837 \nClassic Rock 0.841 0.821 0.859 0.837 0.821 0.498 0.687 0.781 0.797 0.699 0.829 \nWorld Music 0.838 0.806 0.837 0.852 0.836 0.526 0.679 0.759 0.781 0.691 0.817 \nR&B 0.836 0.827 0.821 0.836 0.850 0.506 0.691 0.765 0.791 0.619 0.828 \nJazz 0.531 0.505 0.498 0.526 0.506 0.766 0.579 0.479 0.541 0.343 0.502 \nClassical 0.678 0.719 0.687 0.679 0.691 0.579 0.735 0.682 0.734 0.470 0.712 \nAlternative \nRock0.776 0.791 0.781 0.759 0.765 0.479 0.682 0.750 0.770 0.568 0.788 \nSoundtracks 0.811 0.827 0.797 0.781 0.791 0.541 0.734 0.770 0.833 0.539 0.806 \nBlues 0.614 0.555 0.699 0.691 0.619 0.343 0.470 0.568 0.539 0.878 0.613 \nPop 0.827 0.837 0.829 0.817 0.828 0.502 0.712 0.788 0.806 0.613 0.841\nR&HH CountryClassic \nRockWM R&B Jazz Classical ARSound-\ntracksBlues PopFigure 5 . Similarity matrix for 11 genres. Each cell repre-\nsents the average of similarity between songs in two gen-\nres. The black cells represent the maximum similarity in\neach row, and the gray cells represent the 2nd and 3rd max-\nimum similarity within 10% of the maximum in each row.\n5.2 Evaluation of Query-by-Mixture-of-Example\nComparing with results returned by a human, we investi-\ngated the performance of our proposed method to search\nwith a query by mixture of example. We asked a student\nwho knows music pieces well to choose reasonable songs\nas mixture of input queries listed in Table 2. Then, we\nasked 5 persons to listen input music and output music in-\ncluding both music recommended by human and returned\nbyRhythMiXearch , and to evaluate relevance of the out-\nputs in ﬁve levels. The result is shown in Fig. 6, where\nthe average scores were taken for each question, and the\nquestion numbers correspond to those in Table 2.\nIn some questions, music recommended by human were\nconsidered more relevant than the results returned by Rhyth-\nMiXearch . Our system is inferior to human in performance,\nhowever, the result by human should be regarded as the up-\nper bound in the evaluation. In the questions 4 and 5, the\nresults by RhythMiXearch obtained higher scores, whereas\nin the questions 2 and 3, our method failed to return rele-\nvant results for mixture of example. The result may show\nthat a human can recommend music only for similar two\nmusic like the inputs seen in the question 2 and 3, on the\none hand, our system can search for music even for differ-\nent types of music like the inputs used in the question 4\nand 5.\n6. CONCLUSION\nWe presented a novel method for searching for unknown\nmusic and also presented our developed system RhythMiX-\nearch , which can accept two music inputs and mix those\ninputs to search for music that could reasonably be a result\n481Poster Session 3\n# Input A Input B Human Feature-Preserved Mixture Product Mixture\n1 The Beatles, Let It Be Coldplay, Viva La Vida Bob Dylan, Blowin’ In the Wind Kiss, Dynasty The Black Crowes, Lions\n2 Michael Jackson, Thriller Madonna, Like a Virgin Jamiroquai, Cosmic Girl Jimi Hendrix, The Jimi Hendrix Experience *\n3 Eminem, The Eminem Show Britney Spears, Britney TLC, Silly Ho Green Day, Nimrod *\n4 Eric Clapton, 461 Ocean Boulevard John Lennon, Imagine Eagles, New Kid in Town Eric Clapton, Me and Mr. Johnson Cream, Disraeli Gears\n5The Cardigans, First Band on the Moon Whitney Houston, Whitney Janis Joplin, Half Moon Christina Aguilera, Stripped *\nTable 2 . 5 set of inputs and outputs for evaluation of Query-by-Mixture-of-Example. (* means the same result as Feature-\npreserved Mixture.)\n00.511.522.533.544.5\n12345Average of Score\nQuestion No.Human\nFeature-preserved\nProduct\nFigure 6 . Average of scores for each question\nof the mixture. Our ﬁrst contribution was to characterize\nmusic pieces by reviews with LDA and to evaluate the per-\nformance of the representation of the music pieces. The\nsecond contribution was to propose a probabilistic mixture\nmodel for processing multiple example queries. We be-\nlieve that Query-by-Mixture-of-Examples is an important\nconcept for searching for new music pieces.\n7. ACKNOWLEDGEMENTS\nThis research was supported by Exploratory IT Human Re-\nsources Project (MITOH Program Youth 2008). I express\nmy sincere gratitude to Professor Michiaki Yasumura, Keio\nUniversity, for comments on my work. Thanks to GOGA,\nInc. for supporting the project. Thanks to members of\nTanaka laboratory, especially Yusuke Yamamoto, for help-\ning the user experiments and discussion on my work.\n8. REFERENCES\n[1] J.J. Aucouturier and F. Pachet. Music similarity mea-\nsures: Whatfs the use. In Proc. of the 3rd ISMIR , pages\n157–163, 2002.\n[2] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent dirichlet\nallocation. The Journal of Machine Learning Research ,\n3:993–1022, 2003.\n[3] R. Cai, C. Zhang, C. Wang, L. Zhang, and W.Y. Ma.\nMusicSense: contextual music recommendation using\nemotional allocation modeling. In Proc. of the 15th\nMultimedia , pages 553–556, 2007.\n[4] R.B. Dannenberg and N. Hu. Understanding search\nperformance in query-by-humming systems. In Proc.\nof the 5th ISMIR , pages 232–237, 2004.\n[5] J. Foote, M. Cooper, and U. Nam. Audio retrieval by\nrhythmic similarity. In Proc. of the 3rd ISMIR , pages\n265–266, 2002.[6] M. Goto and K. Hirata. Recent studies on music infor-\nmation processing. Acoustical Science and Technology ,\n25(6):419–425, 2004.\n[7] T.L. Grifﬁths and M. Steyvers. Finding scientiﬁc\ntopics. Proc. of the National Academy of Sciences ,\n101(90001):5228–5235, 2004.\n[8] K. Hevner. Experimental studies of the elements of ex-\npression in music. The American Journal of Psychol-\nogy, pages 246–268, 1936.\n[9] P. Knees, T. Pohle, M. Schedl, and G. Widmer. A music\nsearch engine built upon audio-based and web-based\nsimilarity measures. In Proc. of the 30th SIGIR , pages\n447–454, 2007.\n[10] Peter Knees, Elias Pampalk, and Gerhard Widmer.\nArtist classiﬁcation with web-based data. In Proc. of\nthe 5th ISMIR , pages 517–524, 2004.\n[11] Peter Knees, Tim Pohle, Markus Schedl, Dominik\nSchnitzer, and Klaus Seyerlehner. A document-\ncentered approach to a natural language music search\nengine. In Proc. of the 30th ECIR , pages 627–631,\n2008.\n[12] J. Paulus and A. Klapuri. Measuring the similarity of\nrhythmic patterns. In Proc. of the 3rd ISMIR , pages\n150–156, 2002.\n[13] T. Pohle, P. Knees, M. Schedl, and G. Widmer. Mean-\ningfully Browsing Music Services. In Proc. of the 8th\nISMIR , pages 23–30, 2007.\n[14] M. Tolos, R. Tato, and T. Kemp. Mood-based naviga-\ntion through large collections of musical data. In Proc.\nof the IEEE 2nd CCNC , pages 71–75, 2005.\n[15] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vla-\nhavas. Multilabel classiﬁcation of music into emotions.\nInProc. of the 9th ISMIR , pages 325–330, 2008.\n[16] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Towards musical query-by-semantic-description\nusing the CAL500 data set. In Proc. of the 30th SIGIR ,\npages 439–446, 2007.\n[17] Rainer Typke, Frans Wiering, and Remco C. Veltkamp.\nA survey of music information retrieval systems. In\nProc. of the 6th ISMIR , pages 153–160, 2005.\n[18] G. Tzanetakis, A. Ermolinskyi, and P. Cook. Beyond\nthe query-by-example paradigm: New query interfaces\nfor music information retrieval. In Proc. of the 2002\nICMC , pages 177–183, 2002.\n482"
    },
    {
        "title": "Use of Hidden Markov Models and Factored Language Models for Automatic Chord Recognition.",
        "author": [
            "Maksim Khadkevich",
            "Maurizio Omologo"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415930",
        "url": "https://doi.org/10.5281/zenodo.1415930",
        "ee": "https://zenodo.org/records/1415930/files/KhadkevichO09.pdf",
        "abstract": "This paper focuses on automatic extraction of acoustic chord sequences from a musical piece. Standard and factored language models are analyzed in terms of applicability to the chord recognition task. Pitch class profile vectors that represent harmonic information are extracted from the given audio signal. The resulting chord sequence is obtained by running a Viterbi decoder on trained hidden Markov models and subsequent lattice rescoring, applying the language model weight. We performed several experiments using the proposed technique. Results obtained on 175 manually-labeled songs provided an increase in accuracy of about 2%.",
        "zenodo_id": 1415930,
        "dblp_key": "conf/ismir/KhadkevichO09",
        "keywords": [
            "automatic extraction",
            "acoustic chord sequences",
            "musical piece",
            "standard and factored language models",
            "applicability",
            "chord recognition task",
            "pitch class profile vectors",
            "harmonic information",
            "Viterbi decoder",
            "trained hidden Markov models"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nUSE OF HIDDEN MARKOV MODELS AND FACTORED LANGUAGE\nMODELS FOR AUTOMATIC CHORD RECOGNITION\nMaksim Khadkevich\nFBK-irst, Universit ´a degli studi di Trento,\nVia Sommarive, 14 - Povo - 38050, Trento, Italy\nkhadkevich@fbk.euMaurizio Omologo\nFondazione Bruno Kessler-irst\nVia Sommarive, 18 - Povo - 38050 Trento, Italy\nomologo@fbk.eu\nABSTRACT\nThis paper focuses on automatic extraction of acoustic\nchord sequences from a musical piece. Standard and fac-\ntored language models are analyzed in terms of applica-\nbility to the chord recognition task. Pitch class proﬁle vec -\ntors that represent harmonic information are extracted fro m\nthe given audio signal. The resulting chord sequence is\nobtained by running a Viterbi decoder on trained hidden\nMarkov models and subsequent lattice rescoring, applying\nthe language model weight. We performed several exper-\niments using the proposed technique. Results obtained on\n175 manually-labeled songs provided an increase in accu-\nracy of about 2%.\n1. INTRODUCTION\nAmong all existing musical styles, western tonal music,\nwhich is one of the most popular nowadays, is known for\nits strong relationship to harmony. Harmonic structure can\nbe used for the purposes of content-based indexing and re-\ntrieval since it is correlated to the mood, style and genre\nof musical composition. Automatic analysis of digital mu-\nsic signals has attracted the attention of many researchers ,\nestablishing and evolving the Music Information Retrieval\n(MIR) community. One of the largest research areas of the\ninterdisciplinary science of MIR is music transcription. A\nsubtask of this problem, which deals with the extraction\nof harmonic properties of audio signal, is chord recogni-\ntion. Basically, harmony denotes a combination of simul-\ntaneously or progressively sounding notes, forming chords\nand their progressions. In almost all cases the harmonic\nstructure of a piece of music can be converted into a chord\nsequence. A great interest in chords can be indicated by a\nnumber of websites containing chord databases for existing\npopular songs. Automatic extraction of harmonic structure\ncan also be of great use to musicologists, who perform har-\nmonic analysis over large collections of audio data.\nAs in the case of speech recognition, one of the most\ncritical issues in chord recognition is the choice of the\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .acoustic feature set to use in order to represent the wave-\nform in a compact way. One of the most successfully used\nfeature set is chromagram, which can be represented as\na sequence of chroma vectors. Each chroma vector, also\ncalled Pitch Class Proﬁle (PCP), describes the harmonic\ncontent of a given frame. The amount of energy for each\npitch class is described by one component in the PCP vec-\ntor. Since a chord consists of a number of tones and can be\nuniquely determined by their positions, chroma vectors can\nbe used effectively for chord representation. The chroma\nfeature was ﬁrstly introduced for music computing tasks by\nFujishima [1]. He proposed a real-time chord recognition\nsystem, describing extraction of 12-dimensional chroma\nvectors from the Discrete Fourier Transform (DFT) of the\naudio signal and introducing a numerical pattern match-\ning method using built-in chord-type templates to deter-\nmine the most likely root and chord type. The statistical\nlearning method for chord recognition was suggested by\nSheh and Ellis [2]. They exploited the Expectation-Maxi-\nmization (EM) algorithm to train hidden Markov models,\nwhile chords were treated as hidden states. Statistical in-\nformation about chord progressions in their approach is\nrepresented by the state transitions in HMM. The approach\nof Papadopoulos and Peeters [3] incorporates simultaneous\nestimation of chord progression and downbeats from an au-\ndio ﬁle. They paid a lot of attention to possible interaction\nof the metrical structure and the harmonic information of a\npiece of music.\nIncorporating statistical information on chord progres-\nsions into a chord recognition system is an important issue.\nIt has been addressed in several works through different\ntechniques. Mauch and Dixon [4] used one of the simplest\nforms of N-grams – the bigram language model. In the\napproaches of Papadopoulos and Peeters, Lee and Slaney\n[3,5] chord sequence modeling is introduced through state\ntransition probabilities in HMM. In their case ”language\nmodel” is a part of HMM and is derived from the Markov\nassumption, where chord probability is deﬁned by only\none predecessor. Yoshioka et al. [6] presented an auto-\nmatic chord transcription system which is based on gener-\nating hypotheses about tuples of chord symbols and chord\nboundaries, and further evaluating the hypotheses, taking\ninto account three criteria: acoustic features, chord pro-\ngression patterns and bass sounds. This approach was fur-\nther developed by Sumi et al. [7]. They mainly focused on\nthe interrelationship among musical elements and made an\n561Oral Session 7: Harmonic & Melodic Similarity and Summarization\nattempt to efﬁciently integrate information about bass lin es\ninto chord recognition framework. They used two 2-gram\nmodels, one for major keys and one for minor keys, which\nare obtained in advance from real music. A large study on\nthe modeling of chord sequences by probabilistic N-grams\nwas performed by Scholz et al. [8]. Unal et al. [9] used\nperplexity-based scoring to test the likelihoods of possib le\ntranscription sequences.\nThis paper investigates the applicability of standard and\nfactored language models of high orders (3-gram, 4-gram).\nExperiments with different back-off strategies for factor ed\nlanguage models are carried out.\nThe rest of the paper is organized as follows: section 2\ndescribes the front-end processing. In section 3 the here\nadopted HMM-based classiﬁcation engine is brieﬂy out-\nlined. Language modeling is presented in section 4. Sec-\ntion 5 is devoted to the description of the whole proposed\nchord recognition system. The experimental results and\nconclusion are then given in section 6 and section 7, re-\nspectively.\n2. FRONT-END PROCESSING\nBefore extracting features, the tuning procedure describe d\nin [10] is applied in order to ﬁnd the mis-tuning rate and\nset the reference frequency freffor the ”A4” tone. The\nnecessity of tuning appears when audio was recorded from\ninstruments that were not properly tuned in terms of semi-\ntone scale.\nThe feature extraction process starts with downsam-\npling the signal to 11025 Hz and converting it to the fre-\nquency domain by a DFT applying Hamming window of\n185.7 ms with 50% overlapping. The harmonic content is\nextracted from the frequency range between 100 Hz and 2\nkHz only. The main reason for this is the fact that in this\nrange the energy of the harmonic frequencies is stronger\nthan non-harmonic frequencies of the semitones. A se-\nquence of conventional 12-dimensional Pitch Class Proﬁle\n(PCP) vectors, known as chromagram is used as acoustic\nfeature set. Each element of PCP vector corresponds to the\nenergy of one of the 12 pitch classes. The process of PCP\nextraction can be decomposed into several steps. After ap-\nplying DFT, the energy spectrum is mapped to the chroma\ndomain, as shown in (1).\nn(fk) = 12log2/parenleftbiggfk\nfref/parenrightbigg\n+69,n∈ ℜ+(1)\nwhere frefdenotes the reference frequency of ”A4”\ntone, while fkandnare the frequencies of Fourier trans-\nform and the semitone bin scale index, respectively. To\nreduce transients and noise we apply smoothing over time\nusing median ﬁltering, similarly to Peeters [11] and Mauch\net al. [4]. At the last stage semitone bins are mapped\nto pitch classes, which results in the sequence of 12-\ndimensional PCP vectors:\nc(n) = mod( n,12) (2)Cm aj \nBegin End Bm ag C#m aj \nDm aj \nBm in \nInsertion penalty \nFigure 1 . Connection scheme of trained models for decod-\ning.\n3. HIDDEN MARKOV MODELS\nHidden Markov models, which have been successfully\nused for modeling temporal sequences, are utilized in the\nproposed approach.\nIn contrast to many existing approaches [2, 3, 5], where\nchord is represented as a hidden state in one ergodic HMM,\na separate left-to-right model is here created for each\nchord. In the given system conﬁguration each model con-\nsists of 3 hidden states. The entry and exit states of a\nHMM are non-emitting, while the observation probabili-\nties are identical for all emitting states. Observation vec tor\nprobabilities in the emitting states can be approximated by\na number of Gaussians in 12 dimensions, described by a\nmean vector and a covariance matrix. The feature vector\ncomponents are assumed to be uncorrelated with one an-\nother, so the covariance matrix has a diagonal form. For\neach observation we use a mixture of 512 12-dimensional\nGaussians. Songs from the training set are segmented ac-\ncording to the ground-truth labels so that each segment\nrepresents one chord. Chromagrams extracted from these\nsegments are used for training, which is based on the ap-\nplication of the Baum-Welch algorithm.\nBefore running the recognition task, we extract a chro-\nmagram for each song from the test data. There is no\npreliminary segmentation as done on the training data for\nwhich a chroma vector sequence is extracted for each\nchord segment; only one chromagram is obtained for the\nwhole test song. The trained chord HMMs are connected\nas shown in ﬁgure 1. Such parameter as insertion penalty\nis introduced, which allows for obtaining labels with dif-\nferent degrees of fragmentation. The Viterbi algorithm is\nthen applied to the test data by using the resulting con-\nnected trained model in order to estimate the most likely\nchord sequence for each song and to produce a chord lat-\ntice.\n4. LANGUAGE MODELING\nA lot of different statistical language models have been\nproposed over years. The most successful among them\nappeared to be ﬁnite state transducers. In Natural Lan-\nguage processing N-grams are used for word prediction.\nGiven N−1predecessors, it can provide the probability\nofN-th element appearing. Language models have a va-\nriety of applications such as automatic speech recognition\n56210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nand statistical machine translation. The main goal of lan-\nguage modeling can be explained as follows: having a sen-\ntence, which consists of Kwords ( w1,w2,...w K), gener-\nate a probability model p(w1,w2,...w K). In most common\ncases it can be expressed as (3).\np(w1,w2...wK) =/productdisplay\ntp(wt|w1,w2...wt−1) =/productdisplay\ntp(wt|ht)\n(3)\nwhere htis the history sufﬁcient for determining the\nprobability of wtword. In standard N-gram models the\nhistory consists of the immediately adjacent N−1words.\nFor example, in 3-gram model the probability of current\nword can be expressed as: p(wt|wt−1,wt−2).\nWhile estimating language model parameters, there ex-\nists the problem of sparse data. It is caused by the impos-\nsibility of producing maximum likelihood estimate of the\nmodel, because all combinations of N-word sequences are\nunlikely to be found in the training corpus. Since any train-\ning corpus is limited, some acceptable sequences can be\nmissing from it, which leads to setting zero probability to\nplenty of N-grams. In order to cope with the problem, dif-\nferent techniques, such as back-off, smoothing and inter-\npolation are used [12–14]. The main principle of back-off\nis to rely on lower-order model (e.g p(wt|wt−1)) if there\nis zero evidence for higher-order (e.g. p(wt|wt−1,wt−2))\nmodel. The order of dropping variables is known as back-\noff order. In the case of standard language models it is ob-\nvious that information taken from older predecessor will\nbe less beneﬁcial and it should be dropped prior to other\npredecessors.\nIn the proposed approach we draw direct analogy be-\ntween a sentence in speech and a tune in a piece of mu-\nsic. The above-described strategy can be successfully used\nin chord sequences modeling. In this case a chord is the\nequivalent of a word and the sequence of chords can be\nmodeled by means of the same technique.\n4.1 Factored language models\nWestern music is known to be highly structural in terms of\nrhythm and harmony. In order to take advantage of mutual\ndependency between these two phenomena, we have stud-\nied the interrelationship between beat structure and chord\ndurations. The number of occurrences as a function of\nchord duration in beats histogram is shown in ﬁgure 2. It\nis clearly seen that a greater part of chord durations is cor-\nrelated to the metrical structure (2, 4, 8, 12, 16, 24, 32\nbeats), which suggests that including also chord durations\nin the language model is more convenient than analyzing\njust a sequence of chord symbols. This can be easily done\nwith the help of factored language models (FLMs), which\ntreat a word (chord) as a set of factors. FLMs have been re-\ncently proposed by Bilmes and Kirchoff [15] and showed\npromising results in modeling highly inﬂected languages,\nsuch as Arabic [16].\nIn a factored language model, a word (chord) can be\nrepresented as a bundle of factors: wt={f1\nt,f2\nt,...,fK\nt}.\nThe probability for FLM is given in (4), where π(fk\nt)is0500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 28 29 30 32\nD uration in beats Number of ocurences \nFigure 2 . Chord Duration Histogram.\na set of variables (parents), which inﬂuence the probabil-\nity of fk\nt. In our case to model chord sequences we use\ntwo factors: chord label Ctand chord duration Dt:wt=\n{Ct,Dt}.\np(wt|ht) =/productdisplay\nkp(fk\nt|π(fk\nt)) (4)\nAs opposed to standard language models, where older\npredecessors give less relevant information at the given\ntime instant, in FLMs there is no obvious order to drop\nparents π(fk\nt). There are a lot of possibilities to choose\nless informative factors to drop among the others. More-\nover, keeping some factors of older predecessors can be of\ngreater beneﬁt than keeping the value of some other fac-\ntors, which are more relevant to the given time instant.\nOne of the possible solutions is to use ”generalized paralle l\nback-off”, which was initially proposed and well described\nby Bilmes and Kirchoff [15]. The main idea is to back-off\nfactors simultaneously. The given set of back-off paths is\ndetermined dynamically based on the current values of the\nvariables. (For a more detailed description, see [15]).\nAt the experimental stage we explore the standard back-\noff (a) and the parallel back-off (b) techniques, whose\ngraphs are presented in ﬁgure 3. In both cases the chrono-\nlogical order is kept, while in the standard back-off case a\nhigher priority to the factor of chord symbol is assigned.\nThe arrows are marked with the factor being dropped at\nthe current back-off step; blocks include the variables tha t\ninﬂuence the probability of chord label being estimated.\n5. CHORD RECOGNITION SYSTEM\nThe full scheme of chord recognition system is depicted in\nﬁgure 4.\nFeature extraction part has been described in section 2.\nThe beat extraction algorithm used here is introduced by\nDixon [17] and is exploited as a separate module, called\n563Oral Session 7: Harmonic & Melodic Similarity and Summarization\n1, 1 2 2 ( | , , ) t t t t t P C C D C D − − − − \n2tD−\n1, 1 2 ( | , ) t t t t P C C D C − − − \n2tC−\n1, 1 ( | ) t t t P C C D − − \n1tD−\n1 ( | ) t t P C C −\n1tC−\n( ) tP C 1, 1 2 2 ( | , , ) t t t t t P C C D C D − − − − \n2tD−\n1, 1 2 ( | , )t t t t P C C D C − − −2tC−\n1, 1 ( | ) t t t P C C D − − \n1tD−\n1 ( | ) t t P C C −1tC−\n( ) tP C 1, 1 2 ( | , ) t t t t P C C D D − − − \n1 ( | ) t t P C D −\na) b)\nFigure 3 . Standard back-off (a) and parallel back-off (b)\ngraphs for tri-gram LM.\nBeatRoot1.\nThe key detection module utilizes the approach sug-\ngested by Peeters [11], where trained HMMs are used to\nﬁnd the best score from 24 possible keys for the given se-\nquence of chroma vectors for each test song. In the sug-\ngested system the key is assumed to be constant.\nOn the training stage, features extracted from wave-\nforms are used to train hidden Markov models, while chord\nlabels from training corpus are used as an input for lan-\nguage model parameter estimation. Language model train-\ning includes training either standard LMs or FLMs. For\ntraining standard LMs chord sequences taken from the\ntraining labels are used as input. For building text for\nFLM the information combined from beat extraction mod-\nule and the training labels is used. For each chord symbol\nfrom ground-truth labels we estimate the duration in beats\nand produce an output in the form: ”C-(chord type):D-\n(duration)”. To minimize the problem of sparse data, all\nduration values are quantized by a relatively-small set of\nor integer values. Our codebook consists of the following\nvalues: 1, 2, 3, 4, 6, 8, 12, 16, 24 and 32 beats. The sug-\ngested codebook is supposed to be well-suited for the pop\nsongs. This assumption is made on the basis of metrical\nanalysis of the Beatles data (see ﬁg. 2). The suggested\nscheme however might not be sufﬁcient while modeling\njazz or other genres.\nIn order to make our system key invariant, a key trans-\nformation technique is proposed here. In fact, the training\ncorpus might not contain some type of chords and chord\ntransitions due to the fact that keys with a lot of accidental s\nare much less widespread (G# maj, Ab min). Moreover,\nwhile estimating chord transition probabilities the relat ive\nchange in the context of the given key (e.g. tonic – dom-\ninant – subdominant) is more relevant than exact chord\nnames. For training data we have ground-truth table of\n1http://www.elec.qmul.ac.uk/people/simond/beatroot/ind ex.htmlkeys for each song, while for test data we estimate key in\nthe key detection module. Then, similar to training HMMs,\nby applying circular permutation, features and labels are\nconverted to the Cmaj (in case of major key) or to Amin\n(in case of minor key). After the decoding procedure in\norder to produce ﬁnal labels (in the original key of the an-\nalyzed song) obtained labels are converted back using the\nsame scheme.\nSimilar to the approach of multiple-pass decoding,\nwhich has been successfully used in speech recognition\n[14], the decoding procedure consists of two steps. Dur-\ning the ﬁrst step time-and-space efﬁcient bigram language\nmodel is applied on the stage of Viterbi decoding, produc-\ning a lattice. A lattice can be represented by a directed\ngraph, where nodes denote time instants and arcs are dif-\nferent hypotheses. Since lattices contain the information\non the time boundaries, it is possible to make an estima-\ntion of duration in beats for each hypothesis. During the\nsecond step the obtained lattice is rescored applying more\nsophisticated language models (trigram and higher) on the\nreduced search space. Since the main problem is to ex-\ntract chord labels, it is not necessary to model chord dura-\ntion probabilities explicitly. Our decoding scheme, apply -\ning language modeling, is based on Viterbi decoding and\nsubsequent lattice rescoring, where lattices contain the i n-\nformation on possible chord boundaries. Chord durations\nare used only to deﬁne chord label probabilities and the\nresulting chord boundaries are obtained from the lattices.\nGenerally, standard LMs do not take into account duration\nfactor at all, the only important thing here is just a sequenc e\nof labels. The advantage of FLM is that when applying the\nlanguage model weight on the stage of lattice rescoring,\nchord durations contribute to the probabilities of differe nt\nhypotheses in the lattice.\nStandard LMs are manipulated using HTK2tools,\nwhile FLMs are managed using SRILM [18] toolkit, since\nHTK does not support this type of language models.\n6. EXPERIMENTS\nEvaluation of the proposed system was performed on the\nsongs taken from 12 Beatles albums, ground-truth annota-\ntions for which were kindly provided by C. A. Harte [19].\nThe system can distinguish 24 different chord types (major\nand minor for each of 12 roots). 7th, min7, maj7, minmaj7,\nmin6, maj6, 9, maj9, min9 chords are merged to their root\ntriads; suspended augmented and diminished chords are\ndiscarded from the evaluation task. The percentage of du-\nration of discarded chords results to be 2.71% of the whole\nmaterial. In order to prevent the lack of training data (some\nchord types can appear only few times in the training cor-\npus) only two models are trained: C-major and C-minor.\nFor this purpose, all chroma vectors obtained from labeled\nsegments are mapped to the C-root using circular permuta-\ntion. After that mean vectors and covariance matrices are\nestimated for the two models. All the other models can be\nobtained by a circular permutation procedure.\n2http://htk.eng.cam.ac.uk/\n56410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTraining data \naudio labels Test data \naudio \nTuning module \nFeature extraction \nmodule Training HMMs \nTuning module \nFeature extraction \nmodule Viterbi decoder \nOutput labels Training LMs Lattice rescoring LM \nFLM bigram\nKey detection \nmodule \nBeat detection Beat detection \nFigure 4 . Chord recognition system.\nFor evaluation, the recognition rate measure was used,\nwhich in the given case corresponds to the total duration of\ncorrectly classiﬁed chords divided by the total duration of\nchords, as reported in the following:\nrec.rate =|recognized chords | ∩ |ground −truth chords |\n|ground −truth chords |\n(5)\nThe evaluation was performed frame by frame, as it\nwas done under the MIREX3competition. In our ex-\nperiments 3-gram and 4-gram language models were used.\nWhile working with FLMs, we exploited standard and gen-\neralized parallel back-off strategies (see ﬁgure 3; 4-gram\ngraphs have the same structure and can be obtained from\n3-gram graphs by adding one level).\nIt is worth mentioning that applying different language\nmodel weights on the stage of lattice rescoring one can\nobtain different recognition rates. Figure 5 indicates how\nrecognition rate depends on the LM weight. In this case the\ncurves correspond to the LM- and FLM-based systems; ex-\nperiments were conducted on the fold 1 with 4-gram con-\nﬁguration.\nIn order to estimate the increase in performance intro-\nduced by including LM block and in order to compare efﬁ-\nciency of standard and factored language models, a 5-fold\ncross-validation was accomplished on the given data set.\nThe folds were built in a random way and there is high al-\nbum overlap. The recognition rates are shown in Table 1.\nHere ”bl” is baseline system, ”3lm” ”3ﬂm” ”3ﬂmgpb” are\ntrigram conﬁgurations with key transformation for stan-\ndard LM, FLM, and FLM with generalized parallel back-\noff respectively, ”4lm” ”4ﬂm” ”4ﬂmgpb” are 4-gram con-\nﬁgurations. For any of the given conﬁgurations, an aver-\nage standard deviation of about 15% was also observed,\n3http://www.music-ir.org/mirex/2008/index.php/Main Page69.48 69.98 70.48 70.98 71.48 71.98 72.48 \n1 3 5 7 9 11 13 15 17 19 lm \nflm \nFigure 5 . Recognition rate as a function of LM weight.\nwhich was derived from the recognition rates computed on\na song-by-song basis.\nExperimental results showed that introducing language\nmodeling increases the performance of the system, while\ngeneralized parallel back-off strategy for FLM did not\nshow any advantages over standard back-off for the chord\nrecognition task. Meanwhile, using FLM show very slight\nimprovement (0.25 %) in comparison to standard LM.\nThe differences in the output labels for LMs and FLMs\nare mainly on the junctions of chords. While using stan-\ndard LM one can get a slight boundary deviation from its\nground-truth value (e.g. 1 beat), using FLM ﬁxes this in\nmost cases because it takes into account the duration fac-\ntor. That is why the difference in recognition rates is so\nsmall.\n7. CONCLUSION\nIn this paper a set of experiments on chord recognition\ntask including language modeling functionality as a sep-\narate layer has been conducted. The experimental results\nin a 5-fold cross-validation were conducted on a com-\nmonly used database of the songs by the Beatles. Factored\nlanguage models were compared with standard language\nmodels and showed small increase in performance for the\ntask. The main advantage of FLMs is that they possess\na better chord recognition ability on the chord junctions.\nComparing back-off techniques, we can assume that using\ngeneralized parallel back-off for the chord recognition ta sk\ndoes not result in better performance.\nHowever, the suggested system has a number of limita-\ntions: assuming the key of the song constant, one can not\ncope with key changes. A deeper study on different model\nsmoothing and selection techniques as those addressed by\nScholz et al. [8] could be reprised.\nIn general, experimental results showed that utilizing\nlanguage models leads to an increase in accuracy by about\n2%. This relatively small difference in performance may\nbe due to the size of vocabulary for the chord recognition\ntask in comparison with that of many speech recognition\napplications. The performance of chord recognition sys-\n565Oral Session 7: Harmonic & Melodic Similarity and Summarization\ndata bl 3lm 3ﬂm 3ﬂmgpb 4lm 4ﬂm 4ﬂmgpb\nfold 1 70.81 72.22 72.55 72.56 72.39 72.53 72.27\nfold 2 70.23 70.78 71.15 71.51 71.09 71.38 71.25\nfold 3 65.87 66.81 66.59 67.01 67.22 66.89 67.17\nfold 4 66.20 67.15 67.60 67.61 67.64 67.62 67.51\nfold 5 66.19 69.73 69.72 68.55 68.55 69.72 69.77\naverage 67.86 69.34 69.52 69.45 69.38 69.63 69.59\nTable 1 . Evaluation results: recognition rates.\ntems is perhaps inﬂuenced primarily by relevance and ac-\ncuracy of the extracted features and related acoustic mod-\neling.\n8. REFERENCES\n[1] Takuya Fujishima. Realtime chord recognition of mu-\nsical sound: A system using common lisp music. In\nProceedings of the International Computer Music Con-\nference , Beijing, 1999.\n[2] A. Sheh and D. P. Ellis. Chord segmentation and recog-\nnition using em-trained hidden markov models. In\nProc. 4th International Conference on Music Informa-\ntion Retrieval , 2003.\n[3] H. Papadopoulos and G. Peeters. Simultaneous estima-\ntion of chord progression and downbeats from an audio\nﬁle. In Proc. ICASSP , 2008.\n[4] Matthias Mauch and Simon Dixon. A discrete mixture\nmodel for chord labelling. In Proceedings of the 2008\nISMIR Conference , Philadelphia, 2008.\n[5] K. Lee and M. Slaney. Acoustic chord transcription\nand key extraction from audio using key-dependent\nhmms trained on synthesized audio. IEEE Transactions\non Audio, Speech, and Language Processing , 16(2),\nfebruary 2008.\n[6] T. Yoshioka, T.Kitahara, K. Komatani, T. Ogata, and\nH.G. Okuno. Automatic chord transcription with con-\ncurrent recognition of chord symbols and boundaries.\nInProceedings of the 5th International Conference\non Music Information Retrieval (ISMIR) , Barcelona,\n2004.\n[7] K. Sumi, K. Itoyama, K. Yoshii, K. Komatani,\nT. Ogata, and H. G. Okuno. Automatic chord recog-\nnition based on probabilistic integration of chord tran-\nsition and bass pitch estimation. In Proceedings of the\n2008 ISMIR Conference , Philadelphia, 2008.\n[8] R. Scholz, E. Vincent, and F. Bimbot. Robust model-\ning of musical chord sequences using probabilistic n-\ngrams. In Proc. ICASSP , 2009.\n[9] E. Unal, P. Georgiou, S. Narayanan, and E. Chew. Sta-\ntistical modeling and retrieval of polyphonic music. In\nProc. IEEE MMSP , 2007.[10] M. Khadkevich and M. Omologo. Phase-change based\ntuning for automatic chord recognition. In Proceedings\nof DAFX , Como, Italy, 2009.\n[11] G. Peeters. Chroma-based estimation of musical key\nfrom audio-signal analysis. In Proceedings of the 2006\nISMIR Conference , Victoria, Canada, 2006.\n[12] J. Goodman. A bit of progress in language modeling.\nInComputer, Speech and Language , 2001.\n[13] F. Jelinek. Statistical methods for speech recognitio n.\nInMIT Press , 1997.\n[14] D. Jurafsky and J. H. Martin, editors. Speech and\nLanguage Processing: An Introduction to Natural\nLanguage Processing, Computational Linguistics, and\nSpeech Recognition . Prentice Hall, 2000.\n[15] J. Bilmes and K. Kirchoff. Factored language mod-\nels and generalized parallel backoff. In HLT-NAACL ,\n2003.\n[16] K. Kirchhoff, D. Vergyri, K. Duh, J. Bilmes, and\nA. Stolcke. Morphology-based language modeling for\narabic speech recognition. In Computer, Speech and\nLanguage , 2006.\n[17] S. Dixon. Onset detection revisited. In Proceedings of\nDAFX , McGill, Montreal, Canada, 2006.\n[18] A. Stolcke. Srilm. an extensible language modeling\ntoolkit. In Proc. Intl. Conf. on Spoken Language Pro-\ncessing , 2002.\n[19] C. Harte and M. Sandler. Symbolic representation of\nmusical chords: A proposed syntax for text annota-\ntions. In Proceedings of the 2005 ISMIR Conference ,\n2005.\n566"
    },
    {
        "title": "Using Artist Similarity to Propagate Semantic Information.",
        "author": [
            "Joon Hee Kim",
            "Brian Tomasik",
            "Douglas Turnbull"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416510",
        "url": "https://doi.org/10.5281/zenodo.1416510",
        "ee": "https://zenodo.org/records/1416510/files/KimTT09.pdf",
        "abstract": "Tags are useful text-based labels that encode semantic information about music (instrumentation, genres, emotions, geographic origins). While there are a number of ways to collect and generate tags, there is generally a data sparsity problem in which very few songs and artists have been accurately annotated with a sufficiently large set of relevant tags. We explore the idea of tag propagation to help alleviate the data sparsity problem. Tag propagation, originally proposed by Sordo et al., involves annotating a novel artist with tags that have been frequently associated with other similar artists. In this paper, we explore four approaches for computing artists similarity based on different sources of music information (user preference data, social tags, web documents, and audio content). We compare these approaches in terms of their ability to accurately propagate three different types of tags (genres, acoustic descriptors, social tags). We find that the approach based on collaborative filtering performs best. This is somewhat surprising considering that it is the only approach that is not explicitly based on notions of semantic similarity. We also find that tag propagation based on content-based music analysis results in relatively poor performance.",
        "zenodo_id": 1416510,
        "dblp_key": "conf/ismir/KimTT09",
        "keywords": [
            "tag propagation",
            "data sparsity problem",
            "artist similarity",
            "genres",
            "acoustic descriptors",
            "social tags",
            "collaborative filtering",
            "content-based music analysis",
            "performance",
            "semantic information"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nUSING ARTIST SIMILARITY TO PROPAGATE SEMANTIC\nINFORMATION\nJoon Hee Kim, Brian Tomasik, Douglas Turnbull\nDepartment of Computer Science,\nSwarthmore College\n{joonhee.kim@alum, btomasi1@alum, turnbull@cs }.swarthmore.edu\nABSTRACT\nTags are useful text-based labels that encode semantic\ninformation about music (instrumentation, genres, emo-\ntions, geographic origins). While there are a number of\nways to collect and generate tags, there is generally a data\nsparsity problem in which very few songs and artists have\nbeen accurately annotated with a sufﬁciently large set of\nrelevant tags. We explore the idea of tag propagation to\nhelp alleviate the data sparsity problem. Tag propagation,\noriginally proposed by Sordo et al., involves annotating a\nnovel artist with tags that have been frequently associated\nwith other similar artists. In this paper, we explore four\napproaches for computing artists similarity based on dif-\nferent sources of music information (user preference data,\nsocial tags, web documents, and audio content). We com-\npare these approaches in terms of their ability to accurately\npropagate three different types of tags (genres, acoustic de-\nscriptors, social tags). We ﬁnd that the approach based\non collaborative ﬁltering performs best. This is somewhat\nsurprising considering that it is the only approach that is\nnot explicitly based on notions of semantic similarity. We\nalso ﬁnd that tag propagation based on content-based mu-\nsic analysis results in relatively poor performance.\n1. INTRODUCTION\nTags, such as “hair metal”, “afro-cuban inﬂuences”, and\n“grrl power”, are semantic labels that are useful for seman-\ntic music information retrieval (IR). That is, once we anno-\ntate (i.e., index) each artist (or song) in our music database\nwith a sufﬁciently large set of tags, we can then retrieve\n(i.e., rank-order) the artists based on relevance to a text-\nbased query.\nThe main problem with tag-based music IR is data\nsparsity (sometimes referred to as the cold start problem\n[1]). That is, in an ideal world, we would know the rele-\nvance (or lack thereof) between every artist and every tag.\nHowever, given that there are millions of songs and poten-\ntially thousands of useful tags, this is an enormous anno-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.tation problem. For example, Lamere [2] points out that\nLast.fm, a popular music-oriented social network, has a\ndatabase containing over 150 millions songs each of which\nhave been tagged with an average of 0.26 tags. This prob-\nlem is made worse by popularity bias in which popular\nsongs and artists tend to be annotated with a heavily dis-\nproportionate number of tags. This is illustrated by the fact\nthat Lamere found only 7.5%of artists in his corpus of\n280,000 artists had been annotated with one or more tags.\nOne potential solution to the data sparsity problem is\ntopropagate tags between artists based on artist similarity.\nTo annotate tags for an artist a, we ﬁnd the most similar\nartists to a(referred to as neighbors) and transfer the most\nfrequently occurring tags among the neighbors to artist a.\nNote that while we focus on artist annotation in this pa-\nper, our approach is general in that it could also be use to\npropagate tags between songs as well as other non-music\nrelated items such as movies and books.\nTag propagation has two potential uses. First, it allows\nus to index an unannotated artist if we can calculate the\nsimilarity between the artist and other annotated artists.\nSecond, tag propagation allows us to augment and/or im-\nprove an existing annotation for an artist.\nThis idea was originally proposed by Sordo et al. who\nexplore tag propagation of social tags based on acoustic\nsimilarity [3]. This content-based approach is compelling\nbecause we can automatically calculate artist similarity\nwithout relying on human input. However, as we will show\nin Section 5, the content-based tag propagation performs\npoorly relative to other music information sources.\nIn this paper, we extend their initial exploration by com-\nparing alternative approaches to compute similarity: col-\nlaborative ﬁltering of user preference data, similarity based\non social tags, text-mining of web documents, and content-\nbased analysis of music signals. In addition, we experi-\nment with tag propagation on three different types of tags:\nacoustic descriptors, genres, and social tags.\nWhile our focus is on the use of tag propagation for text-\nbased music IR, we can also view our system as a way to\nevaluate artist similarity metric. That is, the approach that\nresults in the best transfer of semantic information between\nartists may be considered a good approach for accessing\nartist similarity. Since artist similarity is often used for\nmusic recommendation, evaluating tag propagation perfor-\nmance is an automatic alternative to using labor-intensive\nhuman surveys when determining the quality of a music\n375Oral Session 5: Tags\nrecommendation system.\n1.1 Related Work\nThe importance of annotating music with tags is under-\nscored by large investments that have been made by var-\nious companies in recent years. Companies like Pandora\nand AMG Allmusic employ dozens of professional mu-\nsic editors to manually annotate music with a small and\nstructured vocabulary of tags. While this approach tends to\nproduce accurate and complete characterizations of some\nsongs, this labor-intensive approach does not scale with the\nrapidly increasing amount of available music online. For\nexample, 50 Pandora experts annotate about 15,000 songs\nper month and would take over 83 years to annotate the\n15 million songs that are currently in the AMG Allmusic\ndatabase1\nLast.fm and MyStrands use an alternative “crowdsourc-\ning” approach in which millions of registered users are en-\ncouraged to label songs with any open-end free-text tags.\nAs of September 2008, Last.fm had collected over 25 mil-\nlion song-tag annotations and 20 million artist-tag anno-\ntations using a vocabulary of 1.2 million unique tags (al-\nthough only about 11% had been used more than 10 times)\n[4]. Each month, about 300 thousand unique users con-\ntribute more than 2.5 million new song-tag or artist-tag an-\nnotations. However, as mention above, a relatively small\npercentage of artists and songs have ever been tagged and\neven fewer have been thoroughly annotated.\nAcademic research has also focused on the music an-\nnotation problem in recent years. Turnbull et al. suggest\nthat there are ﬁve general distinct approaches to annotat-\ning music with tags: conducting a survey (e.g., Pandora),\nharvesting social tags (e.g., Last.fm), playing annotation\ngames [5, 6], text-mining web documents [7, 8], and ana-\nlyzing audio content with signal processing and machine\nlearning [9–11]. In some sense, tag propagation represents\na sixth approach because it is based on the notions of artist\nsimilarity. That is, propagation can incorporate other forms\nof music information, such as user preference data, to gen-\nerate tags for music. However, it cannot be used in isola-\ntion from these other approaches because it makes direct\nuse of an initial set of annotated artists.\nIn the next section, we present the general tag propa-\ngation algorithm. We then introduce four different music\ninformation sources that are individually useful for calcu-\nlating artist similarity. Section 4 describes the two evalua-\ntion metrics that we use to test our system with a database\nof 3,500 artists, four similarity metrics, and three types of\ntags. We discuss the results in Section 5, and conclude in\nSection 6.\n2. TAG PROPAGATION\nCompared with other automatic tagging algorithm, tag\npropagation is relatively straightforward. Suppose that we\n1Pandora statistics are based on personal notes for a public talk\nby Pandora founder Tim Westergren. AMG statistics were found at\nhttp://www.allmusic.com.want to annotate a novel artist a. We ﬁnd the most simi-\nlar artists of a, combine existing annotations of them, and\nselect the tags that appear frequently.\nMore formally, tag propagation requires two matrices:\nasimilarity matrix Sand a tagmatrix T.Sis an artist-\nby-artist similarity matrix where [S]i,jindicates similarity\nscore between artist iandj.Tis an artist-by-tag matrix\nwhere [T]a,trepresents the strength of association between\nartist aand tag t. In this paper, we consider the entries\ninTto be a binary number of 0 or 1, where 0 represents\nunknown or weak association, and 1 indicates a strong as-\nsociation. We call the a-th row of Tthetag annotation\nvector , and denote as ta.\nOnce we have a similarity matrix S(as described in Sec-\ntion 3), we can use the standard k-Nearest Neighbor (kNN)\nalgorithm to propagate tags. For the artist ain question, we\nﬁnd the kmost similar artists (i.e., the neighbors), which\nwe denote asNa. The neighbors are the columns corre-\nsponding to the klargest values in the a-th row of S. We\naverage the annotation vectors from TofNato estimate\nthe annotation vector ˆtaofa.\nˆta=/summationtext\ni∈Nati\nk(1)\nBased on an exponential grid search with k∈{2i|0≤i≤\n6}, we ﬁnd that kbetween 8 and 64 results in comparable\nperformance for each of our approaches. As such, we set\nk= 32 for each of our experiments in Section 5.\n3. ARTIST SIMILARITY\nIn this section, we describe ways in which we can calcu-\nlate artist similarity matrices from four different sources\nof music information.2In that our goal is to evaluate tag\npropagation, we primarily make use of existing music IR\napproaches [12–15].\n3.1 Collaborative Filtering (CF)\nCollaborative ﬁltering (CF) is a popular commercial tech-\nnique for calculating artist similarity [16] that is based on\nuser preference data. The idea is that two artists are con-\nsidered similar if there is a large number of users that listen\nto both artists. In this paper, we consider two forms of user\npreference data: explicit feedback and implicit feedback.\nFeedback is explicit if a user has indicated directly that he\nor she “likes” an artist. This information is often recorded\nby a user through a button on a music player interface. Im-\nplicit feedback is found by tracking user listening habits.\nFor example, Last.fm monitors which songs each of their\nusers listens to over a long period of time. Implicit feed-\nback assumes that two artists are similar if many users lis-\nten to songs by both artists.\nWe aggregate user preference data from 400,000\nLast.fm users, and build an artist similarity matrix, CF-\nExplicit , by counting the number of users who have ex-\nplicitly indicated that they like both artists. We construct\n2The data that we describe in this paper was collected from the Inter-\nnet in April of 2008.\n37610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTable 1 . Most similar pairs of artists based on CF (explicit) and their top social tags.\nTex Ritter country classic country country roots oldies old timey\nRed Foley country classic country boogie rock american\nUnwound noise rock post-hardcore indie rock math rock post-rock\nYoung Widows noise rock post-hardcore math rock experimental heavy\nDLG salsa latin dlg bachata spanish\nPuerto Rican Power salsa latin mambo latino cuba\nStarkillers dance house trance electro house electronica\nKid Dub electro electro house electronic dub electro-house\nLynda Randle gospel female vocalists christian southern gospel female vocalist\nGeorge Jones country classic country americana singer-songwriter traditional country\nAn Albatross experimental grindcore noisecore hardcore noise\nSee You Next Tuesday grindcore deathcore mathcore experimental noisecore\na second similarity matrix, CF-Implicit , by counting the\nnumber of users who listen to both artists at least 1% of\nthe time.\nOne issue that arises when using the raw co-occurrence\ncounts is that the popular artists tend to occur frequently\nas a “most similarity” artist [16]. A standard solution is to\nnormalize by the popularity of each artists:\n[S]i,j=co(i, j)/radicalbig/summationtext\nk∈Aco(i, k)/radicalbig/summationtext\nk∈Aco(k, j)(2)\nwhereAis the set of 3,500 artists, co(i, j)is the number\nof users that have given feedback for both artist iand artist\nj(explicit or implicit depending on the matrix type). Note\nthat this equation is equivalent to the cosine distance be-\ntween two column vectors of a User-by-Item rating matrix\nif we assume that users give binary rating [16].\nIt could be the case that similarity based on CF is not\nstrongly related to semantic similarity, and thus might not\nbe useful for tag propagation. However, if we look at a cou-\nple of examples (see Table 1), we ﬁnd that similar artists\nshare a number of common tags. This is conﬁrmed in Sec-\ntion 5.1, when we quantitatively compare the performance\nof tag propagation using CF-Explicit andCF-Implicit . We\nalso report on the effect of popularity normalization for\nthese two approaches.\n3.2 Social Tags (ST)\nAs described in Section 1.1, social tags (ST) are socially\ngenerated semantic information about music. Lamere and\nCelma [13] show that computing artist similarity using so-\ncial tags produces better performance for music recom-\nmendation than other approaches such as collaborative ﬁl-\ntering, content-based analysis, or human expert recom-\nmendations.\nFollowing their approach, we collect a set of social tags\n(represented as a tag annotation vector ta) for each artist\nafrom Last.fm. However, when collecting this data set,\nwe found a total of about 30,000 unique tags for our 3,500\nartists from Last.fm. Since Last.fm allows anyone to applyany tag, this vocabulary of tags contains many rare tags that\nseemed to be (inconsistently) applied to a small number of\nartists [1]. In an attempt to clean up the data, we choose\nto prune tags that are associated with less than .5% of the\nartists. This resulted in vocabulary of 949 unique tags.\nThe ST artist similarity matrix Sis built by calculating\ncosine similarity between each annotation vector:\n[S]i,j=ti·tj\n|ti||tj|(3)\nwhere each annotation vector tis a vector over 949 dimen-\nsion.\n3.3 Web Documents (WD)\nWeb documents represent a third source of music informa-\ntion that can be used to calculate music similarity. For each\nartist a, we collect 50 documents from the Google Search\nEngine3with query ‘‘artist name’’ music . We\ncombine the top 50 results into a single document and\nthen represent that document as a bag-of-words. This\nbag-of-words is converted into the term-frequency-inverse-\ndocument-frequency (TF-IDF) document vector daover a\nlarge vocabulary of words [17]. TF-IDF is a standard text-\nIR representation that places more emphasis on words that\nappear frequently in the given document and are less com-\nmon in the entire set of documents.\nWe build the WD artist similarity matrix Sby calculat-\ning cosine similarity score on each pair of TF-IDF docu-\nment:\n[S]i,j=di·dj\n|di||dj|(4)\nwhere i, jare artists.\n3.4 Content-Based Analysis (CB)\nLastly, we explore two content-based (CB) approaches for\ncalculating artist similarity that have performed well in var-\nious MIREX tasks [12,15,18] in recent years. For both ap-\nproaches, we begin by extracting a bag of Mel-Frequency\n3www.google.com\n377Oral Session 5: Tags\nCepstral Coefﬁcients (MFCCs) feature vectors from one\nrandomly selected song by each artist.\nOur ﬁrst approach, which was proposed by Mandel and\nEllis [12] (referred to as CB-Acoustic ), models the bag-\nof-MFCCs with a single Gaussian distribution over the\nMFCC feature space. To calculate the similarity between\ntwo artists, we calculate the symmetric KL divergence be-\ntween the two Gaussian distributions for the songs by the\ntwo artists. For this approach, we use the ﬁrst 20 MFCCs\nand estimate the Gaussian distribution using a full covari-\nance matrix. This approach is chosen because it is fast,\neasy to compute, and a popular baseline within the music-\nIR community.\nThe second approach, proposed by Barrington et al.\n[15] (referred to as CB-Semantic ), involves estimating the\nKL-divergence between the two Semantic Multinomial dis-\ntributions corresponding to the selected songs for each pair\nof artists. A semantic multinomial is a (normalized) vec-\ntor of probabilities over a vocabulary of tags. To calcu-\nlate the semantic multinomial, we ﬁrst learn one Gaussian\nMixture Model (GMM) over the MFCC feature space for\neach tag in our vocabulary. The GMMs are estimated using\ntraining data (e.g., songs that are known to be associated\nwith each tag) in a supervised learning framework. We\nthen take a novel song and calculate its likelihood under\neach of the GMMs to produce a vector of unnormalized\nprobabilities. When normalized, this vector can be inter-\npreted as a multinomal distribution over a semantic space\nof tags. We choose a vocabulary of 512 genres and acous-\ntic tags and use 39-dimensional MFCC+Delta feature vec-\ntors. MFCC+Delta vectors include the ﬁrst 13 MFCCs plus\neach of their 1st and 2nd instantaneous derivatives. This\napproach is chosen because it is based on a top performing\napproach in the 2007 MIREX audio similarity task and is\nbased on a top performing approach in the 2008 MIREX\naudio tag classiﬁcation task.\n4. EXPERIMENTAL SETUP\n4.1 Data\nOur data set consists of 3,500 artists with music that\nspans 19 top-level genres (e.g., Rock, Classical, Elec-\ntronic) and 123 subgenre (e.g., Grunge, Romantic Period\nOpera, Trance). Each artist is associated with 1 or more\ngenre and 1 or more subgenres. The set of 142 genres and\nsubgenres make up our initial Genre vocabulary.\nFor each artist, we collect a set of acoustic tags for songs\nby the artist from Pandora’s Music Genome Project. This\nAcoustic tag vocabulary consists of 891 unique tags like\n“dominant bass riff”, “gravelly male vocalist”, and “acous-\ntic sonority”. In general, these acoustic tags are thought to\nbeobjective in that two trained experts can annotate a song\nusing the same tags with high probability [19]. Lastly, we\ncollect social tags for each artist using the Last.fm public\nAPI as discussed in Section 3.2. After pruning, the Social\ntag vocabulary, it consists of 949 unique tags.\nIn all three cases, we construct a binary ground truth\ntag matrix Twhere [T]s,a= 1 if the tag is present for theartists (or in one of the songs by the artists), and 0 other-\nwise.\n4.2 Evaluation Metrics\nWe use leave-one-out cross-validation to test our system.\nFor each artist a, we hold out the ground truth tag anno-\ntation vector taand calculate the estimated vector ˆtaby\nkNN algorithm. In the artist annotation test, we test how\nwell we can propagate relevant tags to a novel artist by\ncomparing the estimated vector with the ground truth.\nIn the tag-based retrieval test, we generate a ranked\nlist of the artists for each tag based on their association\nstrength to a tag. Then we evaluate how high the relevant\nartists are placed on the ranked list. Each test is described\nin detail below.\nOne of our artist similarity metric is based on the simi-\nlarity of socially generated tags as discussed in Section 3.2.\nWe use tags generated by Last.fm users as our data source\nbecause it provides the largest data set of social tags. Un-\nfortunately, we evaluate our system on the same data as\nwell. Therefore, we use 10-fold cross-validation to evalu-\nate the propagation of social tags based on the similarity\nofsocial tags. That is, for each of 10 folds, we use 90%\nof the tags to estimate a song similarity matrix. This sim-\nilarity matrix is used to propagate the other 10% of the\ntags. We can combine the 10 estimated annotation tag vec-\ntors from each of the 10 folds into one complete annotation\nvector.\n4.2.1 Artist Annotation\nFor each artist a, we evaluate the relevance of the estimated\nannotation vector ˆtaby comparing it to the ground truth\nta. As described earlier, the ground truth data is in binary\nformat. We transform the estimated annotation vector into\nthe same binary vector by setting each value that is above\na threshold to 1, and zero otherwise.\nBy doing so, we move from the estimation problem to\nthe standard retrieval problem [17]. That is, we predict a\nset of relevant tags to describe the artist. We can then cal-\nculate precision, recall and f-measure for the given thresh-\nold. By varying threshold, we compute a precision-recall\ncurve as shown in Figure ??.\n4.2.2 Tag-Based Retrieval\nIn this experiment, we evaluate the performance of tag-\nbased retrieval of relevant artists. For each tag, we gen-\nerate a ranked-list of 3,500 artists. The rank is based on\nthe association score of the tag in each artist’s estimated\nannotation vector. Using the ground truth annotations, we\ncalculate R-precision, 10-Precision, MAP (mean average\nprecision) and AUC (area under the ROC curve) for each\ntag [17]. We then average the performance of the tags in\neach of our three tag vocabularies: Pandora Genre, Pan-\ndora Acoustic, and Last.fm Social.\n37810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTable 2 . Exploring variants of collaborative ﬁltering (CF):\nWe report the average f-measure / area under the ROC\ncurve (AUC) for explicit or implicit user preference infor-\nmation when we have either normalized or not normalized\nfor popularity. Each evaluation metric is the average value\nover the three tag vocabularies.\nUnnormalized Normalized\nExplicit .438 / .867 .495 / .885\nImplicit .410 / .824 .502 /.891\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 \n0 0.2 0.4 0.6 0.8 1 Precision \nRecall CF (Implicit) CF (Explicit) Social Tags Web Docs CB (Semantic) CB (Acoustic) Random \nFigure 1 . Semantic annotation and retrieval model dia-\ngram.\n5. RESULTS\n5.1 CF Comparison\nThe collaborative ﬁltering approach has four variants with\ntwo sets of varying conditions. First, we compare using\nexplicit and the implicit user preference data. Second, the\nsimilarity matrix Swas generated with and without the\npopularity-normalization. We evaluate the performance of\neach variant by comparing f-measure from the artist anno-\ntation test and area under the ROC curve (AUC) from the\ntag-based retrieval test.\nThe result of each test is illustrated in Table 2. In our\nexperiments, we observe no signiﬁcant difference between\nthe explicit and the implicit user preference data. However,\nin both cases, the normalization improves the performance.\nIt is interesting that the normalization boosts the perfor-\nmance of the implicit data more signiﬁcantly than the ex-\nplicit data. This could be due to the fact that implicit data\nmay be more prone to the popularity bias since Last.fm\nradio playlists tend to recommend music from popular\nartists [16].\n5.2 Artist Annotation\nThe precision-recall curves for artist annotation are plotted\nin Figure ??. For each test, we varied the threshold from0.1 to 0.4 with the interval of 0.01 and calculated preci-\nsion, recall, and f-measure. The baseline Random perfor-\nmance is calculated by estimating each annotation vector\nwithk= 32 distinct random neighbors. Except for the\nrandom baseline, the f-measure was maximized at around\na threshold of 0.3.\nIn general, the two variants of the collaborative ﬁlter-\ning (CF) approach perform best, with the implicit feedback\napproach performing slightly better. This is surprising be-\ncause the collaborative ﬁltering approach does not explic-\nitly encode semantic information whereas social tag, web\ndocuments, and CB-Semantic are based on the similarity\nof semantic information. This suggests that collaborative\nﬁltering is useful for determining semantic similarity as\nwell as music recommendation.\n5.3 Tag-based Retrieval\nWe evaluate tag-based music retrieval based on tag prop-\nagation using seven approaches to computing music simi-\nlarity. We report the performance for three vocabularies of\ntags (Genre ,Acoustic , andSocial ) in Table 3.\nAs was the case with artist annotation, both CF-Implicit\nand CF-Explicit show strong performance for all four met-\nrics and all three vocabularies. However, ST has the\nbest performance for R-Precision, 10-Precision, and MAP\nwhen propagating social tags.\nSince area under the ROC curve (AUC) is an evaluation\nmetric that is not biased by the prior probability of rele-\nvant artists for a given tag, we can safely compare average\nAUC values across the different tag vocabularies. Based\non this metric, we see that all of the approaches (except for\nthe CB-Acoustic) have higher AUC values in the order of\nGenre ,Acoustic , and Social tag sets. This suggest that\nit may be easiest to propagate genres and hardest to propa-\ngate social tags to novel artists.\nBoth CB approaches show relatively poor performance\n(though much better than random), which is disappointing\nsince all of the other methods require additional human in-\nput to calculate music similarity for a novel artist. That\nis, if either CB approached showed better performance, we\ncould remedy the data sparsity problem for novel artists\nwith a fully automatic tag propagation approach.\n6. CONCLUSION\nIn this paper, we have explored tag propagation as a tech-\nnique for annotating artists with tags. We explored al-\nternative ways to calculate artist similarity by taking ad-\nvantage of the existing sources of music information such\nas user preference data (CF), social tags (ST), web docu-\nments (WD), and audio content (CB). Each similarity met-\nric was tested on three distinct tag sets: genre ,acoustic ,\nandsocial . Both artist annotation , and tag-based retrieval\ntests show that CF generally performs the best, followed\nby ST, WD, and CB. This result is somewhat surprising\nbecause collaborative ﬁltering (CF) is solely based on the\naggregate trends of listening habits and user preferences,\nrather than explicitly representing music semantics. It con-\nﬁrms the idea that CF similarity (e.g., user behavior) can be\n379Oral Session 5: Tags\nTable 3 . Tag-based music retrieval performance. Each evaluation metric is averaged over all tags for each of the three\nvocabularies. R-precision for a tag is the precision (the ratio of correctly-labelled artists to the total number of retrieved\nartists) when Rdocuments are retrieved, where Ris the number of relevant artists in the ground-truth. Similarly, 10-\nprecision for a tag is the precision when 10 artists are retrieved (e.g., the “search engine metric”). Mean average precision\n(MAP) is found by moving down the ranked list of artists and averaging the precisions at every point where we correctly\nidentify a relevant artist based on the ground truth. The last metric is the area under the receiver operating characteristic\n(ROC) curve (denoted AUC). The ROC curve compares the rate of correct detections to false alarms at each point in the\nranking. A perfect ranking (i.e., all the relevant songs at the top) results in an AUC equal to 1.0. We expect the AUC to be\n0.5 if we randomly rank songs. More details on these standard IR metrics can be found in Chapter 8 of [17].\nApproach Genre (142 tags) Acoustic (891 tags) Social (949 tags)\nr-prec 10-prec MAP AUC r-prec 10-prec MAP AUC r-prec 10-prec MAP AUC\nRandom 0.012 0.015 0.017 0.499 0.025 0.023 0.029 0.495 0.030 0.029 0.033 0.498\nCF (implicit) 0.362 0.381 0.342 0.914 0.281 0.306 0.254 0.882 0.409 0.543 0.394 0.876\nCF (explicit) 0.362 0.388 0.329 0.909 0.282 0.304 0.246 0.878 0.410 0.562 0.396 0.869\nST 0.344 0.349 0.311 0.889 0.267 0.274 0.237 0.874 0.428 0.584 0.413 0.874\nWD 0.321 0.393 0.282 0.861 0.244 0.300 0.200 0.814 0.318 0.478 0.286 0.797\nCB (acoustic) 0.101 0.127 0.076 0.701 0.118 0.132 0.088 0.692 0.117 0.159 0.092 0.661\nCB (semantic) 0.087 0.103 0.069 0.687 0.115 0.123 0.091 0.714 0.107 0.126 0.084 0.662\nused to capture the semantic similarity (e.g., tags) among\nartists. We also found that two content-based approaches\n(CB) performed poorly in our experiments. This is un-\nfortunate because content-based similarity can be calcu-\nlated for novel artists without human intervention, and thus\nwould have solved the data sparsity problem.\n7. REFERENCES\n[1] D. Turnbull, L. Barrington, and G. Lanckriet. Five ap-\nproaches to collecting tags for music. ISMIR , 2008.\n[2] P. Lamere. Social tagging and music information re-\ntrieval. JNMR , 2008.\n[3] M. Sordo, C. Lauier, and O. Celma. Annotating mu-\nsic collections: How content-based similarity helps to\npropagate labels. In ISMIR , 2007.\n[4] P. Lamere and E. Pampalk. Social tags and music in-\nformation retrieval. ISMIR Tutorial, 2008.\n[5] D. Turnbull, R. Liu, L. Barrington, D. Torres, and\nG Lanckriet. Using games to collect semantic informa-\ntion about music. In ISMIR ’07 , 2007.\n[6] E. Law and L. von Ahn. Input-agreement: A new\nmechanism for data collection using human computa-\ntion games. ACM CHI , 2009.\n[7] B. Whitman and D. Ellis. Automatic record reviews.\nISMIR , 2004.\n[8] P. Knees, T. Pohle, M. Schedl, and G. Widmer. A music\nsearch engine built upon audio-based and web-based\nsimilarity measures. In ACM SIGIR , 2007.\n[9] M. Mandel and D. Ellis. Multiple-instance learning for\nmusic information retrieval. In ISMIR , 2008.[10] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Semantic annotation and retrieval of music and\nsound effects. IEEE TASLP , 16(2):467–476, February\n2008.\n[11] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green.\nAutomatic generation of social tags for music recom-\nmendation. In Neural Information Processing Systems\nConference (NIPS) , 2007.\n[12] M.I. Mandel and D.P.W. Ellis. Song-level features and\nsupport vector machines for music classiﬁcation. IS-\nMIR, 2005.\n[13] P. Lamere and O. Celma. Music recommendation tuto-\nrial notes. ISMIR Tutorial, September 2007.\n[14] A. Berenzweig, B. Logan, D. Ellis, and B. Whitman.\nA large-scale evalutation of acoustic and subjective\nmusic-similarity measures. Computer Music Journal ,\npages 63–76, 2004.\n[15] L. Barrington, A. Chan, D. Turnbull, and G. Lanckriet.\nAudio information retrieval using semantic similarity.\nICASSP , 2007.\n[16] O. Celma. Music Recommendation and Discovery in\nthe Long Tail . PhD thesis, Universitat Pompeu Fabra,\nBarcelona, Spain, 2008.\n[17] C.D. Manning, P. Raghavan, and H. Schtze. Introduc-\ntion to Information Retrieval . Cambridge University\nPress, 2008.\n[18] S. J. Downie. The music information retrieval evalu-\nation exchange (2005–2007): A window into music\ninformation retrieval research. Acoustical Science and\nTechnology , 2008.\n[19] T. Westergren. Personal notes from Pandora get-\ntogether in San Diego, March 2007.\n380"
    },
    {
        "title": "Using Harmonic and Melodic Analyses to Automate the Initial Stages of Schenkerian Analysis.",
        "author": [
            "Phillip B. Kirlin"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416654",
        "url": "https://doi.org/10.5281/zenodo.1416654",
        "ee": "https://zenodo.org/records/1416654/files/Kirlin09.pdf",
        "abstract": "Structural music analysis is used to reveal the inner workings of a musical composition by recursively applying reductions to the music, resulting in a series of successively ysis is the most well-developed type of structural analysis, and while there is a wide body of research on the theory, there is no well-defined algorithm to perform such an analysis. A automated algorithm for Schenkerian analysis would be extremely useful to music scholars and researchers studying music from a computational standpoint. The first major step in producing a Schenkerian analysis involves selecting notes from the composition in question for the primary soprano and bass parts of the analysis. We present an algorithm for this that uses harmonic and melodic analyses to accomplish this task.",
        "zenodo_id": 1416654,
        "dblp_key": "conf/ismir/Kirlin09",
        "keywords": [
            "Structural music analysis",
            "Recursive reductions",
            "Schenkerian analysis",
            "Automated algorithm",
            "Scholarly research",
            "Computational standpoint",
            "Primary soprano",
            "Bass parts",
            "Harmonic analyses",
            "Melodic analyses"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nUSINGHARMONICANDMELODICANALYSES TO AUTOMATE THE\nINITIAL STAGES OF SCHENKERIAN ANALYSIS\nPhillip B.Kirlin\nDepartment ofComputerScience,University of Massachusett sAmherst\npkirlin@cs.umass.edu\nABSTRACT\nStructural music analysis is used to reveal the inner work-\nings of a musical composition by recursively applying re-\nductions to the music, resulting in a series of successively\nmoreabstractviewsofthecomposition. Schenkeriananal-\nysis is the most well-developed type of structural analy-\nsis, and while there is a wide body of research on the the-\nory, there is no well-deﬁned algorithm to perform such an\nanalysis. A automated algorithm for Schenkerian analy-\nsis would be extremely useful to music scholars and re-\nsearchersstudyingmusicfromacomputationalstandpoint.\nThe ﬁrst major step in producing a Schenkerian analysis\ninvolves selecting notes from the composition in question\nforthe primary soprano and bass parts of the analysis. We\npresentanalgorithmforthisthatusesharmonicandmelodic\nanalyses toaccomplish thistask.\n1. INTRODUCTION\nNumeroustasksinmusicinformationretrievalcouldbeac-\ncomplished more effectively if information about musical\nstructure were readily available. For example, in the task\nof retrieving musical passages that are similar to a given\npassage, having structural analyses available would allow\nsimilarity metrics to be based on the underlying musical\nstructure of a composition as well as on the musical sur-\nface. An algorithm for structural analysis of music would\ntherefore be an indispensable resource in music informa-\ntionresearch.\nSchenkeriananalysis[1]isatypeofmusicanalysisthat\nemphasizesﬁndingstructuralrelationshipsamongthenote s\nof a composition. Developed by the Austrian music theo-\nrist Heinrich Schenker, Schenkerian analysis differs from\nothertypesofanalysisthatfocusonasingleaspectofmu-\nsic, such as the harmony or melody, to the exclusion of\nother aspects. Schenkerian analysis harnesses all aspects\nof a piece together to create an analysis that explains how\nvarious notes inthepiece function inrelationtoothers.\nOf particular importance in Schenkerian analysis is the\nidentiﬁcation of structural dependences among groups of\nPermission to make digital or hard copies of all or part of this w ork for\npersonal orclassroom use is granted without fee provided th at copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbearthis noticeand thefull citation ontheﬁrst page.\nc/circlecopyrt2009International Society forMusic InformationRetrieval .notes. If the way in which a note Xfunctions in a musi-\ncalpassageisduetothepresenceofanothernoteorgroup\nof notes Y, then Xis said to be dependent upon Y, and\nYis said to be at a higher structural level than X. The\nprocess of ﬁnding structural dependences proceeds recur-\nsivelyduringananalysis. Theﬁnalsetofdependencescan\nbe depicted as a tree, with the surface-level notes as the\nleaves. With each structural dependence located, the more\nstructurallyimportant notes areelevated tohigherlevels .\nThough a tree theoretically can show all the hierarchi-\ncal levels of a Schenkerian analysis, typically analyses ar e\nillustrated through a sequence of Schenker graphs . These\ngraphs are visual depictions of a few contiguous levels of\nthe note hierarchy, using staves with notes as in common\nmusic notation, but using other notation symbols such as\nstems,beams,andslurstoshowrelationshipsamongnotes\nratherthan timingorphrasing information.\nBecause Schenkerian analysis primarily focuses on the\nmain melodic line and the main harmonic bass line of the\nmusic in question, Schenkergraphs are often presented on\ntwo staves, with the primary melodic line on the upper\nstaff and the supporting bass harmony tones on the lower\nstaff. Notes of inner voices are occasionally shown on the\ngraphs,butaresometimesomittedwhentheyservetoonly\nﬁll out the harmony. We focus on foreground graphs , the\ngraphs that show the structural levels closest to the musi-\ncal surface. A foreground graph is usually the ﬁrst graph\nconstructed when completing a Schenkerian analysis; all\nsubsequentgraphsarebased—directlyorindirectly—on\ntheforegroundgraph. Therefore,itiscriticaltochooseth e\ncorrect set of notes to appear in the foreground graph. We\nwillcallaforegroundgraph,afternoteshavebeenselected\nfor its staves but before any reductions have been applied,\napreliminary foreground graph . Consider the ﬁrst eight\nmeasures of Schubert’s Impromptu No. 2 in A-ﬂat major ,\nshown in Figure 1. A preliminary foreground Schenker\ngraph for the Impromptu , with appropriate notes in the so-\nprano and bass parts,would look likeFigure 2.\nInthispaper,wepresentandanalyzeanalgorithm,F ORE-\nGRAPH, for identifying which notes in a score should be-\nlong on the soprano and bass staves of a preliminary fore-\ngroundSchenkergraph,basedonanalysesofharmonyand\nvoiceleading. WebuildontheworkofKirlinandUtgoff\n[2], whose IVI system requires, as a ﬁrst step, isolation of\nthe primary soprano and bass parts prior to analysis. Au-\ntomating Schenkerian analysis has been studied recently\nby Marsden [3–5] and Marsden and Wiggins [6]. These\n423Poster Session 3\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s1/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2/flags.u3\n/noteheads.s2/noteheads.s2/noteheads.s2\n/dots.dot/dots.dot/dots.dot\n/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/dots.dot/dots.dot/dots.dot\n/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s2\n/brace287\n/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2/flags.u3\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s2/dots.dot\n43/accidentals.flat/accidentals.flat/accidentals.flat/accidentals.flat/clefs.F43/accidentals.flat/accidentals.flat/accidentals.flat/accidentals.flat/clefs.G /dots.dot\n/noteheads.s1/noteheads.s1/noteheads.s1\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s1/noteheads.s2/rests.2/noteheads.s2 /noteheads.s2/flags.u3/noteheads.s2/flags.u3/noteheads.s2\n/noteheads.s2/noteheads.s2/dots.dot /noteheads.s2\n/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s1\n/dots.dot /dots.dot/dots.dot/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s1/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\nFigure 1. Anexcerpt from Schubert’s Impromptu No. 2inA-ﬂat major .\n/noteheads.s2\nI/noteheads.s2/noteheads.s2\nI6/noteheads.s2/noteheads.s2\nV65/noteheads.s2/noteheads.s2/noteheads.s2\nV42/noteheads.s2/noteheads.s2\n/brace292/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\n/accidentals.flat/accidentals.flat/accidentals.flat/accidentals.flat/clefs.F/noteheads.s2\nI/noteheads.s2\nI6/noteheads.s2/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\nA/accidentals.flat:V V65/noteheads.s2/noteheads.s2\nI/noteheads.s2/noteheads.s2\nV43/noteheads.s2/noteheads.s2/noteheads.s2\nV/noteheads.s2/noteheads.s2\nI/noteheads.s2/noteheads.s2 /accidentals.flat/accidentals.flat/accidentals.flat/accidentals.flat/clefs.G\nFigure 2. A preliminary foreground graph constructed by\nhand from the Impromptu .\nlinesofworkarepromising,buttheyhaveonlybeentested\non short, sometimes synthetic, musical phrases. Lerdahl\nand Jackendoff developed a grammatical approach to mu-\nsical structure in [7], which Hamanaka, et. al. [8] turned\nintoanalgorithm. Theirsystem,however,requiresmanual\nadjustmentofmanyparametersthatdifferforeachmusical\ncomposition. Older work by Kassler [9] and Smoliar [10]\ndemonstrated understanding of the principles involved in\nautomating analysis, but didnot provide any algorithms.\n2. COMPUTATIONAL METHODS FOR\nHARMONIC ANDMELODIC ANALYSIS\nSchenkeriananalysisisbasedontheprinciplesofharmony\nand voice leading. These two aspects of a composition\nmustbeexaminedpriortobeginningananalysis. Sincewe\ndesire a fully-automated system for producing foreground\ngraphs,wemustexaminevariousalgorithmsfordetermin-\ningtheharmonyatvariouspointsinacomposition,andthe\nvoice leading possibilitiesforany note inapiece.\nWe have chosen MusicXML as our representation of\nchoice. MusicXMLisaﬁleformatthatrepresentscommon\nWestern music notation by encoding the pitches and dura-\ntions of notes. Though the MIDI representation is more\nwidelyusedthanMusicXML,thelatterformatencodesan\nadditional wealth of information that the former does not\nsupply,suchaskeyandtimesignatures,stemandbeaming\ninformation, and slursand phrase marks.\nOne can look at harmonic analysis as occurring in two\nphases. First, a chord-labeling component assigns chord\nlabels (such as “C Major”) to segments of a composition.\nA second pass then uses the chord labels to assign func-\ntional Roman numerals tosegments.\n2.1 Chord Labeling\nA chord labeling component must divide a composition\ntemporallyintosegments,whereeachsegmentcorrespondsto a single harmony. We use a variant of Pardo and Birm-\ningham’s H ARMANalgorithm [11]toaccomplish this.\nHARMANusestwoseparatealgorithmstoperformchord\nlabeling. The labeling algorithm is concerned with deter-\nmining the best chord label for a given segment of music\n(asegmentbeinganintervaloftimewithﬁxedstartingand\nendingtimes), andthe segmentationalgorithm determines\nthe points in the music where the harmony changes. A\nharmony can change at a partition point : any place in the\nmusicwhere anote startsorstops.\nWhile H ARMANdoesaverygoodjob“outofthebox,”\nwe use a modiﬁed version of the algorithm and detail our\nchanges below.\n•Meter— HARMANdoes not take the meter of the\npiece into account, and sometimes it chooses a par-\ntition point in a metrically weak position that is ad-\njacent to a metrically stronger one. Because it is\npreferable to have a change of harmony in an anal-\nysis at a metrically strong position [12], we force\nHARMANchoose each measure boundary as a par-\ntitionpoint.\n•Octave doubling — Because H ARMANanalyzes\neach note in a segment independently, often notes\nthat are doubled at the octave exert too much of an\ninﬂuence over the chord labeling algorithm. There-\nfore, when analyzing a segment, we consider multi-\nple instances of notes with the same pitch class and\nduration as a single note. For example, in Figure\n1, the notes of the melody in measures 5–7 that are\ndoubled at the octave willnot becounted twice.\n•Neighbor tones — Our version of H ARMANig-\nnores“obvious”neighbortoneswithinsegments. An\nobvious neighbor tone is a note Ythat occurs in a\nnote sequence X−Y−Zwhere XandZhave\nthe same pitch, and are separated from Yby a half-\nstep. Without this correction, H ARMANhas trouble\ndistinguishing between chord tones and non-chord\ntones inheavily ﬁgurated contexts.\n2.2 Assignment of Roman Numerals\nGiven a chord labeling, the remaining task in harmonic\nanalysis is mapping the chord labels (such as “C Major”)\nto Roman numeral labels (such as “V6”). While we have\ninvestigatedalgorithmsforcomputingthekeyofacompo-\nsition and the locations of any modulations and toniciza-\ntions, we restrict ourselves for the remainder of this dis-\n42410th International Society for Music Information Retrieval Conference (ISMIR 2009)\ncussion to non-modulating pieces whose key is encoded\ncorrectly in their MusicXML representation. As F ORE-\nGRAPH, the foreground Schenker graph generation algo-\nrithm presented in the next section, relies on a correct Ro-\nman numeral analysis, placing this restriction on the input\nmusicmakesusmorecertainthatwearesupplyingcorrect\nRoman numerals to F OREGRAPH.\nThe second phase detects tonicizations by looking for\nconsecutive chords where the ﬁrst chord functions func-\ntionsasatemporarydominanttothesecond. Forexample,\ninthekeyofCmajor,thiswoulddetectthechordsequence\n“DMajor–GMajor”andchangetheharmonicanalysisof\n“II – V” to “V/V – V.” We stipulate that the ﬁrst chord\ncannot occur normally in the original key, to eliminate the\npossibility of the common “I – IV” chord sequence being\nreinterpreted as atonicization of theIV chord.\n2.3 Voice Leading Analysis\nA voice leading analysis determines, for every note in the\npiece, which notes could logically follow from that note,\naccording to the principles of voice leading [12]. Algo-\nrithms for determining voice leading, however, can differ\nin their interpretations of implicit polyphony [13]. For ex-\nample, given the four notes in the ﬁrst measure of Figure\n3, some algorithms would determine that all four notes\nbelong to a single voice, whereas others would ﬁnd two\nvoicesandinterpretthefournotesasstandingforthetriad s\nshowninthesecondmeasure. Thesecondinterpretationis\nan example of implicit polyphony.\n/noteheads.s2/noteheads.s2/timesig.C44/noteheads.s1/noteheads.s1/noteheads.s1/noteheads.s2 /noteheads.s1 /noteheads.s2/clefs.G\nFigure 3. An example where the voice leading is ambigu-\nous.\nSchenkerian analysis, as it gives primary consideration\nto the linear connections in music [14], requires a voice\nleading analysis that uncovers implicit polyphony. A rea-\nsonable way to handle this is to permit a voice-leading\nconnection between two notes only if the motion between\nthem isstepwise.\nIf one takes this stance, it is easy to construct an algo-\nrithmfordeterminingthevoiceleadingforagivencompo-\nsition. Fora note nin a piece, we examine the set of notes\nthat begin at times later than the ending of n(there can-\nnot be a voice-leading connection between two notes that\noverlap in time). Note nmay have up to three voice lead-\ningconnections: (1)astep-down connection, (2)astep-up\nconnection,and(3)asame-pitchconnection. Foreachtype\nof connection, we ﬁnd the earliest note that satisﬁes the\ncriteria for that kind of connection. We also require that\nifnhas a same-pitch voice-leading connection to a note\nm, then nmay not have any stepwise voice-leading con-\nnections to notes that begin later than m. This is because\nvoice-leadingconnectionsbetweennotesofidenticalpitc h\nare typically strongerthan stepwiseconnections.3. PRODUCING PRELIMINARY FOREGROUND\nGRAPHS\nRecall that our goal is to produce preliminary foreground\nSchenkergraphsliketheoneinFigure2. Sincethepurpose\nofaforegroundgraphistocapturetheprimarysopranoand\nbass tones of the piece, constructing such a graph reduces\ntoselecting notes forthesoprano and bass parts.\nIn most circumstances, the primary melody (soprano)\ntone is the highest one heard at any point in time, and\nthe primary bass tone is the lowest. Therefore, F ORE-\nGRAPHis based on the idea of selecting the highest pitch\nforthe soprano line and the lowest forthe bass line. How-\never, complications arise in situations where the primary\nbassorsopranotonespersistintimeeventhoughtheymay\nhavestoppedsounding. ConsideranAlbertibassline,such\nas in Figure 4. Because this ﬁgure is outlining a chord,\nonly the lowest note of the chord belongs to the primary\nbass line (the other notes belong to inner voices). The\nlow Cs, though they are only represented on the page as\neighthnotes,persistinthemusicalmindthroughtheentire\nmeasure as if they were sounding constantly; the true bass\nline does not skip between the notes of the chord. This is\nthe reason why we require a voice leading algorithm that\ncan detect cases of implicit polyphony, not just in cases\nof arpeggiation, but in any case where the bass or soprano\npart may move between voices.\n/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2/noteheads.s2/noteheads.s2 /noteheads.s2/timesig.C44/clefs.F\nFigure 4. An Albertibass line.\nStill, there are cases where voices start and stop mid-\ncomposition,andanalgorithmthatblindlyfollowstheini-\ntial bass and soprano lines stepwise from the start of the\npiecetotheendwouldnotsufﬁceincases,forexample,of\nregister transfer. Therefore, F OREGRAPHchooses appro-\npriate bass and soprano tones for each harmonic segment\ndeﬁned by the harmonic analysis algorithm, and then fol-\nlows the tones via voice-leading connections to ﬁll out the\nsegment; pseudocode is given in Figure 5. For each har-\nmonic segment in a composition, F OREGRAPHﬁnds the\nlowest and highest pitched notes that belong to the current\nharmony; these notes are added to the primary bass and\nsopranoparts. The F ILLRANGEprocedurethenaddsaddi-\ntional notes by following voice-leading connections from\ntheinitialnotesaddedinthesegment; connections arefol-\nlowed both backwards and forwards in time, and notes are\nonly added if they do not overlap in time with any other\nnotes already added tothesegment.\nThe EXTENDVOICEprocedure then allows the musical\nline in a harmonic segment to be extended into following\nsegments, stopping only upon reaching a note that is con-\nsonantintheprevailingharmonyforthesegment. Because\nthe primary notes are determined independently for each\nharmonic segment, it is possible that the soprano or bass\nlines ﬂeshed out by F ILLRANGEwill not connect musi-\ncally over a segment break. E XTENDVOICEpermits each\n425Poster Session 3\nline to be followed to a logical conclusion without adding\ntoo many notes of what may develop into an irrelevant in-\nner voice. Because E XTENDVOICEhalts upon adding a\nconsonant note in the prevailing harmony, leaps are possi-\nble inthecomputed musical lines oversegment breaks.\nAfterchoosingthenotesforthesopranoandbasslines,\nthey are displayed as noteheads on staves as a prelimi-\nnary foreground graph. F OREGRAPHproduced the output\nshowninFigure6fortheSchubertImpromptuinFigure1.\n/noteheads.s2\nIV65/noteheads.s2/noteheads.s2\nIII6/noteheads.s2/noteheads.s2\nI6/noteheads.s2/noteheads.s2 /noteheads.s2\n/brace292\nV42/noteheads.s2/noteheads.s2/noteheads.s2\n/accidentals.flat/accidentals.flat/accidentals.flat/accidentals.flat/clefs.F/accidentals.flat/accidentals.flat/accidentals.flat/accidentals.flat/clefs.G\n/noteheads.s2/noteheads.s2\n/noteheads.s2/noteheads.s2\nI/noteheads.s2/noteheads.s2\nV43/noteheads.s2/noteheads.s2/noteheads.s2/noteheads.s2\nA/accidentals.flat:V/noteheads.s2/noteheads.s2/noteheads.s2\nI6/noteheads.s2/noteheads.s2 /noteheads.s2\nV65/noteheads.s2/noteheads.s2 /noteheads.s2\nI/noteheads.s2/noteheads.s2/noteheads.s2\nFigure 6. A preliminary foreground graph produced by\nFOREGRAPH.\nIfonecomparesthehand-constructedgraphinFigure2\nto the one produced by F OREGRAPHin Figure 6, only a\nfew differences are apparent. One is that the computer-\nconstructed graph contains instances of adjacent notes of\nidentical pitch. F OREGRAPHdoes not reduce these cases\nto single notes because although this occurs frequently in\nforeground graphs, itisnot always done consistently.\nThe only other differences in the computer-generated\nanalysis are the omitted “V” chord near the middle of the\nanalysis, and the added “III6” chord. Both of these differ-\nences derive from the harmonic analysis component used\nas a preliminary step to F OREGRAPH. The V chord in the\nhand-constructedgraphwasnotgeneratedinthecomputer\nanalysisasitwasabsorbedintotheIchordsoneitherside.\nSimilarly, the ﬁrst-inversion III chord arises from a misin -\nterpretation of chord tones and non-chord tones.\n4. EVALUATION ANDANALYSIS\nIn order to evaluate the correctness of F OREGRAPH, we\nrequire a set of input music scores and correct foreground\ngraphsforthem. WeturnedtoastandardSchenkeriananal-\nysis textbook [14], and encoded the ﬁrst twelve musical\nexamples that had correct analyses provided, and whose\nanalyses contained soprano and bass parts (two of the ex-\namples were monophonic, and so were omitted). The ex-\namplesareallmulti-measureexcerptsfromcommonprac-\nticeperiod works.\nOurmethod ofevaluation isbasedonthestandardmet-\nrics of precision and recall. If one views each note in a\ncomposition as an individual document, then constructing\na preliminary foreground graph is equivalent to executing\ntwo queries: one query to retrieve all notes belonging to\nthe soprano part, and a second query to retrieve all notes\nbelonging to the bass part. We also need to deﬁne what it\nmeans for a note to be “relevant” and “retrieved” to com-\npute precision and recall. We consider a note “retrieved”\nforaqueryifitappearsinthecorrespondingpart(soprano\nor bass) for the computer-constructed foreground graph.\nDeﬁning“relevant”iscomplicatedbecausetheforegroundgraphsastheyappearinthetextbook(1)oftencontainper-\ntinent pitches of inner voices along with the primary so-\nprano and bass parts, and (2) already have had some re-\nductionsappliedinmostcases,whichremovessomenotes\nfrom the ground-truth that would appear in the computer-\ngenerated graphs.\nTherefore, we have two notions of “relevant” and com-\npute statistics based on each deﬁnition. In our ﬁrst set of\ncalculations, we consider a note to be relevant for the so-\nprano(bass)queryifitispresentontheupper(lower)staff\nof the Schenker graph in the textbook analysis. This deﬁ-\nnition, however, considers many notes as relevant that will\nnot be present in the computer-generated analyses as they\nbelong to inner voices. To remedy this, our second deﬁni-\ntion considers a note to be relevant for the soprano (bass)\nqueryifitispresentontheupper(lower)staffintheSchen-\nkergraphinthetextbookanalysis,andhasastempointing\nup(down). Ifitisclearthatstemdirectioninagraphis not\nbeing used to indicate to which voice a note belongs (and\nthe direction is only determined by aesthetics), the restri c-\ntion on stem direction is ignored, and only the presence of\nthe stem is considered. Stems in graphs are indications of\nstructuralimportance,andthereforethesearenotesthatw e\nareparticularlyinterestedinhavingF OREGRAPHidentify-\ningcorrectly.\nWe ran the F OREGRAPHalgorithm on each example\nandcomparedtheresultinggraphstothetextbook’sgraphs.\nFor each example, and for each part (soprano and bass),\nwecomputedprecision(thefractionofretrievednotesthat\nwerealsorelevant)andrecall(thefractionofrelevantnot es\nthatwerealsoretrieved). Toprovideabaselineforcompar-\nisonwith F OREGRAPH,weevaluatedasecondforeground\ngraph creation algorithm, R ANDOM, that selects notes for\nthesopranoandbasspartsfromtheinputmusicrandomly.\nRANDOMalwayschoosesthesamenumberofnotesforthe\nsoprano and bass parts for each example as were selected\nby FOREGRAPHfor the same example. We calculated av-\nerageprecisionandrecallforR ANDOMover500runs. All\nof the precision and recall statistics for F OREGRAPHand\nRANDOMare displayed in Table 1. To show more clearly\ntheimprovementofF OREGRAPHoverRANDOM,Figure7\ncomparestheF1measure(harmonicmeanofprecisionand\nrecall) foreach musical example forthetwo algorithms.\nOne of the excerpts deserves special mention. The ex-\ncerptfromSchubert’s SymphonyinBminor confusedF ORE-\nGRAPHas the accompaniment part is pitched higher than\nthe primary melody. The analysis constructed by F ORE-\nGRAPHcontained a harmony line in the soprano part, and\nthe true melody was not present at all. Because this single\nexample distorted the statistics for the soprano part, Ta-\nble1containsentriesfortheaggregateprecisionandrecal l\nwithand without the Symphony included.\nOverall, we are encouraged by the results of the evalu-\nation. We are especially pleased with the recall values for\nthestemmednotesdeﬁnitionofrelevance;disregardingthe\nSchubert Symphony , FOREGRAPHretrieved almost 90%\nof the relevant bass notes, and almost 80% of the relevant\nsopranonotes. Figure7clearlyindicatesthatF OREGRAPH\n42610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nprocedure FOREGRAPH\nLetV(x,y)be trueifthere isavoice leading connection between notes xandy.\nLetSbe aset of notes fortheprimarysoprano part.\nLetBbe asetof notes forthe primarybass part.\nforeachharmonic segment Hinthecomposition do\nLetnbethe lowest pitched notein Hthat isa memberof H’sharmony.\nAddntoB\nFILLRANGE(n,B,H )\nEXTENDVOICE(B,H)\nLetnbethe highest pitched note in Hthat isamemberof H’sharmony.\nAddntoS\nFILLRANGE(n,S,H )\nEXTENDVOICE(S,H)\nprocedure FILLRANGE(noten, partP,harmonic segment H)\nInitializequeue Qtocontain just n\nwhile Qisnot empty do\nRemove the topnote from thequeue, call it m\nLetNbe theset of allnotes such that if x∈N, then either V(m,x)orV(x,m), andxisinH.\nSortNby increasing length of timebetween mand each note in N\nifNisempty, then return\nfor eachnotex∈Ndo\nifxdoes not conﬂict withany notes in PthenaddxtoPand add xtoQ\nprocedure EXTENDVOICE(partP,harmonic segment H)\nLetcurrbe thelast note in Hthat isalsoin P\nwhilecurrisnot consonant in H’sharmony do\nLetNbe theset of allnotes such that if x∈N, thenV(m,x)\nifNisempty, then return\nLetnbethe note in Nwiththe minimum length oftimeto curr\nAddntoP\ncurr←n\nFigure 5. The FOREGRAPHalgorithm.\nisa largeimprovement overchoosing notes randomly.\nThetwoissuesmentionedearlierthatcomplicatedchoos-\ning an appropriate deﬁnition of relevance cause the preci-\nsion and recall values to not represent the true quality of\nthe graphs produced by F OREGRAPH. The ﬁrst issue is\nthatmanyoftheground-truthanalysescontainnotesofin-\nner voices on the upper and lower staves, as well as notes\nfrom the primary soprano and bass parts. The bass part of\nthe Chopin Nocturne, for example, contains arpeggiated\nchords. F OREGRAPHonly included the lowest note of\neach chordintheprimarybasspart,whilethetextbook in-\ncludedallofthenotesofeachchord,withallbutthelowest\ngiven as inner voices. This lowered the recall value for all\nbass notes inthisexample to23.5%.\nThe second issue is that many of the textbook’s graphs\nhave already had simple reductions applied to the musi-\ncal surface; repeated notes in the textbook’s graphs have\nalso been removed in many cases. Because F OREGRAPH\nonlyselectsnotes for the foreground graphs and does not\nperform any reductions, many of the precision values are\nlower than they would be if those reductions had not been\ndone in the textbook’s graphs. For example, in the French\nSuite; FOREGRAPHplacedmanynotesinthesopranopart\nthat were not present in the textbook’s graph because re-\nductions had already been applied tothem.We are conﬁdent that F OREGRAPHis ready to be used\nas a precursor to an actual Schenkerian reduction algo-\nrithm. Because we are only selecting notes to be placed\nin the soprano and bass parts, the output of F OREGRAPH\nis ready for processing to search for reductions, and any\nlow precision statisticsshould not be alarming.\n5. REFERENCES\n[1] Heinrich Schenker. Der Freie Satz . Universal\nEdition, Vienna, 1935. Published in English as\nFree Composition , translated and edited by E. Oster,\nLongman, 1979.\n[2] Phillip B. Kirlin and Paul E. Utgoff. A framework\nfor automated Schenkerian analysis. In Proceed-\nings of the Ninth International Conference on Music\nInformation Retrieval , pages 363–368, Philadelphia,\nSeptember2008.\n[3] Alan Marsden. Automatic derivation of musical\nstructure: A tool for research on Schenkerian anal-\nysis. InProceedingsoftheEighthInternationalCon-\nference on Music Information Retrieval , pages 55–\n58, 2007. Extended Version.\n[4] Alan Marsden. Generative structural representation\n427Poster Session 3\nAllnotes Stemmed notes\nPrecision Recall Precision Recall\nExcerpt Sop.BassSop.BassSop.BassSop.Bass\nJ.S. Bach, Ariavariata (BWV 989) 0.3200.4380.8890.7000.2000.2501.0000.667\nBeethoven, NinthSymphony , III 0.8180.8000.8181.0000.4550.3000.7141.000\nHaydn,Symphony inD major, No. 104 0.6670.5380.7060.8750.3890.4620.8750.857\nChopin,Prelude inAmajor , Op. 28/7 0.6360.3330.6361.0000.0910.3330.5001.000\nHaydn,Divertimento inB-ﬂat , II 0.6430.6151.0001.0000.2860.3081.0001.000\nSchubert, Standchen fromSchwanengesang 0.7780.5710.2801.0000.5560.4290.5561.000\nJ.S. Bach, Chorale No. 149 0.8000.8571.0000.8570.6000.4291.0000.750\nJ.S. Bach, French Suite inCminor , Sarabande 0.5500.7500.7860.9000.2500.3330.7140.800\nSchubert, Symphony inBminor, No. 8 , I 0.0000.6670.0001.0000.0000.6670.0001.000\nMozart,Symphony inCmajor , K. 425, IV 0.6360.1740.6090.5000.2730.1301.0001.000\nChopin,Nocturne inD-ﬂat major , Op. 27/2 0.8890.4000.5330.2350.3330.4000.5001.000\nBrahms,Rhapsody inE-ﬂat major , Op. 119/4 1.0001.0000.9171.0000.3640.6361.0001.000\nFOREGRAPH,all excerpts 0.5680.5570.5680.7720.2790.3640.7290.911\nFOREGRAPH,allexcept forSchubert’s Symphony 0.6500.5470.6750.7530.3190.3360.7970.896\nRANDOM,all excerpts 0.2550.1540.2550.2130.1040.0830.2730.208\nImprovement of F OREGRAPHover RANDOM 0.3130.4030.3130.5590.1750.2810.4560.703\nTable 1. Precision and recall forevaluation.\nFigure 7. A graph comparing theF1 measures for R ANDOM(dark bars)and for F OREGRAPH(light bars).\nof tonal music. Journal of New Music Research ,\n34(4):409–428, December2005.\n[5] Alan Marsden. Extending a network-of-elaborations\nrepresentation to polyphonic music: Schenker and\nspecies counterpoint. In Proceedings of the First\nSound and Music Computing Conference , pages 57–\n63, 2004.\n[6] Alan Marsden and Geraint A. Wiggins. Schenkerian\nreduction as search. In Proceedings of the Fourth\nConferenceonInterdisciplinaryMusicology ,Thessa-\nloniki, Greece, July2008.\n[7] FredLerdahlandRayJackendoff. AGenerativeThe-\nory of Tonal Music . MIT Press, Cambridge, Mas-\nsachusetts, 1983.\n[8] MasatoshiHamanaka,KeijiHirata,andSatoshiTojo.\nATTA: Implementing GTTM on a computer. In Pro-\nceedings of the Eighth International Conference on\nMusicInformation Retrieval , pages 285–286, 2007.[9] Michael Kassler. APL applied in music theory. APL\nQuote Quad , 18(2):209–214, 1987.\n[10] Stephen W. Smoliar. A computer aid for Schenke-\nrian analysis. Computer Music Journal , 2(4):41–59,\n1980.\n[11] Bryan Pardo and William P. Birmingham. Algo-\nrithms for chordal analysis. Computer Music Jour-\nnal, 26(2):27–49, 2002.\n[12] Edward Aldwell and Carl Schachter. Harmony and\nVoice Leading . Harcourt Brace & Company, Fort\nWorth, Texas, second edition, 1989.\n[13] David Temperley. The Cognition of Basic Musical\nStructures . MIT Press, Cambridge, Massachusetts,\n2001.\n[14] Allen Forte and Steven E. Gilbert. Introduction to\nSchenkerian Analysis . W. W. Norton and Company,\nNew York, 1982.\n428"
    },
    {
        "title": "A Method for Visualizing the Pitch Content of Polyphonic Music Signals.",
        "author": [
            "Anssi Klapuri"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415036",
        "url": "https://doi.org/10.5281/zenodo.1415036",
        "ee": "https://zenodo.org/records/1415036/files/Klapuri09.pdf",
        "abstract": "This paper proposes a method for visualizing the pitch content of polyphonic music signals. More specifically, a model is proposed for calculating the salience of pitch candidates within a given pitch range, and an optimization technique is proposed to find the parameters of the model. The aim is to produce a continuous function which shows peaks at the positions of true pitches and where spurious peaks at multiples and submultiples of the true pitches are suppressed. The proposed method was evaluated using synthesized MIDI signals, for which it outperformed a baseline method in terms of precision and recall. A straightforward visualization technique is proposed to render the pitch salience function on the traditional staves when the musical key and barline information is available.",
        "zenodo_id": 1415036,
        "dblp_key": "conf/ismir/Klapuri09",
        "keywords": [
            "pitch content",
            "polyphonic music signals",
            "pitch salience",
            "salience calculation",
            "optimization technique",
            "true pitches",
            "spurious peaks",
            "staves",
            "visualizing",
            "evaluation"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAMETHODFORVISUALIZINGTHEPITCHCONTENT\nOFPOLYPHONICMUSICSIGNALS\nAnssiKlapuri\nDepartmentofSignalProcessing,TampereUniversityofTec hnology\nanssi.klapuri@tut.fi\nABSTRACT\nThispaperproposesamethodforvisualizingthepitchcon-\ntentofpolyphonicmusicsignals.Morespeciﬁcally,amodel\nisproposedforcalculatingthesalienceofpitchcandidate s\nwithinagivenpitchrange,andanoptimizationtechnique\nisproposedtoﬁndtheparametersofthemodel.Theaim\nistoproduceacontinuousfunctionwhichshowspeaks\natthepositionsoftruepitchesandwherespuriouspeaks\natmultiplesandsubmultiplesofthetruepitchesaresup-\npressed. Theproposedmethodwasevaluatedusingsyn-\nthesizedMIDIsignals,forwhichitoutperformedabase-\nlinemethodintermsofprecisionandrecall. Astraight-\nforwardvisualizationtechniqueisproposedtorenderthe\npitchsaliencefunctiononthetraditionalstaveswhenthe\nmusicalkeyandbarlineinformationisavailable.\n1. INTRODUCTION\nPitchanalysisofpolyphonicmusicisachallengingtask\nwherecomputationalmethodshavenotyetachievedthe\naccuracyandﬂexibilityoftrainedmusicians.Severaldif-\nferentapproacheshavebeenproposedtowardssolvingthe\nproblem.Somemethodsarebasedonastatisticalmodelof\ntheinputsignal[1],whereassomeothersmodelthehuman\nauditorysystem[2].Jointdetectionofmultiplepitchesha s\nbeenproposed[3],contrastedbytechniqueswhichcarry\noutiterativepitchdetectionandcancellation[4]. Some\nmethodsarebasedonunsupervisedlearning[5]andsome\nothersonsupervisedclassiﬁcation[6].Theseexamplesil-\nlustratetheremarkablevarietyofmethodsthathavearisen\ninanattempttomimicthehumanabilitytomakesense\nofcomplexsoundmixtures. Anicereviewofmultipitch\ndetectionalgorithmscanbefoundin[7].\nAdrawbackofmanyoftheexistingmultipitchanaly-\nsismethodsisthattheyproduceadiscretesetofdetected\npitchvalues(or,fundamentalfrequencies,F0s1)insteadof\nacontinuousdetectionfunctionthatwouldshowthelikeli-\nhoodsofallpossiblepitchvalueswithinthepitchrangeof\n1ThetermspitchandF0areusedhereinterchangeably.\nPermissiontomakedigitalorhardcopiesofallorpartofthisw orkfor\npersonalorclassroomuseisgrantedwithoutfeeprovidedth atcopiesare\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbearthisnoticeandthefullcitationontheﬁrstpage.\nc/circlecopyrt2009InternationalSocietyforMusicInformationRetrieval .interest. Forpitchcontentvisualizationandacousticfea -\ntureextractionpurposes,acontinuousdetectionfunction\nisoftenmoredesirablesinceitallowsthehumaneyeor\nasubsequentpost-processingalgorithmtopicktheinter-\nestingfeaturesfromthedetectionfunctionandtodecide\nwhichpeakscorrespondtotruepitches.\nAfundamentaldifﬁcultyincomputingsuchdetection\nfunctions(thinkoftheautocorrelationfunctionforexam-\nple)isthattheydonotshowapeakonlyattheposition\nofthetruepitch,butalsoattwiceandhalfthecorrect\npitch,andoftenatallmultiplesandsubmultiplesofit.\nThisambiguityisparticularlychallenginginmultipitchd e-\ntectionwherethedetectionfunctioneasilybecomescon-\ngestedwithspuriouspeaksduetotheambiguityassociated\nwitheachcomponentsound.\nVarioustechniqueshavebeenproposedtosuppressthe\nextraneouspeaksinadetectionfunction. Forexample,it\nhasbeenproposedtodetectF0seitheriterativelyorjointl y\nandtocancelallthespuriouspeaksthatarealreadyex-\nplainedbythedetectedF0s[3,4].However,thesemethods\nproduceonlyadiscretesetofF0s.KarjalainenandTolo-\nnenproposedamethodwhichproducesanentiredetection\nfunction,wherethespuriouspeaksweresuppressedusing\nan“enhancing”procedure[2].\nInthispaper,weproposeamodelforcalculatingthe\nsalience(or,strength)ofallpitchvalueswithinagiven\nrangeofinterest,andinvestigateanumericaloptimizatio n\ntechniquetoﬁndthemodelparameterssothatthetrulyex-\nistingpitchfrequenciesareindicatedwithpeaksthattend\ntowardsunityvalueandspuriouspeaksareforcedtowards\nzero.Wealsoproposeavisualizationtechnique,wherethe\ncomputedpitchsalienceisrenderedonthestavesofcom-\nmonmusicalnotation.Thisallowspeoplewhoareableto\nreadthemusicnotationtoplaydirectlyfromthevisual-\nization,ortouseittostudyperformancenuances,suchas\npitchglides,vibrato,andexpressivetiming.\n2. METHOD\nIntheproposedmethod,anaudiosignalisﬁrstblocked\nintoframeswhichareshort-timeFouriertransformed.The\nspectraarewhitened(seeSec.2.1)andthenoiseﬂoorin\nthespectrumduetodrumsandothernon-pitchedsoundsis\nestimated(Sec.2.2).Thewhitenedspectrumandthenoise\nspectrumareusedinthepitchsaliencemodel(Secs. 2.3\nand2.4).Thesestepsarenowexplainedinmoredetail.\n615Poster Session 4\n2.1 Levelnormalizationandspectralwhitening\nThetime-domainaudiosignal x(n)isblockedintopartly-\noverlappinganalysisframesthatarewindowedusingthe\nHammingwindow.Thesignalwithineachframeislevel-\nnormalizedtounityvariance,zero-paddedtotwiceitsleng th,\nandthendiscreteFouriertransformedtoobtainthemagni-\ntudespectrum Xt(k)inframe t.Eachframeisprocessed\nindependently,thereforewedroptheframeindex tinthe\nfollowingforconvenience.\nSpectralwhitening,orﬂattening,isappliedon X(k)in\nordertosuppresstimbralinformationandtherebymake\nthesubsequentpitchanalysismorerobusttovarioussound\nsources. Thisisachievedbycalculatingpower σ2\ncofthe\nsignalwithinnarrowfrequencybands candbyscalingthe\nsignalwithineachbandby gc=σν−1\nc,where ν= 0.16is\naparameterdeterminingtheamountofwhitening.Center\nfrequencies fcofthesubbandsaredistributeduniformly\nonthecriticalbandscale, fc= 229(10(0.33c+1)/21.4−1),\nandeachsubband c= 1,... ,96hasatriangularpowerre-\nsponseextendingfrom fc−3tofc+3.Theresultingwhitened\nmagnitudespectrumisdenotedby Y(k).\n2.2 Noiseestimation\nFromtheviewpointofpitchanalysis,thesoundsofdrums\nandthebeginningtransientsofmanypitchedinstruments\nareconsideredas“noise”.Severalmethodshavebeenpro-\nposedintheliteratureforestimatingthe“noise”(stochas tic\nspectralcomponent)inmusic(see[3]forreview).Perhaps\nthemostwidelyusedisthesinusoidsplusnoisemodel,\nwheresinusoidalcomponentsaredetectedandsubtracted\ninthefrequencydomain,andtheresidualisconsidered\nascoloured(ﬁltered)whitenoise. Another,quiterobust\nmethodistocalculateamovingmedianatlocalregionsof\nthemagnitudespectrum.\nHeretheemphasisislaidonthecomputationalefﬁ-\nciencyandontherobustnessofthemethodagainstspectral\npeakswhichareassumedtocorrespondtopitchedsounds\nandshouldnotaffecttheestimate. Theproposednoise\nspectrumestimationmethodconsistsofthefollowingsteps .\nFirst,amovingaverage N′(k)overthewhitenedspectrum\nY(k)iscalculatedas\nN′(k) =1\nuk−lkuk/summationdisplay\nk′=lkY(k′)(1)\nwhere lkandukdeﬁnethelowerandupperboundariesof\nthecritical-bandsubbandswithinwhich N′(k)iscalcu-\nlated. Notethat(1)canbecomputedveryefﬁcientlyby\nﬁrstcalculatingcumulativesum ¯Y(k)overY(k)andthen\nN′(k) =¯Y(uk)−¯Y(lk−1).Thebandboundaries lkand\nukcanbepre-stored.\nTomakethenoiseestimateimmunetospectralpeaks,\nanothermovingaverageiscalculated,butincludinginthe\naveragingonlyfrequencybinsforwhich Y(k)< N′(k).\nTheresultingnoisespectrumestimateisdenotedby N′′(k).\nPerformingonemorelocalaveragingof N′′(k)(bysubsti-\ntuting N′′(k)inplaceof Y(k)in(1))producestheﬁnal\nnoisespectrumestimate N(k).Thepresentednoiseestimationprocedureisbesidessim-\nple,alsocomputationallyefﬁcient.Iftheinputsignalcon -\nsistsofcolouredwhitenoise(withoutpitchedsounds),it\ncanbeshownthat E[N(k)] = 0 .61E[N0(k)],where E[·]\ndenotesexpectationand N0(k)isthetruenoisespectrum\nbeingestimated. Inotherwords,theestimatedspectrum\ndependslinearlyonthetruespectrum. Inpractice,how-\never,theinputsignalmaycontainpitchedsoundswhich\naffecttheestimateandthescalingfactorcanbeanything\nbetween 0.61andabout 1.0. Inourcase,thesubsequent\noptimizationprocessexplainedbelowtakescareofﬁnding\nthelinearscalingfactorfor N(k),thereforetheproposed\nnoiseestimationmethodiswellsuitedhere.\n2.3 Pitchsaliencemodel\nForconvenience,thewhitenedspectrum Y(k)andthees-\ntimatednoisespectrum N(k)arestoredascolumnsinma-\ntrixY,togetherwithanall-one“spectrum”:\nY=\nY(0) N(0) 1\nY(1) N(1) 1\n.........\nY(K−1)N(K−1) 1\n(2)\nLetusalsodeﬁnebasisfunctions\nam(k) = [log( k+ 1)]m−1(3)\nwhere kdenotesthefrequencyindexand m= 1,2,... ,M\nindexesthebasisfunctions.Thisisapolynomialbasison\nthelog-frequencyscale. Forconvenience,thebasesare\ncollectedascolumnsinmatrix A= [a1,a2,... ,aM].\nThecolumnsof Yarelinearlycombinedintoasingle\nspectrum Z(k)accordingtothefollowinglinearmodel:\nZ(k) = (AW.×Y)1(4)\nwhere .×denoteselement-wisemultiplication, Wisama-\ntrixofsize (M×3)thatcontainsthemodelparameters,and\n1isanall-onevectoroflength 3(thenumberofcolumns\ninY).Notethattheproduct AWisamatrixofsize (K×\n3),thesamesizeas Y. Thethreecolumnsdeﬁnefre-\nquencyresponsesforthethreecolumnsof Y,beforethey\naresummed(bymultiplyingwith 1)toobtaintheﬁnal\nspectrum Z(k).Theﬁrstcolumnof AWdeﬁnesthefre-\nquencyresponseofthewhitenedspectrum Y(k).Thesec-\nondcolumndeﬁnesthefrequencyresponseofthenoise\nspectrum N(k)andallowingittobenegativeleadstonoise\nsubtractionfromtheﬁnalspectrum Z(k). Thethirdcol-\numnof AWismultipliedbytheall-onespectrumin Yand\nallowsanadditivefrequency-dependentcurvetobeadded\ntotheﬁnalspectrum Z(k).\nCrucialforthemodelaretheparametersinmatrix W.\nTheMparametersincolumn iofW(togetherwiththe\nﬁxedbasisfunctions A)determinethefrequencyresponse\nofcolumn iinY. Thebasisfunctionsarenecessaryin\nordertobeabletorepresentthefrequencyresponseswith\nonlyafewparametervalues.Analgorithmforlearningthe\nparameterswillbedescribedinSec.2.4.\n61610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAharmonictransformisappliedonthespectrum Z(k)\ntoobtaina“raw”saliencefunction r(τ)whichindicates\nthestrengthofpitchperiodcandidates τ:\nr(τ) =H/summationdisplay\nh=1Z(kτ,h).(5)\nTheperiod τcorrespondstotheF0value fs/τ,where fs\ndenotesthesamplingrate. Thefrequencybin kτ,hcorre-\nspondstothe h:thharmonic(integermultiple)oftheF0\nandisdeterminedbythelargestvalueof Z(k)inthevicin-\nityofthefrequency hK/τ.Moreexactly,themaximumis\nfoundinrange ⌊hK/(τ+ ∆τ/2)⌉,... ,⌊hK/(τ−∆τ/2)⌉,\nwhere ⌊·⌉denotesroundingtothenearestinteger, Kisthe\nlengthoftheFouriertransform,and ∆τ= 0.5denotes\nthespacingbetweensuccessiveperiodcandidates τ. The\nnumberofharmonicpartials H= 20.\nTheharmonictransform(5)ismotivatedbytheFourier\ntheoremwhichstatesthataperiodicsignalcanberepre-\nsentedwithspectralcomponentsatintegermultiplesofthe\ninverseoftheperiod.Pitchperception,inturn,isclosely\nlinkedtothetime-domainperiodicityofsounds.\nThefunction r(τ)containspeaksatthepositionsoftrue\npitchperiods,butitrequiresfurtherprocessingtosuppre ss\npeaksthatoftenoccuratinteger(sub)multiplesofthetrue\nperiod(s).Themethodproposedinthefollowingbearsre-\nsemblancetothe“enhancing”techniqueofKarjalainenet\nal.[2]whichsuppressesthepeaksatintegermultiplesof\nthetrueperiod(s)intheautocorrelationfunction(ACF).\nTheyclippedtheACFtopositivevalues,scaledittotwice\nitslength,andsubtractedtheresultfromtheoriginalclip ped\nACF.Thiswasrepreatedfortime-scalingfactorsuptoabout\nﬁvetosuppressthepeaksoccurringatintegermultiplesof\nthetrueperiod(s).\nThemethodproposedinthefollowingisageneraliza-\ntionoftheaboveidea.First,letuscreatescaledversionso f\nr(τ).Theoriginalfunction r(τ)isscaledbyafactor jby\ninsertingzerosbetweentheoriginalsamples,lowpassﬁl-\nteringtheresultwithcutofffrequency1\n2fs/jandmultiply-\ningtheﬁlteredsignalby j. Theresultingsignal,denoted\nbyrj(τ),isﬁnallytruncatedtothesamelengthas r(τ).\nStretchedversionswithscalingfactors j= 2,3,... ,Jare\ncalculated(here J= 5).\nSecondly,wecalculateshrunkversionsof r(τ)byscal-\ningwithfactor 1/j.Thisisdonebylowpassﬁltering r(τ)\nwithcutofffrequency1\n2fs/jandthencopyingevery j:th\nsampleof r(τ)toasignaldenotedby r1/j(τ). Sincethe\nlengthof r1/j(τ)isonly r:thfractionof r(τ),newvalues\nhavetobecalculatedforlongperiods τusing(5)inorder\nthattheshrunkfunctionwouldbeofthesamelengthasthe\noriginal r(τ).\nForconvenience,thestrecthedandshrunkversionsof\nr(τ)arestoredascolumnsinmatrix R,\nR=/bracketleftbig\nr,1,r2,r3,... ,rJ,r1/2,r1/3,... ,r1/J/bracketrightbig\n(6)\nwherewehavedenoted r≡r(τ),r2≡r2(τ),andsoon\nforconvenience,and 1denotesanall-onevector.\nLetusdeﬁnebasisfunctions\nbn(τ) = [log( τ+ 1)]n−1(7)where τistheperiodand n= 1,2,... ,Nindexestheba-\nsisfunctions.Thisisapolynomialbasisonthelog-period\nscale.Forconvenience,thebasesarecollectedascolumns\ninamatrix B= [b1,b2,... ,bN].\nTheﬁnalpitchsaliencefunction s(τ)iscalculatedasa\nlinearfunctionofthecolumnsof R:\ns(τ) = (BV.×R)1(8)\nwhereVisamatrixofsize (N×2J)thatcontainsthe\nmodelparameters,and 1isanall-onevectoroflength 2J\n(thenumberofcolumnsin R). Theproduct BVgivesa\nmatrixofthesamesizeas R. Itscolumnsdeﬁneperiod-\ndependentweightsforthecolumnsof R,thatis,forthe\noriginalrawsalience randitsstretchedandshrunkver-\nsions,rjandr1/j. Forexample,settingsmallnegative\nweightsforthecolumnsthatcorrespondto r2andr1/2\nimplementssuppressionofthepeaksthatoccuranoctave\naboveandbeloweachtruepitchperiod.The Nparameters\nincolumn jofV(togetherwiththeﬁxedbasisfunctions\nB)determinetheperiod-dependentweightsforcolumn j\ninR.Finally,multiplicationwith 1isequivalenttosum-\nmingoverthecolumnsandyields s(τ).\nTheproposedpitchsaliencemodelisnowfullydeﬁned,\nexceptforthetwoparametermatrices WandVin(4)and\n(8),respectively.\n2.4 Algorithmforlearningtheparameters WandV\nThedescribedpitchsaliencemodelmaylookquitecom-\nplicatedataﬁrstsight,thereforewestartfromasimpliﬁed\ncasetodevelopanintuitionhowthemodelworks.Letus\nsettheparameters Win(4)tozeroforallexcepttheﬁrst\ncolumnwhichcorrespondstotheﬁrstcolumnof Y,andlet\nussetthevaluesintheﬁrstcolumnsothat Z(k)≈K\nkY(k)\nwhere KistheFouriertransformlength.Furthermore,let\nussettheparameters Vin(8)tozeroforallexceptthe\nﬁrstcolumn(whichcorrespondtotheﬁrstcolumnof R),\nandsetthevaluessothat s(τ)≈1\nτr(τ).Substituting r(τ)\nfrom(5),theoverallmodelbecomes\ns(τ)≈1\nτH/summationdisplay\nh=1K\nkτ,hY(kτ,h)≈H/summationdisplay\nh=11\nhY(kτ,h).(9)\nwherethelatterequivalenceisbecause kτ,h≈hK/τ.\nInotherwords,salienceiscomputedas1\nh-weightedsum\nofthepartialamplitudesinthewhitenedspectrum Y(k),\nwhichisareasonable(althoughsimplistic)wayofcom-\nputingpitchsalience.\nTheabovesimpliﬁedmodelisactuallyexactlyhowthe\nparameters WandVareinitializedinthelearningalgo-\nrithmtobedescribedhere.Thesimplemodel(9)isagood\nstartingpoint,fromwhereweiterativelyreﬁnethevalues.\nAnoverviewofthelearningalgorithmisasfollows:\n0)Matrices WandVareinitializedtovaluesthatcor-\nrespondtothesimplemodel(9).\n1)Matrix Wisupdated,keeping Vﬁxed.\n2)Matrix Visupdated,keeping Wﬁxed.\n617Poster Session 4\n3) Steps1and2arerepeateduntil WandVconverge.\nInpractice,itwasfoundtobesufﬁcienttorepeatthesteps\n1and2justacoupleoftimes.\nTheexactgoaloftheoptimizationistoﬁndsuchpa-\nrameters WandVthatthesaliencefunction s(τ)isas\ncloseaspossibletounityvalueatpoints τthatcorrespond\ntotruepitchperiods,andascloseaspossibletozeroat\nnext-largestpeaksthatcorrespondto“false”pitchperiod s.\nThestepsarenowdescribedinmoredetail.\nInitialization.Asalreadymentioned, WandVare\ninitializedtovaluesthatcorrespondtothesimplemodel\nin(9). Matrix Wisinitializedsothat Z(k)≈K\nkY(k)\nandmatrix Vsothat s(τ)≈1\nτr(τ). Theinitialvalues\nintheﬁrstcolumnof Warecalculatedbyleast-squaresﬁt\nw1= (ATA)−1ATα,wherevector α(k) =K/(k+ǫ)de-\nnotesthetargetfunctionandregularizationusing ǫ≈50is\nneededtoavoidﬁttingonlythelargestvaluesnearthezero\nfrequency.Similarly,theinitialvaluesintheﬁrstcolumn\nofVarecalculatedby v1= (BTB)−1BTβ,wherevector\nβ(k) = 1/(τ+ǫ)denotesthetargetfunctionand ǫ≈50\nisagainneededtoavoidﬁttingonlythelargestvaluesnear\nthezeroperiod.\nUpdating W. Inordertolearnbettervaluesfor W,\nsometrainingmaterialisneeeded. Forthispurpose,we\nmixedsamplesfrom32musicalinstrumentswithequal\nmean-squarelevels. Randommixturesuptosixsimulta-\nneoussoundsweregeneratedusingtheMcGillUniversity\nMasterSamples(MUMS)database.\nForeachtraininginstance g,g= 1,2,... ,G,thefol-\nlowingoperationsareperformed:\n1.a) Thesaliencefunction s(τ)iscalculatedusing(8)\nandthecurrentparameters WandV.\n1.b) From s(τ),werecordtheexactperiodvaluesofthe\nPannotatedtruepitchesintraininginstance g. In\naddition,werecordtheperiodvaluesof 10−Pnext-\nlargest“false”peaksin s(τ). Thepeakperiodsare\ndenotedby τp,p= 1,... ,10,andthetypesofthe\npeakby φp= [1,... ,1,0,... ,0]where1indicates\ntruepeaksand0thefalseones.\n1.c) Parameter-speciﬁcsaliencefunctions sm,i(τ)arecal-\nculatedusing(8)andcurrent Vandspecial Wwhich\nhasvalue1atposition [W]m,iand0elsewhere.\n1.d) Foreachtrueorfalsepeak p= 1,... ,10,thevalue\nofsm,i(τp)isstoredinmatrix Qonrow p+10(g−1)\nandcolumn m+(i−1)M.Thepeaktype φpisstored\ninvector conrow p+ 10(g−1).\nAfterallinstances ghavebeenprocessedandthecorre-\nspondingvaluesstoredinmatrix Qandvector c,updated\nparameters wareobtainedbyleast-squaresestimation\nw= (QTQ)−1QTc.(10)\nThecorrespondingmatrix Wisobtainedbystoringthe\n3Mvaluesinvector wtothethreecolumnsof W.Equa-\ntion(10)ﬁndsparameterswhichsatisfy s(τ)≈1atthe\npositionsof“true”peaks,and s(τ)≈0forthefalseones.Updating V. Updatingthematrix Visanalogousto\nabove. Foreachtraininginstance g,thefollowingopera-\ntionsareperformed:\n2.a) and2.b)areidenticalto1.a)and1.b),respectively.\n2.c) Parameter-speciﬁcsaliencefunctions sn,j(τ)arecal-\nculatedusing(8)andcurrent Wandspecial Vwhich\nhasvalue1atposition [V]n,jand0elsewhere.\n2.d) Foreachtrueorfalsepeak p= 1,... ,10,thevalue\nofsn,j(τp)isstoredinmatrix Oonrow p+10(g−1)\nandcolumn n+(j−1)N.Thepeaktype φpisstored\ninvector donrow p+ 10(g−1).\nAfterallcases ghavebeenprocessedandthecorre-\nspondingvaluesstoredinmatrix Oandvector d,updated\nparameters vareobtainedbyleast-squaresestimation v=\n(OTO)−1OTd.Thecorrespondingmatrix Visobtained\nbystoringthe 2JNvaluesinvector vtothe 2Jcolumns\nofV.\n3. RESULTS\nFigure1showssomeexamplesaliencefunctionscalcu-\nlatedforrandomsoundmixturesusingtheproposedmethod\n(rightpanels)and,forcomparison,forabaselinemethod\n(leftpanels).Asabaselinemethod,wechosethesalience\nfunctionproposedin[4,Eq.(3)].2Thebaselinemethodis\npracticallyidenticaltothesimplemodel(9)whichisused\ntoinitializetheparameterlearningprocesshere.\nThetwopanelsontopofFigure1showtheoutputof\nthebaselineandtheproposedmethodforasingleharmonic\nsound.Thetruepitchperiodismarkedwithacircleandthe\nremaininglargestfalsepeaksareindicatedwithcrosses.\nTheproposedmethodiseffectiveinsuppressingtheextra-\nneouspeakstozerolevel(indicatedbythehorizontalline)\nandinforcingthetruepeaktowardsunityvalue. Forcu-\nriosity,thenexttwopanelsshowtheoutputsofthebaseline\nandtheproposedsystemforasinglesinusoidalcompo-\nnent.Thelastfourpanelsshowtheoutputofthebaseline\nsystemandtheproposedsystemforarandomcombina-\ntionoftwoandfoursounds.Asthepolyphonyincreases,\ntheproposedmethodtooshowsmanyspuriouspeaksal-\nthoughitsresultisstillconsiderablycleanerthatthebas e-\nlinemethod.\nFigure2showsprecision,recall,andF-measureforthe\nproposedmethod(solidline)andforthebaselinemethod\n(dashedline)usingsynthesizedMIDIsignalsastestmate-\nrial.Theresultswerecalculatedbyﬁxingathresholdvalue\nT0,pickingallthepeaksinallframesabovethethreshold,\nandthencalculatingtheresultingprecision π=C(corr.)\nC(det.),\nrecallρ=C(corr.)\nC(ref.),andF-measure ϕ= 2πρ/(π+ρ).Here\nC(·),denotesthecountofcorrectpitchesfound(corr.),\ncountofallpitchesdetected(det.),orcountofpitchesin\nthereference(ref.).Byvaryingthethreshold,differentp re-\ncision/recalltradeoffswereobtained.\n2Thesubsequentiterativedetectionandcancellationproce ssin[4]\nwasnotusedhere,sinceitwouldleadtoadiscretesetofF0va lues.\n61810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n36 48 60 72 84 960102030\nPitch (MIDI)Salience\n36 48 60 72 84 96−1−0.500.511.5\nPitch (MIDI)Salience\n36 48 60 72 84 960510152025\nPitch (MIDI)Salience\n36 48 60 72 84 96−1−0.500.511.5\nPitch (MIDI)Salience\n36 48 60 72 84 96010203040\nPitch (MIDI)Salience\n36 48 60 72 84 96−1−0.500.511.5\nPitch (MIDI)Salience\n36 48 60 72 84 96010203040\nPitch (MIDI)Salience\n36 48 60 72 84 96−1−0.500.511.5\nPitch (MIDI)Salience\nFigure1. Examplesaliencefunctionsforthebaseline\nmethod(leftpanels)andfortheproposedmethod(right\npanels). Thefourcasesfromtoptobottomrepresent\n1)harmonicsound, 2)singlesinusoidalcomponent, 3)\nmixtureoftwoharmonicsounds,and4)mixtureoffour\nsounds.Peakscorrespondingtothetruepitcharecircled.\nTheMIDIpieceswereobtainedbysynthesizingrandom\npiecesfromtheRWCPopandRWCGenredatabases[8]\nandfrommidifarm.com.SynthesisoftheMIDIﬁleswas\nusedinordertoensurethecorrectnessandsynchroniza-\ntionbetweenthesynthesizedﬁleandthereferenceMIDI.\nTimiditysoftwaresynthesizerandGeneralUserGS1.4sound -\nfontwereusedforthesynthesis.AscanbeseeninFig.2,\ntheproposedmethodimprovessigniﬁcantlyoverthebase-\nlinemethod.Hereoneshouldnotpaytoomuchattention\nontheabsolutenumericalvalues,sincethepolyphonyof\nthepiecesisquitehighandespeciallythetailsoflong\nsoundscanbeveryweakanddifﬁculttodetect.\n4. APPLICATIONTOPITCHVISUALIZATION\nFigure3showsthecomputedsalience s(τ)asafunctionof\ntimeforthepieceNo.34inRWCPopularMusicdatabase\n[8].Here,theaudiofromthedatabasewasusedinsteadof\nsynthesizingfromMIDI.ThereferenceMIDIﬁleisren-\nderedontopofthesaliencefunctionasboxes. Inthis0.4 0.5 0.6 0.7 0.800.20.40.60.8\nPrecisionRecall\n0.4 0.5 0.6 0.7 0.800.20.40.60.8\nPrecisionF−measure\n024681012140100200300400\nPolyphonyCount\nFigure2. Precision,recall,andF-measurecalculatedfor\nsynthesizedMIDIsignals.Resultsareshownforthepro-\nposedmethod(solidline)andforthebaselinemethod\n(dashedline). Thethirdpanelshowsahistrogramofthe\nnumberofconcurrentsoundsinthetestdata.\n“pianoroll”representation,thenotesarearrangedonthe\nverticalaxisandtimeﬂowsfromlefttoright.\nManypeoplearenotcomfortablewithreadingmusicdi-\nrectlyfromapiano-roll.Thereforeweproposeheretomap\nthedatafromthepiano-rolltothetraditionalstaves.This\n“fuzzyscore”isveryhandysinceitallowsstudyingper-\nformancenuances,suchastimingdeviationsandsinging\npitchglidesandvibratoquiteeasily.\nFigure4illustratesthemappingofdifferentnotesonthe\nlinesandspacesofthestaves. Importanttonoticeisthat\nthenotepositionsdependonthemusicalkeyofthepiece,\nthereforekeyestimationisnecessarytorenderthesalienc e\nfunctiononthestaves. Hereweusedthekeyestimator\nfrom[9].Secondly,themappingisnotlinear:thedistance\nbetweenalineanditsneighbouringspaceonthestavescan\nbeeitheroneortwosemitones.Forthispurpose,thepiano-\nrollrepresentationis“stretched”or“shrunk”toalignwit h\nthestaves.\nAnotherrequirementtomakethescorereadablearebar\nlineswhichfunctionastemporalanchorsandmakethe\ntimingofnotesreadable. Hereweusedthemeteranaly-\nsismethodfrom[10].Thebarlinesareindicatedwithver-\nticallinesandpossibletempochangesappearasvarying\ndistancesbetweenthebarlines.\nFigure5showstheresulting“fuzzyscore”representa-\ntionforthesameexamplethatwasshowninFig.3. The\nreferenceMIDIfromtheRWCdatabaseisdrawnwithcir-\nclesontopofthesalience.Moreexamplesofthecomputed\nfuzzyscoresandthecorrespondingaudioexcerptscanbe\nfoundathttp://www.cs.tut.ﬁ/sgn/arg/klap/ismir09/.\n5. CONCLUSIONS\nTheproposedpitchsaliencemodelwasshowntoimprove\noverthebaselinemethodintermsofprecisionandrecall\nwhendetectingmultiplesimultaneouspitchesinsynthe-\n619Poster Session 4\n33 34 35 36 37 3836486072\nTime (s)Pitch (MIDI)\nFigure3.Computedpitchsalienceforanexcerptofpiece\nNo.34inRWCPopdatabase.\nC4 D4 E4 F4 G4 A4 B4 C5 D5 E5 F5 C major\nC#4D4 E4 F#4G4 A4 B4 C#5D5 E5 F#5D major\n##\nC4 D4 Eb4F4 G4 A4 Bb4C5 D5 Eb5F5 F minor\nbb\nFigure4.Mappingofpitchvaluesonthestavespositions\ninafewexamplemusicalkeys.\nsizedMIDIﬁles.Thisisduetothesaliencemodelwhich\nallowssuppressingthepeaksthatoccurat(sub)multiples\nofthetruepitchesinthesaliencefunction.Themainad-\nvantageoftheproposedmethodcomparedtomanyexist-\ningmultipitchdetectionmethods,however,isthatitpro-\nducesacontinuousfunctionthatindicatesthesalienceof\nallpitchcandidateswithinagivenrange.Thismakesthe\nproposedmethodparticularlysuitableforpitchcontentvi -\nsualization. Tothisend,theproposedmethodwasaug-\nmentedwithmusicalkeyandmeterestimationmethods\nwhichallowrenderingthecomputedsalienceonthestaves\nofcommonmusicalnotation.3\n6. REFERENCES\n[1] H.Kameoka,T.Nishimoto,andS.Sagayama,“Amul-\ntipitchanalyzerbasedonharmonictemporalstructured\nclustering,”IEEETrans.Audio,Speech,andLanguage\nProc.,vol.15,no.3,pp.982–994,2007.\n[2] M.KarjalainenandT.Tolonen,“Multi-pitchandperi-\nodicityanalysismodelforsoundseparationandaudi-\ntorysceneanalysis,”in IEEEInternationalConference\n3This work was supported by the Academy of Finland, project\n129657,(FinnishCentreofExcellenceProgram2006-2011).33 34 35 36 37 38\nbb\nbb\nTime (s)\nFigure5. Computedpitchsaliencedatarenderedonthe\nstaves.\nonAcoustics,Speech,andSignalProcessing ,Phoenix,\nUSA,1999.\n[3]C. Yeh,Multiple fundamental frequency estimation\nofpolyphonicrecordings ,Ph.D.thesis,Universityof\nParisVI,2008.\n[4]A.Klapuri,“Multiplefundamentalfrequencyestima-\ntionbysummingharmonicamplitudes,”in Intl.Conf.\non Music Information Retrieval , Victoria, Canada,\n2006.\n[5]E.Vincent,N.Bertin,andR.Badeau,“Twononnega-\ntivematrixfactorizationmethodsforpolyphonicpitch\ntranscription,”inMIREX’07ExtendedAbstracts ,2007.\n[6]D.P.W.EllisandG.Poliner, “Classiﬁcation-based\nmelodytranscription,” MachineLearning,vol.65,no.\n2–3,pp.439–456,2006.\n[7]A.deCheveign ´e,“MultipleF0estimation,”in Com-\nputationalAuditorySceneAnalysis:Principles,Algo-\nrithmsandApplications ,D.WangandG.J.Brown,\nEds.Wiley–IEEEPress,2006.\n[8]M.Goto,H.Hashiguchi,T.Nishimura,andR.Oka,\n“RWCmusicdatabase:Musicgenredatabaseandmu-\nsicalinstrumentsounddatabase,”in InternationalCon-\nferenceonMusicInformationRetrieval , Baltimore,\nUSA,Oct.2003,pp.229–230.\n[9]M.Ryyn¨anenandA.Klapuri,“Automatictranscription\nofmelody,bassline,andchordsinpolyphonicmusic,”\nComputerMusicJournal ,vol.32,no.3,pp.72–86,\n2008.\n[10] A.Klapuri,A.Eronen,andJ.Astola,“Analysisofthe\nmeterofacousticmusicalsignals,” IEEETrans.Speech\nandAudioProcessing ,vol.14,no.1,2006.\n620"
    },
    {
        "title": "Augmenting Text-based Music Retrieval with Audio Similarity: Advantages and Limitations.",
        "author": [
            "Peter Knees",
            "Tim Pohle",
            "Markus Schedl",
            "Dominik Schnitzer",
            "Klaus Seyerlehner",
            "Gerhard Widmer"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418361",
        "url": "https://doi.org/10.5281/zenodo.1418361",
        "ee": "https://zenodo.org/records/1418361/files/KneesPSSSW09.pdf",
        "abstract": "We investigate an approach to a music search engine that indexes music pieces based on related Web documents. This allows for searching for relevant music pieces by issuing descriptive textual queries. In this paper, we examine the effects of incorporating audio-based similarity into the text-based ranking process – either by directly modifying the retrieval process or by performing post-hoc audiobased re-ranking of the search results. The aim of this combination is to improve ranking quality by including relevant tracks that are left out by text-based retrieval approaches. Our evaluations show overall improvements but also expose limitations of these unsupervised approaches to combining sources. Evaluations are carried out on two collections, one large real-world collection containing about 35,000 tracks and on the CAL500 set.",
        "zenodo_id": 1418361,
        "dblp_key": "conf/ismir/KneesPSSSW09",
        "keywords": [
            "music search engine",
            "indexing music pieces",
            "related Web documents",
            "descriptive textual queries",
            "audio-based similarity",
            "text-based ranking process",
            "post-hoc audiobased re-ranking",
            "search results",
            "improvements",
            "limitations"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAUGMENTING TEXT-BASED MUSIC RETRIEV AL\nWITH AUDIO SIMILARITY\nP. Knees1, T. Pohle1, M. Schedl1, D. Schnitzer1,2, K. Seyerlehner1, and G. Widmer1,2\n1Department of Computational Perception, Johannes Kepler Un iversity Linz, Austria\n2Austrian Research Institute for Artiﬁcial Intelligence (OF AI), Vienna, Austria\nABSTRACT\nWe investigate an approach to a music search engine\nthat indexes music pieces based on related Web documents.\nThis allows for searching for relevant music pieces by is-\nsuing descriptive textual queries. In this paper, we exam-\nine the effects of incorporating audio-based similarity in to\nthe text-based ranking process – either by directly modify-\ning the retrieval process or by performing post-hoc audio-\nbased re-ranking of the search results. The aim of this com-\nbination is to improve ranking quality by including relevan t\ntracks that are left out by text-based retrieval approaches .\nOur evaluations show overall improvements but also ex-\npose limitations of these unsupervised approaches to com-\nbining sources. Evaluations are carried out on two col-\nlections, one large real-world collection containing abou t\n35,000 tracks and on the CAL500 set.\n1. MOTIV ATION AND RELATED WORK\nIn the last years, the development of query-by-description\nmusic search engines has drawn increasing attention [1–\n5]. Given the size of (commercial) digital music collec-\ntions nowadays (several millions of tracks), this is not a\nbig surprise. While most “traditional” music retrieval ap-\nproaches pursue a query-by-example strategy, i.e., given a\nmusic piece, ﬁnd me other pieces that sound alike, query-\nby-description systems are capable of retrieving relevant\npieces by allowing to type in textual queries targeting mu-\nsical or contextual properties beyond common meta-data\ndescriptors. As this method of issuing queries is the com-\nmon way to search the Web, it appears desirable to offer\nthis type of functionality also in the music domain.\nSeveral approaches to accomplish this goal have been\npresented – all of them with a slightly different focus. In [1 ],\nBaumann et al. present a system that incorporates meta-\ndata, lyrics, and acoustic properties all linked together b y\na semantic ontology. Queries are analyzed by means of\nnatural language processing and tokens have to be mapped\nto the corresponding concepts. Celma et al. [2] use a Web\ncrawler focused on audio blogs and exploit the texts from\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .the blogs to index the associated music pieces. Based on\nthe text-based retrieval result, also musically similar so ngs\ncan be discovered. In [3], we propose to combine audio\nsimilarity and textual content from Web documents ob-\ntained via Google queries to create representations of mu-\nsic pieces in a term vector space. A modiﬁcation to this\napproach is presented in [6]. Instead of constructing term\nvector representations, an index of all downloaded Web\ndocuments is created. Relevance wrt. a given query is as-\nsessed by querying the Web document index and applying\na technique called rank-based relevance scoring that takes\ninto account the associations between music tracks and\nWeb documents (cf. Section 2.1). Evaluations show that\nthis document-centered approach is superior to the vector\nspace approach. However, as this method is solely based\non texts from the Web it may neglect important acoustic\nproperties and suffer from effects such as popularity bias.\nFurthermore, inadequately represented tracks and tracks\nnot present on the Web are penalized by this approach. In\nthis paper, we aim at remedying these shortcomings and\nimproving ranking quality by incorporating audio similar-\nity into the retrieval process.\nRecently, the method of relevance scoring has also been\nadapted to serve as a source of information for automat-\nically tagging music pieces with semantic labels. In [5],\nBarrington et al. successfully combine audio content fea-\ntures (MFCC and Chroma) with social context features\n(Web documents and last.fm tags) via machine learning\nmethods and therefore improve prediction accuracy. The\nusefulness of audio similarity for automatic tagging is als o\nshown in [4] where tags from well-tagged tracks are prop-\nagated to untagged tracks based on acoustic similarity.\nThe remainder of this paper is organized as follows: In\nthe next section we review methods for Web-based mu-\nsic track indexing and audio-based similarity computation .\nSection 3 describes two possible modiﬁcations of the ini-\ntial approach that are examined in Section 4. In Section 5,\nbased on these results, we discuss perspectives and lim-\nitations of combining Web- and audio-based approaches\nbefore drawing conclusions in Section 6.\n2. INCORPORATED TECHNIQUES\nIn the following, we explain the methods for constructing\na Web-based retrieval system and calculating audio simi-\nlarity, which we combine in Section 3.\n579Poster Session 4\n2.1 Web-based Indexing and RRS Ranking\nThe idea of Web-based indexing is to collect a high num-\nber of texts related to the pieces in the music collection\nto gather many diverse descriptions (and hence a rich in-\ndexing vocabulary) and allow for a large number of possi-\nble queries. In our ﬁrst approach, we aimed at permitting\nvirtually any query by involving Google for query expan-\nsion [3]. When introducing rank-based relevance scoring\n(RRS) , we renounced this step in favor of reduced com-\nplexity and improved ranking results [6]. From our point it\nis very reasonable to limit the indexing vocabulary to terms\nthat actually co-occur with the music pieces (which is still\nvery large). Construction of an index with a corresponding\nretrieval scheme is performed as follows.\nTo obtain a broad basis of track speciﬁc texts, for each\nmusic piece min the collection M, three queries are issued\nto Google based on the information found in the id3 tags\nof the music pieces:\n1. “artist ” music\n2. “artist ” “album ” music review -lyrics\n3. “artist ” “title” music review -lyrics\nFor each query, at most 100 of the top-ranked Web pages\nare retrieved and joined into a set (denoted as Dmin the\nfollowing). For retrieval, we utilize the open source pack-\nageNutch1. Beside efﬁcient retrieval, a further beneﬁt is\nthat all retrieved pages are also automatically indexed by\nLucene2that uses a tfxidfvariant as scoring function [7].\nThe resulting Web page index is then used to obtain a rel-\nevance ranking of Web pages for arbitrary queries. This\npage ranking, together with the information on associa-\ntions between pages and tracks, serves as input to the RRS\nscheme. Compared to the original formulation in [6], we\nintroduce the additional parameter nthat is used to limit\nthe number of top-ranked documents when querying the\npage index. For large collections, this is necessary to keep\nresponse time of the system short. For a given query qand\nfor each music piece m, scores are calculated as:\nRRS n(m,q) =/summationdisplay\np∈Dm∩Dq,n1 +|Dq,n| −rnk(p,D q,n),\n(1)\nwhere Dmis the set of text documents associated with\nmusic piece m(see above), Dq,nthe ordered set (i.e., the\nranking) of nmost relevant text documents with respect to\nquery q, andrnk(p,D q,n)a function that returns the rank\nof document pinDq,n(highest relevance corresponds to\nrank 1, lowest to rank |Dq,n|). The ﬁnal relevance ranking\nof music tracks is then obtained by sorting the music pieces\naccording to their RRS value.\nNote that, as suggested in [5, 8], we also experimented\nwith a weight-based version of relevance scoring (WRS)\nthat incorporates the scores of the Web page retrieval step\nrather than the ranks. In our framework this modiﬁcation\n1http://lucene.apache.org/nutch\n2http://lucene.apache.orgworsened performance. Possible explanations are the dif-\nferences in the underlying page scoring function or the dif-\nferent sources of Web pages (cf. [8]).\n2.1.1 Pseudo-Document Indexing\nInstead of modifying the page scoring scheme, we invented\na simple alternative approach for text-based indexing that\nlies conceptually between the ﬁrst vector space approach [3 ]\nand the relevance scoring scheme. For each music piece\nm, we concatenate all retrieved texts (i.e., all texts from\nDm) into a single document which we index with Lucene .\nHence, each music piece is represented by a single docu-\nment that contains all relevant texts. Querying this pseudo-\ndocument index results directly in a ranking of music pieces.\nThis rather “quick-and-dirty” indexing method will serve\nas a reference point in the evaluations and give insights\ninto the capabilities of purely Web-based retrieval.\n2.2 Audio-Based Similarity\nFor calculating music similarities, or more precisely, dis -\ntances of tracks based on the audio content, we apply our\nalgorithm which competed successfully in the “Audio Mu-\nsic Similarity and Retrieval” task of MIREX 2007 [9]. For\neach piece of music, Mel Frequency Cepstral Coefﬁcients\n(MFCCs) are computed on short-time audio frames to char-\nacterize the frequency distribution of each frame and hence\nmodel aspects of timbre. On each frame, 25 MFCCs are\ncomputed. Each song is then represented as a Gaussian\nMixture Model (GMM) of the distribution of MFCCs using\na Single Gaussian Model with full covariance matrix [10].\nThe distance between these models is denoted by dG.\nBeside the MFCC-based distance component, also Fluc-\ntuation Patterns (FPs) are computed as proposed in [11]. A\ntrack is represented as a 12-band spectrogram and for each\nband, a Fast Fourier Transformation (FFT) of the ampli-\ntude is taken over a window of six seconds. The result-\ning matrix is referred to as the Fluctuation Pattern of the\nsong. The FPs of two songs are compared by calculating\nthe cosine distance, denoted by dFP. Furthermore, two ad-\nditional FP-related features are computed: Bass (FPB) and\nGravity (FPG) . These two features are scalar and the dis-\ntance between two songs is calculated by subtracting them,\ndenoted by dFPB anddFPG . To obtain an overall distance\nvaluedmeasuring the (dis)similarity of two songs, all de-\nscribed distance measures are z-normalized and then com-\nbined by a simple arithmetic weighting:\nd= 0.7·zG+ 0.1·(zFP+zFPB+zFPG) (2)\nwhere zxis the value of dxafter z-normalization. Fi-\nnally, distances between two songs are symmetrized. For\nsimilarity computation, we ignore all pairs of songs by the\nsame artist (artist ﬁltering, cf. [12]) since this similari ty is\nalready represented within the Web features.\n3. COMBINATION APPROACHES\nThis section describes two different approaches for com-\nbining the purely text-based retrieval approach with the\n58010th International Society for Music Information Retrieval Conference (ISMIR 2009)\naudio-based similarity information. According to [5, 13],\nthe ﬁrst approach can be considered an early fusion ap-\nproach, since it incorporates the audio similarity informa -\ntion directly into the relevance scoring scheme, whereas\nthe second approach can be considered a late fusion ap-\nproach, since it modiﬁes the ranking results obtained from\nthe Web-based retrieval. Basically, both algorithms in-\ncorporate the idea of including tracks that sound similar\nto tracks already present through text-only retrieval. The\nscore of a track mis calculated by summing up a score\nfor being present in the text-based ranking and scores for\nbeing present within the nearest audio neighbors of tracks\nassociated with the text-based ranking.\n3.1 Modifying the Scoring Scheme (aRRS)\nWith this approach, we try to incorporate the audio simi-\nlarity directly into the scoring scheme of RRS. The advan-\ntage is that this has to be calculated only once and does not\nrequire post-processing steps. The audio-inﬂuenced RRS\n(aRRS) is calculated as:\naRRS n(m,q) =/summationdisplay\np∈Pm,q,nRF(p,D q,n)·MF(m,p),(3)\nRF(p,D q,n) = 1 + |Dq,n| −rnk(p,D q,n), (4)\nMF(m,p) =α·I(p,D m) +/summationdisplay\na∈AmI(p,D a),(5)\nwhere Pm,q,n = (Dm∪DAm)∩Dq,n,Na,ktheknear-\nest audio neighbors of a,Amthe set of all tracks athat\ncontain min their nearest audio neighbor set, i.e., all afor\nwhich m∈Na,k,DAmthe set of all documents associ-\nated with any member of Am, andI(x,D)a function that\nreturns 1iffx∈Dand0otherwise. Informally speak-\ning, also tracks sounding similar to track mparticipate if a\npage relevant to moccurs in the page ranking for query q.\nThe parameter αis used to control the inﬂuence of tracks\nthat are directly associated with a Web page (in contrast to\ntracks associated via audio neighbors). In our experiments\nwe set α= 10 . Note that aRRS is a generalization of RRS,\nas they are identical for k= 0.\n3.2 Post-Hoc Audio-Based Re-Ranking (PAR)\nThe second approach incorporates audio similarity into an\nalready existing ranking R. The advantage of this approach\nis that it can deal with outputs from arbitrary ranking al-\ngorithms. The post-hoc audio-based re-ranking (PAR) is\ncalculated as:\nPAR(m,R) =/summationdisplay\nt∈(m∪Am)∩RRF(t,R)·NF(m,t),(6)\nNF(m,t) =α·I(m,{t})+G(rnk(m,N t,k))·I(m,N t,k),\n(7)\nG(i) =e−(i/2)2\n2/√\n2π, (8)We included the gaussian weighting Gin this re-ranking\nscheme because it yielded best results when exploring pos-\nsible weightings. Parameter αcan be used to control the\nscoring of tracks already present in R. Note that for k= 0,\nRremains unchanged.\n4. EV ALUATION\nFor evaluation, we decided to use two test collections with\ndifferent characteristics. The ﬁrst collection is a large r eal-\nworld collection and contains mostly popular pieces. The\nsecond collection is the CAL500 set, a manually annotated\ncorpus of 500 tracks by 500 distinct artists [14]. In the\nfollowing, we describe both test collections in more detail .\n4.1 The c35k Collection\nThec35k collection is a large real-world collection, orig-\ninating from a subset of a digital music retailer’s catalog.\nThe full evaluation collection contains about 60,000 track s.\nFiltering of duplicates (including remixes, live versions ,\netc.; cf. [3]) reduces the number of tracks to about 48,000.\nAs groundtruth for this collection, we utilize last.fm tags .\nTags can be used directly as test queries to the system and\nserve also as relevance indicator (i.e., a track is consider ed\nto be relevant for query qif it has been tagged with tag\nq). From the 48,000 tracks, we were able to ﬁnd track-\nspeciﬁc last.fm tags for about 35,000 of the tracks. To ob-\ntain a set of test queries, we started with last.fm’s list of\ntop-tags and manually removed tags useless for our pur-\npose (such as seen live or tags starting with favorite ). We\nalso searched for redundant tags (such as hiphop ,hip hop ,\nandhip-hop ) and harmonized their sets of tagged tracks.\nHowever, all forms are kept as queries if they translate to\ndifferent queries (in the example above, hiphop translates\nto a query with one token, hip hop to two tokens, and hip-\nhopto a phrase). As result, a set of 223 queries remained.\nFrom the 223 tags we further removed all tags with a num-\nber of associated tracks above the 0.95-percentile and be-\nlow the 0.05-percentile, resulting in 200 test queries. A\ncommon way to increase the number of tagged examples\nis to use artist-speciﬁc tags if no track-speciﬁc tags are\npresent [3, 8]. Since, in our indexing approach, tracks by\nthe same artist share a large portion of relevant Websites,\nwe decided against combination with artist tags to avoid\noverestimation of performance.\n4.2 The CAL500 Set\nThe CAL500 set is a highly valuable collection for mu-\nsic information retrieval tasks [14]. It contains 500 songs\n(each from a different artist) which are manually anno-\ntated by at least three reviewers. Annotations are made\nwrt. a vocabulary consisting of 174 tags describing musi-\ncally relevant concepts such as genres, emotions, acoustic\nqualities, instruments, or usage scenarios. Although we\nconsider the fact that our indexing approach is in princi-\nple capable of dealing with large and varying vocabularies,\nsome of these tags are not directly suited as query, espe-\ncially negating concepts (e.g., NOT-Emotion-Angry ) can\n581Poster Session 4\nRecall Precision Prec@10 r-Precision Avg. Prec. (MAP)\nBaseline 100.00 3.65 3.60 3.65 3.68\nWeb only PAR Web only PAR Web only PAR Web only PAR Web only PAR\nPseudoDoc 93.66 98.79 4.27 3.67 39.25 17.40 30.78 22.94 25.97 18.81\nRRS aRRS PAR RRS aRRS PAR RRS aRRS PAR RRS aRRS PAR RRS aRRS PAR\nn= 10 2.18 3.67 10.71 30.15 18.73 6.81 31.19 31.33 30.85 2.16 3.27 6.22 1.19 1.43 2.21\nn= 20 3.74 6.16 16.89 29.02 17.95 6.57 32.40 32.15 32.40 3.63 5.17 8.46 1.84 2.25 3.37\nn= 50 7.17 11.28 27.76 27.61 16.02 6.17 38.45 37.85 38.40 6.52 8.37 11.66 3.24 3.87 5.53\nn= 100 12.72 19.64 39.41 25.99 13.72 5.66 44.10 43.55 43.95 10.24 12.52 14.74 5.54 6.51 8.44\nn= 200 18.67 28.65 50.98 23.77 12.10 5.25 47.75 47.75 47.65 14.22 16.67 17.82 8.23 9.61 11.51\nn= 500 29.31 44.10 66.60 20.12 9.77 4.81 50.30 49.95 50.15 19.84 21.58 22.02 12.39 14.02 15.91\nn= 1000 40.38 58.17 77.63 16.88 8.12 4.50 52.55 51.80 52.35 24.22 24.52 25.21 16.10 17.56 19.23\nn= 10000 80.50 95.19 96.68 7.29 4.25 3.85 57.45 57.50 38.20 35.20 32.81 32.26 29.98 28.48 26.45\nTable 1 . Evaluation results for the c35k collection: Both re-ranki ng approaches (aRRS and PAR) are compared against the\ntext-only RRS approach for different numbers of maximum con sidered top-ranked pages n. For both aRRS and PAR, we\nsetk= 50 , for PAR, αis also set to 50. Values (given in %) are obtained by averaging over 200 evalu ation queries.\nRecall Precision Prec@10 r-Precision Avg. Prec. (MAP)\nBaseline 100.00 13.32 13.33 13.31 14.31\nWeb only PAR Web only PAR Web only PAR Web only PAR Web only PAR\nPseudoDoc 81.15 98.83 14.50 13.34 30.72 31.15 25.77 26.28 22.66 25.74\nRRS aRRS PAR RRS aRRS PAR RRS aRRS PAR RRS aRRS PAR RRS aRRS PAR\nn= 10 5.96 62.23 59.08 25.77 14.49 14.84 25.77 23.81 23.74 5.61 18.35 18.44 3.58 14.26 13.51\nn= 20 10.19 80.90 75.90 24.87 13.99 14.34 25.98 26.40 25.61 8.84 20.55 20.70 5.30 18.36 17.20\nn= 50 17.99 93.45 89.52 22.84 13.33 13.63 26.06 27.84 26.04 13.49 22.92 22.23 7.57 21.55 20.19\nn= 100 26.80 96.60 94.78 21.02 13.15 13.39 29.30 30.07 29.28 18.05 23.88 23.79 10.59 23.07 22.41\nn= 200 38.63 97.23 96.38 19.15 13.08 13.22 30.60 31.58 30.79 21.58 24.32 24.38 13.84 24.27 23.90\nn= 500 56.31 97.37 97.05 16.86 13.07 13.15 32.68 32.52 33.17 24.06 25.79 25.91 18.02 25.19 25.27\nn= 1000 66.91 97.47 97.18 15.54 13.06 13.13 33.47 33.45 33.60 24.86 25.90 26.52 20.37 25.85 25.64\nn= 10000 73.27 97.61 97.31 14.56 13.05 13.13 33.62 33.74 33.67 25.06 26.95 26.76 21.77 26.58 25.82\nTable 2 . Evaluation results for the CAL500 set: Values are obtained by averaging over 139 evaluation queries. Apart from\nthat, the same settings as in Table 1 are applied.\nnot be used. Hence, we remove all negating tags. Fur-\nthermore, we join redundant tags (mostly genre descrip-\ntors). For tags consisting of multiple descriptions (e.g.,\nEmotion-Emotional/Passionate ) we use every description\nas independent query. This results in a total set of 139 test\nqueries.\n4.3 Evaluation Measures and Results\nTo measure the quality of the obtained rankings and the\nimpact of the combination approaches, we calculate stan-\ndard evaluation measures for retrieval systems, cf. [15].\nTable 1 shows the results for the c35k collection (averaged\nover all 200 queries): The top row contains the baseline\nthat has been empirically determined by repeated evalu-\nation of random permutations of the collection. Not un-\nexpectedly, the incorporation of additional tracks via the\naudio similarity measure leads to an increase in overall re-\ncall while precision is worsened. However, these global\nmeasures are not too important since for rankings one is\nin general more interested in how fast (i.e., at which posi-\ntion in the ranking) relevant results are returned. To this\nend, measures like Precision @ 10 documents ,r-Precision\n(i.e., precision at the rthreturned document, where ris the\nnumber of tracks relevant to the query), and (mean) aver-\nage precision (i.e., the arithmetic mean of precision values\nat all encountered relevant documents) give more insight\ninto the quality of a ranking. For r-Precision and aver-age precision we can clearly see that PAR (and also aRRS)\nperform better than text-based RRS. However, when com-\nparing this to the pseudo-document indexing approach, we\nsee that this simple and efﬁcient ranking technique is in\nmost cases even better than the combination with audio.3\nThus, although audio similarity may improve results, it\ncan not keep up to a well working text-based approach.\nFurthermore, we can see that incorporation of audio wors-\nens results if recall of the initial ranking is already high\n(n=10000, PseudoDoc). The reason is that audio simi-\nlarity introduces a lot of noise into the ranking. Hence,\nto preserve the good performance at the top of the rank-\nings,αshould be set to a high value. On the other hand,\nthis prevents theoretically possible improvements. For th e\nCAL500 set (Table 2), things look a bit different. Here,\nthe aRRS approach performs clearly superior to RRS. Im-\nprovements can even be observed within the ﬁrst ten docu-\nments. For this collection, also results of the PseudoDoc\napproach can be improved by applying post-hoc audio-\nbased re-ranking. For comparison of different retrieval\nstrategies, we calculated precision at 11 standard recall\nlevels . For each query, precision P(rj)at the 11 stan-\ndard recall levels rj,j∈ {0.0,0.1,0.2,...,1.0}is inter-\npolated according to P(rj) =max rj≤r≤rj+1P(r). This\nallows averaging over all queries and results in character-\n3Note that the Web only recall value of PseudoDoc represents the up-\nper bound for all purely text-based approaches.\n58210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.000.10.20.30.40.50.60.70.8\nRecallPrecision\n  \nRRS (n=10,000)\nRRS−PAR (n=10,000)\nRRS (n=500)\nRRS−PAR (n=500)\nRRS (n=100)\nRRS−PAR (n=100)\nPseudo Doc\nPseudo Doc−PAR\nBaseline\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.000.10.20.30.40.50.60.7\nRecallPrecision\n  \naRRS (n=10,000)\naRRS (n=500)\naRRS (n=100)\nRRS (n=10,000)\nRRS (n=500)\nRRS (n=100)\nBaseline\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.000.10.20.30.40.50.60.7\nRecallPrecision\n  \nRRS−PAR (n=1,000, alpha=50)\nRRS (n=1,000)\nRRS−PAR (n=100, alpha=75)\nRRS−PAR (n=100, alpha=50)\nRRS−PAR (n=100, alpha=20)\nRRS−PAR (n=100, alpha=1)\nRRS (n=100)\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.000.10.20.30.40.50.60.7\nRecallPrecision\n  \naRRS (n=1,000, k=100)\naRRS (n=1,000, k=50)\naRRS (n=1,000, k=25)\naRRS (n=1,000, k=10)\nRRS (n=1,000)\naRRS (n=50, k=100)\naRRS (n=50, k=50)\naRRS (n=50, k=25)\naRRS (n=50, k=10)\nRRS (n=50)\nFigure 1 .Precision at 11 Standard Recall Levels plots: The upper left plot depicts selected curves (average d over all\nqueries) from evaluating the c35k set for comparison of the R RS approach and subsequent PAR re-rankings. The upper\nright plot depicts (averaged) curves from the CAL500 set for comparison of the RRS and the aRRS approaches. The lower\nﬁgures are intended to give an impression of the effects of di fferent parameters for PAR (left) and aRRS (right). Both are\ncalculated on the CAL500 set.\nistic curves for each retrieval algorithm, enabling compar i-\nson of distinct algorithms/settings. Figure 1 depicts seve ral\nprecision at 11 standard recall level curves. The two plots\nat the top basically conﬁrm what could be seen in tables 1\nand 2. The two plots at the bottom show the inﬂuence of\nparameters αandkon the retrieval quality.\nUsing the CAL500 set, we can (rather informally) eval-\nuate how audio similarity inﬂuences retrieval of tracks fro m\nthe so-called “long tail”. To this end, we restricted the\nset of relevant tracks for each query to contain only tracks\nfrom the (in general not so well known) online record label\nMagnatune. Absolute numbers resulting from this type of\nevaluation are rather discouraging, however, when compar-\ning the results from RRS 200with those from aRRS 200on\nthis modiﬁed ground truth, a small improvement can be ob-\nserved (e.g., MAP increases from 2.03 to 3.68, rPrec from\n2.44 to 2.82). Optimistically spoken, a positive tendency\nis recognizable – from a more realistic perspective, both\nresults are disappointing. In any case, the impact on long\ntail tracks needs a thorough investigation in future work.5. DISCUSSION\nWe have shown that combining Web-based music index-\ning with audio similarity has the potential to improve re-\ntrieval performance. On the other side, we have also seen\nthat even an improved combined retrieval approach may\nbe outperformed by another, rather simple, text-only ap-\nproach. Possible explanations are inadequate combination\nfunctions and/or an inadequate audio similarity measure.\nTo estimate the potential of the audio similarity measure\nfor this task, we examined the 100nearest audio neighbors\nfor every relevant track for a query and for every query,\ni.e., at each position k= 1...100, we calculated the preci-\nsion (wrt. the currently examined query). Figure 2 shows\nthe result averaged over all seed songs and queries for the\nc35k collection. Within the top 10 neighbors, a precision\nof around 7% can be expected in average based solely on\nthe audio similarity. However, it is questionable whether\nthis can be improved as audio similarity measures (stati-\ncally) focus on speciﬁc musical properties, whereas textua l\n583Poster Session 4\n0 20 40 60 80 1000.0620.0640.0660.0680.070.0720.074\naudio NNPrecision\nFigure 2 . Precision at audio-based nearest neighbor for the\nc35k set (averaged over all queries; for every query average\nof rankings with each relevant track as seed).\nqueries can be aimed at basically every aspect of music,\nfrom different acoustic properties, to cultural context, t o\ncompletely unrelated things.\nIn general it has to be stated that proper combination of\nthese two sources is rather difﬁcult since they target dif-\nferent directions and applications. Furthermore, a com-\nbination function can not be optimized in advance to suit\nevery potential query, i.e., in contrast to, e.g., [5], auto -\nmatic learning of proper combination functions (e.g., via\nmachine learning methods) is not applicable for this task\nsince we have no learning target. More precisely, Web-\nbased music indexing as we currently apply it is an unsu-\npervised approach. This is implied by the requirement to\ndeal with a large and arbitrary vocabulary.\n6. CONCLUSIONS AND FUTURE WORK\nWe proposed two methods to combine a Web-based music\nretrieval system with an audio similarity measure to im-\nprove overall ranking results and enable including tracks\nnot present on the Internet into search results. Based on our\nevaluations, we could show that the overall ranking quality\ncan be improved by integrating purely acoustic similarity\ninformation. However, we were also confronted with the\ncurrent limitations of this combination. The ﬁrst results\ngathered, open up new questions for future work, e.g., if\nanother audio similarity measure could produce more sub-\nstantial results. Also the question of combining the dif-\nferent sources will be taken a step further. Possible fu-\nture enhancements could comprise clustering to ﬁnd co-\nherent groups of songs. This could be based on learning\nfrom many queries and ﬁnding stable relations between\nfrequently co-occurring tracks. Another aspect that will b e\ndealt with in future work is the impact on tracks from the\nlong tail. Ideally, a combination would allow for retrieval\nof relevant tracks irrespective of their presence on the Web .\n7. ACKNOWLEDGMENTS\nThis research is supported by the Austrian Fonds zur F ¨or-\nderung der Wissenschaftlichen Forschung (FWF) under pro-\nject number L511-N15. Peter Knees would like to thank\nthe staff of ChEckiT! - Verein Wiener Sozialprojekte (where\nhe carried out his civilian national service) for toleratin g\nand supporting the development of this publication.8. REFERENCES\n[1] S. Baumann, A. Kl ¨uter, and M. Norlien. Using natural\nlanguage input and audio analysis for a human-oriented\nMIR system. Proc. 2nd WEDELMUSIC , 2002.\n[2] O. Celma, P. Cano, and P. Herrera. Search Sounds: An\naudio crawler focused on weblogs. Proc. 7th ISMIR ,\n2006.\n[3] P. Knees, T. Pohle, M. Schedl, and G. Widmer. A Mu-\nsic Search Engine Built upon Audio-based and Web-\nbased Similarity Measures. Proc. 30th ACM SIGIR ,\n2007.\n[4] M. Sordo, C. Laurier, and O. Celma. Annotating mu-\nsic collections: How content-based similarity helps to\npropagate labels. Proc. 8th ISMIR , 2007.\n[5] L. Barrington, D. Turnbull, M. Yazdani, and G. Lanck-\nriet. Combining audio content and social context for se-\nmantic music discovery. Proc. 32nd ACM SIGIR , 2009.\n[6] P. Knees, T. Pohle, M. Schedl, D. Schnitzer, and K.\nSeyerlehner. A Document-centered Approach to a Nat-\nural Language Music Search Engine. Proc. 30th ECIR ,\n2008.\n[7] O. Gospodneti ´c and E. Hatcher. Lucene in Action .\nManning, 2005.\n[8] D. Turnbull, L. Barrington, and G. Lanckriet. Five ap-\nproaches to collecting tags for music. Proc. 9th ISMIR ,\n2008.\n[9] T. Pohle and D. Schnitzer. Striving for an Improved\nAudio Similarity Measure. 4th MIREX , 2007.\n[10] M. Mandel and D. Ellis. Song-Level Features and Sup-\nport Vector Machines for Music Classiﬁcation. Proc.\n6th ISMIR , 2005.\n[11] E. Pampalk. Computational Models of Music Similar-\nity and their Application to Music Information Re-\ntrieval . PhD thesis, Vienna University of Technology,\n2006.\n[12] A. Flexer. A closer look on artist ﬁlters for musical\ngenre classiﬁcation. Proc. 8th ISMIR , 2007.\n[13] C. Snoek, M. Worring, and A. Smeulders. Early versus\nlate fusion in semantic video analysis. Proc. 13th ACM\nMultimedia , 2005.\n[14] D. Turnbull, L. Barrington, D. Torres, and G. Lanck-\nriet. Semantic annotation and retrieval of music and\nsound effects. IEEE TASLP , 16(2):467–476, February\n2008.\n[15] R. Baeza-Yates and B. Ribeiro-Neto. Modern In-\nformation Retrieval . Addison-Wesley, Reading, Mas-\nsachusetts, 1999.\n584"
    },
    {
        "title": "Automatic Generation of Musical Instrument Detector by Using Evolutionary Learning Method.",
        "author": [
            "Yoshiyuki Kobayashi"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417233",
        "url": "https://doi.org/10.5281/zenodo.1417233",
        "ee": "https://zenodo.org/records/1417233/files/Kobayashi09.pdf",
        "abstract": "This paper presents a novel way of generating information extractors that obtain high-level information from recorded music such as the presence of a certain musical instrument. Our information extractor is comprised of a feature set and a discrimination or regression formula. We introduce a scheme to generate the entire information extractor given only a large amount of labeled dataset. For example, data could be waveform, and label could be the presence of musical instruments in them. We propose a very flexible description of features that allows various kinds of data other than waveform. Our proposal also includes a modified evolutionary learning method to optimize the feature set. We applied our scheme to automatically generate musical instrument detectors for mixed-down music in stereo. The experiment showed that our scheme could find a suitable set of features for the objective and could generate good detectors.",
        "zenodo_id": 1417233,
        "dblp_key": "conf/ismir/Kobayashi09",
        "keywords": [
            "information extractors",
            "high-level information",
            "recorded music",
            "musical instrument",
            "feature set",
            "regression formula",
            "large amount of labeled dataset",
            "waveform",
            "label",
            "stereo"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nAUTOMATIC GENERATION OF MUSICAL INSTRUMENT DETECTOR  BY \nUSING EVOLUTIONARY LEARNING METHOD \n Yoshiyuki Kobayashi  \n SONY Corporation, Japan \nYoshiyuki.Kobayashi@jp.sony.com  \nABSTRACT \nThis paper presents a novel way of generating \ninformation extractors that obtain high1level infor mation \nfrom recorded music such as the presence of a certa in \nmusical instrument. Our information extractor is \ncomprised of a feature set and a discrimination or \nregression formula. We introduce a scheme to genera te \nthe entire information extractor given only a large  \namount of labeled dataset. For example, data could be \nwaveform, and label could be the presence of musica l \ninstruments in them. We propose a very flexible \ndescription of features that allows various kinds o f data \nother than waveform. Our proposal also includes a \nmodified evolutionary learning method to optimize t he \nfeature set. We applied our scheme to automatically  \ngenerate musical instrument detectors for mixed1dow n \nmusic in stereo. The experiment showed that our sch eme \ncould find a suitable set of features for the objec tive and \ncould generate good detectors. \n1.  INTRODUCTION \nMusical information extraction technology has been \nextensively studied for various kinds of applicatio ns. \nGenerally speaking, it extracts some features from input \ndata, and then applies discriminant or regression a nalysis \nto estimate an objective variable from the features . There \nare some popular feature sets like MFCC (Mel1freque ncy \ncepstrum coefficient) [1] and features defined in M peg17 \nstandard [2], along with many other proposed featur es \ndesigned by heuristics. Popular discriminant analys es, \nwhich estimate objective variable from given featur e set, \ninclude SVM, AdaBoost, GMM, HMM and so on. For \nexample, Soo1Chang Pei et al. introduced \ninstrumentation analysis and identification method with \nMFCC, Mpeg17 features, and SVM [3]. T.Kitahara et a l. \nintroduced instrument identification method which c an \nestimate the note1by1note presence probability of m usical \ninstruments by using linear discriminant analysis a nd some features other than MFCC or Mpeg17 [4]. In the se \nstudies, feature sets are designed by human.  \nMeanwhile, there are some studies on Feature \nGeneration [5]. Typically, a feature is obtained wi th a \nfeature extractor composed of some basic functions.  \nGenetic programming (GP) is used to design a featur e \nthat gives optimum objective variable. However, onl y a \nsingle feature could be designed, rather than an ef fective \nset of features for multivariate analysis. As a res ult the \ngenerated extractor is not accurate enough compared  to \npopular methods with discriminant and multi1\ndimensional feature set designed by human. Also the  \ndescription of feature is specialized to waveforms.  As \nsuch, we could not apply this method to other kinds  of \ndata such as log1frequency spectrum. \nIt would appear that we can realize more accurate \ninformation extractor if we could automatically gen erate \na set of effective features specialized for the obj ective. \nThe work presented here is an approach to automatic ally \ngenerate an information extractor from dataset. The  \nresulting extractor includes a set of effective fea tures to \nestimate the objective variable. It also supports v arious \ntypes of data as input. First, we introduce the str ucture of \nthe information extractor that our proposal generat es. \nNext, the modified evolutionary learning method to \noptimize the feature set is presented. And finally as an \napplication of this approach, we introduce our \nexperiment of designing musical instrument detector s. \n2.  STRUCTURE OF INFORMATION \nEXTRACTOR \nFigure 1 shows the structure of information extract or.  \nFigure 1.  Structure of information extractor. X represents \ninput data itself such as waveform. FEF represents a \nfeature extraction function which extracts a single  feature Input data  \nX Feature set  Information extractor  \nx1 = FEF 1(X) \nx2 = FEF 2(X) \nx3 = FEF 3(X) \nx4 = FEF 4(X) \nx5 = FEF 5(X) \n… \nxm = FEF m(X) Discriminant or \nregression formula  \ny = f( x) Objective  \nvariable \n y \n \nPermission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page. \n© 2009 International Society for Music Information Retrieval  \n93Poster Session 1\n  \n \nfrom the input data. x j represents the feature extracted by \nFEF j, and x represents the feature vector consisting of x j. f \nrepresents discriminant or regression formula which  \nestimates the objective variable y based on the fea ture \nvector x.  \nFirst, the information extractor calculates multipl e \nfeatures from input data in accordance with the fea ture \nextraction functions (FEFs). The discriminant or \nregression formula estimates the objective variable  from \nthe extracted features. This structure itself is th e same as \nthe traditional information extractors. The differe nce is \nthat our approach optimizes the entire information \nextractor, i.e. not only the discrimination or regr ession \nformula, but also the feature set. \n2.1  Structure of input data \nIn our scheme, input data is expressed as a multi1\ndimensional matrix. For example, we can express ste reo \nwaveform as a two1dimensional matrix with channel a nd \ntime dimensions (Figure 2). In this example, each \nelement in two1dimensional matrix contains amplitud e of \nthe waveform in the channel at the time. \nFigure 2.  Example of input data of waveform.  \nAlso we can express an image in RGB representation \nas a three1dimensional matrix with color, X, and Y axes \n(Figure 3). In this example, each element in three1\ndimensional matrix contains the brightness in RGB c olor \nspace at the coordinate.  \nFigure 3.  Example of input data of RGB image.  \nTo express video data in this fashion, we would use  \nfour1dimensional matrix obtained just by adding one  \nmore dimension for time to the matrix for image.  W ith \nthis matrix based representation, we can flexibly h andle \nvarious kinds of data as input data. \n2.2  Description method for FEF \nTo support wide variety of input data and features,  we \npropose a very flexible description of FEF. In our \napproach, FEF is formed as a cascade of basic funct ions \n(BFs) like a short computer program to reduce the i nput \ndata matrix to a scalar. We prepared 51 BFs listed in \nTable 1.   \n \nTable1.  List of basic functions. \nThe list includes four arithmetic operations, expon ent \nfunctions, trigonometric functions, normalization \nalgorithms, statistical functions, digital filters,  etc.  \nFigure 4 shows an example of FEF. And Figure 5 show s \nthe calculation of the example FEF.  \n \nFigure 4.  Example of FEF.  \n \n \nFigure 5.  Calculation of the example FEF. \nFirst, FEF represents the input spectrum as two1\ndimensional matrix with time and frequency axes, th en it \ncalculates differential along time axis, finds maxi mal \nvalue and gets the position of maximal value along X RGB image  Three1dimensional matrix \nY \nColor \n(RGB) \n   …         …    \n   …         …    \n   …         …    X \nY Color \n(RGB)  3) Find maximum value and get index of maximum in e ach \nseries along frequency axis.  \nFrequency  \nTime     …         …    \n…    \nTime  \nSet index of maximum \nto each cell  \n4) Apply  lo 1pass filter to time series  \nTime  …    \n5) Calculate standard deviation of time series \nTime  …    x \nFeature     …         …    1) Represent input spectrum as matrix.  \nFrequency  \nTime  \n2) Calculate differential in \neach series along time axis.    …         …    Frequency  \nTime  \nFrequency  \nTime  Spectrum, T#Differential,F#MaxIndex, T#LPF_1;0.861,T#StdDev  Type of input data \nBFs Axis parameter  Parameter of each function  \nExecute left to right Normalize \nNormalizeAvg \nNormalizeEach \nNormalizeEachAvg \nStandardizeEach \nAbs \nSign \nAdd \nMultiply \nInverseSign \nSin \nCos \nTan \nASin \nACos \nATan \nDifferential DiagonalDifferential \nIntegrate  \nLPF_1 \nHPF_1 \nDCCut \nOrder \nWindow_Hanning \nWindow_Gauss \nMovingAverage \nExtract1 \nCut \nLogAxis \nLogAxisOctH \nMean \nRMS \nStdDev \nMMean \n MStdDev \nMaxIndex \nMax \nMin \nMaximumNum \nMinimumNum \nZCR \nTCR \nZCP \nTCP \nDifference \nXDifference \nHistogram_1D \nHistogram01 \nHistogram_2D \nHistogram2D01 \nDownSampling_To  \nTime  Channel L \nR Waveform  Two1dimensional matrix  \n      …      Channel L \nR \nTime  \n9410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nfrequency axis, applies lo1pass filter along time a xis, and \ncalculates standard deviation along time axis. With  this \nformula, it extracts a single feature from input da ta of \ntwo1dimensional matrix. F and T before # represent \nfrequency and time axes, and these are the axis \nparameters representing the axis along which the gi ven \nmatrix is processed. As Figure 4 shows, it executes  \nseveral processes to the matrix of input data by fo llowing \nthe FEF from left to right. The number of dimension s of \nthe matrix was reduced in the course of processing,  and \neventually, a single value is extracted from input data. \nSome BFs have parameters. There are two kinds of \nparameter, one is axis parameter that represents wh ich \naxis to process, and the other is the specific para meter for \neach BF such as the coefficient of lo1pass filter.  \n2.3  Discriminant or regression formula \nWe use linear discriminant or regression analysis w ith \nfeature selection to estimate the objective variabl e from \nthe feature set as below. \ny = f( x) = Σj b jxj + b 0  (1) \nbj represents linear combination coefficients, and b 0 \nrepresents intercept coefficient. We use linear pro cedure \nhere because we can easily calculate contribution r atio \nwhich we later use to optimize the information extr actor \nas a whole. Also it would appear that we can obtain  a \nmeasure of accuracy without non1linear procedure \nbecause FEF can express various non1linear conversi ons. \n3.  MODIFIED EVOLUTIONARY LEARNING \nMETHOD \nInformation extractor is optimized over training da taset \nwhich is a list of input data with label informatio n. Table \n2 shows an example of dataset. The label can be 0 o r 1 \nfor two1class discriminant analysis, or a numeric v alue \nfor regression analysis. \nTable 2.  Example of dataset to generate a vocal presence \ndetector which accepts a segment of waveform and \nestimates the presence of vocal in the waveform. 0 \nsignifies no vocal present in the waveform, and 1 s ignifies \nvocal present.  \nAs previously described, each FEF in the informatio n \nextractor has immense flexibility, so we used \nevolutionary learning method to search for a good f eature \nset from the infinite set of possibilities. One gen eration of \nour evolutionary learning method executes the follo wing \nsteps. \n1. Feature set generation 2. Feature extraction \n3. Linear discriminant or regression analysis with feature \nselection \n4. Calculation of contribution ratio of each featur e \nThese steps are repeated until the learning is stop ped \nby a user. \n3.1  Feature set generation \nIn the first generation, the method synthesizes the  feature \nset which is a list of fixed number of FEFs by comb ining \nBFs randomly. To generate the FEF, first, it choose s a \nBF randomly from the prepared BFs. If the chosen BF  \nhas parameters, they are set also randomly. Then th is \nprocess is repeated to append more BFs until the ma trix \nof input data is reduced to a single value by the F EF. \nIn the second and later generations, the method \ngenerates a new feature set based on the feature se t from \nthe previous generation by evolutionary learning pr ocess. \nIt uses the contribution ratio of each feature calc ulated in \nthe fourth step of the previous generation as the \nevaluation of that feature. Figure 6 shows the sche matic \nof feature set generation in the second and later \ngenerations. First, it selects features in the orde r of \ncontribution ratio and adds them to the feature set  of next \ngeneration unmodified until cumulative contribution  ratio \nbecomes 99%. Next, it generates some features by \nrandomly selecting from highly contributing feature s and \nmutating them by inserting, deleting BFs or modifyi ng \nparameters. Finally, it generates remaining feature s \nrandomly as done in the first generation. Figure 7 shows \nan example of the mutation of FEF. \n \nFigure 6.  Example of feature set generation. τ represents \ngeneration in evolutionary learning process. Featur e set \nin next generation contains highly contributing fea tures \nin the previous generation, features generated by \nmutating the highly contributing features in the pr evious \ngeneration, and those randomly generated. All featu res in \nthe first generation are generated randomly.  \n FEF (τ11)\n1   0.11 \nFEF (τ11)\n2   0.33 \nFEF (τ11)\n3   0.15 \nFEF (τ11)\n4   0.26 \nFEF (τ11)\n5   0.07 \n \n \n… \n \n \nFEF (τ11)\nm   0.01 \n FEF (τ)\n1 = FEF (τ11) \n2 \nFEF (τ)\n2 = FEF (τ11) \n4 \nFEF (τ)\n3 = FEF (τ11) \n3 \nFEF (τ)\n4 = FEF (τ11) \n1 \nFEF (τ)\n5 = FEF (τ11) \n5 \n… \nFEF (τ)\n10  = Mutation(FEF (τ11) \n3) \nFEF (τ)\n11  = Mutation(FEF (τ11) \n1) \nFEF (τ)\n12  = Mutation(FEF (τ11) \n4) \n… \nFEF (τ)\n30  = Random() \nFEF (τ)\n31 = Random() \nFEF (τ)\n32  = Random() \n… \nFEF (τ)\nm = Random() Feature set in \nprevious generation  Feature set in \nnext generation  \nContribution ratio (v j) Highly \ncontributing \nfeatures \nGenerated  \nby  \nmutating  \nGenerated  \nrandomly  1 0 1 0 0 Vocal presence … 5.wav  4.wav  3.wav  2.wav  1.wav  Input data \n95Poster Session 1\n  \n \n \nFigure 7.  Example of mutation of feature. A feature is \nmutated by inserting, deleting BFs or modifying \nparameters randomly.  \n3.2  Feature extraction \nIn this step, FEF j extracts feature x (i) \nj from input data \nwith index i. At this point, we have dataset with i ts \nfeatures. \n3.3  Linear discriminant or regression analysis with \nfeature selection \nIn this step, the method estimates parameters of \ndiscriminant or regression formula ( b) in equation 1 with \nthe dataset and the features calculated in step 2. Because \nsome features are generated randomly, there are man y \nmeaningless or redundant ones in the generated feat ure \nset, particularly in the first generation. Feature selection \nis very important in keeping only the effective fea tures to \nrealize maximum generalization accuracy. It is also  \nimportant for the calculation of fair contribution ratio of \nfeatures from discriminant or regression formula. F or the \nfeature selection, we used local1search to search f or a \ngood combination of features from information crite ria \nperspective. More precisely, first, it prepares par ameter \nuj = {1, 0} which indicates whether the j1th feature is \nselected or not, and sets all bits to 0 at the begi nning. \nThen, it tries inverting a single bit among u j’s one by one \nstarting from the first one, estimates parameters b with \nthe currently selected features by using least squa res \nmethod, and calculates AIC [6] by comparing the \nestimated objective variable and the label in the d ataset. \nAIC = n * log(PMSE) + 2 * (k+1)  (2) \nn represents the number of the input data in the da taset, \nPMSE represents the prediction mean square error, a nd k \nrepresents the number of the features selected in u. \nAmong the possible m bit inversion positions, the o ne at \nwhich the AIC improved the most is selected and \nexecuted, and the local1search is continued. In cas e of no \nimprovement, it finishes the local1search with the \nselected features and the computed b as the optimum \nwith respect to AIC.  \n3.4  Calculation of contribution ratio of each feature \nContribution ratio of each feature is calculated by  the \nfollowing formula.  \nvj = b j / StDev( xj) * StDev( t) * Correl( xj, t) (3) \nvj represents the contribution ratio of the feature w ith \nindex j. t represents objective variable which is the label in the dataset. StDev( xj) represents the standard deviation \nof the feature with index j in the dataset. StDev( t) \nrepresents the standard deviation of the objective \nvariable in dataset. And Correl( xj, t) represents the \ncoefficient of correlation between xj and t. If x j is not \nselected in step 3, v j becomes zero. If there are multiple \nobjectives, we can just use mean contribution ratio  from \neach formula for each objective. With step 1, highl y \ncontributing features will survive and prosper, and  poorly \ncontributing features will die. With iteration of s teps 1 \nthrough 4, the feature set will improve with respec t to the \nobjective compared to the previous generation. Whil e \ntraditional GP methods can optimize only a single f eature, \nour approach can optimize multiple features \nsimultaneously to achieve better generalization acc uracy. \nMoreover because we use contribution ratio to selec t \nfeatures, we maintain the variety of features in th e later \ngenerations, which alleviates the local optimum pro blem. \n4.  APPLICATION TO MUSICAL INSTRUMENT \nDETECTION \nWe used our scheme to automatically generate musica l \ninstrument detectors for mixed sound. \n4.1  Dataset \nWe prepared about 100 commercially available music \nfiles which are sampled at 44.1 kHz in stereo. They  cover \nvariety of genres such as pops, rock, jazz, world, and so \non, and various kinds of musical instruments appear  in \nthese music files. We labeled each 11second interva l \naccording to the presence of 10 kinds of musical \ninstruments which are vocal, harmonize, piano, clea n \nguitar, distortion guitar, distortion guitar solo, strings, \nbrass, bass and drums with true (1), false (0) or u nclear \n(no label). If there is audible sound of the instru ment in \nan interval, we labeled it 1, otherwise 0, and if w e feel it \nis very difficult to determine the presence of the musical \ninstrument from only 11second of waveform even for \nhuman ear, we put no label. We decided that it was not \nnecessary to label the whole music file because the re are \nrepetitions in music, so there are about 40% of unl abeled \nsections. Finally, we got 21,272 segments of 11seco nd \nwaveform in total. Table 3 shows the number of corr ectly \nlabeled segments for each musical instrument. \nTable 3.  Number of segments of waveform with correct \nlabel information.  4414 \n 5078 16208 14836 10185 FALSE 5062 5675 354 1706 2684 TRUE Drums Bass D.G. solo D. guitar C. guitar        10643 12083 8748 12455 7884 FALSE 1810 946 3184 1655 3505 TRUE Strings Brass Piano Harmonize Vocal  Wav,Sqr,T#LPF_1;0.3,T#IndexLR \n0 Example 2. Insertion of BF \nWav,T#LPF_1;0.7,T#IndexLR \n0 Example 3. Change of parameter Example 1. Deletion of BF \n Wav,T#LPF_1;0.3,T#IndexLR \n0 \nWav,T#IndexLR Feature to mutate \n9610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nSegments contain 3.2/10 musical instruments on \naverage and 7.5/10 musical instruments at maximum i f \nwe treat non1labeled instrument as 0.5. And we shuf fled \nthese segments without keeping reference to the son gs \nfrom which they were taken. We used the half for \ntraining, and the other half for testing.  \n4.2  Input data \nOur scheme can handle waveform directly. However, w e \nfound that we can achieve better accuracy by applyi ng \nsuitable pre1processing that emphasizes the \ncharacteristics of the input data for the objective . So, we \nconverted the waveforms into three kinds of input d ata \nwhose names are \"12TonesM\", \"12TonesF\" and \n\"12TonesB\". Each data is two1dimensional matrix wit h \ndimensions of time and musical pitch. The differenc e \namong these three data will be shown later. Origina l \nwaveform is converted to these matrices with the \nfollowing steps. \n4.2.1  Simplified sound source separation  \nWe applied simplified form of the sound source \nseparation algorithm described in [7] to obtain \nforeground and background sounds from the original \nstereo sound. Figure 8 shows the signal flow diagra m of \nthis sound source separation. \nFigure 8.  Signal flow diagram of the simplified sound \nsource separation. FL, BL, FR and BR represent \nforeground1left, background1left, foreground1right and \nbackground1right, respectively.  \nEach channel is analyzed with short1time Fourier \ntransform with rectangle window of 16k samples and \noverlap of 8k samples. This very long frame size is  \nneeded to maintain the quality of separated sound. Then \nthe phase difference between stereo channels in eac h \nfrequency is calculated. If there is a difference g reater \nthan 0.2 PI, the frequency component is labeled as \nbackground. Otherwise, it is labeled as foreground.  Then, \nfor each channel, two waveforms for foreground and \nbackground are synthesized with inverse short1time \nFourier transform with triangle window. This result s in \nfour channels of waveforms. Then, the left and righ t \nforeground channels are mixed, and the same is done  for the background channels. As a result, two waveforms  of \nforeground and background sounds are obtained. With  \nthis sound source separation, monaurally recorded \nsounds such as vocal, bass, snare and kick drums wi ll \nappear in the foreground channel. On the other hand , \nsound recorded in stereo like strings or brass sect ion will \nappear in the background channel.  \n4.2.2  Wavelet transform \nWe applied wavelet transformation to convert single  \nwaveform into two1dimensional matrix with time and \nmusical pitch dimensions. We used band1pass filter \nwhich passes only a single semi1tone, as the mother  \nwavelet. The original waveform was decomposed into \n108 sub1bands corresponding to 12 semi1tones over 9  \noctaves. Then the logarithm of energy in each 7.8ms  in \neach semi1tone is calculated. Figure 9 and Figure 1 0 \nshow the schematic diagram and an example result of  this \nprocess. \n \nFigure 9.  Schematic diagram of wavelet transform. It \nseparates original waveform into 108 sub1bands, and  \ncalculates energy in 7.8ms in each band. \n \n \nFigure 10.  Example of result of wavelet transform. \nBrightness represents energy in each time and each \nmusical1pitch. \nWe used the result of this process from foreground \nsound as \"12TonesF\", result from background sound a s \n\"12TonesB\", and average of foreground and backgroun d \nas \"12TonesM\". \n4.3  Result of learning \nWith our scheme and dataset, we generated musical \ninstrument detection algorithms for mixed sound. \nNumber of features is 1,000, and 165 generations we re \nused in our evolutionary learning method. Figure 11  \nshows the learning curve. For comparison, it also s hows \nthe result for extractors with single feature.  The y are \noptimized with GP by selecting 3% of features most \ncorrelated with the label information in each gener ation.  Compare phase  \ndifference  between  2 channels  Signal of left \nchannel \n (FL + BL ) \nSignal of right \nchannel \n(FR + BR)  STFT  \nSTFT  Frequency domain  \n… \n… Classify to foreground \nand background … \n… \n… \n… FL  \nBL  \nFR  \nBR  Foreground  \n(FL + FR)  \nBackground  \n(BL + BR)  + \n+ iSTFT  \niSTFT  \niSTFT  \niSTFT  Original \nwaveform Filter of each \nsub1band Sub1band \nsignal Energy of \neach sub1band  \nTime  Time  \n ... Convolution   ... B9  \nA#9  \nA9  \nD1  \nC#1  \nC1  \n ... \nTime  \nTime  \nMusical \npitch \n97Poster Session 1\n  \n \n \nFigure 11.  Learning curve. Dashed line represents the F1\nmeasure on training dataset averaged over all music al \ninstrument detectors, and solid line represents the  F1\nmeasure on testing dataset. Dotted line represents the F1\nmeasure of the detector with single feature optimiz ed with \nGP on testing dataset. \nAs the learning curve shows, in the first generatio n, \nour detector realized average F1measures of 0.75 on  \ntesting dataset with features selected from 1,000 \nrandomly generated features of various sorts. In th e final \ngeneration, it realized 0.88 with the feature set o ptimized \nwith our scheme. There is very clear advantage over  the \nresult of extractor with single feature optimized w ith GP. \nAnd Table 4 shows the F1measures for each musical \ninstrument in the final generation on testing set. \n \nTable 4.  F1measures of each musical instrument detector \nin the final generation on testing set. \nTable 5.  Part of highly1contributing features found in \nfinal generation. \nFinally, table 5 shows some examples of generated \nFEF. The first feature in table 5 takes log1frequen cy \nspectrum of foreground as input, calculates differe ntial in \neach series along time axis, calculates standard de viation \nin each series along time axis, processes Hanning \nwindow to frequency series and calculates average f rom \nfrequency series. “Difference” function in table 5 splits \nthe input in two at the boundary specified by the \nparameter, computes the sums for the two parts, and  \noutputs the difference of the sums. It is not easy to \nunderstand what is going on in these generated feat ures \nexplicitly. However, it looks like it found variety  of \nfeatures, not only ones like MFCC and Mpeg17 but al so \nunique features with alien concept.  5.  CONCLUSION \nWe presented a novel method to automatically design  a \ninformation extractors. We introduced a very flexib le \ndescription of features which supports various kind s of \ndata types, and a modified evolutionary learning me thod \nto optimize multiple features given a partially lab eled \ndataset. The method generated complete musical \ninstrument detectors for mixed sound with various \nundiscovered and specialized features. The detector s \nrealized either equal or superior performance compa red \nto other methods even though the feature set is des igned \nautomatically given only the dataset without human \nintervention. Now we are applying the method to bui ld \nvarious kinds of detection or recognition algorithm s such \nas beat detection, attribute estimation, melody lin e \nestimation and more, not just for music recognition  but \nfor image recognition. We would like to report thes e \nresults in the future. \nACKNOWLEDGEMENTS \nThe author would like to thank Francois Pachet, Pie rre \nRoy, Gabriele Barbieri, Hideki Shimomura, Tatsuki \nKashitani and Kei Tateno. \n6.  REFERENCES \n[1] Beth Logan. Mel Frequency Cepstral Coefficients  \nfor music modelling. In International Symposium on \nMusic Information Retrieval, 2000. \n[2] H.G. Kim, N. Moreau and T. Sikora. MPEG17 \nAudio and Beyond. Audio Content Indexing and \nRetrieval. John Wiley & Sons Ltd. 2005. \n[3] Soo1Chang Pei, Nien1Teh Hsu. Instrumentation \nanalysis and identification of polyphonic music \nusing beat1synchronous feature integration and \nfuzzy clustering. Pages 169 – 172, ICASSP 2009.  \n[4] T.Kitahara, et al. Instrument Identification in  \nPolyphonic Music: Feature Weighting to Minimize \nInfluence of Sound Overlaps. EURASIP Journal on \nAdvances in Signal Processing, Volume 2007. \n[5] Pachet, F. and Roy, P. Analytical Features: A \nKnowledge1Based Approach to Audio Feature \nGeneration. Eurasip Journal on Audio Speech and \nMusic Processing, February 2009. \n[6] H. Akaike. Information theory and an extension of \nthe maximum likelihood principle. 2nd International  \nSymposium on Information Theory, pages 2671281, \n1973. \n[7] Dae1young Jang, et al. Center channel separatio n \nbased on spatial analysis. 11th International \nConference on Digital Audio Effects, 2008. Generation  0.5  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  \n1 15  29  43  57  71  85  99  113  127  141  155  \n12TonesF,T#Differential,T#StdDev,F#Window_Hanning,F #Mean \n12TonesF,Add;0.223798,T#Differential,F#Window_Gauss ;0.210 \n475;0.532910,T#StdDev,F#Mean \n12TonesB,T#StdDev,ACos,F#Difference;0.633319 \n12TonesF,T#MaximumNum,F#Difference;0.689421 \n12TonesB,T#StdDev,F#Difference;10.623785 \n12TonesB,T#Mean,F#Difference; 10.703600  0.882  Avg.   0.987  Bass   0.897  C. Guitar   0.92  Piano       0.762  D.G.Sol  0.794  Strings   0.852  Harmonize  0.991  Drums   0.956  D. Guitar   0.751  Brass   0.912  Vocal  \n98"
    },
    {
        "title": "Song Ranking based on Piracy in Peer-to-Peer Networks.",
        "author": [
            "Noam Koenigstein",
            "Yuval Shavitt"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417038",
        "url": "https://doi.org/10.5281/zenodo.1417038",
        "ee": "https://zenodo.org/records/1417038/files/KoenigsteinS09.pdf",
        "abstract": "Music sales are loosing their role as a means for music dissemination but are still used by the music industry for ranking artist success, e.g., in the Billboard Magazine chart. Thus, it was suggested recently to use social networks as an alternative ranking system; a suggestion which is problematic due to the ease of manipulating the list and the difficulty of implementation. In this work we suggest to use logs of queries from peer-to-peer file-sharing systems for ranking song success. We show that the trend and fluctuations of the popularity of a song in the Billboard list have strong correlation (0.89) to the ones in a list built from the P2P network, and that the P2P list has a week advantage over the Billboard list. Namely, music sales are strongly correlated with music piracy.",
        "zenodo_id": 1417038,
        "dblp_key": "conf/ismir/KoenigsteinS09",
        "keywords": [
            "Music sales",
            "dissemination",
            "artist success",
            "social networks",
            "alternative ranking",
            "query logs",
            "peer-to-peer file-sharing",
            "song popularity",
            "Billboard list",
            "P2P network"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSONGRANKINGBASEDONPIRACYINPEER-TO-PEERNETWORKS\nNoamKoenigstein YuvalShavitt\nSchoolofElectricalEngineering\nTelAvivUniversity,Israel\n{noamk,shavitt }@eng.tau.ac.il\nABSTRACT\nMusicsalesareloosingtheirroleasameansformusicdis-\nseminationbutarestillusedbythemusicindustryforrank-\ningartistsuccess,e.g.,intheBillboardMagazinechart.\nThus,itwassuggestedrecentlytousesocialnetworksas\nanalternativerankingsystem;asuggestionwhichisprob-\nlematicduetotheeaseofmanipulatingthelistandthedif-\nﬁcultyofimplementation.Inthisworkwesuggesttouse\nlogsofqueriesfrompeer-to-peerﬁle-sharingsystemsfor\nrankingsongsuccess.Weshowthatthetrendandﬂuctua-\ntionsofthepopularityofasongintheBillboardlisthave\nstrongcorrelation(0.89)totheonesinalistbuiltfromthe\nP2Pnetwork,andthattheP2Plisthasaweekadvantage\novertheBillboardlist. Namely,musicsalesarestrongly\ncorrelatedwithmusicpiracy.\n1. INTRODUCTION\nPeer-to-peer(P2P)networksareoneoftheinternet’smost\npopularapplications. Thenumberofusersandtrafﬁc,is\ngrowingdramaticallyfromyeartoyear.Despiteseveralre-\ncenthighproﬁlelegalcasesagainstP2Pvendorsandusers,\nitseemsthattheP2Pcommunityatlargeremainsstrong\nandhealthy. Infact,P2Pnetworksgainmoreacceptance\nasmanycompaniesandorganizationsdistributesoftware\nandupdatesvianetworkssuchasBitTorrenttosaveband-\nwidth(e.g.,Ubuntu).\nSomestudiessuggestthatmusicpiracymightincrease\nlegalsales[1,2],andcopyrightownersareadvisedtostart\ndevelopingbusinessmodelsthatwillallowthemtogener-\naterevenuefromP2Pactivity. Pioneeringsuggestionsto\nutilizeP2Pnetworksforthebeneﬁtofthemusicindustry\nweremadebyBhattacharjee etal.[3,4],whereP2Pactivity\nwasusedtopredictanalbum’slifecycleontheBillboard’s\ntop200albumschart.\nInourpreviouswork[5]weshowedhowP2Pqueries\ncanbeusedforearlydetectingunknownemergingartists.\nInthisstudywetakeadifferentapproach;wesuggestan\nalternativesongsrankingbasedonﬁlesharingactivity,th at\nmightreplacetraditionalartistsrankingsuchastheBill-\nPermissiontomakedigitalorhardcopiesofallorpartofthisw orkfor\npersonalorclassroomuseisgrantedwithoutfeeprovidedth atcopiesare\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbearthisnoticeandthefullcitationontheﬁrstpage.\nc/circlecopyrt2009InternationalSocietyforMusicInformationRetrieval .board.Wemeasuredmusicpiracyusingadatasetofgeo-\ngraphicallyidentiﬁedP2Pquerystring,andcompareditto\nsongsrankingontheBillboradHot100,whichmeasures\nsalesandairplays. Wecompiledpopularitychartsbased\nonP2Pactivity,andshowastrongcorrelationbetweenmu-\nsicpiracyandlegalsalesandairplays.Wearguethatrank-\ningsongsthroughmeasurementofP2Pqueriesisagood\npredictorofpeoples’taste,andhasmanyadvantagesover\nothermeansofpopularityranking,whichweresuggested\ninthepast,mostnotablyusingsocialnetworks[6].\nTheremainderofthepaperisorganizedasfollows:In\nSection2weintroducethedatasetusedinthisstudy,and\nthemethodologyusedtocollectit.InSection3wefocus\noncomparingsongpopularityinP2Pnetworkswiththeir\nrankingontheBillboard. Wediscussthesigniﬁcanceof\nourﬁndingandourconclusionsinSection4.\n2. DATA-SETSANDMETHODOLOGY\nWeusetwodatasourcesforthisstudy:\n•P2PSearchQueries :Adata-setofqueriescollected\nfromtheGnutellaﬁle-sharingnetworkovertwenty\nthreeweeksfromJanuarythe7th2007toJune8th\n2007.\n•TheBillboardHot100 TheBillboadHot100weekly\nchartsfor2007aspublishedbytheBillboardMaga-\nzine.\nThesetwodata-setswerecollectedindependently,yet\nthisstudyrevealsastrongrelationshipbetweenthem.How-\never,beforeanalyzingthecommonalitiesanddifferences,\nletusﬁrstdescribethedatasetsandthemethodologyused\ntocollectthem.\n2.1 P2PSearchQueries\nQueriesinaﬁlesharingnetworkrepresenttheiruserscur-\nrenttasteandinterests. Aqueryisissueduponarequest\nbyausersearchingforaspeciﬁcﬁle,orcontentrelevantto\nthesearchstring.Inthisstudyweuseddatacollectedfrom\ntheGnutellanetworkusingtheSkyridersystems1. This\ndata-setandthetechnicaldetailsofthemethodologyused\ntocollectitaredescribedinmoredepthin[7].\n1Skyriderwasastartupcompanythatdevelopedﬁlesharingapp li-\ncationsandservices.Ithasrecentlybeencloseddown.Thed ata-setwas\ncollectedwhenthecompanywasstillactive,andisavailable foracademic\nresearch.\n633Poster Session 4\n2.1.1 TheGnutellaFileSharingNetwork\nInastudyperformedbySlyck.com,awebsitewhichtracks\nthenumberofusersofdifferentP2Papplications,Gnutella\nwasfoundamongthethreemostpopularP2Pﬁle-sharing\napplicationstogetherwitheDonkeyandFastTrack[8].Fur-\nthermore,accordingto[9],Gnutellaisthemostpopular\nﬁlesharingnetworkintheInternettodaywithamarket\nshareofmorethan40%.Itismainlyusedforpiracyofmu-\nsic.In[5]thetop500mostpopularqueriesweremanually\nclassiﬁed,anditwasfoundthat68%ofthequerieswere\nmusicrelated. Togetherwithadultcontent(22%),these\ntwocategoriesdominatethequerytrafﬁc,accountingto-\ngetherfor90%ofthequeries.Gnutellaisalsoamongthe\nmoststudiedP2Pnetworksintheliterature[5,7,10–16].\n2.1.2 Methodology\nAquery’soriginIPaddressisrequiredforitsgeographical\nclassiﬁcationaccordingtoitscountryoforigin.Whileitis\npossibletocapturealargequantityofGnutellaqueriesby\ndeployingseveralhundredultrapeernodes2,itwillnotbe\npossibletotelltheoriginIPaddressofmostofthesecap-\nturedqueries.Thebasicprobleminidentifyingtheorigin\nofcapturedqueriesisthatqueriesdonotingeneralcarry\ninformationregardingtheirorigin. Whattheydousually\ncarryisan“OutOfBand”(OOB)returnIPaddress.This\naddressallowsclientsthathavecontentmatchingaquery\ntorespondtoalocationclosetotheoriginofthequery,\nwithouthavingtobacktrackthepathtakenbythequery\nmessage.However,asmostqueriescomefromﬁrewalled\nclients,inmostcasestheOOBaddresswillbelongtothe\nultrapeerconnectedtothequeryorigin,actingasaproxy\nonbehalfofthequeryoriginator. Deductingthemissing\noriginIPaddressisnottrivial. Weresolvedthisproblem\nbyusingahopcountingtechniquethatisfurtherexplained\nin[7].\nThevastmajorityoftheGnutellanetworkiscomprised\nofLimewireclients(80%-85%)andBearshareclients(6%-\n10%)[10]. TheLimewireclientdoesnotallowusersto\nperformanykindofautomaticorroboticqueries.Itdoes\nnotallowquerieswiththeSHA1extension3,nordoesit\nallowtheautomaticre-sendingofqueries. Whenitdoes\nsendduplicatequeries,itusesaconstantMessageIDwhich\nenablesasimpleremovalofanyduplication.Byrecording\nonlyqueriesoriginatingfromLimewireclients,wewere\nabletosigniﬁcantlyreducetheamountofduplicationsand\nautomatic(non-human)queries,withoutlosingtoomuch\nofthetrafﬁc.CapturingonlyLimewirequeriesisaneasy\ntaskasLimewire“signs”themessageIDassociatedwith\neachmessageitsends. Thissignaturecanbeeasilyveri-\nﬁedbytheinterceptingnode,allowingittoignorequeries\nfromallotherclients.\n2Ultrapeernodesarespecialnodesthatroutesearchqueries andre-\nsponsesforusersconnectedtothem\n3SHA1queriesarequeriesinwhichonlythehashkeyofaknown\nﬁleissentwithoutastring.Thisisusefulwhenaclientalre adystarted\ndownloadingandneedsmoresources.RankString Occurrences\n1adult 36,290\n2akon 23,468\n3lilwayne 12,518\n4beyonce 11,987\n5thisiswhyi’mhot 10,746\n6justintimberlake 10,193\n7porn 9,144\n8don’tmatter 9,047\n9fergie 8,979\n10falloutboy 8,077\nTable1.P2Ppopularitychartforweek9of2007\n2.2 DataSetStatistics\nAdailylogﬁleofqueries,typicallycontained25-40mil-\nlionrecordlines,eachlineconsistsofthequerystring,a\ndate/timeﬁeld,andtheIPaddressofthenodeissuingthe\nquery. Theorigincountryforeachquerywasresolved\nusingMaxMindcommercialGeoIpdatabase. Similarly\ntotheBillboardcharts,wewantedtoconcentrateondata\noriginatedfromtheUnitedStates.Wethusremovedallthe\nnonUSqueriesreducing55%-60%ofthedatarecords.\nOurdata-setcomprisedofquerystringscollectedover\naperiodof23weeksfromJanuarythe7th2007toJune\n8th2007.TheactivityontheGnutellanetworksincreases\nby20%-25%overtheweekend[7].Wethususedweekly\nsamplestakenonaSaturdayoraSundayofeveryweek\nofthatperiod. ThetotalnumberofUSoriginatedquery\nstringsprocessedinthisstudyis 185,598,176.\n2.3 TheBillboardHot100\nTheBillboardHot100istheUnitedStatesmusicindustry\nstandardsinglespopularitychartissuedweeklybyBill-\nboardmagazine[17]. Chartrankingsarebasedonradio\nplayandsalesdatacollected10daysbeforethechartisre-\nleased.Therankingprocessdoesnottakeintoaccountﬁle\nsharingactivity.Anewchartiscompiledandofﬁciallyre-\nleasedtothepubliceachThursday.Thechartisdatedwith\ntheweeknumberoftheSaturdayafter,butinthisstudywe\nuseddatesandweeknumbersaccordingtotheactualre-\nleasedateofthechart,andignoredthedateissuedbyBill-\nboardmagazine. Tosimplifytimetrackinginthispaper,\nweuseweeknumbersinsteadoffulldatetochronologi-\ncallyordertheBillboardchartsandtheweeklyﬁlesharing\ndatawecollected.Forexample,theBillboardchartwhich\nwasreleasedonThursdayJanuary11th2007(weeknum-\nber2),wasdatedbybillboardtoJanuary20th(week3)but\nbyustoweeknumber2. Thecurrenttop50singlesare\npublishedweeklyonthemagazinewebsite,whilethefull\nhistoricalchartsareavailabletoon-linesubscribersfor a\nsmallfee.AstatisticalmodelofsongsrankingintheHot\n100chartcanbefoundin[18].\n3. CORRELATIONOFTRENDS\nAsdescribedabove,theBillboardHot100chartrankssongs\nrelativetoeachother,anddoesnotrevealthenumberof\n63410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nsalesorair-playsmeasuredduringthatweek. Inorderto\ncompareittoourﬁle-sharingdata,wecompiledourown\nweeklyP2Ppopularitychartsbasedonthepopularityof\nsearchstrings.Wemeasuredthepopularityofeachstring\nbyaggregatingthenumberofappearancesinterceptedfrom\naUSbasedoriginonthatweek.Table1depictsthetop10\npositionsoftheP2Pchartgeneratedonweek9of2007\n(sampledonMarch12007).\nObviously,theP2Pchartsincludemanynonmusicre-\nlatedstrings.Thestring“adult”forexample,wasranked\nnumberoneoneverychartwecompiled.UnliketheBill-\nboardcharts,theP2Pchartsincludedalsoartistsnames\n(notonlysingletitles),andsometimesevendifferentvari -\nationsofthesamestrings.Inordertoavoidinaccuracies,\nwelookedonlyatthepositionofasong’sexacttitleinthe\nchart.TohavehighprobabilitythattheBillboardsongsare\nrankedonourchart,wecompiledtruncatedchartsofthe\ntop2000stringseach.Aweeklylogﬁlecontainedonaver-\nage1.73milliondifferentstrings.Therefore,thetop2000\nisapproximatelyonethousandthoftheentireP2Ppopu-\nlaritychart.ThetopsongsofourP2Pchartwerequeried\nabout300,000timesperweekintheUSA.Thesongsatlo-\ncation2000werequeriedabout4,500times.Thenumber\nofqueriesperrankfollowsZipf’slaw[7],thuschangesin\narankpositionindicatestrongshiftsinpopularity.When\nasongisnolongeronthetop2000,itexitstheP2Pchart.\nThishowever,doesn’tmeanitisnolongerbeingdown-\nloaded. SimilarlywhenasingleexitstheBillboardHot\n100chart,itdoesn’tmeanitisnotbeingplayedonthe\nradioorsoldinstores. Therefore,whenconsideringthe\ncorrelationoftrendsbetweenthetwocharts,oneshould\nfocusontheweekswhereasongisrankedonbothcharts.\n3.1 CorrelationMeasurements\nWedeﬁne BsandPsasthechartvectorsrepresentingthe\nsongsontheBillboardandP2Pchartrespectively.\nBs={bs(1),bs(2),...,b s(23)}(1)\nPs={ps(1),ps(2),...,p s(23)}(2)\nWhere bs(w)andps(w)arethepositionsofsong sonthe\nBillboardandtheP2Pchartonweek wrespectively. If\nsongswasnotinthechart,wesetitspositionto ∞forthat\nweek.Thesupportofachartvectoristhetimerangethat\nthesongwasrankedinthechart.Namelywhere bs(w)<\n∞orps(w)<∞.Thejointsupportofasong sisthetime\nrangeinwhichitsimultaneouslyrankedinbothcharts.\nFig.1depictsthechartvectors BsandPsfor6differ-\nentsongs. Thesolidbluegraphisthesong’srankingon\ntheBillboardHot100,whilethedashedgreengraphisthe\nsong’srankingontheP2Pchart. Thehorizontalaxis(x-\naxis)depictsthedatemeasuredinweeknumbersin2007.\nThesongtitlesandperformingartistsarewrittenabove\neachgraph. Notethatlowerpartsofthegraphrepresent\nhigherpositiononthecharts(i.e.,thetopofthechartis1,\nwhilethelastplaceis100or2000).LookingatFig.1,one\ncaneasilynoticethecorrelationbetweenthesetwotimese-\nries.Thiscorrelationisvividnotonlyinthegeneraltrend\noftheline,butalsoinminortrendsandﬂuctuations.5 10 15 20120406080100Lost Without U, Robin ThickeBillboard\n5 10 15 20500100015002000\nP2P\n5 10 15 20120406080100Read My Mind, The KillersBillboard\n5 10 15 20500100015002000\nP2P\n5 10 15 20120406080100Stand, Rascal FlattsBillboard\n5 10 15 20500100015002000\nP2P\n5 10 15 20120406080100Waiting On The World To Change, John MayerBillboard\n5 10 15 20500100015002000\nP2P\n5 10 15 20120406080100Wasted, Carrie UnderwoodBillboard\n5 10 15 20500100015002000\nP2P5 10 15 20120406080100What Goes Around...Comes Around, Justin TimberlakeBillboard\n5 10 15 20500100015002000\nP2P\nFigure1.P2PPopularityChartvs.TheBillboardHot100\nWeslightlyalteredthestandarddeﬁnitionofcross-correl ation\ntoconsideronlythejointsupportofthetwoseries Bsand\nPs:\ncorr=we/summationdisplay\ni=ws[(bs(i)−E{Bs})·(ps(i)−E{Ps})]\n/radicaltp/radicalvertex/radicalvertex/radicalbtwe/summationdisplay\ni=ws(bs(i)−E{Bs})2/radicaltp/radicalvertex/radicalvertex/radicalbtwe/summationdisplay\ni=ws(ps(i)−E{Ps})2\n(3)\nWhere [ws,ws+1,...,w e]isthejointsupportand E{Bs}\nandE{Ps}arethemeansofthecorrespondingseries.The\ncorrelationcoefﬁcientisintherangeof −1≤corr≤1,\nwheretheboundsindicatingexactmatchuptoascaling\nfactor,while0indicatesnocorrelation.\nInallourmeasurements,werequiredsongstohavea\njointsupportofatleast4weeks. Thisisthemajorityof\nthedate-set(over80%).Songswithajointsupportofless\nthan4weeksaremainlysongsthatrankedbeforeorafter\nourmeasurements,andhadonlyashort“tail”insideour\nmeasurementperiod.Suchsongspoorlyrepresentcorrela-\ntionofpopularitytrendsovertime.\nWemeasuredthecorrelationcoefﬁcientsofthe135songs\nthathadajointsupportofatleast4weekswithintheﬁrst\ntwentythreeweeksof2007.Theaveragejointsupportwas\n10.9weeks.Theaveragecorrelationcoefﬁcientswas0.67\nwhilethemedianwas0.82,indicatingaverystrongcorre-\nlation.\nOnemightarguethatthehighcorrelationcoefﬁcients\naretheresultoftrendsimilaritiesofanytimeseriesof\nsongsoncharts. Wethusmeasuredthecross-correlation\ncoefﬁcientbetweenthesongsinonechart,andarandom\npermutationintheotherchart.Ofthe52songswhichhad\najointsupportofatleast4weeks,theaveragejointsupport\nwas9.72weeks,theaverageofthecorrelationcoefﬁcients\nwas-0.006,andthemedianwas0.023,whichnegatesthe\nabovehypothesis.\n635Poster Session 4\nTitle Artist NoShiftOneWeek\nWhatGoesAround...ComesAround JustinTimberlake 0.7290.9707\nLostWithoutU RobinThicke 0.76640.948\nReadMyMind TheKillers 0.17640.661\nStand RascalFlatts 0.96170.8965\nWaitingOnTheWorldToChange JohnMayer 0.7230.8965\nWasted CarrieUnderwood 0.86110.9456\nTable2.CorrelationcoefﬁcientsofthesongsinFig.1\n0.764 0.8935 \n00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n-8-7-6-5-4-3-2-101234correlation coefficient \nshift [weeks] average \nmedian \nFigure2.Cross-CorrelationCoefﬁcientsvs.TimeShift\nAsmentionedisSection2,theBillboardchartswere\ndatedaccordingtotheirreleasedate. However,thedata\nusedtocompileeachchart,iscollectedduringthe10days\nbeforethechartispublished. Thus,wewereinterested\ninthecorrelationcoefﬁcientbetweentheP2Pchartand\ntheBillboardchartofthefollowingweek.Byshiftingthe\nBillboardchartvectorsbackwards,wemeasuredthecorre-\nlationcoefﬁcientsofthe130songswithajointsupportof\natleast4weeks.Theaveragejointsupportwas10.8weeks.\nTheaveragecorrelationcoefﬁcientwas0.76,whiletheme-\ndianvaluewas0.89.Thesevaluesarehigherthanthepre-\nviousones,whichindicateashorttimeshiftbetweenthe\ntwoseries.Fig.2depictstheaverageandmedianvaluesof\nthecorrelationcoefﬁcients,asafunctionoftheBillboard ’s\ntimeshift. Clearly,minusoneistheoptimaltimeshift.\nWethusconcludethattrendsontheBillboardchartand\nonthetheP2PchartsarehighlycorrelatedwiththeBill-\nboardlaggingbyoneweek.Table2depictsthecorrelation\ncoefﬁcientsoftheexamplesongsinFig.1withoutshifts,\nandwithaoneweektimeshift. Whencarefullyexamin-\ningFig.1,thistimeshiftisnoticedonsomeofthesong\ngraphs. Theimplicationofthisﬁndingisobvious: P2P\npopularitychartscanbeusedinordertopredicttrendson\ntheBillboardchart.Recordcompanies,forexample,might\nuseP2Pﬁlesharingactivitytoimprovetheirmarketingde-\ncisions.\n3.2 RankingDriftAnalysis\nInSection3.1weshowedthatsongstrends(aclimbor\nadescend)inP2Ppopularitychartsarehighlycorrelated\nwithtrendsontheBillboardHot100.Wenowaskwhether\nthechartsaresimilaralsointherelativeranksofsongs.Fo r\neachweekwetookthe100songsfromtheBillboardchart,\nand“re-ranked”themaccordingtotheirrelativeposition\nontheP2Pchart.InaccordancewithSection3.1,weused\natimeshiftofoneweek. WethuscreatedanalternativeRankBillboard AlternativeChart\n1Irreplaceable,Beyonce WalkItOut,Unk\n2IWannaLoveYou,AkonFeat.SnoopDogg You,LloydFeat.LilWayne\n3Fergalicious,Fergie TimMcGraw,TimMcGrawWithFaithHill\n4SmackThat,AkonFeat.Eminem SmackThat,AkonFeat.Eminem\n5SayItRight,NellyFurtado WeFlyHigh,JimJones\n6MyLove,JustinTimberlakeFeat.T.I. RunawayLove,LudacrisFeat.MaryJ.Blige\n7HowToSaveALife ,TheFray SayItRight,NellyFurtado\n8WeFlyHigh,JimJones WalkAway,PaulaDeAndaFeat.TheDEY\n9WelcomeToTheBlackParade ,MakeItRain,FatJoeFeat.LilWayne\nMyChemicalRomance\n10ItEndsTonight,TheAll-AmericanRejects IWannaLoveYou,AkonFeat.SnoopDogg\nTable3. Billboard’sTopTenPublishedonJanuary11th\n2007vs.TheAlternativeChart\nrankingchartfortheBillboardsongsbasedontheirP2P\nactivity.Thisalternativechartisactuallyaﬁlteredvers ion\nofP2PchartfromSection3.1thatcontainsonlythesongs\nfromtheBillboardchart.\nInTable3weshowthetoptenBillboardsinglesfrom\nthechartreleasedonJanuary11th2007(week2),andour\nalternativesingleschartbasedonP2Pactivityonofthe\npreviousweek.Thetwochartssharefourcommonsongs,\nyettheyarequitedistinct. Forthefull100songscharts,\nthemediandistanceofsongsontheBillboardfromtheir\nrankingonthealternativechartis18.\nInordertobetterunderstandthedifferenceinsongrank-\ning,wedeﬁnetherankingdriftofasongasthedifference\nbetweenitsrankontheBillboardcharttoitsrankonthe\ncorrespondingalternativechart.Wethenplotthecumula-\ntivedistributionfunction(CDF)ofthisdifferenceforall\n100titlesontheBillboard. Fig.3(a)depictstheCDFof\nfourweeklychartsondifferentweeksin2007.Theweek\nnumbersareaccordingtotheBillboardcharts. Thecor-\nrespondenceofthetwochartscanbeevaluatedfromthe\nshapeofgraphs.Aperfectmatchbetweenthetwocharts,\nwouldappearasaperfectstepfunction.Fig.3(a)revealsa\nmoderatecorrespondenceoftheBillboardchartswiththe\nalternativecharts. Forinstance,thepercentageofsongs\nwhoserankdriftisintherange-25to25is60%onaver-\nage.\nCDFchartscanbefurtherusedtocomparethedynam-\nicswithineachchartovertime.Wethusmeasuredthedrift\nofthesongsfromtheirrankingonpreviousweeks(onthe\nsamechart). Fig.3(b)depictstherankingdriftofsongs\nontheBillboardfromtheﬁrstweekof2007,overape-\nriodof3weeks. Asexpectedtherankingdriftincreases\nforlongertimeintervals.Fig.3(c)depictstherankingdri ft\nofsongsonthealternativechartduringthesametimepe-\nriod.Againthedriftincreaseswithtime.Thedriftonthe\nalternativechart,however,issmallerthanthatoftheBill -\nboard,indicatinglesschangeinsongsrankingfromweek\ntoweek.\n4. DISCUSSION\nInpastdecades,air-playsandrecordsaleswerethepri-\nmarymeansofdistributionofpopularmusic. TheBill-\nboardHot100wasthereforeareasonableproxytopopu-\nlarity.Today,however,newtechnologiesinparticularthe\nInternet,havecreatednewmeansfordistributionofmusic.\n63610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n010 20 30 40 50 60 70 80 90 100 \n-100 -50 0 50 100 CDF \nRanking Drift Week 3 \nWeek 7 \nWeek 11 \nWeek 15 \n(a)SongsrankingdriftbetweentheBillboard\nHot100andthealternativechart010 20 30 40 50 60 70 80 90 100 \n-100 -50 0 50 100 CDF \nRanking Drift Week 1 \nWeek 2 \nWeek 3 \nWeek 4 \n(b)SongstimedriftontheBillboardHot100010 20 30 40 50 60 70 80 90 100 \n-100 -50 0 50 100 CDF \nRanking Drift Week 1 \nWeek 2 \nWeek 3 \nWeek 4 \n(c)Songstimedriftonthealternativecharts\nFigure3.CumulativeDistributionofRankingDrift(CDF)\nThegrowingpopularityofﬁlesharingmakerecordsales\nandradioplaysanincreasinglypoorpredictorofpeoples’\ntaste.Therecordindustryattemptstostoptheswappingof\npopmusicthroughtheInternetbytakingsomeP2Pven-\ndorstocourt,butthesteadyspreadofﬁlesharingsystems\nandtheirtechnologicalimprovementsmakethemimpossi-\nbletoshutdown.\nInSection3wesawthatcurrentlytheBillboard’ssales\nbasedrankingsystem,isstillquiteintunewithwhatpeo-\npledownload,butasﬁlesharingbecomesevermorepreva-\nlent,aneedforanewrankingsystemarises.Thisobserva-\ntionwasﬁrstintroducedbyGrace etal.[6],whereitwas\nsuggestedtouseopinionmining(OM)onpublicboards\ntomeasuremusicpopularity.In[6],commentsonartists’\npagesonMySpacewereusedtobuildanalternativepop-\nularitychartofmusicalartists. Theirtoptenalternative\nlistwassubstantiallydifferentthanthatoftheBillboard .It\nwaspreferred,however,overtheBillboard’slistby2-to-1\nratiobytheir74humantestsubjects.\nWearguethatpopularityrankingbasedonP2Pactiv-\nityhasmanyadvantagesoverrankingbasedonopinion\nmining.First,iteliminatesthecomplextaskofclassifyin g\nopinionpolaritiesbasedonidentifyingopinionsemantics .\nWhenP2Pqueriesareconsidered,eachqueryisalwaysa\npositiveindicationofausershowinginterestinthesongor\ntheartist. Second,thelaborioustaskofidentifyingspam\ncontentinopinionmining,becomestrivialinadatasetof\nquerystrings.Ontopofthat,opinionmininginawebsite\nsuchasMySpaceisbiasedtowardsthetypicaluserofsuch\nawebsite,andbiasedagaintowardsactiveuserswhocare\ntocommentonartistspages.Ourmethod,doesn’trequire\nanactiveactiononthesideoftheuser. Werathermea-\nsurequeriesgeneratedaspartoftheﬁlesharingprocess.\nNonetheless,thesequeriesdisclosetheinterestsoftheus er.\nFinally,wearguethatopinionminingismorevulnerable\ntomanipulationsbystakeholderssuchaspublicrelation\ncompaniesactingonbehalfoftheartistortherecordcom-\npany.PlantingcommentsonMySpacebyinterestedenti-\ntiesisrathereasy,whilethetechnologicalbarrierofgene r-\natingmanysearchqueriesinaﬁlesharingnetworkismuch\nhigher.Infact,networkssuchasGnutella,alreadyemploy\ntechniquestoidentifyandeliminatenon-humanautomaticsearchqueries(asdescribedinSection1).\nHowever,rankingsongsbasedonP2Pqueriesstillhas\nsomeopenquestions.Thereare,ofcourse,theethicalis-\nsueswithmusicpiracywhichareyettobeaddressed.Re-\ngardingintegrity,therankingmightbebiasedtowardsthe\npreferencesofﬁleswapperswhichmaydifferintastefrom\nthegeneralpublic.ItisalsopossiblethatasingleP2Pnet-\nwork,howeverlarge,hasausercommunitywhichisbiased\nagainstorforsomegenres,bringingtheneedtobasethe\nchartonallthetopP2Pnetworksandnotjustthelargest\noneaswasdoneinthisstudy.Someoftheopenquestions\nonthealgorithmicsideincludetheneedtodevelopanartist\nrankingalgorithmbasedonsinglesdownloads,andtore-\nsolvetherankingofsongswithconfusingtitles(e.g., Love\norHot).\nItisnotunlikely,thatintheforeseeablefuture,music\ndistributionbasedonﬁlesharingwillbecomethenorm,\nandmusicsaleswillbereducedtoanichemarket.Weex-\npectthatasthepracticeofﬁlesharingbecomesevenmore\nwidespread,thislineofresearchwillbecomeincreasingly\nrelevant.\n5. REFERENCES\n[1]RamD.GopalandG.LawrenceSanders.Doartists\nbeneﬁtfromonlinemusicsharing? JournalofBusi-\nness,79(3):1503–1534,May2006.\n[2]MartinPeitzandPatrickWaelbroeck.Whythemusic\nindustrymaygainfromfreedownloading–theroleof\nsampling.InternationalJournalofIndustrialOrgani-\nzation,24(5):907–913,September2006.\n[3]Sudip Bhattacharjee, Ram D. Gopal, Kaveepan\nLertwachara, and James R. Marsden. Using p2p\nsharing activity to improve business decision mak-\ning: proof of concept for estimating product life-\ncycle.ElectronicCommerceResearchandApplica-\ntions,4(1):14–20,2005.\n[4]Sudip Bhattacharjee, Ram Gopal, Kaveepan\nLertwachara, and James R. Marsden. Whatever\nhappenedtopayola? anempiricalanalysisofonline\n637Poster Session 4\nmusicsharing.Decis.SupportSyst. ,42(1):104–120,\n2006.\n[5] NoamKoenigstein,YuvalShavitt,andTomerTankel.\nSpottingoutemergingartistsusinggeo-awareanalysis\nofp2pquerystrings.In The14thACMSIGKDDIn-\nternationalConferenceonKnowledgeDiscoveryand\nDataMining,2008.\n[6] J.Grace,D.Gruhl,K.Haas,M.Nagarajan,C.Robson,\nandN.Sahoo.Artistrankingthroughanalysisofonline\ncommunitycomments. The17thInternationalWorld\nWideWebConference ,2008.\n[7] AdamShakedGish,YuvalShavitt,andTomerTankel.\nGeographicalstatisticsandcharacteristicsofp2pquery\nstrings.InThe6thInternationalWorkshoponPeer-to-\nPeerSystems.\n[8] DanielStutzbach,RezaRejaie,andSubhabrataSen.\nCharacterizing unstructured overlay topologies in\nmodernp2pﬁle-sharingsystems. IEEE/ACMTransac-\ntionsonNetworking ,16(2),2008.\n[9] PaulResnikoff.Digitalmediadesktopreport,fourth\nquarter, 2007, April 2008. Digital Music Research\nGroup.\n[10] AmirH.Rasti,DanielStutzbach,andRezaRejaie.On\nthelong-termevolutionofthetwo-tiergnutellaoverlay.\nInIEEEGlobalInternetSymposium ,Barcelona,Spain,\nApril2006.\n[11] EytanAdarandBernardoA.Huberman.Freeridingon\ngnutella.FirstMonday,5,2000.\n[12] M.Ripeanu.In FirstInternationalConferenceonPeer-\nto-PeerComputing .\n[13] Matei Ripeanu, Ian Foster, and Adriana Iamnitchi.\nMappingthegnutellanetwork: Propertiesoflarge-\nscalepeer-to-peersystemsandimplicationsforsys-\ntemdesign.IEEEInternetComputingJournal ,6:2002,\n2002.\n[14] A.Klemm,C.Lindemann,M.Vernon,andO.P.Wald-\nhorst.Characterizingthequerybehaviorinpeer-to-\npeerﬁle sharing systems. In Internet Measurement\nConference.\n[15] K.Sripanidkulchai.Thepopularityofgnutellaquerie s\nanditsimplicationsonscalability,February2001.Fea-\nturedonO’Reilly’swww.openp2p.comwebsite.\n[16] MihajloA.Jovanovic.Modelinglarge-scalepeer-to-\npeernetworksandacasestudyofgnutella.Master’s\nthesis,UniversityofCincinatti,Cincinatti,OH,USA,\n2001.\n[17] Wikipediathefreeencyclopedia.Billboardhot100.\nhttp://en.wikipedia.org/wiki/Billboard Hot100 Last\naccessedMay2009.[18] EricT.BradlowandPeterS.Fader.Abayesianlifetime\nmodelforthe”hot100”billboardsongs. Journalofthe\nAmericanStatisticalAssociation ,96:368–381,2001.\n638"
    },
    {
        "title": "Musical Models for Melody Alignment.",
        "author": [
            "Peter van Kranenburg",
            "Anja Volk",
            "Frans Wiering",
            "Remco C. Veltkamp"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415608",
        "url": "https://doi.org/10.5281/zenodo.1415608",
        "ee": "https://zenodo.org/records/1415608/files/KranenburgVWV09.pdf",
        "abstract": "In this paper we show that the modeling of musical knowledge within alignment algorithms results in a successful similarity approach to melodies. The score of the alignment of two melodies is taken as a measure of similarity. We introduce a number of scoring functions that model the influence of different musical parameters. The evaluation of their retrieval performance on a well-annotated set of 360 folk-song melodies with various kinds of melodic variation, shows that a combination of pitch, rhythm and segmentation-based scoring functions performs best, with a mean average precision of 0.83.",
        "zenodo_id": 1415608,
        "dblp_key": "conf/ismir/KranenburgVWV09",
        "keywords": [
            "alignment algorithms",
            "musical knowledge",
            "melodies",
            "similarity approach",
            "score of the alignment",
            "melodic variation",
            "scoring functions",
            "evaluation",
            "folk-song melodies",
            "mean average precision"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMUSICAL MODELS FOR FOLK-SONG MELODY ALIGNMENT\nPeter van Kranenburg, Anja Volk, Frans Wiering, Remco C. Veltkamp\nUtrecht University, Department of Information and Computing Sciences\n{petervk,volk,fransw,Remco.Veltkamp }@cs.uu.nl\nABSTRACT\nIn this paper we show that the modeling of musical knowl-\nedge within alignment algorithms results in a successful\nsimilarity approach to melodies. The score of the align-\nment of two melodies is taken as a measure of similarity.\nWe introduce a number of scoring functions that model\nthe inﬂuence of different musical parameters. The evalua-\ntion of their retrieval performance on a well-annotated set\nof 360 folk-song melodies with various kinds of melodic\nvariation, shows that a combination of pitch, rhythm and\nsegmentation-based scoring functions performs best, with\na mean average precision of 0.83.\n1. INTRODUCTION\nIn this paper we use alignment algorithms to measure the\nsimilarity of melodies. Alignment algorithms are widely\nused for comparison of sequences of symbols. Creating\nan alignment is a way to relate two sequences with each\nother by ﬁnding the best corresponding parts. Especially\nin the ﬁeld of computational biology, where they are used\nto ﬁnd corresponding patterns in protein or nucleotide se-\nquences, many algorithms that align sequences have been\ndeveloped. Sequence alignment is also suitable for assess-\ning musical similarity for several reasons. Firstly, music\nunfolds in time, therefore, a model of music as a one-\ndimensional sequence of events seems appropriate. Sec-\nondly, manual alignments have extensively been used in\nfolk-song research to evaluate relations between melodies.\nThirdly, structural alignment is a prominent model in cog-\nnitive science for human perception of similarity [3].\nMost alignment algorithms use a dynamic program-\nming approach. One of the earliest variants is the Lev-\nenshtein distance [8], which is an edit distance: it com-\nputes how many operations are needed to transform one\nsequence into another. Needleman and Wunsch [9] pro-\nposed an algorithm that ﬁnds an optimal alignment of two\ncomplete sequences. The quality of an alignment is mea-\nsured by the alignment score, which is the sum of the\nalignment scores of the individual symbols. If we con-\nsider two sequences of symbols x:x1, . . . , x i, . . . , x n,\nandy:y1, . . . , y j, . . . , y m, then symbol xican either be\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.aligned with a symbol from sequence yor with a gap. Both\noperations have a score, the substitution score and the gap\nscore. The gap score is mostly expressed as penalty, i.e.\na negative score. The optimal alignment and its score are\nfound by ﬁlling a matrix Drecursively according to:\nD(i, j) = max\n\nD(i−1, j−1) + S(xi, yj)\nD(i−1, j)−γ\nD(i, j−1)−γ,(1)\nwhere S(xi, yj)is the substitution scoring function, γ\nis the gap penalty, D(0,0) = 0 ,D(i,0) = −iγ, and\nD(0, j) =−jγ.D(i, j)contains the score of the optimal\nalignment up to xiandyjand therefore, D(m, n )con-\ntains the score of the optimal alignment of the complete\nsequences. We can obtain the alignment itself by tracing\nback from D(m, n )toD(0,0); the algorithm has both time\nand space complexity O(nm). In our modeling, we use an\nextension of the algorithm proposed by Gotoh [5], which\nemploys an afﬁne gap penalty function without loss of ef-\nﬁciency. In this approach, the extension of a gap gets a\nlower penalty than its opening.\nMongeau and Sankoff [10] were among the ﬁrst to adapt\nalignment algorithms to music. They used an extended ver-\nsion of the Needleman-Wunsch algorithm. Their scoring\nfunction takes both pitch and duration into account. Mon-\ngeau and Sankoff’s approach has been quite inﬂuential,\ne.g. the search algorithm implemented in the search engine\nMELDEX [13] is based on this algorithm. G ´omez et al. [4]\nsuccessfully tested a modiﬁed version on a MIREX data-\nset. In general, alignment algorithms have often been used\nto match short melodic phrases against a larger database\n[1, 4, 7, 13, 14]. Typical tasks addressed with this approach\nare to ﬁnd a tune in the database with QBH [1], differ-\nent arrangements of a piece [14], or similar incipits given\nto the query [4]. We use the alignment between complete\nmelodies in order to ﬁnd melodies that belong to the same\ntune family. The similarity relations that have to be mod-\neled originate in the oral transmission of folk-songs and\ndiffer from those in the previous tasks.\nContribution. In this paper we model various features\nof music as substitution scoring functions, which we incor-\nporate in the Needleman-Wunsch-Gotoh algorithm. Using\na set of melodies that are well-described regarding their\ndifferent kinds of similarity relations, we evaluate the in-\nﬂuence of these scoring functions on the retrieval perfor-\nmance. Our best scoring function combines several musi-\ncal features and outperforms well-known approaches from\nliterature.\n507Poster Session 3\n2. DATA\n2.1 The Annotated Corpus\nThe set of melodies studied in this paper is part of a larger\ncollection of over 6000 encoded Dutch folk-songs hosted\nby the Meertens Institute in Amsterdam. In the ongoing\nproject of digitization at this institute, the melodies are en-\ncoded both from ethnomusicological transcriptions of ﬁeld\nrecordings and from written sources of folk-songs, deliver-\ning several formats (humdrum **kern, MIDI, Lilypond).1\nFor a subset of 360 melodies, detailed annotations have\nbeen created in order to describe similarity relations be-\ntween melodies [16], resulting in a well-documented set of\nsongs, the Annotated Corpus.\nThe melodies are grouped in so-called tune families.2\nAll melodies in one group are considered to be histor-\nically related through the process of oral transmission.\nSince the history of each tune family is not fully docu-\nmented, it is often not known whether two melodies are\nhistorically related. Instead, musicological experts decide\nwhether melodies belong to the same tune family by as-\nsessing their melodic and textual similarity. In order to\nmake the experts’ musical intuition behind the similarity\nassessments explicit, we developed an annotation system\n(described in [16]). For the Annotated Corpus (consisting\nof 26 tune families) several dimensions of perceived sim-\nilarity (contour, rhythm, motives, text) were numerically\nrated by the musicologists such that the similarity between\nthe most typical melody of a tune family (the reference\nmelody) and all other members of the tune family was de-\nscribed. The 26 tune families were chosen from the larger\ncollection by an expert such that this set contains a repre-\nsentative diversity of similarity relations between members\nof a tune family. Comparing the annotations to the retrieval\nperformance of alignment algorithms allows a detailed un-\nderstanding of the success or failure of the models based\non musicological insights.\n2.2 Representation of melodies\nFor applying alignment algorithms, a melody has to be pre-\nsented as a sequence of symbols. In our representation,\neach symbol represents a note. A symbol has a number of\nattributes, including: pitch (in base40 encoding), duration\n(rational number), score time (rational number), time in bar\n(rational number), onset (integer), current bar number (in-\nteger), current phrase number (integer), upbeat (boolean),\ncurrent meter (rational number), free meter (boolean), ac-\ncented (boolean), and time position within phrase (real\nnumber in [0,1]). These attributes are used to compute\nsubstitution scores or other attributes. Figure 1 shows an\nexample with some of the attributes.\nBased on the encoded time signature, two levels of ac-\ncents are distinguished: either accented or not accented.\nThe ﬁrst note of any group of two in a double meter and the\nﬁrst beat in any group of three beats in a triple meter is con-\n1The full collection is browsable at: http://www.liederenbank.nl.\n2At the Meertens Institute the concept of “melody norm” is used, see\n[16] for a more detailed explanation of this concept.\nFigure 1 . Representation of melodies.\nsidered accented. All other notes are unaccented. Thus, in\nsongs in free meter, or in songs with additive or asymmet-\nrical meters,3which are very uncommon in this corpus,\nall notes are unaccented. Furthermore, phrase boundaries\nhave been annotated by the encoders.\n2.3 Rests\nMost notated rests can be considered inessential. In partic-\nular at the end of phrases singers often take a breath, such\nthat timing between the phrases is very variable. The exact\nencoding of rests as performed is therefore not reasonable.\nTo make melodies more comparable, all rests have been\nreplaced by a prolongation of the previous note.\n2.4 Transposition Invariance\nSince songs are notated in different keys, the similarity\nmeasure should be transposition invariant. To achieve this,\na pitch histogram for both melodies is created that indicates\nfor each pitch the total duration during the song. Then the\nshift at which the normalized histograms have maximal in-\ntersection is computed. Since the pitches are represented\nin base40 encoding, the shift of the histogram can be inter-\npreted as the interval with which the one melody should be\ntransposed in order to compare it to the other.\n2.5 Normalization of Alignment Scores\nSince the score of an alignment depends on the length of\nthe sequences, normalization is needed to compare differ-\nent alignment scores. Therefore, we divide the alignment\nscore by the length of the shortest sequence.\n3. SCORING FUNCTIONS\n3.1 Single substitution scoring functions\nIn this section we introduce a number of substitution scor-\ning functions for different musical dimensions. They de-\ntermine substitution scores that are based on musicologi-\ncal knowledge. Each function takes two symbols of the\nmelodic sequence as input. The output of each scoring\nfunction is in the interval [−1,1].\nFirst, we introduce scoring functions that are based on\npitch-related features. The simplest scoring function deter-\nmines whether two pitches are the same or not. The score\n3Asymmetrical meters consist of stacked groupings of dissimilar met-\nrical groups.\n50810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nis either maximal or minimal:\nSexactpitch (xi, yj) =/braceleftbigg1 ifxi=yj\n−1ifxi/negationslash=yj. (2)\nIn oral transmission, slight changes of pitches are likely\nto occur, therefore, we allow substitution with pitches that\nare within a band with certain width:\nSpitchb (xi, yj) =/braceleftbigg\n1−int(xi,yj)\n23ifint(xi, yj)≤23\n−1 otherwise.\n(3)\nWe deﬁne int(xi, yj) = |p(xi)−p(yj)|mod 40 , with\np(x)as the pitch of symbol xin base 40 encoding. A ﬁfth\nis 23 in base 40 encoding. Thus, all intervals up to a ﬁfth\nget a positive substitution score and all larger intervals are\nconsidered a bad match.\nAnother way to express the distance of two pitches is by\ntheir harmonic relation. The substitution of consonances\ngets a higher score than the substitution of dissonances:\nSharm (xi, yj) =\n\n1 prime\n0.5consonance\n0.5augmented prime\n−1dissonance. (4)\nThe intervals are taken modulo octave. Consonances are\nminor and major third, perfect fourth, perfect ﬁfth and mi-\nnor and major sixth. The augmented prime gets a positive\nsubstitution score to favour alignments of songs that have\na minor and a major variant.\nFurthermore, we deﬁne two substitution functions that\nare based on melodic contour, taking either the contour of\na phrase or of the entire melody into account:\nSphrasecont (xi, yj) = 1 −2∗|pphr(xi)−pphr(yj)|.(5)\nSsongcont (xi, yj) = 1 −2∗|psong(xi)−psong(yj)|.(6)\nHere pphr(x)/epsilon1[0,1]indicates the vertical position between\nthe lowest and highest pitches of the phrase that xis part\nof, while psong(x)/epsilon1[0,1]indicates the vertical position be-\ntween the lowest and highest pitches of the entire song. In\ndetermining the highest and lowest pitches, the notes in the\nupbeats of the phrases are disregarded, since these are very\nvariable between variants of a song.\nNext, we deﬁne three scoring schemes that are based on\nrhythmic features. In a simple approach based on note du-\nrations, the score is maximal if the durations are the same,\nand minimal otherwise:\nSexactdur (xi, yj) =/braceleftbigg1 ifd(xi) =d(yj)\n−1ifd(xi)/negationslash=d(yj),(7)\nin which d(x)is the duration of the symbol x.\nMetric accents derived from the notated time signa-\nture describe a further aspect of the rhythmic structure of\nmelodies. We deﬁne a substitution function that uses these\nmetric accents in the following way:\nSaccent (xi, yj) =/braceleftbigg1 ifa(xi) =a(yj)\n−1ifa(xi)/negationslash=a(yj),(8)in which a(x)indicates whether the symbol xis accented\nor not (for deﬁning accents see section 2.2).\nA more complex notion of metric accents based on the\nrhythmic structure of notes instead of the time signature is\nprovided by Inner Metric Analysis (IMA) [15]. We deﬁne\na scoring function that is determined by the metric weights\nof the notes, as computed by IMA:\nSima(xi, yj) = 1 −2∗ |w(xi)−w(yj)|. (9)\nHere w(x)denotes the metric weight of the symbol x\nscaled into the interval [0,1]. For scaling, all weights were\ndivided by the greatest weight in the song. The parameters\nfor the IMA algorithm are the ones that are mostly used:\np= 2,l= 2(e.g., in [15]).\nFurthermore, we want to use the information of phrase\nboundaries given in our data-set. We introduce a scoring\nfunction based on the horizontal position within the phrase:\nSphr(xi, yj) = 1 −2∗ |phr(xi)−phr(yj)|,(10)\nin which phr(x)/epsilon1[0,1]indicates for the symbol xthe hor-\nizontal position in its phrase. This substitution function\nhelps to keep phrases together in alignments.\n3.2 Combination\nThe single substitution scoring functions deﬁned in sec-\ntion 3.1 model isolated aspects of melodies. In order to\nmodel several aspects within one function to get closer to\nthe multidimensionality of melodies, we combine substitu-\ntion functions. We want alignments in which the aligned\nsymbols are similar in all dimensions, therefore, we multi-\nply the individual scores:\nScombination (xi, yj) =n/productdisplay\nk=1Sk(xi, yj), (11)\nin which each Sk(xi, yj)is scaled into the interval [0,1],\nand the ﬁnal score is scaled into [−1,1]back again.\n3.3 Gap penalty function\nWe use an afﬁne gap penalty function in which the penalty\nfor opening a gap is 1, and the penalty for extending a gap\nis 0.1. Thus, variants of songs in which e.g. a phrase is\nrepeated can be better aligned, since these penalties result\nin one long gap instead of many short gaps. Furthermore,\nthe use of an afﬁne gap penalty function prevents gaps from\nbeing scattered all over the alignment.\n4. EVALUATION OF SCORING FUNCTIONS\nThe scoring functions are evaluated by their respective re-\ntrieval performance on our Annotated Corpus as described\nin section 2. To evaluate a scoring scheme each melody is\ntaken once as query and the other melodies are sorted ac-\ncording to the normalized score of the alignment with the\nquery melody. At all ranks the average recall and average\nprecision for all ranking lists is computed. These values\nare plotted in a diagram. The criterion for relevance is the\nmembership of the same tune family.\n509Poster Session 3\n4.1 Evaluation of Single Substitution Functions\nFirst, we study the performance of the single pitch-based\nsubstitution functions introduced in section 3.1. Variation\nin pitch is considered an important element of oral trans-\nmission (see e.g. [6]). Nevertheless, aligning melodies us-\ning the exact pitch information with the simplest function\nSexactpitch results in a relatively good performance (see\nFigure 2). Allowing pitch variation within a small range\nusing the pitch band function improves this performance\nonly slightly.\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1Average precision\nAverage recallexactpitch\npitchb\nharm\nlinecont\nsongcont\nFigure 2 . Retrieval performance of pitch-based substitu-\ntion functions.\nBoth the harmonic and contour-based substitution func-\ntions perform worse than Sexactpitch . Considering the\ncontour instead of the exact pitch sequence does not re-\nsult in a better retrieval performance. Harmonic relations,\nwhich have otherwise successfully been used in models of\nmelodic expectancy [11], do not improve the alignment of\nmelodies of a tune family in comparison to exact pitch in-\nformation.\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1Average precision\nAverage recallaccent\nphrase\nexactdur\nima\nFigure 3 . Retrieval performance of non-pitch-based sub-\nstitution functions.\nFigure 3 shows retrieval performance for the scoring\nfunctions that do not involve pitch information. Although\nrhythmic features have been considered quite stable within\noral transmission (see [6]), all rhythm-related substitu-\ntion functions perform worse than pitch-related functions.Simaperforms at the top of the ranking slightly better than\nSaccent , however Saccent performs slightly better in the\nlow range. In general the difference between the two mod-\nels is quite small, indicating that the accents of the notated\nbars are synchronous to the accents based on notes onsets.\n4.2 Evaluation of Combinations of Single Substitution\nFunctions\nIn a next step, we combine rhythmical, metrical and seg-\nmentation data. First, we combine the best of the pitch-\nrelated functions ( Spitchb ) with rhythmical and phrase\nfunctions. Figures 2 and 3 show that the individual substi-\ntution functions perform worse than Spitchb , but from the\ncurves of Spitchb−accent ,Spitchb−phrase ,Spitchb−exactdur ,\nandSpitchb−imain Figure 4 it appears that combinations\nyield better retrieval performance for all combinations but\nSexactdur . Since Sexactdur is binary and the combination\nis by multiplication, the pitch similarity for symbols with\nno exact correspondence in duration is lost.\nCombination with the other two rhythmic functions\n(SimaandSaccent ) show equal improvement. The rather\nmodest improvement when considering metric accents in\ncomparison to the single substitution function Spitchb con-\ntradicts the hypothesis that pitches among melodies of the\nsame tune family are more stable on metrically accented\nnotes than on metrically weak positions as assumed in [2]:\nobviously pitches on metrically weak positions also vary\nto only a small extent. The phrase information yields the\ngreatest improvement.\nFinally, we evaluate the retrieval performance of the\ncombination of the best substitution functions. We choose\nSimaas the metric scoring function, Spitchb is the best\npitch based scoring function. Sphrimproved retrieval re-\nsults by stimulating phrase boundaries to be aligned. The\nretrieval performance of the combination Spitchb−ima−phr\nshown in Figure 4 shows even better performance results\nthan the combinations of two single substitution functions.\nIf we average the precision of all relevant items for all\nqueries, we get a mean average precision of 0.83 for this\ncombination. Choosing Saccent instead of Sima gives\nnearly the same retrieval performance.\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1Average precision\nAverage recallpitchb\npitchb-accent\npitchb-phrase\npitchb-exactdur\npitchb-ima\npitchb-ima-phrase\nFigure 4 . Retrieval performance of combinations of sub-\nstitution functions.\n51010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTo evaluate the scalability, we performed the same test\nwith a data-set containing all 4863 classiﬁed songs and\nwith the same 360 queries. The results yield a mean av-\nerage precision of 0.67.\n4.3 Comparison with Related Methods\nFigure 5 shows comparisons of our best scoring scheme\nwith alignment methods from literature. For the method\nof Mongeau and Sankoff [10] the parameters were taken\nas given by Mongeau and Sankoff. The normalization was\ndone by dividing the alignment score by the sum of the\ndurations of both sequences. DiffEd and rawEd were taken\nfrom the Simile alignment toolbox without change [12]. It\nappears that our Spitchb−ima−phrperforms best.\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1Average precision\nAverage recallpitchb-ima-phr\nMongeau-Sankoff\nSimile - diffEd\nSimile - rawEd\nFigure 5 . Comparison with related methods.\n5. RETRIEVAL PERFORMANCE PER TUNE\nFAMILY\nThe classiﬁcation of the melodies into tune families by\nmusicological experts is based on a number of musical di-\nmensions. The importance of the different dimensions and\nthe form of interaction between them varies to a great ex-\ntent among tune families. Therefore, ﬁnding a similarity\nmodel that performs well on all tune families is a challeng-\ning task. The retrieval performance of the scoring function\nSpitchb−ima−phrshows per tune family a considerably sta-\nble success, with average precision values ranging from 1\nto 0.8 for 23 out of 26 tune families. Hence, this function\nworks reasonably well on the majority of tune families. For\nthree tune families the function shows only moderate re-\ntrieval performance, which are Een lindeboom stond in het\ndal 1 (short: Lindeboom ),Daar reed een jonkheer 1 (short:\nJonkheer ) and Heer Halewijn 4 (short: Halewijn ), with re-\nspective average precision of 0.71, 0.67 and 0.65. Table\n1 gives an overview of low ranking results extracted from\nrankings in which the reference melody of the tune family\n(see section 2.1) was used as the query. For Lindeboom ,\nFigure 6 shows the ﬁrst line of the query along along with\na good match and the two melodies with ranks 77 and 140.\nThe melodies on these low ranks are quite different from\nthe query. This is reﬂected by the experts’ annotations: for\nthe two melodies on ranks 77 and 140 all global musicalTune family Relevant melodies at rank\nLindeboom 77 and 140\nJonkheer 78, 98 and 167\nHalewijn 19, 40, 74 and 101\nTable 1 . Overview over low ranking results.\nstond De eens den in 't boom dal ne43\nkoe dag len zo 't Was mer op een\ndal ne stond43\nin boom denEen het\nhetdal boomstond43\nEen in neden\nFigure 6 . From top to bottom: incipits of reference melody\n(query), melodies on ranks 1, 77 and 140..\ndimensions (rhythm, contour and importance of motives)\nare rated somewhat similar , while for most of the other\nmelodies of Lindeboom at least two of these dimensions\nare rated very similar .\nIn the tune family Jonkheer the melody on rank 78 is no-\ntated with a different phrase structure than the query, such\nthat two phrases correspond to one phrase of the query.\nSince phrase information is used for the alignment, this in-\nconsistent phrase assignment introduces lower scores. The\nmelody on rank 98 has a very different formal structure\nthan the query: while the query has the form ABA’, this\nmelody has the form AAA’A”. As a consequence, notes in\nphrases that are aligned with each other differ to a great\nextent. The melody on rank 167 is quite different from the\nquery (see Figure 7), which is reﬂected by low ratings of\nthe experts in both local and global musical dimensions.\nThe tune family Halewijn is according to the experts\nmost of all characterized by a similar rhythmic organiza-\ntion. Melodies of this group differ considerably regard-\ning pitch, such that the contour is mostly rated as only\nsomewhat similar . The averaged annotation values for all\nmusical dimensions in this tune family show a signiﬁcant\n(p= 0.02) linear correlation with the distances obtained\nusing the Spitchb−ima−phrrater – – hence the lowest ranked\nmelodies tend to receive low similarity scores in the anno-\ntations. For the melody on rank 74 the expert commented\nthat it is possible that this melody does not belong to the\ntune family.\n6. CONCLUSION AND FUTURE WORK\nWe have shown that the inclusion of musical knowledge\nin alignment algorithms improves the assessment of sim-\nilarity among folk song melodies. By evaluating differ-\nent substitution scoring functions, we found that our pitch-\nrelated functions lead to better recognition than rhythm-\nrelated functions. The use of phrase information improved\n511Poster Session 3\nFigure 7 . Reference melody of Jonkheer (top) and melody\non rank 167 (bottom).\nthe retrieval results considerably. The best combination of\nfunctions, combining a pitch-based, a rhythm-based and a\nsegmentation-based scoring function, outperforms related\nmethods from literature.\nNext, we will develop scoring functions that reﬂect\nmore advanced musicological models. We will use the an-\nnotations to evaluate the results by means of the quality\nof alignments. Since the occurrence of similar motives in\nrelated melodies was considered important by the musi-\ncological experts, we will investigate how corresponding\nmotives can be aligned. For testing the suitability of our\napproach to model similarity within oral transmission in\ngeneral, we will evaluate its performance on different col-\nlections. These steps contribute to the development of a\nsimilarity rater adequate for oral transmission that is based\non musically advanced models of melodic similarity.\nAcknowledgments. This work was supported by the Netherlands Orga-\nnization for Scientiﬁc Research within the WITCHCRAFT project NWO\n640-003-501, which is part of the CATCH-program. We thank Jeroen\nDonkers for suggesting the alignment approach, Daniel M ¨ullensiefen and\nKlaus Frieler for calculating distance matrices for rawEd and diffEd, Bas\nde Haas for his evaluation software, and Carlos G ´omez for his implemen-\ntation of the Mongeau-Sankoff algorithm.\n7. REFERENCES\n[1] N.H. Adams, M.A. Bartsch, J.B. Shifrin and\nG.H. Wakeﬁeld: “Time Series Alignment for Music\nInformation Retrieval”, Proceedings of the 5th Inter-\nnational Conference on Music Information Retrieval ,\nBarcelona, 2004.\n[2] J. Garbers, A. V olk, P. van Kranenburg, F. Wiering,\nL. Grijp, and R.C. Veltkamp: “On pitch and chord sta-\nbility in folk song variation retrieval”, Proceedings of\nthe First International Conference of the Society for\nMathematics and Computation in Music , 2007.[3] R.L. Goldstone: “Similarity, interactive activation,\nand mapping”, Journal of Experimental Psychology:\nLearning, Memory, and Cognition , V ol. 20, pp. 3–28,\n1994.\n[4] C. G ´omez, S. Abad-Mota, and E. Ruckhaus: “An Anal-\nysis of the Mongeau-Sankoff Algorithm for Music In-\nformation Retrieval”, Proceedings of the 8th Interna-\ntional Conference on Music Information Retrieval , Vi-\nenna, Austria, pp. 109–110, 2007.\n[5] O. Gotoh: “An Improved Algorithm for Matching Bi-\nological Sequences”, Journal of Molecular Biology ,\nV ol. 162, pp. 705–708, 1982.\n[6] E. Klusen, H. Moog, and W. Piel: “Experimente zur\nm¨undlichen Tradition von Melodien”, Jahrbuch f ¨ur\nVolksliedforschung , V ol. 23, pp. 11–32, 1978.\n[7] K. Lemstr ¨om and E. Ukkonen: “Including Interval En-\ncoding into Edit Distance Based Music Comparison\nand Retrieval”, Proceedings of the AISB’00 Symposium\non Creative & Cultural Aspects and Applications of AI\n& Cognitive Science , Birmingham, pp. 53–60, 2000.\n[8] V . Levenshtein: “Binary Codes Capable of Correcting\nDeletions, Insertions and Reversals”, Soviet Physics\nDoklady , V ol. 10, No. 8, pp. 707–710, 1966.\n[9] S.B. Needleman and C.D. Wunsch: “A general method\napplicable to the search for similarities in the amino\nacid sequence of two proteins”, Journal of Molecular\nBiology , V ol. 48, No. 3, pp. 443-53, 1970.\n[10] M. Mongeau and D. Sankoff: “Comparison of musical\nsequences”, Computers and the Humanities , V ol. 24,\npp. 161–175, 1990.\n[11] E.H. Margulis: “A Model of Melodic Expectation”.\nMusic Perception , V ol. 22, No. 4, pp. 663–714, 2005.\n[12] D. M ¨ullensiefen and K. Frieler: “Cognitive Adequacy\nin the Measurement of Melodic Similarity: Algorith-\nmic vs. Human Judgements,” Computing in Musicol-\nogy, V ol. 13, pp. 147–177, 2004.\n[13] L.A. Smith, R.J. McNab, and I.H. Witten:\n“Sequence-Based Melodic Comparison: A Dynamic-\nProgramming Approach”, Computing in Musicology ,\nV ol. 11, pp. 101–117, 1998.\n[14] A. Uitdenbogerd and J. Zobel: “Melodic Matching\nTechniques for Large Music Databases”, The Sev-\nenth International Multimedia Conference , Orlando,\nFlorida, 1999.\n[15] A. V olk: “Persistence and Change: Local and Global\nComponents of Metre Induction using Inner Metric\nAnalysis”, Journal of Mathematics and Computation\nin Music , V ol. 2, No. 2, pp. 99–115, 2008.\n[16] A. V olk, P. van Kranenburg, J. Garbers, F. Wier-\ning, R.C. Veltkamp, L.P. Grijp: “A manual annotation\nmethod for melodic similarity and the study of melody\nfeature sets”, Proceedings of the Ninth International\nConference on Music Information Retrieval , pp. 101–\n106, 2008.\n512"
    },
    {
        "title": "MIR in ENP Rule-based Music Information Retrieval from Symbolic Music Notation.",
        "author": [
            "Mika Kuuskankare",
            "Mikael Laurson"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417277",
        "url": "https://doi.org/10.5281/zenodo.1417277",
        "ee": "https://zenodo.org/records/1417277/files/KuuskankareL09.pdf",
        "abstract": "Symbolic music information retrieval is one of the most underrepresented areas in the field of MIR. Here, symbolic music means common practice music notation–the musician readable format. In this paper we introduce a novel rule-based symbolic music retrieval mechanism. The Scripting system–ENP-Script–is augmented with MIR functionality. It allows us to perform sophisticated retrieval operations on symbolic musical scores prepared with the help of the music notation system ENP. We will also give a special attention to visualization of the query results. All the statistical queries, such as histograms, are visualized with the help of common music notation where appropriate. N-grams and more complex queries–the ones dealing with voice leading, for example– are visualized directly in the score. Our aim is to demonstrate the power and expressivity of the combination of common music notation and a rulebased scripting language through several challenging examples.",
        "zenodo_id": 1417277,
        "dblp_key": "conf/ismir/KuuskankareL09",
        "keywords": [
            "Symbolic music",
            "information retrieval",
            "common practice music notation",
            "novel rule-based",
            "scripting system",
            "ENP-Script",
            "music notation system",
            "MIR functionality",
            "sophisticated retrieval operations",
            "visualization of query results"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMIRINENP – RULE-BASED MUSIC INFORMATIONRETRIEVAL FROM\nSYMBOLICMUSICNOTATION\nMika Kuuskankare\nSibeliusAcademy\nCentreforMusicand Technology\nmkuuskan@siba.fiMikaelLaurson\nSibeliusAcademy\nCentreforMusicand Technology\nlaurson@siba.fi\nABSTRACT\nSymbolic music information retrieval is one of the most\nunderrepresented areas in the ﬁeld of MIR. Here, sym-\nbolic music means common practice music notation–the\nmusician readable format. In this paper we introduce\na novel rule-based symbolic music retrieval mechanism.\nTheScriptingsystem–ENP-Script–isaugmentedwithMIR\nfunctionality. Itallowsustoperformsophisticatedretri eval\noperations on symbolic musical scores prepared with the\nhelpofthemusicnotationsystemENP.\nWe will also give a special attention to visualization of\nthe query results. All the statistical queries, such as his-\ntograms, are visualized with the help of common music\nnotation where appropriate. N-grams and more complex\nqueries–theonesdealingwith voiceleading,forexample–\narevisualizeddirectlyin thescore.\nOur aim is to demonstrate the power and expressivity\nof the combinationof common music notation and a rule-\nbased scripting language through several challenging ex-\namples.\n1. BACKGROUND\nMusic (especially Classical music) is primarily a written\ntradition. Throughout the centuries musical compositions\nhavebeenpreservedinmusicnotation. Itisthemostcom-\npleteandwidespreadmethodthatwe knowoffornotating\nthecomplexandinterrelatedpropertiesofamusicalsound:\npitch,intensity,time,timbre,andpace.[1]Commonmusic\nnotation is also an invaluable tool in the ﬁeld of music in-\nformationretrieval. In this paper we introducea symbolic\nmusic retrieval mechanism based on a scripting language\ncalledENP-Script[2]andmusicnotationsystemENP [3].\nENP-scriptisarule-basedobjectorientedscriptinglan-\nguagethat is hereaugmentedwith MIR functionality. The\nextensionsallowtheusertoperformretrievaloperationso n\nscorespreparedwith thehelpof ENP andvisualizethe re-\nsults in a meaningfulway. ENP-Script allows us to deﬁne\ncomplex and musically relevant queries using it’s pattern-\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2009 International Society for MusicInformation Retrieva l.matching language. It has a uniform and simple syntax.\nThe structural elements of the score (e.g., notes, beats,\nmeasures, melodies, harmony, voice-leading, etc.) are ac-\ncessedusingasymbolicnamingschemewhereacollection\nofreservedkeywordsisusedtodenotetheobjectsofinter-\nest.\nOn the score level the retrieval system is based on\nENP’s underlying music representation. ENP provides\nseveralinterestingfeaturesintermsofthepresentapplic a-\ntion: (1)itcanbeusedtostoremusicusingawiderangeof\nnotational styles (Western musical notation roughly from\n17th century onward, including 20th century notation);\n(2) it can be used as a user-interface component allowing\nustoconstructbotheye-catchingandfunctionalvisualiza -\ntions of MIR data; (3) it provides access to its notational\ndata structures, allowing the user to inspect the propertie s\nof the notational objects (e.g., time, pitch, duration); an d\n(4)it providesarichlibraryofstandardanduser-deﬁnable\nexpressions allowing us to annotate the score with analyt-\nical information [4]. One further detail of interest is that\nENP scores can be written using both mensural (metric)\nand non-mensural (piano roll like) notation. The system\ndescribed in this paper works without modiﬁcations on\nboth types of notation. This makes it possible to use this\nsystem forapplicationsdealingwith contemporarymusic.\nInthispaper,we will also givea special attentionto vi-\nsualization. All the statistical queries, such as histogra ms,\nare visualized with the help of common music notation\nwhereappropriate. N-gramsandmorecomplexqueriesare\nvisualized directly in the score. Both approachesallow us\nto associate the queryresults directly with the correct mu-\nsical objects.\nA few approacheshave been introduced where the aim\nis to use some kind of symbolic notation as the basic for\nMIR queries (see, e.g., [5–13]. Most of these systems,\nhowever, primarily address large databases of music en-\ncoded in an array of formats, such as MIDI, Kern, Mu-\nsicXML [14], etc. Most of the approaches can be seen\nas MIR tool chains comprising of several small or even\nlarger utilities chained together. Humdrum is an example\nof such a system. In [13] Humdrum is even married with\nPerl [15] and LilyPond [16] to create kind of a meta tool\nchain. Other approaches use various music formats like\nGUIDO [17] in [6,8,9]; Kern in [5,8]; and MusicXML\nin [18].\nAt the moment the presented retrieval system can be\n699Poster Session 4\nseen as an Analytic/Production MIR System [19]. We\nconcentrate predominantlyon posing questions on a sym-\nbolicmusicalscoreratherthanretrievinginformationfro m\na large music database. Querying a database of all Bach\nChorales, for example, is however not out of the scope of\nthepresentapproach.\nOur retrieval system is part of PWGL [20] which is\nfreely available for Macintosh OS X (Intel and PPC ver-\nsions)andforWindowsat www.siba.fi/PWGL .\nThe rest of the paperis organizedas follows. Section 2\nillustrateshowacollectionofarchetypalMIRassignments\ncanbesolvedusingourretrievalsystem. Theexamplesare\ndividedroughlyintotwo categories: statistical and analy t-\nical. We end the paperwith some concludingremarksand\noutlinesforfuturework.\n2. EXAMPLES\nWhilemassqueriescantellcertainkindsoffactsaboutthe\nmusicinthedatabaseourapproachemphasizesthemusical\nmeaningfulness of the query. On the one hand one might\nﬁndoutthatBachviolatedXtimestheruleYinZchorals.\nOntheotherhandonemightwanttorevealthesecasesina\nmusical score and study why this may have happened. To\nbe able to do so requires a ﬂexible searching mechanism\nandﬂexiblenotationalandvisualizationtools.\nIn this section we present a collection of examples\nbased on more or less archetypal MIR assignments. The\nsection is divided roughly in two parts. In the ﬁrst part\nwe concentrate on statistical queries and more or less tra-\nditional visualizations. The second part, in turn, turns\nfocus on more analytical queries and visualizes the re-\nsults in the score. Altogether, we will address several\nsubjects, e.g., histograms, counting,pitch-class set the ory,\nrhythm,etc. Some of the case studies borrowshamelessly\nfromtheHumdrumexampledatabasepresentedat http:\n//music-cog.ohio-state.edu/Humdrum/ .\nEach subsection is accompanied with a code example\nandpotentiallyalsoavisualizationoftheresult. Itisnot in\nthe scope of this paper to give an exhaustivereview of the\nScriptingsyntax. Thecodeexamplesrequirealittleknowl-\nedgeaboutLispprogramminglanguagebuttheyshouldbe\nclear and simple enough to be followed by anyone with\nsomebackgroundinprogramming.\nApart form the examples presented in this paper, many\nother types of queries could easily made with the current\nsystem includingthose aboutpitch-classset theory,voice -\nleading,wordpainting,harmonicanalysis,etc.\nThe most important points of interest in the following\nexamples are: (1) the terseness and expressivity of the\nquery deﬁnitions, (2) the descriptiveness of the visualiza -\ntion,and(3)theoverallversatilityofthesystem.\n2.1 StatisticalQueries\n2.1.1 Histograms\nOneoftheprototypicalMIRtasksisthehistogram.\nThemusicalscoreusedasastartingpointforExamples\n1–4 is the guitar transcription by Andr´ es Segovia of theTango op. 156a by Isaac Alb´ eniz. The beginning of the\nscoreisshowninpage6(Figure6).\nExample 1 shows the retrieval rule to generate a pitch\nhistogram of the given score. The pattern given in line\n1 means that the rule applies to every note object in the\nscore, thus the traditional wild card *.?1is a vari-\nable to which everynotein the score is boundone by one.\nhistogram is a special MIR function that takes care of\ngatheringthevaluesandvisualizingtheresult.\nAftertheexecutionofthescriptthe pertinentvisualiza-\ntionmethodiscalledtogeneratethepitchhistogramshown\nin Figure 1. Instead of the traditional horizontalbar graph\nwe useherea verticalarrangementinstead. Thehistogram\nvalues are also shown against a set of piano keysto give a\nbetter idea of the register (middle-Cis highlightedusing a\nshadeofgrey).\nOne special aspect of this particular histogram(includ-\ningtheonesshowninFigures2and3)isthattheresultcan\nbe played back. Either as a whole or by selecting a subset\nof the result shown. Especially, in case of tonal music,\nan aural examination of the pitch histogram could among\nother things reveal potential problems in the integrity of\nthesourcematerial.\nFurthermore, as the histogram is realized with the help\nof ENP, it itself can be scripted to select and highlight\npitchesabovecertainthreshold,forexample.\nExample 1 An ENP-script collecting histogram values\nfroma score.\n1(*?1\n2(?if\n3 (histogram :value (m ?1))))\nFigure1. A traditionalpitch histogramplottedusingENP\nasa visualizationtool(Alb´ eniz: Tangoop. 156a).\n70010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n2.1.2 HarmonyHistogram\nIn addition to horizontal events (i.e., melody, as in Exam-\nple1)withENP-Scriptitisalsopossibletoaccessthever-\ntical (harmonic)dimensionofthescore.\nScoresarepartitionedby’harmonicslices’ (resembling\n’moments’ in [5]). A harmonic slice is a vertical entity\ndeﬁned as a point in time when any note event begins or\nends in any part. This structural component allows us to\nperformqueriesinvolvingsimultaneity.\nExample2showsascriptaccessingtheharmonicslices\nto produce a ’moment’ histogram. This can be accom-\nplished simply by introducing the keyword :harmony\nin the patternmatchingpart(see line 1). This instructsthe\nscripttoaccessalltheverticalelementsofthescoreinste ad\nofthehorizontalones. ComparedtoExample1thechange\nisminimalbuttheeffectisdramatic.\nAnother point of interest is the form\n(m ?1 :complete? t) given at the end of\nline 1. This is in fact a condition and it is used here due\nto the implementation of the scripting engine and cannot\nbe explainedin-depthin the score of this paper. Sufﬁce to\nsay thatinadditiontoaccessingthetotalharmony(i.e.,al l\nthe notes sounding at a given point in time) we can also\naccess partial harmonic formations (e.g., subsets of the\nsounding harmony). As we are here interested only in the\ntotal harmonieshencethecondition.\nFurthermore, line 2 introduces yet another additional\ncondition to skip any grace notes that are abundant in our\nexamplescore. Inline4 wesimplyrecordthepitchvalues\noftheharmonygivenby (m ?1) asalistofmidivalues,\ne.g.,(606467).\nExample 2 A script collecting histogram values of type\nharmony.\n1(*?1 :harmony (m ?1 :complete? t)\n2(?if (unless (some #’grace-note-p\n3 (m ?1 :object t))\n4 (histogram :value (m ?1)))))\nThe result is shown as a histogram with the relevant\nparts visualized using common practice notation. In the\nanalysis same kind of chords are grouped together irre-\nspective of register, or pitch spelling. That means that,\ne.g., all major chords are identiﬁed as equal (i.e., pitch-\nclass set 3-11B).1Furthermore, in the histogram the so\ncalled prime form is shown. This is why, for example, the\nsecondto last entry,the seventhchord,is displayedin ﬁrst\ninversion.\n2.1.3 Counting\nOur nextexampledemonstratesthe ability of the scripting\nmechanismto access onlypartsof the givenscore. Exam-\nple 3 shows a simple counting script where we count the\nnumberofeventsinthescore. Here,insteadofcountingall\ntheeventswerestrictthesearchtomeasures8–16(seeline\n1The system used here is similar to that of Forte except that th e let-\nters A or B are added to distinguish between two inversionall y related\ntranspositional set-classes, e.g.,3-11Ais theminortria d, and3-11B is its\ninversion, the major triad.Figure 2. A ’moment’ histogram (Alb´ eniz: Tango op.\n156a).\n1). Our analysisrevealsthat there are 96 eventscombined\ninthemeasures8–16intheTangobyAlb´ eniz. quantity\ninline3simplyincrementsacounterwhenpresentedwith\na new event. Note, that this script counts all noteevents.\nWe could similarly count all chordsby inserting the key-\nword:chord afterthevariable ?1.\nExample 3 A script counting the number of events in a\nscorefrommeasure8through16.\n1(*?1 :measures (8_16)\n2(?if\n3 (quantity :value ?1)))\nFor comparison we give the following Unix script in\nHumdrumperformingtheequivalentoperation:\nyank -n = -r 8-16 Tango | census -k\n2.1.4 RhythmicPatterns\nNext,we deﬁnea searchbasedontherhythmicdimension\nofthescore. Weaimtodeterminethemostcommonrhyth-\nmic pattern spanninga measure. Example 4 shows the re-\ntrieval script. Once againourscript haschangedrelativel y\nlittle. The keyword :measure shown in the ﬁrst line\ndenotes that we want to access measure objects this time.\nThus, the variable ?1is here bound in succession to ev-\nery measure object found in the score. As we want to ac-\ncess the whole rhythmic entity inside the measure we add\nonce again the condition (m ?1 :complete? t) .\nThis ensures that the body of the script is executed only\nwhen the rhythm for the entire measure is known. In line\n4 we read the rhythmicdeﬁnitioncached by the system in\na list form.\n701Poster Session 4\nExample 4 A script for determining the most common\nrhythmicpatternspanninga measure.\n1(*?1 :measure (m ?1 :complete? t)\n2(?if\n3 (histogram :value\n4 (read-key ?1 :rtm-pattern))))\nWe deﬁne here the rhythm histogram again using ENP\nand common music notation as it allows us to visualize\nthe data in a straightforward manner. Figure 3 shows a\npartoftherhythmhistogramdisplayingtheactualrhythms\nin rhythm notation and the corresponding values as a bar\ngraph.\nFigure 3. A per measure rhythm histogram (Tango op.\n156abyAlb´ eniz).\nThe above is certainly more descriptive than, let’s say,\nproducingprint-outslike this:\n((23 ((1 (1 1 1)) (1 (1 1))))\n(18 ((1 (1)) (1 (1 1)))) ...)\n2.2 AnalyticalQueries\n2.2.1 RhythmicPatterns\nAnother kind of rhythmic query is presented in Example\n5. Here, instead of counting all the possible rhythms we\nrestrictoursearchtocertainkindofrhythmicpattern. Fur -\nthermore, we have chosen to visualize the result of the\nquery directly in the score. Figure 4 shows all the occur-\nrences of our rhythmic pattern enclosed inside rectangles\n(drawn in red color in the original score). To save space\nwe show here only the right-hand melody of the original\ncomposition (measures 1–8 of Humoresque in G ♭major\nbyAntoninDvoˇ r´ ak).\nThe retrieval script is now a bit more complex than\nin the previous cases. The pattern matching part reads\n(*?1 ?2 ?3 ?4 as in this script we are now inter-\nested in the rhythmic formation between four consecutive\nnotes.match-rtm? in line 3 is a special functionthat is\nusedmatchthescorerhythmsagainsta givenpattern.\nDecipheringthe rhythmmatchingsyntax can be at ﬁrst\nquitechallenginganditrequiressomeknowledgeaboutthe\ninternalrepresentationofENP (see, e.g.,[21]). We cannot\ngiveacomprehensivereviewoftheformathere. However,\nthe line 5 deﬁnes a pattern where an event having a dura-\ntion of 2 units is followed by a rest (a negative number)\nwith a duration of 1 unit and another event with the dura-\ntion of 1 unit. The durations are relative instead of abso-\nlute. The aforementionedpattern is repeated twice in row5. This description is translated to the following rhythmic\npattern:\nExample 5 A script searchingfor a givenfour-noterhyth-\nmicalpatternina score.\n1(*?1 ?2 ?3 ?4\n2(?if\n3 (when (match-rtm?\n4 (1\n5 ((?1 2) -1 (?2 1) (?3 2) -1 (?4 1))))\n6 (add-expression ’score-expression\n7 ?1 ?2 ?3 ?4\n8 :color :red))))\nFigure 4. A speciﬁc rhythmic pattern visualized in the\nscore(HumoresqueinG ♭majorbyAntoninDvoˇ r´ ak).\n2.2.2 N-grams\nN-gramshavebeenusedextensivelyinMIRinbothmono-\nphonicandpolyphoniccontexts(see,e.g.,[19,22]). Inour\nnextexamplewe will showhowtorepresentn-gramswith\nthe scripting language and how to mark them in a score.\nNaturally, instead of marking the n-grams in the score we\ncould have recorded the statistical distribution of n-gram s\nand display them in the same manner as displayed in Fig-\nure2.\nExample 6 shows the script deﬁnition. This particular\nscriptisdemonstratingyetanotherinterestingabilityof the\nscripting system. Here, instead of deﬁninga ﬁxed pattern,\nas in the previous examples, we write the script in a dy-\nnamic fashion. The pattern matching part becomes now\nquiteminimalisticagain. Thecomplexityofthesearchlies\ninside the script deﬁnition. In order to represent any n-\ngram the user is require to change only the list of context\nlengths enumerated in line 3. Here, we use a list (2 3)\ndenoting di- and tri-grams respectively. The body of the\nscript from line 4 onwards is written so that any sized n-\ngrams can be found and visualized. In line 6 we add an\nENPexpressioninthescoredisplayingtheextentofthen-\ngram and also the sizes of the consecutiveintervalsas can\nbeseen inFigure5.\n70210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n3. DISCUSSION\nCurrentlythisretrievalsystemisnotsuitableforverylar ge\ncorpusofdata. Themechanismusedhererequiresthatthe\nwhole score is read in the memory and that all the musi-\ncal objectsareinstantiated. Onecouldpossiblyprovidean\nalternative loading mechanism that creates only the nec-\nessary data structures needed for the scripting language\nto operate. This would most likely guarantee much more\nshorterloadingtimesandinturnfacilitatelargersearche s.\nOur system needs to support more input formats. Not\nonlyMIDIandthenativeENPﬁleformatsbutalsoatleast\nMusicXMLandperhapsevenKern.\nHowever, at its present state the system is capable of\nperforming very sophisticated and complex queries. Also\nthe visualizationcapabilitiesaresecondto none. Theabil -\nity to be able to mix common music notation with statis-\ntical graphics is also beneﬁcial and allows us to represent\nthequeryresultin amusicianreadableway.\nThe presented notational front-end combined with our\nscripting engine allows us to query, annotate and analyze\nmusical scores and visualize the results in a manner prob-\nablynotmatchedbyanyothersoftwarepackage.\n4. ACKNOWLEDGMENTS\nThe work of Mika Kuuskankare and Mikael Laurson has\nbeen supported by the Academy of Finland (SA 114116\nandSA 105557).\n5. REFERENCES\n[1] CurtisRoads. The ComputerMusic Tutorial . TheMIT\nPress, Cambridge, Massachusetts, London, England,\n1996.\n[2] Mika Kuuskankare and Mikael Laurson. Intelligent\nScriptinginENPusingPWConstraints.In Proceedings\nof International Computer Music Conference , pages\n684–687,Miami,USA,2004.\n[3] Mika Kuuskankare and Mikael Laurson. Expressive\nNotationPackage. ComputerMusicJournal ,30(4):67–\n79,2006.\n[4] Mika Kuuskankare and Mikael Laurson. Annotating\nMusical Scores in ENP. In International Symposium\nonMusicInformationRetrieval ,London,UK,2005.\n[5] MichaelDroettboom.Expressiveandefﬁcientretrieval\nof symbolic musical data. In InternationalSymposium\nonMusicInformationRetrieval ,2001.\n[6] Andreas Kornst¨ adt. The jring system for computer-\nassisted musicologicalanalysis. In ISMIR,2001.\n[7] Shyamala Doraisamy. An approach towards a poly-\nphonic music retrieval system. In International Sym-\nposiumonMusic InformationRetrieval ,2001.\n[8] Matthew J. Dovey. A technique for regular expression\nstyle searching in polyphonic music. In International\nSymposiumonMusic InformationRetrieval ,2001.[9] Holger H. Hoos, Kai Renz, and Marko G¨ org.\nGUIDO/MIR — an experimental musical information\nretrieval system based on GUIDO music notation. In\nInternational Symposium on Music Information Re-\ntrieval,2001.\n[10] Donncha ´O Maid´ ın and Margaret Cahill. Score Pro-\ncessing for MIR. International Symposium on Music\nInformationRetrieval ,pages59–64,2001.\n[11] GoffredoHaus, Maurizio Longari, and Emanuele Pol-\nlastri. A score-driven approach to music information\nretrieval. J. Am. Soc. Inf. Sci. Technol. , 55(12):1045–\n1052,2004.\n[12] David Huron. Music information processing using the\nhumdrum toolkit: Concepts, examples, and lessons.\nComputerMusicJournal ,26(2):15–30,2002.\n[13] IanKnopke.Theperlhumdrumandperllilypondtoolk-\nits for symbolic music information retrieval. In Inter-\nnational Symposium on Music Information Retrieval ,\npages147–152,2008.\n[14] M. Good and G. Actor. Using MusicXML for File In-\nterchange. In Third International Conference on WEB\nDelivering of Music , page 153, Los Alamitos, CA,\n2003.IEEEPress.\n[15] Wikipedia. Perl — wikipedia, the free encyclopedia,\n2009.[Online;accessed8-May-2009].\n[16] Han-Wen Nienhuys and Jan Nieuwenhuizen. Lily-\nPond,asystemforautomatedmusicengraving.In XIV\nColloquium on Musical Informatics (XIV CIM 2003) ,\nFirenze,Italy,2003.\n[17] H. H. Hoos, K. A. Hamel, K. Renz, and J. Kilian.\nThe GUIDO Music Notation Format - A Novel Ap-\nproach for Adequately Representing Score-level Mu-\nsic. InProceedings of International Computer Music\nConference ,pages451–454,SanFrancisco,1998.\n[18] GoffredoHausandAlberto Pinto. Mx structuralmeta-\ndata as mir tools. In Soundand Music Computing '05 ,\n2005.\n[19] J. Stephen Downie. Evaluating a Simple Approach to\nMusic Information Retrieval: Conceiving Melodic N-\ngrams as Text . PhD thesis, University of Western On-\ntario,1999.\n[20] Mikael Laurson, Mika Kuuskankare,and Vesa Norilo.\nAn Overview of PWGL, a Visual Programming Envi-\nronment for Music. Computer Music Journal , 33(1),\n2009.\n[21] MikaKuuskankareandMikaelLaurson.RecentDevel-\nopments in ENP-score-notation. In Sound and Music\nComputing'04 ,Paris,France,2004.\n[22] Shyamala Doraisamy. Polyphonic Music Retrieval:\nThe N-gram Approach .PhD thesis, Universityof Lon-\ndon,2004.\n703Poster Session 4\nExample 6 A script to visualize n-grams directly in an ENP score. Here, di- and tri-grams are shown. Simply by editing\ntheparameterlist showninline3(n-grams)it ispossibleto visualizen-gramsofanysize. Nootherchangesarenecessar y.\n1(*?1\n2(?if\n3 (let ((n-grams ’(2 3)))\n4 (iter (for n-gram in n-grams)\n5 (?incase-let (intervals (m ?1 :L (1+ n-gram) :data-access : int :complete? t))\n6 (add-expression ’group (m ?1 :L (1+ n-gram) :object t)\n7 :kind :bracket-at-end\n8 :info (format () \"˜{˜3,@d ˜ˆ|˜}\"\n9 intervals)))))))\nFigure5. N-gramsvisualizeddirectlyinthe scoreusinga dynamical lyadaptingscript.\nFigure6. Tangoop. 156abyIsaacAlb´ eniznotatedwith ENP(transcri bedforGuitarbyAndr´ esSegovia).\n704"
    },
    {
        "title": "A Music Classification Method based on Timbral Features.",
        "author": [
            "Thibault Langlois",
            "Gonçalo Marques"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417863",
        "url": "https://doi.org/10.5281/zenodo.1417863",
        "ee": "https://zenodo.org/records/1417863/files/LangloisM09.pdf",
        "abstract": "This paper describes a method for music classification based solely on the audio contents of the music signal. More specifically, the audio signal is converted into a compact symbolic representation that retains timbral characteristics and accounts for the temporal structure of a music piece. Models that capture the temporal dependencies observed in the symbolic sequences of a set of music pieces are built using a statistical language modeling approach. The proposed method is evaluated on two classification tasks (Music Genre classification and Artist Identification) using publicly available datasets. Finally, a distance measure between music pieces is derived from the method and examples of playlists generated using this distance are given. The proposed method is compared with two alternative approaches which include the use of Hidden Markov Models and a classification scheme that ignores the temporal structure of the sequences of symbols. In both cases the proposed approach outperforms the alternatives.",
        "zenodo_id": 1417863,
        "dblp_key": "conf/ismir/LangloisM09",
        "keywords": [
            "music classification",
            "audio contents",
            "compact symbolic representation",
            "timbral characteristics",
            "temporal structure",
            "statistical language modeling",
            "Music Genre classification",
            "Artist Identification",
            "publicly available datasets",
            "distance measure"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nA MUSICCLASSIFICATIONMETHODBASEDON TIMBRALFEATURES\nThibault Langlois\nFaculdadedeCiˆ encias daUniversidadedeLisboa\ntl@di.fc.ul.ptGonc ¸aloMarques\nInstitutoSuperiordeEngenhariadeLisboa\ngmarques@isel.pt\nABSTRACT\nThispaperdescribesamethodformusicclassiﬁcationbased\nsolely on the audio contents of the music signal. More\nspeciﬁcally, the audio signal is converted into a compact\nsymbolicrepresentationthatretainstimbralcharacteris tics\nand accounts for the temporal structure of a music piece.\nModels that capture the temporal dependencies observed\nin the symbolic sequences of a set of music pieces are\nbuilt using a statistical language modeling approach. The\nproposed method is evaluated on two classiﬁcation tasks\n(Music Genre classiﬁcation and Artist Identiﬁcation) us-\ningpubliclyavailabledatasets. Finally,adistancemeasu re\nbetween music pieces is derived from the method and ex-\namplesofplaylistsgeneratedusingthisdistancearegiven .\nTheproposedmethodiscomparedwithtwoalternativeap-\nproaches which include the use of Hidden Markov Mod-\nels and a classiﬁcation scheme that ignores the temporal\nstructure of the sequences of symbols. In both cases the\nproposedapproachoutperformsthealternatives.\n1. INTRODUCTION\nTechniquesformanagingaudiomusicdatabasesareessen-\ntial to deal with the rapid growth of digital music distri-\nbution and the increasing size of personal music collec-\ntions. TheMusicInformationRetrieval(MIR)community\nis well aware that most of the tasks pertaining to audio\ndatabasemanagementarebasedonsimilaritymeasuresbe-\ntweensongs[1–4]. Ameasureofsimilaritycanbeusedfor\norganizing, browsing, visualizing large music collection s.\nIt is a valuable tool for tasks such as mood, genre or artist\nclassiﬁcationthatalsocanbeusedinintelligentmusicrec -\nommendationandplaylist generationsystems.\nThe approaches found in the literature can roughly be\ndividedintwo categories: methodsbasedonmetadataand\nmethods based on the analysis of the audio content of the\nsongs. Themethodsbasedonmetadatahavethedisadvan-\ntageofrelyingonmanualannotationofthemusiccontents\nwhich is an expensive and error prone process. Further-\nmore, these methods limit the range of songs that can be\nanalyzedsincetheyrelyontextualinformationwhichmay\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2009 International Society for MusicInformation Retrieva l.not exist. The other approach is based solely on the au-\ndio contents of music signals. This is a challenging task\nmainly due to the fact that there is no clear deﬁnition of\nsimilarity. Indeed, the notion of similarity as perceived\nby humans is hard to pinpoint and depends on a series of\nfactors,somedependentonhistoricalandculturalcontext ,\nothers related to perceptual characteristics of sound such\nastempo,rhythmorvoicequalities.\nVariouscontent-basedmethodsformusicsimilarityhave\nbeenproposedinrecentyears. Mostofthemdividetheau-\ndiosignalinshortoverlappingframes(generally 10-100ms\nwith50%overlap),andextractaset offeaturesusuallyre-\nlated to the spectral representation of the frame. This ap-\nproach convertseach song into a sequence of feature vec-\ntors, with a rich dynamic structure. Nevertheless, most of\nthesimilarityestimationmethodsignorethetemporalcon-\ntents of the music signal. The distribution of the features\nfrom one song or a group of songs are modeled, for in-\nstance, with the k-means algorithm [3], or with a Gaus-\nsian mixture model [1,5,6]. To measure similarity, mod-\nels are compared in a number of ways, such as the Earth-\nMover’sdistance[3],Monte-Carlosampling[1],ornearest\nneighborsearch. Additionally,someinformationaboutthe\ntime-dependenciesofthe audiosignal canbe incorporated\nthrough some statistics of the features over long temporal\nwindows(usuallya fewseconds),likein [4–8].\nIn this work we propose computing a measure of sim-\nilarity between songs based solely on timbral characteris-\ntics. We are aware that relying only on timbre to deﬁne\na music similarity measure is controversial. Human per-\nception of music similarity relies on a much more com-\nplexprocess,albeittimbreplaysanimportantroleinit. As\npointedoutbyJ.-J.AucouturierandJ.Pachet[1],methods\nthat aim at describinga timbral qualityof whole songwill\ntendtoﬁndsimilarpiecesthathavesimilartimbresbutbe-\nlongtoverydifferentgenresofmusic. Forinstance,pieces\nlike aSchumann sonata or a Bill Evans tune will have a\nhigh degree of similarity due to their common romantic\npiano sounds [1]. Following our approach by modeling\ntime dependencies between timbre-based feature vectors,\nwe expect to include some rhythmic aspects in the mod-\nels. As we will see in section 3.3, this approach leads\nto playlists with more variety while conserving the same\noverallmood.\nWeuseasingletypeoflow-levelfeatures: theMelFre-\nquencyCepstralCoefﬁcients(MFCC).TheMFCCvectors\nare commonlyused in audio analysis and are described as\ntimbral features because they model the short-time spec-\n81Poster Session 1\ntral characteristicsofthe signal ontoa psychoacousticfr e-\nquency scale. On their own, the MFCC vectors do not\nexplicitly capture the temporal aspects of the music, and\ntherefore are often associated with the “bag of frames”\nclassiﬁers. In this type of classiﬁers, songs with the same\nMFCC frames in different order would be yield the same\nresults. ItisourcontentionthattheorderofMFCCframes\nis indeed important and that this information can be used\nto estimate a similarity measure between songs. We use a\nlanguage model approachto achieve this result. The most\nrelatedworksincludeSoltau etal.[9],Chen etal.[10],and\nLi andSleep [11].\nIn Soltau et al.[9], each music is converted into a se-\nquence of distinct music events. Statistics like unigram,\nbigram, trigram counts are concatenated to form a fea-\nture vector that is fed into a neural network for classiﬁ-\ncation. In Chen et al. [10] a text categorization technique\nis proposed to perform musical genre classiﬁcation. They\nbuildaHMMfromtheMFCCcoefﬁcientsusingthewhole\ndatabase. The set of symbols is represented by the states\nof the HMM. Music symbols are tokenized by computing\n1 and 2-grams. The set of tokens is reduced using Latent\nSemantic Indexing. In Li and Sleep, a support vector ma-\nchine is used as a classiﬁer. The feature are based on n-\ngramsofvaryinglengthobtainedby a modiﬁedversionof\ntheLempel-Zivalgorithm.\nThispaperisorganizedasfollows: Insection2.we de-\nscribe our method for music similarity estimation. In sec-\ntion3.wereportandanalyzetheresultsofthealgorithmon\nvarioustaskanddatasets. Wealsocompareperformanceof\nour approach to other types of techniques. We close with\nsomeﬁnal conclusionsandfuturework.\n2. PROPOSEDAPPROACH\nThe proposedapproachisdividedintoseveralsteps. First,\nthe music signals are converted into a sequence of MFCC\nvectors1. Then, the vectors are quantized using a hierar-\nchical clustering approach. The resulting clusters can be\ninterpreted as codewords in a dictionary. Every song is\nconverted into a sequence of dictionary codewords. Prob-\nabilistic models are then built based on codeword transi-\ntions of the training data for each music category, and for\nclassiﬁcation, the model that best ﬁts a given sequence is\nchosen. The details of each stage are described in the fol-\nlowing sections. In the last section we consider building\nmodelsbasedon a single musicpiece, anddescribean ap-\nproachthatallowsustodeﬁneadistancebetweentwomu-\nsic pieces.\n2.1 Two-StageClustering\nTheobjectiveoftheﬁrststepofouralgorithmistoidentify ,\nfor each song, a set of the most representativeframes. For\neach track, the distribution of MFCC vectors is estimated\nwithagaussianmixturemodel(GMM)withﬁvegaussians\n1Twelve Mel Frequency Cepstral Coefﬁcients are calculated f or each\nframe,allaudioﬁlesweresampledat22050Hz,monoandeachf ramehas\naduration of 93ms with 50% overlapandfullcovariancematrix( Λi):\npdf(f) =N/summationdisplay\ni=1wiGi(f) (1)\nwith:\nGi(f)=1/radicalbig\n(2π)d|Λi|exp/parenleftbigg\n−1\n2(f−µi)Λ−1\ni(f−µi)⊤/parenrightbigg\n(2)\nwhere µirepresenttheGaussian’smeanand fanMFCC\nframe. We did not perform exhaustive tests in order to\nchose the optimal value for the number of Gaussians ( N)\nbut realized some tests on a reducednumberof tracksand\ndecided to use N= 5. At this step, the use of GMM\nis similar to Aucouturier’s work [12] were some hints are\ngiven about the optimal value of N. The parameters are\nestimatedusingtheExpectation-Maximization(EM)algo-\nrithm. Theprobabilisticmodelsofthesongsareusedtose-\nlect a subset of the most likely MFCC frames in the song.\nForeachtrack a,Fa,istheset of k1framesthatmaximize\nthelikelihoodofthemixture.\nContrastingwithAucouturier’sapproach,wedonotuse\nthe GMM as the representation of tracks in the database.\nThisleadstoan increasedmemoryrequirementduringthe\ntraining phase that is later reduced as we will see in the\nnextsection.\nThe second step consists in ﬁnding the most represen-\ntative timbre vectorsin the set of all music pieces. At this\nstage, the dataset correspondto the frames extracted from\neach song: F=/uniontextNm\njFjand the objective is to deduce\nk2vectors that represent this dataset. This is achieved\nusing the k-means algorithm. As an alternative, a GMM\ntrained on the set Fwas also used. But thanks to the ro-\nbustness,scalabilityandcomputationaleffectivenessof the\nk-meansalgorithm, better results were obtained using this\nsimpler approach. More precisely, the EM algorithm is\nsensible to parameters like the number of gaussians and\nthedimensionandthenumberofdatapoints,andcanresult\ninill-conditionedsolutions. Thatwasveriﬁedinnumerous\ncases,andwemanagedtotrainGMMswithonlyareduced\nnumberofkernelsthat wastoosmall forourobjectives.\nThe output of this two-stage clustering procedure is a\nset of k2twelve-dimensional centroids that represent the\ntimbres found in a set of music pieces. The value of the\nk1parameter must be chosen in order to balance between\nprecision2, computing and space resources. One of the\nadvantagesofdividingintotwostepsisscalability. Indee d,\nthe ﬁrst stage has to be done only once and, as we will\nsee in section 3. can be used to compute various kinds of\nmodels.\n2.2 LanguageModel Estimation\nThe set of k2vectors obtained during the previous step is\nused to form a “dictionary” that allow us to transform a\ntrack into a sequence of symbols. For each MFCC frame\nfa symbol scorresponding to the nearest centroid ciis\nassigned:\ns=argmin d(f, ci)\ni=1..k2\n2We expect that higher values of k1parameter will lead to a more\naccurate description of the setof timbres present in a song.\n8210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure1. Systemstructureforthelanguagemodelingapproach. Them usicsignalsareconvertedintoasequenceofMFCC\nvectors, and a two-stage clustering is performed on all the t raining sequences. Then all the MFFCs are vector quantized\nresultingina sequencesofsymbols. Thesequencesaredivid edbycategory,andthebigramsprobabilitiesareestimated .\nwhere d()isthe Euclidiandistance. Once tracksaretrans-\nformed into sequences of symbols, a language modeling\napproach is used to build classiﬁers. A Markov Model is\nbuilt for each category by computingthe transition proba-\nbilities(bigrams)foreachset ofsequences. Theresultisa\nprobability transition matrix for each categorycontainin g,\nfor each pair of symbols (si, sj), the probability P(sj|si)\nofsymbol sitobefollowedbythesymbol sj.\nThismatrixcannotbeused like thisbecauseit contains\nmany zero-frequency transitions. Many solutions to this\nproblem have been studied by the Natural Language Pro-\ncessing community. Collectively known as “smoothing”\nthe solution consist in assigning a small probability mass\nto each unseen event in the training set. In the context of\nthis workwe experimentedseveral approachessuchas the\nExpected Likelihood Estimator and the Good-Turingesti-\nmator[13]. Neitheroftheseapproachesaresuitableforour\ncase, because the size of our vocabulariesis much smaller\nthan those commonly used in Natural Language Process-\ning. We used a technique inspired by the “add one” strat-\negythatconsistsinaddingonetothecountsofevents. Af-\nter some tests, we concluded that adding a small constant\nǫ= 1.0e−5to each zero probability transition allowed\nustosolvethesmoothingproblemwithoutaddingtomuch\nbiastowardunseenevents.\nOnce a set of models is built, we are ready to clas-\nsify new tracks into one of the categories. A new track is\nﬁrst transformedintoa sequenceof symbols(as explained\nabove). Given a model M, the probability that it would\ngeneratethesequence S=s1, s2, ...snis:\nPM(si=1..n) =PM(s1)n/productdisplay\ni=2PM(si|si−1)(3)whichisbettercalculatedas\nSM(si=1..n)=log( PM(si=1..n))\n=log( PM(s1))+n/summationdisplay\ni=2log(PM(si|si−1))(4)\nThis score is computed for each model Mand the class\ncorrespondingtothemodelthatmaximizethescorevalues\nisassignedtothesequenceofsymbols. Oneofthebeneﬁts\nof ourmethodis that oncethe modelsarecomputed,there\nisnoneedtohaveaccesstotheaudioﬁlesandMFCCfea-\ntures since only the sequences of symbols are used. With\nvocabulary size between 200 and 300 symbols the space\nneededtokeepthissymbolicrepresentationisroughlyone\nbyte/frameor1200bytes/minute.\n2.3 DistanceBetweenMusic Pieces\nGiven a database of music tracks, a vocabulary is build\nfollowingthe stepsdescribedin section 2.1. Then,instead\nof creatinga model for each “class” or “genre”a model is\nbuiltforeachtrack(i.e. aprobabilitytranstionmatrix). Let\nSa(b)be the score of music bgiven the model of music a\n(see section 2.2). We can deﬁnea distance between music\naandmusic bby:\nd(a, b) =Sa(a) +Sb(b)−Sa(b)−Sb(a)(5)\nThis distance is symmetric but it is not a metric distance\nsinced(a, b) = 0 ⇒a=bis not veriﬁed. It is a difﬁ-\nculttasktoevaluateadistancebetweenmusicpiecessince\nthereisno“groundtruth”. Onecanexaminetheneighbor-\nhood of a song and verify to what extend the songs found\nnearby show similarities. In our case, the expected sim-\nilarities should be relative to timbral characteristics si nce\nweareusingfeaturesthatrepresentthetimbre. Acommon\napplication of distances measures over music pieces is to\ngenerate playlists. The user selects a song he likes (the\n83Poster Session 1\nC E J M R W %acc. pre. rec.\nClassical 304 2 0 0 0 14 95.0 0.95 0.95\nElectronic 1 96 0 0 10 7 84.2 0.74 0.84\nJazzBlues 0 2 16 0 6 2 61.5 1.00 0.62\nMetalPunk 0 1 0 24 18 2 53.3 0.89 0.53\nRockPop 1 13 0 3 78 7 77.5 0.63 0.77\nWorld 17 15 0 0 12 78 63.9 0.72 0.64\nTable 1. Confusion matrix, accuracy, precision and recall\nforeachclassoftheISMIR2004dataset.\nseed song) and the system returns a list of similar songs\nfromthe database.\n3. EXPERIMENTAL RESULTS ANDANALYSIS\n3.1 GenreClassiﬁcationtask\nWeusedtheISMIR2004genreclassiﬁcationdatasetwhich\niscomposedofsixmusicalgenreswithatotalof729songs\nfortrainingand729songsfortest3. Themethoddescribed\ninsections2.1and2.2wasusedtoclassifythisdataset. Ta-\nble1showstheconfusionmatrixonthetestset,classiﬁca-\ntionrate,precisionandrecallforeachclass,obtainedusi ng\nparameters k1= 200andk2= 300. The overall accuracy\nis 81.85% if we weight percentages with the prior proba-\nbility of each class. These results compare favorably with\nthoseobtainedwithotherapproaches(seeforexample[5],\n78.78% and [14], 81.71%). As can be seen in the follow-\ning table, the method is not too sensible to its parameters\n(k1andk2).\nk1k2accuracy k1 k2accuracy\n100 25 74.90% 200 200 81.07%\n200 50 77.37% 200 300 81.89%\n100 50 79.70% 200 400 81.48%\n100 100 80.93% 300 300 81.76%\n100 200 81.34% 300 400 81.07%\n100 300 81.76% 300 1000 80.52%\n3.2 ArtistIdentiﬁcationtask\nOne ofourobjectiveswiththistask isto assess theperfor-\nmance of our method when models are based on smaller\ndatasets. Indeed, contrasting with genre classiﬁcation, i n\nthe case of Artist Identiﬁcation, a model is build for each\nartist. Weevaluatedourmethodusingtwodatasets: arti-\nst204that contains 1412 tracks from 20 artists. Each\nartistisrepresentedby6albums. Theseconddatasetfocus\non Jazz music and is based on authors’ collection. It con-\ntains 543 tracks from 17 artists (we will call this dataset\nJazz17). This dataset is smaller than artist20 but\nthe interest here is to see if our system is able to distin-\nguishsongsthatbelongtoasinglegenre. Theabreviations\nused for the names of the 17 artists are: DK: Diana Krall,\nSV: SarahVaughan,DE: DukeEllington,TM: Thelonious\nMonk, CB: Chet Baker, MD: Miles Davis, CJ: Clifford\nJordan, NS: Nina Simone, JC: John Coltrane, FS: Frank\n3Thedistribution ofsongsalongthesixgenresis: classical : 320;elec-\ntronic: 115 jazzblues: 26; metalpunk: 45; rockpop: 101; wor ld: 122 for\nthe training and the test set.This data set was used for the Ge nre Classiﬁ-\ncation contestorganizedinthecontextoftheInternationa l Symposiumon\nMusic Information Retrieval -ISMIR 2004 (http://ismir200 4.ismir.net).\n4This dataset is available upon request, see: http://labros a.ee.-\ncolumbia.edu/projects/artistid/ .Sinatra, LY: Lester Young, OP: Oscar Peterson, EF: Ella\nFitzgerald,AD:AnitaO’Day,BH:BillieHolliday,AT:Art\nTatumandNJ: NorahJones.\nRegardingthe Jazz17 dataset,theresultsareshownin\nthe following table. For two sets of parameter values ( k1\nandk2) the training and test was repeated ten times and\nthe two last columns show the average accuracy and the\ncorrespondingstandarddeviationobservedonthetest set.\nk1k2mean std. dev.\n100 100 73.49% 1.75\n200 200 74.25% 2.25\nBecause of the reduced number of albums per artist, 50%\nofeachartist’ssongswererandomlyselectedandfortrain-\ningwhiletheotherhalfwasusedfortest. Table2 contains\naconfusionmatrixobtainedwith Jazz17. Ascanbeseen\nintheconfusionmatrix,numberofmisclassiﬁcationsoccur\nbetweensongswithstrongvocalsandarethusunderstand-\nable.\nThe results obtained with the artist20 dataset are\nshowninthefollowingtable. Weusedtwodifferentsetups.\nFor rows 1 and 2, 50% of an artist’s songs are randomly\nselected and used for training while the other half is used\nfortesting. Inrows3and4weusedthestrategysuggested\nin [15]. For each artist an album is randomly selected for\ntest andtheotherﬁvealbumsare usedfortraining.\nk1k2mean std. dev.\n1 100 100 57.40% 0.74\n2 200 200 59.14% 1.49\n3 100 200 45.28% 7.27\n4 200 200 48.98% 7.96\nTheresultsshownin rows3 and4 areworse thanthose\nobtainedbyDanEllis[15]sincehisapproachleadsto54%\naccuracyusingMFCCfeaturesand57%usingMFCCand\nchromafeatures.\nAs we can see, choosing the training and testing sets\nrandomly leads to signiﬁcantly better results than keeping\none album for test. This is due to the “album effect” [16].\nThese results show that despite the name of the task, it is\nclear that, at least in our case, the problem solved is not\ntheArtistIdentiﬁcationproblem. Indeed,ourmethodaims\nat classifying songs using models based on timbre. Dif-\nferent albums of the same artist may have very different\nstyles,usedifferentkindsofinstruments,soundeffectsa nd\nrecording conditions. If a sample of each artist’s style is\nfoundinthetrainingset,it ismorelikelythattheclassiﬁe r\nwillrecognizeasongwithsimilartimbre. Ifeverysongsof\nan album are in the test set, then the accuracy will depend\nonhowclosearethemixturesoftimbresofthisalbumfrom\nthoseof thetrainingset. Thisisconﬁrmedbythestandard\ndeviation observed with both approaches. When trying to\navoid the “album effect” we observe a large variation of\nperformance due to the variation of the datasets. In one\nof our tests we reached an accuracyof 62.3% but this was\ndue to a favorable combination of albums in the training\nandtest sets.\n8410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nNotwithstandingtheseobservationstheresultsareinter-\nesting. In particularwith the Jazz17 dataset, we can see\nthat the timbre-based classiﬁcation is quite accurate even\nwith musicpiecesthatbelongtothe samegenre.\n3.3 SimilarityEstimationtask\nThe good results obtained for the classiﬁcation of large\nsets of tracks (Genre classiﬁcation) and more speciﬁc sets\n(Artist Identiﬁcation) led us to consider building models\nbased on a single track. In this section some examples\nof playlists generated using our distance are shown and\ndiscussed. From our Jazz music set (see section 3.2), we\npickedsomewell-knownsongsandgeneratedaplaylistof\n20mostsimilar songs.\nIntheﬁrstexample,theseedsongis“ComeAwayWith\nMe” by Norah Jones. The playlist, shown in table 3, is\ncomposedof songswherevocals are the dominanttimbre.\nIt is interesting to note that with one exception, the artist s\nthat appear in this list are all women. The timbre of Chet\nBaker’svoiceisratherhighandinsometimesmaybecon-\nfused with a women’s voice. However, John Coltrane’s\n“VillageBlues” appearsasanintruderinthislist.\nDist. Artist Song\n0 0 N.Jones ComeAway with Me\n1 4093 N.Jones ComeAway with Me(other version)\n2 10774 D.Krall Cry MeaRiver\n3 11345 N.Jones Feelin’ theSame Way\n4 12212 D.Krall Guess I’ll Hang My Tears OutToDry\n5 12333 J.Coltrane Village blues\n6 13015 D.Krall Every TimeWeSay Goodbye\n7 13201 D.Krall TheNight weCalled itaDay\n8 13210 N.Jones Don’tKnow Why\n9 13401 D.Krall I Remember You\n10 13458 D.Krall Walk On By\n11 13758 D.Krall I’veGrown Accustomed ToYour Face\n12 13852 S.Vaughan Prelude to aKiss\n13 13915 D.Krall TooMarvelous For Words\n14 13969 D.Krall TheBoy from Ipanema\n15 14099 N.Jones Lonestar\n16 14114 C.Baker My Funny Valentine\n17 14405 D.Krall TheLook of Love\n18 14674 N.Jones Lonestar (other version)\n19 15039 D.Krall EsteSeu Olhar\nTable 3. Playlist generatedfrom“ComeAwayWith Me”\nTheplaylistgeneratedstartingwiththeseedsong“Blue\nTrain”byJohnColtrane(Table4)ischaracterizedbySax-\nophone solos and trumpet. Excluding the songs from the\nsame album, the songsfoundin the playlist are performed\nby Miles Davis, Dizzy Gillespie whose trumpets are as-\nsimilated with saxophone and Ella Fitzgerald and Frank\nSinatra whoare accompaniedbya strongset of copperin-\nstruments.\n3.4 OtherApproaches\n3.4.1 Usingunigramsandbigrams\nOur classiﬁcation method is based on models of bigram\nprobabilitieswhereasmost of previousapproachesrelyon\nthe classiﬁcation of frame-based feature vectors or on es-\ntimates of statistical moments of those features computed\nonwidertemporalwindows. Inordertoquantifythebene-\nﬁt of taking into account transition probabilitiesan hybri dDist. Artist Song\n0 0 J.Coltrane Blue Train\n1 11367 J.Coltrane Moment’s Notice\n2 14422 J.Coltrane Lazy Bird\n3 17344 J.Coltrane Locomotion\n4 23418 E.Fitzgerald ItAin’tNecessarily So\n5 25006 E.Fitzgerald IGotPlenty o’ Nuttin’\n6 25818 F.Sinatra I’veGotYou Under My Skin\n7 27054 M.Davis SoWhat\n8 27510 M.Davis Freddie Freeloader\n9 28230 E.Fitzgerald Womanis aSometime Thing\n10 28598 S.Vaughan Jim\n11 28756 F.Sinatra Pennies FromHeaven\n12 29204 D.Gillespie November Afternoon\n13 30299 M.Davis Bess oh Where’s myBess\n14 31796 F.Sinatra TheWay You Look Tonight\n15 31971 E.Fitzgerald There’s aBoat Dat’s Leavin’ Soon for N Y\n16 32129 E.Fitzgerald Dream ALittle Dream of Me\n17 32232 J.Coltrane I’mOld Fashioned\n18 32505 E.Fitzgerald Basin’ Street Blues\n19 34045 M.Davis AllBlues\nTable 4. Playlist generatedfrom“Blue Train”\napproach was implemented. With this approach, the clas-\nsiﬁcationofasequencedependsonalinearcombinationof\nunigrams and bigrams. If we consider only unigrams, the\nscoreofa sequenceossymbols si=1..nis:\nS′\nM(si=1..n) = log ( PM(si=1..n)) =n/summationdisplay\ni=1log (PM(si))\nUsing the score computed for bigrams (see equation 4), a\nlinearcombinationcanbewrittemas:\nS′′\nM(si=1..n) =αS′\nM(si=1..n)+(1−α)SM(si=1..n)(6)\nwhere α∈[0,1]. This approachwas experimentedon the\nISMIR 2004 dataset. The results are shown in the follow-\ningtable:\nα 1.0 0.5 0.0\naccuracy 71.88% 77.64% 81.89%\nWhen α= 1,onlyunigramsaretakenintoaccountwhereas\nα= 0reverts to the case where only bigrams are con-\nsidered. As we can see in this table, the introduction of\nunigrams in the classiﬁcation process in not beneﬁcial. A\ncloser look at unigram probabilities give an explaination\nto these observations. The following table show, for each\nclass, the number of clusters were the class is most repre-\nsented, the averageprobability(andstandarddeviation)o f\nobservingthe class Mgivena symbol s,(P(M|si)).\nCl. El. JB MP RP Wo.\n#C 73 68 4 9 16 30\nP(M|s)0.599 0.503 0.578 0.423 0.409 0.471\nstd.dev. 0.194 0.162 0.180 0.063 0.103 0.139\nOne can see that for three classes this average probabil-\nity is below 0.5 i.e. most symbols represents a mixture of\ntimbres. Thisexplainswhyunigramprobabilitiesarenota\ngoodindicatoroftheclass.\n3.4.2 HiddenMarkovModels\nWeimplementedanothertechniquecommonlyusedtomodel\ntime-varyingprocesses,theHiddenMarkovModels(HMMs).\nThese models were tested on the genre classiﬁcation task\nwith the ISMIR 2004 genre dataset. The same (discrete)\nsequencesusedtotrainthelanguagemodelswerealsoused\n85Poster Session 1\nDK SV DE TM CB MD CJ NS JC FS LY OP EF AD BH AT NJ\nDK14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3\nSV0 9 0 0 1 0 0 0 0 0 0 1 1 5 0 0 0\nDE0 0 5 0 0 1 0 0 0 0 0 0 0 1 0 0 0\nTM 0 0 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0\nCB0 2 0 0 20 1 1 1 0 2 0 0 0 0 0 0 0\nMD 0 2 0 0 1 14 1 0 0 1 0 0 0 0 0 0 0\nCJ0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0\nNS0 1 0 0 1 0 0 9 0 0 1 0 1 0 0 0 0\nJC0 0 0 0 1 2 0 0 2 0 0 0 1 0 0 0 0\nFS0 0 0 0 0 0 0 0 0 20 0 0 3 0 0 0 0\nLY0 1 0 0 4 0 0 0 0 0 11 2 1 1 1 0 0\nOP0 1 0 0 0 0 0 1 0 0 1 11 0 0 0 0 0\nEF0 4 0 0 0 0 0 0 0 0 2 0 9 2 0 0 0\nAD0 0 0 0 1 0 0 0 0 1 0 0 3 15 0 0 0\nBH0 0 0 0 0 0 0 0 0 0 0 0 0 0 17 0 0\nAT0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 18 0\nNJ1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 16\nTable 2. Confusionmatrixobtainedwiththe Jazz17 dataset.\nin the HMM’s training. For classiﬁcation, we calculated\ntheprobabilitiesofagivensequencewiththeHMM’strained\nfor different genres, and assigned the music to the genre\nwith thehighestprobability.\nWe used left-rightmodelswith 2,3and4delays, anda\nfully connected model. We also tested these models with\n10and20hidden states. The results, shown in the fol-\nlowing table, indicate that the performance of the HMMs\nisworsethanourmethod. Nevertheless,itshouldbenoted\nthatinourapproach,weneedasigniﬁcantnumberofstates\n(between 100 and 400) in order to achieve reasonable ac-\ncuracy in timbre modeling. To train an HMM with such a\nnumber of hidden states would require a huge amount of\ndatain orderforthemodeltoconverge.\nHMM LR-2 LR-3 LR-4 FC\n10states68.3% 69.3% 68.7% 69.1%\n20states69.1% 69.8% 69.5% 69.5%\n4. CONCLUSION ANDFUTUREWORK\nWe described a method5for the classiﬁcation of music\nsignals that consists in a two-stage clustering of MFCC\nframes followed by a vector quantization and a classiﬁca-\ntionschemebasedonlanguagemodeling. We veriﬁedthat\nthemethodwassuitableforproblemswithdifferentscales:\nGenre Classiﬁcation, Artist Identiﬁcation and computing\nofadistancebetweenmusicpieces. Thedistancemeasure,\nused on a set of songs belonging to a single genre (Jazz),\nallowedustoderiveconsistentplaylists. Theproposedap-\nproachwascomparedwithanHMM-basedapproachanda\nmethodthatinvolvesalinearcombinationofunigramsand\nbigram. On-going work include testing approaches based\noncompressiontechniquesforsymbolicstrings.\n5. REFERENCES\n[1] J.-J. Aucouturier and F. Pachet, “Music similarity meas ures:\nWhat’s the use?” in ISMIR,France, October 2002.\n[2] A. Berenzweig, B. Logan, D. Ellis, and B. Whitman, “A\nlarge-scale evaluation of acoustic and subjective music si mi-\nlaritymeasures,” Computer MusicJournal , vol.28,no.2,pp.\n63–76, 2004.\n5Thiswork waspartially supported byFCT,through theMulti- annual\nFunding Programme.[3] B.LoganandA.Salomon,“Amusicsimilarityfunctionbas ed\non signal analysis,” in ICME,2001.\n[4] K. West and P. Lamere, “A model-based approach to con-\nstructingmusicsimilarityfunctions,” Journal onAdvancesin\nSignal Processing , 2007.\n[5] E. Pampalk, A. Flexer, and G. Widmer, “Improvements of\naudio-based music similarityand genre classiﬁcation,” in IS-\nMIR, 2005.\n[6] G. Tzanetakis and P. Cook, “Musical genre classiﬁcation of\naudiosingals,” IEEETrans.onSpeechandAudioProcessing ,\nvol. 10, no. 5, pp. 293–302, 2002.\n[7] T. Lidy and A. Rauber, “Evaluation of feature extractors and\npsycho-acoustic transformations for music genre classiﬁc a-\ntion,” inISMIR,2005, pp. 34–41.\n[8] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B. K´ egl ,\n“Aggregate features and AdaBoost for music classiﬁcation, ”\nMachine Learning , vol.65, no. 2-3, pp. 473–484, 2006.\n[9] H. Soltau, T. Schultz, M. Westphal, and A. Waibel, “Recog -\nnition ofmusic types,” in ICASSP,1998.\n[10] K. Chen, S. Gao, Y. Zhu, and Q. Sun, “Music genres classi-\nﬁcation using text categorization method,” in MMSP, 2006,\npp. 221–224.\n[11] M. Li and R. Sleep, “A robust approach to sequence classi ﬁ-\ncation,” in ICTAI,2005.\n[12] J.-J. Aucouturier, F. Pachet, and M. Sandler, “The way i t\nsounds: Timbre models for analysis and retrieval of poly-\nphonic music signals,” IEEE Transactions of Multimedia ,\nno. 6,pp. 1028 – 1035, 2005.\n[13] C. Manning and H.Schutze, Foundations ofStatistical Natu-\nral Language Processing . MIT Press,2002.\n[14] P. Annesi, R. Basili, R. Gitto, A. Moschitti, and R. Peti tti,\n“Audio feature engineering for automatic music genre class i-\nﬁcation,” in RIAO,Pittsburgh,2007.\n[15] D. Ellis, “Classifying music audio with timbral and chr oma\nfeatures,” in ISMIR,2007.\n[16] Y. Kim,D.Williamson,and S.Pilli,“Towards understan ding\nand quantifying the ”album effect” inartistidentiﬁcation ,” in\nISMIR, 2006.\n86"
    },
    {
        "title": "Music Mood Representations from Social Tags.",
        "author": [
            "Cyril Laurier",
            "Mohamed Sordo",
            "Joan Serrà",
            "Perfecto Herrera"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415600",
        "url": "https://doi.org/10.5281/zenodo.1415600",
        "ee": "https://zenodo.org/records/1415600/files/LaurierSSH09.pdf",
        "abstract": "This paper presents findings about mood representations. We aim to analyze how do people tag music by mood, to create representations based on this data and to study the agreement between experts and a large community. For this purpose, we create a semantic mood space from last.fm tags using Latent Semantic Analysis. With an unsupervised clustering approach, we derive from this space an ideal categorical representation. We compare our community based semantic space with expert representations from Hevner and the clusters from the MIREX Audio Mood Classification task. Using dimensional reduction with a Self-Organizing Map, we obtain a 2D representation that we compare with the dimensional model from Russell. We present as well a tree diagram of the mood tags obtained with a hierarchical clustering approach. All these results show a consistency between the community and the experts as well as some limitations of current expert models. This study demonstrates a particular relevancy of the basic emotions model with four mood clusters that can be summarized as: happy, sad, angry and tender. This outcome can help to create better ground truth and to provide more realistic mood classification algorithms. Furthermore, this method can be applied to other types of representations to build better computational models.",
        "zenodo_id": 1415600,
        "dblp_key": "conf/ismir/LaurierSSH09",
        "keywords": [
            "Latent Semantic Analysis",
            "semantic mood space",
            "music by mood",
            "expert representations",
            "MIREX Audio Mood Classification task",
            "Self-Organizing Map",
            "dimensional model",
            "tree diagram",
            "ground truth",
            "computational models"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMUSIC MOOD REPRESENTATIONS FROM SOCIAL TAGS\nCyril Laurier, Mohamed Sordo, Joan Serr `a, Perfecto Herrera\nMusic Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n{cyril.laurier,mohamed.sordo,joan.serraj,perfecto.herrera }@upf.edu\nABSTRACT\nThis paper presents ﬁndings about mood representations.\nWe aim to analyze how do people tag music by mood, to\ncreate representations based on this data and to study the\nagreement between experts and a large community. For\nthis purpose, we create a semantic mood space from last.fm\ntags using Latent Semantic Analysis. With an unsuper-\nvised clustering approach, we derive from this space an\nideal categorical representation. We compare our commu-\nnity based semantic space with expert representations from\nHevner and the clusters from the MIREX Audio Mood\nClassiﬁcation task. Using dimensional reduction with a\nSelf-Organizing Map, we obtain a 2D representation that\nwe compare with the dimensional model from Russell. We\npresent as well a tree diagram of the mood tags obtained\nwith a hierarchical clustering approach. All these results\nshow a consistency between the community and the ex-\nperts as well as some limitations of current expert models.\nThis study demonstrates a particular relevancy of the basic\nemotions model with four mood clusters that can be sum-\nmarized as: happy ,sad,angry andtender . This outcome\ncan help to create better ground truth and to provide more\nrealistic mood classiﬁcation algorithms. Furthermore, this\nmethod can be applied to other types of representations to\nbuild better computational models.\n1. INTRODUCTION\nMusic classiﬁcation by mood1recently emerged as a topic\nof interest in the Music Information Retrieval (MIR) com-\nmunity. The ﬁrst task to tackle this problem is to ﬁnd a\nrelevant representation of mood. In this work, we study\nmood representations with a bottom-up approach, from a\ncommunity point of view.\nSeveral works have shown a potential to model mood in\nmusic (like [3–5] , see [6] for an extensive review). Al-\nthough this task is quite complex, satisfying results can\nbe achieved, especially if we concentrate on the mood ex-\npressed by the music rather than the mood induced [6].\n1In order to simplify the terminology, we will use the words emotion\nand mood independently for the same meaning: a particular feeling char-\nacterizing a state of mind\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.However, almost every work differs in the way that it rep-\nresents emotions. Similarly to psychological studies, there\nis no real agreement on a common model. Comparing\nthese different techniques is a very arduous task. With the\nobjective to evaluate several algorithms within the same\nframework, MIREX (Music Information Retrieval Evalu-\nation eXchange) [7] organized a task on this topic for the\nﬁrst time in 2007. To do so, it was decided to frame the\nproblem into a classiﬁcation task with 5 mutually exclu-\nsive categories. However, it was shown that these clusters\nmight not be optimal as we suspect some semantic over-\nlap between categories [8]. In a nutshell, ﬁnding the right\nmood representation is complex.\nIn this study, we want to address this problem using data\ncollected in an ”everyday life” context (not in controlled\nlaboratory settings like in psychological studies). From\nthis data, we want to create a semantic space for mood.\nIn [10], the authors studied the agreement between experts\nand a community (also based on last.fm tags) for genre\nclassiﬁcation. Levy in [11], studied how tags can be used\nfor genre and artist similarity and proposed a visualization\nof certain words in an emotion space. Both studies inspired\nour approach of using social tags to compare the semantics\nof the wisdom of crowds with expert knowledge.\nThe goal of this paper is to create a semantic mood\nspace where we can represent mood and compare it with\nexisting representations. There are two main motivations\nfor this study. First we aim to verify if the knowledge ex-\ntracted from social tags and the knowledge from the ex-\nperts (psychologists) converges. Then, we want to generate\nmood representations that can serve as a basis for further\nworks like music mood classiﬁcation. In Section 2, we\nexpose the expert mood representations. In Section 3, we\ndetail the dataset of tags and then, in Section 4, its trans-\nformation into a semantic space. In Section 5, we study\nthe categorical representations. In Sections 6 and 7, we\ngenerate and analyze dimensional and hierarchical repre-\nsentations. Finally, Section 8 concludes and summarizes\nthe main ﬁndings.\n2. EXPERT REPRESENTATIONS\nTwo main types of representation coexist in the literature.\nThe ﬁrst one is the categorical model, using for instance\nbasic emotions with around four or ﬁve categories includ-\ning:happiness ,sadness ,fear,anger and tenderness [1].\nSome works propose mood clusters like the eight clusters\nfrom Hevner [9] (see Figure 1) or the ﬁve clusters used\nin the MIREX Audio Mood Classiﬁcation task, detailed\n381Oral Session 5: Tags\nClusters Mood Adjectives\nCluster 1 passionate, rousing, conﬁdent, boisterous, rowdy\nCluster 2 rollicking, cheerful, fun, sweet, amiable/good natured\nCluster 3 literate, poignant, wistful, bittersweet, autumnal, brooding\nCluster 4 humorous, silly, campy, quirky, whimsical, witty, wry\nCluster 5 aggressive, ﬁery, tense/anxious, intense, volatile, visceral\nTable 1 . Clusters of mood adjectives used in the MIREX\nAudio Mood Classiﬁcation task.\nin Table 1. The second type of representation is the di-\nmensional model, based originally on Russell’s circumplex\nmodel of affect [2] (see Figure 2). The two dimensions\nmostly used are arousal and valence2.\nFigure 1 . Hevner’s [9] model with adjectives grouped into\neight clusters.\n3. DATASET\nOur objective is to obtain a mood space based on social\ntags. In order to achieve this goal, we need two compo-\nnents: a list of mood words and social network data.\n3.1 Mood list\nFor this study, we want to observe the way people use\nmood words in a social network. We selected words re-\nlated to emotions based on the main articles in music and\nemotion research. We included words from different psy-\nchological studies like Hevner [9] or Russell [2]. We also\nadded words representing basic emotions and other related\nadjectives [1]. Finally we aggregated the mood terms mostly\nused in MIR [6] and the ones selected for the MIREX\ntask [8]. At the end of this process, we obtained a list of\n120 mood words.\n2In psychology, the term valence describes the attractiveness or aver-\nsiveness of an event, object or situation.\nFigure 2 . Russell’s [2] circumplex model of affect with\narousal and valence dimensions.\n3.2 Social Tags\nLast.fm3is a music recommendation website with a large\ncommunity of users who are very active in associating tags\nwith the music they listen to. With over 30 million users\nin more than 200 countries4, this social network is a good\ncandidate to study how people tag their music. We crawled\n6,814,068 tag annotations from 575,149 tracks in all main\ngenres. From those, 492,634 tags were distinct. This huge\ndataset contains tags of any kind. From the original 120\nmood words, 107 tags were found in our dataset. However\nsome of them did not appear very often. We decided to\nkeep only the tags that appeared at least 100 times, result-\ning in a list of 80 words. We also chose to keep the tracks\nwere the same mood tag has been used by several users.\nThis subset contains 61,080 tracks. We observe that the\nmood tags mostly used are sad,fun,melancholy andhappy .\nFor instance, the tag sadhas been used 11,898 times in our\ndataset. On the contrary, the least used tags are rollicking ,\nsolemn ,rowdy andtense , applied in less than 150 tracks.\nIn average, a mood tag is used in 754 tracks.\n4. SEMANTIC MOOD SPACE\nWe aim to compare mood terms by their co-occurences in\ntracks. Intuitively happy should co-occur more often with\nfunorjoythan with sadordepressed . This co-occurence\ninformation included in the data we crawled from last.fm\nis embodied in a document-term matrix where the columns\nare track vectors representing tags.\nThe main problem we have when dealing with this ma-\ntrix is its high dimensionality and its sparsity. Consequently,\nwe applied a Latent Semantic Analysis (LSA) [12] to project\nthe data into a space of a given lower dimensionality, while\nmaintaining a good approximation of the distances between\ndata points. This technique has been shown to be very ef-\nﬁcient to capture tag representations for genre and artists\n3http://www.last.fm\n4http://blog.last.fm/2009/03/24/lastfm-radio-announcement\n38210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nsimilarity [11]. LSA makes use of algebraic techniques\nsuch as Singular Values Decomposition (SVD) to reduce\nthe dimensionality of the matrix. We decided to use a di-\nmension of 100, which seems to be good trade-off for sim-\nilarity tasks [11]. In the following experiments, we tried to\nchange this dimension parameter (from 10 to 10 000 on a\nlogarithmic scale), with no signiﬁcant impact on the out-\ncomes except less relevant results when selecting a too low\nor too high dimension. Once we have the data into this se-\nmantic space, we compute the distance between terms us-\ning the cosine distance. The distance values are included\nin the range [0,1]. Here are some examples of distances\nbetween mood tags:\ndcos(happy, sad ) = 0.99\ndcos(cheerful, sleepy ) = 0.97\ndcos(anger, aggressive ) = 0.06\ndcos(calm, relaxed ) = 0.03\nWe observe that happy andsadare quite far from each\nother, as well as cheerful andsleepy . On the other hand,\nwe note that anger is close to aggressive and that calm is\nsimilar to relaxed . Even if we show here some prototypical\nexamples, values in the whole distance matrix intuitively\nmake sense.\n5. CATEGORICAL REPRESENTATIONS\nTo study the categorical mood representations, we ﬁrst de-\nrive a folksonomy (community-based taxonomy) represen-\ntation by means of unsupervised clustering from the social\ndata. Then, we evaluate how the expert taxonomies ﬁt into\nthe semantic mood space.\n5.1 Folksonomy representation\nFrom our semantic space, we want to infer what would be\nthe ideal categorical representation. To achieve this goal,\nwe apply an unsupervised clustering method using the Ex-\npectation maximization (EM) algorithm. This algorithm\nand the implementation we used (WEKA) are described\nin [13]. The ﬁrst important question to be answered is\nhow many clusters should we consider. As we want this\nnumber to be inferred by the data itself, we used the v-\nfold cross validation algorithm. We divided the dataset in\nvfolds, training on v−1folds and testing on the remain-\ning one. We measure the log-likelihood computed for the\nobservations in the testing samples. The results for the v\nreplications are averaged to yield a single measure of the\nstability of the model. In Figure 3, we show the results of\nthis process, displaying an average cost value (in our case\n2 times the negative log-likelihood of the cross-validation\ndata). Intuitively the lower is the value, the better is the\ncluster. To choose the ”right” number of clusters, we look\nat the cost value while increasing the number of clusters.\nPractically, we stop when the mean cost value stops de-\ncreasing and select the current number of clusters.\nWe observe that the cost rapidly decreases with the num-\nber of clusters until four clusters. After that, it is stable and\nFigure 3 . Plot of the cost values (2 times the negative log-\nlikelihood) depending on the number of clusters.\neven increases, meaning that the data is overﬁtted. Conse-\nquently, the optimal number of clusters is four. Using this\nnumber for the EM algorithm, we obtained the clusters ex-\nposed in Table 2.\nCluster 1 Cluster 2 Cluster 3 Cluster 4\nangry sad tender happy\naggressive bittersweet soothing joyous\nvisceral sentimental sleepy bright\nrousing tragic tranquil cheerful\nintense depressing good natured happiness\nconﬁdent sadness quiet humorous\nanger spooky calm gay\nexciting gloomy serene amiable\nmartial sweet relax merry\ntense mysterious dreamy rollicking\nanxious mournful delicate campy\npassionate poignant longing light\nquirky lyrical spiritual silly\nwry miserable wistful boisterous\nﬁery yearning relaxed fun\nTable 2 . Folksonomy representation. Clusters of mood\ntags obtained with the EM algorithm. For space and clarity\nreasons, we show only the ﬁrst tags.\nThese four clusters are very similar to the categories\nposed by the main basic emotion theories [1]. Moreover,\nthese clusters represents the four quadrants of the classi-\ncal arousal-valence plane from Russell previously shown\nin Figure 2:\nCluster 1: angry (high arousal, low valence)\nCluster 2: sad, depressing (low valence, low arousal)\nCluster 3: tender, calm (high valence, low arousal)\nCluster 4: happy (high arousal, high valence)\nTo summarize, the semantic space we created is rele-\nvant and coherent with existing basic emotion approaches.\n383Oral Session 5: Tags\nThis result is very encouraging and assesses a certain qual-\nity of this semantic space. Moreover, it conﬁrms that the\ncommunity uses mood tags in a way that converges with\nthe basic emotion theory from psychology.\n5.2 Agreement between experts and community\nIn this section, we want to measure the agreement between\nexperts and community representations. To do so, we per-\nformed a coarse-grained similarity, where we measured\nhow separable the expert-deﬁned mood clusters are in our\nsemantic space. First, we computed the LSA cosine sim-\nilarity among all moods within each cluster (intra-cluster\nsimilarity) and then we computed the dissimilarity among\nclusters, using the centroid of each cluster (inter-cluster\ndissimilarity). The expert representations we selected for\nthis experiment are the eight clusters from Hevner (see\nFigure 1) where we could match more than 50% of the tags\nand the ﬁve clusters from the MIREX taxonomy (see Table\n1) where all 31 tags were matched.\n5.2.1 Intra-cluster similarity\nFor each cluster of the expert representations, we com-\npute the mean cosine similarity between each mood tag\nin the cluster. The results for intra-cluster similarity are\npresented in Figure 4 for the Hevner representation and in\nFigure 5 for the MIREX clusters.\nFigure 4 . Intra-cluster cosine similarity for Hevner’s rep-\nresentation.\nIn the results for the Hevner clusters, we note a high\nintra-cluster similarity value for cluster 1, which is the one\nincluding spiritual andsacred (please look at Figure 1 for\nthe complete list). Cluster 6 performs also quite well ( joy-\nous,bright ,gay,cheerful ,merry ). However we have poor\nintra-cluster similarity for cluster 8, which includes vigor-\nous,martial and majestic . This might be because these\nwords are also some of the less used in our dataset, but\nwe hypothesize that they are less descriptive today than\nwhen the taxonomy was created (1936). Moreover, these\nwords were selected for classical music which is not the\nmain content of the lasf.fm music. The rest of the intra-\ncluster similarity values are in average quite low, meaningthat this representation is not optimal in the semantic mood\nspace.\nFigure 5 . Intra-cluster cosine similarity for MIREX repre-\nsentation.\nFor the MIREX clusters, we remark that the lowest intra-\ncluster similarity is for cluster 2 ( sweet ,good natured ,cheer-\nful,rollicking ,amiable ,fun). Maybe is it quite clear that\nthis category is about happy music, however the words\nused are not so common and may lower this value. In\naverage, the intra-cluster similarity value is quite high for\nthis representation. For comparison purpose, we note that\nthe intra-cluster similarity of the folksonomy representa-\ntion has an average intra-cluster similarity value of 0.82\n(see Table 4). Obviously, as the folksonomy representa-\ntion was made from the semantic space itself, it has better\nresults than the other models.\nIn this part, we have looked at the consistency inside\neach cluster, however it is also crucial to look at the dis-\ntances between clusters to evaluate the quality of the clus-\ntering representations.\n5.2.2 Inter-cluster dissimilarity\nTo measure how separable are the different clusters, we\ncompute the mean cosine distance from each cluster cen-\ntroid to the other cluster centroids. If we look at our folk-\nsonomy representation clusters from Section 5.1, the co-\nsine distance between centroids of clusters are all quite\nhigh (0.9 in average, see Table 4). This is not very supris-\ning as the representation was designed with this data.\nIn Table 3, we show the confusion matrix of the inter-\ncluster dissimilarity for the MIREX clusters. We notice\nthat the lowest value is between cluster 1 and cluster 5,\nmeaning that these clusters are quite similar. This ﬁnding\ncorrelates with the results from the MIREX task, in which\nthe confusion between these two classes was found signif-\nicant [8]. However the confusion between clusters 2 and\n4, also relevant in the MIREX results analysis, is not re-\nﬂected here. Additionally, we observe that the most sepa-\nrated clusters (5 and 2), are also the less confusing in the\nMIREX results. Looking at the confusion matrix for the\n38410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nC1 C2 C3 C4 C5\nC1 0 0.74 0.128 0.204 0.108*\nC2 0.74 0 0.859 0.816 0.876\nC3 0.128 0.859 0 0.319 0.265\nC4 0.204 0.816 0.319 0 0.526\nC5 0.108* 0.876 0.265 0.526 0\nTable 3 . Confusion matrix for the inter-cluster dissimilar-\nity for the MIREX clusters (C1 means cluster 1, C2 cluster\n2 and so on). The values marked with an asterisk are the\nmost similar and in bold are the less similar values.\nHevner clusters (not shown here for space reasons), we re-\nmark that the highest values (dissimilarity above 0.95) are\nbetween clusters 7 and 8, and between clusters 1 and 2. On\nthe contrary, the lowest value (0.09) is between clusters 1\nand 4. Indeed both clusters have words than can appear\nsimilar like spiritual andserene for instance. We summa-\nrize the results of both intra and inter-cluster measures for\nthe different taxonomies in Table 4.\nMood Taxonomy Intra-cluster Inter-cluster\nsimilarity dissimilarity\nHevner 0.55 0.70\nMIREX 0.73 0.56\nFolksonomy 0.82 0.9\nTable 4 . Intra-cluster similarity and inter-cluster dissimi-\nlarity means for each mood taxonomy.\nIn a nutshell, the Hevner clusters are less consistent but\nare more separated than the MIREX ones. Indeed, even if\nthe latter has more intra-cluster similarity, it suffers from\nconfusions between some categories as reﬂected in our re-\nsults.\n6. DIMENSIONAL REPRESENTATION\nDimensional representation is an important paradigm in\nemotion studies. To project our semantic mood space into\na bi-dimensional space, we used the Self-Organizing Map\nalgorithm (SOM). We decided to use SOM for its topology\nproperties and because it stresses more on the local simi-\nlarities and distinguishes groups within the data. Because\nless than half of the Russell’s adjectives are present in our\ndataset, we prefer to compare qualitatively more that quan-\ntitatively the expert and the community models. We trained\na SOM and mapped each tag onto its best-matching unit in\nthe trained SOM. In Figure 6, we plot the resulting orga-\nnization of mood tags (for clarity reasons, we show here a\nsubset of 58 tags).\nWe observe in the 2D projection four main parts. At the\ntop-left, terms related to aggressive , below calm and other\nsimilar words, at the top-right tags related to sadand be-\nlow words close to happiness . We notice the four clusters\ncorresponding to the basic emotions and our folksonomy\nrepresentation mentioned in Section 5.1. This is somehow\n aggressive aggressive rowdy intense bittersweet boisterous boisterous autumnal plaintive majestic sad rousing rousing ﬁery amiable dreamy tragic gloomy spooky anxious dramatic depressing depressing whimsical playful whimsical playful relaxed sleepy humorous silly humorous silly quirky spiritual tranquil tender joyous bright gay joyous bright gay scary light cheerful merry dark funcalmsoothingserenequietlonging mysterious relaxrollickinghappinesscampymelancholy  sentimentalsadness  patheticlyricalpassionate witty wryangry angerFigure 6 . Self-Organizing Map of the mood tags in the\nsemantic space.\nexpected as we already got these clusters from this data.\nHowever, having the same results with a second technique\nconﬁrms our ﬁndings. Comparing with Russell’s dimen-\nsions, we ﬁnd that the diagonal from top-left to bottom-\nright is of high arousal. On the contrary, the diagonal from\ntop-right to bottom-left is of low arousal. The vertical axis\nrepresents the valence dimension. Even though the 2D rep-\nresentation is not equal, there is a clear correlation between\nthe community and the experts when framing the problem\ninto two dimensions.\n7. HIERARCHICAL REPRESENTATION\nThe semantic mood space can be visualized in many differ-\nent ways. In this part we experimented hierarchical clus-\ntering techniques to produce a tree diagram (dendrogram).\nWe applied a common agglomerative hierarchical cluster-\ning method with a complete linkage [14] and the cosine\nmetric. We used the hcluster5implementation. With the\n20 most used tags in our dataset, we computed the cluster-\ning and plot the resulting dendrogram in Figure 7 .\nAlthough there exists some dendrogram representation\nof emotions in the psychology literature [1], the compar-\nison is complex because many of the terms employed are\nnot present in our dataset and also because ﬁnding the right\nmetric to measure the similarity between both is not triv-\nial. The hierarchical clustering starts with two branches.\nLooking at the tags of this ﬁrst branching, we observe a\nvery clear separation in arousal with dreamy andcalm on\nthe left and angry andhappy on the right. Then the two fol-\nlowing branching (resulting in four clusters) represents the\nfour basic emotions also found as the best categorical rep-\nresentation in Section 5 (in order in the dendrogram: calm ,\nsad,angry andhappy ). This conﬁrms another time our\nﬁndings about the relevancy of these four clusters. More-\nover, we notice that the ﬁrst separation is related to arousal,\noften considered as the most important dimension. The re-\nmaining branches group together similar terms like angry\nandaggressive orsadanddepressing .\n5http://code.google.com/p/scipy- cluster\n385Oral Session 5: Tags\ndreamycalmsoothingsleepyquietsweetquirkysaddepressingmelancholybittersweetsentimentalintenseheavyangryaggressivehappyplayfulfungay\nFigure 7 . Dendrogram of the 20 most used tags.\n8. CONCLUSIONS\nThis paper presented convergent evidence about mood rep-\nresentations. We created a semantic mood space based\non a community of users from last.fm . We derived dif-\nferent representations from this data and compared them\nto the expert representations. We demonstrated that the\nbasic emotions: happy ,sad,angry andtender , are very\nrelevant to the social network. We also found that the\narousal and valence dimensions are pertinent. Moreover\nwe have shown that both Hevner’s and MIREX represen-\ntations have advantages and limitations when evaluated in\nthe semantic mood space. The former having better sep-\narated clusters and the latter having more consistent clus-\nters. Observations on the confusion and similarity between\nMIREX clusters conﬁrmed results from previous analysis.\nWe also presented a dendrogram visualization validating\nagain the basic emotion point of view and offering a new\nrepresentation of the mood space. All these ﬁndings show\nthe relevancy of using a mood semantic space derived from\nsocial tags. Folksonomy representations can be used in\ntasks like mood classiﬁcation or regression to improve the\nquality of the audio content processing algorithms. We\ncan also imagine a visualization of a user emotional states\nbased on his listening habits or history. Moreover, one’s\nmusical library can be mapped and explored with a folk-\nsonomy representation derived from the whole social net-\nwork or a particular subset. Finally this approach can be\ngeneralized to ﬁnd other domain-speciﬁc representations.\n9. ACKNOWLEDGMENTS\nWe want to thank the people from the Music Technology\nGroup (Universitat Pompeu Fabra, Barcelona). Data from\nthis work is available at: http://mtg.upf.edu/people/claurier\nThis research has been partially funded by the EU Project\nPHAROS IST-2006-045035.10. REFERENCES\n[1] P. N. Juslin and J. A. Sloboda: Music and Emotion:\nTheory and Research , Oxford University Press, 2001.\n[2] J. A. Russell: “A circumplex model of affect,” Jour-\nnal of Personality and Social Psychology , No. 39,\npp. 1161, 1980.\n[3] T. Li and M. Ogihara: “Detecting emotion in music,”\nProceedings of ISMIR, Baltimore, MD, USA , pp. 239–\n240, 2003.\n[4] Y . H. Yang, Y . C. Lin, Y . F. Su, and H. H. Chen:\n“A regression approach to music emotion recognition,”\nIEEE Transactions on audio, speech, and language\nprocessing , V ol. 16, No. 2, pp. 448–457, 2008.\n[5] C. Laurier, O. Meyers, J. Serr `a, M. Blech, P. Herrera:\n“Music Mood Annotator Design and Integration,” 7th\nInternational Workshop on Content-Based Multimedia\nIndexing, Chania, Crete , 2009.\n[6] C. Laurier, P. Herrera: “Automatic Detection of Emo-\ntion in Music: Interaction with Emotionally Sensitive\nMachines,” Handbook of Research on Synthetic Emo-\ntions and Sociable Robotics: New Applications in Af-\nfective Computing and Artiﬁcial Intelligence ,Chap. 2,\npp. 9–32, IGI Global, 2009.\n[7] J. S. Downie: “The music information retrieval evalu-\nation exchange (2005-2007): A window into music in-\nformation retrieval research,” Acoustical Science and\nTechnology , V ol. 29, No. 4, pp. 247–255, 2008.\n[8] X. Hu, S. J. Downie, C. Laurier, M. Bay, and A. F.\nEhmann: “The 2007 MIREX audio mood classiﬁca-\ntion task: Lessons learned,” Proceedings of ISMIR,\nPhiladelphia, PA, USA , pp. 462–467, 2008.\n[9] K. Hevner: “Experimental studies of the elements of\nexpression im music,” The American Journal of Psy-\nchology , V ol. 48, No. 2, pp. 246–268, 1936.\n[10] M. Sordo, O. Celma, M. Blech, and E. Guaus: “The\nQuest for Musical Genres: Do the Experts and the\nWisdom of Crowds Agree?,” Proceedings of ISMIR,\nPhiladelphia, PA, USA , pp. 255–260, 2008.\n[11] M. Levy and M. Sandler: “A Semantic Space for Mu-\nsic Derived from Social Tags,” Proceedings of ISMIR,\nVienna, Austria , 2007.\n[12] S. Deerwester, S. Dumais, G. W. Furnas, T. K. Lan-\ndauer, and R. Harshman : “Indexing by latent semantic\nanalysis.,” Journal of the Society for Information Sci-\nence, V ol. 14, pp. 391–407, 1990.\n[13] I. H. Witten and E. Frank: Data Mining: Practical Ma-\nchine Learning Tools and Techniques with Java Imple-\nmentations , Morgan Kaufmann, 1999.\n[14] R. Xu and D. C. Wunsch: Clustering , IEEE Press, 2009\n386"
    },
    {
        "title": "Evaluation of Algorithms Using Games: The Case of Music Tagging.",
        "author": [
            "Edith Law",
            "Kris West",
            "Michael I. Mandel",
            "Mert Bay",
            "J. Stephen Downie"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417647",
        "url": "https://doi.org/10.5281/zenodo.1417647",
        "ee": "https://zenodo.org/records/1417647/files/LawWMBD09.pdf",
        "abstract": "Search by keyword is an extremely popular method for retrieving music. To support this, novel algorithms that automatically tag music are being developed. The conventional way to evaluate audio tagging algorithms is to compute measures of agreement between the output and the ground truth set. In this work, we introduce a new method for evaluating audio tagging algorithms on a large scale by collecting set-level judgments from players of a human computation game called TagATune. We present the design and preliminary results of an experiment comparing five algorithms using this new evaluation metric, and contrast the results with those obtained by applying several conventional agreement-based evaluation metrics.",
        "zenodo_id": 1417647,
        "dblp_key": "conf/ismir/LawWMBD09",
        "keywords": [
            "keyword search",
            "music retrieval",
            "novel algorithms",
            "automatic tagging",
            "conventional evaluation",
            "human computation game",
            "TagATune",
            "set-level judgments",
            "player evaluation",
            "five algorithms comparison"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nEVALUATION OF ALGORITHMS USING GAMES:\nTHE CASE OF MUSIC TAGGING\nEdith Law\nCMU\nedith@cmu.eduKris West\nIMIRSEL/UIUC\nkris.west@gmail.comMichael Mandel\nColumbia University\nmim@ee.columbia.eduMert Bay J. Stephen Downie\nIMIRSEL/UIUC\nmertbay, jdownie@uiuc.edu\nABSTRACT\nSearch by keyword is an extremely popular method for re-\ntrieving music. To support this, novel algorithms that au-\ntomatically tag music are being developed. The conven-\ntional way to evaluate audio tagging algorithms is to com-\npute measures of agreement between the output and the\nground truth set. In this work, we introduce a new method\nfor evaluating audio tagging algorithms on a large scale\nby collecting set-level judgments from players of a human\ncomputation game called TagATune. We present the de-\nsign and preliminary results of an experiment comparing\nﬁve algorithms using this new evaluation metric, and con-\ntrast the results with those obtained by applying several\nconventional agreement-based evaluation metrics.\n1. INTRODUCTION\nThere is a growing need for efﬁcient methods to organize\nand search for multimedia content on the Web. This need\nis reﬂected in the recent addition of the audio tag classi-\nﬁcation (ATC) task at MIREX 2008, and the introduction\nof new music tagging algorithms [1, 2]. The conventional\nway to determine whether an algorithm is producing ac-\ncurate tags for a piece of music is to compute the level of\nagreement between the output generated by the algorithm\nand the ground truth set. Agreement-based metrics, e.g.\naccuracy, precision, F-measure and ROC curve, have been\nlong-time workhorses of evaluation, accelerating the de-\nvelopment of new algorithms by providing an automated\nway to gauge performance.\nThe most serious drawback to using agreement-based\nmetrics is that ground truth sets are never fully compre-\nhensive [3]. First, there are exponentially many sets of\nsuitable tags for a piece of music – creating all possible\nsets of tags and then choosing the best set of tags as the\nground truth is difﬁcult, if not impossible. Second, tags\nthat are appropriate for a given piece of music can simply\nbe missing in the ground truth set because they are less\nsalient, worded differently (e.g. baroque versus 17th cen-\ntury classical ), or that they do not facilitate the objectives\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.of the particular annotator. For example, a last.FM user\nwho wants to showcase his expertise on jazz music may\ntag the music with highly obscure and technical terms. In\noutput-agreement games such as MajorMiner [2] and the\nListen Game [4], where the scoring depends on how often\nplayers’ tags match with one another, players are motivated\nto enter (or select) tags that are common, thereby omitting\ntags that are rare or verbose. Furthermore, because an ex-\nhaustive set of negative tags is impossible to specify, when\na tag is missing, it is impossible to know whether it is in\nfact inappropriate for a particular piece of music.\nAgreement-based metrics also impose restrictions on\nthe type of algorithms that can be evaluated. To be eval-\nuated, tags generated by the algorithms must belong to the\nground truth set. This means that audio tagging algorithms\nthat are not trained on the ground truth set, e.g. those that\nuse text corpora or knowledge bases to generate tags, can-\nnot be evaluated using agreement-based metrics.\nTo be useful, tags generated by audio tagging algorithms\nmust, from the perspective of the end user , accurately de-\nscribe the music. However, because we do not yet fully\nunderstand the cognitive processes underlying the repre-\nsentation and categorization of music, it is often difﬁcult\nto know what makes a tag “accurate” and what kinds of\ninaccuracies are tolerable. For example, it may be less dis-\nconcerting for users to receive a folksong when a country\nsong is sought, than to receive a sad, mellow song when a\nhappy, up-beat song is sought. Ideally, an evaluation met-\nric should measure the quality of the algorithm by implic-\nitly or explicitly capturing the users’ differential tolerance\nof incorrect tags generated by the algorithms. The new\nevaluation metric we are proposing in this paper has ex-\nactly this desired property.\nThe problems highlighted above suggest that music tag-\nging algorithms, especially those used to facilitate retrieval,\nwould beneﬁt enormously from evaluation by human users.\nManual evaluation is, however, often too time-consuming\nor costly to be feasible. Human computation is a new\narea of research that studies how to build systems, such\nas simple casual games, to collect annotations from hu-\nman users. In this work, we investigate the use of a hu-\nman computation game called TagATune to collect evalu-\nations of algorithm-generated music tags. In an off-season\nMIREX [5] evaluation task, we compared the performance\nof ﬁve audio tagging algorithms under the newly proposed\nmetric, and present in this paper the preliminary ﬁndings.\n387Oral Session 5: Tags\n2. TAGATUNE AS AN EVALUATION PLATFORM\nTagATune [6] is a two-player online game that collects mu-\nsic tags from players. In each round of the game, two play-\ners are either given the same music clip or different music\nclips, and are asked to type in tags for their given music\nclip. After seeing each other’s tags, players must then de-\ncide whether they were given the same music clip or not.\nFigure 1 . The TagATune interface\nWhen a human partner is not available, a player is paired\nwith a computer bot, which outputs tags that have been pre-\nviously collected by the game for the particular music clip\nserved in each round. This so-called aggregate bot serves\ntags that are essentially the ground truth, since they were\nprovided by human players.\nThe key idea behind TagATune as an evaluation plat-\nform is that the aggregate bot can be replaced by an al-\ngorithm bot , which enters tags that were previously gener-\nated by an algorithm. An interesting by-product of playing\nagainst an algorithm bot is that by guessing same or dif-\nferent, the human player is essentially making a judgment\non the appropriateness of the tags generated by the algo-\nrithm. Unlike the conventional evaluation metrics where a\ntag either matches or does not match a tag in the ground\ntruth set, this evaluation method involves set-level judg-\nments and can be applied to algorithms whose output vo-\ncabulary is arbitrarily different from that of the ground\ntruth set.\n2.1 Special TagATune Evaluation\nTo solicit submissions of audio tagging algorithms whose\noutput can be used to construct the TagATune algorithm\nbots, a “Special TagATune Evaluation” was run off-season\nunder the MIREX rubric. Participating algorithms were\nasked to provide two different types of outputs:\n1. a binary classiﬁcation decision as to whether each\ntag is relevant to each clip.\n2. a real valued estimate of the ‘afﬁnity’ of the clip for\neach tag. Larger values of the afﬁnity score indicate\nthat a tag is more likely to be applicable to the clip.2.1.1 The Dataset\nIn the context of the off-season MIREX evaluation task,\nwe trained the participating algorithms on a subset of the\nTagATune dataset, such that the tags they generated could\nbe served by the algorithm bots in the game. The train-\ning and test sets comprise of 16289 and 100 music clips\nrespectively. The test set was limited to 100 clips for both\nthe human evaluation using TagATune and evaluation us-\ning the conventional agreement-based metrics, in order to\nfacilitate direct comparisons of their results. Each clip is 29\nseconds long, and the set of clips are associated with 6622\ntracks, 517 albums and 270 artists. The dataset is split such\nthat the clips in the training and test sets do not belong to\nthe same artists. Genres include Classical, New Age, Elec-\ntronica, Rock, Pop, World, Jazz, Blues, Metal, Punk etc.\nThe tags used in the experiments are each associated with\nmore than ﬁfty clips, where each clip is associated only\nwith tags that have been veriﬁed by more than two players\nindependently.\n2.1.2 Participating Algorithms\nThere were ﬁve submissions, which we will refer to as\nMandel, Manzagol, Marsyas, Zhi and LabX1from this\npoint on. A sixth algorithm we are using for comparison is\ncalled AggregateBot, which serves tags from a vocabulary\npool of 146 tags collected by TagATune since deployment,\n91 of which overlap with the 160 tags used for training the\nalgorithms. The inclusion of AggregateBot demonstrates\nthe utility of TagATune in evaluating algorithms that have\ndifferent tag vocabulary.\n2.1.3 Game-friendly Evaluation\nAn important requirement for using human computation\ngames for evaluation is that the experiment does not sig-\nniﬁcantly degrade the game experience. We describe here\na few design strategies to maintain the enjoyability of the\ngame despite the use of algorithm bots whose quality can-\nnot be gauged ahead of time.\nFirst, a TagATune round is randomly chosen to be used\nfor evaluation with some small probability x. This prevents\nmalicious attempts to artiﬁcially boost or degrade the eval-\nuation of particular algorithms, which would be easy to do\nif players can recognize that they are playing against an al-\ngorithm bot. Second, while it may be acceptable to use half\nof the rounds in a game for evaluating good algorithms, one\nround may be one too many if the algorithm under evalua-\ntion always generates completely wrong tags. Since we do\nnot know ahead of time the quality of the algorithms being\nevaluated,xmust be small enough such that the effects of\nbad algorithms on the game will be minimized. Finally, us-\ning only a small portion of the game for evaluation ensures\nthat a wide variety of music is served, which is especially\nimportant when the test set is small.\n1The LabX submission was identiﬁed as having a bug which nega-\ntively impacted its performance, hence, the name of the participating lab-\noratory has been obfuscated. Since LabX essentially behaves like an al-\ngorithm that randomly assigns tags, its performance establishes a lower\nbound for the TagATune metric.\n38810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nDespite the small probability of using each round for\nevaluation, the game experience can be ruined by an algo-\nrithm that generates tags are contradictory (e.g. slow fol-\nlowed by fast, orguitar followed by no guitar ) or redun-\ndant (e.g. string ,violins ,violin ). Our experience shows\nthat players are even less tolerant of a bot that appears\n“stupid” than of one that is wrong. Unfortunately, such\nerrors occur quite frequently. Table 1 provides a summary\nof the number of tags generated (on average) by each al-\ngorithm for the clips in the test set, and how many of those\nare removed because they are contradictory or redundant.\nAlgorithm Generated Contradictory or Redundant\nMandel 36.47 16.23\nMarsyas 9.03 3.47\nManzagol 2.82 0.55\nZhi 14.0 5.04\nLabX 1.0 0.00\nTable 1 . Average number of tags generated by algo-\nrithms and contradictory/redundant ones among the gen-\nerated tags\nTo alleviate this problem, we perform the following post-\nprocessing step on the output of the algorithms. First, we\nretain only tags that are considered relevant according to\nthe binary outputs. Then, we rank the tags by afﬁnity. Fi-\nnally, for each tag, starting from the highest afﬁnity, we\nremove lower afﬁnity tags with which it is mutually exclu-\nsive. Although this reduces the number of tags available to\nthe algorithm bots to serve in the game, we believe that this\nis a sensible post-processing step for any tag classiﬁcation\nalgorithms.\nAn alternative method of post-processing would be to\nﬁrst organize the “relevant” tags into categories (e.g. genre,\nvolume, mood) and retain only the tag with the highest\nafﬁnity score in each category, thereby introducing more\nvariety in the tags to be emitted by the algorithm bots. We\ndid not follow this approach because it may bias perfor-\nmance in an unpredictable fashion and favour the output of\ncertain algorithms over others.\n2.1.4 Evaluation Using The TagATune Metric\nDuring an evaluation round, an algorithm is chosen to emit\ntags for a clip drawn from the test set. The game chooses\nthe algorithm-clip pair in a round robin fashion but favors\npairs that have been seen by the least number of unique\nhuman players. In addition, the game keeps track of which\nplayer has encountered which algorithm-clip pair, so that\neach evaluator for a given algorithm-clip pair is unique.\nSuppose a set of algorithms A={ai,...,a |A|}and a\ntest set S={sj,...,s |S|}of music clips. During each\nround of the game, a particular algorithm iis given a clip j\nfrom the test set and asked to generate a set of tags for that\nclip. To be a valid evaluation, we only use rounds where\nthe clips given to the human player and the algorithm bot\nare the same. This is because if the clips are different, an\nalgorithm can output the wrong tags for a clip and actually\nhelp the players guess correctly that the clips are different.A human player’s guess is denoted as G={0,1}and\nthe ground truth is denoted as GT ={0,1}, where 0\nmeans that the clips are the same and 1 means that the clips\nare different. The performance Pof an algorithm ion clip\njunder TagATune metric is as follows:\nPi,j=1\nNN/summationdisplay\nnδ(Gn,j=GTj) (1)\nwhereNrepresents the number of players who were pre-\nsented with the tags generated by algorithm ion clipj,\nandδ(Gn,j=GTj)is a Kronecker delta function which\nreturns 1 if, for clip j, the guess from player nand the\nground truth are the same, 0 otherwise. The overall score\nfor an algorithm is averaged over the test set S:\nPi=1\nSS/summationdisplay\njPi,j (2)\n2.1.5 Evaluation Using Agreement-Based Metrics\nWe have chosen to compute the performance of the partici-\npating algorithms using a variety of agreement-based met-\nrics that were included in the 2008 MIREX ATC task, as\na comparison against the TagATune metric. These metrics\ninclude precision, recall, F-measure [7], the Area Under\nthe Receiver Operating Characteristic curve (AUC-ROC)\nand the accuracy of the positive and negative example sets\nfor each tag. We omitted the “overall accuracy” metric, as\nit is a very biased statistics for evaluating tag classiﬁcation\nmodels where there is a large negative to positive tag ratio.\nAs the TagATune game and metric necessarily focus on\nthe ﬁrst few tags returned by an algorithm (i.e. tags that\nhave the highest afﬁnity scores), we chose to also calculate\nthe Precision-at-N ( P@N) score for each algorithm. This\nadditional set of statistics allows us to explore the effect\nof sampling the top few tags on the performance of the\nalgorithms.\n2.1.6 Statistical Signiﬁcance\nFriedman’s ANOV A is a non-parametric test that can be\nused to determine whether the difference in performance\nbetween algorithms is statistically signiﬁcant [5].\nFor each algorithm, a performance score is computed\nover the test set. Using the TagATune metric, this perfor-\nmance score is the percentage of unique players that cor-\nrectly judged that the clips are the same or not using the\ntags emitted by the algorithm, computed using equation\n(1) and (2). For automated statistical evaluations, such as\nthose performed during the MIREX ATC task, these may\nbe the F-measure or P@Nfor the “relevant” tags generated\nfor each clip, or the AUC-ROC for the “afﬁnity” scores.\nThese scores can be viewed as a rectangular matrix, with\nthe different tagging algorithms represented as the columns\nand the clips (or the tags, in the case of F-measure aggre-\ngated over each tag) forming the rows.\nTo avoid having variance introduced by different tags\naffecting the scaling and distribution of the scores, Fried-\nman’s test replaces the performance scores with their ranks\namongst the algorithms under comparison.\n389Oral Session 5: Tags\nAlgorithm TagATune metric +ve Example Accuracy -ve Example Accuracy Precision Recall F-measure\nAggregateBot 93.00 % – – – – –\nMandel 70.10 % 73.13% 80.29% 0.1850 0.7313 0.2954\nMarsyas 68.60% 45.83% 96.82% 0.4684 0.4583 0.4633\nManzagol 67.50% 13.98% 98.99% 0.4574 0.1398 0.2141\nZhi 60.90% 40.30% 93.18% 0.2657 0.4030 0.3203\nLabX 26.80% 0.33% 99.36% 0.03 0.0033 0.0059\nTable 2 . Evaluation statistics under the TagATune versus agreement-based metrics\nAlgorithm Precision at N Precision for AUC-ROC\n3 6 9 12 15 ‘relevant’ tags\nMandel 0.6133 0.5083 0.4344 0.3883 0.3387 0.1850 0.8514\nMarsyas 0.7433 0.5900 0.4900 0.4308 0.3877 0.4684 0.9094\nManzagol 0.4767 0.3833 0.3222 0.2833 0.2520 0.4574 0.7521\nZhi 0.3633 0.3383 0.3100 0.2775 0.2480 0.2657 0.6697\nLabX – – – – – 0.03 –\nTable 3 . Precision and AUC-ROC statistics collected for each algorithm\nFriedman’s ANOV A is used to determine if there exists\na signiﬁcant difference in performance amongst a set of al-\ngorithms. If a difference is detected, then it is common to\nfollow up with a Tukey-Kramer Honestly Signiﬁcant Dif-\nference (TK-HSD) test to determine which pairs of algo-\nrithms are actually performing differently. This method\ndoes not suffer from the problem that multiple t-tests do\nwhere the probability of incorrectly rejecting the null hy-\npothesis (i.e. that there is no difference in performance)\nincreases in direct proportion to the number of pairwise\ncomparisons conducted.\n3. RESULTS\nTables 2 and 3 provide summaries of the evaluation statis-\ntics collected for each algorithm under the TagATune met-\nric as well as agreement-based metrics. Each of the sum-\nmary results was computed over the 100 clips in the test\nset, while the statistical signiﬁcance tests were computed\nover the results for each individual clip. The following\nsections detail additional statistics that were collected by\nthe TagATune evaluation.\n3.1 Algorithm Ranking\nAccording to the TK-HSD test on the TagATune metric\nresults, AggregateBot’s performance is signiﬁcantly better\nthan all the others. A second group of equally perform-\ning algorithms consists of Mandel, Manzagol, Marsyas,\nand Zhi. LabX is the sole member of the worst perform-\ning group. Figure 2 highlights these TK-HSD performance\ngroupings.\nSeveral authors have speculated on the possibility of a\n“glass-ceiling” on the performance of current music classi-\nﬁcation and similarity estimation techniques. As identiﬁed\nby Aucouturier [8], many of these techniques are based on\n‘bag-of-frames’ approaches to the comparison of the audio\nstreams. Hence, the lack of a signiﬁcant difference among\nthe performances of the correctly functioning algorithms is\nnot surprising.\nThe TK-HSD ordering of the algorithms using the F-measure scores (Table 2 and Figure 3) is different from that\nproduced by the TagATune scores. Notably, the Marsyas\nalgorithm signiﬁcantly outperforms the other algorithms\nand the Zhi algorithm has improved its relative rank con-\nsiderably.\nThese differences may be attributed to the fact that the\nperformance of the Marsyas and Zhi algorithm is more bal-\nanced in terms of precision and recall than the Mandel al-\ngorithm (which exhibits high recall but low precision) and\nthe Manzagol algorithm (which exhibits high precision but\nlow recall). This conclusion is reinforced by the positive\nand negative accuracy scores, which demonstrate the ten-\ndency of the Mandel algorithm to over-estimate and Man-\nzagol to under-estimate relevancy. Metrics that take into\naccount the accuracies of all tags (e.g. F-measure) are par-\nticularly sensitive to these tendencies, while metrics that\nconsider only the top N tags (e.g. the TagATune metric\nandP@N) are affected little by them.\nThese results suggest that the choice of an evaluation\nmetric or experiment must take into account the intended\napplication of the tagging algorithms. For example, the\nTagATune metric may be most suitable for evaluating re-\ntrieval algorithms that use only the highest ranked tags to\nFigure 2 . Tukey-Kramer HSD results based on the\nTagATune metric\n39010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 3 . Tukey-Kramer HSD results based on the F-\nmeasure metric\nFigure 4 . Tukey-Kramer HSD results based on the AUC-\nROC metric\ncompute the degree of relevance of a song to a given query.\nHowever, for applications that consider the all relevant tags\nregardless of afﬁnity, e.g. unweighted tag clouds genera-\ntors, the TagATune metric is not necessarily providing an\naccurate indication of performance, in which case the F-\nmeasure might be a better candidate.\n3.2 Game Statistics\nIn a TagATune round, the game selects a clip from the test\nset and serves the tags generated by a particular algorithm\nfor that clip. For each of the 100 clips in the test set and for\neach algorithm, 10 unique players were elicited (unknow-\ningly) by the game to provide evaluation judgments. This\ntotals to 5000 judgments, collected over a one month pe-\nriod, involving approximately 2272 games and 657 unique\nplayers.\n3.2.1 Number of tags reviewed\nOne complication with using TagATune for evaluation is\nthat players are allowed to make the decision of guessing\nsame or different at any point during a round. This means\nthat the number of tags reviewed by the human player varies\nfrom clip to clip, algorithm to algorithm. As a by-product\nof game play, players are motivated to guess as soon as theybelieve that they have enough information to guess whether\nthe clips are the same or different. Figure 5, which shows\nthat players reviewed only a small portion of the generated\ntags before guessing, reﬂects this situation.\n3.2.2 Correlation with precision\nFigure 6 shows the average number of tags reviewed by\nplayers and how many of the reviewed tags are actually\ntrue positive tags (according to the ground truth) in success\nrounds (where the human player made the correct guess)\nversus failed rounds (where the human player made the\nwrong guess). Results show that generally the number of\ntrue positive tags reviewed is greater in success rounds than\nin failed rounds, suggesting that players are more likely to\nfail at guessing when there are more top-afﬁnity tags that\nare wrong. Additionally, the average number of tags re-\nviewed before guessing is fewer in the failed rounds than\nin the success rounds, with the exception of Mandel, pos-\nsibly due to outliers and the much greater number of tags\nthat this algorithm returns. This suggests that players make\ntheir guesses more hastily when algorithms make mistakes.\n3.2.3 Detectable errors\nA natural question to ask is whether one can detect from\ngame statistics which of the reviewed tags actually caused\nplayers to guess incorrectly.\nSystem failed round success round\nMandel 86.15% 49.00%\nMarsyas 80.49% 45.00%\nManzagol 76.92% 33.33%\nZhi 84.38% 70.10%\nLabX 100.0% 95.77%\nTable 4 . Percentage of the time that the last tag displayed\nbefore guessing is wrong in a failed round versus success\nround\nTo investigate this question, we consult the game statis-\ntics for the most frequent behavior of human players in\nterms of the number of tags reviewed before guessing, in\nthe case when the guess is wrong. For example, we might\nﬁnd that most players make a wrong guess after reviewing\nntags for a particular algorithm-clip pair. The hypothesis\nis that the last tag reviewed before guessing, i.e. the nth\ntag, is the culprit.\nTable 4 shows the percentage of times that the nthtag\nis actually wrong in failed rounds, which is above 75%\nfor all algorithms. In contrast, the probability of the last\ntag being wrong is much lower in success rounds, showing\nthat using game statistics alone, one can detect problematic\ntags that cause most players to make the wrong guess in the\ngame. This trend does not hold for LabX, possibly because\nplayers were left guessing randomly due to the lack of in-\nformation (since this algorithm generated only one tag per\nclip).\n391Oral Session 5: Tags\nFigure 5 . Number of tags available and reviewed by play-\ners before guessing\n4. CONCLUSION\nThis paper introduces a new method for evaluating mu-\nsic tagging algorithms and presents the results of a proof-\nof-concept experiment using a human computation game\nas an evaluation platform for algorithms. This experiment\nhas also been used to explore the behavior of conventional\nagreement-based metrics and has shown that averaged re-\ntrieval statistics, such as F-measure, can be sensitive to\ncertain tendencies (e.g. imbalanced performance in terms\nof precision versus recall) that do not affect the TagATune\nmetric, which considers the accuracies of only the top most\nrelevant tags.\nWhile there are many benchmarking competitions for\nalgorithms, little is said about the level of performance that\nis acceptable for real world applications. In this work, we\nhave shown that the use of aggregate data in the bot pro-\nvides a performance level against which the algorithms can\nbe judged. Speciﬁcally, human players can correctly guess\nthat the music are the same 93% of the times when paired\nagainst the aggregate bot, while only approximately 70%\nof the times when paired against an algorithm bot.\nFinally, our work has shown that TagATune is a feasi-\nble and cost-effective platform for collecting a large num-\nber of evaluations from human users in a timely fashion.\nThis result is particularly encouraging for future research\non using human computation games to evaluate algorithms\nin other domains, such as object recognition and machine\ntranslation.\n5. ACKNOWLEDGMENT\nMIREX has received considerable ﬁnancial support from\nboth the Andrew W. Mellon Foundation and the National\nScience Foundation (NSF) under grants NSF IIS-0328471\nand NSF IIS-0327371. Edith Law is generously funded by\na Microsoft Graduate Research Fellowship. Many thanks\nto those who participated in the MIREX Special TagATune\nEvaluation.\nFigure 6 . Number of overall and true positive tags evalu-\nated in success and failed rounds\n6. REFERENCES\n[1] Doug Turnbull, Luke Barrington, David Torres, and\nGert Lanckriet. Towards musical query-by-semantic-\ndescription using the cal500 data set. In SIGIR , pages\n439–446, 2007.\n[2] Michael I. Mandel and Daniel P. W. Ellis. A web-based\ngame for collecting music metadata. Journal of New\nMusic Research , 37(2):151–165, 2008.\n[3] Edith Law. The problem of accuracy as an evaluation\ncriterion. ICML Workshop on Evaluation Methods in\nMachine Learning , 2008.\n[4] Douglas Turnbull, Ruoran Liu, Luke Barrington, and\nGert Lanckriet. A game-based approach for collecting\nsemantic annotations of music. In ISMIR , pages 535–\n538, 2007.\n[5] J. Stephen Downie. The music information retrieval\nevaluation exchange (2005–2007): A window into mu-\nsic information retrieval research. Acoustical Science\nand Technology , 29(4):247–255, 2008.\n[6] Edith Law and Luis von Ahn. Input-agreement: A new\nmechanism for data collection using human computa-\ntion games. CHI, pages 1197–1206, 2009.\n[7] Keith Rijsbergen. Information Retrieval . Butterworth-\nHeinemann, Newton, MA, USA, 1979.\n[8] Jean-Julien Aucouturier. Ten experiments on the mod-\nelling of polyphonic timbre . PhD thesis, University of\nParis 6, France, June 2006.\n392"
    },
    {
        "title": "An Analysis of ISMIR Proceedings: Patterns of Authorship, Topic, and Citation.",
        "author": [
            "Jin Ha Lee 0001",
            "M. Cameron Jones",
            "J. Stephen Downie"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416618",
        "url": "https://doi.org/10.5281/zenodo.1416618",
        "ee": "https://zenodo.org/records/1416618/files/LeeJD09.pdf",
        "abstract": "This paper presents analyses of peer-reviewed papers and posters published in the past nine years of ISMIR proceedings: examining publication and authorship practices, topics and titles of research, as well as the citation patterns among the ISMIR proceedings. The main objective is to provide an overview of the progress made over the past nine years in the ISMIR community and to obtain some insights into where the community should be heading in the coming years. Overall, the ISMIR community has grown considerably over the past nine years, both in the number of papers and posters published each year, as well as the number of authors contributing. Furthermore, the amount of collaboration among authors, as reflected in co-authorship, has increased. Main areas of research are revealed by an analysis of most commonly used title terms. Also, major authors and research groups are identified by analyzing the co-authorship and citation patterns in ISMIR proceedings.",
        "zenodo_id": 1416618,
        "dblp_key": "conf/ismir/LeeJD09",
        "keywords": [
            "peer-reviewed papers",
            "posters",
            "ISMIR proceedings",
            "publication practices",
            "topics and titles",
            "citation patterns",
            "ISMIR community",
            "growth",
            "number of papers",
            "number of authors"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nAN ANALYSIS OF ISMIR  PROCEEDINGS: PATTERN S OF \nAUTHORSHIP, TOPIC, A ND CITATION  \nJin Ha Lee  M. Cameron Jones  J. Stephen Downie  \nUniversity of Washington  \nThe Information School  \njinhalee @uw.edu University of Illinois at  \nUrbana -Champaign  \nmjones2@ illinois .edu  Univer sity of Illinois at  \nUrbana -Champaign  \njdownie@illinois .edu \nABSTRACT  \nThis paper presents analyses of peer-reviewed papers  and \nposters  published in the past nine years of ISMIR pr o-\nceedings : examining publication and authorship practices, \ntopics and titles of research, as well as the cita tion pat-\nterns among  the ISMIR proceedings. The main objective \nis to provide an overview of the progress made over the \npast nine years in the ISMIR community  and to obtain \nsome insights into where the community should be hea d-\ning in the coming years. Overall, the ISMIR community \nhas grown considerably over the past nine years, both in \nthe number of papers and posters published each year, as \nwell as the number o f authors contributing. Furthermore, \nthe amount of collaboration among authors , as reflected \nin co -authorship, has increased . Main areas of research \nare revealed by an analysis of most commonly used title \nterms. Also, major authors and research groups are ident i-\nfied by analyzing the co -authorship and citation patterns \nin ISMIR proceedings.  \n1. INTRODUCTION  \nThis year, 2009, mark s the tenth iteration  of the Intern a-\ntional Symposium  on Music Information Retrieval  confe-\nrence series  (ISMIR) . ISMIR  was organized with the \nhope that the “resulting information interchange will en a-\nble scholars to move more quickly towards viable sol u-\ntions to many problems” [1] in the field  of Music Info r-\nmation Retrieval (MIR) . \nFutrelle & Downie [2] defined MIR  as “a rapidly \ngrowing interdisciplinary research area encompass ing \ncomputer science and information retrieval, musicology \nand music theory, audio engineering and digital signal \nprocessing, cognitive science, library science, publishing, \nand law. Its agenda, roughly, is to develop ways of ma n-\naging collections of musica l material for preservation, \naccess, research, and other uses”. Necessarily, MIR spans \nboth audio and symbolic representations of music [3], but \nalso includes musi cal metadata, usage data, and other e x-tra-musical information  [4], including user -studies and \nhuman -computer interaction studies of music systems.  To \ndate, most re search in MIR has been content -based [5]. \nIn 2000, MIR  was still a fairly new field  with a great \ndeal of  potential that was gaining the interest of researc h-\ners from  many different domains. Although ISMIR  \nstarted as a small -scale symposium, it has grown i m-\nmensely over the past nine years as more people have \nrecognized the importance of MIR research and have \nbeen drawn in to the field.  The community has grown to \nthe po int of establishing the International Society for M u-\nsic Information Ret rieval, which will help orient , orga n-\nize, and disseminate the communit y‟s future research.  \nWe performed various informetric analyses on the \nISMIR proceedings  from 2000 to 2008 in order to di s-\ncover how the patterns of publications have changed over \nthe past nine years. Through these analyses, we hope to \nobtain insights into what the ISMIR community  has and \nhas not been able to accomplish and which directions it \ncould be heading towards in  the coming years.  \nIn the following, we provide descriptive statistics \nshowing the change in the number of publications and \nauthorship patterns over the past nine years. We a lso pr o-\nvide the results of our analysis of the title terms, looking \nat the most co mmonly used single terms as well as bi -\ngrams. In addition, we performed analyses on the citation \npatterns among the  publications and  authors who have \npublished in the ISMIR proceedings.  \n2. GROWTH OF THE ISMIR COMMUNITY  \nThe first ISMIR conference had just 10 refereed papers \nand 16 posters representing 55 authors, with several other \nscholars presenting invited talks. To date, 881 authors \nhave contributed peer -reviewed papers and posters to the \nISMIR proceedings, not to mention the numerous partic i-\npants in the a nnual Music Information Retrieval Evalu a-\ntion eXchange ( MIREX ), conf erence workshops, demo n-\nstrations, tutorials, and invited talks. The rapid growth in \nparticipation has been paralleled by a similar increase in \nthe number of papers and posters accepted to I SMIR. In \ntotal, over 700 peer -reviewed papers and posters have \nbeen published, comprising a substantial literature on a \nbreadth of topics ranging from signal -processing tec h-\nniques to use r studies of MIR systems.   \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.  \n© 2009 International Society for Music Information Retrieval  \n57Poster Session 1\n  \n \n2.1 Change in the Number of Publications per Yea r \nThe number of publications in ISMIR proceedings has \nbeen steadily increasing over the past nine years. Exact \nnumbers are presented in Table 1  along with the number \nof unique authors published in each year .  \n \nYear  00 01 02 03 04 05 06 07 08 \nPapers  10 21 31 23 60 59 59 62 105 \nPosters  16 16 22 24 44 57 28 65 - \nTotal  26 37 53 47 104 116 87 127 105 \nUnique \nAuthors  55 74 113 108 213 232 185 267 262 \nTable 1. The number of publications and unique authors \nper year . \nWe can better observe the changes in the proportion of \npapers and posters for each year, as well as the changes in \nthe number of authors. The number of publications a l-\nmost doubled in 2004, jumping from 47 in 2003 to 104. \nIn 2008, there was a change in the submiss ion format and \nall paper submissions were to have accompanying posters \nas well. Looking at the number of authors, we can see \nthat there were two sharp increases in 2002 and 2004, and \na major drop in 2006. However, the overall number of \nauthors represented at the conference each year has ge n-\nerally grown over the past nine years .  \nFigure 1 shows the authorship trends within the ISMIR \nproceedings, tracking the proportion of papers with one, \ntwo, three, four, and five -or-more authors. As can be \nseen, the number of single -authored papers has decreased \nyear-over-year. The number of papers with  two co -\nauthors peaked in 2002, and has steadily declined since. However, the number of papers with three authors has \nsteadily increased year -over-year. The average number of \nco-authors on papers and posters published each year has \nincreased over the past nine years, from an average of \n2.27 authors per publication in 2000 to 2.93 authors per \npublication in 2008. Clearly ISMIR is becoming a much \nmore collaborative community as the number of authors \nper paper increase s, and the proportion of single , and \ndoubl e-authored papers diminishes in favor of papers \nwith three or more authors.  \n \nFigure 1. Co-authorship trends tracking the perce n-\ntage of papers with 1, 2, 3, 4, and 5+ authors from \n2000 to 2008.  \n2.2 Co-authorship Analysis  \nWe performed an analysis to identify t he patterns of co -\nauthorship among all the authors who published in \nISMIR proceedings and determine which authors appear \nas the central hubs in the co -authorship graph. Figure 2  0%10%20%30%40%50%60%\n00 01 02 03 04 05 06 07 08\n1 2 3 4 5+\nFigure 2. Co-authorship network among ISMIR authors who have published two or more articles. The 22 authors \nwith the largest co -authorship networks have been highlighted.  \n5810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nwas generated by using Pajek which is a social networ k-\ning analysis and visuali zation tool [6]. In this figure , only \nthe authors who published 2 or more papers/posters are \nincluded in order to simplify the network.  \nSeveral main clusters of pe ople can be visually ident i-\nfied in the figure showing the close connections among \nsome authors. The top 22 authors with the largest number \nof distinct co -authors (12+ ) are labeled in the figure. O f-\nten these authors represent an active research group such \nas National Institute of Advanced Industrial Science and \nTechnology (AIST)  headed by M. Goto in Japan , The I n-\nternational Music Information Retrieval Systems Evalu a-\ntion Laboratory  (IMIRSEL) headed by  J. S. Downie in \nIllinois, Distributed Digital Music Archives and Libraries  \nheaded by I. Fujinaga in Canada,  the Center for Digital \nMusic headed by Mark Sandler in London,  and so on.  \nWhat is evident from these analyses is the growing \nrole of research labs in the ISMIR community, and how \nthey engender collaboration and increase participation in \nresearch. Many European labs and research groups are \ntightly interconnected, and are difficult  to distinguish one \nfrom the other  based on the co -authorship patterns. Fur-\nthermore, not all evidence of collaboration is represented \nin the co -authorship network; for example, the IMIRSEL \nlab appears relatively isolated, despite their central role in \norganizing MIREX. L arge, inter continental , multi -\ninstitutional grant project s, such as the Networked Env i-\nronment for Music Analysis ( NEMA ) project [7], may \nstart to change the shape of collaboration within the \nISMIR community.  \n3. RESEARCH TOPI CS IN ISMIR  \nThe topics explored in the first ISMIR conference laid \nthe foundation for the future growth and evolution of the \nfield.  While ISMIR  has grown, it has remained true to the \noriginal vision laid out in  the early conference programs.  In this sectio n, we present an analy sis of  terms extracted \nfrom the titles and abstracts of ISMIR papers . Only title \nand abstract terms were used as these represent concise \nsummaries of the papers‟ content.  \n3.1 The Most Commonly Used Title Terms  \nIn order to get a n idea as to which research areas have \nbeen of interest over the past nine years , we analyzed the \ntitle terms of all peer -reviewed  papers and  posters in the \nISMIR proceedings . All the terms from the title s of the \npapers and posters were extracted. The words  were \nstemmed using a Perl -based implementation of the Porter \nstemming algorithm [8], and stop -words were removed \nusing a combination of a standard list of common -usage \nEnglish -language words, with the stop -word “music”, as \nthis term appears in almost all titles in the ISMIR pr o-\nceedings . Table 2 shows the top  terms that appeared in \nthe publication titles for each year.  New terms entering \nthe top -ranked lists are high lighted in bold -face.  \nFrom th e table, we can observe that the most often \nused title terms were relatively similar for each year ; \nhowever, it is possible to identify certain trends. For i n-\nstance, there was a strong interest in query by sin g-\ning/humming  syste ms in 2002 and 2003 shown by the \ntitle term query (“queri”) appearing only in the lists of \nthese two years. Research interest in musical genres in-\ncreased in 2005 and 2006, and inter est in music similarity \nresearch peak ed in 2006. Interest in classification  and \nmodeling has been consistent over the past nine years. \nAdditionally, the consistently high rank of  the term “a u-\ndio” suggests that ISMIR re searchers have been focused \nprimarily on audio rather than symbolic representation s. \nWhat is also evident from the title  terms, is how clos e-\nly the field has stuck to the original framing of MIR as \nrepresented in the 2000 ISMIR program. The core co n-\ncepts have remained prevalent throughout the past de c-\n2000  2001  2002  2003  2004  2005  2006  2007  2008  \nRetriev  Retriev  Retriev  Retriev  Audio  Audio  Audio  Audio  Audio  \nInform  Inform  Audio  Automat  Retriev  Retriev  Similar  Retriev  Featur  \nModel  System  Inform  Model  Automat  Classif  Classif  Similar  Retriev  \nSystem  Audio  Queri  Similar  System  Featur  Model  Model  Model  \nAudio  Approach  System  Database  Classif  Inform  Genr  System  Analysi  \nClassif  Model  Automat  Audio  Polyphon  Model  Automat  Recognit  Automat  \nPolyphon  Analysi  Model  Inform  Pattern  Polyphon  Feature  Polyphon  Song  \nSegment  Similar  Polyphon  Queri  Inform  Extract  Approach  Featur  Inform  \nInstrument  Match  Similar  System  Extract  Similar  Perform  Analysi  Similar  \nTechniqu  MIR  Analysi  Classif  Featur  Algorithm  Retriev  Classif  Chord  \nLanguag  Spot  Content   Sound  Genr  Evalu  Automat  Content  \n  Pattern   Tempo   Key Approach   \n  Voic      Evalu   \n       Transcript   \n       Algorithm   \n \nTable 2. Top 10 ranked title terms of each year (w/ ties); new terms are highlighted in bold -face font.  \n59Poster Session 1\n  \n \nade, yet have accommodated expansion into new areas.  \n3.2 Title and Abstract Bi-grams  \nSingl e-term-concepts present a limited view of research \nconcepts and topics, especially after subtle differences in \nterms are merged by stemming (e.g., „using‟ and „users‟ \nhave the same stem, „us‟ , yet carry different connotations \nin usage). Furthermore, the limited text available in titles, \nonly provides a glimpse at the complexity of concepts \nand ideas being researched and published. In order to get \nat more specific concepts which have taken the interest of \nISMIR researchers, we extracted st emmed bi -grams (i.e., \n2-word phrases) from the titles and abstracts of all papers \nand posters. Initially, we examined the bi -grams on a \nyear-by-year b asis, much as we did for single term \nconcepts. However, as expected, the number of bi -grams \nexceeds the nu mber of uni -grams, and the frequency with \nwhich any one bi -gram occurs is much lower. No \nmeaningful or interesting patterns arose in the yearly \nanalysis; however, when considered in aggregate, there is \nstronger evidence of dominant research topics within t he \nfield.  Table 3 shows the top 20 most commonly used bi -\ngrams  in ISMIR proceedings  over the last nine years.  \n \nBi-gram (stemmed)  Count  \ninform_retriev  25 \ncontent_base  24 \ngenr_classif  14 \nweb_base  9 \nhidden_markov  9 \nqueri_hum  9 \npolyphon_audio  8 \nreal_time  7 \nsystem_base  7 \noptic_recognit  7 \naudio_featur  7 \nground_truth  7 \nbase_similar  6 \nfeatur_extract  6 \nplaylist_gener  6 \naudio_fingerprint  6 \nsing_voic  6 \nretriev_system  6 \nautomat_transcript  5 \nmelod_similar  5 \nsimilar_measur  5 \nautomat_genr  5 \nTable 3. Top 20 most commonly used  bi-grams  from \ntitles and abstracts , reflecting the main research foci, m e-\nthods, and approaches of the ISMIR community.  \nThe most common bi -gram is “information retrieval ”, \nfollowed by “content based” , “genre classification ”, and \nso on. Beyond these, we can see the prevalence of the \nweb, and web -based systems, which has paralleled the \nemergence of “web 2.0” and greater access to music and \nmusic systems online within the comm ercial sector. A l-\nthough the frequencies of occurrence of some individual \nconcepts are low, overall we find the topics represented by the bi -gram analysis to be fairly representative of the \nmajor research in terests in the field: such as “music sim i-\nlarity”, “feature extraction”, and so on.  \n4. CITATION PATTERNS  \nMoving  beyond terms  and bi -grams  as representations of \nresearch interest s, the papers themselves published in the \nISMIR proceedings serve as representations of research \ntopics and areas, and references to  them serve as a way of \nhighlighting the prevailing research interests of the co m-\nmunity. Weinstock [9] outlines 15 motivations for why \nacademics cite ea ch other in scholarly writing  including \npaying homage to pioneers, giving credit for related work, \nand so on. We examined the references lists of all peer -\nreviewed ISMIR papers and posters, and looked for refe r-\nences to other peer -reviewed ISMIR papers and posters. \nWe did not consider references to demos, invited talks, \ntutorials, MIREX abstracts, or workshop papers. We also \ndid not attempt to measure reference s to publications ou t-\nside the ISMIR proceedings, nor did we attempt to gauge \nthe number of citations to ISMIR p apers from outside.  \nFirst, we shall outline and describe the general citation \nbehavior of the ISMIR community. Figure 3 shows the \nfrequency distribution of publications by the number of \nreferences to other ISMIR publications they contain. \nMost ISMIR paper s and posters (nearly 50%) do not re f-\nerence any other ISMIR publications.  The average nu m-\nber of ISMIR references per paper/poster was 1.278 with \nthe standard deviation of 2.05 and the maximum of 27.  \n \nFigure 3. Number of ISMIR references in ISMIR papers   \nThe reasons for low internal referencing within the \nISMIR community may be due to the fact that some a u-\nthors preferentially cite journals, books, and theses which \nare extensions of, or refinements of ideas initially pu b-\nlished in ISMIR over the ISMIR public ations. Other \npossible explanations include the fact that ISMIR pr o-\nceedings are not indexed in commonly used digital l i-\nbrary portals, such as the ACM Digital Li brary  or Cit e-\nSeer, and are inconsistently indexed by Google Scholar. \nThe fantastic resource on h ttp://www.ismir.net/, which \nhas been developed and maintained by Michael Finge r-\nhut, contains a near -complete set of the full-text versions \nof all papers and posters published in the ISMIR procee d-0100200300400\n012345679101327Frequency\nNumber of References\n6010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nings; however, it lacks full-text search capabilities itself,  \nand the site does not provide complete, standardized  me-\ntadata records which may improve the visibility of ISMIR \npapers in search engines, and digital library portals.   \n \nAuthor/ Title  # Refs  \nGoto, M., et al. (2002). RWC Music Database: Pop u-\nlar, Classical and Jazz Music Databases  21 \nBello, J. & Pickens, J. (2005). A Robust Mid -Level \nRepresentation for Harmonic Content in Music Si g-\nnals 13 \nTzanetakis, G., Essl, G. & Cook, P. (2001). Automatic \nMusical Genre Classification of Audio Signals  13 \nAucouturier, J.  & Pachet, F.  (2002). Music Similarity \nMeasures: What’s the use?  13 \nSheh, A. & Ellis, D. (2003).  Chord segmentation and \nrecognition using EM -trained hidden markov models  12 \nPampalk, E., Dixon, S. & Widmer, G. (2003). Explo r-\ning music collections by browsing different views  11 \nPaulus, J. & Kalpuri, A. (2002). Measuring the sim i-\nlarity of Rhythmic Patterns  11 \nGoto, M., et al. (2003). RWC Music Database: Music \ngenre database and musical instrument sound dat a-\nbase 10 \nClausen, M., et al. (2000). PROMS: A  Web-based \nTool for Searching in Polyphonic Music  9 \nEllis, D., et al.  (2002). The Quest for Ground Truth in \nMusical Artist Similarity  8 \nLogan, B. (2000). Mel Frequency Cepstral Coeff i-\ncients for Music Modeling  8 \nBirmingham, W., et al. (2001). MUSART: Music R e-\ntrieval Via Aural Queries  8 \nLogan, B. (2004). Music Recommendation from Song \nSets 8 \nAbdallah, S. & Plumbley, M.  (2004). Polyphonic \ntranscription by non -negative sparse coding of power \nspectra  7 \nFoote, J., Cooper, M. & Nam, U. (2002). Audio R e-\ntrieval by Rhythmic Similarity  7 \nMazzoni, D. & Dannenberg, R. (2001). Melody \nMatching Directly from Audio  7 \nVinet, H., Herrera -Boyer, P. & Pachet, F. (2002). The \nCUIDADO Project  7 \nSoulez, F., Rodet, X. & Scharwz, D. (2003). Impro v-\ning polyphonic and poly-instrumental music to score \nalignment  7 \nWhitman, B. & Ellis, D. (2004). Automatic Record \nReviews  7 \nWhitman, B. & Smaragdis , P. (2002). Combining \nMusical and Cultural Features for Intelligent Style \nDetection  7 \nTable 4. Top cited papers and poster s (excluding self -\ncitation) . \nWorking with the references we were able to extract, \nwe filtered self -citations, which we defined as a reference \nto a paper in which an  author  of the citing paper is an au-\nthor on the referenced paper. Tab le 4 shows the top cited \npapers and posters in the ISMIR proceedings, ranked by \nthe number of references we were able to find to each.  \nAmong the top cited papers and posters, there is a d i-\nversity of topics and publications, from which we may \ninfer a range of motivations. The most cited publication \nin the ISMIR proceed ings is  Goto, et al.‟s 2002 poster introducing the RWC database, garnering 21 references. \nFollowing Weinstock‟s taxonomy of citer motivation, the \nreferencing of a data set is most like motivati on three: \nidentifying methodology, equipment, etc. The lack of \nstandardized data sets with ground truth is a recurring \nproblem in the MIR community and t he RWC database \nhas ser ved as a valuable  resource for MIR researchers, as \nit acts as a de facto standar dized collection on which to \nbuild and eva luate systems.  In fact, the presence of Goto, \net al., 2003, and Ellis, et al ., 2002 on this list reiterate the \nimportance of standardized data sets with ground truth \nwithin MIR research.  \nThere are several other met hodological references, i n-\ncluding references to Logan (2000), Tzanetakis, et al. \n(2001), Sheh & Ellis (2003), Goto, et al. (2003). There \nare also elements of “paying homage” in the references to \nseveral papers, especially the seminal work of Beth L o-\ngan, wh o introduced MFCCs to the MIR community.  \n \nAuthor  Ref. \nCount  Co-author \nCount  Paper/Poster \nCount  \nGoto, M  43 24 21 \nEllis, D P W  41 12 12 \nHashiguchi, H  34 5 3 \nNishimura, T  34 5 3 \nOka, R  34 5 3 \nWidmer, G  34 11 19 \nDannenberg, R B  29 15 10 \nLogan, B  29 4 5 \nWhitman, B  28 6 5 \nDownie, J S  26 15 25 \nPampalk, E  26 11 12 \nTzanetakis, G  24 27 15 \nBirmingham, W P  23 11 7 \nPachet, F  22 12 13 \nDixon, S  22 9 9 \nMeek, C  22 10 5 \nPickens, J  21 7 6 \nPauws, S  19 6 8 \nCook, P  19 7 6 \nFujinaga, I  19 31 28 \nTable 5. Top 20 cited authors (excluding self  references ). \nWithout a more in -depth analysis of the individual \ncontexts surrounding each citation, it is difficult to tease \nout the precise motivation s for all the references . Regar d-\nless, the  most referenced works comprise a diversity of \ntopics and areas which span the breadth of research wit h-\nin MIR, including references to signal -processing alg o-\nrithms and methods as well as techniques for handling \nsymbolic representations of music. There are papers co v-\nering music transcription, and rhythm analysis, as well as \nhigh-level tasks such as genre -classification, search and \nrecommendation algorithms, and approaches to unde r-\nstanding audio similarity.  \nTable 5 shows the top 20 cited authors excluding sel f \nreferences. The second  column shows the count of co -\nauthors each of these authors have in ISMIR proceedings  \nand the third column shows the count of papers/posters \neach author published . The most heavily cited author was \n61Poster Session 1\n  \n \nMasataka Goto with 43 references b y other ISMIR a u-\nthors.  Among these top -cited authors, we can see there \nare those who have many references, in part because they \nhave published many papers  (e.g., Goto; Widmer) , and \nthere are authors who are highly cited, but have only  a \nfew publications (e .g., Logan; Whitman). There is, ho w-\never, no correlation (r=0.021 ) between reference count \nand paper count, indicating that the referencing of authors \nis not merely a product of their productivity within the \ncommunity. It is worth noting that among the top -cited \nauthors, there is a strong correlation between the number \nof co -authors an author has, and the numbe r of papers \nhe/she has written ( r=0.815 ). This correlation is not that \nsurprising given our findings from section 2 where we \ndiscussed the trend towar ds collaboration and co -\nauthorship among ISMIR authors.  \n5. CONCLUSION  \nThe ISMIR community has grown significantly, and \nthrough the contributions of nearly 900 researchers, the \nfield of Music Information Retrieval has been well -\ndefined and established. The com munity is a tightly -knit \none, with a high -degree of collaboration and co -\nauthorship, focused around a core set of research topics \nand areas.  \nThe main insights of our analys es can be summarized \nas follows:  \n1) The ISMIR community is becoming more coll a-\nborative as shown by increasing co-authorship;  \n2) The role of research labs is growing in the ISMIR \ncommunity as they promote collaboration and i n-\ncreased participation in research;  \n3) The focus of research has mainly been on audio so \nfar as revealed by the most commonly used title \nand abstract terms ; \n4) The most cited works in the ISMIR proceedings \ncomprise a variety of topics , but primarily point to \ndatasets, techniques, and methods;  \nIn their early ISMIR paper discussing the interdiscipl i-\nnary communities and resear ch issues, Futrelle and \nDownie [2] lists several key research areas in MIR. \nAmong these, our analyses show that areas such as fe a-\nture dete ction and classification/machine learning have \nbeen the major topics  represented to date in the  ISMIR \nproceedings , whereas topics  such as user studies, metad a-\nta, work on symbolic representations, and epistemol o-\ngy/ontology have not been as well represented  as others . \nOur advice for the sustained, future growth of the ISMIR \ncommunity is to encourage greater activity in these areas, \nas they  are relatively uncrowded, open topics of research \nin which great advances can be made.  \nWe would like to continue our informetric  analysis of \nMIR research, and there are several aspects that can be \nfurther analyzed to obtain a broader picture  of MIR.  One \narea in which we could improve our understanding of the \ndomain, is to include external sources and references in our citati on analysis, and track the number of ISMIR re f-\nerences found in other relate d journals and proceedings, \nreferences that are not from ISMIR proceedings and so \non. Additionally, we explored several clustering analyses \nin researching this paper, and none provi ded immediately \ncompelling results. We would like to continue to explore \nhow papers, authors, and research topics cluster based on \nsemantic similarity, co -authorship patterns, citation pa t-\nterns, and bibliographic coupling.  \n6. ACKNOWLEDGEMENTS  \nWe would like to  acknowledge the work of Michael Fi n-\ngerhut in maintaining the ISMIR proceedings database \n(http://ismir.net/), which was our primary resource for \nlocating all of the publications, and was vital to verifying \nour data  integrity; we are very thankful for his diligence . \nWe also thank the Andrew W. Mellon Foundation for its \nfinancial support.  \n7. REFERENCES  \n[1] ISMIR 2000: International Symposium on Music \nInformation Retrieval. Available at: \nhttp://ciir.cs.umass.edu/musi c2000/  \n[2] J. Futrelle, J. S . Downie: “Interdisciplinary \nCommunities and Research Issues in Music \nInformation Retrieval ,” Proceedings of the \nInternational Symposium on Music Information \nRetrieval , pp. 215-221, 2002 . \n[3] D. Byrd,  T. Crawford:  “Problems of Music \nInformat ion Retrieval in the Real World,”  \nInformation Processing and Management  38, pp. \n249-272, 2001.  \n[4] J. H. Lee: Analysis of Information Features in \nNatural Language Queries for Music Information \nRetrieval: Use Patterns and Accuracy . University of \nIllinois, Ph.D. thesis, 2008.  \n[5] N. Orio : “Music Retrieval,” Foundations and Trends \nin Information Retrieval  1(1), pp. 1-90, 2006.  \n[6] V. Batagelj,  A. Mrvar: Pajek - Analysis and \nVisualization of Large Networks in Jünger, M., \nMutzel, P., (Eds.) Gra ph Drawing Software. \nSpringer, Berlin, pp. 77 -103, 2003.   \n[7] Networked Environment for Music Analysis \n(NEMA). Available at: http://nema.lis.uiuc.edu/  \n[8] M. Porter: An algorithm for suffix stripping in \nSparck -Jones, K.; Wil lett, P. (Eds.): Readings in \nInformation Retrie val. Morgan Kaufmann \nPublishers,  pp. 313 -316, 1997.  \n[9] M. Weinstock: “Citation Indexes,”  Encyclopedia of \nLibrary and Information Science  5. Marcel Dekker, \nNew York, 1971.  \n62"
    },
    {
        "title": "Formalizing Invariances for Content-based Music Retrieval.",
        "author": [
            "Kjell Lemström",
            "Geraint A. Wiggins"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418363",
        "url": "https://doi.org/10.5281/zenodo.1418363",
        "ee": "https://zenodo.org/records/1418363/files/LemstromW09.pdf",
        "abstract": "Invariances are central concepts in content-based music retrieval. Musical representations and similarity measures are designed to capture musically relevant invariances, such as transposition invariance. Though regularly used, their explicit definition is usually omitted because of the heavy formalism required. The lack of explicit definition, however, can result in misuse or misunderstanding of the terms. We discuss the musical relevance of various musical invariances and develop a set-theoretic formalism, for defining and classifying them. Using it, we define the most common invariances, and give a taxonomy which they inhabit. The taxonomy serves as a useful tool for idetinfying where work is needed to address real world problems in content-based music retrieval.",
        "zenodo_id": 1418363,
        "dblp_key": "conf/ismir/LemstromW09",
        "keywords": [
            "invariances",
            "musical representations",
            "similarity measures",
            "transposition invariance",
            "explicit definition",
            "heavy formalism",
            "musical relevance",
            "set-theoretic formalism",
            "classifying them",
            "taxonomy"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFORMALIZING INV ARIANCES FOR\nCONTENT-BASED MUSIC RETRIEV AL\nKjell Lemstr ¨om\nDepartment of Computer Science\nUniversity of HelsinkiGeraint A. Wiggins\nDepartment of Computing\nGoldsmiths, University of London\nABSTRACT\nInvariances are central concepts in content-based music re-\ntrieval. Musical representations and similarity measures\nare designed to capture musically relevant invariances, such\nas transposition invariance. Though regularly used, their\nexplicit deﬁnition is usually omitted because of the heavy\nformalism required. The lack of explicit deﬁnition, how-\never, can result in misuse or misunderstanding of the terms.\nWe discuss the musical relevance of various musical in-\nvariances and develop a set-theoretic formalism, for deﬁn-\ning and classifying them. Using it, we deﬁne the most\ncommon invariances, and give a taxonomy which they in-\nhabit. The taxonomy serves as a useful tool for idetinfying\nwhere work is needed to address real world problems in\ncontent-based music retrieval.\n1. INTRODUCTION\nTo effectively perform content-based music retrieval\n(CBMR), the intrinsic features of music must be taken into\naccount. Some of the most important features correspond\ndirectly with invariances. Invariances related to pitch,\ntempo and duration are widely used, but usually without\nproper deﬁnition or discussion of their inter-relationship.\nIndeed, a single term is sometimes used to name multiple\nphenomena, admitting confusion about its real meaning.\nWestern musical scales may be transformed, or trans-\nposed , to any other key so that the corresponding pitch in-\ntervals remain intact. Indeed, Western people tend to listen\nto music analytically, observing pitch intervals rather than\nabsolute pitch values. Thus, musical works are identiﬁed\nregardless of the prevalent musical key. The same observa-\ntion is valid for tempo: two pieces of music are considered\nthe same if the other is just played slower than the other\n(i.e., a different time scale is used). So transposition and\ntime-scale invariance are important in CBMR applications.\nHowever, in some cases mere transposition and time-\nscale invariance are not enough. For example, in query\nby humming, untrained singers often cannot produce pitch\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.intervals accurately enough to constitute a match. To ad-\ndress this, several pitch class generalizations have been\nsuggested, such as pitch contour [9] and qpi classiﬁcation\n[4]. Using these generalizations, only direction of inter-\nval (contour) or the order of magnitude of interval (small,\nmedium or large) is observed, respectively.\nIn this paper, we will deﬁne what it means when a rep-\nresentation or a method (algorithm) is invariant under a\ngiven notion arising from a musical phenomenon. We will\ngive deﬁnitions for widely used invariances related to three\nmain dimensions of music: pitch ,onset time andduration .\nThe latter two are temporal features and, usually, the third\nis derivable from the second. However, it is sometimes use-\nful to separate them since the invariances as applied, cate-\ngorised by our taxonomy, may differ. We will also deﬁne a\nset of more abstract, structural invariances. All of these in-\nhabit a taxonomy that shows the relationships between the\ninvariances, and also serves as a tool for identifying areas\nwhere further work in CBMR is needed.\n2. DEFINING THE INV ARIANCES\n2.1 The representation\nLet us start by deﬁning the notion of a representation . In\nthis context, we are modelling an observed phenomenon\n(music perception), and it is important not to presuppose\nthat the data isthe phenomenon; therefore, making the rep-\nresentation explicit is important too.\nLet the size of a set Sbe denoted by|S|. Let the set\nof ordered subsets of set Sof size between nandm, in-\nclusive, be denoted by Sn...m, and where n=m,Sn;S∗\nis the power set of S. Given a set of features, fi∈F,\neach with a unique type, τi∈τ, identiﬁed by an injection\nT:F/mapsto→τ, anabstract representation ,ρ, is a subset of F.\nThe type of each feature should be a mathematical speciﬁ-\ncation (e.g., linear Abelian group for pitch) which is cho-\nsen to model the corresponding reality appropriately [13].\nGiven an abstract representation, ρ, aconcrete representa-\ntion,r, is a set of tuples\n{/angbracketleftf,Σf,/followsf,Φf,Πf/angbracketright|f∈ρ}\nwhere Σfis an alphabet adequate to express f,/followsfis a par-\ntial order on Σf,ΦfandΠfare sets of functions and predi-\ncates, respectively, which apply to members of Σfdeﬁning\nthe operations and tests required for the algebra of T(f).\nWiggins et al. [13] give detailed examples of datatypes for\n591Poster Session 4\nFeature invariances Structural invariances\nPitch Onset Time Duration\nWeaker/more speciﬁc transposition (2) ω-permutation (11/1)\n↓ pitch-transposition (3) time-position (6) strongly permutation (11/2)\n↓ pitch-warp (4) time-scale (7) time-scale (7) ω-concatenation (12/1)\nStronger/less speciﬁc Parsons (5) time-warp (9) duration-warp (8) strongly concatenation (12/2)\nTable 1 . A sparse taxonomy on considered invariances. An invariance in the table subsumes the invariances above it, if no\nhorizontal line appears in between. The number in parenthesis is that of the associated deﬁnition in Sections 2.4 and 3.\npitch and time. In general, /followsfis needed for the working\nof our formalism, not for the representation itself (there\nwould be a member of Πffor this, where appropriate); it\nis kept separate so that it may be different from any orders\nthat are internal to the feature implementation, if necessary.\nLetˆ.be a function which maps a concrete representation\nto its corresponding abstract representation.\nGiven a concrete representation, r, let an element e,\ne∈r, be a set of values, ei, with concrete datatypes corre-\nsponding with r.\nLet a dataset ,E, be a set of elements. Eis inriff each\neiinEis inr.\n2.2 A concatenator\nTo deﬁne invariances, we use a concatenator constructor.\nAconcatenator ,Cr/prime\nω(E), constructs a lexicographically\nordered multiset1of elements from a dataset, E, repre-\nsented inr. The lexicographical order is speciﬁed by the\nordered setω∈ˆr1...|r|and the/followsiof the members of rcor-\nresponding with the members of ω. The superscript of the\nconcatenator r/prime⊆r, gives the dimensions to be displayed.\nFor example, given a dataset, E, in an (abstract) rep-\nresentation including {pitch, onset, duration}features,\nthe concatenator C{pitch}\n{onset}(E)creates a set of pitches or-\ndered by onset time; one might use it to extract the pitches\nin a monophonic melody. If we generalise this to arbitrary\nfeatures and combinations thereof, and consider only se-\nquences including the ﬁrst note of a piece, we arrive at the\nviewpoint representation of Conklin and Witten [3].\nEvidently, the projective properties of this operator ac-\ncount for representational invariances where the invariant\nfeature is an explicit feature in the representation, or a com-\nbination thereof. We use the term capture to denote this\ncapacity: so projection to subsets of the existing feature\nsetcaptures this kind of invariance.\nFor notational convenience we write operations applied\nto each member of an ordered set in order as operations on\nthe set itself, where this is unambiguous, so, where Ais a\nset of values and eis a value,A·e={a·e|a∈A};\nsimilarly, the elements of two sets of the same size, A,B\nmay be combined pairwise in order under ·:A·B={a·b|\nai∈A,bi∈B}. Finally, to combine a value, v∈Σf,\nunder an operation, ·, with one feature, f, of an element\n1This is a multiset because it is possible for the concatenator to map\nmore than one element of Eto any given element in the resulting repre-\nsentation; it may be necessary to know that this has happened.e, leaving other features unchanged, we write e·fv, so\ne+pitchkaddskto the pitch feature ofe.\nIn order neatly to specify a particular kind of derived\ninvariance, we use /angbracketleftS/angbracketright·\nf, whereSis an ordered set, to de-\nnote the ordered set produced by ordered, pairwise oper-\nation on the feature fof elements si∈Sunder·. So,\n/angbracketleftS/angbracketright−\nf={si+1−fsi|1≤i <|S|}. This oper-\nation has consequences for the representation of the re-\nsult: each feature type must be replaced by a derived type\n(corresponding with predeﬁned ones where appropriate).\nFor our concerns here, pitch is replaced by interval , and\nonset is replaced by ioi(inter-onset-interval), in the obvi-\nous way. We will need also a second-order derived invari-\nance to be used with onsets (arriving at ioi proportions),\nthus:/angbracketleft/angbracketleftS/angbracketright−\nonset/angbracketright÷\nioi={/angbracketleftsi+i/angbracketright−\nonset\n/angbracketleftsi/angbracketright−\nonset|1≤i<|S|−1}.\n2.3 Representational invariance\nSome invariances can be captured by a change of repre-\nsentation. Whether or not this is possible depends on the\nrepresentation used and on the nature of the phenomenon\nmodelled. In many cases, a change of representation like\nthis can usefully be thought of as indexing, and so it is\nhelpful to know what remains invariant.\nFor example, because pitch can be modelled by an\nAbelian group, it follows that for any set of pitches, E, thus\nmodelled, there is another set formed by combining a con-\nstant member of Σpitchunder the plus function in Φpitch\nwith each member of E(the members of Σpitchare by\ndeﬁnition in one-to-one correspondence with a partition of\nΣinterval ). It is implicit in the speciﬁcation of the abstract\nrepresentation that this operation, which is mathematically\ntranslation , models musical pitch transposition . Revers-\ning this argment, it follows that any sequence of pitches\ncan be expressed as a sequence of pitch differences, or\nintervals . Now, again because of the mathematical prop-\nerties of the representation, it happens that each such in-\nterval is represented in Σpitch, and the algebra deﬁned by\nΦpitchmodels the additive behaviour of intervals too: they\nalso form an Abelian group. Thus, it is possible to pro-\nduce a transposition-invariant version of any dataset, E, in\nany representation which contains pitch andonset infor-\nmation, by computing the ordered set whose members are\ncomputed by calculating /angbracketleftC{pitch}\n{onset, pitch}(E)/angbracketright−\np. If the mu-\nsic modelled by Eis monophonic, then this is the familiar\ninterval sequence representation; however, if the music is\nnot monophonic, care must be taken, because the relative\n59210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nnature of this representation makes its values dependent on\ntheir position in the sequence generated by the concate-\nnator; therefore, one cannot apply many of the operations\none would like. This fact is well-known to representers\nof music: an interval-based representation is not readily\namenable to the representation of non-monophonic music.\nHowever, in this change of representation, relatively little\ninformation is lost: just one constant value, which tells us\non what pitch the original dataset started; given that infor-\nmation, the entire original Emay be reconstructed. In this\nsense, we say that the change is structurally conservative .\nHowever, though useful in itself, this property is neither\nnecessary nor sufﬁcient for a transformation to be useful.\nFor example, a familiar invariance transformation is that\nbased on perceptual octave-equivalence, used in comput-\ning a chromagram. Here, perception maps exactly on to\nthe mathematics, and so perceptual octave equivalence can\nbe modelled by a chromatic equality function, deﬁned as\nequality modulo n, wherenis the number of divisions of\nthe octave being used in the underlying scale of the pitch\nsystem. Here, Σchroma can very usefully be a contiguous\nsubset of Σpitch, soZ12does very nicely, and Φchroma and\nΠchroma are equally easily deﬁned. However, this repre-\nsentation change is also not, in general, structurally conser-\nvative, and it is mathematically evident why: the mapping\nfromZtoZ12is many-to-one, and so information is lost.\nThe same principle, with a mapping to Z8, gives scale-\ndegree representation, which is also octave-invariant.\nA more interesting example is contour, an important as-\npect of melodic memory [8, §2.3]; Parsons coding [9] is a\ncommon way to represent the contour of music. However,\nΣParsons ={−,0,+}; it is not possible to give a fully de-\nﬁned plusfunction over this set, while maintaining it as a\nmodel of musical contour, for obvious reasons. Therefore,\nwe conﬁrm that information is lost in changing to a repre-\nsentation whose pitch is based on Parsons coding, and one\ncan argue this in advance because the abstract type of the\nParsons code is not as expressive as a linear Abelian group.\nThus, change of representation to Parsons code from, say,\nMIDI, is not structurally conservative. The same applies\nto comparable but more detailed interval-based representa-\ntions such as the qpi alphabet [4].\nParsons coding captures an invariance which is stronger\nthan transposition invariance in the sense that the equiva-\nlence classes it creates are fewer and larger. We will de-\nﬁne two such invariances. In these, contour is preserved,\nbut interval size is not—formal speciﬁcations are given in\nDeﬁnitions 4 and 5. Transposition from major to paral-\nlel minor is a (rather cautious) example of pitch warping;\nso, more generally, are interval augmentation and diminu-\ntion in contrapuntal theory, or expansion and contraction\nin the music of Bart ´ok. We note that among the passages\ncaptured by pitch warping lie also the equivalent transposi-\ntions, and this conﬁrms that the stronger pitch-warp invari-\nance is a indeed generalisation of transposition invariance.\nTherefore, a content-based music retrieval technique using\nParsons coding can be seen as a ﬁltering technique for ﬁnd-\ning transposed occurrences of a query (only ﬁltering andnot identifying, because false positives will be generated).\nOur remaining common musical features, onset time\nand note duration, and the corresponding invariances (see\nTable 1) can be dealt with in the obvious way using the\nconcatenator. For instance, given two datasets, BandB/prime\nin the same representation2, two ioisequences produced\nby the appropriate concatenator are time-scaled versions of\neach other if there is a number3dsuch that\nC{ioi}\n{onset}(B) =C{ioi}\n{onset}(B/prime)×ioid.\nA similar observation to that above, that time-warp invari-\nance is stronger than time-scale invariance, applies here.\n2.4 Algorithmic invariance\nMusic comparison is usually carried out in practice by an\nalgorithm using a distance measure. Like representations,\nmeasures can be invariant under some property. At this\nlevel, we speak about algorithmic invariances . The fol-\nlowing partial deﬁnition is a necessary but not sufﬁcient\ncondition to that end; it will be completed below.\nDeﬁnition 1 LetMbe a CBMR method and Pa property\nof a ﬁnite space4, where|P|is the size of the space under\nconsideration.Misalgorithmically P-invariant , if work-\ning on datasets in representations in which the underlying\ndatatype(s), explicit or implicit, of Pdoes not introduce a\nfactor into the computational complexity of Mthat is de-\npendent from|P|.\nThis deﬁnition rules out invariances achieved by dis-\ncretizing a search space, enumerating it, and then search-\ning exhaustively. Although such methods are sometimes\ncalledP-invariant in the MIR literature, this is really not\nthe case; they are methods that merely appear to take ad-\nvantange of invariance via brute-force calculation.\n2.4.1 Pitch invariances\nWe now deﬁne the invariances in our taxonomy (Table 1),\nstarting with pitch. Recall that our sets are by default\nordered multisets. We omit duration, which is derivable\nfrom ioi, and abbreviate{pitch, interval, onset, ioi}to\n{p,i,o, ioi}, respectively.\nDeﬁnition 2 Letrbe a representation including pitch and\nonset . A distance function Distransposition-invariant iff\n∀a,b∈Σp.∀A,Binr.D(Cˆr\n{o}(A),Cˆr\n{o}(B)) =\nD(Cˆr\n{o}(A) +pa,Cˆr\n{o}(B) +pb).\nIt may be helpful to visualise Deﬁnition 2, as in Fig. 1. In\nthis example, ˆr={p,o}.\nNote that Deﬁnition 2 captures the exact transposition\ninvariance that a music theorist would expect of that prop-\nerty. At times, however, it is useful to have a more relaxed\n2This restriction is not mathematically necessary, but to admit compar-\nison between representations here would over-complicate the example.\n3What kind of number depends on the kind of time representation: a\nmetrical one would use ZorQ; a real-time one might use R.\n4It may have been derived by quantizing a continuous space P/prime.\n593Poster Session 4\nnator; therefore, one cannot apply many of the operationsone would like. This fact is well-known to representersof music: an interval-based representation is not readilyamenable to the represenation of non-monophonic music.However, in this change of representation, relatively littleinformation is lost: just one constant value, which tells uson what pitch the original dataset started; given that infor-mation, the entire originalEmay be reconstructed. In thissense, we say that the change isstructurally conservative.However, this property is neither necessary nor sufﬁcientfor a transformation to be useful.For example, a familiar invariance transformation is thatbased on perceptual octave-equivalence, as is used, for ex-ample, in computing a chromagram. Here, perception mapsexactly on to the mathematics, and so perceptual octaveequivalence can be modelled by a chromatic equality func-tion, deﬁned as equality modulon, wherenis the num-ber of divisions of the octave being used in the underlyingscale of the pitch system. Here,Σchromacan very use-fully be a contiguous subset ofΣpitch, soZ12does verynicely, andΦchromaandΠchromaare equally easily de-ﬁned. However, this representation change is also not, ingeneral, structurally conservative, and it is now clear why,mathematically: the mapping fromZtoZ12is many-to-one, and so information is lost. The same principle, with amapping toZ8, gives scale-degree representation, which isalso octave-invariant.A more interesting example is contour, an important as-pect of melodic memory [7,§2.3]; Parsons coding [8] is acommon way to represent the contour of music. However,ΣParsons={−,0,+}; it is not possible to give a fully de-ﬁnedplusfunction over this set, while maintaining it as amodel of musical contour, for obvious reasons. Therefore,we conﬁrm that information is lost in changing to a repre-sentation whose pitch is based on Parsons coding, and onecan argue this in advance because the abstract type of theParsons code is not as expressive as a linear Abelian group.Thus, change of representation to Parsons code from, say,MIDI, is not structurally conservative. The same appliesto comparable but more detailed interval-based representa-tions such as theqpi alphabet[4].Parsons coding captures an invariance which isstrongerthan transposition invariance in the sense that the equiva-lence classes it creates are fewer and larger: we call thispitch warping. In pitch warping, contour is preserved, butinterval size is not—a formal deﬁnition is given in§2.4.Transposition from major to parallel minor is a (rather cau-tious) example of pitch warping; so, more generally, areinterval augmentation and diminution in contrapuntal the-ory. We note that among the passages captured by pitchwarping lie also the equivalent transpositions, and this con-ﬁrms that the stronger pitch warping invariance is a gener-alisation of transposition invariance. Therefore, a content-based music retrieval technique using Parsons coding canbe seen as a ﬁltering technique for ﬁnding transposed oc-currences of a query (only ﬁltering and not identifying, be-cause false positives will be generated).The remaining of our common musical features, onsettime and note duration, and the correspondingtime-scaleandtime-warpinvariances (see Table 1) can be dealt within the obvious way using the concatenator. For instance,given two datasets,BandB/primein the same representation2,two duration sequencesC{duration}{onset}(B)={b1, . . . , bn}andC{duration}{onset}(B/prime)={b/prime1, . . . , b/primen}are time-scaled ver-sions of each other if there is a number3dsuch thatb1=d.b/prime1, . . . , bn=d.b/primen. A similar observation to that above,that time-warp and concatenation invariances are strongerthan the corresponding time-scale invariances, applies here.2.4 Methodological invarianceThe actual music comparison or retrieval is carried outby an algorithm based on a distance measure. The mea-sures themselves can be invariant under some notion. Atthis level we speak aboutmethodological invariances.From here on, we abbreviate{pitch,onset,duration}to{p, o, d}, respectively.To be methodologically invariant under a propertyP(or methodologicallyP-invariant), a methodMshould beable to work on datasets in representations in whichPisexplicit or implicit, without enumerating all the possiblevalues ofP—which, of course, is impossible if the under-lying datatype(s) is (are) continuous. In this case, dealingwith the invariance must introduce at most a ﬁnite constantfactor into the computational complexity ofM. This deﬁ-nition deliberately rules out invariances which are achievedby discretizing the search space, enumerating the resultingset and then searching exhaustively. Although such meth-ods are sometimes calledP-invariant in the MIR literature,this is really not the case; they are merely methods that ap-pear to take advantange of invariance via brute-force cal-culation.We now deﬁne all the invariances that are given in ourtaxonomy, in Table 1. Recall that these areorderedsets.We begin with pitch invariances.Deﬁnition 1Letrbe a representation includingpitchandonset. A distance functionD1istransposition-invariantiff∀a, b∈Σp.∀A,Bin r.D1(C{p}{o}(A),C{p}{o}(B)) =D1(C{p}{o}(A)+pa, C{p}{o}(B)+pb).Still using the concatenatorCpo, we now deﬁne the strongerinvariance along the pitch dimension:Deﬁnition 2A distance functionD2ispitch-warp-invari-antiffD2(A,B)=D2(c1(a2−a1), . . . , cm−1(am−am−1),d1(b2−b1), . . . , dm−1(bm−bm−1)),for any interval sequencesA,BinΣ∗pand any positivevaluedc1, . . . , cm−1,d1, . . . , dm−1inΣp.2This restriction is not mathematically necessary, but to admit com-parison between representations here would cloud the example with un-necessary complication.3What kind of number depends on the kind of time representation:a metrical representation would useZorQ; a real-time representationmight useR.nator; therefore, one cannot apply many of the operationsone would like. This fact is well-known to representersof music: an interval-based representation is not readilyamenable to the represenation of non-monophonic music.However, in this change of representation, relatively littleinformation is lost: just one constant value, which tells uson what pitch the original dataset started; given that infor-mation, the entire originalEmay be reconstructed. In thissense, we say that the change isstructurally conservative.However, this property is neither necessary nor sufﬁcientfor a transformation to be useful.For example, a familiar invariance transformation is thatbased on perceptual octave-equivalence, as is used, for ex-ample, in computing a chromagram. Here, perception mapsexactly on to the mathematics, and so perceptual octaveequivalence can be modelled by a chromatic equality func-tion, deﬁned as equality modulon, wherenis the num-ber of divisions of the octave being used in the underlyingscale of the pitch system. Here,Σchromacan very use-fully be a contiguous subset ofΣpitch, soZ12does verynicely, andΦchromaandΠchromaare equally easily de-ﬁned. However, this representation change is also not, ingeneral, structurally conservative, and it is now clear why,mathematically: the mapping fromZtoZ12is many-to-one, and so information is lost. The same principle, with amapping toZ8, gives scale-degree representation, which isalso octave-invariant.A more interesting example is contour, an important as-pect of melodic memory [7,§2.3]; Parsons coding [8] is acommon way to represent the contour of music. However,ΣParsons={−,0,+}; it is not possible to give a fully de-ﬁnedplusfunction over this set, while maintaining it as amodel of musical contour, for obvious reasons. Therefore,we conﬁrm that information is lost in changing to a repre-sentation whose pitch is based on Parsons coding, and onecan argue this in advance because the abstract type of theParsons code is not as expressive as a linear Abelian group.Thus, change of representation to Parsons code from, say,MIDI, is not structurally conservative. The same appliesto comparable but more detailed interval-based representa-tions such as theqpi alphabet[4].Parsons coding captures an invariance which isstrongerthan transposition invariance in the sense that the equiva-lence classes it creates are fewer and larger: we call thispitch warping. In pitch warping, contour is preserved, butinterval size is not—a formal deﬁnition is given in§2.4.Transposition from major to parallel minor is a (rather cau-tious) example of pitch warping; so, more generally, areinterval augmentation and diminution in contrapuntal the-ory. We note that among the passages captured by pitchwarping lie also the equivalent transpositions, and this con-ﬁrms that the stronger pitch warping invariance is a gener-alisation of transposition invariance. Therefore, a content-based music retrieval technique using Parsons coding canbe seen as a ﬁltering technique for ﬁnding transposed oc-currences of a query (only ﬁltering and not identifying, be-cause false positives will be generated).The remaining of our common musical features, onsettime and note duration, and the correspondingtime-scaleandtime-warpinvariances (see Table 1) can be dealt within the obvious way using the concatenator. For instance,given two datasets,BandB/primein the same representation2,two duration sequencesC{duration}{onset}(B)={b1, . . . , bn}andC{duration}{onset}(B/prime)={b/prime1, . . . , b/primen}are time-scaled ver-sions of each other if there is a number3dsuch thatb1=d.b/prime1, . . . , bn=d.b/primen. A similar observation to that above,that time-warp and concatenation invariances are strongerthan the corresponding time-scale invariances, applies here.2.4 Methodological invarianceThe actual music comparison or retrieval is carried outby an algorithm based on a distance measure. The mea-sures themselves can be invariant under some notion. Atthis level we speak aboutmethodological invariances.From here on, we abbreviate{pitch,onset,duration}to{p, o, d}, respectively.To be methodologically invariant under a propertyP(or methodologicallyP-invariant), a methodMshould beable to work on datasets in representations in whichPisexplicit or implicit, without enumerating all the possiblevalues ofP—which, of course, is impossible if the under-lying datatype(s) is (are) continuous. In this case, dealingwith the invariance must introduce at most a ﬁnite constantfactor into the computational complexity ofM. This deﬁ-nition deliberately rules out invariances which are achievedby discretizing the search space, enumerating the resultingset and then searching exhaustively. Although such meth-ods are sometimes calledP-invariant in the MIR literature,this is really not the case; they are merely methods that ap-pear to take advantange of invariance via brute-force cal-culation.We now deﬁne all the invariances that are given in ourtaxonomy, in Table 1. Recall that these areorderedsets.We begin with pitch invariances.Deﬁnition 1Letrbe a representation includingpitchandonset. A distance functionD1istransposition-invariantiff∀a, b∈Σp.∀A,Bin r.D1(C{p}{o}(A),C{p}{o}(B)) =D1(C{p}{o}(A)+pa, C{p}{o}(B)+pb).Still using the concatenatorCpo, we now deﬁne the strongerinvariance along the pitch dimension:Deﬁnition 2A distance functionD2ispitch-warp-invari-antiffD2(A,B)=D2(c1(a2−a1), . . . , cm−1(am−am−1),d1(b2−b1), . . . , dm−1(bm−bm−1)),for any interval sequencesA,BinΣ∗pand any positivevaluedc1, . . . , cm−1,d1, . . . , dm−1inΣp.2This restriction is not mathematically necessary, but to admit com-parison between representations here would cloud the example with un-necessary complication.3What kind of number depends on the kind of time representation:a metrical representation would useZorQ; a real-time representationmight useR.nator; therefore, one cannot apply many of the operationsone would like. This fact is well-known to representersof music: an interval-based representation is not readilyamenable to the represenation of non-monophonic music.However, in this change of representation, relatively littleinformation is lost: just one constant value, which tells uson what pitch the original dataset started; given that infor-mation, the entire originalEmay be reconstructed. In thissense, we say that the change isstructurally conservative.However, this property is neither necessary nor sufﬁcientfor a transformation to be useful.For example, a familiar invariance transformation is thatbased on perceptual octave-equivalence, as is used, for ex-ample, in computing a chromagram. Here, perception mapsexactly on to the mathematics, and so perceptual octaveequivalence can be modelled by a chromatic equality func-tion, deﬁned as equality modulon, wherenis the num-ber of divisions of the octave being used in the underlyingscale of the pitch system. Here,Σchromacan very use-fully be a contiguous subset ofΣpitch, soZ12does verynicely, andΦchromaandΠchromaare equally easily de-ﬁned. However, this representation change is also not, ingeneral, structurally conservative, and it is now clear why,mathematically: the mapping fromZtoZ12is many-to-one, and so information is lost. The same principle, with amapping toZ8, gives scale-degree representation, which isalso octave-invariant.A more interesting example is contour, an important as-pect of melodic memory [7,§2.3]; Parsons coding [8] is acommon way to represent the contour of music. However,ΣParsons={−,0,+}; it is not possible to give a fully de-ﬁnedplusfunction over this set, while maintaining it as amodel of musical contour, for obvious reasons. Therefore,we conﬁrm that information is lost in changing to a repre-sentation whose pitch is based on Parsons coding, and onecan argue this in advance because the abstract type of theParsons code is not as expressive as a linear Abelian group.Thus, change of representation to Parsons code from, say,MIDI, is not structurally conservative. The same appliesto comparable but more detailed interval-based representa-tions such as theqpi alphabet[4].Parsons coding captures an invariance which isstrongerthan transposition invariance in the sense that the equiva-lence classes it creates are fewer and larger: we call thispitch warping. In pitch warping, contour is preserved, butinterval size is not—a formal deﬁnition is given in§2.4.Transposition from major to parallel minor is a (rather cau-tious) example of pitch warping; so, more generally, areinterval augmentation and diminution in contrapuntal the-ory. We note that among the passages captured by pitchwarping lie also the equivalent transpositions, and this con-ﬁrms that the stronger pitch warping invariance is a gener-alisation of transposition invariance. Therefore, a content-based music retrieval technique using Parsons coding canbe seen as a ﬁltering technique for ﬁnding transposed oc-currences of a query (only ﬁltering and not identifying, be-cause false positives will be generated).The remaining of our common musical features, onsettime and note duration, and the correspondingtime-scaleandtime-warpinvariances (see Table 1) can be dealt within the obvious way using the concatenator. For instance,given two datasets,BandB/primein the same representation2,two duration sequencesC{duration}{onset}(B)={b1, . . . , bn}andC{duration}{onset}(B/prime)={b/prime1, . . . , b/primen}are time-scaled ver-sions of each other if there is a number3dsuch thatb1=d.b/prime1, . . . , bn=d.b/primen. A similar observation to that above,that time-warp and concatenation invariances are strongerthan the corresponding time-scale invariances, applies here.2.4 Methodological invarianceThe actual music comparison or retrieval is carried outby an algorithm based on a distance measure. The mea-sures themselves can be invariant under some notion. Atthis level we speak aboutmethodological invariances.From here on, we abbreviate{pitch,onset,duration}to{p, o, d}, respectively.To be methodologically invariant under a propertyP(or methodologicallyP-invariant), a methodMshould beable to work on datasets in representations in whichPisexplicit or implicit, without enumerating all the possiblevalues ofP—which, of course, is impossible if the under-lying datatype(s) is (are) continuous. In this case, dealingwith the invariance must introduce at most a ﬁnite constantfactor into the computational complexity ofM. This deﬁ-nition deliberately rules out invariances which are achievedby discretizing the search space, enumerating the resultingset and then searching exhaustively. Although such meth-ods are sometimes calledP-invariant in the MIR literature,this is really not the case; they are merely methods that ap-pear to take advantange of invariance via brute-force cal-culation.We now deﬁne all the invariances that are given in ourtaxonomy, in Table 1. Recall that these areorderedsets.We begin with pitch invariances.Deﬁnition 1Letrbe a representation includingpitchandonset. A distance functionD1istransposition-invariantiff∀a, b∈Σp.∀A,Bin r.D1(C{p}{o}(A),C{p}{o}(B)) =D1(C{p}{o}(A)+pa, C{p}{o}(B)+pb).Still using the concatenatorCpo, we now deﬁne the strongerinvariance along the pitch dimension:Deﬁnition 2A distance functionD2ispitch-warp-invari-antiffD2(A,B)=D2(c1(a2−a1), . . . , cm−1(am−am−1),d1(b2−b1), . . . , dm−1(bm−bm−1)),for any interval sequencesA,BinΣ∗pand any positivevaluedc1, . . . , cm−1,d1, . . . , dm−1inΣp.2This restriction is not mathematically necessary, but to admit com-parison between representations here would cloud the example with un-necessary complication.3What kind of number depends on the kind of time representation:a metrical representation would useZorQ; a real-time representationmight useR.nator; therefore, one cannot apply many of the operationsone would like. This fact is well-known to representersof music: an interval-based representation is not readilyamenable to the represenation of non-monophonic music.However, in this change of representation, relatively littleinformation is lost: just one constant value, which tells uson what pitch the original dataset started; given that infor-mation, the entire originalEmay be reconstructed. In thissense, we say that the change isstructurally conservative.However, this property is neither necessary nor sufﬁcientfor a transformation to be useful.For example, a familiar invariance transformation is thatbased on perceptual octave-equivalence, as is used, for ex-ample, in computing a chromagram. Here, perception mapsexactly on to the mathematics, and so perceptual octaveequivalence can be modelled by a chromatic equality func-tion, deﬁned as equality modulon, wherenis the num-ber of divisions of the octave being used in the underlyingscale of the pitch system. Here,Σchromacan very use-fully be a contiguous subset ofΣpitch, soZ12does verynicely, andΦchromaandΠchromaare equally easily de-ﬁned. However, this representation change is also not, ingeneral, structurally conservative, and it is now clear why,mathematically: the mapping fromZtoZ12is many-to-one, and so information is lost. The same principle, with amapping toZ8, gives scale-degree representation, which isalso octave-invariant.A more interesting example is contour, an important as-pect of melodic memory [7,§2.3]; Parsons coding [8] is acommon way to represent the contour of music. However,ΣParsons={−,0,+}; it is not possible to give a fully de-ﬁnedplusfunction over this set, while maintaining it as amodel of musical contour, for obvious reasons. Therefore,we conﬁrm that information is lost in changing to a repre-sentation whose pitch is based on Parsons coding, and onecan argue this in advance because the abstract type of theParsons code is not as expressive as a linear Abelian group.Thus, change of representation to Parsons code from, say,MIDI, is not structurally conservative. The same appliesto comparable but more detailed interval-based representa-tions such as theqpi alphabet[4].Parsons coding captures an invariance which isstrongerthan transposition invariance in the sense that the equiva-lence classes it creates are fewer and larger: we call thispitch warping. In pitch warping, contour is preserved, butinterval size is not—a formal deﬁnition is given in§2.4.Transposition from major to parallel minor is a (rather cau-tious) example of pitch warping; so, more generally, areinterval augmentation and diminution in contrapuntal the-ory. We note that among the passages captured by pitchwarping lie also the equivalent transpositions, and this con-ﬁrms that the stronger pitch warping invariance is a gener-alisation of transposition invariance. Therefore, a content-based music retrieval technique using Parsons coding canbe seen as a ﬁltering technique for ﬁnding transposed oc-currences of a query (only ﬁltering and not identifying, be-cause false positives will be generated).The remaining of our common musical features, onsettime and note duration, and the correspondingtime-scaleandtime-warpinvariances (see Table 1) can be dealt within the obvious way using the concatenator. For instance,given two datasets,BandB/primein the same representation2,two duration sequencesC{duration}{onset}(B)={b1, . . . , bn}andC{duration}{onset}(B/prime)={b/prime1, . . . , b/primen}are time-scaled ver-sions of each other if there is a number3dsuch thatb1=d.b/prime1, . . . , bn=d.b/primen. A similar observation to that above,that time-warp and concatenation invariances are strongerthan the corresponding time-scale invariances, applies here.2.4 Methodological invarianceThe actual music comparison or retrieval is carried outby an algorithm based on a distance measure. The mea-sures themselves can be invariant under some notion. Atthis level we speak aboutmethodological invariances.From here on, we abbreviate{pitch,onset,duration}to{p, o, d}, respectively.To be methodologically invariant under a propertyP(or methodologicallyP-invariant), a methodMshould beable to work on datasets in representations in whichPisexplicit or implicit, without enumerating all the possiblevalues ofP—which, of course, is impossible if the under-lying datatype(s) is (are) continuous. In this case, dealingwith the invariance must introduce at most a ﬁnite constantfactor into the computational complexity ofM. This deﬁ-nition deliberately rules out invariances which are achievedby discretizing the search space, enumerating the resultingset and then searching exhaustively. Although such meth-ods are sometimes calledP-invariant in the MIR literature,this is really not the case; they are merely methods that ap-pear to take advantange of invariance via brute-force cal-culation.We now deﬁne all the invariances that are given in ourtaxonomy, in Table 1. Recall that these areorderedsets.We begin with pitch invariances.Deﬁnition 1Letrbe a representation includingpitchandonset. A distance functionD1istransposition-invariantiff∀a, b∈Σp.∀A,Bin r.D1(C{p}{o}(A),C{p}{o}(B)) =D1(C{p}{o}(A)+pa, C{p}{o}(B)+pb).Still using the concatenatorCpo, we now deﬁne the strongerinvariance along the pitch dimension:Deﬁnition 2A distance functionD2ispitch-warp-invari-antiffD2(A,B)=D2(c1(a2−a1), . . . , cm−1(am−am−1),d1(b2−b1), . . . , dm−1(bm−bm−1)),for any interval sequencesA,BinΣ∗pand any positivevaluedc1, . . . , cm−1,d1, . . . , dm−1inΣp.2This restriction is not mathematically necessary, but to admit com-parison between representations here would cloud the example with un-necessary complication.3What kind of number depends on the kind of time representation:a metrical representation would useZorQ; a real-time representationmight useR.sequencepitchnator; therefore, one cannot apply many of the operationsone would like. This fact is well-known to representersof music: an interval-based representation is not readilyamenable to the represenation of non-monophonic music.However, in this change of representation, relatively littleinformation is lost: just one constant value, which tells uson what pitch the original dataset started; given that infor-mation, the entire originalEmay be reconstructed. In thissense, we say that the change isstructurally conservative.However, this property is neither necessary nor sufﬁcientfor a transformation to be useful.For example, a familiar invariance transformation is thatbased on perceptual octave-equivalence, as is used, for ex-ample, in computing a chromagram. Here, perception mapsexactly on to the mathematics, and so perceptual octaveequivalence can be modelled by a chromatic equality func-tion, deﬁned as equality modulon, wherenis the num-ber of divisions of the octave being used in the underlyingscale of the pitch system. Here,Σchromacan very use-fully be a contiguous subset ofΣpitch, soZ12does verynicely, andΦchromaandΠchromaare equally easily de-ﬁned. However, this representation change is also not, ingeneral, structurally conservative, and it is now clear why,mathematically: the mapping fromZtoZ12is many-to-one, and so information is lost. The same principle, with amapping toZ8, gives scale-degree representation, which isalso octave-invariant.A more interesting example is contour, an important as-pect of melodic memory [7,§2.3]; Parsons coding [8] is acommon way to represent the contour of music. However,ΣParsons={−,0,+}; it is not possible to give a fully de-ﬁnedplusfunction over this set, while maintaining it as amodel of musical contour, for obvious reasons. Therefore,we conﬁrm that information is lost in changing to a repre-sentation whose pitch is based on Parsons coding, and onecan argue this in advance because the abstract type of theParsons code is not as expressive as a linear Abelian group.Thus, change of representation to Parsons code from, say,MIDI, is not structurally conservative. The same appliesto comparable but more detailed interval-based representa-tions such as theqpi alphabet[4].Parsons coding captures an invariance which isstrongerthan transposition invariance in the sense that the equiva-lence classes it creates are fewer and larger: we call thispitch warping. In pitch warping, contour is preserved, butinterval size is not—a formal deﬁnition is given in§2.4.Transposition from major to parallel minor is a (rather cau-tious) example of pitch warping; so, more generally, areinterval augmentation and diminution in contrapuntal the-ory. We note that among the passages captured by pitchwarping lie also the equivalent transpositions, and this con-ﬁrms that the stronger pitch warping invariance is a gener-alisation of transposition invariance. Therefore, a content-based music retrieval technique using Parsons coding canbe seen as a ﬁltering technique for ﬁnding transposed oc-currences of a query (only ﬁltering and not identifying, be-cause false positives will be generated).The remaining of our common musical features, onsettime and note duration, and the correspondingtime-scaleandtime-warpinvariances (see Table 1) can be dealt within the obvious way using the concatenator. For instance,given two datasets,BandB/primein the same representation2,two duration sequencesCduration{onset}(B)={b1, . . . , bn}andCduration{onset}(B/prime)={b/prime1, . . . , b/primen}are time-scaled versions ofeach other if there is a number3dsuch thatb1=d.b/prime1, . . . , bn=d.b/primen. A similar observation to that above, that time-warpand concatenation invariances are stronger than the corre-sponding time-scale invariances, applies here.2.4 Methodological invarianceThe actual music comparison or retrieval is carried outby an algorithm based on a distance measure. The mea-sures themselves can be invariant under some notion. Atthis level we speak aboutmethodological invariances.From here on, we abbreviate{pitch,onset,duration}to{p, o, d}, respectively.To be methodologically invariant under a propertyP(or methodologicallyP-invariant), a methodMshould beable to work on datasets in representations in whichPisexplicit or implicit, without enumerating all the possiblevalues ofP—which, of course, is impossible if the under-lying datatype(s) is (are) continuous. In this case, dealingwith the invariance must introduce at most a ﬁnite constantfactor into the computational complexity ofM. This deﬁ-nition deliberately rules out invariances which are achievedby discretizing the search space, enumerating the resultingset and then searching exhaustively. Although such meth-ods are sometimes calledP-invariant in the MIR literature,this is really not the case; they are merely methods that ap-pear to take advantange of invariance via brute-force cal-culation.We now deﬁne all the invariances that are given in ourtaxonomy, in Table 1. Recall that these areorderedsets.We begin with pitch invariances.Deﬁnition 1Letrbe a representation includingpitchandonset. A distance functionD1istransposition-invariantiff∀a, b∈Σp.∀A,Bin r.D1(Cˆr{o}(A),Cˆr{o}(B)) =D1(Cˆr{o}(A)+pa, Cˆr{o}(B)+pb).It may be helpful to visualise Deﬁnition 1, as in Fig. 1. Inthis example,r={p, o}.Still using the concatenatorCpo, we now deﬁne thestronger invariance along the pitch dimension:Deﬁnition 2A distance functionD2ispitch-warp-invari-antiffD2(A,B)=D2(c1(a2−a1), . . . , cm−1(am−am−1),d1(b2−b1), . . . , dm−1(bm−bm−1)),2This restriction is not mathematically necessary, but to admit com-parison between representations here would cloud the example with un-necessary complication.3What kind of number depends on the kind of time representation:a metrical representation would useZorQ; a real-time representationmight useR.nator; therefore, one cannot apply many of the operationsone would like. This fact is well-known to representersof music: an interval-based representation is not readilyamenable to the represenation of non-monophonic music.However, in this change of representation, relatively littleinformation is lost: just one constant value, which tells uson what pitch the original dataset started; given that infor-mation, the entire originalEmay be reconstructed. In thissense, we say that the change isstructurally conservative.However, this property is neither necessary nor sufﬁcientfor a transformation to be useful.For example, a familiar invariance transformation is thatbased on perceptual octave-equivalence, as is used, for ex-ample, in computing a chromagram. Here, perception mapsexactly on to the mathematics, and so perceptual octaveequivalence can be modelled by a chromatic equality func-tion, deﬁned as equality modulon, wherenis the num-ber of divisions of the octave being used in the underlyingscale of the pitch system. Here,Σchromacan very use-fully be a contiguous subset ofΣpitch, soZ12does verynicely, andΦchromaandΠchromaare equally easily de-ﬁned. However, this representation change is also not, ingeneral, structurally conservative, and it is now clear why,mathematically: the mapping fromZtoZ12is many-to-one, and so information is lost. The same principle, with amapping toZ8, gives scale-degree representation, which isalso octave-invariant.A more interesting example is contour, an important as-pect of melodic memory [7,§2.3]; Parsons coding [8] is acommon way to represent the contour of music. However,ΣParsons={−,0,+}; it is not possible to give a fully de-ﬁnedplusfunction over this set, while maintaining it as amodel of musical contour, for obvious reasons. Therefore,we conﬁrm that information is lost in changing to a repre-sentation whose pitch is based on Parsons coding, and onecan argue this in advance because the abstract type of theParsons code is not as expressive as a linear Abelian group.Thus, change of representation to Parsons code from, say,MIDI, is not structurally conservative. The same appliesto comparable but more detailed interval-based representa-tions such as theqpi alphabet[4].Parsons coding captures an invariance which isstrongerthan transposition invariance in the sense that the equiva-lence classes it creates are fewer and larger: we call thispitch warping. In pitch warping, contour is preserved, butinterval size is not—a formal deﬁnition is given in§2.4.Transposition from major to parallel minor is a (rather cau-tious) example of pitch warping; so, more generally, areinterval augmentation and diminution in contrapuntal the-ory. We note that among the passages captured by pitchwarping lie also the equivalent transpositions, and this con-ﬁrms that the stronger pitch warping invariance is a gener-alisation of transposition invariance. Therefore, a content-based music retrieval technique using Parsons coding canbe seen as a ﬁltering technique for ﬁnding transposed oc-currences of a query (only ﬁltering and not identifying, be-cause false positives will be generated).The remaining of our common musical features, onsettime and note duration, and the correspondingtime-scaleandtime-warpinvariances (see Table 1) can be dealt within the obvious way using the concatenator. For instance,given two datasets,BandB/primein the same representation2,two duration sequencesCduration{onset}(B)={b1, . . . , bn}andCduration{onset}(B/prime)={b/prime1, . . . , b/primen}are time-scaled versions ofeach other if there is a number3dsuch thatb1=d.b/prime1, . . . , bn=d.b/primen. A similar observation to that above, that time-warpand concatenation invariances are stronger than the corre-sponding time-scale invariances, applies here.2.4 Methodological invarianceThe actual music comparison or retrieval is carried outby an algorithm based on a distance measure. The mea-sures themselves can be invariant under some notion. Atthis level we speak aboutmethodological invariances.From here on, we abbreviate{pitch,onset,duration}to{p, o, d}, respectively.To be methodologically invariant under a propertyP(or methodologicallyP-invariant), a methodMshould beable to work on datasets in representations in whichPisexplicit or implicit, without enumerating all the possiblevalues ofP—which, of course, is impossible if the under-lying datatype(s) is (are) continuous. In this case, dealingwith the invariance must introduce at most a ﬁnite constantfactor into the computational complexity ofM. This deﬁ-nition deliberately rules out invariances which are achievedby discretizing the search space, enumerating the resultingset and then searching exhaustively. Although such meth-ods are sometimes calledP-invariant in the MIR literature,this is really not the case; they are merely methods that ap-pear to take advantange of invariance via brute-force cal-culation.We now deﬁne all the invariances that are given in ourtaxonomy, in Table 1. Recall that these areorderedsets.We begin with pitch invariances.Deﬁnition 1Letrbe a representation includingpitchandonset. A distance functionD1istransposition-invariantiff∀a, b∈Σp.∀A,Bin r.D1(Cˆr{o}(A),Cˆr{o}(B)) =D1(Cˆr{o}(A)+pa, Cˆr{o}(B)+pb).It may be helpful to visualise Deﬁnition 1, as in Fig. 1. Inthis example,r={p, o}.Still using the concatenatorCpo, we now deﬁne thestronger invariance along the pitch dimension:Deﬁnition 2A distance functionD2ispitch-warp-invari-antiffD2(A,B)=D2(c1(a2−a1), . . . , cm−1(am−am−1),d1(b2−b1), . . . , dm−1(bm−bm−1)),2This restriction is not mathematically necessary, but to admit com-parison between representations here would cloud the example with un-necessary complication.3What kind of number depends on the kind of time representation:a metrical representation would useZorQ; a real-time representationmight useR.Figure 1 . Visualisation of Deﬁnition 2. ˆr={p,o}.\nversion of transposition invariance. Indeed, the following\npitch-transposition invariance , which omits the exact on-\nset times, is often used in music retrieval applications.\nDeﬁnition 3 Letrbe a representation including pitch\nand onset . A distance function Dispitch-transposition-\ninvariant iff\n∀a,b∈Σp.∀A,Binr.D(Cˆr\\{o}\n{o}(A),Cˆr\\{o}\n{o}(B)) =\nD(Cˆr\\{o}\n{o}(A) +pa,Cˆr\\{o}\n{o}(B) +pb).\nStronger kinds of pitch invariance than the above (as\ndeﬁned in Section 2.3) are deﬁned as follows.\nDeﬁnition 4 Letrbe a representation including pitch and\nonset . A distance function Dispitch-warp-invariant iff\n∀KA∈N|A|−1.∀KB∈N|B|−1.∀A,Binr.\nD/parenleftBig\n/angbracketleftCˆr\n{o}(A)/angbracketright−\np,/angbracketleftCˆr\n{o}(B)/angbracketright−\np/parenrightBig\n=\nD/parenleftBig\n/angbracketleftCˆr\n{o}(A)/angbracketright−\np×iKA,/angbracketleftCˆr\n{o}(B)/angbracketright−\np×iKB/parenrightBig\nwhereNis one of Z+,Q+,R+.\nNote that the multiplication operation here needs to be\nduly deﬁnable in terms of functions in Φi. If we omit the\nonset information of that above, we get Parsons invariance:\nDeﬁnition 5 Letrbe a representation including pitch and\nonset . A distance function DisParsons-invariant iff\n∀KA∈N|A|−1.∀KB∈N|B|−1.∀A,Binr.\nD/parenleftBig\n/angbracketleftCˆr\\{o}\n{o}(A)/angbracketright−\np,/angbracketleftCˆr\\{o}\n{o}(B)/angbracketright−\np/parenrightBig\n=\nD/parenleftBig\n/angbracketleftCˆr\\{o}\n{o}(A)/angbracketright−\np×iKA,/angbracketleftCˆr\\{o}\n{o}(B)/angbracketright−\np×iKB/parenrightBig\nwhereNis one of Z+,Q+,R+.\n2.4.2 Temporal invariances.\nWe now move to temporal invariances. The ﬁrst allows\nfor linear time shifts. So, for instance, in musical pattern\nmatching, the pattern may occur anywhere in the database,\nnot just as an incipit. Being additive, it is usually easily\ncombined with the ﬁrst pitch invariances, above.Deﬁnition 6 Letrbe a representation including pitch and\nonset . A distance function Distime-position-invariant iff\n∀a,b∈Σo.∀A,Binr.D(Cˆr\n{o}(A),Cˆr\n{o}(B)) =\nD(Cˆr\n{o}(A) +oa,Cˆr\n{o}(B) +ob).\nNote that the above invariance is not meaningful with\ndurations. The next two temporal invariances are of\nmultiplicative nature, the ﬁrst of which, the time-scale-\ninvariance, is applicable both with onsets and durations.\nDeﬁnition 7 Letrbe a representation including pitch and\nonset . A distance function Distime-scale-invariant iff\n∀FA∈N|A|,FB∈N|B|,KA∈Σ|A|\no,KB∈Σ|B|\no.\n∀A,Binr.D(Cˆr\n{o}(A),Cˆr\n{o}(B)) =\nD(Cˆr\n{o}(A)×oFA+oKA,Cˆr\n{o}(B)×oFB+oKB).\nwhereNis one of Z+,Q+,R+.\nThe next duration-warp invariance is most useful with\nduration sequences; it is “durational Parsons invariance”,\ni.e., the one for which “shorter, longer, same” encoding is\noften used. To this end we use the second order derivation\nof setsAandBwith ioiproportions, abbreviated ipbelow.\nDeﬁnition 8 Letrbe a representation including pitch and\nonset . A distance function Disduration-warp-invariant iff\n∀KA∈N|A|−2,KB∈N|B|−2.∀A,Binr.\nD/parenleftBig\n/angbracketleft/angbracketleftCˆr\n{o}(A)/angbracketright−\no/angbracketright÷\nioi,/angbracketleft/angbracketleftCˆr\n{o}(B)/angbracketright−\no/angbracketright÷\nioi/parenrightBig\n=\nD/parenleftBig\n/angbracketleft/angbracketleftCˆr\n{o}(A)/angbracketright−\no/angbracketright÷\nioi∧ipKA,/angbracketleft/angbracketleftCˆr\n{o}(B)/angbracketright−\no/angbracketright÷\nioi∧ipKB/parenrightBig\nwhere∧is the power operator and Nis one of\nZ+,Q+,R+.\nThe last temporal invariance does not bother with the\nonset information, except in as far as order is preserved.\nThis is the case, for instance, with CBMR methods based\non string representations that omit explicit onset times.\nNote that, although it is temporal, there is no intuitive in-\nterpretation of this invariance to duration information.\nDeﬁnition 9 Letrbe a representation including pitch and\nonset , and letKA∈N|A|,KB∈N|B|be such that\nai−1+oKA(i−1)< a i+oKA(i)and\nbi−1+oKB(i−1)< b i+oKB(i)\nfor2≤i≤|KA|,|KB|. A distance function Distime-\nwarp-invariant iff\n∀A,Binr.D/parenleftBig\nCˆr\\{o}\n{o}(A),Cˆr\\{o}\n{o}(B)/parenrightBig\n=\nD/parenleftBig\nCˆr\\{o}\n{o}(A) +oKA,Cˆr\\{o}\n{o}(B) +oKB/parenrightBig\nwhereNis one of Z,Q,R.\nNow, we can fully deﬁne algorithmic invariance.\nDeﬁnition 10 A methodMisalgorithmically P-invariant\niffMsatisﬁes Deﬁnition 1 and its similarity measure sat-\nisﬁes the deﬁnitions above corresponding with property P.\n59410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n3. STRUCTURAL INV ARIANCES\nLet us now consider a set of stronger invariances that relate\nprimarily not to the music represented, but to the results\nproven using our order-based formalism. To be maximally\nuseful, it is helpful to know how strongly the results apply:\nin particular, does the order imposed by our concatenator\nmake a difference to the outcome? For example, in the fol-\nlowing permutation invariances , when applied to contour-\nbased melody comparison, onset-order matters, but in a\npitch-class-distribution comparison, it probably does not.\nDeﬁnition 11 Letrbe a representation and ω⊆ˆr. A\ndistance functionDisω-permutation-invariant iff\n∀A,Binr.D(Cˆr\nω(A),Cˆr\nω(B)) =\nD(P(Cˆr\nω(A)),P(Cˆr\nω(B)))\nwherePis any size-preserving permutation operator on ω.\nIfω= ˆr, the distance function is strongly permutation-in-\nvariant .\nFurther, it may be useful to know that a distance is pre-\nserved no matter which dimension is used for ordering.\nDeﬁnition 12 Letrbe a representation. A distance func-\ntionDisω- concatenation-invariant iff\n∀ω1,ω2⊂ˆr.∀A,Binr.D(Cˆr\nω1(A),Cˆr\nω1(B)) =\nD(Cˆr\nω2(A),Cˆr\nω2(B)).\nIfω1,ω2= ˆr, the distance function is strongly concatena-\ntion-invariant .\nFor a strongly concatenation-invariant distance function\nthe ordering does not make any difference at all. Note that\na strongly permutation invariant distance function is also a\nstrongly concatenation invariant, and vice versa.\n4. INV ARIANCES IN POLYPHONIC\nCONTENT-BASED MUSIC RETRIEV AL\n4.1 Representations of non-monophonic music\nThe concatenated representations used here are evidently\ndirectly applicable when dealing with monophonic music.\nIn the case of (discretely represented) polyphonic music,\na geometrical representation [1, 11, 12, 14] is a more ef-\nfective and natural choice [5]. An example of geometrical\nmusic matching (under transpositional equivalence, in this\nexample) is given in Figure 2, where the common pitch-\nagainst time-representation, giving the onset times but not\ndurations, is used. Several possible ways to represent du-\nrations have been suggested [10, 11, 12].\nAs Figure 2 suggests, the maximal subset match of the\ngiven query pattern of length mwithin the database of\nlengthncan be found by observing the translation vec-\ntors. Note that a translation corresponds to two musically\ndistinct phenomena: a vertical move corresponds to pitch-\nshift while a horizontal move corresponds to aligning the\npattern time-wise; the combination of these is what a mu-\nsician calls “a transposition” (to be distinguished from the\nprocess of transposition, performed during performance).\n234pitch\ntime\nP TFigure 2 . PointsetP, to the right, represents a pointset\n(musical) pattern to be matched against a pointset database\nto the left. The arrows represent translation vectors, from\npattern to database, that give maximal occurrence.\nThus, working on the translation vectors captures transpo-\nsition and position invariances, in the terms deﬁned here.\nUkkonen et al. [12] gave an algorithm to solve the max-\nimal subset matching problem in O(mnlogm)time. It\nis still the fastest known deterministic algorithm for the\nproblem. Clifford et al. [2] showed that quadratic running\ntimes are probably the best one can achieve for this prob-\nlem by proving that the maximal subset matching problem\nis3SUM -hard. They also gave a randomized algorithm for\nthe problem that works in time O(nlogn).\n4.2 Combining invariances\nWhen using the sequence (string) representation, pitch-\ntransposition invariance is easily combined with time-warp\ninvariance (and the latter serves as a ﬁltering method for\ntime-scale invariance). However, the explicit encoding of\nthe onset times in the geometrical representation makes\nit difﬁcult to combine transposition invariance with most\nof the temporal invariances, such as time-scale invariance.\nThe difﬁculty of combining transposition invariance and\ntime-scale invariance is due to the fact that the former is an\nadditive property, while the latter is multiplicative.\nRomming and Selfridge-Field [10] gave the only known\nnon-brute-force algorithm capable of dealing with poly-\nphonic music, transposition invariance and time-scale in-\nvariance. Their algorithm is based on geometrical hashing\nand works in O(n3)space andO(n2m3)time. By apply-\ning a window on the database such that wis the maximum\nnumber of events that occur in any window, the above com-\nplexities can be restated as O(w2n)andO(wnm3), re-\nspectively. The algorithm works on all three of the mu-\nsical features discussed here (pitch, onset time and dura-\ntion), ﬁnding a maximal subset match in such a scenario.\nHowever, as with the SIA algorithm family [7], its appli-\ncability to real world problems is reduced due to the fact\nthat matches are mathematically exact, and so performance\nexpression and error is difﬁcult to account for.\n595Poster Session 4\n5. CONCLUSIONS\nIn this paper we have discussed invariances related to\ncontent-based music retrieval; they are central concepts\nin deﬁning and developing effective representations, sim-\nilarity measures and algorithms to that end. Because of\ntheir centrality to the matter, invariances are widely used\nin the literature—but very seldom are they properly deﬁned\nor their relationship discussed which has occasionally re-\nsulted in misuse of the term and confusion.\nWe have given a sparse taxonomy of the invariances\nalong three featural dimensions of music—pitch, onset\ntime and duration. We also deﬁned stronger invariances,\nintrinsic to our formalism. The taxonomy shows explicitly\nthe relationships of these invariances to each other. More-\nover, we have precisely deﬁned them, minimizing confu-\nsion in future discussion. The taxonomy works also as a\nuseful tool in discussing what has been done, and in iden-\ntifying where there is still much space for future develop-\nments towards efﬁcient and effective CBMR tools.\nIt seems that the geometrical framework provides the\nbest (and most natural) representation when dealing with\npolyphonic music. Using this framework, however, it is\nnot easy to combine translation and time-scale invariances\nin a computationally efﬁcient way; there is still a huge\ngap to be bridged in this respect to be able to meet the\nreal world requirements for responsive and error-tolerant\ndatabase queries. One way to improve error-tolerance—as\nis evident in our taxonomy—would be to adapt the geo-\nmetrical frameworks to work also on the level of the more\ngeneral invariances. To date, there is next to no work in\nthis direction, though Lubiw and Tanur [6] presented an\nalgorithm that measures the distance between the desired\npitches and observed pitches that are combined in a ﬁnal\nsimilarity value. So, with respect to our taxonomy, their\nwork resides somewhere in between the two ends. Their\nmethod, although built on discrete space, does not straight-\nforwardly lend itself to a non-strict time-scale invariance.\nWe are currently studying how to adapt the geometri-\ncal approach to the more general classes of our taxonomy\nthus achieving more error-tolerant geometrical methods for\ncontent-based music retrieval. Another direction is to re-\nﬁne the deﬁnitions in order to be able to discriminate meth-\nods that allow“gaps” (as the geometrical methods usually\ndo) from those that do not (for instance, methods based on\nexact string matching).\n6. ACKNLOWEDGEMENTS\nWe gratefully acknowledge funding for Kjell Lemstr ¨om\nfrom the Academy of Finland (Grants #108547 and\n#129909). The work was carried out during his visit to\nthe ISMS group at Goldsmiths, University of London. We\nalso acknowledge the intellectual contribution of the ISMS\ngroup, in particular Richard Lewis.\n7. REFERENCES\n[1] M. Clausen, R. Engelbrecht, D. Meyer, and\nJ. Schmitz. Proms: A web-based tool for searchingin polyphonic music. In Proc. ISMIR’00 , Plymouth,\nMA, October 2000.\n[2] R. Clifford, M. Christodoulakis, T. Crawford,\nD. Meredith, and G. Wiggins. A fast, randomised,\nmaximal subset matching algorithm for document-\nlevel music retrieval. In Proc. ISMIR’06 , pp. 150–\n155, Victoria, BC, Canada, October 2006.\n[3] D. Conklin and I. H. Witten. Multiple viewpoint sys-\ntems for music prediction. J. New Music Research ,\n24:51–73, 1995.\n[4] K. Lemstr ¨om and P. Laine. Musical information re-\ntrieval using musical parameters. In Proc. ICMC’98 ,\npages 341–348, Ann Arbor, MI, 1998.\n[5] K. Lemstr ¨om and A. Pienim ¨aki. On comparing edit\ndistance and geometric frameworks in content-based\nretrieval of symbolically encoded polyphonic music.\nMusicae Scientiae , 4a:135–152, 2007.\n[6] A. Lubiw and L. Tanur. Pattern matching in\npolyphonic music as a weighted geometric transla-\ntion problem. In Proc. ISMIR’04 , pages 289–296,\nBarcelona, October 2004.\n[7] D. Meredith, K. Lemstr ¨om, and G. A. Wiggins. Al-\ngorithms for discovering repeated patterns in multi-\ndimensional representations of polyphonic music. J.\nNew Music Research , 31(4):321–345, 2002.\n[8] D. M ¨ullensiefen, G. A. Wiggins, and D. Lewis.\nHigh-level feature descriptors and corpus-based mu-\nsicology: Techniques for modelling music cogni-\ntion. In A. Schneider, editor, Systematic and Com-\nparative Musicology: Concepts, Methods, Findings ,\nnumber 24 in Hamburger Jahrbuch f ¨ur Musikwis-\nsenschaft. Peter Lang, Frankfurt am Main, 2008.\n[9] D. Parsons. The Directory of Tunes and Musical\nThemes . S. Brown (Cambridge, Eng.), 1975.\n[10] C. A. Romming and E. Selfridge-Field. Algorithms\nfor polyphonic music retrieval: The hausdorff metric\nand geometric hashing. In Proc. ISMIR’07 , Vienna,\nAustria, October 2007.\n[11] R. Typke, P. Giannopoulos, R. C. Veltkamp, F. Wier-\ning, and R.v. Oostrum. Using transportation distances\nfor measuring melodic similarity. In Proc. ISMIR’03 ,\npp. 107–114, Baltimore, MA, October 2003.\n[12] E. Ukkonen, K. Lemstr ¨om, and V . M ¨akinen. Geo-\nmetric algorithms for transposition invariant content-\nbased music retrieval. In Proc. ISMIR’03 , pages 193–\n199, Baltimore, MA, October 2003.\n[13] G. A. Wiggins, M. Harris, and A. Smaill. Represent-\ning music for analysis and composition. In M. Bala-\nban, K. Ebcioglu, O. Laske, C. Lischka, and L. Sori-\nsio, editors, Proc. 2nd IJCAI AI/Music Workshop ,\npages 63–71, Detroit, Michigan, 1989.\n[14] G. A. Wiggins, K. Lemstr ¨om, and D. Meredith.\nSIA(M)ESE: An algorithm for transposition invari-\nant, polyphonic content-based music retrieval. In\nProc. ISMIR’02 , pages 283–284, Paris, France, Oc-\ntober 2002.\n596"
    },
    {
        "title": "Optical Audio Reconstruction for Stereo Phonograph Records Using White Light Interferometry.",
        "author": [
            "Beinan Li",
            "Jordan B. L. Smith",
            "Ichiro Fujinaga"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416230",
        "url": "https://doi.org/10.5281/zenodo.1416230",
        "ee": "https://zenodo.org/records/1416230/files/LiSF09.pdf",
        "abstract": "Our work focuses on optically reconstructing the stereo audio signal of a 33 rpm long-playing (LP) record using a white-light interferometry-based approach. Previously, a theoretical framework was presented, alongside the primitive reconstruction result from a few cycles of a stereo sinusoidal test signal. To reconstruct an audible duration of a longer stereo signal requires tackling new problems, such as disc warping, image alignment, and eliminating the effects of noise and broken grooves. This paper proposes solutions to these problems, and presents the complete workflow of our Optical Audio Reconstruction (OAR) system.",
        "zenodo_id": 1416230,
        "dblp_key": "conf/ismir/LiSF09",
        "keywords": [
            "optically reconstructing",
            "stereo audio signal",
            "33 rpm long-playing record",
            "white-light interferometry",
            "stereo sinusoidal test signal",
            "audio reconstruction",
            "disc warping",
            "image alignment",
            "noise elimination",
            "broken grooves"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   OPTICAL AUDIO RECONSTRUCTION FOR STEREO PHONOGRAPH RECORDS USING WHITE LIGHT INTERFEROMETRY   Beinan Li  Jordan B. L. Smith  Ichiro Fujinaga Music Technology Area Schulich School of Music McGill University beinan.li@ mail.mcgill.ca Music Technology Area Schulich School of Music McGill University jordan.smith2@ mail.mcgill.ca Music Technology Area Schulich School of Music McGill University ich@music.mcgill.ca  ABSTRACT  Our work focuses on optically reconstructing the stereo audio signal of a 33 rpm long-playing (LP) record using a white-light interferometry-based approach. Previously, a theoretical framework was presented, alongside the primitive reconstruction result from a few cycles of a stereo sinusoidal test signal. To reconstruct an audible duration of a longer stereo signal requires tackling new problems, such as disc warping, image alignment, and eliminating the effects of noise and broken grooves. This paper proposes solutions to these problems, and presents the complete workflow of our Optical Audio Recon-struction (OAR) system.  1. INTRODUCTION  OAR has proven to be an effective contactless approach to digitizing monophonic phonograph records [1] [2] [3] [4]. Furthermore, it is an available solution for restoring broken records. Li et al. previously presented a theoretical framework for optically reconstructing audio with a white-light interferometry (WLI) microscope and image processing [5]. A few cycles of stereo sinusoidal signal, extracted from a small number of images, illustrated that their approach is capable of extracting stereo signals from LPs. To reconstruct a few seconds of audio, however, the scanning region must be scaled up to a much larger disc area, resulting in thousands of images. A sophisticated image acquisition and post-capture processing workflow is thus desired to tackle the challenges that emerge from large-scale scanning: e.g., disc surface warping, image alignment errors, groove damages, and unwrapping the grooves into a one-dimensional audio signal.  In Section 2, we review previous OAR systems. Our system to acquire record groove images is introduced in Section 3, followed in Section 4 by our image processing procedures for extracting audio from the scanned images. The reconstructed result is illustrated and discussed in Section 5.   2. EXISTING OAR APPROACHES  In this section, four previous OAR approaches are described. Although they operate on recordings of different formats, most OAR frameworks follow the same high-level three-step procedure for reconstructing an audio recording: first, the grooves are scanned; second, the groove undulations are isolated and extracted; third, these undulations are converted into audio. Approaches vary significantly in terms of the hardware used, some using a general-purpose commercial product such as a confocal microscope, others using a custom installation. The hardware, in turn, affects how the grooves are scanned and thus how groove undulations must be extracted. By contrast, the audio conversion step (which may include post-processing, such as equalization) depends solely on the record production procedures that were used for the particular item being scanned. This step almost always includes filtering the signal to undo the RIAA equalization used in production and obtain the audio.  The systems developed by Iwai et al. and Nakamura et al. use a ray-tracing method to obtain the groove contour of a phonograph record [6] [7] [8]. The groove is illuminated with a laser beam, and the groove undulations are measured by detecting the angle at which the beam is reflected. In this way the laser functions as a simulated stylus—a replacement for the mechanical stylus—and can output an analog audio signal directly.  Unfortunately, since such systems must trace out the grooves, they are unable to handle broken records. In addition, two types of errors limit this approach: the errors caused by the finite laser beam width, which leads to echoes and high- and low-frequency noise in the extracted audio signals, and the tracking errors that may occur when the beam misses the groove entirely.  Fadeyev and Haber built an OAR system for 78-rpm records based on confocal laser scanning microscopy [1]. With the help of a low-mass probe, they built another one for wax cylinders [2]. Their system is capable of scanning the record in 3D with a vertical accuracy of around 4.0 microns. However, in their work on 78-rpm records only 2D imaging is emphasized, at a resolution of 0.26 x 0.29 microns per pixel. It takes their system 50 minutes to scan about 1 second of recorded audio, corresponding to 0.5–5 GB of image data. The groove bottom is obtained using 2D edge detection on the pixel illumination data,  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2009 International Society for Music Information Retrieval  \n627Poster Session 4\n   and the groove undulation is defined as the radial deviation of the groove bottom with respect to an unmodulated trajectory about the centre of the record.  Lengths of the groove are skipped when dust and debris occlude the image and no edges are detectable, but no solution to data restoration is provided; such skipping may thus cause a loss of data. Fadeyev and Haber compared their optically reconstructed audio sample, a turntable-digitized audio sample, and a remastered CD sample of the same recording. The quality of the reconstructed sample was judged to be better than the turntable-reconstructed version, but poorer than the CD version: while the OAR system produced fewer clips and pops than the turntable, and had a lower continuous noise level, it also contained background hissing and low-frequency modulation absent from the CD version.  Stotzer et al. created a system that performs a multi-step optical reconstruction for 78-rpm records [3]. First, a 2D analog camera is used to rapidly photograph the records and the images are preserved on film. This film is then scanned and digitized into 2D digital images, which are further analyzed to extract the audio. During the scanning, the film is placed on a rotating tray with an overhead stationary camera that carefully captures the groove images passing through its field of view (FOV). The rotation of the tray simulates the rotation of the record during playback, and effectively unwraps the groove segments to become uniformly oriented in the resulting images. Similar to Fadeyev and Haver’s strategy, edge detection is then used on these images to extract the groove undulations, which are simply described by the edge that separates the groove valley from the space between grooves.  In this system, the imaging resolution is compro-mised by shading blur, motion blur, and sampling blur introduced by the illumination and the rotating scanner. The blur is estimated to be roughly 24.6 microns along the direction of rotation. The system also suffers from various acquisition artifacts, such as the very low-frequency noise caused by the off-axis placement of the film.  In an effort to restore damaged grooves, smoothing and corrupted-pixel-map-based enhancement are per-formed. The robustness of the damage detection is nevertheless questionable due to the simplifying assumption that damages such as scratches are solely perpendicular to the grooves. The blurring described above also makes it difficult to reliably detect scratches in the grooves.  The reconstructed sound quality achieved by their system was evaluated according to several standard audio engineering parameters; for example, the signal-to-noise ratio of the system was found to be roughly 16dB.  Tian used 3D scanning based on dual-focal microscopy for reconstructing audio from 78-rpm records. The ability to handle LPs is claimed, but not yet implemented [4]. Contrary to the aforementioned approaches, the groove undulation is defined by the groove sidewall orientation at each tangential increment relative to the disc center, instead of by the edges of either the top or bottom of the groove. Ray-tracing is used to create a 3D image of the entire record groove surfaces, including the sidewalls. The stylus movement across the grooves is represented by the optical flow derived from groove image intensity derivatives. The sidewall orientations are obtained by using dense depth maps and projecting complex surface onto the groove cross-section plane. The microscope in use has a lateral resolution of 1µm per pixel. Tian’s optical flow approach requires 1390 x 36 images of the FOV 640 x 480 pixels to represent a two-second audio signal, although on which groove revolution the reconstruction is performed is not reported. It takes three days for four workstations to generate the image representation of a three-second audio. The image acquisition time is unclear in the literature. The equivalent audio sampling rate in their experiment is about 2404.71 Hz.  Although laser turntables can serve as a solution to optically retrieve audio from phonograph records, they require emulating the exact groove-following behavior of a turntable. We would like to find a general image-acquisition-based preservation solution without mimicking turntable behavior to derive digital audio directly from images. We also wish to obtain 3D information: although Fadeyev and Haber claimed their system can be adapted to a 3D groove profile, they did not implement it, while the system of Stotzer el al. does not retrieve 3D information. Tian’s system is 3D-based, but his experiments are not performed on stereo LPs, the target considered here. Thus, in contrast to these works, our research focuses on optically reconstructing a digital stereo audio signal from LPs by extracting the lateral and vertical groove undulations from 3D groove information.  3. WLI-BASED IMAGE ACQUISITION  WLI is a powerful scanning technique based on physical optics, as opposed to the geometrical-optics-based approaches that include confocal microscopy and ray-tracing. In WLI, a broadband light source simulates an ensemble of narrow-band interferometers to make high- precision measurements. We used a Wyko NT8000 WLI microscope equipped with both Michelson and Mirau interferometers. Adjusting the vertical focus of these allows one to perform vertical scanning interferometry with a vertical resolution of better than 1 nm [9]. (The amplitudes of the groove depth are typically on the order of 25–100 µm [3].) In the current experiment, only the Michelson interferometer (10X magnification) is used, with a 0.644 x 0.483 mm (or 640 x 480 pixel) FOV. Using the 3X vertical scan speed with 20% overlap between fields of view, it takes roughly 27 minutes to scan one second of audio content. This configuration provides a reasonable tradeoff between acquisition quality and time cost.  Due to warping of the disc surface, the phonograph record is not perfectly flat. Expanding the scanning range to span all possible groove depths is unfeasible because of the greatly increased time cost. On the other hand, re-estimating for each FOV the range of groove depths to scan takes too long as well. To minimize the time cost \n62810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   without risking unfocused images, a hierarchical scanning scheme was chosen: the entire grid of FOVs is divided into sub-regions, and the scanning depth is adjusted only for each sub-region. Compared to the approach with a global range, this adaptive scheme reduces the scanning time by half.   4. AUDIO EXTRACTION BY IMAGE PROCESSING  Once the images have been acquired, the next step is to extract from them the groove undulations, which can then be converted into audio. A stereo phonograph audio signal is encoded in lateral and vertical groove undulations, so both must be extracted. To do so, first the entire grid of FOVs must be realigned using a dynamic programming algorithm. Next, the structures that define groove undulations are identified (and restored, where damaged). Finally, the groove is traced over the entire field and unwrapped, permitting the straightforward extraction of the groove undulations. The extracted undulations may then be decoded into stereo audio. This workflow is illustrated in Figure 1. \n Figure 1. Diagram of the implemented system. 4.1. Image Alignment As in most OAR systems, the FOVs are scanned with a degree of overlap. Unfortunately, mechanical translation during the image acquisition usually results in the images being misaligned. Before the information in the images can be extracted and combined, the FOVs must be realigned with each other.  This realignment can be achieved with an iterative frame-by-frame image registration approach, using either of two algorithms: a greedy one and a dynamic programming one [10]. In the greedy algorithm, the alignment of local FOV pairs is optimized, ignoring the precision of the overall grid alignment. It therefore can suffer from cumulative registration errors, resulting in unrecoverable gaps between the last few rows of the grid. By comparison, dynamic programming can be used to achieve a globally optimal alignment by forcing the elimination of any inter-row gaps as a constraint. This latter approach was selected. \n Figure 2. The top view of a typical groove image. The false colors represent vertical coordinates, except for the valleys, which are left monochromatic for clarity. The light color bands are the spaces between the grooves, and the light, thin lines are the valley bottoms.  4.2. The Image Model Figure 2, a sample FOV, is a top view of a typical scanned region on a phonograph. The three parts of a groove can be seen readily: the tops (T), the valleys (V), and the bottoms (B). The goal is to extract the undulations of these grooves, but first the three parts have to be identified and separated. This is done using connected-component analysis (CCA). Similar to standard CCA in 2D image processing, a binarization process (thresholding) is performed as pre-processing to separate the global height levels of the T’s and B’s in the FOV; then with CCA, individual T’s, V’s and B’s are recognized.  The original 2D Cartesian coordinates are converted into polar coordinates (r, θ), with the origin being the disc center. We then distinguish three types of connected components (CCs): the top edges T(r, θ), the valleys V(r, θ), and the groove bottoms B(r, θ). Two useful properties are used in the identification of these regions: First, each region should be tangentially continuous, meaning that only one CC is supposed to be found on a single revolution of a T, V, or B. Second, the geometrical relationship between them is known: the B’s are completely contained by the V’s, while the T’s and V’s do not contain one another because they both stretch across the entire FOV. 4.3. Noise Removal and Groove Restoration The raw CCs usually include noise from dust and dirt on the record and must be cleaned. Although heuristics based on the size of the CCs may seem intuitive, they turn out not to be robust in practice. Instead, we simply resort to the theoretical geometrical properties of the CCs, described in Section 4.2: since the tops and the valleys do not contain one another, any top found  \n629Poster Session 4\n   \n  \nFigure 3. Groove restoration: CCA fails to identify the correct grooves (left) because the scratch connects certain grooves and disconnects others. These faults are corrected in the restored image (right). contained by a valley (or vice versa) should be noise. This appeared to be a robust noise removal method.  Grooves may also be interrupted by scratches, or appear to be so due to occlusion by dust. Dust may also cause neighbouring grooves to appear attached to each other. These conditions create difficulties for extracting the groove undulations. To restore such grooves, two heuristics based on the tangential continuity property are used. First, by locating discontinuous V(r, θ) on the same revolution, broken grooves are detected; these may be restored by simply interpolating and reconnecting them. Similarly, attached grooves are detected and restored by locating and tangentially reconnecting discontinuous T(r, θ) that exist on the same revolution. Both situations are illustrated in Figure 3. Note that cleaning the records before scanning does not guarantee a better noise condition, because dust accumulates throughout the long time span of the scan. (Hosting the microscope in a dust-free environment may be one way to reduce this source of noise.) 4.4. Extracting Groove Undulations The lateral undulations of the inner and outer groove top edges (with respect to the centre of the disc), and the vertical undulations of the groove bottom are the results of the recording stylus cutting into and across the disc surface. Following CCA and groove restoration, each FOV has been segregated into groove top, valley, and bottom regions. The next step is to trace the undulations, defined by the oscillations of both edges of the groove, as well as by the depth of the groove. Using a search-based edge detection algorithm, the inner and outer (with respect to the centre of the disc) groove top edges can be located; these are denoted as Ti(r, θ) and To(r, θ), respectively. The edges Ti(r, θ), To(r, θ), and the groove bottom B(r, θ) are iteratively overlapped and matched across adjacent, properly-aligned FOVs. In doing so, and in unwrapping the spiral-shaped grooves, the three 1D undulation sequences are obtained. The lateral undulations Ti and To are then averaged to obtain a single sequence T corresponding to the center of the groove. Note that phase unwrapping needs to be performed to obtain a continuous “time line” of the undulations, because the polar coordinates have the range of [0, 2π). This is due to the fact that the derivation of polar coordinates is done to individual FOVs before their temporal topological order in the audio signal is clear. 4.5. Converting Groove Undulation to Audio The raw, unwrapped groove undulations need to be resampled at an audio sampling rate to be converted to digital audio. Since the digital image format used here uses rectangular pixel tessellation, the pixel density along the undulation varies. It is therefore necessary to interpolate when sampling the image. We chose a reasonably high industry standard sampling rate (96kHz).  According to the constant velocity cutting scheme of LP production, the tangential velocities contain the audio undulation. This is achieved by performing numerical differentiation on the unwrapped undulations. The stereo audio is derived according to the following equations:  Channelleft(t) = ΔT(t) – ΔD(t) Channelright(t) = ΔT(t) + ΔD(t)  where ΔT(t) and ΔD(t) are the resampled and differentiated sequences of the center points and the depths of the groove, respectively. Finally, a counter-RIAA equalization filter is applied to the audio.  5. RESULTS AND DISCUSSIONS  In our experiment, the OAR system presented above was used to extract a roughly 1.8-second stereo audio signal. The result was compared to a turntable-digitized version of the same signal. Waveforms and frequency responses for both signals are displayed in Figures 4 and 5.  \n63010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n      \n Figure 4. The reconstructed stereo signal: a 1kHz sine wave in the right channel and a silent left channel. A three millisecond segment of the waveform is shown in (a); in (b), the magnitude response of the extracted right channel signal up to 6kHz; in (c), the magnitude response of the same signal up to the Nyquist frequency (48kHz).       \n Figure 5. The turntable-digitized version of the same stereo signal.         \n631Poster Session 4\n    It can be observed that the reconstructed version very much resembles the turntable-digitized version of the same stereo signal. As expected, the most salient component in the output stereo signals is 1kHz. However, the reconstructed left channel signal is not complete silence. In addition, close inspection of the peaks and troughs of the reconstructed waveform reveals a non-zero DC offset; moreover, the peaks appear to fluctuate slightly over time. This low-frequency wow noise, as in Fadeyev and Haber’s system, is partly due to the error in estimating the disc center, which, as the origin of the polar coordinate system, forms the basis of the estimated lateral groove undulations.   6. CONCLUSIONS AND FUTURE WORK  OAR methods have been proven to be effective alternatives in digitizing mono phonograph records. Our WLI-based OAR system has successfully reconstructed digital stereo audio signals from LPs. Future work will be directed to improving the audio quality while decreasing the scanning time. Better center correction strategies will be studied, along with other configurations capable of pushing the audio quality higher. To push down the tremendous time costs, we will also investigate the minimum scanning resolution required to produce an acceptable audio result. On the other hand, the time costs may be reduced as the scanning hardware improves to provide, for instance, a larger field of view and faster vertical scanning speed.  7. ACKNOWLEDGEMENTS  We would like to thank the Canada Foundation for Innovation and the Daniel Langlois Foundation for their financial support.  8. REFERENCES  [1] Fadeyev, V., and C. Haber. Reconstruction of mechanically recorded sound by image processing. LBNL Report 51983, 2003. [2] Fadeyev, V., C. Haber, C. Maul, J. McBride, and M. Golden. Reconstruction of recorded sound from an Edison cylinder using three-dimensional non-contact optical surface metrology. LBNL Report 54927, 2004. [3] Stotzer, S., O. Johnsen, F. Bapst, C. Sudan, and R. Ingold. Phonographic sound extraction using image and signal processing. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing 4:289–92, Montreal, Canada, 2004. [4] Tian, B. Reproduction of sound signal from gramophone records using 3D scene reconstruction. Ph.D. thesis. Department of Computer Science, The University of Western Ontario, 2006. [5]  Li, B., S. de Leon, and I. Fujinaga. Alternative digitization approach for stereo phonograph records using optical audio reconstruction. Proceedings of International Symposium of Music Information Retrieval 165–6, Vienna, Austria, 2007. [6] Iwai, T., T. Asakura, T. Ifubuke, and T. Kawashima. Reproduction of sound from old wax phonograph cylinders using the laser-beam reflection method. Applied Optics 25 (5): 597−604, 1986. [7] Nakamura, T., T. Ushizaka, J. Uozumi, and A. Toshimitsu. Optical reproduction of sounds from old phonographic wax cylinders. Proceedings of SPIE 3190: 304−13, 1997. [8] ELP Corporation. ELP Laser Turntable: plays vinyl records without a needle. http://www.elpj.com/main.html (accessed 10 August 2009).  [9] Veeco Metrology Group. WYKO NT8000 setup and operation guide. Tucson: Veeco Instruments, Inc., 2003. [10] Chow, S., H. Hakozaki, D. Price, N. MacLean, T. Deerinck, J. Bouwer, M. Martone, S. Peltier, and M. Ellisman. Automated microscopy system for mosaic acquisition and processing. Journal of Microscopy 222 (2): 76–84, 2006. \n632"
    },
    {
        "title": "Cover Song Retrieval: A Comparative Study of System Component Choices.",
        "author": [
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414896",
        "url": "https://doi.org/10.5281/zenodo.1414896",
        "ee": "https://zenodo.org/records/1414896/files/LiemH09.pdf",
        "abstract": "The Cover Song Retrieval (CSR) problem has received considerable attention in the MIREX 2006-2008 evaluation sessions. While the reported performance figures provide a general idea about the strengths of the submitted systems, it is not clear what actually causes the reported performance of a certain system. In other words, the question arises whether some system component design choices are more critical for a system’s performance results than others. In order to obtain a better understanding of the performance of current CSR approaches and to give recommendations for future research in the field of CSR, we designed and performed a comparative study involving system component design approaches from the best-performing systems in MIREX 2006 and 2007. The datasets used for evaluation were carefully chosen to cover the broad spectrum of the cover song domain, while still providing designated test cases. While the choice of the dissimilarity assessment method was found to cause the largest CSR performance boost and very good retrieval results were obtained on classical opus retrieval cases, results obtained on a new test case, involving recordings originating from different microphone sets, point out new challenges in optimizing the feature representation step.",
        "zenodo_id": 1414896,
        "dblp_key": "conf/ismir/LiemH09",
        "keywords": [
            "MIREX 2006-2008 evaluation sessions",
            "Cover Song Retrieval (CSR) problem",
            "system component design choices",
            "performance results",
            "current CSR approaches",
            "recommendations for future research",
            "datasets used for evaluation",
            "dissimilarity assessment method",
            "new challenges in optimizing feature representation",
            "microphone sets"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nCOVER SONG RETRIEV AL: A COMPARATIVE STUDY OF SYSTEM\nCOMPONENT CHOICES\nCynthia C.S. Liem, Alan Hanjalic\nDepartment of Mediamatics, Delft University of Technology , The Netherlands\n{c.c.s.liem,a.hanjalic }@tudelft.nl\nABSTRACT\nThe Cover Song Retrieval (CSR) problem has received\nconsiderable attention in the MIREX 2006-2008 evalu-\nation sessions. While the reported performance ﬁgures\nprovide a general idea about the strengths of the submit-\nted systems, it is not clear what actually causes the re-\nported performance of a certain system. In other words,\nthe question arises whether some system component de-\nsign choices are more critical for a system’s performance\nresults than others. In order to obtain a better understand-\ning of the performance of current CSR approaches and\nto give recommendations for future research in the ﬁeld\nof CSR, we designed and performed a comparative study\ninvolving system component design approaches from the\nbest-performing systems in MIREX 2006 and 2007. The\ndatasets used for evaluation were carefully chosen to cover\nthe broad spectrum of the cover song domain, while still\nproviding designated test cases. While the choice of the\ndissimilarity assessment method was found to cause the\nlargest CSR performance boost and very good retrieval re-\nsults were obtained on classical opus retrieval cases, resu lts\nobtained on a new test case, involving recordings originat-\ning from different microphone sets, point out new chal-\nlenges in optimizing the feature representation step.\n1. INTRODUCTION\nCover Song Retrieval (CSR) generally refers to the prob-\nlem of identifying different interpretations of the same\nmusical work. Since 2006, this challenge has been in-\ncluded in the centralized yearly Music Information Re-\ntrieval (MIR) evaluation sessions known as the MIR EX-\nchange (MIREX). Ever since, several systems for this task\nhave been submitted and evaluated on a ﬁxed, but undis-\nclosed dataset. As the results obtained by these systems\nare expressed in the form of general performance numbers,\nno information is provided that could reveal the inﬂuence\nof speciﬁc CSR system component design choices and the\ncomposition of the evaluation dataset on the obtained re-\ntrieval results.\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .Although CSR appears to be more speciﬁc than e.g.\nmusic genre retrieval, cover songs still span a broad range\nof types, each with their own variants and invariants, pos-\ning speciﬁc challenges on the design of the CSR system.\nIn order to validate design motivations and identify which\nsystem aspects are most critical for performance results, i t\nis necessary to consider CSR systems as combinations of\ngeneral system components and review performance with\nrespect to these components. Additionally, the design of\nthe evaluation dataset is critical for obtaining true insig ht\ninto the performance of CSR systems.\nIn this paper, a comparative study is presented with spe-\ncial attention to the inﬂuence of individual system compo-\nnents and the composition of evaluation datasets on CSR\nsystem performance. We look at the two best-performing\nsystems in MIREX 2006-2007, breaking them down into\nseparate, generic components, which are recombined into\nalternative combinations. These are evaluated on 4 differ-\nent datasets. Attention will hereby be paid to the validatio n\nof several ‘semantically intuitive’ choices in the systems .\nIn this way, we aim at achieving better understanding of\ncurrent CSR approaches, identifying which system com-\nponents are most critical for the ﬁnal performance results\nand which research directions deserve further attention in\nfuture CSR research.\n2. PROBLEM DESCRIPTION\n2.1 Deﬁnition of ‘Cover Song’\nWhile the term ‘cover song’ (or simply ‘cover’) used to\nsuggest a pop music phenomenon, it has more recently\nbeen deﬁned as ‘a recording of a song or tune which has\npreviously been recorded by someone else’1. This broad\ndeﬁnition has typically been accepted in the MIR research\nﬁeld, accepting alternate takes of a song by the same artist\nto be covers as well. When considering the broad range\nof cover songs according to this deﬁnition, many musical\naspects can be thought of that may vary among different\ncovers. Several good suggestions for musical aspects that\ncan be used in characterizing cover songs are given in [1].\n2.2 Cover Song System Components\nFor the CSR problem discussed in this paper, a setting is\nassumed in which an example raw audio ﬁle is provided\n1This also is seen in dictionaries, e.g. see http://dictionary.\ncambridge.org/define.asp?key=17817&dict=CALD ,\naccessed May 2009.\n573Oral Session 7: Harmonic & Melodic Similarity and Summarization\nas a query to a dataset, after which the audio ﬁles in the\ndataset are returned in an ordered way, according to their\nsimilarity score compared to the query. In this setting, CSR\nsystems can be characterized using a general model, con-\nsisting of two main components:\n•Feature representation , transforming a raw audio ﬁle\ninto a representation suitable for further matching;\n•Dissimilarity assessment , achieving the actual\nmatching, applying a chosen dissimilarity measure.\nTwo more system aspects concerned with post-processing\nof the feature representation will further be considered:\n•The typical approach of using short-time harmonic\nfeatures for the feature representation produces very\nmuch data. In order to reduce this amount of data,\nanaveraging step is adopted.\n•In order to handle varying sound intensity levels,\nwhich can be caused both by the quality of the\nrecording and by musical dynamics, a normalization\nprocedure is usually applied to the chosen feature\nrepresentation.\nThe musical variants expected in cover songs have in-\nﬂuenced design choices for the mentioned system com-\nponents. For the feature representation, chromagrams\nare commonly chosen2. These are considered to model\nmelodic/harmonic progression over time without the need\nfor exact transcriptions, while being robust to speciﬁc in-\nstrument timbres. Besides, multiple interpretations of a\nsong will inevitably introduce tempo and timing variations ,\nwhich also should be accounted for in CSR systems.\nIf system evaluations are done as a whole, it will not be\nclear from the results which of these component choices\nare most important to the ﬁnal performance results. Addi-\ntionally, validation of design choice motivations (such as\nthe timbre-robustness of chromagrams) will be difﬁcult.\n2.3 Importance of Evaluation Dataset Composition\nWhile during the system design, attention is paid to possi-\nble musical variants in cover songs, these do not appear to\nbe considered with the same importance in system eval-\nuation. Evaluation datasets typically are colorful cross-\nsections of private music collections, which are sought to\ncontain as much musical variation as possible. However,\nthe more variants in the dataset, the more difﬁcult it will\nbe to interpret an overall performance number. Given the\nbroadness of the cover song spectrum, understanding of a\nsystem’s performance can only be achieved if attention is\npaid to the types of cover song similarity test cases posed\nby a dataset.\nA common problem in audio-based MIR research is the\nlack of public benchmark data. When different authors re-\nport performance numbers on different private music col-\nlections, comparison of their approaches cannot be made\n2see for example the extended abstracts on http://www.\nmusic-ir.org/mirex/[yearofsession]/index.php/\nAudioCoverSongIdentification Results , with 2006,\n2007 and 2008 as possible session years (accessed May 2009).easily. The MIREX endeavour offered a centralized so-\nlution to this, comparing multiple algorithms on the same\nevaluation data. However, as details regarding the eval-\nuation dataset composition are not revealed to the partic-\nipants, only comparative information on total system per-\nformance is provided, while algorithm behavior on speciﬁc\ntest cases once again remains unclear.\n2.4 Contribution\nTo address the problems described above and gain more\nin-depth understanding of CSR performance in current ap-\nproaches, in this paper, we describe a comparative study\nwith two main focus points:\n•to investigate the impact of choices in each individ-\nual general CSR system component listed above on\nthe CSR performance;\n•to relate the achieved performance results to speciﬁc\ntest cases provided by the evaluation data.\nThe setup of this comparative study is explained in Sec-\ntion 3, while the results are reported and discussed in Sec-\ntion 4. We ﬁnish the paper in Section 5 with conclusions\nand recommendations for future work.\n3. EV ALUATION SETUP\nIn this section we ﬁrst explain the systems we selected and\nimplemented for our comparative study.\n3.1 Basic Systems\n3.1.1 Best CSR system in MIREX 2006\nThe system proposed by Ellis et al. in [2] was the best-\nperforming system in the ﬁrst MIREX CSR Task, held in\n2006. We use the implementation that has been made avail-\nable by the author [3], which is very similar to this original\n2006 MIREX CSR submission.\nRegarding the feature representation, chromagrams\nbased on instantaneous frequency (CIF) are used. Features\nare averaged over beats, which appears to be a semantically\nintuitive choice, allowing robustness to tempo variances; a\nbeat tracker is needed in order to achieve this. For normal-\nization, each 12-bin chroma vector in the chromagram is\nnormalized to unit norm.\nFor similarity assessment, cross-correlation (CC) is per-\nformed. In order to allow for different key transposi-\ntions, all 12 possible chroma transpositions are considere d\nin this correlation step. Subsequently, a similarity score\nis achieved through the maximum peak correlation value\nfound. This can be changed into a dissimilarity score by\ntaking the reciprocal of this value.\n3.1.2 Best CSR system in MIREX 2007\nThe system proposed by Serr `a et al. in [4] was the best-\nperforming system in the second MIREX CSR Task, held\nin 2007. This system showed a striking performance in-\ncrease compared to all other systems; an improved version\n57410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nalso convincingly showed the best performance results in\nthe 2008 MIREX CSR Task [5].\nAs no implementations of this system are publicly avail-\nable, for the experiments described, the system has been\nreimplemented from the literature, using the information\nin [1, 4, 6]. The preprocessing steps (transient localiza-\ntion and spectral normalization) have still been omitted in\nour implementation, as it was not completely clear which\nprocedures were exactly followed for these steps. For the\nsame reasons, the system changes and parameter tunings\nmentioned in [5] could not be implemented, so our im-\nplemented system will show the most resemblance to the\nMIREX 2007 submission by the authors.\nFor feature representation, Harmonic Pitch Class Pro-\nﬁles (HPCPs) [6] are used. These are chromagrams (or\npitch class proﬁles) in which each spectral peak contribu-\ntion is weighted across multiple chroma bins. Additional\ncontribution is weighted into the ﬁnal representation by\ntaking into account the ﬁrst 8 harmonics of each spectral\npeak. Averaging is done over a ﬁxed number of frames,\nas beat tracking was found to include additional errors that\ndecreased performance (this also was noted in [7]). Nor-\nmalization is performed by dividing a HPCP instance by\nthe maximal value found in this instance, yielding a proﬁle\nin which the maximum value is 1.\nFor matching, a procedure was devised called Dynamic\nProgramming Local Alignment (DPLA), using binary sim-\nilarity. For the two audio HPCP vectors to be matched,\nﬁrst an Optimal Transposition Index is computed. Sub-\nsequently, after applying the found optimal key transpo-\nsition, a binary similarity matrix is constructed, based on\nremaining optimal transposition indices per HPCP short-\ntime instance after the global transposition. Subsequentl y,\nin a way similar to string or DNA matching, a dynamic\nprogramming procedure with local constraints (for tempo\nﬂuctuations) is applied. The best path found will decide\nthe similarity score, which is normalized to a dissimilar-\nity score. More information on these procedures can be\nfound in [1]. Parameter choices have been directly taken\nfrom [1]; as for the averaging factor, the choice was made\nto consider an averaging factor of 10 frames. Furthermore,\nonly 12-bin HPCPs are considered instead of the suggested\n36 bins, as 12 bins were used both in the Ellis et al. system\nand in later versions of the Serr `a et al. system.\n3.2 Considered Approaches\nUsing the systems described above, several possible gen-\neral design choices can be extracted. The following\nchoices have been veriﬁed in our algorithms:\n•The general choice of feature representation: (1)\nChromagrams based on Instantaneous Frequency\n(CIF) , (2) Harmonic Pitch Class Proﬁles (HPCP)\nand (3) Pitch Class Proﬁles (PCP) , which are con-\nstructed similarly to HPCPs, but omitting the addi-\ntional harmonic weighting.\n•The averaging factor for the feature representation:\n(1)averaging over beats and (2) averaging over aﬁxed number of frames .\n•The matching procedure for dissimilarity assess-\nment: (1) cross-correlation (CC) and (2) Dynamic\nProgramming Local Alignment (DPLA) .\nAll possible combinations of these choices have been\ntested, with three possible normalization choices regard-\ning feature representation: (1) no normalization , (2) nor-\nmalization to unit norm and (3) normalization by the max-\nimum .\n3.3 Performance measures\nWe evaluate the systems using 2 evaluation measures,\nwhich also were adopted in the most recent MIREX evalu-\nations [8]:\n•(Arithmetic) Mean of average precisions (MAP);\n•Mean rank of 1st correctly identiﬁed cover (MR1st).\nThe most recent MIREX evaluations employ two more\nevaluation measures focusing on the top-10 retrieval re-\nsults. However, in our experiments, only MAP and MR1st\nwill be suitable performance indicators: our datasets,\nwhich are discussed hereafter, contain cover sets of dif-\nferent sizes, as opposed to the MIREX dataset which con-\ntained 10 relevant cover versions per query song.\n3.4 Datasets\n4 datasets have been used in our experiments, which will\nbe described now. The construction and choice for the\ndatasets has largely been motivated by the need to provide\nclear and designated test cases. The choice was made to\nuse 4 separate datasets in order to provide a clear-cut cor-\npus per dataset. All audio tracks have been converted to the\nMP3 format. For each dataset, each audio ﬁle in the dataset\nwas matched against all other ﬁles in the same dataset.\n3.4.1 Covers80 dataset\nThis dataset, containing pop song covers, was made avail-\nable by Ellis [3]. 166 recordings are included, encompass-\ning 80 ‘cover sets’, which means the average number of\nversions is just 2.05. With the dataset being constructed\nrather randomly, musical variants within the dataset diffe r\ngreatly and interpreting performance measures will be dif-\nﬁcult. We decided to include results on this set anyway for\nreference reasons.\n3.4.2 Beethoven piano sonatas\nThis dataset contains multiple interpretations of move-\nments from 4 Beethoven piano sonatas. The data in this\ndataset originates from private music collections of the au -\nthors and the Beeld en Geluid (BeG) vinyl collection3in\nthe European archive. The dataset contains 128 record-\nings, encompassing 13 ‘cover sets’. As piano sonatas are\n3http://europarchive.org/collection.php?id=\npublicclassical musicBeG, accessed May 2009.\n575Oral Session 7: Harmonic & Melodic Similarity and Summarization\nconsidered, all covers will consist of very similar instru-\nment timbres and will be played in exactly the same key.\nTherefore, the set has very clear invariants and poses well-\ndeﬁned (although not too challenging) similarity tasks. A\nset of similar composition was used for a CSR system men-\ntioned in [9], which showed near-perfect performance.\n3.4.3 Songs\nThis dataset departs from recordings of classical art songs\nthat were performed at the 1st International Student Lied\nDuo Competition, held in Enschede in April 2009. It con-\ntains study recordings from one of the participating duos,\nmade at rehearsals and try-outs in preparation for the com-\npetition. Additionally, recordings of all the participant s\nmade during the ofﬁcial competition rounds are included.\nMore speciﬁcally, included songs encompass compulsory\nsongs, as well as songs that were performed by multiple\ndifferent participants. Finally, the set was extended with\nextra song interpretations from private music collections ,\nthe BeG vinyl collection and the vinyl recordings from the\nKing’s Sound Archive4. In total, the dataset contains 205\nrecordings, encompassing 21 ‘cover sets’.\nAt the competition, recordings have been made with\ntwo different pairs of microphones at two different loca-\ntions in the hall (on stage and in the hall). While record-\nings from these two pairs contain exactly the same musical\ninterpretation, the recordings do show considerable acous -\ntical differences. As this poses an interesting test case fo r\nthe CSR algorithms, the takes from both microphone pairs\nhave been included in the dataset.\nBoth this songs dataset and the Beethoven dataset con-\nsider multiple interpretations of exactly the same score.\nThe problem of retrieving such interpretations has some-\ntimes been considered as a subtask within CSR, known as\nopus retrieval. The difference between both sets is that the\nsongs set shows much more variation in instrument timbre\nand musical keys, as the performing singers have different\nvoice types.\n3.4.4 Beatles\nThis dataset aims at being a slightly more speciﬁc dataset\nthan the covers80 set with larger ‘cover set’ sizes, while\nstill reﬂecting a similar corpus. The dataset contains orig -\ninal Beatles songs (including alternative takes and ver-\nsions), as well as various covers taken from tribute CDs,\nincluding Baroque, R&B, Latin and easy listening styles.\nIn total there are 197 audio ﬁles, encompassing 51 ‘cover\nsets’. On one of the CDs used, 4 covers were present of\nsongs from individual Beatles members. These were in-\ncluded in our database without providing alternative ver-\nsions. Typical CSR evaluation experiments contain even\nmore of such ‘outlier noise’ ﬁles in evaluation datasets (e. g.\nMIREX), but in our experiments they explicitly have not\nbeen included extensively in order to focus on system be-\nhavior on actual covers.\n4http://www.kcl.ac.uk/kis/schools/hums/music/\nksa/ksa sound.html , accessed May 2009.4. RESULTS\nEach of the possible combinations mentioned in Subsec-\ntion 3.2 has been tested on all 4 datasets. The resulting\nMAP and MR1st scores are plotted twice, both in Figure 1\nand Figure 2. While the performance scores are the same\nin both ﬁgures (expressed in data points at the same loca-\ntions on the vertical axis), the used data markers indicate\ndifferent system choices. In Figure 1, the data point mark-\ners indicate corresponding combinations of feature rep-\nresentations and dissimilarity assessment, while the data\npoint markers in Figure 2 indicate corresponding combi-\nnations of normalization and averaging choices. In the ﬁg-\nures, baseline results for random guessing are indicated as\nwell, which were obtained by generating 50 random simi-\nlarity matrices for each dataset and averaging the obtained\nresults. Because of space limitations, only the results of\nthe best-performing system combinations are numerically\nexpressed in Table 1.\nDataset MAP MR1st\ncovers80 0.648 15.817\nBeethoven 1 1\nsongs 0.986 1\nBeatles 0.693 5.699\nTable 1 . Best performance scores for each of the datasets\nThe best results turn out to occur for the same combina-\ntion consistently: the CIF feature representation, averag ed\nover a ﬁxed number of 10 frames, normalized to unit norm\nand with dissimilarity assessment based on DPLA. This\nmeans aspects from both studied original systems combine\ninto an optimally performing system.\nWith respect to the feature representation choice, the\nCIF representation generally does not perform worse than\nHPCPs or PCPs. In the pop music datasets (covers80 and\nBeatles), it even performs clearly better than HPCPs and\nPCPs. Besides, as mentioned above, the CIF represen-\ntation consistently occurs in the best-performing system\ncomponent combinations for each of the 4 datasets. Re-\ngarding the difference between HPCPs and PCPs, the har-\nmonic weighting in the HPCPs does not give convincing\nperformance increases when compared to PCPs. While\nHPCPs were known to yield the highest correlation scores\nwhen compared to symbolic note information [6], this does\nnot appear to be a convincing advantage in the CSR prob-\nlem, which deals with approximate matches.\nThe notion in [1,4] that DPLA dissimilarity assessment\nyields much better results than CC is convincingly con-\nﬁrmed for all 4 datasets. This also holds for the state-\nment in [1, 4, 7] that averaging over a ﬁxed number of\nframes improves performance in comparison to averag-\ning over tracked beats. While all better-performing sys-\ntem combinations contain normalized feature representa-\ntions, the performance increase from the normalization\nstep is much smaller than the increases caused by choos-\ning DPLA dissimilarity and averaging over a ﬁxed number\nof frames. Furthermore, there is no speciﬁc normalization\n57610th International Society for Music Information Retrieval Conference (ISMIR 2009)\ncovers80 Beethoven songs Beatles00.10.20.30.40.50.60.70.80.91\ndatasetMAPMAP following from different feature and dissimilarity assessment choices\ncovers80 Beethoven songs Beatles01020304050607080\ndatasetMR1stMR1st following from different feature and dissimilarity assessment choices\n  \nfeature:CIF feature:HPCP feature:PCP dissimilarity:CC dissimilarity:DPLA random baseline\nFigure 1 . MAP and MR1st for the 4 datasets with feature and dissimilar ity assessment choices indicated.\ncovers80 Beethoven songs Beatles00.10.20.30.40.50.60.70.80.91\ndatasetMAPMAP following from different normalization and averaging choices\ncovers80 Beethoven songs Beatles01020304050607080\ndatasetMR1stMR1st following from different normalization and averaging choices\n  \nnorm:none norm:unit norm:max avg:beat avg:frame random baseline\nFigure 2 . MAP and MR1st for the 4 datasets with normalization and aver aging choices indicated.\nchoice performing convincingly better than other normal-\nization choices.\nAs expected, the results for the classical opus retrieval\ncases (Beethoven and songs) are better than those on the\npop music datasets. However, it is remarkable how close\nthe performance on both classical datasets is, despite the\nmuch larger variations in timbre and key in the songsdataset. Errors in near-perfect results on the Beethoven\nset are caused by the historic vinyl recordings, which are\ndegraded in quality compared to modern recordings. How-\never, as shown in Table 1, the best-performing system com-\nbination was robust to the vinyl recording sound distor-\ntions, having perfect retrieval results on this set. In the\nBeatles database, if besides a query multiple alternative\n577Oral Session 7: Harmonic & Melodic Similarity and Summarization\nrecordings of the same artists exist, these recordings are\nranked very high in the retrieval results. However, the per-\nformance results worsen because of the inability of all im-\nplemented approaches to deal with the freer covers, such\nas the easy listening piano versions of the Beatles songs.\nThe similarity test cases posed by the songs dataset\ndemonstrate some other interesting properties of the cur-\nrent CSR approaches. If a song is available in multiple\ninterpretations from the same musicians, these interpreta -\ntions are usually ranked higher than interpretations of oth er\nmusicians. This might be due to timing aspects rather than\ntimbral aspects, as interpretations of other singers of the\nsame voice type as the singer in the query do not consis-\ntently rank higher than interpretations of other singers of\nother voice types or even the other gender. This validates\nthe hypothesis that the followed approaches show timbre-\nrobustness. The claimed key invariance of all approaches\nis also conﬁrmed in our results, as songs sung in the same\nkey as a query do not always rank higher than recordings\nof the song in other keys.\nIn the best system combinations for the songs database,\nif an alternate microphone recording of a given song is\navailable, it is retrieved as the best-matching song. How-\never, while such recording pairs undoubtedly contain ex-\nactly the same musical interpretation, the found dissimi-\nlarity scores of both pair members compared to a query\nof another interpretation are not identical. It even is not\nguaranteed that both pair members will be neighbors in the\ncorresponding dissimilarity ranking to the query. This is a n\ninteresting notion that does not match our human notion of\ninterpretation similarity.\n5. CONCLUSION AND DISCUSSION\nIn this paper, more insight into the performance of current\nCSR approaches was sought through a comparative study,\nin which different combinations of CSR system compo-\nnents were evaluated on 4 carefully constructed datasets.\nThe obtained results show that choices that semantically\nseemed intuitive do not necessarily yield better perfor-\nmance results: including harmonic weighting into a fea-\nture representation does not convincingly show perfor-\nmance improvement, while averaging the representations\nover beats actually makes the results worse.\nRegarding the system components, the best feature rep-\nresentation found consistently in our experiments is the\nCIF representation, which is not the representation used\nin the best MIREX systems of 2007 and 2008. However,\nthe dissimilarity assessment method used in those systems,\nbinary similarity using DPLA, gives a large performance\nincrease in comparison to using CC. This suggests that the\ndissimilarity measure has been the crucial factor in the suc -\ncess of the best MIREX CSR system submissions of 2007\nand 2008. The remaining system aspect that was tested,\nthe feature normalization, only gives a slight increase in\nperformance.\nSuccessful CSR system component combinations can\ndeal very well with opus retrieval tasks, even if large tim-\nbre and key variance is present. However, ranking resultsfor the different microphone recording pairs in the songs\ndataset show different ranks for identical musical interpr e-\ntations which only differ in terms of the acoustical con-\nditions. Therefore, the difference in the dissimilarity mu st\nbe due to the feature representation, suggesting that furth er\nimprovement is still possible here.\nWhile the major changes from the best-performing sys-\ntem of MIREX 2007 to that of 2008 mainly focused on\nimproving the dissimilarity assessment part [5], improve-\nment possibilities in the other system components, espe-\ncially the feature representation, are clearly not exclude d.\nFurther experiments are needed into alternatives that will\nbe able to yield results that better approach our human no-\ntions of cover song similarity.\n6. REFERENCES\n[1] J. Serr `a. Music similarity based on sequences of\ndescriptors: Tonal features applied to audio cover\nsong identiﬁcation. Master’s thesis, University Pompeu\nFabra, Barcelona, Spain, September 2007.\n[2] D.P.W. Ellis and G.E. Poliner. Identifying ‘cover\nsongs’ with chroma features and dynamic program-\nming beat tracking. In Proc. of the Intl. Conf. on\nAcoustics, Speech, and Signal Processing (ICASSP) ,\nvolume IV , pages 1429–1432, Honolulu, USA, April\n2007.\n[3] D. Ellis. The covers80 cover song data set. Web\nresource, available: http://labrosa.ee.\ncolumbia.edu/projects/coversongs/\ncovers80 , 2007.\n[4] J. Serr `a, E. G ´omez, P. Herrera, and X. Serra. Chroma\nbinary similarity and local alignment applied to cover\nsong identiﬁcation. IEEE Trans. on Audio, Speech and\nLanguage Proc. , 16:1138–1151, August 2008.\n[5] J. Serr `a, E. G ´omez, and P. Herrera. Improving bi-\nnary similarity and local alignment for cover song\ndetection. MIREX 2008 extended abstract, available:\nhttp://www.music-ir.org/mirex/2008/\nabs/CSSerra.pdf , September 2008.\n[6] E. G ´omez. Tonal Description of Music Audio Sig-\nnals. PhD thesis, University Pompeu Fabra, Barcelona,\nSpain, July 2006.\n[7] J.P. Bello. Audio-based cover song retrieval using ap-\nproximate chord sequences: Testing shifts, gaps, swaps\nand beats. In Proc. of the Intl. Conf. on MIR (ISMIR) ,\nVienna, Austria, September 2007.\n[8] J.S. Downie, M. Bay, A.F. Ehmann, and M.C. Jones.\nAudio cover song identiﬁcation: MIREX 2006-2007\nresults and analyses. In Proc. of the Intl. Conf. on MIR\n(ISMIR) , Philadelphia, USA, September 2008.\n[9] M.A. Casey, R. Veltkamp, M. Goto, M. Leman,\nC. Rhodes, and M. Slaney. Content-based music in-\nformation retrieval: Current directions and future chal-\nlenges. Proc. of the IEEE , 96(4):668–696, April 2008.\n578"
    },
    {
        "title": "Music Paste: Concatenating Music Clips based on Chroma and Rhythm Features.",
        "author": [
            "Heng-Yi Lin",
            "Yin-Tzu Lin",
            "Min-Chun Tien 0001",
            "Ja-Ling Wu"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415758",
        "url": "https://doi.org/10.5281/zenodo.1415758",
        "ee": "https://zenodo.org/records/1415758/files/LinLTW09.pdf",
        "abstract": "In this paper, we provide a tool for automatically choosing appropriate music clips from a given audio collection and properly combining the chosen clips. To seamlessly concatenate two different music clips without causing any audible defect is really a hard nut to crack. Borrowing the idea from the musical dice game and the DJ’s strategy and considering psychoacoustics, we employ the currently available audio analysis and editing techniques to paste music sounded as pleasant as possible. Besides, we conduct subjective evaluations on the correlation between pasting methods and the auditory quality of combined clips. The experimental results show that the automatically generated music pastes are acceptable to most of the evaluators. The proposed system can be used to generate lengthened or shortened background music and dancing suite, which is useful for some audio-assisted multimedia applications.",
        "zenodo_id": 1415758,
        "dblp_key": "conf/ismir/LinLTW09",
        "keywords": [
            "Automatic music clip selection",
            "Audio collection analysis",
            "Combining music clips",
            "Psychoacoustics",
            "Objective evaluation",
            "Subjective evaluations",
            "Audio-assisted multimedia",
            "Background music generation",
            "Dancing suite creation",
            "Audio-assisted applications"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMUSIC PASTE: CONCATENATING MUSIC CLIPS\nBASED ON CHROMA AND RHYTHM FEATURES\nHeng-Yi Lin †Yin-Tzu Lin ‡Ming-Chun Tien ‡Ja-Ling Wu †‡\nNational Taiwan University\n†Department of Computer Science and Information Engineering\n‡Graduate Institute of Networking and Multimedia\n{waquey,known,trimy,wjl }@cmlab.csie.ntu.edu.tw\nABSTRACT\nIn this paper, we provide a tool for automatically choos-\ning appropriate music clips from a given audio collection\nand properly combining the chosen clips. To seamlessly\nconcatenate two different music clips without causing any\naudible defect is really a hard nut to crack. Borrowing\nthe idea from the musical dice game and the DJ’s strat-\negy and considering psychoacoustics, we employ the cur-\nrently available audio analysis and editing techniques to\npaste music sounded as pleasant as possible. Besides, we\nconduct subjective evaluations on the correlation between\npasting methods and the auditory quality of combined clips.\nThe experimental results show that the automatically gen-\nerated music pastes are acceptable to most of the evalua-\ntors. The proposed system can be used to generate length-\nened or shortened background music and dancing suite,\nwhich is useful for some audio-assisted multimedia appli-\ncations.\n1. INTRODUCTION AND MOTIV ATION\nNowadays, more and more music lovers prefer to create\ntheir own music from the existing music audio collections,\nfor the purpose of generating background music or danc-\ning suite with speciﬁc length or composing a new song\nwith all the favorite parts from different songs. However,\nthey often confront difﬁculties in reaching a desirable re-\nsult. The main problem lies in how to choose appropri-\nate music clips from a large database and ﬁnd out proper\nconnecting-positions among these chosen clips. To our big\nsurprise, studies on the relationship between the “hearing\nquality” and the “connecting-positions” in music combin-\ning has been long ignored. Conventionally, professional\nusers would rely on their music sense and a few music the-\nories to choose the clips and the connecting-positions, but\nthe editing process is still try-and-error. As the amount\nof tasks increases, the process becomes time-consuming\nand labor-intensive. Therefore, the goal of this paper is\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.two-fold: (i)providing a tool for automatically choosing\nappropriate music clips from given audio collections and\ncombining the chosen clips as euphonious as possible, and\n(ii)conducting several experiments and investigating the\nrelationship between pasting methods and the correspond-\ning auditory quality. The ultimately combined music is\nnamed as “music paste” because it is just like the concept\nof pasting. The terminology “euphonious” is deﬁned as\nfollows: (i)listeners do not notice the transitions in the\nmusic paste, or (ii)listeners do notice the transition but\nthey do not perceive the exact connecting-positions or the\ntransitions sound pleasant to them.\n2. RELATED WORK\n2.1 Combine Music in Symbolic Domain\nCombining two music clips in symbolic domain has been\nearly studied. In the European classical era, preeminent\ncomposers developed a kind of musical dice game called\nMusikalische W ¨urfelspiele [1] . Composers composed num-\nbers of music clips for each measure in advance. While\nplaying the game, players throw a dice to select the pre-\ndetermined music clips. The action is performed for ev-\nery measure. The generated music piece would not be\nstrange because the music clip candidates for the same\nmeasure usually consist of the same chord or similar domi-\nnant tones. Based on this idea, Cope [2] conducted numer-\nous experiments and developed a music-generating system.\nIn the system, music clips of master composers have been\nanalyzed and recombined to generate a new master style\nmusic piece.\nThe advantage of combining music clips in symbolic\ndomain is that it causes less artifacts in auditory aspect. It\nis simple to transpose the midi clips to the same scale and\ndirectly combine two midi clips without causing artifacts\nwhile artifacts are usually inevitable in combined audio\nclips. However, the approaches in symbolic domain are not\neasy to be applied in audio domain due to the complication\nof polyphonic audio ﬁles. Moreover, current state of the\nart music transcription and separation techniques are not\naccurate enough to extract all the musical notes from poly-\nphonic clips. Thus, the most commonly applicable editing\noperations in audio domain are only tempo change, remix,\nand concatenation.\n213Poster Session 2\nTransition Segments clip 1\nclip 2\nclip 2\nTransition Length Transition  Length Figure 1 . An example of pasting at the measures with the\nsame chords.\n2.2 Combine Music in Waveform Domain\nConventionally, what a DJ does can be treated as a human\nversion of music-combining system. DJs often have tal-\nents for combining music clips appropriately. They can\nobtain hidden messages from the music clips by hearing\neven without the score information. In addition to choos-\ning proper clips and connecting-positions, they also change\nthe tempi around the connecting-positions of music clips\nto make the pasted music clips be pleasant for hearing.\nBased on this concept, Tristan [3, 4] proposed an auto-\nmated DJ system by extracting auditory features, connect-\ning the clips at rhythm-similar segments and aligning the\nbeats of clips. However, the concatenated result may be\ndiscordant if these rhythm-similar segments are not pitch-\nsimilar. Thus, in this work, we adopt the chroma-based\nsimilarity measurement to solve this problem. Moreover,\nseveral useful schemes for ﬁltering out dissimilar music\nclips are presented as well.\n3. SYSTEM OVERVIEW\nThe key idea of the proposed system is as follows:\nPeople usually anticipate the succeeding notes while lis-\ntening to music [1]. Figure 1 shows an example of 2 in-\nput clips: clip1,clip2. Originally, each input clip ﬁts peo-\nple’s expectation. To continue the expectation between the\nclips, we choose the most similar segments (pitch-similar\nand rhythm-similar, inspired by the musical dice game and\nautomated DJ) between them as the connecting-position.\nThen, we can ensure that in the pasted music, from the be-\nginning through the connecting-position to the end will all\nconform to people’s anticipation. In this example, the last\nthree chords of clip1are the same as the ﬁrst three chords\nofclip2. So, we connect these 2 clips by superimposing the\nbeginning of clip2ontoclip1at the position of the third\nlast chord. We deﬁne the overlapping parts (the marked\nchords) as “transition segments.” It will be determined by\nﬁnding the most alike segments of the two combined clips.\nBesides, we use another term “transition length” to repre-\nsent the length (in beat) we need for gradually adjusting the\ntempo from clip1to that of clip2if there is a discrepancy\nof tempi in these two clips.\nThe proposed system framework is illustrated in Figure 2.\nFirst, we extract all the features we need from music clips\nsuch as loudness, chroma, rhythm, and tempo. Then, we\nﬁlter out dissimilar music clips by pair-wise comparisons.\nMusic Audio Clips \nDi t M t i T iti S t Feature Extraction Di stance Matrix \nConstruction Trans iti on Segmen ts \nLocating \nT iti L th \nMusic Ordering Music Filtering \nh i Trans iti on Leng th  \nDetermination \nSynt hes is Process \nOutput Audio Files Figure 2 . The block diagram of the proposed system.\nNext, we construct a distance matrix by chroma and rhythm\nfeatures. With this matrix, we determine the transition seg-\nments and decide an appropriate pasting ordering. After\nthat, we determine the transition lengths and adjust the\ntempi within them. Then, we rearrange the volume within\nthe transition segments and synthesize all the processed\nmusic clips. For better understanding, we will ﬁrstly de-\nscribe how to paste two music clips in section 4. And the\nmusic ordering and ﬁltering schemes for dealing with more\nthan two clips will be illustrated in section 5.\n4. CONCATENATION OF TWO CLIPS\nIn this section, we describe the process of pasting two mu-\nsic clips. We name these two clips as clip1andclip2.\n4.1 Transition Segments Locating\nThe common-used similarity/distance matrix [5] method\nis applied to measure the similarities between clip1and\nclip2. We extract chroma-based [6] and rhythm features\n[7] per beat and then calculate their Euclidian distance.\nThus, the smaller the values are, the more similar the seg-\nments are. Let Dc12(i, j)andDr12(i, j)represent the chroma\nand rhythm distance values between clip1’sithbeat and\nclip2’sjthbeat, respectively. That is,\nDc12(i, j) =||/vectorC1i−/vectorC2j||2 (1)\nDr12(i, j) =||/vectorR1i−/vectorR2j||2 (2)\nwhere /vectorC1iand/vectorC2idenote clip1’sithandclip2’sjthchroma\nvectors, respectively. And similarly, /vectorR1iand/vectorR2irepresent\nthe rhythm feature vectors. The two matrices Dc12(i, j)\nandDr12(i, j)are linearly combined into a new matrix\nDcr12(as shown in Eqn. (3)), which is the distance matrix\nwe used for ﬁnding transition segments:\nDcr12(i, j) =αDc12(i, j) + (1 −α)Dr12(i, j) (3)\nwhere α∈[0,1]. We set α= 0.5 as default to equally\nconsider the two features. Figure 3(a) depicts a distance\nmatrix ( Dcr12) of 2 clips chosen from Chinese pop songs:\n“real man” ( clip1), “Let’s move it” ( clip2). The darker the\ncolor is, the more similar the segments are. Since the tran-\nsition segment of clip1and the transition segment of clip2\nshould be similar beat by beat, we trace the values diago-\nnally by applying overlapping window with LmintoLmax\nbeats long and compute the average value within each win-\ndow. We pick the windows with the minimum average\n21410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n(a) (b)Figure 3 . (a) The distance matrix of the two clips : “Real\nman” and “Let’s move it.” (b) The ignored areas.\nvalue as the transition segments. Moreover, for the purpose\nof reducing the computational load and avoiding promptly\nswitching clips, we consider only the last half of clip1and\nthe ﬁrst half of clip2. Figure 3(a) shows the most similar\nsegment we found. Figure 3(b) shows the ignored areas\nmarked with thick crosses. The process is described by\n[i∗, j∗, L∗] = arg min\ni,j,L1\nL+ 1L/summationdisplay\nl=0Dcr12(i+l, j+l)(4)\nwhere L∈[Lmin, Lmax],i≥N\n2,j≤M\n2, N and M are\nthe total beat number of clip1andclip2, respectively.\n4.2 Transition Length Determination\nIn musical theory, tempo is deﬁned as the speed of a given\npiece [8] , usually measured by the number of beats per\nminute (BPM). The tempo value at the ithbeat ( T(i)) can\nbe calculated as follows:\nT(i) =60\nbeat i+1−beat i(5)\nwhere beat iandbeat i+1are the time indices (in seconds)\nof the ithand the (i+ 1)thbeats of a clip extracted from\nthe state-of-the-art tempo tracker: beatroot [9]. In order to\ngradually adjust the tempi from clip1toclip2, the “transi-\ntion length” should be long enough to let the difference be-\ntween the adjacent T(i)within the transition length small\nenough. An example is shown in Figure 4. To change the\ntempi from Tempo1 to Tempo2, the changing ratio ( rc) of\nthe adjacent tempi within K beats is\nrc=K/radicalBigg\nTempo 2\nTempo 1(6)\nBy choosing a proper value of rc, we can determine the\nminimum value of K. We adopt the concept of just no-\nticeable difference [10] (JND) in the domain of psychoa-\ncoustics to determine rc. JND is deﬁned as the minimum\ndifference of stimuli that people can perceive. These stim-\nuli include loudness, tempo and pitch. According to We-\nber’s law , the JND can be computed with the Weber’s Con-\nstant . However, the Weber’s Constant of tempo varies with\nchanges in the environment. Thus, inspired by Thomas\n[11], we conduct experiments to ﬁnd the JND of tempo\non our music clip datasets. For quick (i.e. fast tempo)\nclips, we found out that the ratio of the tempi from 0.95\nto 1.03 will not be perceived. For slow clips, the JND\nTempo1 Tempo2 rcrcrcrc\nTempo1 Tempo2 \nKFigure 4 . Diagram for changing tempi within a length K.\nTransition Segments i* \nclip 1\nclip 1 N\nx\nclip 2M\nitmp \ny\n(*)j* clip 2\n1itmp \njtmp \nOverlapped length (L* )\nTransition length ( x+ y+ L* ) \nFigure 5 . The sketch map of ﬁnding the transition length.\nrange is from 0.96 to 1.04. Nevertheless, real world mu-\nsic clips may contain more than one tempo, e.g. the pieces\nwith accelerando or ritardando. Therefore, we developed\nAlgorithm 1 to ﬁnd the transition length and the target tempi\nTt1,Tt2. The procedure is also illustrated in Figure 5.\nThen, we use phase vocoder [12] to adjust the tempi from\nT1,T2toTt1,Tt2, respectively. Figure 6 shows the cor-\nresponding results of two song clips: “Let’s move it” and\n“Real man.” The ratio of change appears like a linear decay\nbecause the ratios are usually very close to 1.\nAlgorithm 1\nInput: the tempi of clip1andclip2:T1(i),T2(j), fori=\n1. . . N ,j= 1. . . M\n1:forx= 0toi∗,y= 0to(M−L∗−j∗)do\n2:itmp⇐(i∗−x)\n3:jtmp⇐(j∗+L∗+y)\n4:rc⇐x+y+L∗/radicalBig\nT2(jtmp)\nT1(itmp)\n5: ifrcis within JND then\n6: break\n7: end if\n8:end for\n9:Tt1(i)⇐/braceleftbiggT1(i), fori≤itmp\nT1(itmp)×r(i−itmp)\nc ,otherwise.\nTt2(j)⇐/braceleftbiggT2(j), fori≥jtmp\nT2(jtmp)×r−(jtmp−j)\nc ,otherwise.\nOutput: the target tempi Tt1, Tt2\n4.3 Synthesis Process\nAfter changing the tempi, we align clip2at the start posi-\ntion of clip1’s transition segment. Then, we apply cross-\nfading on the transition segments. The effect of cross-\nfading is achieved by using the log-transform method [3]\nbecause it better ﬁts the actual human auditory system.\n5. MUSIC FILTERING AND ORDERING\nIn this section, we describe the extra steps for dealing with\nmore than two clips: music ﬁltering and ordering.\n215Poster Session 2\nFigure 6 . Tempi change in the transition length of the clips\nof “Let’s move it” and “Real man.”\n5.1 Music Filtering\nIn order to reduce the probability of pasting quite distinct\nclips and the computational load in the ordering process\n(c.f. Section 5.2), we eliminate clips with extreme values\nby pair-wise comparison. A clippis said to be extreme\ndissimilar and should be eliminated if there are more than\nhalf of the other clips ( clipq) in the database dissimilar to\nclipp. The dissimilarity and similarity of any two clips are\nmeasured sequentially as follows.\n5.1.1 Loudness Dissimilarity\nThe loudness dissimilarity is deﬁned by the ratio rL(p, q)\nof the average loudness value of two clips clippandclipq,\nas shown in Eqn. (7)\nrL(p, q) =|Ldp−Ldq|\nLdp, q= 1. . . W, q /negationslash=p (7)\nwhere LdpandLdqare the average loudness values of the\npthand the qthclips in the datasets and Wis the total num-\nber of clips. The loudness values are computed by accumu-\nlating log-energy (in db) in all the frequency bands. clipp\nandclipqare said to be loudness-dissimilar if rL(p, q)is\ngreater than a certain threshold. By Weber’s law [10], the\nJND of loudness in db is 0.1, i.e. we will perceive the loud-\nness change between clippandclipqwhen the changing\nratio ( rL(p, q)) is greater than 0.1. Since we have applied\nlog-transform mechanism to smooth the change of volume\nin the sound effect module, we set the threshold value as\n0.2 instead of the original strict standard.\n5.1.2 Tempo Dissimilarity\nBorrowing the concept in section 4.2, clippandclipqare\nsaid to be tempo-dissimilar if there are not enough length\nfor them to gradually adjusting the tempi from one to the\nother. The tempo dissimilarity is deﬁned by rT(p, q):\nrT(p, q) =/parenleftbiggTq\nTp/parenrightbigg 1\nLp+Lq\n, q= 1. . . W, q /negationslash=p (8)\nwhere TpandLpare the minimal tempo value of the last\nquarter in clippand the corresponding length from the po-\nsition of Tpto the end of clipp. Similarly, TqandLqare\nthe maximal tempo value of the ﬁrst quarter in clipqand\nthe corresponding length, as shown in Figure 7. If rT(p, q)\ndoes not lie in the range of JND mentioned in section 4.2,\nthere will not be enough transition length for changing\ntempi from clipptoclipqand they should be regarded as\ntempo-dissimilar.\nTpMinimal  value clip clip p\nThe last quarter Lpq\nThe first quarter \nclip \nTclip q\nMaximal  value TqLqFigure 7 . Finding tempo dissimilarity.\n5.1.3 Chroma Histogram Similarity\nIn this module, we tend to avoid concatenating clips with\ndifferent pitch distribution. The reason is as follows: the\nmusic paste will be unpleasant if we directly combine clips\nof different tonalities (e.g. C Major →e/flatminor) with-\nout modulation. Generally speaking, music clips with the\nsame tonality contain similar pitch distributions. Thus, we\nconstruct a chroma histogram for each clip to represent its\ndominant pitch distribution and compare the clips by this\nhistograms. For the 12 dimensional chroma vector ( /vectorCpi)\nof the ithbeat in clipp, we choose the index of its maximal\nvalue to represent the chroma dominant pitch ( CMpi) of\nthis beat. That is,\nCMpi= arg max\nuCpi(u), u= 1. . .12 (9)\nThe chroma histogram of clipp(CHp) is constructed from\nCMpi’s. Inspired by the commonly used color histogram\nintersection method [14] in the computer vision ﬁeld, we\ndeﬁne the chroma histogram similarity between clippand\nclipqby\nSH(p, q) =/summationtext12\nu=1min(CHp(u), CH q(u))/summationtext12\nu=1CHp(u)(10)\nwhere q= 1. . . W ,p/negationslash=q. Analogous to the two previ-\nous subsections, clippandclipqare viewed as dissimilar if\nSH(p, q)is less than 0.5.\n5.2 Music Ordering\nIn the music ordering process, we tend to ﬁnd an appropri-\nate order to minimize the average distance values between\neach clip pair. For example, if the transition segments be-\ntween clip1andclip3is not similar enough, maybe clip2\ncan be the bridge of them. Besides, the transition segments\nfrom clip1toclip2may be less similar as compared with\nthe transition segments from clip2toclip1. Therefore,\nthe ordering problem can be formulated as ﬁnding a path\nwhich goes through all clips in the datasets with minimum\ncost in the ordering matrix ( Do) deﬁned as follows:\nDo(p, q) = min\ni,j,L1\nL+ 1L/summationdisplay\nl=0Dcrpq(i+l, j+l) (11)\nwhere L∈[Lmin, Lmax]. To reduce the computation, we\nuse a method analogous to the greedy algorithm but the\npath found cannot be guaranteed to reach the global opti-\nmum. The procedure is as follows:\n21610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nclip 1clip 2clip 3clip 4\nclip 10 0.3486 0.329 0.342 \nclip 20.3936 0 0.4704 0.4577 \nclip30.2609 0.537 0 0.4806 p3\nclip 02898 04826 03732 0 clip 40.2898 0.4826 0.3732 0Figure 8 . An example of the ordering matrix for 4 clips.\n1. Find the minimum value in the ordering matrix and\nset the corresponding two clips as the initial clips.\n2. Find the minimum value in the row that correspond-\ning to the last clip in the order found previously (each\nclip can only be visited once) and then add the cor-\nresponding clips to the order.\n3. Repeat step 2 until all the values in the target row are\nlarger than a predeﬁned threshold or all clips have\nbeen visited.\nFigure 8 shows an example of an ordering matrix con-\nstructed by four clips. First, we look for the minimum\nvalue in the matrix: 0.2609. We set the order as 3 →1.\nThen, we check the values of ﬁrst row: {0, 0.3486, 0.3290,\n0.3420 }. Since the ﬁrst entry (0) represents clip1goes to\nclip1itself and the third entry (0.3290) means clip1goes\ntoclip3again, we would not consider these two values.\nWe ﬁnd the minimum value of the rest: {0.3486, 0.3420 }\nis 0.3420. Thus, the order becomes 3 →1→4. Next,\nwe check the fourth row and ﬁnd 0.4826 is the only left\nvalue, so we compare it with the predeﬁned threshold. If it\nis smaller than the threshold, the order would become 3 →\n1→4→2. Otherwise, we would not concatenate clip2\nand the order would be just 3 →1→4. Currently, the\nthreshold is 0.5.\n6. EXPERIMENTS AND USER EV ALUATIONS\nThe experiments are conducted on the basis of user eval-\nuations. In order to reduce the impact of the prejudices,\nevaluators will not be informed the methods used in the\ntesting sequences.\n6.1 Overlap Length Discussion\nAssuming that the smoothness of the results depends on\nthe overlap length, we let 15 evaluators judge the music\npastes with different overlap lengths. 8 sets of clips ( ≈40\nsecs/clip) from different types of Chinese pop songs are\nused. We generate music pastes with 2 overlap lengths\n(force L∗= 4, 12 beats) and give each of them three dif-\nferent αvalues (c.f. Eqn. (3)) in the transition segments\nﬁnding process. Figure 9 describes the overall results. The\nvertical axis represents the percentages of how many peo-\nple prefer each method. We found that results with longer\noverlap length aren’t really more acceptable than the shorter\nones. The reason is probably that the similarity of transi-\ntion segments decreases as the overlap length grows. An-\n0 4 0.6 0.8 1“4 beats” is better \n“12 beats” is better \nThe same \n00.2 0.4\nChroma Rhythm Both Figure 9 . Overlap length comparison results.\n0.5 \n0.25 \n00\nChroma Rh ythm Both \nFigure 10 . Comparison of 3 measurements.\nother observation is that the evaluator’s acceptance varies\nwith the types of the music clips. For instance, the accepted\noverlap length between two rap clips may be shorter than\nthose of two lyric clips. Besides, over 60% of the evalua-\ntors preferred 4 beats as the overlapping length. Hence, we\nset the default overlap length to 4 beats long in the next sec-\ntion to compare the inﬂuence of different similarity mea-\nsurements.\n6.2 Comparison of different similarity measurements\nThe similarity measurements we used for comparison are\nchroma only, rhythm only and both chroma and rhythm,\ni.e.α= 0, 1, 0.5. We utilized 8 sets of clips from songs\nin different languages. Fifteen evaluators gave scores from\n1 to 10 to represent their satisfactions (higher score means\nbetter satisfaction) with respective to the feeling of intru-\nsion. Figure 10 shows the percentages of how many peo-\nple prefer each method. We found that chroma may be the\nmost preferred measurement. Therefore, we choose the\nchroma measurements to conduct the following compari-\nson with automated DJ.\n6.3 Comparison with Automated DJ\nIn the automated DJ system [4], we can only use the mu-\nsic clips existing on the Amazon website and they should\noverlap at least 2 seconds. Thus, we selected 5 sets of pop\nsongs available on Amazon and set the overlap length to 8\nbeats (≈2 secs). Each set consists of two music pastes, one\nis generated by automated DJ and the other is by our ap-\nproach. Seventeen evaluators participate in choosing their\npreferable method. Similarly, Figure 11 shows the per-\ncentages of how many people prefer each method. Overall\nspeaking, the music pastes generated by our approach are\npromising and preferred as compared to those generated by\nthe automated DJ. The acceptances may vary with different\nsets. For instance, our method is superior to automated DJ\n217Poster Session 2\n1Auto DJ[4] is better \n0.8 Our method is better \nThe same \n0.6 The  same \n0.4 \n0 2 0.4 \n00.2\n0\nSet 1 Set 2 Set 3 Set 4 Set 5 Figure 11 . Comparisons with Automated DJ.\nin a great level for set 2. The reason is that set 2 is a combi-\nnation of a female voice and a male voice singing in quite\ndifferent pitches. The results will be a little bit intrusive\nif we just concatenate these clips by rhythm-similar seg-\nments. Instead, we choose pitch-similar segments where\nthe pitch of female voice downwards and the pitch of male\nvoice upwards. The results would be more pleasant to hear.\n6.4 Discussion\nAccording to the above experimental results, we discov-\nered the following factors affecting people’s feeling toward\nthe music paste: (i) Language and lyrics. The music\npastes with unfamiliar languages will be probably more ac-\nceptable. In our datasets, half of sets are with unfamiliar\nlanguages to the evaluators. And 75% of this kind of pastes\nare scored higher than the average scores. This is proba-\nbly because the intrusiveness increases when the lyrics of\nclips in familiar language conﬂict with each other. In con-\ntrast, it is not easy for evaluators to perceive the transition\nin clips with unfamiliar languages. (ii) The ending po-\nsition of phrases in the clips. The music pastes will be\nprobably more acceptable if the clips are transformed at\nthe ending position of phrases. We have gathered statistics\non our experiment datasets. There are 50% of the sets ﬁt\nthe mentioned condition. The scores are all higher than the\naverage scores of all sets. The reason is probably that peo-\nple’s anticipation for the ending phrases will smooth the\nintrusiveness at the transition. (iii) Familiarity with the\nmusic clips. The music pastes are probably less acceptable\nif the evaluators have heard the music clips before. 71% of\nthe evaluators gave higher score to unfamiliar music pastes\nthan familiar ones. (iv) The inﬂuence of vision. The tran-\nsition in music paste would be less noticeable if vision in-\nformation involves. We combined one of our music pastes\nwith a photo slideshow and let the evaluators view and lis-\nten again. Over 90% of evaluators gave higher scores to it\nbecause they almost did not notice the transition.\n7. CONCLUSION AND FUTURE WORKS\nIn this paper, we provide a tool for automatically choos-\ning proper music clips from a given audio collection and\ncombining the chosen clips as euphonious as possible. We\nemploy common auditory music features and borrow the\nconcept from distance matrix to determine the transition\nsegment and choosing music clips. The transition length\nis determined by Weber’s Law. Besides, we apply phasevocoder to adjust the audio ﬁles and use cross-fading in\nsynthesis process. Moreover, we conduct subjective evalu-\nations on the correlation between pasting methods and au-\nditory quality of combined clips. The overall experiment\nresults show that the generated music pastes are acceptable\nto humans.\nThere are rooms for improving the proposed system.\nFirst, the pasting method is restricted to clips with similar\nenough transition segments. Perhaps the clips can be con-\nnected by automatically generating appropriate intermezzo\nor bridge music. Second, the proposed work can be me-\nliorated by the improvement of music analysis techniques.\nMore similarity measurements closer to style-similarity (tim-\nbre, rhythm) would improve the ﬁltering process. Further-\nmore, more representative auditory features and similar-\nity measurements, techniques for music structure analysis\nand phrase boundary extraction would help the process of\nlocating transition segments. Third, studies on the vari-\nant overlapped length range in the transition segments are\nstill worth investigating while currently the whole transi-\ntion segments are overlapped. In the future, we will con-\ntinue our investigation in these directions.\n8. REFERENCES\n[1] G. Loy: Musimathics , pp. 295–296, 347–350, The MIT\nPress, 2006.\n[2] D. Cope: Experiments in Musical Intelligence , Madison, WI:\nA-R Editions, 1996.\n[3] T. Jehan: “Creating music by listening,” PhD thesis, MIT\nMedia Lab, Cambridge, MA, 2005.\n[4] T. Jehan: “This is My Jam,” visited at Feb. 18, 2008; http:\n//thisismyjam.com/\n[5] M. Cooper, and J. Foote: “Automatic Music Summariza-\ntion via Similarity Analysis,” Proceedings of the Interna-\ntional Symposium on Music Information Retrieval (ISMIR\n’02), Paris, France, 2002.\n[6] C. A. Harte and M. B. Sandler: “Automatic chord identiﬁ-\ncation using a quantised chromagram,” Proceedings of the\nAudio Engineering Society , Spain, 2005.\n[7] M. Cicconet: “Rhythm features,” visited at Dec. 13,\n2008;http://w3.impa.br/∼cicconet/cursos/\nae/spmirPresentation.html\n[8] “Virginia Tech Multimedia Music Dictionary,” visited\nat July 31, 2009; http://www.music.vt.edu/\nmusicdictionary/ .\n[9] S. Dixon: “Evaluation of the Audio Beat Tracking System\nBeatRoot,” Journal of New Music Research, V ol. 36, No. 1,\npp. 39–50, 2007;\n[10] G.T. Fechner: Elements of psychophysics 1, Holt, Rinehart &\nWinston, New York, 1860.\n[11] Kim Thomas: “Just Noticeable Difference and Tempo\nChange,” Journal of Scientiﬁc Psychology, May 2007.\n[12] M. Dolson: “The phase vocoder: a tutorial,” Computer Music\nJournal , V ol. 10, No. 4, pp. 14–27, 1986.\n[13] C.J. Plack and R.P. Carlyon: “Loudness perception and in-\ntensity coding,” Hand book of Perception and Recognition\n6: Hearing , pp. 123-160, Editorial B.C.J. Moore, Academic\nPress, London, 1995.\n[14] M. J. Swain and D. H. Ballard: “Color indexing,” Interna-\ntional Journal of Computer Vision , V ol. 7, No. 1, pp. 11–32,\n1991.\n218"
    },
    {
        "title": "Adaptive Multimodal Exploration of Music Collections.",
        "author": [
            "Dominik Lübbers",
            "Matthias Jarke"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415518",
        "url": "https://doi.org/10.5281/zenodo.1415518",
        "ee": "https://zenodo.org/records/1415518/files/LubbersJ09.pdf",
        "abstract": "Discovering music that we like rarely happens as a result of a directed search. Except for the case where we have exact meta data at hand it is hard to articulate what song is attractive to us. Therefore it is essential to develop and evaluate systems that support guided exploratory browsing of the music space. While a number of algorithms for organizing music collections according to a given similarity measure have been applied successfully, the generated structure is usually only presented visually and listening requires cumbersome skipping through the individual pieces. To close this media gap we describe an immersive multimodal exploration environment which extends the presentation of a song collection in a video-game-like virtual 3-D landscape by carefully adjusted spatialized plackback of songs. The user can freely navigate through the virtual world guided by the acoustic clues surrounding him. Observing his interaction with the environment the system furthermore learns the user’s way of structuring his collection by adapting a weighted combination of a wide range of integrated content-based, meta-data-based and collaborative similarity measures. Our evaluation proves the importance of auditory feedback for music exploration and shows that our system is capable of adjusting to different notions of similarity.",
        "zenodo_id": 1415518,
        "dblp_key": "conf/ismir/LubbersJ09",
        "keywords": [
            "guided exploratory browsing",
            "music space",
            "immersive multimodal exploration",
            "video-game-like virtual 3-D landscape",
            "carefully adjusted spatialized playback",
            "audio feedback",
            "users way of structuring",
            "adaptation to different notions",
            "collaborative similarity measures",
            "system evaluation"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nADAPTIVE MULTIMODAL EXPLORATION OF MUSIC COLLECTIONS\nDominik L ¨ubbers∗†, Matthias Jarke∗\n∗Informatik 5\nRWTH Aachen University\nAachen, Germany†Dept. Applied Information Technology\nGerman University of Technology\nMuscat, Sultanate of Oman\nABSTRACT\nDiscovering music that we like rarely happens as a result\nof a directed search. Except for the case where we have\nexact meta data at hand it is hard to articulate what song\nis attractive to us. Therefore it is essential to develop and\nevaluate systems that support guided exploratory browsing\nof the music space.\nWhile a number of algorithms for organizing music col-\nlections according to a given similarity measure have been\napplied successfully, the generated structure is usually only\npresented visually and listening requires cumbersome skip-\nping through the individual pieces.\nTo close this media gap we describe an immersive mul-\ntimodal exploration environment which extends the pre-\nsentation of a song collection in a video-game-like virtual\n3-D landscape by carefully adjusted spatialized plackback\nof songs. The user can freely navigate through the virtual\nworld guided by the acoustic clues surrounding him.\nObserving his interaction with the environment the sys-\ntem furthermore learns the user’s way of structuring his\ncollection by adapting a weighted combination of a wide\nrange of integrated content-based, meta-data-based and col-\nlaborative similarity measures.\nOur evaluation proves the importance of auditory feed-\nback for music exploration and shows that our system is\ncapable of adjusting to different notions of similarity.\n1. INTRODUCTION\nEarly work in Music Information Retrieval primarily con-\ncentrated on the development and evaluation of systems to\nsupport the identiﬁcation of songs in a collection given a\nwell-formulated query. According to Cunningham [1], this\nretrieval paradigm hardly matches the way we usually look\nfor CDs in a music shop. Instead of searching for a ded-\nicated album, participants in a user study showed a more\nexploratory browsing behaviour, which can be summarized\nas “shopping around” in contrast to “shopping for”. This\nexploratory behaviour is however not completely chaotic:\nUsers are reported to prefer some sort of structure in a mu-\nsic collection (e.g. a categorization according to genres),\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.as long as this organization is intuitively understandable to\nthem.\nEven having a speciﬁc song in mind, we may ﬁnd it dif-\nﬁcult to articulate the information demand properly, if the\nname of the artist and the song title are unknown. Query\nby Example approaches like Query by Humming can only\npartly bridge this media discontinuity gap.\nThese reasons have led to an increased interest in ex-\nploration environments for music over the last years [2–\n4]. Most of theses approaches focus on visualizing a mu-\nsic collection with only standard playback functionality,\nwhich results in a media discontinuity problem in the oppo-\nsite direction and does not exploit the human’s capability\nto orientate himself in a complex environment of simulta-\nneously playing spatialized sounds.\nTherefore, we developed and evaluated an exploration\nprototype that provides an immersive virtual environment,\nin which the user can navigate guided by acoustic clues\nfrom song playbacks surrounding him.\nAs in previous approaches, the placement of pieces in\nthis environment is based on a similarity function. The\nnotion of similarity is known to be multifaceted, highly\nuser-dependent and also inﬂuenced by the song collection\nat hand. We therefore allow the user to move songs in\nthe environment as well as raise or lower borders between\nsong clusters. Observing the user’s interaction with the\nlandscape we furthermore adapt a linear combination of\ncontent-based and collaborative similarity measures to best\nﬁt his understanding of similarity.\nTo our knowledge our prototype is therewith the ﬁrst\nmultimodal exploration environment which integrates an\nimmersive virtual 3D-landscape of clustered songs with\nspatialized audio playback respecting humans’ auditory per-\nception limitations and furthermore adapts to the user’s\nstrategy of organizing his collection by learning the weights\nof a wide range of different music similarity measures.\nIn the next section we give a brief overview of related\nwork on exploration environments for music collections.\nThen we list the integrated base similarity functions used\nas components of a user-adaptive similarity measure. The\nfollowing section describes our exploration environment in\ndetail. We continue with the explanation of the similar-\nity measure adaption process, which is followed by results\nfrom an qualitative and quantitative evaluation of our sys-\ntem and concluded by some ﬁnal remarks and an outlook\non further research.\n195Poster Session 2\n2. RELATED WORK\nOver the last years, a number of proposals for visualizing\nmusic collections have been made.\nPampalk et al. reduce the audio signal of a song to the\nmedian of frame-based Fluctuation Patterns , which model\nloudness periodicities in different frequency bands of the\nsignal [5]. These features are used to train a small-size\nrectangular Self-Organizing Map (SOM). They interpret\nthe estimated song densities of the cells as the height pro-\nﬁle of a map. Applying an appropriate color map gener-\nates an intuitive visualization of similar song clusters posi-\ntioned on “Islands of Music” separated by blue water.\nThe approach by Moerchen et al. is conceptually simi-\nlar [7]. Their work mainly differs in the use of a compact\nbut highly discriminative content-based feature set and the\ndistribution of the collection items over a larger, emer-\ngent SOM. Still, Moerchen et al. do not integrate any kind\nof acoustic presentation besides a standard playback func-\ntionality of a selected song.\nIn contrast to this, Hamanaka and Lee focus on audio-\nonly exploration of a given song set [8] without the need\nfor a display. By spatializing songs according to different\npre-deﬁned allocation schemes, a user wearing a special\nheadphone has the impression of being surrounded by si-\nmultaneously playing sound sources from different direc-\ntions. Sensors mounted on the headphone detect the move-\nment of the head and allow the user to change focus to\nsongs he perceives from left or right. This interaction pro-\nmotes the impression of an immersive virtual environment.\nAdditionally, he can narrow the range of sounding sources\nby putting his hands behind the ear and thereby fading out\nsongs that are not placed directly in front of him. This re-\nsembles the focus of perception mechanism we introduced\nin [9] and supports humans’ ability to concentrate on spe-\nciﬁc sounds in a complex mixture, known as the cocktail\nparty effect.\nTo our knowledge, the approach by Knees et al. is the\nﬁrst one that combines SOM-based structuring of music\ncollections with three-dimensional visualization and au-\nralization to an immersive multimodal exploration envi-\nronment [10]. Their work extends the Island of Music\nmetaphor by using the smoothed height proﬁle of SOM\ncells to generate a virtual 3D-landscape that the user can\nintuitively explore. Songs in the neighborhood of the cur-\nrent position sound from the respective direction. Knees\nand et. do not implement a focus mechanism, which seems\nto be critized by one of the comments in their user study,\nthat asks for a larger landscape especially when facing\ncrowded regions.\nAll of the above exploration environments quantify sim-\nilarity between songs according to a ﬁxed measure, that is\nsupposed to reﬂect a generic similarity understanding by\nthe average user. Recognizing the diversity of the similar-\nity notion, Pampalk et al. align three SOMs representing\ntimbral, rhythmic and metadata-provided aspects and al-\nlow the user to gradually change between these presenta-\ntions [11].\nBaumann linearly combines content-based similaritywith cultural similarity and text-based similarity of the lyr-\nics [12]. The user can adjust the weights of this trimodal\nmeasure by moving a virtual joystick into the direction of\nthe favoured similarity aspect.\nInstead of forcing the user to learn the semantics of dif-\nferent similarity measures and to decide for the individual\nimportance of them, we propose a machine learning strat-\negy that induces the weights of each component from the\nuser’s interaction with our immersive multimodal explo-\nration environment.\nFigure 1 depicts the stages involved in generating and\nadapting this environment. The following sections describe\nthese phases in detail.\n3. SIMILARITY\nTo model a user’s notion of similarity as precisely as pos-\nsible, it is mandatory to combine a number of base similar-\nity measures covering different musical aspects and let the\nsystem adapt their weights.\nWe therefore decided to integrate timbral similarity mea-\nsures (based on stochastic MFCC models as proposed by\nLogan/Salomon [13] and Aucouturier/Pachet [14] or the\n20-feature set proposed by Moerchen et al. [7]) as well as\nmore rhythm-based measures (Fluctuation Patters and Pe-\nriodicity Histograms [11]). Furthermore we calculate the\naverage and variance of 15 frame-based audio features as\nprovided by the MIRtoolbox library [15]. These features\nare of varying complexity, ranging from simple RMS val-\nues over spectral centroids and roughness measures to key\nclarity and tempo estimates.\nAdditionally, we use ID3 metadata to make contextual\ninformation available. In particular, we calculate the time\nperiod between the publication of two pieces. To group\nsongs by the same artist even in the commonly encountered\npresence of small typing errors, we furthermore calculate\nthe edit distance between ID3 artist strings.\nThese similarity measures are complemented by three\ncollaborative approaches based on direct last.fm similarity\nlinks, last.fm top tags and co-occurrence on playlists pub-\nlished on Art of the Mix.\nlast.fm offers the compilation of recommended tracks\nto a personalized music stream based on the user’s proﬁle.\nThis requires the establishment of similarity links between\ntracks. last.fm allows access to this information by a web\nservice that returns a number of similar tracks to a given\nsong. Each of these similar tracks is assigned a match\nvalue that quantiﬁes the degree of similarity scaled to 100\nfor the most similar song. We consider the presence of a di-\nrect similarity link as a strong indication of similarity, even\nif the match value might be low. Therefore we transform\nthe match score with a compressed exponential function to\na distance value. Averaging the mutual distances to guar-\nantee symmetry leads to the following calculation for two\ntrackstriandtrj:\ndDL(tri,trj) = 0.5(e−cDL·mstri(trj)\n100 +e−cDL·mstrj(tri)\n100 ),\nwhere mstri(trj)denotes the match score of track trjin\n19610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 1 . Data transformation stages for building and adapting the exploration environment.\nthe list of similar tracks to track triif present and 0 oth-\nerwise. We empirically chose a value of cDL= 5 for the\ncompression factor.\nWhile a track-based similarity measure is very speciﬁc,\nit may be difﬁcult to ﬁnd enough collaborative data for a\nreliable estimate. We therefore calculate the distance be-\ntween the artists of two songs in the same way as above and\ncombine it linearly with the track-based measure weighting\nthe more precise track distance double.\nInstead of assigning ﬁxed genre categories to songs,\nlast.fm allows users to tag tracks with arbitrary keywords\nfavouring the emergence of a folksonomy over the deﬁni-\ntion of a static genre hierarchy. The comparison of these\nsong descriptions is another valueable source of similarity.\nRetrieving the top tags for a song results in a list ranked\naccording to the frequency used to annotate the song. Un-\nfortunately, last.fm’s count attribute does not quantify\nthis per-track frequency but the overall popularity of a tag.\nLacking further information, we consider the tags as nat-\nural language terms in a text about the track. This allows\nus to assume that the tag distribution follows Zipf’s law\nand approximate tag frequencies by a Zipﬁan density func-\ntion. Likewise, we do not have access to the ratio of tracks\nthat are tagged with a certain keyword and have to estimate\nthe inverse document frequency on the basis of the overall\npopularity of a tag.\nThese approximations can be used to weight the impor-\ntance of a tag for a song according to the standard tf ·idf\nscheme. The track-based top tag-similarity between two\nsongs can ﬁnally be calculated as the cosine between a-\nligned weight vectors. For the same reasons as above we\nalso calculate top tag-similarity on artist level.\nThe last distance calculation we derive from collbora-\ntive data is based on co-occurrences of songs on playlists\n(called mixes ) that are published by users on the Art of the\nMix portal1. We follow the assumption that two pieces\noccuring on the same list ﬁt the same taste and can be con-\nsidered as similar. To quantify this notion we use a simple\noverlap distance measure:\ndAotM (si,sj) = 1−|M(si)∩M(sj)|\nmin{|M(si)|,|M(sj)|},\nwhereM(si)denotes the set of mixes that contain song si.\nAs done for the other collaborative measures, we combine\nthis distance with its artist-based variant.\n1www.artofthemix.orgSince some of the presented measures (like Logan/Sa-\nlomon) are based on pairwise comparisons between songs,\nthe composed distance values are arranged in a (symmet-\nric) matrix. As the SOM training algorithm requires the\nrepresentation of each item as a feature vector in Euclidean\nspace, we apply multi-dimensional scaling (MDS) to ﬁnd\nd-dimensional coordinates for each song such that the Eu-\nclidean distance between two song vectors resembles the\ndistance matrix value (see ﬁgure 1). In our experiments we\nchose a value of d= 20 , which matches the dimensionality\nof the data space used for the MusicMiner-SOM [7].\n4. EXPLORATION ENVIRONMENT\n4.1 SOM Training\nAs humans are used to intuitively estimate distances be-\ntween points on a 2-dimensional plane, dimensionality re-\nduction techniques that map high-dimensional data to low-\ndimensional representations while preserving distances as\nmuch as possible are popular data visualization strategies.\nOne of these techniques is the Self-Organizing Map\n(SOM) proposed by Kohonen,which arranges disjoint cells\n{yi}on a usually rectangular grid. Each yiis associated\nwith a model vector mifrom data space. We initialize\nthe model vectors with linear combinations of the ﬁrst two\nprincipial components of the song feature values according\nto the grid coordinate of their cell.\nIn each iteration twe randomly choose a data vector xj\nand identify the cell bmwith the closest model vector to\nxj, i.e. that minimizes ||xj−mbm||. The model vectors\nof this Best Matching Unit bmand its neighborhood are\nmoved towards xjaccording to the following equation:\nmi(t+ 1) =mi(t) +α(t)·hi,bm(t)[xj−mi(t)],\nwhereα(t)denotes the learning rate at time tandhi,bm(t)\nquantiﬁes the distance between xiandbm, usually by some\nGaussian function centered around bm. Sinceα(t)and\nhi,bm(t)decrease with each iteration and thereby weaken\nthe adaption process with time, the map converges to a con-\nﬁguration where the Best Matching Units of similar data\npoints are located close to each other.\nIn contrast to clustering algorithms like k-Means, a SOM\nis also capable of adaquately representing data points that\nlie in between clusters and reveals the macro-structure of\nthe data space by retaining similarity relationships between\nclusters themselves.\n197Poster Session 2\nThe distribution of model vectors over the grid that is\ngenerated on the ﬂy during the adaption contains additional\nvalueable information about the similarity space: This in-\nformation can be visualized by the U-Matrix [6], which as-\nsigns to each cell the average distance of its model vector\nto the model vectors of its neighbors. High values thereby\nindicate clear borders separating coherent regions of simi-\nlar objects on the map.\n4.2 Visual Presentation\nDisplaying these U-Matrix values and placing songs at their\nBest Matching Unit already yields an untuitively under-\nstandable visualization of the collection. But if we inter-\npret the U-Matrix values as heights of a landscape we can\ngenerate a 3-D terrain and allow the user to leave his bird’s\neye-view on the music space in favor of becoming part in\nan immersive virtual environment.\nOur prototype is based on Microsoft’s game framework\nXNA 3.0 to realize efﬁcient state-of-the-art visualization.\nWe generate a high-resolution terrain mesh by bilinear in-\nterpolation of the U-Matrix height values and use a cus-\ntomized shader for visualization which appropriately com-\nbines sand, grass, mountain and snow textures according\nto the height.\nBy default, songs are visualized as small cubes textured\nwith the cover image of their album if available. The po-\nsition of a cube is mainly determined by the coordinates\nof the song’s Best Matching Unit. To avoid clumping at\ngrid points, we slightly move it towards the location in the\nimmediate neighborhood where the bilinearly interpolated\nmodel vector is closest to the feature vector of the song.\nThe user can freely run through the landscape, move his\nhead around and lift up to get an overview of the scenery.\nFigure 2 shows a screenshot of our environment taken from\ndifferent elevation levels. The user is standing in (or over)\na valley that contains songs from the German hiphop group\nFanta4 . As can be seen, these songs are clearly separated\nfrom different pieces by surrounding hills.\n4.3 Auditory Presentation\nMusic is described best by music. This asks for the pres-\nence of acoustic information as guidance in the exploration\nprocess: Since humans are used to differentiate well be-\ntween sound sources from different directions, exposing\nthe user to simultaneously playing spatialized music facil-\nitates efﬁcient and well-informed navigation through the\ncollection.\nFortunately, the above virtual environment can be ex-\ntended naturally to incorporate the presentation of acoustic\ninformation, simply by associating each cube with a sound\nsource playing the song from its location in the landscape.\nAs described in [9] the unrestricted simultaneous play-\nback of many songs quickly overwhelms the user’s audi-\ntory system and confuses more than it helps. Following\nideas from visual perception we therefore deﬁne the point\nthe user is currently looking at as the Focus of Perception\nand attenuate the volume of songs the more they deviate\nfrom the view direction. To allow for broad “listeningaround” as well as for clearly focussing on the sound in\nfront we model the strength of this attenuation by a Gaus-\nsian function with user-adjustable variance. More precisely,\nthe gain factor due to perception focussing is given as fol-\nlows:\ngPF(ϕ) =e−ϕ2\nσ2,\nwhereϕdenotes the angle between the direction to the\nsong and the view direction and σ2=−AoP\nln(gAoP )is the vari-\nance for the user-adjustable Angle of Perception AoP , such\nthatgPF(AoP) =gAoP .\nWe describe the inﬂuence of a song’s distance to its gain\nby an inverse distance model:\ngDist(d) = min(1,decSpeed\nd−decSpeed\nminDist+ 1),\nwheredis the distance to the song, decSpeed parame-\nterizes the speed of gain decrease per distance unit and\nminDist denotes the distance at which no attenuation takes\nplace.\nTo summarize, the overall gain for a song sat location\n/vector psassuming a listener’s position /vector pand a view direction /vectorvd\nis the product of its gain inﬂuences:\ng(s,/vector p) =gDist(||/vector p−/vector ps||)·gPF(/negationslash(/vectorvd,/vector p s−/vector p))·gmuff (s,/vector p).\ngmuff (s,/vector p)reduces the gain for a song, that is hidden be-\nhind a rise of the terrain. To generate the impression of a\nmufﬂed sound this is complemented by a highcut ﬁlter.\nStill, the simultaneous playback of all songs in the col-\nlection is too demanding (as well from an computational as\nfrom a perceptual point of view). We tested several song\nselection criteria and decided for a simple approach that\nguarantees perceptual separability and does not change the\nset of active sources when the user rotates his head: First,\nall songs in the neighborhood of the listener’s position are\nsorted according to their gain factor. Following this order\nwe then successively activate songs as long as they do not\nsound from a direction similar to the one of already playing\nsongs.\n4.4 User Interaction\nA standard xBox 360 game controller can be used to nav-\nigate in the virtual world. Besides this, the user can cus-\ntomize the landscape as follows:\n•Songs that seem to be misplaced in the opinion of\nthe user can be moved easily.\n•Alternatively, songs can be released to let the system\nﬁnd a new location during the next adaption cycle.\n•Landmarks can be placed to emphasize and easily\nrecover locations on the terrain. The user can choose\nbetween different sign types that can be labeled or\ntextured with arbritary images. Figure 2 shows two\ntriangular landmarks.\n•The terrain can be altered by raising or lowering its\nheight at the position the user points to. This allows\nthe formation of new separating hills between song\n19810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 2 . Screenshot of the exploration prototype: Views from different elevation levels\nclusters that are perceived as different or the remove-\nment of borders between areas that the user judges\nsimilar.\n5. USER ADAPTATION\nAs Cunningham observes, music listeners organize their\npersonal collections according to different criteria. Some\nmay sort their albums by the year of publication, some may\ncluster their music by genre, for others rhythmic content\nplays a dominant role. An exploration environment should\nbe ﬂexible enough to follow the user’s organization strat-\negy.\nInstead of asking the user to articulate his structure prin-\nciples explicitly we decided to learn his similarity notion\nfrom his interaction with the environment. Adapting the\nweights in the linear similarity model properly allows us\nto reposition songs that have been released by the user or\nto place new songs that are added to the collection.\nThe user can build or destroy separating hills between\nsongs. To account for these terrain changes, we numeri-\ncally integrate over the height proﬁle ( hn) between the lo-\ncationspiandpjand compare this to the situation before\nthe change ( ho):\ntdt(si,sj) =1\n||/vector pi−/vector pj||·/parenleftBigg/integraldisplay/vector pj\n/vector pi(hn(/vector p)−ho(/vector p))d/vector p/parenrightBigg\nThe combination of tdtwith the Euclidean distance be-\ntween the (interpolated) model vectors of two songs’ loca-\ntions on the map is stored in a target distance matrix. Each\nentry of this matrix is considered a training case for a linear\nregression learner, that adapts the weighting of the imple-\nmented base distances to approximate the target distance.\nAs ﬁgure 1 shows, the updated similarity model is sub-\nsequently used to rebuild the environment by the same pro-\ncess chain as before. To avoid drastic changes in the ex-\nploration space that potentially disorientate the user, we\ninitialize the vector representation of each ﬁxed song by\nits old value before the MDS optimization starts. Like-\nwise, we guarantee topographic stability of the SOM byconstantly taking a song’s old location as its Best Match-\ning Unit during training.\n6. EVALUATION\nWe conducted a user study with nine participants showing\ndifferent music taste, listening habits and experience with\ncomputer games.\nIn a ﬁrst experiment we aurally presented an unknown\nsong and measured the time needed to ﬁnd it in a collection\nof about 100 tracks, that were randomly distributed over a\nﬂat exploration plane. Cover and metadata of the wanted\nsong were not given to the user. We repeated the task for a\ndifferent song and collection, this time providing the SOM-\nbased organization. To eliminate effects from the choice of\nsong and collection, we shufﬂed task and data for different\nparticipants.\nA similar pair of experiments investigates the impor-\ntance of spatialized acoustic clues when navigating through\nthe exploration space by comparing this feature with stan-\ndard media player functionality which requires to explic-\nitly start and stop the playback of a song.\nWe found reductions in search time of 61% and 58%\non average, which demonstrate, how signiﬁcantly the user\nbeneﬁts from a well structured collection and acoustic clues\nduring the exploration.\nThe last group of experiments evaluate the adaptation\ncapabilities of our system to a user’s notion of similarity:\nWe asked the participants to customize a collection of 20\ntracks by moving the songs and changing the terrain struc-\nture. Similar to a leave-one-out evaluation we successively\nrelease one song and compare its original position to the\nlocation that would be assigned by the SOM training. This\nplacement error is calculated with and without executing\nthe adaptation procedure. The ﬁrst data series in ﬁgure 3\nshows the relative difference between these two runs and\nreveals, that generally the adaptation works well, but re-\nduces the placement error only slightly. One reason for\nthat might be the that the initial similarity measure already\ncaptured the user’s notion rather well.\n199Poster Session 2\nFigure 3 . Relative reduction of placement error by adap-\ntation to users’ similarity notion\nTherefore, we asked the users to organize the collec-\ntion according to tempo independent of the genre and again\ncomputed the relative improvement in placement error. As\ncan be seen from the second data series in ﬁgure 3 our sys-\ntem also adapts generally well to this more drastic change\nin similarity notion.\nAfter these quantitative experiments we handed out an\nextensive questionnaire for qualitative evaluation. Study\nparticipants consistently judged the usability of the system\nas high but repeatedly proposed the addition of a 2-D map\nview to the environment to avoid disorientation in the ex-\nploration landscape.\n7. CONCLUSION AND OUTLOOK\nWe presented an immersive multimodal exploration envi-\nronment, that visualizes and auralizes music collections or-\nganized according to an user-adaptable similarity model,\nwhich combines content-based, meta-data-based and col-\nlaborative similarity measures. While our evaluation shows\nthe general tractability of our approach, some open ques-\ntions for further research remain:\nSo far, we did not focus on scalability issues in our\nwork. We found, that collections of up to 400 songs are\nstill manageable in our environment. Larger numbers of\ntracks require some form of hierarchical organization to\nremain accessible. We may can adopt ideas from [16] to\nextend the SOM-based placement algorithm.\nSince they can model more complex relationships than\nvector-based distances, we deliberately integrated similar-\nity measures that require pairwise computation of distances.\nBecause of this the complexity of the similarity calculation\nstage is inO(n2). To alleviate the scalability problems\narising from this, one could restrict the calculation to some\nanchor songs . The MDS stage is already prepared to han-\ndle sparse distance matrices.\nAs shown by the evaluation, the adaption to the user’s\nsimilarity notion still has room for improvement. A rea-\nson for this might be that a linear model is not expressive\nenough to capture the intended combination of base simi-\nlarities. More complex models should therefore be investi-\ngated in future research.8. REFERENCES\n[1] S. Cunningham, N. Reeves, and M. Britland: “An\nethnographic study of music information seeking: im-\nplications for the design of a music digital library”\nJCDL ’03: Proceedings of the 3rd ACM/IEEE-CS Joint\nConference on Digital Libraries , pp. 5–16, 2003.\n[2] M. Goto and T. Goto: “Musicream: New Music Play-\nback Interface for Streaming, Sticking, Sorting, and\nRecalling Musical Pieces” Proc. ISMIR , 2005.\n[3] R. van Gulik, F. Vignoli, and H. van de Wetering:\n“Mapping Music in the Palm of Your Hand, Explore\nand Discover Your Collection” Proc. ISMIR , 2004.\n[4] E. Pampalk and M. Goto: “Musicsun: A New Ap-\nproach to Artist Recommendation” Proc. ISMIR , 2007.\n[5] E. Pampalk, A. Rauber, and D. Merkl: “Content-based\nOrganization and Visualization of Music Archives”\nProceedings ACM Multimedia , 2002.\n[6] A. Ultsch: “Self-Organizing Neural Networks for Vi-\nsualization and Classiﬁcation” Proc. GfKI , 1992\n[7] F. M ¨orchen, A. Ultsch, M. N ¨ocker, and C. Stamm:\n“Databionic Visualization of Music Collections Ac-\ncording to Perceptual Distance” Proc. ISMIR , 2005.\n[8] M. Hamanaka and S. Lee: “Music Scope Headphones:\nNatural User Interface for Selection of Music” Proc.\nISMIR , 2006.\n[9] D. L ¨ubbers: “soniXplorer: Combining Visualization\nand Auralization for Content-Based Exploration of\nMusic Collections” Proc. ISMIR , 2005.\n[10] P. Knees, M. Schedl, T. Pohle, and G. Widmer:\n“Exploring Music Collections in Virtual Landscapes”\nIEEE MultiMedia , V ol. 14, No. 3, 2007.\n[11] E. Pampalk, S. Dixon, and G. Widmer: “Exploring\nMusic Collections by Browsing Different Views” Proc.\nISMIR , 2003.\n[12] S. Baumann, T. Pohle, and S. Vembu: “Towards a\nSocio-cultural Compatibility of MIR Systems” Proc.\nISMIR , 2005.\n[13] B. Logan and A. Salomon: “A Music Similarity Func-\ntion Based on Signal Analysis” Proceedings ICME ,\n2001.\n[14] J.-J. Aucouturier and F. Pachet: “Finding Songs That\nSound the Same” IEEE Workshop on Model based Pro-\ncessing and Coding of Audio 2002.\n[15] O. Lartillot and P. Toiviainen: “A Matlab Toolbox\nfor Musical Feature Extraction From Audio” Proc.\nDAFx-07 , 2007.\n[16] A. Rauber, E. Pampalk, and D. Merkl: “Using psycho-\nAcousic Models and Self-Organizing Maps to Create\na Hierarchical Structuring of Music by Sound Similar-\nity”Proc. ISMIR 2002\n200"
    },
    {
        "title": "From Multi-Labeling to Multi-Domain-Labeling: A Novel Two-Dimensional Approach to Music Genre Classification.",
        "author": [
            "Hanna M. Lukashevich",
            "Jakob Abeßer",
            "Christian Dittmar",
            "Holger Großmann"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417535",
        "url": "https://doi.org/10.5281/zenodo.1417535",
        "ee": "https://zenodo.org/records/1417535/files/LukashevichADG09.pdf",
        "abstract": "In this publication we describe a novel two-dimensional approach for automatic music genre classification. Although the subject poses a well studied task in Music Information Retrieval, some fundamental issues of genre classification have not been covered so far. Especially many modern genres are influenced by manifold musical styles. Most of all, this holds true for the broad category “World Music”, which comprises many different regional styles and a mutual mix up thereof. A common approach to tackle this issue in manual categorization is to assign multiple genre labels to a single recording. However, for commonly used automatic classification algorithms, multilabeling poses a problem due to its ambiguities. Thus, we propose to break down multi-label genre annotations into single-label annotations within given time segments and musical domains. A corresponding multi-stage evaluation based on a representative set of items from a global music taxonomy is performed and discussed accordingly. Therefore, we conduct 3 different experiments that cover multi-labeling, multi-labeling with time segmentation and the proposed multi-domain labeling.",
        "zenodo_id": 1417535,
        "dblp_key": "conf/ismir/LukashevichADG09",
        "keywords": [
            "Automatic music genre classification",
            "novel two-dimensional approach",
            "Music Information Retrieval",
            "fundamental issues",
            "genre classification",
            "World Music",
            "manifold musical styles",
            "common approach",
            "multiple genre labels",
            "commonly used automatic classification algorithms"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFROM MULTI-LABELINGTOMULTI-DOMAIN-LABELING:\nANOVEL TWO-DIMENSIONALAPPROACH TOMUSIC GENRE\nCLASSIFICATION\nHanna Lukashevich, JakobAbeßer, ChristianDittmar, Holge rGrossmann\nFraunhoferInstituteforDigitalMediaTechnologies,Ilme nau, Germany\n{lkh,abr, dmr,grn }@idmt.fraunhofer.de\nABSTRACT\nIn this publication we describe a novel two-dimensional\napproach for automatic music genre classiﬁcation.\nAlthough the subject poses a well studied task in Music\nInformation Retrieval, some fundamental issues of genre\nclassiﬁcationhavenotbeencoveredsofar. Especiallymany\nmodern genres are inﬂuenced by manifold musical styles.\nMost of all, this holds true for the broad category “World\nMusic”, which comprises many different regional styles\nand a mutual mix up thereof. A common approach to\ntackle thisissue in manualcategorizationis to assignmul-\ntiplegenrelabelstoasinglerecording. However,forcom-\nmonly used automatic classiﬁcation algorithms, multi-\nlabeling poses a problem due to its ambiguities. Thus,\nwe propose to break down multi-label genre annotations\ninto single-label annotations within given time segments\nand musical domains. A correspondingmulti-stage evalu-\nation based on a representative set of items from a global\nmusic taxonomy is performed and discussed accordingly.\nTherefore, we conduct 3 different experiments that cover\nmulti-labeling, multi-labeling with time segmentation an d\ntheproposedmulti-domainlabeling.\n1. INTRODUCTION\nIntheﬁeldofMusicInformationRetrieval,automaticgenre\nclassiﬁcation has been covered in numerous publications.\nAlthoughgenrelabelsasbeingusedinonlinemusicstores\normusic journalsmostlyrepresentmarketingterms,genre\nitself embodies both a culturally relevant term and an in-\ntuitive concept to categorize music. Single genre labels\nusually reﬂect somesort of stylistic elementsinherentto a\npiece of music. Especially nowadays, music is inﬂuenced\nby an increasing amount of different musical styles. This\nleads to the necessity of describing single recordingswith\nmultiplegenrelabels. Atthesametime,thisincreasesam-\nbiguity in case a distinct genre classiﬁcation result is in-\ntended. Westumbledacrossthisproblemwhileattempting\ntotrainsupervisedclassiﬁersforagivensub-genreclassi ﬁ-\nPermission to make digital or hard copies of all or part of thi s work for\npersonal orclassroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2009 International Society forMusicInformation Retrieva l.cationtaxonomyofglobalmusiccontent. Itisobviousthat\nthebroadterm“WorldMusic”isoneofthemostill-deﬁned\ntagswhenbeingusedto lumpall “exoticgenres”together.\nIt lacks justiﬁcation because this category comprises such\nahugevarietyofdifferentregionalstyles,inﬂuences,and a\nmutualmixupthereof. Ontheonehand,retainingthestrict\nclassiﬁcation paradigm for such a high variety of musical\nstylesinevitablylimitstheprecisionandexpressiveness of\naclassiﬁcationsystemthatshallbeappliedtoaworld-wide\ngenre taxonomy. On the other hand, multi-labeling is not\nstraightforwardtodeployforautomaticsupervisedclassi -\nﬁcationsincedatasetswithmultipleclassassignmentsare\nnot well suited as training data due to theirinherent ambi-\nguity. Totackletheseissues,we consideredtobreakdown\nthe multi-label genre classiﬁcation problem into a set of\nsingle-label genre classiﬁcation tasks, where each classi -\nﬁer can be trained and optimized using well-deﬁned data.\nThe novelty of the proposed 2-dimensional approach for\nmulti-labelgenreclassiﬁcationconsistsinthecombinati on\nofsegment-wiseanddomain-speciﬁcgenreclassiﬁcations.\nThe term “domain” refers to the perceived semantic di-\nmensionsofmusicinwhichtheclassiﬁcationisperformed,\nin our case timbre, rhythm and tonality, which represents\nmelody and harmony. We call the introduced approach\n“multi-domain labeling”. In this paper we evaluate and\ndiscuss the potential of a more detailed approach directly,\ncompared to multi-label genre classiﬁcation. The rest of\nthis paper is organized as follows. We give an overview\nover related work to this topic in the subsequent section.\nThen,afterexplainingournovelapproachinsection3,we\ngiveanoverviewoverthe utilizeddatabasedaswell asthe\nmanual genre annotations corresponding to the proposed\nmethodin section 4. In the followingsection, we describe\nthe3evaluationexperimentsthatweperformed. Detailson\nfeatureextraction,featureselection,featurespacetran sfor-\nmation as well as on the applied classiﬁcation algorithms\nare presented in section 6. After discussing the results of\nthe experiments in section 7, we conclude our work and\nprovideperspectivesforfuturedirectionsinsection8.\n2. RELATED WORK\nVarious classiﬁcation schemes for automatic genre clas-\nsiﬁcation have been proposed during the last years. [17]\nprovides a comprehensive overview over existing publi-\ncations in the domain. There different approaches related\n459Poster Session 3\nto expert systems, unsupervised classiﬁcation, and super-\nvised classiﬁcation systems have been covered. Consider-\ningthegeneralconfusionbetweensimilargenres,relaxing\nthestrictclassiﬁcationparadigmandallowingformultipl e-\ngenreclassiﬁcationseemedtobeareasonablefuturedirec-\ntion to the authors to implement a more realistic classiﬁ-\ncation system. The earlier work of [3] gave a more pes-\nsimisticoutlookbyconsideringthetermgenretobeintrin-\nsicallyill-deﬁnedandhardlygroundedintimbrecharacter -\nistics. Already in one of the basic works on music genre\nclassiﬁcationbyTzanetakis[21],separatefeaturesetsre p-\nresentingtimbre,rhythm,andtonalitywereintroducedtha t\nallowed for different types of similarity measures. How-\never,separatedomain-speciﬁcgenremodelshavenotbeen\nproposedthere.[17]providesalsoanoverviewofdifferent\nclassiﬁer approaches applied in genre classiﬁcation, such\nasSupportVectorMachines(SVM),HiddenMarkovMod-\nels (HMM) or Artiﬁcial Neural Networks (ANN). Other\npublications such as [21] utilized Gaussian mixture mod-\nels (GMM) for this purpose. Among others, the authors\nof [18] used ensemble-based decision approaches namely\na one-against all and a round-robin algorithm to combine\nbinary classiﬁers. Different feature-space transformati on\nmethodssuchasLinearDiscriminantAnalysisare applied\nto increase discriminationbetween the classes resulting i n\nbetter classiﬁcations scores [17]. Novel musically moti-\nvatedlow-andmid-levelfeaturessuchastheOctave-based\nModulation Spectral Contrast [11] or multiscale spectro-\ntemporalmodulationfeatures[15]werereportedtooutper-\nform conventional features such as Mel-Frequency Cep-\nstral Coefﬁcients (MFCCs). Moreover, an increasing\namountof publicationsfocusedonhigh-levelfeaturesthat\nare supposed to better characterize musicological proper-\nties as described for instance in [14], [16], and [1]. Fur-\ntherrelevant publicationsregardingfeaturedesign are re f-\nerencedinSection6.1.\nWhile most research has been conducted using west-\nern popularmusic, only a few works were related to more\ndiverse global music content. A study on the applicabil-\nityofdifferentclassiﬁersforautomaticgenreclassiﬁcat ion\nof traditional Malaysian music was conducted in [7]. The\ngeneralissueofmulti-labelannotationshasbeenaddresse d\nonlyinafewpublicationssofar. In[13],theauthorsexper-\nimented with SVM-based “binary relevance” multi-label\ngenreclassiﬁcation in conjunctionwith MARSYAS-based\nfeatures[21]. Thisapproachwas continuedin [23],where\ntheauthorsmodiﬁedak-NearestNeighborsclassiﬁerinor-\nder to handle multi-label data directly. In [20], automatic\nmood estimation was modeled as a multi-label classiﬁca-\ntion task where every item may belong to more than one\nclass. Tothecurrentknowledgeoftheauthors,nopublica-\ntion so far discussed an approach similar to multi-domain\nlabeling, that will be explained in detail in the following\nsection.\n3. MULTI-DOMAIN-LABELING\nAsexplainedinSection1,whiledealingwithmusicalcon-\ntent from various regional music genres (often referred toas“Worldmusic”),theproblemfrequentlyarisesthatsongs\ncannot solely be labeled with one single genre label. In-\nstead, various rhythmic, melodic and harmonic inﬂuences\nconﬂate into multi-layered mixtures. Common classiﬁer\napproachesfailbecauseoftheirimmanentassumptionthat\nfor all song segments, one dominant genre exists and thus\nisretrievable.\nTo overcome these problems, we introduce a novel ap-\nproach called “multi-domain labeling”. We aim at break-\ning down multi-label annotations towards single-label an-\nnotationswithindifferentmusicaldomains,namely timbre,\nrhythm,andmelody/harmony thatarewell-knownaspects\nof perceivable music similarity. Furthermore, a separate\nannotation of each temporal segment of the overall song\nis enabled. This leads to a more meaningful and realis-\ntic two-dimensional description of multi-layered musical\ncontent. In addition, the approach facilitates a more pre-\ncisetrainingofaclassiﬁerbyavoidingfuzzymulti-labele d\ndatasamples.\n00.511.522.533.5\nMarrabentaGypsy BrassRockJazz\nKlesmerFunk\nChimurenga MbiraMbira\nSlow−Rock\nTraditional NubianMiddle EastReggaeGypsyHip HopBalkanForroTotal duration of multilabeled songs, in hours\nFigure1. Structureofthe database\n4. DATABASE& ANNOTATIONS\nThe music collection that we used for our investigations\nconsists of 430 full-length tracks from the 16 world mu-\nsic genres. For each genre, the database includes approx-\nimately two hours of music on average (see Fig. 1 for de-\ntails). Thismusicdatacollectionwasprovidedbythecon-\ntent partner of the research project GlobalMusic2One1.\nTheresearchprojectinvolveseducatedmusicologistswork -\ning with a world music label and being in regular contact\nwith musicians associated with the applied genres. Anno-\ntations were manually made by using an annotation soft-\nware allowing to label music genres in different domains\nwith regardto an arbitraryamount of time segments. This\nannotationsoftwareincludesautomaticsegmentationalgo -\nrithm, which makes the ﬁrst suggestion in order to speed\nuptheannotationprocess. Theexpertshadafullyfreedom\ntomodifybordersandassessmentsofthesegmentsineach\nofdomains.\nInthispaper,weappliedaﬂattaxonomywithallafore-\nmentioned genres considered to be situated at the same\nhierarchical level. Above all, for our experiments we se-\nlected tracks that have been annotated with multi-labels\n1http://www.globalmusic2one.net\n46010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nin at least one time segment. To evaluate our new anno-\ntation approach, the data set was annotated following the\nprinciplesofmulti-domain-labelingasdescribedinSec.3 .\nMusic experts were allowed to assign up to 4 different\ngenreconceptsforeachsegment-aglobalgenre,atimbre-\nrelated genre, a rhythm-related genre, and a genre related\nto the melodic and harmonic content. The three domain\nspeciﬁc annotations were not mandatory. If there were\nmultiple genre inﬂuences audible in a single segment, the\nexperts were only allowed to assign one genre label for\neach domain. This proceeding ensured single-label anno-\ntations within each segment and domain. One observation\nthat we made was that these domain-speciﬁc genre inﬂu-\nences seem to be stable for each segment. The resulting\nlabel cardinality (average number of labels per track) of\nmulti-labeledsongspergenrewasbetween1.1and2.0for\nthe selected genres, with 1.0 being a genre that has never\nbeen assigned in conjunction with another genre. The la-\nbel cardinality appeared to be different depending on the\nmusicgenres.\n5. THREEEVALUATION EXPERIMENTS\nToevaluatetheimprovementoftheclassiﬁerperformance,\nwe performthree differentexperimentsas depictedin Fig.\n2(a) - 2(c). Therefore, we are moving stepwise from the\nfuzzy case of multi-labeled songs towards single-labeled\nsegmentswithindifferentmusicaldomainsasdescribedin\ntheprevioussection.\nMulti-labeling(Exp.1)\nIn the ﬁrst experiment, all multi-labeled songs are gener-\nally used to train multiple classiﬁers, more precisely all\nclassiﬁerrelatedtotheannotatedgenres.\nMulti-labelingwith time segmentation(Exp.2)\nBearing the temporal structure of music in mind, we fur-\nthermore consider single segments in the second experi-\nment. Multi-labeledsegmentsare repeatedlyused as class\ninstancesaccordingto theirassignedgenrelabels.\nMulti-domain-labelingwithtime segmentation(Exp.3)\nInthethirdexperiment,weareusingtemporalsegmentsto\ntrain three different domain-related classiﬁers. Therefo re,\nwerestrictedourselvestofeaturesthatcanbesemanticall y\nassignedtowardstheparticularmusicaldomain,aswill be\ndetailedin6.1.\n6. SYSTEM WORK-FLOW\n6.1 Featureextraction\nFor the experiments conducted in this paper, we utilize\na broad palette of features commonly reported in the lit-\nerature (see Sec. 2). Besides low-level acoustic features,\nseveral mid-level representations [4] are extracted. Thes e\nmeasures are computed from excerpts of approximately 5\nseconds duration by deriving specialized descriptive mea-\nsures (including musical knowledge) from the observed\nM B I R A    ,    F U N K   ,   A R A B I C \n(a) Experiment 1: Multi-labeling\nM B I R A M B I R A , \nF U N K M B I R A , \nA R A B I C M B I R A , \nF U N K \n(b) Experiment 2: Multi-labeling with time segmentation\nM B I R A \nM B I R A F U N K A R A B I C \nA R A B I C F U N K M B I R A \nM B I R A M B I R A \nM B I R A M B I R A M B I R A \nM\nHT\nR\n(c) Experiment 3: Multi-domain labeling ( Timbre,Rhythm,\nMelody/Harmony)\nFigure2. Evaluationexperiments\nevolution of low-level features. Besides indifferent usag e\nof all features (in Exp. 1 and Exp. 2), groups of features\nare assigned to the aforementioneddomainsin the follow-\ningmanner.\nTimbre\nIn addition to common features, such as Mel-Frequency\nCepstral Coefﬁcients (MFCC), Audio Spectrum Centroid\n(ASC), Spectral Crest Factor (SCF) or Spectral Flatness\nMeasurement(SFM),modulationspectralfeatures[2]have\nprovedtobeextremelyusefultocaptureshorttermdynam-\nics of the low-level features. We applied a cepstral low-\npassﬁlteringto themodulationcoefﬁcientsto reducetheir\ndimensionalityanddecorrelatethemasdescribedin [6].\nRhythm\nAll rhythmicfeaturesused in thecurrentsetup are derived\nfromexcerptsofthedifferentbandsoftheAudioSpectrum\nEnvelope(ASE) feature. Part of the measures, such as the\nPercussiveness [22] and the Envelope Cross-Correlation,\nare based on the envelope signals. The other part is de-\nrived from the Auto Correlation Function (ACF) domain.\nBesidesthemeasuresdescribedin[6],thelog-lagACFand\nitsdescriptivestatistics areextractedaccordingto[10] .\nTonality\nTonality descriptors are computed from a Chromagram\nbasedonEnhancedPitchClassProﬁles(EPCP)[12],[19].\nThe EPCP undergoes a statistical tuning estimation and\ncorrection to account for tunings deviating from the equal\ntempered scale. Most important, the so-called symmetry\nmodel, a pitch-space representations as described in [9]\nare derived from the Chromagram as mid-level features.\nThe model provides an analytic description of aspects of\nmusical consonance and dissonance, as well as functional\nrelationshipsbetweenprobablenotes.\n461Poster Session 3\n6.2 Dimension Reduction\nMIRsystemsusuallyuseamultitudeoflow-levelandmid-\nlevel acoustic features. Each feature is designed to corre-\nlate with one of the aspects of perceptual similarity, e.g.\ntimbre, tempo, loudness or harmony. The distinct acous-\ntical features are joined together into so called acousti-\ncal feature vectors. While temporal changes in one fea-\ntureoftencorrespondtotemporalchangesintheotherfea-\nture(forinstance,timbreischangingalongwithloudness) ,\nthe individual dimensions of the feature vectors can often\nbe strongly correlated and cause information redundancy.\nThese raw feature vectors could cause various problems\non classiﬁcation stage. One of the usual ways to suppress\nredundant information in the feature matrix is to utilize\ndimension reduction techniques. Their purpose is to de-\ncrease the feature dimension Nwhile keeping or even re-\nvealing the most characteristic data properties. Generall y,\nall dimension reduction methods can be divided into su-\npervisedand unsupervisedones. Amongthe unsupervised\napproaches the one most often used is Principal Compo-\nnent Analysis (PCA). The key idea of PCA [8] is to ﬁnd a\nsubspacewhosebasisvectorscorrespondtothemaximum-\nvariance directions in the original feature space. Dimen-\nsion reductionis obtainedthenbysimply discardingthose\ncolumnvectorswith thesmallest eigenvalues.\n6.3 Classiﬁcation\nIn this section we shortly describe the applied classiﬁer\nand bring the architecture details regarding all three ex-\nperiments.\nGaussian MixtureModels\nGaussian Mixture Models (GMM) is a commonly used\ngenerative classiﬁer. Single data samples of the class are\nthought of as generated from various sources and each\nsource is modeled by a single multivariate Gaussian. The\nprobabilitydensityfunction(PDF)ofthefeatureframesis\nestimatedasaweightedsumofthemultivariatenormaldis-\ntributions. Each single i-th mixture is characterizedby its\nmean vector µiand covariance matrix Σi. Thus, a GMM\nis parametrized in Θ = {ωi, µi,Σi},i=1, M, where ωi\nis the weight of the i-th mixtures and/summationtext\niωi= 1. The\ngeneralization properties of the model can be adjusted by\nchoosingthenumberofGaussianmixtures M. Theparam-\netersoftheGMMcanbeestimatedusingtheExpectation-\nMaximizationalgorithm[5].\nClassiﬁer architectureforthreeexperiments\nOn the classiﬁcation stage for each data frame the likeli-\nhoods of all class models are calculated. We do not use\nprior distribution information. The classiﬁcation decisi on\nis therefore made using maximum likelihood rule. In a\ncase of Exp. 1 and Exp. 2 the same data samples may be-\nlong to multiple data classes. To tackle the problem, here\ntheclassiﬁcationtaskisreducedtoasetabinaryclassiﬁca -\ntion decisions, where every binary classiﬁer Hcis trained\nto make a binary decision (if the data sample belong to a\nclasscor not). These decisions of binary classiﬁers arejoined together to form the multi-label classiﬁcation. In a\ncase of Exp. 3 as described above only single labels are\nusedwithineachdomainandtimesegment. Thusforeach\ndomainwetrainoneGMMclassiﬁer. Ontheclassiﬁcation\nstage ﬁrstly each domain is classiﬁed and post-processed\n(see Sec. 6.4 for details) independently, and later the re-\nsultsforall domainsarejointtogether.\n6.4 Post-processing\nClassiﬁcationwithGMMresultsinclassdecisionforeach\nframe of the feature vector. Thus we apply the following\npost-processingproceduretoreduceframe-levelclassiﬁc a-\ntiontothefull-trackmulti-labels. ForExp.1andExp.2the\nprocedureis identical. Forall framesof the track foreach\nof the genres we sum up the number of frames associated\nto these genres. Then be build the normalized histogram\nof these data. The maximum of this histogram is pointing\nout the most probable genre for this track. As we are ex-\npectingmorethensinglelabelpertrack,probably,we also\nhavetoacceptthesecondmaximumofthenormalizedhis-\ntogram. Thisdecisionismadebya simple thresholdingof\nthe normalized histogram. The track is considered to be\nassociatedtothosegenres,wherethevaluesofthenormal-\nized histogram are above the threshold. As the histogram\nisnormalized,thethresholdissetto (0, . . .,1). Thechoice\nofthethresholdcruciallyinﬂuencestheperformanceofthe\nsystem. For instance, too low threshold causes high recall\nvalues,butmightleadtopoorprecision. Thusthethreshold\nvalueshavetobeoptimizedforeachoftheexperiments. In\na case of Exp.3 we ﬁrst performthe thresholdingforeach\nofdomainsindependently,andthenjointtheresults.\n6.5 EvaluationMeasures\nIn multi-label classiﬁcation each data sample (in our case\neach song or song segment) is associated with a set of la-\nbelsY⊆L, where Lis a full set of labels. Let Dbe a\nmulti-labeldataset,consistingof |D|multi-labelexamples\n(Xi, Yi),i= 1. . .|D|,Yi⊆L, where Xiis a featurema-\ntrix of the data example iandYiis a set of (ground-truth)\nlabelsassociatedtothedataexample i. Thelabelcardinal-\nity ofDis deﬁnedasfollows:\nLC(D) =1\n|D||D|/summationdisplay\ni=1|Yi|. (1)\nGiven the multi-label classiﬁer H, the estimated set of la-\nbelsforsample iisZi=H(Xi). The traditionalinforma-\ntion retrieval evaluation measures for multi-label case ar e\nwrittenas:\nPrecision (H, D) =1\n|D||D|/summationdisplay\ni=1|Yi∩Zi|\n|Zi|,(2)\nRecall (H, D) =1\n|D||D|/summationdisplay\ni=1|Yi∩Zi|\n|Yi|,(3)\nF−measure (H, D) =1\n|D||D|/summationdisplay\ni=12∗ |Yi∩Zi|\n|Yi|+|Zi|.(4)\n46210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n7. RESULTS\nFirstofallwedetailthesettingsofthesystem. Thefeature\nextraction procedure results in 233dimensions for timbre\nfeatures, 768dimensionforrhythmicfeatures,and 187di-\nmensions for tonal features. All in all it leads to 1188\ndimensions of the feature vector. It is well known that\nGMMsare sensitiveto thecurse ofdimensionality. As the\navailableannotateddatabaseisrelativelysmall,weappli ed\nPCAtoreducethedimensionalityoffeaturevectorswithin\neach of domains to 100dimensions. The PCA algorithm\nhasbeentrainedontherandomlychosentrainingset( 70%\nofthedatabase)andthenappliedtothetestset( 30%ofthe\ndatabase). This PCA-transformed data have been used in\nall 3 experiments. The GMMs have been trained with 1,\n5,20, and50mixtures, only diagonal covariancematrices\nhave been used. The threshold for the post-processing(as\ndescribed in Sec. 6.4) has been varied within a range of 0\nand1forExp. 1, Exp. 2, and for each of three domainsin\nExp. 3. Figure 3 depictsthe dependencyof the F-measure\non the thresholding for all above mentioned cases. It is\ninteresting to note, that for Exp. 1 and Exp. 2 achieved\nF-measure signiﬁcantly differs depending on the amount\nof mixtures in the GMM, while in all domains for Exp. 3\nthevaluesofF-measurebecomecomparable. Using 5mix-\ntures results into highest F-measure values for all experi-\nments. Theoptimalthresholdsvaluesarewithinarangeof\n0.15and0.25.\nWithinExp.3wefoundout,thattheoptimalthresholds\nfor each of domains separately do not form the optimal\ncombination of the thresholds leading to the best\nF-measure performance when the domains are joined to-\ngether. Thus,inacaseofGMM1(usingonlyonegaussian\nto model the class) the optimal thresholds in all domains\nare found within a range of 0.20and0.25, while in a case\nof GMM20 the optimal thresholds lies within a range of\n0.30and0.35. Figure4depictstheF-measureperformance\nfor all three experiments. The F-measure values for each\nnumber of mixtures in GMM are increased for Exp. 3 in\ncomparison to Exp. 1 and Exp. 2. The best performance\nis achived in Exp. 1 for GMM with 5 mixtures reaching\nthe F-measure of 0.61. The signiﬁcant performance raise\nof about 10%is observed for the case of using only one\ngaussian to model the class information. It can be explain\nwith a fact, that in a case of Exp. 3 the classes are less\noverlappedand easier to model then in a case of the set of\nbinaryclassiﬁers(asinExp.1 andExp.2).\nNote that for Exp. 3 the involvedGMMs include about\ntwo times less free parameters than in a case of Exp. 1\nand Exp. 2. As we used only diagonal covariance matri-\nces, the number of free parameters for each GMM can be\napproximated as m·(2d+ 1), where mis a number of\nmixtures and dis the dimensionality of the feature vector.\nThus for Exp. 3 the number of all free parameters com-\nprises 3·k·m(2d′+ 1), where kis a number of classes\nandd′is the features dimensionality within one domain;\nGMMsaretrainedwithineachofthreedomains. Whilstin\na case of Exp.1 and Exp.2 the amountof free parameters\nforthesetofbinaryclassiﬁersreaches 2k·m(2·3d′+ 1).0 0.2 0.4 0.6 0.8 100.51Experiment 1\n0 0.2 0.4 0.6 0.8 100.51Experiment 2\n0 0.2 0.4 0.6 0.8 100.51Experiment 3: timbre\n0 0.2 0.4 0.6 0.8 100.51Experiment 3: rhythm\n0 0.2 0.4 0.6 0.8 100.51Experiment 3: harmony\n  \nGMM1\nGMM5\nGMM20\nGMM50\nFigure 3. Dependency of F-measure on the thresholding\nwhile post-processingas describedin Sec. 6.4. ForExp. 1\nand Exp. 2 the F-measure performance strongly depends\nonthenumberifmixturesin GMM.\n8. CONCLUSIONS & FUTUREWORK\nThe paper presented a novel two dimensional approach\nto music genre classiﬁcation. It allows to decompose the\nmulti-labelclassiﬁcationproblemintomultiplesingle-c lass\nclassiﬁcation problemsbybreakingit downintwo dimen-\nsions. First results demonstrate high potential of the pro-\nposed approach. Future work will be directed towardsap-\nplying Support Vector Machines as alternative classiﬁca-\ntiontechnique,asithasbeenprovedtoperformbetterthan\nGMM for binary classiﬁcation. In a case of multi-domain\nclassiﬁcation we shall make use of supervised feature se-\nlection and feature space transformation methods, which\ncan not be utilized in a case of multi-label classiﬁcation.\nFurthermore, in the context of the research project Glob-\nalMusic2One , we are going to use vocalsandinstrumen-\ntationasadditionaldomains. We believethepresentedap-\nproachtobeextensibletoothermusicgenresastheseman-\ntic partitioning of music into different musical domains is\nuniversalformostoftheworld’sregionalmusicstyles.\n463Poster Session 3\nExp.1 Exp.2 Exp.300.20.40.6\n  \nGMM1\nGMM5\nGMM20\nGMM50\nFigure4. F-measuresforall threeexperiments\n9. ACKNOWLEDGMENTS\nThis work is part of the German research project Global-\nMusic2One funded by the German Ministry of Education\nandResearch(BMBF-FKZ: 01/S08039).\n10. REFERENCES\n[1] J. Abeßer, C. Dittmar, and H. Großmann. Automatic\ngenreand artist classiﬁcation byanalyzingimprovised\nsolopartsfrommusicalrecordings.In Proc.of theAu-\ndioMostlyConference(AMC) ,Pite˚ a,Sweden,2008.\n[2] L. Atlas and S. A. Shamma. Joint acoustic and modu-\nlation frequency. EURASIP Journal on Applied Signal\nProcessing ,2003:668–675,2003.\n[3] J.-J. Aucouturier and F. Pachet. Representing musical\ngenre: A state of the art. Journal of New Music Re-\nsearch,32:83–93,2003.\n[4] J.P.BelloandJ.Pickens.Arobustmid-levelrepresen-\ntation for harmonic content in music signals. In Proc.\nof Intl. Conf. on Music InformationRetrieval (ISMIR) ,\nLondon,UK,2005.\n[5] A.P.Dempster,N.M.Laird,andD.B.Rdin.Maximum\nlikelihood from incomplete data via the em algorithm.\nJournaloftheRoyalStatisticalSociety,SeriesB ,39:1–\n38,1977.\n[6] C. Dittmar, C. Bastuck, and M. Gruhne. Novel mid-\nlevel audio features for music similarity. In Proc.\nof the Intl. Conf. on Music Communication Science\n(ICOMCS) ,Sydney,Australia,2007.\n[7] S. Doraisamy, S. Golzari, N. M. Norowi, M. N. B.\nSulaiman, and N. I. Udzir. A study on feature selec-\ntion and classiﬁcation techniques for automatic genre\nclassiﬁcation of traditional Malay music. In Proc. of\nIntl. Conf. on Music Information Retrieval (ISMIR) ,\nPhiladelphia,Pennsylvania,USA,2008.\n[8] K.Fukunaga. IntroductiontoStatisticalPatternRecog-\nnition, Second Edition (Computer Science and Scien-\ntiﬁc Computing Series) . Academic Press, September\n1990.\n[9] G. Gatzsche, M. Mehnert, D. Gatzsche, and K. Bran-\ndenburg. A symmetry based approach for musical\ntonality analysis. In Proc. of Intl. Conf. on Music In-\nformationRetrieval(ISMIR) ,Vienna,Austria,2007.[10] M.GruhneandC.Dittmar.Improvingrhythmicpattern\nfeatures based on logarithmic preprocessing. In Proc.\nofthe126thAESConvention,Munich,Germany ,2009.\n[11] C.-H. Lee, J.-L. Shih, K.-M. Yu, and J.-M. Su. Au-\ntomatic music genre classiﬁcation using modulation\nspectral contrast feature. In Proc. of the IEEE Intl.\nConf.onMultimediaandExpo(ICME) ,2007.\n[12] K.Lee.Automaticchordrecognitionfromaudiousing\nenhanced pitch class proﬁle. In Proc. Intl. Computer\nMusicConf.(ICMC), NewOrleans,USA ,2006.\n[13] T.LiandM.Ogihara.Detectingemotioninmusic. Pro-\nceedingsof the Fifth InternationalSymposium on Mu-\nsic InformationRetrieval ,pages239–240,2003.\n[14] C. McKay and I. Fujinaga. Automatic genre classiﬁ-\ncation using large high-level musical feature sets. In\nProc.ofIntl.Conf.onMusicInformationRetrieval(IS-\nMIR),2004.\n[15] I. Panagakis, E. Benetos, and C. Kotropoulos. Music\ngenre classiﬁcation: A multilinear approach. In Proc.\nof Intl. Conf. on Music InformationRetrieval (ISMIR) ,\n2008.\n[16] C.P´ erez-Sancho,P.J.PoncedeLe´ on,andJ.M.I˜ nesta .\nA comparison of statistical approaches to symbolic\ngenrerecognition.In Proc.oftheIntl.ComputerMusic\nConf.(ICMC) ,pages545–550,2006.\n[17] N. Scaringella, G. Zoia, and D. Mlynek. Automatic\ngenre classiﬁcation of music content: a survey. IEEE\nSignalProcessingMagazine ,23:133–141,2006.\n[18] C. N. Silla, C. A. A. Kaestnerlso, and A. L. Koerich.\nAutomatic music genre classiﬁcation using ensemble\nof classiﬁers. In Proc. of IEEE Intl. Conf. on Sys-\ntems, Man, and Cybernetics , pages 1687–1692, Octo-\nber2007.\n[19] M. Stein, B. M. Schubert, M. Gruhne, G. Gatzsche,\nand M. Mehnert. Evaluation and comparison of au-\ndio chroma feature extractionmethods. In Proc. of the\n126thAESConvention,Munich,Germany ,2009.\n[20] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vla-\nhavas.Multilabelclassiﬁcationofmusicintoemotions.\nInProc. of Intl. Conf. on Music Information Retrieval\n(ISMIR),Philadelphia,Pennsylvania,USA, 2008.\n[21] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntionofaudiosignals. IEEETransactionsonSpeechand\nAudioProcessing ,10(5):293–302,2002.\n[22] C. Uhle,C. Dittmar,andT.Sporer.Extractionofdrum\ntracks from polyphonic music using independent sub-\nspace analysis. In Proc. of the 4th Intl. Symposium on\nIndependentComponentAnalysis ,2003.\n[23] A. Wieczorkowska, P. Synak, and Z. W. Ras. Multi-\nLabel Classiﬁcation of Emotions in Music . Springer\nBerlinHeidelberg,2006.\n464"
    },
    {
        "title": "Visualising Musical Structure Through Performance Gesture.",
        "author": [
            "Jennifer MacRitchie",
            "Bryony Buck",
            "Nicholas J. Bailey"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418205",
        "url": "https://doi.org/10.5281/zenodo.1418205",
        "ee": "https://zenodo.org/records/1418205/files/MacRitchieNB09.pdf",
        "abstract": "A musical performance is seen as the performer’s interpretation of a musical score, illuminating the interaction between the musical structure and implied emotive character [1]. It has been demonstrated that performers’ physical gestures correlate with structural and emotional aspects of the piece they are performing and that this information can be decoded by an audience when presented with a visualonly performance [2]. This paper investigates the relationship between direction of physical movement and underlying musical structures. The Vicon motion capture system is used to record 3D movements made by nine university-level pianists performing Chopin preludes op.28 Nos 6 and 7. The examination of several pianists provides insight into the similarity and differences in gestures between performers and how these relate to structure. Principal Component Analysis (PCA) of these performances and consequent analysis of variance reveals a relationship between extrema of the first six significant components and timing of phrasing structure in Prelude 7 where motion troughs consistently lag behind the occurence of phrase boundaries in the audio. This relationship is then examined for Prelude 6 which encompasses longer, expanded phrases and changes in rhythm. These expanded phrases are associated with elongated or split gestures, and variations of the motif with changes in movement.",
        "zenodo_id": 1418205,
        "dblp_key": "conf/ismir/MacRitchieNB09",
        "keywords": [
            "musical performance",
            "performers interpretation",
            "musical score",
            "interaction between structure",
            "implied emotive character",
            "physical gestures",
            "structural and emotional aspects",
            "audience decoding",
            "Vicon motion capture system",
            "nine university-level pianists"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nVISUALISING MUSICAL STRUCTURE THROUGH PERFORMANCE\nGESTURE\nJ.MacRitchie, B.Buck and N.J.Bailey\nCentre for Music Technology, University of Glasgow\n{j.macritchie, b.buck, n.j.bailey }@elec.gla.ac.uk\nABSTRACT\nA musical performance is seen as the performer’s inter-\npretation of a musical score, illuminating the interaction\nbetween the musical structure and implied emotive charac-\nter [1]. It has been demonstrated that performers’ physical\ngestures correlate with structural and emotional aspects of\nthe piece they are performing and that this information can\nbe decoded by an audience when presented with a visual-\nonly performance [2].\nThis paper investigates the relationship between direc-\ntion of physical movement and underlying musical struc-\ntures. The Vicon motion capture system is used to record\n3D movements made by nine university-level pianists per-\nforming Chopin preludes op.28 Nos 6 and 7. The examina-\ntion of several pianists provides insight into the similarity\nand di ﬀerences in gestures between performers and how\nthese relate to structure.\nPrincipal Component Analysis (PCA) of these perfor-\nmances and consequent analysis of variance reveals a rela-\ntionship between extrema of the ﬁrst six signiﬁcant compo-\nnents and timing of phrasing structure in Prelude 7 where\nmotion troughs consistently lag behind the occurence of\nphrase boundaries in the audio. This relationship is then\nexamined for Prelude 6 which encompasses longer, ex-\npanded phrases and changes in rhythm. These expanded\nphrases are associated with elongated or split gestures, and\nvariations of the motif with changes in movement.\n1. INTRODUCTION\nStructural communication in performance is well under-\nstood for parameters such as timing and dynamics, and\ncertain relationships between these parameters and struc-\ntures have been clariﬁed [3]. We now know that perform-\ners tend to slow the tempo towards the end of a phrase and\nuse dynamics to ‘shape’ a phrase often using a diminuendo\ntowards the end. These are of course context-dependent as\na performer can use the same parameters to mark di ﬀer-\nent structural features [4]. Performers also have personal\nstyles of playing and will not all use the same performance\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.parameter to emphasise the same structural feature.\nThis personal style becomes a much bigger issue when\ntreading into the ﬁeld of physical gestures. No such\nstraightforward relationships exist between physical ges-\ntures and musical structure, although it has been demon-\nstrated that performers’ gestures do contain information\nabout the music being performed [5, 6]. Perception stud-\nies have also shown that typical audience members can ac-\ncurately perceive information about tension and phrasing\nfrom visual-only performances [2].\nThis paper aims to establish relationships between body\nmovement and the phrasing structure it attempts to con-\nvey, exploring how these change between di ﬀerent pieces\nof music. This investigation of phrasing and gesture re-\nlationships will be conducted by recording piano perfor-\nmances through the Vicon motion capture system in syn-\nchrony with audio recordings, and subsequent analysis\nof the movement results alongside traditional analyses of\nstructure in the chosen pieces.\n2. CHOPIN PRELUDES\nSeveral factors fuelled the choice of two Chopin preludes\nfor gestural analysis.\n•In order to make general statements about phras-\ning structure, it was necessary to provide some\namount of scientiﬁc control, otherwise the exercise\ncould become tantamount to guesswork. Two pieces\nwere therefore chosen to examine the progression of\nmovement and structure relationships.\n•The pieces’ genre may have an e ﬀect on perfor-\nmance gestures, so ideally those from the roman-\ntic period would provide the most expressive perfor-\nmances.\n•The motion capture system coped better with shorter\nrecordings and so brief pieces were preferred.\nThe preludes chosen for this investigation were Pre-\nludes op.28 No.s 6 & 7. Prelude No. 7 in A major has\na strict, rigid structure, with the rhythmically identical two\nbar phrase occurring eight times in total. As can be seen\nfrom Figure 1, this binary form 16-bar piece has the main\nboundary occurring exactly halfway through at bar 8, and a\nharmonic arrival point occurring with the chord at the end\nof bar twelve [7]. The two sections of the piece are thought\nto each contain a set of antecedent-consequent phrases.\n237Poster Session 2\nFigure 1 . Chopin prelude op.28 No.7 with two-bar phrases\nmarked in red and line boundary for end of ﬁrst section.\nPrelude No.6 in B minor comprises three sections [8]\nfrom bars 1-8, bars 9-22 and a coda section from bars 23-\n26. The ﬁrst section represents an ‘extended idea’. As\nseen in Figure 2, Chopin begins with another two-bar motif\nin B minor. This motif is repeated with a slightly higher\npitch range in the next two bars. The ﬁrst part of the motif\nis repeated again and then expands into a four bar phrase\nending at bar 8, the ﬁrst sectional boundary. The second\nsection represents an expansion of this idea. At bar 9, The\ntwo-bar motif from the beginning is repeated with the next\nexpansion moving into C major. A new four bar phrase\nis introduced at bar 15, answered by the consequent four\nbar phrase concluding on the tonic at bar 22 at the second\nsectional boundary. The piece ends with a slight coda in B\nminor in its ﬁnal phrase1\nAs many di ﬀerent analyses of one piece can exist, each\npianist’s own interpretation of phrasing is noted within this\nexperiment. Analysis of each pianist’s performance of the\nrigidly structured prelude no.7 will provide an impression\nof performance style. This piece provides the opportunity\nto observe movements for each phrase in isolation before\nmoving on to examine the other prelude containing slightly\nmore complicated structures.\n3. METHOD\n3.1 Performance Motion Capture\nUsing the Vicon 3D-motion capture system [9], perfor-\nmances of the two selected Chopin preludes by nine highly\ntrained pianists2were recorded. All pianists were asked\n1This analysis of Chopin’s Prelude Op.28 No.6 is combined from Koﬁ\nAgawu, V . ’Concepts of Closure and Chopin’s opus 28’ in Music Theory\nSpectrum 9:1–17, 1987. and comments made by Jennifer MacRitchie,\nUniversity of Glasgow, and David Lewis and Christophe Rhodes, Gold-\nsmiths, University of London\n2These nine performers consisted of ﬁve music performance under-\ngraduate students, four at the University of Glasgow and one at the Uni-\nversity of Edinburgh, two postgraduate students from the Royal Scottish\nAcademy of Music and Drama and two amateur pianists with more than\nten years of performance experience. Each pianist was paid a one-o ﬀsum\nof £25 for their participation in the experiment.\nFigure 2 . Chopin prelude op.28 No.6 with phrases marked\nin red and line boundaries for the ends of sections.\nto play the pieces from memory in an e ﬀort to ensure an\nin-depth knowledge of both pieces. The only performance\ndirection given to the pianists was to play as if they were in\na normal concert setting. Pianists’ interpretation of phras-\ning structure and gestural expression were taken by means\nof a self-report following the recording.\nThe Vicon motion capture system consists of twelve\ninfra-red cameras placed around the room to ensure cap-\nture of a particular volume of space. Retro-reﬂective mark-\ners were placed onto a velcro jacket and hat worn by the\nperformers in the conﬁguration shown in the head and up-\nper body model in Figure 3. This particular model com-\nbined the upper body model from Cutti et al. [10] with\nfour reference markers for the head positions. Each cam-\nera tracks the coordinates of the 28 markers and trian-\ngulates their position in order to build a 3-D model of\neach performer. Each video was recorded at 120 frames\nper second in synchrony with an analogue input for the\nrecorded sound. The models were then reconstructed by\npost-processing and any points where the cameras had\nfailed to pick up a certain marker were ﬁlled with the esti-\nmation models available from the Vicon Nexus software.\nProblems were encountered particularly with the mark-\ners placed on the elbows of the performers. As the markers\nwere placed not directly onto the skin but onto a velcro\njacket, there were several points in the recordings where\nthe marker was lost by the camera as the jacket had moved\n23810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 3 . Vicon marker model for upper body of pianists.\nround the elbow and displaced the marker. Although the\nVicon interpolation algorithms ﬁlled most of these elbow\ngaps, the system is proprietary, so these algorithms are un-\navailable for inspection. The accuracy of reconstruction\nfor these elbow gaps must therefore be considered suspect.\n4. RESULTS\n4.1 Motion Capture Analysis\nAs motion capture always produces such an overwhelm-\ning plethora of data, the traditional phrase analysis of each\nprelude provides us with points from which to start inves-\ntigation of gestural cues at phrasing boundaries. Each per-\nformer’s audio recording was annotated in Audacity [11]\nwith the timings of the phrase boundaries explained in sec-\ntion 2. by a separate professional pianist. Each performer’s\nown view of the phrase segmentation was also noted in\ncase of any di ﬀerences to traditional analysis. The pi-\nanists’ self-reports also conveyed a wide view on the role\nof movement in performance, with some branding move-\nments extra to sound productive ones as completely un-\nnecessary and something they tried to limit, whilst oth-\ners felt it vital to move in order to ‘feel’ the music they\nwere performing. Although physical gestures in perfor-\nmance can be classiﬁed as movements necessary to the\nactual sound-production, or movements that are related to\nthe music but not necessary for the actual sound (i.e. an-\ncillary) [12], it is acknowledged that gestures may still be\nmulti-functional. To view the overall general motion char-\nacteristics of each performer, principal component analysis\n(PCA) was performed through designated pca modules us-\ning singular value decomposition algorithms [13] on the\ncomplete set of motion data for each pianist. Each per-\nson’s principal component score was mapped against the\ntimings of each phrasing boundary to determine if there\nwas a pattern of movement for each phrase. Reduced-\ndimension curves such as these are good at expressing a\ngeneral overview but tend to lose some semantics of the\nactual movement being performed and so each individual\nmarker is then also examined for reference to phrases, mea-\nsures and beats.\nThree pianists have been chosen to demonstrate the\nspread of results concisely. These pianists were chosen\naccording to their ability, their standard deviation and vari-\nFigure 4 . First two principal components of movement for\nprelude 7, Performer 1 mapped against phrase boundaries.\nance of movement calculated for intra-performance data\nand also their views on movement during a performance.\nPerformer 1 is a highly trained amateur pianist and had a\nsmall standard deviation of movement. Performer 2 is a\nconservatory trained postgraduate student and had a large\nstandard deviation of movement, and Performer 3 is a mu-\nsic undergraduate student and had a mid-range standard\ndeviation. Normalization of results allows the movements\nto be correlated with phrase structure independent of dif-\nferences in amplitude. The arrows in each graph indicate\nthe point in time where the last note of each phrase ends in\nthe audio stream.\n4.2 Prelude 7 in A major\nStarting with prelude 7, in which the pianists self-reporting\nanalysis agreed with the traditional phrase segmentation\nmarked in section 2., Figures 4, 5 and 6 show the ﬁrst\ntwo principal components accounting for around 70% of\nthe overall movement. These appear to relate to the phrase\nboundaries as dictated by traditional music analysis. For\neach performer, the correlation between markers and the\nresultant PCA curves i.e. the loadings, clariﬁed that in-\nstead of a few markers being prevalent in causing the most\nvariance in motion, the PCA curves were a result of the\nvariances in a combination of several markers and these\ndiﬀered slightly for each pianist.\nInterestingly, Performer 1’s self-report on conclusion\nof the recordings expressed the opinion that movement in\nperformance did not convey any information on phrasing\nand that during performances, he /she attempted to mini-\nmize movements and facial expressions. However, Per-\nformer 1’s movement, shown in Figure 4, shows a clear\nrelationship between physical gestures and phrasing struc-\nture where each phrase boundary precedes a trough in the\nmotion graph. The loadings for Performer 1 related highly\nto movements in the upper arms, the elbows, the wrists and\nthe chest.\nPerformer 2’s main component loadings consist of\nmovement in every section of markers: the wrists, elbows,\nupper arms, shoulders, chest and the head. A pattern can\n239Poster Session 2\nFigure 5 . First two principal components of movement for\nprelude 7, Performer 2 mapped against phrase boundaries.\nbe seen in Figure 5 for the ﬁrst principal component(red),\nwhere the phrase boundary occurs at a motion peak for all\nbut phrase endings 1, 5 and 6 whose peaks precede the\nboundary in time. The second component (green) shows a\nrelationship between the phrase endings and peaks in the\ngraph.\nFigure 6 . First two principal components of movement for\nprelude 7, Performer 3 mapped against phrase boundaries.\nPerformer 3’s main component loadings relate to move-\nments in the wrists, upper arms, chest and head. A pattern\ncan be seen in Figure 6 where each phrase boundary pre-\ncedes a trough in the main component in all phrases. The\npattern of movement for each phrase in this component is\nchanged slightly during phrase six. This could be to em-\nphasise the harmonic arrival in bar 12 of the piece.\nThe addition of the weighted values of the ﬁrst six prin-\ncipal component scores for each performer produces a vi-\nsualisation of data accounting for more than 90% of the\nvariance in motion, the weightings calculated from the\npercentage variance of each component over the dataset.\nThese have been resampled with 10,000 points and time-\nwarped to allow more direct comparison between per-\nformers. The distance between each audio phrase bound-\nary is 0.1 and quoted means and standard deviations arecalculated for the distances between the troughs of the\nmotion trajectory and its corresponding phrase bound-\nary. Figure 8 shows a pattern in all phrases except one,\nﬁve and six (mean =0.0369,s.d =0.015). Phrases 1 and\n5 are the ﬁrst phrases in the each section of prelude 7\nwhilst phrase 6 contains the harmonic arrival point. Fig-\nures 7 and 9 show a clear relationship between the move-\nment and phrase boundaries.(mean =0.0186,s.d =0.0116\nand mean =0.0204,s.d =0.0068 respectively) The calibra-\ntion of this hypothesis with prelude 7 now provides us with\na useful tool to observe the structure of prelude 6 for the\nsame performers.\nFigure 7 . Combination of ﬁrst six components for prelude\n7, Performer 1 mapped against phrase boundaries.\nFigure 8 . Combination of ﬁrst six principal components\nof movement for prelude 7, Performer 2 mapped against\nphrase boundaries.\n4.3 Prelude 6 in B minor\nThe initial two-bar motif in prelude 6 is in the left hand\nmelody marked in the score seen in section 2. This motif\nis varied in the subsequent phrases, ﬁrst in pitch for the\nsecond phrase, then also in rhythm for the third phrase\nending at bar 8. This is echoed in the movements made\nby performers. This initial step looks at the ﬁrst three\n24010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 9 . Combination of ﬁrst six principal components\nof movement for prelude 7, Performer 3 mapped against\nphrase boundaries.\nphrases of prelude 6 as agreement on phrase segmenta-\ntion between analyses and performers’ self-reports diverge\nfrom this point onwards. The means and standard devia-\ntions of distance between motion trough and phrase bound-\nary are for the ﬁrst three phrases only.\nFigure 10 . Combination of ﬁrst six principal components\nof movement for prelude 6, Performer 1 mapped against\nphrase boundaries.\nPerformer 1’s weighted combination of principal com-\nponents of movement, shown in Figure 10, shows a dis-\ntinct pattern of movement for the two-bar motif established\nin phrase one of the piece (mean =0.0223,s.d =0.0110). Its\nelongation in phrase 3 is mimicked by an elongated ges-\ntural movement. The change in motif at bar 15 beginning\nwith a four bar phrase (phrase 5), is marked with a di ﬀer-\nent pattern of movement. This movement is repeated as the\nconsequent four bar phrase is played.\nThe ﬁrst six principal components for Performer 2, as\nseen in Figure 11, also shows a clear pattern within the\nﬁrst three phrases (mean =0.0129,s.d =0.0115 this particu-\nlar mean is negative as each trough occurs slightly before\nthe phrase boundary). Interestingly, in phrase 3 where the\noriginal two-bar motif is expanded, we clearly see two sep-\nFigure 11 . Combination of ﬁrst six principal components\nof movement for prelude 6, Performer 2 mapped against\nphrase boundaries.\narate movements. As the length of the phrase being per-\nformed is just under 12 seconds long, we refer to the theory\nof gestures being separated into gesture-units i.e. action-\nchunking [14]. At which points within a long phrase this\naction-chunking occurs is most likely related to the smaller\nrhythmical groupings within the particular phrase.\nFigure 12 . Combination of ﬁrst six principal components\nof movement for prelude 6, Performer 3 mapped against\nphrase boundaries.\nThe ﬁrst principal component for Performer 3 as seen\nin Figure 12 again shows this pattern between movement\nand phrasing (mean =-0.0069,s.d =0.0086). Again at phrase\n3, we can see the beginning of the two-bar motif related\nmovement before the phrase is expanded and the resulting\ngesture elongated as well.\n4.4 Analysis Across All Performers\nOne-way ANOV As were conducted to investigate the ef-\nfects of performer style and phrasing motion. No sig-\nniﬁcant e ﬀect of performer was found which, when cou-\npled with variance analyses of individual performers, re-\nvealed that performers were consistent in their movement\n241Poster Session 2\npatterns throughout the piece, showing no signiﬁcant dif-\nference between performers in their overall motion timing\nvariance. A signiﬁcant e ﬀect of trough location was found\nbetween performers (F =17.32, p<0.001). When consid-\nered with the minimal variance within performances, these\nresults show that performers were consistent in the location\nof their coherant motion patterns (troughs) with respect to\nphrasing boundaries. As performers’ interpretations of the\nlatter part of prelude 6 di ﬀer, such straightforward compar-\nisons cannot be performed for this piece.\nComparisons of pianists’ singular marker movements\nfor both preludes showed similarities in movement in the\ny axis (along the length of the keyboard) of every single\nmarker of the upper body. Troughs in each plot occurred\neither slightly before or slightly after the audio phrase\nboundary. This changed between pianists but was con-\nsistent between performances of two preludes with di ﬀer-\ning structure and di ﬀerent melodies. Di ﬀerences in pi-\nanists’ marker movements showed for some pianists, a\nclear relationship with phrasing in all three axes of move-\nment for each marker. Some used their heads to mark\nout phrasing whereas others preferred to use their upper\nbody. These movements were performer speciﬁc and oc-\ncurred across both pieces. Within some markers denoting\nphrasing, movements corresponding to the measures and\nbeats within the piece were found. Markers which were not\nphrasing speciﬁc also appeared to highlight beats or mea-\nsures of the piece. Despite this, the overall general move-\nment reﬂected by the PCA shows a clear phrasing pattern.\n5. CONCLUSIONS\nStructural information appears to be inherent in pianists’\ndirectional movements across the three axes. Princi-\npal component analysis conﬁrms the relationship between\ngeneral movement in the upper body and head with com-\nposed structure. Variance analysis shows that each per-\nformer’s general movement consistently lags behind the\noccurence of a phrase boundary in the audio stream. By ex-\namining pianists’ movements in performances of Chopin’s\nprelude op.28 no.7, it is conﬁrmed that short phrases in iso-\nlation, with the same rhythmical pattern appear to invoke\nsimilar movements by the performers. Movement also ap-\npears to change when the motif is varied, as examined in\nperformances of Chopin’s prelude op.28 no.6. In compar-\nison with the short two-bar motif, phrases with a longer\nduration have di ﬀerent elongated gestures and are some-\ntimes split into sections in a process referred to as action-\nchunking.\nThis investigation provides the initial step of relating\nmovements to phrases. Further investigations into e ﬀect\nof genre of the music being performed on the structural\nrelationship with movement are required to make more\ngeneral statements across a wider variety of music. Fur-\nther steps for this research are to clarify whether physi-\ncal gestures are related to other compositional or perfor-\nmance attributes. Empirically relating general movement\nto structural aspects of performed music contributes to the\nargument that ancillary performer movements may havea music-related function. This has implications for piano\npedagogy and furthers the understanding of the wider rela-\ntionship between music and movement.\n6. ACKNOWLEDGEMENTS\nThe authors acknowledge Prof. Frank Pollick and researcher\nLukasz Piwek of the Psychology Dept at the University of Glas-\ngow for their support and access to equipment. This research is\nEPSRC funded.\n7. REFERENCES\n[1] Sha ﬀer, L. Musical Performance as Interpretation Psy-\nchology of Music ,23, 17–28, 1995.\n[2] Vines,B. et al. Cross-modal Interactions in the Percep-\ntion of Musical Performance. Cognition , 101, 80–113,\n2006.\n[3] Friberg, A. and Battel, G.U. Structural Communication\nin Parncutt, R. and McPherson, G.E. (Eds.) The Sci-\nence and Psychology of Music Performance , Oxford\nUniversity Press, 2002, 199–218\n[4] Clarke, E.F. The Semiotics of Expression in Musical\nPerformance Contemporary Music Review , 1998, 17,\nPart 2, 87–102\n[5] Wanderley, M.M. Quantitative Analysis of Non-\nObvious Performer Gestures Gesture and Sign Lan-\nguage in Human-Computer Interaction: International\nGesture Workshop , 2001.\n[6] Jane W. Davidson. Qualitative insights into the use\nof expressive body movement in solo piano perfor-\nmance: a case study approach. Psychology of Music ,\n35(3):381–401, 2007.\n[7] Kresky, J. A Reader’s Guide to the Chopin Preludes\nGreenwood Press, 1994.\n[8] Koﬁ Agawu, V . Concepts of Closure and Chopin’s\nopus 28 Music Theory Spectrum , 9:1–17, 1987.\n[9] Motion Capture Systems from Vicon http://www.\nvicon.com .\n[10] Cutti,A.G. et al. Soft tissue artefact assessment in\nhumeral axial rotation Gait and Posture , 2005, 21, 341-\n349.\n[11] Ardour - the new digital audio workstation. http://\nwww.ardour.org .\n[12] Cadoz, C. and Wanderley, M. Gesture-Music in Wan-\nderley, M. and Battier, M. (ed.) Trends in Gestural\nControl of Music Iracam - Centre Pompidou, 2000\n[13] Risvik, H. PCA Python module http://folk.uio.\nno/henninri/pca_module\n[14] Godøy, R. I. Knowledge in Music Theory by Shapes of\nMusical Objects and Sound-Producing Actions. In M.\nLeman (ed.) Music, Gestalt, and Computing , Springer\nVerlag, Berlin (1997) pp. 106–110\n242"
    },
    {
        "title": "Steerable Playlist Generation by Learning Song Similarity from Radio Station Playlists.",
        "author": [
            "François Maillet",
            "Douglas Eck",
            "Guillaume Desjardins",
            "Paul Lamere"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416280",
        "url": "https://doi.org/10.5281/zenodo.1416280",
        "ee": "https://zenodo.org/records/1416280/files/MailletEDL09.pdf",
        "abstract": "This paper presents an approach to generating steerable playlists. We first demonstrate a method for learning song transition probabilities from audio features extracted from songs played in professional radio station playlists. We then show that by using this learnt similarity function as a prior, we are able to generate steerable playlists by choosing the next song to play not simply based on that prior, but on a tag cloud that the user is able to manipulate to express the high-level characteristics of the music he wishes to listen to.",
        "zenodo_id": 1416280,
        "dblp_key": "conf/ismir/MailletEDL09",
        "keywords": [
            "song transition probabilities",
            "audio features",
            "radio station playlists",
            "steerable playlists",
            "tag cloud",
            "user manipulation",
            "music characteristics",
            "prior",
            "high-level characteristics",
            "high-level characteristics"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSTEERABLE PLAYLIST GENERATION BY LEARNING SONG\nSIMILARITY FROM RADIO STATION PLAYLISTS\nFranc ¸ois Maillet, Douglas Eck, Guillaume Desjardins\nDIRO, Universit ´e de Montr ´eal, CIRMMT\nMontreal, Canada\n{mailletf,eckdoug,desjagui }@iro.umontreal.caPaul Lamere\nThe Echo Nest\nSomerville, USA\npaul.lamere@gmail.com\nABSTRACT\nThis paper presents an approach to generating steerable\nplaylists. We ﬁrst demonstrate a method for learning song\ntransition probabilities from audio features extracted from\nsongs played in professional radio station playlists. We\nthen show that by using this learnt similarity function as a\nprior, we are able to generate steerable playlists by choos-\ning the next song to play not simply based on that prior,\nbut on a tag cloud that the user is able to manipulate to ex-\npress the high-level characteristics of the music he wishes\nto listen to.\n1. INTRODUCTION\nThe celestial jukebox is becoming a reality. Not only are\npersonal music collections growing rapidly, but online mu-\nsic streaming services like Spotify1or Last.fm2are get-\nting closer everyday to making all the music that has ever\nbeen recorded instantly available. Furthermore, new play-\nback devices are revolutionizing the way people listen to\nmusic. For example, with its Internet connectivity, Apple’s\niPhone gives listeners access to a virtually unlimited num-\nber of tracks as long as they are in range of a cellular tower.\nIn this context, a combination of personalized recommen-\ndation technology and automatic playlist generation will\nvery likely form a key component of the end user’s listen-\ning experience.\nThis work’s focus is on providing a way to generate\nsteerable playlists, that is, to give the user high-level con-\ntrol over the music that is played while automatically choos-\ning the tracks and presenting them in a coherent way. To\naddress this challenge, we use playlists from professional\nradio stations to learn a new similarity space based on song-\nlevel audio features. This yields a similarity function that\ntakes audio ﬁles as input and outputs the probability of\nthose audio ﬁles being played successively in a playlist.\nBy using radio station playlists, we have the advantage of\n1http://www.spotify.com\n2http://www.last.fm\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.having a virtually unlimited amount of training data. At\nthe same time, we are able to generalize the application of\nthe model to any song for which we have the audio ﬁles.\nWe believe this will be the case in any real-life application\nwe can foresee for the model.\nFurthermore, we use the concept of a steerable tag cloud\n[2] to let the user guide the playlist generation process.\nTags [6], a type of meta-data, are descriptive words and\nphrases applied to any type of item; in our case, music\ntracks. Tags are words like like chill,violin ordream pop .\nThey have been popularized by Web 2.0 websites like Last.fm,\nwhere users can apply them to artists, albums and tracks.\nThe strength of tags, especially when used in a social con-\ntext, lies in their ability to express abstract concepts. Tags\ncommunicate high-level ideas that listeners naturally use\nwhen describing music. We tag all tracks in our playlists\nusing an automatic tagging system [7] in order to ensure\nthat they are all adequately tagged. Then, given a seed\nsong, the learnt similarity model is used to preselect the\nmost probable songs to play next, after which the similar-\nity between the user’s steerable tag cloud and each of the\ncandidate songs’ cloud is used to make the ﬁnal choice.\nThis allows users to steer the playlist generator to the type\nof music they want to hear.\nThe remainder of this paper is organized as follows.\nSection 2 gives a brief overview of related work in music\nsimilarity and playlist generation. Section 3 explains how\nthe radio station playlists data set was collected and assem-\nbled. Section 4 presents the creation and evaluation of our\nnew similarity space. Section 5 explains how we propose\nimplementing a steerable playlist generator. Finally, sec-\ntion 6 explores future research avenues.\n2. PREVIOUS WORK\nAn increasing amount of work is being conducted on au-\ntomatic playlist generation, with considerable focus being\nplaced on the creation of playlists by means of acoustic or\nmeta-data similarity [10–14].\nMore recently, connectivity graphs derived from mu-\nsic social networks are being used to measure similarity.\nFor example, [5] uses network ﬂow analysis to generate\nplaylists from a friendship graph for MySpace3artists.\nIn [4], the authors use Last.fm collaborative ﬁltering data\n3http://www.myspace.com\n345Oral Session 4: Music Recommendation and Playlist Generation\nto create a similarity graph considering songs to be sim-\nilar if they have been listened to by similar users. They\nthen embed the graph into a Euclidean space using LMDS,\nwhere similar artists would appear near one another.\nAnother approach [3] uses a case-based reasoning sys-\ntem. From its pool of real human-compiled playlists, the\nsystem selects the most relevant ones in regards to the user’s\none song query and mixes them together, creating a new\nplaylist.\nWe are aware of only one other attempt to use radio\nstation playlists as a source of data. In [1] radio station\nplaylists are used to construct a weighted graph where each\nnode represents a song and each arc’s weight is the num-\nber of times the two songs are observed one after the other.\nFrom the graph, the authors are able to infer transition\nprobabilities between songs by creating a Markov random\nﬁeld. Our approach is similar, with the advantage that we\ncan generalize to songs not observed in the training data.\n3. CONSTRUCTING THE DATA SET\nOur model is trained on professional radio station playlists.\nFor this experiment, we consider a playlist to be a sequence\nof 2 or more consecutive plays uninterrupted by a commer-\ncial break. Suppose a radio station plays the tracks ta,tb\nandtcone after the other, we will consider {ta,tb}and\n{tb,tc}as two 2-song sequences ∈S2, and{ta,tb,tc}as\none 3-song sequence ∈S3. We consider the sequences\n{ta,tb}and{tb,ta}as two distinct sequences. The model’s\noutput will thus be non-symmetric in regards to the order\nin which the songs are presented.\nThe playlist data we used came from two sources which\nwe will cover in section 3.1.\n3.1 Playlist sources\n3.1.1 Radio Paradise\nRadio Paradise4(RP) is a free Internet-streamed radio sta-\ntion that deﬁnes its format as “eclectic online radio.” RP\nprovided us with playlist data including every play from\nJanuary 1st 2007 to July 28th 2008 (575 days). The data\nconsists of 195,692 plays, 6,328 unique songs and 1,972\nunique artists.\n3.1.2 Yes.com\nYes.com is a music community web site that provides, among\nother things, the playlists for thousands of radio stations\nin the United States. Developers are able to access the\nplaylist data via a free web based API5that returns the\ndata in JSON format. One API call allows the developer to\nget a list of radio stations, either by searching by genre, by\nname or even by proximity to a given ZIP code. Then, for\neach retrieved station, the API provides access to that sta-\ntion’s play history for the last 7 days. The self-assigned and\nnon-exclusive genres of the available radio stations cover\nall major musical styles. The stations we used to build\nour own dataset were not chosen for any particular reason.\n4http://www.radioparadise.com\n5http://api.yes.comRather, we made a few searches with the API by genre until\nwe obtained enough data for our work, that is 449 stations.\nThe proportion of stations’ non-exclusive association with\nthe different genres is detailed in Table 1.\nTable 1 . Proportion of the 449 Yes radio stations associ-\nated with each genre. Because the genres are non-exclusive\nthe sum of the percentages is >100.\nLatin 11.2% Christian 11.4%\nCountry 20.6% Hip-Hop 17.2%\nJazz 4.3% Metal 14.1%\nPop 23.3% Punk 1.6%\nRock 39.4% R&B/Soul 13.6%\nWe used data mined from the Yes API from Novem-\nber 13th 2008 to January 9th 2009 (57 days), totaling 449\nstations, 6,706,830 plays, 42,027 unique songs and 9,990\nunique artists.\nUnlike RP, Yes did not provide any indication of where\nthe commercial breaks were located in the list of plays.\nWe inferred where they were by looking at the interval\nbetween the start time of every pair of consecutive songs.\nAs we will explain in section 4.1, we only used sequences\nmade of tracks for which we had the audio ﬁles. This al-\nlowed us to calculate song length and to infer when com-\nmercials were inserted. Speciﬁcally, if the second of the\ntwo songs did not start within ±20seconds of the end of\nthe ﬁrst one, we assumed that a commercial break had been\ninserted and thus treated the two songs as non-sequential.\nThis approach is more precise than the method used in [1],\nwhere breaks were inserted if the elapsed time between two\nsongs was greater than 5 minutes.\n3.2 Putting the data together\nCombining all the data yielded 6,902,522 plays, with an\naverage of 15,338 plays per station. As we will explain in\n4.1, the features we used as input to our model required\nus to have access to each of the songs’ audio ﬁle. Of the\n47,044 total songs played in the playlists we used, we were\nable to obtain the audio ﬁles for 7,127 tracks. This reduced\nthe number of distinct usable song sequences to 180,232\nand 84,668 for the 2 and 3-song sequence case respectively.\nThe sequences for which we had all the audio ﬁles were\ncombinations from 5,562 tracks.\nFinally, we did not possess a set of explicit negative ex-\namples (i.e. two-song sequences that a radio station would\nnever play). In order to perform classiﬁcation we needed\nexamples from both the positive and negative class. To ad-\ndress this, we considered any song sequence that was never\nobserved in the playlist as being a negative example. Dur-\ning training, at each new epoch, we randomly sampled a\nnew set of negative examples matched in size to our pos-\nitive example set. With this strategy it is possible that we\ngenerated false-negative training examples (i.e. two-song\nsequences that we didn’t see as positive examples in our\ndata set but that in fact a radio station would play). How-\n34610th International Society for Music Information Retrieval Conference (ISMIR 2009)\never, since we resample new negative examples after every\ntraining epoch, we do not repeatedly show the model the\nsame false-negative pair, thus minimizing potential impact\non model performance.\n4. SONG SIMILARITY MODEL\n4.1 Features\nWe use audio-based features as input to our model. First,\nwe compute 176 frame-level autocorrelation coefﬁcients\nfor lags spanning from 250ms to 2000ms at 10ms intervals.\nThese are aggregated by simply taking their mean. We then\ndown sample the values by a factor of two, yielding 88 val-\nues. We then take the ﬁrst 12 Mel-frequency cepstral co-\nefﬁcients (MFCC), calculated over short windows of audio\n(100ms with 25ms overlaps), and model them with a sin-\ngle Gaussian (G1) with full covariance [16]. We unwrap\nthe values into a vector, which yields 78 values.\nWe then compute two song-level features, danceability\n[9] and long-term loudness level (LLML) [8]. Danceabil-\nity is a variation of detrended ﬂuctuation analysis, which\nindicates if a strong and steady beat is present in the track,\nwhile the LLML gives an indication of the perceived loud-\nness of the track. Both of these features yield a single nu-\nmeric value per song.\nThese 4 audio features are concatenated to form an 180\ndimensional vector for each track.\n4.2 Learning models\nWe formulate our learning task as training a binary clas-\nsiﬁer to determine, given the features for a sequence of\ntracks, if they form a song sequence that has been observed\nin our radio station playlists. If a sequence has been ob-\nserved at least once, it is considered a positive example.\nAs mentioned above, the negative examples are randomly\nsampled from the pool of all unseen sequences.\nWe use three types of learning models in our experi-\nments: logistic regression classiﬁers, multi-layer percep-\ntrons (MLP) and stacked denoising auto-encoders (SdA).\nLogistic regression, the simplest model, predicts the prob-\nability of a song-sequence occurrence as a function of the\ndistance to a linear classiﬁcation boundary. The second\nmodel, a multi-layer perceptron, can also be interpreted\nprobabilistically. It adds an extra “hidden” layer of non-\nlinearity, allowing the classiﬁer to learn a compact, nonlin-\near set of basis functions.\nWe also use a type of deep neural network called a stacked\ndenoising auto-encoder (SdA) [17]. The SdA learns a hi-\nerarchical representation of the input data by successively\ninitializing each of its layers according to an unsupervised\ncriterion to form more complex and abstract features. The\ngoal of this per-layer unsupervised learning is to extract\nan intermediate representation which preserves informa-\ntion content whilst being invariant to certain transforma-\ntions in the input. SdAs are exactly like neural networks\nwith the exception that they have multiple hidden layers\nthat are initialized with unsupervised training.In our experiments, the models operated directly on pairs\n(or 3-tuples in the case of predicting sequences of length 3)\nof audio features. The input xof our model is thus a vector\nof length 180·n, withn∈{2,3}, formed by concatenating\nthe features of each track into a single vector.\nWe used 75% of our unique pairs/triplets for training,\nkeeping 12.5% for validating the hyper-parameters and 12.5%\nfor testing. We did not perform any cross-validation.\n4.3 Similarity evaluation\nMeasuring the quality of the similarity space induced by\nthe model is not easy and highly subjective. We will ﬁrst\nlook at its performance on the learning task (4.3.1), and\nthen try to evaluate it in a more qualitative way (4.3.2).\n4.3.1 Learning performance\nClassiﬁcation errors for the different models we trained are\npresented in Table 2. The errors represent the proportion\nof real sequences that were classiﬁed as false sequences\nby each model, or vice versa, on the test set, for the best\ncombination of hyper-parameters.\nWhile the logistic regression clearly lacks learning ca-\npacity to adequately model the data, the MLPs and SdAs\nhave similar performance. SdAs have been shown to out-\nperform MLPs in complex image classiﬁcation tasks ( [17])\nbut were unable to learn a signiﬁcantly better representa-\ntion of the features we are using for this task. This could\nmean that the feature set was not sufﬁciently rich or that\nthe task was simply too difﬁcult for the hierarchical model\nto ﬁnd any kind of compositional solution to the problem.\nTable 2 . Classiﬁcation errors on the test set for the differ-\nent models we trained as well as a random baseline. SdA- n\nrepresents an SdA with nhidden layers.\nModel 2-song seq. 3-song seq.\nrandom 50.00% 50.00%\nlogistic regression 31.73% 21.08%\nMLP 8.53% 5.79%\nSdA-2 8.38% 5.58%\nSdA-3 8.62% 5.78%\n4.3.2 Retrieval evaluation\nBy using the original radio station playlists as the ground\ntruth, we can evaluate the retrieval performance of our model.\nThe evaluation is done using TopBucket (TB) [7], which is\nthe proportion of common elements in the two top- Nlists.\nConstructing the ground truth from the playlists is done\nas follows. Each 2-song sequence S2\nn∈S2is made up\nof tracks{t1\nn,t2\nn}and has been observed |S2\nn|times. We\nconstruct one top list Lti∀ti∈T, as the set of all sequences\nS2\nnfor whicht1\nn=ti, ordered by|S2\nn|.Ltiessentially\ngives a list of all the tracks that have followed tiordered\nby their occurrence count. In the 3-song sequence case,\nwe construct a top list L{ti,tj}for pairs of tracks since in\n347Oral Session 4: Music Recommendation and Playlist Generation\nTable 3 . Retrieval performance based on the TopBucket (TB) measure of our models compared to random, popularity-\nbiased random, acoustic similarity and autotags similarity. Each score represents the average percentage (and standard\ndeviation) of songs in the ground truth that were returned by each model.\n2-song sequences 3-song sequences\nModel TB10 TB20 TB5 TB10\nrandom 0.25%±0.16% 0.58%±1.75% 0.11%±1.45% 0.25%±1.52%\npopularity-biased random 1.01%±3.15% 2.19%±3.34% 0.51%±3.17% 0.96%±3.15%\nacoustic (G1C) 1.37%±3.80% 2.37%±3.73% 0.63%±3.48% 1.61%±4.01%\nautotags (Cosine distance) 1.43%±3.98% 2.34%±3.86% 0.58%±3.34% 2.26%±4.89%\nlogistic regression 2.47%±5.08% 6.41%±6.40% 0.20%±2.00% 1.16%±3.40%\nMLP 16.61%±14.40% 23.48%±13.17% 7.72%±13.92% 20.26%±17.85%\nSdA-2 13.11%±12.05% 19.13%±11.19% 7.25%±13.66% 21.74%±19.75%\nSdA-3 13.17%±11.31% 18.22%±10.04% 9.74%±18.00% 26.39%±22.74%\npractice, a playlist generation algorithm would know the\nlast two songs that have played.\nFor our experiments, we used the top 10 and 20 ele-\nments and 5 and 10 elements for the 2 and 3-song sequence\ncase respectively. The results, shown in Table 3, represent\nthe average number of common elements in the ground\ntruth’s top list and each of the similarity models’ for ev-\nery song.\nBecause most sequences were only observed once ( |S2\nn|=\n1), we were often in the situation where all the sequences\ninLtihad an occurrence of 1 ( ∀S2\nn∈Lti:|S2\nn|= 1) and\nthe number of sequences in Ltiwas greater than Nfor a\ntop-Nlist. Because in such a situation there was no way\nto determine which sequences should go in the top-list, we\ndecided to extend the top- Nlist to all the sequences that\nhad the same occurrence count as the Nthsequence. In\nthe 2-song sequence case, we also ignored all sequences\nthat had|S2\nn|= 1 to keep the top- Nlists from growing a\nlot larger than N. Ignoring as well all the songs that did\nnot have at least Ntracks in their top-list, we were left,\nin the 2-song sequence case, with 834 songs that had an\naverage of 14.7 songs in their top-10 list and 541 songs\nwith an average of 28.8 songs in their top-20 list. In the\n3-song sequence case, the top-5 list was made up of 1,181\nsongs with a 6.6 top songs average and the top-10 list was\ncomposed of 155 songs with an average of 13.8 top songs.\nWe compared our model’s retrieval performance to two\nother similarity models. First, we computed the similarity\nin the space of autotags [7] from the cosine distance over\nsong’s tags vector [2]. The second comparison was per-\nformed by retrieving the most acoustically similar songs.\nAcoustic similarity was determined by using G1C [15] which\nis a weighted combination of spectral similarity and in-\nformation about spectral patterns. We also compared our\nmodel to a popularity-biased random model that proba-\nbilistically chooses the top songs based on their popular-\nity. Each song’s popularity was determined by looking at\nthe number of sequences it is part of.\nIn the 3-song sequence case, for the autotag and acous-\ntic similarity, we represent the similarity sim({t1,t2},t3)\nas the mean of sim(t1,t3)andsim(t2,t3).\nThe results of Table 3 clearly show that there is moreinvolved than simple audio similarity when it comes to re-\nconstructing sequences from radio station playlists. The\nperformance of the audio and autotag similarity are indeed\nsigniﬁcantly lower than models that were trained on actual\nplaylists.\nFurthermore, the TB scores of Table 3 are from the\nmodels that have the best classiﬁcation error (see Table 2).\nIt is interesting to note that some models with a worst clas-\nsiﬁcation error have better TB scores. While classiﬁcation\nis done by thresholding a model’s certainty at 50%, TB\ngives an indication of the songs for which a model has the\nhighest certainty. Since these are the songs that will be\nused when generating a playlist, this metric seems more\nappropriate to judge the models. The relation between\nclassiﬁcation error and TB scores is a topic for further in-\nvestigation.\n5. STEERABLE PLAYLIST GENERATION\nWhile the model presented above is able to build a sim-\nilarity space in which nearby songs ﬁt well together in a\nplaylist, it does not provide a mechanism for allowing the\nuser to personalize the sequence for a given context. To ad-\ndress this, ﬁnal song selection was done using the Aura6\n[2] recommendation engine from Sun Microsystems Labs.\nAura is able to generate transparent and steerable recom-\nmendations by working with a textual representation —\na tag cloud — of the items it is recommending. Speciﬁ-\ncally it ﬁnds the most similar items to any other in its pool\nby computing the cosine distance on their respective tag\nclouds. It can also explain to the user why an item is rec-\nommended by showing the overlap between tag clouds.\nWe use Aura as a means to allow users to personal-\nize (“steer”) the playlist generation by allowing them to\ncreate a personal tag cloud that represents the music they\nwish to listen to. In order to generate tag clouds for our\ntracks, we used Autotagger [7], a content-based machine\nlearning model. This model is designed to generate a tag\ncloud (speciﬁcally a weighted vector of 360 music-relevant\nwords) from an audio track, thus allowing us to use Aura’s\ncosine distance measure to compute the similarity between\n6http://www.tastekeeper.com\n34810th International Society for Music Information Retrieval Conference (ISMIR 2009)\neach track and the user’s personal cloud.\n5.1 Steps for generating a steerable playlist\nOur playlist generation algorithm works as follows :\n1. A seed track ts∈Tis selected amongst all possible\ntracks.\n2. The similarity model is used to compute transitional\nprobabilities between the seed song and all other ones (with\nmore similar songs having higher transition probabilities),\nkeeping only the top ϕ, or thresholding at a certain transi-\ntion probability ρ. LetTbe the group of these top songs:\nT= arg max\nti∈T\\tsϕM(ts,ti) (1)\n3. The user is then invited to create a tag cloud CUby\nassigning weights to any of the 360 tags in the system. In\nthis way the cloud is personalized to represent the mood or\ntype of songs the user would like to hear. The higher the\nweight of a particular tag, the more impact it will have on\nthe selection of the next song.\n4. Autotagger is used to generate a tag cloud Ctjfor all\ntrackstj∈T. The cosine distance ( cd(·)) between these\ntag clouds andCUis used to ﬁnd the song that best matches\nthe abstract musical context the user described with his or\nher cloud:\ntmin= arg min\ntj∈Tcd(CU,Ctj) (2)\n5. The track tminis selected to play next. Since the sys-\ntem is transparent, we can tell the user we chose the song\ntmin because it has a certain transition probability from\nthe seed song but also because its tag cloud overlapped\nwithCUin a particular way. The user can then go back\nand modify the tag cloud CUto inﬂuence how subsequent\nsongs will be selected.\nNaturally, a lot of extra factors can be used when de-\ntermining which song to play in step 4. For instance, we\ncould consider the user’s taste proﬁle to take into account\nwhat types of songs he normally likes, mixing his current\nsteerable cloud to the one representing his musical tastes.\nWe could also include a discovery heuristic to balance the\nnumber of novel songs selected as opposed to ones the user\nalready knows.\n5.2 Example playlists\nTo illustrate the effect of the steerable tag cloud, we gener-\nate two playlists seeded with the same song but with very\ndifferent steerable clouds. The ﬁrst 9 iterations of both\nplaylists are shown in Table 4. The effect of the cloud is\nclearly visible by the different direction each playlist takes.\nIn our view, this transition is done smoothly because it is\nconstrained by the underlying similarity model.\nTo visualize the similarity space and the playlist gener-\nating algorithm, we compute a full track-to-track similarity\nmatrix and reduce its dimensionally using the t-SNEE [18]\nalgorithm (see Figure 1). We chose t-SNEE because it\ntends to retain local distances while sacriﬁcing global dis-\ntances, yielding an appropriate two-dimensional visualiza-\ntion for this task (i.e. the distance between very similarTable 4 . Both the following playlists are seeded with the\nsong Clumsy byOur Lady Peace . To give a clear point\nof reference, we use the tag clouds of actual songs as the\nsteerable cloud. The softtag cloud is made up of the tags\nforImagine byJohn Lennon and the hard tag cloud with\nthe tags for Hypnotize bySystem of a Down .\nSoft tag cloud\nViva la Vida by Coldplay\nWish You Were Here by Pink Floyd\nPeaceful, Easy Feeling by Eagles\nWith or Without You by U2\nOne by U2\nFields Of Gold by Sting\nEvery Breath You Take by The Police\nGold Dust Woman by Fleetwood Mac\nEnjoy The Silence by Depeche Mode\nHard tag cloud\nAll I Want by Staind\nRe-Education (Through Labor) by Rise Against\nHammerhead by The Offspring\nThe Kill by 30 Seconds To Mars\nWhen You Were Young by The Killers\nHypnotize by System of a Down\nBreath by Breaking Benjamin\nMy Hero by Foo Fighters\nTurn The Page by Metallica\nsongs is more important to us than the relative global place-\nment of, e.g., jazz with respect to classical). We have over-\nlaid the trajectory of the two playlists in Table 4 to illustrate\ntheir divergence.\n6. CONCLUSIONS\nWe have demonstrated a method for learning song simi-\nlarity based on radio station playlists. The learnt model\ninduces a new space in which similar songs ﬁt well when\nplayed successively in a playlist. Several classiﬁers were\nevaluated on a retrieval task, with SdAs and MLPs per-\nforming better than other similarity models in reconstruct-\ning song sequences from professional playlists. Though\nwe were unable to show that SdAs outperform MLPs, we\ndid show much better performance than logistic regres-\nsion and measures such as G1C over standard audio fea-\ntures. Furthermore we argue that our model learns a direct\nsimilarity measure in the space of short song sequences\nrather than audio or meta-data based similarity. Finally,\nwe showed a way of doing steerable playlist generation by\nusing our similarity model in conjunction with a tag-based\ndistance measure.\nThough this model is only a ﬁrst step, its power and sim-\nplicity lie in the fact that its two components play very dif-\nferent but complementary roles. First, the similarity model\ndoes the grunt work by getting rid of all unlikely candi-\ndates, as it was trained speciﬁcally for that task. This then\ngreatly facilitates the steerable and ﬁne-tuned selection of\nthe subsequent track based on the textual aura, as most of\n349Oral Session 4: Music Recommendation and Playlist Generation\nFigure 1 . Part of the 2-d representation of the track-to-\ntrack similarity matrix generated by a 2-song sequence\nSdA model. The trajectories of the two playlists described\nin Table 4 are overlaid over the tracks. Both playlists are\nseeded with the same song, which is represented by the\nbigger dot. Each playlist diverges because of the steerable\ntag cloud that is guiding its generation.\nthe obvious bad picks have already been removed.\nFuture work should attempt to use the number of occur-\nrences of each sequence to give more importance to more\nreliable sequences. Also, the model might learn a better\nsimilarity space by being trained with richer features as in-\nput. For example, adding meta-data such as tags, a measure\nof popularity, the year the song was recorded, etc., might\nprove helpful. Such a richer input space is likely necessary\nto show a performance gain for SdAs over competing prob-\nabilistic classiﬁers. Our experiments also led us to believe\nthat an increase in the quality of the learnt similarity could\nprobably be attained by simply adding more training data,\nsomething that can be easily accomplished as thousands of\nsongs are played on the radio everyday.\n7. REFERENCES\n[1] R. Ragno, CJC Burges, C. Herley: “Inferring similarity\nbetween music objects with application to playlist gen-\neration,” Proceedings of the 7th ACM SIGMM interna-\ntional workshop on Multimedia information retrieval ,\npp. 73–80, New York, USA, 2005\n[2] S. Green, P. Lamere, J. Alexander, F. Maillet: ‘Gen-\nerating Transparent, Steerable Recommendations from\nTextual Descriptions of Items,” Proceedings of the\n3rd ACM Conference on Recommender Systems , New\nYork, USA, 2009\n[3] C. Baccigalupo, E. Plaza: “Case-based sequential or-\ndering of songs for playlist recommendation,” Lecture\nNotes in Computer Science , V ol. 4106, pp. 286–300,\n2006.\n[4] O. Goussevskaia, M. Kuhn, M. Lorenzi, R. Watten-\nhofer: “From Web to Map: Exploring the World of\nMusic,” IEEE/WIC/ACM International Conference on\nWeb Intelligence , Sydney, Australia, 2008.[5] B. Fields, C. Rhodes, M. Casey, K. Jacobson: “So-\ncial Playlists and Bottleneck Measurements: Exploit-\ning Musician Social Graphs Using Content-Based Dis-\nsimilarity and Pairwise Maximum Flow Values,” Pro-\nceedings of the 9th International Conference on Music\nInformation Retrieval , Philadelphia, USA, 2008.\n[6] P. Lamere: “Social Tagging And Music Information\nRetrieval,” Journal of New Music Research , V ol. 37,\nNo. 2, 2008.\n[7] T. Bertin-Mahieux, D. Eck, F. Maillet, P. Lamere: “Au-\ntotagger: a model for predicting social tags from acous-\ntic features on large music databases,” Journal of New\nMusic Research , V ol. 37, No. 2, pp. 115–135, 2008.\n[8] E. Vickers: “Automatic long-term loudness and dy-\nnamics matching,” Proceedings of the AES 111th Con-\nvention , New York, USA, 2001.\n[9] S. Streich: “Music Complexity: a multi-faceted de-\nscription of audio content,” Ph.D. Dissertation, UPF,\nBarcelona, 2007.\n[10] J.J. Aucouturier, F. Pachet: “Scaling up Music Playlist\nGeneration,” Proceedings of IEEE International Con-\nference on Multimedia and Expo (ICME) , V ol. 1,\npp. 105–108, Lausanne, Switzerland, 2002.\n[11] B. Logan: “Content-based playlist generation: Ex-\nploratory experiments,” Proceedings of the 3rd Inter-\nnational Conference on Music Information Retrieval ,\n2002\n[12] B. Logan: “Music Recommendation from Song Sets,”\nProceedings of the 5th International Conference on\nMusic Information Retrieval , Barcelona, Spain, 2004.\n[13] E. Pampalk, T. Pohle, G. Widmer: “Dynamic Playlist\nGeneration Based on Skipping Behaviour,” Proceed-\nings of the 6th International Conference on Music In-\nformation Retrieval , London, UK, 2005.\n[14] A. Flexer, D. Schnitzer, M. Gasser, G. Widmer:\n“Playlist Generation Using Start and End Songs,” Pro-\nceedings of the 9th International Conference on Music\nInformation Retrieval , Philadelphia, USA, 2008.\n[15] E. Pampalk: “Computational Models of Music Simi-\nlarity and their Application in Music Information Re-\ntrieval, Ph.D. dissertation, Vienna University of Tech-\nnology, 2006.\n[16] M. Mandel, D. Ellis: “Song-Level Features and Sup-\nport Vector Machines for Music Classication,” Pro-\nceedings of the 6th International Conference on Music\nInformation Retrieval , London, UK, 2005.\n[17] P. Vincent, H. Larochelle, Y . Bengio, P.-A. Manzagol:\n“Extracting and Composing Robust Features with De-\nnoising Autoencoders,” International Conference on\nMachine Learning , 2008\n[18] L.J.P. van der Maaten, G.E. Hinton: “Visualizing High-\nDimensional Data Using t-SNE,” Journal of Machine\nLearning Research , 9(Nov):2579-2605, 2008.\n350"
    },
    {
        "title": "Probabilistic Segmentation and Labeling of Ethnomusicological Field Recordings.",
        "author": [
            "Matija Marolt"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415532",
        "url": "https://doi.org/10.5281/zenodo.1415532",
        "ee": "https://zenodo.org/records/1415532/files/Marolt09.pdf",
        "abstract": "The paper presents a method for segmentation and labeling of ethnomusicological field recordings. Field recordings are integral documents of folk music performances and typically contain interviews with performers intertwined with actual performances. As these are live recordings of amateur folk musicians, they may contain interruptions, false starts, environmental noises or other interfering factors. Our goal was to design a robust algorithm that would approximate manual segmentation of field recordings. First, short audio fragments are classified into one of the following categories: speech, solo singing, choir singing, instrumental or bell chiming performance. Then, a set of candidate segment boundaries is obtained by observing how the energy of the signal and its content change, and finally the recording is segmented with a probabilistic model that maximizes the posterior probability of segments given a set of candidate segment boundaries with their probabilities and prior knowledge of lengths of segments belonging to different categories. Evaluation of the algorithm on a set of field recordings from the Ehtnomuse archive is presented.",
        "zenodo_id": 1415532,
        "dblp_key": "conf/ismir/Marolt09",
        "keywords": [
            "ethnomusicological",
            "field recordings",
            "segmentation",
            "labeling",
            "interviews",
            "performers",
            "actual performances",
            "live recordings",
            "amateur folk musicians",
            "robust algorithm"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nPROBABILISTIC SEGMENTATI ON AND LABELING OF \nETHNOMUSICOLOGICAL FIELD RECORDINGS \n Matija Marolt  \n Faculty of Computer and Information Science \nUniversity of Ljubljana, Slovenia \nmatija.marolt@fri.uni-lj.si   \nABSTRACT \nThe paper presents a method for segmentation and labe-\nling of ethnomusicological fiel d recordings. Field record-\nings are integral documents of folk music performances \nand typically contain interviews with performers intert-wined with actual performances. As these are live record-ings of amateur folk musicians, they may contain inter-ruptions, false starts, environmental noises or other inter-fering factors. Our goal was to design a robust algorithm \nthat would approximate manual segmentation of field re-\ncordings. First, short audio fr agments are classified into \none of the following categor ies: speech, solo singing, \nchoir singing, instrumental or bell chiming performance. Then, a set of candidate segment boundaries is obtained \nby observing how the energy of the signal and its content \nchange, and finally the recording is segmented with a probabilistic model that maximizes the posterior proba-bility of segments given a set of candidate segment boun-daries with their probabilities and prior knowledge of lengths of segments belonging to different categories. \nEvaluation of the algorithm on a set of field recordings \nfrom the Ehtnomuse archive is presented. \n1. INTRODUCTION \nEthnomusicological field recordings are recordings made \n“in the field”, capturing music in its natural habitat. Start-ing in the early 20th century and continuing to the present \nday, ethnomusicologists and folklorists have travelled \nand made recordings in various parts of the world primar-ily to preserve folk music, but also to make it available for further researches, such as studies of acculturation and change in music, comparativ e studies of music cultures \nand studies of the music maki ng process and its effect \nthrough performance. Segmentation of field recordings into meaningful units, such as speech, sung or instrumen-tal parts is one of the first tasks researchers face when a recording is first being studied. It is also a prerequisite for further automatic processing, such as extraction of key-words, melodies and other semantic descriptors.  \nSegmentation of audio recordings has been extensively \nexplored for applications such as speech recognition (re-\nmoval of non-speech parts, speaker change detection), \nsegmentation in broadcast news or broadcast monitoring. Typically, the distinction is made between speech, music and silence regions. Approach es to segmentation include \neither first classifying short periods of the signal into de-sired classes using some set of features and then making \nthe segmentation [1-3], or first finding change points in \nfeatures and forming segments and later classifying the segments [4-6]. Authors use a variety of features, clas-sifiers and distances depending on the nature of signals to be segmented. More recently, Ajmera [7] performed clas-\nsification and segmentation jointly by using a combina-\ntion of standard hidden Markov models and multilayer perceptrons for speech/music discrimination of broadcast news. Pikrakis et al. [8] used a three step approach: first they identified regions in the signal which are very likely to contain speech or music with a region growing algo-\nrithm. Then, they segmented the remaining short (few \nseconds long) regions with a maximum likelihood model that maximized the probability of class labels given frame-level features and se gment length limits. A Baye-\nsian network was used to estimate the posterior probabili-ty of a music/speech class label given a set of features. \nFinally, a boundary correction algorithm was applied to \nimprove the found boundaries. Their use of a probabilis-tic model is somewhat similar to the proposed segmenta-tion method, but as we describe further on, we use a max-imum likelihood approach to segment an entire field re-\ncording by first labeling signal fragments, then finding \ncandidate boundaries, and finally maximizing the proba-bility of segmentation considering probabilities of boun-daries and segment lengths given their class. \nThe algorithm presented in this paper was designed to \nrobustly label and segment ethnomusicological field re-\ncordings into consistent units, such as speech, sung and \ninstrumental parts. Resulting segmentations should be comparable to manual segmentations researchers make when studying recordings. Field recordings are docu-ments of entire recording sessions and typically contain interviews with performers intertwined with actual per-\nformances. As these are live recordings of amateur folk \nmusicians, they usually contain lots of “noise” and inter-ruptions, such as silence when performers momentarily \n \nPermission to make digital or hard copies of all or part of this work fo r\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for prof it or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page. \n© 2009 International Society for Music Information Retrieval  \n75Poster Session 1\n  \n \nforget parts of songs, false starts and restarts, laughter, \ndancing noises, interruptions by other persons, dogs bark-\ning or cars driving by. Performances may also change character; singing may become reciting, a second voice may join or drop out of a performance etc.  \nThe described nature of fiel d recordings calls for a ro-\nbust segmentation algorithm that would not over-segment \na recording at each interrupti on – for example; we are not \ninterested in each boundary separating speech and sung parts, as only some of them are actual segment bounda-ries. We would also like to distinguish between several different classes of segments and would like to take some prior knowledge of the classes into account. And last, we \nare not interested in millisecond-exact segment bounda-\nries or exact labeling of eac h small recording fragment; \nsometimes placing a boundary between two performances is a very soft decision and accuracy of a few seconds is good enough. Taking these points into account, we pro-\npose a three step algorithm for segmentation. First, a \nstandard classification algorith m is used to classify short \naudio segments into a set of predefined classes. Then, a set of candidate segment boundaries is obtained by ob-serving how the energy and cl ass distribution change, and \nfinally the recording is segmented with a probabilistic \nmodel that maximizes the posterior probability of seg-\nments given a set of candidate segment boundaries with their probabilities and prior knowledge of lengths of \nsegments belonging to different classes.  \n2. CLASSIFICATION \nClassification of short field recording fragments into a set \nof predefined categories repres ents the first part of our \nsegmentation algorithm. We base our work on field re-cordings from the EthnoMus e digital archive [9]. The \narchive contains folk song, music and dance collections of the Institute of Ethnomusicology, Scientific Research Centre of Slovene Academy of Sciences and Arts. Audio \nrecordings represent the larg est part of the archive and \ncomprise recordings of folk songs and melodies, with the oldest on wax cylinders from 1914 and around 30.000 field recordings on magnetic tape and digital media dat-ing from 1955 onwards. Only parts of the archive are di-gitally annotated. Field recordings are typically around an \nhour long and contain interviews with performers intert-\nwined with performances. The latter include singing (solo or group), reciting, instrument al pieces (a large variety of \ninstruments is used, depending on the region), as well as bell chiming, which is a Slovenian folk tradition of play-ing rhythmic patterns on church bells. The quality of re-\ncordings varies a lot and depends on their age, equipment \nused, location (inside, outside) and type of event (ar-ranged recording session or recording of a public event).  \nWe identified five categories into which field record-\ning fragments are to be clas sified: speech, solo singing, \nchoir singing (any performance with two or more voices belongs to this class), instrumental (including instrumen-\ntal with singing) and bell chiming. We then evaluated a \nset of features often used for speech/music discrimination and timbre recognition to find the ones most suitable for classification into these categ ories. The following nine \nfeatures were selected: \n• the quotient of RMS energy  variance over the squared \nmean of RMS energy. RMS energy r is defined as: \n \n1\n2\n01W\ni\nirxW−\n==∑ ,  (1) \nwhere x represents the time domain signal and W the \nwindow size. The feature describes the amount of sig-nal energy fluctuations and is typically larger for speech than for other types of signals; \n• mean spectral entropy, as defined by Pikrakis [10]. \nThe entropy represents the instability of signal energy \ncalculated over a number of spectral sub-bands and is typically low for bell chiming recordings, somewhat higher for music, and high for other signal types. It is calculated as: \n \n11\n001\n2\n0log\nLL\njj\njjL\nii\niXXX XH\n−−\n==−\n==−\n∑∑∑ , (2) \nwhere L represents the number of spectral sub-bands \nand Xi the energy of the i-th sub-band (see [10] for \nmore details); \n• variance of spectral entropy deltas. Deltas are calcu-\nlated as a linear trend over five consecutive windows; \n• variances of the first thr ee MFCC coefficients (omit-\nting the zero- th). MFCC coefficients describe the \nshape of the signal spectrum and are thus very appro-priate for our classification task; \n• variances of deltas of the first three MFCC coeffi-\ncients (omitting the zero- th). Deltas are calculated as a \nlinear trend over five consecutive windows. \nTo train and test a classifier, we manually labeled 1760 \n3 second long field record ing fragments from the Ethno-\nMuse digital archive. All feat ures were calculated on sig-\nnals windowed with a 46ms Hamming window with \n23ms overlap. Feature means and variances were calcu-\nlated over 3 second periods, thus taking approx. 130 fea-ture values into account. A multinomial logistic regres-sion classifier [11]  was chosen for classification, because \nit’s simple and gives good results. Furthermore, its output can be regarded as a probability distribution over all \nclasses. We trained the clas sifier to classify each frag-\nment into one of the five previously described classes. 2/3 of the labeled fragments were used for training and 1/3 for testing. Table  1 shows the average confusion matrix \nof our classifier for 10 trai ning/test runs. The overall ac-\ncuracy is at 78% of corr ectly classified instances. \nMost of the errors made by the classification algorithm \nare easy to explain. The co nfusion of speech and solo \nsinging segments is understandable, if we take into ac-\n7610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \ncount that singers are not prof essional musicians, they are \noften old persons and their singing close to reciting or \nvery monotonous. Confusion between solo and choir singing occurs in choir segments sung in unison, as well as duet singing, while instrumental and bell chiming segments are correctly classifi ed in most cases with con-\nfusion mostly arising between the two classes. \n \n classified as \n speech solo choir instr. bell ch.\nspeech 79% 14% 4% 3% 0%\nsolo singing 13% 61% 24% 1% 1%\nchoir singing 2% 10% 82% 3% 3%\ninstrumental 1% 3% 3% 82% 11%\nbell chiming 0% 0% 2% 7% 91%\nTable 1 . Confusion matrix of the classification algo-\nrithm.  \n3. SEGMENTATION \nTo segment a recording, we first find a set of candidate \nsegment boundaries and calculate the probability of split-ting the recording at each boundary. Segmentation is then \nperformed by maximizing the joint probability of all \nsegments, taking prior knowledge of segment lengths of different signal classes into account.  \n3.1 Finding and Evaluating  Candidate Boundaries \nWe consider two criteria for boundary placement: a crite-\nrion based on change in signal energy, such as when per-\nformances are separated by regions of silence, and a crite-\nrion based on change in signal content, such as when speech is followed by singin g. To observe changes in \nenergy, we calculate RMS energy e of the audio signal; \nchanges in signal content are detected by calculating the symmetric Kullback-Leibler (KL) divergence d [12] be-\ntween probabilities of signal classes as calculated by the \nlogistic classifier described in section 2. We find a set of candidate segment boundaries B by low-pass filtering \nboth measures to obtain their filtered versions e\n f and d f \nand finding all candidate boundary regions ( bl, br) that \nsatisfy:   \n()\n()10\n10max , or\n(, ) | [, ] :\nmax ,f\ntt\nlr lrf\nttee E E\nbb t bb\ndd D D⎧⎫ ⎡<⎪⎪⎢ =∀ ∈⎨⎬⎢>+ ⎪⎪⎣ ⎩⎭B , (3) \nwhere E0 and E1 are the global and relative thresholds \nthat determine the selection of energy-based candidate \nboundary regions and D0 and D1 the global and relative \nthresholds that determine the selection of divergence-based candidate boundary regions (see also Figure 1 for illustration). \nThus, the set of all candidate segment boundaries con-\ntains regions of the signal wher e its energy falls below, or \nthe amount of change in signal content rises above an adaptive threshold. This is illustrated in Figure 1, which displays a 13 minute long field recording excerpt. The overall RMS energy e (in dB) is displayed on top, the \nsymmetric KL divergence d below. Both adaptive thre-\nsholds are indicated with a dotted line; regions where the curves fall below (energy) or raise above (KL diver-gence) the threshold represent candidate segment bounda-ries. True segment boundaries are indicated in the middle. \nAs shown, the candidate boundary regions correspond \nwell with true boundaries. Many segments are clearly se-\nparated by regions of silence, as the energy plot shows. On the other hand, KL divergence is high where signal content changes, such as between speech and instrumen-tal or sung parts.  \n \n \nFigure 1 . Finding candidate boundaries.  \nSelecting all of the candidate boundary regions as true \nboundaries and splitting a recording accordingly is not the best idea; for example energy fluctuates a lot in speech parts (as can be seen in Figure 1) and these parts would consequently be over-segmented. One could at-tempt to find the best values for relative thresholds D\n1 \nand E1, but as we show, we can do better by treating the \nboundary selection process as  a classification task. For \nthis purpose, we trained two logistic regression classifiers (one for energy, one for KL divergence) to predict the probability of splitting the segment at a candidate boun-dary.  \nThe following features were found to be useful for \nenergy-based boundary classification: the amount of sig-nal energy below the energy threshold ( s\ne) and the maxi-\nmum difference in signal content to the left and right of the boundary region ( m\nc). They are calculated as: \n 10 max( , )\n1max ( ) ( )1r\nl\nl r\nlrb\nf\net t\ntb\nb bN\nct tctb N tbse E E e\nmP c c P c cN=\n+\n=− ==−\n== − =+∑\n∑∑ , (4) \nwhere P(ct = c) denotes the probability that the signal at \ntime t belongs to class c, as calculated by the classifica-\ntion algorithm presented in section 2 and N the number of \nframes taken into account to the left or right of the boun-dary region. The most useful features for the KL diver-\n77Poster Session 1\n  \n \ngence-based classifier were found to be the amount of \ndivergence above the threshold ( sd) and the total amount \nof divergence within the boundary region ( td): \n 10 log 1 max( , )\nlog 1r\nl\nr\nlb\nf\ndt t\ntb\nb\ndt\ntbsd d D D\ntd=\n=⎛⎞=+− +⎜⎟⎜⎟⎝⎠\n⎛⎞=+⎜⎟⎜⎟⎝⎠∑\n∑ . (5) \nBoth classifiers were trained and tested on a set of 30 \nfield recordings from the Ethnomuse archive, which were manually segmented and labeled, containing a total of \n840 segments. The classifiers were trained to predict \nwhether a found candidate boundary represents a true segment boundary or not. RMS energy e\nt was calculated \nas the average RMS energy within a 3s window around t \nand a step size of 0.5s. Symmetric KL divergence dt was \ncalculated between 10 second long segments to the left \nand right of t with the same step si ze. Such large window \nsizes were chosen primarily to make the algorithm more robust to “noise” in perform ances, such as false starts, \nperformers forgetting songs, interruptions etc. To obtain the smoothed vectors e\n f and d f, we zero-phase filtered e \nand d with a first order low-pass Butterworth filter with \ncutoff frequency of 0.01 π. The values of other parameters \nwere experimentally obtained and set to: E1=0.2, E0=10-6, \nD1=0.1, D0=3 and N=9. Using these parameters, we ex-\ntracted approximately 2400 candidate boundary regions from the field recordings and used two thirds of this set to train each classifier to predict whether a candidate boun-dary is a true segment boundary or not. We evaluated the performance of the two classi fiers on the remaining third \nof the dataset and compared it to an alternative of using \nan optimal fixed threshold fo r candidate selection. Table \n2 displays average precision and recall scores on the test set for 10 training/test runs. Compared to choosing a fixed threshold for boundary selection, logistic classifiers \nimprove the accuracy of selection. An additional advan-\ntage is that their output can be regarded as the probability of splitting the recording at a candidate boundary; a fact exploited by our segmentation algorithm described in sec-tion 3.2.  \ncriterion select. method precision recall \nenergy  \n best fixed threshold 0.71 0.57 \nlogistic classifier 0.7 0.67 \nKL  \ndivergence best fixed threshold 0.77 0.71 \nlogistic classifier 0.79 0.78 \nTable 2 . Selection of boundary candidates. \n \n3.2 Segmentation algorithm \nWe perform segmentation by following the logic of \nBayesian modeling and infer the most probable segmen-\ntation by maximizing:  (| ) ( |) ( )P seg data P data seg P seg ∝  (6) \nTo obtain a generative segmentation model, we define \nsegmentation as a sequence of segments Si1, Si2, ..., SiN, \n0<i1<i2< ... < iN, where Si1 starts at time 0 and ends at \ncandidate boundary Bi1, Si2 starts at candidate boundary \nBi1 and ends at Bi2, Si3 starts at Bi2 and ends at Bi3 and so \non. We treat each candidate boundary BtאB as a discrete \nrandom variable with two outco mes: either the candidate \nboundary represents an actual boundary and splits the re-cording into two segments, or not. The probability mass function for the variable is defined by outputs of the \nenergy ( P\ne) and KL divergence ( Pkl) classifiers, as de-\nscribed in section 3.1: \n () () m a x ( ) , ( )te t k l tPBt r u e P B P B==  (7) \nIn our model, the probability of each segment is only de-\npendent on location of the previous segment, so we can express the joint probability of all segments as: \n \n12 13 2 1() ( | ) ( | ) . . .( | )ii ii i i N i NPS PS S PS S PS S− ⋅⋅ . (8) \nTo calculate the probability of segment Si given Sj, we \nmust consider all candidate boundaries within the seg-ment, as well as its duration. If the segment is to start at \ntime j and end at i, values of all candidate boundary va-\nriables within the segment must be false , while the value \nof candidate boundary variable at time i must be true. \nSegmentation is further constrained by our previous knowledge of typical lengths of segments given their \nclass, leading to the following formulation: \n \n(|) (|,) ( ) ( ) .ij i i j i k\njkiP S S P D S S P B true P B false\n<<== = ∏  (9) \nEquation (8) then becomes: \n( )\n(, )() ( | , ) ( )ii i j j\nij jPB t r u ePD S S PB f a l s e\n∈∉=× =∏ ∏\nSS, (10) \nwhere S is the set of all segment indices and ( i,j) a pair \nof consecutive indices from this set.  \nProbability of segment durati on given its boundaries is \ndependent on the class of th e segment, as calculated by \nthe classifier presented in section 2. By analyzing dura-tions of segments in our coll ection of field recordings, we \nestimated the means and standard deviations for all seg-\nment classes (\nµc, σc); for example the duration of speech \nsegments varies a lot and ranges from several seconds to over ten minutes, while the average length of choir sing-ing segments is around three minutes and their standard deviation below two minutes. By additionally enforcing minimal segment duration D\nmin, we obtain the following \nexpression: \n (| , ) ( , , )\n(| ,)\n0,ii j c c\ncii j\nminPC c S S Gi j\nPD S S\nijDμσ ⎧=−⎪=⎨\n⎪−<⎩∑\n, (11) \nwhere P(Ci=c|Si, Sj) represents the pr obability that seg-\nment Si belongs to class c and is calculated as the average \n7810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nprobability of classification of  frames within the segment \ninto class c. G is the unscaled Gaussian function.  \nTo find the sequence of segments that maximizes Equ-\nation (10) and thus provides an optimal solution, we resort to dynamic programming that leads us to a simple and efficient solution.  For each segment S\ni ending at the \ncandidate boundary Bi we can calculate the most probable \nsegmentation that ends with this boundary d(Si) by the \nfollowing rules: \n ()\n00.5 0\n()() m a x ( ) ( , ) 0\n(, ) ( | , ) ( )i\nijji\nii j k\njkii\ndSPB t r u e dS ci j i\nci j PD S S PB f a l s e<= <\n<<= ⎧⎪=⎨=>⎪⎩\n==∏ ,    (12) \nwhere S0 represents the segment boundary at time 0;  S0 is \na boundary if a performance st arts at time 0, or not if \nthere is silence or noise pres ent, so we give it a probabili-\nty of 0.5.  \nIn our implementation, we minimize the negative log-\nlikelihood of segmentation, so  all products become sum-\nmations. When the function d(Si) is calculated for all can-\ndidate boundaries, the most likely segmentation can be recovered by tracking back the calculation and retrieving \noptimal boundary indices.  \nAfter segmentation is calcula ted, segments can be la-\nbeled by finding the class c that maximizes P(C\ni=c|Si, Sj); \nas mentioned before, the latte r and is calculated as the \naverage probability of classifi cation of frames within the \nsegment Si into class c. \n3.3 Evaluation  \nAs with boundary selection, we evaluated our segmenta-\ntion algorithm on a set of 30 field recordings from the \nEthnomuse archive, which were manually segmented and labeled, containing a total of 840 segments. Because of its specific nature, it is difficult to directly compare the algo-\nrithm to other segmentation approaches. We therefore \nprovide a comparison of the proposed method to a simple \nthresholding algorithm, where segments are formed by thresholding either the energy, KL divergence or the maximal energy/KL diverg ence candidat e boundary \nprobabilities. Results are given in Table 3. Average preci-\nsion and recall scores of true vs. estimated segment \nboundaries for all 30 recordings for the three thresholding \nand the proposed pr obabilistic method are shown.  \n \n average \nprecision average \nrecall \nthresholding Pe(Bt) 0.61 0.61 \nthresholding Pd(Bt) 0.65 0.64 \nthresholding max( Pe(Bt),Pd(Bt)) 0.73 0.78 \nproposed algorithm 0.78 0.81 \nTable 3 . Comparison of segmentation algorithms. \nThe probabilistic al gorithm is quite robust and im-\nproves segmentation accuracy over the more naive thre-sholding approaches. Most of the false positives occur in \nspeech sections containing ve ry long regions of silence \nthat for example occur when pe ople reflect on past events \n(consequently causing large drops in energy), or in solo singing performances that are interleaved with reciting or \nspoken statements, such as “t his is repeated three times \nand we start dancing in a circle so and so ...” (causing \nhigh KL divergence).  False negatives occur when per-\nformances follow each other w ithout signifi cant changes, \nfor example several songs sung in a row almost without interruptions, or when the st art or end of a segment is \nmissed, because it interleaves with speech, so that the \nboundary is placed either too so on or too late in a record-\ning.  \nTo evaluate the influence of the choice of relative and \nglobal thresholds (see eq. (3)) on segmentation, we eva-luated the algorithm’s performance by varying values of the four thresholds individually, with other parameters \nfixed. The resulting precision/recall curves are given in \nFigure 2.  \n \n \nFigure 2 . Precision/recall curves obtained by varying \nthe four thresholds that influence candidate boundary region selection: E\n0 and E1 for energy (both are shown \nin dB), D0 and D1 for KL divergence curves. \nWe can observe that precisi on is only marginally af-\nfected by both global thresholds ( E0 and D0) – raising \nthem will result in a smaller number of boundaries found, \nthus decreasing recall, while  precision w ill not increase \nby much, as the false positives seem to be almost equally spread between weak (low global threshold) and strong (high global threshold) candidate boundary regions. On the other hand, precision is more strongly affected by rel-ative threshold selection ( E\n1 and D1); small relative thre-\nshold values will result in many false positives, as any \nsignificant drop in energy or rise in the KL divergence curve will result in a new b oundary candid ate. Higher \nvalues increase precision and decrease recall, as expected. \nThe accuracy of classification of correctly found seg-\nments into one of the five cl asses is 86%; errors are simi-\nlar to the ones described in section 2.  \n79Poster Session 1\n  \n \n4. CONCLUSION \nThe proposed algorithm for segmentation and labeling of \nethnomusicological field recordings provides a good starting point for further de velopment of automatic me-\nthods for analysis of such recordings. Its accuracy is good enough for practical use an d the algorithm has already \nbeen integrated in to the tools of the Ethnomuse archive \nand is available to its users. For further improvements, we \nneed to start looking into the inner structure of each seg-ment, which may help us to improve the found bounda-ries. We also plan to explor e hierarchical segment classi-\nfication to classify instrument al segments into typical en-\nsemble types, speech and sing ing segments into male and \nfemale etc. \nAcknowledgments. This work was supported in part by \nthe Slovenian Government-Founded R&D project Eth-\nnoCatalogue: creating semantic descriptions of Slovene \nfolk song and music. \n5. REFERENCES \n[1] E. Scheirer and M. Slaney, \"Construction and \nevaluation of a robust multifeature speech/music \ndiscriminator,\" in IEEE Internationa l Conference on \nAcoustics, Speech, and Signal Processing , 1997, pp. \n1331-1334 vol.2. \n[2] L. Lie , et al. , \"Content-based audio segmentation \nusing support vector machines,\" in IEEE \nInternational Conference on Multimedia and Expo,  \n2001, pp. 749-752. \n[3] G. Williams  and D. P. W. Ellis, \"Speech/music \nDiscrimination Based On Posterior Probability Features,\" in Eurospeech'99 , Budapest, Hungary, \n1999, pp. II-687-690. [4] G. Tzanetakis and P.  Cook, \"Multifeature audio \nsegmentation for browsing and annotation,\" in \nApplications of Signal Processing to Audio and Acoustics, 1999 IEEE Workshop on , 1999, pp. 103-\n106. \n[5] C. Panagiotakis and G. Tziritas, \"A speech/music \ndiscriminator based on RMS and zero-crossings,\" \nMultimedia, IEEE Tr ansactions on, vol. 7, pp. 155-\n166, 2005. \n[6] M. Cettolo , et al. , \"Evaluation of BIC-based \nalgorithms for audio segmentation,\" Computer \nSpeech & Language, vol. 19, pp. 147-170, 2005. \n[7] J. Ajmera, \"Robust Audio Segmentation,\" Ph.D., \nFaculte des sciences et techniques de l'ingenieur, Ecole Polytechnique Federale de Lausanne, \nLausanne, 2004. \n[8] A. Pikrakis , et al. , \"A Speech/Music Discriminator \nof Radio Recordings Based on Dynamic \nProgramming and Bayesian Networks,\" Multimedia, \nIEEE Transactions on, vol. 10, pp. 846-857, 2008. \n[9] M. Marolt , et al. , \"Ethnomuse: Archiving Folk \nMusic and Dance Culture,\" in Eurocon 2009 , St. \nPetersburg, Russia, 2009. \n[10] A. Pikrakis , et al. , \"A computationally efficient \nspeech/music discriminator for radio recordings,\" in \nISMIR 2006, 7th International Conference on Music Information Retrieval , Victoria, Canada, 2006. \n[11] I. H. Witten and E. Frank, Data Mining . San \nFrancisco, USA: Morgan Kaufmann, 2005. \n[12] W. D. Penny \"Kullback-Liebler divergences of \nnormal, gamma, Dirichlet and Wishart densities,\" \nTechnical report , Wellcome Department of \nCognitive Neurology, London, UK, 2001. \n \n \n80"
    },
    {
        "title": "Musical Structure Retrieval by Aligning Self-Similarity Matrices.",
        "author": [
            "Benjamin Martin 0001",
            "Matthias Robine",
            "Pierre Hanna"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416502",
        "url": "https://doi.org/10.5281/zenodo.1416502",
        "ee": "https://zenodo.org/records/1416502/files/MartinRH09.pdf",
        "abstract": "We propose a new retrieval system based on musical structure using symbolic structural queries. The aim is to compare musical form in audio files without extracting explicitly the underlying audio structure. From a given or arbitrary segmentation, an audio file is segmented. Irrespective of the audio feature choice, we then compute a selfsimilarity matrix whose coefficients correspond to the estimation of the similarity between entire parts, obtained by local alignment. Finally, we compute a binary matrix from the symbolic structural query and compare it to the audio segmented matrix, which provides a structural similarity score. We perform experiments using large databases of audio files, and prove robustness to possible imprecisions in the structural query.",
        "zenodo_id": 1416502,
        "dblp_key": "conf/ismir/MartinRH09",
        "keywords": [
            "symbolic structural queries",
            "selfsimilarity matrix",
            "audio feature choice",
            "local alignment",
            "binary matrix",
            "structural similarity score",
            "large databases",
            "imprecisions",
            "robustness",
            "experiments"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMUSICAL STRUCTURE RETRIEVALBYALIGNING SELF-SIMILARITY\nMATRICES\nBenjamin Martin, Matthias RobineandPierre Hanna\nLaBRI -UniversityofBordeaux\n351, coursdelaLib´ eration\n33405TALENCECedex -FRANCE\nfirstname.name@labri.fr\nABSTRACT\nWeproposeanewretrievalsystembasedonmusicalstruc-\nture using symbolic structuralqueries. The aim is to com-\npare musical formin audioﬁles without extractingexplic-\nitly the underlying audio structure. From a given or arbi-\ntrary segmentation, an audio ﬁle is segmented. Irrespec-\ntive of the audio feature choice, we then compute a self-\nsimilarity matrix whose coefﬁcients correspond to the es-\ntimationofthesimilaritybetweenentireparts,obtainedb y\nlocalalignment. Finally,wecomputeabinarymatrixfrom\nthe symbolic structural query and compare it to the audio\nsegmented matrix, which provides a structural similarity\nscore. We perform experiments using large databases of\naudio ﬁles, and prove robustness to possible imprecisions\nin thestructuralquery.\n1. INTRODUCTION\nContent-based search on very large audio ﬁles databases\nis an important issue in music information retrieval. New\nbrowsingtools proposeto compareaudio songsaccording\nto music propertiessuch as style, rhythm,melody, timbre,\netc. Amongall of these properties, takinginto accountin-\nformationaboutstructuremaybe veryusefulfordiscrimi-\nnating songs, since it maybe closely linked to music style\nor music composer. In this paper, we propose to focus on\nthese structuralproperties.\nMusical structure has been of major concern over the\nlast years. Thisﬁeld aimsto retrieveandcomparehuman-\nrecognizable musical structure within an audio piece. To\nthisend,Footeproposedin1999[1]aself-similaritymatri x-\nshaped representation whose coefﬁcients carry structural\ninformationoverthemusicalpiece. Thismatrixisobtained\nanalyzing repeated sections within the piece; it describes\nbothaglobalstructureanddifferentlocalstructures.\nExisting works about music structure generally focus\nonly on structural analysis in audio ﬁles. Foote et al.pro-\nposed a music summarization method by summing scores\ninitsself-similaritymatrixrepresentation[2]. Dannenb erg\ntested several transcription methods by adapting them to\nthe nature of the given audio ﬁle, in order to explicitly re-\ntrieve the underlyingstructure [3]. M¨ uller et al.proposed\na method to extract relevant paths in a self-similarity ma-\ntrix, deducingthe precise structure of the music piece [9].\nBartsch developedan automatic thumbnailing system that\nretrieve relevant parts from audio [7], and Goto focused\nlater in chorus extraction over songs taking into account\npossible modulations or variable durations of their occur-\nrences in the audio musical piece [6]. Peeters used [5] a\nrepresentation in terms of ”states” of music and proposed\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2009 International Society for MusicInformation Retrieva l./g44/g87/;##################/g87/;###/g448/g286/g272/g410/g381/g396/g400 \n/g400/g286/g395/g437/g286/g374/g272/g286/g400 /g44/g87/;##################/g87/;###/g448/g286/g272/g410/g381/g396/g400 \n/g400/g286/g395/g437/g286/g374/g272/g286/g400 /;####/g437/g282/g349/g381 \n/g400/g286/g336/g373/g286/g374/g410/g400 /;####/g437/g282/g349/g381/;###/g400/g349/g336/g374/g258/g367\n/g94/g286/g395/g437/g286/g374/g272/g286/g400 \n/g381/g296/;###/g44/g87/;##################/g87 \n/;#################/g349/g374/g258/g396/g455/;###/;####/g437/g282/g349/g381 \n/g400/g286/g336/g373/g286/g374/g410/g400 /;####/g437/g282/g349/g381 \n/g400/g286/g336/g373/g286/g374/g410/g400 /g94/g286/g336/g373/g286/g374/g410/g258/g410/g349/g381/g374 \n/g44/g87/;##################/g87/;###/g272/g381/g373/g393/g437/g410/g258/g410/g349/g381/g374 \n/g62/g381/g272/g258/g367/;###/g258/g367/g349/g336/g374/g373/g286/g374/g410 /g94/g455/g373/g271/g381/g367/g349/g272/g400/g410/g396/g437/g272/g410/g437/g396/g258/g367/;###\n/g395/g437/g286/g396/g455 \n/g94/g286/g367/g296/g882/g400/g349/g373/g349/g367/g258/g396/g349/g410/g455/g373/g258/g410/g396/g349/g454 /;#################/g349/g374/g258/g396/g455/;###\n/g94/g286/g367/g296/g882/g400/g349/g373/g349/g367/g258/g396/g349/g410/g455/g373/g258/g410/g396/g349/g454 \n/g94/g272/g381/g396/g286/;###/g381/g296/;###/g400/g410/g396/g437/g272/g410/g437/g396/g258/g367/;###\n/g400/g349/g373/g349/g367/g258/g396/g349/g410/g455 /g1006/;########################/;###/g272/g381/g373/g393/g258/g396/g349/g400/g381/g374 \nFigure1. Overviewofthequery-by-structuremethodpro-\nposed.\nan algorithm that computes a structural summarization in\nseveralpassesovertheaudioﬁle. Bartsch etal.worked[4]\non a summarization centered on choruses using chroma\nfeatures. Paulus et al.used [8] several audio features to\nbuilda probabilityspace inorderto analyzethe bestprob-\nableunderlyingstructure.\nOncethestructureiscorrectlyanalyzed,aretrievalsys-\ntemmaybedirectlydevelopedbycomparingthesequences\nof structure. The problem is the correctness of the struc-\nture estimated. Although several systems have been pro-\nposed and evaluated, analysis errorssigniﬁcantly limit th e\naccuracy of a retrieval system based on such approaches.\nLately,IzumitaniandKashinoproposedamethodthatesti-\nmatesthestructuralsimilaritybetweentwoexcerptsbydi-\nrectlycomparingtheself-similaritymatricescomputed[1 2].\nThismethodis appliedtocoversongdetection.\nThe method we propose trails after the same principle:\ncomparing musical form without extracting explicitly the\nunderlying audio structure. In this paper, we bring a new\nretrievalsystembasedonmusicalstructureusingsymbolic\nqueries. In Section 2, we describe the system proposed.\nIn Section 3, we present differentexperimentson real pop\nmusic databases. Finally, we conclude and open perspec-\ntivesinSection4.\n2. METHOD\nThe proposed method searches the database for the musi-\ncal piece that best matches a given symbolic query. See\nFigure1forthemethodglobalschematicview.\n2.1 AudioSelf-Similaritymatrixcomputation\nFirst, the system splits an audioﬁle into naudio segments\naccordingto a givensegmentation. Theappliedsegmenta-\ntionusedinourretrievalsystem willbe describedlater.\n483Poster Session 3\n2.1.1 Features\nIntheproposedmethod,thecomparisonis basedonmusi-\ncal structures, not directly on audio features. Therefore, it\nis fundamental to keep in mind that the chosen audio fea-\nturescan be changed,regardlessof the furthersteps of the\nmethod.\nAs a feature set, we use Harmonic Pitch Class Proﬁles\n(HPCP) [11]. HPCPs, which provide tonal information,\nare robust to noise, timbre, dynamics, tuning or loudness\nvariations, and ensure then an accurate tonal description.\nThe different tonality vectors are extracted the same way\nasin [11]or[10],framebyframe.\nThis analysis transforms an input audio ﬁle into a se-\nquence H= (− →hi)1≤i≤nofN B-dimensional vectors− →hi,\nwhere Ndenotesthenumberofframesinthemusicalpiece,\nandBdenotesthechosenchromabinresolution(generally\n12,24or36). Oursystemsettlesfora12binsresolution.\nThe method proposed compares the signal to itself in\nordertoretrievestructuralinformation(seeSection2.1. 2).\nThat’s why an adapted measure that enables the compar-\nison between two HPCP vectors is needed. We choose\nthe binary local alignment technique described in [10] to\nthispurpose. OntopofbeingadaptedtoHPCPs, thismea-\nsurecomputestheoptimaltranspositionindexbetweentwo\nchromastoprovideasimilarityscore,whichallowsourde-\ntection to be robust to key changes within the same audio\nmusical piece. This comparison measure is able to com-\npute from two features sequences H1andH2a similarity\nscorebylocalalignment.\n2.1.2 Segmentedself-similarity matrix ofanaudioﬁle\nAs explainedbefore,we use self-similaritymatrices in or-\nder to represent the repeated sections. In our model, self-\nsimilaritymatricescontainelements,whoserangeis [0,1],\nthat stand for the likeliness between two parts of a struc-\nture. Horizontal and vertical axis of the matrix represent\ntime,thatrunsfromlefttorightaswellasfromtoptobot-\ntom.\nInclassicusesofself-similaritymatrices,each i, jcoef-\nﬁcient is computed by comparing the feature correspond-\ning to the time iof the musical piece with the one corre-\nspondingtothetime jofthesamemusicalpiece. However,\nin our model, contrary to the general self-similarity com-\nputing process, time does not run uniformly on both axis.\nIndeed, each element i, jof the matrix correspondsto the\nsimilarity measure between two entire parts PiandPjof\nthe musical piece. Thus, each coefﬁcient corresponds to\ntheevaluationofthesimilaritybetweentwosetsoffeature\nvectors,notdirectlybetweentwovectors.\nBased on the naudio ﬁle segments computed from the\naudio signal, nsequences H1, H2, . . . , H nof HPCP vec-\ntorsarecomputed,eachonecorrespondingtoanaudioseg-\nment. Therefore, comparing two HPCP sequences means\ncomparingtwo segmentedparts of the audiosignal. Thus,\nfor each couple of HPCP sequences H1andH2, we com-\npute a similarity score usingthe binarylocal alignmentby\ndynamic programming technique inspired from Gomez’s\nworkanddescribedin2.1.1. Thisprovidesaself-similarit y\nmatrix R, whose coefﬁcients stand for the comparison of\ntwo partswithinthemusicalpiece:\nR= (alignment (Hi, Hj))1≤i≤n,1≤j≤n(1)\nwhere alignment ()denotes the binary local alignment by\ndynamicprogrammingtechniqueusedtocomparetwo\nHPCP sets. Anexampleofreferencesegmentedself-simi-\nlaritymatrixisshownonFigure2(right).\n2.2 Querymatrixcomputation\nIn a ﬁrst approach,the queryused forcomparisonsis a bi-\nnaryself-simlaritymatrixcomputedfromasymbolicstruc-/;#### /;#### /;################# /;################## /;#### /;################# /;######################## \n/;####\n/;####\n/;#################\n/;##################/;#### /;#### /;################# /;################## /;#### /;################# /;######################## \n/;####\n/;####\n/;#################\n/;################## /;##################\n/;####\n/;#################\n/;########################/;##################\n/;####\n/;#################\n/;########################\nFigure 2. Left: Symbolic query self-similarity matrix of\nThe Beatles - All My Loving musical piece. Right: Seg-\nmented reference self-similarity matrix of the same musi-\ncalpiece. Lettersshowthedifferentrecognizablepattern s;\nwhite pixels stand for repeated sections, and black pixels\nstandfordistinctsections. Theleftreferencematrixisse g-\nmentedaccordingtothegroundtruthforthismusicalpiece\n(exactstructure).\ntural query, that can be arbitrarily deﬁned or taken from a\ngroundtruth.\n2.2.1 Symbolicstructuralquery\nOur model uses symbolic structural queries, which are a\nsymbolic representation of the underlying structure of a\nmusicalpiece. Asymbolicquerycanbeseenasasequence\nof symbols that represent a particular musical form. Each\npart within a symbolic structural query has its own dura-\ntion. It can represent a simple note as well as an entire\nexcerpt.\nA symbolic structural query can be seen as a sequence\nof symbols, for instance ‘aabca’, combinedor not to sym-\nbolsrepresentingdurationinformation. Twoidenticalsym -\nbols within the sequence indicate a similarity between the\ntwo corresponding parts, and two different symbols indi-\ncate adissimilarity.\n2.2.2 Self-similaritymatrix ofasymbolicquery\nSymbolicqueriesarecomparabletopatternsequencesthat\nimpose two kindsof constraints: similarities, i.e.remark-\nablerepetitions,anddissimilarities.\nThe symbolicquery self-similaritymatrix is createdby\nanalyzing the provided patterns sequence. Assuming that\n(sk)1≤k≤nrepresentsasymbolssequenceoflength n(e.g.\ns= ‘ababc′), the queryself-similarity matrix Qis deﬁned\nasfollows:\n∀(i, j)∈ {1. . .n}2, Qi,j=/braceleftBig1if s i=sj\n0if s i/ne}ationslash=sj(2)\nTheresultingmatrixisbinary,andstandsfor2typesof\nconstraints: similarity and dissimilarity. An example of a\nself-similarity matrix created from a symbolic query can\nbeviewedinFigure2(left).\n2.3 Matricescomparison\nIn order to assign a similarity score between the matrices\nwe compute, we use three different algorithms that have\ndifferentproperties. These threealgorithmsprovidea nor -\nmalizedsimilarityscoreaccordingtoabinaryquerymatrix\nand a reference matrix. From now on, the two matrices\ncomparedaredenotedas QandR.\nThe ﬁrst approach consists in computing the similarity\nbetween matrices using a pixel-to-pixel algorithm, based\nonaneuclideandistancealgorithm. However,weconsider\nmore sophisticated algorithms that show different charac-\nteristics.\n48410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n/g894/g349/g895 \n/g894/g349/g349/g895 \nFigure 3. Possible compared patterns taken into account\nin local alignment with (i) Izumitani’s algorithm and (ii)\nLecroq’salgorithm.\nAnapproachto thecomparisonproblemcanbeyielded\nby local alignment algorithms. In our context, this prin-\nciple is extended to matrices comparison; possible opera-\ntions in order to transform the ﬁrst matrix into the second\nwork on symbols that can be seen as clusters of the com-\nparedmatrices.\nInsomecases, it canbe veryrelevantto considerpixel-\nwise deletions or insertions in the matrices comparison.\nMoreover, if the query binary matrix is smaller than the\ncompared referencematrix (structure excerptssearch), lo -\ncal alignment techniquesare able to cope with the dimen-\nsiondifferenceandto providealocalsimilarity score.\n2.3.1 IzumitaniandKashino’salgorithm\nThe ﬁrst local alignment algorithm tested is the one pre-\nsented in [12]. This algorithm takes as an input two self-\nsimilarity matrices. Since these matrices are symmetrical\nand have constantmaximumdiagonal,this algorithmonly\nworksonthelowertriangles. Itisbasedonadynamicpro-\ngramming method that searches the diagonal direction of\nthe reference self-similarity matrix. Indeed, the Izumita ni\nand Kashino’s algorithm compares each entire line of the\nreferencematrix with each entire line of the query matrix.\nThus, foreach comparison,it allowsthe 3 differentopera-\ntions (insertion/deletion/substitution)on a unique patt ern:\nentirelinesofthelowertriangleofthecomparedmatrices.\nThispatterncanbeseenonFigure3(left).\nFurthermore, the dynamic programming matching me-\nthod is based on “matched element indices sets” recur-\nsively computed, which means that at each step n, match-\ningelementsbetweentwocomparedlinesarededucedfrom\nthen−1step (see [12], 2.3, p.612). In other words, if an\nelement does not match the compared line at a step, the\nwhole following column elements will not be taken into\naccountin anyfurthercomparison. Thisrepresentsa limi-\ntationofthisalgorithm.\n2.3.2 Lecroqet al.’salgorithm\nInordertoimprovethealignmentcomparison,wechoseto\nconsider a different method. Indeed, reducing considered\npatterns for the comparisons to lines only seemed to be\nratherlimited,whichledustoevaluatea newmethod.\nThesecondlocalalignmentalgorithmstudiedisadapted\nfrom [13]. It was developed and used in order to com-\npare symbolic dialog annotations, and is particularly spe-\ncializedinaligning2-dimensionalpatterns. Lecroq et al.’s\nalgorithm browses the matrices element by element, and\nallowsthe3 typicaloperationsondifferentpatterns: a sin -\ngle element (pixel),a part of a line or an entire line, a part\nof a columnor an entirecolumn,and a part ofa line andapart of a column simultaneously. These different patterns\narerepresentedonFigure3(right).\nOur adaptation consisted in not comparing text entries\nbutpatternsincludedinself-similaritymatrices,taking into\naccount their properties and adapting the comparison to a\nnon-binarysimilaritymeasure.\n2.4 Retrievalsystemspeciﬁcities\n2.4.1 Query-basedsegmentation\nSymbolic structural queries can be combined or not to in-\nformationsaboutthedurationofeachpattern.\nIfthequeryindicatesabsolutetimeinformations(inframe s\nor seconds),these canbe usedto split the musicalpiece in\nsegments.\nIfthequeryindicatesrelativetimeinformations,theglob al\ndurationofthepiececanbeusedtosplitthemusicalpiece.\nIf no time information is provided in the symbolic query,\nsegmentationisarbitrary: forinstance,itcanbeuniforml y\nprocessed,eachparthavingthesamedurationthantheoth-\ners.\n2.4.2 Pre-processing\nBefore evaluating a similarity score, the reference self-s i-\nmilarity matrix must be pre-processed. Indeed, since the\nreferencematrixcontainsthesimilarityscoresbetweenth e\ndifferent parts of the musical piece, the distribution of th e\nvalues of its coefﬁcients is likely to vary according to the\nconsidered musical piece. For some audio ﬁles, the tonal\ndistinction between two parts that are supposed to be dif-\nferent,e.g. achorusandaverse,willbeveryclear,whereas\nit will turnoutto bevagueinsomeothercases.\nLetµandσberespectivelytheaverageandthestandard\ndeviationvaluesof the coefﬁcients’distributionin the re f-\nerence matrix R. The normalizedreferenceself-similarity\nmatrix ˆRiscomputedasfollows:\n∀(i, j)∈ {1. . .n}2,ˆRi,j=(Ri,j−µ)\nσ·ˆσ+µ(3)\nwhere ˆσis a constant corresponding to the new standard\ndeviationto apply. It must be adaptedto the pixel-to-pixel\ncomparison constants (see 2.4.3). In our model, we used\nˆσ= 0,31.\n2.4.3 Adaptedeuclideandistance\nIn a ﬁrst approach,we comparematricesby computingan\neuclidean score, based on pixel-to-pixel comparisons. As\nexplained before, the binary query matrix contains simi-\nlarity and dissimilarity constraints. However, our method\ndoesnotgivethese twoconstraintsthesameimportance.\nActually, we can reasonably hypothesizethat two parts\nthataresupposedtobesimilar( i.e.thatarerepresentedby\nthe same symbol in the query)present two close tonal de-\nscriptions, whereas two parts that are supposed to be dis-\nsimilar can be either close, or different in their tonal de-\nscriptions: thereisnogradationonthedissimilaritynoti on.\nTherefore, it is necessary to make an distinction between\nthestrongsimilarityconstraintsandtheweakdissimilari ty\nconstraintsthatimposesthesymbolicquery.\nOur adaptedeuclidean distance computesthen two dif-\nferent scores, which takes into account this distinction: A\nsimilarity score s=∈[0,1], that is established only with\nsimilarityconstraintsimposedbythequery,\nA dissimilarity score s/negationslash=∈[0,1], that is established only\nwith thedissimilarityconstraintsofthequery.\nLetQandRdenotethequeryandreferencecomparedma-\ntrices, respectively. We introducetheset\nR=={(i, j)|Qi,j= 1, i < j }\nthatdenotesthereferencematrixindicesthatcorrespondt o\nasimilarity(whitepixel,value1)inthequery. Inthesame\nway,weintroducetheset\n485Poster Session 3\nR/negationslash=={(i, j)|Qi,j= 0, i < j }\nthat denotes the reference matrix indices that correspond\nto a dissimilarity (black pixel, value 0) in the query. s=is\ncomputed comparingeach similarity element of the query\nwiththecorrespondingelementinthereferencethefollow-\ningway:\ns==1\n|R=|/summationdisplay\n(i,j)∈R=||1−Ri,j|| (4)\nwhere ||.||denotes the classical euclidean norm. s/negationslash=is\ncomputedcomparingeachdissimilarityelementoftheque-\nrywiththecorrespondingelementinthereferencethefol-\nlowingway:\ns/negationslash==1\n|R/negationslash=|/summationdisplay\n(i,j)∈R/negationslash=f(Ri,j) (5)\nwhere fdenotes an exponential shaped function that was\nempiricallydetermined:\nf(x) =1\ne8−1·(e8·x+ 1)\n3. EXPERIMENTS\nInordertoevaluateourmethodinthemostsystematicway,\nweestablishedaserieofexperimentsthatunderlineitsdif -\nferentcharacteristics.\n3.1 Databasesestablishment\nWe based our research on a structural ground truth cor-\npus created by M. Levy, K. Noland and G. Peeters1. It\nincludes 60accurate XML annotations for western pop\nsongs, and numbersfor each musical piece every detected\nsection with its duration. It was found to be one of the\nmostusedcorporainthisﬁeld,inmanypriorstudies,such\nas[14]or[15].\nWe used two different audio ﬁles databases in order to\nvalidateourmodel.\n- The ground-truthcorrespondingaudio ﬁles database Dg,\nthatincludesthe 60musicalpiecesannotatedinthecorpus;\n- A noise database Dm, that contains 200audio western\npopmusicaudioﬁlesfromdifferentauthors.\nObviously,we respectedthefollowinginclusion:\nDg⊂ D m. Audio ﬁles were taken from commercial CD\nversions.\nWe analyzed signals using HPCP features (see 2.1.1)\nwithanoverlapof 50%andawindowsizeof 744millisec-\nonds.\n3.2 Exactstructuralqueries\nThe ﬁrst experiment consists in searching musical pieces\nthrougha large database with the prior knowledgeof their\nexactstructureandtimesegmentation. Thus,wegenerated\nqueries according to the symbolic representation and seg-\nmentation provided by our ground truth corpus. To com-\npute the reference matrices, we segmented the Dmﬁles\naccording to each available ground truth data given in Dg\n(60ﬁles). Segmentationwas carriedoutwith relativetime\ninformationprovidedbythe queries.\nWethencomputedsimilarityscoresbetweeneachquery\nandthe 200well-segmentedreferences,andcheckedwhe-\ntherthe best matchingwasmadeon theﬁle corresponding\nto the query. To do so, since the appliedsegmentationand\nsymbolic query were supposed to be exact, we used the\nadapted euclidiandistance describedin Section 2.4.3. Be-\ncause of the high accuracy of the used symbolic queries\nand segmentations, this simple algorithm was as efﬁcient\naslocalalignmenttechniquesforthisexperiment.\n1http://www.elec.qmul.ac.uk/digitalmusic/downloads/i ndex.html#segment/;################# /;################## /;#### /;#################/;################## /;######################## /;############################ /;################## /;################# /g894/g349/g895 \n/g894/g349/g349/g895 \n/;#################/;####/;############################/g38/g38/;########################/;#################/;#### /g894/g349/g349/g349/g895 \nFigure 4. Structural excerpts concordance between the\nthreebestresultsmatchingwith All MyLoving excerpt. (i)\nStop the rock , (ii)All My Loving - the queried structural\nexcerpt, and (iii) A Day in the Life . Rectangles widths are\ncommensurate with pattern durations; Dashed lines show\nstructuralconcordances.\nFor all of the 60cases tested, the experiment was con-\nclusive: the corresponding audio ﬁle was best matched.\nTherefore, the system is able to retrieve exactly a musical\npieceprovidingitsexactstructureandsegmentation.\n3.3 Structureexcerptsqueries\nThesecondexperimentconsistsinsearchingmusicalpieces\nthrough a database with the prior knowledge of a part of\ntheirexactstructureandtimesegmentation. Inotherwords ,\nknowingan excerptof a structureand the segmentationof\na musical piece, we now aim at retreiving the entire orig-\ninal musical piece as well as any piece that contains the\nsame givenstructuralexcerpt.\nAsasymbolicquery,wechoseastructuralexcerptfrom\ntheAll My Loving byTheBeatles musicalpiece:\n’VCBV’ (Verse-Chorus-Bridge-Verse).\nWe assume that the time segmentation of this structure is\nknown,i.e.weknowhowlongeachpartlasts.\nSince the symbolic query matrix and the referencema-\ntrixdonothavethesamesize,thepixel-to-pixeldistancei s\nirrelevant here. Thus, we tested the alignment of the sub-\nrequest matrix on every musical piece of Dgsegmented\naccording to the The Beatles piece with Izumitani et al.’s\nandLecroqetal.’salgorithms.\nWitheachofbothalgorithms,thebestmatchingwasob-\ntained on the original Beatles piece. However, the second\nbestmatchedresultdiffersfromonealgorithmtotheother.\nFigure4 shows the matched structure excerpt on the best\nmatched pieces: the original The Beatles piece (ii), that\nwas best matched on both of the algorithms, the second\nbest matched piece with Izumitani’salgorithm (i), and the\nsecond best matched piece with Lecroq’s algorithm (iii).\nTheindicatedpatternscorrespondtogroundtruthdatarel-\native to each audio musical piece. Here are the full sym-\nbolicstructuresofthesepieces:\nAll My Loving : ’A ABC BAB D’\nA DayintheLife : ’A BBBCD EB CF G’\nStoptheRock : ’A AA A AB CDB AE F F DBA’\nBy segmenting musical pieces according to the All My\nLovingground truth, the structures of pieces (i) and (iii)\nwere re-segmented, exhibiting a new structure that highly\nmatchedthe queriedstructureexcerpt(ii). Dashedlines in\nFigure4 show the high pattern matching: pattern similar-\nities and dissimilarities are nearly identical between eac h\nofthethreebest matchedpieces.\n3.4 Time robustness\nIn the experiments described above, we processed exact\nsegmentations taken from the ground truth. However, our\nquery-by-structuremethodaimsat gettingridoftime con-\nstraints, and at being able to retrieve correctly providing\nexclusivelyasymbolicqueryasanentry.\n48610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n/g1007/g1004 /g1008/g1004 /g1009/g1004 /g1010/g1004 /g1011/g1004 /g1012/g1004 /g1013/g1004 /g1005/g1004/g1004 \n/g1007/g1004 /g1008/g1004 /g1009/g1004 /g1010/g1004 /g1011/g1004 /g1012/g1004 /g1013/g1004 /g1005/g1004/g1004 \n/g400/g1030 /g286/g1031 /g400/g1030 /g286/g1031 \n/g894/g349/g895 /g894/g349/g349/g895 \n/g1007/g1004 /g1008/g1004 /g1009/g1004 /g1010/g1004 /g1011/g1004 /g1012/g1004 /g1013/g1004 /g1005/g1004/g1004 \n/g400/g1030 /g286/g1031 /g894/g349/g895 /g894/g349/g349/g895 \n/g894/g349/g349/g349/g895 /g62/g286/g336/g286/g374/g282/g855\n/g68/g258/g410/g272/g346/g349/g374/g336 \n/g94/g272/g381/g396/g286/;###/g894/g1081/g895 \n/;#################/g381/g396/g282/g286/g396/;###/g393/g381/g400/g349/g410/g349/g381/g374 /g75/g396/g349/g336/g349/g374/g258/g367/;###/g395/g437/g286/g396/g455 \n/g271/g381/g396/g282/g286/g396/;###/g393/g381/g400/g349/g410/g349/g381/g374 \n/g400/g1030 /g286/g1031 \n/g94/g410/g258/g396/g410/;###/g381/g296/;###/g410/g346/g286/;###/g296/g349/g396/g400/g410/;###/g393/g258/g396/g410 /;############################/g374/g282/;###/g381/g296/;###/g410/g346/g286/;###/g400/g286/g272/g381/g374/g282/;###/g393/g258/g396/g410 \nFigure 5. Alteringa single time borderbetweentwo parts\nof a structure: consequences on similarity scores. (i):\nIzumitani and Kashino alignment algorithm, (ii): Lecroq\nalignmentalgorithm,(iii): Adaptedpixel-to-pixeldista nce.\nPlainlinestandforthe AllMyLoving,TheBeatles musical\npiece, while dashedline stands for the second best match-\ning score. Horizontal axis show the border position, from\nthe start of the ﬁrst part to the end of the second, in sec-\nonds. Itslengthisabout15seconds.\nTherefore,it isnecessaryto test ouralgorithmsontime\nconstraints, i.e.tochangemoreorless thepriorexactseg-\nmentationinordertoevaluatetheirrobustnesstowardsthi s\ncriterion.\nAsexplainedbefore,asegmentationcombinedtoasym-\nbolic query provides a series of symbolic parts indexed in\ntime. Fromnowon,wewilldenoteasaborderoftwocon-\nsecutive parts the exact point in time when ends the ﬁrst\npart and starts the second one. The followingtime robust-\nness experiments consist in changing the position of one\nor several borders in order to observe the impact on the\nmatchingscores.\n3.4.1 Changingonesegmentationborder\nTheﬁrsttimerobustnesstest consistsinchangingonebor-\nder overa givensegmentation. Fromthe groundtruthcor-\npus, we chose a musical piece ( All My Loving ),and modi-\nﬁed itsexactsegmentation.\nHereisthesymbolicstructureofthismusicalpiece:\nVerse- Verse-Chorus-Bridge- Verse-Chorus-Outro\nWe chose to work on the ﬁrst Chorus / Bridge border, for\nit generallydemarcatestwo distinct partsin theirtonalde -\nscription.\nLets1,e1,s2ande2denote respectively the beginning\nand the end of the ﬁrst chorus, and the beginning and the\nend of the bridge. In order to estimate time robustness on\nour method,we generateda seriesof 20differentsegmen-\ntationschangingtheborderpositionfrom s1toe2,keeping\nthe constraint that e1=s2. Thus, the only difference be-\ntween two generated segmentations is the position of this\nborder.\nThe experiment results are shown on Figure5. The\nplain line identiﬁes the scores for the considered musical\npiece, whereas the dashed line indicates the second best\nmatching score over the database Dg. In the x-axis, the\ntestedpositionofthebordervariesfrom s1toe2,theverti-\ncal dashed and dotted line indicating the original position\noftheborderinthegroundtruth.\nForthe 3comparisonmethods,wecanseethatthescores\nobtainedforthemusicalpiece(plainlines)varyslightlyi n\nanintervalthatcorrespondsto ±6secondsaroundtheorig-\ninal position of the border. Above this value, the border\nseemstoalterthescoreinsuchawaythatthetestedmusi-\ncal piecedoesnotbestmatchthesymbolicquerymatrix./g1010/g1004 /g1010/g1009 /g1011/g1004 /g1011/g1009 /g1012/g1004 /g1012/g1009 /g1013/g1004 /g1013/g1009 /g1005/g1004/g1004 \n/g1004 /g1006 /g1008 /g1010 /g1012 /g1005/g1004 /g1010/g1004 /g1010/g1009 /g1011/g1004 /g1011/g1009 /g1012/g1004 /g1012/g1009 /g1013/g1004 /g1013/g1009 /g1005/g1004/g1004 \n/g1004 /g1006 /g1008 /g1010 /g1012 /g1005/g1004 \n/g1005/g1004/g1004 /g894/g349/g895 /g894/g349/g349/g895 \n/g1010/g1004 /g1010/g1009 /g1011/g1004 /g1011/g1009 /g1012/g1004 /g1012/g1009 /g1013/g1004 /g1013/g1009 /g1005/g1004/g1004 \n/g1004 /g1006 /g1008 /g1010 /g1012 /g1005/g1004 \n/g894/g349/g349/g349/g895 /g62/g286/g336/g286/g374/g282/g855 \n/g68/g258/g410/g272/g346/g349/g374/g336 \n/g94/g272/g381/g396/g286/;###/g894/g1081/g895 \n/g68/g258/g454/g349/g373/g437/g373/;###/g393/g286/g396/g272/g286/g374/g410/g258/g336/g286/g381/g296/;###\n/g271/g381/g396/g282/g286/g396/g400/g258/g367/g410/g286/g396/g258/g410/g349/g381/g374 \nFigure 6. Altering every time borders between parts in a\nstructure: consequencesonsimilarity scores. (i) Izumita ni\nand Kashino alignment algorithm, (ii) Lecroq et al. align-\nmentalgorithm,(iii)Adaptedpixel-to-pixeldistance. Pl ain\nline stand for the average over the 10 draws, and dashed\nlinesstandforthegreatestandlowestobtainedvalues. The\nmatchingscorescorrespondtotheoutputcomputedbythe\ndifferentalgorithms.\n3.4.2 Changingevery segmentationborders\nAfter changinga single borderin a segmentation,we esti-\nmated time robustness on our algorithms by changing ev-\nery borders in the same segmentation. Let dbe the total\nconsidered musical piece duration. We introduce pmax,\nexpressed as a percentage of the total musical piece dura-\ntion, that corresponds to the maximum variation of each\nborder in the segmentation (in percent). In other words,\nintroducing tmax=pmax·d, each border in the segmen-\ntation may be changedto a new time that does not exceed\ntheoldone ±tmax. We chose 10differentvaluesfor pmax\nfrom1%to10%of the total piece duration, and gener-\nated a random segmentation whose borders were changed\naccording to the above principle. This operation was re-\npeated 10timesforeachvalueof pmax.\nResults can be seen in Figure6. The ﬁgures show the\naverage scores (plain lines) as well as the minimum and\nmaximumscores(dashedlines)obtainedoverthe 10draws.\nThey show the evolution of similarity scores between the\nsymbolic query and the randomly altered segmentations\naccording to the maximum alteration factor. Considering\nthesecondbestmatchingscoresobtainedonthesamesym-\nbolic query, the three algorithms seem to be robust to a\nmaximumvariationof thebordersof6% ofthe total piece\nduration. Above this value, the best matching is realized\nona differentpiece.\n3.5 Query-by-Structure\nWe nowconsidertimelessqueriesthatonlyimposeasym-\nbolic structure. From now on, no information about time\nsegmentationisgiven: theprincipleistosearchforamusi-\ncalpieceinthedatabaseprovidingexclusivelyitssymboli c\nstructure.\nInordertorealizethisexperiment,wefocusedonafew\npieces that make part of the ground truth. This way, we\ncouldgettheannotatedstructureofeachmusicalpieceand\nuse it as the input symbolic query of our method. How-\never, borders timings were not retrieved from the anno-\ntation ﬁles, which yielded queries without timing indica-\ntions. Lacking any indication about time in the segmen-\ntation, our system assumes that the different symbols are\n487Poster Session 3\nRank\n#MusicalPiece EuclIzumLecroq\n1AllMyLoving 764\n2DevilinHerHeart 212\n3Drive 113\n4ItWon’tBeLong 111\n5Lonestar 8411126\n6Misery 111\n7NotaSecondTime 111\n8TakeOnMe 291355\n9Thubthumpning 3153\n10Wannabe 13128\n11WhenI’mSixtyFour 1009054\n12WithaLittleHelp.. 111\n13Words 2910\n14YouReallyGot.. 111\nAverageRank 17.5718.7912.14\nMedianRank 23.53\nMRR 0.5440.5370.480\nTable 1. Query-by-Structureresults with 3different algo-\nrithms: euclidean(Eucl),Izumitani(Izum)andLecroq.\nuniformly distributed over the excerpt. In other words, it\nassignsthesamedurationtoeachsymbolin thequery.\nResults can be viewed in Table1. We tested 14differ-\nent symbolic structural queries with no time informations\ncompared to each ﬁle of Dm. The result table shows the\nretrieval ranks obtained for every musical pieces and for\neachofthe 3algorithms.\nAs shown by the two previous experiments, the algo-\nrithms used are able to deal with an inexact segmenta-\ntion over the audio ﬁles. However, we saw limitations\non the maximum time variation applied on borders. After\nanalysing the ground truth relative to the different tested\nmusicalpieces,we couldsplit thetest set intwo classes:\n- Regular structured pieces, whose recognizable patterns\nhave close durations (less than 10% of the duration of the\npiece). ThisclasscontainspiecesNo. 1,2,3,4,6,7,9,12,\n13,14inthetable;\n- Irregular structured pieces, whose recognizable pattern s\ndurationsmay vary signiﬁcantly (possibly more than 10%\nofthedurationofthepiece). ThisclasscontainspiecesNo.\n5,8,10,11.\nAswecansee,irregularstructuredexcerptsranksarerathe r\nhigh with the three algorithms, which shows the limita-\ntionsofoursystem. However,regularstructuredpiecesare\nmuchmorepreciselyretrieved.\nAt comparingthedifferentalgorithmsused,we cansee\nthat the three algorihms seem to be efﬁcient, euclidian a-\ndapted distance providingthe best Mean Reciprocal Rank\n(MRR).\n4. CONCLUSION ANDFUTUREWORK\nWehaveproposedamusicretrievalsystembasedonstruc-\ntural similarity. Considering a symbolic representation o f\nthe underlying structure of a musical piece, an audio ﬁle\ncan be segmented and comparedusing self-similarity rep-\nresentationonHPCP features,providinga similarityscore\nthat indicates the structural likeliness. We used three dif -\nferent algorithms to compare matrices. We proved that\nnotonlythealgorithmworksexactlyonaccuratesymbolic\nqueries,butitpresentsalsoasigniﬁcantrobustnesstosli ght\ntime variations, which even allow searching for structures\nignoringtiminginformations.\nAn interesting idea as a perspective is to work on a\nquery-by-example oriented system. This time, the query\nwill not be a symbolic structural information but an au-\ndio ﬁle, arbitrarily segmented. Then, the retrieval will befocused on ﬁnding the closest structures to the underly-\ning structure of the input audio ﬁle. In our ﬁrst tests, we\nmanagedtoget fromaninputaudioﬁle a verystructurally\nsimilar musicalpiece.\nFinally, our system is based on a tonal representation\nof the signal. Nevertheless, since the retrieval operation\nworks on structural data, this parameter could be changed\nto any other one. We should consequentlytest our system\nonotherfeatures,suchasrhythmrepresentationsortimbre\nanalysis.\nThisworkispartoftheSIMBALSproject(JC07-188930),\nfundedbythe FrenchNationalResearchAgency.\n5. REFERENCES\n[1] J. Foote: “VisualizingAudioSegmentationusingSelf-\nSimilarity,” Proc.of ACMMultimedia ,77-80,1999.\n[2] J. Foote and M. Cooper: “Automatic Music Summa-\nrizationviaSimilarityAnalysis,” ISMIR02,2002.\n[3] R. B. Dannenberg and N. Hu: “Pattern Discovery\nTechniquesforMusicAudio,” ISMIR02,2002.\n[4] M. A. Bartsch and G. H. Wakeﬁeld: “Audio Thumb-\nnailingofPopularMusicUsing Chroma-BasedRepre-\nsentations,” IEEE Transactions on multimedia, Vol. 7 ,\n2005.\n[5] G.Peeters, A.L.BurtheandX.Rodet: “TowardAuto-\nmatic Music AudioSummaryGenerationFromSignal\nAnalysis,” Proc.ofISMIR ,2002.\n[6] M. Goto: “A Chorus-Section Detecting Method for\nMusicalAudioSignals,” Proc. ofICASSP ,2003.\n[7] M.A.BartschandG.H.Wakeﬁels: “Tocatchachorus:\nusing chroma-based representations for audio thumb-\nnailing,”Proc.ofWASPAA ,2001.\n[8] J. Paulus and A. Klapuri: “Music Structure Analy-\nsis Using a Probabilistic Fitness Measure and an In-\ntegratedMusicologicalModel” Proc.ofISMIR ,2008.\n[9] M. M¨ uller and F. Kurth: “TowardsStructural Analysis\nof Audio Recordings in the Presence of Musical Vari-\nations,”EURASIPJournalon Advancesin SignalPro-\ncessing,Vol.07,2007.\n[10] J. Serra,E. Gomez,P. Herrera,andX. Serra: “Chroma\nBinary Similarity and Local Alignment Applied to\nCover Song Identiﬁcation,” IEEE Transactions on Au-\ndio,SpeechandLanguageProcessing ,2008.\n[11] E.Gomez: “Tonaldescriptionofmusic audiosignals,”\nPh.D. dissertation, , MTG, Universitat Pompeu Fabra,\nBarcelona,Spain,2006.\n[12] T. Izumitani and K. Kashino: “A Robust Musical\nAudio Search Method Based on Diagonal Dynamic\nProgramming Matching of Self-Similarity Matrices,”\nProc.ofISMIR ,2008.\n[13] E. Chanoni, T. Lecroq and A. Pauchet: “Une nouvelle\nheuristique pour l’alignement de motifs 2D par pro-\ngrammationdynamique (in french),” JFPDA08 (Plan-\niﬁcation, Decision and Learning French-Speaking\nDays),Metz,France,pp.83-92,2008.\n[14] E. Peiszer, T. Lidy and A. Rauber: “Automatic Audio\nSegmentation: Segment Boundary and Structure De-\ntectioninPopularMusic,” Proc.ofLSAS ,Paris,France,\n2008.\n[15] M. Casey and M. Slaney: “The Importance of Se-\nquences in Musical Similarity,” Proc. of ICASSP06 ,\nVol.5,Toulouse,France,2006.\n488"
    },
    {
        "title": "Using Musical Structure to Enhance Automatic Chord Transcription.",
        "author": [
            "Matthias Mauch",
            "Katy C. Noland",
            "Simon Dixon"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414844",
        "url": "https://doi.org/10.5281/zenodo.1414844",
        "ee": "https://zenodo.org/records/1414844/files/MauchND09.pdf",
        "abstract": "Chord extraction from audio is a well-established music computing task, and many valid approaches have been presented in recent years that use different chord templates, smoothing techniques and musical context models. The present work shows that additional exploitation of the repetitive structure of songs can enhance chord extraction, by combining chroma information from multiple occurrences of the same segment type. To justify this claim we modify an existing chord labelling method, providing it with manual or automatic segment labels, and compare chord extraction results on a collection of 125 songs to baseline",
        "zenodo_id": 1414844,
        "dblp_key": "conf/ismir/MauchND09",
        "keywords": [
            "Chord extraction",
            "audio computing task",
            "different chord templates",
            "smoothing techniques",
            "musical context models",
            "repetitive structure",
            "song segments",
            "chroma information",
            "multiple occurrences",
            "baseline comparison"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nUSING MUSICAL STRUCTURE TO\nENHANCE AUTOMATIC CHORD TRANSCRIPTION\nMatthias Mauch, Katy Noland, Simon Dixon\nQueen Mary University of London, Centre for Digital Music\n{matthias.mauch, katy.noland, simon.dixon }@elec.qmul.ac.uk\nABSTRACT\nChord extraction from audio is a well-established music\ncomputing task, and many valid approaches have been pre-\nsented in recent years that use different chord templates,\nsmoothing techniques and musical context models. The\npresent work shows that additional exploitation of the repet-\nitive structure of songs can enhance chord extraction, by\ncombining chroma information from multiple occurrences\nof the same segment type. To justify this claim we mod-\nify an existing chord labelling method, providing it with\nmanual or automatic segment labels, and compare chord\nextraction results on a collection of 125 songs to baseline\nmethods without segmentation information. Our method\nresults in consistent and more readily readable chord la-\nbels and provides a statistically signiﬁcant boost in label\naccuracy.\n1. INTRODUCTION\nThe automatic extraction of chords from audio has appli-\ncations in music retrieval, cognitive musicology, and auto-\nmatic generation of lead sheets. In this work we present\na technique that allows us to generate more authentic lead\nsheets than previously possible with automatic methods,\nby making use of musical structure. Much of musical struc-\nture is deﬁned by repetition, a core principle in music [1,\np. 229].\nIn popular songs a repeated verse -chorus format is com-\nmon, in which the chord sequence is the same in all sec-\ntions of the same type. In lead sheets, for better readabil-\nity these sections would normally only be notated once,\nwith repeats indicated. Our method mirrors this improve-\nment by assigning the same chord progression to repeated\nsections. In addition, having found repeating sections, we\nhave available several instances of a given chord sequence\nfrom which to estimate the chords, so we expect an im-\nprovement in estimation accuracy. We demonstrate the\nimprovements in readability and accuracy using manually-\nannotated descriptions of the musical structure, and show\nthat the improvement can also be achieved using an auto-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.matic structure annotation algorithm tailored to the task.\nIn Section 2 we describe related work. In Section 3 we\ndescribe the chord extraction method used and present a\nnew segmentation technique that is tailored to our task of\nﬁnding repeated chord sequences. We give examples of\nchord estimation with and without the segmentation tech-\nnique in Section 4, and present quantitative chord estima-\ntion results in Section 5. In Section 6 we discuss our ﬁnd-\nings, and present our conclusions in Section 7.\n2. RELATED WORK\nThe majority of approaches to automatic chord estimation\nrely on framewise chroma features [2] as a representation\nof the relative energy in each pitch class for a given time\nwindow, then apply some further processing to estimate\nthe chords. When template-matching is used to identify\nchords, additional smoothing over time, for example by a\nmedian ﬁlter [3], is necessary due to musical variation and\nnoise. Inference in hidden Markov models (HMMs) [4]\nsimultaneously performs template-matching and smooth-\ning. These methods treat chords as isolated features of the\nmusic, which is a considerable simpliﬁcation. In reality,\nchords are heard in context, together with the melody, key,\nrhythm, form, instrumentation, and other attributes. Some\nchord estimation methods account for additional musical\nattributes during the estimation process such as key [5], or\nkey and rhythm together [6, 7], which is a step towards a\nuniﬁed music analysis model.\nIn this work we extend the concept of uniﬁed music\nanalysis by using repetition in the structure to enhance\nchord estimation. Dannenberg [8] shows that knowledge\nof the musical structure can greatly improve beat tracking\nperformance, but to our knowledge the principle has not\nyet been applied to chord estimation.\nPrevious automatic music structure extraction tech-\nniques include those that primarily search for section\nboundaries, indicated by a sudden change in the feature of\ninterest, which could be timbre [9], spectral evolution [10],\nor combinations of features [11]. A common approach is to\ncluster together frames that are similar, then label contigu-\nous similar frames as a segment. However, this relies on a\nparticular feature remaining approximately constant for the\nduration of a section. We are interested in chords, which\ndo change during a section, so an approach that searches\nfor repeated progressions [12, 13] is more appropriate for\nour purposes. Methods using this paradigm rely on a self-\n231Poster Session 2\nsimilarity matrix [14], which is a symmetric, square ma-\ntrix that contains a measure of the similarity between every\npair of frames. Repeated sections appear as parallel diago-\nnal lines, and can be extracted with some post-processing,\nsuch as application of a low pass ﬁlter to reduce noise [15]\nfollowed by a thresholding operation to ﬁnd contiguous\nframes with high similarity. In Section 3.3 we present a\nnew variation which is similar to algorithms proposed by\nOng [16] and Rhodes and Casey [17] and extracts repeated\nchord progressions of equal length.\n3. METHOD\nIn a song, we call a chord sequence that describes a section\nsuch as the verse or chorus a segment type . Any segment\ntype may occur one or more times in a song and we call\neach occurrence a segment instance . To make use of seg-\nment repetition as part of the chord estimation process, we\nrely on segment types whose instances are not only har-\nmonically very similar, but also have the same length in\nbeats (see Section 3.4). This is not required of a general\npurpose segmentation task, and hence generic segmenta-\ntions are not directly utilisable. In Section 3.2 we de-\nscribe how we preprocess manual segmentations to meet\nour needs. For automatic segmentation we choose to im-\nplement our own algorithm, which fulﬁlls the above re-\nquirements by design (Section 3.3). First, we describe the\nmethod for calculating our basic features, beat-synchronous\nchromagrams (Section 3.1).\n3.1 Beat-Synchronous Chromagrams\nThe automatic segmentation and chord estimation algo-\nrithms both rely on chroma features that are synchronised\nto the musical beat. The features represent the importance\nof each pitch class at the current beat. The initial, short\nchroma frames are generated from a note salience repre-\nsentation similar to a constant-Q transform, at a hopsize\nof 512 samples (46 ms) from audio that has been down-\nsampled to 11025 Hz. For the chord extraction algorithm\nwe split the salience representation to obtain separate bass\nand treble chromagrams, but the chromagram used by the\nsegmentation algorithm covers both the bass and the treble\nrange. For details see [18].\nIn order to produce beat-synchronous chromagrams we\nobtain a single chroma vector for each beat by taking the\nmedian (in the time direction) over all the chroma frames\nfalling between two consecutive beat times. We use one\nof two sorts of beat times: manual or automatic. The col-\nlection of manual beat annotations covers 125 songs per-\nformed by the rock group The Beatles. The automatic\nbeat times were extracted using Davies’s automatic beat-\ntracker [19] on the same set of songs.\n3.2 Manual Structural Segmentation\nThe manual structural segmentations cover the same 125\nsongs by The Beatles as we have beat annotations for: 29songs were annotated for a previous project1, and 96 were\nnewly annotated for the present work. The basis for all\nannotations are Pollack’s song analyses [20].\nEvery song contains several segment types, some of\nwhich have multiple instances. In some songs, the in-\nstances of a segment type differ in length. In that case,\nto fulﬁll the requirement of equal length instances, the seg-\nment type is divided to create one or more new segment\ntypes whose instances all have the same length. This may\nresult in new segment types having only one instance in the\nsong.\n3.3 Automatic Segmentation Algorithm\nThe automatic segmentation method has two main steps:\nﬁnding approximately repeated chroma sequences in a song,\nand a greedy algorithm to decide which of the sequences\nare indeed segments. We calculate the Pearson correlation\ncoefﬁcients between every pair of chroma vectors, which\ntogether represent a beat-wise self-similarity matrix R=\n(rij)of the whole song. This is similar to the matrix of\ncosine distances used by Ong [16]. In the similarity ma-\ntrix, parallel diagonal lines indicate repeated sections of a\nsong. In order to eliminate short term noise or deviations\nwe run a median ﬁlter of length 5 (typically just more than\none bar) diagonally over the similarity matrix. This step\nensures that locally some deviation is tolerated.\nWe perform a search of repetitions over all diagonals\nin the matrix over a range of lengths. We assume a mini-\nmum length of m1= 12 beats and a maximum length of\nmM= 128 beats for a segment, leading to a very large\nsearch space. We minimise the number of elements we\nhave to compare by considering as section beginnings only\nthose beats that have a correlation rgreater than a thresh-\noldtr, and assuming that section durations are quantised to\nmultiples of four beats. We found that a value of tr= 0.65\nworked well. In future work we would like to learn trfrom\ndata. We further reduce the search space by allowing seg-\nments to start only at likely bar beginnings. Likely bar\nbeginnings are beats where the convolution of a function\nrepresenting the likelihood of a change in harmony, and a\nkernel with spikes every two beats has a local maximum\n(details in [18]).\nTo assess the similarity of a segment of length lstarting\nat beat ito another one of the same length starting at jwe\nconsider the diagonal elements\nDi,j,l= (ri,j, ri+1,j+1, . . . , r i+l,j+l) (1)\nof the matrixR. If the segments starting at iandjare\nexactly the same, then Dijwill be a vector of ones, and\nhence we can characterise a perfect match by\nmin{Di,j,l}= 1. (2)\nTo accomodate variation arising in a practical situation,\nwe relax the requirement (2) by using the empirical p-\n1Segmentations available at http://www.elec.qmul.ac.uk/\ndigitalmusic/downloads/index.html#segment .\n23210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nquantile function2instead of the minimum (which is the 0-\nquantile), and choosing a segment threshold tslower than\nunity. The triple (i, j, l)hence describes a repetition, if\nquantilep{Di,j,l}> ts. (3)\nThe two parameters p= 0.1andts= 0.6are chosen\nempirically. In future work we would like to learn these\nvalues from the ground truth data. The set of repetitions\nRil={j: quantilep{Di,j,l}> ts}is then added to a list\nLof repetition sets, if it has more than one element j, i.e.\nif it actually describes at least one repetition. If two seg-\nments (i, j1, l)and(i, j2, l)overlap, only the index of the\none with the higher score is retained in Ril.\nEach of the setsRilrepresent a potential segment type,\nand its elements represent the start beats of instances of\nthat segment type. However, there are typically many more\nrepetition sets than there are segment types. To ﬁnd repeti-\ntion sets relating to actual segment types we use the heuris-\ntic of a music editor who tries to save paper: he will ﬁrst\ntake the repetition set in which l×|R il|is maximal, and\nthen repeat this kind of choice on the remaining segments\nof the song, resulting in a greedy algorithm. The only\nexception to that rule is the case in which he ﬁnds that a\nsub-segment of a repetition is repeated more often than the\nwhole segment. He then chooses the Rilpertaining to the\nsub-segment.\n3.4 Using Repetition Cues in Chord Extraction\nWe use structural segmentation to combine several instances\nof a segment type in a song and then infer a single chord\nsequence from the combination.\nThe baseline is an existing chord labelling method [6],\nwhich extracts chords from beat-synchronous treble and\nbass chromagrams. Using a dynamic Bayesian network\n[21] similar to a hierarchical hidden Markov model the net-\nwork jointly models metric position, chords and bass pitch\nclass and infers the most probable sequence from the beat-\nsynchronous chromagrams of the whole song. The method\nmodels four different chord classes: major, minor, dimin-\nished and dominant3.\nIn order to integrate the knowledge of repeating seg-\nments, we split the chromagram for the whole song into\nsmaller chromagram chunks, each belonging to one seg-\nment instance. If a segment type has more than one in-\nstance, all its chromagram chunks are averaged by tak-\ning the mean of the respective elements, thus creating a\nnew chromagram chunk representing all instances of the\nsegment type. The chord extraction is then performed on\nthe newly generated chromagram chunk, and the estimated\nchords are transcribed as if they had been extracted at the\nindividual segment instances.\n4. EXAMPLES\nIn this section we present some example chord transcrip-\ntions with and without the segmentation technique, for the\n2http://www.mathworks.com/access/helpdesk/\nhelp/toolbox/stats/quantile.html\n3strictly speaking: major with a minor seventhfully automatic method. Figure 1 shows a complete song\nsegmentation, and indicates regions where the chord ex-\ntraction was correct with and without the segmentation\ntechnique. Figures 2 and 3 show some excerpts on a\nlarger scale, with the chord estimation detail visible. It\nis clear that the segmentation technique has had a defrag-\nmentation effect on the chord labels. A change in re-\nalisation of a repeated chord sequence between segment\ninstances, such as a difference in melody, has in numer-\nous places caused the standard transcription to incorrectly\nchange chord, but when repeated segments are averaged\nthese inconsistencies are removed. Examples include the\nE:min chord in the third row of Figure 2 and the frag-\nmented F/sharpchords in the third row of Figure 3. This not\nonly improves the chord accuracy (see Section 5), but also\nresults in more natural transcriptions that include repeated\nchord progressions, so could be used to generate compact\nlead-sheets with each segment written exactly once. The\nﬁgures demonstrate how the segmentation technique gen-\nerates chord progressions that are indeed identical for all\ninstances of a given segment type.\nFor a few songs the segmentation caused the chord es-\ntimation accuracy to decrease. Figure 4 shows an excerpt\nfrom A Taste of Honey , a song with one of the greatest re-\nductions in chord accuracy due to segmentation. The tran-\nscription in the second row is good in general, but the long\nF sharp minor chord has been incorrectly labelled as ma-\njor, an error that repeats three times in the song. The ﬁnal\nchord in the song is F sharp major, and the segmentation al-\ngorithm has incorrectly marked this chord as a repetition of\nthe minor chords earlier on. The problem is compounded\nby the behaviour of the automatic beat tracker at the end\nof the song: when the true beats stop, the beat tracker con-\ntinues at a much faster tempo, which has caused the last\nchord to appear to have the same length in beats as the\nmuch longer (in seconds) F sharp minor chords through-\nout the song. This poor case, then, still produces a good\ntranscription but with a parallel major-minor error caused\nin part by the beat tracker giving too much importance to\nthe ﬁnal chord.\n5. QUANTITATIVE RESULTS\nWhile the previous section has demonstrated how segmen-\ntation can help create consistent and more readily read-\nable chord transcriptions, this section examines their over-\nall performance. To that end we compare the six different\ncombinations arising from two different beat annotations\n(manual and automatic) and three different segmentation\nannotations (manual, automatic, and none).\nFor each of the ground truth chords, we make a mu-\nsical judgement regarding whether it should fall into one\nof the chord classes we investigate: major, minor, dimin-\nished, dominant or no chord. If there is no clear suitable\nmapping, for example for an augmented chord, our chord\nestimation will always be treated as incorrect. We use as\nan evaluation measure the relative correct overlap per song\nin physical time against a reference of Harte’s chord tran-\n233Poster Session 2\nA A A B A B A B A B A B A\nA A A B A B A B A B A B A\nA A A B A B A B A B A B AFigure 1 .Dizzy Miss Lizzy (complete). First row: automatic segmentation. Second row: regions of correctly-labelled\nchords using segmentation. Third row: regions of correctly-labelled chords without using segmentation.\nFigure 2 . Extract from Dizzy Miss Lizzy . First row: automatic segmentation. Second row: automatic chord labels using\nsegmentation. Third row: automatic chord labels without using segmentation. Fourth row: hand-annotated chord labels.\nFigure 3 . Extract from Please Mister Postman . First row: automatic segmentation. Second row: automatic chord labels\nusing segmentation. Third row: automatic chord labels without using segmentation. Fourth row: hand-annotated chord\nlabels.\nFigure 4 . Extract from A Taste of Honey . First row: automatic segmentation. Second row: automatic chord labels using\nsegmentation. Third row: automatic chord labels without using segmentation. Fourth row: hand-annotated chord labels.\nscriptions [22], i.e.\nO=summed duration of correct chords\nduration of song. (4)\nA chord is considered correct if its chord type matches that\nof the ground truth chord and its root note matches that of\nthe ground truth or its enharmonic equivalent. In Table 1\nwe report mean overlap scores over the 125 songs. For\ncompleteness we also report the equivalent scores using the\nchord classes used in the MIREX chord detection task [23],\nin which only two chord classes are distinguished. We rec-\nommend that these numbers are used only to assess the ap-\nproximate performance of the algorithm because—as can\nbe seen in Figure 5—the distribution is multimodal with a\nwide spread, due to the large range of difﬁculty between\nsongs. An evaluation method that takes into account these\n“row effects” is the Friedman analysis of variance [24]\nbased on ranking the results per song. The associated p-\nvalue is below double precision, suggesting that at least\none method is signiﬁcantly different from the others. The\nmultiple comparison analysis4in Figure 6 shows that the\n4http://www.mathworks.com/access/helpdesk/\nhelp/toolbox/stats/multcompare.htmlimprovements due to segmentation cues for both manual\nsegmentation and automatic segmentation are signiﬁcant.\nFigure 7 illuminates why this is so: the use of segmentation\ninformation leads to an improved relative overlap score in\nmost of the songs, for example, automatic segmentation\nimproves accuracy on 74% of songs.\nTable 1 shows that the choice of segmentation method\nmakes very little difference to our results, with a much\ngreater difference caused by the beat annotation method.\nSince the automatic beat tracker was adjusted for quick\ntempos, several songs were tracked at double tempo with\nrespect to the manual annotations, so our results suggest\nthat the chord estimation method works better with higher\nbeat granularity.\n6. DISCUSSION\nThe method presented here is not tied to the individual al-\ngorithms. Using other chord extraction or segmentation\nmethods could further improve results and shed more light\non the performance of its constituent parts. As mentioned\nin Section 3.3 we plan to investigate the effects of training\nsome of the segmentation parameters. It would also be in-\n23410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n0 20 40 60 80 1000246810song−wise performance, normal evaluation, auto−auto\nrelative correct overlap in percentnumber of songsFigure 5 . Relative correct overlap for the conﬁgura-\ntion using automatic beats and automatic segmentation:\nHistogram showing song frequencies. The clearly non-\nGaussian distribution suggests that the mean correct over-\nlap should not be the main evaluation technique.\n1.41.61.8 22.22.42.62.8auto/noneauto/autoauto/man.\nThe mean column ranks of groups auto/auto and auto/none are significantly different\nFigure 6 . Multiple comparison test of the three best-\nperforming variants (automatic beat extraction) at a conﬁ-\ndence level of 99%, based on Friedman analysis of vari-\nance. The upper two rows show that of the two meth-\nods using manual (auto/man.) and automatic (auto/auto)\nsegmentation signiﬁcantly outperform the one without\n(auto/none), while the difference between automatic and\nmanual segmentation is not signiﬁcant.\n−10 010020406080100120\npercent pointsauto/none −− auto/autosong number\n−10 010020406080100120\npercent pointsman./none −− man./man.\nFigure 7 . Song-wise improvement in correct relative over-\nlap for the methods using segmentation cues: using au-\ntomatic beats, automatic segmentation improves perfor-\nmance on 74% of songs (left); for manual beats, manual\nsegmentation improves 68% of songs (right).conﬁguration four classes MIREX\nman. beatman. segm. 64.4 71.8\nauto segm. 64.1 71.5\nno segm. 61.7 69.1\nauto beatman. segm. 66.4 73.7\nauto segm. 65.9 73.0\nno segm. 63.4 70.7\nTable 1 . Mean relative overlap in percent and mean rank\nresults. The four classes measure is our preferred mea-\nsure for this task. The MIREX measure gets higher scores,\nsince it maps all chords to two classes, in particular domi-\nnant and major chords are taken to be equivalent.\nteresting to determine whether using the median (instead\nof the mean) to average chromagram chunks would lead to\nimprovements for cases like A Taste of Honey , where one\nmajor chord has tipped the mean to the parallel major.\nThe present work focussed on early rock music. We\nexpect that—given a good segmentation—improvements\nin recognition results could be even greater for jazz: while\nthe extraction of chords in jazz is more difﬁcult than in\nrock music due to improvisation and more complex chord\ntypes, the repetition of segment types is often more rigid.\nThe method to share information globally between seg-\nments we used for this work is a simple one. Integrating\nthis process with the chord extraction itself is a more ele-\ngant solution, but would require structure learning.\n7. CONCLUSIONS\nWe have shown that using knowledge of repeating struc-\nture in a song can improve chord recognition in two ways.\nFirstly, by design the chord estimates are more consis-\ntent between instances of the same segment type, which\nleads to a more natural transcription that could be used\nto generate realistic lead sheets with structure markings.\nSecondly, we have shown that our method of averaging\nthe different instances of each segment type has signif-\nicantly improved the measured chord accuracy. This is\ndemonstrated by examples that show how non-repeating\nincorrect chord fragments are removed by the averaging\nprocess. The improvement is observed both when using\nmanually-annotated beat times and segments, which shows\nthat the principle is valid, and when using a fully-automatic\nmethod, which shows that the principle can be applied to\nreal systems, and is effective even when there are some er-\nrors in the beat or segment labels.\nThe results we have presented support the wider hy-\npothesis that uniﬁed music analysis improves estimation\nof individual features [6–8]. We would like to extend\nthis approach in our future work to allow chord estimation\nto be informed by a complete musical context, including\nmelody, tonality, timbre and metrical structure.\n235Poster Session 2\n8. REFERENCES\n[1] David Huron. Sweet Anticipation: Music and the Psy-\nchology of Expectation . MIT Press, 2006.\n[2] Takuya Fujishima. Real time chord recognition of mu-\nsical sound: a system using Common Lisp Music. In\nProceedings of the International Computer Music Con-\nference (ICMC) , pages 464–467, 1999.\n[3] Christopher Harte and Mark Sandler. Automatic chord\nidentifcation using a quantised chromagram. In Pro-\nceedings of 118th Convention . Audio Engineering So-\nciety, 2005.\n[4] Juan P. Bello and Jeremy Pickens. A Robust Mid-\nlevel Representation for Harmonic Content in Music\nSignals. In Proceedings of the 6th International Con-\nference on Music Information Retrieval, ISMIR 2005,\nLondon, UK , pages 304–311, 2005.\n[5] Kyogu Lee and Malcolm Slaney. Acoustic Chord Tran-\nscription and Key Extraction From Audio Using Key-\nDependent HMMs Trained on Synthesized Audio.\nIEEE Transactions on Audio, Speech, and Language\nProcessing , 16(2):291–301, February 2008.\n[6] Matthias Mauch and Simon Dixon. Simultaneous esti-\nmation of chords and musical context from audio. To\nbe published in IEEE Transactions on Audio, Speech,\nand Language Processing.\n[7] H ´el`ene Papadopoulos and Geoffroy Peeters. Simulta-\nneous estimation of chord progression and downbeats\nfrom an audio ﬁle. In Proceedings of the 2008 ICASSP\nConference , pages 121–124, 2008.\n[8] Roger B. Dannenberg. Toward automated holistic beat\ntracking, music analysis, and understanding. In Pro-\nceedings of the 6th International Conference on Music\nInformation Retrieval , London, 2005.\n[9] S. Abdallah, K. Noland, M. Sandler, M. Casey, and\nC. Rhodes. Theory and evaluation of a Bayesian mu-\nsic structure extractor. In Proceedings of the 6th Inter-\nnational Conference on Music Information Retrieval,\nISMIR 2005, London, UK , pages 420–425, 2005.\n[10] G. Peeters, A. La Burthe, and Xavier Rodet. Toward\nautomatic music audio summary generation from sig-\nnal analysis. In Proceedings of the 3rd International\nConference on Music Information Retrieval , Paris,\n2002.\n[11] Namunu C. Maddage. Automatic structure detection\nfor popular music. IEEE Multimedia , 13(1):65–77,\n2006.\n[12] Meinard M ¨uller and Frank Kurth. Towards structural\nanalysis of audio recordings in the presence of musical\nvariations. EURASIP Journal on Advances in Signal\nProcessing , 2007.[13] Masataka Goto. A chorus-section detecting method\nfor musical audio signals. In Proceedings of the 2003\nIEEE Conference on Acoustics, Speech and Signal Pro-\ncessing , pages 437–440, 2003.\n[14] Jonathan Foote. Visualizing music and audio using\nself-similarity. In Proceedings of the 7th ACM Interna-\ntional Conference on Multimedia (Part 1) , pages 77–\n80, 1999.\n[15] Mark A. Bartsch and Gregory H. Wakeﬁeld. Audio\nthumbnailing of popular music using chroma-based\nrepresentations. IEEE Transactions on Multimedia ,\n7(4), February 2005.\n[16] Bee Suan Ong. Structural Analysis and Segmenta-\ntion of Music Signals . PhD thesis, Universitat Pompeu\nFabra, 2006.\n[17] Christophe Rhodes and Michael Casey. Algorithms\nfor determining and labelling approximate hierarchi-\ncal self-similarity. In Proceedings of the 2007 ISMIR\nConference, Vienna, Austria , pages 41–46, 2007.\n[18] Matthias Mauch. A chroma extraction method and\na harmonic change detection function. Technical\nreport, Queen Mary, University of London. Available\nat http://www.elec.qmul.ac.uk/digitalmusic/\npapers/2009/Mauch09-C4DM-TR-09-05.pdf .\n[19] Matthew Davies. Towards Automatic Rhythmic Accom-\npaniment . PhD thesis, Queen Mary University of Lon-\ndon, London, UK, August 2007.\n[20] Alan W. Pollack. Notes on... series, 1995. Available at\nhttp://www.recmusicbeatles.com .\n[21] Kevin P Murphy. Dynamic Bayesian Networks: Repre-\nsentation, Inference and Learning . PhD thesis, Univer-\nsity of California, Berkeley, 2002.\n[22] Christopher Harte, Mark Sandler, Samer A. Abdallah,\nand Emilia Gomez. Symbolic representation of musi-\ncal chords: A proposed syntax for text annotations.\nInProceedings of the 6th International Conference\non Music Information Retrieval, ISMIR 2005, London,\nUK, pages 66–71, 2005.\n[23] MIREX audio chord detection subtask, music in-\nformation retrieval evaluation exchange, 2008.\nhttp://www.music-ir.org/mirex/2008/index.php/\nAudio_Chord_Detection .\n[24] David Hull. Using statistical testing in the evaluation\nof retrieval experiments. In Proceedings of the 16th an-\nnual international ACM SIGIR conference on Research\nand development in information retrieval , pages 329–\n338. ACM New York, USA, 1993.\n236"
    },
    {
        "title": "Hierarchical Sequential Memory for Music: A Cognitive Model.",
        "author": [
            "James B. Maxwell",
            "Philippe Pasquier",
            "Arne Eigenfeldt"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414850",
        "url": "https://doi.org/10.5281/zenodo.1414850",
        "ee": "https://zenodo.org/records/1414850/files/MaxwellPE09.pdf",
        "abstract": "We propose a new machine-learning framework called the  Hierarchical  Sequential  Memory  for  Music,  or HSMM. The HSMM is an adaptation of the Hierarchical Temporal Memory (HTM) framework, designed to make it better suited to musical applications. The HSMM is an online learner, capable of recognition, generation, continuation, and completion of musical structures.",
        "zenodo_id": 1414850,
        "dblp_key": "conf/ismir/MaxwellPE09",
        "keywords": [
            "machine-learning",
            "framework",
            "Hierarchical Sequential Memory",
            "Music",
            "HTM",
            "adaptation",
            "online learner",
            "recognition",
            "generation",
            "continuation"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nHierarchical Sequential Memory for Music: A Cognitive Model\nJames B. MaxwellPhilippe PasquierArne Eigenfeldt\nSimon Fraser University\njbmaxwel@sfu.caSimon Fraser University\npasquier@sfu.caSimon Fraser University\narne_e@sfu.ca\nABSTRACT\nWe propose a new machine-learning framework called \nthe Hierarchical Sequential Memory for Music, or \nHSMM. The HSMM is an adaptation of the Hierarchical \nTemporal Memory (HTM) framework, designed to make \nit better suited to musical applications. The HSMM is an \nonline learner, capable of recognition, generation, con-\ntinuation, and completion of musical structures.\n1.INTRODUCTION\nIn our previous work on the MusicDB [10] we outlined a \nsystem inspired by David Cope's notion of “music recom-\nbinance” [1]. The design used Cope's “SPEAC” system \nof structural analysis [1] to build hierarchies of musical \nobjects. It was similar to existing music representation \nmodels [7, 9, 13], in that it emphasized the construction \nof hierarchies in which the objects at each consecutively \nhigher level demonstrated increasing “temporal invari-\nance” [5]—i.e., an “S” phrase in SPEAC analysis, and a \n\"head\" in the Generative Theory of Tonal Music [9], both \nuse singular names at higher levels to represent se-\nquences of musical events at lower levels.\nOther approaches to learning musical structure include \nneural network models [8], recurrent neural network \nmodels (RNNs) [11], RNNs with Long Short-Term \nMemory [3], Markov-based models [12, 14], and statist-\nical models [2]. Many of these approaches have achieved \nhigh degrees of success, particularly in modeling melodic \nand/or homophonic music. With the HSMM we hope to \nextend such approaches by enabling a single system to \nrepresent melody, harmony, homophony, and various \ncontrapuntal formations, with little or no explicit a priori \nmodeling of musical \"rules\"—the HSMM will learn only \nby observing musical input. Further, because the HSMM \nis a cognitive model, it can be used to exploit musical \nknowledge, in real time, in a variety of interesting and in-\nteractive ways.\n2.BACKGROUND: THE HTM FRAMEWORK\nIn his book “On Intelligence”, Jeff Hawkins proposes a \n“top-down” model of the human neocortex, called the \n“Memory Prediction Framework” (MPF) [6]. The model \nis founded on the notion that intelligence arises through \nthe interaction of perceptions and predictions; the percep-\ntion of sensory phenomena leads to the formation of pre-\ndictions, which in turn guide action. When predictions \nfail to match learned expectations, new predictions are \nformed, resulting in revised action. The MPF, as realized \ncomputationally in the HTM [4, 5], operates under the as-sumption of two fundamental ideas: 1) that memories are \nhierarchically structured, and 2) that higher levels of this \nstructure show increasing temporal invariance.\nThe HTM is a type of Bayesian network, and is best \ndescribed as a memory system that can be used to discov-\ner or infer “causes” in the world, to make predictions, and \nto direct action. Each node has two main processing mod-\nules, a Spatial Pooler (SP) for storing unique “spatial pat-\nterns” (discrete data representations expressed as single \nvectors) and a Temporal Pooler (TP) for storing temporal \ngroupings of such patterns.\nThe processing in an HTM occurs in two phases: a \n“bottom-up” classification phase, and a “top-down” re-\ncognition, prediction, and/or generation phase. Learning \nis a bottom-up process, involving the storage of discrete \nvector representations in the SP, and the clustering of \nsuch vectors into “temporal groups” [4], or variable-order \nMarkov chains, in the TP. A node's learned Markov \nchains thus represent temporal structure in the training \ndata. As information flows up the hierarchy, beliefs about \nthe identity of the discrete input representations are \nformed in each node's SP, and beliefs about the member-\nship of those representations in each of the stored Markov \nchains are formed in the TP. Since the model is hierarch-\nical, higher-level nodes store invariant representations of \nlower-level states, leading to the formation of high-level \nspatio-temporal abstractions, or “concepts.”\nA simplified representation of HTM processing is giv-\nen in Figure 1. Here we see a 2-level hierarchy with two \nnodes at L1 and one node at L2. This HTM has already \nreceived some training, so that each L1 node has stored \nfour spatial patterns and two Markov chains, while the L2 \nnode has stored three spatial patterns and two Markov \nchains. There are two input patterns, p1 and p2. It can be \nseen that p1 corresponds to pattern 4 of Node 1, and that \npattern 4 of Node 1 is a member of Markov chain b. \nWhen presented with p1, the node identifies pattern 4 as Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page.\n© 2009 International Society for Music Information Retrieval           \nFigure 1. Simplified HTM processing.\n429Poster Session 3\nthe stored pattern most similar to p1, calculates the mem-\nbership of pattern 4 in each of the stored Markov chains, \nand outputs the vector [0, 1], indicating the absence of \nbelief that p1 is a member of Markov chain a, and the cer-\ntainty that p1 is a member of Markov chain b.\nIt can also be seen from Figure 1 that the outputs of \nthe children in hierarchy are concatenated to form the in-\nputs to the parent. The SP of node 3 thus treats the con-\ncatenated outputs of nodes 1 and 2 as a discrete repres-\nentation of their temporal state at a given moment—i.e., \ntime is ‘frozen’ by the parent node's SP. Node 3 then \nhandles its internal processing in essentially the same \nmanner as nodes 1 and 2. \nThe dotted lines indicate the top-down processes by \nwhich discrete state representations can be extracted or \ninferred from the stored Markov chains, and passed down \nthe network. Top-down processing can be used to support \nthe recognition of inputs patterns, to make predictions, or \nto generate output.\n3.MOTIVATIONS BEHIND THE HSMM\nOur interest in the HTM as a model for representing mu-\nsical knowledge derives from its potential to build spatio-\ntemporal hierarchies. The current HTM implementation \nfrom Numenta Inc., however, is focused primarily on \nvisual pattern recognition [4, 5], and is currently incap-\nable of learning the sort of high-level temporal structure \nfound in music. This structure depends not only on the \ntemporal proximity of input patterns, but also on the spe-\ncific sequential order in which those patterns arrive. The \nHSMM treats sequential order explicitly, and can thus \nbuild detailed temporal hierarchies.\nAnother motivation behind the HSMM lies in the fact \nthat the HTM is strictly an offline learner. For composi-\ntional applications, we are interested in a system that can \nacquire new knowledge during interaction, and exploit \nthat knowledge in the compositional process. We have \nthus designed the HSMM with four primary functions in \nmind:\n1)Recognition: The system should have a represent-\nation of the hierarchical structure of the music at \nany given time in a performance.\n2)Generation: The system should be capable of \ngenerating stylistically integrated musical output.\n3)Continuation: If a performance is stopped at a \ngiven point, the system should continue in a styl-\nistically appropriate manner.\n4)Pattern Completion: Given a degraded, or partial \ninput representation, the system should provide a \nplausible completion of that input (i.e., by adding \na missing note to a chord).\n4.MUSIC REPRESENTATION\nFor the current study, we are working with standard MIDI \nfiles from which note data is extracted and formatted into \nthree 10-member vectors: one for pitch data, one for \nrhythmic data, and one for velocity data. The incoming \nmusic is first pooled into structures similar to Cope’s \n“Groups” [1]—vertical ‘slices’ of music containing the \ntotal set of unique pitches at a given moment. A new \nGroup is created every time the harmonic structure \nchanges, as shown in Figure 2. The Groups are prepro-\ncessed using a simple voice-separation algorithm, which divides polyphonic material across the 10 available \nvoices in the vector representation. Group pitches are first \nsorted in ascending order, after which the voice separa-\ntion routine follows one simple rule: tied (sustained) \nnotes must not switch voices.\nPitch material is represented using inter-pitch ratio \n[16], calculated by converting the MIDI notes to hertz, \nand dividing the pitch at time t-1 by the pitch at time t. In \norder to avoid misleading values, as a result of calculat-\ning the ratio between a rest (pitch value 0.0) and a pitch, \nrests are omitted from the pitch representation, and the \nvelocity representation is used to indicate when notes are \nactive or inactive (see Figure 2).\nIt will be noted that velocity is not given using con-\nventional MIDI values, but is rather used as a flag to in-\ndicate the state of a given voice in the Group. Positive \nvalues indicate onsets, negative values indicate sustained \nnotes, and zeros indicate offsets. We have simplified the \nnon-zero values to 1 and -1 in order to avoid attributing \ntoo much weight to note velocity in the training and in-\nference process. \nThe rhythmic values used represent the times at which \neach voice undergoes a transition either from one pitch to \nanother, or from a note-on to a note-off. We use the inter-\nevent time between such changes, and calculate the ratio \nbetween consecutive inter-event times, for each voice n, \naccording to the following:\ninterEventRatiot[n]=interEventTimet−1[n]\ninterEventTimet[n] (1)\nThe final representation for the HSMM will thus consist \nof one 10-member inter-pitch ratio vector, one 10-mem-\nber inter-event time ratio vector, and one 10-member ve-\nlocity flag vector.\n5.HSMM LEARNING AND INFERENCE\nFigure 3 shows a four-level HSMM hierarchy with inputs \nfor pitch, rhythm, and velocity information, an “associ-\nation” node (L2) for making correlations between the L1 \noutputs, and two upper-level nodes for learning higher-\nordered temporal structure. The association node at L2 \nprovides the necessary connectivity between pitch, \nrhythm, and velocity elements required for the identifica-\ntion of musical “motives.” The upper-level nodes at L3 \nand L4 are used to learn high-order musical structure \nfrom the motives learned at L2.Figure 2. Music representation for the HSMM.\n43010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n5.1  Online Learning in the HSMM\nWhereas the TP in the HTM builds Markov chains during \nits training phase, in the HSMM we focus simply on con-\nstructing discrete sequences from the series of patterns \ninput to the node. As in the HTM, the patterns stored in \nthe SP will be referred to as “coincidences.” The se-\nquences stored by the TP will be referred to simply as \n“sequences.”\n5.1.1 Learning and Inference in the Spatial Pooler\nThe objective of SP learning is to store unique input pat-\nterns as coincidences, while the objective of SP inference \nis to classify input patterns according to the stored coin-\ncidences. The algorithm used is given in Figure 4. As in \nthe HTM, when a new input is received the SP checks the \ninput against all stored coincidences, C. The result is an \nSP output vector yt, calculated according to:\nyt[i]=e−dp,C[i]2/2, for i=0 to ∣C∣−1 (2)\nwhere d(p,C[i]) is the Euclidean distance from input p to \ncoincidence C[i], and σ is a constant of the SP. The con-\nstant σ is used to account for noise in the input, and is \nuseful for handling rhythm vectors, where frequent fluc-\ntuations of timing accuracy are expected. The output yt is \na belief distribution over coincidences, in which a higher \nvalue indicates greater similarity between input pattern p \nand stored coincidence C[i], and thus greater evidence \nthat p should be classified as C[i]. If the similarity of p to \nall stored coincidences is less than the minumum allowed \nsimilarity, simThreshold, p is added as a new coincidence. \nIn the event that a new coincidence is added, the al-\ngorithm uses the value of maxSimilarity—i.e., the belief \nin the coincidence most similar to input p—as the initial \nbelief value when adding the new coincidence. It then \nnormalizes yt in order to scale the new belief according to \nthe belief over all stored coincidences. In order to decide \nwhether a new coincidence is required at higher levels, \nwe start by first determining whether the input pattern λ \n(see Figure 1) should be stored as a new coincidence. \nThis is simply a matter of checking the length of the λ \nvector at time t against the length at time t-1. If the length \nhas increased, we know that at least one of the children \nhas learned a new representation in the current time step, \nand that a new coincidence must be added in order to ac-\ncount for the additional information. For each new coin-\ncidence stored by the SP, a histogram called the counts \nvector is updated. In the HTM, the update is an integer \nincrementation—a count of how many times the coincid-\nence has been seen during training. However, because the HSMM is an online learner, an integer incrementation is \nnot appropriate, as it would lead to counts of vanishingly \nsmall proportions being assigned to new coincidences if \nthe system were left running for long periods of time. \nThus, in the HSMM, the counts vector is updated accord-\ning to the following:\ninc=∣C∣×0.01(3)\ncountst[topCoinct]=countst−1[topCoinct]inc (4)\ncountst[i]=countst[i]\n1inc,fori=0to∣counts∣−1 (5)\nwhere C is the number of learned coincidences, inc is the \nincrementation value, and topCoinct is the coincidence \nthat rated as having the maxSimilarity (Figure 4) to the \ninput. Because counts is regularly normalized, it repres-\nents a time-limited histogram in the HSMM.\nSP inference above L1 is calculated as in the HTM, but \nwe outline it here for the sake of clarity. At higher levels \nwe want to calculate the probability that the new input λ \nshould be classified as one of the stored coincidences. \nWhen the node has more than one child, we consider \neach child’s contribution to the overall probability separ-\nately:\nC=C1∪...∪CM\n=1∪...∪M\nyt[i]=∏j=1M\nmaxkCj[k,i]×j[k], for i=0 to ∣C∣−1 (6)\nwhere M is the number of child nodes, Cj is the portion \nof coincidence vector k attributed1 to child j, and λj is the \nportion of λ attributed to child j. Figure 5 shows an ex-\nample calculation for a hypothetical SP with two stored \ncoincidences.\npThe current input pattern\nCThe table of stored coincidences\nmaxSimilarityThe maximum similarity value found\nsimThresholdThe minimum degree of similarity between input p and coincidence C[i] required for p to be classified as C[i]\nunmatchedCoincA count of the number of times input p was \nfound to be insufficiently similar to all co-\nincidences in C\n    Set maxSimilarity to 0\n    For each stored coincidence C[i] in C\n      Calculate yt[i], given input p, according to Equation 2\n      If yt[i] > maxSimilarity\n        Set maxSimilarity to yt[i]\n      If yt[i] < simThreshold\n        Increment unmatchedCoinc count\n    If unmatchedCoinc count = size of C\n      add input p to stored coincidences C\n      append maxSimilarity to end of yt vector\n      normalize yt vector \n    Figure 4. Online SP learning and inference.\n1Recall that when the node has more than one child, each \ncoincidence will be a concatenation of child outputs.        \nFigure 3. A four-level HSMM hierarchy.\n431Poster Session 3\n5.1.2 Learning in the Temporal Pooler\nThe objective of TP learning is to construct sequences \nfrom the series of belief vectors (y) received from the SP. \nWhen a new input to the TP is received, the TP first cal-\nculates the winning coincidence of yt:\ntopCoinct=argmaxiyt[i] (7)\nIt then determines whether this coincidence has changed \nsince the previous time step—i.e., whether topCoinct \nequals topCoinct-1—and stores the result in a flag called \nchange.\nThe next step is to determine whether the transition \nfrom topCoinct-1→ topCoinct exists among the TP’s \nstored sequences. To do this, we depart from the HTM \nentirely, and use an algorithm we refer to as the Sequen-\ncer algorithm. In the Sequencer algorithm, we consider \ntwo aspects of the relationship between topCoinct and a \ngiven stored sequence, Seqn: 1) the position of topCoinct \nin Seqn (zero if topCoinct ∉ Seqn), referred to as the “se-\nquencer state”, and 2) the cumulative slope formed by the \nhistory of sequencer states for Seqn. Thus, if Seqn is four \ncoincidences in length, and each successive topCoinct \nmatches each coincidence in Seqn, then the history of se-\nquencer states will be the series: {1, 2, 3, 4}, with each \ntransition having a slope of 1.0. We use a vector called \nseqState to store the sequencer states, and a vector called \nseqSlope to store the cumulative slope for each sequence, \nformed by the history of sequencer states. The slope is \ncalculated as follows:\nseqSlopet[i]=1\nseqStatet[i]−seqStatet−1[i] (8)\nseqSlopet[i]=\n{seqSlopet−1[i]−seqSlopet[i],i=1.0\nseqSlopet−1[i]−∣seqSlopet[i]∣,i≠1.0 (9)\nseqSlopet[i]=2\n1e−seqSlopet[i]−1 (10)\nwhere seqStatet[i] indicates the position of topCoinct in \nsequence i (zero if non-member). The sigmoid scaling \nperformed in Equation 10 helps to constrain the cumulat-\nive slope values. Figure 6 shows an example of using cu-\nmulative sequence slopes to reveal the best sequence. \nAt levels above L1, we only update the seqSlope vec-\ntor when change = 1, in order to help the TP learn at a \ntime scale appropriate to its level in the hierarchy. A node \nparameter, slopeThresh, is used to determine the minim-\num slope required for the TP to pass onto the inference \nstage without adding a new sequence or extending an ex-\nisting sequence. If the maximum value in seqSlope does \nnot exceed the value of slopeThresh, then either a new se-\nquence is created, or an existing sequence extended.\nGenerally, we allow only one occurrence of any given \ncoincidence in a single sequence at all levels above L1, \nthough any number of sequences may share that coincid-\nence. This is done to avoid building long sequences at the \nbottom of the hierarchy, thus dividing the construction of \nlonger sequences across the different levels. We allow \nconsecutive repetitions of coincidences at L1, but do not \nallow non-consecutive repetitions. This is a musical con-\nsideration, given the frequent use of repetitions in music-\nal language.5.1.3 Inference in the Temporal Pooler\nThe objective of TP inference is to determine the likeli-\nhood that topCoinct is a member of a given stored se-\nquence. At each time step, the TP uses the counts vector, \nfrom the SP, to update a Conditional Probability Table, \ncalled the weights matrix, which indicates the probability \nof a specific coincidence occurring in a given sequence. \nThe weights matrix is calculated as:\nweights[i,j]=counts[j]×Ii,j/∑i=1k\ncounts[k]×Ii,j (11)\n     Ii,j=\n{1,C[j]∈S[i]\n0,C[j]∉S[i]\nwhere C[j] is the jth stored coincidence and S[i] is the ith \nstored sequence.\nThe probabilities stored by the weights matrix are used \nduring TP inference, and also when forming top-down \nbeliefs in the hierarchy, as introduced in Section 2. It is a \nrow-normalized matrix where rows represent sequences \nand columns represent coincidences. Because the counts \nvector maintains its histogram of topCoinct occurrences \nover a limited temporal window, the weights matrix in the \nHSMM is able to act as a form of short-term memory for \nthe node. \nThe output of TP inference is the bottom-up belief \nvector z, which indicates the degree of membership of \ntopCoinct in each of the stored sequences. The argmax of \nz thus identifies the sequence most strongly believed to \nbe active, given topCoinct. To calculate z, we use a vari-\nant of the “sumProp” and “maxProp” algorithms used in \nthe HTM [6], which we refer to as pMaxProp. The al-\ngorithm uses the weights matrix to calculate a belief dis-\ntribution over sequences, as follows:\nz[i]=maxj=1i\nweights[i,j]×y[j] (12)\nAn example run of the pMaxProp algorithm is given in \nFigure 7, using the coincidences and sequences from Fig-\nure 6. Because the weights matrix in the HSMM is a    Figure 6. Using seqSlope to find the best sequence.\n              \n                \n   Figure 5. SP inference calculations above L1.\n43210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nshort-term memory, and the pMaxProp algorithm is a \n“one-shot” inference, with no consideration of the previ-\nous time step, we combine the results of pMaxProp with \nthe results of the Sequencer algorithm, to yield the final \nbottom-up belief vector:\nzt[i]=zt[i]seqSlopet[i]\n2 (13)\n5.2  Belief Formation in an HSMM Node\nThe final belief vector to be calculated, a belief distribu-\ntion over coincidences called BelC, represents the combin-\nation of the node's classification of a given input, and its \nprediction regarding that input in the current temporal \ncontext. Thus, for every bottom-up input there is a top-\ndown, feedback response. Bottom-up vector representa-\ntions passing between nodes are denoted with λ, while \ntop-down, feedback representations are denoted with π2. \nA schematic of node processing can be seen in Figure 8. \nThe top-down, feedback calculations used in the HSMM \nare the same as those used in the HTM, but we outline \nthem here for completeness.\nThe first step in processing the top-down message is to \ndivide the top-down parent belief π by the node's bottom-\nup belief λ (at the top of the hierarchy, the bottom-up be-\nlief z is used for the top-down calculations):\n'[i]=[i]/[i] (14)\nNext, the π’ vector is used to calculate the top-down be-\nlief distribution over stored coincidences as:\ny[i]=maxSeqi∈SweightsT[i,j]×'[j]\n for i=0 to ∣C∣−1 (15)\nwhere weightsT[i,j] is the transposed weights matrix, and \ny↓ is the top-down belief over coincidences, and S is the \ntable of stored sequences. Figure 9 gives an example, as-\nsuming the coincidences and sequences from Figure 7. \nThe BelC vector is then calculated as the product of the \ntop-down (y↓) and bottom-up (y↑) belief distributions over \ncoincidences:\nBelC[i]=y[i]×y[i] (16)\nThis calculation ensures that the belief of the node is al-\nways based on the available evidence both from above \nand below the node’s position in the hierarchy. At all \nlevels above L1, the top-down output of the node (the \nmessage sent to the children) is calculated using the BelC \nvector and the table of stored coincidences C:\n[i]=argmaxC[i]∈CC[i]×BelC[j]\nfor i=0 to ∣C∣−1 (17)\n2At the node level, the λ and z vectors are equivalent. The \nnaming is intended to distinguish the between-node pro-\ncesses from the within-node processes. This calculation ensures that each child portion of the \ntop-down output is proportional to the belief in the node. \nIn cases where the parent node has two or more children, \nthe π vector is divided into segments of length equal to \nthe length of each child’s λ vector (i.e., reversing the con-\ncatenation of child messages used during bottom-up pro-\ncessing). The various stages of top-down processing are \nillustrated on the right side of Figure 8.\nOne extra step, in support of TP inference in the \nHSMM, is added that is not present in the HTM. In ac-\ncordance with the ideas of the MPF, it seemed intuitively \nclear to us that predictions could be used locally in the TP \nto support the learning process by disambiguating SP in-\nputs whenever possible. With this in mind we added a \ncalculation to the TP inference algorithm that biases the \nSP belief vector, y, according to the state of the change \nflag, and the current top-down inference over sequences. \nIn cases where the sequence inferred by top-down pro-\ncessing at time t-1 contains topCoinct-1, and change = 0, \nthe belief value for  topCoinct-1 is strengthened. However, \nwhen change = 1, belief in the next coincidence in the in-\nferred sequence is strengthened. The algorithm is given in \nFigure 10. Thus, when the state of the node appears to be \nchanging, belief is biased slightly toward what is most \nlikely to occur, whereas when the state appears to be \nstable, the most recent belief is assumed to be correct.\n6.DISCUSSION AND CONCLUSION\nThe strength of the HSMM lies in its balancing of hier-\narchical interdependence with node-level independence. \nEach node learns in the context of the hierarchy as a \nwhole, but also forms its own representations and beliefs \nover a particular level of musical structure. At L1, simple \nmotivic patterns can be recognized and/or generated, and \nat the higher levels, larger musical structures like phrases, \nmelodies, and sections can also be learned, classified, and               \nFigure 9. Using the weights matrix to calculate the \ntop-down belief over coincidences.    Figure 7. The pMaxProp algorithm calculations.\n    Figure 8. HSMM node processing.\n.\n433Poster Session 3\ngenerated. Further, since nodes above L1 all process in-\nformation in an identical manner, and only require a \nsingle child, additional high-level nodes could be added, \nenabling the learning of higher levels of formal structure\n—songs, movements, compositions. Each node can be \nmonitored independently, and its state exploited for com-\npositional, analytical, or musicological purposes. Com-\nposition tools could be developed, offering various levels \nof interactivity, while maintaining stylistic continuity \nwith the user's musical language. In the area of classic \nMusic Information Retrieval, low levels of the HSMM \ncould be used to identify common motivic gestures \namong a given set of works, while higher levels could be \nused to recognize the music of individual composers, or \nto cluster a number of works by stylistic similarity. \ntopSeqt-1The sequence inferred by top-down processing\npredCoincThe predicted coincidence\n    For each coincidence c in topSeqt-1\n       If topSeqt-1[c] equals topCoinct-1\n          Set predCoinc to topSeqt-1[c+1]\n    If change = 0\n       y[topCoinct-1] = y[topCoinct-1] * 1.1\n    else if change = 1\n      y[predCoinc] = y[predCoinc] * 1.1\n    Figure 10. Biasing the predicted coincidence.\nThe HSMM exploits short-term and long-term \nmemory structures, and uses an explicit sequencing mod-\nel to build its temporal hierarchies, thus giving it the ca-\npacity to learn high-level temporal structure without the \ntree-like topologies required by HTM networks.\nTremendous progress has been made in the cognitive \nsciences and cognitive modeling, but such work has re-\nmained largely unexplored by the computer music com-\nmunity, which has focused more on pure computer sci-\nence and signal processing. The HSMM offers a first step \ntoward the development and exploitation of a realistic \ncognitive model for the representation of musical know-\nledge, and opens up a myriad of areas for exploration \nwith regard to the associated cognitive behavior.\n7.FUTURE WORK\nA working prototype of the HSMM has been implemen-\nted, and initial tests have shown great promise. A future \npaper will cover the evaluation in detail, with an emphas-\nis on exploiting the strengths offered by the cognitive \nmodel.\nWe are interested in exploring alternative distance \nmetrics for the L1 nodes—particularly the pitch and \nrhythm nodes, where more musically-grounded metrics \nmay be effective. We are also interested in exploring dif-\nferent topologies for the hierarchy, in particular, topolo-\ngies that isolate individual voices and allow the system to \nlearn both independent monophonic hierarchies and asso-\nciative polyphonic hierarchies. Along similar lines, we \nwould like to explore the possibilities offered by state-\nbased gating of individual nodes in more complex hier-\narchies, in order to simulate the cognitive phenomenon of \nattention direction. 8.REFERENCES\n[1]D. Cope: Computer Models of Musical Creativity. 87-123, \n226-242, MIT Press, Cambridge, MA, 2005.\n[2]S. Dubnov, G. Assayag, and O. Lartillot: “Using Machine-\nLearning Methods for Musical Style Modelling,” \nComputer, 36/10, 2003.\n[3]D. Eck and J. Schmidhuber: “Learning the Long-Term \nStructure of the Blues,” Lecture Notes in Computer \nScience, Vol. 2415, Springer, Berlin, 2002.\n[4]D. George. “How the Brain Might Work: A Hierarchical \nand Temporal Model for Learning and Recognition,” PhD \nThesis, Stanford University, Palo Alto, CA, 2008.\n[5]D. George and J. Hawkins: “A Hierarchical Bayesian \nModel of Invariant Pattern Recognition in the Visual \nCortex,” Redwood Neuroscience Institute, Menlo Park, \nCA, 2004.\n[6]J. Hawkins and S. Blakeslee: On Intelligence, Times \nBooks, New York, NY, 2004.\n[7]K. Hirata and T. Aoyagi: “Computational Music \nRepresentation Based on the Generative Theory of Tonal \nMusic and the Deductive Object-Oriented Database,” \nComputer Music Journal, 27/3, 2003.\n[8]D. Hörnel and W. Menzel: “Learning Music Structure and \nStyle with Neural Networks,” Computer Music Journal, \n22/4, 1998.\n[9]F. Lerdhal and R. Jackendoff: A Generative Theory of \nTonal Music, MIT Press, Cambridge, MA, 1983.\n[10]J. Maxwell and A. Eigenfeldt: “The MusicDB: A Music \nDatabase Query System for Recombinance-based \nComposition in Max/MSP,” Proceedings of the 2008 \nInternational Computer Music Conference, Belfast, \nIreland, 2008.\n[11]M.C. Mozer: “Neural Network Music Composition by \nPrediction: Exploring the Benefits of Psychoacoustic \nConstraints and Multi-scale Processing,” Connection \nScience, 6/2-3, 1994.\n[12]E. Pollastri and G. Simoncelli: “Classification of Melodies \nby Composer with Hidden Markov Models,” Proceedings \nof the First International Conference on WEB Delivering \nof Music, 2001.\n[13]Y. Uwabu, H. Katayose and S. Inokuchi: “A Structural \nAnalysis Tool for Expressive Performance,” Proceedings \nof the International Computer Music Conference, San \nFransisco, 1997.\n[14]K. Verburgt, M Dinolfo and M. Fayer: “Extracting Patterns \nin Music for Composition via Markov Chains,” Lecture \nNotes in Computer Science, Springer, Berlin, 2004\n[15]G. Wilder: “Adaptive Melodic Segmentation and Motivic \nIdentification,” Proceedings of the International Computer \nMusic Conference, Belfast, Ireland, 2008.\n434"
    },
    {
        "title": "Heterogeneous Embedding for Subjective Artist Similarity.",
        "author": [
            "Brian McFee",
            "Gert R. G. Lanckriet"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416284",
        "url": "https://doi.org/10.5281/zenodo.1416284",
        "ee": "https://zenodo.org/records/1416284/files/McFeeL09.pdf",
        "abstract": "We describe an artist recommendation system which integrates several heterogeneous data sources to form a holistic similarity space. Using social, semantic, and acoustic features, we learn a low-dimensional feature transformation which is optimized to reproduce human-derived measurements of subjective similarity between artists. By producing low-dimensional representations of artists, our system is suitable for visualization and recommendation tasks.",
        "zenodo_id": 1416284,
        "dblp_key": "conf/ismir/McFeeL09",
        "keywords": [
            "heterogeneous",
            "holistic",
            "similarity space",
            "social",
            "semantic",
            "acoustic",
            "feature transformation",
            "human-derived",
            "visualization",
            "recommendation tasks"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nHETEROGENEOUS EMBEDDING FOR SUBJECTIVE ARTIST\nSIMILARITY\nBrian McFee\nComputer Science and Engineering\nUniversity of California, San Diego\nbmcfee@cs.ucsd.eduGert Lanckriet\nElectrical and Computer Engineering\nUniversity of California, San Diego\ngert@ece.ucsd.edu\nABSTRACT\nWe describe an artist recommendation system which inte-\ngrates several heterogeneous data sources to form a holistic\nsimilarity space. Using social, semantic, and acoustic fea-\ntures, we learn a low-dimensional feature transformation\nwhich is optimized to reproduce human-derived measure-\nments of subjective similarity between artists. By produc-\ning low-dimensional representations of artists, our system\nis suitable for visualization and recommendation tasks.\n1. INTRODUCTION\nA proper notion of similarity can dramatically impact per-\nformance in a variety of music applications, such as search\nand retrieval, content-based tagging engines, and song or\nartist recommendation. When designing such a system,\npractitioners must choose an appropriate measure of sim-\nilarity for the task at hand. Often, this involves selecting\namong multiple heterogeneous feature types, which may\nnot be directly comparable, e.g., social network connec-\ntivity and probabilistic models of keywords. Integration\nof diverse features must be conducted carefully to ensure\nthat the resulting similarity measure sufﬁciently captures\nthe qualities desired for the application.\nIn music applications, the problem of selecting an opti-\nmal similarity measure is exacerbated by subjectivity : peo-\nple may not consistently agree upon whether or to what\ndegree a pair of songs or artists are similar. Even more ﬂex-\nible notions of similarity, such as ranking, may suffer from\nthe effects of inconsistency, which must be understood and\ncounteracted.\nIn this work, our goal is to construct artist-level similar-\nity measures, adhering to two key principles. First, a simi-\nlarity measure should integrate heterogeneous features in a\nprincipled way, emphasizing relevant features while being\nrobust against irrelevant features. Second, instead of rely-\ning solely on features, the measure should learn from peo-\nple and be optimized for the task at hand, i.e., predicting\nhuman perception of similarity. Using recently developed\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.algorithms, we demonstrate how to learn optimal metrics\nfor subjective similarity while seamlessly integrating mul-\ntiple feature modalities. We do not mean to imply that\nthere exists a fully consistent ground truth in musical sim-\nilarity. Rather, we seek to construct similarity measures\nwhich are maximally consistent with human perception.\n1.1 Related work\nThere has been a considerable amount of research devoted\nto the topic of musical similarity, primarily in the realms\nof playlist generation and recommendation [3,14,18]. The\npresent work is perhaps most similar to that of Slaney,\net al. [21], in which convex optimization techniques were\napplied to learn metric embeddings optimized according\nto side information. Our work differs in that we focus on\nartist similarity, rather than classiﬁcation, and we use direct\nmeasurements of human perception to guide the optimiza-\ntion.\nBarrington, et al. applied multiple-kernel learning to a\nclassiﬁcation task [4]. Our approach uses a different for-\nmulation of multiple-kernel learning which allows greater\nﬂexibility in assigning weights to the features and training\nset, and produces a metric space instead of a linear separa-\ntor.\nEllis, et al. and Berenzweig, et al. studied the issue of\nconsistency in human perception of artist similarity, and\nevaluated several acoustic- and socially-driven similarity\nmeasures against human survey data [5, 9]. Their work fo-\ncused on the comparison of existing measures of similarity\n(e.g., playlist co-occurrence), rather than learning an opti-\nmal measure.\n2. EMBEDDING ALGORITHMS\nOur approach to the artist recommendation task is to em-\nbed each artist from a set Xinto a Euclidean space so that\ndistances correspond to human perception of dissimilarity.\nAlthough it has been documented that notions of similar-\nity between artists can vary dramatically from person to\nperson, rankings of similarity between pairs of artists are\ncomparatively more robust [9].\nOne simple ranking method involves comparisons of\nartistsjandkrelative to a ﬁxed reference artist i. This\nyields similarity triplets (i,j,k ), indicating that the pair\n(i,j)are more similar to each-other than the pair (i,k).\nData of this variety are becoming increasingly common\n513Poster Session 3\nAerosmith\nBon Jovi\n112\nBon JoviBryan Adams\nBon Jovi(a)\nAerosmith Bon Jovi 112 Bryan Adams (b)\nFigure 1 . Similarity triplets can be interpreted as a directed\ngraph over pairs of artists: an edge (i,j)→(i,k)indicates\nthatiandjare more similar than iandk. (a) The graph\nrepresentation of two triplets: (Bon Jovi, Aerosmith, 112)\nand(Bon Jovi, Bryan Adams, 112) . (b) An example of a\n1-dimensional embedding that satisﬁes these triplets.\nfor general ranking and human perception modeling tasks,\nsuch as the Tag-a-Tune bonus round [12].\nIn this setting, we seek a Euclidean embedding function\ng:X→RDsuch that each given triplet (i,j,k )yields\n/bardblg(i)−g(j)/bardbl2+ 1</bardblg(i)−g(k)/bardbl2, (1)\nwhere the unit margin is enforced for numerical stability.\nIn other words, distance in the embedding space corre-\nsponds to perceived similarity. This framework eliminates\nthe need to normalize quantitative similarity scores (as in\nmulti-dimensional scaling), and does not over-simplify the\ndescription language to a binary problem (e.g., same ver-\nsusdifferent ).\nSeveral algorithms have been proposed to solve embed-\nding problems in this framework [1, 16, 20]. Here, we\nbrieﬂy summarize the partial order embedding (POE) al-\ngorithm of [16].\n2.1 Partial order constraints\nA collection of similarity triplets can be equivalently repre-\nsented as a directed graph in which each vertex represents\na pair of artists, and a directed edge indicates a compar-\nison of pairwise similarities (see Figure 1). Interpreting\nthe similarity triplets as a graph allows us to simplify the\nembedding problem by pruning edges which may be re-\ndundant or inconsistent.\nIf the triplets give rise to a directed acyclic graph (DAG),\nthis deﬁnes a partial order over distances, which implies\nthe existence of some similarity space which is consistent\nwith the measured triplets. If the graph contains cycles,\nthen no similarity function can satisfy all of the triplets,\nand we say that the triplets are inconsistent . In practice,\nthere are always inconsistencies in human similarity per-\nception, but the graph representation provides a direct way\nto locate and quantify these inconsistencies. Section 4.1\ndescribes an experiment and methodology to analyze in-\nconsistencies in a collection of similarity measurements.\n2.2 Multi-kernel embedding\nSince our eventual goal is to recommend similar artists\nwhen presented with a previously unseen artist, we will\nneed to provide a means to map unseen artists into the em-\nbedding space after training, without requiring any simi-\nlarity measurements for the new artist. POE achieves this\nFeature space 1\nFeature space 2\nFeature space m......\nInput space Embedding spaceFigure 2 . The embedding procedure ﬁrst maps a point x\nintomdifferent non-linear spaces (encoded by mdifferent\nkernel matrices), and then learns a set of projections Np\n(p= 1...m ) which form the embedding space.\nby restricting the choice of embedding functions to lin-\near projections from a given feature space. This readily\ngeneralizes to non-linear embeddings through the use of\nkernel functions [19]. Artists are ﬁrst mapped into a high-\ndimensional inner-product space by a feature transform de-\nﬁned by a kernel function k(·,·). POE then learns a pro-\njection from this feature space into a low-dimensional Eu-\nclidean space. This leads to the parameterization\ng(x) =NKx,\nwhereNis a linear projection matrix, and Kxis the vector\nformed by evaluating a kernel function k(x,i)against all\npointsiin the training set.\nSince formulating the problem in terms of Nwould lead\nto a non-convex optimization problem — with perhaps in-\nﬁnitely many parameters — POE instead optimizes over a\npositive semideﬁnite matrix W=NTN/followsequal0[6].Nmay\nbe inﬁnite-dimensional (as is the case in Gaussian kernels),\nbut an approximation to Ncan be recovered from Wby\nspectral decomposition:\nNTN=W=VΛVT=VΛ1/2Λ1/2VT\n=/parenleftBig\nΛ1/2VT/parenrightBigT/parenleftBig\nΛ1/2VT/parenrightBig\n=˜NT˜N(2)\nwhereVandΛcontain the eigenvectors and eigenvalues\nofW.\nIn MIR tasks, it is becoming common to combine data\ndescriptions from multiple feature modalities, e.g., social\ntags and spectral features [4]. POE accomplishes this by\nlearning a separate transformation Npfor each ofmkernel\nmatricesKp(p= 1...m ), and concatenating the resulting\nvectors (see Figure 2). This formulation allows (squared)\ndistance computations in the embedding space to be de-\ncomposed as\nd(i,j) =m/summationdisplay\np=1/parenleftbig\nKp\ni−Kp\nj/parenrightbigTWp/parenleftbig\nKp\ni−Kp\nj/parenrightbig\n.(3)\nThe multi-kernel POE algorithm is given as Algorithm 1.\nThe objective function has three components: the ﬁrst term,/summationtext\ni,jd(i,j)maximizes the variance of the embedded points,\nwhich has been demonstrated to be effective for reducing\ndimensionality in manifold data [23]. In the present appli-\ncation, variance maximization diminishes erroneous rec-\nommendations by pushing all artists far away from each-\n51410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nother, except where prevented from doing so by similarity\nordering constraints.\nThe second term,−β/summationtext\nCξijk, incurs hinge-loss penal-\nties for violations of similarity constraints, scaled accord-\ning to a free parameter β. The last term,−γ/summationtextTr (WpKp),\nregularizes the solution and enforces sparsity in the solu-\ntion, again scaled by a free parameter γ. Parameters βand\nγare tuned by cross-validation, similar to the Cparameter\nin support vector machines [7].\nThere are four types of constraints in Algorithm 1. The\nﬁrst,d(i,j)≤∆C, bounds the diameter of the embedding\nto resolve scale invariance.1The second set of constraints,\nd(i,j) + 1−ξijk≤d(i,k)enforces consistency between\nthe learned distances and similarity triplets in the training\nset, as in Equation 1. The slack terms ξijk≥0allow\nsimilarity constraints to be violated, provided it yields an\noverall increase in the value of the objective function. Fi-\nnally,Wp/followsequal0forces eachWpto be positive semideﬁnite,\nso that theNpmatrices can be recovered as in Equation 2.\nThe optimal solution/parenleftbig\nW1,W2,...,Wm/parenrightbig\nis computed\nby gradient ascent, and then each matrix is decomposed to\nproduce the embedding function\ng(x) = (NpKp\nx)m\np=1, (4)\nwhere (NpKp\nx)m\np=1denotes the concatenation over all m\nvectorsNpKp\nx.\nAlgorithm 1 Multi-kernel partial order embed-\nding [16]. d(i,j)is deﬁned as in Equation 3, and/parenleftbig\nW1,W2,...,Wm/parenrightbig\nare optimized by gradient ascent.\nInput: kernel matrices K1,K2,...,Km,\ntripletsC={(i,j,k ) : (i,j)more similar than (i,k)}\nOutput: matricesW1,W2,...,Wm/followsequal0.\nmax\nWp,ξ/summationdisplay\ni,jd(i,j)−β/summationdisplay\nCξijk−γ/summationdisplay\npTr (WpKp)\ns.t.\n∀i,j∈X d(i,j)≤∆C\n∀(i,j,k )∈Cd(i,j) + 1−ξijk≤d(i,k)\nξijk≥0\n∀p∈1,2,...,m Wp/followsequal0\n3. DATA\nTo evaluate our system, we designed experiments around\ntheaset400 data set of Ellis, et al [9]. The data consists of\n412 popular artists, and similarity triplets collected with a\nweb survey in 2002. We augmented the data set with sev-\neral types of features, both human-derived (tags and text),\nand purely content-driven, as described below.\n3.1 Text features\nOur text-based features were collected from Last.FM be-\ntween January and May of 2009. To standardize the list\n1∆Cis computed from the structure of the similarity triplets graph,\nand is not a free parameter. See [16] for details.of artist names, we used the search artists method of the\nEcho Nest API [17].\nWe then collected for each artist two types of textual\nfeatures from Last.FM: biography summaries and the top\n100 tags [11]. The tags were ﬁltered by a small set of regu-\nlar expressions to resolve common spelling variations. For\nexample, r-n-b ,r&b,r-and-b were all mapped to rnb, and\nthe merged tag rnbreceived a score equal to the sum of\nscores of its constituent tags.\nThe tags and biographies were ﬁltered by stop-word\nremoval and stemming, resulting in dictionaries of 7737\nunique tag words, and 16753 biography words. Each artist\nwas summarized as two bags of words (one for tags and\none for biographies), which were then re-weighted by TF-\nIDF. Finally, to compare similarity between artists, we con-\nstructed kernels KtagandKbiodeﬁned by the cosine simi-\nlarity between word vectors.\n3.2 Acoustic features\nFor each artist, we selected between one and ten songs at\nrandom (depending on availability), with an average of 3.8\nsongs per artist. From these songs, we extracted a va-\nriety of content-based features. Since content-based fea-\ntures relate to songs and not directly to artists, we do not\nexpect them to perform as well the textual features de-\nscribed above. We are primarily interested in integrating\nheterogeneous features, and quantifying the improvements\nachieved by optimizing for artist similarity.\n3.2.1 MFCC\nMel-frequency cepstral coefﬁcients (MFCCs) have been\ndemonstrated to capture timbral or textural qualities, and\nperform well in a variety of MIR applications [13, 15].\nFor each song, we compute the ﬁrst 13 MFCCs for up\nto 10000 half-overlapping short-time segments (23 msec),\nalong with the ﬁrst and second instantaneous derivatives.\nThis results in a collection of 39-dimensional delta-MFCC\nvectors for each song.\nEach artist was summarized by modeling the distribu-\ntion of delta-MFCC vectors in all songs belonging to that\nartist, using a Gaussian mixture model (GMM) of 8 com-\nponents and diagonal covariances. Then, to compare mod-\nels between artists, we construct a probability product ker-\nnel (PPK) between the GMMs:\nKMFCC\nij =/integraldisplay/radicalBig\np/parenleftbig\nx;θMFCC\ni/parenrightbig\np/parenleftbig\nx;θMFCC\nj/parenrightbig\ndx,\nwhereθMFCC\ni andθMFCC\nj are the GMM model parameters\nfor artistsiandj[10]. Unlike kernels derived from Kull-\nback Leibler divergence, PPK can be computed in closed-\nform for mixtures of Gaussians.\n3.2.2 Chroma\nFor each song in our database, we modeled the distribution\nof spectral energy present in frequencies corresponding to\nthe chromatic scale, resulting in a 12-dimensional vector\nfor every 250 msec of audio. Although chroma features\nare not speciﬁcally suited to the artist similarity task, they\n515Poster Session 3\nhave been shown to work well in other applications when\ncombined with other features, such as MFCCs [8]. We\nsummarized each artist by collecting chroma features for\neach of the artist’s songs, which were then modeled with\na single full-covariance Gaussian distribution/parenleftbig\nθch/parenrightbig\n. From\nthese chroma models, we construct an artist similarity ker-\nnel2from symmetrized KL-divergence:\nDKL(i,j) =/integraldisplay\np/parenleftbig\nx;θch\ni/parenrightbig\nlogp/parenleftbig\nx;θch\ni/parenrightbig\np/parenleftbig\nx;θch\nj/parenrightbigdx\nKch\nij= exp/parenleftbigg\n−DKL(i,j) +DKL(j,i)\nµ/parenrightbigg\n,\nwhereµis the mean KL-divergence over all pairs i,j. Since\nwe are not using mixture models here, this can be com-\nputed in closed form.\n3.2.3 Content-based auto-tagging\nIn contrast to the low-level acoustic features, we also eval-\nuate high-level conceptual features which were automat-\nically synthesized from audio content. To achieve this,\nwe computed semantic multinomial distributions using the\nsystem described in [22]. For each song, the auto-tagger\nexamines the acoustic content and produces a multinomial\ndistribution over a vocabulary Vof 149 words, e.g., mel-\nlow,dance pop ,horn section ,etc. The semantic model\nparametersθSM\nifor an artistiwere computed by averaging\nthe parameters of each of that artist’s song-level models.\n(We also tested a version using the point-wise maximum\nof song-level models, but it yielded little quantitative dif-\nference.) To compare models between artists, we construct\na semantic multinomial kernel using the multinomial PPK:\nKSM\nij=/parenleftBigg/summationdisplay\nx∈V/radicalBig\np/parenleftbig\nx;θSM\ni/parenrightbig\np/parenleftbig\nx;θSM\nj/parenrightbig/parenrightBiggs\n.\nThis is equivalent to a homogeneous polynomial kernel of\ndegreesover the model parameter vectors. For our exper-\niments, setting s= 75 yielded reasonable results.\n4. EXPERIMENTS\n4.1 Quantifying inconsistency\nThe aset400 data set consists of 412 popular artists, and\nsimilarity triplets gathered from a web-based survey. In\nthe survey, an informant was presented with a query artist\ni, and was asked to select, from a list of ten artists, the re-\nsponsejmost similar to the query artist. Then, for each of\nthe remaining responses kwhich were not selected, mea-\nsurements (i,j,k )were recorded. Note that in a list of ten\npotential responses, there may be several “good” choices.\nBeing forced to choose a single best response therefore re-\nsults in numerous inconsistencies in the triplets, which we\nset out to quantify.\nThe survey data contains 98964 triplets, generated from\n10997 queries to 713 human informants. We analyze the\nﬁltered version of the data, which has been reduced to\n2Symmetrized KL-divergence does not generally produce a PSD ker-\nnel matrix, but the POE algorithm is still correct for indeﬁnite kernels.\nTotal number of edges\nRetained edges after prun ing \ndirect inconsistencies\nAverage size of maximal \nconsistent subgraphs\nNumber of edges included  \nin all acyclic subgraphs16385\n13420100%\n81.9%\n54.8%\n52.5%8975.2\n8598Figure 3 . Quantitative summary of consistency within the\naset400 ﬁltered triplets. Directly inconsistent triplets are\nthose where both (i,j,k )and(i,k,j )are present.\n16385 measurements wherein the informant was likely to\nbe familiar with the artists in question. Although this greatly\nreduces the amount of noise present in the full set, the ﬁl-\ntered set still contains numerous inconsistencies.\nConsistency in the similarity measurements can be quan-\ntiﬁed by analyzing their graph representation. As a ﬁrst\nstep, we ﬁlter out all measurements (i,j,k )if(i,k,j )is\nalso present, i.e., those artist-pairs which the informants\ncould not consistently rank. We refer to these triplets as di-\nrectly inconsistent . Removing these triplets decreases the\nnumber of edges by 18.1% to 13420.\nHowever, simply removing all length-2 cycles from the\ngraph does not ensure consistency: all cycles must be re-\nmoved. Finding a maximum acyclic subgraph is NP-hard,\nbut we can ﬁnd an approximate solution by Algorithm 2.\nSince the algorithm is randomized, we repeat it several\ntimes to compute an estimate of the average maximal acyclic\nsubgraph. With 10 trials, we ﬁnd consistent subsets of av-\nerage size 8975.2.\nTo evaluate the stability of these subgraphs, we count\nthe number of edges present in all solutions, i.e., those\nmeasurements which are never pruned. Over 10 trials,\n8598 edges (95.8%) were common to all solutions, leaving\n4.2% variation across trials. Our results are summarized in\nFigure 3.\nAlgorithm 2 Approximate maximum acyclic subgraph\nInput : Directed graph G= (V,E)\nOutput : Acyclic graph G/prime\nE/prime←∅\nforeach(u,v)∈Ein random order do\nifE/prime∪{(u,v)}is acyclic then\nE/prime←E/prime∪{(u,v)}\nend if\nend for\nG/prime←(V,E/prime)\n4.2 Order prediction\nThe goal of our system is to recommend similar artists in\nresponse to a query. To evaluate the system, we test its abil-\nity to predict for artists i,jandk(whereiis unseen), the\nordering of similarity between (i,j)and(i,k), i.e., which\nof the artists jorkis more similar to artist i.\n51610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n4.2.1 Experimental Setup\nWe split the data for 10-fold cross validation, resulting in\n370 training and 42 test artists for each fold. All directly\ninconsistent triplets were removed from both training and\ntest sets, as described in Section 4.1. For each training\nset, we ﬁltered the triplets to produce a maximal acyclic\nsubgraph, retaining only those measurements which were\nincluded in all of 10 trials of Algorithm 2. The acyclic\nsubgraphs were then pruned down to their transitive re-\nductions , i.e., minimal graphs with equivalent transitivity\nproperties [2]. This effectively removes the measurements\nwhich could be deduced from others, thereby reducing the\ncomplexity of the embedding problem with no loss of qual-\nity. The resulting training sets have an average of 6252.7\nsimilarity measurements. The corresponding graphs have\naverage diameter 30.2, indicating the longest contiguous\nchain of comparisons which can be followed in the train-\ning sets.\nFor each test set, we included only those triplets (i,j,k )\nwhereiis in the test set and j,kare in the training set, re-\nsulting in an average of 1149.6 triplets per test set. Aside\nfrom pruning directly inconsistent triplets, no further pro-\ncessing was done to enforce consistency in the test set.\nTherefore, we cannot expect 100% prediction accuracy on\nthe test set. As shown in Figure 3, we can expect a lower-\nbound on the achievable accuracy of 67% ( 8975.2/13420 ).\nThis is consistent with the upper-bound of 85% constructed\nin [9].\n4.2.2 Results\nWe tested the embedding method on each of the kernels\ndescribed in Sections 3.1 and 3.2 independently, and then\ncombined. The free parameters βandγwere tuned by\nsweeping over β∈[100,10000] andγ∈[100,1000] . Af-\nter learning, performance was evaluated by counting the\nnumber of test-triplets correctly predicted by Euclidean dis-\ntance in the embedding space.\nFigure 5 illustrates two regions of an embedding pro-\nduced by the combination of tags and biography features,\nincluding several query points which were mapped in after\nlearning. The nearest neighbors of the query points pro-\nvide reasonable recommendations, and the neighborhoods\nare generally consistent. Moreover, neighborhoods which\nare largely dissimilar (e.g., female vocals andpunk ) have\nbeen pushed to opposite extremes of the space by the vari-\nance maximization objective.\nFor comparison purposes, we also evaluated the predic-\ntion accuracy of distance-based ranking in the native fea-\nture spaces. Native multi-kernel results were computed by\nconcatenating the kernels together to form feature vectors,\nwhich is equivalent to setting each Np=I. This pro-\nvides an intuitive and consistent way to compute distances\nto neighbors in one or more feature spaces.\nFigure 4 lists the quantitative results of our experiments.\nIn all cases, prediction accuracy improves signiﬁcantly af-\nter learning the optimal embedding. Moreover, the im-\nprovement is more signiﬁcant than it may at ﬁrst seem,\n0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8Optimized\nNative\nTags\nBiography\nTags+BioMFCC\nChroma\nSM\nMFCC+Chroma\nMFCC+SM\nTags+MFCC\nTags+Bio+MFCC0.6200.535\n0.5610.507\n0.5900.554\n0.6110.522\n0.6140.556\n0.7760.705\n0.7050.514\n0.7900.640\n0.7730.693\n0.7830.640Figure 4 . Triplet prediction accuracy for each feature and\ncombinations, before and after learning.\nsince the maximum achievable performance is less than\n100% due to inconsistencies in the test set.\nIt is not surprising that textual features give the best per-\nformance, and there are two main factors which explain\nthis effect. First, only textual features were attributed di-\nrectly to artists and not songs. Second, textual features de-\nrive from natural language, which is well-suited to describ-\ning subtle differences. We achieve signiﬁcant improve-\nments by optimizing the similarity metric, with gains of\n7% for tags and 19% for biographies. Moreover, combin-\ning both types of textual features results in better perfor-\nmance than either feature on its own.\nAs expected, embeddings based on acoustic features\nperform signiﬁcantly worse than those derived from text.\nWe believe this is primarily due to the fact that acoustic\nfeatures relate directly to songs, and variation across an\nartist’s songs introduces noise to the artist-level models.\nNote that combining a kernel which performs poorly (e.g.,\nchroma) does not signiﬁcantly degrade the overall perfor-\nmance, indicating that the algorithm correctly selects the\nmost relevant features available.\n4.2.3 Comparison\nOur results can be directly compared to the “unweighted\nagreement” score measurements of [9]. Particularly of in-\nterest is the comparison of our biography-based embed-\nding, which is analogous to the text-based measures in [9].\nOur biography features natively achieve 51.4% accuracy,\ncompared to 57.4% for the web documents in [9]. How-\never, the optimized embedding improves prediction accu-\nracy to 70.5%.\n5. CONCLUSION\nIn this paper, we demonstrated a method for optimizing\nmulti-modal musical similarity measures to match human\nperception data. We believe that the techniques illustrated\nhere could be applicable in other subjective similarity tasks,\nparticularly at the song level, and this will be the focus of\nfuture work.\n6. ACKNOWLEDGEMENTS\nWe thank Luke Barrington and Douglas Turnbull for their\nadvice. Gert Lanckriet is supported by NSF grant DMS-\nMSPA 0625409.\n517Poster Session 3\nToni BraxtonDebelah Morgan3LW\nJessica Andrews\nFaith HillJanet JacksonEn VogueDeana CarterChristina Aguilera\nMelissa EtheridgeMadonna\nBelinda CarlisleMariah Carey\nShania Twain\nWhitney HoustonJennifer PaigeSealLa BoucheNelly Furtado\nBette Midler\nCyndi LauperPaula AbdulJessica SimpsonSarah McLachlan\nTracy Chapman\nSadeSelenaAnouk\nBonnie TylerRichard MarxNick Drake\nChris IsaakMichael JacksonEdwin McCain\nEurythmicsSheryl CrowPink\nErasureStevie Wonder\nNeil SedakaLionel Richie\nTina TurnerStingKenny LogginsCulture Club\na−haGeorge Michael\nAce of BaseAqua\nChristopher Cross\nPet Shop BoysAretha FranklinWham!\nRednex\nBilly Joel\nNatalie ImbrugliaPrinceAll SaintsCulture Beat\nFiona Apple\nMelanie CEverything but the Girl\nWilson PhillipsSpice Girls\nAni DiFrancoSamantha Mumba\nDonna SummerDido\nGabrielle\nTraining\nTestCeline Dione(a) Female vocalist/pop\n (b) Punk/metal/alternative\nFigure 5 . Two neighborhoods at opposite extremes of an optimized text-based embedding.\n7. REFERENCES\n[1] Sameer Agarwal, Joshua Wills, Lawrence Cayton, Gert\nLanckriet, David Kriegman, and Serge Belongie. Gen-\neralized non-metric multi-dimensional scaling. In Pro-\nceedings of the Twelfth International Conference on\nArtiﬁcial Intelligence and Statistics , 2007.\n[2] A. V . Aho, M. R. Garey, and J. D. Ullman. The tran-\nsitive reduction of a directed graph. SIAM Journal on\nComputing , 1(2):131–137, 1972.\n[3] Jean-Julien Aucouturier and Franc ¸ois Pachet. Music\nsimilarity measures: What’s the use? In Inerna-\ntional Symposium on Music Information Retrieval (IS-\nMIR2002) , pages 157–163, 2002.\n[4] Luke Barrington, Mehrdad Yazdani, Douglas Turnbull,\nand Gert Lanckriet. Combining feature kernels for se-\nmantic music retrieval. In International Symposium on\nMusic Information Retrieval (ISMIR2008) , pages 614–\n619, September 2008.\n[5] A. Berenzweig, B. Logan, D. P. W. Ellis, and B. Whit-\nman. A large-scale evaluation of acoustic and subjec-\ntive music-similarity measures. Computer Music Jour-\nnal, 28(2):63–76, 2004.\n[6] Stephen Boyd and Lieven Vandenberghe. Convex Op-\ntimization . Cambridge University Press, 2004.\n[7] Christopher J. C. Burges. A tutorial on support vec-\ntor machines for pattern recognition. Data Mining and\nKnowledge Discovery , 2(2):121–167, 1998.\n[8] D. Ellis. Classifying music audio with timbral and\nchroma features. In International Symposium on Music\nInformation Retrieval (ISMIR2007) , pages 339–340,\n2007.\n[9] D. Ellis, B. Whitman, A. Berenzweig, and\nS. Lawrence. The quest for ground truth in musi-\ncal artist similarity. In Proeedings of the International\nSymposium on Music Information Retrieval (IS-\nMIR2002) , pages 170–177, October 2002.\n[10] Tony Jebara, Risi Kondor, and Andrew Howard. Prob-\nability product kernels. Journal of Machine Learning\nResearch , 5:819–844, 2004.\n[11] Last.FM, 2009. http://www.last.fm/.\n[12] Edith Law and Luis von Ahn. Input-agreement: a new\nmechanism for collecting data using human computa-\ntion games. In CHI ’09: Proceedings of the 27th in-\nternational conference on Human factors in computingsystems , pages 1197–1206, New York, NY , USA, 2009.\nACM.\n[13] B. Logan. Mel frequency cepstral coefﬁcients for mu-\nsic modeling. In International Symposium on Music In-\nformation Retrieval (ISMIR2000) , 2000.\n[14] B. Logan. Music recommendation from song sets. In\nInternational Symposium on Music Information Re-\ntrieval (ISMIR2004) , 2004.\n[15] M. Mandel and D. Ellis. Song-level features and sup-\nport vector machines for music classiﬁcation. In Inter-\nnational Symposium on Music Information Retrieval\n(ISMIR2005) , pages 594–599, 2005.\n[16] Brian McFee and Gert Lanckriet. Partial order em-\nbedding with multiple kernels. In Proceedings of the\nTwenty-sixth International Conference on Machine\nLearning , 2009.\n[17] The Echo Nest, 2009. http://www.echonest.com/.\n[18] E. Pampalk, A. Rauber, and D. Merkl. Content-based\nOrganization and Visualization of Music Archives. In\nProceedings of the ACM Multimedia , pages 570–579,\nJuan les Pins, France, December 1-6 2002. ACM.\n[19] Bernhard Scholkopf and Alexander J. Smola. Learn-\ning with Kernels: Support Vector Machines, Regular-\nization, Optimization, and Beyond . MIT Press, Cam-\nbridge, MA, USA, 2001.\n[20] Matthew Schultz and Thorsten Joachims. Learning a\ndistance metric from relative comparisons. In Sebas-\ntian Thrun, Lawrence Saul, and Bernhard Sch ¨olkopf,\neditors, Advances in Neural Information Processing\nSystems 16 , Cambridge, MA, 2004. MIT Press.\n[21] M. Slaney, K. Weinberger, and W. White. Learning\na metric for music similarity. In International Sym-\nposium on Music Information Retrieval (ISMIR2008) ,\npages 313–318, September 2008.\n[22] Douglas Turnbull, Luke Barrington, David Torres, and\nGert Lanckriet. Semantic annotation and retrieval of\nmusic and sound effects. IEEE Transactions on Au-\ndio, Speech and Language Processing , 16(2):467–476,\nFebruary 2008.\n[23] Kilian Q. Weinberger, Fei Sha, and Lawrence K. Saul.\nLearning a kernel matrix for nonlinear dimensionality\nreduction. In Proceedings of the Twenty-ﬁrst Interna-\ntional Conference on Machine Learning , pages 839–\n846, 2004.\n518"
    },
    {
        "title": "Using ACE XML 2.0 to Store and Share Feature, Instance and Class Data for Musical Classification.",
        "author": [
            "Cory McKay",
            "John Ashley Burgoyne",
            "Jessica Thompson 0001",
            "Ichiro Fujinaga"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418173",
        "url": "https://doi.org/10.5281/zenodo.1418173",
        "ee": "https://zenodo.org/records/1418173/files/McKayBTF09.pdf",
        "abstract": "This paper introduces ACE XML 2.0, a set of file formats that are designed to meet the special representational needs of research in automatic music classification. Such standardized formats are needed to facilitate the sharing and long-term storage of valuable research data. ACE XML 2.0 is designed to represent a broad range of musical information clearly using a flexible, extensible, selfcontained and formally structured framework. An emphasis is placed on representing extracted feature values, feature descriptions, instance annotations, class ontologies and related metadata.",
        "zenodo_id": 1418173,
        "dblp_key": "conf/ismir/McKayBTF09",
        "keywords": [
            "file formats",
            "research in automatic music classification",
            "standardized formats",
            "sharing",
            "long-term storage",
            "valuable research data",
            "clearly",
            "flexible",
            "extensible",
            "self-contained"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   USING ACE XML 2.0 TO STORE AND SHARE FEATURE, INSTANCE AND CLASS DATA FOR MUSICAL CLASSIFICATION Cory McKay John Ashley Burgoyne Jessica Thompson Ichiro Fujinaga CIRMMT McGill University cory.mckay@ mail.mcgill.ca CIRMMT McGill University ashley@ music.mcgill.ca Music Technology McGill University jessica.thompson@ mail.mcgill.ca CIRMMT McGill University ich@ music.mcgill.ca ABSTRACT This paper introduces ACE XML 2.0, a set of file formats that are designed to meet the special representational needs of research in automatic music classification. Such standardized formats are needed to facilitate the sharing and long-term storage of valuable research data. ACE XML 2.0 is designed to represent a broad range of musi-cal information clearly using a flexible, extensible, self-contained and formally structured framework. An empha-sis is placed on representing extracted feature values, fea-ture descriptions, instance annotations, class ontologies and related metadata. 1. INTRODUCTION Many music information retrieval (MIR) research pro-jects involve three core tasks: collecting and annotating ground-truth data; extracting feature values from in-stances; and training classification models using machine learning. These tasks require well-designed data represen-tations, as insufficiently expressive representations can prevent learning algorithms from accessing valuable in-formation.  Representational formats also have an important im-pact on the ability of MIR researchers to share valuable data with one another, particularly since ground-truth datasets can be expensive to acquire. Legal restrictions on distributing such datasets make the ability to share ex-tracted feature values and ground-truth annotations par-ticularly valuable. The absence of expressive, flexible, well-defined and well-supported standardized representa-tional formats tends to result in individual research labo-ratories generating their own in-house data, with conse-quent wasteful repeated effort and lower quality data. Standardized file formats are also needed to facilitate compatibility of MIR toolkits such as CLAM, jMIR, Marsyas, MIRtoolbox and Sonic Visualiser. Powerful packages such as these each have their own advantages, and a common representational format is needed if re-search performed using different toolkits is to be com-bined. Standardized file formats are also needed to facilitate the evaluation and comparison of techniques from differ-ent research groups, something that has become apparent in the yearly Music Information Retrieval Evaluation Ex-change (MIREX) competition [1]. The lack of an ac-cepted standardized representational format necessitates the development of custom formats for each sub-task, which results in compromises with respect to expressivity and longevity. For example, the MIREX audio genre classification competition is carried out each year using ground-truth where each piece is labeled with only one genre label, despite general recognition that this is an un-realistic limitation that compromises results. The avail-ability of standardized formats such as ACE XML that can be easily used to associate multiple classes with each instance could help to address such problems. ACE XML 2.0 is proposed as a standard for represent-ing information associated with the application of ma-chine learning to music, including feature values, instance labels, class ontologies and associated metadata. ACE XML 2.0 has been developed as part of the Networked Environment for Musical Analysis (NEMA) [2] project, a multinational and multidisciplinary effort to create a gen-eral music information processing infrastructure.  2. ALTERNATIVE REPRESENTATIONS There are a number of existing approaches that can be used to represent information related to automatic music classification. One is to simply store such information as raw binary data, such as Matlab [3] MAT files. Although this can be an easy and efficient way of storing data, it has problems with respect to portability, readability and longevity. Customized software is needed to parse or write each binary file type, and such software is often proprietary and can only be expected to have a limited life span. Text files are an alternative to binary files. Although they are usually less space efficient, they address the weaknesses of binary files with respect to longevity, port-ability and readability. They can also be structured in a variety of standardized ways, ranging from simple delim-ited formats like CSV to markup languages like XML [4]. Weka ARFF [5] is a text-based format designed for general machine learning. Although ARFF files are cur-rently the closest thing to a standard in the MIR commu-nity, they do have some significant limitations, such as  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2009 International Society for Music Information Retrieval  \n303Poster Session 2\n   inabilities to associate windows with instances, to group the values of a feature array, to store important metadata and to associate multiple classes with a single instance. Another approach is to store feature values in audio files themselves, such as SDIF [6]. This technique can have a limited expressivity with respect to pertinent metadata, however, and is not appropriate for dealing with mixtures of cultural, symbolic and audio data. Music Ontology [7] is one of the few representational frameworks designed specifically with MIR in mind, and it has many admirable strengths. It can represent musical ontologies of essentially any kind using RDF [8]. Music Ontology has a much broader scope than ACE XML, but is arguably less suited specifically to machine learning and automatic music classification, despite its advantages over ACE XML in other MIR domains. The advantages of ACE XML relative to semantic web solu-tions in general include greater conciseness; a reduced need for markup not directly relevant to the core problem domain; a lower barrier to entry for non-ontologists, par-ticularly with respect to simplicity and convenience; a cleaner and more explicit structuring that is advantageous from a machine learning perspective; human readability, which is useful for application debugging and develop-ment; a self-contained nature that avoids the network de-pendence of RDF that can cause problems with respect to data integrity, robustness and accessibility, particularly considering the typically large size of feature data; and the simplicity of relying on only a single technology that is well-known in the MIR community (i.e., XML).  3. AN OVERVIEW OF ACE XML 3.1 General Overview The primary design priorities behind ACE XML 2.0 are the maximization of expressivity, flexibility and extensi-bility while at the same time maintaining as much sim-plicity, accessibility and structure as possible. There are four core ACE XML file types: Feature Value, Feature Description, Instance Label and Class Ontology. These file types hold, respectively, feature val-ues extracted from instances; abstract information about features and their extraction parameters; class labels as-sociated with particular instances and their subsections, as well as general metadata about instances; and onto-logical relationships between abstract classes. These XML file types may each be used independently, or they may be packaged with one another if desired (see Section 3.5).  The ACE XML file types are explained individually in Section 4, although space constraints prohibit more de-tailed descriptions. The XML DTDs shown in Figures 1 to 4 do specify their functionality in greater detail, how-ever. Sample code excerpts for each of the four core ACE XML file types are also provided in Figures 5 to 8. It is important to note that these excerpts only demonstrate a reduced subset of ACE XML’s expressivity, however, as many of the ACE XML elements and attributes are op-tional so that they can be included only when appropriate. This makes it possible to use simple and concise files by default, while maintaining the potential for much greater expressivity when needed.  ACE XML consists of multiple file types rather than just one because of the advantages, with respect to data portability and reusability, of explicitly separating fun-damentally different types of information. One might, for example, extract features once from a large number of recordings and then reuse the resulting Feature Value file for multiple purposes, such as classification by per-former, composer, genre and mood.  ACE XML is implemented in XML partly because it is a standardized format for which parsers are widely avail-able. XML is also very flexible while maintaining the ability to structure data formally and clearly. XML is also relatively easily readable by both humans and machines. ACE XML 2.0 is a significantly updated and expanded version of the earlier ACE XML 1.1, which was origi-nally designed specifically for use with ACE [9]. It be-came apparent that certain important types of information could not be expressed with ACE XML 1.1, so ACE XML 2.0 was developed in order to address these needs and to make ACE XML useful to the MIR community outside the specific scope of ACE.  3.2 jMIR Support jMIR [10] is a powerful suite of software applications developed for use as MIR research tools. Each of the jMIR applications reads and writes ACE XML, some-thing that provides ACE XML early adopters with a pow-erful set of tools that are ready for immediate use:   • jAudio: An audio feature extractor. • jSymbolic: A MIDI feature extractor. • jWebMiner: A feature extractor that extracts cultural and demographic information from the web. • ACE: A meta-learning system for machine learning. • jMusicMetaManager: Software for managing and cataloguing large musical datasets. • Codaich, Bodhidharma MIDI, SAC: research datasets. 3.3 Incorporating ACE XML into Other Software A key factor in the effectiveness of any effort to encour-age researchers to adopt new file formats is the ease with which they can incorporate the formats into their own software. Open-source code libraries are therefore cur-rently in the process of being implemented to support ACE XML 2.0. These libraries are implemented in Java in order to increase portability, and do not rely on any additional technologies that might require special installa-tion. They will provide functionality for parsing, writing and merging ACE XML files; for submitting search que-ries in JDOQL or SQL; and for performing various utility functions such as translating ACE XML data to and from Weka data. They will also include standard data struc-\n30410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   tures that external code can access via a simple and well-documented API or a GUI ACE XML editor. 3.4 Linking ACE XML 2.0 to External Resources It can be advantageous to associate instances, features or classes with various types of external information. Al-though ACE XML 2.0 can represent a broad range of metadata internally, the strong structuring that makes ACE XML advantageous for machine learning ultimately imposes limitations relative to the much more freely structured RDF, for example. ACE XML addresses this issue by permitting the use of RDF-like triples via the optional uri XML element and  <!ELEMENT ace_xml_feature_value_file_2_0 (comments?,              related_resources?, instance+)> <!ELEMENT comments (#PCDATA)> <!ELEMENT related_resources (feature_value_file*,              feature_description_file*, instance_label_file*,              class_ontology_file*, project_file*, uri*)> <!ELEMENT feature_value_file (#PCDATA)> <!ELEMENT feature_description_file (#PCDATA)> <!ELEMENT instance_label_file (#PCDATA)> <!ELEMENT class_ontology_file (#PCDATA)> <!ELEMENT project_file (#PCDATA)> <!ELEMENT uri (#PCDATA)> <!ATTLIST uri predicate CDATA #IMPLIED> <!ELEMENT instance (instance_id, uri*, extractor*, coord_units?,                     s*, precise_coord*, f*)> <!ELEMENT instance_id (#PCDATA)> <!ELEMENT extractor (#PCDATA)> <!ATTLIST extractor fname CDATA #REQUIRED> <!ELEMENT coord_units (#PCDATA)> <!ELEMENT s (uri*, f+)> <!ATTLIST s b CDATA #REQUIRED e CDATA #REQUIRED> <!ELEMENT precise_coord (uri*, f+)> <!ATTLIST precise_coord coord CDATA #REQUIRED> <!ELEMENT f (fid, uri*, (v+ | vd+ | vs+ | vj))> <!ATTLIST f type (int | double | float | complex | string)             #IMPLIED> <!ELEMENT fid (#PCDATA)> <!ELEMENT v (#PCDATA)> <!ELEMENT vd (#PCDATA)> <!ATTLIST vd d0 CDATA #REQUIRED d1 CDATA #IMPLIED               d2 CDATA #IMPLIED d3 CDATA #IMPLIED               d4 CDATA #IMPLIED d5 CDATA #IMPLIED              d6 CDATA #IMPLIED d7 CDATA #IMPLIED              d8 CDATA #IMPLIED d9 CDATA #IMPLIED> <!ELEMENT vs (d+, v)> <!ELEMENT d (#PCDATA)> <!ELEMENT vj (#PCDATA)> Figure 1: XML DTD for the ACE XML 2.0 Feature Value file format.    <!ELEMENT ace_xml_feature_description_file_2_0 (comments?,              related_resources?, global_parameter*, feature+)> <!ELEMENT comments (#PCDATA)> <!ELEMENT related_resources (feature_value_file*,              feature_description_file*, instance_label_file*,              class_ontology_file*, project_file*, uri*)> <!ELEMENT feature_value_file (#PCDATA)> <!ELEMENT feature_description_file (#PCDATA)> <!ELEMENT instance_label_file (#PCDATA)> <!ELEMENT class_ontology_file (#PCDATA)> <!ELEMENT project_file (#PCDATA)> <!ELEMENT uri (#PCDATA)> <!ATTLIST uri predicate CDATA #IMPLIED> <!ELEMENT feature (fid, description?, related_feature*, uri*,              scope, dimensionality?, data_type?, parameter*)> <!ELEMENT fid (#PCDATA)> <!ELEMENT description (#PCDATA)> <!ELEMENT related_feature (fid, relation_id?, uri*,                             explanation?)> <!ELEMENT relation_id (#PCDATA)> <!ELEMENT explanation (#PCDATA)> <!ELEMENT scope (#PCDATA)> <!ATTLIST scope overall (true|false) #REQUIRED                 sub_section (true|false) #REQUIRED                 precise_coord (true|false) #REQUIRED> <!ELEMENT dimensionality (uri*, size*)> <!ATTLIST dimensionality orthogonal_dimensions CDATA #REQUIRED> <!ELEMENT size (#PCDATA)> <!ELEMENT data_type (#PCDATA)> <!ATTLIST data_type type (int | double | float | complex |                           string) #REQUIRED> <!ELEMENT global_parameter (parameter_id, uri*, description?,                             value?)> <!ELEMENT parameter (parameter_id, uri*, description?, value?)> <!ELEMENT parameter_id (#PCDATA)> <!ELEMENT value (#PCDATA)> Figure 2: XML DTD for the ACE XML 2.0 Feature De-scription file format.  <!ELEMENT ace_xml_instance_label_file_2_0 (comments?,              related_resources?, instance+)> <!ELEMENT comments (#PCDATA)> <!ELEMENT related_resources (feature_value_file*,              feature_description_file*, instance_label_file*,              class_ontology_file*, project_file*, uri*)> <!ELEMENT feature_value_file (#PCDATA)> <!ELEMENT feature_description_file (#PCDATA)> <!ELEMENT instance_label_file (#PCDATA)> <!ELEMENT class_ontology_file (#PCDATA)> <!ELEMENT project_file (#PCDATA)> <!ELEMENT uri (#PCDATA)> <!ATTLIST uri predicate CDATA #IMPLIED> <!ELEMENT instance (instance_id, misc_info*, related_instance*,                     uri*, coord_units?, section*,                     precise_coord*, class*)> <!ATTLIST instance role (training | testing | predicted)                          #IMPLIED> <!ELEMENT instance_id (#PCDATA)> <!ELEMENT related_instance (instance_id, relation_id?, uri*,                             explanation?)> <!ELEMENT relation_id (#PCDATA)> <!ELEMENT explanation (#PCDATA)> <!ELEMENT misc_info (info_id, uri*, info)> <!ELEMENT info_id (#PCDATA)> <!ELEMENT info (#PCDATA)> <!ELEMENT coord_units (#PCDATA)> <!ELEMENT section (uri*, class+)> <!ATTLIST section begin CDATA #REQUIRED                   end CDATA #REQUIRED> <!ELEMENT precise_coord (uri*, class+)> <!ATTLIST precise_coord coord CDATA #REQUIRED> <!ELEMENT class (class_id, uri*)> <!ATTLIST class weight CDATA \"1\"> <!ATTLIST class source_comment CDATA #IMPLIED>  <!ELEMENT class_id (#PCDATA)> Figure 3: XML DTD for the ACE XML 2.0 Instance Label file format.     <!ELEMENT ace_xml_class_ontology_file_2_0 (comments?,              related_resources?, class+)> <!ATTLIST ace_xml_class_ontology_file_2_0 weights_relative              (true|false) #REQUIRED> <!ELEMENT comments (#PCDATA)> <!ELEMENT related_resources (feature_value_file*,              feature_description_file*, instance_label_file*,              class_ontology_file*, project_file*, uri*)> <!ELEMENT feature_value_file (#PCDATA)> <!ELEMENT feature_description_file (#PCDATA)> <!ELEMENT instance_label_file (#PCDATA)> <!ELEMENT class_ontology_file (#PCDATA)> <!ELEMENT project_file (#PCDATA)> <!ELEMENT uri (#PCDATA)> <!ATTLIST uri predicate CDATA #IMPLIED> <!ELEMENT class (class_id, misc_info*, uri*, related_class*,                  sub_class*)> <!ELEMENT class_id (#PCDATA)> <!ELEMENT misc_info (info_id, uri*, info)> <!ELEMENT info_id (#PCDATA)> <!ELEMENT info (#PCDATA)> <!ELEMENT related_class (class_id, relation_id?, uri*,                          explanation?)> <!ATTLIST related_class weight CDATA \"1\"> <!ELEMENT relation_id (#PCDATA)> <!ELEMENT explanation (#PCDATA)> <!ELEMENT sub_class (class_id, relation_id?, uri*,                      explanation?)> <!ATTLIST sub_class weight CDATA \"1\"> Figure 4: XML DTD for the ACE XML 2.0 Class On-tology file format.  \n305Poster Session 2\n   its associated predicate attribute. This enables links to be specified to external resources of essentially any kind without compromising ACE XML’s structured and self-contained design philosophy. In particular, it makes it easy to link ACE XML files to large RDF ontologies. 3.5 ACE XML 2.0 Project and ZIP Files Although it is beneficial to be able to specify the informa-tion encapsulated in each of the four ACE XML formats in separate files, in practice users will want to use multi-ple ACE XML files together. The ACE XML 2.0 Project file format facilitates this by providing functionality for linking ACE XML (and other) resources together.  It is also possible to package multiple associated ACE XML files together into a single ACE XML 2.0 ZIP file for simplified storage and distribution. This is also advan-tageous because of the reduced file sizes resulting from data compression. Of course, the original ACE XML files may be extracted from this ACE XML ZIP file whenever desired. The supporting ACE XML software includes functionality for automatically generating, accessing and otherwise processing ACE XML Project and ZIP files. 4. THE CORE ACE XML 2.0 FILE FORMATS This section provides descriptions of each of the four core ACE XML file formats: Feature Value, Feature Descrip-tion, Instance Label and Class Ontology.  4.1 Feature Value Files Feature Value files are used to express feature values that have been extracted from instances that are to be classi-fied or used as training data. There is no assumed associa-tion with any specific kind of data, and so features may be extracted from audio recordings, symbolic recordings, textual or numeric cultural data, images of album art, etc.  Features may be extracted from instances as a whole (e.g., an entire score), from subsections of instances (e.g., audio analysis windows) or from a mixture of the two. Subsections may or may not overlap, may or may not be of equal size and may or may not cover an instance com-prehensively. Each instance or subsection may also con-tain an arbitrary and potentially differing number of fea-tures, which makes it possible to omit features when ap-propriate or if they are unavailable. Each instance in a Feature Vector file has an in-stance_id tag that may be used to associate it with class labels and metadata stored in an Instance Label file. Simi-larly, each feature has an fid tag that may be used to asso-ciate it with feature metadata stored in a Feature Descrip-tion file. Other information that can be represented in a Feature Value file includes the data type (integer, double, string, etc.) of the feature, the feature extractor used to extract the feature values and links to external resources. ACE XML 2.0 allows feature values to be expressed using any one of four methodologies, including one that is based on JavaScript Object Notation (JSON) [11]. Each such representation has its own advantages with re-spect to the maximum dimensionality of feature arrays, the ability to represent sparse feature arrays, human read-ability and space efficiency. An example of the most flexible (but not most space efficient) of these options is shown in Figure 5. This option allows feature arrays of any dimensionality and size to be represented, including sparse arrays and arrays that vary in size. 4.2 Feature Description Files Feature Description files are used to express abstract in-formation about features. These files do not specify actual feature values, as this information is instead specified in Feature Value files.  The information that may be represented in Feature Description files includes: details of pre-processing re-quired before feature extraction (e.g., downsampling); feature extraction parameters; notations as to whether fea-tures are associated with instances as a whole or only with instance subsections; the dimensionality and size of each feature (i.e., a single-value feature, a feature vector or a feature array); the data type of each feature; qualita-tive feature descriptions; relationships between different features; and links to external resources. There are other possible applications for Feature De-scription files beyond simply using them to represent in-formation associated with Feature Value files. Examples include catalogues of features that can be extracted by particular feature extraction applications and lists of fea-tures and associated parameters that have been found to be useful for particular music classification applications. 4.3 Instance Label Files Instance Label files are used to specify class labels and miscellaneous metadata about instances. These files are typically used to express ground-truth annotations or pre-dicted labels, but there are certainly other uses as well. Class labels may be assigned to instances as a whole, to subsections of instances, or to both. Subsections may be overlapping and may be of varying sizes. Weighted multi-class membership is also permitted. Additional in-formation that may be associated with instances and their subsections includes the source of the class labels (e.g., a listener survey); whether the class label(s) for an instance are predicted labels or ground-truth; relationships of any kind between instances (e.g., one is a cover song of an-other); miscellaneous field-labeled qualitative metadata (e.g., the performer or composer of a piece); and links to external resources. Instance Label files may be linked with Feature Value files using matching instance_id tags and with Class Ontology files using matching class tags. \n30610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n    <instance>    <instance_id>An Artificial Instance</instance_id>    <f>       <fid>A Single Value Feature</fid>       <v>1</v>    </f>    <f>       <fid>Feature Array of Value</fid>       <vs><d>0</d><d>0</d><v>1</v></vs>       <vs><d>0</d><d>1</d><v>2</v></vs>       <vs><d>0</d><d>2</d><v>3</v></vs>       <vs><d>1</d><d>0</d><v>11</v></vs>       <vs><d>1</d><d>1</d><v>22</v></vs>       <vs><d>1</d><d>2</d><v>33</v></vs>    </f> </instance> Figure 5: An excerpt from a sample ACE XML 2.0 Fea-ture Value file indicating two artificial features extracted from a single instance. The first feature has a value of 1, and the second is the 2 by 3 feature array: [[1,2,3],[11,22,33]]. In practice, a Feature Value file could contain multiple such instances as well as features extracted from subsections of instances.           <feature>    <fid>Beat Histogram</fid>    <description>Tempo histogram calculated using                  Autocorrelation.</description>    <related_feature>       <fid>Tempo Peak</fid>       <relation_id>derivative feature</relation_id>    </related_feature>    <scope overall=\"true\" sub_section=\"false\"           precise_coord=\"false\"></scope>    <dimensionality orthogonal_dimensions=\"1\">       <size>161</size>    </dimensionality>    <data_type type=\"double\"></data_type>    <parameter>       <parameter_id>normalized</parameter_id>       <value>true</value>    </parameter> </feature> Figure 6: An excerpt from a sample ACE XML 2.0 Fea-ture Description file indicating information about a sin-gle feature called Beat Histogram. It is noted that Beat Histogram is related to another feature called Tempo Peak that can be calculated from the Beat Histogram feature, that Beat Histogram is configured to be ex-tracted only for files as a whole, that it consists of a sin-gle vector of size 161, that feature values are stored as doubles and that the values are normalized. In practice, a Feature Description file would contain multiple such feature clauses, each for a different feature.  <instance role=\"predicted\">    <instance_id>C:\\Symbolic\\piece_42.midi</instance_id>    <coord_units>ms</coord_units>     <section begin=\"0\" end=\"85673\">       <class>          <class_id>Sonata Exposition</class_id>       </class>    </section>    <section begin=\"85674\" end=\"278894\">       <class>          <class_id>Sonata Development</class_id>       </class>    </section>    <section begin=\"278895\" end=\"525419\">       <class>          <class_id>Sonata Recapitulation</class_id>       </class>    </section>     <class weight=\"3\">       <class_id>Haydn</class_id>    </class>    <class weight=\"1\">       <class_id>Mozart</class_id>    </class> </instance> Figure 7: An excerpt from a sample ACE XML 2.0 In-stance Label file specifying class labels for a MIDI file. As indicated by the role attribute, the labels are pre-dicted classifier outputs. The subsections are classified by form and the overall instance is classified by com-poser. The classification system has expressed that this piece is three times as likely to be by Haydn than by Mozart. In practice, an Instance Label file would contain multiple such instance clauses.  <class>    <class_id>Robert Johnson</class_id> </class>  <class>    <class_id>Muddy Waters</class_id>    <related_class weight=\"10\">       <class_id>Robert Johnson</class_id>       <relation_id>Influenced By</relation_id>    </related_class>    <related_class weight=\"1\">       <class_id>Eric Clapton</class_id>       <relation_id>Influenced By</relation_id>    </related_class> </class>  <class>    <class_id>Eric Clapton</class_id>    <related_class weight=\"30\">       <class_id>Robert Johnson</class_id>       <relation_id>Influenced By</relation_id>    </related_class>    <related_class weight=\"10\">       <class_id>Muddy Waters</class_id>       <relation_id>Influenced By</relation_id>    </related_class> </class> Figure 8: An excerpt from an artificial ACE XML 2.0  Class Ontology file indicating class labels consisting of names of Blues musicians. A type of relationship be-tween classes is also specified, namely musicians influ-enced by other musicians. In this example, there is no relationship from Robert Johnson to the other musicians because he was not influenced by them. Both of the other musicians are influenced by Johnson, however. Clapton is more influenced by Johnson than by Muddy Waters, and Muddy Waters is strongly influenced by Johnson but only slightly influenced by Clapton, as indi-cated by the weight values.  \n307Poster Session 2\n   4.4 Class Ontology Files Class Ontology files are used to specify candidate class labels for a particular classification domain as well as weighted ontological relationships between classes. These files do not, however, specify the labels of any actual instances, as this is the domain of Instance Label files.  The ability to specify ontological class structuring has several important benefits. From a musicological perspec-tive, it provides a simple, machine-readable way of speci-fying a variety of musical relationships. From a machine learning perspective, it has the dual advantages of ena-bling the use of powerful hierarchical classification meth-odologies that exploit this structuring, as well as learning schemes that utilize weighted penalization to punish “better” misclassifications less severely as training proceeds. The information that may be expressed in Class Ontol-ogy files includes weighted taxonomical links to other classes; weighted general ontological links to other classes; structured or unstructured descriptions of such links; miscellaneous qualitative structured metadata (e.g., the birthplace of a composer if music is being classified by composer); and links to external resources. 5. CONCLUSIONS This paper has emphasized the need for more effective representational formats for use in MIR and automatic music classification research. ACE XML 2.0 was pre-sented as a solution to these needs. It is hoped that ACE XML will help to facilitate communication and data shar-ing between research groups involved in the computa-tional study of music and correspondingly increase the efficiency and quality of research. Much more detailed information, including sample ACE XML 2.0 files and an in-depth ACE XML 2.0 man-ual, are available at jmir.sourceforge.net. 6. FUTURE RESEARCH Future work will focus on continuing to produce devel-oper tools to help facilitate the integration of ACE XML functionality into other software. Once work is completed on implementing the ACE XML 2.0 support software (the ACE XML 1.1 software is already complete) in Java it will then be ported to other languages, such as Python, C++ and Matlab. There are also plans to write ACE XML 2.0 plug-ins for the popular MIR software toolkits and to implement tools for translating ACE XML to other repre-sentational formats. The upgrading of all jMIR compo-nents from ACE XML compatibility 1.1 to ACE 2.0 com-patibility is a particular priority.  Another priority is the continuing extension and over-all improvement of the ACE XML standard. This will include the expression of more strictly constrained rules specified using XSD or Relax NG schemas. The publication of a common repository for data stored in ACE XML files is another key goal. This will enable such data to be posted and shared amongst re-searchers. This will also be a forum where best practices and extensions to the ACE XML standard can be dis-cussed and agreed upon by the MIR community. Indeed, ideas from the MIR community for future improvements to ACE XML in general are very welcome, and upgrades to the file formats will continue, with the provision that backwards compatibility is maintained. 7. ACKNOWLEDGEMENTS The authors would like to thank the Andrew W. Mellon Foundation and the Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT) for their generous financial support, as well as the members of the Networked Environment for Musical Analysis (NEMA) group for their valuable critiques and suggestions. 8. REFERENCES [1] MIREX 2009. Retrieved 11 May 2009, from http://www.music-ir.org/mirex/2009. [2] NEMA. Retrieved 11 May 2009, from http://nema.lis.uiuc.edu. [3] Mathworks. Retrieved 11May 2009, from http:// www.mathworks.com. [4] Ray, E. T. 2003. Learning XML. Sebastopol, CA: O’Reilly Media. [5] Witten, I. H., and E. Frank. 2005. Data mining: Practical machine learning tools and techniques. New York: Morgan Kaufman. [6] Burred J. J., C. E. Cella, G. Peeters, A. Röbel, and D. Schwarz. 2008. Using the SDIF Sound Description Interchange Format for audio features. Proceedings of the International Conference on Music Information Retrieval. 427–32. [7] Raimond, Y., S. Adbdallah, M. Sandler, and F. Giasson. 2007. The Music Ontology. Proceedings of the International Conference on Music Information Retrieval. 417–22. [8] Powers, S. 2003. Practical RDF. Sebastopol, CA: O’Reilly Media. [9] McKay, C., R. Fiebrink, D. McEnnis, B. Li, and I. Fujinaga. 2005. ACE: A framework for optimizing music classification. Proceedings of the International Conference on Music Information Retrieval. 42–9. [10] McKay, C., and I. Fujinaga. 2009. jMIR: Tools for automatic music classification. Accepted for publication in the Proceedings of the International Computer Music Conference. [11] JSON. Retrieved 11 May 2009, from http://json.org. \n308"
    },
    {
        "title": "Exploring African Tone Scales.",
        "author": [
            "Dirk Moelants",
            "Olmo Cornelis",
            "Marc Leman"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416338",
        "url": "https://doi.org/10.5281/zenodo.1416338",
        "ee": "https://zenodo.org/records/1416338/files/MoelantsCL09.pdf",
        "abstract": "Key-finding is a central topic in Western music analysis and development of MIR tools. However, most approaches rely on the Western 12-tone scale, which is not universally used. African music does not follow a fixed tone scale. In order to classify and study African tone scales, we developed a system in which the pitch is first analyzed on a continuous scale. Peak analysis is then applied on these data to extract the actual scale used. This system has been applied to a selection of African music, it allows us to look for similarities using cross-correlation. Thus it provides an interesting tool for query-by-example and database management in collections of ethnic music which can not be simply classified according to keys. Next to this the data can be used for ethnomusicological research. The study of the intervals used in this collection, e.g., gives us evidence for Western influence, with recent recordings having a tendency to use more regular intervals.",
        "zenodo_id": 1416338,
        "dblp_key": "conf/ismir/MoelantsCL09",
        "keywords": [
            "key-finding",
            "Western music analysis",
            "MIR tools",
            "12-tone scale",
            "African music",
            "pitch analysis",
            "continuous scale",
            "peak analysis",
            "cross-correlation",
            "ethnomusicological research"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   EXPLORING AFRICAN TONE SCALES Dirk Moelants Olmo Cornelis Marc Leman Ghent University Dirk.Moelants@UGent.be University College Ghent Olmo.Cornelis@hogent.be Ghent University Marc.Leman@UGent.be ABSTRACT Key-finding is a central topic in Western music analysis and development of MIR tools. However, most approaches rely on the Western 12-tone scale, which is not universally used. African music does not follow a fixed tone scale. In order to classify and study African tone scales, we devel-oped a system in which the pitch is first analyzed on a con-tinuous scale. Peak analysis is then applied on these data to extract the actual scale used. This system has been ap-plied to a selection of African music, it allows us to look for similarities using cross-correlation. Thus it provides an interesting tool for query-by-example and database man-agement in collections of ethnic music which can not be simply classified according to keys. Next to this the data can be used for ethnomusicological research. The study of the intervals used in this collection, e.g., gives us evidence for Western influence, with recent recordings having a tendency to use more regular intervals. 1. INTRODUCTION Scale recognition has a long tradition in the analysis of Western music. Already in medieval music theory, deter-mining the mode and classifying pieces according to their mode was a central topic. Also in the music theories of the Middle-East and India, classification of music according to the scale (often connected to a certain ‘mood’) is an im-portant topic. Not surprisingly, with the advent of compu-tational methods, researchers started to design systems to perform the process of scale recognition automatically [1]. In recent years the focus has shifted from symbolic ap-proaches, based on MIDI or score representations, to the analysis of musical audio files (e.g. [2 - 6]). Various sys-tems have reached a reasonable level of success in labeling music according to the keys of the Western tonal system (cf. MIREX 2005 [7]).  Automatic analysis and classification of scales in music that is not organized according to the Western tonal sys-tem is much less developed. Some efforts that have been done to extract the scales of e.g. Australian aboriginal did-jeridu music [8] or Indian classical music [9] use a reduc-tion to Western pitch classes, thus avoiding the problems raised by irregular temperaments. Although this approach can be efficient to a certain extent, it seems limited to music with a pitch organisation that has a certain resem-blance to the Western system and is problematic in terms of culture specific information. In some music the pitch-set used is as such not very important, but rather the musi-cal gestures associated with playing or moving from one tone to another are the most characteristic aspects. This has been used in the study of Chinese guqin music [10] and Carnatic (South-Indian classical) music [11], using prototypical gestural patterns or melodic atoms to describe the melodic content of music in which the pitch is seldom stable.  Some work has been done on the scale analysis of music of the Middle-East, more precisely on Turkish [12] and Persian [13] modes. This music is characterized by the occurrence of intervals based on (roughly) a quarter tone scale. Therefore, an analysis based on a chromatic (half-tone) division of the octave can not be used. Therefore the pitch is analysed on a more continuous scale, then trans-formed to pitch histograms, which can be attributed to schemata that represent the modes used in this specific repertoire. Pitch organisation in the music of Sub-Saharan Africa does not rely on a fixed theoretical framework. Ethno-musicological research has shown that a large variety of scales is in use. Often these scales use intervals that do not conform to the European chromatic scale, e.g. the use of intervals around 240 cents in (roughly) equidistant penta-tonic scales [14]. However, standardized tuning systems or culture-specific classification systems do not exist. In this paper we will propose a system to explore African scales with applications in Music Information Retrieval and ethnomusicology. First we will present the collection on which the scale-detection system is applied, as well as the test-set which will be analyzed in detail. In the next chap-ter we give a brief description of the pitch detection and peak extraction systems used to analyze the music and how the output can be coupled with the metadata associ-ated with the original sound files.  2. BACKGROUND The audio set which has been used in this research, is a selection from the audio archive of the RMCA (Royal Mu-seum for Central Africa) in Belgium. It is one of the larg-est collections worldwide of music from Central Africa. The audio collection consists of about 50,000 sound re-cordings (with a total of 3,000 hours of music), dating from the early 20th century up to now. Aiming for durable conservation and enhanced accessibility, the audio archive has been digitized entirely. Not only the audio but also ac-companying metadata and contextual information have \n489Poster Session 3\n   been digitized. A database and website were developed containing complete descriptions and fragments of the audio. The results of this project can be accessed on the website http://music.africamuseum.be.  For this study a selection of 901 audio files was used. In order to be sure to get a selection that consists only of music that uses a relatively fixed tone scale (and not e.g. music for percussion ensemble), we extracted music using four common types of musical instruments: musical bow (N = 132), zither (N = 134), flute (N = 385) and lamello-phone (thumb piano) (N = 250). The selection was limited to music described as solo performances, mostly they con-tain only the sound of the instrument, in some cases the performer also sings, accompanying him/herself on the instrument.  3. ANALYSIS 3.1 Pitch detection  The pitch algorithm used in this paper has originally been designed to perform automated transcription of sung audio into a sequence of pitch classes and their duration [15]. Original goal of this tool was the development of a query-by-humming system for retrieving pieces of music from a digitized musical library.  In this original system, the acoustic signal is turned into a parametric representation of the time-frequency information. A note is assigned to the segment by identifying the highest peak in the histogram of the frame-level pitch frequencies found in the segment, and by computing the average of the pitches lying in that bin. The pitch is then converted to a MIDI note rounding the computed annotation to the closest note frequency. For the application of pitch recognition to the study of African scales, some important adaptations had to be made. First, the time segmentation, necessary to create melodies, was not of importance for building the pitch scales and was left out. Second, the quantization of the an-notations into MIDI notes was unwanted, as we want to describe music that does not necessarily follows the equally-tempered scale. Therefore, the actual frequencies were used as pitch annotations. The output of this pitch algorithm consists of a list in which every line represents 10 ms, listing six potential frequencies, each with a proba-bility.  This allows extension to polyphonic textures. In this case however, we choose to work with largely monophonic music. Therefore only the pitch with the highest probability was retained for every 10 ms, at least if this probability was higher than a minimal threshold (in this case 0.5). Then the frequencies were transformed to a cents scale, setting C0 to zero (cents). For comparing his-tograms, all listed values were reduced to one octave, gen-erating a chromavector of 1200 values, representing the scale of the piece. A typical example of the graphical rep-resentation generated by the pitch detection system is shown in Figure 1. 3.2 Peak Extraction  The pitch analysis gives us a precise representation of the pitch content used in every piece. To extract the scale, a peak analysis was performed on the histograms. As a first step, the 1200 integers are ranked by their value. Starting from the highest value, peaks are assigned. A peak is ac-cepted if it meets all parameters. Parameters for the selec-tion are width of the area around the peak in which another peak cannot be assigned, the size (volume) of the peak and height relative to average height. Parameters were manu-ally optimized for this data set by trial and error. As the histograms show a large variance (small and wide peaks, high and low peaks, high and low noise levels), a mean of the best individual settings was chosen as final parameter settings (see Table 1). The analysis gives us the number of peaks, average height and a precise description of the lo-cation and size of the individual peaks (Table 2).  Parameter Definition Value  Places number of lines in the input file 1200 Peakradius  width of the peaks 30 Overlap  tolerated overlap between peaks 0.25 Accept  maximal proportion: volume peak without overlap/volume peak with overlap 25 Volfact  minimal volume of a peak: vol-fact*(average height of histo-gram)*(1+2*peakradius) 1 Heightfact  minimal height of a peak: heightfact*(average height of the histogram)  1 Table 1. Parameters used in the peak detection, with the settings used in the current analysis in the Reith column.  \n Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2009 International Society for Music Information Retrieval  \n49010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   \n Figure 1. Example of the graphical output of the pitch analysis. Peaks (cents) volume height left side % peak height  right side % peak height 91 20441 922 61 0,15 121 0,21 837 17953 538 807 0,47 867 0,29 325 9603 371 295 0,13 355 0,29 476 8305 301 446 0,17 506 0,16 1050 12313 296 1020 0,57 1080 0,52 Table 2. Example of the output of the peak analysis for the piece shown in Figure 1, showing the pitches, together with information on the size of the peak. 3.3 Metadata All the meta-data that were originally associated with the collection were digitized. Thus we get a large number of data fields from different categories: identification (num-ber/id, original carrier, reproduction rights, collector, date of recording, duration), geographic information (country, province, region, people, language), and musical content (function, participants, instrumentation). Unfortunately, not for every recording all fields are available and often these data cannot be traced, as a large part of the collection is made up of unique historical sources.  The results of the pitch and scale analysis can be cou-pled with existing meta-data such as instrumentation, geo-graphical information or date of recording. This can give us valuable information on the use of certain scales, such as geographical spread or evolution through time. As the current selection of pieces is relatively small, we used broad categories for the geographical origin (West-Africa, Southern Africa,…) and the recording time (before 1960, between 1960 and 1975, after 1975). An example of such a coupling is given in Figure 2. It gives the amount of peaks for each piece for each of the three time periods. This shows that in recent recordings, hexatonic and heptatonic scale become relatively more important while the import-ance of pentatonic and tetratonic scales diminishes.  Figure 2. Bar chart representing the amount of peaks (2-9), for each the three categories of recording time: before 1960 (n = 288), between 1960 and 1975 (n = 296) and af-ter 1975 (n = 317). 4. APPLICATIONS The analyses made can be applied in different areas. First we will show the application of the pitch detection for data-mining applications, using cross-correlation of the pitch profiles to look for similarities. Next we will show an application of the techniques for ethnomusicological research, studying the intervals used in African scales. 4.1 Correlation analysis  The chromavectors given by the pitch analysis can be cross-correlated with each other in order to search for similar scales. As African music doesn’t have a standardi-zed tuning pitch, we need to allow a shift of pitch. There-fore, the cross-correlation technique uses every permuta-tion of the original chromavector and returns the highest correlation from a list of 1200 correlations together with amount of cents it had to be shifted (Figure 4). Thus this method can be used for a query-by-example in which the output is a list of pieces with similar scales. This applica-tion allows to retrieve a song from a database without knowing any concrete fact about it, which is an important element for the usability of a search engine in a database of largely unknown music.  Next to this, the method can be useful for database management. The technique allows to check whether some songs are already present in their archive (so called double listed items), looking for perfect correlations without pitch shift. It can also help to establish groups of pieces with a similar origin, detecting possible links between recordings from different origins (cf. Figure 3). This could eventually lead to determination of missing meta-data. Although the results of this analysis are promising, still some optimizations have to be done. Thus e.g. noisy pitch profiles with broad peaks, indicating less stable pitches 02004006008001000120001002003004005006007008009001000\n491Poster Session 3\n   (e.g. from singing) (cf. Figure 3) are more likely to gener-ate high correlations compared to pieces with very clearly defined pitches. Similarly the larger the number of peaks the more difficult it gets to obtain high correlations. Some mechanisms to deal with these differences should still be developed. \n Figure 3. Graphical representation of a query-by-example, in this case the correlation is very high (r = .98) and no shift in pitch is necessary, which could point to a similar origin, despite the different sources. \n \n Figure 4. Two examples of a cross-correlation analysis, where the optimal correlation is found through a pitch shift. In the upper example a relatively large shift of 296 cents (about a minor third) reveals the highest similarity (r = .80), while in the lower example only a small shift of 26 is necessary to obtain the maximum (r = .89). 4.2 Interval analysis In the analysis of 20th century Western classical music, so-called ‘interval vectors’ are used to express the intervallic content of a pitch-class set [16]. Using a Western chro-matic scale, interval vectors are limited to an array of six numbers, expressing the amount of occurrences of each possible pitch interval (from a minor second to a tritone). With the variety of intervals found in African scales, this reduction to six numbers is not possible. Nevertheless, creating a global view on the intervals that constitute the scales can give us interesting insights in the pitch structure of the music. Are there for example any specific intervals that occur often, can we see regional differences or is there an evolution through time.  For this analysis the scales obtained from the peak analysis are transformed to an array of all possible inter-vals that can be built with this scale. As we work with scales reduced to one octave, the distinction between ris-ing and falling intervals can not be made. Therefore the maximum interval size is set at 600 cents (a tritone or half an octave). For the analysis presented here, the intervals were grouped in bins of 5 cents, which gives us interval vectors of 120 elements.  \n Figure 5. Comparison of all the pitch intervals found in the scale analysis from (above) the 42 pieces from the J.S. Bach’s six cello suite and (below) our collection of 901 pieces of African music. First we can make a global analysis of the intervals. In figure 5, a comparison is made between our complete col-lection of 901 pieces and a small sample of Western tonal music (Johann Sebastian Bach’s six cello suites, played by Mstislav Rostropovich, a collection of 42 movements). In the interval analysis of the Western music we clearly see peaks corresponding to the standard intervals of 100 cents. For the African music the situation is much less clear. One similarity could be the importance of the 500 cents inter-vals (corresponding to the pure fourth/fifth), but the other 0200400600800100012000100200300400500600700800\nMR.1993.12.2!B12 (dotted) ! MR.1988.1.2 (line)   Corr=0.9816   Delay=0\n0200400600800100012000500100015002000250030003500\nMR.1997.6.8!4 (dotted) ! MR.1971.29.3!7 (full)   Corr=0.8023   Delay=296\n02004006008001000120001002003004005006007008009001000\nMR.1964.1.2!32 (dotted) ! MR.1964.1.2!33 (line)   Corr=0.8871   Delay=26\n49210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   peaks are much less well-defined and in some cases sharp peaks appear at odd intervals (e.g. 160, 370 cents). Now we can also couple the interval vectors with the meta-data. As an example we can look if we can find some differences in interval content between the three time peri-ods. The analysis of the meta-data already revealed that tone scales with larger number of pitches became more important in recent recordings (cf. supra). Do we also find an influence on the pitch content? All three interval pro-files are very irregular and show peaks at unexpected places, as seen in the global analysis. An interesting evolu-tion is seen if we look at the relative share of the intervals corresponding to the Western equally-tempered scales. Counting the relative share of the 5 relevant intervals by taking the two bins around the correct interval (e.g. 95-105 cents for the minor second), we see that the share of these intervals almost doubles in the recent recordings (Table 3). Only for the minor third we don’t see an increase, and the change is especially remarkable for the major seconds (also containg the minor sixths) and the pure fourths/fifths. A detailed view on the area in which pure fourths and fifths are found reveals an interesting evolution (Figure 6). The main peak seems to shift from 530 cents in the earliest recordings to 515 cents in the middle period to end up at 500 cents in the most recent recordings. This possibly also indicates a gradual evolution to a Western pure-fifth based tuning.  Interval < 1960 1960-1975 > 1975 min. 2nd 1,46 1,87 2,20 maj. 2nd 1,57 1,71 5,20 min. 3rd 2,26 3,39 2,84 maj. 3rd 1,25 1,28 2,78 4th/5th 2,55 2,56 5,31 sum 9,10 10,81 18,33 Table 3. Relative share (in %) of pitches in an area of 10 cents around the Western equally-tempered intervals, for three recording time periods. \nFigure 5. Relative share (in %) of pitches in bins of 5 cents between 450 and 550 cents for three recording time periods. A detailed analysis of regional differences goes beyond the scope of this paper. Yet, we can see some interesting ele-ments in relation to the global analysis of intervals. We see for example that the peak at 160 cents is present in every region. This shows that it is not a feature of a particular culture, but a ‘pan-African’ characteristic.  Further ethno-musicological work is necessary to find a possible expla-nation for the importance of this interval. Interestingly similar interval sizes are found in the music of the Middle-East, where they are classified as ‘neutral seconds’ (nei-ther minor nor major, but in between). The pitch system there however is organized according to completely differ-ent principles, so it is not clear if a direct link can be estab-lished.  5. DISCUSSION AND CONCLUSIONS We proposed a number of methods to deal with non-standardized tone-scales, as they are found in African mu-sic. Avoiding working with a priori determined categories (such as the pitches of the chromatic scale), allows a rep-resentation and analysis of a large variety of tone scales. This was illustrated by a sample of solo-music on four dif-ferent instrument types taken from the archive of the Bel-gian Royal Museum of Central-Africa. The results are promising, both for data-mining application and as a start-ing point for ethnomusicological research. Before we can expand these techniques to the whole database, several problems still have to be solved. Some important obstacles for are the presence of unaccompanied vocal music, which usually has a fluctuating pitch. This makes it very hard to extract the exact scale automatically, without applying a kind of pitch correction first. Also there is a problem with the use of percussion. The presence of percussive sounds tends to obscure the actual pitch scale used and to generate one large peak associated with the pitch of the percussion instrument. Therefore a system to suppress these percus-sive sounds should also be developed. Using this relatively small sample of 901 pieces, we could already develop some methods for ethnomusicological re-search, creating a more elaborate view on scales and tem-peraments in African music in an automated way. A global comparison between the intervals found in Western and in African scales, shows that African music does not conform to the fixed chromatic scale nor has another fixed scale, however in recent recordings there seems to be a tendency to the use of more elaborate, equally-tempered scales. Fur-ther research has to be done in these historical aspects as well as on the geographical aspects of African tone scales. These techniques lead to usable applications for query-by-example, database management and classification.  \n493Poster Session 3\n   6. REFERENCES [1] H. C. Longuet-Higgins & M. J. Steedman: “On interpreting Bach,” Machine Intelligence, vol. 6, pp. 221–241, 1971. [2] M. Leman: Music and Schema Theory, Berlin, Springer Verlag, 1995. [3] S. Pauws: “Extracting the Key from Music,” in W. Verhaegh, E. Aarts & J. Korst (eds.) Intelligent Algorithms in Ambient and Biomedical Computing, pp. 119–132, 2006. [4] Ö. Izmirli: “Localized Key Finding from Audio Using Nonnegative Matrix Factorization for Segmentation,” in Proceedings of the 8th International Conference on Music Information Retrieval (ISMIR2007), 2007. [5] C. Chuan & E. Chew: “Audio key finding: considerations in system design and case studies on Chopin’s 24 preludes,” EURASIP Journal on Applied Signal Processing, Vol. 2007, No. 1, 15 pp., 2007. [6] E. Gomez: Tonal Description of Music Audio Signals, Ph.D. Thesis, Universitat Pompeu Fabra, Barcelona, 2006. [7] J. S. Downie, K. West, A. Ehmann & E. Vincent: “The 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005): Preliminary Overview”, in Proceedings of the Sixth International Conference on Music Information Retrieval (ISMIR 2005), pp. 320-323, 2005. [8] A. Nesbit, L. Hollenberg & A. Senyard: “Towards Automatic Transcription of Australian Aboriginal Music,” in Proceedings of the 5th International Conference on Music Information Retrieval (ISMIR 2004), pp. 326-330, 2004. [9] P. Chordia, M. Godfrey & A. Rae: “Extending Con-tent-Based Recommendation: the Case of Indian Classical Music,” Proceedings of the 9th Interna-tional Conference on Music Information Retrieval (ISMIR2008), pp. 571-576, 2008. [10] H. Li & M. Leman: “A Gesture-baseed Typology of Sliding-tones in Guqin Music,” Journal of New Music Research, Vol. 36, pp. 61-82, 2007. [11] A. Krishnaswamy: “Multi-dimensional Musical Atoms in South-Indian Classical Music,” Proceed-ings of the 8th International Conference on Music Perception and Cognition (ICMPC8), 2004. [12] B. Bozkurt: “An Automatic Pitch Analysis Method for Turkish Maqam Music,” Journal of New Music Research, Vol. 37, pp. 1-13, 2008. [13] P. Heydarian, L. Jones & A. Seago: “The Analysis and Determination of the Tuning System in Audio Musical Signals,” Paper presented at the 123rd con-vention of the Audio Engineering Society, 5 pp., 2007.  [14] G. Kubik: Theory of African Music, Willemshaven, F. Noetzel, 1994.  [15] L.P. Clarisse, J.P. Martens, M. Lesaffre, B. De Baets, H. De Meyer & M. Leman: “An Auditory Model Based Transcriber of Singing Sequences, ” Proceed-ings of the 3rd International Conference on Music In-formation Retrieval (ISMIR2002), pp. 116-123, 2002. [16] A. Forte: The Structure of Atonal Music, Yale University Press, 1973.  \n494"
    },
    {
        "title": "A Discrete Filter Bank Approach to Audio to Score Matching for Polyphonic Music.",
        "author": [
            "Nicola Montecchio",
            "Nicola Orio"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418093",
        "url": "https://doi.org/10.5281/zenodo.1418093",
        "ee": "https://zenodo.org/records/1418093/files/MontecchioO09.pdf",
        "abstract": "This paper presents a system for tracking the position of a polyphonic music performance in a symbolic score, possibly in real time. The system, based on Hidden Markov Models, is briefly presented, focusing on specific aspects such as observation modeling based on discrete filterbanks, in contrast with traditional FFT-based approaches, and describing the approaches to decoding. Experimental results are provided to assess the validity of the presented model. Proof-of-concept applications are shown, which effectively employ the described approach beyond the traditional automatic accompaniment system.",
        "zenodo_id": 1418093,
        "dblp_key": "conf/ismir/MontecchioO09",
        "keywords": [
            "polyphonic music performance",
            "symbolic score",
            "Hidden Markov Models",
            "observation modeling",
            "discrete filterbanks",
            "traditional FFT-based approaches",
            "decoding",
            "experimental results",
            "validity assessment",
            "proof-of-concept applications"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nA DISCRETE FILTER BANK APPROACH TO AUDIO TO SCORE\nMATCHING FOR POLYPHONIC MUSIC\nNicola Montecchio, Nicola Orio\nDepartment of Information Engineering\nUniversity of Padova\n{nicola.montecchio,nicola.orio }@dei.unipd.it\nABSTRACT\nThis paper presents a system for tracking the position of a\npolyphonic music performance in a symbolic score, pos-\nsibly in real time. The system, based on Hidden Markov\nModels, is brieﬂy presented, focusing on speciﬁc aspects\nsuch as observation modeling based on discrete ﬁlterbanks,\nin contrast with traditional FFT-based approaches, and de-\nscribing the approaches to decoding. Experimental results\nare provided to assess the validity of the presented model.\nProof-of-concept applications are shown, which effectively\nemploy the described approach beyond the traditional au-\ntomatic accompaniment system.\n1. INTRODUCTION\nThe concept of audio to score alignment refers to the abil-\nity of a system to align a digital audio signal recorded from\na music performance with its score. More precisely, given\na recording of a music performance and its score, the aim\nof such alignment system is to match each sample of the\naudio stream with the musical event it belongs to. There\nare a number of possible applications of such technology,\nranging from the “automatic accompanist”, a software al-\nlowing solo players to practice their part while the com-\nputer plays the orchestral accompaniment, to tools for mu-\nsicological analysis or augmented audio access.\nMost systems currently used for audio to score align-\nment are based on statistical models. In particular Hidden\nMarkov Models (HMMs) [1, 5], possibly with hybrid ap-\nproaches that make use of Bayesian networks and HMMs [8]\nor Hidden Hybrid Markov / semi-Markov chains [3].\nIn this paper we propose an HMM-based system that\nfocuses on handling highly polyphonic music through the\nuse of a ﬁlterbank approach.\n2. MODEL DESCRIPTION\nThe main idea of the proposed approach is that the most\nrelevant acoustic features of a music performance can be\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.modeled statistically as observations of a Hidden Markov\nModel (HMM). The process of performing a music work\ncan be regarded as stochastic because of the freedom of\ninterpretation, yet the knowledge of the work that can be\nobtained from the score can be exploited to model the pos-\nsible performances. In the presented system, a HMM is\nbuilt according to the data contained in the music score.\nThe incoming audio signal is divided into frames of ﬁxed\nlength, with every frame corresponding to one time step\nof the HMM; the HMM performs a transition every time a\nnew audio frame is observed and the advancement of the\nperformance in the score is tracked by performing the de-\ncoding of the HMM. The crucial point is the deﬁnition of\nthe graph topology and the observation modeling while de-\ncoding is performed with well-known algorithms.\n2.1 Score Graph Modeling\nThe score modeling step aims at obtaining a graph struc-\nture representing the music content of the score. In partic-\nular, a score is represented as a sequence of events, imply-\ning that it can be transformed into a simple graph where\nstates are connected as in a chain. Two levels of abstrac-\ntion can be distinguished in the resulting graph: a score\nlevel modeling the macro-structure of the piece, that is the\nsequence of music events, and an event level dealing with\nthe structure of each music event; the distinction between\nthe two reﬂects the conceptual separation between differ-\nent sources of mismatch: the former deals with possible\nerrors both by the musicians and in the score, while the\nlatter models the duration and the acoustic features of each\nevent, which vary depending on interpretation, instrumen-\ntation, recording conditions and other factors.\n2.1.1 Score Parsing\nThe ﬁrst step in building the HMM graph is the transforma-\ntion of the symbolic score into a sequence of events. In the\ncase of a monophonic score, all the notes and explicit rests\ncorrespond to an event, while events in a polyphonic score\nare bounded by any single onset and offset of all the notes\nthat are played by the various instruments/voices (see Fig-\nure 1). Due to the large availability of already transcribed\nmusic, MIDI has been used as the score representation for-\nmat although, being provided by end users, most of the\nMIDI ﬁles contain transcription errors that may inﬂuence\nthe alignment effectiveness.\n495Poster Session 3\n(a) Original score\n (b) Event sequence\nFigure 1. Score representation\n2.1.2 Graph Topology – Score Level\nIn its simplest form, the topology of the score level graph\ndirectly represents the succession of events: the states, each\ncorresponding to a single music event, form a linear chain,\nas seen in Figure 2(a). This approach has no explicit model\nfor local differences between the score representation and\nthe actual performance that has to be aligned, thus the over-\nall alignment can be affected by local mismatches. For in-\nstance, a skipped event, which should create only a local\nmisalignment, can extend its effect also when subsequent\ncorrect events are played resulting in larger differences in\nthe alignment.\nIn order to overcome these problems, a special type of\nstates was introduced, namely ghost states – as opposed\ntoevent states, which correspond to real events in the mu-\nsic work. Ghost states were proposed in [4]. The basic\ngraph topology is modiﬁed so that each event state can\nperform a transition to an associated ghost state, which in\nturn can perform either a self-transition or a forward tran-\nsition to subsequent event states. The ﬁnal representation\nis made of two parallel chains of nodes, as shown in Fig-\nure 2(b). This approach can model local differences be-\ntween the score and the performance, because in this case\nthe most probable path can pass through one or more ghost\nstates during the mismatch and realign on the lower chain\nwhen the performance matches again the score. The transi-\ntion probabilities from event states to corresponding ghost\nstates are typically ﬁxed, while the transition probabilities\nfrom a ghost state to subsequent event states follow a de-\ncreasing function of distance: this resembles the idea of\nlocality of a mismatch due to an error.\n2.1.3 Graph Topology – Event Level\nThe event level models the expected acoustic features of\nthe incoming audio signal. Every state of this level is mod-\neled as a chain of nsustain states, each having a self-loop\nprobabilityp, possibly followed by a rest state, as shown\nin Figure 2(c). Sustain states model the features of the sus-\ntained part of an event, while rest states model the possi-\nble presence of silence at the end of each event that can\nbe due to effects such as staccato playing style. As de-\nscribed in [7], the probability of having a segment duration\ndis modeled by a negative binomial distribution, with ex-\npected value µ=n\n1−pand variance σ2=np\n(1−p)2. The\nduration of an event is modeled by setting the values of n\nandpaccordingly; in particular µis set equal to the event\nduration in the score.\nTwo cases can be distinguished depending on the choice\nof havingnﬁxed or variable. In the former case event du-\nN1 N2 N3 N4(a) Score level (simplest topology)\nN1 N2 N3 N4G1 G2 G3 G4\n(b) Score level, with ghost states\np p p p r1−prS1 Si Sn R1−p\n1−p\n21−p\n2\n(c) Event level\nFigure 2. Graph topologies\nration is modeled by self-loop probability. This approach\nis easy to implement and with a small nthe total number\nof states in the graph is relatively small and proportional to\nthe number of events; on the other hand the variance of the\ndistribution changes with events duration. The latter case\nallows for a more precise modeling of event duration. It is\nreasonable to compute nandpin order to have σ2=kµ,\nwherepis constant for all the events, and the only parame-\nter responsible for the event duration is the number of sus-\ntain states, of which the total number is thus proportional\nto the duration of the score.\n2.2 Modeling the Observations\nThe fundamental assumption of the model is that states of\nthe event level emit the expected acoustic features of the\nincoming signal. Because polyphonic pitch detection is\nstill unreliable, the signal itself is not analyzed, instead its\nharmonic features are compared to the expected features of\nthe emissions of the HMMs.\n2.2.1 Sustain States\nThe core feature used by the observation modeling of sus-\ntain states is the similarity, for each audio frame, between\nthe spectrum of the incoming signal and an ideal spec-\ntrum of the sustain state that is being considered. Sophis-\nticated techniques have been proposed making use of spe-\nciﬁc knowledge of instrument timbre [2]. Although very\neffective in speciﬁc situations, such as contemporary mu-\nsic performances where the instruments can be sampled,\nthis kind of approach is not suitable for the general case\nwhere the instrument cannot be known in advance from\nthe score.\nTypically, spectrum analysis is done via the Fast Fourier\nTransform: the energies for the various frequency bands\nare computed by summing the energies in the appropriate\nFFT bins. The problem with this approach is that the linear\nfrequency resolution of the FFT leads to a signiﬁcant loss\nof precision in the lower frequency range. While the situa-\ntion is partially compensated by upper harmonics a differ-\n49610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nent strategy can nevertheless improve the performances of\na system.\nIn our approach, the frequency resolution problem is\nhandled using a bank of discrete ﬁlters. In particular, each\none is a second order ﬁlter of the form\nHi(z) =(1−ri)/radicalbig\n1−2ricos(2θi) +r2\ni\n(1−rie−jθiz−1)(1−riejθiz−1)(1)\nwhich has unit gain at θi(the normalized nominal frequency\nof thei-th note), and allows, by changing the pole radius\nri, to set the ﬁlter bandwidth; each ﬁlter output is then\nrouted to a delay line in order to compensate for the dif-\nferent group delays: assuming that each ﬁlter has the same\nbandwidth in semitones, the ﬁlters corresponding to the\nlowest notes have a much higher group delay than the high-\nest ones. We assume that this delay, which can be removed\noff-line or compensated in real time applications, is to be\npreferred to a lack of frequency resolution for lower notes.\nA comparison of FFT and Filterbank analysis is presented\nin Section 3.3.\nThe observation probability of a note is computed by\npartitioning the spectrum into frequency bands, with each\nband corresponding to a note in the music scale. Let Ef\ni\nbe the energy of the i-th ﬁlter output signal in the current\nframe, i.e.Ef\ni=/summationtext\nty2\ni(t); the energy En\nicorresponding\nto thei-th note can be deﬁned as\nEn\ni=/summationdisplay\njEf\ni+h(j)(2)\nwherewj= 1 andh(j)is a simple map between the in-\ndex of a harmonic and the corresponding note index. In\nthis very simple instrument model, the energy for the note\nC3 is computed as the sum of the energies for the ﬁlters\ncorresponding to the notes C3, C4, G4, C5, E5, and so on.\nThe observation probability for the i-th sustain state is\ncomputed as\nb(s)\ni=F(En\ni\nEtot) (3)\nwhereEiis the energy in the expected frequency bands\nandEtotis the total energy of the audio frame. F(·)is the\nunilateral exponential probability density function\nF(x) =eλ\neλ−1λeλ(x−1)0≤x≤1 (4)\nOther similarity functions can be applied with similar re-\nsults, in particular the cosine distance between the vec-\ntor representations of the simple instrument model used to\ncomputeEiand the ﬁlter output energies.\nWhile the above approach is robust enough for mono-\nphonic alignment, the complexity of polyphony makes it\npreferable to apply a different weighting of the harmonics\nin the instrument model. A possible solution is to modify\nEquation 2 by adding decreasing weights to the note har-\nmonics to reﬂect a more realistic instrument model. When\nﬁlters overlap for some harmonics of different notes, the\nweight assigned to that harmonic in the instrument model\ncan be either the sum or the maximum of the individual\nweights; the latter solution seems to perform better, and theintuitive explanation is that typical scores do not contain\nprecise information about the loudness of each note/part,\nso a simpler model is more general.\n2.2.2 Rest States\nThe observation probability for the i-th rest state is com-\nputed as a decreasing function of the ratio of the current\naudio frame energy over a reference threshold represent-\ning the maximum signal energy.\nb(r)\ni=F(Etot\nEthres) (5)\nThe threshold is adaptive, to compensate for possible dif-\nferences in the overall recording volume of different input\nstreams.\n2.2.3 Ghost States\nA simple approach for modeling the observations of ghost\nstates is to assign a ﬁxed value to the observation proba-\nbilities, because these states are meant to provide a sort of\n“emergency exit” for local matches. The approach can be\nimproved by computing the observation probability for the\ni-th ghost state as:\nb(g)\ni=i+k/summationdisplay\nj=iwi(j)b(s)\nj (6)\nthat is, a weighted sum of the sustain observation proba-\nbilities of the following event states, where wi(·)is a de-\ncreasing discrete distribution function and its presence is\nmotivated by the fact that, intuitively, in case of wrong or\nskipped notes, the notes actually played would probably be\nclose to the expected ones. In case of errors in the score,\nthe weighting function induces the system to quickly re-\nalign on near notes.\n2.3 Decoding Strategies\nThe proposed system exploits the decoding algorithms de-\nscribed in [6], depending on the application context, namely\nforward decoding andforward-backward decoding . These\nstrategies determine, at each time interval, the most proba-\nble state , without forcing the decoded sequence of states to\nactually be the most probable sequence of states as is the\ncase for Viterbi decoding . Preliminary tests showed that\nthe system recovers more quickly, because the decoded se-\nquence does not need to be a feasible state sequence.\nFigure 3 compares a typical evolution of the state prob-\nabilities for the forward and forward-backward decoding\nalgorithms. The latter is characterized by a more precise\nevolution, a highly desirable behavior in the case of subse-\nquent events with the same set of harmonics: if no model-\ning of a note attack is employed – as is the case with the\ncurrent version of the system – and the rest states at the\nend of the lower level event chain do not help discrimi-\nnating the events, the evolution of forward-backward de-\ncoding automatically assigns to the events a duration in\nthe alignment which is proportional to the duration in the\nscore.\n497Poster Session 3\n(a) Forward decoding\n (b) Forward-backward decoding\nFigure 3. Evolution of state probabilities\n3. EXPERIMENTAL RESULTS\nThe evaluation of an audio to score alignment system is\na difﬁcult task, mainly because of the lack of a manually\naligned test collection of polyphonic music. For instance,\nthe MIREX test collection is not publicly available because\nof copyright reasons and it contains mainly monophonic\nrecordings. For this reason, two test collections have been\nprepared, the former made up of single-instrument poly-\nphonic pieces and chamber music and the latter compris-\ning excerpts of more complex orchestral works. A experi-\nmental comparison of the FFT and Filterbank analysis ap-\nproaches is presented using recordings of tuba and cello\nmusic, characterized by a low frequency content.\n3.1 Single Instrument and Chamber Music Collection\nThe audio collection is made up of excerpts from well known\npiano, violin, and chamber music works1extracted from\nCD and home recordings; the MIDI ﬁles were downloaded\nfrom the Internet. The ﬁles in the collection have been cho-\nsen so that the complexity of their polyphony is representa-\ntive of pieces which could be realistically used in a typical\nautomatic accompaniment system, with real time require-\nments. The resulting alignments were manually checked,\nvisually inspecting the mismatches and aurally verifying\nthem by listening to a stereo recording containing the orig-\ninal piece and a synthesized version generated from the\nalignment data on different channels.\nOut of 20 test recordings, none caused the system to get\nlost, but in one case the alignment was very unstable (it\nwas always in proximity of the “true” alignment but never\nprecise) so its contribution will not be considered. For the\nother recordings the mismatches were classiﬁed according\nto their duration as either brief (shorter than two seconds)\nor long (larger time intervals, although never more than 10\nseconds); the former type of mismatches occurred 41 times\nwhile the latter 10 times, mostly on complex passages of\npolyphonic material. Example alignments can be viewed\nand heard in the authors’ home pages2, where more de-\ntailed statistics can also be found.\nBecause of the real time requirements, the forward de-\ncoding algorithm was used to compute the alignments. If\n1Bach: Italian Concerto, Goldberg Variations, Chaconne from the Vi-\nolin Partita in D minor; Beethoven: Piano Sonata op. 13, String Quar-\ntet op. 18 n. 1; Mozart: Piano Sonata KV333; Ravel: String Quartet;\nSchubert: Quartettsatz D703; Schumann: Waldszenen op. 82.\n2http://www.dei.unipd.it/˜montecc2/ismir09/\n4 6 8 10 12 14 16 18 20\naudio time (s)468101214161820 midi time (s)(a) Forward decoding\n4 6 8 10 12 14 16 18 20\naudio time (s)468101214161820 midi time (s) (b) Forward-backward decoding\nFigure 4. Typical alignment evolution\nreal time is not a constraint, usually forward-backward de-\ncoding gives better results, in which many of the glitches\nin the forward-decoded alignment are eliminated. Such an\nexample is shown in Figure 4.\nAll the alignments have been performed using the same\nmodel parameters; further experiments showed that some\nimprovements can be obtained by assigning different weights\nto the harmonics in Equation 2 for piano and string works.\nEssentially, the different weighting reﬂects the suitability\nof a more reﬁned instrument model, in particular the piano\nmodel is characterized by more rapidly decaying overtones\nthan the string model.\n3.2 Orchestral Music Collection\nThe orchestral music collection comprises 48 excerpts of\n40 seconds from CD recordings of symphonic works3; the\nMIDI scores are generally much less accurate than the ones\nused in the chamber music collection.\nA simple evaluation methodology was devised in order\nto present results for this collection. The output of the\nalignment system for a single performance/score couple\nis a list of value pairs in the form [audiotime,miditime].\nOnce all the performances in a collection are aligned to\ntheir corresponding score, these alignments are analyzed\nto extract a measure of precision based on the average de-\nviation of the alignment data from the best ﬁtting line. This\nmeasure is based on the hypothesis that an orchestra plays\nmore or less a tempo, at least in short time intervals, thus\na graphic representation of the alignment should follow a\nstraight line. While this is clearly a potentially incorrect\nassumption, the suitability of the particular performances\nin the test collection was veriﬁed by the authors. The best\nﬁtting line computed from the alignment data is thus as-\nsumed to be the correct alignment; ∆avgis deﬁned as the\naverage deviation of the alignment data point from the best\nﬁtting line. Under the assumption of a performance char-\nacterized by a steady tempo, the lower is ∆avgthe higher\nis the alignment accuracy. This evaluation methodology\nwas not used for the chamber music collection because the\ntempo was not steady enough.\nFigure 5 shows the histograms of the slope and ∆avg\ndistributions for the best ﬁtting lines obtained from the\nalignments. The tempo of the recorded performances and\nof the respective MIDI ﬁles are roughly comparable, so\n3Beethoven: Symphonies n. 3, 7, 9; Haydn: Symphony n. 104;\nMendelssohn: Symphony n. 4; Mozart: Symphonies and Serenades\nK136, K412, K525, K550; Vivaldi: The Four Seasons.\n49810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nslope histogram0246810(a) Slopes histogram\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.50510152025 (b)∆avghistogram\nFigure 5. Orchestral collection alignment results\nthe expected histogram of the slopes should be centered\naround 1; an alignment can thus be safely considered in-\ncorrect when slope values are outside the interval (0.6, 2).\nThis simple assumption allows to quickly interpret the graph-\nical results and deduce that the performance of the system\nwith orchestral music is, as expected, clearly worse than\nthe case for single instrument or chamber music, in which\nall alignments were essentially correct. Manual inspection\nof the results showed that the correct alignments were 36;\nfor those, the average ∆avgwas 0.47.\nA closer analysis pointed out that in the correct and in-\ncorrect sets of alignment the elements are homogeneous\nwith respect to the music work, e.g. all Vivaldi’s and most\nof Mozart’s music was correctly aligned while most of\nBeethoven’s were not. The reason for this was found out\nto be the fact that in the recordings of Beethoven’s works\nthe reference pitch was slightly higher than the standard\n440 Hz for A4; correcting this setting considerably im-\nproved the results for Beethoven’s music. This situation\nis a clear example of how a single set of parameters is not\nsuitable for all the possible situations, but this is typically\nnot a requirement: in the ofﬂine case multiple alignments\ncan be performed and only the best one, according to the\nsimple heuristics discussed above, can then be presented to\nthe user, while when real time is required, it is reasonable\nto assume that the system parameters can be adjusted using\nprevious rehearsals as reference.\nIn the above results, the forward decoding algorithm\nwas used to compute the alignments; the reason is that the\nforward-backward algorithm turned out to be less robust\nfor aligning performances where an alignment computed\nwith forward decoding was not precise.\n3.3 Comparison of FFT and Filterbank analysis\nSeveral experiments were performed on a small collection\nof recordings of tuba and cello music, to show the advan-\ntages of discrete Filterbank analysis over traditional FFT\nfor observation modeling on music characterized by a low\nfrequency content. The recordings were aligned manually\nin order to count the number of wrongly recognized or\nskipped notes. Of 105 total events, the FFT based system\ndid not recognize 12 and skipped 1, while the Filterbank\nbased system did not recognize only 4 notes and skipped\nnone. It should be noted that in almost all cases of not\nrecognized notes both system realigned on the correct note\nimmediately, and that the parameters of the systems were\nnot tuned for this particular situation, so that better per-\n0 2 4 6 8 10\naudio time (s)012345678 midi time (s)FFT based system\nFilterbank based systemFigure 6. Comparison of FFT and Filterbank approaches\nformances can be expected; forward decoding was used to\nsimulate a real-time operation. The alignments of the worst\nperforming recording are shown in Figure 6.\n4. APPLICATIONS\nTwo applications are presented that make use of audio to\nscore alignment technology for music analysis tasks.\n4.1 AudioZoom\nAudioZoom is a software for the auditory highlight of sin-\ngle instruments in a complex polyphony. The basic idea\nis that the alignment can help dividing a polyphonic music\nperformance into its individual components: the general\nproblem is known as source separation, which is usually\ndeﬁned blind when it is assumed that almost no informa-\ntion is available about the role of each source. In our case,\nhaving the score as a reference, the system has a com-\nplete knowledge about the notes played, at each instant,\nby all the instruments. The user, typically a teacher who\nmay exploit this tool to highlight particular instruments or\npassages to students that are not able to follow a complex\nscore, can select one or more instruments, one or more par-\nticular musical themes or patterns, or any combination, and\nthe system can selectively amplify the chosen elements.\nThe ﬁnal effect is to put on the front, or zooming, the\ninterested elements. A prototype of AudioZoom has been\ndeveloped, based on a bank of bandpass ﬁlters centered\naround the harmonics of a selected instrument, using an\napproach similar to the instrument model described in Sec-\ntion 2.2.1. The user selects one channel from the MIDI ﬁle\nthat represents the score, and the system aligns the differ-\nent ﬁlterbanks with the audio recording. An example of\nthe effect of AudioZoom, applied to the viola part of the\nbeginning of Haydn’s Symphony n. 104, is shown in the\nsonograms of Figure 7.\n4.2 Interpretation Analysis\nAnalyzing different interpretations of a music work is a\ncentral activity of musicological analysis. Of all the fea-\ntures that characterize a personal rendition, tempo is prob-\nably the most perceivable one. The alignment of two audio\nperformances allows to compare the relative tempos, but\nneither can be considered as a reference since no interpre-\ntation can be neutral. It can be noted that the concept of\nneutral interpretation is itself not well deﬁned.\n499Poster Session 3\nfrequency (Hz)\ntime (s)\n010002000300040005000\n0 5 10 15 20(a) Original excerpt\nfrequency (Hz)\ntime (s)\n010002000300040005000\n0 5 10 15 20 (b) Zoomed excerpt\nFigure 7. Effect of AudioZoom\nFigure 8. Different interpretations of the same piece\nThe alignment of two interpretations to the score allows\na musicologist to draw some considerations on the differ-\nent interpretations, for instance by comparing the instanta-\nneous tempo at each bar. Figure 8 shows an early prototype\nof a tool for the comparison of different performances, in\nwhich two interpretations of the beginning of J. S. Bach’s\nItalian Concerto are juxtaposed using the measures in the\nscore as a reference. Clearly, the prototype can be extended\nby representing the differences in loudness, the use of ac-\ncelerandi andrallentandi or more complex features related\nto timbre perception.\n5. CONCLUSIONS AND FUTURE WORK\nA system is proposed for the alignment of an audio per-\nformance with a score. The system is based on the use\nof ﬁlterbanks to extract pitch related information from the\nperformances. Comparative evaluations with previous ver-\nsions of the system showed that observation modeling based\non discrete ﬁlterbanks has some advantages with respect\nto the simpler FFT approach, resulting in higher effective-\nness. In general, evaluation showed that the approach can\nbe effectively applied to real application scenarios; many\nareas however can be improved, and below we propose\nsome research directions which seem the most promising.\nA clear priority is the creation of a collection which\ncomprises precise manual alignments, in order to properly\nevaluate the effectiveness of the approach but also to train\nthe model parameters in a rigorous way. This is a very\ntime-consuming task, requiring music experts and speciﬁc\nannotation tools for properly marking the matches between\nthe events in the scores and the corresponding time in-\nstants in the recordings. The only viable solution in ouropinion is to involve other research teams in building a\nshared collection of reasonable size; such collaborative ef-\nfort would also help in devising appropriate data and eval-\nuation methodologies for alignment system. A good start-\ning point is the collection used for the MIREX campaigns,\nwhich should be improved adding polyphonic scores and a\nclearer time reference for the alignment evaluation.\nThe introduction of a reﬁned modeling for the attack of\nnotes is desirable for many instruments with percussive at-\ntacks – in particular the piano – to better handle repeated\nnotes, but with the appropriate decoding strategies this is-\nsue is not critical. Another improvement regards the mod-\neling of complex events, such as trills or glissandi, which\nare hard to extract from MIDI ﬁles, resulting in potentially\nless effective models.\n6. ACKNOWLEDGMENTS\nThe work has been partially supported by a grant of the\nUniversity of Padova for the project “Analysis, design, and\ndevelopment of novel methodologies for the study and the\ndissemination of music works”.\n7. REFERENCES\n[1] P. Cano, A. Loscos and J. Bonada. Score-Performance\nMatching using HMMs. In Proceedings of the Interna-\ntional Computer Music Conference, pp. 441-444 1999.\n[2] A. Cont. Realtime Audio to Score Alignment for Poly-\nphonic Music Instruments Using Sparse Non-Negative\nConstraints and Hierarchical HMMs. In IEEE Interna-\ntional Conference in Acoustics and Speech Signal Pro-\ncessing, pp. V245–V248, 2006.\n[3] A. Cont. Modeling Musical Anticipation: From the\nTime of Music to the Music of Time. PhD. thesis, 2008.\n[4] N. Montecchio and N. Orio. Automatic Alignment of\nMusic Performances with Scores Aimed at Educational\nApplications. In Proceedings of the International Con-\nference on Automated solutions for Cross Media Con-\ntent and Multi-channel Distribution, pp. 17–24, 2008.\n[5] N. Orio and F. D ´echelle. Score Following Using Spec-\ntral Analysis and Hidden Markov Models. In Proceed-\nings of the International Computer Music Conference\n(ICMC), pp. 125-129, 2001.\n[6] L.R. Rabiner. A Tutorial on Hidden Markov Models\nand Selected Applications in Speech Recognition. In\nProceedings of the IEEE, 77(2):257-286, 1989.\n[7] C. Raphael. Automatic Segmentation of Acoustic Mu-\nsical Signals Using Hidden Markov Models. In IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence, 21(4):360–370, 1999.\n[8] C. Raphael. Aligning Music Audio with Symbolic\nScores using a Hybrid Graphical Model. Machine\nLearning, 65:2(389–409), 2006.\n500"
    },
    {
        "title": "Robust Segmentation and Annotation of Folk Song Recordings.",
        "author": [
            "Meinard Müller",
            "Peter Grosche",
            "Frans Wiering"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417099",
        "url": "https://doi.org/10.5281/zenodo.1417099",
        "ee": "https://zenodo.org/records/1417099/files/MullerGW09.pdf",
        "abstract": "Even though folk songs have been passed down mainly by oral tradition, most musicologists study the relation between folk songs on the basis of score-based transcriptions. Due to the complexity of audio recordings, once having the transcriptions, the original recorded tunes are often no longer studied in the actual folk song research though they still may contain valuable information. In this paper, we introduce an automated approach for segmenting folk song recordings into its constituent stanzas, which can then be made accessible to folk song researchers by means of suitable visualization, searching, and navigation interfaces. Performed by elderly non-professional singers, the main challenge with the recordings is that most singers have serious problems with the intonation, fluctuating with their voices even over several semitones throughout a song. Using a combination of robust audio features along with various cleaning and audio matching strategies, our approach yields accurate segmentations even in the presence of strong deviations.",
        "zenodo_id": 1417099,
        "dblp_key": "conf/ismir/MullerGW09",
        "keywords": [
            "segmentation",
            "stanzas",
            "folk songs",
            "score-based transcriptions",
            "audio recordings",
            "intonation",
            "visualizations",
            "searching",
            "navigation interfaces",
            "robust audio features"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nROBUST SEGMENTATION AND ANNOTATION OF\nFOLK SONG RECORDINGS\nMeinard M ¨uller\nSaarland University and\nMPI Informatik\nSaarbr ¨ucken, Germany\nmeinard@mpi-inf.mpg.dePeter Grosche\nSaarland University and\nMPI Informatik\nSaarbr ¨ucken, Germany\npgrosche@mpi-inf.mpg.deFrans Wiering\nDepartment of Information and\nComputing Sciences, Utrecht University\nUtrecht, Netherlands\nfrans.wiering@cs.uu.nl\nABSTRACT\nEven though folk songs have been passed down mainly by\noral tradition, most musicologists study the relation be-\ntween folk songs on the basis of score-based transcrip-\ntions. Due to the complexity of audio recordings, once\nhaving the transcriptions, the original recorded tunes are\noften no longer studied in the actual folk song research\nthough they still may contain valuable information. In this\npaper, we introduce an automated approach for segment-\ning folk song recordings into its constituent stanzas, whic h\ncan then be made accessible to folk song researchers by\nmeans of suitable visualization, searching, and navigatio n\ninterfaces. Performed by elderly non-professional singer s,\nthe main challenge with the recordings is that most singers\nhave serious problems with the intonation, ﬂuctuating with\ntheir voices even over several semitones throughout a song.\nUsing a combination of robust audio features along with\nvarious cleaning and audio matching strategies, our ap-\nproach yields accurate segmentations even in the presence\nof strong deviations.\n1. INTRODUCTION\nGenerally, a folk song is referred to as a song that is sung\nby the common people of a region or culture during work\nor social activities. Since many decades, signiﬁcant ef-\nforts have been carried out to assemble and study large\ncollections of folk songs [7, 12]. Even though folk songs\nwere typically transmitted only by oral tradition without\nany ﬁxed symbolic notation, most of the folk song research\nis conducted on the basis of notated music material, which\nis obtained by transcribing recorded tunes into symbolic,\nscore-based music representations. After the transcripti on,\nthe audio recordings are often no longer studied in the ac-\ntual research. Since folk songs are part of oral culture, one\nmay conjecture that performance aspects enclosed in the\nrecorded audio material are likely to bear valuable infor-\nmation, which is no longer contained in the transcriptions.\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .Furthermore, even though the notated music material may\nbe more suitable for classifying and identifying folk songs\nusing automated methods, the user may want to listen to\nthe original recordings rather than to synthesized version s\nof the transcribed tunes.\nIt is the object of this paper to indicate how the orig-\ninal recordings can be made more easily accessible for\nfolk song researches and listeners by bridging the gap be-\ntween the symbolic and the audio domain. In particular, we\npresent a procedure for automatically segmenting a given\nfolk song recording that consists of several repetitions of\nthe same tune into its individual stanzas. Using folk song\nrecordings of the Onder de groene linde (OGL), main chal-\nlenges arise from the fact that the songs are performed by\nelderly non-professional singers under poor recording con -\nditions. The singers often deviate signiﬁcantly from the\nexpected pitches and have serious problems with the into-\nnation. Even worse, their voices often ﬂuctuate by several\nsemitones downwards or upwards across the various stan-\nzas of the same recording. As our main contribution, we in-\ntroduce a combination of robust audio features along with\nvarious cleaning and audio matching strategies to account\nfor such deviations and inaccuracies in the audio record-\nings. Our evaluation on folk song recordings shows that\nwe obtain reliable segmentations even in the presence of\nstrong deviations.\nThe remainder of this paper is organized as follows. In\nSect. 2, we describe the relationship of these investigatio ns\nto folk song research and describe the folk song collec-\ntion we employ. In Sect. 3, we show how the recorded\nsongs can be segmented and annotated by locally com-\nparing and aligning the recordings’ feature representatio ns\nwith available transcriptions of the tunes. In particular,\nwe introduce various methods for achieving robustness to\nthe aforementioned pitch ﬂuctuations and recording arti-\nfacts. Then, in Sect. 4, we report on our systematic ex-\nperiments conducted on a representative selection of folk\nsong recordings. Finally, in Sect. 5, we indicate how our\nsegmentation results can be used as basis for novel user\ninterfaces, sketch possible applications towards automat ed\nperformance analysis, and give prospects on future work.\nFurther related work is discussed in the respective section s.\n735Oral Session 9-A: Folk Songs\n2. FOLK SONG RESEARCH\nFolk song reseach has been carried out from many different\nperspectives. An important problem is to reconstruct and\nunderstand the genetic relation between variants of folk\nsongs [12]. Furthermore, by systematically studying en-\ntire collections of folk songs, researchers try to discover\nmusical connections and distinctions between different na -\ntional or regional cultures [7]. To support such research,\nseveral databases of encoded folk song melodies have been\nassembled, the best known of which is the Essen folk song\ndatabase,1which currently contains roughly 20000 folk\nsongs from a variety of sources and cultures. This collec-\ntion has also been widely used in MIR research.\nEven though folk songs have been passed down mainly\nby oral tradition, most of the folk song research is con-\nducted on the basis of notated music material. How-\never, various folk song collections contain a considerable\namount of audio data, which has not yet been explored at\na larger scale. One of these collections is Onder de groene\nlinde (OGL), which is part of the Nederlandse Liederen-\nbank (NLB). The OGL collection comprises several 7277\nDutch folk song recordings along with song transcriptions\nas well as a rich set of metadata.2This metadata in-\ncludes date and location of recording, information about\nthe singer, and classiﬁcation by (textual) topic. OGL con-\ntains7277 recordings, which have been digitized as MP3\nﬁles. Nearly all of recordings are monophonic, and the\nvast majority is sung by elderly solo female singers. When\nthe collection was assembled, melodies were transcribed\non paper by experts. Usually only one strophe is given in\nmusic notation, but variants from other strophes are reg-\nularly included. The transcriptions are somewhat ideal-\nized: they tend to represent the presumed intention of the\nsinger rather than the actual performance. For about 2500\nmelodies, transcribed stanzas are available in various sym -\nbolic formats including LilyPond,3from which MIDI rep-\nresentations have been generated (with a tempo set at 120\nBPM for the quarter note).\nAn important step in unlocking such collections of\norally transmitted folk songs is the creation of content-\nbased search engines. The creation of such a search engine\nis an important goal of the WITCHCRAFT project [8].\nThe engines should enable a user to search for encoded\ndata using advanced melodic similarity methods. Further-\nmore, it should also be possible to not only visually present\nthe retrieved items, but also to supply the corresponding\naudio recordings for acoustic playback. One way of solv-\ning this problem is to create robust alignments between re-\ntrieved encodings (for example in MIDI format) and the\naudio recordings. The segmentation and annotation pro-\ncedure described in the following section exactly accom-\nplishes this task.\n1http://www.esac-data.org/\n2The OGL collection is currently hosted at the Meertens Insti tute\nin Amsterdam. The metadata of the songs are available through www.\nliederenbank.nl\n3www.lilypond.org86/accidentals.2\nvond adag zon een op wasHet86/accidentals.2\n/accidentals.2\ndeurde in stond jemeis't/accidentals.2\ntoen \n/accidentals.2\nten wach te naarmin haarop /accidentals.2\nAl\n  \n0 2 4 6 8C C#D D#E F F#G G#A A#B \n00.20.40.60.81\n  \n0 2 4 6 8C C#D D#E F F#G G#A A#B \n00.20.40.60.81\n  \n0 2 4 6 8 10C C#D D#E F F#G G#A A#B \n00.20.40.60.81\n  \n0 2 4 6 8 10C C#D D#E F F#G G#A A#B \n00.20.40.60.81(a)\n(b) (c)\n(d) (e)\nFigure 1 .Representations of the beginning of the ﬁrst stanza of\nNLB73626 (a)Score representation. (b)Chromagram of MIDI\nrepresentation. (c)Smoothed MIDI chromagram (CENS). (d)\nChromagram of audio recording (CENS). (e)F0-enhanced chro-\nmagram (see Sect. 3.4).\n3. FOLK SONG SEGMENTATION\nIn this section, we present a procedure for automatically\nsegmenting a folk song recording that consists of sev-\neral repetitions of the same tune into its individual stan-\nzas. Here, we assume that we are given a transcription\nof a reference tune in form of a MIDI ﬁle. Recall from\nSect. 2 that this is exactly the situation we have with the\nsongs of the OGL collection. In the ﬁrst step, we trans-\nform the MIDI reference as well as the audio recording\ninto a common mid-level representation. Here, we use\nthe well-known chroma representation, which is summa-\nrized in Sect. 3.1. On the basis of this feature representa-\ntion, the idea is to locally compare the reference with the\naudio recording by means of a suitable distance function\n(Sect. 3.2). Using a simple iterative greedy strategy, we\nderive the segmentation from local minima of the distance\nfunction (Sect. 3.3). This approach works well as long as\nthe singer roughly follows the reference tune and stays in\ntune. However, this is an unrealistic assumption. In par-\nticular, most singers have signiﬁcant problems with the in-\ntonation. Their voices often ﬂuctuate by several semitones\ndownwards or upwards across the various stanzas of the\nsame recording. In Sect. 3.4, we show how the segmenta-\ntion procedure can be improved to account for poor record-\ning conditions, intonation problems, and pitch ﬂuctuation s.\n3.1 Chroma Features\nIn order to compare the MIDI reference with the au-\ndio recordings, we revert to chroma-based music features,\nwhich have turned out to be a powerful mid-level represen-\ntation for relating harmony-based music, see [1, 6, 9, 11].\n73610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n0 0.1 0.2 0.3 0.4 0.5−60−40−200\n0 0.1 0.2 0.3 0.4 0.5−60−40−200dB\ndB\nFrequency ω69.5 93.569 93\nFigure 2 .Magnitude responses in dB for some of the pitch ﬁl-\nters of the multirate pitch ﬁlter bank used for the chroma compu-\ntation. Top: Filters corresponding to MIDI pitches p∈[69 : 93]\n(with respect to the sampling rate 4410 Hz). Bottom: Filters\nshifted half a semitone upwards.\nHere, the chroma refer to the 12traditional pitch classes\nof the equal-tempered scale encoded by the attributes\nC,C♯, D,..., B. Representing the short-time energy con-\ntent of the signal in each of the 12pitch classes, chroma\nfeatures do not only account for the close octave relation-\nship in both melody and harmony as it is prominent in\nWestern music, but also introduce a high degree of robust-\nness to variations in timbre and articulation [1]. Further-\nmore, normalizing the features makes them invariant to dy-\nnamic variations.\nIt is straightforward to transform a MIDI representation\ninto a chroma representation or chromagram. Using the\nexplicit MIDI pitch and timing information one basically\nidentiﬁes pitches that belong to the same chroma class\nwithin a sliding window of a ﬁxed size, see [6]. Fig. 1\nshows a score and the resulting MIDI reference chroma-\ngram. For transforming an audio recording into a chroma-\ngram, one has to revert to signal processing techniques.\nMost chroma implementations are based on short-time\nFourier transforms in combination with binning strate-\ngies [1]. In this paper, we revert to chroma features ob-\ntained from a pitch decomposition using a multirate pitch\nﬁlter bank as described in [9]. The employed pitch ﬁl-\nters possess a relatively wide passband, while still prop-\nerly separating adjacent notes thanks to sharp cutoffs in\nthe transition bands, see Fig. 2. Actually, the pitch ﬁlters\nare robust to deviations of up to ±25cents4from the re-\nspective note’s center frequency. The pitch ﬁlters will pla y\nan important role in Sect. 3.4. Finally, in our implementa-\ntion, we use a quantized and smoothed version of chroma\nfeatures referred to as CENS features [9] with a feature res-\nolution of 10Hz (10features per second), see (c) and (d) of\nFig. 1. For technical details, we refer to the cited literatu re.\n3.2 Distance Function\nWe now introduce a distance function that expresses the\ndistance of the MIDI reference chromagram with suitable\nsubsegments of the audio chromagram. More precisely,\nletX= (X(1),X(2),... ,X (K))be the sequence of\nchroma features obtained from the MIDI reference and\n4The cent is a logarithmic unit to measure musical intervals. The\nsemitone interval of the equally-tempered scale equals 100 ce nts.0 10 20 30 40 50 60 70 80 90 10000.10.20.30.40.5\n0 10 20 30 40 50 60 70 80 90 100\nFigure 3 .Top: Distance function ∆for NLB73626 using orig-\ninal chroma features (gray) and F0-enhanced chroma features\n(black). Bottom: Resulting segmentation.\nletY= (Y(1),Y(2),... ,Y (L))be the one obtained\nfrom the audio recording. In our case, the features X(k),\nk∈[1 :K], and Y(ℓ),ℓ∈[1 :L], are normalized\n12-dimensional vectors. We deﬁne the distance function\n∆ := ∆ X,Y: [1 :L]→R∪ {∞} with respect to Xand\nYusing a variant of dynamic time warping (DTW):\n∆(ℓ) :=1\nKmin\na∈[1:ℓ]/parenleftBig\nDTW/parenleftbig\nX , Y(a:ℓ)/parenrightbig/parenrightBig\n,(1)\nwhere Y(a:ℓ)denotes the subsequence of Ystarting at\nindex aand ending at index ℓ∈[1 :L]. Furthermore,\nDTW( X,Y(a:ℓ))denotes the DTW distance between X\nandY(a:ℓ)with respect to a suitable local cost measure\n(in our case, the cosine distance). The distance function ∆\ncan be computed efﬁciently using dynamic programming.\nFor details on DTW and the distance function, we refer\nto [9]. The interpretation of ∆is as follows: a small value\n∆(ℓ)for some ℓ∈[1 :L]indicates that the subsequence\nofYstarting at index aℓ(withaℓ∈[1 :ℓ]denoting the\nminimizing index in (1)) and ending at index ℓis similar\ntoX. Here, the index aℓcan be recovered by a simple\nbacktracking algorithm within the DTW computation pro-\ncedure. The distance function ∆for NLB73626 is shown\nin Fig. 3 as gray curve. The ﬁve pronounced minima of ∆\nindicate the endings of the ﬁve stanzas of the audio record-\ning.\n3.3 Audio Segmentation\nRecall that we assume that a folk song audio recording ba-\nsically consists of a number of repeating stanzas. Exploit-\ning the existence of a MIDI reference and assuming the\nrepetitive structure of the recording, we apply the follow-\ning simple greedy segmentation strategy. Using the dis-\ntance function ∆, we look for the index ℓ∈[1 :L]min-\nimizing ∆and compute the starting index aℓ. Then, the\ninterval S1:= [aℓ:ℓ]constitutes the ﬁrst segment . The\nvalue ∆(ℓ)is referred to as the cost of the segment. To\navoid large overlaps between the various segments to be\ncomputed, we exclude a neighborhood [Lℓ:Rℓ]⊂[1 :L]\naround the index ℓfrom further consideration. In our strat-\negy, we set Lℓ:= max(1 ,ℓ−2\n3K)andRℓ:= min( L,ℓ+\n2\n3K), thus excluding a range of two thirds of the reference\nlength to the left as well as to the right of ℓ. To achieve the\nexclusion, we modify ∆simply by setting ∆(m) :=∞\nform∈[Lℓ:Rℓ]. To determine the next segment S2,\n737Oral Session 9-A: Folk Songs\nthe same procedure is repeated using the modiﬁed dis-\ntance function, and so on. This results in a sequence of\nsegments S1,S2,S3,.... The procedure is repeated until\nall values of the modiﬁed ∆lie above a suitably chosen\nquality threshold τ >0. LetNdenote the number of re-\nsulting segments, then S1,S2,... ,S Nconstitutes the ﬁnal\nsegmentation result, see Fig. 3 for an illustration.\n3.4 Enhancement Strategies\nRecall that the comparison of the MIDI reference and the\naudio recording is performed on the basis of chroma rep-\nresentations. Therefore, the segmentation algorithm de-\nscribed so far only works well in the case that the MIDI\nreference and the audio recording are in the same musical\nkey. Furthermore, the singer has to stick roughly to the\npitches of the well-tempered scale. Both assumptions are\nviolated for most of the songs. Even worse, the singers of-\nten ﬂuctuate with their voice by several semitones within\na single recording. This often leads to poor local minima\nor even completely useless distance functions as illustrat ed\nFig. 4. To deal with local and global pitch deviations as\nwell as with poor recording conditions, we use a combina-\ntion of various enhancement strategies.\nIn our ﬁrst strategy, we enhance the quality of the\nchroma features similar to [4] by picking only dominant\nspectral coefﬁcients, which results in a signiﬁcant atten-\nuation of noise components. Dealing with monophonic\nmusic, we can go even one step further by only picking\nspectral components that correspond to the fundamental\nfrequency (F0). More precisely, we use a modiﬁed au-\ntocorrelation method as suggested in [3] to the estimate\nthe fundamental frequency for each audio frame. For each\nframe, we then determine the MIDI pitch having a cen-\nter frequency that is closest to the estimated fundamen-\ntal frequency. Next, in the pitch decomposition used for\nthe chroma computation, we assign energy only to the\npitch subband that corresponds to the determined MIDI\npitch—all other pitch subbands are set to zero within this\nframe. Finally, the resulting sparse pitch representation is\nprojected onto a chroma representation and smoothed as\nbefore, see Sect. 3.1. The cleaning effect on the result-\ning chromagram, which is also referred to as F0-enhanced\nchromagram , is illustrated by (d) and (e) of Fig. 1.\nEven though the folk song recordings are monophonic,\nthe F0 estimation is often not accurate enough in view of\napplications such as automated transcription. However,\nusing chroma representations, octave errors as typical in\nF0 estimations become irrelevant. Furthermore, the F0-\nbased pitch assignment is capable of suppressing most of\nthe noise resulting from poor recording conditions. Fi-\nnally, local pitch deviations caused by the singers’ into-\nnation problems as well as vibrato are compensated to a\nsubstantial degree. As a result, the desired local minima of\nthe distance function ∆, which are crucial in our segmen-\ntation procedure, become more pronounced. This effect is\nalso illustrated by Fig. 3.\nNext, we show how to deal with global pitch deviations\nand continuous ﬂuctuation across several semitones. To0 50 100 150 200 25000.20.40.60.8\n0 50 100 150 200 250\n0 50 100 150 200 250\n0 50 100 150 200 250∆\n∆trans\n∆fluc\nFigure 4 .Distance functions ∆(light gray), ∆trans(dark gray),\nand∆ﬂuc(black) for the song NLB73286 as well as the resulting\nsegmentations.\nStanza 1 2 3 4 5 6 7 8 9 10\n12shift 5 5 5 4 4 4 4 3 3 3\n24shift 5.0 5.0 4.5 4.5 4.0 4.0 3.5 3.5 3.0 3.0\nTable 1 .Shift indices (cyclically shifting the audio chroma-\ngrams upwards) used for transposing the various stanzas of the\naudio recording of NLB73286 to optimally match the MIDI ref-\nerence, see also Fig. 4. The shift indices are given in semitones\n(obtained by ∆trans) and in half semitones (obtained by ∆ﬂuc).\naccount for a global difference in key between the MIDI\nreference and the audio recording, we revert to the ob-\nservation by Goto [5] that the twelve cyclic shifts of a\n12-dimensional chroma vector naturally correspond to the\ntwelve possible transpositions. Therefore, it sufﬁces to\ndetermine the shift index that minimizes the chroma dis-\ntance of the audio recording and MIDI reference and then\nto cyclically shift the audio chromagram according to this\nindex. Note that instead of shifting the audio chromagram,\none can also shift the MIDI chromagram in the inverse di-\nrection. The minimizing shift index can be determined ei-\nther by using averaged chroma vectors as suggested in [11]\nor by computing twelve different distance functions for the\ntwelve shifts, which are then minimized to obtain a sin-\ngle transposition invariant distance functions. We detail on\nthe latter strategy, since it also solves part of the problem\nhaving a ﬂuctuating voice within the audio recording. A\nsimilar strategy was used in [10] to achieve transposition\ninvariance for music structure analysis tasks.\nWe simulate the various pitch shifts by considering all\ntwelve possible cyclic shifts of the MIDI reference chro-\nmagram. We then compute a separate distance function\nfor each of the shifted reference chromagrams and the orig-\ninal audio chromagram. Finally, we minimize the twelve\nresulting distance functions, say ∆0,... ,∆11, to obtain a\nsingle transposition invariant distance function ∆trans:\n[1 :L]→R∪ {∞} :\n∆trans(ℓ) := min i∈[0:11]/parenleftBig\n∆i(ℓ)/parenrightBig\n. (2)\nFig. 4 shows the resulting function ∆transfor a folk song\nrecording with strong ﬂuctuations. In contrast to the orig-\ninal distance function ∆, the function ∆transexhibits a\nnumber of signiﬁcant local minima that correctly indicate\n73810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nthe segmentation boundaries of the stanzas.\nSo far, we have accounted for transpositions that refer to\nthe pitch scale of the equal-tempered scale. However, the\nabove mentioned voice ﬂuctuation are ﬂuent in frequency\nand do not stick to a strict pitch grid. Recall from Sect. 3.1\nthat our pitch ﬁlters can cope with ﬂuctuations of up to\n±25cents. To cope with pitch deviations between 25and\n50cents, we employ a second ﬁlter bank, in the following\nreferred to as half-shifted ﬁlter bank , where all pitch ﬁl-\nters are shifted by half a semitone ( 50cents) upwards, see\nFig. 2. Using the half-shifted ﬁlter bank, one can compute\na second chromagram, referred to as half-shifted chroma-\ngram . A similar strategy is suggested in [4, 11] where gen-\neralized chroma representations with 24or36bins (instead\nof the usual 12bins) are derived from a short-time Fourier\ntransform. Now, using the original chromagram as well as\nthe half-shifted chromagram in combination with the re-\nspective 12cyclic shifts, one obtains 24different distance\nfunctions in the same way as described above. Minimiza-\ntion over the 24functions yields a single function ∆ﬂuc\nreferred to as ﬂuctuation invariant distance function . The\nimprovements achieved by this novel distance function are\nillustrated by Fig. 4. Table 1 shows the optimal shift in-\ndices derived from the transposition and ﬂuctuation invari -\nant segmentation strategies, where the decreasing indices\nindicate to which extend the singer’s voice rises across the\nvarious stanzas of the song.\n4. EXPERIMENTS\nOur evaluation is based on a dataset consisting of 47repre-\nsentative folk song recordings selected from the OGL col-\nlection, see Sect. 2. The evaluation audio dataset has a to-\ntal length of 156minutes, where each of the recorded song\nconsists of 4to34stanzas amounting to a total number of\n465stanzas. The recordings reveal signiﬁcant deteriora-\ntions concerning the audio quality as well as the singer’s\nperformance. Furthermore, in various recordings the tunes\nare overlayed with sounds such as ringing bells, singing\nbirds, or barking dogs, and sometimes the songs are inter-\nrupted by remarks of the singers. We manually annotated\nall audio recordings by specifying the segment boundaries\nof the stanzas’ occurrences in the recordings. Since for\nmost cases the end of a stanza more or less coincides with\nthe beginning of the next stanza and since the beginnings\nare more important in view of retrieval and navigation ap-\nplications, we only consider the starting boundaries of the\nsegments in our evaluation. In the following, these bound-\naries are referred to as ground truth boundaries .\nTo assess the quality of the ﬁnal segmentation result,\nwe use precision and recall values. To this end, we check\nto what extent the 465manually annotated stanzas within\nthe evaluation dataset have been identiﬁed correctly by the\nsegmentation procedure. More precisely, we say that a\ncomputed starting boundary is a true positive , if it coin-\ncidences with a ground truth boundary up to a small toler-\nance given by a parameter δmeasured in seconds. Other-\nwise, the computed boundary is referred to as a false pos-\nitive. Furthermore, a ground truth boundary that is not inStrategy F0 P R F α β γ\n∆ − 0.898 0.628 0.739 0.338 0.467 0.713\n∆ + 0.884 0.688 0.774 0.288 0.447 0.624\n∆trans− 0.866 0.817 0.841 0.294 0.430 0.677\n∆trans+ 0.890 0.890 0.890 0.229 0.402 0.559\n∆ﬂuc− 0.899 0.901 0.900 0.266 0.409 0.641\n∆ﬂuc+ 0.912 0.940 0.926 0.189 0.374 0.494\nTable 2 .Performance measures for various segmentation strate-\ngies using the tolerance parameter δ= 2and the quality threshold\nτ= 0.4. The second column indicates whether original ( −) or\nF0-enhanced ( +) chromagrams are used.\nδ P R F\n1 0.637 0.639 0.638\n2 0.912 0.940 0.926\n3 0.939 0.968 0.953\n4 0.950 0.978 0.964\n5 0.958 0.987 0.972τ P R F\n0.1 0.987 0.168 0.287\n0.2 0.967 0.628 0.761\n0.3 0.950 0.860 0.903\n0.4 0.912 0.940 0.926\n0.5 0.894 0.944 0.918\nTable 3 .Dependency of the PR-based performance measures on\nthe tolerance parameter δand the quality threshold τ. All values\nrefer to ∆ﬂucusing F0-enhanced chromagrams. Left: PR-based\nperformance measures for various δand ﬁxed τ= 0.4.Right:\nPR-based performance measures for various τand ﬁxed δ= 2.\naδ-neighborhood of a computed boundary is referred to as\nafalse negative . We then compute the precision Pand the\nrecall Rboundary identiﬁcation task. From these values\none obtains the F-measure F := 2 ·P·R/(P + R) .\nTable 2 shows the PR-based performance measures of\nour segmentation procedure using different distance func-\ntions with original as well as F0-enhanced chromagrams.\nIn this ﬁrst experiment, the tolerance parameter is set to\nδ= 2 and the quality threshold to τ= 0.4. Here, a tol-\nerance of up to δ= 2 seconds seems to us an acceptable\ndeviation in view of our intended applications. For exam-\nple, the most basic distance function ∆with original chro-\nmagrams yields an F-measure of F = 0 .739. Using F0-\nenhanced chromagrams instead of the original ones results\ninF = 0 .774. The best result of F = 0 .926is obtained\nwhen using ∆ﬂucwith F0-enhanced chromagrams. Note\nthat all of our introduced enhancement strategies result in\nan improvement in the F-measure. In particular, the recall\nvalues improve signiﬁcantly when using the transposition\nand ﬂuctuation-invariant distance functions.\nA manual inspection of the segmentation results showed\nthat most of the false negatives as well as false positives\nare due to deviations in particular at the stanzas’ begin-\nnings. The entry into a new stanza seems to be a problem\nfor some of the singers, who need some seconds before\ngetting stable in intonation and pitch. A typical example\nis NLB72355. Increasing the tolerance parameter δ, the\nPR-based performance measures improve substantially, as\nindicated by Table 3 (left). For example, using δ= 3 in-\nstead of δ= 2, the F-measure increase from F= 0.926\ntoF= 0.953. Other sources of error are that the tran-\nscriptions sometimes differ signiﬁcantly from what is ac-\ntually sung, as is the case for NLB72395. Here, as was\nalready mentioned in Sect. 2, the transcripts represent the\npresumed intention of the singer rather than the actual per-\nformance. Finally, structural differences between the var -\n739Oral Session 9-A: Folk Songs\nious stanzas are a further reason for segmentation errors.\nThe handling of such structural differences constitutes an\ninteresting research problem, see Sect. 5. In a further ex-\nperiment, we investigated the role of the quality threshold\nτon the ﬁnal segmentation results, see Table 3 (right). Not\nsurprisingly, a small τyields a high precision and a low\nrecall. Increasing τ, the recall increases at the cost of a de-\ncrease in precision. The value τ= 0.4was chosen, since it\nconstitutes a good trade-off between recall and precision.\nFinally, to complement our PR-based evaluation, we in-\ntroduce a second type of more softer performance mea-\nsures that indicate the signiﬁcance of the desired minima.\nTo this end, we consider the distance functions for all songs\nwith respect to a ﬁxed strategy and chroma type. Let α\nbe the average over the cost of all ground truth segments\n(given by the value of the distance function at the corre-\nsponding ending boundary). Furthermore, let βbe the av-\nerage over all values of all distance functions. Then the\nquotient γ=α/β is a weak indicator on how well the de-\nsired minima (the desired true positives) are separated fro m\npossible irrelevant minima (the potential false positives ).\nA low value for γindicates a good separability property of\nthe distance functions. As for the PR-based evaluation, the\nsoft performance measures shown in Table 2 support the\nusefulness of our enhancement strategies.\n5. APPLICATIONS AND FUTURE WORK\nBased on the segmentation of the folk song recordings,\nwe now sketch some applications that allow folk song\nresearchers to include audio material in their investiga-\ntions. Once having segmented the audio recording into\nstanzas, each audio segment can be aligned with the MIDI\nreference by a separate MIDI-audio synchronization pro-\ncess with the objective to associate note events given by\nthe MIDI ﬁle with their physical occurrences in the au-\ndio recording, see [9]. The synchronization result can be\nregarded as an automated annotation of the entire audio\nrecording with available MIDI events. Such annotations\nfacilitate multimodal browsing and retrieval of MIDI and\naudio data, thus opening new ways of experiencing and re-\nsearching music [2]. Furthermore, aligning each stanza of\nthe audio recording to the MIDI reference yields a multi-\nalignment between all stanzas. Exploiting this alignment,\none can implement interfaces that allow a user to seam-\nlessly switch between the various stanzas of the recording\nthus facilitating a direct access and comparison of the au-\ndio material [9]. Finally, the segmentation and synchro-\nnization techniques can be used for automatically extract-\ning expressive aspects referring to tempo, dynamics, and\narticulation from the audio recording. This makes the au-\ndio material accessible for performance analysis, see [13] .\nFor the future, we plan to extend the segmentation sce-\nnario dealing with the following kind of questions. How\ncan the segmentation be done if no MIDI reference is\navailable? How can the segmentation be made robust to\nstructural differences in the stanzas? In which way do\nthe recorded stanzas of a song correlate? Where are the\nconsistencies, where are the inconsistencies? Can one ex-tract from this information musical meaningfully conclu-\nsions, for example, regarding the importance of certain\nnotes within the melodies? These questions show that the\nautomated processing of folk song recordings constitutes\na new challenging and interdisciplinary ﬁeld of research\nwith many practical implications to folk song research.\nAcknowledgement. The ﬁrst two authors are supported\nby the Cluster of Excellence on Multimodal Computing\nand Interaction at Saarland University. Furthermore, the\nauthors thank Anja V olk and Peter van Kranenburg for\npreparing part of the ground truth segmentations.\n6. REFERENCES\n[1] M. A. B ARTSCH AND G. H. W AKEFIELD ,Audio thumbnail-\ning of popular music using chroma-based representations ,\nIEEE Trans. on Multimedia, 7 (2005), pp. 96–104.\n[2] D. D AMM , C. F REMEREY , F. K URTH , M. M ¨ULLER ,AND\nM. C LAUSEN ,Multimodal presentation and browsing of mu-\nsic, in Proceedings of the 10th International Conference on\nMultimodal Interfaces (ICMI 2008), 2008.\n[3] A. DECHEVEIGN ´E AND H. K AWAHARA ,YIN, a fundamen-\ntal frequency estimator for speech and music , The Journal of\nthe Acoustical Society of America, 111 (2002), pp. 1917–\n1930.\n[4] E. G ´OMEZ ,Tonal Description of Music Audio Signals , PhD\nthesis, UPF Barcelona, 2006.\n[5] M. G OTO,A chorus-section detecting method for musical au-\ndio signals , in Proc. IEEE ICASSP, Hong Kong, China, 2003,\npp. 437–440.\n[6] N. H U, R. D ANNENBERG ,AND G. T ZANETAKIS ,Poly-\nphonic audio matching and alignment for music retrieval , in\nProc. IEEE WASPAA, New Paltz, NY , October 2003.\n[7] Z. J UH´ASZ,A systematic comparison of different European\nfolk music traditions using self-organizing maps , Journal of\nNew Music Research, 35 (June 2006), pp. 95–112(18).\n[8] F. W IERING , L. P. G RIJP, R. C. V ELTKAMP , J. G AR-\nBERS , A. V OLK,AND P.VAN KRANENBURG ,Modelling\nfolksong melodies , Interdiciplinary Science Reviews, 34.2\n(2009), forthcoming.\n[9] M. M ¨ULLER ,Information Retrieval for Music and Motion ,\nSpringer, 2007.\n[10] M. M ¨ULLER AND M. C LAUSEN ,Transposition-invariant\nself-similarity matrices , in Proceedings of the 8th Interna-\ntional Conference on Music Information Retrieval (ISMIR\n2007), September 2007, pp. 47–50.\n[11] J. S ERR`A, E. G ´OMEZ , P. H ERRERA ,AND X. S ERRA ,\nChroma binary similarity and local alignment applied to\ncover song identiﬁcation , IEEE Transactions on Audio,\nSpeech and Language Processing, 16 (2008), pp. 1138–1151.\n[12] P. VAN KRANENBURG , J. G ARBERS , A. V OLK, F. W IER-\nING, L. G RIJP,AND R. V ELTKAMP ,Towards integration of\nmusic information retrieval and folk song research , Tech. Re-\nport UU-CS-2007-016, Department of Information and Com-\nputing Sciences, Utrecht University, 2007.\n[13] G. W IDMER , S. D IXON , W. G OEBL , E. P AMPALK ,AND\nA. T OBUDIC ,In search of the Horowitz factor , AI Mag., 24\n(2003), pp. 111–130.\n740"
    },
    {
        "title": "Towards Automated Extraction of Tempo Parameters from Expressive Music Recordings.",
        "author": [
            "Meinard Müller",
            "Verena Konz",
            "Andi Scharfstein",
            "Sebastian Ewert",
            "Michael Clausen"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416024",
        "url": "https://doi.org/10.5281/zenodo.1416024",
        "ee": "https://zenodo.org/records/1416024/files/MullerKSEC09.pdf",
        "abstract": "A performance of a piece of music heavily depends on the musician’s or conductor’s individual vision and personal interpretation of the given musical score. As basis for the analysis of artistic idiosyncrasies, one requires accurate annotations that reveal the exact timing and intensity of the various note events occurring in the performances. In the case of audio recordings, this annotation is often done manually, which is prohibitive in view of large music collections. In this paper, we present a fully automatic approach for extracting temporal information from a music recording using score-audio synchronization techniques. This information is given in the form of a tempo curve that reveals the relative tempo difference between an actual performance and some reference representation of the underlying musical piece. As shown by our experiments on harmony-based Western music, our approach allows for capturing the overall tempo flow and for certain classes of music even finer expressive tempo nuances.",
        "zenodo_id": 1416024,
        "dblp_key": "conf/ismir/MullerKSEC09",
        "keywords": [
            "performance",
            "musician",
            "individual vision",
            "interpretation",
            "score",
            "analysis",
            "artistic idiosyncrasies",
            "accurate annotations",
            "note events",
            "temporal information"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTOWARDS AUTOMATED EXTRACTION OF TEMPO PARAMETERS\nFROM EXPRESSIVE MUSIC RECORDINGS\nMeinard M ¨uller, Verena Konz, Andi Scharfstein\nSaarland University and MPI Informatik\nSaarbr ¨ucken, Germany\n{meinard,vkonz,ascharfs}@mpi-inf.mpg.deSebastian Ewert, Michael Clausen\nBonn University, Computer Science\nBonn, Germany\n{ewerts,clausen}@iai.uni-bonn.de\nABSTRACT\nA performance of a piece of music heavily depends on\nthe musician’s or conductor’s individual vision and per-\nsonal interpretation of the given musical score. As ba-\nsis for the analysis of artistic idiosyncrasies, one requir es\naccurate annotations that reveal the exact timing and in-\ntensity of the various note events occurring in the perfor-\nmances. In the case of audio recordings, this annotation is\noften done manually, which is prohibitive in view of large\nmusic collections. In this paper, we present a fully auto-\nmatic approach for extracting temporal information from\na music recording using score-audio synchronization tech-\nniques. This information is given in the form of a tempo\ncurve that reveals the relative tempo difference between an\nactual performance and some reference representation of\nthe underlying musical piece. As shown by our experi-\nments on harmony-based Western music, our approach al-\nlows for capturing the overall tempo ﬂow and for certain\nclasses of music even ﬁner expressive tempo nuances.\n1. INTRODUCTION\nMusicians give a piece of music their personal touch by\ncontinuously varying tempo, dynamics, and articulation.\nInstead of playing mechanically they speed up at some\nplaces and slow down at others in order to shape a piece\nof music. Similarly, they continuously change the sound\nintensity and stress certain notes. The automated analysis\nof different interpretations, also referred to as performance\nanalysis , has become an active research ﬁeld [1–4]. Here,\none goal is to ﬁnd commonalities between different inter-\npretations, which allow for the derivation of general perfo r-\nmance rules. A kind of orthogonal goal is to capture what\nis characteristic for the style of a particular musician. Be -\nfore one can analyze a speciﬁc performance, one requires\nthe information about when and how the notes of the un-\nderlying piece of music are actually played. Therefore, as\nthe ﬁrst step of performance analysis, one has to annotate\nthe performance by means of suitable attributes that make\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .explicit the exact timing and intensity of the various note\nevents. The extraction of such performance attributes con-\nstitutes a challenging problem, in particular for the case o f\naudio recordings.\nMany researchers manually annotate the audio mate-\nrial by marking salient data points in the audio stream.\nUsing novel music analysis interfaces such as the Sonic\nVisualiser [5], experienced annotators can locate note on-\nsets very accurately even in complex audio material [2, 3].\nHowever, being very labor-intensive, such a manual pro-\ncess is prohibitive in view of large audio collections. An-\nother way to generate accurate annotations is to use a\ncomputer-monitored player piano . Equipped with optical\nsensors and electromechanical devices, such pianos allow\nfor recording the key movements along with the acoustic\naudio data, from which one directly obtains the desired\nnote onset information [3, 4]. The advantage of this ap-\nproach is that it produces precise annotations, where the\nsymbolic note onsets perfectly align with the physical on-\nset times. The obvious disadvantage is that special-purpos e\nhardware is needed during the recording of the piece.\nIn particular, conventional audio material taken from CD\nrecordings cannot be annotated in this way. Therefore,\nthe most preferable method is to automatically extract the\nnecessary performance aspects directly from a given audio\nrecording. Here, automated approaches such as beat track-\ning[6, 8] and onset detection [9] are used to estimate the\nprecise timings of note events within the recording. Even\nthough great research efforts have been directed towards\nsuch tasks, the results are still unsatisfactory, in partic u-\nlar for music with weak onsets and strongly varying beat\npatterns. In practice, semi-automatic approaches are ofte n\nused, where one ﬁrst roughly computes beat timings using\nbeat tracking software, which are then adjusted manually\nto yield precise beat onsets.\nIn this paper, we present a novel approach towards\nextracting temporal performance attributes from music\nrecordings in a fully automated fashion. We exploit the\nfact that for many pieces there exists a kind of “neutral”\nrepresentation in the form of a musical score (or MIDI ﬁle)\nthat explicitly provides the musical onset and pitch infor-\nmation of all occurring note events. Using music synchro-\nnization techniques, we temporally align these note events\nwith their corresponding physical occurrences in the mu-\nsic recording. As our main contribution, we describe vari-\nous algorithms for deriving tempo curves from these align-\n69Poster Session 1\n0 1 2 3 4 5 6 7 8 9−0.4−0.200.20.4\nFigure 1 .First measure of Beethoven’s Path ´etique Sonata\nOp. 13. The MIDI-audio alignment is indicated by the arrows.\nments which reveal the relative tempo differences between\nthe actual performance and the neutral reference represen-\ntation. We have evaluated the quality of the automatically\nextracted tempo curves on harmony-based Western music\nof various genres. Besides a manual inspection of a rep-\nresentative selection of real music performances, we have\nalso conducted a quantitative evaluation on synthetic audi o\nmaterial generated from randomly warped MIDI ﬁles. Our\nexperiments indicate that our automated methods yield ac-\ncurate estimations of the overall tempo ﬂow and, for cer-\ntain classes of music such as piano music, of even ﬁner\nexpressive tempo nuances.\nThe remainder of this paper is organized as follows.\nAfter reviewing some basics on music synchronization\n(Sect. 2), we introduce various algorithms for extracting\ntempo curves from expressive music recordings (Sect. 3).\nOur experiments are described in Sect. 4, and prospects on\nfuture work are sketched in Sect. 5. Further related work\nis discussed in the respective sections.\n2. MUSIC SYNCHRONIZATION\nThe largest part of Western music is based on the equal-\ntempered scale and can be represented in the form of musi-\ncal scores, which contain high-level note information such\nas onset time, pitch, and duration. In the following, we as-\nsume that a score is given in the form of a “neutral” MIDI\nﬁle, where the notes are played with a constant tempo in\na purely mechanical way. We refer to this MIDI ﬁle as\nreference representation of the underlying piece of mu-\nsic. On the other hand, we assume that the performance\nto be analyzed is given in the form of an audio recording.\nIn a ﬁrst step, we use conventional music synchronization\ntechniques to temporally align the note events with their\ncorresponding physical occurrences in the audio record-\ning [10, 11]. The synchronization result can be regarded\nas an automated annotation of the audio recording with the\nnote events given by the MIDI ﬁle, see Fig. 1.\nMost synchronization algorithms rely on some variant\nof dynamic time warping (DTW) and can be summarized\nas follows. First, the MIDI ﬁle and the audio recording\n  \n0 2 4 602468\n00.20.40.60.81\n0 2 4 602468\nFigure 2 .Left: Cost matrix and cost-minimizing alignment\npath for the Beethoven example shown in Fig. 1. The reference\nrepresentation (MIDI) corresponds to the horizontal and the per-\nformance (audio) to the vertical axis. Right: Original (black)\nand onset-rectiﬁed alignment path (red). The MIDI note onset\npositions are indicated by the blue vertical lines.\nto be aligned are converted into feature sequences, say\nX:= (x1,x2,... ,x N)andY:= (y1,y2,... ,y M), re-\nspectively. Then, an N×Mcost matrix Cis built up\nby evaluating a local cost measure cfor each pair of fea-\ntures, i. e., C((n,m)) =c(xn,ym)forn∈[1 :N] :=\n{1,2,... ,N }andm∈[1 :M]. Each tuple p= (n,m)\nis called a cellof the matrix. A (global) alignment path\nis a sequence (p1,... ,p L)of length Lwithpℓ∈[1 :\nN]×[1 :M]forℓ∈[1 :L]satisfying p1= (1,1),\npL= (N,M)andpℓ+1−pℓ∈Σforℓ∈[1 :L−1].\nHere,Σ ={(1,0),(0,1),(1,1)}denotes the set of admis-\nsible step sizes. The cost of a path (p1,... ,p L)is deﬁned\nas/summationtextL\nℓ=1C(pℓ). A cost-minimizing alignment path, which\nconstitutes the ﬁnal synchronization result, can be com-\nputed via dynamic programming from C, see Fig. 2. For\na detailed account on DTW and music synchronization we\nrefer to [11].\nBased on this general strategy, we employ a synchro-\nnization algorithm based on high-resolution audio feature s\nas described in [12]. This approach, which combines the\nhigh temporal accuracy of onset features with the robust-\nness of chroma features, generally yields robust music\nalignments of high temporal accuracy. In the following,\nwe use a feature resolution of 50Hz with each feature vec-\ntor corresponding to 20milliseconds of MIDI or audio. For\ndetails, we refer to [12].\n3. COMPUTATION OF TEMPO CURVES\nThe feeling of pulse and rhythm is one of the central com-\nponents of music and closely relates to what one gener-\nally refers to as tempo. In order to deﬁne some notion of\ntempo, one requires a proper reference to measure against.\nFor example, Western music is often structured in terms of\nmeasures and beats, which allows for organizing and sec-\ntioning musical events over time. Based on a ﬁxed time\nsignature, one can then deﬁne the tempo as the number of\nbeats per minute (BPM). Obviously, this deﬁnition requires\na regular and steady musical beat or pulse over a certain\nperiod in time. Also, the very process of measurement is\nnot as well-deﬁned as one may think. Which musical enti-\nties (e. g., note onsets) characterize a pulse? How precisel y\ncan these entities be measured before getting drowned in\nnoise? How many pulses or beats are needed to obtain a\n7010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nmeaningful tempo estimation? With these questions, we\nwant to indicate that the notion of tempo is far from be-\ning well-deﬁned. Different representations of timing and\ntempo are presented in [7].\nIn this paper, we assume that we have a reference repre-\nsentation of a piece of music in the form of a MIDI ﬁle gen-\nerated from a score using a ﬁxed global tempo (measured\nin BPM). Assuming that the time signature of the piece is\nknown, one can recover measure and beat positions from\nMIDI time positions. Given a speciﬁc performance in the\nform of an audio recording, we ﬁrst compute a MIDI-audio\nalignment path as described in Sect. 2. From this path we\nderive a tempo curve that describes for each time position\nwithin the MIDI reference (given in seconds or measures)\nthe tempo of the performance (given as a multiplicative\nfactor of the reference tempo or in BPM). Fig. 4 and Fig. 5\nshow some tempo curves for various performances.\nIntuitively, the value of the tempo curve at a certain ref-\nerence position corresponds to the slope of the alignment\npath at that position. However, due to discretization and\nalignment errors, one needs numerically robust procedures\nto extract the tempo information by using average values\nover suitable time windows. In the following, we describe\nthree different approaches for computing tempo curves us-\ning a ﬁxed window size (Sect. 3.1), an adaptive window\nsize (Sect. 3.2), and a combined approach (Sect. 3.3).\n3.1 Fixed Window Size\nRecall from Sect. 2 that the alignment path p=\n(p1,... ,p L)between the MIDI reference and the perfor-\nmance is computed on the basis of the feature sequences\nX= (x1,... ,x N)andY= (y1,... ,y M). Note that one\ncan recover beat and measure positions from the indices\nn∈[1 :N]of the reference feature sequence, since the\nMIDI representation has constant tempo and the feature\nrate is assumed to be constant.\nTo compute the tempo of the performance at a speciﬁc\nreference position n∈[1 :N], we basically proceed as\nfollows. First, we choose a neighborhood of ngiven by\nindices n1andn2withn1≤n≤n2. Using the alignment\npath, we compute the indices m1andm2aligned with n1\nandn2, respectively. Then, the tempo at nis deﬁned as\nquotientn2−n1+1\nm2−m1+1. The main parameter to be chosen in\nthis procedure is the size of the neighborhood. Further-\nmore, there are some technical details to be dealt with.\nFirstly, the boundary cases at the beginning and end of the\nreference need special care. To avoid boundary problems,\nwe extend the alignment path pto the left and right by set-\ntingpℓ:= (ℓ,ℓ)forℓ <1andpℓ:= (N+ℓ−L,M+ℓ−L)\nforℓ > L . Secondly, the indices m1andm2are in general\nnot uniquely determined. Generally, an alignment path p\nmay assign more than one index m∈[1 :M]to a given\nindex n∈[1 :N]. To enforce uniqueness, we chose the\nminimal index over all possible indices. More precisely,\nwe deﬁne a function ϕp:Z→[1 :M]by setting\nϕp(n) := min {m∈[1 :M]| ∃ℓ∈Z:pℓ= (n,m)}.\nWe now give the technical details of the sketched pro-0 10 20 30 40 50 60 700.80.850.90.951\n0 10 20 30 40 50 60 700.80.850.90.951\n0 10 20 30 40 50 60 700.80.850.90.951\n0 10 20 30 40 50 60 700.80.850.90.951(a)\n(b)\nFigure 3 .Ground truth tempo curve (step function) and various\ncomputed tempo curves. (a)τFW\nwusing a ﬁxed window size with\nsmall w(left) and large w(right). (b)τAW\nv using an adaptive\nwindow size with small v(left) and large v(right).\ncedure for the case that the neighborhoods are of a ﬁxed\nwindow (FW) size w∈N. The resulting tempo curve is\ndenoted by τFW\nw: [1 :N]→R≥0. For a given alignment\npathpand an index n∈[1 :N], we deﬁne\nn1:=n−/floorleftbigw−1\n2/floorrightbig\nand n2:=n+/ceilingleftbigw−1\n2/ceilingrightbig\n.(1)\nThenw=n2−n1+1and the tempo at reference position\nnis deﬁned by\nτFW\nw(n) =w\nϕp(n2)−ϕp(n1) + 1. (2)\nThe tempo curve τFW\nwcrucially depends on the window\nsizew. Using a small window allows for capturing sudden\ntempo changes. However, in this case the tempo curve be-\ncomes sensible to inaccuracies in the alignment path and\nsynchronization errors. In contrast, using a larger window\nsmooths out possible inaccuracies, while limiting the abil -\nity to accurately pick up local phenomena. This effect is\nalso illustrated by Fig. 3 (a), where the performance is syn-\nthesized from a temporally warped MIDI reference. We\ncontinue this discussion in Sect. 4.\n3.2 Adaptive Window Size\nUsing a window of ﬁxed size does not account for speciﬁc\nmusical properties of the piece of music. We now intro-\nduce an approach using an adaptive window size, which\nis based on the assumption that note onsets are the main\nsource for inducing tempo information. Intuitively, in pas -\nsages where notes are played in quick succession one may\nobtain an accurate tempo estimation even when using only\na small time window. In contrast, in passages where only\nfew notes are played one needs a much larger window to\nobtain a meaningful tempo estimation.\nWe now formalize this idea. We assume that the note\nonsets of the MIDI reference are given in terms of fea-\nture indices. Furthermore, for notes with the same on-\nset position we only list one of these indices. Let O=\n{o1,... ,o K} ⊆[1 :N]be the set of onset positions with\n1≤o1< o2< ... < o K≤N. The distance between\ntwo neighboring onset positions is referred to as inter on-\nset interval (IOI). Now, when computing the tempo curve\nat position n∈[1 :N], the neighborhood of nis speciﬁed\nnot in terms of a ﬁxed number wof feature indices but in\n71Poster Session 1\nterms of a ﬁxed number v∈Nof IOIs. This deﬁnes an\nonset-dependent adaptive window (AW). More precisely,\nletτAW\nv: [1 : N]→R≥0denote the tempo function to\nbe computed. To avoid boundary problems, we extended\nthe set Oto the left and right by setting ok:=o1+k−1\nfork <1andok:=oK+k−Kfork > K . First,\nwe compute τAW\nvfor all indices nthat correspond to onset\npositions. To this end, let n=ok. Then we deﬁne\nk1:=k−/floorleftbigv−1\n2/floorrightbig\nand k2:=k+/ceilingleftbigv−1\n2/ceilingrightbig\n.\nSetting n1:=ok1andn2:=ok2, the tempo at reference\nposition n=okis deﬁned as\nτAW\nv(n) :=n2−n1+ 1\nϕp(n2)−ϕp(n1) + 1. (3)\nNote that, opposed to (2), the window size n2−n1+ 1is\nno longer ﬁxed but depends on the sizes of the neighbor-\ning IOIs around the position n=ok. Finally, τAW\nv(n)is\ndeﬁned by a simple linear interpolation for the remaining\nindices n∈[1 :N]\\O. Similar to the case of a ﬁxed\nwindow size, the tempo curve τAW\nvcrucially depends on\nthe number vof IOIs, see Fig. 3 (b). The properties of the\nvarious tempo curves are discussed in detail in Sect. 4.\n3.3 Combined Strategy\nSo far, we have introduced two different approaches us-\ning on the one hand a ﬁxed window size and on the other\nhand an onset-dependent adaptive window size for com-\nputing average slopes of the alignment path. Combining\nideas from both approaches, we now present a third strat-\negy, where we ﬁrst rectify the alignment path using onset\ninformation and then apply the FW-approach on the recti-\nﬁed path for computing the tempo curve. As in Sect. 3.2,\nletO={o1,... ,o K} ⊆ [1 :N]be the set of on-\nsets. By possibly extending this set, we may assume that\no1= 1 andoK=N. Now, within each IOI given\nby two neighboring onsets n1:=okandn2:=ok+1,\nk∈[1 :K−1], we modify the alignment path pas follows.\nLetℓ1,ℓ2∈[1 :L]be the indices with pℓ1= (n1,ϕp(n1))\nandpℓ2= (n2,ϕp(n2)), respectively. While keeping the\ncellspℓ1andpℓ2, we replace the cells pℓ1+ 1,... ,p ℓ2−1\nby cells obtained from a suitably sampled linear function\nhaving the slopen2−n1+1\nϕp(n2)−ϕp(n1)+1. Here, in the sampling,\nwe ensure that the step size condition given by Σis ful-\nﬁlled, see Sect. 2. The resulting rectiﬁcation is illustrat ed\nby Fig. 2 (right). Using the rectiﬁed alignment path, we\nthen compute the tempo curve using a ﬁxed window size\nw∈Nas described in Sect. 3.1. The resulting tempo\ncurve is denoted by τFWR\nw . This third approach, as our ex-\nperiments show, generally yields more robust and accurate\ntempo estimations than the other two approaches.\n4. EXPERIMENTS\nIn this section, we ﬁrst discuss some representative exam-\nples and then report on a systematic evaluation based on\ntemporally warped music. In the following, we specify\n1 2 3 4 5 6 7 8 9 10 1110203040\n1 2 3 4 5 6 7 8 9 10 11102030404 5(a)\n(b)\n(c)\nTime in measuresBPM BPM\nFigure 4 .Tempo curves of four different interpretations played\nby different pianists of the ﬁrst ten measures (slow introductory\ntheme marked Grave ) of Beethoven’s Path ´etique Sonata Op. 13.\n(a)Score of measures 4and5.(b)Tempo curves τFWR\nw forw∝\n3seconds. (c)Tempo curves τAW\nvforv= 10 IOIs.\nthe window size win terms of seconds instead of sam-\nples. For example, by writing w∝3seconds, we mean\nthatw∈Nis a window size with respect to the feature\nrate corresponding to 3seconds of the underlying audio.\nIn our ﬁrst example, we consider Beethoven’s\nPath´etique Sonata Op. 13. The ﬁrst ten measures corre-\nspond to the slow introductory theme marked Grave . For\nthese measure, Fig. 4 (b) shows the tempo curves τFWR\nw\nfor four different performances using the combined strat-\negy with a window size w∝3seconds. From these curves,\none can read off global and local tempo characteristics. For\nexample, the curves reveal the various tempi chosen by the\npianists, ranging from roughly 20to30BPM. One of the\npianists (red curve) signiﬁcantly speeds up after measure\n5, whereas the other pianists use a more balanced tempo\nthroughout the introduction. It is striking that all four pi -\nanists signiﬁcantly slow down in measure 8, then acceler-\nate in measure 9, before slowing down again in measure\n10. Musically, the last slow-down corresponds to the fer-\nmata at the end of measure 10, which concludes the Grave .\nSimilarly, the curves indicate a ritardando in all four per-\nformances towards the end of measure 4. In this passages,\nthere is a run of 64thnotes with a closing nonuplet, see\nFig. 4 (a). Using a ﬁxed window size, the ritardando effect\nis smoothed out to a large extent, see Fig. 4 (b). How-\never, having many consecutive note onsets within a short\npassage, the ritardando becomes much more visible when\nusing tempo curves with an onset-dependent adaptive win-\ndow size. This is illustrated by Fig. 4 (c), which shows the\nfour tempo curves τAW\nvwithv= 10 IOIs.\nAs a second example, we consider the Schubert Lied\nDer Lindenbaum (D. 911 No. 5). The ﬁrst seven measures\n(piano introduction) are shown in Fig. 5 (a). Using the\ncombined strategy with a window size w∝3seconds,\nwe computed tempo curves for 13different interpretations,\nsee Fig. 5 (b). As shown by the curves, all interpretations\nexhibit an accelerando in the ﬁrst few measures followed\n7210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n(a)\n(b)\n1 2 3 4 5 6 720406080100\nTime in measuresBPM\nFigure 5 .Tempo curves of 13different performances of the\nbeginning of the Schubert song Der Lindenbaum .(a)Score of\nmeasures 1to7.(b)Tempo curves τFWR\nw forw∝3seconds.\nby a ritardando towards the end of the introduction. Inter-\nestingly, some of the pianists start with the ritardando in\nmeasure 4already, whereas most of the other pianists play\na less pronounced ritardando in measure 6. These exam-\nples indicate that our automatically extracted tempo curve s\nare accurate enough for revealing interesting performance\ncharacteristics.\nIn view of a more quantitative evaluation, we computed\ntempo curves using different approaches and parameters\non a corpus of harmony-based Western music of various\ngenres. To allow for a reproduction of our experiments,\nwe used pieces from the RWC music database [13]. In\nthe following, we consider 15representative pieces, which\nare listed in Table 1. These pieces include ﬁve classical pi-\nano pieces, ﬁve classical pieces of various instrumentatio ns\n(full orchestra, strings, ﬂute, voice) as well as ﬁve jazz\npieces and pop songs. To automatically determine the ac-\ncuracy of our tempo extraction procedures, we temporally\nmodiﬁed MIDI ﬁles for each of the 15pieces. To this end,\nwe generated continuous piecewise linear tempo curves\nτGT, referred to as ground-truth tempo curves . These\ncurves have a constant slope on segments of roughly 10\nseconds of duration, where the slopes are randomly gen-\nerated either using a value v∈[1 : 2] (corresponding\nto an accelerando) or using a value v∈[1/2 : 1] (cor-\nresponding to a ritardando). These values cover a range\nof tempo changes of ±100% of the reference tempo. In-\ntuitively, the ground-truth tempo curves simulate on each\nsegment a gradual transition between two tempi to mimic\nritardandi and accelerandi. For an example, we refer to\nFig. 6. We then temporally warped each of the original\nMIDI ﬁles with respect to a ground-truth tempo curve τGT\nand generated from the modiﬁed MIDI ﬁle an audio ver-\nsion using a high-quality synthesizer. Finally, we com-\nputed tempo curves using the original MIDI ﬁles as ref-\nerence and the warped audio versions as performances.\nTo determine the accuracy of a computed tempo curve\nτ, we compared it with the corresponding ground-truth\ntempo curve τGT. Here, the idea is to measure devia-\ntions by scale rather than by absolute value . Therefore,0 10 20 30 40 50 60 70 80 90 100 110 12000.511.52\nFigure 6 .Piecewise linear ground-truth tempo curve (red) and\ncomputed tempo curves (black).\nFW A W FWR\nRWC ID (Comp./Int., Instr.) µ σ µ σ µ σ\nC025 (Bach, piano) 3.29 7.30 2.60 5.05 1.59 2.86\nC028 (Beethoven, piano) 3.24 6.98 6.36 21.14 2.66 6.72\nC031 (Chopin, piano) 3.32 7.72 2.77 4.76 1.75 3.42\nC032 (Chopin, piano) 2.54 4.17 3.05 4.67 1.56 2.34\nC029 (Schumann, piano) 4.52 8.86 4.18 5.97 2.44 5.13\nC003 (Beethoven, orchestra) 4.20 5.39 10.58 22.97 3.56 4.79\nC015 (Borodin, strings) 2.44 2.85 4.68 9.85 2.25 2.71\nC022 (Brahms, orchestra) 1.70 1.95 2.41 2.96 1.31 1.66\nC044 (Rimski-K., ﬂute/piano) 1.62 2.59 2.47 4.27 1.61 2.58\nC048 (Schubert, voice/piano) 2.61 3.27 3.95 7.76 2.07 2.98\nJ001 (Nakamura, piano) 1.44 1.87 1.44 2.43 1.03 1.59\nJ038 (HH Band, big band) 2.24 2.96 3.20 5.41 1.91 2.74\nJ041 (Umitsuki, sax/bass/perc.) 1.88 2.40 3.75 4.69 1.72 2.34\nP031 (Nagayama, electronic) 2.01 2.42 8.35 14.89 1.94 2.39\nP093 (Burke, voice/guitar) 2.50 3.26 6.21 14.74 2.34 3.13\nAverage over all 2.64 4.27 4.40 8.77 1.98 3.16\nTable 1 .Tempo curve evaluation using the approaches FW and\nFWR (with w∝4seconds) and AW (with v= 10 IOIs). The ta-\nble shows for each of the 15pieces the mean error µand standard\ndeviation σ(given in percent) of the computed tempo curves and\nthe ground truth tempo curve. For generating the ground-truth\ntempo curves, MIDI segments of 10seconds were used.\nas distance function, we use the average multiplicative dif -\nference and standard deviation (both measured in percent)\nofτandτGT. More precisely, we deﬁne\nµ(τ,τGT) = 100 ·1\nN·N/summationdisplay\nn=1/parenleftbig\n2|log2(τ(n)/τGT(n))|−1/parenrightbig\n.\nSimilarly, we deﬁne the standard deviation σ(τ,τGT). For\nexample, one obtains µ(τ,τGT) = 100% in the case\nτ= 2·τGT(double tempo) and in the case τ=1\n2·τGT\n(half tempo). Similarly, a computed tempo of 110BPM or\n90.9BPM would imply a mean error of µ= 10% assum-\ning a ground-truth tempo of 100BPM.\nIn a ﬁrst experiment, we computed the curves τFW\nwand\nτFWR\nw withw∝4seconds as well as τAW\nvwithv= 10\nIOIs for each of the 15pieces. Table 1 shows the mean\nerrorµand standard deviation σbetween the computed\ntempo curves and the ground truth tempo curves. For ex-\nample, for the Schubert song Der Lindenbaum with iden-\ntiﬁer C048 , the mean error between the computed tempo\ncurve τFW\nw and the ground-truth tempo τGTamounts\nto2.61%. This error decreases to 2.07% when using\nthe FWR-approach based on the rectiﬁed alignment path.\nLooking at the average mean error over all pieces, one\ncan notice that the error amounts to 2.64% for the FW-\napproach, 4.40% for the AW-approach, and 1.98% for\nthe FWR-approach. For example, assuming a tempo of\n100 BPM, the last number implies a mean difference of\nless than 2BPM between the computed tempo and the ac-\ntual tempo.\nIn general, the FWR-approach yields the best tempo es-\n73Poster Session 1\nFW FWR A Ww[sec]µ σ µ σv[IOI]µ σ\n1 10.62 49.88 5.58 12.47 2 14.50 31.00\n2 5.37 14.21 3.58 6.16 4 9.54 23.44\n3 4.39 6.90 3.42 5.34 6 7.34 17.34\n4 4.62 6.52 3.99 5.74 8 6.18 12.99\n5 5.48 7.08 5.06 6.63 10 5.65 10.66\n6 6.79 8.02 6.52 7.74 12 5.46 9.48\n7 8.40 9.19 8.22 9.00 16 5.54 8.20\n8 10.15 10.51 10.03 10.38 20 5.98 8.09\nTable 2 .Tempo curve evaluation using the approaches FW, AW,\nand FWR with various window sizes w(given in seconds) and\nv(given in IOIs). The table shows the average values over all\n15pieces, see Table 1. For generating the ground-truth tempo\ncurves, MIDI segments of 5seconds were used.\ntimation, whereas the AW-approach often produces poorer\nresults. Even though the onset information is of crucial\nimportance for estimating local tempo nuances, the AW-\napproach relies on accurate alignment paths that correctly\nalign the note onsets. Synchronization approaches as de-\nscribed in [12] can produce highly accurate alignments in\nthe case of music with pronounced note attacks. For ex-\nample, this is the case for piano music. In contrast, such\ninformation is often missing in string or general orches-\ntral music. This is the reason why the purely onset-based\nAW-strategy yields a relatively poor tempo estimation with\na mean error of 10.58% for Beethoven’s Fifth Symphony\n(identiﬁer C003 ). On the other hand, using a ﬁxed window\nsize without relying on onset information, local alignment\nerrors cancel each other out, which results in better tempo\nestimations. E. g., the error drops to 3.56% for Beethoven’s\nFifth Symphony when using the FWR-approach.\nFinally, we investigated the dependency of the accuracy\nof the tempo estimation on the window size. We generated\nstrongly ﬂuctuating ground-truth tempo curves using MIDI\nsegments of only 5seconds length (instead of 10seconds\nas in the last experiment). For the corresponding synthe-\nsized audio ﬁles, we computed tempo curves for various\nwindow sizes. The mean errors averaged over all 15pieces\nare shown in Table 2. The numbers show that the mean\nerror is minimized when using medium-sized windows.\nE. g., in the FWR-approach, the smallest error of 3.42%\nis attained for a window size of w∝3seconds. Actually,\nthe window size constitutes a trade-off between robustness\nand temporal resolution. On the one hand, using a larger\nwindow, possible alignment errors cancel each other out,\nthus resulting in a gain of robustness. On the other hand,\nsudden tempo changes and ﬁne agogic nuances can be re-\ncovered more accurately when using a smaller window.\n5. CONCLUSIONS\nIn this paper, we have introduced automated methods for\nextracting tempo curves from expressive music recordings\nby comparing the performances with neutral reference rep-\nresentations. In particular when using a combined strategy\nthat incorporates note onset information, we obtain accu-\nrate and robust estimations of the overall tempo progres-\nsion. Here, the window size constitutes a delicate trade-\noff between susceptibility to alignment errors and sensibi l-\nity towards timing nuances of the performance. In prac-tice, it becomes a difﬁcult problem to determine whether\na given change in the tempo curve is due to an align-\nment error or whether it is the result of an actual tempo\nchange in the performance. Here, one idea for future work\nis to use tempo curves as a means for revealing problem-\natic passages in the music representations where synchro-\nnization errors may have occurred with high probability.\nFurthermore, it is of crucial importance to further improve\nthe temporal accuracy of synchronization strategies. This\nconstitutes a challenging research problem in particular\nfor music with less pronounced onset information, smooth\nnote transitions, and rhythmic ﬂuctuation.\nAcknowledgements: The ﬁrst three authors are sup-\nported by the Cluster of Excellence on Multimodal Com-\nputing and Interaction at Saarland University. The last au-\nthor is funded by the German Research Foundation (DFG\nCL 64/6-1).\n6. REFERENCES\n[1] J. Langner and W. Goebl, “Visualizing expressive perfor-\nmance in tempo-loudness space,” Computer Music Journal ,\nvol. 27(4), pp. 69–83, 2003.\n[2] C. S. Sapp, “Comparative analysis of multiple musical per-\nformances,” in ISMIR Proceedings , pp. 497–500, 2007.\n[3] G. Widmer, “Machine discoveries: A few simple, robust lo-\ncal expression principles,” Journal of New Music Research ,\nvol. 31(1), pp. 37–50, 2002.\n[4] G. Widmer, S. Dixon, W. Goebl, E. Pampalk, and A. Tobudic,\n“In search of the Horowitz factor,” AI Magazine , vol. 24(3),\npp. 111–130, 2003.\n[5] Sonic Visualiser. Retrieved 19.03.2009, http://www.\nsonicvisualiser.org/ .\n[6] S. Dixon, “Automatic extraction of tempo and beat from\nexpressive performances,” Journal of New Music Research ,\nvol. 30, pp. 39–58, 2001.\n[7] H. Honing, “From Time to Time: The Representation of\nTiming and Tempo,” Computer Music Journal , vol. 25(3),\npp. 50–61, 2001.\n[8] E. D. Scheirer, “Tempo and beat analysis of acoustical mu-\nsical signals,” Journal of the Acoustical Society of America ,\nvol. 103, no. 1, pp. 588–601, 1998.\n[9] J. P. Bello, L. Daudet, S. Abdallah, C. Duxbury, M. Davies,\nand M. B. Sandler, “A Tutorial on Onset Detection in Music\nSignals,” IEEE Trans. on Speech and Audio Proc. , vol. 13,\nno. 5, pp. 1035–1047, 2005.\n[10] N. Hu, R. Dannenberg, and G. Tzanetakis, “Polyphonic audio\nmatching and alignment for music retrieval,” in Proc. IEEE\nWASPAA, New Paltz, NY , October 2003.\n[11] M. M ¨uller, Information Retrieval for Music and Motion .\nSpringer, 2007.\n[12] S. Ewert, M. M ¨uller, and P. Grosche, “High resolution audio\nsynchronization using chroma onset features,” in Proceed-\nings of IEEE International Conference on Acoustics, Speech,\nand Signal Processing , (Taipei, Taiwan), 2009.\n[13] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka, “RWC\nmusic database: Popular, classical and jazz music databases,”\ninISMIR , 2002.\n74"
    },
    {
        "title": "Lyric Extraction and Recognition on Digital Images of Early Music Sources.",
        "author": [
            "Eric Nichols",
            "Donald Byrd"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417046",
        "url": "https://doi.org/10.5281/zenodo.1417046",
        "ee": "https://zenodo.org/records/1417046/files/NicholsB09.pdf",
        "abstract": "Optical music recognition (OMR) is one of the most promising tools for generating large-scale, distributable libraries of musical data. Much OMR work has focussed on instrumental music, avoiding a special challenge vocal music poses for OMR: lyric recognition. Lyrics complicate the page layout, making it more difficult to identify the regions of the page that carry musical notation. Furthermore, users expect a complete OMR process for vocal music to include recognition of the lyrics, reunification of syllables when they have been separated, and alignment of these lyrics with the recognised music. Unusual layouts and inconsistent practises for syllabification, however, make lyric recognition more challenging than traditional optical character recognition (OCR). This paper surveys historical approaches to lyric recognition, outlines open challenges, and presents a new approach to extracting text lines in medieval manuscripts, one of the frontiers of OMR research today.",
        "zenodo_id": 1417046,
        "dblp_key": "conf/ismir/NicholsB09",
        "keywords": [
            "Optical music recognition",
            "vocal music",
            "lyric recognition",
            "lyric recognition challenges",
            "medieval manuscripts",
            "unusual layouts",
            "syllabification",
            "alignment",
            "traditional optical character recognition",
            "frontiers of OMR research"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   LYRIC EXTRACTION AND RECOGNITION ON DIGITAL IMAGES OF EARLY MUSIC SOURCES John Ashley Burgoyne Johanna Devaney Yue Ouyang Laurent Pugin Tristan Himmelman Ichiro Fujinaga Centre for Interdisciplinary Research in Music and Media Technology McGill University Montréal, Québec, Canada {ashley,devaney,laurent,ich}@music.mcgill.ca {yue.ouyang,tristan.himmelman}@mail.mcgill.ca  ABSTRACT Optical music recognition (OMR) is one of the most promising tools for generating large-scale, distributable libraries of musical data. Much OMR work has focussed on instrumental music, avoiding a special challenge vocal music poses for OMR: lyric recognition. Lyrics compli-cate the page layout, making it more difﬁcult to identify the regions of the page that carry musical notation. Fur-thermore, users expect a complete OMR process for vocal music to include recognition of the lyrics, reuniﬁcation of syllables when they have been separated, and alignment of these lyrics with the recognised music. Unusual lay-outs and inconsistent practises for syllabiﬁcation, how-ever, make lyric recognition more challenging than tradi-tional optical character recognition (OCR). This paper surveys historical approaches to lyric recognition, out-lines open challenges, and presents a new approach to extracting text lines in medieval manuscripts, one of the frontiers of OMR research today. 1. INTRODUCTION Researchers in music information retrieval (MIR) have gradually been building bigger databases of music that will enable large-scale computational musicology. One tool to expedite the development of such databases for older music is optical music recognition (OMR), the musical analogue to optical character recognition (OCR). When developing databases of vocal music, however, OMR alone is not enough: lyrics need to be recognised and stored along with the music, and because of certain particularities of musical notation, standard OCR tools are often insufﬁcient for this task. Moreover, lyrics com-plicate the page layout, making it more difﬁcult to con-duct the basic image processing necessary to feed the OMR and OCR pipelines Vocal music predominates among medieval music manuscripts, copied by hand from the ninth through the sixteenth centuries, as well as early printed music from the ﬁfteenth and sixteenth centuries. These sources pose still more challenges for OMR and lyric recognition. Due to the relatively loose document production techniques of the time, page layouts can be highly non-standard and oriented more for display than consumption. Further-more, as a result of ageing, early music documents are often in physically poor condition. Typical problems in-clude non-uniform illumination, stains, and irregular page shape. Ink frequently has bled through from the reverse side of the page or shows through as a result of high-contrast microﬁlm photography. All of these degradations can inhibit the performance of segmentation (identifying the regions of the image that correspond to musical nota-tion, lyrics, or other elements) and recognition (interpret-ing image shapes as musical notes or letters) [1]. Finally, scanning conventions for early documents themselves can require extra preprocessing to remove elements like rulers and colour bars before images are even sent to an OMR system [2]. This paper outlines some previous approaches to lyric recognition, highlights the open challenges, especially with respect to early documents, and presents two tools we have developed to facilitate work with digital images of early music: a new lyric editor for the Aruspix OMR package [3] and a new technique for extracting lyric lines from digital facsimiles of medieval manuscripts.  2. BACKGROUND OMR systems have been working to handle lyrics for some time, and most systems share common challenges. One is layout analysis: how can these systems determine which regions of a page carry music and which lyrics? Some systems rely on heuristics based on projections, run lengths, and other local image features, generally seeking to extract the music ﬁrst and deﬁne the remaining regions as text [3,4]; others run OCR ﬁrst, using those regions where the system successfully identiﬁes letters as lyric regions [6]. Once the regions have been separated, most systems then send the music regions and lyric regions to parallel OMR and OCR pipelines for fuller processing. OCR itself is difﬁcult for lyrics, however, because of inconsistent syllabiﬁcation [6]. When sung over many notes, lyric words are usually (but not always) separated into their constituent syllables, which poses problems for  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2009 International Society for Music Information Retrieval  \n723Oral Session 8: Lyrics\n   traditional OCR methods that rely on statistical language models with word dictionaries [7,8]. The need for sylla-bicated dictionaries limits the number of languages avail-able for OCR and increases the number of unrecognised inﬂections [9]. Early documents add another twist on ac-count of archaic spellings and scribal abbreviations [10]. Some systems sidestep these concerns by noting that the output of standard OCR on these documents is often good enough for users to be able to locate documents in a data-base [11], but for archival purposes, the ultimate goal of these systems is to have lyrics that are complete and ac-curate. 3. EDITING LYRICS WITH ARUSPIX No automatic lyric recognition system will ever be per-fect, however, and because digital archivists are among the primary target users for recognition software, these mistakes need to be corrected. These corrections can in-cur signiﬁcant labour expenses in the absence of efﬁcient software tools, to the point that, as we have seen in [11], sometimes they are not made at all. From the perspective of software design, then, an integrated editor for lyrics is a useful adjunct to any OMR package for vocal music. We chose to extend Aruspix, an open-source OMR appli-cation that already includes an integrated editor for musi-cal symbols, to include a new editor for lyrics. In the model underlying our lyric editor, lyrics are as-sociated with notes in a many-to-one relationship, i.e., each musical note can be associated with one or more lyric elements. The user can modify these relationships and the lyrics themselves with a convenient graphical interface that pairs the editing region with the analogous portion of the original music image, as illustrated in Figure 1. Similar to the music editor in Aruspix, our lyric editor operates in one of two modes: Lyric Editing, which allows users to change the location of lyrics and their as-sociated notes, and Lyric Insertion, which allows users to enter new lyrics and modify existing ones. Using this edi-tor, any recognition errors can be identiﬁed and correctly quickly, which reduces labour costs for archival projects and facilitates rapid production of ground truth for re-searchers. 4. A METHOD FOR LYRIC-LINE EXTRACTION Text-line detection is the ﬁrst step in most OCR systems, and a great number of approaches have been developed for different types of documents: projection-based meth-ods, grouping methods, and the Hough transform, among others [12]. Our lyric-line extraction algorithm, illustrated in Figure 2, is derived from an approach to text-line de-tection that is optimal for undulating lines [13,14]. Al-though lyrics are laid out less consistently than the text in text-only documents, they are almost always grouped along straight horizontal lines. After the removal of the staves, these straight lyric lines contrast sharply with the undulating lines, also detected by these algorithms, that trace the path of musical notes. More speciﬁcally, if base-lines of both lyrics and notes are generated, the former will be almost straight while the latter will be highly curved and undulating. Thus, a line of lyrics can be ex-tracted with conﬁdence if many straight segments are found along the line. This assumption is fairly safe when there are a good number of words within a lyric line, e.g., \n \nFigure 1. Screenshot of the lyric editor in Aruspix. The original image appears in the top pane. The lower pane contains the lyric editor. Each lyric is linked to a note; in this case, the word inter is linked to the third note on the top staff. Figure 2. Workflow for extracting lyric lines. During preprocessing, the image is binarised and staves are re-moved. A three-step extraction process follows for lyrics. Original image Image without staff Staffspace height \nLyric-line mask Staff       Removal Lyric-line            Detection Baseline detection     Mark local minimum vertices Detect potential baseline segments Merge adjacent po-tential segments Validate potential baselines Lyric baseline frag-ments Lyric height estimation     Lyric fragment reconstruction Lyric height estimation Lyric height Lyric-line region re-construction Binarisation Staff removal / Staffspace estimation \n72410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   \n  (a) Original image  \n (c) Local minima after staff-removal.      (b) Reconstructed staff lines   \n (d) Extracted lyric lines    they are arranged particularly sparsely. Figure 3. Results of text-line extraction. Image (a), the original image, illustrates some of the layout challenges inherent to medieval documents. Reconstructed staff lines from our staff-removal step appear in (b). The “local minima” of each connected component appear in (c), and the final output of lyric-line extraction is in (d).  / \n725Oral Session 8: Lyrics\n   the lyric lines in Figure 3(a), but it can miss lyrics when they are arranged particularly sparsely. Before detecting the lyric lines, the images must be preprocessed by global deskewing and staff removal. Be-cause music recognition takes place in a distinct, parallel process, we developed a new staff-removal algorithm that damages noteheads slightly but removes staff lines more reliably in degraded documents than traditional tech-niques. The technique is in the style of the median-filter approach in [15]. Local horizontal projections and verti-cal run-length coding are used to estimate staff-space height and staff height. These values are used to construct a directional median-filter window in the form of a thin vertical bar. The bar is set to be tall enough to remove staff lines while remaining short enough to preserve notes and lyrics. Because the window is thin, this filter is able to remove curved staff lines effectively. Unlike some ap-proaches, our algorithm is also able to locate the lyrics in documents without staff lines, e.g., early Aquitanian chant manuscripts; for such images, we obviously skip the staff-removal filter. A sample of staff lines extracted by this algorithm appears in Figure 3(b). Following preprocessing, our method has three broad steps: baseline detection, estimation of lyric height, and reconstruction of lyric regions. Baseline estimation begins by binarising the image (classifying pixels as foreground or background) and identifying all connected components of foreground pixels. Every component is represented by groups of vertices constituting the “local minima” of the compo-nent: the component is broken into vertical strips that are about as wide as a staff space is high, and the point clos-est to the bottom of the page is retained as a local mini-mum for the component. Figure 3(c) illustrates a chart of these local minima for one of our test images. We then “connect the dots” to extract baselines. Each unconnected local minimum is connected to its nearest neighbour with respect to a quadratic thresholding func-tion that weights the distance between the two points and the angle between them, privileging short distances and approximately horizontal lines (see Figure 4). This thres-holding function is rather strict, and so unless lyrics are packed densely across a line, the connected baseline seg-ments usually underscore individual words or letters ra-ther than longer lyric lines. A second pass with a more permissive thresholding function is made to connect suf-ﬁciently long segments extracted from the ﬁrst pass. Finally, all connected segments are validated to ensure that they contain a reasonable number of local minima and have an overall horizontal slope. This ﬁnal validation step removes any segments that might arise from musical lines with repeated notes or sequences of notes that are close in pitch space. In extracting the baselines, we discard all information about the height of the lyrics, and so it is necessary to re-construct lyric regions from the baselines. The process is the inverse of marking the local minima: for each local minimum identiﬁed in a baseline, the corresponding con-nected component is included in the lyric region. Some of these connected components, however, include non-lyric elements, especially when lyric elements close to a staff overlap with low notes. In order to compensate for these problems, an upper bound for lyric height is chosen based on the size of a staff space. Connected components above this upper bound are cropped. After the lyric height has been estimated, complete lyric-line regions are generated from the baselines. In the absence of other information about page layout, the lyric lines are extended from the left-most to the right-most points of the page. A simple peak-picking process along the y-axis groups extracted baselines that are part of common lyric lines; the peak picking is tuned with the estimated lyric height as described above. In our experi-ments, a simple linear regression on the baselines com-bined with estimated lyric height yields good lyric re-gions in most cases, although when the pages are non-linearly skewed, higher-order polynomials are necessary. We tested our algorithm on a set of 40 images from the Digital Image Archive of Medieval Music (DIAMM) chosen for their particularly challenging layouts [16]. A sample image is presented in Figure 3(a), and the output of our algorithm on this image is presented in Figure 3(d). Note in particular that the algorithm proved robust to the two-column format. Despite the challenging lay-outs and (in some cases) considerable document degrada-tion, our algorithm was able to recall 80.4 percent of the text lines with 88.4 percent precision overall. For clean images, however, the results are nearly perfect: there was only one recall error across our 12 cleanest samples with 100-percent precision. 5. SUMMARY AND FUTURE WORK Automatic lyric recognition is challenging for any musi-cal document on account of varied page layouts and in-consistent syllabiﬁcation. This challenge is exacerbated for early music documents, which suffer from an even wider variety of layouts and, often, signiﬁcant degrada-tion. We have developed software that helps researchers generate ground truth quickly for early music documents direction distance Figure 4. Thresholding function for connecting local minima of the connected components. The boundary is quadratic and privileges horizontal directions. \n72610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   and that helps archivists correct any errors in automatic recognition with minimum labour cost and musical sense. We have also extended approaches for document image analysis for historical text documents to be sensitive to the particularities of music manuscripts, resulting in reli-able text-line extraction from a number of difﬁcult older documents. This work is in progress, and we are currently using data we have extracted from these systems to experiment with full-scale recognition using features similar to those in [7] and a variety of labelling models, including hidden Markov models [17] and conditional random ﬁelds [18]. The long-term goal of our project is to produce a set of extensions to Aruspix that will make it a fully functional OMR system for early vocal music, including OMR for medieval plainchant notations and automatic lyric recog-nition for the most common languages of the period. 6. ACKNOWLEDGEMENTS The authors would like to thank the Canada Foundation for Innovation (CFI) and the “Image, Text, Sound, and Technology” program of the Social Sciences and Hu-manities Research Council of Canada (SSHRC) for sup-porting this research. 7. REFERENCES [1] Pinto, J. C., P. Vieira, M. Ramalho, M. Mengucci, P. Pina, and F. Muge. 2000. Ancient music recovery for digital libraries. In Proceedings of the 4th European Conference on Research and Advanced Technology for Digital Libraries, 24–34. [2] Ouyang, Y., J. A. Burgoyne, L. Pugin, and I. Fuji-naga. 2009. A robust border-detection algorithm with application to medieval manuscripts. In Proceedings  of the International Computer Music Conference. [3] Pugin, L. 2006. Optical music recognition of early typographic prints using hidden Markov models. In Proceedings of the 7th International Conference on Music Information Retrieval, 53–6. [4] Choudhury, G. S., T. DiLauro, M. Droettboom, I. Fujinaga, and K. MacMillan. 2001. Strike up the score: Deriving searchable and playable digital for-mats from sheet music. D-Lib Magazine 7 (2). [5] Jones, G., B. Ong, I. Bruno, and K. Ng. 2007. Opti-cal music imaging: Music document digitization, recognition, evaluation. In Interactive Multimedia Music Technologies, ed. K. Ng and P. Nesi, 50–79. Hershey, PA: IGI Global. [6] Droettboom, M. 2004. Beyond transcription: Case studies in special document analysis requirements. In Proceedings of the International Workshop on Document Image Analysis for Libraries. [7] Vinciarelli, A., S. Bengio, and H. Bunke. 2004. Ofﬂine recognition of unconstrained handwritten texts using HMMs and statistical language models. IEEE Transactions on Pattern Analysis and Machine Intelligence 26 (6): 709–20. [8] Steinherz, T., E. Rivlin, and N. Intrator. 1999. Ofﬂine cursive script word recognition: A survey. In-ternational Journal on Document Analysis and Rec-ognition 2 (2–3): 90–100. [9] Wingenroth, B., M. Patton, and T. DiLauro. 2002. Enhancing access to the Levy sheet music collection: Reconstructing full-text lyrics from syllables. In Pro- ceedings of the ACM-IEEE Joint Conference on Digital Libraries, 308–9. [10] Ernst-Gerlach, A., and N. Fuhr. 2007. Retrieval in text collections with historic spelling using linguistic and spelling variants. In Proceedings of the ACM-IEEE Joint Conference on Digital Libraries, 333–41. [11] Diet, J., and F. Kurth. 2007. The Probado music re-pository at the Bavarian State Library. In Proceed-ings of the 8th International Conference on Music In-formation Retrieval, 501–4. [12] Likforman-Sulem, L., Z. Abderrazak, and T. Bruno. 2007. Text line segmentation of historical docu-ments: A survey. International Journal on Document Analysis and Recognition 9 (2): 123–38. [13] Feldbach, M. and K. D. Tonnies. 2001. Line detec-tion and segmentation in historical church registers. In Proceedings of the 6th International Conference on Document Analysis and Recognition, 743–7. [14] Pu, Y., and Z. Shi, Z. 1998. A natural learning algo-rithm based on Hough transform for text lines extrac-tion in handwritten documents. In Proceedings of the 6th International Workshop on Frontiers in Hand-writing Recognition, 637–46. [15] Fornes, A., J. Llados, and G. Sanchez. 2005. Staff and graphical primitive segmentation in old hand-written scores. In Artiﬁcial Intelligence Research and Development, ed. B. López, J. Meléndez, P. Radeva, and J. Vitrià, 83–90. Amsterdam: IOS Press. [16] Digital Image Archive of Medieval Music. http://www.diamm.ac.uk/  (accessed 22 May 2009). [17] Artières, T., N. Gauthier, P. Gallinari, and B. Dorizzi. 2002. A hidden Markov models combina-tion framework for handwriting recognition. Interna-tional Journal on Document Analysis and Recogni-tion 5 (4): 233–43. [18] Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceed-ings of the International Conference on Machine Learning, 282–9. \n727"
    },
    {
        "title": "Relationships Between Lyrics and Melody in Popular Music.",
        "author": [
            "Eric Nichols",
            "Dan Morris 0001",
            "Sumit Basu",
            "Christopher Raphael"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417321",
        "url": "https://doi.org/10.5281/zenodo.1417321",
        "ee": "https://zenodo.org/records/1417321/files/NicholsMBR09.pdf",
        "abstract": "Composers of popular music weave lyrics, melody, and instrumentation together to create a consistent and compelling emotional scene. The relationships among these elements are critical to musical communication, and understanding the statistics behind these relationships can contribute to numerous problems in music information retrieval and creativity support. In this paper, we present the results of an observational study on a large symbolic database of popular music; our results identify several patterns in the relationship between lyrics and melody.",
        "zenodo_id": 1417321,
        "dblp_key": "conf/ismir/NicholsMBR09",
        "keywords": [
            "lyrics",
            "melody",
            "instrumentation",
            "musical communication",
            "music information retrieval",
            "creativity support",
            "symbolic database",
            "patterns",
            "relationships",
            "emotional scene"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nRELATIONSHIPS BETWEEN LYRICS AND MELODY  \nIN POPULAR MUSIC \nEric Nichols 1, Dan Morris 2, Sumit Basu 2, and Christopher Raphael 1 \n1Indiana University \nBloomington, IN, USA \n{epnichol,craphael}@indiana.edu  2Microsoft Research \nRedmond, WA, USA \n{dan,sumitb}@microsoft.com  \nABSTRACT \nComposers of popular music weave lyrics, melody, and \ninstrumentation together to create a consistent and com - \npelling emotional scene. The relationships among these \nelements are critical to musical communication, and un-\nderstanding the statistics behind these relationships can \ncontribute to numerous problems in music information \nretrieval and creativity support. In this paper, we present  \nthe results of an observational study on a large symbol ic \ndatabase of popular music; our results identify several \npatterns in the relationship between lyrics and melody.  \n1.  INTRODUCTION \nPopular music uses several streams of information to \ncreate an emotionally engaging experience for the liste n- \ner. Lyrics, melody, chords, dynamics, instrumentation, \nand other aspects of a song operate in tandem to produce \na compelling musical percept. Extensive previous work \nhas explored each of these elements in isolation, and cer-\ntain relationships among these components – for exam- \nple, the relationship between melody and chords – have \nalso been addressed in the research community. However, \ndespite their salience and central role in music cognitio n, \nlyrics have not been addressed by computational analysis \nto the same degree as other aspects of popular music. \nIn this study, we examine the relationship between \nlyrics and melody in popular music. Specifically, we in- \nvestigate the assumption that songwriters tend to alig n \nlow-level features of a song’s text with musical feature s. \nComposer Stephen Sondheim, for example, has com- \nmented that he selects rhythms in music to match the nat- \nural inflections of speech [1], and popular books on \nsongwriting suggest considering the natural rhythms of \nspeech when writing melodies [2]. With this qualitative \nevidence in mind, we quantitatively examine relation- \nships between text and music using a corpus of several \nhundred popular songs. Specifically, we investigate the \ngeneral hypothesis that textual salience is correlated with \nmusical salience, by extracting features representative of \neach and exploring correlations among those features.  \nThis study contributes fundamental statistics to musi- cology and music-cognition research, and makes the fol- \nlowing specific contributions to the music information \nretrieval community: \n1)  We establish new features in the hybrid space of lyr- \nics and melody, which may contribute to musical in- \nformation and genre analysis as well as music rec- \nommendation. \n2)  We demonstrate a quantitative correlation between \nlyrical and melodic features, motivating their use in \ncomposition-support tools which help composers \nwork with music and text. \n3)  We strengthen the connection between MIR and \nspeech research; the features presented here are \nclosely related to natural patterns in speech rhythm \nand prosody. \n4)  We make analysis of lyrics and melody in popular \nmusic more accessible to the community, by releas- \ning the parsing and preprocessing code developed for \nthis work. \n2.  RELATED WORK \nPrevious work in the linguistics and speech communities \nhas demonstrated that inherent rhythms are present even \nin non-musical speech (e.g. [3,4]). Additional work has \nshown that the rhythms inherent to a composer’s native \nlanguage can influence instrumental melodic composi- \ntion. Patel and Daniele [5] show a significant influen ce of \nnative language (either English or French) on composers’ \nchoice of rhythmic patterns, and Patel et al. [6] exte nd \nthis work to show a similar influence of native langua ge \non the selection of pitch intervals. This work does not  in- \nvolve text per se, only the latent effect of language on in-\nstrumental classical music. Beyond the rhythmic aspects \nof speech, additional work has demonstrated that vowels \nhave different intrinsic pitches [7], and even that pho-\nnemes present in musical lyrics can influence a listen er’s \nperception of pitch intervals [8]. This work supports our \nclaim that there is a strong connection between not only  \nrhythmic aspects of speech and music, but also between \nlinguistic, phonemic, pitch, and timbral aspects of spee ch \nand music. \nIn addition to these explorations into the fundamental \nproperties of speech and lyrics, preliminary applications  \nof the statistics of lyrics have begun to emerge for bo th \ncreativity support tools and problems in music informa- \ntion retrieval and analysis. Proposing a creativity suppo rt \ntool to explore alignments of melodies and lyrics, [9] uses Permission to make digital or hard copies of all or  part of this work for \npersonal or classroom use is granted without fee pr ovided that copies \nare not made or distributed for profit or commercia l advantage and that \ncopies bear this notice and the full citation on th e first page. \n© 2009 International Society for Music Information Retrieval  \n471Poster Session 3\n  \n \na series of hand-coded heuristics to align a known set of \nlyrics to the rhythm of a known melody. Oliveira et al. \n[10] develop a preliminary system that addresses the \nproblem of generating text to match a known rhythm; this \nworks also includes a preliminary analysis of a small da-\ntabase to qualitatively validate the authors’ assumptio ns. \nWang et al. [11] and Iskandar et al. [12] use higher-\nlevel properties of lyrical structure to improve the a uto- \nmatic alignment of recordings with corresponding lyrics.  \nLee and Cremer [13] take a similar approach to match \nhigh-level segments of lyrics to corresponding segments \nin a recording. Recent work in the music information re- \ntrieval community has also applied lyric analysis to pr ob- \nlems in topic detection [14], music database browsing \n[15], genre classification [16], style identification [17], \nand emotion estimation [18]. This work motivates the \npresent study and suggests the breadth of applications \nthat will benefit from a deeper, quantitative understan d- \ning of the relationship between lyrics and melody. \n3.  METHODS \n3.1  Data Sources and Preprocessing \nOur database consisted of 679 popular music lead sheets \nin MusicXML format. 229 of our lead sheets came from a \nprivate collection; the remaining 450 came from Wikifo-\nnia.org, an online lead sheet repository. Our data spans a \nvariety of popular genres, including pop, rock, R&B, \ncountry, Latin, and jazz, with a small sampling of folk  \nmusic. \nEach lead sheet in our database contains a melody, \nlyrics, and chords for a single song (chords were not used \nin the present analysis). Lyrics are bound to individual \nnotes; i.e., no alignment step was necessary to assign lyr- \nics to their corresponding notes. Word boundaries were \nprovided in the MusicXML data so it was possible to de- \ntermine which syllables were joined to make whole \nwords without consulting a dictionary. Key and time sig-\nnature information was also provided for each song (in- \ncluding any changes within a song). For all analyses pre- \nsented in this paper, we ignored measures of music with a  \ntime signature other than 4/4. Lead sheets were processe d \nto build a flat table of notes (pitch and duration) and  their \ncorresponding syllables, with repeats flattened (expanded \nand rewritten without repeats) to allow more straightfor - \nward analysis. \n3.2  Computed Musical Features \nThis section describes the three features that were c om- \nputed for each note in our melody data. \n3.2.1  Metric Position \nFor each note, the “Metric Position” feature was ass igned \nto one of five possible values based on the timing of th e \nnote’s onset: downbeat (for notes beginning on beat 1),  \nhalf-beat  (for notes beginning on beat 3), quarter beat  (beginning on beats 2 or 4), eighth beat  (beginning on \nthe “and” of any quarter beat), and other . \n3.2.2  Melodic Peak \nThe “Melodic Peak” feature is set to True  for any note \nwith a higher pitch than the preceding and subsequent \nnotes. It is set to False  otherwise (including notes at the \nbeginning and end of a song). We selected this feature \nbecause previous research has connected melodic con- \ntours to a number of features in instrumental music [19].  \n3.2.3  Relative Duration  \nFor a note in song s, the “Relative Duration” feature is \ncomputed by calculating the mean duration (in beats) for \nall notes in s and then dividing each note’s duration by \nthe mean. Thus “Relative Duration” values greater than 1 \nindicate notes longer than mean duration for the asso- \nciated song. \n3.3  Computed Lyrical Features \nThis section describes the three features that were c om- \nputed for each syllable in our lyric data, based on the syl-\nlable itself and/or the containing word. \nWe determined the pronunciation of each syllable by \nlooking up the containing word in the CMU Pronouncing \nDictionary [20], a public-domain, machine-readable Eng- \nlish dictionary that provides phoneme and stress level  in- \nformation for each syllable in a word. In cases where the \ndictionary provided alternate pronunciations, we selected  \nthe first one with the correct number of syllables. Un- \nknown words and words whose associated set of notes in \nour MusicXML data did not correspond in number to the \nnumber of syllables specified by the dictionary were re - \nmoved from the data. Note that this dictionary provides \npronunciation for isolated words. Stress patterns can \nchange based on the surrounding context, so this pronun- \nciation data is only an approximation of natural speech. \n3.3.1  Syllable Stress \nThe CMU dictionary gives a stress level according to th e \nfollowing ordinal scale: Unstressed , Secondary Stress , \nand Primary Stress ; each syllable was assigned one of \nthese three values for the “Syllable Stress” feature.  Sec- \nondary stress is typically assigned in words with more \nthan two syllables, where one syllable receives some  \nstress but is not the primary accent. For example, in the \nword “letterhead”, the first syllable is assigned a pr imary \nstress, the second is unstressed, and the third is assig ned a \nsecondary stress. \n3.3.2  Stopwords \nStopwords are very common words that carry little se- \nmantic information, such as “a”, “the”, and “of”. Stop- \nwords are generally ignored as “noise” in text processing \n47210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nsystems such as search engines. There is no definitive or \nabsolutely correct list of English stopwords; we use the  \nmonosyllabic subset of the online adaption [21] of the \nfairly canonical stopword list originally presented by v an \nRijsbergen [22]. We specifically choose the monosyllabi c \nsubset so that we are conservative in our identificat ion of \nstopwords; we consider words such as “never”, while \nperhaps too common for certain applications, to be se- \nmantically rich enough to merit treatment as non-\nstopwords. The “Stopword” feature is set to True  or \nFalse  for each monosyllabic word, and is undefined for \nmultisyllable words. \n3.3.3  Vowels \nEach syllable in the dictionary may include multiple con- \nsonants, but only one vowel. We extract the vowel for \neach syllable; this categorical feature can take on one  of \n15 possible values, enumerated in Table 1. \n4.  RESULTS \nHaving established a set of features in both the melodi c \nand lyrical spaces, we now turn our attention to exploring \ncorrelations among those features. 4.1  Syllable Stress \nBased on our general hypothesis that musical salience is \nfrequently associated with lyrical salience, we hypothe - \nsized that stressed syllables would tend to be associated  \nwith musically accented notes. We thus explored correla- \ntions between the “Syllable Stress” feature and eac h of \nour melodic features. Each analysis in this subsection w as \nperformed only using note data associated with polysyl- \nlabic words, so that stress values are meaningful. \n4.1.1  Syllable Stress and Metric Position \nA stronger syllable stress is associated with a stron ger \nmetric position, as we see in Figures 1 and 2. These giv e \ntwo different views of the data, based on conditioning \nfirst by either metric position  or syllable stress . \nFigure 1 demonstrates that the half beat and downbeat \npositions strongly favor stressed syllables, and are ra rely \nassociated with unstressed syllables. For comparison, \nstressed and unstressed syllables occur with approximate- \nly equal a priori probabilities (P(primary stress) = 0.46  \nCMU Vowel  IPA  (Pan -English)  Example  \nAH  ə hut  \nUH  ʊ hood  \nIH  ɪ it  \nER  ɜ hurt  \nEH  ɛ Ed  \nAE  ӕ at  \nAA  ɑː  odd  \nIY  iː eat  \nUW  uː two  \nAY  aɪ hide  \nAO  ɔ: ought  \nOW  oʊ oat  \nEY  eɪ ate  \nAW  aʊ cow  \nOY  ɔɪ  toy  \nTable 1.  Vowels used in our analysis (sorted by in- \ncreasing average associated relative note duration – \nsee section 4.4). In order to classify vowels as short, \nlong, or diphthong, vowels from the CMU dictionary \nwere translated to Pan-English IPA (International \nPhonetic Alphabet) symbols according to [23]. Sym- \nbols ending in a colon (:) represent long vowels; \nsymbols containing two characters (e.g. o ʊ) \nrepresent diphthongs. As is further elaborated in \nSection 4, we highlight that when sorted by average \nmusical note duration, short vowels are correlated \nwith shorter durations than long vowels and diph- \nthongs in all cases, and with the exception of one \nlong vowel (AO, or ɔ:), diphthongs are assigned \nlonger durations than long vowels. \nP(Metric Position | Syllable Stress)  \nFigure 2 . P(metric position | syllable stress) . Un- \nstressed syllables are very unlikely to show up on a \ndownbeat, but very likely at an 8 th  beat position. Pri- \nmary stresses rarely occur on off-beats. 00.1 0.2 0.3 0.4 0.5 0.6 \nUnstressed Secondary Stress Primary Stress Off beat \n8th beat \nQuarter beat \nHalf beat \nDownbeat P(Syllable Stress  | Metric Position)  \nFigure 1 . P(syllable stress | metric position) . The \nstronger a note’s metric position, the more likely it i s \nthat the associated syllable has a primary stress. \nSecondary stresses are rare overall and were omitted \nfrom this graph. 00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nOff beat 8th beat Quarter beat Half beat Downbeat Unstressed \nPrimary Stress \n473Poster Session 3\n  \n \nand P(unstressed) = 0.48). Figure 2 similarly shows that \nunstressed syllables are very unlikely to show up on a \ndownbeat, but very likely at an 8 th -beat position, and that \nprimary stresses rarely occur on off-beats. Pearson’s C hi-\nSquare test confirms a significant relationship between \nthese features ( p < 0.0001). \n4.1.2  Syllable Stress and Melodic Peaks \nFigure 3 shows that stronger syllable stress is also s trong- \nly associated with the occurrence of melodic peaks. This \nrelationship holds in both directions: the probability of a \nprimary stress is significantly higher at syllables co rres- \nponding to melodic peaks than at non-peaks, and the \nprobability of a melodic peak is much higher at stressed \nsyllables than non-stressed syllables. Pearson’s Chi-\nSquare test confirms a significant relationship between \nthese features (p < 0.0001). \n4.1.3  Syllable Stress and Note Duration \nIn Figure 4, the “Relative Duration” feature has been dis- cretized into two values: “Short” (Relative Duration ≤ 1, \ni.e. notes shorter than the mean duration within a song), \nand “Long” (Relative Duration > 1). Figure 4 shows tha t \nlong notes are more likely to associated with stressed s yl- \nlables than unstressed syllables, and short notes are  more \nlikely to be associated with unstressed syllables. The i n- \nverse relationship is true as well; most notes (55%) ass o- \nciated with unstressed syllables are short, and most n otes \n(55%) associated with primary-stress syllables are long . \nPearson’s Chi-Square test confirms a significant rel ation- \nship between these features (p < 0.0001).  \n4.2  Stopwords \nBased on our general hypothesis that musical salience is \nfrequently associated with lyrical salience, we hypothe - \nsized that semantically meaningful words would tend to \nbe associated with musically salient notes, and conse-\nquently that stopwords – which carry little semantic in- \nformation – would be associated with musically non-\nsalient notes. In this subsection, only notes associat ed \nwith monosyllabic words are used in the analysis, since \nour list of stopwords includes only monosyllabic words. \n4.2.1  Stopwords and Metric Position \nFigure 5 shows the probability of finding a stopword at \neach metric position. The stronger the metric position,  the \nless likely the corresponding word is to be a stopword. \nThe overall probability of a stopword (across all metr ic \npositions) is 0.59. However, the half-beat and downbeat \npositions favor non-stopwords. Pearson’s Chi-Square test  \nconfirms a significant relationship between these fe atures \n(p < 0.0001). \n4.2.2  Stopwords and Melodic Peaks \nFigure 6 shows that melodic peaks are more frequently \nassociated with non-stopwords than with stopwords. The \ninverse relationship holds as well: the probability of  ob- \nserving a stopword at a melodic peak is lower than at a \nnon-peak. Pearson’s Chi-Square test confirms a signifi-\ncant relationship between these features (p < 0.0001).  P(Melodic Peak | Syllable Stress)  \nFigure 3 . P(melodic peak | syllable stress) . The \nprobability of a melodic peak increases with increas- \ning syllable stress. 00.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 \nUnstressed Secondary Stress Primary Stress \nP(Syllable Stress | Relative Duration)  \nFigure 4 . P(syllable stress | relative duration) . The \n“Relative Duration” feature was discretized into two \nvalues: “Short” (Relative Duration ≤ 1, i.e. notes \nshorter than the mean duration within a song), and \n“Long” (Relative Duration > 1). Shorter note duratio ns \nare more likely to be associated with unstressed syl- \nlables; longer durations are more likely to be asso- \nciated with stressed syllables. 00.1 0.2 0.3 0.4 0.5 0.6 \nShort Long Unstressed \nPrimary Stress P(Stopword | Metric Position)  \nFigure 5 . P(stopword | metric position). This graph \nshows metric positions moving from weak (left) to \nstrong (right), and the corresponding decrease in the \nprobability of stopwords at corresponding syllables.  \n 00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 \nOff beat 8th beat Quarter beat Half beat Downbeat \n47410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \n4.3  Vowels \nWe hypothesized that vowel sounds would vary reliably \nwith note durations, reflecting both the aesthetic p roper- \nties of different vowel types and the impact of diffe rent \nvowel types on a singer’s performance. We thus looked at \ncorrelations between the “phonetic length” of vowels \n(short, long, or diphthong) and the average durations of \ncorresponding notes. We assign phonetic length to vowel \nlength according to the IPA convention for Pan-English \ninterpretation of phonemes (Table 1). \n4.3.1  Vowels and Relative Duration \nFigure 7 is a sorted plot of mean relative duration of n otes \nfor each vowel type. In general agreement with our hypo-\nthesis, the shorter vowels all have mean relative durat ion \nless than 1 (i.e. short vowels have shorter duration t han \naverage in a song); long vowels and diphthongs have \nmean relative duration greater than 1 (i.e. long vowels  \nhave longer duration than average). We highlight that \nshort vowels are correlated with shorter durations than \nlong vowels and diphthongs in all cases, and with the ex- \nception of one long vowel (AO, or ɔ:), diphthongs are as- \nsigned longer durations than long vowels. \nIf we generate a Boolean feature indicating whether a \nvowel is long (including diphthongs) or short, and we si- \nmilarly use the Boolean version of the “Relative Dura-\ntion” feature (see Figure 5), we can proceed as in pre- \nvious sections and correlate vowel length with relative \nduration. Figure 8 shows that longer notes are more likel y \nto be associated with long vowels, and short notes wit h \nshort vowels. Pearson’s Chi-Square test confirms the sig- \nnificance of this relationship (p < 0.0001). \n5.  DISCUSSION \n5.1  Summary of Findings \nWe have introduced an approach for analyzing relation- \nships between lyrics and melody in popular music. Here \nwe summarize the relationships presented in Section 4: \n1)  Level of syllabic stress is strongly correlated with \nstrength of metric position. 2)  Level of syllabic stress is strongly correlated with t he \nprobability of melodic peaks. \n3)  Level of syllabic stress is strongly correlated with \nnote duration. \n4)  Stopwords (which carry little semantic weight) are \nstrongly correlated with weak metric positions. \n5)  Stopwords are much less likely to coincide with me- \nlodic peaks than non-stopwords. \n6)  Short vowels tend to be associated with shorter notes \nthan long vowels, which tend to be associated with \nshorter notes than diphthongs. \nThese findings support our highest-level hypothesis: \nsongwriters tend to align salient notes with salient ly rics. \nThe strength of these relationships – and our ability to \nfind them using intuitive features in both lyrics and me- \nlody – suggests the short-term potential to apply these \nrelationships to both MIR and creativity support tools.  \n5.2  Applications and Future Work \nThe analysis presented here used features that were eas ily \naccessible in our database of symbolic popular music. Fu-\nture work will explore similar relationships among more P(Melodic Peak | Stopword)  \nFigure 6 . P(melodic peak | stopword) . Melodic peaks \nare significantly more likely to coincide with non-\nstopwords than with stopwords. 00.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 \nNon-stopword Stopword Mean Relative Duration by Vowel  \nFigure 7 . Mean relative duration of notes associated \nwith each vowel, sorted form short notes (left) to long \n(right). The resulting partitioning of similar vowel \ntypes shows that short vowels are correlated with \nshorter durations than long vowels and diphthongs in \nall cases, and with the exception of one long vowel \n(AO), diphthongs are correlated with longer duration s \nthan long vowels. \n 0.4 0.5 0.6 0.7 0.8 0.9 11.1 1.2 \nAH UH IH ER EH AE AA IY UW AY AO OW EY AW OY Short \nLong \nDiphthong \nP(Vowel Type | Relative Duration)  \nFigure 8 . P(vowel type | relative duration) . Short \nnotes are more frequently associated with short vo- \nwels, and long notes with long vowels. 00.1 0.2 0.3 0.4 0.5 0.6 \nShort Note Long Note Short Vowel \nLong Vowel \n475Poster Session 3\n  \n \ncomplex features of both lyrics (e.g. valence, parts of  \nspeech) and music (e.g. tone and timbre, dynamics, and \npronunciation data extracted from vocal performances). \nUnderstanding the statistics of lyrics alone will con-\ntribute to the many of the same applications that will ben- \nefit from our understanding of the relationship betwee n \nlyrics and music. Therefore, future work will also incl ude \na large-scale study that more deeply explores the statist ics \nand grammatical patterns inherent to popular lyrics, as \ncompared to non-musical text corpora.  \nMost importantly, future work will explore applica- \ntions of a quantitative understanding of the relations hip \nbetween lyrics and melody. For example, these relatio n- \nships can provide priors for lyric transcription and lyri c \nalignment to audio recordings. Similarly, strengthening \nthe connection between music and lyrics will allow us to  \nmore easily borrow techniques from the speech commu- \nnity for problems such as artist identification and score-\nfollowing for popular music. \nFurthermore, a quantitative understanding of the rela- \ntionship between lyrics and melody has applications in \ntools that support the creative process. Composers and \nnovices alike may benefit from systems that can suggest \nlyrics to match a given melody or vice versa, and under-\nstanding the relationships presented in this paper is an \nimportant first step in this direction. One might similar ly \nimagine a “grammar checker” for popular composition, \nwhich provides suggestions or identifies anomalies not in \ntext, but in the relationship between melody and lyrics . \n6.  PREPROCESSING TOOLKIT \nIn order to stimulate research in this area and allow repl i- \ncation of our experiments, we provide the preprocessing \ncomponents of our analysis toolkit to the community at: \nhttp://www.music.informatics.indiana.edu/code/musicxml \nThe archive posted at this location does not include our \ndatabase (for copyright reasons), but we provide instruc- \ntions for downloading the Wikifonia data set. \n7.  ACKNOWLEDGEMENTS \nData sets were provided by Wikifonia  and Scott Switzer . \n8.  REFERENCES \n[1]  M. Secrest: Stephen Sondheim, A Life . New York, \nAlfred A. Knopf, 1998. \n[2]  J. Peterik, D. Austin, and M. Bickford: Songwriting \nfor Dummies . Hoboken, Wiley, 2002. \n[3]  F. Cummins: Speech Rhythm and Rhythmic \nTaxonomy. Proc Speech Prosody , April 2002. \n[4]  M. Brady, R. Port. Quantifying Vowel Onset \nPeriodicity in Japanese. Proc 16th Intl Congress of \nPhonetic Sciences , Aug 2007. \n[5]  A.D. Patel and J.R. Daniele: An empirical \ncomparison of rhythm in language and music. Cognition , v87, p35-45, 2002. \n[6]  A.D. Patel, J.R. Iversen, and J.C. Rosenberg: \nComparing the rhythm and melody of speech and \nmusic: The case of British English and French. J. \nAcoustic Soc. Am , 119(5), May 2006. \n[7]  S. Sapir: The intrinsic pitch of vowels: Theoretical, \nphysiological and clinical considerations. Journal of \nVoice  (3) 44-51, 1998. \n[8]  F. Russo, D. Vuvan, and W. Thompson: Setting \nwords to music: Effects of phoneme on the \nexperience of interval size. Proc 9th Intl Conf on \nMusic Perception and Cognition (ICMPC) , 2006. \n[9]  E. Nichols: Lyric-Based Rhythm Suggestion. To \nappear in  Proc Intl Comp Music Conf (ICMC) 2009 . \n[10]  H. Oliveira, A. Cardoso, F.C. Pereira: Tra-la-Lyrics : \nAn approach to generate text based on rhythm. 4th  \nIntl Joint Workshop on Comp Creativity , 2007. \n[11]  Y. Wang, M.-Y. Kan, T.L. Nwe, A. Shenoy, and J. \nYin: LyricAlly: Automatic Synchronization of \nAcoustic Musical Signals and Textual Lyrics. Proc \nACM Multimedia , Oct 2004. \n[12]  D. Iskandar, Y. Wang, M.-Y. Kan, H. Li: Syllabic \nLevel Automatic Synchronization of Music Signals \nand Text Lyrics. Proc ACM Multimedia , Oct 2006. \n[13]  K. Lee and M. Cremer: Segmentation-Based Lyrics-\nAudio Alignment Using Dynamic Programming. \nProc ISMIR 2008 . \n[14]  F. Kleedorfer, P. Knees, and T. Pohle: Oh Oh Oh \nWhoah! Towards Automatic Topic Detection in \nSong Lyrics. Proc ISMIR 2008.  \n[15]  H. Fujihara, M. Goto, and J. Ogata: Hyperlinking \nLyrics: A Method for Creating Hyperlinks Between \nPhrases in Song Lyrics. Proc ISMIR 2008 . \n[16]  R. Mayer, R. Neumayer, and A. Rauber: Rhyme and \nStyle Features for Musical Genre Classification by \nSong Lyrics. Proc ISMIR 2008 . \n[17]  T. Li and M. Ogihara: Music artist style \nidentification by semi-supervised learning from both \nlyrics and content. Proc ACM Multimedia , Oct 2004. \n[18]  D. Wu., J.-S. Chang, C.-Y. Chi, C.-D. Chiu, R. Tsai, \nand J. Hsu: Music and Lyrics: Can Lyrics Improve \nEmotion Estimation for Music? Proc ISMIR 2008 . \n[19]  Z. Eitan: Highpoints: A Study of Melodic Peaks.  \nPhiladelphia, Univ of Pennsylvania Press, 1997. \n[20]  http://speech.cs.cmu.edu/cgi-bin/cmudict \nDownloaded on May 20, 2009. \n[21]  http://dcs.gla.ac.uk/idom/ir_resources/linguistic_util \ns/stop_words. Retrieved on May 15, 2009. \n[22]  C.J. van Rijsbergen: Information Retrieval  (2nd  \nedition). London, Butterworths, 1979. \n[23]  http://en.wikipedia.org/wiki/IPA_chart_for_English \n_dialects. Retrieved on May 15, 2009. \n476"
    },
    {
        "title": "Improving Accuracy of Polyphonic Music-to-Score Alignment.",
        "author": [
            "Bernhard Niedermayer"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415220",
        "url": "https://doi.org/10.5281/zenodo.1415220",
        "ee": "https://zenodo.org/records/1415220/files/Niedermayer09.pdf",
        "abstract": "This paper presents a new method to refine music-to-score alignments. The proposed system works offline in two passes, where in the first step a state-of-the art alignment based on chroma vectors and dynamic time warping is performed. In the second step a non-negative matrix factorization is calculated within a small search window around each predicted note onset, using pretrained tone models of only those pitches which are expected to be played within that window. Note onsets are then reset according to the pitch activation patterns yielded by the matrix factorization. In doing so, we are able to resolve individual notes within a chord. We show that this method is feasible of increasing the accuracy of aligned note’s onsets which are already aligned relatively near to the real note attack. However it is so far not suitable for the detection and correction of outliers which are displaced by a large timespan. We also compared our system to a reference method showing that it outperforms bandpass filtering based onset detection in the refinement step.",
        "zenodo_id": 1415220,
        "dblp_key": "conf/ismir/Niedermayer09",
        "keywords": [
            "chroma vectors",
            "dynamic time warping",
            "non-negative matrix factorization",
            "pretrained tone models",
            "pitch activation patterns",
            "outliers",
            "bandpass filtering",
            "refinement step",
            "reference method",
            "alignment based on chroma vectors"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nIMPROVING ACCURACY OF POLYPHONIC MUSIC-TO-SCORE\nALIGNMENT\nBernhard Niedermayer\nDepartment for Computational Perception\nJohannes Kepler University Linz, Austria\nbernhard.niedermayer@jku.at\nABSTRACT\nThis paper presents a new method to reﬁne music-to-score\nalignments. The proposed system works ofﬂine in two\npasses, where in the ﬁrst step a state-of-the art alignment\nbased on chroma vectors and dynamic time warping is per-\nformed. In the second step a non-negative matrix factor-\nization is calculated within a small search window around\neach predicted note onset, using pretrained tone models of\nonly those pitches which are expected to be played within\nthat window. Note onsets are then reset according to the\npitch activation patterns yielded by the matrix factoriza-\ntion. In doing so, we are able to resolve individual notes\nwithin a chord. We show that this method is feasible of\nincreasing the accuracy of aligned note’s onsets which are\nalready aligned relatively near to the real note attack. How-\never it is so far not suitable for the detection and correction\nof outliers which are displaced by a large timespan. We\nalso compared our system to a reference method showing\nthat it outperforms bandpass ﬁltering based onset detection\nin the reﬁnement step.\n1. INTRODUCTION\nOpposed to blind audio analysis there are several applica-\ntions where the recording of an already known piece of\nmusic has to be analysed. These applications range from\ncomputational musicology, especially performance analy-\nsis, and pedagogical systems to augmented audio players\nand editors as well as special query engines. Knowing that\na huge number of symbolic transcriptions of classical as\nwell as modern pieces are publicly available, this leads to\nthe task of automatic music-to-score alignment.\nMost current approaches are based on a local distance\nmeasure – mainly chroma vectors or features derived from\nchroma vectors – to compare the similarity between one\ntime frame of the audio and one time frame of the score\nrepresentation. These distances are then used by a global\noptimization algorithm, usually Dynamic Time Warping\n(DTW) or Hidden Markov Models (HMM), which ﬁnds\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.the best matching alignment between these two feature se-\nquences.\nRecently much attention has been drawn on online algo-\nrithms for audio-to-score alignment, also known as score\nfollowing, like described in [1]. However less work has\nfocused on improvements of the accuracy of ofﬂine algo-\nrithms. In this paper we present ongoing work towards ac-\ncurate measurement of individual notes’ parameters. The\ncalculation of accurate alignments is not only of use for the\nabove mentioned applications but can also provide training\nand test data for less informed tasks like blind audio tran-\nscription [2].\nWe propose a two-pass system where in the ﬁrst step a\nstandard alignment routine based on chroma vectors and\nDTW is performed. In the second step this alignment is\nreﬁned using a non-negative matrix factorization (NMF)\napproach. For each note a search window is set around\nthe estimated note onset. With each of theses windows\nan NMF using pretrained tone models of only those notes\nexcepted to occur within the respective audio segment plus\na noise component is performed. In doing so, the system is\nable to resolve individual note onsets within whole chords.\nWe will show that this method provides a good means\nof reﬁning the estimated onset times of notes that are rel-\natively well detected by standard alignment. However in\nhard cases where the alignment deviates considerably from\nthe ground truth the method shown here is prone to errors\nas well.\nSection 2 is a brief overview of related work. In Sec-\ntions 3 and 4 we explain the ﬁrst alignment step and the\nNMF-based reﬁnement respectively. Section 5 contains a\ndescription of the evaluation method used as well as the\nexperimental results before we conclude our work in Sec-\ntion 6.\n2. RELATED WORK\nMuch work, including [2–5], has focused on audio-to-score\nalignment based on acoustic features and Dynamic Time\nWarping (DTW). In [6] chroma vectors, Pitch Histograms,\nand two Mel-Frequency Cepstrum Coefﬁcient (MFCC) re-\nlated features have been compared in the context of DTW\nbased audio matching and alignment. It was shown that\nchroma vectors perform signiﬁcantly better than the other\nfeatures.\nSince DTW applied on two sequences of length nis of\n585Poster Session 4\ncomplexity O(n2)in time as well as in space the resolution\nof the features used is limited by runtime as well as mem-\nory constraints. One way of reﬁning audio alignments is to\nincrease this resolution while keeping computational costs\nwithin reasonable bounds. This is done by multi-scale ap-\nproaches like described in [5] or [7] where the resolutions\nare increased iteratively but on the other hand search paths\nare constrained by tentative solutions found so far.\nThe resolution based reﬁnement does not overcome an\nimportant side effect of alignments based on dynamic time\nwarping. Notes that are struck together in the score, like\nit is the case for chords, can not be treated independently.\nThis is a major drawback in applications like performance\nanalysis, where the accurate timing of individual chord\nnotes is an important expressive characteristic. [8] and [9]\nuse pitch speciﬁc energy levels in order to estimate the tim-\nings of individual notes.\nAnother method to iteratively reﬁne audio alignments\nis a bootstrap approach as described by [4]. There an au-\ndio segmenter is trained on an initial alignment. This seg-\nmenter can produce a reﬁned alignment which is then used\nfor a repeated training step. This method allows for the ap-\nplication of supervised machine learning techniques with-\nout the need for external training data.\nNon-negative matrix factorization, as used here, was\nﬁrst applied to audio alignment in [10]. There, the com-\nbination of NMF and Hidden Markov Models was able to\ncreate alignments for polyphonic instruments in realtime.\n3. BASIC ALIGNMENT\n3.1 Chroma Feature\nIn the ﬁrst pass the proposed system performs a state-of-\nthe-art audio-to-midi alignment based on chroma vectors\nand Dynamic Time Warping. Chroma vectors have 12 ele-\nments representing the single pitch classes (i.e. C, C#, D,\nD#,. . . ). The values are calculated based on a short time\nFourier transform. Each frequency bin is then related to\nthe index iof a pitch class by\ni=round/parenleftbigg\n12 log2/parenleftbiggfk\n440/parenrightbigg/parenrightbigg\n+ 9 mod 12 (1)\nwhere fkis the center frequency of the kthbin. The\ntuning frequency is supposed to be 440 Hz but can easily\nbe changed to any other value. The summand 9shifts the\nvector such that the pitch class C has index 0. The individ-\nual values are then obtained by summing up the energies\nof all bins corresponding to a certain pitch class.\nA similar feature that yields comparable results has been\nsuggested by [11] which on the one hand takes only bins\ncontaining energy peaks into account but on the other hand\nalso considers harmonics. At the extraction of the so called\nHarmonic Pitch Class Proﬁle the energy of a frequency bin\nkdoes not only contribute to the pitch class best matching\nthe center frequency fkbut also to those pitch classes best\nmatching fk/hwithh= 2,3,4. . . . This accommodatesfor the assumption that the energy in bin kcan also rep-\nresent the hthharmonic of a pitch. Since the energy of a\npartial decreases with the order of the harmonic, an addi-\ntional weighting factor of wharm =dh−1with0< d≤1\nis introduced.\nThe calculation of the chroma representation based on\na MIDI ﬁle instead of audio data is straightforward since\neach MIDI event can be directly assigned to the corre-\nsponding pitch class. However when using the Harmonic\nPitch Class Proﬁle, errors are made when letting the en-\nergy of the actual f0contribute to the pitch classes cor-\nresponding to f0/3,f0/5,. . . . This inexactness has to be\nreproduced in order to obtain equivalent representations of\naudio and score. Likewise when using default chroma vec-\ntors, contributions of a note to other pitch classes than the\none corresponding to the f0caused by harmonics can be\nconsidered as well.\nPreliminary experiments have shown that chroma vec-\ntors and Harmonic Pitch Class Proﬁles yield comparable\nresults. Therefore chroma vectors have been used for the\nremainder of this work due to computational advantages.\n3.2 Dynamic Time Warping\nBased on this chroma representation a globally optimal\nalignment is calculated. Therefore a sequence of chroma\nvectors for the audio ﬁle as well as for the score represen-\ntation is calculated. In doing so the score MIDI is divided\ninto time frames such that the overall number of frames\nand the overlap ratio between frames is the same as of the\nSTFT applied on the audio data. The Euclidean distance is\nused to compute a similarity matrix SM comparing each\nframe of one feature sequence to each frame of the other\nsequence, after all feature vectors have been normalized.\nMapping corresponding frames to each other is the same\nas ﬁnding a minimal cost path through this similarity ma-\ntrix. A path through SM ijis then equivalent to the align-\nment of frame iof the score feature sequence to frame jof\nthe performance feature sequence. Dynamic time warping\n(DTW) is a well-established dynamic programming based\nalgorithm that ﬁnds such optimal paths. A detailed tutorial\ncan be found in [12].\nIn order to get meaningful results an alignment path has\nto meet several constraints.\nContinuity The constraint of continuity forces a path to\nproceed through adjacent cells within the similarity\nmatrix. Jumps would be equal to skipping frames\nwithout considering the costs of this operation.\nMonotonicity The constraint of monotonicity in both di-\nmensions guarantees that the alignment has the same\ntemporal order of events as the reference sequence.\nEnd-point constraint The end-point constraint forces the\nends of the path to be the diagonal corners of the\nsimilarity matrix. In doing so it is assured that the\nalignment covers the whole sequences.\nThe optimal path according to DTW is calculated in two\nsteps. The forward step starts a partial path at the point\n58610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n[0,0]and rates it with the cost SM ij. Then it calculates the\nminimum path costs for all other partial alignments ending\nwith frame iof the score being aligned to frame jof the\nrecorded performance in a recursive manner according to\nequation 2.\nAccu (i, j) = min\n\nAccu (i−1, j−1) +SM ij∗wd\nAccu (i−1, j) +SM ij∗ws\nAccu (i, j−1) +SM ij∗ws\n(2)\nThe three options correspond to partial paths ending\nwith a diagonal step, an upwards step, and step to the right\nwithin the similarity matrix SM. In addition to the actual\nlocal distances, weights wdandwsare needed to yield rea-\nsonable path costs. If there were no such weights, diagonal\npaths would be strongly favored over straight ones which\nare twice as long. Experiments have shown that the values\n1.4and1.0(still giving diagonal steps a preference over\nstraight ones) perform well. In our implementation we do\nthis cost calculation in place, i.e. overwriting the values\nSM ijbyAccu (i, j)in order to save memory space.\nThe backtracking step of DTW starts as soon as all val-\nuesAccu (i, j)have been calculated. Accu (N−1, M−1)\nis the minimal cost of a complete alignment between the\ntwo feature sequences. Therefore the optimal path is re-\nconstructed starting from [N−1, M−1]going back to\n[0,0]. In order to be able to do so, a second matrix is built\nduring the forward step, memorizing whether the last step\nleading to a point [i, j]was diagonal, upwards, or to the\nright.\n4. NMF-BASED REFINEMENT\n4.1 Non-negative Matrix Factorization\nWithin the last few years non-negative matrix factorization\n(NMF) has become of increasing interest in the domain of\nblind audio transcription. The basic idea is that an input\nmatrix Vof size m×nis decomposed into two output\nmatrices WandHof size m×randr×nrespectively\nwhere the elements of all these matrices are strictly non-\nnegative and\nV≈WH (3)\nAssuming that Vrepresents real-world data such fac-\ntorizations will most likely not be perfect. The reconstruc-\ntion error caused by any deviation of WH fromVcan be\nmeasured by a cost for which the Euclidean distance or the\nI-divergence are common choices. In minimizing this cost\nfunction, WandHare learned as an initially determined\nnumber rof basis vectors and their activation patterns over\ntime respectively.\nPerforming such a decomposition on a spectrogram, as\nobtained by a short time Fourier transform, will result in a\ndictionary Wof weighted frequency groups and their oc-\ncurrence Hover time. According to the input Vand the\nparameter r, the base components in Wwill, in the ideal\ncase, represent models of single pitches or chords playedon a certain instrument. But due to the unsupervised nature\nof the method, elements of Wmight as well correspond to\nspecial frequency patterns during the attack, sustain, or de-\ncay phase of a note, single partial or just noise.\nHowever, as soon as the piece and its score are known,\nas it is the case in the context of audio alignment, the in-\nstrument(s) used to perform the piece are most probably\nknown as well. So there is no need to learn a set of base\ncomponents. Instead a number rof tone models can be\ntrained in advance which overcomes the above mentioned\nuncertainty of unsupervised learning. Also the number and\nkind of tone models can be adjusted to the respective piece.\nWith only Hbeing left unknown Equation 3 can be\nrewritten as\nv≈W·h (4)\nwhere Wis the ﬁxed dictionary of tone models. vand\nhare single column vectors of VandHthat can now be\nprocessed independently, which leads to a much simpler\ndecomposition task [13]. The vectors hare very sparse in\nnature and represent an f0estimation for the corresponding\nframe.\nThroughout this work the mean square criterion given\nas\ncerr=1\n2/bardblWh−v/bardbl2\n2 (5)\nis used as cost measure for factorization errors since\ncomputationally efﬁcient algorithms for its optimization\nare available [14].\n4.2 Tone Model Training\nIn order to get meaningful factorizations at least one tone\nmodel per possible pitch has to be contained in W. Given\na set of training samples, such tone models can be trained\nin advance using the same method as described above. In\nthe ideal case those training samples are audio recordings\nof single pitches played on a certain instrument. Starting\nfrom Equation 3 again, WandHbecome vectors wandh\nsince there is only one basis component present ( r= 1).\nhcan further be approximated by the amplitude envelope,\nleaving only wto be unknown. The actual computation is\nthen done by the same implementation as used during the\nperforming step of the algorithm.\nThroughout this work we use an additional basis com-\nponent representing white noise. Experiments have shown\nthat such a noise model signiﬁcantly improves the align-\nment results.\n4.3 Local Reﬁnement\nIn the ﬁrst stage of the proposed system a music-to-score\nalignment has already been performed. The advantage of\nthis alignment is that it is globally optimized and very ro-\nbust. However independent from all parameters that can\nbe set, accuracy is limited by the fact, that such an align-\nment algorithm can never differentiate between notes that\nare struck together in the score.\n587Poster Session 4\nTo overcome this limitation and still preserve high ro-\nbustness we deﬁne a search window of length laround the\ninitially estimated onset time. Within this local context the\nreﬁnement step tries to ﬁnd the exact temporal position of\neach individual (chord-)note. The parameter lhas been\nchosen to be 2 seconds since preliminary evaluation of the\nﬁrst alignment step has shown that only a marginal number\nof outliers deviates from the ground truth by more than a\nsecond.\nFor each such search window the contained notes and\ntheir pitches are determined in order to deﬁne the tonal\ncontext of the note under consideration. This information\nis used to build a dictionary Wlocal made up by tone mod-\nels describing only those pitches that are present within the\nlocal context plus an additional (white) noise component.\nThe resulting activation patterns Hare smoothed using a\nmedian ﬁlter and used in order to extract following features\nfor each time frame.\nActivation energy Since activation patterns Hare very\nsparse in nature (even when sparsity is not enforced),\nactivation energies greater than zero are strong indi-\ncators for note positions.\nEnergy slopes The ﬁrst derivative of the activation energy\ncorresponds to energy changes. Positive slopes as\nthey occur at note onsets are ﬁltered by half wave\nrectiﬁcation.\nRelative energy slopes Since transients at note onsets are\ncharacterized by energy burst across the whole spec-\ntrum, other pitches – especially ones with shared\nharmonics – might show low activation energies dur-\ning such phases as well. Therefore the increases in\nenergy of the pitch under consideration in relation to\nthe overall frame energy is also taken into account.\nExperiments have shown that the maxima of the deriva-\ntives are good predictors for note attacks while the maxi-\nmal activation energy itself has turned out to be less sig-\nniﬁcant. Comparing the slope of the absolute energy to the\none of the relative energy revealed a slight advantage of\nthe relative energy derivative which was therefore chosen\nas onset detection criterion.\n5. EXPERIMENTAL RESULTS\n5.1 Evaluation Method\nWe limit our evaluation to classical piano music using a\ndatabase consisting of the ﬁrst movements of 11 Mozart\nsonatas played by a professional pianist. The performance\nwas done on a computer monitored B ¨osendorfer SE290\ngrand piano, producing an automatic MIDI transcription\nof the exact ground truth of played notes as well as pedal\nevents. Aligning a single movement instead of a whole\nsonata at a time is a valid simpliﬁcation since individual\nmovements are per default separate tracks on audio CDs.\nNevertheless the overall performance time of this test set\nis still about one hour containing more than 30.000 notes.The tone models used for the NMF-based reﬁnement\nhave been learned from single tones played on the same\ngrand piano. Since such a recording was not available for\neach pitch, the missing models have been acquired by sim-\nple interpolation.\nFor evaluation purpose we calculated an alignment for\neach piece using the audio recording of the expressive per-\nformance and a mechanical score representation in MIDI\nformat. We compared the resulting onset times to our given\nground truth data and took the absolute displacement as\nevaluation criterion. This evaluation was done for the ini-\ntial alignment step only as well as for the whole system\nincluding the reﬁnement.\nInitial alignments were done using a short time Fourier\ntransform (STFT) with a window length of 4096 samples\nand a hop size of 441 samples, which corresponds to a time\nresolution of 100 frames per second. For the reﬁnement\nstep a search window of radius one second was used and\nthe STFT hop size was reduced to 256 samples, resulting\nin time frames of a length of 5.8 ms.\nFirst experiments with this setup have shown that al-\nthough the calculation of the factorization base feature is\nnarrowed down to a small search window as well as a small\npitch range, it is still not as robust as expected. About 10%\nof the notes have not been detected by the factorization step\nand therefore left unchanged during reﬁnement.\nConcerning the remaining notes it turned out to be the\nbest strategy to only modify those notes where the initial\nalignment position and the timing resulting from reﬁne-\nment are approximately consistent. This is the case for\nabout half of the overall number of notes. In situations\nwhere these two onset candidates differ by more than 20\nframes (i.e. 116 ms) a conﬂict is detected – although its\nresolution has been left to future work. One cause for such\nconﬂicts are repeated notes which cannot be handled by\nthe simple detection mechanism as described above.\n5.2 Evaluation Results\nIn Table 1 the limits of the quartiles as well as the 95th\npercentile are given. Within the ﬁrst three quartiles the\nreﬁnement has improved results for each individual piece.\nHowever concerning notes that are displaced by more than\n100 ms in the initial alignment tend to be displaced even\nfurther by the reﬁnement step.\nFor most applications a transcription is good as soon as\na human listener can not distinguish it from the original.\nThis implies that in the context of music-to-score align-\nment a note can be counted as correctly aligned if its devi-\nation from the ground truth is less than the just noticeable\ndifference of the human perception. In an experimental\nenvironment, where listeners were asked to adjust the tim-\ning of one tone within a series, such that the inter-onset\nintervals became perfectly regular, this just noticeable dif-\nference was investigated [15]. It was found to be around\n10 ms for notes shorter than 250 ms and about 5% of the\nnote duration for longer ones.\nTherefore an evaluation based on this criterion was done\nas well. In Table 2 the amount of notes with a time dis-\n58810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n25%< x 50%< x 75%< x 95%< x\npiece # notes durationbas. ref. bas. ref. bas. ref. bas. ref.\nkv279-1 2803 4:55 7 ms 5 ms 16 ms 12 ms 30 ms 27 ms 103 ms 101 ms\nkv280-1 2491 4:48 11 ms 5 ms 23 ms 14 ms 42 ms 34 ms 126 ms 127 ms\nkv281-1 2648 4:29 12 ms 6 ms 24 ms 15 ms 42 ms 36 ms 114 ms 112 ms\nkv282-1 1907 7:35 10 ms 6 ms 23 ms 15 ms 53 ms 44 ms 337 ms 380 ms\nkv283-1 3304 5:22 7 ms 5 ms 15 ms 12 ms 27 ms 26 ms 62 ms 65 ms\nkv284-1 3700 5:17 7 ms 6 ms 15 ms 13 ms 31 ms 29 ms 97 ms 98 ms\nkv330-1 3160 6:14 7 ms 5 ms 15 ms 11 ms 28 ms 24 ms 118 ms 124 ms\nkv332-1 3470 6:02 9 ms 7 ms 20 ms 18 ms 39 ms 37 ms 138 ms 147 ms\nkv333-1 3774 6:44 8 ms 5 ms 16 ms 13 ms 29 ms 20 ms 79 ms 80 ms\nkv457-1 2993 6:15 10 ms 6 ms 19 ms 15 ms 37 ms 35 ms 214 ms 257 ms\nkv475-1 1284 4:58 13 ms 11 ms 30 ms 24 ms 78 ms 75 ms 360 ms 393 ms\nall 31534 1:02:39 8.3 ms 5.6 ms 18 ms 14 ms 35 ms 32 ms 132 ms 137 ms\nTable 1 . Comparison between accuracy after the basic alignment step (bas.) and the additional reﬁnement (ref.)\nx <10ms x <50ms\npiecebas. ref. bas. ref.\nkv279-1 33.8% 43.2% 88.2% 88.4%\nkv280-1 22.4% 42.5% 81.5% 85.0%\nkv281-1 20.1% 38.5% 80.4% 83.4%\nkv282-1 25.3% 39.2% 73.7% 76.8%\nkv283-1 36.2% 44.2% 92.6% 92.2%\nkv284-1 34.6% 41.7% 86.9% 87.2%\nkv330-1 35.5% 46.7% 89.9% 89.7%\nkv332-1 27.1% 32.5% 83.0% 82.7%\nkv333-1 31.5% 42.2% 90.1% 90.1%\nkv457-1 27.3% 35.9% 82.5% 83.2%\nkv475-1 20.0% 23.6% 63.9% 66.8%\nall 29.6% 40.0% 84.8% 85.6%\nTable 2 . Comparison between accuracy after the basic\nalignment step (bas.) and the additional reﬁnement (ref.)\nplacement less than 10 ms is shown for the initial and the\nreﬁned alignment. According to the chosen STFT time\nresolution this corresponds to a deviation of one frame at\nmaximum. In addition the number of notes having a dis-\nplacement error less than 50 ms is given as well since this\nis a common evaluation criterion in onset detection.\nAgain it is shown that the reﬁnement improves those\nnotes already aligned relatively close to their real onset.\nThe amount of notes with displacement errors less than\n10 ms was increased from about 30% to 40% while the\nnumber of notes with errors below 50 ms was only mod-\nerately changed from 84.8% to 85.6%.\n5.3 Feature comparison\nFrom the list of related work presented in section 2, [8] is\nthe one that presents the approach which is most similar to\nthe system proposed here. There onset detection by selec-\ntive bandpass ﬁltering is described in the context of score\nsupported audio transcription. According to this method a\nnote is found by summing up the energy in all frequencyfact. s.b.f.\n25%< x 5.6 ms 10.0 ms\n50%< x 14 ms 20 ms\n75%< x 32 ms 40 ms\n95%< x 137 ms 128 ms\nx <10ms 40.0% 24.9%\nx <50ms 85.6% 81.3%\nTable 3 . Comparison between reﬁnement based on fac-\ntorization (fact.) and based on selective bandpass ﬁltering\n(s.b.f.) [8]\nbands corresponding to the f0as well as the harmonics of\na pitch and then ﬁnding a maximum in the derivative of\nthis indication function. In order to avoid the inﬂuence of\nother pitches with overlapping harmonics, partials that col-\nlide with those of an other note struck at the same time are\nneglected.\nWe have compared our system to an own implemen-\ntation of this approach. In doing so, we used the same\ncomputational framework and only exchanged the factor-\nization feature in the reﬁnement step by this onset detector\nbased on selective bandpass ﬁltering. The accumulated re-\nsults on the whole test set are shown in Table 3. It demon-\nstrates that bandpass ﬁltering yields results less accurate\nthan those produced by NMF, and mostly even less accu-\nrate than those achieved by the alignment based on chroma\nvectors. A possible reason is that the STFT based ver-\nsion of selective bandpass ﬁltering relies on just a few fre-\nquency bins while NMF takes the whole spectrogram into\naccount.\n6. CONCLUSION AND FUTURE WORK\nWe have introduced a new method to increase accuracy of\nmusic-to-score alignments by a two-pass system. Whereas\nthe ﬁrst step consists of a state-of-the-art alignment using\nchroma features and dynamic time warping the second step\nis a reﬁnement based on non-negative matrix factorization.\n589Poster Session 4\nWe have shown that this reﬁnement step performs very\nwell on notes which have already been detected relatively\nclose to their real onset time by the alignment step. The\nnumber of notes placed with a time deviation below the\njust noticeable difference according to [15] of 10 ms has\nbeen increased from about 30% to 40%. This is remark-\nable since so far only those notes without any conﬂicting\nfeatures have been modiﬁed.\nHowever the method does not bring any improvements\nfor notes where the deviation of the initial alignment from\nthe ground truth is large. On one hand the reﬁnement step\nonly works within a search window which should be kept\nas small as possible. Notes that are misaligned such that\nthe actual onset is out of this window can never be cor-\nrected by the method described here. On the other hand\nchroma features as well as factorization based pitch sepa-\nration rely on prominent energy peaks in the spectrogram.\nIf the spectrogram is blurred due to heavy use of pedal or\nvery rich polyphony both approaches are prone to errors.\nThis clearly dictates future work to concentrate on the\nproblem of detecting and handling possible outliers and\n’hard’ regions. The most obvious approach is to develop\na method of handling conﬂicting features as this is the case\nfor about 40% of all notes. We think that introducing a\ntempo model and enforcing reasonable inter-onset inter-\nvals entails the potential of further improvements.\nAlso the 10% of notes that have not been covered by\nthe factorization based feature are worth being reconsid-\nered. Standard STFT favors the detection of higher pitches\ndue to its linear frequency scale. Additional spectral trans-\nformations like multi-rate ﬁlterbanks or a constant-Q trans-\nform could help to enhance the note detection, especially\nwithin low pitch ranges.\n7. ACKNOWLEDGMENTS\nThe research presented in this paper is supported by the\nAustrian Fonds zur F ¨orderung der Wissenschaftlichen For-\nschung (FWF) under project number P19349-N15.\n8. REFERENCES\n[1] S. Dixon: “Live Tracking of Musical Performances\nUsing On-Line Time Warping”, Proceedings of the\n8th International Conference on Digital Audio Effects\n(DAFx) , Madrid, 2005.\n[2] R. J. Turetsky and D. P. W. Ellis: “Ground-Truth Tran-\nscriptions of Real Music from Force-Aligned MIDI\nSyntheses”, Proceedings of the 4th International Sym-\nposium of Music Information Retrieval (ISMIR) Balti-\nmore, MD, 2003.\n[3] Y . Meron and K. Hirose: “Automatic alignment of a\nmusical score to performed music”, Acoustical Science\nand Technology , V ol. 22, No. 3, pp. 189–198, 2001.\n[4] N. Hu and R. B. Dannenberg: “A Bootstrap Method for\nTraining an Accurate Audio Segmenter”, Proceedingsof the 6th International Conference on Music Informa-\ntion Retrieval (ISMIR) , London, 2005.\n[5] M. M ¨uller, F. Kurth, and T. R ¨oder: “Towards an efﬁ-\ncient algorithm for automatic score-to-audio synchro-\nnization”, Proceedings of the 7th International Con-\nference on Music Information Retrieval (ISMIR) , Vic-\ntoria, 2006.\n[6] N. Hu, R. B. Dannenberg, and G. Tzanetakis: “Poly-\nphonic Audio Matching and Alignment for Music Re-\ntrieval”, IEEE Workshop on Applications of Signal\nProcessing to Audio and Acoustics , New York, 2003.\n[7] N. Adams, D. Marquez, and G. Wakeﬁeld : “Iterative\nDeepening for Melody Alignment and Retrieval”, Pro-\nceedings of the 6th International Conference on Music\nInformation Retrieval (ISMIR) , London, 2005).\n[8] E. D. Scheirer: “Using Musical Knowledge to Ex-\ntract Expressive Performance Information from Au-\ndio Recordings”, Readings in Computational Audi-\ntory Scene Analysis , H. G. Okuno and D. F. Rosenthal\n(eds.), Lawrence Erlbaum Publication, Mahweh, NJ,\n1997.\n[9] M. M ¨uller, F. Kurth, and M. Clausen: “Audio Matching\nvia Chroma-based Statistical Features”, Proceedings of\nthe 5th International Conference on Music Information\nRetrieval (ISMIR) , Barcelona, 2005.\n[10] A. Cont: “Realtime Audio to Score Alignment for\nPolyphonic Music Istruments Using Sparse Non-\nnegative Constraints and Hierarchical HMMs”, Pro-\nceedings of the IEEE International Conference in\nAcoustics and Speech Signal Processing (ICASSP) ,\nToulouse, 2006.\n[11] E. G ´omez and P. Herrera: “Automatic Extraction of\nTonal Metadata from Polyphonic Audio Recordings”,\nProceedings of 25th International AES Conference ,\nLondon, 2004.\n[12] Rabiner, L. R. and Juang, B.-H. “Fundamentals of\nspeech recognition”. Prentice Hall, Englewood Cliffs,\nNJ, 1993.\n[13] F. Sha and L. Saul: “Real-time pitch determination of\none or more voices by nonnegative matrix factoriza-\ntion”, Advances in Neural Information Processing Sys-\ntems 17 , K. Saul, Y . Weiss, and L. Bottou (eds.), MIT\nPress, Cambridge, MA, 2005.\n[14] Lawson, C. L. and Hanson, R. J. “Solving least squares\nproblems”, Prentice Hall , Lebanon, Indiana, 1974.\n[15] A. Friberg and J. Sundberg: “Perception of just no-\nticeable time displacement of a tone presented in a\nmetrical sequence at different tempos”, Proceedings of\nthe Stockholm Music Acoustics Conference , pp. 39–43,\nStockholm, 1993.\n590"
    },
    {
        "title": "The Intersection of Computational Analysis and Music Manuscripts: A New Model for Bach Source Studies of the 21st Century.",
        "author": [
            "Masahiro Niitsuma",
            "Tsutomu Fujinami",
            "Yo Tomita"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417935",
        "url": "https://doi.org/10.5281/zenodo.1417935",
        "ee": "https://zenodo.org/records/1417935/files/NiitsumaFT09.pdf",
        "abstract": "This paper addresses the intersection of computational analysis and musicological source studies. In musicology, scholars often find themselves in the situation where their methodologies are inadequate to achieve their goals. Their problems appear to be twofold: (1) the lack of scientific objectivity and (2) the over-reliance on new source discoveries. We propose three stages to resolve these problems, a preliminary result of which is shown. The successful outcome of this work will have a huge impact not only on musicology but also on a wide range of subjects.",
        "zenodo_id": 1417935,
        "dblp_key": "conf/ismir/NiitsumaFT09",
        "keywords": [
            "computational analysis",
            "musicological source studies",
            "scholars methodologies",
            "scientific objectivity",
            "new source discoveries",
            "three stages",
            "preliminary result",
            "huge impact",
            "musicology",
            "wide range of subjects"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTHE INTERSECTION OF COMPUTATIONAL ANALYSIS AND MUSIC\nMANUSCRIPTS: A NEW MODEL FOR BACH SOURCE STUDIES OF THE\n21ST CENTURY\nMasahiro Niitsuma †Tsutomu Fujinami ‡Yo Tomita †\n†School of Muisc and Sonic Arts, Queen’s University, Belfast\n‡School of Knowledge Science, Japan Advanced Institute of Sc ience and Technology (JAIST)\nniizuma@nak.ics.keio.ac.jp, fuji@jaist.ac.jp, y.tomit a@qub.ac.uk\nABSTRACT\nThis paper addresses the intersection of computational\nanalysis and musicological source studies. In musicology,\nscholars often ﬁnd themselves in the situation where their\nmethodologies are inadequate to achieve their goals. Their\nproblems appear to be twofold: (1) the lack of scientiﬁc\nobjectivity and (2) the over-reliance on new source discov-\neries. We propose three stages to resolve these problems, a\npreliminary result of which is shown. The successful out-\ncome of this work will have a huge impact not only on\nmusicology but also on a wide range of subjects.\n1. INTRODUCTION\nRecent developments in computer and information tech-\nnology have brought signiﬁcant changes to the ways in\nwhich we conduct research in a wide range of domains,\nand musicology is not an exception.\nYet in historical musicology the majority of scholars\nstill conduct their research without making full use of this\ntechnological advancement, thus creating huge potential\nfor future advancement.\nBy nature, their research methods are less scientiﬁc, i.e.\nthey tend not to, or ﬁnd it impossible to disclose all the in-\nformation they used in order to arrive at their conclusions,\nand hence it is often difﬁcult to verify their ﬁndings re-\ngardless of whether or not there are elements of subjective\njudgment in them.\nThere is a separate problem in musicology in that the\nmajority of source-based studies heavily rely on the redis-\ncovery of new sources.1Thus, if a new source is not found,\nthere is often little discussion to challenge the existing i n-\nterpretation offered by scholars in the past. Is there reall y\nno way of improving the theories unless a new source is\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .\n1Sources refer to manuscript sources, that is written scores b y hand.\nBefore the invention of printing, music was preserved either by oral trans-\nmission or by MS copies.discovered? How can a computer assist musicologists in\nanalysing the information contained in the known sources?\nThe main objective of this study is to solve such prob-\nlems in historical musicology by addressing the following\nquestions:\n1. Can computational analysis offer the same conclu-\nsions as those arrived at by historical musicologists?\n2. Are there any oversights in the musicologists’ anal-\nysis of the sources?\nTo achieve our objectives, it is necessary to address the\nfollowing issues:\n1. How to deﬁne a data structure for storing Bach’s\nmanuscripts in digital format;\n2. How to extract information from the digitised manu-\nscripts;\n3. How to analyse the extracted information.\nThis paper is structured as follows: Section 2 describes\nthe relationship between the proposed methods and ex-\nisting scholarly debates in the ﬁeld; Section 3 discusses\nthe research methods to be employed; Section 4 shows a\npreliminary result of the proposed method; Section 5 il-\nlustrates the contribution that the proposed research will\nmake; and Section 6 offers concluding remarks.\n2. PREVIOUS RESEARCH\nThere are numerous research projects dealing with com-\nputation in musicology and different kinds of data formats\nhave been proposed to encode musical data [1–3]. How-\never, all of them deal with limited musical information\nsuch as pitch or rhythm derived from printed scores, and\nthe majority of previous research on computational music\nanalysis [4–9] is based on those data formats.\nThere is also a certain amount of research related to au-\ntomatic music analysis using the signal-processing tech-\nnique with acoustic sources [10–14], which record musical\nperformance from published scores. But if we investigate\nonly published scores, rather than the original manuscript s,\nwe miss important information that has been lost in the\nprocess of creating an edition.\n519Poster Session 3\nRecent journal articles or proceedings of ISMIR [15–\n17] includes a considerable number of researches on the\nOptical Music Recognition (OMR). Most of them deal with\nstaff removal algorithm, which eases the preprocessing of\nthe digitised images of the manuscripts such as the music\nsymbol recognition.\nWith regard to the research related to manuscript anal-\nysis, Tomita developed a database of variants and errors\nwhich supposedly lists all the extant manuscripts and early\nprints of the Well-Tempered Clavier II, a work well known\nfor its complex history of compilation, revision and trans-\nmission [18]. The database contains all kinds of informa-\ntion extracted from manuscripts – not only musical variants\nbut also notational errors and variants that may have been\ninherited from its model or may cause errors when fresh\ncopies were made from it – giving us many insights into\nhow the future database should be developed.\n3. METHODOLOGY\nThere are three stages in this project:\n1. To deﬁne of a data structure for storing Bach’s\nmanuscripts in digital format;\n2. To develop a methodology to automatically extract\ndata from the digitised images of music manuscripts;\n3. To develop a methodology to analyse these data to\nﬁnd signiﬁcant information for musicological study.\nIn the ﬁrst instance, a data structure that is appropriate\nto be analysed by computers needs to be deﬁned. This data\nstructure should be designed in such a way that it can en-\ncode all the information extracted from manuscripts – not\nonly musical aspects such as pitch or rhythm, but also the\nphysical aspects of the manuscript which may account for\nthe scribe’s unintentional omissions, misplacement, supe r-\nﬂuous symbols that were somehow caused by the appear-\nance of its exemplar. This has been investigated with the\ncollaboration of musicologists.\nSecondly, a method will be developed to harvest the in-\nformation useful for research from the digitised images of\nthe manuscripts. At the moment, we consider primarily\nthe visible information such as the direction of stems or\nthe position of note-heads. The ﬁrst task is the recogni-\ntion of each music symbol such as staff line, bar line, note\nstem, note head and clef. The Gamera [19] framework will\nbe used for this task.\nFinally, a method to analyse the data will be proposed.\nIn order to achieve this, powerful machine learning meth-\nods such as bagging [20], boosting [21], and random for-\nest [22] will be adopted.\nFigure 1 illustrates how the proposed method operates.\nFirst, a digitised image ﬁle is created by physically scan-\nning the manuscripts. Secondly, symbolic data is extracted\nfrom the digitised image ﬁle. Thirdly, computational anal-\nysis is carried out using the symbolic data.Start\nPhysical \nmanuscript data \nScanning \nDigitised \nimage ﬁle Data \nextraction \nSymbolic \ndata \nComputational \nanalysis \nEnd \nFigure 1 . Flowchart of the proposed method\n4. PRELIMINARY EXPERIMENT\n4.1 An overview of the preliminary experiment\nThis sections presents a preliminary result of the third sta ge\ndescribed under ”3. Methodology”. Currently, the ﬁrst and\nsecond stages are conducted manually, while the program\nwas developed for the third stage. To demonstrate the per-\nformance of the latter, the simplest example would be to\nexamine the origin and authenticity of variants. Because\nWTC II was so popular among Bach’s pupils and admir-\ners during and after his lifetime, numerous manuscripts\nwere made, copied and edited, which not only increased\nthe number of errors or variant readings, but also resulted\nin introducing contamination to the texts in some sources\n[23, 24]. This program produces a source afﬁliation dia-\ngram showing how closely these sources were related, tak-\ning into account the differences that may be caused either\nby accident or on purpose while being copied.\nIn this paper, we focus on the sources of Viennese ori-\ngin, which are considered to have been originated from a\ncopy that was brought from Berlin to Vienna in 1777 by\nGottfried van Sweieten (1734-1803). How the unique text\nof the Viennese sources evolved up has been the principal\ninterest for musicologists, for this was the state of musica l\ntext which Mozart learned in 1782. In [25], Tomita inves-\ntigated the Viennese sources, thereby proposing a source\nafﬁliation diagram of them, an excerpt of which is shown\nin Figure 2.\n4.2 Preliminary result\nWe describe one approach to this task using the database\ndeveloped by Tomita [24], an excerpt of which is shown\nin Figure 3, where S/N is the serial number given to each\nexamination point; Bar indicates in which measure(s) the\nelements are examined; V , bt/pos stands for V oice, Beat\nand Position, respectively; Element speciﬁes the target of\nenquiry; Spec. Loc gives graphic representation of infor-\nmation under examination; Classiﬁed suggests text-critic al\nsigniﬁcance.\nFirstly, the distance between two manuscripts should be\ndeﬁned. The simplest way is to count the number of differ-\nent factors between two manuscripts.\nIn Figure 3, “Q11731” has no different factors from\n52010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nNo. \n543 Nydahl \nFigure 2 . Score afﬁliation diagram of the Well-tempered\nClavier Book II, generated by human analysis (excerpted\nfrom [18])\nthose of “No.543”, thus the distance between “Q11731”\nand “No.543” is 0. On the other hand, “Q11731” has three\nfactors which are different from those of “Nydahl”, thus\nthe distance between “Q11731” and “Nydahl” is 3. How-\never, such observation dose not reﬂect the reality sufﬁ-\nciently. To improve the accuracy of observation, we should\nconsider how easily each factor can change. For instance,\nnotational factors such as the direction of the stem or po-\nsition of the note-head are more likely to change than mu-\nsical factors such as pitch or duration. Taking this into\nconsideration, genealogical distance is deﬁned by the fol-\nlowing equation,\nD(MSS1, MSS 2) =SNX\ni=1αTypeiI(MSS1[i], MSS 2[i]) (1)\nwhere, MSS1andMSS2denote two different manuscripts,\nMSS[i]denotes the ith content of MSS, αTypeiis the\nweight considering the ﬂuidity of each type of the con-\ntent, and I(x,y)is the indicator function which returns 0\nifx=yelse 1. In this paper, all αTypeiwere equalized,\nleaving an adjustment of αTypeias a future task.\nNo.543 Nydahl\nFigure 3 . Database used for the experiment (excerpted\nfrom [18]).\nSecondly, manuscripts are clustered by a hierarchicalcluster analysis using a set of dissimilarities calculated on\nthe basis of Equation (1). Initially, each manuscript is as-\nsigned to its own cluster and then the algorithm proceeds\niteratively, at each stage joining the two most similar clus -\nters, continuing until there is just a single cluster. At eac h\nstage distances between clusters are recomputed by the\nLance-Williams dissimilarity update formula according to\nthe complete linkage method.\nS.M.210\nNydahl\nNo.543\nQ10782 Q1173122 24 26 28 30 32 34Cluster Dendrogram\nhclust (*, \"complete\")distHeight\nFigure 4 . Score afﬁliation diagram of Fugue No.22 in B ♭\nminor from the Well-tempered Clavier Book II, generated\nby computational analysis\nFigure 4 illustrates an example of source afﬁliation di-\nagram automatically generated by the proposed algorithm.\nManuscripts of Fugues 10, 12 and 14 were used to cal-\nculate the distance between each manuscript. This result\nis almost consistent with that of human analysis, while\nthe position of No.543 (Berea) is considered to be differ-\nent. This result indicates that this database is sufﬁcient t o\nachieve a rough classiﬁcation; but to achieve a more re-\nliable classiﬁcation or for further analysis, it is necessa ry\nto develop a new data structure that is suitable for a more\ndetailed computational analysis. The manual weighting of\nαTypeican reﬂect the expert knowledge of musicologists;\nhowever it could also reﬂect their own subjectivity. To ex-\nclude it, a method for automatic weighting of these factors\nshould be investigated.\nThere are numerous possibilities of using these databases\nfor analysis and the potential is far-reaching. Figure 5\nshows biplot of the result of the principle component anal-\nysis. This reveals that there exists a large gap between\nAdd.35021 (Bach’s autograph manuscript) and the Vien-\nnese sources. Figure 6 shows the result of the variable im-\nportance estimation for the classiﬁcation of the manuscrip ts\nof Fugue 23 by random forest, where y-axis corresponds\nto S/N of the text critical database. This indicates that S/N\n475, and 136 are important for computer to classify them.\nThese analyses using appropriate databases are considered\n521Poster Session 3\n−0.8 −0.6 −0.4 −0.2 0.0 0.2 0.4−0.8 −0.6 −0.4 −0.2 0.0 0.2 0.4\nPC1PC2\nAdd.35021Federhofer\nNo.543Nydahl\nQ10782Q11731\nS.M.210.1S.M.210.2−10 −5 0 5\n−10 −5 0 5V1\nV2V3V4V5V6V7\nV8V9V10\nV11V12V13V14\nV15V16\nV17\nV18V19V20V21\nV22V23V24V25\nV26V27V28V29V30\nV31V32V33V34V35V36\nV37\nV38V39\nV40V41\nV42V43V44V45\nV46V47\nV48V49 V50V51V52\nV53V54V55V56\nV57V58\nV59V60V61\nV62V63V64V65\nV66V67\nV68\nV69V70\nV71V72V73V74V75\nV76\nV77V78\nV79V80V81V82V83V84V85\nV86\nV87V88V89\nV90\nV91V92\nV93V94\nV95V96\nV97V98\nV99 V100V101\nV102V103V104V105V106\nV107V108V109V110\nV111V112 V113\nV114V115\nV116V117\nV118V119V120V121\nV122\nV123V124V125\nV126V127V128V129\nV130V131\nV132V133V134V135\nV136V137V138\nV139V140V141\nV142V143V144\nV145\nV146V147\nV148V149V150V151\nV152 V153V154V155V156\nV157V158\nV159V160\nV161\nV162V163\nV164V165\nV166\nV167V168\nV169V170\nV171V172\nV173V174\nV175V176V177\nV178V179V180V181\nV182V183V184V185\nV186V187V188\nV189V190\nV191V192\nV193V194\nV195\nV196\nV197V198\nV199V200V201V202\nV203V204\nV205\nV206V207\nV208\nV209V210\nV211V212V213\nV214\nV215V216\nV217\nV218V219V220\nV221V222\nV223V224\nV225V226V227\nV228\nV229V230\nV231V232V233V234V235\nV236V237\nV238V239V240\nV241V242\nV243\nV244\nV245V246\nV247V248V249V250\nV251\nV252V253V254\nV255V256\nV257V258V259V260V261\nV262V263V264V265\nV266V267\nV268\nV269V270V271\nV272V273V274\nV275V276V277V278\nV279 V280\nV281V282V283V284\nV285V286\nV287V288V289V290V291\nV292V293\nV294\nV295V296\nV297\nV298\nV299V300\nV301V302\nV303V304V305V306V307V308V309\nV310V311\nV312\nV313V314\nV315V316\nV317V318V319\nV320V321V322\nV323V324V325\nV326V327\nV328\nV329V330V331V332\nV333V334V335\nV336V337V338\nV339V340V341V342V343\nV344V345\nV346V347\nV348V349V350V351V352\nV353V354V355\nV356V357\nV358V359\nV360\nV361V362V363\nV364V365V366V367\nV368V369V370\nV371V372V373\nV374V375\nV376V377V378V379V380V381\nV382V383\nV384V385V386V387\nV388V389V390\nV391V392V393\nV394V395V396V397V398V399\nV400V401\nV402V403\nV404\nV405V406\nV407\nV408\nV409V410V411\nV412V413\nV414V415V416\nV417V418V419V420V421V422\nV423\nV424V425 V426V427\nV428\nV429V430\nV431\nV432V433V434V435\nV436\nV437\nV438V439\nV440V441V442\nV443V444\nV445\nV446V447V448\nV449V450V451\nV452\nV453\nV454V455V456V457\nV458\nV459V460\nV461V462V463V464V465\nV466\nV467\nV468V469V470V471\nV472\nV473V474V475\nV476V477 V478\nV479V480\nV481V482V483\nV484V485\nV486V487\nV488\nV489\nV490V491\nV492V493 V494\nV495\nV496V497\nV498V499\nV500V501V502\nV503V504V505\nV506V507V508V509V510V511V512V513\nV514\nV515V516\nV517V518V519V520\nFigure 5 . Biplot produced from the output of the principle\ncomponent analysis of the text critical database of Fugue\n23\nto bring the objectivity and new ﬁndings to historical mu-\nsicology.\nAnother area of investigation is an automatic handwrit-\ning analysis. The method for identifying handwriting in\nnoisy document images [26] cannot directly be applied to\nmusic manuscripts. This is because handwriting identiﬁ-\ncation needs not only visual information such as curvature\n(which represents the shape of the curves or bending an-\ngle) but also multifaceted information such as the purpose\nfor which a manuscript was written, the scribe’s habits, the\nconditions under which the manuscript was made, and so\non. The proposed method is expected to overcome such\ndifﬁculties by taking into account the multifaceted infor-\nmation with the appropriate database for computational ana l-\nysis.\n5. CONTRIBUTION\nThis research makes main contributions in the following\nareas:\n1. The proposed method will provide a way to verify\nprevious research in historical musicology;\n2. It will be possible to offer new information about the\nsources from the already known sources;V297V290V224V221V186V162V69V16V7V89V98V429V272V265V195V93V19V2V515V430V61V318V489V446V465V308V435V340V447V475\n0.40 0.50 0.60\nMeanDecreaseAccuracyV471V159V311V299V497V68V126V115V121V62V285V288V490V15V470V93V316V248V11V74V447V297V171V315V35V486V295V10V242V136\n0.00 0.02 0.04\nMeanDecreaseGini\nFigure 6 . Result of variable importance estimation for\nthe classiﬁcation of Viennese sources by a random for-\nest, where y-axis corresponds to S/N of Fugue 23 shown\nin [18]: for example, V475 is notation difference of rest in\nbar 89; V136 is the existence of accidental in bar32.\n3. The proposed method can be a prototype of an em-\npirical research method.\nThe result of the proposed research has a good potential\nfor becoming a road map for musicological research of the\nfuture, and empirical research method would offer an al-\nternative to the previous research methods often criticise d\nfor their inherent subjectivism. Consequently, it is hoped\nthat the majority of previous research may be reworked by\nusing the proposed methods. In this process, new discov-\neries can still be made that would shed new light on the\nmusical works concerned without requiring the rediscov-\nery of new sources. Moreover, the results of the proposed\nresearch may also serve as a prototype in other areas of\nresearch, such as archaeology, historical literature or ot her\nsocial science subjects that involve the study of historica l\nsources.\n6. CONCLUSION\nIn this paper, we have shown the necessity of using the\ncomputational approach in source studies. We also ad-\ndressed the problems of subjective attitudes and its over-\nreliance on new source discoveries in traditional research\nmethods in musicology. Three stages that may resolve\nthese problems have been discussed. The outcome of this\nwork should affect not only musicology but also a wide\nrange of subjects.\n52210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n7. REFERENCES\n[1] Content-based uniﬁed interfaces and descriptors for\naudio/music databases available.\nhttp://www.cuidado.mu.\n[2] Online music recognition and searching.\nhttp://www.elec.qmul.ac.uk/research/projects/\nnsf9905842 omras.html.\n[3] Web delivering of music.\nhttp://www.wedelmusic.org.\n[4] Greg Aloupis, Thomas Fevens, Stefan Langerman, To-\nmomi Matsui, Antonio Mesa, Yurai Nuez, David Rap-\npaport, Godfried, and Toussaint. Algorithms for com-\nputing geometric measures of melodic similarity. Com-\nputer Music Journal , 30(3):67–76, 2006.\n[5] David Huron. Music information processing using the\nhumdrum toolkit: Concepts, examples and lessons.\nComputer Music Journal , 26(2):11–26, 2002.\n[6] Steven Jan. Meme hunting with the humdrum toolkit:\nPrinciples, problems and prospects. Computer Music\nJournal , 28(4):68–84, 2004.\n[7] David Meredith. The ps13 pitch spelling algorithm.\nJournal of New Music Research , 35(2):121–159, 2006.\n[8] Wai Man Szeto and Man Hon Wong. A graph-\ntheoretical approach for pattern matching in post-\ntonal music analysis. Journal of New Music Research ,\n35(4):304–321, 2006.\n[9] Heinrich Taube. Automatic tonal analysis: Toward the\nimplementation of a music theory workbench. Com-\nputer Music Journal , 23(4):16–32, 1999.\n[10] Lee Kyogu and Slaney Malcolm. Acoustic chord tran-\nscription and key extraction from audio using key-\ndependent hmms trained on synthesized audio. IEEE\nTransactions on audio, speech, and language process-\ning, 16(2):291–301, 2008.\n[11] Olivier Lartillot. A musical pattern discovery system\nfounded on a modeling of listening strategies. Com-\nputer Music Journal , 28(3):56–67, 2004.\n[12] Pierre Leveau, Emmanuel Vincent, and Gal Richard.\nInstrument-specic harmonic atoms for mid-level music\nrepresentation. IEEE Transactions on audio, speech,\nand language processing , 16(1):116–128, 2008.\n[13] Rui Pedro Paiva, Teresa Mendes, and Amlcar Cardoso.\nMelody detection in polyphonic musical signals: Ex-\nploiting perceptual rules, note salience, and melodic\nsmoothness. Computer Music Journal , 30(4):80–89,\n2006.\n[14] Li Yipeng and Wang DeLiang. Musical sound sep-\naration using pitch-based labeling and binary time-\nfrequency masking. Proceedings of International Con-\nference on Acoustics Speech and Signal Processing ,\npages 173–176, 2008.[15] C. Dalitz, B. Czerwinski M. Droettboom, and I. Fuji-\nnaga. A comparative study of staff removal algorithms.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence , 30(5):753–66, 2008.\n[16] J. Hockman Pugin, L., J.A. Burgoyne, and I. Fujinaga.\nGamera versus aruspix: Two optical music recognition\napproaches. Proceedings of the International Confer-\nence on Music Information Retrieval , pages 419–424,\n2008.\n[17] John Ashley Burgoyne, Johanna Devaney, Laurent Pu-\ngin, and Ichiro Fujinag. Enhanced bleedthrough cor-\nrection for early music documents with recto-verso\nregistration. Proceedings of the International Confer-\nence on Music Information Retrieval , pages 407–412,\n2008.\n[18] Yo Tomita. J. S. Bach’s ‘Das Wohltemperierte Clavier\nII’ A Critical Commentary. Volume II: All the Extant\nManuscripts . Leeds: Household World, 1995.\n[19] The Gamera framework for building custom recogni-\ntion systems. Michael droettboom and karl macmillan\nand ichiro fujinaga. Proceedings of the Symposium on\nDocument Image Understanding Technologies , pages\n275–86, 2003.\n[20] L Breiman. Bagging predictors. Machine Learning ,\n24(2):123–140, 1996.\n[21] Y Freud and R.E Schapire. Experiments with a new\nboosting algorithm. Proceedings of the Thirteenth In-\nternational Conference on Machine Learning , pages\n148–156, 1996.\n[22] L Breiman. Random forests. Machine Learning ,\n45(1):5–32, 2001.\n[23] Yo Tomita. Breaking the limits: some consideration on\nan e-science approach to source studies. Musicology\nand Globalization. Proceedings of the international\nCongress of the Musicological Society of Japan , pages\n233–237, 2002.\n[24] Yo Tomita and Tsutomu Fujinami. Managing a large\ntext-critical database of Johann Sebastian Bach’s Well-\nTempered Clavier II with XML and relational database.\nProceedings of the International Musicological Con-\nference , 2002.\n[25] Yo Tomita. The Sources of J.S. Bach’s Well-Tempered\nClavier II in Vienna, 1777-1801. BACH: Journal of the\nRiemenschneider Bach Institute , 29(2):8–79, 1998.\n[26] Yefeng Zheng, Huiping Li, and David Doermann. Ma-\nchine printed text and handwriting identiﬁcation in\nnoisy document images. Pattern Analysis and Machine\nIntelligence , 26(3):337–353, 2004.\n523"
    },
    {
        "title": "A Measure of Melodic Similarity based on a Graph Representation of the Music Structure.",
        "author": [
            "Nicola Orio",
            "Antonio Rodà"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415010",
        "url": "https://doi.org/10.5281/zenodo.1415010",
        "ee": "https://zenodo.org/records/1415010/files/OrioR09.pdf",
        "abstract": "Content-based music retrieval requires to define a similarity measure between music documents. In this paper, we propose a novel similarity measure between melodic content, as represented in symbolic notation, that takes into account musicological aspects on the structural function of the melodic elements. The approach is based on the representation of a collection of music scores with a graph structure, where terminal nodes directly describe the music content, internal nodes represent its incremental generalization, and arcs denote the relationships among them. The similarity between two melodies can be computed by analyzing the graph structure and finding the shortest path between the corresponding nodes inside the graph. Preliminary results in terms of music similarity are presented using a small test collection.",
        "zenodo_id": 1415010,
        "dblp_key": "conf/ismir/OrioR09",
        "keywords": [
            "content-based music retrieval",
            "similarity measure",
            "melodic content",
            "symbolic notation",
            "musicological aspects",
            "structural function",
            "graph structure",
            "terminal nodes",
            "internal nodes",
            "arcs"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nA MEASURE OF MELODIC SIMILARITY BASED ON A GRAPH\nREPRESENTATION OF THE MUSIC STRUCTURE\nNicola Orio\nDepartment of Information Engineering\nUniversity of Padova\nnicola.orio@dei.unipd.itAntonio Rod `a\nLab. A VIRES\nUniversity of Udine\nantonio.roda@uniud.it\nABSTRACT\nContent-based music retrieval requires to deﬁne a similar-\nity measure between music documents. In this paper, we\npropose a novel similarity measure between melodic con-\ntent, as represented in symbolic notation, that takes into\naccount musicological aspects on the structural function of\nthe melodic elements. The approach is based on the rep-\nresentation of a collection of music scores with a graph\nstructure, where terminal nodes directly describe the mu-\nsic content, internal nodes represent its incremental gen-\neralization, and arcs denote the relationships among them.\nThe similarity between two melodies can be computed by\nanalyzing the graph structure and ﬁnding the shortest path\nbetween the corresponding nodes inside the graph. Pre-\nliminary results in terms of music similarity are presented\nusing a small test collection.\n1. INTRODUCTION\nOne approach to content-based access to music documents\nis to provide users with tools to retrieve music documents\nthat are similar to a set of one or more documents already\nknown, which are the starting point of a query-by-example\nparadigm. The effectiveness of the results depends on the\nway a measure of music similarity is computed. This task\nis difﬁcult to deﬁne, because the notion of music similarity\nis subjective and also because the role played by the dif-\nferent music dimensions – i.e., rhythm, melody, harmony,\ntimbre, orchestration, tempo – is task dependent. For in-\nstance, the perceived similarity of two ballroom songs is\nmainly related to rhythm, while in jazz music it can de-\npend on chord progressions.\nIn recent years, a major interest has been given to the\nretrieval of audio documents, typically in compressed for-\nmats such as MP3. This trend is explained by the increas-\ning availability of large collections of audio ﬁles, and by\nthe fact that users without a music training are usually not\ninterested in accessing symbolic representations. For this\nreason, the notion of music similarity has been biased to-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.wards the music dimensions that, on the one hand, are\nmore relevant for music listeners and, on the other hand,\ncan be reliably extracted from audio ﬁles. A typical ap-\nproach is to extract some timbre descriptors to address dif-\nferent tasks, such as genre and artist identiﬁcation, auto-\nmatic playlist generation, or music collection visualiza-\ntion [12].\nMusic documents can be represented also in symbolic\nforms, such as a notated digital score or a MIDI ﬁle. The\naccess to these documents can be based on higher level fea-\ntures, such as melodic proﬁle and harmonic progressions,\nwhich are not easily extracted from audio ﬁles. In partic-\nular, the melodic proﬁle has been often used as the main\ndimension to compute the similarity between music doc-\numents [1, 7]. A typical task of melody-based retrieval is\nthe automatic identiﬁcation of a melody sung or hummed\nby the user. For this application, the similarity measure has\nto be robust to local mismatches due to imprecise recall\nfrom memory and to a lack of singing skills by the users,\nbecause it is assumed that the query and the relevant docu-\nments are representing the same information. Although the\nterm query-by-humming was very popular in the early days\nof MIR research [3], recently it has been often replaced by\nthe more general term query-by-example because it is as-\nsumed that users can easily record with a portable device\nan excerpt of the song they are interested to retrieve and\nare not willing to hum a melody in front of an user inter-\nface. Approaches of this kind may be based on approxi-\nmate matching to identify the music work corresponding\nto the recorded performance, a task that is usually deﬁned\nas cover [5] or music [8] identiﬁcation.\nThe computation of melodic similarity can be useful\nalso for applications other than an identiﬁcation task. For\ninstance, in musicological analysis the study of the melodic\nmaterial used by different composers, or consistently used\nby a given composer, is of particular interest. Also eth-\nnomusicological studies can take advantage from melodic\nsimilarity in order to track the evolution of a given song\nover the centuries and its diffusion in different geographi-\ncal regions. Melodic similarity can be exploited to retrieve\nmusic that “sounds like” other well known songs, for in-\nstance to ﬁnd a suitable soundtrack for a TV show.\nWe propose a novel similarity measure computed be-\ntween music content in symbolic format, that takes into\naccount the musical structure of the composition through\nthe application of an analytic method. Our approach aims\n543Oral Session 7: Harmonic & Melodic Similarity and Summarization\nat representing a music collection with a graph structure\nwhere terminal nodes directly describe the music content,\ninternal nodes represent its incremental generalization, and\narcs take track of the relationships among them. Results\nare presented using an excerpt of the dataset used for the\nmelodic similarity task at the Music Information Retrieval\nEvaluation eXchange (MIREX) campaign of 2005 [17].\n2. MEASURING MELODIC SIMILARITY\nA common approach to the computation of melodic sim-\nilarity is to make some assumptions on the perception of\npure tones and melodic intervals and to simplify the melodic\nrepresentation accordingly. This process can be considered\nas a variant of stemming applied to the music domain, be-\ncause it aims at conﬂating into a single stem all the melodic\nvariants that are musically or perceptually similar. A sim-\nple example is the representation of the melodic proﬁle us-\ning only three classes of pitch intervals – ascending, de-\nscending, and same nome – as proposed initially in [3].\nClearly, more complex representations are possible using\neither a ﬁner quantization of the intervals or the analysis of\nthe harmonic role of melodic intervals. In all these cases,\nit is assumed that similar melodic excerpts share the same\nrepresentation. Segmentation can be applied to melodic in-\nformation [10], where the similarity between melodies can\nbe computed as a weighted sum of the similarities between\npairs of segments. This latter approach aims also at efﬁ-\nciency, because retrieval can be based on indexing [2].\nAnother approach is to exploit the properties of well\nknown distance measures, such as the edit distance using\napproximate string matching techniques [4] or the earth\nmover’s distance [16], in order to deal with variants in mu-\nsic content. An alternative is to apply statistical modeling,\nsuch as Markov chains described in [14], to cope with lo-\ncal variations. The general idea is that the melody used\nto query the system is transformed in order to be matched\nwith the melodies in the collection, assuming that the cost\nof the transformation is related to the melodic similarity.\nOne limitation of these approaches is that they make little\nuse of structural information and musicological analysis.\nThe similarity measure proposed in this paper is based\non a different approach. The basic idea is that all melodies\n– the ones in the music collection and the query – un-\ndergo a process of generalization (or simpliﬁcation). The\nidea is motivated by the results of musicological studies,\nsuch as the Generative Theory of Tonal Music [6], the\nImplication-Realization Model [9], and Schenkerian anal-\nysis [13]. In particular, we aim at ﬁnding structural de-\npendencies among the notes of a composition in order to\norganize them into a coherent hierarchy. This task can\nbe achieved by means of a series of simpliﬁcations of the\nmelodic content of a piece, assuming that these simpliﬁca-\ntions correspond to a generalization of the melodic proﬁle.\nThe central part of our approach is the determination of\nwhich notes in a music passage are more structurally sig-\nniﬁcant than others. We use this property to build a hierar-\nchical representation of a single music document and, in-\ncrementally, of a collection of documents. At each step, the\nPS analysisdoc 1\nSegmentationmakeGraphPS analysisdoc 2\nSegmentationmakeGraphPS analysisdoc N\nSegmentationmakeGraphmergeGraphsshortestPath...\nCodingCodingCodingPSR doc 1PSR doc 2PSR doc NPSR collectionPSSHarmonic anal.Harmonic anal.Harmonic anal.Figure 1 . From the music document to the Pseudo-\nStructural Representation.\nmelodic content is analyzed and transformed in a melody\nwith fewer notes and with a simpler proﬁle that should rep-\nresent the most musicologically relevant content informa-\ntion. The procedure ends when the melodic segments are\nrepresented by a single note. We called this representation\nPseudo-Structural Representation (PSR), because it is in-\nspired by structural analysis, yet the algorithm implement-\ning it exploits some simpliﬁcations. The PSR is a graph-\nbased representation, in which the terminal nodes are re-\nlated to music surface and the internal nodes are progres-\nsive generalizations of the surface.\nThe steps to build the PSR of a collection of documents\nare depicted in Figure 1. First, each music document un-\ndergoes harmonic analysis, which highlights the harmonic\nfunction – i.e., tonic, dominant, subdominant – of each\nchord. Although a number of automatic routines is avail-\nable for the computation of the harmonic function, in our\nexperiments we choose to manually annotate the chords.\nThis task, which is the only manual intervention, is in gen-\neral not required for polyphonic documents for which re-\nliable systems for inferring the chord progression already\nexists. Given that the evaluation has been carried out us-\ning the MIREX 2005 dataset for the Symbolic Melodic\nSimilarity task, which documents contain only the main\nmelody, we preferred to manually annotate the chords to\nnot introduce possible sources of mismatch while evaluat-\ning a novel approach. Automatic chord annotation will be\naddressed in future work, with the aim to create a totally\nautomatic procedure. At this stage we prefer to not intro-\nduce possible sources of preprocessing errors that are not\ndependent on the approach.\nThe second step consists in the progressive simpliﬁca-\ntion of the melodic proﬁle, by means of an algorithm in-\nspired by musicological analyzes. First of all, the surface\nmelody is processed to assign three weight coefﬁcients to\neach note. These coefﬁcients are related to: the underly-\ning harmonic function (harmonic weight), the metric po-\nsition (metric weight), and the pitch interval between the\ntone of the melody and the root of the underlying chord\n54410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 2 . Example of Pseudo-Structural analysis on a\nBach’s Choral.\n(melodic weight). A number of weighting schemes have\nbeen devised and evaluated experimentally, as presented\nin Section 4. The basic idea is to assign higher weights\nto relevant notes. For instance, a note with tonic function\nhas a higher harmonic weight than a note with a subdomi-\nnant function, the same applies to notes in strong beats in\nrespect to notes in weak beats for the metric weight; as re-\ngards the melodic weight, a perfect ﬁfth with the chord root\nhas higher weight than a perfect four or a major second.\nThe algorithm works locally on a sliding window, pro-\ngressively eliminating notes with smallest weights within a\nwindow. In the particular implementation, window length\nhas been set equal to the double of the minimum dura-\ntion of the melody, thus a window contains at most two\nnotes. For instance, the surface melody of Figure 2 has a\nminimum duration of an eight-note and consequently the\nwindow length is a quarter-note. In case the window con-\ntains two notes with the same weight, the algorithm applies\nadditional heuristics that take into account (in descend-\ning order of relevance): only melodic weight, only metric\nweight, only harmonic weight, and ﬁnally the relative po-\nsition. The less relevant note is removed and the other one\nis prolonged to cover the duration of the removed note.\nWhen the sliding window reaches the end of the melody,\nthe ﬁrst level of abstraction is completely calculated. Then,\nthe algorithm is applied iteratively to calculate the higher\nlevels. The algorithm stops when the highest level has\none or two notes. Figure 2 shows an example of Pseudo-\nStructural analysis applied to the ﬁrst six bars of the Bach’s\nChoral BWV345, with four levels of progressive general-\nization. The higher the level the more general representa-\ntion of the melodic proﬁle.\nThe analyzed documents are then segmented in musi-\ncal phrases, a task that can be carried out using one of the\ndifferent algorithms that have been presented in the litera-\nture [11]. With the aim of separately measuring the effect\nof all the components, we perform segmentation both man-\nually and automatically, using the algorithms provided by\nthe Miditoolbox. All the approaches have been evaluated\non the same test collection. Segmentation is carried out at\nthe surface level and inherited by the higher levels. In gen-\neral segments can overlap, although automatic segmenta-\ntion algorithms usually provide non overlapping segments.\nThe subsequent step regards the coding of the pitch se-\nbwv345.3-7bwv345.11-15bwv345.24-31bwv345.16-21Figure 3 . Pseudo-Structural Representation (partial view)\nof the document analyzed in Figure 2.\nquences, one for each segment. Because duration is used\nto perform the Pseudo-Structural analysis, it is implicitly\nmodeled at the higher levels of generalization and not di-\nrectly represented in the PSR. Pitch information is repre-\nsented in the form of melodic intervals, that is the differ-\nence between two subsequent notes. Apart from the seg-\nments representing the melodic surface, pitch information\nundergoes different levels of quantization, from a coarse\nrepresentation using three symbols ( 0unison; ±1up/down\ntone) to the distance in semitones.\nEach coded segment is then inserted into the PSR graph\nstructure. In particular, terminal nodes contain segments of\nthe melodic surface and internal nodes contain segments\nwhich are the results of the generalization process. As a\nresult of the Pseudo-Structural analysis, an internal node\nis connected with a directed arc to all the nodes, either in-\nternal or terminal, that correspond to the immediate lower\nlevel of generalization. Nodes that hold the same content\nare joined together in a single one, that inherits the ances-\ntors and the descendants of the starting nodes, obtaining a\ndirect acyclic graph. An example of the PSR of a music\ndocument is shown in Figure 3. The PSR of a complete\ncollection can be built by iterating the process for all the\nsegments and the documents in the collection.\nGiven a PSR, we can deﬁne the distance between two\nmelodic segments s1ands2, represented by the terminal\nnodes n1andn2, as the length of shortest path from n1and\nn2considering PSR as an undirect graph where all the arcs\nhave the weight set to 1. It can be noted that PSR could be\nalso a weighted graph, where each arc has a weight that\ntakes into account the kind of applied simpliﬁcation or the\nrelative frequency nodes appear in a music work and in a\ncollection. This aspect will be investigated in future work.\nThe way the distance is computed can be described through\nan example. With reference to the music document shown\nin Figure 3, the distance between the melody segment from\nbeat 3 to beat 7 (coded by [1 2 -2 -1]) and the melody\nsegment from beat 24 to beat 31 (coded by [-1 2 2 1 2 -\n2 -1]) is equal to 6. This distance is a metric, because it\nis clearly reﬂexive and symmetric. Moreover, being based\non the shortest path inside an undirect graph, it is easy to\nshow that the triangular inequality holds.\n545Oral Session 7: Harmonic & Melodic Similarity and Summarization\n3. APPLICATIONS\nThe ﬁrst application of the similarity measure is a musico-\nlogically grounded approach to content-based retrieval of\nmusic documents using a query by example paradigm. For\nthis application, retrieval can be carried out by perform-\ning the same Pseudo-Structural analysis also to the query,\nwhich is then (temporarily) added to the PSR graph, in or-\nder to compute the similarity. As for other approaches,\nmelodic similarity of the complete documents can be com-\nputed as a weighted sum of the melodic similarity of their\nsegments. In particular, the distance between two docu-\nments d(ci, cj)is deﬁned as the mean of the PSD between\nall the segments of ciandcj. The similarity s(ci, q)be-\ntween cibelonging to a collection of Ndocuments and the\nquery qis calculated through equation\ns(ci, q) =/parenleftBigg\n1 +d(ci, q)\n/summationtextN\nj=1d(ci,cj)\nN−1/parenrightBigg−1\n, (1)\nwhere the similarity is proportional to the reciprocal of the\ndistance d(ci, q)between the document ciand the query q\ndivided by a normalizing factor, which is the mean distance\nbetween ciand all the other documents in the collection.\nThe normalizing factors can be computed off-line in order\nto speed up retrieval.\nThe graph representation provides a simple way to de-\nﬁne the maximum allowable distance between two seg-\nments. For instance, the user may choose to limit the length\nof the paths across the graph or to deﬁne the maximum al-\nlowed level of generalization, that is the number of times a\npath can jump to a higher level. Moreover, the user can be\npresented with a list of documents, their relevant segments,\nand a representation of the internal nodes that shows which\nis the path across the PSR that transforms the query into the\nretrieved segments. It is interesting to note that through\nthis approach, the user can modify its personal view of the\nPSR, because past queries can be stored in the graph and\neventually affect the results of the current retrieval session.\nThe analysis of the structure of the PSR can provide\nnovel tools for exploring a music collection. For instance,\nthe user can choose a branch of the PSR and explore the\nmelodic excerpt that are represented by internal or by ter-\nminal nodes and, in the latter case, to listen to the composi-\ntions they belong to. Thus, the user can navigate inside the\nmusic segments of a collection, and their generalization\nbased on musicological properties. To this end, informal\ntests showed that PSR tends to group similar composing\nstyles in close regions of the graph. This ability of the pro-\nposed approach will be explored in future work.\n4. EXPERIMENTAL EV ALUATION\nThe methodology has been experimentally evaluated using\nthe dataset provided for the Symbolic Melodic Similarity\ntask at MIREX 2005 [17]. The dataset is based on the\nRISM collection of incipits, where relevance judgments\non melodic similarity have been provided by a pool of ex-\npert musicologists. We present three different measures#symbols ADR AP R-P\n3 0.65 0.60 0.54\n5 0.66 0.60 0.52\n7 0.65 0.59 0.51\nno quantization 0.67 0.64 0.56\nTable 1 . Results with manual segmentation and using dif-\nferent levels of quantization.\nsegmentation ADR AP R-P\nmanual 0.67 0.64 0.56\ngestalt 0.69 0.64 0.55\nprobabilistic 0.67 0.61 0.53\nLBDM 0.61 0.53 0.50\nTable 2 . Results with no quantization and using different\napproaches to segmentation.\nof retrieval effectiveness. The common measures average\nprecision (AP) and R-precision (RP), and the Average Dy-\nnamic Recall (ADR) which takes into account that rele-\nvance judgments are not binary [15] and has been used as\nthe main parameter at MIREX 2005.\nResults on a subset of 110incipits using 11queries are\nreported in Table 1, showing the effect of different levels\nof quantization when manual segmentation is applied. The\nreduced size of the collection is due to the fact that, for this\ninitial evaluation, part of the process shown in Figure 1\n– annotation of the harmonic function and segmentation –\nhas been carried out manually. As it can be seen, results are\nsimilar, with slightly better performances when no quanti-\nzation is applied, although the differences are not statisti-\ncally signiﬁcative. This aspect should be investigated in\nmore detail with a larger collection because a coarse quan-\ntization allows us to reduce the size of the PSR improving\nefﬁciency, yet a ﬁne quantization preserves more informa-\ntion about the melodic content.\nWe carried out an experiment using three automatic ap-\nproaches to segmentation when no quantization was ap-\nplied. Segmentation algorithms are the ones provided by\nthe Miditoolbox, which are based on gestalt concepts, on\nprobabilistic model, and on the Local Boundaries Detec-\ntion Model respectively. Results are reported in Table 2,\nshowing that the gestalt-based approach gives results com-\npletely comparable with manual segmentation. A test on\nthe statistical signiﬁcance of the differences between these\nresults showed that none of the differences reached the sig-\nniﬁcance, thus this step can be carried out automatically\nwithout affecting retrieval effectiveness.\nOther experiments have been carried out to evaluate dif-\nferent weighting schemes for the Pseudo-Structural analy-\nsis. As regards the harmonic weight, we tested the effec-\ntiveness of grouping the harmonic functions in 3,4, or7\nclasses (denoted with letter H). For instance, for 3Hwe\ngrouped the harmonic functions depending on their degree\non the scale, namely I and VI had the highest weight, IV\nand V had a intermediate weight and other degrees had the\n54610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nweighting scheme ADR AP R-P\n3H,MH ,3M 0.67 0.64 0.56\n4H,MH ,3M 0.65 0.63 0.56\n7H,MH ,3M 0.65 0.64 0.56\n3H,MH ,4M 0.67 0.64 0.55\n3H,MH ,7M 0.61 0.60 0.51\n3H,MS,3M 0.66 0.63 0.52\nTable 3 . Results using different weighting schemes.\nsmallest weight. The metric weight was varied considering\neither a simple subdivision (MS) in strong and weak beats\nor a hierarchical organization (MH) depending on the po-\nsition in the measure. Finally, the melodic weight has been\ntested in a similar fashion of the harmonic weight, with 3,\n4, and 7classes (denoted with letter M) where, for exam-\nple, in 3Mnotes forming an interval of a unison/octave,\nthird or ﬁfth from the fundamental had the highest weight,\na seventh had an intermediate weight, and other intervals\nhad the smallest weight. Weighting schemes with more\nclasses, such as 4Hand 7Hfor harmonic weight, sim-\nply introduce new classes either taking into account new\nharmonic functions or splitting an existing class in two or\nmore smaller classes. Similar considerations apply to the\nmelodic weights.\nResults are reported in Table 3, for some combination\nof weighting schemes, showing that the use of three classes\nfor both the harmonic ( 3H) and the melodic ( 3M) weights\nwith a hierarchical metric (MH) weight gave the best per-\nformances, although differences are minimal and not sta-\ntistically signiﬁcative. Considering that in all the exper-\niments the three effectiveness measures are considerably\nhigher than the ones obtained at MIREX, we can assume\nthat with a larger collection the performances will be at\nleast comparable with other approaches.\nAn important characteristic of the PSR is the relation-\nship between the number of documents in the collection\nand the number of different nodes in its graph represen-\ntation. It is expected that the size of the graph will in-\ncrease with sublinear trend when documents are added to\nthe PSR. Figure 4 shows this trend, where the number of\nnew nodes – both internal and terminal – that are added\nwith each new document decreases with the number of\ndocuments, although local variations can still be seen due\nto the reduced size of the test collection.\nFigure 5 shows the results of a nearest neighbor query\non the collection, with the aim of highlighting which fea-\ntures are captured by the proposed similarity measure and\nwhether this deﬁnition can take into account for progres-\nsive differences among melodies. The searched pattern is\nthe surface melody of the ﬁrst segment of 000.109.406−\n1.1.1in the RISM collection. Pseudo-Structural analysis\nof this composition highlights that the notes at positions 2,\n4, and 6 (notes B, B, and G respectively) are less relevant\nthan the others. Indeed, all these notes are passing notes on\na weak metric position. At an higher level of generaliza-\ntion in the PSR, these notes are therefore omitted, and the\nFigure 4 . Number of new nodes added to the PSR graph\nwhen new music documents are added. The bold line is a\nlogaritmic aproximation in a least square error sense.\nsegment is represented by an ascending melodic interval\n(A-C), followed by two descending melodic intervals (C-\nA and A-F), which correspond to the code [+2 -2 -2] using\nthe coarsest quantization. At a distance d= 1, the result\nreports a segment with identical pitch, but with augmented\nduration values. It can be noted that the ratio among du-\nrations are unchanged in respect to the query segment. At\na distance d= 2, there are segments with similar but not\nequal pitches, and with minor metric variations. Finally, at\ndistance d= 3andd= 4there are melodic segments com-\nposed by notes that at least share with the query segment\nthe same higher level code [+2 -2 -2].\n000.109.446-1.1.1.\n000.112.625-1.1.1\n000.116.073-1.1.1\n230.004.687-1.1.1\n250.004.931-1.4.1\n800.000.399-1.1.1\n400.008.388-1.1.1\n000.127.493-1.1.1d=0d=1d=2d=3d=4\nFigure 5 . Results of a nearest neighbor query.\n5. CONCLUSIONS\nThe proposed approach aims at representing the structural\nrelationships of a music collection with an undirected graph,\nwhich is built from the analysis of the melodic content of\nmusic documents. Terminal nodes represent melodic seg-\nments of the documents, while internal nodes represent a\nprogressive simpliﬁcation/generalization of their content.\nMusic similarity is then measured by the length of the short-\nest path between terminal nodes. This representation al-\n547Oral Session 7: Harmonic & Melodic Similarity and Summarization\nlows us to retrieve music documents that are relevant at\nleast from a musicological point of view. Moreover, the\nproposed similarity is a metric because it is based on a\ntopological distance, allowing us to efﬁciently carry out a\nnumber of retrieval tasks, such as range queries, k-nearest\nneighbor, and document clustering.\nThe approach has been tested with a collection of in-\ncipits. Moreover, qualitative analysis have been carried\nout on the relationships between the graph structure and\nthe melodic content of the documents. Results are encour-\naging, both in terms of average precision of the retrieval\nresults and in terms of musicological signiﬁcance. The ap-\nproach can be further exploited for browsing a collection\nof music documents based on the traversal of the graph\nrepresenting the music documents and their relationship.\nThe described similarity measure is tailored to music\ngenres where harmony plays a functional role, like in West-\nern tonal music, because the weighting schemes presented\nin Section 2 are mostly based on the harmonic content.\nThe idea itself of generalizing the melodic content through\nstructural analysis is motivated by musicological studies\non Western music. Although we believe that it is difﬁcult,\nif not impossible, to develop a general purpose approach to\nmusic similarity searches, it is likely that the idea of rep-\nresenting music content with a hierarchical graph, where\nlevels are associated to an incremental simpliﬁcation of the\nmusical content, can be generalized to other music features\nand to other genres.\nA major limitation is that, at the moment, the method-\nology is still partially based on manual annotation of the\nchord progressions of the musical documents. Given the\nencouraging results, future work will focus on the com-\nplete automatization of the analyzes.\n6. ACKNOWLEDGMENTS\nThe work has been partially supported by a grant of the\nUniversity of Padova for the project “Analysis, design, and\ndevelopment of novel methodologies for the study and the\ndissemination of music works”.\n7. REFERENCES\n[1] J.S. Downie. Music information retrieval. Annual Re-\nview of Information Science and Technology , 37:295–\n340, 2003.\n[2] J.S. Downie and M. Nelson. Evaluation of a simple and\neffective music information retrieval method. In Pro-\nceedings of the ACM International Conference on Re-\nsearch and Development in Information Retrieval (SI-\nGIR) , pages 73–80, 2000.\n[3] A. Ghias, J. Logan, D. Chamberlin, and B.C. Smith.\nQuery by humming: Musical information retrieval in\nan audio database. In Proceedings of the ACM Confer-\nence on Digital Libraries , pages 231–236, 1995.\n[4] H.H. Hoos, K. Renz, and M. G ¨org. GUIDO/MIR –\nan experimental musical information retrieval systembased on GUIDO music notation. In Proceedings of\nthe International Symposium on Music Information Re-\ntrieval , pages 41–50, 2001.\n[5] P. Herrera J. Serr ´a, E. G ´omez and X. Serra. Chroma\nbinary similarity and local alignment applied to cover\nsong identiﬁcation. IEEE Transactions on Audio,\nSpeech, and Language Processing , 16(6):1138–1151,\n2008.\n[6] F. Lerdhal and R. Jackendoff. A Generative Theory of\nTonal Music . The MIT Press, Cambridge, MA, 1983.\n[7] M. Melucci and N. Orio. Combining melody process-\ning and information retrieval techniques: Methodol-\nogy, evaluation, and system implementation. Journal\nof the American Society for Information Science and\nTechnology , 55(12):1058–1066, 2004.\n[8] R. Miotto and N. Orio. A music identiﬁcation system\nbased on chroma indexing and statistical modeling. In\nProceedings of the International Conference on Music\nInformation Retrieval , pages 301–306, 2008.\n[9] E. Narmour. The Analysis and Cognition of Ba-\nsic Melodic Structures . University of Chicago Press,\nChicago, MI, 1990.\n[10] G. Neve and N. Orio. A comparison of melodic seg-\nmentation techniques for music information retrieval.\nInProceedings of the European Conference on Digital\nLibraries , pages 49–56, 2005.\n[11] B.S. Ong. Structural analysis and segmentation of mu-\nsic signals. Master’s thesis, Universitat Pompeu Fabra,\n2006.\n[12] N. Orio. Music retrieval: A tutorial and review. Foun-\ndations and Trends in Information Retrieval , 1(1):1–\n90, 2006.\n[13] H. Schenker. Der Freie Satz, Neue musikalische Theo-\nrien und Phantasien . Universal Wien, O. Jonas, 1956\nedition, 1935.\n[14] J. Shifrin, B. Pardo, C. Meek, and W. Birmingham.\nHMM-based musical query retrieval. In Proceedings of\nthe ACM/IEEE Joint Conference on Digital Libraries ,\npages 295–300, 2002.\n[15] R. Typke, R.C. Veltkamp, and F. Wiering. Evaluating\nretrieval techniques based on partially ordered ground\ntruth lists. In Proceedings of the International Confer-\nence of Multimedia and Expo , 2006.\n[16] R. Typke, F. Wiering, and R.C. Veltkamp. A search\nmethod for notated polyphonic music with pitch and\ntempo ﬂuctuations. In Proceedings of the International\nConference of Music Information Retrieval , pages\n281–288, 2004.\n[17] Mirex 2005 Wiki. First annual Music Informa-\ntion Retrieval Evaluation eXchange, July 2006.\nhttp://www.music-ir.org/mirex2005/.\n548"
    },
    {
        "title": "Template-based Chord Recognition : Influence of the Chord Types.",
        "author": [
            "Laurent Oudre",
            "Yves Grenier",
            "Cédric Févotte"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414884",
        "url": "https://doi.org/10.5281/zenodo.1414884",
        "ee": "https://zenodo.org/records/1414884/files/OudreGF09.pdf",
        "abstract": "This paper describes a fast and efficient template-based chord recognition method. We introduce three chord models taking into account one or more harmonics for the notes of the chord. The use of pre-determined chord models enables to consider several types of chords (major, minor, dominant seventh, minor seventh, augmented, diminished...). After extracting a chromagram from the signal, the detected chord over a frame is the one minimizing a measure of fit between the chromagram frame and the chord templates. Several popular measures in the probability and signal processing field are considered for our task. In order to take into account the time persistence, we perform a post-processing filtering over the recognition criteria. The transcription tool is evaluated on the 13 Beatles albums with different chord types and compared to state-of-theart chord recognition methods. We particularly focus on the influence of the chord types considered over the performances of the system. Experimental results show that our method outperforms the state-of-the-art and more importantly is less computationally demanding than the other evaluated systems.",
        "zenodo_id": 1414884,
        "dblp_key": "conf/ismir/OudreGF09",
        "keywords": [
            "chord recognition",
            "template-based",
            "fast and efficient",
            "chromagram extraction",
            "chord models",
            "harmonics",
            "pre-determined chord models",
            "probability and signal processing",
            "post-processing filtering",
            "transcription tool"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTEMPLATE-BASED CHORD RECOGNITION : INFLUENCE OF THE\nCHORD TYPES\nLaurent Oudre1, Yves Grenier1, C´edric F ´evotte2\n1Institut TELECOM ; TELECOM ParisTech ; CNRS LTCI\n2CNRS LTCI ; TELECOM ParisTech\n37-39 rue Dareau, 75014 Paris, France\n{oudre,grenier,fevotte }@telecom-paristech.fr\nABSTRACT\nThis paper describes a fast and efﬁcient template-based\nchord recognition method. We introduce three chord mod-\nels taking into account one or more harmonics for the notes\nof the chord. The use of pre-determined chord models\nenables to consider several types of chords (major, mi-\nnor, dominant seventh, minor seventh, augmented, dimin-\nished...). After extracting a chromagram from the signal,\nthe detected chord over a frame is the one minimizing a\nmeasure of ﬁt between the chromagram frame and the chord\ntemplates. Several popular measures in the probability and\nsignal processing ﬁeld are considered for our task. In or-\nder to take into account the time persistence, we perform a\npost-processing ﬁltering over the recognition criteria. T he\ntranscription tool is evaluated on the 13 Beatles albums\nwith different chord types and compared to state-of-the-\nart chord recognition methods. We particularly focus on\nthe inﬂuence of the chord types considered over the per-\nformances of the system. Experimental results show that\nour method outperforms the state-of-the-art and more im-\nportantly is less computationally demanding than the other\nevaluated systems.\n1. INTRODUCTION\nChord transcription is a compact representation of the har-\nmonic content and structure of a song. Automatic chord\ntranscription ﬁnds many applications in the ﬁeld of Musi-\ncal Information Retrieval such as song identiﬁcation, quer y\nby similarity or structure analysis.\nThe features used for chord recognition may differ from\na method to another but are in most cases variants of the\n12-dimensional Pitch Class Proﬁles [1]. Every component\nrepresents the spectral energy of a semi-tone on the chro-\nmatic scale regardless of the octave. The succession of\nthese chroma vectors over time is called chromagram : the\nchord recognition task consists in outputting a chord label\nfor every chromagram frame.\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnot made or distributed for proﬁt or commercial advantage and th at copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval .The ﬁrst chord recognition systems consider many chord\ntypes. The method proposed by Fujishima [1] considers\n27 chord types. The transcription is done either by mini-\nmizing the Euclidean distance between Pitch Class Proﬁles\nand 12-dimensional chord templates constituted by 1’s (for\nthe chromas present in the chord) and 0’s (for the other\nchromas) or by maximizing a weighted dot product. Sheh\n& Ellis [2] use a Hidden Markov Model composed of 147\nhidden states each representing a chord (7 types of chords\nand 21 root notes). All the HMM parameters are learned\nby a semi-supervised training with an EM algorithm.\nThese two methods have been improved upon by reduc-\ning the number of chord types considered. Fujishima’s sys-\ntem is improved in [3] by reducing the number of chords\ntypes from 27 to 4 (major, minor, augmented, diminished)\nand by calculating a more elaborate chromagram includ-\ning notably a tuning algorithm. Chord transcription is then\nrealized by retaining the chord with larger dot product be-\ntween the chord templates and the chromagram frames.\nSheh & Ellis method is modiﬁed in [4] : the number of\nhidden states is reduced from 147 to 24 by only consid-\nering major and minor chords for the 12 semi-tones root\nnotes. Musical knowledge is introduced into the model by\ninitializing the HMMs parameters with values inspired by\nmusical and cognitive theory. Since then, almost all the\nchord transcription methods [5], [6], [7], [8], [9], only co n-\nsider major and minor chords.\nOur chord recognition system is based on the intuitive\nidea that for a given 12-dimensional chroma vector, the\namplitudes of the chromas present in the chord should be\nlarger than the ones of the non-played chromas. By intro-\nducing chord templates for different chord types and roots,\nthe chord present on a frame should therefore be the one\nwhose template is the closest to the chroma vector accord-\ning to a speciﬁc measure of ﬁt.\nThe paper is organized as follows. Section 2 gives a de-\nscription of our recognition system. Section 3 describes th e\ncorpus and the protocol of evaluation. Section 4 presents\nthe results of our system, a study on the inﬂuence of the\nchord types considered, a comparison with the state-of-the -\nart and an analysis of the frequent errors. Finally the main\nconclusions of this work are summarized in Section 5.\n153Poster Session 1\n2. DESCRIPTION OF THE SYSTEM\n2.1 General idea\nGiven Nsuccessive chroma vectors {cn}n,Kchord tem-\nplates {pk}kand a measure of ﬁt D, we deﬁne :\ndk,n=D(hk,ncn;pk). (1)\nhk,nis a scale parameter whose role is to ﬁt the chroma\nvectorcnwith the chord template pkaccording to the mea-\nsure of ﬁt used. In practice, hk,nis calculated such as :\nhk,n= argmin\nhD(hcn;pk). (2)\nThe detected chord ˆknfor frame nis then the one min-\nimizing the set {dk,n}k:\nˆkn= argmin\nk{dk,n}. (3)\nIn our system, the chroma vectors are calculated from\nthe music signal with the same method as Bello & Pickens\n[4]. The frame length is set to 753 ms and the hop size is\nset to 93 ms. We use the code kindly provided by these\nauthors.\nWe have omitted for sake of conciseness the expressions\nofdk,nandhk,nwhich are easily obtained by canceling the\ngradient of (1) wrt hk,n.\n2.2 Chord models\nThe intuitive chord model is a simple binary mask consti-\ntuted of 1’s for the chromas present in the chord and 0’s for\nthe other chromas [1], [3].\nYet, the information contained in a chromagram cap-\ntures not only the intensity of every note but a blend of in-\ntensities for the harmonics of every note. Like Gomez [10]\nand Papadopoulos [5], we assume an exponentially de-\ncreasing spectral proﬁle for the amplitudes of the partials .\nAn amplitude of 0.6i−1is added for the ithharmonic of\nevery note in the chord.\nIn our system three chord models are deﬁned, corre-\nsponding to 1, 4 or 6 harmonics. Examples for C major\nand C minor chords are displayed on Figure 1.\nFrom these three chord models we can build chord tem-\nplates for all types of chords (major, minor, dominant sev-\nenth, diminished, augmented,...). By convention in our\nsystem, the chord templates are normalized so that the sum\nof the amplitudes is 1.\n2.3 Measures of ﬁt\nWe consider for our recognition task several measures of\nﬁt, popular in the ﬁeld of signal processing : the Euclidean\ndistance (later referred as EUC ), the Itakura-Saito diver-\ngence [11] and the Kullback-Leibler divergence [12].\nSince the Itakura-Saito and Kullback-Leibler divergence\nare not symmetrical, they can be calculated in two ways.\nD(hk,ncn|pk)will respectively deﬁne IS1andKL1, while\nD(pk|hk,ncn)will deﬁne IS2andKL2.CC#DD#EFF#GG#AA#B00.20.4C major with 1 harmonic\nCC#DD#EFF#GG#AA#B00.20.4C major with 4 harmonics\nCC#DD#EFF#GG#AA#B00.20.4C major with 6 harmonicsCC#DD#EFF#GG#AA#B00.20.4C minor with 1 harmonic\nCC#DD#EFF#GG#AA#B00.20.4C minor with 4 harmonics\nCC#DD#EFF#GG#AA#B00.20.4C minor with 6 harmonics\nFigure 1 . Chord templates for C major / C minor with 1, 4\nor 6 harmonics.\n2.4 Filtering methods\nIn order to take into account the time-persistence, we intro -\nduce some post processing ﬁltering methods which work\nupstream on the calculated measures and not on the se-\nquence of detected chords.\nThe new criterion ˜dk,nis based on Lsuccessive val-\nues {dk,n′}n−L−1\n2≤n′≤n+L−1\n2previously calculated. In\nour system two types of ﬁltering are used.\nThelow-pass ﬁltering takes the mean of the Lvalues.\nIt tends to smooth the output chord sequence and to reﬂect\nthe long-term trend in the chord change.\nThemedian ﬁltering takes the median of the Lvalues.\nIt has been widely used in image processing and is partic-\nularly efﬁcient to correct random errors.\nIn every case, the detected chord ˆknon frame nis the\none that minimizes the set of values/braceleftBig\n˜dk,n/bracerightBig\nk:\nˆkn= argmin\nk/braceleftBig\n˜dk,n/bracerightBig\n(4)\n3. EV ALUATION\n3.1 Corpus\nThe evaluation database used in this paper is made of the\n13 Beatles albums (180 songs, PCM 44100 Hz, 16 bits,\nmono). The chord annotations for these 13 Beatles albums\nare kindly provided by Harte and Sander [13].\nIn these annotation ﬁles, 17 types of chords and one\n‘no chord’ label (N) corresponding to silences or untuned\nmaterial are present.\nThe most common chord types in the corpus are major\n(63.89% of the total duration), minor (16.19%), dominant\nseventh (7.17%) and ‘no chord’ states (4.50%). Figure 2\nshows the repartition of the chord types among the 13 al-\nbums of the Beatles. We can see that the number of major,\nminor and dominant seventh chords varies much with the\nalbum. Yet, the last six albums clearly contain more chord\n15410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMajor Minor Dominant Seventh Others01020304050607080Percent (%)Repartition of the chord types over the 13 Beatles albums\nFigure 2 . Repartition of the chord types as percentage of\nthe total duration for the 13 Beatles albums.\ntypes (other than major, minor and dominant seventh) than\nthe ﬁrst seven ones.\n3.2 Protocol of evaluation\nThe evaluation method used in this paper corresponds to\nthe one used in MIREX 08 for the Audio Chord Detection\ntask.1\nAs the evaluation method only takes into account major\nand minor chords, the 17 types of chords present in the an-\nnotation ﬁles are ﬁrst mapped into major and minor types\nfollowing the rules used in MIREX 08 :\n•major : maj, dim, aug, maj7, 7, dim7, hdim7, maj6,\n9, maj9, sus4, sus2\n•minor : min, min7, minmaj7, min6, min9\nFor the systems detecting more chord types (dominant\nseventh, diminished, etc.), once the chords have been de-\ntected with their appropriate models, they are then mapped\nto the major and minor following the same rules than for\nthe annotation ﬁles.\nA score is calculated for each song as the ratio between\nthe lengths of the correctly analyzed chords and the to-\ntal length of the song. The ﬁnal Average Overlap Score\n(AOS) is then obtained by averaging the scores of all the\n180 songs. An example of calculation of an Overlap Score\nis presented on Figure 3.\n4. RESULTS\nThe ﬁve previously described measures of ﬁt ( EUC ,IS1,\nIS2,KL1 andKL2), three chord models (1, 4 or 6 harmon-\nics) and two ﬁltering methods (low-pass and median) with\nneighborhood sizes from L= 1toL= 25 are tested. For\nevery method we only present the results for the optimal\nparameters (measure of ﬁt, chord models, ﬁltering method\nand neighborhood size).\n1http://www.music-ir.org/mirex/2008/4.1 Results with major/minor chord types\nConsidering only major and minor chords (like most of the\nchord recognition methods of the actual state-of-art), we\nobtain a Average Overlap Score of 0.70 over the 13 Beatles\nalbums. The optimal parameters are the Kullback-Leibler\ndivergence KL2, the single harmonic chord model and the\nmedian ﬁltering with a neighborhood size of L= 17 .\n4.2 Introduction of other chord types\nThe simplicity of our method allows to easily introduce\nchord templates for chord types other than major and mi-\nnor : we study here the inﬂuence of the chord types consid-\nered over the performances of our system. The choice of\nthese chord types is guided by the statistics on the corpus\npreviously presented : we introduce in priority the most\ncommon chords types of the corpus.\n4.2.1 Dominant seventh and minor seventh chords\nIn the Beatles corpus, the two most common chord types\nother than major and minor are dominant seventh (7)and\nminor seventh (min7) chords. The results for major, minor,\ndominant seventh and minor seventh chords are presented\nin Table 1. The score displayed in a case is the best Average\nOverlap Score obtained by considering the chord types of\nthe corresponding row and column.\nmin min7 min & min7\nmaj 0.70 0.64 0.69\n7 0.69 0.63 0.65\nmaj & 7 0.71 0.66 0.69\nTable 1 . Average Overlap Scores with major, minor, dom-\ninant seventh and minor seventh chords.\nThe best results are obtained by detecting major, minor\nand dominant seventh chords, with the Kullback-Leibler\ndivergence KL2, the single harmonic chord model and the\nmedian ﬁltering with L= 17 giving a recognition rate of\n71%. Only the introduction of dominant seventh chords,\nwhich are very common in the Beatles corpus, enhances\nthe results. The introduction of minor seventh chords, whic h\nare less common, degrades the results. Indeed, the struc-\nture of minor seventh chords (for example Cmin7 ) leads to\nconfusion between the actual minor chord and the relative\nmajor chord ( E♭in our example).\n4.2.2 Augmented and diminished chords\nAugmented and diminished chords have been considered\nin many template-based chord recognition systems [1], [3].\nInterestingly, while the augmented and diminished chords\nare very rare in the Beatles corpus (respectively 0.62% and\n0.38% of the total length), the introduction of chord tem-\nplates for augmented and diminished chords does not de-\ngrade the results. We obtain a recognition rate of 69%\nby considering major, minor, augmented and diminished\nchords and of 71% by taking into account major, minor,\ndominant seventh, augmented and diminished chords.\n155Poster Session 1\nground truth :\ntranscription :\noverlap :\nOverlap Score =3+4\n10= 0.70C major A minor\nC major F major A minor\nFigure 3 . Example of calculation of an Overlap Score.\n4.2.3 Other chord types\nThe introduction of other chord types (ninth, major sev-\nenth, sus4, etc.) does not improve the results. This can\nbe explained either by the structures of the chords which\ncan lead to confusions with other chord types or by the\nlow number of chords of these types in the Beatles cor-\npus. Indeed, the introduction of a model for a new chord\ntype gives a better detection for chords of this type but also\nleads to new errors such as false detections. Therefore only\nfrequent chords types should be introduced, ensuring that\nthe enhancement caused by the better recognition of these\nchord types is larger than the degradation of the results\ncaused by the false detections.\n4.3 Inﬂuence of the album\n1234567891011121300.10.20.30.40.50.60.70.80.9\nBeatles albumAOSMeasures minimization (maj−min)\n1234567891011121300.10.20.30.40.50.60.70.80.9\nBeatles albumAOSMeasures minimization (maj−min−7)\nFigure 4 . Average Overlap Scores for the 13 Beatles al-\nbums (in chronological order) for the major/minor and the\nmajor/minor/dominant seventh methods.\nWe can see on Figure 4 that results are better for the ﬁrst\nseven albums : this can be explained by the low number\nof chords other than major, minor and dominant seventh\non these albums (see Figure 2). Surprisingly the introduc-\ntion of dominant seventh chords tend to improve results not\nnecessarily on albums containing many dominant seventh\nchords (for example album number 3) but on albums con-\ntaining many chords other than major, minor and dominant\nseventh (for example albums number 8 & 11).4.4 State-of-the-art\nOur method is now compared to the following methods that\nentered MIREX 08.\nBello & Pickens [4] use 24-states HMM with musically\ninspired initializations, Gaussian observation probabil ity\ndistributions and EM-training for the initial state distri bu-\ntion and the state transition matrix.\nRyyn ¨anen & Klapuri [6] use 24-states HMM with ob-\nservation probability distributions computed by comparin g\nlow and high-register proﬁles with some trained chord pro-\nﬁles. EM-training is used for the initial state distributio n\nand the state transition matrix.\nKhadkevich & Omologo [7] use 24 HMMs : one for\nevery chord. The observation probability distributions ar e\nGaussian mixtures and all the parameters are trained throug h\nEM.\nPauwels, Verewyck & Martens [8] use a probabilis-\ntic framework derived from Lerdahl’s tonal distance metric\nfor the joint tasks of chords and key recognition.\nThese methods have been tested with their original im-\nplementations on the same Beatles corpus than before and\nevaluated with the same protocol (AOS). Results of this\ncomparison with the state-of-the-art are presented on Ta-\nble 2.\nAOS Time\nOur method (Maj-Min-7) 0.71 796s\nBello & Pickens 0.70 1619s\nOur method (Maj-Min) 0.70 790s\nRyyn ¨anen & Klapuri 0.69 1080s\nKhadkevich & Omologo 0.64 1668s\nPauwels, Varewyck & Martens 0.62 12402s\nTable 2 . Comparison with the state-of-the-art.\nFirst of all it is noticeable that all the methods give\nrather close results : there is only a 9% difference between\nthe methods giving the best and worse results. Our method\ngives the best results, but more importantly with a very\nlow computational time. It is indeed twice as fast as the\nbest state-of-the-art method (Bello and Pickens).\n15610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n4.5 Analysis of the errors\nIn most chord transcription systems, the errors are often\ncaused by the structural similarity (common notes) and\nthe harmonic proximity between the real chord and the\nwrongly detected chord.\nTwo chords are likely to be mistaken one for another\nwhen they look alike , that is to say, when they share notes\n(especially in template-based systems). Given a major or\nminor chord, there are 3 chords which have 2 notes in com-\nmon with this chord : the parallel minor/major, the relative\nminor/major (or submediant) and the mediant chord.\nBesides the structural similarity, errors can also be cause d\nby the harmonic proximity between the original and the de-\ntected chord. Figure 5 pictures the doubly nested circle of\nﬁfths which represents the major chords (capital letters),\nthe minor chords (lower-case letters) and their harmonic\nrelationships. The distance linking two chords on this dou-\nbly nested circle of ﬁfths is an indication of their harmonic\nproximity.\nGiven a major or minor chord, the 4 closest chords on\nthis circle are the relative (submediant), mediant, subdom -\ninant and dominant. One can notice that these 4 chords\nare also structurally close to the original chord, since the y\nshare 1 or 2 notes with it.\nFigure 5 . Doubly nested circle of ﬁfths [4].\nWe have therefore brought out 5 potential sources of\nerrors among the 23 possible ones (i.e., the 23 other wrong\ncandidates for one reference chord). Examples of these\npotential sources of errors for C major and C minor chords\nare displayed on Figure 6.\nReference chord C Cm\nparallel Cm C\nrelative (submediant) Am A♭\nmediant Em E♭\nsubdominant F Fm\ndominant G Gm\nFigure 6 . Particular relationships between chords and po-\ntential sources of errors : examples for C major and C mi-\nnor chords.\nFigure 7 displays the repartition of these error types as a\npercentage of the total number of errors for every evaluatedmethod. Errors due to the bad detection of the ’no chord’\nstates are represented with the ’no chord’ label.\nThe main sources of errors correspond to the situations\npreviously described and to the errors caused by silences\n(’no chord’). Actually, in most methods, the 5 types of\nerrors previously considered (over the 23 possible ones)\nrepresent approximately 60% of the errors.\nThe introduction of the dominant seventh chords clearly\nreduces the proportion of the errors due to relative (subme-\ndiant) and mediant (-9%). Another noteworthy result is\nthat the methods by Ryyn ¨anen & Klapuri, Bello & Pick-\nens and our major/minor method approximately have the\nsame error repartition despite the different structures of the\nmethods, which proves that the semantic of the errors is\ninherent to the task. Pauwels, Varewyck & Martens’ sys-\ntem is mostly penalized by the wrong detection of the ’no\nchord’ states, when Khadkevich & Omologo’s method pro-\nduces a wider range of errors.\n5. CONCLUSION\nOur system offers a novel perspective about chord detec-\ntion. The joint use of popular measures and ﬁltering meth-\nods distinguishes from the predominant HMM-based ap-\nproaches. The introduction of chord templates allows to\neasily consider many chord types instead of only major and\nminor chords. Since our method is only based on the chro-\nmagram no information about style, rhythm or instruments\nis required and thank to the fact that no training or database\nis needed, the computation time can be kept really low.\n6. ACKNOWLEDGMENT\nThe authors would like to thank J. Bello, M. Khadkevich,\nJ. Pauwels, M. Ryyn ¨anen for making their code available.\nWe also wish to thank C. Harte for his very useful annota-\ntion ﬁles.\nThis work was realized as part of the Quaero Programme,\nfunded by OSEO, French State agency for innovation.\n7. REFERENCES\n[1] T. Fujishima. Realtime chord recognition of musical\nsound: a system using Common Lisp Music. In Pro-\nceedings of the International Computer Music Confer-\nence (ICMC) , pages 464–467, Beijing, China, 1999.\n[2] A. Sheh and D.P.W. Ellis. Chord segmentation and\nrecognition using EM-trained hidden Markov mod-\nels. In Proceedings of the International Symposium on\nMusic Information Retrieval (ISMIR) , pages 185–191,\nBaltimore, MD, 2003.\n[3] C.A. Harte and M.B. Sandler. Automatic chord identi-\nﬁcation using a quantised chromagram. In Proceedings\nof the Audio Engineering Society , Barcelona, Spain,\n2005.\n157Poster Session 1\n22%\n10%\n7%\n12% 9%8%32%Measures minimization (maj−min)\n22%\n4%\n4%\n12%\n11%8%39%Measures minimization (maj−min−7)\n14%\n16%\n8%\n11% 11%9%31%Bello & Pickens\n24%\n7%\n10%\n13%8%6%31%Ryynänen & Klapuri\n10%\n6%\n6%\n10%\n8%\n9%51%Khadkevich & Omologo\n7%\n7%\n6%\n13%\n11%\n24%32%Pauwels, Varewyck & Martens\n  parallels\nrelatives\nmediants\nsubdominants\ndominants\nno chord\nothers\nFigure 7 . Repartition of the errors as a percentage of the total numbe r of errors.\n[4] J.P. Bello and J. Pickens. A robust mid-level represen-\ntation for harmonic content in music signals. In Pro-\nceedings of the International Symposium on Music In-\nformation Retrieval (ISMIR) , pages 304–311, London,\nUK, 2005.\n[5] H. Papadopoulos and G. Peeters. Large-scale study of\nchord estimation algorithms based on chroma repre-\nsentation and HMM. In Proceedings of the Interna-\ntional Workshop on Content-Based Multimedia Index-\ning, pages 53–60, Bordeaux, France, 2007.\n[6] M.P. Ryyn ¨anen and A.P. Klapuri. Automatic transcrip-\ntion of melody, bass line, and chords in polyphonic mu-\nsic.Computer Music Journal , 32(3):72–86, 2008.\n[7] M. Khadkevich and M. Omologo. Mirex audio chord\ndetection. Abstract of the Music Information Retrieval\nEvaluation Exchange, 2008.\n[8] J. Pauwels, M. Varewyck, and J-P. Martens. Audio\nchord extraction using a probabilistic model. Abstract\nof the Music Information Retrieval Evaluation Ex-\nchange, 2008.\n[9] K. Lee and M. Slaney. Acoustic chord transcription and\nkey extraction from audio using key-dependent HMMs\ntrained on synthesized audio. IEEE Transactions onAudio, Speech and Language Processing , 16(2):291–\n301, 2008.\n[10] E. G ´omez. Tonal description of polyphonic audio for\nmusic content processing. In Proceedings of the IN-\nFORMS Computing Society Conference , volume 18,\npages 294–304, Annapolis, MD, 2006.\n[11] F. Itakura and S. Saito. Analysis synthesis telephony\nbased on the maximum likelihood method. In Proceed-\nings of the International Congress on Acoustics , pages\n17–20, Tokyo, Japan, 1968.\n[12] S. Kullback and R.A. Leibler. On information and suf-\nﬁciency. Annals of Mathematical Statistics , 22(1):79–\n86, 1951.\n[13] C. Harte, M. Sandler, S. Abdallah, and E. Gomez.\nSymbolic representation of musical chords: A pro-\nposed syntax for text annotations. In Proceedings of\nthe International Symposium on Music Information Re-\ntrieval (ISMIR) , pages 66–71, London, UK, 2005.\n158"
    },
    {
        "title": "Music Genre Classification Using Locality Preserving Non-Negative Tensor Factorization and Sparse Representations.",
        "author": [
            "Yannis Panagakis",
            "Constantine Kotropoulos",
            "Gonzalo R. Arce"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416414",
        "url": "https://doi.org/10.5281/zenodo.1416414",
        "ee": "https://zenodo.org/records/1416414/files/PanagakisKA09.pdf",
        "abstract": "A robust music genre classification framework is proposed that combines the rich, psycho-physiologically grounded properties of auditory cortical representations of music recordings and the power of sparse representation-based classifiers. A novel multilinear subspace analysis method that incorporates the underlying geometrical structure of the cortical representations space into non-negative tensor factorization is proposed for dimensionality reduction compatible to the working principle of sparse representationbased classification. The proposed method is referred to as Locality Preserving Non-Negative Tensor Factorization (LPNTF). Dimensionality reduction is shown to play a crucial role within the classification framework under study. Music genre classification accuracy of 92.4% and 94.38% on the GTZAN and the ISMIR2004 Genre datasets is reported, respectively. Both accuracies outperform any accuracy ever reported for state of the art music genre classification algorithms applied to the aforementioned datasets.",
        "zenodo_id": 1416414,
        "dblp_key": "conf/ismir/PanagakisKA09",
        "keywords": [
            "music",
            "genre",
            "classification",
            "framework",
            "auditory",
            "cortical",
            "representations",
            "sparse",
            "representation-based",
            "classification"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMUSIC GENRE CLASSIFICATION USING LOCALITY PRESERVING\nNON-NEGATIVE TENSOR FACTORIZATION AND SPARSE\nREPRESENTATIONS\nYannis Panagakis∗Constantine Kotropoulos∗,†\n∗Dept. of Informatics\nAristotle University of Thessaloniki\nBox 451 Thessaloniki, GR-54124, Greece\n{panagakis,costas }@aiia.csd.auth.grGonzalo R. Arce†\n†Dept. of Electrical & Computer Engineering\nUniversity of Delaware\nNewark, DE 19716-3130, U.S.A.\narce@ece.udel.edu\nABSTRACT\nA robust music genre classiﬁcation framework is proposed\nthat combines the rich, psycho-physiologically grounded\nproperties of auditory cortical representations of music re-\ncordings and the power of sparse representation-based clas-\nsiﬁers. A novel multilinear subspace analysis method that\nincorporates the underlying geometrical structure of the\ncortical representations space into non-negative tensor fac-\ntorization is proposed for dimensionality reduction com-\npatible to the working principle of sparse representation-\nbased classiﬁcation. The proposed method is referred to\nasLocality Preserving Non-Negative Tensor Factorization\n(LPNTF). Dimensionality reduction is shown to play a cru-\ncial role within the classiﬁcation framework under study.\nMusic genre classiﬁcation accuracy of 92.4% and 94.38%\non the GTZAN and the ISMIR2004 Genre datasets is re-\nported, respectively. Both accuracies outperform any ac-\ncuracy ever reported for state of the art music genre classi-\nﬁcation algorithms applied to the aforementioned datasets.\n1. INTRODUCTION\nDespite the lack of a commonly agreed deﬁnition of music\ngenre due to genre dependence on cultural, artistic, or mar-\nket factors and the rather fuzzy boundaries between differ-\nent genres, music genre is probably the most popular de-\nscription of music content [1].\nPsycho-physiology indicates that the acoustic stimulus\nis encoded in the primary auditory cortex by its spectral\nand temporal characteristics. This is accomplished by cells\nwhose responses are selective to a range of spectral and\ntemporal resolutions resulting into a neural representation.\nIn particular, when the acoustic stimulus is either speech or\nmusic, its perceptual properties are encoded by slow spec-\ntral and temporal modulations [13, 18].\nThe appealing properties of slow spectro-temporal mod-\nulations from the human perceptual point of view and the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.strong theoretical foundations of sparse representations [4,\n6] have motivated us to propose a robust framework for\nautomatic music genre classiﬁcation here. To this end, the\nauditory model [17] is used in order to map a given music\nrecording to a three-dimensional (3D) representation of its\nslow spectral and temporal modulations with the same pa-\nrameters as in [15]. This 3D representation is referred to\nascortical representation and exploits the properties of the\nhuman auditory system [18]. The cortical representations\nform an overcomplete dictionary of basis signals for music\ngenres, which is exploited for sparse representation-based\nclassiﬁcation (SRC) as proposed in [19]. That is, ﬁrst each\nmusic recording is represented by its cortical representa-\ntion. Second, each cortical representation is modeled as\na sparse weighted sum of the basis elements (atoms) of\nan overcomplete dictionary, which stems from the corti-\ncal representations associated to training music recordings\nwhose genre is known. If sufﬁcient training music record-\nings are available for each genre, it is possible to express\nany test cortical representation as a compact linear combi-\nnation of the dictionary atoms of the genre, where it ac-\ntually belongs to. This representation is designed to be\nsparse, because it involves only a small fraction of the dic-\ntionary atoms and can be computed efﬁciently via /lscript1op-\ntimization. The classiﬁcation is performed by assigning\neach test recording to the class associated with the dictio-\nnary atoms, that are weighted by non-zero coefﬁcients.\nSince we would like to build an overcomplete dictio-\nnary extracted from training cortical representations, the\ndimensionality of dictionary atoms must be much smaller\nthan the cardinality of the training set. Such a dimension-\nality reduction facilitates the treatment of missing data,\nnoise, and outliers. Conventional linear subspace analy-\nsis methods, such as Principal Component Analysis, Lin-\near Discriminant Analysis, and Non-Negative Matrix Fac-\ntorization (NMF) deal only with vectorial data. By vec-\ntorizing a typical 3D cortical representation of 6scales,\n10rates, and 128frequency bands, a vector of dimensions\n7680×1results. Handling such high-dimensional patterns\nis computationally expensive not to mention that eigen-\nanalysis cannot be easily performed. Despite the imple-\nmentation issues, by reshaping a 3D cortical representa-\ntion into a vector the natural structure of the original data\n249Poster Session 2\nis destroyed. Thus, dimensionality reduction applied di-\nrectly to tensors rather than their vectorized versions is\ndesirable. Unsupervised multilinear dimensionality reduc-\ntion techniques, such as Non-Negative Tensor Factoriza-\ntion (NTF) [2] or Multilinear Principal Component Analy-\nsis (MPCA) [12] as well as supervised ones including Gen-\neral Tensor Discriminant Analysis (GTDA) [20] or Dis-\ncriminant Non-Negative Tensor Factorization (DNTF) [21]\ncould be considered. However, the just mentioned meth-\nods do not take into account the geometrical structure of\nthe original data space. To reduce tensor dimensions in a\nconsistent manner with the working principle of SRC, we\nshould guarantee that two data points, which are close in\nthe intrinsic geometry of the original data space are also\nclose in the new data space after multilinear dimension-\nality reduction. To this end, we propose a novel algo-\nrithm, where the geometrical information of the original\ndata space is incorporated into the objective function op-\ntimized by NTF. In particular, we encode the geometri-\ncal information by constructing a nearest neighbor graph.\nFurthermore, the non-negativity of cortical representations\nis preserved to maintain their physical interpretation. The\nproposed method is referred to as Locality Preserving Non-\nNegative Tensor Factorization (LPNTF). We derive a mul-\ntiplicative updating algorithm for LPNTF, which extracts\nfeatures from the cortical representations. For comparison\npurposes, NTF, MPCA, GTDA, DNTF, and random pro-\njections are also tested.\nNext, the features extracted by the aforementioned mul-\ntilinear dimensionality techniques are classiﬁed by SRC.\nPerformance comparisons are made against the SVMs em-\nploying a linear kernel. The reported genre classiﬁcation\naccuracies are juxtaposed against the best ones achieved by\nthe state of the art algorithms applied to the GTZAN and\nISMIR2004 Genre datasets. More speciﬁcally, two sets of\nexperiments are conducted. First, stratiﬁed ten-fold cross-\nvalidation is applied to the GTZAN dataset. The proposed\ngenre classiﬁcation method, that extracts features using the\nLPNTF, which are then classiﬁed by SRC (i.e. LPNTF plus\nSRC), yields an accuracy of 92.4%. Second, experiments\non the ISMIR2004Genre dataset are conducted by adher-\ning to the protocol employed during ISMIR2004 evalua-\ntion tests. This protocol splits the dataset into two equal\ndisjoint subsets with the ﬁrst one being used for training\nand the second one being used for testing. Features ex-\ntracted by NTF, which are then classiﬁed by SRC, yield an\naccuracy of 94.38%. An accuracy of 94.25% was achieved\nwhen the LPNTF plus SRC framework is employed. To\nthe best of the authors’ knowledge, the just quoted genre\nclassiﬁcation accuracies are the highest ever reported for\nboth datasets .\nThe paper is organized as follows. In Section 2, ba-\nsic multilinear algebra concepts and notations are deﬁned.\nThe LPNTF is introduced in Section 3. The SRC frame-\nwork, that is applied to music genre classiﬁcation, is de-\ntailed in Section 4. Experimental results are demonstrated\nin Section 5 and conclusions are drawn in Section 6.2. NOTATION AND MULTILINEAR ALGEBRA\nBASICS\nTensors are considered as the multidimensional equivalent\nof matrices (i.e., second-order tensors) and vectors (i.e.,\nﬁrst-order tensors) [9]. Throughout this paper, tensors are\ndenoted by boldface Euler script calligraphic letters (e.g.\nX,A), matrices are denoted by uppercase boldface letters\n(e.g. U), and vectors are denoted by lowercase boldface\nletters (e.g. u).\nA high-order real valued tensor Xof order Nis deﬁned\nover the tensor space RI1×I2×...×IN, where Ii∈Zand\ni= 1,2, . . . , N . Each element of tensor Xis addressed by\nNindices, i.e. xi1i2i3...iN. Mode- nunfolding of tensor\nXyields the matrix X(n)∈RIn×(I1...In−1In+1...IN). In\nthe following, the operations on tensors are expressed in\nmatricized form [9].\nAnN-order tensor Xhas rank 1, when it is decomposed\nas the outer product of Nvectors u(1),u(2), . . . , u(N), i.e.\nX=u(1)◦u(2)◦. . .◦u(N). That is, each element of the\ntensor is the product of the corresponding vector elements,\nxi1i2...iN=u(1)\ni1u(2)\ni2. . . u(N)\niNfor all in= 1,2, . . . , I n.\nThe rank of an arbitrary N-order tensor Xis the mini-\nmal number of rank- 1tensors that yield Xwhen linearly\ncombined. Next, several products between matrices will\nbe used, such as the Kronecker product denoted by ⊗, the\nKhatri-Rao product denoted by ⊙, and the Hadamard prod-\nuct denoted by ∗, whose deﬁnitions can be found in [9] for\nexample.\n3. LOCALITY PRESERVING NON NEGATIVE\nTENSOR FACTORIZATION\nLet{Xq|Q\nq=1}be a set of Qnon-negative tensors Xq∈RI1\n+\n×I2×...×INof order N. Let us also assume that these Q\ntensors lie in a nonlinear manifold Aembedded into the\ntensor spaceRI1×I2×...×IN\n+ . Accordingly, we can repre-\nsent such a set by a (N+ 1) -order tensor A∈RI1×I2×...\n+\n×IN×IN+1withIN+1=Q. Conventional NTF operates in\nthe Euclidean space and does not consider the intrinsic ge-\nometrical structure of the data manifold [2]. To overcome\nthe just mentioned limitation of NTF, we propose LPNTF\nby incorporating a geometrically-based regularizer stem-\nming from locality preserving projections [7] into the op-\ntimization problem to be solved.\nGiven {Xq|Q\nq=1}, one can model the local structure of\nAby constructing the nearest neighbor graph G. By ex-\nploiting the heat kernel function [7], one can deﬁne the ele-\nments of the weight matrix SofGassqp=e−||Xq−Xp||2\nτ if\nXqandXpbelong to the same class and 0 otherwise, where\n|| ||2denotes the tensor norm [9]. Accordingly, the Lapla-\ncian matrix is deﬁned as L=Γ−S, where Γis a diagonal\nmatrix with elements γqq=/summationtext\npsqp, i.e. the column sums\nofS. LetZ(i)=U(N+1)⊙. . .⊙U(i+1)⊙U(i−1)⊙. . .⊙\nU(1). Since the Laplacian matrix is analogous to Laplace-\nBeltrami operator on compact Riemannian manifolds [7],\none can incorporate the local geometry of Ainto NTF by\nconstructing the following objective function for LPNTF\n25010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nin matrix form:\nfLPNTF/parenleftbig\nU(i)|N+1\ni=1/parenrightbig\n=||A(i)−U(i)/bracketleftbig\nZ(i)/bracketrightbigT||2\n+λtr/braceleftBig/bracketleftbig\nU(N+1)/bracketrightbigTL U(N+1)/bracerightBig\n,(1)\nwhere λ > 0is a parameter, which controls the trade off\nbetween goodness of ﬁt to the data tensor Aand locality\npreservation. Consequently, we propose to minimize (1)\nsubject to the non-negativity constraint on factor matrices\nU(i)∈RIi×k\n+,i= 1,2, . . . N + 1, where kis the desirable\nnumber of rank- 1tensors approximating Awhen linearly\ncombined.\nLet∇U(i)fLPNTF =∂fLPNTF\n∂U(i)be the partial derivative\nof the objective function fLPNTF (U(i)|N+1\ni=1)with respect\ntoU(i). Since U(i),i= 1,2, . . . , N + 1,Γ, andSare\nnon-negative, the partial derivatives of the objective func-\ntion can be decomposed as differences of two non-negative\ncomponents denoted by ∇+\nU(i)fLPNTF and∇−\nU(i)fLPNTF ,\nrespectively. It can be shown that for i= 1,2, . . . , N we\nhave\n∇U(i)fLPNTF =U(i)/bracketleftbig\nZ(i)/bracketrightbigTZ(i)\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright\n∇+\nU(i)fLPNTF−A(i)Z(i)\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\n∇−\nU(i)fLPNTF,(2)\nwhile for i=N+1by invoking the deﬁnition of the Lapla-\ncian we obtain\n∇U(N+1)fLPNTF =\nU(N+1)/bracketleftbig\nZ(N+1)/bracketrightbigTZ(N+1)+λΓU(N+1)\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n∇+\nU(N+1)fLPNTF\n−/parenleftbig\nA(N+1)Z(N+1)+λS U(N+1)/parenrightbig\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\n∇−\nU(N+1)fLPNTF. (3)\nFollowing the strategy employed in the derivation of NMF\n[10], we can obtain an iterative alternating algorithm for\nLPNTF as follows. Given N+1randomly initialized non-\nnegative matrices U(i)|N+1\ni=1∈RIi×k\n+, a local minimum\nof the optimization problem (1) subject to non-negativity\nconstraints can be found by the multiplicative update rule:\nU(i)\n[t+1]=U(i)\n[t]∗∇−\nU(i)\n[t]fLPNTF\n∇+\nU(i)\n[t]fLPNTF, (4)\nwhere the division in (4) is elementwise and tdenotes the\niteration index. The multiplicative update rule (4) suffers\nfrom two drawbacks: (1) The denominator may be zero;\n(2)U(i)\n[t+1]does not change when U(i)\n[t]=0and∇U(i)[t]\nfLPNTF <0. In order to overcome these drawbacks, we\ncan modify (4) as in [11]. A robust multiplicative update\nrule for LPNTF is then\nU(i)\n[t+1]=U(i)\n[t]−¯U(i)\n[t]\n∇+\nU(i)\n[t]fLPNTF +δ∗ ∇U(i)\n[t]fLPNTF ,\n(5)\nwhere ¯U(i)\n[t]=U(i)\n[t]if∇U(i)\n[t]fLPNTF ≥0andσother-\nwise. The paremeters σ,δare predeﬁned small positive\nnumbers, typically 10−8[11].4. SPARSE REPRESENTATION-BASED\nCLASSIFICATION\nFor each music recording a 3D cortical representation is\nextracted by employing the computational auditory model\nof Wang et al. [17] with the same parameters as in [15].\nThus, each ensemble of recordings is represented by a 4th-\norder data tensor, which is created by stacking the 3rd-\norder feature tensors associated to the recordings. Con-\nsequently, the data tensor A∈RI1×I2×I3×I4\n+ , where I1=\nIscales = 6,I2=Irates = 10 ,I3=Ifrequencies = 128 ,\nandI4=Isamples is obtained.\nDetermining the class label of a test cortical representa-\ntion, given a number of labeled training cortical represen-\ntations from Nmusic genres is addressed based on SRC\n[19]. Let us denote by Ai= [ai1|ai2|. . .|aini]∈R7680×ni\n+\nthe dictionary that contains nicortical representations stem-\nming from the ith genre as column vectors (i.e., atoms).\nGiven a test cortical representation y∈R7680\n+that belongs\nto the ith class, we can assume that yis expressed as a lin-\near combination of the atoms that belong to the ith class,\ni.e.\ny=ni/summationdisplay\nj=1aijcij=Aici, (6)\nwhere cij∈Rare coefﬁcients, which form the coefﬁcient\nvector ci= [ci1, ci2, . . . , c ini]T. Let us, now, deﬁne the\nmatrix D= [A1|A2|. . .|AN] =AT\n(4)∈R7680×Isamples\n+\nby concatenating Isamples cortical representations, which\nare distributed across Ngenres. Accordingly, a test cor-\ntical representation ythat belongs to the ith genre can be\nequivalently expressed as\ny=D c, (7)\nwhere c= [0T|. . .|0T|cT\ni|0T|. . .|0T]Tis the augmented\ncoefﬁcient vector whose elements are zero except those as-\nsociated with the ith genre.\nSince the genre label of any test cortical representation\nis unknown, we can predict it by seeking the sparsest solu-\ntion to the linear system of equations (7). Let ||.||0be the\n/lscript0quasi-norm of a vector, which returns the number of its\nnon-zero elements. Formally, given the matrix Dand the\ntest cortical representation y, sparse representation aims to\nﬁnd the coefﬁcient vector csuch that (7) holds and ||c||0is\nminimum, i.e.\nc⋆= arg min\nc||c||0subject to Dc=y. (8)\n(8) is NP-hard due to the underlying combinational opti-\nmization. An approximate solution to (8) can be obtained\nby replacing the /lscript0norm with the /lscript1norm, i.e.\nc⋆= arg min\nc||c||1subject to D c=y, (9)\nwhere ||.||1denotes the /lscript1norm of a vector. The optimiza-\ntion problem (9) can be solved by standard linear program-\nming methods in polynomial time [5].\nSince we are interested in creating overcomplete dictio-\nnaries derived from the cortical representations, the dimen-\nsionality of atoms must be much smaller than the training\n251Poster Session 2\nset cardinality. Thus, we can reformulate the optimization\nproblem in (9) as follows:\nc⋆= arg min\nc||c||1subject to W D c =Wy,(10)\nwhere W∈Rk×7680withk/lessmuchmin(7680 , Isamples )is\na projection matrix. The projection matrix Wcan be ob-\ntained by LPNTF or any other multilinear dimensionality\nreduction technique, such as NTF [2], MPCA [12], GTDA\n[20], or DNTF [21]. Alternatively, one can even employ\na random projection matrix whose elements are indepen-\ndently sampled from a zero-mean normal distribution, and\neach column is normalized to unit length as proposed in\n[19]. More particularly, when LPNTF, NTF, or DNTF is\napplied to the data tensor A, four factor matrices U(i)∈\nRIi×k\n+,i= 1,2,3,4, are obtained, which are associated\nto scale, rate, frequency, and sample modes respectively.\nThe projection matrix Wis given by either W= (U(3)⊙\nU(2)⊙U(1))TorW= (U(3)⊙U(2)⊙U(1))†, where (.)†\ndenotes the Moore-Penrose pseudoinverse. Accordingly,\nevery column of D(i.e. vectorized cortical representation\nof a music recording) is a linear combination of the ba-\nsis vectors, which span the columns of the basis matrix\nWTwith coefﬁcients taken from the columns of matrix\n[U(4)]T. That is, D=AT\n(4)=WT[U(4)]T. For MPCA or\nGTDA, three factor matrices U(i)∈RIi×Ji, with Ji< Ii,\ni= 1,2,3, are obtained, which are associated to scales,\nrates, and frequencies, respectively. The columns of Dare\nobtained by applying the projection matrix W= (U(3)⊗\nU(2)⊗U(1))TorW= (U(3)⊗U(2)⊗U(1))†to vec-\ntorized training tensors vec(Xq). The dimensionality re-\nduction of the original cortical representations data space\nhas two beneﬁts: (1) It reduces the computational cost of\nlinear programming solvers for (9) [5]; (2) It facilitates the\ncreation of a redundant dictionary out of training cortical\nrepresentations.\nA test cortical representation can be classiﬁed as fol-\nlows. First, yis projected onto the reduced dimensionality\nspace through the projection matrix Wasˆy=Wy. Then,\nthe following optimization problem is solved\nc⋆= arg min\nc||c||1subject to W D c =ˆy. (11)\nIdeally, the coefﬁcient vector c⋆contains non-zero entries\nin positions associated with the columns of WD associ-\nated with a single genre, so that we can easily assign the\ntest auditory representation yto that genre. However, due\nto modeling errors, there are small non-zero elements in\nc⋆that are associated to multiple genres. To cope with\nthis problem, each auditory modulation representation is\nclassiﬁed to the genre that minimizes the /lscript2norm residual\nbetween ˆyand˘y=W D ϑi(c), where ϑi(c)∈Rnis a\nnew vector whose non-zero entries are only the elements\nincthat are associated to the ith genre [19].\n5. EXPERIMENTAL EV ALUATION\nIn order to assess both the discriminating power of the fea-\ntures derived by LPNTF applied to cortical representationsfor dimensionality reduction and the accuracy of sparse\nrepresentation-based classiﬁcation, experiments are con-\nducted on two widely used datasets for music genre clas-\nsiﬁcation [3, 8, 14, 16]. The ﬁrst dataset, abbreviated as\nGTZAN, was collected by G. Tzanetakis [16] and consists\nof 10 genre classes. Each genre class contains 100 audio\nrecordings 30 sec long. The second dataset, abbreviated\nas ISMIR2004 Genre, comes from the ISMIR 2004 Genre\nclassiﬁcation contest and contains 1458 full audio record-\nings distributed across 6 genre classes. All the recordings\nwere converted to monaural wave format at a sampling fre-\nquency of 16kHz and quantized with 16 bits. Moreover,\nthe music signals have been normalized, so that they have\nzero mean amplitude with unit variance in order to remove\nany factors related to the recording conditions. Since the\nISMIR2004 Genre dataset consists of full length tracks, we\nextracted a segment of 30 sec just after the ﬁrst 30 sec of\na recording in order to exclude any introductory parts that\nmay not be directly related to the music genre the recording\nbelongs to. The cortical representation is extracted for the\naforementioned segment of 30sec duration for any record-\ning from both datasets. The best reported music genre\nclassiﬁcation accuracies obtained for the aforementioned\ndatasets are summarized in Table 1.\nReference Dataset Accuracy\nBergstra et al. [3] GTZAN 82.5%\nHolzapfel et al. [8] ISMIR2004 83.5%\nPampalk et al. [14] ISMIR2004 82.3%\nTable 1 . Best classiﬁcation accuracies achieved by music\ngenre classiﬁcation approaches on standard datasets.\nFollowing the experimental set-up used in [3, 15, 16],\nstratiﬁed 10-fold cross-validation is employed for experi-\nments conducted on the GTZAN dataset. Thus each train-\ning set consists of 900audio ﬁles. Accordingly, the train-\ning tensor AGTZAN ∈R6×10×128×900\n+ is constructed by\nstacking the cortical representations. The experiments on\nISMIR 2004 Genre dataset were conducted according to\nthe ISMIR2004 Audio Description Contest protocol. The\nprotocol deﬁnes training and evaluation sets, which consist\nof729audio ﬁles each. Thus the corresponding training\ntensorAISMIR ∈R6×10×128×729\n+ is constructed.\nThe projection matrix Wis derived from each training\ntensor AGTZAN andAISMIR by employing either LP-\nNTF, NTF, DNTF, MPCA or GTDA. Throughout the ex-\nperiments the value of λin LPNTF was empirically set\nto0.5, while the parameter τof the heat kernel was set\nequal to 1. In order to determine automatically the param-\netersλandτone can apply cross-validation to the training\nset. However, the systematic setting of these parameters\ncould be a subject of future research. In order to deter-\nmine the dimensionality of factor matrices, the ratio of the\nsum of eigenvalues retained over the sum of all eigenvalues\nfor each mode- ntensor unfolding is employed as in [12].\nBy using this ratio as a speciﬁcation parameter, the num-\nber of retained principal components for each mode (e.g.\nscale, rate, and frequency) was determined, as is demon-\n25210th International Society for Music Information Retrieval Conference (ISMIR 2009)\n78808284868890929423456789101112\nPortion of total scatter retained (%)Number of principal components\n  \nRate subspace\nScale subspace\nFrequency subspace\n78808284868890929423456789101112\nPortion of total scatter retained (%)Number of principal components\n  \nRate subspace\nScale subspace\nFrequency subspace(a) (b)\n788082848688909294406080100120140160180200220\nPortion of total scatter retained (%)Feature dimension\n78808284868890929420406080100120140160180200220\nPortion of total scatter retained (%)Feature dimension\n(c) (d)\nFigure 1 . Total number of retained principal components in each mode (e.g. rate, scale, and frequency) as a function of\nthe portion of total scatter retained for the: a) GTZAN dataset and b) ISMIR 2004 Genre dataset. Feature dimension as a\nfunction of the portion of the total scatter retained for the: c) GTZAN dataset and d) ISMIR 2004 Genre dataset.\nstrated in Figure 1 for the GTZAN and the ISMIR Genre\n2004 datasets. The different subspace analysis methods are\ncompared for equal dimensionality reduction. That is, the\nsame J1=Jscales ,J2=Jrates andJ3=Jfrequencies\nwere used in MPCA and GTDA, while k=J1J2J3for\nLPNTF, NTF, and DNTF. The same value of parameter k\nis used in order to construct the random projection ma-\ntrix. Since the low dimensional features obtained by the\naforementioned multilinear dimensionality reduction algo-\nrithms are linearly combined for classiﬁcation, SVMs with\nlinear kernel are tested as alternatives to SRC.\nIn Figure 2, the classiﬁcation accuracy achieved by the\nthree different classiﬁers is plotted as a function of the por-\ntion of the total scatter retained, when various subspace\nanalysis methods are applied to both GTZAN and ISMIR\n2004 Genre datasets. On the GTZAN dataset the best clas-\nsiﬁcation accuracy (92.4%) was obtained when LPNTF ex-\ntracts features, that are classiﬁed by SRC. In this case,\nk= 135 , as shown in Figure 1(c). The standard devia-\ntion of the classiﬁcation accuracy was estimated thanks to\n10-fold cross-validation. At the best classiﬁcation accu-\nracy, its standard deviation was found to be 2%. The re-\nported classiﬁcation accuracy outperforms those listed in\nTable 1. The interval ±one standard deviation is overlaid\nin all plots for the various values of the portion of the total\nscatter retained.\nOn the ISMIR 2004 Genre dataset the best classiﬁcation\naccuracy (94.38%) was obtained, when the NTF with k=\n135extracts the low dimensional features that are classiﬁed\nby SRC next. When the LPNTF with k= 105 extracts\nfeatures that are classiﬁed by SRC next, the classiﬁcation\naccuracy is found equal to 94.25%, that is very close to the\nbest accuracy. Both accuracies outperform the previously\nreported ones, which are listed in Table 1.It is seen that the classiﬁcation accuracy obtained by\nLPNTF and SRC outperforms the accuracy obtained with\nfeatures extracted by all other multilinear subspace analy-\nsis techniques, which are next classiﬁed by either SRC or\nlinear SVMs, for all the values of the portion of the total\nscatter retained but one. Moreover, the classiﬁcation ac-\ncuracy obtained with features extracted by LPNTF, NTF,\nMPCA or GTDA that are subsequently classiﬁed by SRC\nexceeds 80% for both datasets despite the reduced dimen-\nsions of the feature space extracted that are plotted in Fig-\nure 1(c) and (d). The experimental results reported in this\npaper indicate that the dimensionality reduction is crucial,\nwhen SRC is applied to music genre classiﬁcation. This\nwas not the case for face recognition [19].\n6. CONCLUSIONS\nIn this paper, a robust music genre classiﬁcation frame-\nwork has been proposed. This framework resorts to corti-\ncal representations for music representation, while sparse\nrepresentation-based classiﬁcation has been employed for\ngenre classiﬁcation. A multilinear subspace analysis tech-\nnique (i.e. LPNTF) has been developed, which incorpo-\nrates the underlying geometrical structure of the cortical\nrepresentations with respect to the music genre into the\nNTF. The crucial role of feature extraction and dimension-\nality reduction for music genre classiﬁcation has been dem-\nonstrated. The best classiﬁcation accuracies reported in\nthis paper outperform any accuracy ever obtained by state\nof the art music genre classiﬁcation algorithms applied to\nboth GTZAN and ISMIR2004 Genre datasets.\nIn many real applications, both commercial and private,\nthe number of available audio recordings per genre is lim-\nited. Thus, it is desirable that the music genre classiﬁca-\ntion algorithm performs well for such small sets. Future\n253Poster Session 2\n78808284868890929430405060708090100\nPortion of total scatter retained (%)Classification Accuracy (%)\n  \nLPNTF\nNTF\nDNTF\nMPCA\nGTDA\nRandom\n78808284868890929430405060708090100\nPortion of total scatter retained (%)Classification Accuracy (%)\n  \nLPNTF\nNTF\nDNTF\nMPCA\nGTDA\nRandom(a) (b)\n78808284868890929430405060708090100\nPortion of total scatter retained (%)Classification Accuracy (%)\n  \nLPNTF\nNTF\nDNTF\nMPCA\nGTDA\nRandom\n78808284868890929430405060708090100\nPortion of total scatter retained(%)Classification Accuracy (%)\n  \nLPNTF\nNTF\nDNTF\nMPCA\nGTDA\nRandom\n(c) (d)\nFigure 2 . Classiﬁcation accuracy for various feature extraction methods and classiﬁers. (a) SRC on GTZAN dataset; (b)\nSRC on ISMIR2004 Genre dataset; (c) Linear SVM on GTZAN dataset; (d) Linear SVM on ISMIR2004 Genre dataset.\nresearch will address the performance of SRC framework\nunder such conditions.\n7. REFERENCES\n[1]J. J. Aucouturier and F. Pachet: “Representing Musical Genre: A\nState of the Art,” Journal of New Music Research , pp. 83–93, 2003.\n[2]E. Benetos and C. Kotropoulos: “A Tensor-Based Approach for Au-\ntomatic Music Genre Classiﬁcation,” Proceedings of the European\nSignal Processing Conference , Lausanne, Switzerland, 2008.\n[3]J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B. Kegl: “Ag-\ngregate Features and AdaBoost for Music Classiﬁcation,” Machine\nLearning , V ol. 65, No. 2-3, pp. 473–484, 2006.\n[4]E. J. Cand `es, J. Romberg, and T. Tao: “Robust Uncertainty Prin-\nciples: Exact Signal Reconstruction From Highly Incomplete Fre-\nquency Information,” IEEE Transactions on Information Theory ,\nV ol. 52, No. 2, pp. 489–509, 2006.\n[5]S. S. Chen, D. L. Donoho, and M. A. Saunders: “Atomic Decom-\nposition by Basis Pursuit,” SIAM Journal Scientiﬁc Computing , V ol.\n20, No.1 pp. 33–61, 1998.\n[6]D. L. Donoho, “Compressed sensing,” IEEE Transactions on Infor-\nmation Theory , V ol. 52, No. 4, pp. 1289–1306, 2006.\n[7]X. He and P. Niyogi: “Locality Preserving Projections,” Advances in\nNeural Information Processing Systems , V ol. 16, MIT Press, 2004.\n[8]A. Holzapfel and Y . Stylianou: “Musical Genre Classiﬁcation Using\nNonnegative Matrix Factorization-Based Features,” IEEE Transac-\ntions on Audio, Speech, and Language Processing , V ol. 16, No. 2,\npp. 424–434, 2008.\n[9]T. Kolda and B. W. Bader: “Tensor Decompositions and Applica-\ntions,” SIAM Review , V ol. 51, No. 3, to appear.\n[10] D. D. Lee and H. S. Seung: “Algorithms for Non-negative Matrix\nFactorization,” Advances in Neural Information Processing Systems ,\nV ol. 13, pp. 556–562, 2001.\n[11] C. -J. Lin: “On the Convergence of Multiplicative Update Algo-\nrithms for Nonnegative Matrix Factorization,” IEEE Transactions on\nNeural Networks , V ol. 18, No. 6, pp. 1589–1596, 2007.[12] H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos: “MPCA:\nMultilinear Principal Component Analysis of Tensor Objects,” IEEE\nTransactions on Neural Networks , V ol. 19, No. 1, pp 18–39, 2008.\n[13] N. Mesgarani, M. Slaney, and S. A. Shamma: “Discrimination\nof Speech from Nonspeech Based on Multiscale Spectro-temporal\nModulations,” IEEE Transactions on Audio, Speech, and Language\nProcessing , V ol. 14, No. 3, pp. 920–930, 2006.\n[14] E. Pampalk, A. Flexer, and G. Widmer: “Improvements of Audio-\nbased Music Similarity and Genre Classiﬁcation,” Proceedings of\nthe Sixth International Symposium on Music Information Retrieval ,\nLondon, UK, 2005.\n[15] I. Panagakis, E. Benetos, and C. Kotropoulos: “Music Genre Classi-\nﬁcation: A Multilinear Approach,” Proceedings of the Seventh Inter-\nnational Symposium on Music Information Retrieval , Philadelphia,\nUSA, 2008.\n[16] G. Tzanetakis and P. Cook: “Musical Genre Classiﬁcation of Audio\nSignal,” IEEE Transactions on Speech and Audio Processing , V ol.\n10, No. 3, pp. 293–302, 2002.\n[17] K. Wang and S. A. Shamma: “Spectral Shape Analysis in the Central\nAuditory System,” IEEE Transactions on Speech and Audio Process-\ning, V ol. 3, No. 5, pp. 382–396, 1995.\n[18] S. Woolley, T. Fremouw, A. Hsu, and F. Theunissen: “Tuning for\nSpectro-temporal Modulations as a Mechanism for Auditory Dis-\ncrimination of Natural Sounds,” Nature Neuroscience , V ol. 8, pp.\n1371–1379, 2005.\n[19] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y . Ma “Robust Face\nRecognition via Sparse Representation,” IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence , V ol. 31, No. 2, pp. 210–227,\n2009.\n[20] D. Tao, X. Li, X. Wu, and S. J. Maybank: “General Tensor Dis-\ncriminant Analysis and Gabor Features for Gait Recognition,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence , V ol. 29,\nNo. 10, pp. 1700–1715, 2007.\n[21] S. Zafeiriou: “Discriminant Nonnegative Tensor Factorization Algo-\nrithms,” IEEE Transactions on Neural Networks , V ol. 20, No. 2, pp.\n217–235, 2009.\n254"
    },
    {
        "title": "Easy Does It: The Electro-Acoustic Music Analysis Toolbox.",
        "author": [
            "Tae Hong Park",
            "Zhiye Li",
            "Wen Wu"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416674",
        "url": "https://doi.org/10.5281/zenodo.1416674",
        "ee": "https://zenodo.org/records/1416674/files/ParkLW09.pdf",
        "abstract": "In this paper we present the EASY (Electro-Acoustic muSic analYsis) Toolbox software system for assisting electro-acoustic music analysis. The primary aims of the system are to present perceptually relevant features and audio descriptors via visual designs to gain more insight into electro-acoustic music works and provide easy-touse “click-and-go” software interface paradigms for practical use of the system by non-experts and experts alike. The development of the EASY system exploits MIR techniques with particular emphasis on the electroacoustic music repertoire – musical pieces that concentrate on timbral dimensions rather than traditional elements such as pitch, melody, harmony, and rhythm. The project was mainly inspired by the lack of software tools available for aiding electro-acoustic music analysis. The system’s frameworks, feature analysis algorithms, along with the initial analyses of pieces are presented here.",
        "zenodo_id": 1416674,
        "dblp_key": "conf/ismir/ParkLW09",
        "keywords": [
            "Electro-Acoustic music analysis",
            "Perceptually relevant features",
            "Audio descriptors",
            "Visual designs",
            "Non-experts",
            "Experts",
            "MIR techniques",
            "Timbral dimensions",
            "Software system",
            "Click-and-go interface"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nEASY DOES IT:  \nTHE ELECTRO -ACOUSTIC  MUSIC ANAL YSIS TOOLBOX  \nTae Hong Park Zhiye Li Wen Wu \nTulane University \npark@tulane.edu  Tulane University \nzli3@tulane.edu  Tulane University \nwwu1@tulane.edu  \nABSTRACT \nIn this paper we present the EASY (Electro-Acoustic \nmuSic analYsis) Toolbox software system for assisting electro-acoustic music analysis . The primary aims of the \nsystem are to present perceptu ally relevant features and \naudio descriptors via visual designs to gain more insight into electro-acoustic music works and provide easy-to-use “click-and-go” software interface paradigms for prac-\ntical use of the system by non-experts and experts alike. The development of the EASY system exploits MIR techniques with particular emphasis on the electro-acoustic music repertoire – musical pieces that concen-\ntrate on timbral dimensions rather than traditional ele-ments such as pitch, melody, harmony, and rhythm. The project was mainly inspired by the lack of software tools available for aiding electro- acoustic music analysis. The \nsystem’s frameworks, feature analysis algorithms, along with the initial analyses of  pieces are presented here. \n1. INTRODUCTION \nThe idea for EASY Toolbox originated between 1999-2000 in the form of a master’s thesis entitled “Salient Feature Extraction of Musical Instrument Signals” [11] which included a Java-based feature extraction and visu-alization software called Ja fep (Java Feature Extraction \nProgram). Since then, the pr oject has somewhat been \ndormant, at least in direct relation to its original intention. Portions of the research evolved to automatic instrument recognition studies and further lead to the FMS software synthesis system [10] and most recently has developed \ninto the EASY Toolbox to assist in the analysis of elec-tro-acoustic music.  \nThere is much interest and on-going research in MIR \non various dimensions of music and a wealth of research can be found in pertinence to traditional music especially \npopular music. Some examples include rhythm analysis, melody analysis, tonality, traditional harmony, music rec-ommendation, genre classification, instrument identifica-tion, and composer identification to name a few [2,16]. As far as MIR techniques and its applications in the area of music are concerned, much  of the focus seems to be \noutside  the realm of electro-ac oustic music. One of the \nreasons for the scarcity in MIR-based research for elec-\ntro-acoustic music may perhaps be attributed to the need \nfor MIR researchers to be interested and actively be in-\nvolved in composing or be deeply engaged in electro-acoustic music on a musical level. Another reason for this somewhat imbalance may be th at the community seems to \nprioritize resources to the more standard musical reper-toire that the general public accesses.  \nSome works related to the topic of music analysis soft-\nware include Jafep (Java Feature Extraction Program), jAudio, Wavesurfer, Vivo, JRing, SoniXplorer, Sonic Visualizer [1, 3, 4, 6, 8, 11, 14], and others [17, 18]. Jafep is a Java-based feature extr action system for displaying \nfeature vectors in a two-dimensional canvas and includes a harmonic follower designed mainly for analysis of mu-sical instruments which is similar to jAudio. However, jAudio further concentrates on providing a feature extrac-tion library/repository. Wavesu rfer is a system for speech \nresearch and displays waveforms, pitch information, spectrograms, and formants. Vivo and JRing focus on pitch-based music, where JRing additionally deals with incorporating traditional scores for musicological studies. SoniXplorer is an interesting application which again primarily pays attention to traditional and popular music using self-organizing clustering algorithms. The Sonic Visualizer seems to be designed to address traditional music also, that is, pieces involving pitch,  harmony, and \nrhythm. Although it has the ability to display audio fea-tures, perhaps due to the original design of the software architecture, when analyzing electro-acoustic music type \nsignals, the visualization environment does not seem to be ideally suited for such situations. Marsyas (and to a lesser degree MIR Toolbox) is probably the most exten-sive environment for MIR research. It seems especially well suited for “DSP experts” and for the more experi-enced software developers/res earchers but is perhaps not \nideal for “users” who are looking for out-of-the-box software applications with simple and intuitive GUI inter-faces as well as viewing cap abilities – ready to use appli-\ncations for specific purposes.  \nThe EASY Toolbox is a modest initial step towards \napplying MIR theories with particular emphasis on elec-tro-acoustic music focusing on its potential in gaining musical insights based on salient feature extraction tech-niques and clustering with the primary objective being that of an analytical tool wrapped with an intuitive GUI environment. \n2. THE EASY TOOLBOX \n2.1 Core Concept \nOne of the important charact eristics of numerous electro-\nacoustic music, especially those pieces that are in the tape \n693Poster Session 4\n  \n \nmusic genre, is that they ar e often concerned with aspects \nof timbre and sound color opposed to traditional musical \nelements such as pitch, harmony, and rhythm. However, although there are examples of software systems for ana-lyzing “pitch-based music” as discussed in the introduc-tion, there does not seem to be much of any software that is available for the analysis of music that do not adhere to those time-honored musical parameters. There is much software available for view ing raw waveforms and spec-\ntrograms but that type of information does not really of-fer too much insight by itself. Hence, our approach is to utilize salient feature extraction techniques as the basis for music analysis to uncover hidden information that is timbrally and perceptually relevant and perhaps even helpful in revealing additional data about a given work. We have also included segmentation/clustering algo-rithms using model-based and distance-based techniques. The algorithms that are implemented and used for dis-playing various features are hidden from the user as much as possible in order to render an easy-to-use inter-face. Furthermore, we have attempted to present the fea-ture vectors in intuitive ways by plotting data in the time/frequency-domain and timbre spaces using 3D re-presentation/navigation techniques. With a straightfor-ward “click-and-go” environment provided by EASY, we hope that users will be encouraged to explore various timbral dimensions thereby help better understand sound objects and music.  \n2.2 EASY Features \n2.2.1 The  EASY Interface \nThe two main canvases in EASY are time-domain and \nfrequency-domain displays as shown in Figure 1. \nFigure 1 . Screenshot of EASY \nThe approach of designing the EASY interface was dri-\nven by the aim of providing the user a 3D visualization environment for sonic exploration and interaction. For example, the waveforms for stereo files or multichannel files are presented in a cascading style along with the cor-responding spectrogram.   \nThe control areas of EASY include time/frequency-\ndomain parametric control and feature selection for anal-ysis/display. Standard functionalities such as zoom-in, zoom-out, 3D navigation/rotation, viewing options inhe-rited from MATLAB\n®, the real-time input DAQ option \n(see Section 2.2.3), and a transport control are also in-cluded. Further controls are available for clustering and segmentation such as featur e selection for clustering, \nnumber of clusters, and clustering algorithms as further discussed in Section 3. \n2.2.2 EASY 3D Timbre Space Plots: the timbregram \nEASY provides intuitive 3D timbre space representations \nadopted from [7] for sonic exploration which we call \ntimbregrams. Figure 2 shows a timbregram example of a time-sequenced three instrument signal – bass guitar fol-lowed by clarinet and French  horn with three timbre di-\nmensions (spectral spread, sp ectral centroid, and spectral \nflux).  \n \nFigure 2 . Timbre Space Example \nThe dots and dashed lines portray the 3D timbral tra-\njectory as a function of time where the right pointing tri-angle refers to the beginning of the sample and the left pointing triangle the end of the sample. Each node represents a time unit equal to the frame/hop size. During audio playback, feature vector following occurs not only in the time-domain and frequency-domain canvases but also in the timbregram canvas itself (displayed in a sepa-\nrate window as shown in Fig. 1). This allows intuitive observation of sonic events via synchronization between the visuals and the audio that is played back. \n2.2.3 “Real-Time” and MATLAB\n® Data Acquisition \nToolbox \nOne of the advantages in using MATLAB® is the incred-\nible resource of toolboxes available for data analysis and \nmanipulation. One such example is the Data Acquisition (DAQ) Toolbox used for real-time analysis applications. The EASY system exploits the DAQ for analyzing and displaying input signals (mic/lin e input) in “real-time.” It \ncan display one or multiple features (selectable by the user) in the time and frequency-domain as well as the timbregram canvas.  \n69410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \n2.3 EASY Algorithms \nA total of 26 features in the time and frequency-domain \nare implemented in this current version of EASY – am-plitude envelope, amplitude modulation, attack time, crest factor, dynamic tightness, frequency modulation, low en-ergy ratio, noise content, pitch, release time, sound field vector, temporal centroid, zero-crossing rate, 3D spectral envelope, critical band, harmonic compression, harmonic expansion, inharmonicity, MFCC, modality/harmonicity/ noisiness, spectral centroid, spectral flux, spectral jitter, spectral roll-off, spectral shimmer, and spectral smooth-ness. Many of the feature extraction algorithms them-\nselves were developed in the FMS Toolbox [10, 12] and have been customized for use in EASY. Below, we pre-sent a short description of a select number of new fea-tures that we developed. \nThe dynamic tightness feature measures the quantized \ntime-amplitude histogram on a frame-by-frame basis and provides insights into the “tightness” or “holiness” of the distribution of quantized sample values. This idea is shown in Figure 3 showing a highly compressed electric bass slide sample displaying a densely populated bed of samples throughout the amplitude axis bounded by the compressor threshold value. \n \nFigure 3 . Electric Bass Slide: Compressed  \nModality/harmonicity/noisi ness is a method for ana-\nlyzing a signal in terms of its harmonic, modal, and noise content. As shown in Figure 4, the harmonicity, modality, and noise floor levels of a signal are computed and dis-played over time. One way of computing the harmonicity and modality is via the fundamental frequency ( f\n0) and \nthe drift ( ek) in Hz. ek is found by first determining spec-\ntral peaks, followed by com puting their distances with \nrespect to the closest ideal harmonic locations. The mod-ality (“excessive inharmonic ity”) of each harmonic com-\nponent can be then computed as the ratio of the drift and the fundamental frequency. As  expressed in Equation (1) \nand (2), taking the mean of the inharmonicities of all the harmonics can be used to derive the modality of a signal.  \nkf e f eModalityk 0 0 1 / ... /++=  (1) \n Harmonicity  = 1 – Modality (2) \n   \nHarmonicity/\n(1 - Modality)TimeNoise \nFloorHarmonicity \nProfileModality \nProfile\nFigure 4 . Modality/Harmonicity/Noisiness \nThe computation of the noise floor is based on sound \nflatness measure (SFM) – the ratio of the geometric mean and the arithmetic mean whic h has been used in speech \nresearch to extract voiced and unvoiced signals. When the signal is considered to be above the noise threshold (via SFM), the fundamental frequency is estimated which is then followed by modality analysis. On the other hand, if the signal’s SFM value is determined to be below the noise threshold, it will be considered as noise. Another feature included in EASY is the multi-channel sound field vector developed by Travis Scharr while at Tulane University. This feature enables mutli-channel audio file display as a vector sum of th e energy in each of the audio \nchannels as a function of time.  \n3. SEGMENTATION ALGORITHMS \nThe two segmentation methods that we developed are based on clustering and distance measurement-based techniques as described in this section. \n3.1 Model-based Segmentation: Clustering \nThe model-based approach for segmentation exploits a \ntimbral feature vector clustering scheme. The audio input is first subjected to a silence detector followed by frame-by-frame feature extraction. The N-dimensional feature \nspace is then piped to the clustering algorithm (eg. k-means). The clusters are then remapped to the time-\ndomain in a color-coded fashion for visual clarity as shown in Figure 5.  \n3.2 Distance-based Segmentation \nThe distance-based segmentation algorithm applies statis-\ntical analysis of extracted feat ures selectable by the user. \nThe statistical analysis itself uses a long-term windowing scheme (main frames) to com pute the average feature tra-\njectory on a window-by-window (via sub-frames) basis – each sub-frame represents a single data point. Each main frame is then analyzed for its mean and standard devia-\ntion – the standard deviation is the distance measure used for segmentation. The distan ce can be computed via Euc-\nlidian distance, Kullback-Leibl er distance, Bhattacharyya \ndistance, Gish distance, Entropy loss or Mahalanobis dis-tance. \n695Poster Session 4\n  \n \nFigure 5.  Time-Remapping in Clustering-based Segmen-\ntation for 3 Features \n4. PREMLINARY ANALYSIS RESULTS \nWe used two pieces to conduct preliminary analysis of \nelectro-acoustic works – Machine Stops  (Tae Hong Park) \nand Riverrun  (Barry Truax). We chose Machine Stops  as \nwe have first-hand detaile d knowledge about the con-\nstruction of the piece and Riverrun  as it’s not only an \nelectro-acoustic masterpiece, but  also because it is very \nmuch based on timbral compositional strategies.  \nAmplitude\nBirth\nDeath\n \nFigure 6.  Segmentation Map of Machine Stops  \nA number of general obser vations could be made \njust by using single features such as modal-ity/harmonicity/noisiness (MHN), dynamic tightness (DT), spectral centroid (SC), and the spectrogram (SG) itself. The extracted information included insights about where harmonic sections started and ended, where more modal sections occurred (via MHN), locating timbrally bright sounding parts (SC), exposing dynamically com-pressed areas (DT), and observing overall energy distri-butions and shifts (SG). However, what was most inter-esting in our initial analysis was discovering “segmenta-tion maps,” “timbregram trajectories,” and “segmenta-tion/cluster tracks” as shown in Figures 6, 7, and 8. Looking at the segmentation map we can generally iden-tify four sections (A, B, C, A’) via the color-coded seg-mentation regions and the amplitude envelope. The intro A (labeled as “birth”) shows a triangular structure with \na general build-up of energy. This is mirrored ，slightly \nfragmented ，in A’ during the “death” phase of the piece \nwhich illustrates the overall arching shape of the piece \nitself. A’ also includes an extended portion of the begin-ning part of the piece, a dding a prolongation of decay \ntowards the end (the “machine” coming to a “stop”). \nTime\nClustercd\nabf\nAmplitudeeB\nAA’\nC\n \nFigure 7.  Segmentation/Cluster tracks ( Machine Stops ) \nFigure 7 which displays the decomposition of the seg-mentation map into individual “cluster tracks,” further exposes this build up and loss of energy of parts A and A’ and also depicts the introduction of section C (cluster \nf) as new material ( ○\n5 in Figure 8). Section B generally \nrepresents a sparse timbral construct exemplified by sin-\ngle and harmonically distorted sine-waves (in the HMN analysis plot, harmonicity is maximal in region B – not shown here).  \nNormalized spectral centroid\n \nFigure 8.  Timbregram Trajectory of Machine Stops   \nAs shown in the timbregram plot (Figure 8) we can \nclearly view (when following the cursor during playback) \nthe timbral trajectory which generally follows ○1 to ○2, \n○3, ○4, and ○5 during the “birth” and “development” sec-\ntions of the piece. The timbreg ram is also useful in dis-\nplaying continuous timbral changes between cluster a, b, \n69610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nand e while also showing abrupt jumps in the timbre \nspace between clusters e and a as well as a and f. The \nclosing triangular portion follows the inverse trajectory \n○3 to ○1.  \nNormalized CentroidNormalized Flux\n \nFigure 9.  Timbregram and Segmentation Map of  \nRiverrun \nA similar analysis was conducted for Riverrun  where \nwe concentrated in particular on the segmentation map, \ncluster tracks, and timbgregram. It was quite straightfor-ward to identify sectional divisions in the spectrogram as expected, but what was particularly interesting in the timbrgram was the finding that, unlike in Machine Stops , \nthe colonization of the timbr e space portrayed a distinct \nseparation of one particular cluster from the rest – the timbral cluster pertaining to the closing section of the piece with high spectral centroid as shown in Figure 9. At \nthe same time, the continuous development as described in [13] of Riverrun  can also be clearly seen in Figure 91 \nbeginning from a sparse quiet group of droplets, develop-ing to rivulets, streams, a nd massive oceans towards the \nmain part of the piece. Various feature sets have been employed in generating clusters, segmentation maps, and timbregrams. Interestingly enough, for the majority of the cases, the ensuing results have  been quite similar when \ninterpreting the various plots. The shapes, however, at times looked quite different in the timbregrams for ex-ample, but the overall timbral trajectories usually gravi-tated to the same conclusions. The same was also true when changing the number of clusters. In general, more clusters gave finer detail in grouping subtleties in the timbre space, whereas smaller number of clusters merged closely spaced clusters into a “supercluster.\" This is evi-\ndent in Figure 9, where the ultimate section of the piece \nbecomes one large cluster extending vertically (amplitude) when employing 5 clusters. \n5. SUMMARY AND FUTURE WORK \n5.1 Summary \nIn this paper we presented a new software system for as-sisting analysis of electro-ac oustic music with particular \nemphasis on timbre. We described the functionalities of the toolbox, some of the feature extraction algorithms, the timbre space display interface,  real-time possibilities us-\ning EASY, conducted preliminary analysis of two musi-cal examples, and discussed pattern recognition modules to help reveal structural elem ents of an audio signal. The \nsystem has been designed with ease of use in mind by providing a “click-and-go” interface while at the same time offering advanced options for more detailed para-metric control.  \n5.2 Future Work \nThe current version of the EASY Toolbox already in-\ncludes 26 features but we fore see that more features, es-\npecially those that are speci fic to electro-acoustic music \nwill be encountered in the fu ture as we further develop \nthis system. To facilitate adding new features we plan on providing a template for third party development. We plan to further extensively test and use the EASY Tool-box for analyzing a number of classic electro-acoustic works and expect to report our findings in the near future. \nOne very interesting and potentially exciting area that \ncould provide promising application for EASY is exploit-ing more pattern recognition techniques on feature vec-tors to analyze for “horizontal” and “vertical” relation-ships and correlations in a given audio signal. That is, analyzing and displaying feature trajectories and patterns not only by comparing frames as one unit but also ana-lyzing the vertical relationships vs. time as shown in Fig-ure 9.  This could be very useful in displaying detailed relation-ships between frames, sections, motifs, formal structures, referential cues, and many other patterns that can provide insights into the music under scrutiny. One way of im-plementing such a feature would be using labels to dis-play various icons in the time/frequency-domain can-vases and timbregram, which will further allow for anno-tation possibilities. \nAnother area that we are inte rested in exploring is the \nliterature concerning cognitive studies especially those \nthat are related to mood and sound [5, 15]. We are not explicitly interested in m easuring mood per se but we \nwould also like to examine ot her angles to help extract \nperceptual and cognitive dimensions from the music that is being analyzed. \nOn top of providing analysis results from feature vec-\ntors, we also plan on offering supplementary cultural in-formation acquired from the In ternet via search engines \nand online digital libraries.  One approach is using search strings as implemented in jWebMiner [9], which is a software package for extracti ng cultural features from the \nweb using hit counts. Current MIR technologies such as fingerprinting, artist identification, and genre classifica-tion are used for automatically recommending similar musical styles, composers, and artists. Although these technologies have not been specifically applied to “music analysis” software systems that we know of, we foresee great potential in incorporating and exploiting such tech-nologies not just for electro-acoustic music alone, but also for musical research, musicological studies, peda-\ngogy, and composition in genera l. It is not difficult to \n697Poster Session 4\n  \n \nimagine being able to have easy access to supplementary \ninformation such as scores, program notes, com-poser/performer/“machine” biographical information, graphics/pictures/videos, or any other related materi-als/media at one’s fingertips and at the click of one but-ton. \n \nFigure 10.  Verticality AND horizontality  \nAlthough the current software version is already a \nstand-alone MATLAB® application and can run on any \nmachine that has the MATALB® run-time library, we \nplan on porting it to faster and more efficient compiler-based platforms like Cocoa. \n6. REFERENCES \n[1] Cannam, C., Landone C., Sandler M., Bello J., “The Sonic Visualizer: A Visu alization Platform for \nSemantic Descriptors”, Proceedings of the \nInternational Conference on Music Information Retrieval 2006 , Victoria Canada \n[2] Cao, C., Li M., Liu J., Yan, Y., “Singing Melody Extraction in Polyphonic Music by Harmonic Tracking”, Proceedings of the International \nConference on Music Information Retrieval 2007 , \nVienna, Austria. \n[3] Kornstädt, A., “The JRing System for Computer-Assisted Musicological Analysis”, Proceedings of \nthe International Conference on Music Information Retrieval 2001 , Indiana, USA \n[4] Kuuskankare, M.\n，Laurson, M., “Vivo - Visualizing \nHarmonic Progressions and Voice-Leading in PWGL”, Proceedings of the International \nConference on Music Information Retrieval  2007 , \nVienna, Austria. \n[5] Li, T., Ogihara, M., “Detecting emotion in music”, Proceedings of the Inte rnational Conference on \nMusic Information Retrieval  2003, Washington D.C., USA [6] Lubbers, D., “SoniXplorer: Combining Visualization and Auralization for Content-Based Exploration of Music Collections”, Proceedings of the International Conference on Music Information Retrieval  2005 , \nLondon\n， U.K.  \n[7] McAdams, S., Winsberg, S., Donnadieu, S., De Soete, G., and Krimphoff, J. 1995. Perceptual \nScaling of Synthesized Musical Timbres: Common Dimensions, Specificities, and Latent Subject Classes , Psychological Research 58, 177 - 192. \n[8] McEnnis D., McKay C., Fujinaga I., Depalle P., “jAudio: An Feature Extraction Library”, Proceedings of the Inte rnational Conference on \nMusic Information Retrieval  2005, London, U.K.  \n[9] McKay, C., Fujinaga, I., “jWebMiner: A Web-based Feature Extractor”\n， Proceedings of the \nInternational Conference on Music Information Retrieval  2007 , Vienna, Austria.  \n[10] Park T. H., Biguenet J., Li Z., Richardson C., Scharr T., “Feature Modulation Synthesis”, Proceedings of the Inter national Computer Music \nConference 2007 , August, 2007, Copenhagen, \nDenmark. \n[11] Park, T. H. “Salient Feature Extraction of Musical \nInstrument Signals” , Dartmouth College, M.A. \nDissertation, 2000. \n[12] Park, T. H., Z. Li, Biguenet J., “Not Just More FMS: \nTaking It To The Next Level”, Proceedings of the 2008 ICMC, Belfast, Ireland. \n[13] Simoni, M., “Analytical Methods of Electroacoustic \nMusic”, Routledge, 2006, Ch. 8, pp. 187 – 238. \n[14] Sjölander, K., Beskow, J., “Wavesurfer – An Open \nSource Speech Tool”, Proceedings of ICSLP 2000 , \nBeijing, China \n[15] Thayer, R.E. 1989. The Biopsychology of Mood and \nArousal . New York: Oxford University Press \n[16] Tzanetakis, G., Essel, G. Cook, P., “Musical genre \nclassification of audio signals” ，Proceedings of  \nISMIR 2001 , Indiana, USA \n[17] Tzanetakis, G., Cook, P., “MARSYAS: a \nframework for audio analysis”,  Organised Sound , \nVol. 4 ,  Issue 3, 1999, Cambridge University Press \n[18] Lartillot, O., Toiviainen, P. “A MATLAB TOOLBOX FOR MUSICAL FEATURE EXTRACTION FROM AUDIO”, Proceedings of \nthe 10th Int. Conference on Digital Audio Effects (DAFx-07) , Bordeaux, France. \n698"
    },
    {
        "title": "On Rhythm and General Music Similarity.",
        "author": [
            "Tim Pohle",
            "Dominik Schnitzer",
            "Markus Schedl",
            "Peter Knees",
            "Gerhard Widmer"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418229",
        "url": "https://doi.org/10.5281/zenodo.1418229",
        "ee": "https://zenodo.org/records/1418229/files/PohleSSKW09.pdf",
        "abstract": "The contribution of this paper is threefold: First, we propose modifications to Fluctuation Patterns [14]. The resulting descriptors are evaluated in the task of rhythm similarity computation on the “Ballroom Dancers” collection. Second, we show that by combining these rhythmic descriptors with a timbral component, results for rhythm similarity computation are improved beyond the level obtained when using the rhythm descriptor component alone. Third, we present one “unified” algorithm with fixed parameter set. This algorithm is evaluated on three different music collections. We conclude from these evaluations that the computed similarities reflect relevant aspects both of rhythm similarity and of general music similarity. The performance can be improved by tuning parameters of the “unified” algorithm to the specific task (rhythm similarity / general music similarity) and the specific collection, respectively. 1 INTRODUCTION Many of the rhythm descriptors proposed so far eventually reduce the rhythm to a representation that discards information about which frequency band the rhythmic feature originates from. We begin this paper by asking: “Can the performance of rhythm descriptors be improved by adding frequency information?” To this end, we follow two directions. First, we propose and evaluate descriptors that retain information about the frequency range in which a given rhythm feature (more precise: periodicity strength) was measured. Related work in this direction includes [10]. Second, we add frequency information in the form of a “timbral” component (cf. [3]). The paper is organized as follows. In Section 2, we suggest a number of modifications to Fluctuation Patterns (FPs) [14]. Relative to our evaluation setting, the modified variant seems to capture rhythmic similarity better than the unmodified algorithm. In Section 3, we go on by adding frequency information to the proposed rhythm Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c⃝2009 International Society for Music Information Retrieval. descriptors in the form of a “timbral” component, and find that in our evaluation setting, rhythm similarity computation is improved further this way. We consider this finding as complementary to the practice of using rhythm descriptors to improve the performance of (general) music similarity measures (e.g., [14]). Based on this observation, we design an algorithm that seems to perform well both in the task of rhythm similarity and in the task of general music similarity computation (Section 4). In our evaluation setting, this combined algorithm outperforms approaches that are specifically designed for the respective tasks. 2 GETTING THE RHYTHM This section is dedicated to rhythm descriptors and their evaluation on the Ballroom Dancers collection.",
        "zenodo_id": 1418229,
        "dblp_key": "conf/ismir/PohleSSKW09",
        "keywords": [
            "rhythm descriptors",
            "Ballroom Dancers collection",
            "frequency information",
            "timbral component",
            "rhythm similarity",
            "general music similarity",
            "algorithm",
            "evaluation setting",
            "performance improvement",
            "task of rhythm similarity"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nON RHYTHM AND GENERAL MUSIC SIMILARITY\nTim Pohle1, Dominik Schnitzer1,2, Markus Schedl1, Peter Knees1and Gerhard Widmer1,2\n1Department of Computational Perception, Johannes Kepler University, Linz, Austria\n2Austrian Research Institute for Artiﬁcial Intelligence (OFAI), Wien, Austria\nmusic@jku.at\nABSTRACT\nThe contribution of this paper is threefold:\nFirst, we propose modiﬁcations to Fluctuation Patterns\n[14]. The resulting descriptors are evaluated in the task of\nrhythm similarity computation on the “Ballroom Dancers”\ncollection.\nSecond, we show that by combining these rhythmic de-\nscriptors with a timbral component, results for rhythm sim-\nilarity computation are improved beyond the level obtained\nwhen using the rhythm descriptor component alone.\nThird, we present one “uniﬁed” algorithm with ﬁxed\nparameter set. This algorithm is evaluated on three differ-\nent music collections. We conclude from these evaluations\nthat the computed similarities reﬂect relevant aspects both\nof rhythm similarity andof general music similarity. The\nperformance can be improved by tuning parameters of the\n“uniﬁed” algorithm to the speciﬁc task (rhythm similarity\n/ general music similarity) and the speciﬁc collection, re-\nspectively.\n1 INTRODUCTION\nMany of the rhythm descriptors proposed so far eventually\nreduce the rhythm to a representation that discards infor-\nmation about which frequency band the rhythmic feature\noriginates from. We begin this paper by asking: “Can the\nperformance of rhythm descriptors be improved by adding\nfrequency information?” To this end, we follow two di-\nrections. First, we propose and evaluate descriptors that\nretain information about the frequency range in which a\ngiven rhythm feature (more precise: periodicity strength)\nwas measured. Related work in this direction includes\n[10]. Second, we add frequency information in the form\nof a “timbral” component (cf. [3]).\nThe paper is organized as follows. In Section 2, we\nsuggest a number of modiﬁcations to Fluctuation Patterns\n(FPs) [14]. Relative to our evaluation setting, the mod-\niﬁed variant seems to capture rhythmic similarity better\nthan the unmodiﬁed algorithm. In Section 3, we go on\nby adding frequency information to the proposed rhythm\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.descriptors in the form of a “timbral” component, and ﬁnd\nthat in our evaluation setting, rhythm similarity computa-\ntion is improved further this way. We consider this ﬁnding\nas complementary to the practice of using rhythm descrip-\ntors to improve the performance of (general) music simi-\nlarity measures (e.g., [14]). Based on this observation, we\ndesign an algorithm that seems to perform well both in the\ntask of rhythm similarity andin the task of general music\nsimilarity computation (Section 4). In our evaluation set-\nting, this combined algorithm outperforms approaches that\nare speciﬁcally designed for the respective tasks.\n2 GETTING THE RHYTHM\nThis section is dedicated to rhythm descriptors and their\nevaluation on the Ballroom Dancers collection.\n2.1 Rhythm Descriptors\nBelow, the rhythm descriptors evaluated in this paper are\ndescribed. These are the well-known Fluctuation Patterns,\nand our proposed extensions Onset Patterns (OPs) and On-\nsetCoefﬁcients (OCs).\n2.1.1 Fluctuation Patterns (FPs)\nFluctuation Patterns (FPs) [14] measure periodicities of the\nloudness in various frequency bands, considering a num-\nber of psychoacoustic ﬁndings. We use the implementation\nof the MA Toolbox1with the proposed parameter set, so\nthat the frequency bands correspond to 20critical bands.\nDetails about the computation are given e.g. in [14]. An\nevaluation of the importance of the various psychoacoustic\nprocessing steps in FP calculation is given in [10].\n2.1.2 Onset Patterns (OPs)\nWe suggest a number of changes to FPs (cf. [4, 17, 18]).\nTo this end, a number of preliminary experiments was con-\nducted. The most important changes to FPs are listed here,\nbefore the points are discussed in detail:\n•Reduce the signal to the parts of increasing ampli-\ntude (i.e., likely onsets).\n•Use semitone bands to detect onsets instead of fewer\ncritical bands.\n•Use Hanning window and zero padding before de-\ntecting periodicities with FFT.\n1http://www.pampalk.at/ma/\n525Oral Session 6: Similarity\n•Represent periodicity in log scale instead of linear\nscale.\nWe only consider onsets (or increasing amplitudes) in\na given frequency band. To detect such onsets, we use a\ncent-scale representation of the spectrum with 85bands of\n103.6cent width, with frames being 15.5 ms apart. On\neach of these bands, an unsharp-mask like effect is applied\nby subtracting from each value the mean of the values over\nthe last 0.25 sec in this frequency band, and half-wave rec-\ntifying the result. This aims to detect also slow-attack in-\nstrument onsets in melodies that have notes with only one\n(or few) semitones apart. Subsequently, values are trans-\nformed by taking the logarithm, and reducing the number\nof frequency bands from 85to38which is closer to the\nnumber of critical bands.\nAs in the computation of FPs, segments of frames are\nanalyzed for periodicities. We use segments of 2.63 sec\nlength with a superimposed Hanning window, zero-padded\nto six seconds. Adjacent segments are 0.25 sec apart. Each\nof these segments is analyzed for periodicities in the range\nfromT0= 1.5 sec up to about 13.3 Hz (40to about 800\nbpm), separately in each of the 38frequency bands. A cru-\ncial point in this transformation is that we do not represent\nperiodicities on a linear scale (as in FPs), but rather we use\na log-representation. Thus, after taking the FFT on the six\nseconds of a given frequency band, a log ﬁlterbank is ap-\nplied to represent the selected periodicity range in 25log-\nscaled bins. In this representation, periodicity (measured\nin Hz) is doubled every 5.8bins (i.e., going 6bins to the\nright means measuring a periodicity about twice as fast).\nBy using this log scale, all activations in an OP are shifted\nby the same amount in the x-direction when two pieces\nhave the same onset structure but different tempi. While\nthis representation is not blurred (as done in the computa-\ntion of FPs), the applied techniques induce a smearing in\nthe lower periodicity range (cf. Figure 1). After a segment\nis computed, each of the 25periodicities is normalized to\nhave the same response to a broadband noise modulated by\na sine with the given periodicity. This is done to eliminate\nthe ﬁlter effect of the onset detection step and the transfor-\nmation to logarithmic scale.\nTo arrive at a description of an entire song, the values\nover all segments are combined by taking the mean of each\nvalue over all segments. We call the resulting representa-\ntion of size 38·25Onset Patterns (OPs). In this paper, the\ndistance between OPs is calculated by taking the Euclidean\ndistance between the OPs considered as column vectors.\n2.1.3 OnsetCoefﬁcients (OCs)\nOnsetCoefﬁcients are obtained from all OP segments of\na song by applying the two-dimensional discrete cosine\ntransformation (DCT) on each OP segment, and discard-\ning higher-order coefﬁcients in each dimension. The DCT\nleads to a certain abstraction from the actual tempo (cf.\n[5, 18]) andfrom the frequency spectrum (like in MFCCs).\nThis is motivated by the notion that slightly changing rhythm\nand sounds does not have a big impact on the perceived\ncharacteristic of a rhythm, while the same rhythm played\nFigure 1 . FP and OP of the same song. Doubling of peri-\nodicity appears evenly spaced in the OP. A bass drum plays\nat regular rate of about 2 Hz . The piece has a tap-along\ntempo of about 4 Hz , while the measured periodicities at\nabout 8 Hz are likely caused by offbeats in between taps.\nwith a drastically different tempo may have a different per-\nceived characteristic. For example, one can imagine that\na slow and laid-back drum loop, used in a Drum’n’Bass\ntrack played back two or three times as fast, is perceived as\ncheerful.\nThe number of DCT coefﬁcients kept in each dimension\n(periodicity / frequency) is an important parameter. The\nselected coefﬁcients are stacked into a vector. For example,\nkeeping coefﬁcients 0to7in the periodicity dimension,\nand coefﬁcients 0to2in the frequency dimension yields a\nvector of length 8·3 = 24 . We abbreviate this selection as\n7×2. Based on the vectors for all segments, the mean and\nfull covariance matrix (i.e, a single Gaussian) is calculated,\nwhich is the OC feature data for a song.\nThe OC distance Dbetween two Songs (i.e., Gaussians)\nXandYis calculated by the Jensen-Shannon (JS) diver-\ngence (cf. [11]).\nD(X, Y ) =H(M)−H(X) +H(Y)\n2(1)\nwhere Hdenotes the entropy, and Mis the Gaussian re-\nsulting from merging XandY. We calculate the merged\nGaussian following [20]. We use the square root of this\ndistance.\n2.2 Setup for Rhythm Experiments\nWe evaluate the rhythm descriptors on the ballroom dance\nmusic set2previously used by other authors, e.g. [5, 4, 2,\n2data from ballroomdancers.com\n52610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n15, 7] and for the ISMIR’04 Dance Music Classiﬁcation\nContest3. This set consists of 698tracks assigned to 8\ndifferent dance music styles (“genres”). The classiﬁcation\nbaseline is 15.9%.\nThe purpose of the descriptors discussed above is to\nmeasure rhythmic similarity . For evaluation, we assume\nthat tracks that are in the same class have a similar rhythm.\nTo facilitate comparison to previous work [5, 4], we use a\n1-nearest-neighbor (1NN) stratiﬁed 10-Fold cross valida-\ntion (averaged over 32runs) in spite of a certain variance\ninduced by the random selection of folds. We assume that\nthe only information that is available is the audio signal.\nUsing 1NN 10fold cross validation, [5] report up to 79.6%\naccuracy.\nWhen using more sophisticated classiﬁcation algorithms\n(and other features), higher accuracies are obtained. For\nexample, [2] report a classiﬁcation accuracy of up to 82%\nusing only automatically computed features (i.e., without\nusing correct tempo annotation or manually corrected ﬁrst\nbar annotations). The highest classiﬁcation accuracy we\nare aware of is 86.9%, obtained by kNN classiﬁcation [7].\nThe mentioned accuracies are obtained when the audio\nsignal is the only data source made available to the algo-\nrithms. It has to be noted that the algorithms yield higher\naccuracies when also the correct tempo annotation is given\nas feature data. In this case (which is not considered in this\npaper), an accuracy of 95.1%(or96.0%when also human-\ncorrected bar annotations are used [2]) have been obtained.\n2.3 Results for Rhythm-Only Descriptors\nFPs as implemented in the MA toolbox, compared by Eu-\nclidean distance, yield an accuracy of 75.0% . OPs com-\npared with Euclidean distance yield 86.7% . The results for\nvarious settings of using only OnsetCoefﬁcients for sim-\nilarity estimation are shown in Figure 2. It can be seen\nthat the highest values are obtained when keeping more\nthan16coefﬁcients in the periodicity dimension and when\nonly keeping the 0th coefﬁcient in the frequency dimension\n(which corresponds to averaging over all frequencies). In\nthis range, values increase when including more periodic-\nity coefﬁcients, which seems consistent with the ﬁndings in\n[5]. In this range, we obtain an average value of 87.7%4.\n3 ADDING “TIMBRE” INFORMATION\nTo examine how the discussed rhythmic descriptors can\nbe used in conjunction with “bag of frames” audio sim-\nilarity measures, we combine them with a “timbral” au-\ndio similarity measure. The used frame-based features are\nthe well-known MFCCs (coefﬁcients 0..15), Spectral Con-\ntrast Coefﬁcients [9] (using the 2N approach [1], coefﬁ-\ncients 0..15), and the descriptors Harmonicness andAt-\ntackness . The latter two describe the amount of harmonic\nand percussive elements (cf. [13]) in a cent-scaled spectro-\ngram with frequency bands being 66cent and frames being\n3http://mtg.upf.edu/ismir2004/contest/rhythmContest/\n4We take the average rather than the maximum value as an indicator\ndue to variances introduced by 10fold CV .46 ms apart. Percussive elements are detected by applying\na5×5ﬁlter with the kernel (-0.14, -0.06, 0.2, 0, 0) repli-\ncated over ﬁve rows. The analogous ﬁlter to detect har-\nmonic elements has the form (−0.09,−0.01,0.2,−0.01,\n−0.09)T, replicated over ﬁve columns. The Harmonic-\nness value for a frame is the sum of the half-wave recti-\nﬁed responses of this ﬁlter centered at the frequency bins\nof the considered frame. The frame’s Attackness value\nis calculated the same way but using the ﬁlter for per-\ncussive elements. Altogether, these are 34descriptor val-\nues for a frame, which are combined over a song by tak-\ning their mean and full covariance matrix. Two songs are\ncompared by taking the Jensen-Shannon divergence as de-\nscribed above.\nWe combine the discussed rhythm descriptors with this\ntimbral component by simply summing up the two distance\nvalues (i.e., timbral and rhythm component are weighted\n1 : 1). For comparison, e.g., in the G1C algorithm [14], FP\nbased features are weighted with 30%, and a MFCC com-\nponent is weighted with 70%. Our weighting decision is\nnot based on systematic evaluations but rather it is mainly\nbased on impressions gained from non-representative lis-\ntening experiments. To bring the two distances (rhythm\nbased and timbre based) to a comparable magnitude, for\neach song the distances of this song to all other songs in the\ncollection are normalized by mean removal and division by\nstandard deviation5. Subsequently, the distances are sym-\nmetrized by summing up the distances between each pair\nof songs in both directions. This preprocessing step is done\nfor each component (timbral and rhythm) independently\nbefore summing them up.\n3.1 Combination Experiment\nWe repeat the experiment shown in Figure 2, but this time\ncombining the rhythm descriptors with the timbral com-\nponent as described. The 1NN 10fold cross validation ac-\ncuracy is 54.0%when considering only the timbral com-\nponent, 79.4%in combination with FPs, and 87.1%with\nOPs. From the results in Figure 3, it can be seen that\nclassiﬁcation results are improved when combining OCs\nwith the timbral component. This time, average results\nof90.2%are obtained over the parameter range discussed\nabove (compared to 87.7%in the the ﬁrst experiment, Fig-\nure 2). The highest obtained 1NN accuracy is 91.3%.\nResults are summarized in Table 1. The results for the\ncombined method are above the values obtained for each\ncomponent (rhythm and timbre) alone. We think this is an\nindication that rhythm similarity computations can be im-\nproved by including timbre information. This is in line\nwith [19] who reason that tempo can be detected better\nwhen considering timbre information. In a way, this is\ncomplementary to previous approaches where descriptors\nof rhythmical properties were added to timbre descriptors\nin order to improve music similarity computations (e.g. the\n5This is done once before splitting up training and test sets for clas-\nsiﬁcation. No class labels are used in this step. We expect the impact of\ndetermining the normalization factors only on the respective (stratiﬁed)\ntraining set to be negligible.\n527Oral Session 6: Similarity\nFigure 2 . Dance genre classiﬁcation based on OnsetCoefﬁcients; distances calculated according to Equation 1. 1NN 10fold\nCV accuracies obtained on ballroom dataset when including coefﬁcients 0 up to the given number in the respective dimen-\nsion. For example, including coefﬁcients 0..17 in the periodicity dimension and coefﬁcients 0..1 in frequency dimension\n(resulting in 18·2 = 36 dimensional feature data) yields an accuracy of 85.9%. Low results at right border are caused\nby numerical instabilites when calculating the determinant during entropy computation. For better visibility, gray shades\nindicate ranks instead of actual values.\nAlgorithm 1NN\nBaseline 15.9%\nFP 75.0%\nOP 86.7%\nOC up to around 87.7%\nTimbre 54.0%\nTimbre+FP 79.4%\nTimbre+OP 87.1%\nTimbre+OC up to around 90.2%\nTable 1 . Ballroom dataset: 10fold CV accuracies obtained\nby the evaluated methods. The methods below the line are\ncombined by distance normalization and addition.\nG1C algorithm [14]). This duality leads to the experiments\npresented next.\n4 THE “UNIFIED” ALGORITHM\nEncouraged by the experiments presented in the previous\nsection, we examine the performance of this algorithm not\nonly in the task of rhythm similarity computation, but also\nin the task of general music similarity. Our aim is to ﬁnd\na selection of OCs that perform well in both tasks, which\neventually leads to a “uniﬁed” music retrieval algorithm\nthat reﬂects both rhythm and timbre similarity.\n4.1 Data Sets\nMusic similarity experiments are performed on the set from\nthe ISMIR’04 genre classiﬁcation contest (ISMIR’04)6,\nand on the “Homburg” data set (HOMBURG) [8]. Like the\nballroom set, these collections are available to the research\ncommunity, which facilitates reproduction of experiments\nand gives a benchmark for comparing different algorithms.\nThere are two variants of the ISMIR’04 collection. The\nﬁrst is the “training” set which consists of 729tracks from\nsix genres. The second consists of all the tracks in the\n“training” and “development” sets, which are 1458 tracks\n6http://ismir2004.ismir.net/genre contest/index.htmfrom six genres. We use the central two minutes from each\ntrack. The HOMBURG set consists of 1886 excerpts of 10\nseconds length.\n4.2 Combination Experiment\nIn this section, we conduct a similar experiment as in Sec-\ntion 3.1 on the ISMIR’04 training collection. The aim is to\nevaluate the impact of OCs on the performance in general\nmusic similarity computation (i.e., not limited to rhythm\nsimilarity). The results from these experiments are used\nto create the “uniﬁed” algorithm, which will then be eval-\nuated on all three collections (including the HOMBURG\ncollection).\nFollowing previous work [1, 14], we take genre classi-\nﬁcation accuracy as an indicator of the algorithm’s ability\nto ﬁnd similar sounding music. We use the same evalua-\ntion methodology as before. The timbre component alone\nyields 83.8%. Combining it with FPs as described, ac-\ncuracy drops to 83.6%. Using OPs instead, accuracy in-\ncreases to 85.2%. With OCs, accuracy can be improved up\nto87.8%in the parameter range shown in Figure 4. This\nﬁgure shows an outlier for 19×0OCs, for which unfor-\ntunately we did not ﬁnd an obvious explanation such as\noutliers in the distance matrix or numerical instabilities.\nComparing Figures 3 and 4, it seems that a good tradeoff\nbetween the two collections is found when using 16×1\nOCs. This selection yields 17·2 = 34 -dimensional feature\ndata, i.e., the rhythm feature data consists of a mean vector\nof length 34and a covariance matrix of size 342= 1156 .\n4.3 Final Evaluation and Optimization\nIn Table 2, 10fold CV results obtained with this setting are\nlisted. For comparison to previous work, also the highest\nclassiﬁcation accuracies obtained so far that we are aware\nof are listed. These accuracies refer to methods only us-\ning audio descriptors without additional human-annotated\nclues. On all three collections, the results of the “uniﬁed”\nalgorithm are above these previously reported results.\n52810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 3 . Combination of OCs with timbral component on the ballroom dancers collection, 1NN 10fold cross validation.\nFigure 4 . Combination of OCs with timbral component, ISMIR’04 training collection.\nCollection 1NN highest kNN Literature\n(obtained at k)\nBallroom 88.4% 89.2% (k=5) 86.9%[7]\nISMIR’04 train 87.6% 87.6% (k=1) 84.0%[16]\nISMIR’04 1458 90.4% 90.4% (k=1) 83.5%[6]\nHOMBURG 50.8% 57.0% (k=10) 55% [12]\nTable 2 . Accuracies obtained by the “uniﬁed” algorithm\non the various collections.\nWhile these results show that our “uniﬁed” algorithm\noutperforms the respective specialized approaches, we ob-\nserve that when tuning to the particular collections, our\ntechniques can be used to obtain even higher accuracies.\nFor these experiments, we use leave-one-out evaluation for\ntwo reasons. First, doing 10fold cross validation (and re-\npeating it several times for averaging) has a clearly longer\nruntime, as we evaluate a ﬁxed matrix of pairwise dis-\ntances. Second, in the 10fold cross validation experiments,\nwe observe a certain variance between repeated experi-\nments.\nOur non-exhaustive tuning experiments indicate that\neven the normalization step used to combine two measures\n(Section 3) alone in some cases increases accuracy. On the\nBallroom Dancers collection, a 3NN accuracy of 91.8%\nis obtained when including normalised OCs up to 24×\n0. Using only the normalised timbre component, on the\nISMIR’04 training set a 1NN accuracy of 88.8%, and on\nthe full ISMIR’04 set an accuracy of 91.8%is reached.\nOn the HOMBURG set, 11NN classiﬁcation using onlythe normalised timbre component yields 58.4%.\nCommon sense indicates that the “uniﬁed” algorithm is\na better choice for similarity estimation than such tuned\nvariants, as the tuned variants do not perform well on all\ncollections. In particular, these experiments show that dis-\ncarding the rhythm component and using the timbre com-\nponent alone, higher accuracies than those of the “uniﬁed”\nalgorithm are obtained both on the ISMIR’04 set and the\nHOMBURG set. But with this setting, accuracy decreases\nclearly on the “Ballroom Dancers” collection. This may\nindicate the existence of an evaluation glass ceiling in the\nsense that an improved general music similarity algorithm\nmight even yield lower accuracies.\n5 CONCLUSIONS\nWe have presented modiﬁcations of Fluctuation Patterns\n(FPs) that can be used to obtain higher classiﬁcation accu-\nracies on the audio signal of the “Ballroom Dancers collec-\ntion” than FPs compared by Euclidean distance. By adding\nfrequency information to these proposed rhythm descrip-\ntors in the form of a “timbral” component results are fur-\nther improved.\nBased on these results, we suggest a “uniﬁed” algo-\nrithm. The presented experiments indicate that the simi-\nlarities computed by this algorithm both reﬂect aspects of\nrhythm similarity andaspects of general music similarity.\nIn both respects, classiﬁcation accuracies obtained in our\ntest setting are at least comparable to those previously re-\nported for algorithms speciﬁcally designed for the respec-\ntive tasks.\nGoing beyond this, presented preliminary results show\n529Oral Session 6: Similarity\nthat by using different parameter settings (including selec-\ntion of used OCs, and relative weighting of timbral and\nrhythm component) for different collections, the accura-\ncies obtained with the “uniﬁed” algorithm can be further\nimproved. As by doing so, one loses the generality of the\nalgorithm, we refrain from further optimizations in this di-\nrection.\n6 ACKNOWLEDGMENTS\nThis work is supported by the Austrian Fonds zur F ¨orderung\nder Wissenschaftlichen Forschung under project number\nL511-N15.\n7 REFERENCES\n[1] Jean-Julien Aucouturier and Francois Pachet. Improv-\ning timbre similarity: How high is the sky? Journal of\nNegative Results in Speech and Audio Sciences , 1(1),\n2004.\n[2] Simon Dixon, Fabien Gouyon, and Gerhard Widmer.\nTowards characterisation of music via rhythmic pat-\nterns. In Proc. International Conference on Music In-\nformation Retrieval (ISMIR’04) , 2004.\n[3] A. Flexer, F. Gouyon, S. Dixon, and G. Widmer. Prob-\nabilistic combination of features for music classiﬁca-\ntion. In Proc. International Conference on Music In-\nformation Retrieval (ISMIR’06) , 2006.\n[4] Fabien Gouyon and Simon Dixon. Dance Music Clas-\nsiﬁcation: A Tempo-Based Approach. In Proc. Inter-\nnational Conference on Music Information Retrieval\n(ISMIR’04) , 2004.\n[5] Fabien Gouyon, Simon Dixon, Elias Pampalk, and\nGerhard Widmer. Evaluating rhythmic descriptors for\nmusical genre classiﬁcation. In Proc. AES 25thInter-\nnational Conference , 2004.\n[6] A. Holzapfel and Y . Stylianou. Musical genre classi-\nﬁcation using nonnegative matrix factorization-based\nfeatures. IEEE Transactions on Audio, Speech, and\nLanguage Processing , 16(2):424–434, 2008.\n[7] Andre Holzapfel and Yannis Stylianou. A scale trans-\nform based method for rhythmic similarity of music.\nInProc. IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , 2009.\n[8] Helge Homburg, Ingo Mierswa, B ¨ulent M ¨oller, Katha-\nrina Morik, and Michael Wurst. A benchmark dataset\nfor audio classiﬁcation and clustering. In Proc. Inter-\nnational Conference on Music Information Retrieval\n(ISMIR’05) , 2005.\n[9] Dan-Ning Jiang Jiang, Lie Lu, Hong-Jiang Zhang,\nJian-Hua Tao, and Lian-Hong Cai. Music type classiﬁ-\ncation by spectral contrast feature. In Proc. IEEE Inter-\nnational Conference on Multimedia and Expo (ICME) ,\n2002.[10] Thomas Lidy and Andreas Rauber. Evaluation of fea-\nture extractors and psycho-acoustic transformations for\nmusic genre classiﬁcation. In Proc. International Con-\nference on Music Information Retrieval (ISMIR’05) ,\n2005.\n[11] Jianhua Lin. Divergence measures based on the shan-\nnon entropy. IEEE Transactions on Information The-\nory, 37:145–151, 1991.\n[12] Fabian Moerchen, Ingo Mierswa, and Alfred Ultsch.\nUnderstandable models of music collections based on\nexhaustive feature generation with temporal statistics.\nInProc. 12thACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , 2006.\n[13] Nobutaka Ono, Kenichi Miyamoto, Hirokazu\nKameoka, and Shigeki Sagayama. A real-time\nequalizer of harmonic and percussive components in\nmusic signals. In Proc. International Conference on\nMusic Information Retrieval (ISMIR’08) , 2008.\n[14] E. Pampalk. Computational Models of Music Similar-\nity and their Application in Music Information Re-\ntrieval . Doctoral dissertation, Vienna University of\nTechnology, 2006.\n[15] Geoffroy Peeters. Rhythm classiﬁcation using spectral\nrhythm patterns. In Proc. International Conference on\nMusic Information Retrieval (ISMIR’05) , 2005.\n[16] T. Pohle, P. Knees, M. Schedl, and G. Widmer. Au-\ntomatically adapting the structure of audio similarity\nspaces. In Proc. 1stWorkshop on Learning the Seman-\ntics of Audio Signals (LSAS 2006), 1stInternational\nConference on Semantics and Digital Media Technol-\nogy (SAMT 2006) , 2006.\n[17] Yuan-Yuan Shi, Xuan Zhu, Hyoung-Gook Kim, Ki-\nWan Eom, and Kim Ji-Yeun. Log-scale modulation fre-\nquency coefﬁcient: A tempo feature for music emo-\ntion classiﬁcation. In Proc. 1stWorkshop on Learning\nthe Semantics of Audio Signals (LSAS 2006), 1stInter-\nnational Conference on Semantics and Digital Media\nTechnology (SAMT 2006) , 2006.\n[18] Kris West. Novel techniques for Audio Music Classi-\nﬁcation and Search . Doctoral dissertation, School of\nComputing Sciences, University of East Anglia, 2008.\n[19] Linxing Xiao, Aibo Tian, Wen Li, and Jie Zhou. Us-\ning a Statistic Model to Capture the Association Be-\ntween Timbre and Perceived Tempo. In Proc. Interna-\ntional Conference on Music Information Retrieval (IS-\nMIR’08) , 2008.\n[20] Wei Xu, Jacques Duchateau, Kris Demuynck, and\nIoannis Dologlou. A New Approach to Merging Gaus-\nsian Densities in Large V ocabulary Continuous Speech\nRecognition. In Proc. IEEE Benelux Signal Processing\nSymposium , 1998.\n530"
    },
    {
        "title": "You Call That Singing? Ensemble Classification for Multi-Cultural Collections of Music Recordings.",
        "author": [
            "Polina Proutskova",
            "Michael A. Casey"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418183",
        "url": "https://doi.org/10.5281/zenodo.1418183",
        "ee": "https://zenodo.org/records/1418183/files/ProutskovaC09.pdf",
        "abstract": "The wide range of vocal styles, musical textures and recording techniques found in ethnomusicological field recordings leads us to consider the problem of automatically labeling the content to know whether a recording is a song or instrumental work. Furthermore, if it is a song, we are interested in labeling aspects of the vocal texture: e.g. solo, choral, acapella or singing with instruments. We present evidence to suggest that automatic annotation is feasible for recorded collections exhibiting a wide range of recording techniques and representing musical cultures from around the world. Our experiments used the Alan Lomax Cantometrics training tapes data set, to encourage future comparative evaluations. Experiments were conducted with a labeled subset consisting of several hundred tracks, annotated at the track and frame levels, as acapella singing, singing plus instruments or instruments only. We trained frame-by-frame SVM classifiers using MFCC features on positive and negative exemplars for two tasks: per-frame labeling of singing and acapella singing. In a further experiment, the frame-by-frame classifier outputs were integrated to estimate the predominant content of whole tracks. Our results show that frame-byframe classifiers achieved 71% frame accuracy and whole track classifier integration achieved 88% accuracy. We conclude with an analysis of classifier errors suggesting avenues for developing more robust features and classifier strategies for large ethnographically diverse collections.",
        "zenodo_id": 1418183,
        "dblp_key": "conf/ismir/ProutskovaC09",
        "keywords": [
            "vocal styles",
            "musical textures",
            "recording techniques",
            "automatic labeling",
            "song vs. instrumental",
            "vocal texture",
            "acapella singing",
            "singing with instruments",
            "Alan Lomax Cantometrics training tapes",
            "whole track classification"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n       YOU CALL THAT SINGING?ENSEMBLE CLASSIFICATION FOR MULTI-CULTURALCOLLECTIONS OF MUSIC RECORDINGS Polina ProutskovaMichael CaseyDepartment of ComputingGoldsmiths, London, UKp.proutskova@gold.ac.ukBregman Music Research LaboratoryDartmouth College, USAmcasey@Dartmouth.eduABSTRACTThewiderangeofvocalstyles,musicaltexturesandre-cordingtechniquesfoundinethnomusicologicalfieldre-cordingsleadsustoconsidertheproblemofautomatic-allylabelingthecontenttoknowwhetherarecordingisasongorinstrumentalwork.Furthermore,ifitisasong,weareinterestedinlabelingaspectsofthevocaltexture:e.g.solo,choral,acapellaorsingingwithinstruments.Wepresentevidencetosuggestthatautomaticannotationisfeasibleforrecordedcollectionsexhibitingawiderangeofrecordingtechniquesandrepresentingmusicalculturesfromaroundtheworld.OurexperimentsusedtheAlanLomaxCantometricstrainingtapesdataset,toencouragefuturecomparativeevaluations.Experimentswerecon-ductedwithalabeledsubsetconsistingofseveralhun-dredtracks,annotatedatthetrackandframelevels,asacapellasinging,singingplusinstrumentsorinstrumentsonly.Wetrainedframe-by-frameSVMclassifiersusingMFCCfeaturesonpositiveandnegativeexemplarsfortwotasks:per-framelabelingofsingingandacapellasinging.Inafurtherexperiment,theframe-by-frameclas-sifieroutputswereintegratedtoestimatethepredominantcontentofwholetracks.Ourresultsshowthatframe-by-frameclassifiersachieved71%frameaccuracyandwholetrackclassifierintegrationachieved88%accuracy.Weconcludewithananalysisofclassifiererrorssuggestingavenuesfordevelopingmorerobustfeaturesandclassifi-erstrategiesforlargeethnographicallydiversecollec-tions.1.INTRODUCTIONWeexploreapproachesforMIRandethnomusicologytosupporteachotherintheareaofcross-culturalresearchandtocontributenewtasksandobservationstobothfields.Ethnomusicologicalrecordingsconstituteamajorchallenge to MIR tools, due to their musical, acoustic andtechnicaldiversity,sotheycanhelpimproveourunder-standingofmachine-musicinteraction.MIRmethodsarealsodrivinginterestinlarger-scale,data-intensivecross-cultural studies in music. Ethnomusicologicalrecordingsdocumentmusic-alrepertoriesoutsideofWesternclassicalandpopularmusic,oftenthosethatareendangeredorextincttoday.Theserecordingsareusedforeducationorresearchontheserepertories.Somecollectionshavebeencommer-ciallyreleasedbyrecordlabelsorculturalorganizations[1].Now,duetoeasilyaccessiblerecordingequipment,thevolumeofrecordingsisgrowingexponentially.Thisposesnewchallengesinmanagingethnomusicologicalcollectionswhichcanholduptohundredsofthousandsrecordeditemswithtenstothousandsofhoursofrecord-ings,thoughonlyafractionofthesearecurrentlydigit-ized [2].Recordingqualitywithincollectionsvariesgreatlyandthereisoftenlittleornoinformationaboutthetechnicalandacousticcontextfortherecording.Sometimesthesingerortheleadinginstrumentarenotthemostdominantpartoftherecording;socialcontextsvarygreatlyfromtheconcertoralbumsettingscommoninWesternmusicalculture;fieldrecordingsoftencontainsoundsofsocialandnaturalenvironmentsaswellasoth-ernoiseconditions.However,thegreatestchallengeisthevarianceinmusicalmaterial:evenifagivencollectionishomogeneous,thecontentwilldiffergreatlyfromWest-ern music so requiring new MIR approaches.Thispaperconcernsautomaticannotation,bothattheframelevelandtracklevel,ofethnomusicologicalfieldrecordings.Thequalitativenatureoftheirresearchrequiresustoapproachethnomusicologistscarefullywhenproposingnewtechnology.Forexample,asingleclassificationerrorcanhaveafarreachingimpactonin-terpretation,sowemustconsiderclassificationerrorsandtheircausesanddocumenttheseforusersofautomatic-allylabeledarchives.Tothisend,wegiveanoverviewofpreviousworkinSection2,describeourclassificationexperimentsinSection3,presentourresultsandofferde-tailedobservationsonclassifiererrorsinSection4andconclude in Section 5.2.PREVIOUS WORKDownie[3]andTzanetakisetal.[4]havestressedtheneedforresearchonethnomusicologicalcollections.Butpublicationsinthisareaarestillrareinlargepartduetotherebeingfewrecordedcollectionsavailablewithan-notationstouseasgroundtruthdata.Tzanetakisetal.[4]Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopies bear this notice and the full citation on the first page.© 2009 International Society for Music Information Retrieval \n759Oral Session 9-B: Sociology and Ethnomusicology\nprovideanoverviewofMIRworkrelatedtonon-West-ernmusicalcontentandsuggestsbasicguidelinesforthiskindofstudy,butthisworkdoesnotconsidercross-cul-turalresearch,involvingheterogeneouscollections,which we are predominantly interested in.  Previoussystemsfordetectionofsingingem-ployframe-by-frameclassificationondataconsistingprimarilyofWesternpopularmusic[5,6,7].Thesestud-iesemployMFCCfeatures,sometimeswithderivativesandotherspectralfeatures,combinedwithstatisticalmodelsusingcombinationsofNeuralNetworks,HMMs,GMMsorSVMstoclassifyintotwocategories:framescontainingsingingandnon-sungframes.Temporalsmoothingisoftenappliedtoreducelabeledregionfrag-mentation[6].Ourworkdiffersinthevarianceofbothacousticandmusicalconditionsofthetrainingandtest-ingdataandinthedetailedconsiderationofclassifierer-rors with respect to this variation.WembuandBaumann[8]suggestedthatSVMclassifiersyieldslightlybetterresultsthanHMMsandGMMs.Onlyafewstudiesusedframe-by-framelabelingofthegroundtruth[9]whichweconsidertobeessentialtoaccurateevaluation.TwoMIRstudiesarerelatedtosinginginnon-Westerncultures:singeridentificationinGreekRembetiko[9]andinSouthIndianCarnaticmusic[10].BothoftheseemployedMFCCs.Thelatterteamusedsignalseparationfordistinguishingbetweenvocaland instrumental frames. Otherstudiesinvolvingnon-Westernmusicre-cordingsconsidersinglemusicalculturesorrepertories,eachofwhichprovidessomehomogeneityofthemusic-almaterial.Severalresearchersperformedrhythmicana-lysisandclassificationbasedonbeatfeaturesforsuchrepertorieslikeMalay,Greek,CentralAfricantraditionalmusicaswellasAfro-Cubanmusic[11,12,13].Chordiaetal.[14]foundastatisticalmeasurebasedonpitchclassesthatdistinguishesbetweendifferentragas.Srid-harandGeetha[10]developedCarnaticintervalcepstralcoefficients(CICC),basedonthedivisionoftheoctavein22Sruti,tobettersuitthetonalitystructureoftheIndi-an Carnatic music. Holzapfeletal.[9]compiledadatabaseofRembetikosingersfortheirartistrecognitionexperi-ment.Theydeliberatelychoserecordingsthatareverysimilarinstyletoavoididentificationduetostylediffer-ences.Theylabelledtrainingdataframe-by-framewithonesecondwindowhop.Theyusedaggregate“worldmodel”GMMstodistinguishvocalfrominstrumentalframesusingintersectionofthemaximum-likelihoodandminimumlikelihoodframesofopposingclassifiers.Thistechniqueresultedinaclassificationaccuracyof99%.Thedatasetincludedhistoricalgrammophonerecord-ings,butthestudywasforahomogenousmusicalstyleover 21 Rembetiko singers.Cross-culturalMIRstudiesincludediscriminat-ingmoodtaxonomyofChinesetraditionalmusicandWesternclassicalmusic[15],retrievalthroughmetricsimilarityforGreekandCentralAfricanmusic[12]andastudyonmetricalambiguityinBossaNova,Gahu,Rumba, Soukous, Son, and Shiko [16].Wetakeamoregeneralapproach.Toestablishamodelforcross-culturalresearchwerequireasuffi-ciencyoftrainingdatatoaccountforvariancecausedbydifferenceinculturalorigins,inrecordingtechniquesandin musical textures.  3.VOCAL/INSTRUMENT CLASSIFICATION Thepurposeofourstudyistheevaluationofthebase-lineperformanceofwidely-usedMIRmethodsonaneth-nographicallydiversedatasetandtogaininsightintofu-tureresearchpotentialbyperformingadetailedanalysisofanymisclassifications.Wepreparedadatasetconsist-ingofexcerptstakenfromtheLomaxCantometricstrain-ingtapescollection[17,18]whichcontainsahighdegreeofcultural,technicalandtexturalvariancesincethedatawasoriginallycollectedtofindcorrelationsbetweenmu-sicalstyleandculturaltraitssuchassocialorganizationof a society. 3.1DataTheLomaxdatasetconsistedof1000tracksfromallovertheglobeincludingrecordingssungindifferentlan-guages,musicplayedon“exotic”instruments,singing,polyrhythmicaswellasrhythmicallyfreemelodies,non-diatonic,non-temperedscales,agreatdiversityofvoicetimbres,rhythms,harmoniesandtexturesfromhetero-phonictouncoordinated,withconsiderablevariationinthe social organization of the performing group.Forourexperimentsweused355ofapproxim-ately1000soundsamples.Ofthese355tracks297con-tainsingingand58arepurelyinstrumental.Ofthesingingtracks185area'capellasingingand112containaccompanyinginstruments;110aresungsolo,130arechoraland57containbothsoloandgroupsinging;166trackscontainprimarilymalesinging(soloorgroup),60femalesingingand51mixedmaleandfemalesinging,10aresungbychildren.Morethan50culturesarerepresen-tedinthedatabasefrom5continentsaswellasfromlargeandsmallislands.Instrumentsincludeallkindsofidiophones(rattles,drums,framedrums,sticks,gamelans,xylophones),aerophones(flutes,clarinets,trumpets,tuba,didgeridoo),chordophones(allkindsoflutes,zithers,bowchordophoneslikefiddlesandclassicalviolins).WereceivedtheaudioinMP3128kbt/sec,44,1kHz.Fileshaddurationsofbetween10and150seconds.Foreachaudiofileweextracted20-bandMelFrequencyCepstralCoefficients(MFCC).Thesewereex-tractedusingashort-timeFouriertransformwithhopsize100ms(2205samples),windowlength185.76ms(8192samples),FFTlength16384samples(2.69Hzfrequencybins).Forclassificationandevaluationwedevelopedtools in Matlab using the libsvm package [19].Audiofileswereannotatedatthewholetracklevelasbeingpredominantlysungacapella(sa),instru-mental(i)orsingingplusinstruments(si).Forasubsetweperformedframe-by-framelabellingat50msincre-ments:111forsinging(sa+si)vspurelyinstrumental(i)and77foracapellasinging(sa)vsinstrumentaloraccom-panied singing (i + si). \n76010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n3.2Frame-level and track-level classificationThefirstexperimentwastodeterminetheperformanceofframe-by-frametwo-classSVMclassifiersconsistingofa)frameswithsinging,consistingof(sa)acapellaandsingingwithinstruments(si),versesinstrumentsonly(io)andb)acapellaframes(sa)versusnon-acapellaframes;i.e.framescontainingsingingwithinstruments(si)andframescontaininginstrumentsonly(io).ThepositiveandnegativetraininglabelsusedforthebinarySVMclassifi-ers are summarized in Table 1. Buildingonthesetwoclassifiers,wedefinedathirdtasktoclassifywholetracksaspredominantlyacapellasinging(SA),instrumentsonly(IO)orsingingplusinstruments(SI).Themethodusedforthethirdtaskwaswhole-trackintegrationoftheframe-by-frametwo-class SVM classifier outputs from the first two tasks.ClassifierLabels +Labels -Sung Frame (s)(sa) (si) (io) Acapella Frame (sa)(sa) (si) (io)Table1:traininglabelsforbinaryclassifiersusedintheexperiments.Forthefirstroundofourexperimentweconductedleave-one-outcross-validationon36songslabelledframe-by-frameforsinging(s)vs.pureinstrumental(si)and30tracksforacapellasinging(sa)vsinstrumental(si).Wetestedonwholesongssothatthetestsetdidnotconsistof frames drawn from any training track.Inthesecondroundweused111labelledtracksforcross-validation(77tracksforpuresingingvsinstru-mentalclassifier).Weapplieddifferentwaysofprepro-cessingfeaturestoobtainbetterresults:i.e.removingthefirstMFCCband,unit-normingfeaturevectors,detectingandremovingquietframes.Wealsoincorporatedtempor-alaspectsoffeaturesintwoways:derivativesofthefea-turevectorsandconcatenationofsequencesofthreetofivefeaturevectors.Thesemodificationsdidnotinflu-ence our results significantly.Athirdexperimentwasconductedforthesung(s)vspureinstrumental(io)classifierforwhichwetrainedanSVMmodelonall111frame-by-framela-belledsongsandpredictedlabelsusingthismodelforatestsetof244newtracks,ofwhich237containedsingingand7werepurelyinstrumental.Weintegratedtheclassifieroutputlabelstoconstructwholesongpredic-tionsforthetestsetusing30%sungframesasathresholdforasungtrack.Wecomparedthesepredictionswithourmanual annotation of the test songs to evaluate accuracy.4.RESULTTheresultsaresummarizedinTables2-4.Thefirstexper-imentyieldedameanaccuracyof74%forthesungframeclassifierand77%fortheacapellaframeclassifier.Therewereanumberofproblemcaseswhichhadamajorim-pactonpredictionaccuracy,thesearediscussedinthenextsection.Whenrecordingsfromtheseproblemgroupswereremovedfromthedataset,themeanaccuracyofthecross-validationonremaining27songswas87.9%with18%standarddeviationforthesungframeclassifier,asubstantiveimprovement.Forthenextroundoftheex-perimentwepaidspecialattentiontotheseproblemcasesandincludedadditionaltrackswiththesecharacteristicsinto the training set.Forthethirdexperiment,theaccuracywas83.6%forsungtrackclassificationand62.2%ofacapellatracksinthecollectionwerecorrectlyidentified.However,97%oftrackslabeledassungcontainedsingingwhichmeansthatthefalsepositiverateforinstrumentaltrackswasim-pactingperformance.Wediscussthefalsepositivesinthenextsectionaswellaswhatmightbedonetoimproveclassifier performance.27tracks(problemcases excluded)Mean accuracy87.9%Std. deviation18.0%36 tracksMean accuracy73.6%Std. deviation26.0%111 tracksMean accuracy71.5%Std. deviation22.4%Mean recall singing83.9%Mean precision singing52.6%Mean recall pure instrumental76.1%Mean precision pure instrumental60.5%Table2:singingvs.pureinstrumentalclassifier:leave-one-out cross-validation results.27tracks(problemcases excluded)Mean accuracy85.3%Std. deviation19.5%30 tracksMean accuracy77.4%Std. deviation26.6%77 tracksMean accuracy71.1%Std. deviation22.4%Mean recall singing85.2%Mean precision singing51.9%Mean recall pure instrumental76.7%Mean precision pure instrumental58.9%Table3:acapellasingingvs.instrumentalclassifier:leave-one-out cross-validation results.Singingvspurein-strumental classifier Acapellasingingvsin-strumental classifier Nr training tracks11177Nr test tracks244278Accuracy83.61%62.23%Recall positive85.65%42.28%Precision positive97.13%76.83%Recall negative14.29%85.27%Precision negative 2.86%56.12%Table 4: whole track tests results.\n761Oral Session 9-B: Sociology and Ethnomusicology\n5.PROBLEM CASES ANALYSIS5.1False positives / false negatives analysisIngeneral,thereasonableaccuracyofourframe-by-frameclassifiers,whichisfurtherimprovedinthewholetrackpredictions,suggeststhiskindofclassificationcanbeachievedindependentlyoftheoriginofmusicalmater-ial and the variance in stylistic parameters.Similarresultsofthefirstandthesecondroundshowthatwewereabletoincludesomeofthevarianceofthedatawhichcausedproblemsinthefirstroundintothetrainingsetofthesecondround.Thefollowingsec-tionsoutlineouranalysisoftheerrorsofclassificationand what might have caused them.Thefollowinginstrumentalsoundswerepre-dictedbadlyduringthefirstexperimentandalsonegat-ivelyinfluencedtheabilityoftheclassifierstopredictsinging when they were present in the training set:1.woodwind instruments with a lot of “air” in thesound, like pan pipes also organs, mouth organs2.shortly plucked or hammered instruments, likembira, xylophone or marimba, some lutes3.thedominantinstrumentplayingthemelodywasmisclassified as singing 4.trackscontainingbothsingingandinstrumentswere misclassified as purely instrumental5.wellblendedchoralsingingwithawidepitchrange was misclassified as instrumental6.yodeling was misclassified as instrumentalCases3and4couldbepracticallyeliminatedinthesecondroundofcross-validationwithmoretrainingdata.Theperformanceoftheclassifierswiththeotherproblemcaseshasimprovedwithadditionaltrainingdata,butex-amplesofthesecaseswerestillpresentamongfalsepos-itives/false negatives in the second round.Inthesecondexperimentthereweresometracksthatweremisclassified:mbiraandmouthorgan,gamelansandxylophones,flutesandclarinets(especiallywhenplayedwhile“singing”intothem)could“cheat”theclassifieryieldingframe-by-frameaccuraciesof30%andlowerinthesecases.FalseinstrumenttalpositiveswerecausedbytheRussianstatechoirwithverywellblended,widerangingvocals;theraspy,narrow,heterophonicchoralsingingofMarajinfromAustralianArnhemland;agospelchoirwiththeroaringsoundofLouisArmstrongand an extremely high soprano voice. Anothergroupoftrackswithhadclassificationresultsclosetochance.Thesealsoincludedtheexamplesfrom above plus:i.choralsingingofcomplexinterlockedmotivicstructures;thisiswhatVictorGrauer[20]callspygmy/bushmenstyle.Alsointerlockedpanpipesplaying,whichheconsiderstobeevolu-tionary related to the pygmy/bushmen styleii.discoordinated singing in a big groupiii.fiddles,whistles,country/bluesguitarandmouthhariv.wind section of a classical orchestraThewholetracktestsproblemcasesanalysisexposedsimilarproblemcases(mbira,xylophones,flutes,disco-ordinatedsinging,narrow,lowpitchedvoice,complexpolyphonicchoralperformance)butalsointroducednewcaseswhichapparentlywerenotincludedintothetrain-ingdata,suchassingingwithstrongaccents,likee.g.NativeAmericansfromtheIroquoisConfederacy;voiceimitatinginstruments,e.g.percussion;sprechgesang(very fast spoken/sung text).Tosummarize,followingclassesofsoundsarelikely to cause misclassification:1. Instrumental:•Allkindsofidiophones:drums,percussions,rattles;xylophones;lamellohponeslikembira;gongs like gamelans•Aerophones:flutes,clarinets,whistles,panpipes(but not bagpipes), mouth harp, mouth organ •Fiddle and guitar, all kinds of lutes2. V ocal (solo and homophonic):•Singingvoicewithextremecharacteristics:veryloworveryhighpitched;verynarrow,nasalorraspy;voiceswithsignificantnon-harmoniccomponentsinthespectrum;brilliance(stronghigherfrequencycomponents)inthevoice;yo-deling.•V oiceimitatinginstrumentsorsingingwithverystrong accents•sprechgesang, very fast spoken/sung text3. Polyphonic textures•contrapuntal,heterophonic,interlockedaswellasdiscoordinatedperformancesinawiderange,by an orchestra and/or a choir Recordingqualityisanimportantfactorforclassificationaccuracy.ThoughthequalityofaudioonCantometricstrainingtapesismuchmorevariedthenofanymoderncollectionofclassicalorpopularmusic,itisconsiderablybetterthanmanyethnomusicologicaldatasets.Weob-servedmisclassificationofrecordingswithextremesounddistortion,butingeneraltheclassifierswereableto cope with significant variation in recording quality.Temporalchangespresumablyplayanimportantroleindistinguishingsinging.Asisknownfromspeechsignalprocessing,humanspeechaswellassingingcon-tainsspeechformantswhicharespecificforeachvocal('a','e','i','o','u')andchangefromsyllabletosyllable.Thiskindofchangeintheformantsisabsentinthespec-trum of practically all musical instruments.Wetriedincorporatingtemporalchanges,usingMFCCderivatives,intothefeaturesbutthatdidn'tshowanysignificantdifferenceintheresult.Thissuggestsus-ingashorterhopsizeandwindowsizeforourfeatures.Theshorterhopsizewillincreasethenumberoffeaturessothiswilllikelypushtherunningtimeforexperimentsabove acceptable durations.5.2Future ResearchAMIREX-likecomparisonofperformanceontheCanto-metrictrainingtapesdatasetwoulddeterminethebestandcheapestapproachandwoulduncovermodelsimpli-\n76210th International Society for Music Information Retrieval Conference (ISMIR 2009)\ncitlyrelyingonmusicalfeaturesofaspecificculturelikeWesternpopularmusics.Alsosystematicresearchintothefeatureselectionforthistypeofclassificationisneeded. ThenextstepinapproachingthequestionofgeneralizationwithrespecttoculturaloriginandmusicalstylewouldbetotesttheSVMmodelwehavetrainedonothermusicalcollections.Willwebeabletodetectsingingwithincollectionsofclassicalmusic,popularmu-sic,folksongs,non-Westernrecordings?Ifnot,howmuchtrainingdataismissing,whatkindofvarianceisnotcoveredbyourtrainingset?Isthegoalofhavingasingle model to detect singing in all music achievable? ThestatisticalframeworkusingSVMsofourex-perimentsisscalableforusewithtensofthousandsoftracks.Thetrainingon111trackstakesafewminutesandpredictiontakesabout1secpersongonacurrenthigh-endlaptop.Predictionrunssequentiallyoneverysong,thusthetestingisO(n)ofthenumberoftrackstobepre-dicted.Withthecurrentmodelweexpecttheapproximaterunningtimeofpredictionfor10000trackstobelessthan3hoursonoursystem.Assumingthegeneralityofthemodel,itallowsoursoftwaretobeappliedtomuchbigger collections of any musical style and origin.Itisalsoapparentthatthesamestatisticalinfra-structurecanbeusedtoautomaticallyclassifyotherframe-levelmusicalfeatures.Thiswillneedanewroundofframe-by-frameaswellaswholetracklabeling.Weplantousethisapproachtoclassifytrackswithsingingintosoloandchoralsinging,male,femaleandmixedsinging,todetectspecificstylepatternslikeyodelinganddrones.Italsosuggeststhatweshouldsegmentaudioac-cordingtothepredictionofmoregeneralclassifiersanddesignhierarchicalclassification,forinstancemale/fe-maleonsegmentswithsinging;orotherwisetousethissegmentationasapreliminarystepforothertechniques,such as pitch extraction for solo singing.Combiningthesefeatureswithmusicalparamet-ersobtainedbyothertechniques(suchastheamountofpercussivity)orofalargerscope(suchasaveragepitch)onewouldgetamulti-facetdescriptionofthemusicalstyleofatrack.SucharepresentationofamusicalstyleappliedtotheCantometricstrainingtapesopensupvari-ouspossibilities:tostudygeographicdistributionofamusicalparameter,acombinationofparametersorstylepatterns(e.g.choralvs.solosingingoryodel);torevisedelineationofmusiccultures;tostudythedynamicsofmusicalstylespreadandinfluence.Sincethisstylede-scriptioniscompactandcanbeextractedautomaticallyfromaudio,itiseasytoaddfurthertrackstothedatasetand continually refine this research. 6.CONCLUSIONInthispaperwepresentedourworkonmanualannota-tionofadiverseethnomusicologicalcollectionforthepurposesoftestingMIRtoolsforautomaticannotation.Weconductedthreeexperimentsthatservedtodemon-strategoodperformanceonthedatasetforthetaskofla-belingregionsofdifferenttypesofsungtracksversusnon-sungtracks.Wealsopresentedananalysisoferrorsthatsuggestsstrategiesforimprovingtheoverallaccur-acy of such classifiers.Wewouldliketoseethesemethodsappliedtolargerrealworldcollectionsinethnomusicologicalarchivesen-hancingaccesstoourculturalheritage.Practicallyeverypublicethnomusicologicalarchivehaspoorlyannotatedholdings.Alsosmallprivatearchivesaregrowingfastandcouldbenefitfromthiskindofautomaticannotation.Today,morethanever,technologicalinfrastructureisneededforthesetypesofrecordings:archivesarechal-lengedtoopenuptheircollections,tomakethem“user-friendly”,toprovidenotonlycontent,butaddedvaluelikeexpertise,easyaccessandfun.Havingawholecol-lectionannotatedinaconsistentwaywouldallowthedesignofnewuserinterfacesthatareabletographicallyrepresentstylepatternsandregions.Socialtaggingcouldbeusedtocountertheerrorsgeneratedinautomatican-notation.Combinedwitharchivists'expertiseandmoder-ation,thisapproachwillenablearchivestoclosegapsinannotationandofferhands-onactivitiestotheirusercom-munities.7.REFERENCES[1] R. Reigle, “Humanistic motivations in ethnomusicolo-gical recordings,” in Recorded Music - Philosoph-ical and Critical Reﬂections (M. Dogantan-Dack,ed.), Middlesex University Press, 2009. with CD[2] P. Proutskova, “Musical memory of the world - datainfrastructure in ethnomusicological archives,”Proceedings of the International Symposium onMusic Information Retrieval, 2007.[3] J. S. Downie, “Music information retrieval,” AnnualReview of Information Science and Technology,vol. 37, pp. 295–340, 2003.[4] G. Tzanetakis, A. Kapur, W. A. Schloss, andM. Wright, “Computational ethnomusicology,”journal of interdisciplinary music studies, vol. 1,no. 2, pp. 1–24, 2007.[5] G. Tzanetakis, “Song-speciﬁc bootstrapping ofsinging voice structure,” Multimedia and Expo,2004. ICME ’04. 2004 IEEE International Confer-ence on, vol. 3, pp. 2027– 2030, June 2004.[6] A. Berenzweig, D. Ellis, and S. Lawrence, “Usingvoice segments to improve artist classiﬁcation ofmusic,” AES 22nd International Conference, 2002.[7] A. Berenzweig and D. Ellis, “Locating singing voicesegments within music signals,” IEEE Workshopon Applications of Signal Processing to Audio andAcoustics, 2001.[8] S. Vembu and S. Baumann, “Separation of vocalsfrom polyphonic audio recordings,” Proceedingsof the International Symposium on Music Informa-tion Retrieval, 2005.[9] A. Holzapfel and Y . Stylianou, “Singer identiﬁcationin rembetiko music,” Sound and Music Comput-ing, 2007.[10] R. Sridhar and T. Geetha, “Music information re-trieval of carnatic songs based on carnatic musicsinger identiﬁcation,” Computer and ElectricalEngineering, 2008. ICCEE 2008. InternationalConference on, pp. 407 – 411, Dec 2008.[11] S. Doraisamy, S. Golzari, N. M. Norowi, N. Sulai-man, and N. I. Udzir, “A study on feature selectionand classiﬁcation techniques for automatic genreclassiﬁcation of traditional malay music,” Pro-\n763Oral Session 9-B: Sociology and Ethnomusicology\nceedings of the International Symposium on MusicInformation Retrieval, 2008.[12] I. Antonopoulos, A. Pikrakis, S. T. O. Cornelis,D. Moelants, and M. Leman, “Music retrieval byrhythmic similarity applied on greek and africantraditional music,” Proceedings of the Internation-al Symposium on Music Information Retrieval,2007.[13] M. Wright, G. Tzanetakis, and A. Schloss, “Analyz-ing afro-cuban rhythm using rotation-aware clavetemplate matching with dynamic programming,”Proceedings of the International Symposium onMusic Information Retrieval, 2008.[14] P. Chordia and A. Rae, “Raag recognition usingpitch-class and pitch-class dyad distributions,”Proceedings of the International Symposium onMusic Information Retrieval, 2007.[15] W. Wu and L. Xie, “Discriminating mood taxonomyof chinese traditional music and western classicalmusic with content feature sets,” Image and Sig-nal Processing, 2008. CISP ’08. Congress on,vol. 5, pp. 148 – 152, May 2008.[16] G. Toussaint, “A mathematical analysis of african,brazilian, and cuban clave rhythms,” Proceedingsof BRIDGES: Mathematical Connections in Art,Music and Science, pp. 157–168, 2002.[17]A. Lomax,FolkSongStyleandCulture.NewBrun-swick, New Jersey: Transaction Books, 1968.[18] A. Lomax, Cantometrics: An Approach To The An-thropology Of Music. The University of Califor-nia, 1976. accompanied by 7 cassettes.[19] C.-C. Chang and C.-J. Lin, LIBSVM: a library forsupport vector machines, 2001. Software availableat http://www.csie.ntu.edu.tw/cjlin/libsvm.[20] V . Grauer, “Echoes of our forgotten ancestors,” TheWorld Of Music, vol. 2, 2006.s\n764"
    },
    {
        "title": "Learning to Control a Reverberator Using Subjective Perceptual Descriptors.",
        "author": [
            "Zafar Rafii",
            "Bryan Pardo"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418035",
        "url": "https://doi.org/10.5281/zenodo.1418035",
        "ee": "https://zenodo.org/records/1418035/files/RafiiP09.pdf",
        "abstract": "The complexity of existing tools for mastering audio can be daunting. Moreover, many people think about sound in individualistic terms (such as “boomy”) that may not have clear mappings onto the controls of existing audio tools. We propose learning to map subjective audio descriptors, such as “boomy”, onto measures of signal properties in order to build a simple controller that manipulates an audio reverberator in terms of a chosen descriptor. For example, “make the sound less boomy”. In the learning process, a user is presented with a series of sounds altered in different ways by a reverberator and asked to rate how well each sound represents the audio concept. The system correlates these ratings with reverberator parameters to build a controller that manipulates reverberation in the user’s terms. In this paper, we focus on developing the mapping between reverberator controls, measures of qualities of reverberation and user ratings. Results on 22 subjects show the system learns quickly (under 3 minutes of training per concept), predicts users responses well (mean correlation coefficient of system predictiveness 0.75) and meets users’ expectations (average human rating of 7.4 out of 10).",
        "zenodo_id": 1418035,
        "dblp_key": "conf/ismir/RafiiP09",
        "keywords": [
            "learning",
            "mapping",
            "subjective audio descriptors",
            "audio reverberator",
            "user ratings",
            "reverberator controls",
            "measures of qualities",
            "reverberation",
            "controller",
            "reverberator parameters"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nLEARNING TO CONTROL A REVERBERATOR\nUSING SUBJECTIVE PERCEPTUAL DESCRIPTORS\nZafar Raﬁi\nEECS Department\nNorthwestern University\nEvanston, IL, USA\nZafarRafii2011@u.northwestern.eduBryan Pardo\nEECS Department\nNorthwestern University\nEvanston, IL, USA\npardo@northwestern.edu\nABSTRACT\nThe complexity of existing tools for mastering audio can\nbe daunting. Moreover, many people think about sound in\nindividualistic terms (such as “boomy”) that may not have\nclear mappings onto the controls of existing audio tools.\nWe propose learning to map subjective audio descriptors,\nsuch as “boomy”, onto measures of signal properties in or-\nder to build a simple controller that manipulates an audio\nreverberator in terms of a chosen descriptor. For example,\n“make the sound less boomy”. In the learning process, a\nuser is presented with a series of sounds altered in differ-\nent ways by a reverberator and asked to rate how well each\nsound represents the audio concept. The system correlates\nthese ratings with reverberator parameters to build a con-\ntroller that manipulates reverberation in the user’s terms.\nIn this paper, we focus on developing the mapping be-\ntween reverberator controls, measures of qualities of re-\nverberation and user ratings. Results on 22 subjects show\nthe system learns quickly (under 3 minutes of training per\nconcept), predicts users responses well (mean correlation\ncoefﬁcient of system predictiveness 0.75) and meets users’\nexpectations (average human rating of 7.4 out of 10).\n1. INTRODUCTION\nIn recent decades, many audio production tools have been\nintroduced to enhance and facilitate music creation. Of-\nten, these tools are complex and conceptualized in terms\n(“high cut”,“density”) that are unfamiliar to many users.\nThis makes learning these tools daunting, especially for\ninexperienced users.\nOne solution would be to redesign the standard inter-\nfaces to manipulate audio in terms of commonly used de-\nscriptors (e.g. “warm” or “enveloping”). This can be prob-\nlematic, since the meanings of many words used to de-\nscribe sound differ from person to person or between dif-\nferent groups [1]. For example, the audio signal proper-\nties associated with “warm” and “clear” have been shown\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.to vary between English speakers from the UK and the\nUS [2]. Since it may not be possible to create general con-\ntrollers for terms whose meaning varies between groups,\nwe propose mapping descriptive terms onto the controls\nfor audio tools on a case-by-case basis.\nWhile there has been much work on adaptive user inter-\nfaces [3], there has been relatively little on personalization\nof audio tools. A previous study showed success in per-\nsonalizing an equalization tool [4]. Here, we propose to\nsimplify and personalize the interface to one of the most\nwidely applied classes of audio effect: Reverberation .\nReverberation is created by the reﬂections of a sound\nin an enclosed space causing a large number of echoes to\nbuild up and then slowly decay as the sound is absorbed by\nthe walls and air [5]. The reﬂections modify the perception\nof the sound, its loudness, timbre and spatial characteris-\ntics [6]. Reverberation can be simulated using multiple\nfeedback delay circuits to create a large, decaying series\nof “echoes” [7], and many reverberation tools have been\nbuilt. Fig. 1 shows the interface of a typical reverberation\ntool. Note the 7 buttons and 14 sliders that control parame-\nters (such as “density”) whose meaning in this context are\nunfamiliar to the average person and to many musicians.\nFigure 1 .Logic Audio ’sPlatinumverb complex interface.\nWe propose a system that learns an audio concept a user\nhas in mind (“boomy”, for example) and builds a simple\n285Poster Session 2\nreverberation controller to manipulate sound in terms of\nthat descriptor. By automatically adapting the interface to\nan individual user’s conceptual space, we hope to bypass\nthe creative bottleneck caused by complex interfaces and\nindividual differences in the meaning of descriptive terms.\nThe paper is organized as follows. The method used to\nmap descriptive terms onto audio signal characteristics is\ndescribed in Section 2. Section 3 presents the reverbera-\ntion control used to perceptually alter sound. Experimen-\ntal evaluation of the approach is described in Section 4.\nFinally, conclusions are given in Section 5.\n2. LEARNING DESCRIPTIVE TERMS\nWe now give an overview of the process by which the sys-\ntem learns to build a controller that controls the reverbera-\ntion of a signal in terms of a user-deﬁned descriptive word.\n2.1 The Training Process\nIn the training process, the user is presented with the Per-\nceptual Learner interface shown in Fig. 2. The user selects\na descriptive word (such as “boomy” or “church-like”) to\nteach the system. The user is then presented with a series of\naudio examples generated from an original audio ﬁle and\nprocessed by the reverberator using a variety of reverbera-\ntion settings. The reverberation settings used are chosen to\nexplore the space of likely parameter settings for a digital\nreverberator, as described in Section 4.\nThe user moves a slider to rate each audio example on\nhow well it represents the audio descriptor. Ratings range\nfrom 1 (captures the concept perfectly) to -1 (does not cap-\nture it at all). Training typically takes about 30 ratings\n(around two minutes for a ﬁve-second ﬁle). Fig. 3 illus-\ntrates the process.\nFigure 2 . Interface of the Perceptual Learner.\n2.2 Mapping Signal Statistics to User Ratings\nThe system collects ﬁve impulse response measures (de-\nscribed in Section 3.2) for the reverberation applied to each\nexample rated by the user. Once user ratings are collected,\nthe system relates user ratings to each of the ﬁve measures\nusing linear regression . This lets us build a model that\npredicts the expected user rating, given a reverberation im-\npulse response signal characterized by these measures.\nThis mapping is used to build a controller that lets the\nuser easily manipulate the audio in terms of the descriptor\nFigure 3 . The training process: (1) audio examples are\ngenerated from an original sound using a reverberator set to\na variety of parameter settings (5 control parameters shown\nby 5 different bars); (2) the user listen to the audio exam-\nples and uses a slider to rate how well each one ﬁts the\naudio concept she/he has in mind.\n(such as “boomy”) using a simple slider as shown in Fig.\n4. This slider affects all ﬁve reverberation measures in par-\nallel, although not necessarily in the same direction. For\nexample, “boomy” may be positively correlated with cen-\ntral time and negatively correlated with spectral centroid.\nFigure 4 . Interface of the Perceptual Controller.\n3. THE REVERBERATION CONTROL\nTo build the new interface, we must map human feedback\nto reverberation controls. We do not, however, map user\nfeedback directly to parameters for a speciﬁc reverberator,\nbut onto measures of the reverberation (Section 3.2). This\nlets us use mappings learned using one reverberator to con-\ntrol another one, chosen later. The only requirement is that\nboth reverberators have known mappings between control\nparameters and reverberation measures.\n3.1 The Digital Reverberator\nThe approach we describe, while not tied to any particular\nreverberation approach, works best if the reverberator can\ngenerate a wide variety of impulse response functions on\nthe ﬂy. Thus, rather than use a convolution reverberator\nthat selects from a ﬁxed library of impulse responses, we\nhave developed a digital stereo reverberation unit inspired\nby Moorer’s work [8]. The reverberator, shown in Fig. 5,\nis easy to manipulate through the control parameters. The\nreverberation measures described in Section 3.2 can be de-\nrived easily as functions of those parameters. This is im-\nportant for learning a mapping between human feedback\nand reverberator settings.\n28610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 5 . The digital stereo reverberation unit.\nThe reverberator uses six comb ﬁlters in parallel to sim-\nulate the complex modal response of a room by adding\nechoes together. Each comb ﬁlter is characterized by a\ndelay factor dkand a gain factor gk(k=1..6). The delay\nvalues are distributed linearly over a ratio of 1:1.5 with a\nrange between 10 and 100 msec, so that the delay of the\nﬁrst comb ﬁlter d1, deﬁned as the longest one, determines\nthe other delays. The gain factor of the ﬁrst comb ﬁlter g1\nhas the smallest gain and has a range of values between 0\nand 1. Although a comb ﬁlter gives a non-ﬂat frequency re-\nsponse, a sufﬁcient number of comb ﬁlters in parallel with\nequal values of reverberation time helps to reduce the spec-\ntral coloration.\nAnall-pass ﬁlter is added in series to increase the echo\ndensity produced by the comb ﬁlters without introducing\nspectral coloration, and doubled into two channels to sim-\nulate a more “natural sounding” reverberation in stereo.\nThe all-pass ﬁlter is characterized by a delay factor daof\n6 msec and a gain factor gaﬁxed to1√\n2. A small differ-\nence mis introduced between the delays to insure a dif-\nference between the channels, therefore the delays become\nd7=da+m\n2for the left channel and d8=da−m\n2for\nthe right channel. The range of values for m=d7−d8is\nthen deﬁned between 0 and 12 msec. Note that to prevent\nexactly overlapping echoes, the delay values for the comb\nand the all-pass ﬁlters are set to the closest inferior prime\nnumber of samples.\nTo simulate air and walls absorption, a ﬁrst-order low-\npass ﬁlter of gain gcdeﬁned from its cut-off frequency fc\nis added at each channel [9]. fcranges between 0 and half\nof the frequency sampling fs. Finally, a gain parameter\nG,whose range of values is between 0 and 1, controls the\nwet/dry effect. In summary, a total of only ﬁve independent\nparameters are needed to control the reverberator: d1,g1,\nm,fcandG. The other parameters can be deduced from\nthem according to the relations above.\n3.2 The Reverberation Measures\nWe now deﬁne ﬁve measures commonly used to character-\nize reverberation and describe formulae to estimate values\nfor these measures in terms of the parameters for our re-\nverberator. For details on how we derive these formulae,\nwe refer the reader to [10].\n•Reverberation Time (T60) is deﬁned as the time in sec re-quired for the reﬂections of a direct sound to decay by 60\ndB below the level of the direct sound [5]. Based on the re-\nverberation time of the comb ﬁlter and the other gains, we\nestimated the reverberation time of the whole reverberation\nunit as follows in Eq. 1.\nT60= max\nk=1..6/parenleftbigg\ndklog/parenleftbigg10−3\nga(1−gc)G/parenrightbigg\n/loggk/parenrightbigg\n(1)\n•Echo Density (Dt) is deﬁned as the number of echoes per\nsecond at a time t. In practice, we computed the average\nechoes per second between time 0 and time t. We estimated\nthe echo density of the whole reverberation unit at time t =\n100 msec, as a combination of echo densities of the digital\nﬁlters, as follows in Eq. 2.\nD=t\nda6/summationdisplay\nk=11\ndk(2)\n•Clarity (Ct) describes the ratio in dB of the energies in\nthe impulse response pbefore and after a given time t. It\nprovides indication of how “clear” the sound is [11]. The\ndeﬁnition of Ctin discrete time is given by Eq. 3.\nCt= 10 log10/parenleftBiggt/summationdisplay\nn=0p2[n]/∞/summationdisplay\nn=tp2[n]/parenrightBigg\n(3)\nWe estimated the clarity of the whole reverberation unit at\nt = 0, the arrival time of the direct sound, as shown in Eq.\n4, assuming that the total energy of the reverberator is a\nlinear combination of the energies of its ﬁlters.\nC=−10 log10/parenleftBigg\nG21−gc\n1 +gc6/summationdisplay\nk=1gk2\n1−gk2/parenrightBigg\n(4)\n•Central Time (TC) is the “center of gravity” of the energy\nin the impulse response p, [11], deﬁned in discrete time by\nEq. 5.\nTC=∞/summationdisplay\nn=0np2[n]/∞/summationdisplay\nn=0p2[n] (5)\nBased on the same assumption as for clarity, we estimated\nthe central time of the whole reverberation unit as the com-\nbination of central times of the ﬁlters, as follows in Eq. 6.\nTC=6/summationdisplay\nk=1dkgk2\n(1−gk2)2/6/summationdisplay\nk=1gk2\n1−gk2+da (6)\n•Spectral Centroid (SC) is the “center of gravity” of the en-\nergy in the magnitude spectrum Pof the impulse response\np, deﬁned in discrete time by Eq. 7, where fsis the sam-\npling frequency.\nSC=fs/2/summationdisplay\nn=0nP2[n]/fs/2/summationdisplay\nn=0P2[n] (7)\nWe estimated the spectral centroid of the whole reverber-\nation unit from the characteristics of its low-pass ﬁlter, as\n287Poster Session 2\nfollows in Eq. 8.\nSC=fs/2/summationdisplay\nn=0n\n1 +gc2−2gccos(2πn)\nfs/2/summationdisplay\nn=01\n1 +gc2−2gccos(2πn)(8)\nBased on the relations between the parameters deﬁned\nin section 3.1, the measures can be redeﬁned as ﬁve func-\ntions of ﬁve independent parameters: T60(d1,g1,fc,G),\nD(d1,m),C(g1,fc,G),TC(d1,g1,m)andSC(fc).\nNote that these functions are not entirely invertible, es-\npecially for d1andg1. When necessary, we estimate d1and\ng1from a reverberation measure by using tables of values.\n4. EV ALUATION\nWe have implemented the system in Matlab on a PC with\nan Intel Core2 Quad CPU of 2.66GHz and 6GB of RAM.\nThe system was evaluated by 22 participants, 14 males and\n8 females, between the ages of 18 and 29. All reported\nnormal hearing and were native English speakers. 10 had a\nlittle or no musical background and 12 had a strong musi-\ncal background i.e. practicing one or several instruments,\nmore than 1 hour per week and for more than 10 years, or\nmore than 6 hours per week and for more than 6 years.\nAll audio examples created were based on a 5.5 sec ane-\nchoic recording of an unaccompanied singing male sam-\npled at 44,100 Hz. Prior to the study, a database of 1024\nimpulse response functions was generated using the rever-\nberator described in Section 3.1. These impulse response\nfunctions were selected to evenly cover a range of the ﬁve\nreverberation measures (Section 3.2). The Reverberation\nTime ranged from 0.5 to 8 sec, the Echo Density from 500\nto 10,000 echoes/sec, the Clarity from -20 to 10 dB, the\nCentral Time from 0.01 to 0.5 sec, and the Spectral Cen-\ntroid from 200 to 11,025 Hz (no low-pass ﬁltering). These\nranges were chosen by audio inspection so that they evenly\ncover a range of “good” values in the space of reverbera-\ntion measures leading to natural sounding reverberation.\n4.1 Experiment\nStudy participants were seated in a quiet room with a com-\nputer that controlled the experiment and recorded the re-\nsponses. The stimuli were presented binaurally over head-\nphones. Participants were allowed to adjust the sound level\nprior to starting the study. Prior to beginning the study, par-\nticipants were quickly trained on the task. Each participant\nparticipated in a single one-hour session.\nEach participant was asked to rate the same ﬁve de-\nscriptive words: bright ,clear (two common audio descrip-\ntors), boomy (often related to reverberation), church-like\nandbathroom-like (related to models of space, respectively\na church and a bathroom). These words were presented to\neach participant in a random order. For each descriptive\nword, the participant was asked to perform three tasks.\nFirst, the participant was asked to rate a series of 60 au-\ndio examples. For each example the participant heard theaudio modiﬁed by an impulse response function. The par-\nticipant moved an on-screen slider (Fig. 2) to indicate the\nextent to which each sound exempliﬁed the current word\ndescriptor. Values ranged from 1 (captures the concept per-\nfectly) to -1 (does not capture it at all). These 60 audio\nexamples contained 35 examples chosen randomly from\nour database of 1024 examples. We then duplicated 25\nof the 35 and added the duplicates to the set in random\norder, for a total of 60 examples. The 25-example dupli-\ncate set was used to measure consistency of user responses,\nwhile the 35-example training set was for system training.\nA previous study showed that around 25 examples are suf-\nﬁcient to model a user’s preferences for an equalization\ncontroller [4], which is a closely related task.\nOnce the ﬁrst task was completed, the system created a\nmodel of the effect of each reverberation measure on the\nuser ratings, as described in Section 2.2. The data set used\nwas the user ratings of the 35 non-duplicate examples in\nthe ﬁrst task. The new model was used to select a new set\nof audio examples. This set contained 11 audio examples\nchosen to evenly cover the range of user ratings from -1 to\n1 (as predicted by the learned model) and 14 audio exam-\nples selected at random, for a total of 25. The participant\nwas asked to rate the 25 new audio examples as she/he did\nin the ﬁrst part.\nFinally, the system used the learned model to build a\nslider (Fig. 4) that controls reverberation in terms of the\nlearned descriptor. The controller mapped 11 audio exam-\nples chosen to evenly cover the range of user ratings from\n-1 to 1 onto slider positions. As the slider is moved to a new\nlocation, a different variant of the sound is played. This let\nthe participant move the slider to change the degree of the\neffect. The user was asked to play with the controller for\nas long as neccesary to get a feel for how well it worked.\nThe user was then asked to rate how well it manipulated\nthe sound in terms of the learned descriptive word. Human\nratings ranged from 0 (really bad) to 10 (really good).\n4.2 Results\nThe average training time, over all the descriptive words\nand the participants, was 4 min 2 sec. Since only 35 of\nthe 60 user-ratings in the ﬁrst task were actually used for\ntraining the system, a model for a descriptive word was\nlearned in only 2 min 20 sec of training (the mean time for\na user to rate for 35 examples).\nUser consistency on a descriptive word was measured\nby computing the within-user correlation coefﬁcient on rat-\ning the 25 pairs of duplicate examples in the ﬁrst task. Av-\nerage user consistency over all words and users was 0.65.\nSystem predictiveness (how well the system learned) for\na descriptive word was measured by computing the corre-\nlation coefﬁcient between the user’s observed ratings and\nthe system’s prediction of the user ratings on the second set\nof user-rated examples. System predictiveness was 0.75,\naveraged over all words and users.\nSystem predictiveness was measured on a different data\nset than user consistency, so the results are not directly\ncomparable. That said, the consistency of user ratings on\n28810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nmatched pairs of stimuli gives an indication that one might\nnot be able to expect signiﬁcantly better predictive power\nthan that shown by our approach.\nAverage human rating over all words and users given\nto the ﬁnal controller was 7.4 out of 10. This means that\noverall, the participants felt the system succeeded in pro-\nviding a controller that lets the user manipulate the sound\nin terms of the descriptive words.\nMean correlation coefﬁcients between user ratings and\neach of the ﬁve control measures (Section 3.2) used to gen-\nerate the audio examples are shown in Tables 1 and 2. Ta-\nble 1 shows values for the 10 participants with little or no\nmusical background. Table 2 shows these values for the 12\nparticipants with strong musical background.\nbright clear boomy bath’ church\ntraining time 4’07” 3’34” 4’35” 4’29” 4’57”\nReverb. Time -0.02 0.19 -0.07 0.02 0.08\nEcho Density -0.01 -0.10 0.13 0.06 0.03\nClarity 0.39 0.33 0.08 0.06 0.16\nCentral Time -0.45 -0.51 0.28 -0.22 0.52\nSpec. Centroid 0.36 0.38 -0.23 0.08 0.02\nTable 1 . Average training time and correlation coefﬁcients\nof the measures for the ﬁve descriptive words over the par-\nticipants with little or no musical background.\nbright clear boomy bath’ church\ntraining time 3’14” 3’42” 4’17” 4’05” 3’36”\nReverb. Time 0.03 0.21 -0.04 -0.04 0.06\nEcho Density -0.02 -0.06 0.06 0.01 0.02\nClarity 0.29 0.44 0.14 -0.08 -0.03\nCentral Time -0.17 -0.57 0.46 0.06 0.70\nSpec. Centroid 0.42 0.29 -0.21 0.13 -0.03\nTable 2 . Average training time and correlation coefﬁcients\nof the measures for the ﬁve descriptive words over the par-\nticipants with strong musical background.\nAs we can see, participants with strong musical back-\nground completed the training more quickly. Both groups\nshowed similar results for the user consistency , the sys-\ntem predictiveness and the human ratings . However there\nare relevant differences between these two groups in which\nsignal measures most affect ratings of examples.\nFor both groups, bright andclear show overall a cor-\nrelation with the Clarity and the Spectral Centroid , and a\nnegative correlation with the Central Time . Table 1 indi-\ncates that participants with little or no musical background\nmay have confounded bright andclear , while they seem\ndistinct to people with strong musical background. In-\ndeed, we should expect bright to be more correlated with\ntheSpectral Centroid andclear with the Clarity , as shown\nin Table 2 by participants with strong musical background.\nThat said, user consistency ,system predictiveness and\nhuman ratings are reasonably high on these words for both\ngroups, even though the deﬁnitions of these words clearly\nvary between groups. These results indicate people withlittle musical experience can still deﬁne these terms with\nenough consistency for the system to model their prefer-\nences and provide a useful controller.\nBoomy shows a signiﬁcant correlation with the Central\nTime (in bold) and a negative correlation with the Spec-\ntral Centroid . Participants with strong musical background\nshowed higher correlation with the Central Time . Further-\nmore, the distribution of the correlation coefﬁcients of the\nmeasures for participants with strong musical background\nhas a smaller standard deviation, which means that they\nshowed a common understanding of the concept, while the\nstandard deviation for participants with a little or no mu-\nsical background is higher, especially for the Central Time\nand the Spectral Centroid , which means that the deﬁnition\nof the concept varied more greatly between them.\nTable 3 highlights how well the system performs on a\ndescriptive word where there was substantial disagreement\nbetween individuals. The table compares the correlation\ncoefﬁcients of the measures, the system predictiveness cor-\nrelation coefﬁcients, and the human ratings between four\nparticipants with little or no musical background for the\ndescriptive word “boomy”.\nboomy user 11 user 12 user 13 user 22\nReverb. Time 0.01 -0.04 -0.10 -0.18\nEcho Density 0.26 -0.08 0.24 0.01\nClarity -0.43 0.10 0.14 0.36\nCentral Time -0.33 -0.17 0.69 -0.32\nSpec. Centroid -0.74 -0.58 -0.15 0.17\npredictiveness 0.90 0.77 0.86 0.79\nhuman ratings 7.0 10.0 8.0 8.0\nTable 3 . Comparison of the results between four partici-\npants with little or no musical background for boomy (the\nhighest correlation coefﬁcient is in bold and the highest\nnegative correlation coefﬁcient is in italic, for each user).\nWe can see that the correlation coefﬁcients of the audio\nmeasures are very different from one participant to another,\nand yet, the system predictiveness and the human ratings\nare high. Again, this indicates our approach worked well\nto personalize a controller for each of these individuals,\ndespite the variation in their personal deﬁnition of boomy .\nParticipants showed great variation in their responses\ntobathroom-like and distributions of the correlation coef-\nﬁcients between acoustic measures and user ratings show\nhigh standard deviation, especially for the Clarity , theCen-\ntral Time and the Spectral Centroid . Table 4 compares\nthe results for bathroom-like between four different partic-\nipants: users 03 and 08 have a strong musical background,\nand users 12 and 13 have little or no musical background.\nCorrelation coefﬁcients of the measures are very different\nbetween participants, yet the system predictiveness and the\nhuman ratings are high.\nChurch-like shows overall a high correlation with the\nCentral Time (in bold), especially for participants with a\nstrong musical background. The distribution of the correla-\ntion coefﬁcients of the measures show also signiﬁcant stan-\ndard deviation, especially for participants with little or no\n289Poster Session 2\nbathroom-like user 03 user 08 user 12 user 13\nReverb. Time -0.14 0.13 0.05 0.07\nEcho Density 0.10 -0.09 0.21 0.17\nClarity -0.02 0.12 0.25 -0.63\nCentral Time 0.78 -0.44 0.01 0.74\nSpec. Centroid -0.27 0.47 0.60 -0.09\npredictiveness 0.83 0.77 0.81 0.93\nhuman ratings 7.0 8.0 10.0 7.0\nTable 4 . Comparison of the correlation coefﬁcients of the\nmeasures between four different users for bathroom-like .\nmusical background. The same conclusions can be drawn\nhere: participants have their own way of understanding the\nconcept, and overall the system succeeds in grasping it to\nbuild a controller which meets participants’ expectations.\nOverall, clear shows the best mean results across all\nusers: user consistency , 0.73, system predictiveness , 0.85,\nandhuman rating , 8.5. Overall, bathroom-like shows the\nworst results: user consistency , 0.62, system predictive-\nness, 0.62, and human rating , 6.8. Fig. 6 shows the dis-\ntributions over all the participants of the user consistency\nandsystem predictiveness correlation coefﬁcients, and the\nhuman ratings forclear andbathroom-like .\nFigure 6 . Left boxplot: distributions of user consistency\nandsystem predictiveness correlation coefﬁcients for the\nbest performing word: clear (left) and the worst perform-\ning word: bathroom (right) ; right boxplot: distributions of\nhuman ratings forclear (left) and bathroom (right).\n5. CONCLUSION\nA method for mapping subjective terms onto perceptual\naudio measures useful for digital reverberation control has\nbeen presented. This lets us build a simple controller to\nmanipulate sound in terms of a subjective audio concept,\nbypassing the bottleneck of complex interfaces and indi-\nvidual differences in descriptive terms. The evaluation of\nour system showed that audio descriptors can be effectively\nand rapidly learned and controlled with this method.\nOur study showed that people have different deﬁnitions\nof the same descriptor, and yet our system succeeds inlearning an individual’s concept so that people are satis-\nﬁed with the ﬁnal controller. This supports our contention\nthat individualizing controllers is a useful approach.\nThere are a number of directions we expect to take in\nthis work. We wish to conduct a more grounded psychoa-\ncoustic study to determine meaningful ranges for the set of\nreverberation measures. Finally, joint learning of controls\nfor multiple audio effects (reverberation and equalization,\nfor example) can be considered, to span a wider range of\npossible manipulations of sound. This work was supported\nby NSF grant number IIS-0757544.\n6. REFERENCES\n[1] Mihir Sarkar, Barry Vercoe, and Yang Yang. “Words\nthat Describe Timbre, A Study of Auditory Perception\nThrough Language,” Language and Music as Cogni-\ntive Systems Conference , Cambridge, UK, May 2007.\n[2] Alastair C. Disley and David M. Howard. “Spec-\ntral correlates of timbral semantics relating to the\npipe organ,” Joint Baltic-Nordic Acoustics Meeting ,\nMariehamn, Aland, Finland, 8-10 June 2004.\n[3] Victor Alvarez-Cortes, Benjamin E. Zayas-Perez, Vic-\ntor Huga Zarate-Silva, and Jorge A. Ramirez Uresti.\n“Current Trends in Adaptive User Interfaces: Chal-\nlenges and Applications,” Electronics, Robotics and\nAutomotive Mechanics Conference , pp. 312-317, 2007.\n[4] Andrew T. Sabin and Bryan Pardo. “Rapid learning\nof subjective preference in equalization,” 125th Audio\nEngineering Society Convention , San Francisco, CA,\nUSA, 2-5 October 2008.\n[5] Carl R. Nave. HyperPhysics , Georgia State Univer-\nsity, Atlanta, GA, USA, 2006, http://hyperphysics.phy-\nastr.gsu.edu/hbase/hph.html.\n[6] Pavel Zahorik. “Perceptual Scaling of Room Reverber-\nation,” Journal of the Acoustical Society of America ,\nV ol. 15, No. B, pp. 2598-2598, 2001.\n[7] Manfred R. Schroeder and Benjamin F. Logan. “’Col-\norless’ Artiﬁcial Reverberation,” Journal of the Audio\nEngineering Society , V ol. 9, No. 3, July 1961.\n[8] James A. Moorer. “About This Reverberation Busi-\nness,” Computer Music Journal , July 1979.\n[9] Fernando A. Beltr ´an, Jos ´e R. Beltr ´an, Nicolas Holzem,\nand Adrian Gogu. “Matlab Implementation of Rever-\nberation Algorithms,” Journal of New Music Research ,\nV ol. 31, No. 2, pp. 153-161, June 2002.\n[10] Zafar Raﬁi and Bryan Pardo. “A Digital Reverbera-\ntor controlled through Measures of the Reverberation,”\nNorthwestern University, EECS Department, Techni-\ncal Report NWU-EECS-09-08, 2009.\n[11] Fons Adriaensen. “Acoustical Impulse Response Mea-\nsurement with ALIKI,” 4th International Linux Audio\nConference , Karlsruhe, Germany, 27-30 April 2006.\n290"
    },
    {
        "title": "Symbolic and Structural Representation of Melodic Expression.",
        "author": [
            "Christopher Raphael"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417817",
        "url": "https://doi.org/10.5281/zenodo.1417817",
        "ee": "https://zenodo.org/records/1417817/files/Raphael09.pdf",
        "abstract": "A method for expressive melody synthesis is presented seeking to capture the structural and prosodic (stress, direction, and grouping) elements of musical interpretation. The interpretation of melody is represented through a hierarchical structural decomposition and a note-level prosodic annotation. An audio performance of the melody is constructed using the time-evolving frequency and intensity functions. A method is presented that transforms the expressive annotation into the frequency and intensity functions, thus giving the audio performance. In this framework, the problem of expressive rendering is cast as estimation of structural decomposition and the prosodic annotation. Examples are presented on a dataset of around 50 folk-like melodies, realized both from hand-marked and estimated annotations.",
        "zenodo_id": 1417817,
        "dblp_key": "conf/ismir/Raphael09",
        "keywords": [
            "expressive melody synthesis",
            "structural decomposition",
            "prosodic annotation",
            "audio performance",
            "time-evolving frequency",
            "intensity functions",
            "folk-like melodies",
            "hand-marked annotations",
            "estimated annotations",
            "problem of expressive rendering"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSYMBOLIC AND STR UCTR UALREPRESENT ATION OFMELODIC\nEXPRESSION\nChristopher Raphael\nSchool ofInformatics andComputing\nIndiana Univ.,Bloomington\nABSTRA CT\nAmethod forexpressi vemelody synthesis ispresented seek-\ningtocapture thestructural andprosodic (stress, direction,\nandgrouping) elements ofmusical interpretation. Thein-\nterpretation ofmelody isrepresented through ahierarchical\nstructural decomposition andanote-le velprosodic annota-\ntion. Anaudio performance ofthemelody isconstructed\nusing thetime-e volving frequenc yandintensity functions.\nAmethod ispresented thattransforms theexpressi veanno-\ntation intothefrequenc yandintensity functions, thus giv-\ningtheaudio performance. Inthisframe work,theproblem\nofexpressi verendering iscast asestimation ofstructural\ndecomposition andtheprosodic annotation. Examples are\npresented onadataset ofaround 50folk-lik emelodies, re-\nalized both from hand-mark edandestimated annotations.\n1.INTR ODUCTION\nAtraditional musical score represents music symbolically\ninterms ofnotes, formed from adiscrete alphabet ofpos-\nsible pitches anddurations. Human performance ofmusic\noften deviates substantially from thescore' sliteral inter-\npretation, byin\u0003ecting, stretching andcoloring themusic\ninwaysthat bring ittolife. Expr essive music synthesis\nseeks algorithmic approaches tothisexpressi verendering\ntask, sonatural tohumans.\nThere isreally agreat deal ofpast workonexpressi ve\nsynthesis more than canbesummarized here, though\nsome oftheleading authors giveanovervie wofseveral\nimportant lines ofworkin[1]. Most past work, forex-\nample [2],[3],[4],aswell asthemanyRENCON piano\ncompetition entries, forexample [5][6],hasconcentrated\nonpiano music. Thepiano isattracti veforonesimple rea-\nson: apiano performance canbedescribed bygiving the\nonset time, damping time, andinitial loudness ofeach note.\nSince apiano performance iseasy torepresent, itiseasy\ntode\u0002ne thetask ofexpressi vepiano synthesis asanes-\ntimation problem: onemust simply estimate these three\nnumbers foreach note.\nThis worksupported byNSF grants IIS-0739563 andIIS-0812244\nPermission tomakedigital orhard copies ofallorpartofthisworkfor\npersonal orclassroom useisgranted without feeprovided thatcopies are\nnotmade ordistrib uted forpro\u0002t orcommercial advantage andthatcopies\nbear thisnotice andthefullcitation onthe\u0002rstpage.\nc/circlecopyrt2009 International Society forMusic Information Retrie val.Incontrast, wetreat here thesynthesis ofmelody ,which\n\u0002nds itsrichest form with continuously controlled in-\nstruments, such astheviolin, saxophone orvoice. This\narea hasbeen treated byahandful ofauthors, including\ntheKTH group [7],[8],aswell asanumber others, in-\ncluding acommercial singing voice system. Continuously\ncontrolled instruments simultaneously modulate manydif-\nferent parameters, leading towide variety oftone color ,ar-\nticulation, dynamics, vibrato, andother musical elements,\nmaking itdif\u0002cult torepresent theperformance ofamelody .\nHowever,itisnotnecessary toreplicate anyofthese fa-\nmiliar instruments toeffectivelyaddress theheart ofthe\nmelody synthesis problem. Wewillpropose aminimal au-\ndiorepresentation wecallthetheremin, duetoitsobvi-\nousconnection with theearly electronic instrument bythe\nsame name [9]. Ourtheremin controls only time-v arying\npitch andintensity ,thus giving arelati velysimple, yetca-\npable, representation ofamelody performance.\nThe efforts cited aboveinclude some ofthemost suc-\ncessful attempts todate. Allofthese approaches map ob-\nservable elements inthemusical score, such asnote length\nandpitch, toaspects oftheperformance, such astempo\nanddynamics. One example istherule-based KTH sys-\ntem, which growsoutofseveraldecades offocused effort.\nInthissystem, each rule maps various musical conte xts\nintoperformance decisions, which canbelayered, sothat\nmanyrules canbesimultaneously applied. Therules were\nchosen, anditerati velyre\u0002ned, byamusic expert seeking\ntoarticulate andgeneralize awealth ofexperience intoper-\nformance principles. Incontrast, theworkofWidmer [2],\n[4]takesamachine learning perspecti vebyautomatically\nlearning rules from actual piano performances. Weshare\ntheperspecti veofmachine learning. In[4],phrase-le vel\ntempo and dynamic curveestimates arecombined with\nthelearned rule-based prescriptions, through acase-based\nreasoning paradigm. That is,thisapproach seeks musical\nphrases inatraining setthatareclose tothephrase being\nsynthesized, using thetempo anddynamic curvesfrom the\nclosest training example. Aswith theKTH work,theper-\nformance parameters arecomputed directly from theob-\nservable score attrib utes with norealattempt todescribe\nanyinterpr etive goals such asrepose, passing tone, local\nclimax, surprise, etc.\nOurworkdifferssigni\u0002cantly from these, andallother\npastworkweknowof,byexplicitly trying torepresent the\ninterpr etation itself .Previous workdoes notrepresent the\ninterpretation, butrather treats theconsequences ofthisin-\n555Oral Session 7: Harmonic & Melodic Similarity and Summarization\nterpretation, such asdynamic andtiming changes. Werep-\nresent theinterpretation intwoways. This \u0002rstuses atree-\nlikestructural decomposition thatmakesexplicit various\nlevelsofrepetition orparallelism inthemelody .This idea\nisfamiliar from other worksuch as[3],though weintro-\nduce aframe workforautomatically estimating thestruc-\nture. This approach hasconnections with [10], which \u0002nds\nphrase decompositions from symbolic music. Secondly ,\nweintroduce ahidden sequence ofvariables representing\ntheprosodic interpretation (stress andgrouping) itself, by\nannotating theroleofeach note inthelargerprosodic con-\ntext.Webelie vethese representations arenaturally posi-\ntioned between themusical score andtheobserv able as-\npects oftheinterpretation. Thus theseparate problems\nofestimating therepresentations andgenerating theactual\nperformance from therepresentations require shorter leaps,\nandaretherefore easier ,than directly bridging thechasm\nthatseparates score andperformance.\n2.THE THEREMIN\nOurgoal ofexpressi vemelody synthesis must, intheend,\nproduce actual sound. Weintroduce here anaudio rep-\nresentation webelie veprovides agood trade-of fbetween\nexpressi vepowerandsimplicity .\nConsider thecase ofasine waveinwhich both fre-\nquenc y,f(t),andamplitude, a(t),aremodulated overtime:\ns(t)=a(t)sin(2π/integraldisplayt\n0f(τ)dτ). (1)\nThese twotime-v arying parameters aretheones controlled\nintheearly electronic instrument knownasthetheremin .\nContinuous control ofthese parameters canproduce ava-\nriety ofmusical effects such asexpressi vetiming, vibrato,\nglissando, variety ofattack anddynamics. Thus, thetheremin\niscapable ofproducing arich range ofexpression. One\nsigni\u0002cant aspect ofmusical expression thetheremin can-\nnotcapture istone color asatime varying sine wave,\nthetimbre ofthetheremin isalwaysthesame. Partly be-\ncause ofthisweakness, wehavemodi\u0002ed theaboverep-\nresentation toallowtone color tochange asafunction of\namplitude:\ns(t)=H/summationdisplay\nh=1Ah(a(t),f(t))sin(2πh/integraldisplayt\n0f(τ)dτ) (2)\nwhere the{Ah}arehand-designed functions, monotoni-\ncally increasing inthe\u0002rst argument. Thus oursound is\nstillparametrized byf(t)anda(t),while weincrease the\npercei veddynamic range.\n3.REPRESENTING MUSICAL\nINTERPRET ATION\nThere are,nodoubt, more aspects ofmusical interpreta-\ntionthan canpossibly betreated here. Palmer [11] givesa\nverynice overvie wofcurrent thinking onthissubject from\nthePsychology perspecti ve.Broadly speaking, there are\nFigur e1.Amazing Grace(top)and Danny Boy (bot)\nshowing thenote-le vellabeling ofthemusic using sym-\nbols from ouralphabet.\natleast three important components tomusical interpreta-\ntion: conveying musical structure, and, inparticular ,the\nwayitrelates tothenotion ofphrase;musical prosody \ntheplacing, avoidance, andforeshado wing oflocal (note-\nlevel)stress andtheassociated low-levelgroupings that\nfollow;andmusical affect such happ y,sad, intense, ag-\nitated, etc. Wewill focus only onphrase structure and\nprosody here, ackno wledging thatthisisonly apiece of\nthelargerinterpreti vepicture.\nThe folk-lik emusic wetreat here ismostly composed\nofsimple musical structure, with ahigh degree ofrepeti-\ntion ofrhythm, pitch contour ,chord sequence, andother\nmusical elements. Typically thehierarchical structure of\nthese melodies iscaptured bysimple treestructures, often\ninvolving binary groupings atvarious levelsofgrouping: it\nisnoaccident that34outofthe48melodies inourdataset\nhave2nmeasures forsomen.Within thishierarch y,mu-\nsical phrases correspond tolevels ofthistree. When a\nmelody isnotcaptured byaperfectly regular treestruc-\nture, itoften corresponds totheconcatenation ofsuch reg-\nulartrees. Forinstance, thefamiliar melody ,God Save the\nQueen ,may bedescribed (2-2-2)+((2-2)-(2-2)) where each\nnumber represents agroup ofmeasures, '+'denotes con-\ncatenation and'-'denotes grouping. Thus themelody has\n3groups oftwomeasures followed byatwolevelsofbi-\nnary structure forthelasteight measures. While there isa\nsubjecti vecomponent tothepartition intophrases,the\u0002rst\n6andlast8measures seem likereasonable choices, per-\nhaps splitting thelast8measures into two4-bar phrases.\nInthisexample phrase boundaries correspond exactly to\nmeasure boundaries, though often thisisnotthecase. Thus\nwemust also indicate thelength ofthepickup foreach\ngroup ofmeasures.\nWhile conveying musical structure isanimportant part\nofexpressi vesynthesis, themain focus ofourefforthere is\nonmusical prosody .Weintroduce nowawayofrepresent-\ningthedesired musicality inamanner thatmakesclear in-\nterpreti vechoices andconveysthese unambiguously .Our\nrepresentation labels each melody note with asymbol from\nasmall alphabet,\nA={l−,l×,l+,l→,l←,l∗}\ndescribing therole thenote plays inthelargerconte xt.\nThese labels, tosome extent, borro wfrom thefamiliar vo-\ncabulary ofsymbols musicians usetonotate phrasing in\nprinted music. Thesymbols {l−,l×,l+}alldenote stresses\norpoints ofarrival.The variety ofstress symbols al-\nlowsforsome distinction among thekinds ofarrivalswe\n55610th International Society for Music Information Retrieval Conference (ISMIR 2009)\ntn t  -t t  -ttn+1\npp\nnn+1\nglis\nn+1 n+1bend\nFigur e2.Agraph ofthefrequenc yfunction, f(t),be-\ntween twonotes. Pitches arebent inthedirection ofthe\nnextpitch andmakesmall glissandi overthetransitions.\ncanrepresent: l−isthemost direct andasserti vestress; l×\nisthesoft landing stress inwhich werelax intorepose;\nl+denotes astress thatcontinues forwar dinanticipation\noffuture unfolding, aswith some phrases thatendinthe\ndominant chord. Examples oftheuseofthese stresses, as\nwell astheother symbols aregiveninFigure 1.Thesym-\nbols{l→,l∗}areused torepresent notes that movefor-\nwardtowards afuture goal (stress). Thus these areusually\nshorter notes wepass through without signi\u0002cant event.Of\nthese, l→isthegarden-v ariety passing tone, while l∗is\nreserv edforthepassing stress, asinabrief dissonance,\nortohighlight arecurring beat-le velemphasis, stillwithin\ntheconte xtofforw ardmotion. Finally ,thel←symbol de-\nnotes receding movement aswhen anote isconnected to\nthestress thatprecedes it.This commonly occurs when re-\nlaxing outofastrong-beat dissonance enroute toharmonic\nstability .Wewillwritex=x1,...,xNwithxn∈Afor\ntheprosodic labeling ofthenotes.\nThese concepts areillustrated with theexamples ofAmaz-\ningGraceandDanny Boy inFigure 1.Ofcourse, there\nmay beseveralreasonable choices inagivenmusical sce-\nnario, however,wealsobelie vethatmost labellings donot\nmakeinterpreti vesense andofferevidence ofthisisSec-\ntion7.Ourentire musical collection ismark edinthisman-\nnerandavailable at\nhttp://www .music.inf ormatics .indiana.edu/papers/ismir09\n4.FROM LABELING TOAUDIO\nUltimately ,theprosodic labeling ofamelody ,using sym-\nbols fromA,must betranslated intotheamplitude andfre-\nquenc yfunctions weuseforsound synthesis. Wehave\ndevised adeterministic mapping from ourprosodically-\nlabeled score totheactual audio parameter outlined here.\nOur synthesis off(t)anda(t)begins bymodifying\ntheliteral interpretation ofmusical timing expressed inthe\nscore toinclude ritardandi (slowing down)attheends of\nphrases. While wehavenotdone sohere, [3]recommends\nlargerchanges athigher levelsofthephrase hierarch y,as\nexpressed byourstructural representation. Wefurther mod-\nifyf(t)toinclude vibrato tolong andstressed notes. Fi-\nnally ,webend each pitch intowards thefollowing pitch\nwith a\u0002nal glissando toencourage asense oflegato.Fig-\nure2showsashort piece ofthispitch function overthe\ntwoconsecuti vetwonotes.0 5 10 150.0 0.2 0.4 0.6 0.8 1.0Theramin Parameters\nsecs(h z / a m p)>>>− \n> >>>>−\n \n< x\n >>>− \n>>>>>− \n \nFigur e3.Thefunctions f(t)(green) anda(t)(red) forthe\n\u0002rstphrase ofDanny Boy.These functions havedifferent\nunits sotheir ranges havebeen scaled to0-1tofacilitate\ncomparison.\nTheheart ofthetransformation, however,isinthecon-\nstruction oftheamplitude function a(t).This function is\ncreated through aseries ofsoftconstraints thatareplaced\nontheamplitude de\u0002ned atvarious knot locations over\ntime. These constraints aretakenfrom from theprosodically-\nannotated score andthestructural representation. Forin-\nstance, wewantphrase beginnings, asindicated bythe\nstructural representation, tobelowinamplitude; thus we\naddaquadratic penalty thatencourages thischaracteristic.\nSimilarly ,wewantstressed notes tobehigh inamplitude\nandaddsimilar quadratic penalties toencourage this. In\naddition wewantforw ard-mo ving notes tobeincreasing\ninamplitude, andthus addquadratic terms that encour -\nagethisrelationship between aforw ard-mo ving note and\nitssuccessor .Similar terms areadded forreceding notes.\nWethen compute thevalues attheknot locations bymin-\nimizing thequadratic penalty function, andinterpolate the\nresulting amplitudes attheknot locations. Amore detailed\npresentation ofthisprocess isdescribed in[12]. Anex-\nample ofboth thea(t)andf(t)functions forafamiliar\nexamples aregiveninFigure 3.\n5.HOWMUCH MUSICALITY DOES THE\nREPRESENT ATION CAPTURE?\nThetheremin parameters, f(t),a(t),andhence theaudio\nsignal, s(t),depend entirely onthestructural representa-\ntion, theprosodic labeling, andthemusical score, through\nthemapping described inSection 4.Wewanttounder -\nstand thedegree towhich ourrepresentation captures mu-\nsically important interpreti venotions. Tothisend, wehave\nconstructed adataset ofabout 50simple melodies contain-\ningacombination ofgenuine folk songs, folk-lik esongs,\nChristmas carols, andexamples from popular andartmusic\nofvarious eras. Themelodies were chosen tobefamiliar ,\nhaving simple chords, simple phrase structure, allatmod-\n557Oral Session 7: Harmonic & Melodic Similarity and Summarization\nerate toslowtempo, andappropriate forlegato phrasing.\nExamples include Danny Boy,Away inaMang er,Loch\nLomond ,BytheWatersofBabylon ,etc. These melodies\nwere painstakingly hand-annotated with structure andprosody\nbytheauthor .\nWerendered these melodies intoaudio according toour\nhand-mark edannotations andtheprocess ofSection 4.For\neach ofthese audio \u0002les weprovide harmonic conte xtby\nsuperimposing sustained chords, asindicated inthescores.\nTheentire collection ofsymbolic melodies, along with ren-\ndered audio \u0002les, isavailable attheaforementioned web\nsite.\nWedoobserv esome aspects ofmusical interpretation\nthatarenotcaptured byourrepresentation. Forexample,\ntheinterpretation ofDanny Boyclearly requires aclimax\natthehighest note, asdoanumber ofthemusical exam-\nples. Wecurrently donotrepresent such aneventthrough\nourmarkup. Itispossible thatwecould addanewcate-\ngory ofstress corresponding tosuch ahighpoint, though\nwesuspect thatthedegree ofemphasis iscontinuous, thus\nnotwell captured byadiscrete alphabet ofsymbols.\nAnother occasional shortcoming isthefailure todistin-\nguish contrasting material, asinOCome OCome Emanuel .\nThis melody hasaGregorian chant-lik efeel andshould\nmostly berendered with deliberate calmness. However,the\nshort outburstcorresponding tothewordRejoice takes\nonamore declarati veaffect. Our prosodically-oriented\nmarkup simply hasnowaytorepresent such acontrast of\nstyles, though itishinted atinthestructural decomposition\nof((3-3)-(3-3))+(2-2)+3.\nThere are,perhaps, some other general shortcomings of\ntheinterpretations, though webelie vethere isquite abit\nthatisright inthem, especially considering thesimplic-\nityofourrepresentation ofinterpretation. However,we\nhope readers willmakeindependent judgments.\n6.ESTIMA TING THE INTERPRET ATION\nThe essential goal ofthisworkistoalgorithmically gen-\nerate expressi verenderings ofmelody .Having formally\nrepresented ournotion ofmusical interpretation, wecan\ngenerate anexpressi verendering byestimating thisrepre-\nsentation.\n6.1Estimating Phrase Structur e\nWeestimate thestructural decomposition ofourmelody\nbymaximizing anobjecti vefunction de\u0002ned onthede-\ncomposition using dynamic programming. The approach\nbegins bylabeling each note subsequence containing two\nbarlines asaterminal state ,andscoring theplausibility of\neach possible label forthesubsequence (thescore function\nwillbediscussed presently). Wethen proceed inductively\nto\u0002nd theoptimal labelings ofprogressi velylargersub-\nsequences, ultimately terminating with alabeling forthe\nentire melody .\nSuppose wehavehavefound thepossible labelings of\neach note subsequence containing m−1barlines, andhave\ncomputed thebest-scoring derivation ofeach such labeledsubsequence (thelabels willbedescribed below).Wecan\n\u0002nd theoptimal score ofeach label oneach contiguous\nregion containing mbarlines bypiecing together various\ncontiguous subsequences containing lessthanmbarlines.\nWeallowthree possible waystodothis, asfollows\n1.Wecanlabel asubsequence containing mbarlines\nasaterminal state, corresponding toasingle group-\ningwith nosubdi visions. Welabel such agroup of\nmeasures asmthenumber ofmeasures compos-\ningthegroup. The subsequence need notbeginor\nendatameasure boundary .\n2.Ifthenumber ofmeasures, m,hasafactor ,f,in\n{2,3,...,5},weconsider allpartitions oftheregion\nintofcontiguous regions each containing k=n/f\nbarlines. Foreach such partition, weconsider piec-\ningtogether kidentically labeled segments andla-\nbeling theresult as(k−k−...−k).Forinstance,\nifweconsider aregion containing 8barlines and\nconsider composing thisregion oftwoidentically\nlabeled contiguous regions, wecould group regions\nlabeled aseither 4or(2-2). Anysuch production\nwould result inaregion labeled as(4-4), denoting\nthebinary split. Wecannot combine twocontiguous\nregions labeled as4and(2-2) tomakea(4-4) region.\n3.Forthe\u0002nal production phase, which considers the\ncomplete collection ofmelody notes containing, say,\nMbarlines, weallowthepreviously-described pro-\nductions aswell asaconcatenation operation. The\nconcatenation pieces together anypair ortriple of\ncontiguous regions composing thecomplete melody .\nSuch concatenations will bedenoted asA+Bor\nA+B+Cwhere A,B,Careanypossible label-\nings oftheindividual regions.\nEach ofthese productions generates ascore forthere-\nsulting labeling. When weusetheterminal state label, we\nwantthecollection ofmeasures tomakesense asaniso-\nlated unit. Thus wewillscore such labels torewardrela-\ntivelylong \u0002nal notes andchord changes atthefollowing\nbarline.\nWhen applying ourfactoring rule, wewish togroup to-\ngether note sequences thatexhibit parallelism. Therhyth-\nmicparallelism between twonote groups canbemeasured\nbythesymmetric difference oftherhythms thenum-\nberofnotes thatdonote line upwhen thebarlines are\naligned. This measure rewards similar rhythmic structures\nandencourages groups tohavethesame pickup length.\nWhen more than twogroups areconsidered, wecancom-\npute anaverage symmetric difference. Wehaveused such\naverage symmetric differences onrhythm, pitch, andchord\ntoachie veanoverall measure ofparallelism. Thescore of\naparticular factor label willthen bethesum oftheindivid-\nuallabeled subsequence scores plus thescore foroverall\nparallelism.\nThe \u0002nal production type isconcatenation. Generally\nspeaking, wewish todiscourage such explanations, sowe\ngivea\u0002xedpenalty everytime theconcatenation operation\n55810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nisinvoked.Thus thescore foralabel involving concatena-\ntionisthesum oftheindividual scores, plus aparallelism\nscore between theconcatenated sections, plus theconcate-\nnation penalty .\nWiththisdescription inmind, itissimple to\u0002nd the\noverall bestscoring labeling. After computing andscoring\nallpossible labelings ofregions containing mbarlines,\nweretain only thebest scoring parse foreach particular\nlabel thisistheessential idea ofdynamic programming.\nFinally ,when weconsider theentire collection ofnotes,\nwechoose thebestscoring ofalllabelings asourstructure\nestimate.\nAtpresent wehavesimply hand-chosen thescore func-\ntionandmakenoclaims fortheoptimality ofthischoice.\nBoth theautomatic training andevaluation ofthismethod\narethefocus ofongoing work. Asanexample, oural-\ngorithm recognized OCome OCome Emmanuel as((3-\n3)-(3-3))+7 with each segment containing aquarter note\npickup, showing anability torecognize interesting asym-\nmetries. Appropriately ,most often werecognized simple\nbinary structures toourmelodies.\n6.2Estimating theProsodic Labeling\nOurestimation oftheunobserv edsequence ofprosodic la-\nbels,x1,...,xN,depends onvarious observ ables, y1,...,yN,\nwhere thefeature vectoryn=y1\nn,...,yJ\nnmeasures at-\ntributes ofthemusical score atthenthnote. The fea-\ntures weconsider aresurface-le velattrib utesofthemusical\nscore. While agreat manypossibilities were considered,\nweultimately culled thesettothemetric strength ofthe\nonset position, aswell asthe\u0002rst andsecond differences\nofnote length, inseconds, andMIDI pitch.\nOurfundamental modeling assumption viewsthelabel\nsequence, x,asaMark ovchain, giventhedata,y:\np(x|y)=p(x1|y1)N/productdisplay\nn=2p(xn|xn−1,yn,yn−1)(3)\n=p(x1|y1)N/productdisplay\nn=2p(xn|xn−1,zn)\nwhere zn=(yn,yn−1).Theintuition behind thisassump-\ntionistheobserv ation (oropinion) thatmuch ofphrasing\nresults from acyclic alternation between forw ardmoving\nnotes, {l→,l∗},stressed notes, {l−,l+,l×},andoptional\nreceding notes {l←}.Often structural boundaries occur\nwhen onemovesfrom either stressed orreceding states to\nforw ardmoving states. Thus thenotion ofstate ,asina\nMark ovchain, seems toberelevant.\nWeestimate theconditional distrib utions p(xn|xn−1,zn)\nforeach choice ofxn−1∈A,aswell asp(x1|y1),using\nourlabeled data. Wewillusethenotation\npl(x|z)def=p(xn=x|xn−1=l,zn=z)\nforl∈A.Intraining these distrib utions wesplit ourscore\ndata into|A|groups, Dl={(xli,zli)},where Dlisthe\ncollection ofall(class label, feature vector) pairs overall\nnotes thatimmediately followanote ofclassl.l∗l→l←l−l×l+total\nl∗135 112 0 18 2 0 267\nl→62 1683 8 17 0 0 1770\nl←3 210 45 6 2 0 266\nl−49 48 4 103 15 0 219\nl×5 32 2 65 30 0 134\nl+0 3 0 12 3 0 18\ntotal 254 2088 59 221 52 0 2674\nFigur e4.Confusion matrix oferrors overthevarious\nclasses. The rowsrepresent thetrue labels while the\ncolumns represent thepredicted labels. The block struc-\nture indicated inthetable showstheconfusion onthe\ncoarser categories ofstress, forw ardmovement, andreced-\ningmovement\nWemodel thepl(x|z)distrib utions using theclassi\u0002ca-\ntiontreemethodology ofCAR T[13]. That is,foreachDl\nwebeginwith asplit, zj>cseparating Dlintotwosets:\nD0\nl={(xli,zli):zj\nli>c}andD1\nl={(xli,zli):zj\nli≤\nc}.Wechoose thefeature, j,andcutof f,c,toachie vemax-\nimal purity inthesetsD0\nlandD1\nlasmeasured bythe\naverage entrop yovertheclass labels. Wecontinue tosplit\nthesetsD0\nlandD1\nl,splitting their offspring, etc., ina\ngreedy manner ,until thenumber ofexamples atatreenode\nislessthan some minimum value. Ourestimate ˆpl(x|z)is\nthen computed by\u0002nding theterminal treenode associated\nwithzandusing theempirical label distrib ution overthe\nclass labels {xli}whose associated {zli}falltothesame\nterminal treenode.\nGivenapiece ofmusic with feature vectorz1,...,zN,we\ncancompute theoptimizing labeling\nˆx1...,ˆxN=argmax\nx1,...,xNˆp(x1|y1)N/productdisplay\nn=2ˆp(xn|xn−1,zn)\nusing dynamic programming.\n7.RESUL TS\nWeestimated alabeling foreach oftheC=48pieces in\nourcorpus bytraining ourmodel ontheremaining C−1\npieces and\u0002nding themost likelylabeling, ˆx1,...,ˆxN,as\ndescribed above.When computing themost likelylabeling\nforeach melody inourcorpus wefound atotal of678/2674\nerrors (25.3%) with detailed results aspresented inFigure\n4.\nThenotion oferror issome what ambiguous, however,\nsince there really isnocorrect labeling. Inparticular ,the\nchoices among theforw ard-mo ving labels: {l∗,l→},and\nstress labels: {l−,l×,l+}areespecially subject tointer-\npretation. Ifwecompute anerror rateusing these cate-\ngories, asindicated inthetable, theerror rateisreduced to\n15.3%.\nOne should note amismatch between ourevaluation\nmetric ofrecognition errors with ourestimation strate gy.\nUsing aforw ard-backw ard-lik ealgorithm itispossible to\n559Oral Session 7: Harmonic & Melodic Similarity and Summarization\ncompute p(xn|y1,...,yN).Thus ifwechoose\n¯xn=argmax\nxn∈Ap(xn|y1,...,yN),\nthen thesequence ¯x1,...,¯xNminimizes theexpected num-\nberofestimation errors\nE(errors|y1,...,yN)=/summationdisplay\nnp(xn/negationslash=¯xn|y1,...,yN)\nWehavenotchosen thislatter metric because wewanta\nsequence thatbehavesreasonably .Itthesequential nature\nofthelabeling thatcaptures theprosodic interpretation, so\nthemost likelysequence ˆx1,...,ˆxnseems likeamore rea-\nsonable choice.\nInanefforttomeasure what webelie vetobemost im-\nportant thepercei vedmusicality oftheperformances\nweperformed asmall user study .Wetook asubset\nofthemost well-kno wnmelodies ofthedataset andcre-\nated audio \u0002les from therandom, hand, andestimated an-\nnotations. Theestimated annotations were produced using\nground truth forthestructure while estimating theprosodic\nlabelings. Wepresented allthree versions ofeach melody\ntoacollection of23subjects who were students inourUni-\nversity' smusic school, aswell assome other comparably\neducated listeners. Thesubjects were presented with ran-\ndom orderings ofthethree versions, with different order -\nings foreach user,andaskedtorespond tothestatement:\nThe performance sounds musical andexpressi vewith\ntheLikert-style ratings 1=strongly disagree, 2=disagree,\n3=neutral, 4=agree, 5=strongly agree, aswell astorank\nthethree performances interms ofmusicality (the rank-\ningdoes notalwaysfollowfrom theLikertratings). Out\nofatotal of244 triples that were evaluated inthisway,\ntherandomly-generated annotation recei vedamean score\nof2.96 while thehand andestimated annotations recei ved\nmean scores of3.48 and3.46. The rankings showed no\npreference forthehand annotations overtheestimated an-\nnotations (p=.64),while both thehand andestimated an-\nnotations were clearly preferred totherandom annotations\n(p=.0002 ,p=.0003 ).\nPerhaps themost surprising aspect ofthese results is\nthehigh score oftherandom labelings inspite ofthe\nmeaningless nature ofthese labelings, thelisteners were,\ninaggre gate, neutral injudging themusicality ofthe\nexamples. Webelie vethereason forthisisthatmusical\nprosody ,accounts foronly aportion ofwhat listeners re-\nspond to.Allofourexamples were rendered with human-\nsupplied structural representations andthesame sound en-\ngine ofSection 4which tries tocreate asense ofsmooth-\nness inthedeliverywith appropriate useofvibrato and\ntimbral variation. Weimagine thatthelisteners were partly\nswayed bythese aspects, evenwhen theuseofprosody\nwasnotsatisf actory .Theresults alsoshowthatourestima-\ntion produced annotations thatwere, essentially ,asgood\nasthehand-labeled annotations. This demonstrates asuc-\ncess ofourresearch. The computer -generated interpreta-\ntions clearly demonstrate some musicality with anaverage\nlistener rating of3.46 halfw aybetween neutral and\nagree. However,there isconsiderable room forimpro ve-\nment.Themelodies were alsorendered using structural repre-\nsentations estimated asinSection 6.2,thus leaving theen-\ntiremusical interpretation tothecomputer .Theaudio \u0002les\ndocumenting thisexperiment areavailable ontheafore-\nmentioned web site.\n8.REFERENCES\n[1]Goebl W.,Dixon S.,DePoli G.,Friber gA.,and\nBresin R.andGerhard Widmer .Sense inexpressive\nmusic performance: Data acquisition, computational\nstudies, andmodels ,chapter 5,pages 195242. Logos\nVerlag, Berlin, may 2008.\n[2]Widmer G.andGoebl W.Computational models for\nexpressi vemusic performance: The state oftheart.\nJournal ofNewMusic Resear ch,33(3):203216, 2004.\n[3]Todd N.P.M.The kinematics ofmusical expres-\nsion. Journal oftheAcoustical Society ofAmerica ,\n97(3):19401949, 1995.\n[4]Widmer G.andTobudic A.Playing Mozart byanal-\nogy: Learning multi-le veltiming anddynamics strate-\ngies. Journal ofNewMusic Resear ch,33(3):203216,\n2003.\n[5]Hirag aR.,Bresin R.,Hirata K.,andKatayose H.Ren-\ncon2004: Turing Testformusical expression, Proceed-\ningsofthe2004 Conference onNewInterf aces forMu-\nsical Expression (NIME04), 120123, 2004.\n[6]Hashida Y.,Nakra T.,Katayose H.,andMurao Y.\nRencon: Performance Rendering Contest forAuto-\nmated Music Systems, Proceedings ofthe10th Int.\nConf. onMusic Perception andCognition (ICMPC 10),\nSapporo, Japan, 53-57, 2008.\n[7]Sundber gJ.TheKTH synthesis ofsinging. Advances\ninCognitive Psychology.Special issue onMusic Per-\nformance ,2(2-3):131143, 2006.\n[8]Friber gA.,Bresin R.andSundber gJ.Overvie wofthe\nKTH rule system formusical performance. Advances\ninCognitive Psychology,2(2-3):145161, 2006.\n[9]Roads C.TheComputer Music Tutorial .MIT Press,\n1996.\n[10] Bod R.AUni\u0002ed Model ofStructural Organization\ninLanguage andMusic. Journal ofArti\u0002cial Intelli-\ngence Resear ch,17:289-308, 2002.\n[11] Palmer C.Music Performance. Annual ReviewPsy-\nchology,48:115-138, 1997.\n[12] omitted forreviewRepresentation andSynthesis of\nMelodic Expression. Proc.ofInt.Joint Conf .onArt.\nInt.(IJCAI)) ,toappear .\n[13] Breiman L.,Friedman J.,Olshen R.andStone C.\nClassi\u0002cation and Regression Trees.Wadsw orth and\nBrooks, Montere y,CA, 1984.\n560"
    },
    {
        "title": "Minimum Classification Error Training to Improve Isolated Chord Recognition.",
        "author": [
            "Jeremy Reed",
            "Yushi Ueda",
            "Sabato Marco Siniscalchi",
            "Yuuki Uchiyama",
            "Shigeki Sagayama",
            "Chin-Hui Lee 0001"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417163",
        "url": "https://doi.org/10.5281/zenodo.1417163",
        "ee": "https://zenodo.org/records/1417163/files/ReedUSUSL09.pdf",
        "abstract": "Audio chord detection is the combination of two separate tasks: recognizing what chords are played and determining when chords are played. Most current audio chord detection algorithms use hidden Markov model (HMM) classifiers because of the task similarity with automatic speech recognition. For most speech recognition algorithms, the performance is measured by word error rate; i.e., only the identity of recognized segments is considered because word boundaries in continuous speech are often ambiguous. In contrast, audio chord detection performance is typically measured in terms of frame error rate, which considers both timing and classification. This paper treats these two tasks separately and focuses on the first problem; i.e., classifying the correct chords given boundary information. The best performing chroma/HMM chord detection algorithm, as measured in the 2008 MIREX Audio Chord Detection Contest, is used as the baseline in this paper. Further improvements are made to reduce feature correlation, account for differences in tuning, and incorporate minimum classification error (MCE) training in obtaining chord HMMs. Experiments demonstrate that classification rates can be improved with tuning compensation and MCE discriminative training.",
        "zenodo_id": 1417163,
        "dblp_key": "conf/ismir/ReedUSUSL09",
        "keywords": [
            "audio chord detection",
            "hidden Markov model (HMM)",
            "automatic speech recognition",
            "word error rate",
            "continuous speech",
            "ambiguous word boundaries",
            "frame error rate",
            "classification rates",
            "tuning compensation",
            "MCE discriminative training"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMINIMUM CLASSIFICATION ERROR TRAINING TO IMPROVE\nISOLATED CHORD RECOGNITION\nJ.T. Reed1, Yushi Ueda2, S. Siniscalchi3,Yuki Uchiyama2, Shigeki Sagayama2, C.-H. Lee1\n1School of Electrical and Computer Engineering\nGeorgia Institute of Technology, Atlanta, GA 30332\n{jreed,chl}@ece.gatech.edu\n2Graduate School of Information Science and Technology\nThe University of Tokyo, Hongo, Bunkyo-ku, Tokyo 113-8656 Japan\n{ueda,uchiyama,sagayama }@hil.t.u-tokyo.ac.jp\n3Department of Electronics and Telecommunications\nNorwegian University of Science and Technology, Trondheim, Norway\nmarco77@iet.ntnu.no\nABSTRACT\nAudio chord detection is the combination of two separate\ntasks: recognizing what chords are played and determining\nwhen chords are played. Most current audio chord detec-\ntion algorithms use hidden Markov model (HMM) classi-\nﬁers because of the task similarity with automatic speech\nrecognition. For most speech recognition algorithms, the\nperformance is measured by word error rate; i.e., only the\nidentity of recognized segments is considered because word\nboundaries in continuous speech are often ambiguous. In\ncontrast, audio chord detection performance is typically\nmeasured in terms of frame error rate, which considers\nboth timing and classiﬁcation. This paper treats these two\ntasks separately and focuses on the ﬁrst problem; i.e., clas-\nsifying the correct chords given boundary information. The\nbest performing chroma/HMM chord detection algorithm,\nas measured in the 2008 MIREX Audio Chord Detection\nContest, is used as the baseline in this paper. Further im-\nprovements are made to reduce feature correlation, account\nfor differences in tuning, and incorporate minimum classi-\nﬁcation error (MCE) training in obtaining chord HMMs.\nExperiments demonstrate that classiﬁcation rates can be\nimproved with tuning compensation and MCE discrimina-\ntive training.\n1. INTRODUCTION\nAs online music databases continue to grow in size, more\neffective retrieval mechanisms are needed. In particular,\nrecognizing certain musicological, acoustical, and cultural\nfactors in a musical piece impact notions of similarity. One\nsuch musicological factor which has seen an increased re-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.search focus is automatic chord detection, which is a mid-\nlevel representation and a ﬁrst-step in identifying the har-\nmony of a given musical work.\nMost recent approaches to identifying chords from the\nacoustic signal are based on using chroma features as in-\nputs into a hidden Markov model (HMM) based system.\nAn early approach in literature using an HMM-based sys-\ntem was [1], where an ergodic HMM provides the ini-\ntial chord progression modeling and updated using N-best\nrescoring techniques. Sheh and Ellis [2] deal with an inad-\nequate amount of training data by assuming that chroma\nvectors from the same mode (e.g., Major ) and different\npitch classes can be considered as rotated versions of one\nanother. Bello et al . [3] incorporate musical knowledge\ninto the transition probabilities and HMM parameters to\nimprove the results. Lee and Slaney [4] increase the amount\nof training data available by synthesizing audio to provide\naccurate chord and boundary information. Improvement is\nmade in [5] by using key-dependent ergodic HMMs and\nwarping the chroma features into tonal centroid features\n[6], which gives the relation of the chroma features on the\ncircles of ﬁfths, minor thirds, and major thirds.\nThe HMM framework is inspired by automatic speech\nrecognition (ASR), where HMMs represent words or sub-\nword units. However, the ASR community only considers\nthe recognition rate (i.e., what was said) as important and\nignores timing information (i.e., recognizing when each\nspoken unit begins and ends). In contrast, audio chord\ndetection is measured in terms of frame error rate (FER),\nwhich incorporates both tasks. This paper proposes opti-\nmizing these two task separately and focuses on the prob-\nlem of classiﬁcation rate (i.e., recognizing what was said).\nTo the authors’ knowledge, only [2] evaluates these tasks\nseparately. Speciﬁcally, Sheh and Ellis consider forced\nalignment, where the correct chord sequence is known and\nthe timing information is extracted.\nThis paper implements several improvements to evalu-\nate the limits of the chroma/HMM system for classiﬁca-\ntion rates. In particular, the goal of this paper is to improve\n609Poster Session 4\nthe classiﬁcation rate between highly confused chords in\nthe feature space. For instance, one of the most common\nchord detection errors is between parallel modes, which\nshare the same root, but differ in their key signature; e.g, C\nMajor versus C minor . The cause for confusion is because\nthe difference between a Major andminor chord is a ﬂat-\ntening of the major third to a minor third, which may only\nbe a few hertz.\nThe baseline system adopted in this study is the current\nstate-of-the-art and placed ﬁrst in the 2008 MIREX Audio\nChord Detection task (Task 2: no pre-training) [7]. To at-\ntenuate percussive sounds, the harmonic-percussive source\nseparation (HPSS) algorithm [8] is used in the baseline\nsystem to isolate the harmonic part of the spectrum prior\nto chroma extraction and maximum likelihood (ML) esti-\nmation. This paper incorporates the improvements of au-\ntomatic tuning compensation and minimum classiﬁcation\nerror (MCE) training [9].\nThe automatic tuning algorithm is a simpliﬁcation of\nthe one proposed in [3]. Small, uniform databases, such\nas the Beatles Chord Database [10], experience improved\nperformance with tuning normalization because slight dif-\nferences in tuning cause confusion between highly com-\npeting chords. This can lead to confusion among parallel\nmodes since they differ by a single note, for example. Due\nto the trade-off between spectral and time-based resolution\nin frame-based music processing, a slight difference in tun-\ning could allow for energy to bleed into neighboring energy\nbands, leading to confusion.\nMCE, a highly successful discriminative training ap-\nproach, enhances ASR performance by overcoming two\nassumptions made by parametric approaches. Like speech,\nthe assumption that the true distribution of chroma vec-\ntors is an HMM is an approximation to yield a parametric\nﬁt. ML techniques estimate parameters corresponding to\nthe mode of the likelihood function. However, if the true\ndistribution differs from the assumed model, the ML tech-\nnique is not guaranteed to yield an optimal performance. In\naddition, the strength of the parametric ﬁt relies on accu-\nrate parameter estimates. However, current acoustic chord\ndatabases are quite small in size and contain around 100\nsongs from one to ﬁve artists. With such small artist di-\nversity, it is unlikely that current databases are a good rep-\nresentation of the entire acoustic space. MCE integrates\na discriminative training approach into the parameter es-\ntimation problem by directly optimizing the performance\nclassiﬁcation; i.e., classiﬁcation error. Speciﬁcally, a logis-\ntic transform incorporates the classiﬁcation error rate into\nthe objective function so that gradient probabilistic descent\nwill yield an improved set of parameters.\nThe baseline algorithm, is described in Section 2 and\nimprovements are detailed in Section 3. Experimental re-\nsults in Section 4 compare modiﬁcations to the ML base-\nline. Finally, Section 5 gives concluding remarks.2. BASELINE ALGORITHM: MIREX 2008\nSUBMISSION\n2.1 Harmonic/Percussion Source Separation\nAs noted in [11], transients and noise decrease the chord\nrecognition accuracy in chroma-based approaches. This\nis largely due to percussive sources, which spread energy\nacross the entire frequency spectrum. While the authors\nin [11] use a median ﬁlter to smooth percussive effects,\nthis paper uses the HPSS algorithm [8], which integrates\nthe harmonic and percussive separation into the objective\nfunction\nJ(H,P) =1\n2σ2\nH/summationdisplay\nk,n(Hk,n−1−Hk,n)2\n+1\n2σ2\nP/summationdisplay\nk,n(Pk−1,n−Pk,n)2(1)\nwhereHk,nandPk,nare the values of the power spectrum\nat frequency index kand time index nfor the harmonic\nspectrum, H, and the percussive spectrum, P, respectively.\nThe parameters σ2\nPandσ2\nHneed to be set experimentally.\nTo ensure that each time-frequency component of the har-\nmonic and percussive spectrum components sum to a value\nequal to the original spectrum, Wk,n, and to ensure that\npower spectrums remain positive, the following constraints\nare added to the minimization of (1)\nHk,n+Pk,n=Wk,n (2)\nHk,n≥0 (3)\nPk,n≥0 (4)\nNote that minimizing (1) is equivalent to maximum like-\nlihood estimation under the assumption that (Hk,n−1−\nHk,n)and(Pk−1,n−Pk,n)are independent Gaussian dis-\ntributed variables. This simpliﬁcation leads to a set of it-\nerative update equations for the harmonic and percussive\nspectrums. At the output of HPSS are two waveforms; one\nof these contains a percussive-dominated spectrum and the\nother a harmonic-dominated spectrum. Further details can\nbe found in [8].\n2.2 Chromagram\nChroma vectors are the most common features in audio\nchord detection algorithms and describe the energy distri-\nbution among the 12 chromas; i.e., pitch classes. To derive\nchroma vectors, the harmonic-emphasized music signal is\nﬁrst downsampled to 11025 Hz. Next, the signal is broken\ninto frames of 2048 samples with a 50% overlap. The con-\nstant Q transform [12] provides spectral analysis using a\nlogarithmic spacing of the frequency domain, whereas the\ntraditional discrete Fourier transform (DFT) uses a linear\nspacing of the frequency domain. The resulting spectrum,\nS, of the audio signal s(t)is given by\nS(k) =T(k)−1/summationdisplay\nt=0w(t,k)s(t)e−j2πfkt(5)\n61010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nwhere the analysis window, w(t,k), and the window size,\nT(k), are functions of the frequency bin index, k. The\ncenter frequency of the k-th bin is designed to match the\nequal-temperament scale [13]. For example, if it is desired\nto have one bin per note on an 88-piano keyboard, then the\nbin center frequencies are\nfl= 2l/βfref (6)\nwith the number of bins per octave, β, set to 12, the min-\nimum reference frequency, fref, set to the frequency of A0\n(i.e., 27.5 Hz), and l={1,2,...,88}. The resulting chroma\nvector for frame nis\ncn(b) =R/summationdisplay\nr=0|S(b+rβ)| (7)\nwhereb={1,2,...,β}is the chroma bin number and Ris\nthe number of octaves considered.\n2.3 HMM classiﬁer\nThe optimal chord sequence, W∗is decoded such that [14]\nW∗= arg max\nWP(W|C)\n= arg max\nWP(C|W)P(W)\nP(C)\n∝arg max\nWP(C|W)P(W) (8)\nwhereC={c1,c2,...,cN}is the sequence of chroma\nvectors. The probabilities of the acoustic model and tonal-\nity model are P(C|W)andP(W), respectively. Note that\nin speech,P(W)is the language model; i.e., the prior\nprobability for a sequence of words, W. For this paper, the\ntonality model assumes that every chord is equally likely.\nThe reason for the proportionality in (8) is that P(C)is the\nsame for all chord sequences. The acoustic model is the\nprobability of producing the observed chroma vectors for\nchordWand is modeled with a HMM; i.e.,\nP(C|W) =πq0N/productdisplay\nn=1aqn−1qnbqn(cn) (9)\nwhereπq0is the initial state probability, aqn−1qnis the tran-\nsition probability from state qn−1toqn, andbqn(cn)is the\noutput likelihood, which is modeled by a Gaussian mixture\nmodel (GMM)\nbqn(cn) =D/summationdisplay\nd=1ωdN(µd,Σd) (10)\nwhereDis the number of mixtures, ωdis the mixture\nweight for component, d, and N(µd,Σd)is a Gaussian den-\nsity with mean µdand covariance Σd. If the features are\nuncorrelated, then the covariance matrix is diagonal, Σd=\ndiag(σ1,σ2,...,σ 12). Note that a single state HMM is\nequivalent to a GMM.\nFigure 1 . Cross correlations of chroma features. Right:\noriginal chroma features. Left: DFT chroma features.\nDark shades (or red if in color) indicate higher correlation\n(light shades (or blue in color) indicate low correlation.\n3. IMPROVED ALGORITHM\nSince the baseline algorithm is in a format compatible with\nthe automatic speech recognition paradigm, it provides a\ngood framework to test more advanced speech processing\ntechniques, such as MCE, as an alternative model estima-\ntion step. In addition, parameter reduction and tuning com-\npensation are implemented and compared to the baseline.\n3.1 Fourier Transform Chroma Features\nAs noted in [3], chroma features are highly correlated be-\ncause harmonics of different pitch classes overlap and is\ndemonstrated in the left part of Figure 1. For instance, the\nthird harmonic of C4(261.63 Hz fundamental, 784.89 Hz\nthird harmonic) is highly confusable with G5(783.99 Hz\nfundamental). However, as shown on the right of Figure 1,\nthe resulting feature dimensions have less cross-correlation\nafter applying a DFT on the chroma features.\n3.2 Tuning Compensation\nA second enhancement is tuning compensation. Standard\ntuning is such that the Anote above middle Con a piano\nkeyboard (i.e., A4) is approximately 440 Hz. However,\nartists may intentionally or unintentionally have a refer-\nence tuning different from the standard. This can lead\nto confusion in algorithms which assume that all music\nis tuned to the standard reference, as shown in Figure 2.\nIn the upper part of Figure 2, a 12-dimensional chroma is\napplied to a piece of music whose energy distribution is\nhigher in frequency than standard tuning ( A4/similarequal440Hz).\nTherefore, the signal energy is distributed between the in-\ntended note (e.g., C3) and the neighboring note (e.g., C#3).\nConsidering that Major andminor chords differ by only\none semi-tone in a single note, this can lead to large confu-\nsion between the Major andminor modes of a given chord.\nTo account for differences in tuning, a simpliﬁed ver-\nsion of [3] is implemented. The original tuning compen-\nsation algorithm uses 36 bins per octave ( β= 36 in (7))\nin the calculation of chroma vectors. A peak picking algo-\nrithm produces a histogram, which gives information about\n611Poster Session 4\nC3C#3D3 C7C#7D\nC31C32C33C#31C#32C#33D31D32D33C71C72C73C#71C#72C#73D71D72D73Figure 2 . Upper: Hypothetical mistuned energy distribu-\ntion. Bottom: Find tuning alignment giving maximum en-\nergy distribution at sampled points.\nthe tuning of the piece. A circular shift is then applied\nto the chroma vector as a corrective factor. The reason\nfor peak picking is that noise sources (e.g., percussion and\ntransients) corrupt the chroma vectors with non-harmonic\nsources.\nHowever, because of HPSS, a simpliﬁed procedure ﬁl-\nters percussive noise sources and leaves energy due to har-\nmonic sources. The new algorithm takes a 36-dimension\nchroma vector for each frame in a song, so that each note\nconsidered is divided into a three bins\n˜c(α)\nn(b) =R/summationdisplay\nr=0|S(b+α+rβ)| (11)\nwhereα={1,2,3}andb={1,2,..., 12}. The algo-\nrithm then retains the set the αwhich produces the chroma\nvector with the greatest Euclidean length\ncn= arg max\n˜c(α)\nn/parenleftBig\n˜c(α)\nn·˜c(α)\nn/parenrightBig\n(12)\n3.3 Minimum Classiﬁcation Error Learning\nAs mentioned in the Introduction, MCE is a highly suc-\ncessful discriminative training approach to improving au-\ntomatic speech recognizers over ML and MAP estimation.\nThe optimization criterion in MCE is to minimize the esti-\nmated classiﬁcation loss\nL(Λ) =1\nJJ/summationdisplay\nj=1M/summationdisplay\nm=1lm(Xj; Λ)1(Xj∈Ωm) (13)\nwhere Λare the model parameters, Jis the number of\ntraining examples, {X1,X2,...,XJ},Mis the number\nof categories (i.e., chords), lm(·)is a loss function, and\n1(Xj∈Ωm)is one ifXjis in category Ωmand zero oth-\nerwise. Typically, a 0-1 loss is used for lm(·), which makes\nthe objective function discrete and difﬁcult to optimize.\nHowever, a common approximation for the loss function\nis to replace the 0-1 loss with a logistic function [9],\nlm(Xj; Λ) =1\n1 + exp(−γdm(Xj; Λ) +θ)(14)whereγandθare experimental constants and dm(Xj; Λ)\nis a misclassiﬁcation measure, which is negative with a\ncorrect classiﬁcation and positive when a classiﬁcation er-\nror is made.\nA good indication of misclassiﬁcation is the distance\nbetween the correct class and competing classes; therefore,\nthe chosen misclassiﬁcation measure is based on the gen-\neralized log-likelihood ratio [9]:\ndm(X; Λ) =−loggm(X; Λ) + log [Gm(X; Λ)]1/η(15)\nwhere\ngm(X; Λ) = max\nqπ(m)\nq0N/productdisplay\nn=1a(m)\nqn−1qnb(m)\nqn(cn) (16)\nGm(X; Λ) =1\nM−1/summationdisplay\np,p/negationslash=mexp[gp(X; Λ)η] (17)\nwhereηis an experimental positive constant and the su-\nperscript(m)refers to the m-th HMM. Note the misclas-\nsiﬁcation measure in (15) compares the probability of the\ntarget class against a geometric average of the competing\nclasses. The parameter ηdetermines the importance of the\ncompeting classes by the degree of competition with the\ntarget class. In particular, as η→∞ , (17) returns only the\nmost competitive class. A gradient probabilistic descent\nprocedure [9] produces a set of parameters that yields a\nlocal optimum of (13) through the update equations\nΛτ+1= Λτ−/epsilon1∂lm(Xj; Λ)\n∂Λ/vextendsingle/vextendsingle/vextendsingle/vextendsingle\nΛ=Λτ(18)\nIn order to keep the necessary constraints for an HMM den-\nsity, the following transformations are used [9]:\n˜µ(m)\nd(b) =µ(m)\nd(b)\nσ(m)\nd(b)(19)\n˜σ(m)\nd(b) = logσ(m)\nd(b) (20)\n4. RESULTS\n4.1 Experimental Setup\nThe evaluation database is the set of studio albums by The\nBeatles , which were transcribed at the chord level by Chris\nHarte et. al [10]. As in the 2008 MIREX contest, only the\nMajor andminor chords are used for the evaluation. All\nextended chords, Augmented , and diminished chords are\nmapped to the the base root, Major , and minor chords, re-\nspectively. A two-fold evaluation is implemented, where\nhalf the albums are used as a training set and the remain-\ning half are used as a test set in the ﬁrst fold. In the second\nfold, the roles of the training and test set are swapped. Note\nall songs from a particular album occur in either the test or\ntraining set for each individual fold. This is the same setup\nas MIREX 2008, but the third fold in MIREX 2008 was\nremoved because it was observed that the test cases where\nalready covered in the ﬁrst two folds. Prior to HPSS, audio\nis downsampled to 11025 Hz. In addition, chord boundary\n61210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFold BL FT FT+TC FT+TC+MCE\n1 57.84 57.84 61.91 73.59\n2 61.74 61.74 64.97 71.35\nTotal 59.95 59.95 63.57 72.37\nTable 1 . Classiﬁcation accuracies for Fourier transform\nfeatures and tuning compensation (BL = baseline, FT =\nFourier Transform, TC = tuning compensation, MCE =\nminimum classiﬁcation error).\nFigure 3 . Classiﬁcation accuracy versus iteration number.\ninformation is assumed to be known and results are given\nin percentage of correctly recognized isolated chord seg-\nments, except in Section 4.3, where results are given in\nframe accuracy.\n4.2 Isolated Chord Recognition Results\nTable 1 details the improvement over the ML approach,\nwhere chords are modeled with a single Gaussian distribu-\ntion with a full covariance matrix for the baseline, Fourier\ntransform, and Fourier transform with tuning compensa-\ntion cases. The MCE results listed used 50 iterations of\nthe gradient probabilistic descent algorithm. The parame-\ntersγ,θ, andηwere found experimentally by using a set\nof ﬁve songs from the training set as a cross-validation set.\nAfter cross-validation, the entire training set is used to re-\ntrain the system.\nTuning compensation provides a modest, but consistent\ngain in performance. Applying a Fourier transform does\nnot change the performance from the baseline. However,\nthe main advantage of applying a discrete Fourier trans-\nform is to attenuate the correlation in chroma features, and\nis equivalent to a discrete cosine transform for real, sym-\nmetric data [14]. In addition speech processing algorithms,\nsuch as MCE, assume diagonal covariance matrices in the\nGMM observation probability. Therefore, applying MCE\nis straightforward and results in a drastic increase in perfor-\nmance over ML estimation. In particular Figure 3 demon-\nstrates, generally, each iteration of the gradient probabilis-\ntic descent algorithm improves the classiﬁcation rate.\nTo understand the types of errors that remain, the con-\nfusion matrix is presented in Table 2. It is observed that\nMajor chords are classiﬁed more accurately than minorFold BL FT FT+TC FT+TC+MCE\n1 74.96 74.96 77.04 77.90\n2 72.95 72.95 73.54 74.51\nTotal 73.46 73.46 75.20 76.10\nTable 3 . Frame accuracy for continuous chord recognition.\n# Frames BL FT+TC FT+TC+MCE\n0 73.91 75.20 76.12\n1 76.59 77.92 78.93\n2 78.61 79.94 80.99\nTable 4 . Frame accuracy versus number of frames re-\nmoved at chord boundary.\nchords. Speciﬁcally, many errors are due to recognizing\nminor chords with the correct root, but wrong mode; i.e.,\nthe parallel Major chord. For example, 82% of c minor\nchords are recognized as C Major . The second most com-\nmon type of error is in mistaking a Major chord for its\nminor , which are chords that share the same key signa-\nture, but differ in the root note. For example, e minor is\nconfused with G Major 12% of the time. Note, that no lan-\nguage model is used in this current paper since the goal of\nthis paper is to study the confusions that arise due acoustic\nconfusability in the chroma/HMM framework.\n4.3 Continuous Chord Recognition\nWhile this paper is mainly concerned with isolated chord\nclassiﬁcation, an additional experiment demonstrates the\nperformance of continuous chord recognition. In this case,\nthe frame accuracy is used as the performance metric. The\nresults are presented in Table 3. As expected, improvement\nis less pronounced with adequate boundary information.\nFurther analysis shows that one reason for the performance\ndrop is due to identifying chord boundaries. As shown in\nTable 4, allowing a tolerance region of two frames on ei-\nther side of a true chord transition point increases the frame\naccuracy. Speciﬁcally, 20% of the error occurred within\ntwo frames of a chord transition point when at least one\nchord to either side of the transition point was detected\ncorrectly. In [2], it was demonstrated that chroma/HMM\nsetup performed well during forced alignment (i.e., when\nthe chord sequence is given), but poorly when no informa-\ntion on the chord sequence was given. These results indi-\ncate that the chord detection problem might beneﬁt from\ntreating the two tasks separately and optimizing each task\nindividually.\n5. CONCLUSIONS\nThis paper considers audio chord detection as two sep-\narate tasks: (1) classifying what chords are played and\n(2) determining when chords begin and end. Several ad-\nvanced pre-processing techniques are implemented such as\nHPSS, which attempts to separate transients and percussive\nsources from the harmonic spectrum. Further, eliminating\n613Poster Session 4\nC C# D D# E F F# G G# A A# B c c# d d# e f f# g g# a a# b\nC 91 2 2 2 2\nC# 71 2 2 12 5 7\nD 1 89 1 2 2 1 3\nD# 81 1 9 1 6 1\nE 94 1 2 1\nF 3 91 2 1 1 1 2\nF# 1 1 91 1 1 3 2 1\nG 1 1 94 1\nG# 3 83 3 10\nA 1 1 94 1 1\nA# 1 1 1 91 1 1 3 1 1\nB 1 1 1 1 3 1 85 1 2 3\nc 82 9 9\nc# 1 3 1 5 1 2 87\nd 6 12 13 3 3 1 57 4 1\nd# 33 67\ne 5 1 19 12 5 50 8 1\nf 1 1 1 11 16 6 63 1\nf# 2 1 2 7 11 76 1\ng 3 14 9 1 1 71\ng# 4 12 85\na 5 2 2 13 78\na# 2 2 40 56\nb 6 1 20 71\nTable 2 . Chord confusion matrix (%). Rows are true chords, columns are hypothesized chords. Capital letters represent\nMajor chords and lowercase letters represent minor chords.\nthe correlation between chroma features allows for the use\nof many speech processing tools because these tools are\nbuilt using the assumption of diagonal matrices in the ob-\nservation probability densities.\nIn this paper, tuning compensation and MCE enhance\nthe chord recognition task over traditional maximum like-\nlihood by reducing the confusion due to noise in the feature\nextraction stage. In the future, the authors hope to incorpo-\nrate other advanced speech processing techniques, such as\nN-best re-scoring, to combat other areas of confusion such\nas the confusion between minor chords and their relative\nand parallel Major equivalents. Finally, it was observed\nthat even when chords are detected correctly, 20% of the\nerror occurred at chord transition points. Therefore, the\nauthors are investigating chord transition detection algo-\nrithms to optimize the second task of chord detection.\n6. REFERENCES\n[1] T. Kawakami, M. Nakai, H. Shimodaira, S. Sagayama:\n“Hidden Markov Model Applied to Automatic Harmo-\nnization of Given Melodies,” IPSJ Technical Report ,\n99-MUS-34, pp. 59-66, Feb., 2000. ( in Japanese )\n[2] A. Sheh and D.P.W. Ellis: “Chord Segmentation and\nRecognition Using EM-trained Hidden Markov Mod-\nels,” Proc. ISMIR , pp. 183–189, 2003.\n[3] J.P. Bello and J. Pickens: “A Robust Mid-level Rep-\nresentation for Harmonic Content in Musical Signals,”\nProc. ISMIR , pp. 304-311, 2005.\n[4] K. Lee and M. Slaney: “Automatic Chord Recognition\nfrom Audio Using an HMM with Supervised Learn-\ning,” Proc. ISMIR , pp. 133-137, 2006.\n[5] K. Lee and M. Slaney: “Acoustic Chord Transcription\nand Key Extraction from Audio Using Key-dependentHMMs Trained Synthesized Audio,” IEEE TASLP ,\nV ol. 16, No. 2, pp. 291-301.\n[6] C.A. Harte, M.B. Sandler, and M. Gasser: “Detecting\nHarmonic Change in Musical Audio,” Proc. Audio Mu-\nsic Comput. Multimedia Workshop , pp. 21-26, 2006.\n[7] J. Downie: “Music Information Retrieval Eval-\nuation eXchange (MIREX),” [Online]. Available:\nhttp://www.musicir.org/mirex/2008/index.php/\n[8] N. Ono, K. Miyamoto, J. Le Roux, H. Kameoka,\nS. Sagayama “Separation of a Monaural Audio Sig-\nnal into Harmonic/Percussive Components by Comple-\nmentary Diffusion on Spectrogram,” Proc. EUSIPCO ,\n2008.\n[9] B.-H. Juang, W. Chou, C.-H. Lee: “Minimum Classi-\nﬁcation Error Rate Methods for Speech Recognition,”\nIEEE TSAP , V ol. 5, No. 3, pp. 257-265, 1997.\n[10] C. Harte, M. Sandler, S. Abdallah, and E. G ´omez\n“Symbolic Representation of Musical Chords: A Pro-\nposed Syntax for Text Annotations,” Proc. ISMIR ,\npp. 66-71, 2005.\n[11] H. Papadopoulos and G. Peeters: “Large-scale Study of\nChord Estimation Algorithms Based on Chroma Rep-\nresentation and HMM,” Intern. Wkshp. Content-Based\nMultimedia Indexing , pp. 53-60, 2007.\n[12] J. Brown: “Calculation of a constant Q spectral trans-\nform,” J. Acoust. Society America , V ol. 89, No. 1,\npp. 425-434, 1991.\n[13] S. Kostka and D. Payne: Tonal Harmony , McGraw\nHill, 2004.\n[14] L. Rabiner and B.-H. Juang: Fundamentals of Speech\nRecognition , Prentice Hall, New Jersey, 1993.\n614"
    },
    {
        "title": "Meter Class Profiles for Music Similarity and Retrieval.",
        "author": [
            "Matthias Robine",
            "Pierre Hanna",
            "Mathieu Lagrange"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415788",
        "url": "https://doi.org/10.5281/zenodo.1415788",
        "ee": "https://zenodo.org/records/1415788/files/RobineHL09.pdf",
        "abstract": "Rhythm is one of the main properties of Western tonal music. Existing content-based retrieval systems generally deal with melody or style. A few existing ones based on meter or rhythm characteristics have been recently proposed but they require a precise analysis, or they rely on a low-level descriptor. In this paper, we propose a midlevel descriptor: the Meter Class Profile (MCP). The MCP is centered on the tempo and represents the strength of beat multiples, including the measure rate, and the beat subdivisions. The MCP coefficients are estimated by means of the autocorrelation and the Fourier transform of the onset detection curve. Experiments on synthetic and real databases are presented, and the results demonstrate the efficacy of the MCP descriptor in clustering and retrieval of songs according to their metric properties.",
        "zenodo_id": 1415788,
        "dblp_key": "conf/ismir/RobineHL09",
        "keywords": [
            "meter",
            "rhythm",
            "tonal music",
            "content-based retrieval",
            "melody",
            "style",
            "meter class profile",
            "MCP",
            "tempo",
            "beat subdivisions"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMETERCLASS PROFILESFORMUSIC SIMILARITYAND RETRIEVAL\nMatthias Robineand PierreHanna\nLaBRI -UniversityofBordeaux\n351,cours delaLib´ eration\n33405TALENCECedex -FRANCE\nfirstname.name@labri.frMathieuLagrange\nTelecomParisTech\n46,rueBarrault\n75634PARIS Cedex 13 -FRANCE\nlagrange@telecom-paristech.fr\nABSTRACT\nRhythm is one of the main properties of Western tonal\nmusic. Existing content-based retrieval systems generall y\ndeal with melody or style. A few existing ones based on\nmeter or rhythm characteristics have been recently pro-\nposed but they require a precise analysis, or they rely on\na low-level descriptor. In this paper, we propose a mid-\nleveldescriptor: theMeterClassProﬁle(MCP).TheMCP\niscenteredonthetempoandrepresentsthestrengthofbeat\nmultiples,includingthemeasurerate,andthebeatsubdivi -\nsions. TheMCPcoefﬁcientsareestimatedbymeansofthe\nautocorrelation and the Fourier transform of the onset de-\ntectioncurve. Experimentsonsyntheticandrealdatabases\nare presented, and the results demonstrate the efﬁcacy of\ntheMCP descriptorin clusteringandretrievalofsongsac-\ncordingtotheirmetricproperties.\n1. INTRODUCTION\nThe amount of digital music is rapidly increasing, and is\nmostly comprised of Western pop music. New interfaces\nforbrowsing,classifyingorsearchinghavetobeproposed.\nContent-based retrieval systems generally consider musi-\ncal properties related to melody or style. But taking into\naccount other characteristics such as rhythm may lead to\nthedevelopmentofusefultoolsforhelpinguserstobrowse\nintolargedatabases.\nResearch regarding rhythmic or metric properties typ-\nically involves tempo or meter analysis. Several systems\nhavebeendevelopedforimprovingtempoinduction[1,2],\nmeteranalysis[3,4],orestimationofthetimesignatureof\naudiosongs[5]. Otherworksfocusontheautomaticclassi-\nﬁcationofsongsaccordingtotheirrhythmicproperties[6] .\nIncontrast,onlyafewmethodshavebeenpresentedforre-\ntrievingsongsbyrhythmicsimilarity. A few existingones\nconsider a low-level descriptor such as beat spectrum [7],\nacoustic features [8], or spectral descriptors [9]. Others\nprecisely analyze tempo, meter and/or time signature [5]\nandareconsequentlylimitedbytheerroranalysis.\nInthis paper,we presenta new mid-leveldescriptorfor\nretrieving music according to the metric properties. The\nestimation of this descriptor requires neither a complete\nanalysis of the time signature nor the meter of a song, but\nPermission to make digital or hard copies of all or part of thi s work for\npersonal or classroom useis granted without fee provided th at copies are\nnotmadeordistributed forproﬁtorcommercialadvantagean dthatcopies\nbear this notice and the full citation on theﬁrstpage.\nc/circlecopyrt2009 International Society for MusicInformation Retrieva l.only the prior knowledge of its tempo. In comparison to\na low-level feature, the essential metric information is re -\nduced to a few values which characterize the metric prop-\nerties of any song. In Section 2, we discuss the notion of\nmetrical structure. The new mid-level descriptor and the\nassociatedanalysismethodaredetailedinSection3. Then,\nclustering and retrieval experiments are presented in Sec-\ntion4. Finally,discussionoftheseresultsandperspectiv es\nareproposedin Section5.\n2. MUSICAL METER\n2.1 MetricalStructure\nIn Western tonal music, the periodic alternation of strong\nandweakbeatsleadstoametricalhierarchyknownasmet-\nrical structure. The symbolic representation of this struc -\nture is present in score notation using markings such as a\ntime signature,barlines, anddynamicaccents.\nThe Generative Theory of Tonal Music (GTTM) [10]\nproposesamodelofthemetricalstructure,wherethemeter\nof a musical piece may be represented by multiple levels\nof beats. The periodicity of beats is reinforced from level\nto level, and it is the interaction of the differentlevels th at\nproducesthesensationofmeter. Thisrepresentationisals o\nusedbyTemperley[11]inhisproposalofapreferencerule\nsystem formeter.\nAccording to [10], while there are ﬁve or six metri-\ncal levels in a piece, one is particularly central: the tac-\ntuslevel. The tactus identiﬁes a perceptually prominent\nlevel, with the levels immediately smaller and immedi-\nately larger. It refers to the perceived tempo, the internal\nclock[12]. Itcorrespondshighlywiththenotatedunittime\nof a musical piece, but it can differs. Lee [13] indicates\nthus that listeners may revise meter to always get a tactus\nbetween300msand600ms.\nIn the following, we assume that the tactus level corre-\nsponds to the tempo and the basic unit time of the music,\nwhen it is known or notated. We choose also to use with-\nout distinction the terms tactusandbeat. As the metrical\nstructure is hierarchical, levels lower than that of the tac -\ntus can be called subtactus levels and represent divisions\nofthebeat. Thesmallestdivisionisgenerallycalled tatum\nortick. Alternatively,higher levels are termed supertactus\nlevels,andcontainmultiplesofthebeatduration,includi ng\nthemeasurelevel.\n2.2 Time Signature\nWe canrestrictthenotionofmeterto twolevels,thefaster\nof which provides the element, and the slower of which\n639Poster Session 4\ngroup them. According to Gouyon [14], this is close to\ntheusualdescriptionofmeterthatcanbefoundinascore,\ngivenbythetime signatureandthebarlines.\nThe time signature consists of 2 integers arranged ver-\ntically, e.g.4\n4or6\n8. The upper number of the signature in-\ndicates the numberof units in a bar, with the value of unit\ngiven by the lower number of the signature ( e.g.4 for a\nquarternote). Iftheuppernumberisdivisibleby3andthe\nlower by2, the time iscompoundandthe numberof beats\npermeasureisgivenbytheuppernumberdividedby3. In\nthis case, the unit of time, i.e.the beat, is divided triply\nat the smaller level. Otherwise, in simple time, the upper\nnumberindicatesthe numberof beatspermeasureandthe\nbeat is dividedduply. Table 1 presents the time signatures\nmainlyusedinWestern tonalmusic.\n3. METER CLASS PROFILE\nWe introduce in this section a new descriptor to represent\nthemetricalstructureofthemusiccalledMeterClassPro-\nﬁle (MCP). We describe the method used for estimating\ntheMCP, andprovideexamplesforillustrationofitsuse.\n3.1 Properties\nMCP is a real-valued vector providing information of the\nstrengthofthedifferentmetricallevelswithinthemusic. It\niscenteredonthetactuslevel: thebeatmultiples,includi ng\nthe measurelevel, are representedon the left, and the beat\nsubdivisions, including the tatum level, on the right. The\nchoice was made to represent the relative strength of ac-\ncentsatmultipleratesofthetempo: 2,3,4,5,7,9,and11,\nand also at subdivisionsof the tempo: 1/2,1/3,1/4,1/6,\n1/8,1/12. MCP isthusa vectorofthirteendimensions.\nA MCP corresponding to a 4/4time signature would\ncontainhighamplitudesforthebincorrespondingto 4times\nthe tempo, i.e.beat multiple 4representing the measure\nperiodicity, for beat multiple 2(in4/4, there are accents\nevery two beats), for beat subdivision 1/2(because 4/4\nis in simple time), and perhaps beat subdivisions 1/4and\n1/8(if16thor32thnotesoccur). Afewexamplesarepre-\nsentedinFigure2.\nAs the MCP is independent from tempo, its represen-\ntation does not change with tempo variations. It may be\nconsidered as a mid-level descriptor, since the metric in-\nformation is summarized to a 13-dimension vector with-\nout identifying a particular time signature. Additionally ,\nthe amplituderatiosfromthe differentmetricallevelspro -\nvide a meaningful way to handle the meter of a musical\npiece. TheMCPcould,forexample,alsoindicateadegree\nof swing, by considering the balance between duple and\ntriplebeatsubdivisions.\n3.2 Estimation\nThe method proposed here for computing MCP relies on\nexisting analysis methods recently described for estimat-\ning tempo [15]. It has been implemented using the MIR\ntoolbox [16]. The main steps are illustrated on Figure 1,\nwiththeexampleofthecountrysong Wanted(A.Jackson),\nannotatedwitha 3/4timesignature.\nIn this paper, we consider only one global MCP per\naudio musical signal. We thus choose to analyze a large\nframeofmusic(length 60seconds). Themeterisassumedto be stationary during this frame. An onset-energy func-\ntion is ﬁrst extracted from the audio signal by taking into\naccount spectral energyﬂux [17]. Then, dominantperiod-\nicities(orfrequencies)areestimated.\nTwo types of observations, respectively termed Onset\nDiscrete Fourier Transform (ODFT) or Onset Autocorre-\nlation Function(OAF), are considered. Theircomplemen-\ntary properties are discussed for tempo estimation in [2].\nIn the experiment presented in this paper, the OAF and\nODFT are both computed and normalized and the tempo\nfrequencyisknown.\nThe analysis method of MCP locates both periodicities\ncorresponding to beat multiples (related to measure) and\nbeat subdivisions. Estimation of multiples and subdivi-\nsions is carried out usingthe complementarypropertiesof\nthetwoobservations. Ononehand,theODFTofaperiodic\nsignalisasetofharmonicallyrelatedfrequencies,anditi s\ndifﬁcult to determine predominant frequencies above the\ntempofrequency. Therefore,we onlyestimate frequencies\nlower than the tempo frequency. These frequenciescorre-\nspond to beat multiples (ﬁrst part of MCP), in particular\nthat ofthe measure.\nOn the other hand, the OAF of a periodic signal is a\nset of periodically related lags. It is thus difﬁcult to mea-\nsure predominant periodicities higher than the tempo pe-\nriod. The OAF is only considered for estimating periods\nlower thanthe tempoperiods. These periodsarerelated to\nbeatsubdivisions(secondpartoftheMCP).\n0 1 2 3 4 5 6 7 8 9−0.200.20.40.6Onset autocorrelation\nlag (s)coefficients\n0 2 4 6 8 10 12 14 16 18 20012345Onset spectrum\nfrequency (Hz)magnitude\n119754321/21/31/41/61/81/1200.20.40.60.8\nbeat multiple/subdivisionamplitudeMeter Class Profile\nFigure1. DifferentstagesoftheanalysismethodofMeter\nClass Proﬁle forthe song Wantedof A. Jackson(time sig-\nnature 3/4): fromtop to bottom, the autocorrelationfunc-\ntion, the spectrumof the onset functionand the MCP esti-\nmated, showing peaksat beat multiple 3and beat subdivi-\nsion1/2.\nWith priorknowledgeof the tempo,the ﬁrst part of the\nMCP is estimated from the ODFT and the second part is\nestimated from the OAF. Consequently, period bands cor-\nresponding to harmonics of the frequency tempo are ana-\nlyzed when considering the OAF. Only the six harmonics\n(2,3,4,6,8, and12) are taken into account. The amount\nofenergyofOAF withinathin frequencybandaroundthe\nrelated periodicities directly determines the amplitude r e-\n64010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nDuple Triple Quadruple 5-uple 7-uple 9-uple 11-uple\nSimple2\n12\n22\n42\n83\n13\n23\n43\n84\n14\n24\n44\n85\n15\n25\n45\n87\n17\n27\n47\n89\n19\n29\n49\n811\n111\n211\n411\n8\nCompound6\n26\n46\n86\n169\n29\n49\n89\n1612\n212\n412\n812\n1615\n215\n415\n815\n1621\n221\n421\n821\n1627\n227\n427\n827\n1633\n233\n433\n833\n16\nTable1. Timesignatures. Mostcommonsignaturesareduple,triple andquadrupletimeinWesterntonalmusic. Notations\n9/2,9/4and9/8maybeusedeitherfora simplemeasureof9 pul ses,orfora compoundmeasureof3 pulses.\nlated to the beat subdivisions of the MCP. In our imple-\nmentation, the width of the frequency bands for cumulat-\ning the energy in the ODFT and OAF has been set to 5%.\nIn Figure 1, the tempo has been annotatedto 1.5Hz and is\nshowed on the OAF with a solid line. When considering\nperiods lower than1\n1.5= 0.66s, energy is located around\nperiodof 0.33s,correspondingtothebeatsubdivision 1/2.\nThecontributiontothebeatsubdivisionofthecorrespond-\ning MCP is thus signiﬁcant and clearly indicates a simple\nmeter.\nThe ODFT is considered in a similar way. Only the\nsevensub-harmonics( 2,3,4,5,7,9, and11)ofthe tempo\nfrequencyareconsidered. Energywithinathinbandaround\nthese sub-harmonics determines the amplitude related to\nthebeatmultiples. InFigure1,onlytheenergyaroundfre-\nquencies1.5\n2,1.5\n3, ...,1.5\n11, contributes to the ﬁrst part of\nthe MCP. In this example, the sub-harmonic1.5\n3is signif-\nicantly predominantand results in a substantial amplitude\nin the MCP. This high amplitude thus indicates that the\nsongischaracterizedby 3beatspermeasure.\nOtherexamplesofMCPcomputationforrealaudiosongs\nare shown in Figure 2. In each example, the MCP looks\nverydifferent,accordingto their metricproperties. Inpa r-\nticular, the highest value in the ﬁrst part of the MCP gen-\nerally indicates the number of beats per measure, whereas\nthe highest value of the second part is related to the beat\nsubdivision.\n3.3 DistancebetweenMCP\nTheMCPisproposedformusicretrievalpurposes. There-\nfore, a method for computing a matching score between\ntwoMCPhastobedeﬁned. Severaldistancesarepossible,\nhoweverthisisadifﬁcultselectionduetothedifferencein\ntheanalysisprocessesofthetwopartsoftheMCP.Wethus\nproposeto considera globalscore sas the combinationof\nthetwoscores s1ands2obtainedwiththetwopartsofthe\nMCP:\ns=αs1+ (1−α)s2 (1)\nwhere αisa ﬁxedweightingvaluein theinterval [0; 1],s1\nthe comparison score related to the meter multiple part of\nthe MCP, and s2the comparison score related to the me-\nter subdivision . These two scores s1ands2are calculated\naccordingto correlation:\nsi(MCP1,MCP2) =c(MCP1,MCP2)/radicalbig\nc(MCP1,MCP1)/radicalbig\nc(MCP2,MCP2)\nc(MCP1,MCP2) =N/summationdisplay\ni=1MCP1(i)MCP2(i) (2)\nwhereMCP1andMCP2areMCP vectorsofsize N.119754321/21/31/41/61/81/1200.10.20.30.4amplitudeI Hung my Head (9/8)\n119754321/21/31/41/61/81/1200.20.40.60.8Take my Breath Away (4/4)amplitude\n119754321/21/31/41/61/81/1200.050.10.150.2Rhythm of my Heart (12/8)amplitude\n119754321/21/31/41/61/81/1200.10.20.30.4AmplitudeTake Five (5/4)\nFigure 2. Examples of MCP computed from real audio\nsongswithdifferenttimesignatures,respectively 9/8,4/4,\n12/8and5/4.\n4. EXPERIMENTS\nInthissection,experimentsarepresentedthatdemonstrat e\ntheabilityofMCPtodiscriminatesongsthathavedifferent\nmetriccharacteristics. Theﬁrstexperimentsdealwithclu s-\nteringabilities,andothersecondconcernssongretrieval .\n4.1 Databases\nTwodatabasesareconsideredinthispaper. Theﬁrstoneis\ncomposed of short artiﬁcial audio musical pieces ( 60sec-\nondslongat 16kHz)thathavebeensynthesizedaccording\nto different metric properties. The tempo was set to 1Hz\nandclasseshavebeenconstitutedconsideringthetimesig-\nnature. Classes with 2,3,4,5,7,9and11beats per mea-\nsurehavebeenbuilt,insimpletimeandincompoundtime.\nFor each class, 2differentdistributionsof the strong beats\ninthemeasurehavebeenchosen,tosynthesize200pieces.\nA process has been achieved to randomly add 16thnotes\nin thepieces. Thedatabasecontains2800differentﬁles.\nThe second database is a collection of real pop audio\nsongs indexed using the time signature. The noisecol-\nlection contains 476simple-meter songs with 2or4beats\nper measure (sampling rate 44.1kHz).We constitute an-\nother collection of 54songs with different metric proper-\nties. Someofthemareincompoundtime,whileothersare\n641Poster Session 4\ncharacterized by a different number of beats per measure.\nTherefore, 7different classes are assumed according to 7\ndifferent time signatures. The composition of each class\nis presented in Table 2. Different classes are also deduced\nfrom time signatures: one class is composed of 13songs\nwith3beatspermeasure,anotherclasscomprises 24com-\npound time songs. For all these 54songs, tempo has been\nmanuallyannotated.\nIt isimportantto noticethat all groundtruthannotation\nof the songs from the noisedatabase have not been pre-\ncisely veriﬁed; ambiguous meter, large tempo variations,\nand short-duration time signature changes may result in\nevaluationerrorsthatmayunderestimatethe qualityofthe\nclusteringandretrievalsystemspresentedhere.\nSimple Compound\n3/45/47/411/46/89/812/8\n1110 7 2 6216\nTable 2. Numberofsongswithineachmeterclassconsid-\neredforthe experiments.\n4.2 Clustering\nWepresentheretheclusteringabilitiesoftheproposedme-\nter featureonthetwodifferentdatabases.\n4.2.1 EvaluationMetrics\nThe task can be here reworded as follows: “Do the ele-\nments classiﬁed together actually belong to the same time\nsignature?”. The similarity of the elements of the given\ndatabase is ﬁrst computed. As the databases considered\nin those experimentsare of relatively small sizes, we con-\nsider an unsupervised clustering scheme (the k-means) to\nperformthe clusteringtask, i.e.each element ekis givena\nclusteringtag tk. Thecorrectnumberofclassesisgivento\ntheclusteringalgorithm.\nThe clusteringmatrix Mis nextcomputed,whereeach\nentryisdeﬁnedas:\nM(Cx, Cy) =#{(ek, el)|tk=tl∧ek∈Cx∧el∈Cy}\n#{(ek, el)|ek∈Cx∧el∈Cy}\n(3)\nwhere CxandCyaretheclassesgivenasgroundtruth,and\n(ek, el)is a couple of elements. In order to attenuate the\nimpact of the random initialization of the k-means algo-\nrithm, the classiﬁcation is done 10times and the mean re-\nsultoverthe 10iterationsisconsidered. Thebettertheclas-\nsiﬁcation, the higherthe ratiobetweenthe diagonalvalues\nofMandtheremainingofthematrix.\nEvaluationoverthe syntheticdatabaseallowsustoval-\nidate the proposed approach in a controlled environment.\nFigure 3 depicts the clustering matrix Mfor the synthetic\ndatabase. Theresultsareverysatisfyingingeneralasmost\nof the values are concentrated on the diagonal, meaning\nthatmostoftheclustergeneratedbythek-meansalgorithm\nfrom the features correspond to the actual metrical struc-\ntureclasses. Typical errors are due to a confusionbetween\nsimple and compoundtime, e.g. between3/4 and9/8 time\nsignatures, or sometimes between classes for which the\nnumbers of beats per measure have common factors (e.g.,\n4/4and2/4,or9/4and3/4).2−46−83−49−84−412−85−415−87−421−89−427−811−433−82−4\n6−8\n3−4\n9−8\n4−4\n12−8\n5−4\n15−8\n7−4\n21−8\n9−4\n27−8\n11−4\n33−8\nFigure3.Clusteringmatrix overthe syntheticdatabase.\n6_8 3_4 9_8 12_8 5_4 7_4 11_46_8\n3_4\n9_8\n12_8\n5_4\n7_4\n11_4\nFigure4.Clustering matrixovertherealdatabase.\nReal data is now considered to evaluate the robustness\nof the MCP. Evaluations based on clustering allows us to\nﬁnelyanalyzethepropertiesoftheMCPwithrespecttothe\ndifferent time signature classes, as shown in Figure 4. We\ncan note that a clear distinction is made between simple\nandcompoundtime.\n4.3 Retrieval\nIn this section, we propose the evaluation of a music re-\ntrieval system based on the MCP. The 54songs from the\nreal song collection are successively considered as query.\nThe retrieval system computes a similarity score between\nthequeryandallthesongsofthedatabase. Thedatabaseis\ncomprisedof 530differentsongs,includingallthequeries\nand all the noisecollection. These songs are then ranked\nfrom most to least similar. For each query, we expect to\nretrieveall the songsbelongingtothe class ofthe queryat\nthe top rank. In the following evaluation, results are pre-\nsented with Precision at Top 1,Nand2N, in which N\ndenotesthesize oftheclassofthequery.\n4.3.1 SyntheticQuery\nTheﬁrstexperimentsconcernretrievalbasedonasynthetic\nquery. This query is a ﬂat input, i.e.a MCP deﬁned by a\nbinary string. For example, if the songs searched have a\n3/4timesignature,thesyntheticqueryisthe MCP\n[0,0,0,0,0,1,0,1,0,1,0,1,0]\nwheretheonlynon-nullvaluescorrespondtothebeatmul-\ntiple3and the beat subdivisions1\n2,1\n4and1\n8are related to\n64210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nsimple timesubdivisions. Otherexperimentsonlyconcern\nthe left or right half of the MCP, since retrieval may fo-\ncus on beat multiples or beat subdivisions. For example,\nretrieving compound time songs is tested by considering\nonly the second part of the MCP with a synthetic query\nsuchas:\n[0,0,0,0,0,0,0,0,1,0,1,0,1]\nwhichexhibitsbeatsubdivisions1\n3,1\n6,and1\n12.\nTable 3 shows the results of the retrieval experiments\nfromthosesyntheticqueries. At theexceptionoftime sig-\nnatures 11/4and9/8, one song of the correct class is al-\nwayscorrectlyretrievedattheﬁrstrank. Thesetwoexcep-\ntions may be explained by the small size the two classes\nconcerned (each comprised of only two songs). Concern-\ning the 11/4class, related songs are retrieved at ranks 2\nand11. Concerning the 9/8class, related songs are re-\ntrieved at ranks 3and5. Even if the precision at top 1is\nnull,theretrievalresultsarethusquitegood.\nAverage precision at top 2Nindicates that more than\nhalf the songs are generally retrieved within the ﬁrst 2N\nranks. Considering one part of the MCP for retrieving\nsongsseemstobeeffective. Forexample,asyntheticquery\nallows the retrieval of 75%of the songs of the compound\ntime class at the ﬁrst Nranks. Almost all the compound\ntime songs are ranked within the ﬁrst 2Nbest matches.\nTheresultsoftheseexperimentsconﬁrmthequalityofthe\nMCP as a metric descriptor for retrieval, since the MCP\ncomputedfromarealaudioﬁleissimilartoitstimesigna-\ntureproperties.\nClass SizeTop 1Top N Top 2N\n6/8 6 1 0.333 0.500\n3/4 11 1 0.364 0.636\n9/8 2 0 0.000 0.500\n12/8 16 1 0.312 0.375\n5/4 10 1 0.800 0.900\n7/4 7 1 0.286 0.429\n11/4 2 0 0.500 0.500\nTotal byClass 70.714 0.371 0.549\n3beats/mes 13 1 0.846 1.000\n5beats/mes 10 1 0.800 1.000\n7beats/mes 7 1 0.571 0.571\n11beats/mes 2 1 0.500 0.500\nCompound 24 1 0.750 0.958\nTable 3. Results of the retrieval system based on MCP,\nconsideringa syntheticquery.\n4.3.2 AudioQuery\nWe present a second experiment to test the ability of re-\ntrievingreal songsusinga realsongasa query. Theappli-\ncationsrelatedtotheseexperimentsareQuery-by-Example\nsystems, which allows users to perform a database search\nfor songsthat are similar to a givenquerysong. The simi-\nlarityherereliesonthemetricproperties,butdoesnothav e\nto beexplicitlydeterminedbytheuser.\nSince the query is always retrieved at the ﬁrst rank, we\npropose to removethe query from the database. Precision\nat top Nis the number of correct songs retrieved in the\nﬁrstN−1ranks divided by N−1,Nbeing the size ofthe class considered. Precisionat top 2Nis the numberof\ncorrect songs retrieved in the ﬁrst 2N−1ranks divided\nby2N−1. By using all the songs of each class as a\nquery, Nprecisions are computed and averaged for each\nclass. Then, the total average is computedby query,or by\nclass. It is respectively denoted Total by Query , andTotal\nby Class. Such evaluations are respectively named First\nTierandSecondTier in [18].\nTable 4 shows the results of the retrieval experiments.\nThe value αdetermines the weighting between the two\nparts of the MCP, and has been set to 0.6. The average\nresultsby queryindicate that 44%of the queriesallow the\nretrieval of one song of the same class at the ﬁrst rank,\n53%of the class is retrieved at the ﬁrst 2Nranks. The\naccuracy is lower than the results obtained with synthetic\nqueries. This can be explained by the presence of songs\nin thenoisedatabase that may be similar to the query. For\nexample,aquerywithtimesignature 3/4oftenleadstothe\nretrieval of songs with time signature 9/8, since the num-\nberofbeatspermeasurearethesameforeachofthesetwo\nclassesandsincethebeatsubdivisionsmaybevaryingdur-\ning the analyzed song (for example in the case of swing).\nAttheopposite,theclass 5/4leadstothebestresults: 80%\nofthecorrectsongsare retrieved.\nMoreover, difﬁculties with annotations of time signa-\ntures may lead to errors in evaluation. For example, it is\nsometimes difﬁcult to discriminate 6/8songs from 12/8\nsongs. This difﬁculty is illustrated by the poor results for\nclass6/8, whereasthe compoundtime class leads to good\nresults.\nClasses SizeTop 1Top N Top 2N\n6/8 60.000 0.033 0.242\n3/4 11 0.727 0.500 0.649\n9/8 20.000 0.000 0.333\n12/8 16 0.562 0.579 0.714\n5/4 10 0.700 0.567 0.695\n7/4 70.000 0.095 0.132\n11/4 20.000 0.000 0.000\nTotal byClass 70.284 0.253 0.395\nTotalby Query 54 0.444 0.394 0.529\n3beats/mes 13 1.000 0.686 0.825\nCompound 24 0.875 0.784 0.863\nTable 4. Results of the retrieval system based on MCP,\nconsideringan audiosongasquery.\n5. CONCLUSION ANDPERSPECTIVES\nIn this paper, we have proposed a new mid-level descrip-\ntor, related to the beat multiples and subdivisions. Exper-\niments with synthetic and real songs show that consider-\ningthe MCP allowsthe retrievalofsongsbelongingto the\nsame metricclass.\nWhen focusing on the time signature, we reduce the\nsearch information from the descriptor. Other considera-\ntions are also of interest, such as the amplitudes of all the\nbeat subdivisions, which may denote a certain rhythmic\ncomplexityofthemusic. TheMCPofoneminuteof Fever\nbyRayCharles is showninFigure5. Ifthe time signature\nof this piece is generally notated 4/4, beat subdivisions at\n643Poster Session 4\n1/3 are moreprevalentthan at 1/2. As studied in [19],this\nisaconsequenceoftheswing. We seeherethatMCP con-\ntains information more complex than the time signature\nonly, and it could thus be used for very speciﬁc retrieval\npurpose.\n119754321/21/31/41/61/81/1200.10.20.30.4\nbeat multiple/subdivisionamplitudeMCP − Fever\nThe estimation of the MCP assumes the prior knowl-\nedgeofthecorrecttempo. Itsrobustnessagainsttempoes-\ntimationhastobeimprovedinthefuture. Ifthetempomay\nbeautomaticallyestimated,errorsareunavoidableandwil l\nsigniﬁcantly limit the accuracy of the MCP, and thus the\naccuracyoftheretrievalsystem.\nFurthermore, since metric properties may change dur-\ning a song, a song may be represented by a sequence of\nMCP(computedduringshortframes),inthesamewaythat\ntonalpropertiesofasongcanberepresentedbyasequence\nof chromas [20]. Such representation may allow the dis-\ncrimination of songs with the same metric properties, but\nwith differentevolutionswithrespectto time.\n6. ACKNOWLEDGEMENT\nThe authors would like to thank Jason Hockman for his\nuseful comments during the writing of the paper. This\nwork has been partly fundedby the Quaero project within\nthe task 6.4: “Music Search bySimilarity” and the French\nGIPANRundercontractsANR-06-JCJC-0027-01,DESAM\nandANR-JC07-188930,SIMBALS.\n7. REFERENCES\n[1] F.Gouyon,A.Klapuri,S.Dixon,M.Alonso,G.Tzane-\ntakis, C. Uhle,andP. Cano. Anexperimentalcompari-\nson ofaudiotempoinductionalgorithms. IEEE Trans-\nactions on Speechand Audio Processing , 14(5):1832–\n1844,2006.\n[2] G.Peeters. Template-basedestimationoftime-varying\ntempo.EURASIP Journal on Advances in Signal Pro-\ncessing,2007(1),2007.14pages.\n[3] J.Brown.Determinationofthemeterofmusicalscores\nbyautocorrelation. JournaloftheAcousticalSocietyof\nAmerica,94(4):1953–1957,1993.\n[4] A. Klapuri, A. Eronen, and J. Astola. Analysis of\nthe meter of acoustic musical signals. IEEE Transac-\ntionsonSpeechandAudioProcessing ,14(1):342–355,\n2006.\n[5] C. Uhle andJ. Herre. Estimationof tempo,microtime\nand time signature from percussive music. In Proc. of\nthe International Conference on Digital Audio Effects\n(DAFx,London,UK,2003.\n[6] S. Dixon, E. Pampalk, and G. Widmer. Classiﬁcation\nof dance music by periodicity patterns. In Proc. oftheInternationalConferenceonMusicInformationRe-\ntrieval(ISMIR) ,pages159–165,Baltimore,2003.\n[7] J.Foote,M.D.Cooper,andU.Nam.Audioretrievalby\nrhythmicsimilarity. In Proc. of the InternationalCon-\nferenceonMusicInformationRetrieval(ISMIR) ,pages\n265–266,Paris,France,2002.\n[8] J. Paulus and A. Klapuri. Measuring the similarity of\nrhythmicpatterns.In Proc.oftheInternationalConfer-\nenceon Musical InformationRetrieval(ISMIR) ,pages\n150–156,Paris,France,2002.\n[9] A.HolzapfelandY.Stylianou.Ascaletransformbased\nmethod for rhythmic similarity of music. In Proc.\nof the IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP) , Taipei, Tai-\nwan,2009.\n[10] F. Lerdahl and R. Jackendoff. A Generative Theory of\nTonal Music . MIT Press, Cambridge, Massachussetts,\n1985.\n[11] D. Temperley. The Cognition of Basic Musical Struc-\ntures. TheMITPress, 2001.\n[12] D.J. Povel and P. Essens. Perception of temporal pat-\nterns.MusicPerception ,2:411–440,1985.\n[13] C. S. Lee. Representing Musical Structure , chapter\nTheperceptionofmetricalstructure: Experimentalev-\nidenceandamodel,pages59–127.P.Howell,R.West,\nandI. Cross,London,UK,1991.\n[14] F.Gouyon. TowardsAutomaticDescriptionofMusical\nAudio Signals - Representations, Computional Mod-\nels and Applications . PhD thesis, Universitat Pompeu\nFabra,Barcelona,Spain,2003.\n[15] G. Peeters. Rhythm Classiﬁcation Using Spectral\nRhythm Patterns. In Proceedings of the 6th Interna-\ntionalConferenceon Music InformationRetrieval (IS-\nMIR),pages644–647,London,UK, 2005.\n[16] O. Lartillot and P. Toiviainen. A Matlab Toolbox for\nMusical Feature Extraction From Audio. In Proc. of\nthe International Conference on Digital Audio Effects\n(DAFx),Bordeaux,France,2007.\n[17] M.Alonso,G.Richard,andB.David.TempoAndBeat\nEstimationOfMusicalSignals.In Proc.oftheInterna-\ntionalConferenceon Music InformationRetrieval (IS-\nMIR),Barcelona,Spain,2004.\n[18] B. De Haas, R. Veltkamp, and F. Wiering. Tonal Pitch\nStep Distance: A Similarity Measure for Chord Pro-\ngressions. In Proc. of the International Conference on\nMusic Information Retrieval (ISMIR) , pages 51–56,\nSeptember14-182008.\n[19] K.A. Lindsay and P.R. Nordquist. A technical look at\nswing rhythm in music. The Journal of the Acoustical\nSocietyof America ,120(5):3005–3005,2006.\n[20] E. G´ omez. Tonal Description of Music Audio Sig-\nnals.PhDthesis,UniversityPompeuFabra,Barcelona,\nSpain,July 2006.\n644"
    },
    {
        "title": "A Quantitative Evaluation of a Two Stage Retrieval Approach for a Melodic Query by Example System.",
        "author": [
            "Justin Salamon",
            "Martin Rohrmeier"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416324",
        "url": "https://doi.org/10.5281/zenodo.1416324",
        "ee": "https://zenodo.org/records/1416324/files/SalamonR09.pdf",
        "abstract": "We present a two-stage approach for retrieval in a melodic Query by Example system inspired by the BLAST algorithm used in bioinformatics for DNA matching. The first stage involves an indexing method using n-grams and reduces the number of targets to consider in the second stage. In the second stage we use a matching algorithm based on local alignment with modified cost functions which take into account musical considerations. We evaluate our system using queries made by real users utilising both short-term and long-term memory, and present a detailed study of the system’s parameters and how they affect retrieval performance and efficiency. We show that whilst similar approaches were shown to be unsuccessful for Query by Humming (where singing and transcription errors result in queries with higher error rates), in the case of our system the approach is successful in reducing the database size without decreasing retrieval performance.",
        "zenodo_id": 1416324,
        "dblp_key": "conf/ismir/SalamonR09",
        "keywords": [
            "melodic Query by Example",
            "BLAST algorithm",
            "n-grams",
            "local alignment",
            "modified cost functions",
            "real users",
            "short-term and long-term memory",
            "database size",
            "retrieval performance",
            "efficiency"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nA QUANTITATIVE EV ALUATION OF A TWO STAGE RETRIEV AL\nAPPROACH FOR A MELODIC QUERY BY EXAMPLE SYSTEM\nJustin Salamon\nMusic Technology Group\nUniversitat Pompeu Fabra, Barcelona, Spain\njustin.salamon@upf.eduMartin Rohrmeier\nCentre for Music & Science, Faculty of Music,\nUniversity of Cambridge, United Kingdom\nmrohrmeier@cantab.net\nABSTRACT\nWe present a two-stage approach for retrieval in a melodic\nQuery by Example system inspired by the BLAST algo-\nrithm used in bioinformatics for DNA matching. The ﬁrst\nstage involves an indexing method using n-grams and re-\nduces the number of targets to consider in the second stage.\nIn the second stage we use a matching algorithm based on\nlocal alignment with modiﬁed cost functions which take\ninto account musical considerations.\nWe evaluate our system using queries made by real users\nutilising both short-term and long-term memory, and present\na detailed study of the system’s parameters and how they\naffect retrieval performance and efﬁciency. We show that\nwhilst similar approaches were shown to be unsuccessful\nfor Query by Humming (where singing and transcription\nerrors result in queries with higher error rates), in the case\nof our system the approach is successful in reducing the\ndatabase size without decreasing retrieval performance.\n1. INTRODUCTION\nThe transition to digital media and the growing popularity\nof portable media devices over the past decade has resulted\nin much research into new ways of organising and search-\ning for music. Of note are Content Based Music Retrieval\n(CBMR) systems [21] which search the musical content\ndirectly as opposed to using song meta-data for retrieval.\nA speciﬁc case of CBMR is that of performing a melodic\nsearch in a collection of music, where the input query can\nbe made either symbolically (e.g. text, score, MIDI con-\ntroller) [8, 9, 19, 22] or by the user singing/humming the\nquery, called Query by Humming (QBH) [4, 15]. For con-\nvenience we will refer to the symbolic input case as Query\nby Symbolic Example (QBSE). Both QBSE and QBH rely\non an underlying model of melodic similarity [6]. In [17],\na detailed review of algorithms for computing symbolic\nmelodic similarity is provided. In recent years QBH sys-\ntems have become increasingly popular, as they do not re-\nquire musical knowledge such as playing an instrument or\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.understanding musical notation. On the other hand, QBSE\ncan be advantageous over QBH in certain cases – ﬁrstly, it\naffords more elaborate query speciﬁcation, which might be\npreferred by advanced users and music researchers. Sec-\nondly, it does not require the automatic transcription of\naudio queries, which introduces additional errors into the\nqueries.\nIn [4] a detailed comparative evaluation of different al-\ngorithms for QBH was carried out, comparing approaches\nbased on note intervals, n-grams, melodic contour, HMMs\nand the Qubyhum system. The authors noted that the most\nsuccessful approaches lacked a fast indexing algorithm,\nwhich is necessary in order to apply them to large databases.\nThey studied a potential solution to the problem using a\ntwo-stage approach – an n-gram algorithm is used as a ﬁrst\nstage for ﬁltering targets in the database. The remaining\ntargets are then passed to the second stage which uses a\nslower note interval matching algorithm which has better\nretrieval performance. The authors concluded that the ap-\nproach was unsuccessful, as any signiﬁcant improvement\nin search time resulted in a dramatic degradation in re-\ntrieval performance. They attributed this degradation to the\nerrors introduced into the queries due to singing and tran-\nscription errors, which inhibited successful exact matching\nin the n-gram stage.\nNonetheless, other approaches for searching symbolic\ndata exist for which efﬁcient indexing is possible. Set-\nbased methods (which also support polyphonic queries)\nhave been shown to be effective for both matching and in-\ndexing – Clausen et al. use inverted ﬁles [3], Romming\nand Selfridge-Field use geometric hashing [16], Lemstr ¨om\net al. use index based ﬁlters [10] and Typke et al. use van-\ntage objects [18]. For string based approaches (such as\nours) many efﬁcient indexing algorithms exist for metric\nspaces [2]. However, due to the melodic similarity measure\nused in our system (section 2.1.3), we can not use these\nalgorithms and require an alternative solution (a recent so-\nlution to indexing non-metrics is also proposed in [20]).\nIn the following sections we present a two-stage index-\ning and matching approach for a QBSE system, inspired\nby the BLAST algorithm used in bioinformatics for DNA\nmatching [5]. The ﬁrst stage involves an indexing method\n(section 2.1.2) similar to the n-gram approach studied and\nevaluated in [4]. As our system avoids the need to tran-\nscribe user queries, the degree of errors in the queries de-\npends only on the user, and is lower as a result. Conse-\n255Poster Session 2\nquently, it allows us to considerably reduce the database\nsize for the second stage without degrading retrieval per-\nformance. In section 2.1.3 we present the second stage\nin which we perform matching using local alignment. We\nthen detail the evaluation methodology used to evaluate our\nsystem, using real user queries utilising both short-term\nand long-term memory. Finally in the results section we\nshow that this two-stage approach can be applied success-\nfully in the case of QBSE, and study how the parameters of\nthe indexing and matching algorithms affect retrieval per-\nformance and efﬁciency.\n2. THE SONGSEER QBSE SYSTEM\n2.1 System overview\nSongSeer is a complete query by symbolic example sys-\ntem. The user interacts with the system through a graphical\nuser interface implemented as a Java applet, allowing ac-\ncess from any web-browser1. The interface is further de-\nscribed in section 2.2. User queries are sent to a server ap-\nplication which contains a database of songs and performs\nthe matching and returns the results to the client applet.\n2.1.1 Query and target representation\nThe internal representation of user queries and database\ntargets is based on the one proposed in [15] – pitch is rep-\nresented as pitch intervals and rhythm as LogIOI Ratios\n(LogIOIR) [14]. This representation is independent of key\nand tempo, as well as concise, allowing us to match queries\nagainst targets even if they are played in another key or at\na different tempo.\nThe targets are extracted from polyphonic MIDI ﬁles –\nevery track in the MIDI ﬁle results in a single monophonic\ntarget. Tracks that are too short, as well as the drum track\nwhich is easily detectable are ﬁltered out, but otherwise all\ntracks are considered. This allows the user to search for\nmelodic lines other than the melody, though at the cost of\nconsiderably increasing the database size and adding tar-\ngets which could possibly interfere with the search.\n2.1.2 Indexing\nThe ﬁrst stage in our two-stage approach is the indexing al-\ngorithm. Indexing is required in order to avoid comparing\nthe query against every target in the database, thus improv-\ning system scalability an efﬁciency. As previously noted,\nour melodic matching is non-metric meaning we can not\nuse existing indexing approaches, leading us to propose an\nalternative solution based on the BLAST algorithm.\nThe BLAST algorithm [5] was designed for efﬁciently\nsearching for DNA and protein sequences in large databases.\nThe steps of the original BLAST algorithm are the fol-\nlowing: ﬁrst, low-complexity regions or sequence repeats\nare removed from the query. Then, the query is cut into\n“seeds” – smaller subsequences of length nwhich are eval-\nuated for an exact match against all words (of the same\nlength) in the database using a scoring matrix. High scor-\ning words (above a threshold T) are collected and the database\n1http://www.online-experiments.com/SongSeeris then scanned for exact matches with these words. The\nexact matches are then extended into high-scoring pairs\n(HSP) by extending the alignment on both sides of a hit till\nthe score starts to decrease (in a later version gaps in the\nalignment are allowed). HSPs with scores above a cutoff\nscore Sare kept, and their score is checked for statistical\nsigniﬁcance. Signiﬁcant HSPs are locally aligned, and an\nexpectation value Eis calculated for the alignment score.\nMatches with Esmaller than a set threshold are reported\nas the ﬁnal output.\nOur indexing algorithm is based on this concept of pre-\nceding the slower local alignment stage with a fast exact\nmatching stage. Given a query, we cut it into “seeds” as in\nBLAST, and search for exact matches in the database. This\ncan be efﬁciently implemented by storing the targets of the\ndatabase in a hash table where every key is a seed which\nhashes to a collection of all targets containing that seed.\nWe also implement the idea of ﬁltering less informative\nparts of the query as detailed in section 4.5. This ﬁrst stage\nallows us to return a much reduced set of targets, which we\nthen compare to the query using our matching algorithm.\nA crucial parameter of this approach is the seed size n–\na longer seed will return less targets making the retrieval\nfaster, but requires a longer exact match between query and\ntarget potentially reducing performance for queries which\ncontain errors.\n2.1.3 Matching\nFor determining the similarity of a query to a target, we\nuse the dynamic programming approach for local align-\nment [13], similar to the one proposed in [15] with one\nsigniﬁcant difference – in an attempt to make the match-\ning procedure more musically meaningful, we replace the\nskip andreplace costs in the local alignment algorithm\nwith cost functions . These functions determine the skip\nand replace cost based on the speciﬁc pitch intervals and\nLogIOIRs being compared. The underlying assumption is\nthat some errors should be penalised less heavily than oth-\ners, based on the errors we can expect users to make when\nmaking a query:\n•Repeated notes – the user might repeat a note more\nor less times than in the original melody (for exam-\nple when translating a sung melody into piano strokes).\nThus, the penalty for skipping a repeated note should\nbe reduced.\n•Pitch contour – the user might not remember the exact\npitch interval, but remember the pitch contour correctly\n(big/small jump up/down or same). Thus, the penalty\nfor replacing an incorrect interval which has the correct\ncontour should be reduced.\n•Rhythm contour – the user might not remember the ex-\nact rhythm ratio between two notes, but remember the\n“rhythmic contour” correctly (slower, faster or same).\nThus, the penalty for replacing an incorrect LogIOIR\nwhich has the correct contour should be reduced.\n•Octave errors – the user might play the correct pitch\nclass but in the wrong octave relative to the previous\n25610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nnote. Thus, the penalty for replacing a note which is\noff by an integer number of octaves should be reduced.\nFollowing this rationale, we deﬁne two cost parame-\nters – a full cost and a reduced cost . When one of the\naforementioned cases is detected the cost functions return\nthe reduced cost, and in all other cases the full cost. An-\nother issue is the relative importance we give to the pitch\nand rhythm match scores. As the pitch and rhythm of a\nquery might be represented with different degrees of accu-\nracy, the match scores should be weighted differently when\ncombining them to obtain the ﬁnal match score. To do\nthis we introduce a pitch factor andrhythm factor which\nweight the pitch and rhythm scores when combining them.\nBy default, the full cost is set to 2 and the reduced cost\nto 1, and both pitch and rhythm factors are set to 1 (so\nthat the pitch and rhythm scores are weighted equally and\nsummed into the ﬁnal score). In the results section we\nexplain how these parameters are optimised based on real\nuser queries.\n2.2 The SongSeer GUI\nThe SongSeer GUI is displayed in Figure 1. Two query in-\nput methods are provided – a text search allowing to make\ntextual queries similar to the ones supported by the The-\nmeﬁnder [8] system by Huron et al. (including pitch and\nrhythm contour), and a virtual keyboard that can be played\nusing the mouse, the computer keyboard or a connected\nMIDI controller. Once a query is made the top 10 results\nare displayed back to the user with a percentage indicat-\ning the matching degree, and the user can select a song\nand play back the corresponding MIDI ﬁle stored in the\ndatabase.\nFigure 1 . The SongSeer user interface.\n3. EV ALUATION METHODOLOGY\n3.1 Test collections and machines used\nFor the evaluation, we compiled a corpus of 1,076 poly-\nphonic MIDI ﬁles of pop and rock music, including 200\nsongs by the Beatles. After ignoring short tracks and drum\ntracks, this translates into 6,541 targets in the database. Forscalability tests we have also compiled several more cor-\npora of increasing size, the largest containing 18,017 songs\nwhich translates into 88,034 targets.\nAll user experiments were run on standard pentium IV\nPCs running windows XP. The quantitative evaluation was\nrun on a server machine with two Intel R/circlecopyrtDual Core Xeon R/circlecopyrt\n5130 @ 2GHz with 4MB Cache and 4GB RAM, running\nLinux 2.6.17-10 and Java HotSpotTM64-Bit Server VM.\n3.2 Collecting user queries\nWe conducted a user experiment with 13 participants of\nvarying musical experience, ranging from amateur guitar\nplayers to music graduates. The ﬁrst part of the experiment\ninvolved a usability test in which the subjects were asked to\ncomplete a set of tasks using the SongSeer interface, which\nalso allowed them to familiarise themselves with the sys-\ntem. The second part involved the subjects making queries\nwhich would then be used to perform a quantitative eval-\nuation of the system. For the purpose of the quantitative\nevaluation, all subjects were asked to play on the virtual\nkeyboard, using either the mouse or computer keyboard.\nTo collect queries, subjects were presented with a list\nof 200 Beatles songs, and asked to record queries of songs\nthey can remember from the list. This stage simulates the\nevent where a user remembers part of a song they have\nnot heard recently, and resulted in 63 “long term memory”\nqueries. Next, subjects were asked to listen to 10 audio\nrecordings of Beatles songs and then record a query, simu-\nlating the event where the user has recently heard the song,\nresulting in 123 “short term memory” queries. This gives\nus a total of 186 real user queries for system evaluation.\n3.3 Evaluation metrics\nAs there is always only a single correct result (the database\ncontained no cover versions), we use a metric based on the\nrank of the correct song based on match score, the Mean\nReciprocal Rank (MRR) which is given by:\nMRR =1\nKK/summationdisplay\ni=11\nrank i(1)\nwhere Kis the numbers of queries evaluated and rank iis\nthe rank of the correct song based on the match score for\nquery i. This is similar to taking the average rank of the\ncorrect song over all queries but is less sensitive to poor\nranking outliers, and returns a value between 1/Mand 1\n(where Mis the number of songs in the database) with\nhigher values indicating better retrieval performance.\nIt is important to note however that as it is possible for\nseveral songs to have the same match score, they may share\nthe same rank (in which case they are returned by alpha-\nbetical order in the results list). In order to evaluate per-\nformance from a user perspective (where the position of a\nsong in the result list is signiﬁcant), we introduce a second\nmetric – the ordered MRR (oMRR) which is computed in\nthe same way as the MRR but where the rank is based not\non the match score but on the actual position of the song in\nthe ﬁnal results list.\n257Poster Session 2\n4. RESULTS\n4.1 Initial results\nIn order to asses the performance of our approach, we start\nby estimating a baseline performance for the problem. The\nbaseline is estimated using the following procedure: Given\na query, we randomly generate the rank of the correct song\nin the result list (between 1 and 1076). We repeat this pro-\ncess 99 times for the same query, saving the best randomly\ngenerated rank out of the 99 repetitions. We perform this\nprocedure for all 186 queries, and use the saved ranks to\ncompute an overall oMRR. This gives us an oMRR of 0.211\n(with a variance of 0.051).\nNext we turn to evaluate our algorithm. As a ﬁrst step,\nwe compute the MRR and oMRR taking only the pitch in-\nformation into account, using a seed size n= 3 and the\nmatching parameters set to their default values (full cost =\n2, reduced cost = 1). The results are presented in Table 1.\nQuery Group #Queries MRR oMRR\nAll queries 186 0.800 0.659\nLong term memory 63 0.765 0.627\nShort term memory 123 0.818 0.675\nTable 1 . Initial results.\nThe table shows that our oMRR values are signiﬁcantly\n(P < 10−10, Wilcoxon rank-sum test) higher than the\nbaseline. Though results for the short term memory queries\nare slightly higher than for the long term memory queries,\nthe difference is not statistically signiﬁcant, and for the rest\nof the evaluation we use all the queries together. Finally,\nwe observe that, as expected, the MRR values are higher\nthan the oMRR values. This suggests that songs are not\nsufﬁciently distinguished using the default parameters.\n4.2 The effect of rhythm on performance\nWe now include the rhythm information in the matching\nprocedure, setting the pitch and rhythm factors to 1:1. The\nMRR and oMRR go down from 0.800 and 0.659 (pitch\nonly) to 0.677 and 0.594 (pitch+rhythm) respectively. This\nindicates that giving rhythm equal importance as pitch de-\ngrades performance. We could argue that as the rhythm\ninformation is less detailed compared to the pitch informa-\ntion, it will match a greater set of songs, so when given\nequal importance as the pitch information it ends up de-\ngrading the results. Another possibility is that due to the\nuse of a virtual keyboard rather than a real one users found\nit harder to accurately play the rhythm of a query.\n4.3 Choice of seed size\nAs previously mentioned, in [4] it was shown that a two\nstage retrieval process for QBH was unsuccessful in reduc-\ning search time without considerably degrading retrieval\nperformance. In Figure 2 we evaluate the effect of the seed\nsizenin our indexing algorithm (the ﬁrst stage of our two-\nstage approach) on retrieval performance and search time.\n!\"!#$\"!#%\"!#&\"!#'\"!#(\"!#)\"!#*\"!#+\"!#,\"\n%\" &\" '\" (\"\n!\"\"#$%&'\"$-..\"/\"01234\"5678\"\n-..\"/\"01234\"9\"\n:4824;\"\n5-..\"/\"01234\"5678\"\n5-..\"/\"01234\"9\"\n:4824;\"\n<=>#\":?2:1?=@7\"A;?\"\n0?:\"BC?:8\"DE?356FEG\"\n<=>#\"H:@3A56\"5H\"IJ\"\n:?2C:6?F\"K8\"16F?L16>\"Figure 2 . MRR, oMRR, retrieval time and DB reduction\nvs seed size.\nFigure 2 shows that the number of targets to search re-\nturned by the indexing algorithm is almost halved every\ntime we increase the seed size, and consequently the search\ntime goes down. Interestingly, the MRR and oMRR val-\nues remain stable as we increase the seed size (only the\nMRR for pitch only shows slight signs of decline). Increas-\ning the seed size ndoes however require the user query to\ncontain at least nsequential correct pitch intervals, in ad-\ndition to increasing memory and storage requirements for\nthe database. All in all it is a trade-off between retrieval\nperformance, efﬁciency and resource usage. For the rest of\nthe evaluation we choose a seed size of 4, providing a sig-\nniﬁcant reduction in database size (reduced to 23%) with\npractically no degradation of retrieval performance.\n4.4 Parameter optimisation\nIn section 2.1.3 we introduced the notion of having a full\ncost and a reduced cost in the matching algorithm, and in\nsection 4.2 we saw that giving rhythm equal importance\nas pitch in the matching is detrimental to retrieval per-\nformance. In this section we optimise these parameters,\nnamely the ratio between the full and reduced costs, and\nthe ratio between the pitch and rhythm factors.\nTo do so we divided the queries into two groups of\nroughly equal size – the optimisation is performed using\nthe queries of group 1, and then validated on the queries\nof group 2. For the optimisation we use the Simulated\nAnnealing approach for global optimisation [7]. The per-\nformance for the two query groups before optimisation is\ngiven in Table 2.\nQuery Pitch:Rhythm Full:Reduced MRR oMRR\nGroup Ratio Cost\n1 1:1 2:1 0.704 0.646\n2 1:1 2:1 0.634 0.574\nTable 2 . Results for the groups before optimisation.\nWe start by optimising the pitch and rhythm weighting\nfactors. The effect of these parameters on performance is\nvisualised in Figure 3. The optimal pitch to rhythm ra-\ntio was found to be 3:1 (Table 3), and is used in all fur-\n25810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n010203040\n01020304000.20.40.60.8\nRhythm FactorPitch FactoroMRRFigure 3 . oMRR as a function of the pitch and rhythm\nfactors.\nther evaluations. A slightly higher MRR value could be\nachieved by ignoring rhythm altogether, but at the cost of\nsigniﬁcantly reducing the oMRR indicating that the rhythm\ninformation is useful for distinguishing between targets which\nhave the same pitch match score.\nWe next perform the optimisation for the cost parame-\nters in the matching algorithm. The optimal values were\nfound to be 3 for full cost and 1 for reduced cost (Table\n3 last row). This suggests that the modiﬁed cost functions\nprovide an improvement to performance, as otherwise the\noptimal full and reduced costs would have been equal to\neach other (Table 3 penultimate row).\nPitch:Rhythm Ratio Full:Reduced Cost MRR oMRR\n1:1 2:1 0.704 0.646\n3:1 2:1 0.784 0.745\n3:1 1:1 0.759 0.717\n3:1 3:1 0.787 0.761\nTable 3 . Results for group 1 before and after pitch:rhythm\noptimisation and full:reduced optimisation.\nFinally we compute the MRR and oMRR for query group\n2 and for all queries together using the optimised param-\neters. Figure 4 shows that in all cases performance is im-\nproved, though only in the case of oMRR for all queries\n(Group 1&2) is the improvement statistically signiﬁcant\n(p=0.018, Wilcoxon rank-sum test).\n4.5 Seed ﬁltering\nNext we examine the distribution of seeds in the queries\nand the database, displayed in Figure 5.\nBoth distributions constitute a power law probability\ndistribution, obeying a kind of Zipf’s law for musical inter-\nval sequences [23]. Accordingly, the most frequent seeds\ncomprise the largest proportion of the database but convey\nthe least amount of information useful for distinguishing\nbetween songs. By ﬁltering from the query the seeds which\nare most common in the database we can further reduce the\n!\"!#$\"!#%\"!#&\"!#'\"!#(\"!#)\"!#*\"!#+\"!#,\"\n-..\"/0123415\" 3-..\"/0123415\" -..\"/67145\" 3-..\"/67145\"8439:\"$\"\n8439:\"%\"\n8439:\"$;%\"Figure 4 . MRR and oMRR results, before and after opti-\nmisation.\n100101102103100101102\n500 most common seeds occurring in queriesNumber of occurrences\n100101102103102103104\n500 most common seeds occurring in the databaseNumber of occurrences\nFigure 5 . Seed distributions for queries and songs in the\ndatabase.\nfraction of the database returned by the indexing algorithm\nwhile maintaining retrieval performance.\nWhen ﬁltering just the three most common seeds in the\ndatabase, we reduce the database size by a further 40%\n(from 23% to 14% of the original size) while the MRR and\noMRR values go down by less than 1.3%. This concept\ncould be further extended by introducing a seed weight-\ning function, for example using tf∗id f[1] like weighting\nbased on seed distributions. This could also help in rank-\ning songs which have the same match score after the sec-\nond stage, however we have not explored this option and\nleave it for future work.\n4.6 Scalability\nFinally, we evaluate how retrieval performance and efﬁ-\nciency are affected as we scale the database size up to\n18,017 songs which translates into 88,034 targets. The re-\nsults are presented in Figures 6 and 7.\nFirst we note that our indexing algorithm provides a\nconsiderable reduction in the fraction of the database re-\nturned by the ﬁrst stage, reducing it to 15% of its origi-\nnal size. Nonetheless, further work would be required for\nour approach to be applicable to collections of millions of\n259Poster Session 2\n!\"#\"$%&'(&)\"*\"+,%$-+\"\n./\"#\"$%,,,('\"\n$\"&$$$$\"-$$$$\"+$$$$\"($$$$\"'$$$$\"\n$\" -$$$$\" ($$$$\" 0$$$$\" 1$$$$\" &$$$$$\"!\"#$%&'()#*'+%)*'),)-*.%\n/,01*)%23%'()#*'+%4-%'5*%.('(1(+*%Figure 6 . Avg. #targets returned by the ﬁrst stage vs #tar-\ngets in the database.\n!\"#\"$%&%'()*+,-\".\"/&/0/1\"\n23\"#\"%&14'4\"\n!\"#\"$%&%'1)*+,-\".\"/&/'01\"\n23\"#\"%&145(0\"\n%\"%&/\"%&0\"%&6\"%&'\"%&(\"%&5\"%&7\"%&4\"\n%\" 0%%%%\" '%%%%\" 5%%%%\" 4%%%%\" /%%%%%\"!\"#$%#&'()\"*\n+,&-\"#*%$*.'#/\".0*1(*.2\"*3'.'-'0\"*899\"\n:899\"\n;:<&+899-\"\n;:<&+:899-\"\nFigure 7 . MRR and oMRR vs numbers of targets in the\ndatabase.\nsongs, ideally obtaining a sublinear relation between the\nnumber of targets in the database and the number of tar-\ngets returned by the indexing stage.\nNext we note that both the MRR and oMRR decrease as\nO(log(n))as the database size nis increased ( R2>0.98),\nindicating that performance scales well with database size.\n5. CONCLUSIONS\nIn this paper we introduced a two-stage retrieval approach\nfor a melodic QBSE system. We demonstrated that whilst\nfor QBH systems similar approaches were unsuccessful,\nfor QBSE this approach can successfully reduce the database\nsize while maintaining high MRR values. We provided\na detailed study of the effect of different parameters of\nthe system, namely the seed size, the relative weighting\nof pitch and rhythm and the full and reduced costs in the\nmatching algorithm.\nFinally we consider some ideas for future work. Instead\nof considering almost every track in a MIDI ﬁle as a tar-\nget, we could aim to extract only the most relevant melodic\nparts of the piece, as done in [11, 12]. Next, it would\nbe interesting to further study the seed distributions in the\nqueries and the database, which could help develop more\nelaborate seed ﬁltering and/or a seed weighting scheme.2\n6. REFERENCES\n[1] R. Baeza-Yates and B. Riberto-Neto. Modern Information\nRetrieval . Addison Wesley and ACM Press, 1999.\n[2] T. Bozkaya and M. Ozsoyoglu. Indexing Large Metric\nSpaces for Similarity Search Queries. ACM Transactions on\nDatabase Systems , 24(3):361–404, September 1999.\n2This work was supported in part by Microsoft Research through the\nEuropean PhD Scholarship Programme, and the Programa de Formaci ´on\ndel Profesorado Universitario (FPU) of the Ministerio de Educaci ´on de\nEspa ˜na.[3] M. Clausen, R. Engelbrecht, D. Meyer, and J. Schmitz.\nPROMS: A Web-based Tool for Searching in Polyphonic\nMusic. In ISMIR Conference Proceedings , 2000.\n[4] R. B. Dannenberg, W. P. Birmingham, B. Pardo, N. Hu,\nC. Meek, and G. Tzanetakis. A Comparative Evaluation\nof Search Techniques for Query-by-Humming Using the\nMUSART Testbed. Journal of the American Society for In-\nformation Science and Technology , February 2007.\n[5] W.J. Ewens and G. Grant. Statistical Methods in Bioinfor-\nmatics: An Introduction (Statistics for Biology and Health) .\nSpringer, 2nd edition, 2005.\n[6] W. Hewlett and E. Selfridge-Field, editors. Melodic Simi-\nlarity: Concepts, Procedures and Applications . MIT Press,\nCambridge, 1998.\n[7] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization\nby Simulated Annealing. Science , 220(4598), 1983.\n[8] A. Kornst ¨adt. Themeﬁnder: A Web-based Melodic Search\nTool. Computing in Musicology , 11:231–236, 1998.\n[9] K. Lemstr ¨om, V . M ¨akinen, A. Pienim ¨aki, M. Turkia, and\nE. Ukkonen. The C-BRAHMS Project. In ISMIR Conference\nProceedings , pages 237–238, 2003.\n[10] K. Lemstr ¨om, N. Mikkil ¨a, and V . M ¨akinen. Fast Index Based\nFilters for Music Retrieval. In ISMIR Conference Proceed-\nings, pages 677–682, 2008.\n[11] S. T. Madsen, R. Typke, and G. Widmer. Automatic Re-\nduction of MIDI Files Preserving Relevant Musical Content.\nIn6th International Workshop on Adaptive Multimedia Re-\ntrieval (AMR 2008) , Berlin, Germany, 26-27 June 2008.\n[12] C. Meek and W.P. Birmingham. Automatic Thematic Extrac-\ntor.Journal of Intelligent Information Systems , 2003.\n[13] G. Navarro and M. Rafﬁnot. Flexible Pattern Matching in\nStrings . Cambridge University Press, Cambridge, UK, 2002.\n[14] B. Pardo and W.P. Birmingham. Encoding Timing Informa-\ntion for Musical Query Matching. In ISMIR Conference Pro-\nceedings , Paris, France, 2002.\n[15] B. Pardo, J. Shifrin, and W. Birmingham. Name That Tune: A\nPilot Study in Finding a Melody From a Sung Query. Journal\nof the American Society for Information Science and Technol-\nogy, 2004.\n[16] C. A. Romming and E. Selfridge-Field. Algorithms for poly-\nphonic music retrieval: the Hausdorff metric and geometric\nhashing. In ISMIR Conference Proceedings , pages 457–462,\nVienna, Austria, 2007.\n[17] R. Typke. Music Retrieval based on Melodic Similarity . PhD\nthesis, Utrecht University, Netherlands, 2007.\n[18] R. Typke, P. Giannopoulos, R. C. Veltkamp, F. Wiering, and\nR. Van Oostrum. Using Transportation Distances for Measur-\ning Melodic Similarity. In ISMIR Conference Proceedings ,\npages 107–114, 2003.\n[19] R. Typke, R. C. Veltkamp, and Wiering F. Searching No-\ntated Polyphonic Music using Transportation Distances. In\nProceedings of the ACM Multimedia Conference , pages 128–\n135, New York, 2004.\n[20] R. Typke and A. Walczak-Typke. A Tunneling-Vantage In-\ndexing Method for Non-Metrics. In ISMIR Conference Pro-\nceedings , pages 683–688, 2008.\n[21] R. Typke, F. Wiering, and R. C. Veltkamp. A Survey of Mu-\nsic Information Retrieval Systems. In ISMIR Conference Pro-\nceedings , pages 153–160, 2005.\n[22] A. L. Uitdenbogerd. N-gram Pattern Matching and Dynamic\nProgramming for Symbolic Melody Search. In Proceedings\nof the Third Annual Music Information Retrieval Evaluation\neXchange , Sept. 2007.\n[23] D. H. Zanette. Zipf’s Law and the Creation of Musical Con-\ntext. Musicae Scientiae , 10(1):3–18, 2006.\n260"
    },
    {
        "title": "Multiple F0 Estimation in the Transform Domain.",
        "author": [
            "Christopher Santoro",
            "Corey Cheng"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416242",
        "url": "https://doi.org/10.5281/zenodo.1416242",
        "ee": "https://zenodo.org/records/1416242/files/SantoroC09.pdf",
        "abstract": "A novel algorithm is proposed to estimate the fundamental frequencies present in polyphonic acoustic mixtures expressed in a transform domain. As an example, the algorithm operates on Modified Discrete Cosine Transform (MDCT) coefficients in order to demonstrate the utility of the method in commercially available perceptual audio codecs which use the MDCT. An auditory model is developed along with several optimizations that deal with the constraints of processing in the transform-domain, including an interpolation method, a transform-domain half-wave rectification model, tonal component estimation, and sparse convolution. Test results are separated by instrument and analyzed in detail. The proposed algorithm is shown to perform comparably to state of the art time-domain",
        "zenodo_id": 1416242,
        "dblp_key": "conf/ismir/SantoroC09",
        "keywords": [
            "fundamental frequencies",
            "polyphonic acoustic mixtures",
            "transform domain",
            "Modified Discrete Cosine Transform (MDCT)",
            "perceptual audio codecs",
            "auditory model",
            "optimizations",
            "constraints",
            "processing in the transform-domain",
            "interpolation method"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n    MULTIPLE F0 ESTIMATION IN THE TRANSFORM DOMAINChristopher A. Santoro+* Corey I. Cheng*#   +LSB Audio Tampa, FL 33610 chris@lsbaudio.com  *University of Miami Music Engineering Technology Frost School of Music Coral Gables, FL 33124  #University of Miami Department of Electrical and Computer Engineering coreyc@miami.edu  ABSTRACT A novel algorithm is proposed to estimate the fundamental frequencies present in polyphonic acoustic mixtures expressed in a transform domain. As an example, the algorithm operates on Modified Discrete Cosine Transform (MDCT) coefficients in order to demonstrate the utility of the method in commercially available perceptual audio codecs which use the MDCT. An auditory model is developed along with several optimizations that deal with the constraints of processing in the transform-domain, including an interpolation method, a transform-domain half-wave rectification model, tonal component estimation, and sparse convolution. Test results are separated by instrument and analyzed in detail. The proposed algorithm is shown to perform comparably to state of the art time-domain methods.  1. INTRODUCTION Perceptually coded formats such as mp3 and AAC have become the dominant storage and distribution format for commercial digital music. These formats are popular because they greatly reduce bandwidth and memory requirements related to transmission and storage. As a result of the successes of these formats, portable media players are becoming increasingly important platforms for the analysis and synthesis of digital media. These devices have limited processing power and battery life, and therefore require analysis and synthesis algorithms with minimal computational complexity where possible.      One emerging family of algorithms that is finding increased applicability in music information processing is multiple fundamental (F0) estimation. Loosely speaking, the purpose of multiple F0 estimation is to estimate the perceived pitches of multiple harmonic series, such as those created by the human voice or various musical instruments, when sounding concurrently. Multiple F0 estimation finds widespread use as a front-end to various pitch tracking and source separation algorithms.      State of the art analysis algorithms are typically designed to begin their operations on uncompressed PCM audio signals in the time-domain. Because music files on portable devices are stored in a perceptually coded format, they must first be decoded before the algorithms can begin their analysis. For example, many perceptual audio codecs spend considerable resources in using the well-known Inverse Modified Discrete Cosine Transform (IMDCT) to synthesize a time-domain signal during the decoding process. Therefore, it would be especially advantageous to avoid this expensive decoding process where possible, and operate directly on the native MDCT representation used in a perceptually coded file.      This work adapts a state of the art multiple F0 estimation algorithm to operate directly on a transform-domain representation used in modern audio codecs as a starting point for this research. Very few authors have researched transform-domain processing. Previous works include beat detection [1], music/speech classification [2], and sinusoidal analysis [3]. A primary reason for the limited body of work related to F0 estimation algorithms is the limited frequency resolution used in transform-based audio coders. When working in the transform domain, we are stuck with whatever frame size the coder uses. A secondary difficulty with transform domain processing is that some time-domain processes do not easily lend themselves to operation in the transform-domain. In this work several novel modifications to the auditory model are proposed that successfully mitigate both of these limitations. A third difficulty with transform domain processing is that processing in some transform domains, such as the MDCT domain, can be problematic because of some of the aliasing properties of the transforms [10].       We propose an algorithm for multiple F0 estimation in the transform domain that adapts the work of Klapuri [4] to function in the MDCT domain. Like [4], the proposed algorithm uses a model of the human auditory system along with iterative estimation and cancellation to estimate component F0s, as the auditory model described by Klapuri consists of an auditory filterbank followed by half-wave rectification and low pass filtering. However, in this work, each of these processes is adapted to operate in the transform-domain. We also incorporate modifications to the iterative estimation portion of the algorithm by Paz [6] in order to improve performance.  2. OVERVIEW OF REFERENCE ALGORITHM The design of the proposed algorithm is based on the work of Klapuri [4]. The basic framework of Klapuri’s  \n165Poster Session 1\n   \n Figure 1. Overview of reference multiple F0 estimation method \n Figure 2. Detail of auditory model used in reference method method is shown in Figure 1. A key feature of the algorithm is the auditory model, shown in Figure 2. The auditory model used by Klapuri consists of a 70 channel gammatone filterbank design, using the computationally efficient implementation by Slaney [11], followed by half-wave rectification and low-pass filtering. Finally, a Discrete Fourier Transform (DFT) is taken in each channel, and the spectra in each channel are summed to create a summary magnitude spectrum.      To identify F0s, the salience of each candidate F0 is calculated using (1), where USMS is the summary magnitude spectrum, Kτ,m is a region where each partial is expected to be based on the period (τ) of the F0 and the harmonic number (m), and ω(τ,m) is an exponentially decreasing weighting function dependent on F0 and harmonic number.   (1)       The salience can be interpreted as a measure of the perceptual strength of each candidate F0. On each iteration, the F0 with the highest salience is chosen. Its partials are then identified and partially subtracted from the mixture according to an exponentially decreasing weighting scheme. This process is repeated until a known number of F0s have been estimated. In Klapuri’s work the polyphony considered to be known a priori in most cases. 3. PROPOSED ALGORITHM Our proposed algorithm comprises three main stages: interpolation, a low complexity transform-domain auditory model plus iterative estimation and subtraction. The algorithm makes changes mainly to the auditory model and adds transform domain interpolation to the front  end  of  the  salience  calculation  in  an attempt to  Figure 3. Example of interpolation process. In this figure the peak has been shifted to the left after interpolation based on the distribution of energy around the old peak.  deal with the limited frequency resolution of the transform-domain representation used in audio codecs. 3.1. Interpolation of MDCT coefficients As stated previously, one of the fundamental limitations of transform-domain processing is that we are stuck with whatever frame size the codec uses. Codecs like AAC use large frame sizes of 2048 samples for tonal content, and smaller frame sizes of 256 samples for transients [9]. A frame size of 2048 samples at 44.1 kHz yields a frequency resolution of roughly 21.5 Hz. Since F0s are more closely spaced at lower frequencies in contemporary western scales, this means that a peak in one MDCT bin can span as many as six notes. In fact, F0s are not spaced more than 21 Hz apart until the 4th octave (E4 or 330 Hz). If nothing is done to address this, this means that any peak corresponding to an F0 below 330 Hz could be one of many F0s. This assumes an equal tempered scale. The details of other tuning systems will be different.      To solve this problem, we use a simple interpolation method in the transform-domain. While this method does not increase the real frequency resolution of the transform-domain representation, it does have the effect of shifting peaks to a more accurate location corresponding to the true F0, making estimates of frequency when peak picking more accurate. The implementation of this method consists of zero padding/upsampling, then performing a zero order hold and low pass filtering. An upsampling factor of 3 was used here, but any odd factor may be used.       The low pass filtering was implemented as a simple FIR filter, which takes the form of a convolution with a  sinc function. We found a filter length of 24 samples to be adequate for an upsampling factor of 3. An example of the result of the interpolation process is shown in Figure 3.      What is left as a result of the interpolation process can no longer be called a realistic MDCT spectrum, but this is of no consequence to the proposed algorithm. What we do have at this point is a reasonable estimation !\"#$%&'()*&#+,-#+.%$/()01)2)34'5&.$6784.6+,)#+%+6%+#)94'%$4,7:\"554'()*4;.$%\"#+:9+6%'\"5-%+'4%+<$5+=#&54$.4\"#$&)7$;.4,>7%$54%+#017\n!\"#$%&'()*$+%,'-./01.+2)3.4,)5,6%$!6.%$&/7&8)9.::)*$+%,'$/;<)=*>)<\n<)=*>)<???<)=*>)<<)=*>)<???!>$@,)=&@.$/)!\"#$&)A$;/.+A\"@@.'()B.;/$%\"#,)AC,6%'\"@5155205255305355400246810\nBin NumberMagnitude\n  Original MDCT CoefficientsZero PaddedLow Pass Filtered\n16610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   of the magnitude spectrum of our signal, and a better estimation of true peak locations due to the interpolation process. It is important to remark that this is a computationally expensive process, which could probably be replaced by a less intensive interpolation method. However, this method was chosen here for its simplicity and good results. 3.2. Transform-Domain Auditory Model After interpolation, the spectrum is passed onto a transform-domain auditory model, where we implement a modified version of the Unitary model of human pitch perception, proposed by Meddis [7]. In this section, we describe our modifications and improvements to the model’s four following steps:  1. The stimulus is passed through a filterbank of band-pass filters, which simulate the action of the basilar membrane. 2. Each sub-band signal is compressed, half wave rectified, and low-pass filtered to obtain the time domain amplitude envelope. 3. Periodicity estimation is carried out on each sub-band. 4. Periodicity information from each sub-band is combined across channels.       Step 1 of the unitary model is said to mimic the frequency selectivity of the inner ear. Typically a gammatone filterbank implementation by Slaney is used, although the number of channels necessary to achieve good results is debated. Depending on the application, previous works using auditory models use as few as 2 channels [8] and as many as 70 [4]. For this reason, we implemented filterbanks with 8, 16, 32, 64, and 70 channels to explore the effect on performance of the algorithm. If the number of channels can be reduced, then the computational complexity of the algorithm can be reduced significantly.      Step 2 of the unitary model processes information contained in the time domain amplitude envelope of the stimulus signal. Many musicians know the information we are looking for here as beating. Beating occurs when two sinusoids that have slightly different frequencies cancel and/or reinforce each other periodically. The fundamental period of the beating corresponds to the difference in frequency between the two sinusoids. Thus, this process (half wave rectification and low pass filtering) can be considered as a way of analyzing the intervals between harmonics, which corresponds to the F0 of a harmonic sound. This type of information is called spectral interval information.      Steps 3 and 4 are merely ways to extract the periodicity of the time-domain amplitude envelope, which is reinforced in step 2. Typically an autocorrelation function (time-domain) or a Discrete Fourier Transform (frequency-domain) is used in each channel. Given these processes, it is easy to see why we do not want a large number of channels if it is not necessary.      Some of these steps lend themselves easily to a transform domain implementation, while others prove more difficult. For example, step 1 of the auditory model is trivial in the transform domain. The auditory filterbank can be implemented easily by a matrix multiply if we store the magnitude response of each channel in an N × nC matrix, where N is the number of coefficients in our upsampled MDCT spectrum and nC is the number of channels in our filterbank. The magnitude response of each channel of the auditory filterbank can be obtained easily by first obtaining a standard time-domain design, and processing each channel with an impulse. Taking the magnitude DFT of the result yields the magnitude response of each channel. Since the filterbank is static, it can be calculated once, and the magnitude response can simply be stored in memory.  Step 2 of the auditory model is the most difficult to adapt for the transform domain. It is not obvious how half-wave rectification can be translated into the transform domain. However, Klapuri [5] points out that half wave rectification can be modeled as a convolution operation. The mathematical details of that argument are beyond the scope of this paper, but the interested reader is encouraged to check the source for an in depth analysis. Instead, we simply note that since the goal of this step of the model is to reinforce spectral interval information, that a convolution operation is an intuitive method for extracting that information. There are two difficulties with the convolution of spectra to extract spectral interval information. One is that spectra have a DC offset due to the fact that all magnitude coefficients are positive. This causes a triangular shaped buildup around DC that obscures peaks indicating prominent spectral intervals. The other is that the process is prohibitively expensive computationally, especially since this is a process that must be performed on each channel individually. A standard way to attack the first problem is to subtract the mean from the signal. Not only does this not work in this case because the DC offset is caused by a small region of disproportionately large peaks, but it also does not address the second problem. We propose here a method for solving both of these problems based on tonal component estimation. Since we are looking for intervals between peaks, we begin the process by finding the locations of the peaks in the subband spectrum. This is called tonal component estimation (TCE). First, the derivative of each subband is taken. Next, the derivative is used to analyze the slope of each peak. Using a sliding window of 15 coefficients, local maxima are found by identifying locations in which the derivative transitions from a positive value to a negative value, and the difference between the two is greater than some threshold. The mean of each subband signal was found to be a good threshold, though this value may be changed to adjust sensitivity. Once we have identified the locations of tonal components, we replace each peak   \n167Poster Session 1\n   \n Figure 4. Comparison of TCE and sparse convolution to traditional convolution of MDCT spectra.  with a single spike that has the same amplitude as the peak. Using this process, each subband signal is reduced to only the most pertinent information (i.e., the locations and magnitudes of harmonics). This transforms the signal into a sparse vector, since most of the elements in each subband signal are now zeros. Next, a sparse convolution may be used to extract spectral interval information. This process is shown in Figure 4 and compared to a standard convolution. Not only does it solve the problem of DC buildup, but it also reduces the computational complexity drastically. The result of this process is a vector of extracted spectral interval information, Vc. By combining this with the original subband spectrum, we can obtain both spectral interval and spectral location information. Therefore, we calculate a weighted combination of the original spectrum (Xc), using (2), where α is a simple parameter which can be used to adjust the importance of spectral interval information. Yc is the resulting signal in each channel after the half wave rectification process.   (2)  Step 3 of the auditory model is performed by a DFT in the reference method. Here, we are already in the frequency domain, so this step can be skipped, yielding a large computational savings. Step 4 is also trivial and is computed by summing across channels to create a summary magnitude spectrum, USMS. This is shown in (3). In this step, channels that have peaks in the same location in their Vc components (meaning their spectral interval information is in agreement) reinforce each other to accentuate (or in some cases, reproduce) the peaks corresponding to the F0s in the mixture.               (3)  3.3. Iterative Estimation and Subtraction Once the summary magnitude spectrum has been calculated, the algorithm performs an iterative estimation and subtraction process largely similar to that in the reference algorithm. On each iteration, the salience is calculated for all fundamental candidate periods τ as described in (1). The candidate period with the maximum salience is chosen to be an F0. Next, we attempt to identify the peaks that contributed to the salience of the currently estimated F0. An adaptive scheme is used to capture peaks as well as their side lobes, which was developed by Paz [6] and was found to improve performance significantly. First local maxima are identified within each region defined by Kτ,m. Next, the boundaries are expanded until they lie at adjacent local minima. Once this is completed, a detected spectrum UD is formed consisting of the partials of the estimated fundamental. These partials are then weighted by the same weighting function that was used to calculate the salience. This allows us to remove some of the energy in each partial, but not all of it. This is critical for cases in which multiple sounds have partials that overlap. Finally, a residual spectrum UR is formed by subtracting UD from USMS. The process of calculating the salience and estimating the partials of F0s is repeated on UR a number of times that is equal to the polyphony, which is known a priori. The estimated F0s are then quantized to the nearest frequency value corresponding to a valid note on the equal tempered scale, with A4 corresponding to 440Hz.  4. RESULTS The proposed algorithm was tested in a similar manner to the reference algorithm. Polyphonic mixtures of 2, 4, and 6 notes were created from four different types of instruments: Saxophone, Flute, Violin, and Cello. Sample recordings of individual notes were used from the University of Iowa1 database. For each polyphony and instrument, 100 mixtures were created by lining up the onsets of notes and mixing them at equal RMS levels. Each individual file was then encoded using the LAME mp3 encoder2 at 128 kbps. The results presented in all tests for the reference algorithm are based on a careful implementation based on the information given in the papers published by the author. 4.1. Decoder Model To modify an actual decoder to return just MDCT coefficients (prior to taking the IMDCT and performing overlap and add) would have taken considerable time and effort. Instead, we constructed a simplified decoder                                                              1 http://theremin.music.uiowa.edu/MIS.html 2 http://lame.sourceforge.net/ !\"!!#$!!%#!!%&!!'(!!$%!!$)!!(*!!*'!!\"!!!!!+%!!+%!+$!+*!+&##+%#+$#+*#+&\n,-./0.12345617809.:;;.2<=>;=?>1@>A087>1=B1=C76D=E>A3FD>13\nG0HH519=G7615AI?=?>1@>A087>1=J.-K\n!\"!!#$!!%#!!%&!!'(!!$%!!$)!!(*!!*'!!\"!!!!!+%!!+%!+$!+*!+&##+%#+$\n,-./0.12345617809.:;15<=>;?@;1.18=AB87?587;1AC:D4E:AF=:GHEI=>G4JGHAH:C>GHKGIL:DGH=G,=:GHEI=>G4JGHAH:C=GHIM\n16810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   \nFigure 5. Multiple F0 estimation error rates for several musical instruments and several polyphonies. The reference method is in black. The proposed algorithm is tested for a Unitary model of hearing having 8, 16, 32, and 70 frequency bands. model that fully decodes the mp3 file and then reverses the last two steps by windowing and then taking the MDCT. While a real partial decoder would be best, we consider this decoder model to be a sufficient first attempt at multiple F0 estimation in the transform-domain. To implement the MDCT, we used a “fast MDCT” which utilizes an FFT with two rotations to perform an MDCT. In order to have a consistent basis for comparison with the reference algorithm, we used a frame size of 46ms, which corresponds to 2048 time-domain samples. This is also the largest frame size used in AAC [9]. 4.2. F0 Estimation Results The results of the F0 estimation tests for each instrument and filterbank design are shown in the top of Figure 5.  Error rate is calculated in the same manner as in the reference. The most important result of the F0 estimation results is that performance is roughly equal for filterbank designs with as few as 16 channels. The algorithm performed significantly worse when using less than 16 channels. Interestingly, a 16 channel filterbank design of the range of 60 Hz to 2.2 kHz roughly corresponds to a 1/3 octave filterbank design (which would have 19 channels in this case). This is a common psychoacoustically motivated design for equalizers in stereo systems. This result is important, as it demonstrates that we can drastically reduce the complexity of our filterbank while paying a minimal performance penalty. Additionally, the results show that the algorithm performs well, outperforming the reference algorithm in most cases. The error rates published here are slightly higher than previously published for the reference using a 46ms window. This could be due to implementation inaccuracies or a discrepancy in test material. 4.3. Chroma Estimation Results In some applications, the exact octave that a note is from may not be as important as the chroma of the note. That is, in a mixture that contains the notes C3, E4, and G3, an estimation of C, E, and G may be sufficient. To investigate the proposed algorithm’s perfomance in chroma estimation, the F0 estimation tests were re-run, but this time octave errors were not counted as errors. The results in the bottom half of Figure 5 show that the error rates dropped drastically for all instruments except for Cello. Error rates for each filterbank design with more than 8 channels were less than 5% for low polyphonies. This shows that a majority of the errors from the F0 estimation tests were octave errors. 4.4. Discussion While the proposed algorithm’s performance was impressive on each task, the question still remains as to why the Cello performed so poorly, while the other instruments performed well. One would think that the inharmonicity of stringed instruments as well as the low frequency range of the cello played a part. An analysis of the distribution of samples for each instrument was conducted and this revealed that indeed the cello had a distribution that occupied a significantly lower range than the other instruments. This is likely to have played a larger role than inharmonicity, since the algorithm had no problem dealing with the violin samples.  This reveals a primary weakness of the proposed algorithm. It does not seem to deal well with lower F0s. This is most likely due to inadequate frequency resolution for F0s below the 3rd octave. A higher upsampling rate in the interpolation stage may mitigate this somewhat, but this would increase computational complexity. !\"#$%$!$&$\"$'$#$($)$*$%$$+,--./0,12/342,564-7/+85.94\n:.-;<8.=;>55.5/?42,%#($)&!5,@!\"#$%$!$&$\"$'$#$($)$*$%$$+,-./01/2.034./564,70895:;4\n<:,=>9:?=@55:50A4./%#($)&!5/B\n!\"#$%$!$&$\"$'$#$($)$*$%$$+,-./0.123425637,6289,:;3<08.=,\n>.:?/0.1?@88.83A,62%#($)&!82B\n!\"#$%$!$&$\"$'$#$($)$*$%$$+,-.,/01234056427,6.809:7-;6\n<-.=>:-/=?77-70@642%#($)&!72A!\"#$%$!$&$\"$'$#$($)$*$%$$+,--./0,12/342,564-7/894:2/;$\n<.-=>?.@=855.5/A42,%#($)&!5,B\n!\"#$%$!$&$\"$'$#$($)$*$%$$+,-./01/2.034./564,70894:.0+$\n;<,=>?<@=855<50A4./%#($)&!5/B!\"#$%$!$&$\"$'$#$($)$*$%$$\n+,-./0,1.233,34567896:,/0,184;8<74=6783>6-?42:6@74A$\n%#($)&!38B\n!\"#$%$!$&$\"$'$#$($)$*$%$$+,-.,/01234056427,6.809:6;40<$\n=-.>?@-/>977-70A642%#($)&!72B\n169Poster Session 1\n   \n Figure 6. Polar comparison of flute (left) and cello (right) sample distributions, by chroma (angle) vs. octave (radius). 5. CONCLUSIONS AND FUTURE WORK In conclusion, we have shown here that it may be possible to perform multiple F0 estimation entirely in the transform-domain. We have adapted a state of the art algorithm to work in the transform-domain, which includes a model of human pitch perception. We have shown that upsampling and interpolation of MDCT coefficients is a viable strategy for mitigating the inadequate frequency resolution of frame sizes native to perceptual audio coders. However, we have also found that F0s in the lower octaves still remain a problem due to limited frequency resolution. For the purposes of comparing this algorithm against other multiple F0 estimation algorithms, it would be useful to use a MIREX database [12] for future test material. This would provide more reliable grounds on which to make comparisons. The test material used here was intended to be as close as possible to that used in the reference method. Furthermore, while a large effort was made to accurately implement the reference algorithm, mistakes will always be made because limited publication space inevitably causes some details to be left out. Future work should also include a more detailed decoder model, as well as further experimentation with different upsampling factors and filterbank designs. The MDCT spectra used for this investigation, while fine for a starting point on this research, are certainly not an exact representation of what we would see coming from an actual decoder. Strategies will need to be developed to deal with the limitations of more realistic representations of MDCT coefficients in perceptually coded files. While this work does not address these tedious details, it lays the groundwork for an evolution in that direction. 6. REFERENCES [1] E. Ravelli, G. Richard, and L. Daudet: ''Fast MIR in a sparse transform domain,” Proceedings of the International Symposium on Music Information Retrieval, pp. 527-532, 2008. [2] G. Tzanetakis and P. Cook: ”Sound analysis using MPEG compressed audio,” Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, pp. 761-764, 2000. [3] S. Merdjani and L. Daudet: “Direct estimation of frequency from MDCT-encoded files,” Proceedings of the 6th International Conference on Digital Audio Effects, 2003. [4] A. Klapuri: “A perceptually motivated multiple-F0 estimation method,” IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2005. [5] A. Klapuri: “Signal processing methods for the automatic transcription of music,” PhD Thesis, Tampere University of Technology, 2004. [6] L. Paz: “Multiple-F0 estimation using auditory models,” M. Sci. Thesis, University of Miami, 2008. [7] R. Meddis: “Virtual pitch and phase sensitivity of a computer model of the auditory periphery. 1. Pitch idenficitaion,” Journal of the Acoustical Society of America, vol. 86, no. 6, pp. 2886-2882, 1991. [8] T. Tolonen and M. Karjalainen: “A computationally efficient multipitch analysis model,” IEEE Trans. On Speech and Audio Processing, vol. 8, no. 6, 2000. [9] M. Bosi, et al.: “ISO/IEC Advanced Audio Coding,” Journal of the Audio Engineering Society, vol. 45, no. 10, pp. 789-811, 1997. [10] C. Cheng: “Method for Estimating Magnitude and Phase in the MDCT Domain,” Audio Engineering Society (AES) 116th Convention, Berlin, Germany, 2004. [11] M. Slaney: “An Efficient Implementation of the Patterson Holdsworth Auditory Filter Bank,” Perception Group, Apple Computer, Tech. Rep. 35, 1993. [12] J. Downie: “The Music Information Retrieval Evaluation Exchange: A window into music information retrieval research,” Acoustical Science and Technology, vol. 29, no. 4, pp. 247-255, 2008. \n170"
    },
    {
        "title": "Chronicle: Representation of Complex Time Structures.",
        "author": [
            "Wijnand Schepens"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417353",
        "url": "https://doi.org/10.5281/zenodo.1417353",
        "ee": "https://zenodo.org/records/1417353/files/Schepens09.pdf",
        "abstract": "Chronicle is a novel open source system for representing structured data involving time, such as music. It offers an XML-based file format, object models for internal representation in various programming languages, and software libraries and tools for reading and writing XML and for data transformations. Chronicle defines basic blocks for representing timebased information using events, a hierarchy of groups and instantiable templates. It supports two modes of timing: local timing within a group and association with other elements. The built-in mechanism for resolving time references can be used to implement both timescale mappings and tagging of information. Chronicle aims to be a powerful and flexible foundation on which new file formats and software can be built. Chronicle focuses on structure and timing, but leaves the actual content free to choose. Thus formator softwaredevelopers can specify their own domain-model. This makes it possible to make representations for different types of musical information (scores, performance data, ...) in different styles or cultures (CMN, non-western, contemporary, ...), but also for other domains like choreography, scheduling, task management, and so on. It is also ideal for structured tagging of audio and multimedia (movie subtitles, karaoke, synchronisation, ...) and for representing ”internal” data used in music algorithms. The system is organized in four levels of increasing complexity. Software developed for a specific level and domain will also accept lower level data, while users can choose to represent data in a higher level and use Chronicle tools to reduce the level.",
        "zenodo_id": 1417353,
        "dblp_key": "conf/ismir/Schepens09",
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nCHRONICLE: REPRESENTATION OF COMPLEX TIME STRUCTURES\nWijnand Schepens\nUniversity College Ghent, Belgium\nwijnand.schepens@hogent.be\nABSTRACT\nChronicle is a novel open source system for represent-\ning structured data involving time, such as music.\nIt offers an XML-based ﬁle format, object models for\ninternal representation in various programming languages,\nand software libraries and tools for reading and writing\nXML and for data transformations.\nChronicle deﬁnes basic blocks for representing time-\nbased information using events, a hierarchy of groups and\ninstantiable templates. It supports two modes of timing:\nlocal timing within a group and association with other el-\nements. The built-in mechanism for resolving time refer-\nences can be used to implement both timescale mappings\nand tagging of information.\nChronicle aims to be a powerful and ﬂexible founda-\ntion on which new ﬁle formats and software can be built.\nChronicle focuses on structure and timing, but leaves the\nactual content free to choose. Thus format- or software-\ndevelopers can specify their own domain-model. This makes\nit possible to make representations for different types of\nmusical information (scores, performance data, ...) in dif-\nferent styles or cultures (CMN, non-western, contempo-\nrary, ...), but also for other domains like choreography,\nscheduling, task management, and so on. It is also ideal\nfor structured tagging of audio and multimedia (movie sub-\ntitles, karaoke, synchronisation, ...) and for representing\n”internal” data used in music algorithms.\nThe system is organized in four levels of increasing com-\nplexity. Software developed for a speciﬁc level and domain\nwill also accept lower level data, while users can choose to\nrepresent data in a higher level and use Chronicle tools to\nreduce the level.\n1. INTRODUCTION\nSince the beginning of computing hundreds of music en-\ncodings (representations, formats) have been invented, and\nnew ones are still being developed today. For overviews\nsee e.g. [1–3]. Part of the reason is that the landscape of\nforms of musical information is so large, ranging from au-\ndio to symbolic, from performance to score, from western\nto non-western, from classical to popular, from ancient to\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.contemporary. Some of these terrains are relatively well\nestablished, even standardized, others are still being ex-\nplored.\nMost symbolic music encodings focus on common west-\nern music notation (CWMN). However, many new appli-\ncations are pushing the limits of conventional encodings.\nSome examples:\n•non common western music, e.g. traditional African\nmusic, medieval music\n•special notations such as percussion notations, tab-\nlature, Gregorian plainchant\n•extra data such as lyrics, choreography, instrument-\nspeciﬁc notations, harmony. Also: auxiliary data for\nmusic software, e.g. chroma vectors, onset times, ...\n•synchronization or alignment with multimedia, an-\nnotations and labeling\nExisting encodings are not always suitable to accommo-\ndate the storage of these types of data. Developers are\nforced to invent their own encoding, or to abuse existing\nencodings such as MIDI [1].\nThe most important common factors in these applica-\ntions are time and structure. We have developed a new sys-\ntem for dealing with structured data involving time, called\nChronicle . The system deals with time and structure, but\nleaves the actual content or data free to choose. A devel-\noper has the freedom to represent the content in any form\nhe wishes. Thus the Chronicle system can serve as the\nskeleton for a variety of applications, allowing the devel-\noper to concentrate on content-speciﬁc issues.\nIn the early 90s, one of the ﬁrst systems dealing with\ntime was HyTime [4] on which the symbolic music encod-\ning SMDL [5] was based. Although both are international\nISO/IEC standards, and have had a lot of inﬂuence on later\ninitiatives, they have never been used in practice. There\nare a number of reasons for this, but the main reason is\nprobably that the system was far too complex. Chronicle\nis similar in some aspects, but aims to be as simple as pos-\nsible. Related efforts for symbolic music can be seen in\nMusic Space [6] and SMI [7] or IEEE 1599 [8].\nSimilar time-based approaches can be found in the ﬁeld\nof multimedia, notably with SMIL [9] and its recent off-\nspring Timesheets [10]. For a more theoretical background\nplease refer to [11]. These systems were designed to sched-\nule multimedia presentations. The use of parallel and se-\nquential groups of elements (sound, image, movie) can\n267Poster Session 2\nalso be found in Chronicle, but Chronicle offers much more\nadvanced mechanisms for annotations, timescale mappings,\ntemplates etc.\nChronicle is intended to aid developers of (music) soft-\nware and ﬁle formats, by providing simple yet powerful\nand ﬂexible basic building blocks and structuring mecha-\nnisms. It offers support for working with events, groups,\nrelative time, parallel and sequential layout, timescales,\ntimescale mappings, association, parametrized templates\nand more. The system was developed primarily for sym-\nbolic music, but is also applicable in other domains like\naudio, multimedia, choreography, job scheduling etc.\nThe Chronicle system consists of different parts:\n•external representation: XML-based ﬁle format\n•software tools for manipulating Chronicle XML ﬁles\n•internal representations: object models (interfaces,\nclasses, ...)\n•software libraries for manipulating, storing, parsing...\nInternal representations and libraries are developed in var-\nious programming languages1.\nThe main aim is to facilitate the development of new\ndomain-speciﬁc encodings and software by providing in-\nternal data representations (in the form of interfaces and\nclasses) and software-libraries for various tasks such as\nwriting and parsing XML, processing events, manipulat-\ning structure, querying information etc.\n2. DESIGN\n2.1 Elements and ID’s\nThe basic elements are events andgroups . A group con-\ntains child-elements, which are either events or sub-groups.\nSub-groups contain other elements and so on. The root-\ngroup is the common ancestor of all elements. The result is\na hierarchy or tree-structure, comparable to a ﬁle system.\nEvery element in the tree, except the root, has a parent\ngroup .\nEvery element is identiﬁed within its parent group by a\nunique integer number called local ID. By default elements\nare numbered 0, 1, 2, ..., but it is possible to override this\nby an explicit ID-deﬁnition.\nEvery element in the hierarchy is uniquely identiﬁed by\nits local ID, the local ID of its parent group, the local ID\nof its grandparent group etc. This sequence of local IDs is\ncalled the elements global ID.\nIt is convenient to introduce path-notation , where a global\nID is speciﬁed by concatenating the local IDs top-down\n(starting from the root) separated by slashes, similar to a\nﬁle path or URL. The root itself is notated by /. Thus, for\nexample, global ID /2/3 identiﬁes the element with local\nID 3 in the sub-group with local ID 2 in the root group.\nOptionally, an element can have a name (String) which is\nalso unique within its parent group. Names can be used to\nembellish path-notation, e.g. /part1/section3/4 .\n1currently Java and ActionScriptPath expressions starting with /are called absolute , be-\ncause they identify an element starting from the root. It\nis also possible to identify elements using a relative ID\nfrom within another element. As in a ﬁle-path, one can\nuse ”..” in path-notation to indicate the parent (group). For\nexample ../../section/2 refers to the element with\nlocal ID 2 in a group named ”section” in the grandparent\ngroup of the current element. As an alternative, the no-\ntation @name can be used to refer to an element named\nname anywhere up in the hierarchy. First the parent group\nis searched. If the element is not found there, the grand-\nparent is searched, and so on, until the root is reached.\nThis mechanism is similar to the lookup of variable names\nin nested scopes. For example, @section/2 is equiva-\nlent to ../../section/2 if the grandparent contains a\nchild named ”section”, but the parent doesn’t.\n2.2 Time\nAll elements have a timestamp, which is expressed either\ninlocal time or in non-local time .\nLocal time is represented by an integer number and is to\nbe interpreted relative to the timescale of the parent group.\nThe parent group has its own (start-)time and, optionally, a\nscale. An example in XML-form:\n<group time=\"100\" scale=\"4\">\n<event time=\"10\" />\n</group>\nIn this fragment the ”absolute” time of the event would\nbe100+4 ×10 = 140 . If an element’s time is not speciﬁed,\nit defaults to zero (local time). The scale defaults to one.\nAlternatively, an element can specify its timestamp in\nnon-local time using a time reference . A time reference\nis represented by a path-expression, either absolute or rela-\ntive, which can be written using path-notation starting with\n/,..or@. If this path refers to an element which exists\nin the tree, then the referring element gets the same time as\nthe element it refers to.\n<event name=\"e1\" time=\"100\" />\n...\n<group>\n<event name=\"e2\" time=\"@e1\" />\n</group>\nIn this example event ”e2” refers to event ”e1”, so it also\nhas time 100. Here we used @e1 in the path expression.\nEquivalently, one could specify this using a relative path\n../e1 or an absolute path.\nTime references can also be chained (refer to a refer-\nence...) and mixed with local timing:\n<group name=\"e1\" time=\"100\" />\n<event time=\"10\" />\n<event time=\"20\" />\n</group>\n...\n<group time=\"@e1/0\">\n<event name=\"e2\" time=\"1\" />\n</group>\nHere event ”e2” uses local time, which is added to the\nparent group time, resulting in @e1/1 . This path refers to\n26810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nthe second element (id=1) of the group ”e1”. The result is\nthat the time of event ”e2” resolves to 100 + 20 = 120 .\nThe technique of time references is very powerful. It\ncan be used for associating elements with other elements\n(tagging...), but also to implement mappings between time-\nscales. This will be illustrated in section 3.\nChronicle also supports layout-schemes which allows\nautomatic determination of element times. The most com-\nmon examples are sequential and parallel layout which sched-\nule elements resp. one after the other and at the same time.\n2.3 Levels\nChronicle is organized in four levels of increasing com-\nplexity.\nA level 0 ﬁle consists of one list of events. There is one\nglobal timescale. All events use local time relative to this\ntimescale. The events are ordered in ascending time order:\nevery event must have a time greater than or equal to that\nof its predecessor. No restrictions are imposed on the data\ncarried by the events.\nLevel 1 adds the possibility of groups and nesting, with\nthe restriction that groups have starting time equal to zero.\nThis means that local times in groups are equivalent to ”ab-\nsolute” times in the global timescale. In this (and higher)\nlevels, the events needn’t be ordered in time.\nIn level 2 all elements, including groups, can specify\ntheir (start) time as a local time or using a time reference,\nand groups can use a layout-scheme to set child element\ntimes.\nFinally, level 3 introduces templates and template in-\nstances. Note that every level is a subset of the higher lev-\nels. Chronicle provides tools to transform data to a lower\nlevel.\n2.4 Domain\nAs noted in the introduction the Chronicle system only\ndeals with structure and timing, not with the actual domain-\nspeciﬁc content, which can be chosen freely. Different\nChronicle applications can be developed for speciﬁc do-\nmains representing different types of musical information\n(scores, performance data, ...) in different styles or cultures\n(CMN, non-western, contemporary, ...), but also for other\ndomains like choreography, scheduling, task management,\nand so on. It is also ideal for structured tagging of audio\nand multimedia (movie subtitles, karaoke, synchronisation\nof score and audio, ...) and for representing ”internal” data\nused in music algorithms (chroma, coordinates, ...)\nIn the XML-format domain-speciﬁc information is en-\ncoded in the content of event-elements , either as text or as\none or more child elements. Additionally it is possible to\ngive the event an attribute type . To illustrate this we show\nsome possibilities for encoding a chord-symbol:\n<event>Am7</event>\n<event type=\"chord\">Am7</event>\n<event><chord>Am7</chord></event>\n<event>\n<chord><root>A</root><kind>m7</kind></chord>\n</event>We have provided API’s to read and write Chronicle el-\nements (event, group, ...), but it is up to the user to deal\nwith event-content. Groups cannot carry data themselves,\nalthough it is possible to specify a group-type using the\ntype -attribute. If necessary, extra events can be intro-\nduced in a group to encode group-related information.\n3. EXAMPLE\nWe will demonstrate the key features of the Chronicle sys-\ntem and the level reduction process by means of an ex-\nample. We present the material in XML-form, although\nit should be borne in mind that Chronicle also supports\n”internal” representations and transformations in different\nprogramming languages.\n<chronicle version=\"2.0\" level=\"3\"\ndomain=\"http://.../leadsheet\" />\n<template name=\"note\" >\n<group>\n<event time=\"0\">\n<note_on pitch=\"#pitch\" />\n</event>\n<event time=\"#dur\">\n<note_off pitch=\"#pitch\" >\n</event>\n</group>\n</template>\n<template name=\"voice\">\n<group>\n<group name=\"notes\" layout=\"sequential\">\n<instance model=\"/note\"\npitch=\"D4\" dur=\"12\" />\n<instance model=\"/note\"\npitch=\"D4\" dur=\"12\" />\n<instance model=\"/note\"\npitch=\"D4\" dur=\"9\" />\n<instance model=\"/note\"\npitch=\"E4\" dur=\"3\" />\n<instance model=\"/note\"\npitch=\"F#4\" dur=\"12\" />\n... MORE NOTES ...\n</group>\n<group type=\"lyrics\" time=\"@notes/0\">\n<event time=\"0\" type=\"lyric\">Row,</event>\n<event time=\"1\" type=\"lyric\">row,</event>\n<event time=\"2\" type=\"lyric\">row</event>\n<event time=\"3\" type=\"lyric\">your</event>\n<event time=\"4\" type=\"lyric\">boat</event>\n... MORE LYRICS ...\n</group>\n</group>\n</template>\n<group name=\"song\" time=\"@ticks/0\">\n<group name=\"canon\" layout=\"sequential\">\n<group time=\"0\" name=\"first\">\n<instance model=\"/voice\" />\n</group>\n<group time=\"48\" name=\"second\">\n<instance model=\"/voice\" />\n</group>\n<group time=\"96\" name=\"third\">\n<instance model=\"/voice\" />\n</group>\n<group time=\"144\" name=\"fourth\">\n<instance model=\"/voice\" >\n</group>\n</group>\n<instance name=\"unison\" model=\"/voice\">\n</group>\n<group name=\"ticks\" time=\"@ms/0\" >\n269Poster Session 2\n<event id=\"0\" time=\"0\" />\n<event id=\"192\" time=\"19200\" />\n<event id=\"240\" time=\"21600\" />\n</group>\n</chronicle>\nThis example illustrates an encoding of the song ”Row,\nrow, row your boat”. It is important to note that this is only\none of many possible encodings.\nAfter the XML-preamble, the ﬁle starts with a root-\nelement chronicle . The attribute version speciﬁes\nthe version of the Chronicle system itself. The attribute\nlevel indicates the encoding level used, and the attribute\ndomain speciﬁes the domain (content types and structural\nrestrictions). In this case the attribute points to a URI.\nThe ﬁrst element deﬁnes a template note . A template\nis a kind of prototype which can be instantiated (copied)\nmultiple times. Templates are useful for avoiding code-\nduplication. A template can have parameters, which makes\nit possible to vary the instances. In this case the note -\ntemplate is used as a convenient way to bundle a note-on\nand note-off event. It represents a single note with a certain\npitch and duration. The pitch is encoded as a simple string\nwhich indicates pitch class (e.g. F#) and register or octave\n(4th octave). This is not dictated by Chronicle but is a\nchoice made by the domain-developer.\nNext, the template voice deﬁnes notes and lyrics of\nthe song. The group song instantiates ﬁve copies at dif-\nferent times, representing four voices sung in canon, fol-\nlowed by one in unison. Finally, group ticks relates the\nticks-timescale to milliseconds.\nThe example is a level-3 encoding. We will now illus-\ntrate how it can be reduced to lower levels.\nThe ﬁrst phase, which transforms from level-3 to level-\n2, is template instantiation, also called expansion. It is car-\nried out bottom-up: ﬁrst the innermost elements, then their\nparents and so on. In this case, the voice-template con-\ntains instances of note-templates which are instantiated ex-\npanded ﬁrst, the parameters #pitch and#dur being sub-\nstituted by their actual values deﬁned in attributes pitch\nanddur. Subsequently, the four voice-instances in the\nsong-group are expanded. Since this template has no pa-\nrameter, the instances are exact copies. In the resulting\nlevel-2 ﬁle the templates have disappeared:\n<chronicle version=\"2.0\" level=\"2\"\ndomain=\"http://.../leadsheet\" />\n<group name=\"song\" time=\"@ticks/0\">\n<group time=\"0\" name=\"first\">\n...\n</group>\n<group time=\"48\" name=\"second\">\n<group>\n<group name=\"notes\" layout=\"sequential\">\n<group>\n<event time=\"0\">\n<note_on pitch=\"D4\" />\n</event>\n<event time=\"12\">\n<note_off pitch=\"D4\" />\n</event>\n</group>\n... MORE NOTES ...<group>\n<event time=\"0\">\n<note_on pitch=\"E4\" />\n</event>\n<event time=\"3\">\n<note_off pitch=\"E4\" />\n</event>\n</group>\n... MORE NOTES ...\n</group>\n<group type=\"lyrics\" time=\"@notes/0\">\n<event time=\"0\" type=\"lyric\">Row,</event>\n<event time=\"1\" type=\"lyric\">row,</event>\n<event time=\"2\" type=\"lyric\">row</event>\n<event time=\"3\" type=\"lyric\">your</event>\n<event time=\"4\" type=\"lyric\">boat</event>\n... MORE LYRICS ...\n</group>\n</group>\n</group>\n... THIRD AND FOURTH VOICE ...\n</group>\n<group name=\"ticks\" time=\"@ms/0\" >\n<event id=\"0\" time=\"0\" />\n<event id=\"192\" time=\"19200\" />\n<event id=\"240\" time=\"21600\" />\n</group>\n</chronicle>\nReduction from level-2 to level-1 is carried out in two\nphases. The ﬁrst phase is the layout phase. In the example,\nthenotes -group has sequential layout, which means that\nits elements must be scheduled one after the other. Techni-\ncally, the (start) time of element i+ 1is equal to the (start)\ntime of element iplus the duration of element i, for all i.\nThe duration of a group is equal to the largest local time\nof (grand)child events. If necessary the duration can also\nbe speciﬁed explicitly. In the case of the notes, the duration\nof a note-group is equal to the time of the offset-event.\nIn the resulting ﬁle the element groups have acquired\nan explicit time, and the layout-indications are gone. In\nthe example, the note-groups have times 0,0 + 12 = 12 ,\n12 + 12 = 24 ,24 + 9 = 33 ,33 + 3 = 36 and so on.\nThis is illustrated in the following fragment which shows\nthe onset-event of the fourth note (E4) in the second voice,\nand its ancestor groups:\n<group name=\"song\" time=\"@ticks/0\">\n<group time=\"48\">\n<group>\n<group name=\"notes\">\n...\n<group time=\"33\">\n<event time=\"0\">\n<note_on pitch=\"E4\" />\n</event>\nThe layout phase is followed by the time resolution phase.\nThe timestamps of all elements are resolved in the manner\nillustrated in section 2.2.\nConsider for example the onset of the fourth note (E4)\nin the fragment above. Following the way up from parent\nto parent, it can be seen that the additions result in a time\nequal to @ticks/81 . This means that that note starts on\ntick 81.\nThe domain-developer has chosen to encode the lyrics\nin a separate group which is associated with the notes-\n27010th International Society for Music Information Retrieval Conference (ISMIR 2009)\ngroup. In the group lyrics the event times are added\nto the group-time, yielding value @notes/0 ,@notes/1\nand so on. The @notes -reference points to the notes -\ngroup. Therefore the timestamps of the lyric-events are\nsubstituted by the timestamps of the note-groups which\nthey refer to, in this case @ticks/0 ,@ticks/12 and\nso on.\nThe reference @ticks in turn points to the ticks -\ngroup deﬁned near the end of the example. As this group\ncontains only three events with local ID 0,192and 240,\na reference like /ticks/12 doesn’t point to a real el-\nement. Such a reference is called virtual . In that case\ntimes are resolved by a linear interpolation between real\nelements. In the example ID 0maps to time 0and ID 192\nmaps to 19200 , so each tick in this range has a duration of\n100 ms. This means, for instance, that @ticks/12 re-\nsolves to @ms/1200 which signiﬁes that tick 12occurs\nafter 1200 milliseconds. Ticks in the range up to 240have\na duration equal to (21600 −19200) /(240−192) = 50 ms.\nAs a result, the ﬁfth instance of the voice-template (named\n”unison”) is played in double tempo.\nThe net effect is that the ticks -group deﬁnes a map-\nping between timescales. Note that the mechanism for re-\nsolving (local or non-local) times is used to accomplish\ntwo different goals: a. the lyrics are associated (tagged) to\nnotes, b. the ticks timescale is mapped to the millesecond\ntimescale.\nNote that all references can be resolved to a form ms/x\nwhere xis an integer number. The reference @ms/...\ncannot be resolved any further - this is the ”global” timescale.\nIf we set all group times to zero, then the absolute times are\nequivalent to relative times. The result is a level-1 ﬁle, with\nthe timescale msspeciﬁed in the root-element:\n<chronicle version=\"2.0\" level=\"1\"\ndomain=\"http://.../leadsheet\"\ntimescale=\"ms\" />\n<group name=\"song\">\n<group name=\"first\">\n...\n</group>\n<group name=\"second\">\n<group >\n<group name=\"notes\" >\n<group>\n<event time=\"4800\">\n<note_on pitch=\"D4\" />\n</event>\n<event time=\"6000\">\n<note_off pitch=\"D4\" />\n</event>\n</group>\n...\n... MORE NOTES ...\n<group>\n<event time=\"8100\">\n<note_on pitch=\"E4\" />\n</event>\n<event time=\"8400\">\n<note_off pitch=\"E4\" />\n</event>\n</group>\n...\n... MORE NOTES ...\n</group>\n<group type=\"lyrics\" >\n<event time=\"4800\" type=\"lyric\">Row, </event>\n<event time=\"6000\" type=\"lyric\">\nrow, </event>\n<event time=\"7200\" type=\"lyric\">\nrow </event>\n<event time=\"8100\" type=\"lyric\">\nyour </event>\n<event time=\"8400\" type=\"lyric\">\nboat </event>\n...\n... MORE LYRICS ...\n</group>\n</group>\n</group>\n...\n... THIRD AND FOURTH VOICE ...\n</group>\n...\n... TICKS ...\n</chronicle>\nTo reduce from level-1 to level-0 one more transforma-\ntion is needed. In this ﬁnal phase groups are serialized or\nﬂattened into one long series of events, and they are or-\ndered in ascending time order. In the resulting level-0 ﬁle,\nthere are no more groups:\n<chronicle version=\"2.0\" level=\"0\"\ndomain=\"http://.../leadsheet\"\ntimescale=\"ms\" />\n...\n<event time=\"4800\">\n<note_on pitch=\"D4\" />\n</event>\n<event time=\"4800\" type=\"lyric\">Row,</event>\n<event time=\"6000\">\n<note_off pitch=\"D4\" />\n</event>\n...\n<event time=\"8100\">\n<note_on pitch=\"E4\" />\n</event>\n<event time=\"8100\" type=\"lyric\">your</event>\n<event time=\"8400\">\n<note_off pitch=\"E4\" />\n</event>\n...\n</chronicle>\n4. IN PRACTICE\nHow can the Chronicle system be used in practice?\nThe XML-format can be used by software-developers\nas a convenient means to persist data. Chronicle provides\neasy-to-use libraries for writing and reading the XML-for-\nmat. For new software projects it may even be advisable\nto use the Chronicle classes and interfaces as the basis for\nthe object model, even if XML-persistence is not needed.\nIt is up to the developer to specify the domain-model by\nconstraining content types and level.\nConsider, for example, a Chronicle-encoding of MIDI-\ninformation (see e.g. [1]) By its very nature, this applica-\ntion is ideal for a level-0 encoding, i.e. a ﬂat list of simple\nevents. The domain model establishes event-types (note-\non, note-off, control change, program change) and their\nXML-encoding. A typical event could look like this:\n<event time=\"196\">\n<note_on key=\"100\" velocity=\"127\" channel=\"0\">\n</event>\n271Poster Session 2\nSoftware applications operating on this data, for exam-\nple for playing the music, typically process the events one\nby one. Whereas the processing software is happy to con-\nsume level-0 data, from a musical point of view it may\nbe desirable to add some structure. This can be achieved\nby encoding in a higher level, using mechanisms such as\ngrouping, association, sequences, templates etc. Chronicle\ntools can then be used to transform to level-0 and feed the\nreduced data to the processing software.\nThe nice thing is that users can choose the organiza-\ntion which best suits their needs. For example, one might\nchoose a ”part-by-part” organization using parallel groups\nfor different instrument parts, or a ”frame-by-frame” or-\nganization using a sequence of parallel note-groups. One\ncan choose to use sequential groups for measures, for sec-\ntions (movements). It is also possible to play around with\nassociations, timescales and time-mappings, and so on.\nOne important point which hasn’t been addressed is that\nChronicle ﬁles can be embedded as subgroups within an-\nother chronicle ﬁle using an include -element. The mech-\nanism of association by time-references can be used to add\ninformation to a group without changing it. This is espe-\ncially useful if the target is an embedded group, or if it is\nexternal data such as an audio or multimedia ﬁle.\nChronicle is currently being tested for the encoding of\nleadsheets for wikifonia.org . In the near future, we are\nplanning to create more domain-speciﬁc applications, and\nhope that other developers will do the same.\nDocumentation, tools and open source code can be found\nathttp://code.google.com/p/chronicle-xml/ .\n5. REFERENCES\n[1] E. Selfridge-Field: Beyond MIDI: The Handbook of\nMusical Codes , MIT Press, Cambridge MA, 1997.\n[2] K. Ng, and P. Nesi: Interactive Multimedia Music\nTechnologies , Information Science Publishing, 2007.\n[3] R. Cover: “XML and Music,” retrieved July 6, 2009\nfrom http://xml.coverpages.org/xmlMusic.html, 2006\n[4] C. Goldfarb: Standards: “HyTime: A standard for\nstructured hypermedia interchange,” IEEE Computer\nmagazine , 24(8), 1991.\n[5] D. Sloan: “Aspects of Music Representation in Hy-\nTime SMDL.,” Computer Music Journal , 17, 51-59,\n1993.\n[6] J. Steyn: “Introducing Music Space,” Proceedings of\nthe 4th Open Workshop of MUSICNETWORK: Integra-\ntion of Music in Multimedia Applications , Barcelona,\n2004.\n[7] G. Haus, and M. Longari: “A multi-layered, time-\nbased music description approach based on XML,”\nComputer Music Journal , 29, 70-85, 2005.\n[8] L. Lucidovo: IEEE 1599: “a Multi-layer Approach\nto Music Description,” Journal of Multimedia , 4(1),\n2009.[9] D. Bulterman, J. Jansen, et al.: “Synchronized Mul-\ntimedia Integration Language (SMIL 3.0),” retrieved\nJuly 6, 2009, from http://www.w3.org/TR/2008/REC-\nSMIL3-20081201/, 2008.\n[10] P. Vuorimaa, D. Bulterman, and P. Cesar: “SMIL\nTimesheets 1.0 - W3C Working Draft,” retrieved\nJuly 6, 2009, from http://www.w3.org/TR/2008/WD-\ntimesheets-20080110/, 2008.\n[11] S. Boll, U. Klas, and W. Westermann: “A Com-\nparison of Multimedia Document Models Concerning\nAdvanced Requirements,” Technical Report Ulmer\nInformatik-Berichte No 99-01, 1999.\n272"
    },
    {
        "title": "Efficient Acoustic Feature Extraction for Music Information Retrieval Using Programmable Gate Arrays.",
        "author": [
            "Erik M. Schmidt",
            "Kris West",
            "Youngmoo E. Kim"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416698",
        "url": "https://doi.org/10.5281/zenodo.1416698",
        "ee": "https://zenodo.org/records/1416698/files/SchmidtWK09.pdf",
        "abstract": "Many of the recent advances in music information retrieval from audio signals have been data-driven, i.e., resulting from the analysis of very large data sets. Widespread performance evaluations on common data sets, such as the annual MIREX events, have also been instrumental in advancing the field. These endeavors incur a large computational cost, and could potentially benefit greatly from more rapid calculation of acoustic features. Traditional, clusterbased solutions for large-scale feature extraction are expensive and spaceand power-inefficient. Using the massively parallel architecture of the field programmable gate array (FPGA), it is possible to design an application specific chip rivaling the speed of a cluster for large-scale acoustic feature computation at lower cost. Recent advances in development tools, such as the Xilinx Blockset in Simulink, allow rapid prototyping, simulation, and implementation on actual hardware. Such devices also show potential for the implementation of MIR systems on embedded devices such as cell phones and PDAs where hardware acceleration would be an absolute necessity. We present a prototype library for acoustic feature calculation for implementation on Xilinx FPGA hardware. Furthermore, using a genre classification task we compare the performance of simulated hardware features to those computed using standard methods, demonstrating a nearly negligible drop in classification performance with the potential for large reductions in computation time.",
        "zenodo_id": 1416698,
        "dblp_key": "conf/ismir/SchmidtWK09",
        "keywords": [
            "music information retrieval",
            "audio signals",
            "data-driven",
            "performance evaluations",
            "common data sets",
            "computational cost",
            "FPGA",
            "feature extraction",
            "massively parallel architecture",
            "Xilinx Blockset"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nEFFICIENT ACOUSTIC FEATURE EXTRACTION FOR MUSIC\nINFORMATION RETRIEV AL USING PROGRAMMABLE GATE ARRAYS\nErik M. Schmidt\nMET-lab, Drexel University\neschmidt@drexel.eduKris West\nIMIRSEL, University of Illinois\nkris.west@gmail.comYoungmoo E. Kim\nMET-lab, Drexel University\nykim@drexel.edu\nABSTRACT\nMany of the recent advances in music information retrieval\nfrom audio signals have been data-driven, i.e., resulting\nfrom the analysis of very large data sets. Widespread per-\nformance evaluations on common data sets, such as the\nannual MIREX events, have also been instrumental in ad-\nvancing the ﬁeld. These endeavors incur a large computa-\ntional cost, and could potentially beneﬁt greatly from more\nrapid calculation of acoustic features. Traditional, cluster-\nbased solutions for large-scale feature extraction are ex-\npensive and space- and power-inefﬁcient. Using the mas-\nsively parallel architecture of the ﬁeld programmable gate\narray (FPGA), it is possible to design an application spe-\nciﬁc chip rivaling the speed of a cluster for large-scale\nacoustic feature computation at lower cost. Recent ad-\nvances in development tools, such as the Xilinx Blockset in\nSimulink, allow rapid prototyping, simulation, and imple-\nmentation on actual hardware. Such devices also show po-\ntential for the implementation of MIR systems on embed-\nded devices such as cell phones and PDAs where hardware\nacceleration would be an absolute necessity. We present a\nprototype library for acoustic feature calculation for imple-\nmentation on Xilinx FPGA hardware. Furthermore, using\na genre classiﬁcation task we compare the performance of\nsimulated hardware features to those computed using stan-\ndard methods, demonstrating a nearly negligible drop in\nclassiﬁcation performance with the potential for large re-\nductions in computation time.\n1. INTRODUCTION\nThe extraction of appropriate acoustic features is the ﬁrst\nstep for nearly all audio-based music information retrieval\napplications. Many recent advances in MIR systems are\nthe result of large-scale, data-driven analysis of audio sam-\nples. Such corpora may contain thousands (or even mil-\nlions, in the case of some commercial databases) of audio\nﬁles, and the accompanying analyses of these data sets de-\nmands vast computational resources. In seeking improved\nmethods for music classiﬁcation and understanding, resear-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.chers are constantly searching for more informative feature\nsets, which requires the ability to rapidly prototype and\nevaluate new features on very large databases. Addition-\nally, performance evaluations, such as the annual MIREX\nevents [1], of multiple approaches to speciﬁc application\ntasks on common data sets have proven to be invaluable\nfor advancing the state-of-the-art in MIR research. These\nevaluations, however, are increasingly difﬁcult to adminis-\nter, since both the number of participants and the size of\nthe data sets continues to grow annually.\nThe most common solution to problems having such\ncomputational demands involves an investment in comput-\ning clusters, which are expensive and inefﬁcient (in terms\nof both their utilization of hardware resources and energy\nconsumed). Using the massively parallel architecture of\nthe ﬁeld programmable gate array (FPGA) it is possible to\nachieve parallel processing on a scale similar to that of a\nsmall cluster (for speciﬁc applications) on a single chip.\nCurrent tools such as the Xilinx System Generator (XSG)\nfor DSP1enable rapid prototyping of DSP algorithms in\nthe graphical language of Simulink with the ability to in-\ncorporate hardware in the modeling and design loop. Ad-\nditionally, any algorithm built using the Simulink Xilinx\nBlockset can be easily compiled into Verilog or VHDL\ncode and incorporated into a larger hardware system de-\nsign. One possible implementation would be MIR on em-\nbedded, mobile devices, such as cell phones and PDAs,\nwhere the computation of acoustic features would not be\npossible using the onboard CPU. Such a hardware accel-\neration system could be designed both for the computation\nof acoustic features, and evaluation of the decision func-\ntion of a pre-trained classiﬁer.\nWe have developed an acoustic feature extraction li-\nbrary, implemented using XSG, that can be synthesized\ndirectly on supported FPGA hardware. The library sup-\nports the calculation of both Mel-Frequency Cepstral Coef-\nﬁcients (MFCC) [2] and common Statistical Spectrum De-\nscriptors (SSDs). Here, we present preliminary classiﬁca-\ntion results using ﬁxed-point MFCC features calculated by\nthe XSG (simulating an FPGA implementation). The accu-\nracy of these features is veriﬁed through their use in a genre\nclassiﬁcation task on a medium-sized audio database, and\nwe demonstrate that the resulting classiﬁcation performance\nis comparable to that of a ﬂoating-point MATLAB and\ndouble-precision Java implementation of similar feature cal-\n1Xilinx System Generator: http://www.xilinx.com/ise/\noptional_prod/system_generator.htm\n273Poster Session 2\nculation algorithms.\nFurthermore, we also present a process for migrating\nacoustic features designed using MATLAB to the Xilinx\nSystem Generator in Simulink. The ultimate goal of this\nendeavor is to develop a system in which features can be\nrapidly and easily prototyped within Simulink, synthesized\nto Verilog hardware description language (HDL), and em-\nbedded into a larger standalone FPGA-based system-on-\nchip design. Tools developed for such a platform could be\neasily shared between members of the MIR research com-\nmunity and could potentially allow even an inexperienced\nHDL programmer to take advantage of the performance\ngained from implementing feature extraction in hardware.\n2. BACKGROUND\nEarly efforts at implementing acoustic feature computation\nin hardware required systems built entirely from scratch,\nwhich is a signiﬁcant and time-consuming endeavor when\nworking directly with low-level hardware descriptor lan-\nguages (HDLs). Prior work has almost exclusively targeted\nMFCC feature calculation for automatic speech recogni-\ntion. For example, [3] presents an optimized algorithm\nfor efﬁcient computation of MFCCs using an FPGA im-\nplementation, while [4] focuses on implementing only the\nFFT sub-calculation in hardware for eventual use in MFCC\ncomputation. In the direction of easing the dependence\non hardware arithmetic units, [5] proposes modifying the\nMFCC algorithm from a triangular ﬁlterbank to a mel-\nspaced rectangular ﬁlterbank, and their results demonstrate\nonly a minimal decrease in classiﬁcation accuracy. Other\nwork has focused on full hardware system integration for\nspeech recognition. In [6] an on-chip, retrainable hardware\nspeech recognition system is presented using MFCC fea-\ntures and Hidden Markov Models (HMMs) for statistical\npattern recognition.\nThe annual Music Information Retrieval Evaluation eX-\nchange (MIREX) tasks, initiated in 2005, have become a\ncore component of the ﬁeld of MIR in terms of advanc-\ning and disseminating the latest research and results. With\nthe number of tasks, participants, and data sets increasing\nannually, the evaluations have become exponentially more\ndifﬁcult to administer. The International Music Informa-\ntion Retrieval Systems Evaluation Laboratory (IMIRSEL),\nthe organizers of MIREX, has traditionally gone above and\nbeyond in order to accommodate a wide range of imple-\nmentation platforms and architectures and retains a range\nof machines, including an ever-expanding cluster, for this\npurpose, resulting in additional complexity. This conﬁgu-\nration, while offering a great deal of ﬂexibility, limits their\nability to take advantage of parallel processing implemen-\ntations, which are still highly platform-speciﬁc. A 72-hour\nruntime cap is enforced for all submissions, but even so, re-\ncent evaluations have required up-to 1000 person-hours of\neffort. A signiﬁcant speedup in feature computation could\ngreatly reduce the runtime requirements for MIREX.3. HARDWARE IMPLEMENTATION OF\nFEATURES\nThe Xilinx System Generator (XSG) tools in Simulink en-\nable the prototyping of complex signal processing algo-\nrithms for hardware implementation in a relatively straight-\nforward manner. For our initial implementation, we lim-\nited ourselves to analysis windows with a length of 512\nsamples and 50% overlap, 40 triangular mel-ﬁlters, and 20\nDCT coefﬁcients. These parameters were chosen because\nof their wide application in audio and music processing al-\ngorithms. Other research has shown that the number of\nﬁlters and DCT coefﬁcients can be greatly reduced while\nstill maintaining adequate performance [7].\nMATLABSimulink(Xilinx Blockset)Xilinx SystemGeneratorHardwareDeployment\nFigure 1 . Implementation ﬂowchart for audio feature ex-\ntraction algorithms.\nThe ﬁrst processing stage for our audio features requires\na DFT calculation, which is performed using the pipelined\nXilinx core FFT v5.0, producing real-time serial FFT data.\nNext, using two multipliers, we square the results of the\nreal and imaginary parts and add them together to obtain\nthe energy spectral density (ESD), which is quantized to\n32-bits. In this case, ESD is preferred to the magnitude\nFFT because the square root function necessary to obtain\nmagnitude is not easily implemented in hardware. XSG in-\ncludes Coordinate Rotation Digital Computer (CORDIC)\nalgorithms which can compute square roots as well as trig-\nonometric, logarithmic, and division functions using only\naddition, subtraction, bitshift, and table lookup, but in a\npractical design these algorithms tend to take up large ar-\neas of the FPGA fabric and often incur large delays. In\ngeneral, we avoided the use of these functions whenever\npossible.\nCmFmRmSpectralCentroidSpectralFluxSpectralRolloff\nMm,cDCTlog(•)MelFilterbankAudioSegmentmXilinx Core FFT:  XmESD:  Re{Xm}2 + Im{Xm}2\nFigure 2 . Hardware implementation of audio feature com-\nputation.\n27410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n3.1 Mel-Frequency Cepstral Coefﬁcients\n3.1.1 Algorithm\nMel-frequency cepstral coefﬁcients (MFCCs) are among\nthe most widely used acoustic features in speech and audio\nprocessing. MFCCs are essentially a low-dimensional rep-\nresentation of the spectrum warped according to the mel-\nscale, which reﬂects the nonlinear frequency sensitivity of\nthe human auditory system [2]. In our implementation,\nMFCCs are deﬁned as\nMm,c=40/summationdisplay\nb=1ˆXm,bcos/bracketleftbigg\nc/parenleftbigg\nb−1\n2/parenrightbiggπ\n40/bracketrightbigg\n,c= 1,2,...,20\n(1)\nˆXm,b= log/parenleftBigg\nfb[k]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN−1/summationdisplay\nn=0xm[n]e−j2πnk\nN/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightBigg\n, (2)\nwheremrepresents the current frame. Normally, MFCCs\nare implemented over short-time segments, and accord-\ningly our implementation divides the audio into overlap-\nping segments and applies a Hanning window function to\nreduce edge effects. The Discrete Fourier Transform (DFT)\nof each short-time segment is computed using the FFT\nalgorithm. The magnitude of the frequency components\nis determined and fb[k], the mel-spaced triangular ﬁlters\n(Figure 3), are applied via multiplication in the frequency\ndomain. Continuing with the cepstrum calculation, the log\nof the mel-ﬁltered energies is calculated ( ˆXm,b), which to\nsome extent, serves to deconvolve the audio by transform-\ning multiplications in the frequency-domain (and thus, con-\nvolutions in the time-domain) into additions. As a ﬁnal\nstep, the inverse DFT is applied to ˆXm,busing the DCT\n(sine components are not needed since the input is guaran-\nteed to be real and even). This step is also used to reduce\nthe dimensionality of the data to the desired quantity, and\nit has been shown that the DCT has the additional effect of\ndecorrelating the vector of feature components [8].\n0 1000 2000 3000 4000 5000 6000 7000 800000.0050.010.015Mel−spaced Triangular FiltersAmplitude\nFrequency (Hz)\nFigure 3 . 40-band mel-warped triangular ﬁlterbank\n3.1.2 FPGA Implementation\nCalculation of the MFCC features consists of applying the\nmel-ﬁlterbank to the spectrum, taking the log, and com-\nputing the DCT. Applying the mel-ﬁlterbank requires 40\nread-only memory (ROM) elements, 40 multipliers, and\n40 accumulators. A control register is placed on the out-\nput which is triggered by FFT completion to only allow\nthe ﬁlterbank output to change once for every frame. Tominimize the size of the ROM elements and the number of\nmultiplies, the mel-ﬁlter coefﬁcients are restricted to 16-\nbits. This stage is the most resource intensive part of the\ndesign due to the number of multipliers required.\nOnce the data is ﬁltered, it is again serialized in order\nto compute the log. This requires a wait of 512 samples\nuntil the next FFT frame is supplied, therefore ample time\nis available to serialize the 40 ﬁlter band values. Using a\nsingle CORDIC log, all 40 values are computed and sub-\nsequently quantized to 32-bits.\nThe ﬁnal step involves computation of the DCT, where\nthe DCT coefﬁcients are stored in 20 ROM units and have\nbeen quantized to 16-bit resolution. In addition, this step\nrequires 20 multipliers and 20 accumulators. The output\nis triggered using a simple control register such that the\noutput values change only once every 512 values. Since no\nfurther processing is required, this data is decimated by a\nfactor of 512 and returned as the ﬁnal output.\n3.2 Statistical Spectrum Descriptors\nIn music and audio processing, Statistical Spectrum De-\nscriptors (SSDs) are often related to timbral texture [9].\nFor each spectral shape function, we begin by dividing the\ndata into short-overlapping segments, applying a Hanning\nwindow, and computing the magnitude DFT.\n3.2.1 Spectral Centroid\nSpectral centroid is deﬁned as the weighted-average (cen-\nter of mass) of the spectrum,\nCm=/summationtextK−1\nk=0F[k]|Xm[k]|\n/summationtextK−1\nk=0|Xm[k]|, (3)\nwhereXm[k]is the DFT of short-time segment m, and\nF[k]is a vector of frequencies corresponding to the bins\nof the magnitude spectrum.\nComputation of the spectral centroid requires a multi-\nplication, two accumulators, and a division. Here the spec-\ntrum is summed with one accumulator and the spectrum,\nmultiplied by the respective spectral bin values, is summed\nby the other accumulator. After passing a control register\nto ensure only one value is returned for each frame, the re-\nsult is divided by the CORDIC divider provided by XSG.\n3.2.2 Spectral “Flux”\nSpectral ﬂux is deﬁned as the Euclidean distance between\nsuccessive spectral frames. We compute the square of this\nfeature, which is deﬁned as follows:\nFm=K−1/summationdisplay\nk=0(|Xm[k]|−|Xm−1[k]|)2. (4)\nAgain,Xm[k]is the discrete spectrum of the current anal-\nysis framemandXm−1[k]is the spectrum of the previous\nframe.\nSpectral ﬂux is the simplest of all of the spectral shape\nfeatures to compute. The hardware consists of a 512 sam-\nple delay block to maintain a copy of the previous frame,\n275Poster Session 2\nan adder, a multiplier, and an accumulator. An important\nnote is that we are not computing the square root, but sim-\nply the sum of the squares as to avoid the use of additional\nCORDIC functions.\n3.2.3 Spectral Rolloff\nSpectral rolloff is deﬁned as the frequency beneath which a\ngiven proportion of the total spectral energy lies, typically\n85%:\nRm=fs\nKrm=fs\nK/parenleftBigg\narg\nrmrm/summationdisplay\nk=0|Xm[k]|= 0.85K−1/summationdisplay\nk=0|Xm[k]|/parenrightBigg\n.\n(5)\nHere|Xm[k]|is the magnitude of the k-th frequency sam-\nple of the current frame and rmis the frequency sample\nnumber that produces the desired 85% rolloff.\nThe core of the spectral rolloff implementation consists\nof two accumulators. The ﬁrst accumulator sums the spec-\ntrum to obtain the total energy and multiplies it by 0.85,\nwhere as the second sums the delayed spectrum until the\ntotal energy reaches 85%. Once this value is obtained, the\nfrequency value of the correspond spectral bin is returned.\nOf these features, spectral rolloff is probably the most ro-\nbust to quantization effects as it is returning values of an\nalready discretized function.\n4. HARDWARE PERFORMANCE AND USAGE\nThe initial hardware target for this project is Digilent’s\nVirtex-II Pro Development System. The Xilinx Virtex-II\nFPGA on the board (XC2VP30) contains 13,969 slices,\n136 18-bit multipliers, 2,448Kb of block RAM, and two\nPowerPC Processors. While the available amount of FPGA\nfabric is highly constrained, at an academic discounted cost\nof$299.00USD, the board is an attractive target, and its\nwidespread adoption in education creates greater opportu-\nnities for algorithm experimentation and deployment.\nImplementing only the ESD algorithm on this chip re-\nquires 299 slices, 8 18-bit multipliers, and no block RAM.\nConsidering the total size and resource restrictions the most\nlimiting factor is the number of multipliers required. Us-\ning all 136 multipliers and 5,083 slices we could compute\nthe ESD for up to 17 analysis frames in parallel. For a\nsingle FFT implementation, the ﬁrst frame requires 1123\nclock cycles to compute and subsequent frames require an\nadditional 512 cycles. With 17 in parallel, the ﬁrst set of\nframes will still need 1123 clock cycles, although we will\nnow output 17 frames at a time. Assuming the fabric is\nclocked at 80MHz, a conservative clock speed, a break-\ndown of performance is shown in Table 1.\nWe compared the hardware performance using a sin-\ngle FFT unit on an FPGA to that of the M2K toolkit [10]\nand MATLAB on a set of 600 audio clips, each 30 sec-\nonds in duration (the data set used in the classiﬁcation task\nis detailed in the next section). The M2K and MATLAB\nfeatures were calculated using a single processor core of\na 2.4 GHz Intel Core 2 CPU. Table 2 reveals the compu-\ntation times for each feature set, averaged across all 600First Subsequent Three Thirty\nFFTs frame ( µs) frames ( µs) secs (ms) secs (ms)\n1 14.04 6.400 3.316 33.08\n17 14.04 0.376 0.206 1.953\nTable 1 . Performance of spectrogram calculation on sim-\nulated hardware.\nclips, and we observe that there is more than an order of\nmagnitude difference between the hardware and software\nimplementations. While MATLAB and M2K each produce\nresults in just under a half second for each 30 second clip,\nthe hardware requires only around 33ms. Additionally, it\ncan be seen from Table 3 that as the number of FFT units\navailable on the FPGA increases, the hardware can achieve\nsub 1ms computation times.\nToolkit Computation Time (s)\nM2K 0.483\nMATLAB 0.364\nFPGA 0.033\nTable 2 . Comparison of feature computation times be-\ntween software and hardware implementations.\nIn its current form, implementing the full system-on-\nchip (including MFCC and SSD calculations) with a sin-\ngle FFT unit requires 15,542 slices, 145 18-bit multipli-\ners, and 1,080 Kb of block RAM, which exceeds the ca-\npacity (in terms of slices and multipliers) of the hardware\ntarget. With some additional optimization, perhaps trad-\ning off some parallelism to reduce hardware resource uti-\nlization, we believe the full system could be implemented\non the targeted Virtex-II VC2VP30-based system. Other\nproducts in the Virtex-II FPGA family provide additional\nhardware resources, which offer the possibility of com-\nbining multiple feature computation engines on a single\nchip. For example, the Virtex-II XC2VP100 provides suf-\nﬁcient slices and multipliers to easily accommodate 3 fea-\nture computation engines. Additionally, certain develop-\nment boards contain multiple FPGA chips, adding further\nparallelization opportunities. With two FPGAs, we could\npotentially accommodate 6-8 computation engines on a sin-\ngle system board.\nThe current design requires 2048 clock cycles to pro-\nduce the ﬁrst output for the spectral shape features and\n2560 for MFCCs. As with the ESD, each additional frame\ntakes 512 cycles and compute time decreases proportion-\nally with the addition of each parallel computational en-\ngine. More speciﬁc timing results for a single computation\nengine (again clocked at a conservative 80 MHz) are pro-\nvided in Table 3, as well as theoretical performance num-\nbers if six parallel feature computation engines could be\nimplemented on a single system. Although such develop-\nment hardware is currently quite expensive (approximately\n$10K, on the order of a small cluster), the performance is\npotentially faster by more than an order of magnitude.\n27610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nComp. First Three Thirty\nFeature Engines frame ( µs) secs (ms) secs (ms)\nS. Shape 1 25.60 3.328 33.09\nS. Shape 6 25.60 0.576 5.536\nMFCC 1 32.00 3.334 33.10\nMFCC 6 32.00 0.582 5.542\nTable 3 . Performance of feature extraction on simulated\nhardware.\n5. CLASSIFICATION EXPERIMENT\nIn order to conﬁrm the efﬁcacy of the hardware extracted\nfeatures, we have conducted an evaluation of genre classi-\nﬁcation systems based on the features produced and com-\npared its performance to that of classiﬁers based on the\nsame features computed in MATLAB and the M2K toolkit\n[10]. We conducted two experiments using a collection of\n600 tracks drawn from the Magnatune collection of Cre-\native Commons licensed music [11], divided into six gen-\nres. An overview of the collection is given in table 4.\nGenre Number of tracks\nAmbient 100 tracks\nClassical 100 tracks\nElectronic 100 tracks\nEthnic 100 tracks\nJazz and Blues 100 tracks\nRock 100 tracks\nTotal 600 tracks\nTable 4 . Composition of dataset for genre classiﬁcation\ntask.\nPampalk [12] identiﬁes the potential for the over-ﬁtting\nof the characteristics of a particular artist to inﬂate accu-\nracy scores in the evaluation of audio content-based genre\nclassiﬁcation systems, particularly when evaluating sys-\ntems on small collections. Hence, we have conducted both\nartist-ﬁltered and conventional cross-validated classiﬁca-\ntion experiments based on 5-fold random 80:20 splits and\n5-fold stratiﬁed cross-validation, respectively.\n5.1 Pre-processing of Features\nThe feature extractors yield a very large number of fea-\nture vectors for each track (based on 23 ms windows with\n50% overlap), and in order to effectively and efﬁciently\nclassify tracks, the features must be summarised to pro-\nduce a smaller number of more informative vectors. One\napproach that has been effectively used by many authors\n[12–15] is to summarise the distribution of feature frames\nover the track. This may be performed by, for example,\nestimating the parameters of a single Gaussian distribution\nor a mixture of Gaussians. However, [16] and [12] pro-\nvide experimental evidence that the performance of tech-\nniques based on mixtures of Gaussian distributions are at\nbest equal to that of single Gaussian based approaches,\nmaking their extreme additional computational cost im-possible to justify. This lack of additional discriminative\npower for the use of mixture distributions is at odds with\nresearch in many other audio indexing problems [16]. Hence,\nin our evaluation the feature stream is summarised as a ﬂat-\ntened single Gaussian distribution (mean vector and and\nﬂattened upper triangular covariance matrix).\n5.2 Classiﬁcation Algorithms\nThe classiﬁcation algorithms tested were drawn from the\nM2K toolkit [10] and Weka [17] and include: Fisher’s Cri-\nterion Linear Discriminant Analysis (LDA) [18], Classi-\nﬁcation and Regression Trees (CART) [19], a ﬁrst-order\nlinear Support Vector Machine (based on John C. Platt’s\nSequential Minimal Optimisation, SMO, algorithm [20])\nand the J48 decision tree algorithm [17].\n5.3 Classiﬁcation Results\nThe results of the artist-ﬁltered and unﬁltered classiﬁcation\nexperiments are given in Tables 5 and 6, respectively. For\neach classiﬁcation method, the highest-performing feature\nset is highlighted in bold.\nClassiﬁer CART J48 LDA Linear SMO\nFeature set\nM2K 36.43% 36.79% 35.70% 45.36%\nMatlab 34.24% 35.15% 37.16% 46.63%\nFPGA 34.24% 34.97% 37.89% 49.00%\nTable 5 . Artist-ﬁltered classiﬁcation results.\nClassiﬁer CART J48 LDA Linear SMO\nFeature set\nM2K 41.67% 44.50% 41.17% 59.17%\nMatlab 38.83% 40.83% 40.67% 56.67%\nFPGA 42.67% 39.83% 41.33% 57.50%\nTable 6 . Cross-validated classiﬁcation results.\n6. DISCUSSION AND FUTURE WORK\nThe results above demonstrate that the implementation of\nacoustic feature computation in hardware can potentially\nreduce computation times by orders of magnitude, accom-\npanied, at worst, by a nearly negligible decrease in classiﬁ-\ncation accuracy. This initial implementation demonstrates\na potential pathway for migrating MATLAB feature ex-\ntraction code to the Xilinx Blockset in Simulink and ulti-\nmately to hardware. The current implementation, however,\ndoes not allow for easy deployment on FPGA hardware\nand additional challenges lie in integrating FPGA-based\nfeature computation in a full MIR evaluation system.\nOur next step is to create a custom platform for full\nhardware deployment in a standalone system. In this sys-\ntem, based on the Virtex-II Pro development board, the lo-\ncal computer will communicate with the FPGA board via\ngigabit ethernet. The onboard hardware PowerPC cores\n277Poster Session 2\nwill ease development for the standalone platform by al-\nlowing us to write C code to manage the communication\nlink with a host PC and the data ﬂow in and out of the\nfeature extraction logic.\nSuch a custom FPGA platform would allow algorithms\nto be quickly designed and tested in Simulink, and then\ncompiled into Verilog code to be synthesized into the larger\nproject. Given a full deployment system, it will be possi-\nble to run the system at much higher clock speeds than\nin the Simulink simulation and hardware-in-the-loop co-\nsimulation. The fully deployed system will ultimately be\na massively parallel system where multiple analysis win-\ndows are processed simultaneously.\n7. ACKNOWLEDGEMENTS\nThe authors would like to thank the International Music\nInformation Retrieval Systems Evaluation Lab (IMIRSEL)\nat the University of Illinois at Urbana Champaign for their\nhelp in implementing the experiments reported. This work\nis partially supported by National Science Foundation grant\nIIS-0644151.\n8. REFERENCES\n[1] J.S. Downie. The music information retrieval evalu-\nation exchange (2005–2007): A window into music\ninformation retrieval research. Acoustical Science and\nTechnology , 29(4):247–255, 2008.\n[2] S. B. Davis and P. Mermelstein. Comparison of para-\nmetric representations for monosyllabic word recog-\nnition in continuously spoken sentences. IEEE Trans-\nactions on Acoustics, Speech and Signal Processing ,\nASSP-28, No. 4:357–366, August 1980.\n[3] J. C. Wang, J. F. Wang, and Y . S. Weng. Chip design of\nMFCC extraction for speech recognition. Integration ,\n32(1-2):111–131, Jan 2002.\n[4] M. Nilsson and K. K. Paliwal. Speaker veriﬁcation in\nsoftware and hardware. Microelectronic Engineering\nResearch Conference , 2001.\n[5] W. Han, C. Chan, C. Choy, and K. Pun. An efﬁcient\nMFCC extraction method in speech recognition. Pro-\nceedings 2006 IEEE International Symposium on Cir-\ncuits and Systems (ISCAS) , page 4, Jan 2006.\n[6] S. Nedevschi, R. Patra, and E. Brewer. Hardware\nspeech recognition for user interfaces in low cost, low\npower devices. Design Automation Conference, 2005.\nProceedings. 42nd , pages 684 – 689, May 2005.\n[7] S. Sigurdsson, K. B. Petersen, and T. Lehn-Schiøler.\nMel frequency cepstral coefﬁcients: An evaluation of\nrobustness of MP3 encoded music. In Proceedings of\nthe Seventh International Conference on Music Infor-\nmation Retrieval (ISMIR) , 2006.[8] B. Logan. Mel frequency cepstral coefﬁcients for mu-\nsic modeling. In Proceedings of the First International\nSymposium on Music Information Retrieval (ISMIR) ,\nOctober 2000.\n[9] G. Tzanetakis and P. Cook. Musical genre classiﬁca-\ntion of audio signals. Speech and Audio Processing,\nIEEE Transactions on , 10(5):293–302, 2002.\n[10] J. S. Downie, A. F. Ehmann, and D. Tcheng. Music-\nto-knowledge (M2K): a prototyping and evaluation en-\nvironment for music information retrieval research. In\nSIGIR ’05: Proceedings of the 28th annual interna-\ntional ACM SIGIR conference on Research and devel-\nopment in information retrieval , pages 676–676, New\nYork, NY , USA, 2005. ACM.\n[11] J. Buckman. Magnatune: Mp3 music and music licens-\ning, April 2006.\n[12] E. Pampalk. Computational Models of Music Similar-\nity and their Application in Music Information Re-\ntrieval . PhD thesis, Johannes Kepler University, Linz,\nMarch 2006.\n[13] K. West. Novel techniques for Audio Music Classiﬁ-\ncation and Search . PhD thesis, School of Computing\nSciences, University of East Anglia, Norwich, United\nKingdom, September 2008.\n[14] B. Logan and A. Salomon. A music similarity function\nbased on signal analysis. In Proceedings of IEEE Inter-\nnational Conference on Multimedia and Expo (ICME) ,\nAugust 2001.\n[15] M. Mandel and D. Ellis. Song-level features and sup-\nport vector machines for music classiﬁcation. In Pro-\nceedings of ISMIR 2005 Sixth International Confer-\nence on Music Information Retrieval , 2005.\n[16] J.-J Aucouturier. Ten Experiments on the Modeling of\nPolyphonic Timbre . PhD thesis, University of Paris 6,\nFrance, June 2006.\n[17] I. H. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes,\nand S. J. Cunningham. Weka: Practical Machine\nLearning Tools and Techniques with Java Implementa-\ntions. ICONIP/ANZIIS/ANNES , pages 192–196, 1999.\n[18] K. West and S. Cox. Features and classiﬁers for the au-\ntomatic classiﬁcation of musical audio signals. In Pro-\nceedings of ISMIR 2004 Fifth International Conference\non Music Information Retrieval , 2004.\n[19] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J.\nStone. Classiﬁcation and Regression Trees . Wadsworth\nand Brooks/Cole Advanced books and Software, 1984.\n[20] J. C. Platt. Fast training of support vector machines us-\ning sequential minimal optimization. Advances in Ker-\nnel Methods: Support Vector Learning , 1999.\n278"
    },
    {
        "title": "A Filter-and-Refine Indexing Method for Fast Similarity Search in Millions of Music Tracks.",
        "author": [
            "Dominik Schnitzer",
            "Arthur Flexer",
            "Gerhard Widmer"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417831",
        "url": "https://doi.org/10.5281/zenodo.1417831",
        "ee": "https://zenodo.org/records/1417831/files/SchnitzerFW09.pdf",
        "abstract": "We present a filter-and-refine method to speed up acoustic audio similarity queries which use the Kullback-Leibler divergence as similarity measure. The proposed method rescales the divergence and uses a modified FastMap [1] implementation to accelerate nearest-neighbor queries. The search for similar music pieces is accelerated by a factor of 10−30 compared to a linear scan but still offers high recall values (relative to a linear scan) of 95 −99%. We show how the proposed method can be used to query several million songs for their acoustic neighbors very fast while producing almost the same results that a linear scan over the whole database would return. We present a working prototype implementation which is able to process similarity queries on a 2.5 million songs collection in about half a second on a standard CPU.",
        "zenodo_id": 1417831,
        "dblp_key": "conf/ismir/SchnitzerFW09",
        "keywords": [
            "filter-and-refine method",
            "Kullback-Leibler divergence",
            "rescaling divergence",
            "modified FastMap",
            "nearest-neighbor queries",
            "search for similar music pieces",
            "accelerated by a factor of 10−30",
            "high recall values",
            "working prototype implementation",
            "2.5 million songs collection"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nA FILTER-AND-REFINE INDEXING METHOD FOR\nFAST SIMILARITY SEARCH IN MILLIONS OF MUSIC TRACKS\nDominik Schnitzer\nAustrian Research Institute\nfor Artiﬁcial Intelligence\ndominik.schnitzer@ofai.atArthur Flexer\nAustrian Research Institute\nfor Artiﬁcial Intelligence\narthur.ﬂexer@ofai.atGerhard Widmer\nJohannes Kepler University\nLinz, Austria\ngerhard.widmer@jku.at\nABSTRACT\nWe present a ﬁlter-and-reﬁne method to speed up acous-\ntic audio similarity queries which use the Kullback-Leiblerdivergence as similarity measure. The proposed methodrescales the divergence and uses a modiﬁed FastMap [1]implementation to accelerate nearest-neighbor queries.The search for similar music pieces is accelerated by a fac-tor of 10−30compared to a linear scan but still offers high\nrecall values (relative to a linear scan) of 95−99% .\nWe show how the proposed method can be used to query\nseveral million songs for their acoustic neighbors very fastwhile producing almost the same results that a linear scanover the whole database would return. We present a work-ing prototype implementation which is able to process sim-ilarity queries on a 2.5million songs collection in about\nhalf a second on a standard CPU.\n1. INTRODUCTION\nToday an unprecedented amount of music is available on-\nline. As of April 2009, the Apple iTunes music store alonelists more than 10 million downloadable songs in its cata-log. Other online music stores like Amazon MP3 still offera 5 million songs catalog to choose from. With the catalog\nnumbers constantly reaching new record highs, the need\nfor intelligent music search algorithms that provide new\nways to discover and navigate music is apparent.\nUnfortunately many of the intelligent music processing\nalgorithms that have been published do not easily scale to\nthe millions of music pieces available in an online music\nstore. In particular, this is true for music recommendationalgorithms which compute acoustic music similarity using\na Gaussian timbre representation and the Kullback-Leibler\ndivergence, as in [2], [3] or [4].\nEspecially the Kullback-Leibler divergence, as it is used\nin the referenced works, poses multiple challenges whendeveloping a large scale music recommendation system:(1) the divergence is very expensive to compute, (2) it isnot a metric and thus makes building indexing structures\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.around it very hard and (3) in addition, the extracted acous-\ntic music similarity features have a very high degree offreedom, which too is a general problem for indexing so-\nlutions (“curse of dimensionality”) [5].\nBut on the other hand, systems using this technique reg-\nularly rank in the very top places in the yearly MIREXAutomatic Music Recommendation evaluations\n1, which\nmakes them a tempting but challenging target for broadusage in real applications.\n1.1 Related Work\nThe idea of using FastMap-related techniques for computa-\ntionally heavy non-metric similarity measures and nearest\nneighbor retrieval was ﬁrst demonstrated by Athitsos [6].\nThey use BoostMap [7] to improve the speed of classify-\ning handwritten digits. Cano et al. [8] use FastMap to mapthe high dimensional music timbre similarity space into a2-dimensional space for visualization purposes.\nRoy et al. [9] present a music recommendation sys-\ntem which uses a Monte-Carlo approximation of theKullback-Leibler (KL) divergence as similarity measure.The Monte-Carlo approximation of the KL divergence isfar more expensive to compute and less accurate than theclosed form of the KL divergence which is used in our pa-per and recent music similarity algorithms. To speed up asimilarity query, they narrow the number of nearest neigh-bor candidates by incrementally increasing the accuracy ofthe Monte-Carlo sampled divergence measure.\nAnother interesting approach, which was pursued by\nGarcia [10], is to compute computationally expensive sim-ilarity measures on modern graphics processors (GPUs).\nModern GPUs offer high ﬂoating-point performance and\nparallelism. As an example Garcia shows how a lin-ear brute force nearest neighbor scan using the Kullback-Leibler divergence can be accelerated on a GPU compared\nto computing it on a standard CPU. The idea to use the\nGPU to process similarities could be combined with themethods presented in this paper.\nWith muﬁn.com there also exists a commercial music\nrecommendation service which computes acoustic audiosimilarities for a very large database of music (6 milliontracks as of April 2009). However, their website gives nohint on how their service works\n2.\n1http://www.music-ir.org/mirexwiki/\n2http://www.muﬁn.com/us/faq.html\n537Oral Session 6: Similarity\n1.2 Contributions of this paper\nThe contribution of this paper is three-fold:\n•First, we present a ﬁlter-and-reﬁne method based on\nFastMap which allows quick music similarity queryprocessing. It is designed to work with very largemusic databases which use Gaussian timbre modelsand the Kullback-Leibler divergence as music simi-larity measure.\n•Second, we show how a rescaling of the divergence\nvalues and a new FastMap pivot object selectionheuristic substantially increase the nearest neighborrecall of the algorithm.\n•Finally, we present an implementation of a music\nrecommendation system using the proposed tech-\nniques which handles a 2.5million tracks evaluation\ncollection in a very efﬁcient way.\n2. PRELIMINARIES\n2.1 DataThroughout this paper we use a collection of 2.5million\nsongs to evaluate the performance and to show the practicalfeasibility of our approach. The 2.5million tracks consist\nof30second snippets of songs gathered by crawling an\nonline music store offering free audio preview ﬁles.\n2.2 Similarity\nWe extract timbre features from the snippets and compute\na single Gaussian timbre representation using the methodproposed by Mandel & Ellis [2]. We compute 25Mel\nFrequency Cepstrum Coefﬁcients (MFCCs) for each audio\nframe, so that a Gaussian timbre model xﬁnally consists\nof a 25-dimensional mean vector μand covariance matrix\nΣ. For performance reasons we also precompute and store\nthe inverted covariance matrix Σ\n−1.\nTo compute acoustic timbre similarity we use the sym-\nmetrized version ( SKL ) of the Kullback-Leibler diver-\ngence ( KL, [11]), deﬁned between two multivariate nor-\nmal distributions x1∼N (μ1,Σ1)andx2∼N (μ2,Σ2):\nSKL (x1,x2)=1\n2KL(x1,x2)+1\n2KL(x2,x1).(1)\nA query for similar songs is processed in a linear scan\nby computing the SKL between the Gaussian x1of the\nseed song and all other songs in the database. The songs\nwith the lowest divergence to the seed song are its nearestneighbors and possible recommendations.\n2.3 Nearest neighbor recall\nTo compare the effectiveness of the nearest neighbor re-\ntrieval variants evaluated, we used what we call nearest\nneighbor ( NN) recall. We deﬁne it as the ratio of true near-\nest neighbors found by some algorithm ( NN\nfound )t ot h e\nnumber of true nearest neighbors ( NNtrue). The true near-\nest neighbors are found by a full linear scan.recall =|NNfound ∩NNtrue|\n|NNtrue|(2)\n3. THE METHOD\nTo build our ﬁlter-and-reﬁne method for fast similarity\nqueries we use an adopted version of FastMap [1], a Mul-tidimensional Scaling (MDS) technique. MDS [12] is awidely used method for visualizing high-dimensional data.It takes the distance matrix of a set of items as input andmaps the data to vectors into an arbitrary-dimensional Eu-clidean space. FastMap is straightforward to use even for\nlarge databases since it only needs a low and constant num-\nber of rows of the similarity matrix to compute the vectormapping. However, FastMap requires the distances to ad-here to metric properties.\n3.1 Original FastMap\nThe original FastMap [1] algorithm uses a simple mapping\nformula (Equation 3) to compute a k-dimensional projec-\ntion of objects into the Euclidean vector space. The dimen-sionkis arbitrary and can be chosen as required. Usually\nhigher dimensions yield a more accurate mapping of the\noriginal similarity space.\nTo project objects into a k-dimensional Euclidean vec-\ntor space, ﬁrst two pivot objects from the feature databasehave to be selected for each of the kdimensions. The orig-\ninal algorithm uses a simple heuristic to select those pivot\nobjects: for each dimension ( j=1..k), (1) chose a random\nobject x\nrfrom the database, (2) search for the most distant\nobject of xrusing the original distance measure D()and\nselect it as the ﬁrst pivot object xj,1for the dimension, (3)\nthe second pivot object xj,2is the object most distant to\nxj,1in the original space.\nAfter the 2kpivot objects have been selected, the vector\nrepresentation of an object xis computed by estimating\nFj(x)for each dimension ( j=1..k):\nFj(x)=D(x, xj,1)2+D(xj,1,xj,2)2−D(x, xj,2)2\n2D(xj,1,xj,2)\n(3)\nThis method depends on metric properties of Dto pro-\nduce meaningful mappings. However, it has been noted\nthat FastMap works surprisingly well also for non-metric\ndivergence measures [7].\nAs FastMap only requires a distance function Dand\npivot objects to compute the vector mapping, it can be in-\nstantly applied to map the Gaussian timbre models with theSKL as distance function to Euclidean vectors (ignoring\nthe fact that the SKL is not metric).\n3.2 A Filter-And-Reﬁne Method using FastMap\nTo use FastMap to quickly process music recommenda-\ntion queries, we initially use it to map the Gaussian timbremodels to k-dimensional vectors. In a two step ﬁlter-and-\nreﬁne process we then use those vectors as a preﬁlter: ﬁrst\n53810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nweﬁlter the whole collection in the vector space (with the\nsquared Euclidean distance) to return a number ( ﬁlter-size )\nof possible nearest neighbors, then we reﬁne the result by\ncomputing the exact SKL on the candidate subset to re-\nturn the nearest neighbors. By using the SKL toreﬁne\nthe results, the correct nearest neighbor ranking is ensured.\nWe set the parameter ﬁlter-size to a fraction of the whole\ncollection.\nSince the complexity of a single SKL comparison is\nmuch higher than a simple vector comparison, the use ofthe squared Euclidean distance to preﬁlter the data resultsin large speedups compared to a linear scan over the wholecollection using the SKL . Table 1 compares the computa-\ntional cost (in ﬂoating point operations, flops ) of the SKL\nto the squared Euclidean distance d\n2using different vector\ndimensions ( k) to preﬁlter candidate nearest neighbors.\nDivergence flops flops/flops SKL\nSKL 3552 1\nd2,k=2 0 60 0.017\nd2,k=4 0 120 0.034\nd2,k=6 0 180 0.051\nTable 1 . The computational complexity (in flops ) of com-\nputing the squared Euclidean distance ( d2) is, even for high\nmapping dimensions like k=6 0 , much lower than the\ncosts of computing a single SKL comparison. Note: We\nalready use an optimized implementation of the SKL ex-\nploiting matrix symmetry and the sequence of matrix op-erations [13].\nUnfortunately, as we show in the next section (3.3), ap-\nplying FastMap to the problem without any modiﬁcationsyields very poor results.\n3.3 Modiﬁcations\nIn our implementation we have included two important\nmodiﬁcations which improve the quality of FastMap map-pings for nearest neighbor retrieval. The modiﬁcationsare centered around two thoughts: (1) a metric divergencemeasure would produce better vector mappings, and (2) a\nmore specialized heuristic for pivot object selection could\nproduce better mappings especially for the near neighbors,which are the center of our interest.\n3.3.1 Rescaling\nBefore mapping the objects x\ni∈Xto ak-dimensional\nvector (Equation 3), we propose to rescale the original\nsymmetric Kullback-Leibler divergences ( SKL ) by taking\nthe square-root:\nD(x1,x2)=/radicalbig\nSKL (x1,x2). (4)\nThis rescaling has the effect of making the SKL behave\nmore like a metric. As the SKL already has the important\nproperties of being symmetric and non-negative, it onlyfails to fulﬁll the triangle inequality. Taking the square roothas the effect to partly ﬁx the divergence, making it more1 10 100 200 5000.40.450.50.550.60.650.70.750.80.850.90.951\nNearest NeighborsNearest Neighbor RecallA, Median, none\nB, Basic, none\nC, Median, √x\nD, Median, e−1/100 x\nE, Median, e−1/50 x\nFigure 1 . Nearest neighbor (NN) recall of two pivot object\nselection methods ( median : the proposed pivot object se-\nlection heuristic, basic : the original FastMap heuristic) in\ncombination with three divergence rescaling methods ( no-\nrescaling ,eλx,√x). NN recall is averaged over ﬁve in-\ndependent evaluation runs ( 10.000 queries per run), each\ntime using a new random collection. Parameters: k=4 0 ,\nﬁlter-size=10% ,collection size=100.000 .\nmetric [14]. Another more common way is to rescale the\nSKL witheλSKL ()(see [3] or [2]).\nWe have experimentally veriﬁed the effect of rescaling\non a collection of 100.000 randomly drawn Gaussian tim-\nbre models (Table 2), by checking the triangle inequality.The table clearly shows that exponentiating indeed reducesthe number of cases where the triangle inequality is vio-lated, but it does not work as well as taking the squareroot, which makes the SKL obey the triangle inequality\nin more than 99% of the cases in our experimental setup.\nDivergence % triangle inequality\nSKL () 91.57%\n1−eλSKL (),λ=−1\n10093.71%\n1−eλSKL (),λ=−1\n5095.60%/radicalbig\nSKL () 99.32%\nTable 2 . Percentage of Gaussian object triples fulﬁlling\nthe triangle inequality ( D(x, z)≤D(x, y)+D(y,z)) with\nand without rescaling. The triangle inequality was checkedfor all possible triples in a collection of 100.000 randomly\nselected Gaussian timbre models.\n3.3.2 Pivot Object Selection\nTo select the pivot objects which are needed to map an ob-\njectxto a vector space, the original algorithm uses two ob-\njects for each dimension which lie as far away from each\nother as possible (see Section 3.1). In contrast to the orig-\ninal heuristic we propose to select the pivot objects usingan adapted strategy: (1) ﬁrst we randomly select an objectx\nrand compute the distance to all other objects; (2) we\n539Oral Session 6: Similarity\nthen select the ﬁrst pivot object x1to be the object lying at\nthe distance median, i.e. the object at the index i=⌊N/2⌋\non the sorted list of divergences; (3) likewise, the second\npivot object x2is selected to be the object with the distance\nmedian of all divergences from x1to all other objects.\nBy using pivot objects at the median distance we avoid\nusing objects with extremely high divergence values whichoften occur in the divergence tails when using the SKL .\nSince we are also particularly interested in optimally map-\nping the near neighbors and not the whole divergence\nspace, this strategy should also help in preserving theneighborhoods.\n3.3.3 Improvements\nFinally, we measure how these modiﬁcations improve the\nﬁlter-and-reﬁne method by experimentally computing thenearest neighbor (NN) recall of each change on a 100.000\nsongs collection. Figure 1 shows the result of the exper-\niment. A huge improvement in the nearest neighbor re-call can be seen for all strategies which use the medianpivot object selection heuristic ( A,C,D,E) compared to\nthe original FastMap heuristic ( B). The ﬁgure also shows\nthat rescaling the SKL values helps to further increase the\nNN recall. The suggested strategy ( C) using the median\npivot object selection strategy together with square-root-\nrescaling gives the best results.\n4. IMPLEMENTATION\nThe implementation of the ﬁlter-and-reﬁne music recom-\nmendation engine is straightforward: in an initial step thewhole collection is preprocessed with the proposed map-\nping method, transforming the database objects into a k-\ndimensional vector space. This is a linear process since\nonly 2kpivot objects have to be selected and each object in\nthe database is mapped to a vector using Equation 3 once.Our implementation saves the pivot objects for each di-mension and the vector mappings of processed objects todisk. This allows fast restarting of the system and easy\nprocessing of new objects.\nTo query for similar objects we use the previously de-\nscribed ﬁlter-and-reﬁne method, ﬁltering out a predeﬁnednumber ( ﬁlter-size , a percentage of the collection size) of\nnearest neighbor candidates using the vector representationand reﬁning the result with the exact SKL .\nThis outlines the general method we propose, but obvi-\nously two parameters which have a huge impact on the re-trieval quality (nearest neighbor (NN) recall) and the queryspeed have not been discussed yet: the number of vectordimensions kand the ﬁlter-size .\n4.1 Recall and Speed\nIt is obvious that a larger ﬁlter-size results in better NN\nrecall values but higher computational costs. Likewise, a\nhigher kused for the vector mapping results in a more ac-\ncurate mapping of the divergence space, but with each di-mension the computational costs to compute the squaredEuclidean distance in the preﬁlter steps are increased./;#23 /;#23/;#23#23/;#23/;#23#23#23 /;#23/;#23#23/;#23/;#23#23#23#23 /;#23/;#23#23/;#23/;#23#23#23#23#23 /;#23/;#23#23/;#23/;#23#23#23#23#23#23 /;#23/;#23#23/;#23#23#23#23#23#23#23 /;#23/;#23#23/;#23#23#23#23#23#23#23/;#23#23#23 /;#23/;#23#23/;#23#23#23#23#23#23#23/;#23#23#23#23 /;#23/;#23#23/;#23#23#23#23#23#23#23/;#23#23#23#23#23 /;#23/;#23#23/;#23#23#23#23#23#23#23/;#23#23#23#23#23#23/;#23/;#23#23/;#23#23#23#23#23#23#23#23/;#23/;#23#23/;#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23/;#23#23/;#23#23#23#23#23#23/;#23/;#23#23/;#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23/;#23#23/;#23#23#23#23#23#23#23#23#23#23/;#23/;#23#23/;#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23/;#23 /;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23 /;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23 /;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23\n/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23/;#23 /;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23/;#23\n/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23/;#23 /;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23 /;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23/;#23\n/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23/;#23 /;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23 /;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23/;#23\n/;#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g31/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g32/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/g33/g34/g35\nFigure 2 . Plot relating the nearest neighbor recall and\nthe ﬂoating point operations resulting from different ﬁlter-\nand-reﬁne parameter combinations, to a full linear scan\n(flops/flops SKL ). Recall is computed for the 10nearest\nneighbors for different parameter combinations of kand\nﬁlter-size in a collection of 100.000 songs. A good com-\nbination (good recall, low computational cost) would be\nmapped to the upper left corner of the plot.\nFigure 2 evaluates different parameter combinations of\nkand ﬁlter-size and their impact on nearest neighbor re-\ncall and computational cost (and thus query speed). The\ndiagram was compiled using a collection of 100.000 Gaus-\nsian timbre models. It shows the 10-NN retrieval recall and\nquery speed (computational cost in terms of ﬂops).\nThe ﬁgure shows that a parameter combination of k=\n20and ﬁlter-size =5 % can be selected to achieve about\n95% 10-NN recall. That combination would take only 7%\nof the query time required by a linear scan with the SKL .\nIf a 10-NN recall of 85% is acceptable a parameter com-\nbination requiring only 3.5% the computational cost of a\nlinear scan is possible ( k=2 0 and ﬁlter-size =2 % ). Al-\nmost perfect 10-NN recall values ( >98−99% ) can be\nreached when setting ﬁlter-size to about 10% of the collec-\ntion size, which still requires only 10% of the time a linear\nscan would need.\nThis evaluation shows how a good parameter combina-\ntion for a collection should be selected. In Section 5 weplot a similar diagram (Figure 3) to select the best parame-ters for a 2.5million song collection achieving 99% 1-NN,\n98% 10-NN and 95% 100-NN recall on the collection.\n4.2 Errors\nAnother aspect which is of interest is how falsely reported\nnearest neighbors (false positives) affect the average qual-\nity of music recommendations. We have done a 1-NN\ngenre evaluation (with artist ﬁlter, see [3]). This is a stan-\ndard evaluation to test the quality of a music recommenda-tion algorithm.\nWe tested four different collections (three in-house col-\nlections and the Ismir 2004 Magnatune music collection\n54010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nwhich is freely available for testing purposes3). Table 3\nsummarizes the results. It appears that the errors whichare being made do not affect the classiﬁcation accuracy\nin an adverse way. Classiﬁcation accuracy decreases only\nby about 0.1%for the two larger collections and by about\n0.5%for the two small collections.\nCollection, size Genres F&R Full Scan\n#1,N= 16781 21 30.17% 30.28%\n#2,N= 9369 16 28.55% 28.66%\n#3,N= 2527 22 28.27% 28.78%\nIsmir 2004 ,N= 729 6 64.47% 64.88%\nTable 3 . 1-NN genre evaluation results (with artist ﬁlter)\non four different collections. The table compares the genreclassiﬁcation accuracy of the ﬁlter-and-reﬁne (F&R) ap-proach presented in this paper with a full exact linear scan.Parameters: k=4 0 ,ﬁlter-size= 5%\n5. PROTOTYPE PERFORMANCE\nTo show the practical feasibility of using this ﬁlter-and-\nreﬁne method with large music databases we use themethod on the 2.5 million song collection and build a pro-totype music recommendation system. The system shouldbe able to answer queries for the 100 nearest neighborswith high speed and recall.\n1 10 100 200 50000.10.20.30.40.50.60.70.80.91\nNearest NeighborsNearest Neighbor Recall0.1%\n0.4%\n0.6%\n1%\n2%\n5%\n8%\n10%Filter−Size:\nFigure 3 . NN recall with different ﬁlter-sizes evaluated on\n1% ( =2 5.000)o ft h e 2.5million songs collection. With a\nﬁlter-size of 5% one can achieve 95% 100-NN recall and\n98% 10-NN and 99% 1-NN recall. k=4 0 .\nTo select the optimal parameter we ran an experiment\nto determine the best ﬁlter-size ,kwas set to 40. Figure 3\nshows the recall values for different NN and ﬁlter-sizes .I t\ncan be seen that the true 1-NN and 10-NN are retrievedalmost always if the ﬁlter-size is set to 5%,8%or10% of\nthe collection size.\n3http://ismir2004.ismir.net/genre contest/index.htmIn a second experiment (Figure 4) we compare the ac-\ntual query response times of three different ﬁlter-size set-\ntings ( ﬁlter-size =8 % ,5%,2%,k=4 0 ) to a full linear\nscan. It can be seen that the system running on a single\nstandard CPU core is capable of answering music recom-\nmendation queries in half a second while returning about\n95% of the correct 100 nearest neighbors compared to a\nlinear scan which would take about 7.8secon the system.\n/;#23/;#23#23/;#23#23#23/;#23#23#23#23/;#23#23#23#23#23/;#23#23#23#23#23#23/;#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23#23#23\n/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23/;#23#23#23/;#23/;#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\n/g31/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g32/g33/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g34/g35/g36/;#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g37/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g35/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23 /g38/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23\nFigure 4 . Comparison of the time it takes to query a 2.5\nmillion song collection for nearest neighbors using a fullscan compared to a scan using the ﬁlter-and-reﬁne method\nproposed. The PC used a standard Intel Core Duo CPU\n(2.5GHz) and had all Gaussian models loaded to RAM.\n6. CONCLUSIONS\nWe have described a ﬁlter-and-reﬁne method for fast ap-\nproximate music similarity search in large collections. Themethod is designed for Gaussian music timbre features us-ing the symmetric Kullback-Leibler divergence to com-\npute acoustic similarity, but could be generalized to other\ndistance measures. A prototype implementation of ourmethod handling 2.5million tracks is able to answer mu-\nsic similarity queries in about half a second on a standarddesktop CPU.\nBy accelerating similarity queries by a factor 10 to 30,\nwe show how a large scale music recommendation servicerelying on recent music information retrieval techniquescould operate.\nACKNOWLEDGMENTS\nThis research is supported by the Austrian Research Fund\n(FWF) under grant L511-N15, and by the Austrian Re-\nsearch Promotion Agency (FFG) under project number815474-BRIDGE.\n7. REFERENCES\n[1] C. Faloutsos and K.I. Lin. FastMap: A fast algorithm\nfor indexing, data-mining and visualization of tradi-tional and multimedia datasets. In Proceedings of the\n1995 ACM SIGMOD international conference on Man-\nagement of data , pages 163–174. ACM New York, NY ,\nUSA, 1995.\n[2] M. Mandel and D. Ellis. Song-level features and sup-\nport vector machines for music classiﬁcation. In Pro-\nceedings of the 6th International Conference on Music\nInformation Retrieval (ISMIR05), London, UK , 2005.\n541Oral Session 6: Similarity\n[3] E. Pampalk. Computational models of music similar-\nity and their application in music information retrieval.Docteral dissertation, Vienna University of Technol-\nogy, Austria , 2006.\n[4] T. Pohle and D. Schnitzer. Striving for an improved au-\ndio similarity measure. 4th Annual Music Information\nRetrieval Evaluation Exchange , 2007.\n[5] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft.\nWhen is“ nearest neighbor” meaningful? Lecture\nNotes in Computer Science , pages 217–235, 1999.\n[6] V . Athitsos, J. Alon, and S. Sclaroff. Efﬁcient nearest\nneighbor classiﬁcation using a cascade of approximatesimilarity measures. In IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition,2005. CVPR 2005 , volume 1, 2005.\n[7] V . Athitsos, J. Alon, S. Sclaroff, and G. Kollios. Boost-\nMap: A method for efﬁcient approximate similarityrankings. In Computer Vision and Pattern Recogni-\ntion, 2004. CVPR 2004. Proceedings of the 2004 IEEEComputer Society Conference on , volume 2.\n[8] P . Cano, M. Kaltenbrunner, F. Gouyon, and E. Batlle.\nOn the use of FastMap for audio retrieval and brows-ing. In Proc. Int. Conf. Music Information Retrieval\n(ISMIR) , pages 275–276, 2002.\n[9] P . Roy, J.J. Aucouturier, F. Pachet, and A. Beurive. Ex-\nploiting the tradeoff between precision and cpu-time\nto speed up nearest neighbor search. In Proceedings of\nthe 6th International Conference on Music InformationRetrieval (ISMIR05), London, UK , 2005.\n[10] V . Garcia, E. Debreuve, and M. Barlaud. Fast k nearest\nneighbor search using GPU. In IEEE Computer Society\nConference on Computer Vision and Pattern Recogni-tion Workshops, 2008. CVPR Workshops 2008 , pages\n1–6, 2008.\n[11] W.D. Penny. Kullback-Leibler divergences of normal,\ngamma, Dirichlet and Wishart densities. Wellcome De-\npartment of Cognitive Neurology , 2001.\n[12] T.F. Cox and M.A.A. Cox. Multidimensional scaling .\nCRC Press, 2001.\n[13] D. Schnitzer. Mirage – High-Performance Music Sim-\nilarity Computation and Automatic Playlist Genera-\ntion. Master’s thesis, Vienna University of Technology ,\n2007.\n[14] K.L. Clarkson. Nearest-neighbor searching and met-\nric space dimensions. Nearest-Neighbor Methods for\nLearning and Vision: Theory and Practice , pages 15–\n59, 2006.\n542"
    },
    {
        "title": "Unsupervised Detection of Cover Song Sets: Accuracy Improvement and Original Identification.",
        "author": [
            "Joan Serrà",
            "Massimiliano Zanin",
            "Cyril Laurier",
            "Mohamed Sordo"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418063",
        "url": "https://doi.org/10.5281/zenodo.1418063",
        "ee": "https://zenodo.org/records/1418063/files/SerraZLS09.pdf",
        "abstract": "The task of identifying cover songs has formerly been studied in terms of a prototypical query retrieval framework. However, this framework is not the only one the task allows. In this article, we revise the task of identifying cover songs to include the notion of sets (or groups) of covers. In particular, we study the application of unsupervised clustering and community detection algorithms to detect cover sets. We consider current state-of-the-art algorithms and propose new methods to achieve this goal. Our experiments show that the detection of cover sets is feasible, that it can be performed in a reasonable amount of time, that it does not require extensive parameter tuning, and that it presents certain robustness to inaccurate measurements. Furthermore, we highlight two direct outcomes that naturally arise from the proposed framework revision: increasing the accuracy of query retrieval-based systems and detecting the original song within a set of covers.",
        "zenodo_id": 1418063,
        "dblp_key": "conf/ismir/SerraZLS09",
        "keywords": [
            "cover songs",
            "query retrieval framework",
            "unsupervised clustering",
            "community detection",
            "cover sets",
            "state-of-the-art algorithms",
            "reasonable amount of time",
            "parameter tuning",
            "robustness",
            "query retrieval-based systems"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nUNSUPERVISED DETECTION OF COVER SONG SETS: ACCURACY\nIMPROVEMENT AND ORIGINAL IDENTIFICATION\nJoan Serr `a†, Massimiliano Zanin‡, Cyril Laurier†, and Mohamed Sordo†\n†Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain.\n‡Universidad Aut ´onoma de Madrid, Madrid, Spain.\njoan.serraj@upf.edu, massimiliano.zanin@hotmail.com, {cyril.laurier,mohamed.sordo }@upf.edu\nABSTRACT\nThe task of identifying cover songs has formerly been stu-\ndied in terms of a prototypical query retrieval framework.\nHowever, this framework is not the only one the task al-\nlows. In this article, we revise the task of identifying cover\nsongs to include the notion of sets (or groups) of covers. In\nparticular, we study the application of unsupervised clus-tering and community detection algorithms to detect cover\nsets. We consider current state-of-the-art algorithms and\npropose new methods to achieve this goal. Our experi-\nments show that the detection of cover sets is feasible, that\nit can be performed in a reasonable amount of time, that\nit does not require extensive parameter tuning, and that\nit presents certain robustness to inaccurate measurements.\nFurthermore, we highlight two direct outcomes that natu-\nrally arise from the proposed framework revision: increas-\ning the accuracy of query retrieval-based systems and de-\ntecting the original song within a set of covers.\n1. INTRODUCTION\nCover song identiﬁcation has been a very active area\nof study within the music information research (MIR) com-\nmunity over the last years [1]. Traditionally, cover song\nidentiﬁcation has been set up as a typical information re-\ntrieval (IR) task where queries are processed in a batch\nmode [2] (p. 74): the user submits a query (a song) and\nreceives an answer back (a list of songs ranked by their\nrelevance to the query). Such a setup has conditioned the\nway of implementing and evaluating cover song identiﬁca-\ntion systems [1, 3].\nHere we take a new qualitative approach by consider-\ning cover song sets instead of isolated cover song queries.\nMore concretely, we automatically identify sets (or groups)\nof covers of the same underlying musical piece in a music\ncollection. We do so by utilizing grouping algorithms on\ntop of an existing cover song identiﬁcation system. We\nfocus on unsupervised clustering [4,5] and community de-\ntection [6, 7] algorithms.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.The usage of grouping algorithms can be intuitively jus-\ntiﬁed from multiple perspectives. First, grouping algo-\nrithms are a natural choice given the output of current cover\nsong identiﬁcation systems. Because this usually consists\nin a set of pairwise distances1, we can directly assume that\nthe preliminary issues of a typical pattern grouping task\n[5], namely feature extraction and distance measurement,\nare appropriately dealt with. Second, grouping algorithms\nmay help in obtaining more coherent answers for isolated\nqueries. In particular, the answers to any query song be-\nlonging to a given cover set would coherently contain the\nother songs in the set (notice that this property is not en-\nsured by the distance measurements nor by batch-mode\nquery systems). Third, grouping algorithms may proﬁt\nfrom second order cover song associations. For instance,\nif cover song pairs si,sjandsj,skare independently de-\ntected, the cover song relation between siandskcould au-\ntomatically be inferred. This way, the system would take\nadvantage of these collaborative effects and, among other\nthings, increase the overall accuracy. Fourth, grouping al-\ngorithms can provide insightful clues to the study of inter\nand intra-group relations (e.g. by using hierarchical clus-\ntering algorithms [4, 5]).\nThe Music Information Retrieval Evaluation eXchange\n(MIREX) provides a batch-mode query framework for eval-\nuating cover song identiﬁcation systems2. Nonetheless,\nsome participants have started moving towards the cover\nset detection framework. This framework has been indi-\nrectly and scantily introduced in [8] and, simultaneously,\nin our previous work in [9]. In [8], a multidimensional\nscaling analysis was performed on the basis that the con-\nﬁguration of the music collection under study was known\n(i.e. the number of cover sets and their cardinality was a\npriori deﬁned, and the latter was assumed to be constant\nfor all sets). This analysis was shown to substantially in-\ncrease the ﬁnal system’s identiﬁcation accuracy. A com-\nparable increase was also achieved by the post-processing\nstep mentioned in [9], whose details we now disclose.\nBelow, we ﬁrst present the grouping algorithms that we\nuse for detecting cover song sets (Sec. 2). We then summa-\nrize the followed methodology (Sec. 3) and subsequently\npresent the obtained results (Sec. 4). We also show two\n1In this article we pragmatically use the term distance to refer to any\nsimilarity or dissimilarity measure between cover songs.\n2http://www.music-ir.org/mirex/2008/index.php/\nAudio_Cover_Song_Identification\n225Poster Session 2\nnatural outcomes of detecting cover sets (Sec. 5): increas-\ning the accuracy of batch-mode query systems (Sec. 5.1)\nand detecting the original song (Sec. 5.2). We ﬁnally close\nthe article with a conclusions section (Sec. 6).\n2. STUDIED ALGORITHMS\n2.1 K-medoids\nThe K-medoids algorithm is a common choice when the\ncomputation of means is unavailable, as it solely operates\non pairwise distances and can exhibit some advantages com-\npared to the standard K-means algorithm [4]. We employ\nthe K-medoids implementation of the TAMO3package,\nwhich incorporates several heuristics4to achieve an opti-\nmal K value [4]. We use the default parameters and try all\npossible heuristics provided in the aforementioned imple-\nmentation.\n2.2 Hierarchical clustering\nWe also consider four representative agglomerative hierar-\nchical clustering methods [4, 5]: single linkage, complete\nlinkage, group average linkage (UPGMA), and weighted\naverage linkage (WPGMA). We use the hcluster5imple-\nmentation with the default parameters, and we try different\ncluster validity criteria [4] such as6checking descendants\nfor inconsistent values, or considering the maximal or the\naverage inter-cluster cophenetic distance.\n2.3 Proposed method 1\nThe present and subsequent methods perform community\ndetection [6, 7] on a complex network [10]. A weighted\ncomplex network [11] can be easily built from pairwise\ndistances. Given a music collection S={si},i=1,...,N S,\nwithNSsongs, we query a cover song identiﬁcation sys-\ntem for each song and obtain a set of answers A={Ai},\nwhere Aicontains NAituples {sj,d(si,sj)},sj∈S,\nranked from low to high according to the provided dis-\ntance measure d. In our case, NAican be different for\neachAiand it can be signiﬁcantly lower than NS.A s\nwe do not expect cover songs to have high distances or\nranks in Ai, we determine NAiby applying a distance and\na rank threshold dThandrTh, respectively. From A,w e\nconstruct a graph GwithNSvertices ( V={vi},V↔S)\nandNAiweighted edges for each vertex (an edge with a\nweight wi,j=1\nd(si,sj)+/epsilon1,/epsilon1being an arbitrary small con-\nstant, e.g. /epsilon1=0.01, is assigned between vertices viandvj\nifsj⊂Aiorsi⊂Aj).\nThe method performs community detection by just look-\ning at connected vertices in Gin such a way that all con-\nnected vertices are assigned to the same community. There-\nfore, this algorithm strongly relies on dThand/or rThpa-\nrameters. This approach presents some analogies with the\ncommon nearest neighbor clustering approach [5].\n3http://fraenkel.mit.edu/TAMO\n4http://fraenkel.mit.edu/TAMO/documentation/\nTAMO.Clustering.Kmedoids.html\n5http://code.google.com/p/scipy-cluster\n6http://www.soe.ucsc.edu/ ˜eads/cluster.html2.4 Proposed method 2\nThe aforementioned approach could be further improved\nby reinforcing triangular connections in the complex net-\nwork before the last step of checking for connected ver-\ntices. Following the intuitive reasonings advanced in Sec. 1,\nthe algorithm proposed here tries to reduce the “uncer-\ntainty” generated by triplets of vertices connected by two\nedges. In other words, it tries to reinforce coherence in a\ntriangular sense.\nIfvi,vjandvj,vkare respectively connected, we can\ninduce more coherence by either creating a connection be-\ntween viandvk(i.e. forcing the existence of a triangle),\nor by deleting one of the existing edges. This coherence\ncan be measured through an objective function fOwhich\nconsiders complete and incomplete triangles in the whole\ngraph G. We deﬁne fOas a weighted difference between\nthe number of complete triangles NΘand the number of\nincomplete triangles NΦ(three vertices connected by only\ntwo links) that can be computed from a pair of vertices:\nfO(NΘ,NΦ)=NΘ−αNΦ. The constant α, which weights\nthe penalization for having incomplete triangles in G,i ss e t\nexperimentally.\nThe method starts by building a complex network Gas\ndescribed in the previous section. Then, for each pair of\nvertices vi,vj,t h ev a l u eo f fOis calculated for two situ-\nations: (i) when an edge between viandvjis artiﬁcially\ncreated and (ii) when such an edge is deleted. Then, the\noption which maximizes fOis preserved and the adjacency\nmatrix of Gis updated as necessary. The process of as-\nsigning cover sets is again the connected vertices criterion.\n2.5 Proposed method 3\nWe can substantially reduce the computation time of pro-\nposed method 2 by considering for the computation of fO\nonly those vertices whose connections seem to be uncer-\ntain. If the distance between two songs is extremely high\nor low, this means that the cover song detection system\nhas clearly detected a match or a mismatch. Accordingly,\nwe just consider the pairs of vertices whose edge weight\nis around wTh=1\ndTh+/epsilon1. In particular, for each vertex vi,\na pre-selection of adjacent vertices is performed according\nto a certain weight margin wMa, which we set manually.\nThis way, vjis associated to vifor further processing iff\n|wi,j−wTh|<w Ma,w h e r e |·|indicates absolute value. The\nprocess of building the initial complex network and assign-\ning cover sets is the same as in Sec. 2.3.\n2.6 Modularity optimization\nWe also consider the method in [12], which extracts the\ncommunity structure from large networks based on the op-\ntimization of the network modularity [6, 7, 12]. This algo-\nrithm outperforms all other known community detection\nmethods in terms of computational time while maintain-\ning a high accuracy [12]. We use the implementation by\nAynaud7and we build the initial network as in Sec. 2.3.\n7http://perso.crans.org/ ˜aynaud/communities/\nindex.html\n22610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n3. METHODOLOGY\n3.1 DataFor the generality of our experiments, we employ two\nsources of data: artiﬁcial and real-world symmetric dis-\ntance matrices. Artiﬁcial distances are randomly gener-\nated given a predeﬁned noise level σ\nξ. More concretely,\nthe distance d(si,sj)=d(sj,si)between songs siandsj\nis drawn from a normal distribution N(μ,σ)with mean μ\nand standard deviation σsuch that\nd(si,sj)=⎧\n⎪⎨\n⎪⎩0 ifi=j,\n|N(0,σξ)| ifi>j andsi,sjcovers,\n1−| N(0,σξ)|otherwise.\n(1)\nReal-world distances are provided by a state-of-the-art\ncover song identiﬁcation system [13].\n3.2 Experimental setups\nA cover song music collection can be characterized by cer-\ntain parameters constituting a setup [1]: the total number of\nsongs NS, the number of cover sets NCthe collection in-\ncludes, the cardinality Cof these cover sets, and the num-\nber of added noise songs8NN. Because some setups can\nlead to wrong accuracy estimations [1], it is safer to con-\nsider several of them, including ﬁxed and variable cardi-\nnalities. In our experiments we use the setups summarized\nin Table 1. For real-world data we use an extension of the\nmusic collection described in [13] which comprises a vari-\nety of genres and styles, as well as different types of cov-\ners. The characteristics of this music collection correspond\nto setup 3. For other setups we simply sample cover sets\nfrom setup 3 and repeat the experiments NTtimes (num-\nber of trials, average accuracies reported). We either sam-\nple cover sets with a ﬁxed cardinality ( C=4, the expected\ncardinality of setup 3) or without ﬁxing it (variable cardi-\nnality, C=ν). When using artiﬁcial data, we construct\nthe distance matrix following Eq. (1). As ﬁxed cardinal-\nity we use C=4 as well, and as variable cardinality we\ntakeν∼⌊2+e(1/2)⌋,w h e r e ⌊·⌋denotes ﬂoor value and\ne(1/λ)corresponds to an exponential distribution9with\nrate parameter λ.\n3.3 Evaluation measures\nTo evaluate batch-mode query systems we employ the mean\nof average precisions (MAP) over all queries [3, 14]. The\naverage precision for a query si(APi) is calculated from\nthe retrieved answer Aias AP i=1\nC−1/summationtextNS−1\nr=1Pi(r)Ii(r),\nwhere Piis the precision of the sorted list Aiat rank r,\nPi(r)=1\nr/summationtextr\nl=1Ii(l),a n dIiis a relevance function such\nthatIi(z)=1 if the song with rank zinAiis a cover of si,\nIi(z)=0 otherwise.\n8Bynoise songs we mean songs that do not belong to any cover set\nincluded in the collection.\n9An exponential distribution is used to imitate the distribution of C\nwith setup 3 (see [13]). With λ=2, an expected value /angbracketleftν/angbracketright=4is obtained.Setup Parameters\nNCCN NNSNT\n1.1 25 4 0 100 20\n1.2 25 ν 0 /angbracketleft100/angbracketright20\n1.3 25 4 100 200 20\n1.4 25 ν 100 /angbracketleft200/angbracketright20\n2.1 125 4 0 500 20\n2.2 125 ν 0 /angbracketleft500/angbracketright20\n2.3 125 4 400 900 20\n2.4 125 ν 400 /angbracketleft900/angbracketright20\n3 525 ν 0 2135 1\nTable 1 . Experimental setup summary. The /angbracketleft·/angbracketrightdelimiters\ndenote expected value.\nTo evaluate cover set detection we resort to the classical\nF-measure with even weighting [2,14]: F=2PR\nP+R.H e r e P\nandRcorrespond to precision and recall, respectively. For\nour evaluation, we compute these quantities independently\nfor all songs and average afterwards. More concretely, for\neach song si, we count the number of true positives TP i\n(i.e. the number of cover songs of sibelonging to the the\nsame group10assi), the number of false positives FP i\n(i.e. the number of songs belonging to the same group as\nsithat are not covers of si), the number of false negatives\nFNi(i.e. the number of cover songs of sithat do not be-\nlong to the same group as si), and average Pi=TP i\nTP i+FP i\nandRi=TP i\nTP i+FN iacross all NSsongs11.\n4. RESULTS\n4.1 Artiﬁcial dataWe ﬁrst evaluate the algorithms’ accuracy as a function of\nthe noise level σ\nξintroduced to the artiﬁcial data for se-\ntups 1.1 to 1.4. Before computing the reported accuracies,\nwe independently performed a parameter optimization for\neach algorithm12,σξ, and setup. Then, using the optimal\nparameters found for a given σξ,w ep l o ta v e r a g e Fover\n20 trials versus σξ(Fig. 1). In general, we observe that the\naccuracy for all algorithms starts to drop for σξ>0.2un-\ntil it reaches an F<0.3forσξ>0.5. We also see that the\nK-medoids and single-linkage algorithms are less robust to\nnoise than the others. Low accuracies for the K-medoids\nalgorithm might be explained by the difﬁculty to automat-\nically set the correct K value. Furthermore, cover sets with\nvariable cardinality, such as the ones used for setups 1.2\nand 1.4, might further decrease the accuracy [4]. Low ac-\ncuracies for the single linkage algorithm with respect to\nother hierarchical clustering algorithms conﬁrm the ﬁnd-\nings in the literature [5]. UPGMA, and WPGMA accura-\ncies are slightly higher than other algorithms under noise\nlevels σξ∈[0.2,0.4].\n10Through this subsection by group we mean the cover set detected by\nthe evaluated algorithm.\n11Note that, unlike other clustering evaluation measures [15], Fis not\ncomputed on a per-cluster basis, but on a per-song basis.\n12This parameter optimization was not critical to achieve near-optimal\naccuracies (see Sec. 4.2). We did not consider proposed method 2 at thisstage due to its computational complexity (see Sec. 4.3).\n227Poster Session 2\n00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91\nσξFKM\nSL\nCL\nUPGMA\nWPGMA\nPM1\nPM3\nMO\nFigure 1 . Accuracy under different noise levels for setup\n1.4. Other setups lead to similar curves. Here and in sub-\nsequent tables and ﬁgures KM stands for K-medoids, SL\nfor single-linkage, CL for complete-linkage, UPGMA for\ngroup average linkage, and WPGMA for weighted average\nlinkage, PM for proposed method, and MO for modularity\noptimization.\nAlgorithm Setup\n2.1 2.2 2.3 2.4\nKM 0.517 0.497 0.520 0.500\nSL 0.656 0.690 0.654 0.683\nCL 0.816 0.820 0.814 0.821\nUPGMA 0.895 0.900 0.897 0.902\nWPGMA 0.879 0.889 0.883 0.893\nPM1 0.713 0.723 0.716 0.718\nPM3 0.665 0.698 0.668 0.704\nMO 0.721 0.735 0.716 0.744\nTable 2 . Accuracy ( F) for artiﬁcial data with σξ=0.25.\nTo better assess the algorithms’ performance, we show\nthe accuracies achieved with setups 2.1 to 2.4 under a ﬁxed\nnoise level of σξ=0.25(Table 2). Here, differences be-\ntween accuracies can be better estimated, as we are em-\nploying a bigger music collection and quite a high noise\nlevel. We see that UPGMA and WPGMA deﬁnitely per-\nform best under the speciﬁed σξ.\n4.2 Real-world data\nTo evaluate the algorithms’ accuracy with real-world data\nwe independently optimized all possible parameters for\neach algorithm on setups 1.1 to 1.4. Within this pre-\nanalysis, we saw that the deﬁnition of a distance thresh-\nold13was, in general, the only critical parameter for all\nalgorithms. Apart from this, all other parameters turned\nout not to be critical for obtaining near-optimal accura-\ncies. Methods that had specially broad ranges of these\nnear-optimal accuracies were K-medoids, proposed method\n2, and all considered hierarchical clustering algorithms.\nWe report the accuracies with optimal parameters for\nsetups 2.1 to 3 (Table 3). We see that accuracies for pro-\nposed methods 1 and 3 are comparable to the ones achieved\n13Either cophenetic distances, dTh,o rrTh(see Sec. 2).Algorithm Setup\n2.1 2.2 2.3 2.4 3\nKM 0.637 0.642 0.656 0.666 n.c.\nSL 0.776 0.783 0.833 0.828 0.676\nCL 0.777 0.768 0.860 0.853 0.756\nUPGMA 0.797 0.812 0.865 0.875 0.796\nWPGMA 0.800 0.804 0.866 0.862 0.788\nPM1 0.804 0.813 0.853 0.853 0.751\nPM2 0.761 0.738 n.c. n.c. n.c.\nPM3 0.788 0.790 0.841 0.848 0.733\nMO 0.808 0.809 0.856 0.858 0.762\nTable 3 . Accuracy ( F) for real-world data. Due to algo-\nrithms’ complexity, some results were not computed (de-\nnoted as n.c., see Sec. 4.3).\nby the other algorithms and, in some setups, even better.\nOverall, we corroborate the ﬁndings of the previous sec-\ntion, although differences between algorithms are not so\nlarge now. We hypothesize that these similar (as well as\nnear-optimal) accuracies are due to the relatively good dis-\ntance measure provided by the employed cover song iden-\ntiﬁcation system (we have already seen that these differ-\nences get stressed with artiﬁcial data).\n4.3 Time performance\nIn the application of these techniques to big real world mu-\nsic collections, computational complexity is of great im-\nportance. To qualitatively evaluate this aspect, we report\nthe average amount of time spent by each algorithm to\nachieve a solution for each setup (Fig. 2).\nWe see that K-medoids and proposed method 2 are com-\npletely inadequate for processing collections with more\nthan 2000 songs (e.g. setup 3). The steep rise in the time\nspent by hierarchical clustering algorithms to ﬁnd a clus-\nter solution for setup 3 also raises some doubts as to the\nusefulness of these algorithms for huge music collections.\nFurthermore, the hierarchical clustering algorithms, as well\nas the K-medoids algorithm, take the full pairwise distance\nmatrix as input. Therefore, with a music collection of, say,\n1.1 1.2 1.3 1.4 2.1 2.2 2.3 2.4 3−10123\nSetupTime [log10(sec)]KM\nSL\nCL\nUPGMA\nWPGMA\nPM1\nPM2\nPM3\nMO\nFigure 2 . Average time performance for each considered\nsetup. Algorithms were run with an Intel(R) Pentium(R) 4\nCPU 2.40GHz with 512M RAM.\n22810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n10 million songs, this distance matrix might be difﬁcult to\nhandle. In contrast, algorithms based on complex networks\nsuch as modularity optimization and proposed methods 1\nand 3, only need a single list of answers A. Moreover, the\nlength of the elements of this list can be very small duetod\nThandrThdeﬁned above (e.g. in our tests we found\nunnecessary to consider rTh>3). It should also be noticed\nthat the resulting network is sparse, i.e. the number of links\nis much lower than NS2[10] and, therefore, calculations\non such graphs can be strongly optimized both in mem-\nory requirements and computational costs as demonstrated,\nfor instance, by [12]. Thus, methods based on complex\nnetworks, despite not achieving the highest accuracies for\ncover set detection, represent a robust and scalable option\nfor processing big music collections.\n5. OUTCOMES\n5.1 Accuracy increase\nIn this section we show that the detection of cover song sets\ncan improve the accuracy of batch-mode query systems. In\nparticular, we study the relative MAP increase for the best\ncover set detection algorithms found. For comparison pur-\nposes, we take the output Aof an initial batch-mode query\nsystem and transform it according to the grouping solution\nachieved. More concretely, we divide the distances in each\nAiby the maximum distance value found and add an arbi-\ntrary constant β>1to the songs that are not detected as\nbelonging to the cover set where siis included.\nWe plot the relative MAP increment versus the cardi-\nnality Cof the cover sets for artiﬁcial data in Fig. 3. We\nshow that one can get a MAP accuracy improvement of up\nto 25%. A comparable improvement can also be achieved\nin the case C=ν. In general, it can be seen that the higher\nthe cardinality, the higher the improvement we can get by\ndetecting cover sets. Improvements for real-world data are\nmuch more modest as we do not have many sets with high\nC. With setup 2.4, average improvements are 2.8% for\nUPGMA, 2.1% for WPGMA, 5.1% for proposed method\n1, and 4.9% for modularity optimization.\n2 3 4 5 6 v−5051015202530\nCRelative MAP increase [%]UPGMA\nWPGMA\nPM1\nMO\nFigure 3 . Relative MAP increment with artiﬁcial data. We\nuseσξ=0.25and, except C, the same parameters as in\nsetups 2.3 and 2.4.A further out-of-sample test was done within the\nMIREX 2008 audio cover song identiﬁcation contest,\nwhere we submitted two versions of the same system [9]\nand obtained the two highest accuracies14achieved to date.\nThe ﬁrst version corresponded to the algorithm we use herefor obtaining the real-world data [13]. The second version\ncomprised the same algorithm, plus an additional post-\nprocessing step performing cover set detection based on\nproposed method 1 (the only method we had available at\nthat time) and the maximum distance value normalization\ndescribed in the present section. The MAP achieved with\nthe former was 0.66 while the MAP of the latter was 0.75,\nwhich corresponds to a 13.6% relative increment. This\nincrement is higher than the one achieved here with real-\nworld data most probably because in the MIREX task\nC=10 .\n5.2 Original detection\nIn the clustering context, many applications exploit com-\npact cluster descriptions such as centroids or medoids [4,\n5]. Analogously to cluster centroids and medoids, the cover\nset centroids and medoids can be determined. This way,\nthe centroid of a cover set would correspond to the “aver-\nage realization” and the medoid would correspond to the\n“prototype”\n15of the underlying musical piece. We here\nfocus on the latter and leave the former for future consid-\neration. In general, we could consider this prototype to be\nthe most referential, inﬂuential, or inspirational song in a\ncover set (e.g. the musical piece covered by the majority of\nthe other pieces). We here make an oversimpliﬁcation and\nassume that the medoid of a cover song set corresponds to\nthe original version.\nTo evaluate this option for detecting original songs we\nmanually check for original versions in setup 3 of the used\nreal-world data (we ﬁnd 426 originals out of 525 cover\nsets) and we consider an ideal cover set detection algo-\nrithm (with all cover sets correctly estimated) as well as\nthe best performing algorithms found in previous sections.\nFor these last ones, we discard for evaluation the detected\nsets that do not contain an original.\nTo automatically determine the original song we take all\npossible pairwise distances within songs in a detected set\nand select the one which has a minimal distance sum to the\nother songs in the set. Let ˆS={ˆsj},j=1,...,ˆC,b eas e t\nof detected covers with cardinality ˆC. Then, the index iof\nthe prototype song corresponds to16\ni=a r g m i n\nj⎧\n⎪⎨\n⎪⎩ˆC/summationdisplay\nk=1\nk/negationslash=jd(ˆsj,ˆsk)⎫\n⎪⎬\n⎪⎭. (2)\nThe results in Table 4 show the percentage of hits and\nmisses, which can be compared to the null hypothesis of\nrandomly selecting one song in the set. We observe that,\n14http://www.music-ir.org/mirex/2008/index.php/\nAudio_Cover_Song_Identification_Results\n15Standard, typical, or best example.\n16Notice that analogous formulae can be derived by employing the no-\ntion of betweenness or closeness centrality in a complex network [10,11].\n229Poster Session 2\nAlgorithm C\n2 3456\nIdeal 50.0 51.8∗∗39.2∗36.8∗41.7∗∗\nUPGMA 50.0 58.0∗∗55.0∗∗60.4∗∗50.4∗∗\nWPGMA 50.0 55.9∗∗46.6∗63.2∗∗47.5∗∗\nPM1 50.0 62.7∗∗61.3∗∗70.0∗∗50.0∗∗\nMO 50.0 62.8∗∗61.0∗∗70.0∗∗50.0∗∗\nNull 50.0 33.3 25.0 20.0 16.7\nRef.NC 187 85 51 38 24\nTable 4 . Accuracies (%) for the original song detection\ntask depending on C.T h e∗and∗∗symbols denotes sta-\ntistical signiﬁcance at p<0.05andp<0.01, respectively.\nThe last line shows a referential NCcorresponding to the\nnumber of cover sets obtained with an ideal grouping solu-\ntion.\nin general, accuracies are around 50% with all considered\ncardinalities. This exactly corresponds to the null hypoth-\nesis of sets with C=2but, as soon as C>2, accuracies\nbecome higher than the null hypothesis and statistical sig-\nniﬁcance arises (statistical signiﬁcance is assessed with the\nbinomial test [16]). Discarding cover sets with no original\nsong explains why accuracies for the algorithms studied\nhere become slightly higher than the ones achieved by the\nideal grouping algorithm. We hypothesize that our algo-\nrithms tend to split the ideal cover sets into “more coher-\nent” or compact subsets and, therefore, within these, the\nmethod of Eq. 2 can better determine the original song.\n6. CONCLUSIONS\nIn this paper we propose and study a framework for cover\nsong identiﬁcation based on the notion of cover sets that\nsubsumes the current batch-mode query framework (the\nlatter is naturally incorporated as an important part of the\nformer). Through comprehensive experiments we show\nthat the detection of cover sets is feasible, that it does not\nrequire extensive parameter tuning, and that it is quite ro-\nbust to noisy distance measurements. Furthermore, we\npropose three versions of an unsupervised community de-\ntection algorithm that, when compared to existing state-of-\nthe-art methods, achieve comparable accuracies with sim-\nilar computation time (proposed method 3) or even faster\n(proposed method 1). We evidence that this new frame-\nwork can provide new outcomes and can give raise to new\napplications. In addition to showing that cover set de-\ntection can substantially increase the accuracy (and co-\nherence) of current systems, we here provide a proof-of-\nconcept application to detect the original song within a\ncover set, which is inspired by the notion of cluster\nmedoids.\nFinally, we would like to highlight that, in spite of fo-\ncusing on cover songs, the intuitive reasonings followed\nthoughout the paper could be as well applied to any IR\nbatch-mode query system, and especially to other MIR sys-\ntems (e.g. query-by-humming, query-by-example, audio\nﬁngerprinting, or music similarity).7. ACKNOWLEDGMENTS\nThe authors would like to thank E. G ´omez, M. Haro,\nP. Herrera, R. Marxer, and J. Salamon for useful discus-\nsions. This research has been partially funded by the EU-\nIP project PHAROS17IST-2006-045035.\n8. REFERENCES\n[1] J. Serr `a, E. G ´omez, and P. Herrera. Audio cover song identi-\nﬁcation and similarity: background, approaches, evaluation,and beyond . Springer, 2009. In press.\n[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information\nRetrieval . ACM Press Books, 1999.\n[3] J. S. Downie, M. Bay, A. F. Ehmann, and M. C. Jones. Au-\ndio cover song identiﬁcation: Mirex 2006-2007 results and\nanalyses. Int. Symp. on Music Information Retrieval (ISMIR) ,\npages 468–473, September 2008.\n[4] R. Xu and D. C. Wunsch. Clustering . IEEE Press, 2009.\n[5] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a\nreview. ACM Computing Surveys , 31(3):264–323, September\n1999.\n[6] S. Fortunato and C. Castellano. Community structure in\ngraphs . Springer, Berlin, 2009.\n[7] L. Danon, A. D ´ıaz-Aguilera, J. Duch, and A. Arenas. Com-\nparing community structure identiﬁcation. Journal of Statis-\ntical Mechanics , 9008:09008, 2005.\n[8] A. Egorov and G. Linetsky. Cover song identiﬁcation with if-\nf0 pitch class proﬁles. MIREX extended abstract , September\n2008.\n[9] J. Serr `a, E. G ´omez, and P. Herrera. Improving binary simi-\nlarity and local alignment for cover song detection. MIREX\nextended abstract , September 2008.\n[10] S. Boccaletti, V . Latora, Y . Moreno, M. Chavez, and D. U.\nHwang. Complex networks: structure and dynamics. Physics\nReports , 424:4–5, 2006.\n[11] A. Barrat, M. Barth ´elemy, R. Pastor-Satorras, and\nA. Vespignani. The architecture of complex weighted\nnetworks. Proc. of the National Academy of Sciences ,\n101:3747, 2004.\n[12] V . D. Blondel, J. L. Guillaume, R. Lambiotte, and\nE. Lefebvre. Fast unfolding of communities in large net-\nworks. Journal of Statistical Mechanics , 10:10008, 2008.\n[13] J. Serr `a, X. Serra, and R. G. Andrzejak. Cross recurrence\nquantiﬁcation for cover song identiﬁcation. New Journal of\nPhysics . Under review.\n[14] C. D. Manning, R. Prabhakar, and H. Schutze. An\nintroduction to Information Retrieval . Cambridge Uni-\nversity Press, 2008. Available online: http://www.\ninformationretrieval.org .\n[15] N. Sahoo, J. Callan, R. Krishnan, G. Duncan, and R. Padman.\nIncremental hierarchical clustering of text documents. ACM\nInt. Conf. on Information and Knowledge Management ,\npages 357–366, 2006.\n[16] P. H. Kvam and B. Vidakovic. Nonparametric statistics with\napplications to science and engineering . John Wiley and\nSons, Hoboken, New Jersey, USA, 2007.\n17http://www.pharos-audiovisual-search.eu\n230"
    },
    {
        "title": "Browsing Music Recommendation Networks.",
        "author": [
            "Klaus Seyerlehner",
            "Peter Knees",
            "Dominik Schnitzer",
            "Gerhard Widmer"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416972",
        "url": "https://doi.org/10.5281/zenodo.1416972",
        "ee": "https://zenodo.org/records/1416972/files/SeyerlehnerKSW09.pdf",
        "abstract": "Many music portals offer the possibility to explore music collections via browsing automatically generated music recommendations. In this paper we argue that such music recommender systems can be transformed into an equivalent recommendation graph. We then analyze the recommendation graph of a real-world content-based music recommender systems to find out if users can really explore the underlying song database by following those recommendations. We find that some songs are not recommended at all and are consequently not reachable via browsing. We then take a first attempt to modify a recommendation network in such a way that the resulting network is better suited to explore the respective music space.",
        "zenodo_id": 1416972,
        "dblp_key": "conf/ismir/SeyerlehnerKSW09",
        "keywords": [
            "music portals",
            "music recommendations",
            "recommendation graph",
            "content-based music recommender systems",
            "real-world analysis",
            "song database",
            "exploring music space",
            "recommendation network modification",
            "underlying song database",
            "browsing recommendations"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nBROWSING MUSIC RECOMMENDATION NETWORKS\nKlaus Seyerlehner\nDept. of Computational Perception\nJohannes Kepler University\nLinz, Austria\nklaus.seyerlehner@jku.atPeter Knees\nDept. of Computational Perception\nJohannes Kepler University\nLinz, Austria\npeter.knees@jku.at\nDominik Schnitzer\nAustrian Research Institute for AI\nVienna, Austria\ndominik.schnitzer@jku.atGerhard Widmer\nAustrian Research Institute for AI\nVienna, Austria\ngerhard.widmer@jku.at\nABSTRACT\nMany music portals offer the possibility to explore mu-\nsic collections via browsing automatically generated mu-\nsic recommendations. In this paper we argue that such\nmusic recommender systems can be transformed into an\nequivalent recommendation graph. We then analyze the\nrecommendation graph of a real-world content-based mu-\nsic recommender systems to ﬁnd out if users can really\nexplore the underlying song database by following those\nrecommendations. We ﬁnd that some songs are not rec-\nommended at all and are consequently not reachable via\nbrowsing. We then take a ﬁrst attempt to modify a recom-\nmendation network in such a way that the resulting net-\nwork is better suited to explore the respective music space.\n1. INTRODUCTION\nNow that millions of songs are available for purchase and\ndownload on modern music platforms, developing concepts\nthat help customers to navigate and explore the underly-\ning song database becomes more and more important. A\nstraight forward solution that is used in many commercial\nsettings to assist users in ﬁnding songs in a database is to\nsimply present lists of recommendations. Users are then\nable to explore a collection by moving from recommenda-\ntion to recommendation. Exploring a music collection via\nsuch a sequence of recommendations is called browsing .\nWe believe that browsing will be a key feature of modern\nmusic portals and consequently it is important to view rec-\nommendation not just in terms of individual recommen-\ndation queries only, but also as a continuous process. To\nanalyze recommender systems with respect to their ability\nto support users to browse throughout a music collection,\nwe can view a music recommender as a recommendation\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.network. Recent research work on analyzing music rec-\nommendation networks [1, 2] indicates that many songs in\nsuch a network stay hidden in the so-called Long Tail [3,4].\nOne reason why songs stay hidden in the Long Tail is that\nit is hard to navigate through the network to reach those\nunknown songs. Thus, it seems to be an essential property\nof such a recommendation network that each song can be\nreached via browsing the recommendations. The goal of\nthis paper is to analyze music recommendation networks\nwith respect to their browsability .\nThe rest of this paper is organized as follows: In section\n2 we start with formally deﬁning the general recommen-\ndation scenario. In section 3 we show that under some\nrestrictions any recommender system can be transformed\ninto an equivalent recommendation graph . We then deﬁne\nproperties for a recommendation graph that make such a\ngraph useful for browsing the underlying music database\nand introduce the notion of a browsing graph . In section\n4 an analysis of a recommendation graph of a real world\ncontent-based music recommender system illustrates the\nlimitations of a simple recommender system with respect\nto the reachability of database items. We then propose in\nsection 4.2 an algorithm which effectively modiﬁes a rec-\nommendation graph to overcome these reachability limita-\ntions. Finally, we give an outlook on the application of the\nproposed method and some future work.\n2. RECOMMENDATION SCENARIO\nAlthough many different music recommender systems have\nbeen proposed so far, the fundamental principle is basically\nthe same. Independent of the actual recommendation ap-\nproach we can give a formal model of a recommendation\nscenario for item-based recommendation:\nGiven a set of database items Uof size Nand a speciﬁc\nitemo∈Uthat a user is currently focusing on, a recom-\nmendation is a subset of items R⊂Urelated to o, where\nthe size of the subset Ris far smaller than the total number\nof items in the database. This very simple recommendation\nscenario can be extended by generating a recommendation\nnot only based on the current item obut additionally spec-\n129Poster Session 1\nifying a user proﬁle p∈P, where Pis a set of all user\nproﬁles stored in the recommender system. We call a tu-\npleq= (o, p)arecommendation query and the item set\nR(q)returned by the recommender system the result set or\nrecommendation .\nActual recommender systems then differ in the way the\nrecommendations are generated in this scenario. With re-\nspect to music recommender systems, there seem to exist\nﬁve general recommendation approaches: collaborative ﬁl-\ntering approaches, content-based approaches, web-mining\nbased approaches, expert-based approaches and hybrid ap-\nproaches.\nOur investigations in the next sections are in general\nindependent of the recommendation approach. The only\nrequirement is that the recommender system under inves-\ntigation returns, for any query q(o, p), an ordered set of\nrecommended items of a given length ksuch that the rec-\nommended items are ordered according to a measure of\nrelatedness.\n3. RECOMMENDATION GRAPHS\nAn intuitive way of exploring a music catalog is to pick\nan arbitrary item out of the database and than navigate\nthroughout the database moving from recommendation to\nrecommendation. One important requirement of such a\nbrowsing system is reachability. Reachability essentially\nensures that a user will be able to access all songs in the\ncollection by means of exploration and will not be limited\nto a small subset by the recommender system. To be able\nto show that reachability is ensured for a speciﬁc recom-\nmendation algorithm, we have to establish a formal model\nof the browsing process.\nBased on our deﬁnition of a recommendation scenario\n(see section 2), browsing can be seen as an extension to\nrecommendation from a single query to a consecutive se-\nquence of queries s= (q1, q,2, ..., q N). Two consecutive\nqueries qi= (oi, p)andqi+1= (oi+1, p)within such\na browsing sequence are related by the fact that the item\noi+1of the next recommendation query qi+1is an element\nof the result set of the previous recommendation query qi.\nConsequently, a sequence sof recommendation queries of\nlength Nis a valid browsing sequence in the case that the\nfollowing property is fulﬁlled:\n∀i < N :oi+1∈R(qi) (1)\nTo guarantee this essential reachability property for a\nrecommender system we have to show that starting from a\narbitrary but ﬁxed database item, all other database items\ncan be reached by a ﬁnite sequence of recommendation\nqueries. Formally, reachability starting from an arbitrary\nbut ﬁxed item o1holds if:\n∀o∈U:∃i∈N:∀j < i : (2)\noj+1∈R(qj)∧qj+1= (oj+1, p)∧o∈R(qi)\nBefore we can start drawing any conclusions about reach-\nability, we have to make some additional assumptions aboutthe recommender system. The reason is that for dynamic\nrecommenders, e.g., based on collaborative ﬁltering, where\nthe recommendations may change as a result of system use,\nit is impossible to prove reachability, since we cannot make\nany assumption about future recommendations. Therefore\nwe have to assume a static recommender system where the\nrecommendation will not change over time. It is impor-\ntant to note that this is not in principle a loss in generality;\nit just implies that if there are any changes in the recom-\nmender system then we also have to prove reachability for\nthis new recommendation state.\nFurthermore we constrain our analysis to systems where\nthe recommendation result is independent of the user pro-\nﬁle. This implies that all users get the same recommen-\ndations for one and the same query item. Once more this\nis not in principle a loss in generality as we could handle\nsuch systems by proving reachability for each user sepa-\nrately. In practice, however, analyzing recommender sys-\ntems that generate personalized recommendations seems to\nbe impossible due to the potential enormous computational\ncosts.\nGiven these restrictions, we can now transform every\npossible recommender system into a recommendation net-\nwork orrecommendation graph . A recommendation graph\nis a directed graph G= (V, E), where each vertex in the\ngraph corresponds to a database item. For each item oin\nthe database the corresponding vertex in the graph has a\ndirected edge to all the items in the result set R(q)of the\nrecommendation query q= (o, p). (Note that based on\nour assumptions R(q)does not depend on p, an optionally\ngiven user proﬁle.) To prove reachability for such a recom-\nmendation graph we can for instance apply the depth ﬁrst\nsearch algorithm for each vertex in the graph separately.\nWhile this is not a very practical or fast method to prove\nreachability, in most cases it is quite trivial to disprove\nreachability either by showing that the recommendation\ngraph is not connected, or by identifying a single source .\nA source is a vertex vwhich has no incoming edges, i.e.,\nhas an indegree of zero ( deg−(v) = 0 ). This implies that\nthere is a song in the database that does not occur in the\nresult set of any possible recommendation query and is\nconsequently not reachable at all. Sources are especially\nproblematic with respect to browsing: not only are they\nnot reachable if one starts from some speciﬁc song in the\ndatabase, but they are not reachable from anyother song\nin the database. In section 4 we show, based on empirical\nanalysis of a real world music recommender system, that\nin contrast to what one would expect it is rather likely that\nthere are many sources in a simple recommendation graph.\nIdentifying sources in a graph is a fast operation and can\nbe done in O(n).\nProving and disproving reachability is of course an im-\nportant analysis, however in the likely case that we are able\nto disprove reachability, what can we do about it? How\ncan we ﬁnd a recommendation algorithm that guarantees\nreachability? To put it another way, can we modify a rec-\nommendation graph in such a way that the recommenda-\ntion graph guarantees reachability? In section 4 we will\n13010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nshow that it is quite likely that a recommendation graph\ndoes not fulﬁll the reachability property. We then propose\nan algorithm that transforms a recommendation graph into\nabrowsing graph , a recommendation graph that besides\nreachability has some other properties that we are going to\nintroduce in the next section.\n3.1 Further Requirements and Constraints\nUp to now we have only considered reachability as an im-\nportant property of a recommendation graph. But we can\nderive additional constraints for the recommendation graph\nby analyzing user requirements of browsing systems.\nThe ﬁrst requirement that jumps to the eye is that the\nresult set should be relatively small — ﬁrst of all, because\nthe display space for recommendations is in general lim-\nited on output devices, and secondly, because too large a\nresult set would confuse the user and make for a very un-\nfocused search. Thus it is a natural constraint that the size\nof the result set should not exceed a maximum number of\nrecommendations kmax. For the corresponding recommen-\ndation graph this implies that the outdegree of all vertices\nis less or equal to kmax. We call this property maximum\noutdegree property .\n∀v∈V: deg+(v)≤kmax (3)\nThe second constraint is that if item Bis a recommenda-\ntion for item Athen item Ashould also be a recommenda-\ntion of item B. This corresponds not only to humans’ intu-\nition that similarity relations are symmetric, but also allows\nto easily go back each recommendation step. The symme-\ntry property as deﬁned in (4) implies that the browsing\ngraph is an undirected graph.\n∀e1= (v1, u1)∈E:\n∃e2= (v2, u2)∈E: (4)\nv1=u2∧u1=v2\nFinally, we extend our notion of reachability. Reachability\njust ensures that starting from an arbitrary vertex there is at\nleast a single path to each other vertex. This could make it\nrather difﬁcult to ﬁnd this path. Therefore we require each\nvertex to have a minimum number of incoming edges. For\nthe browsing graph this implies that each vertex has a min-\nimum indegree kminand means that each item is reachable\nby recommendations from at least kminother items. This\nproperty is called minimum indegree property .\n∀v∈V: deg−(v)≥kmin (5)\nAs a result from this requirement analysis we claim that a\nrecommendation graph is better suited for browsing a mu-\nsic archive if these four properties are ensured. We then\ncall such a graph no longer a recommendation graph , but a\nbrowsing graph instead.\nIn the next section we illustrate the limitations of a sim-\nple recommendation graph based on a real world content-\nbased music recommender system and show that in most\ncases such a recommendation graph is not adequate for\nbrowsing. We then introduce a heuristic algorithm that can\ntransform a recommendation graph into a browsing graph.4. BROWSING GRAPHS\n4.1 An Empirical Study\nIn this section we will show that properties like reachabil-\nity are essential and cannot be neglected when designing\na recommender or browsing system. To do so we analyze\na real world content-based music recommender system at-\ntached to the music portal. The FM4 Soundpark1is an\ninternet platform of the Austrian public radio station FM4.\nThis internet platform allows artists to present their music\nfree of any cost in the WWW. All interested parties can\ndownload this music free of any charge. At the moment\nthis music collection contains about 10000 songs and is\nsteadily growing. In our experiments we were allowed to\nuse a subset of 7665 songs out of the whole collection.\nThe recommender system attached to the FM4 Sound-\npark music portal is based on a standard similarity measure\nfor music audio ﬁles. Each song is modeled as a distribu-\ntion of local spectral features, namely Mel Frequency Cep-\nstrum Coefﬁcients (MFCCs). MFCCs are a compact rep-\nresentation of the spectral envelope of a short audio frame\nand are one of the most widespread features used in the\nMusic Information Retrieval (MIR) community. A sin-\ngle multivariate Gaussian distribution is used to model the\ndistribution of MFCCs of a song. Recommendations can\nthen be generated by comparing these distributions. This is\ncommonly done by computing the Kullback-Leibler (KL)\ndivergence [5] or relative entropy between the distributions\nof two songs. For more details on the feature extraction\nprocess and the generation of music recommendations we\nrefer to [6–8]. Using the MIR system of the FM4 Sound-\npark we were able to generate lists of recommended songs\nof a given length k, ordered according to the similarity to\nthe query song, exactly as required by our general scenario\n(see section 2).\nAssuming a ﬁxed sized result set of krecommenda-\ntions for each query, we systematically created all recom-\nmendation graphs for k= 1. . .100, where we denote\nkas the degree of the recommendation graph. For each\nof these graphs we computed the indegree for all vertices\nand counted the number of sources in each graph. Figure\n1 shows that for small result sets the number of sources\nis extremely high. For example, in the recommendation\ngraph of degree 5 there are 2661 sources, which implies\nthat 34.72% of all the songs in the music collection are not\nreachable at all within this graph. By increasing the result\nset size the number of sources decreases, but even for a\nquite large result set of size 20 we still have approximately\n1320 sources. Consequently still 17.22% of the songs in\nthe collection cannot be reached. From ﬁgure 2 we can\nsee how the number of sources scales with the collection\nsize. To simulate different collection sizes songs were ran-\ndomly removed from the collection. Figure 2 illustrates\nthat the problem gets worse for increasing collection sizes.\nIn fact the analysis of the recommendation graph that cor-\nresponds to the online version of the FM4 Soundpark —\nthere are only three recommendations per song — revealed\n1http://fm4.orf.at/soundpark/main\n131Poster Session 1\nFigure 1 .For small result sets, the number of sources is\nextremely large and decreases with an increasing number\nof recommendations per query, whereas the maximum in-\ndegree over all vertices in each graph increases. For a\nresult set size of 100, there is one song that appears in the\nrecommendation list of 2628 other songs, or in 34.29% of\nall recommendation lists.\nthat only 56,79% of all songs are reachable by recommen-\ndations, the remaining 43,21% of the songs are sources and\nare never recommended.\nIn addition to the number of sources, we also computed\nthe maximum indegree over all vertices in each graph, visi-\nble in ﬁgure 1. Obviously, while some songs are not reach-\nable at all, some others are directly reachable from very\nmany songs. However it is of course quite implausible\nthat a single song is similar to several hundred other songs.\nSongs that have a very high indegree, but do not share any\nperceptual similarity with the referring songs are called\nhub-songs according to [9]. In our case the hub problem\nseems to be related to the content-based audio similarity\nmeasure itself. Interestingly, hubs naturally appear in so-\ncial networks (including collaboration networks) as well\n[10]. Regardless of the reasons for hubs and sources, both\nessentially reduce the usability of music recommender sys-\ntems to explore the music spaces. In the following we pro-\npose a heuristic algorithm that transforms a recommenda-\ntion graph into a browsing graph that fulﬁlls the properties\nintroduced in section 3.\n4.2 Constructing a Browsing Graph\nThe main idea behind our approach is to transform a rec-\nommendation graph into a browsing graph, simply by re-\nplacing all directed edges by undirected edges and then it-\neratively (and heuristically) removing edges from the re-\nsulting graph such that the maximum outdegree and the\nminimum indegree property are satisﬁed for all vertices.\nThe symmetry property is automatically ensured because\nthe graph is undirected. Furthermore, reachability is guar-\nFigure 2 .The number of sources in a recommendation\ngraph scales with the number of items in a database. Fur-\nthermore the number of sources depends on the number\nof recommendations for each query. This is illustrated for\nﬁxed result set sizes of k= 5,10,15,20,25,30.\nanteed if the resulting graph is connected.\nThe proposed algorithm has three important parameters.\nThere is the minimum indegree kminand the maximum\noutdegree kmax, which directly result from the required\nproperties. It is easy to see that in combination with the\nsymmetry property this implies that each vertex in the ﬁ-\nnal browsing graph will have to have an edge degree be-\ntween kminandkmax. The proposed algorithm starts from\nthe directed version of the recommendation graph. One\ncould of course start the algorithm from a recommenda-\ntion graph with outdegree kmax, but since we want to give\nour algorithm additional ﬂexibility during the process of\nremoving edges, it is required that the original recommen-\ndation graph has an outdegree of at least kstart for all ver-\ntices. This simply means that for each item we can gener-\nate at least kstart recommendations and is in line with the\nrequirement on recommender systems in section 2. The\nthree parameters are related to each other as stated in (6).\nkmin< kmax< kstart (6)\nThe only thing left to do is to remove edges till each ver-\ntex has a degree in between kminandkmax. This should\nbe done in such a way that each vertex tries to remove its\n‘weakest’ links (i.e., those with the lowest degree of relat-\nedness), since the recommendations should be as good as\npossible. This can be done as follows:\n1. Put all vertices into a priority queue q, where all\nvertices are sorted according to their degree deg(v);\nbreak ties among same-degree nodes randomly;\n2. Pop the vertex with the highest degree from the queue.\n3. If this vertex already has a degree smaller than or\n13210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nequal to kmax, then all vertices in the queue have a\ndegree smaller or equal to kmax. We are done.\n4. As the current vertex has too many edges, remove an\nedge that connects this vertex to another vertex hav-\ning a degree greater than kmin. Choose the edge to\nremove according to the indegree of the neighboring\nvertices. Remove the edge connecting to the ver-\ntex with the highest indegree and if there are several\nvertices of the same indegree remove the vertex with\nthe weakest (lowest similarity) edge. If this vertex\nis not connected to any other vertex having a degree\ngreater than kmin, then we are not able to ensure the\nmaximum indegree property for this node. Stop in\nthis case.\n5. Since we have removed an edge, the indegrees of the\ntwo vertices connected by the edge have changed.\nRemove them from the queue and reinsert them such\nthat the queue is up to date.\n6. Go back to step 2.\nOf course it is true that this algorithm might ﬁnd a so-\nlution where individual vertices have an edge degree higher\nthankmax, violating the maximum outdegree property. This\ncan be due to the fact that for given constraints there sim-\nply does not exist any solution. In such a case weakening\nthe constraint till enough solutions to the problem exist can\nhelp. If there are enough solutions, simply rerunning the\nalgorithm might help. Vertices of the same edge count are\ninserted into the priority queue in random order. There-\nfore the algorithm might ﬁnd other solutions. However our\nexperiments indicate that it is quite easy to ﬁnd a valid\nsolution. Furthermore, the proposed algorithm does not\nguarantee that the resulting graph is connected, but in all\nour conducted experiments the resulting browsing graph\nturned out to be connected.\n4.2.1 Time Complexity\nOne major advantage of this algorithm is that it is of time\ncomplexity O(nlog(n)). At most n(kstart−kmin)edges\nhave to be removed. Therefore we have to perform a maxi-\nmum of 3n(kstart−kmin)removal or insertion operations\non the sorted priority queue. Sorting and removing ele-\nments from a priority queue can be done in O(log(n)),\ne.g., by using a balanced red-black tree. Therefore remov-\ning all the additional edges from the graph can be done in\nO(nlog(n)). The initial insertion operation of all elements\nin the priority queue is also of complexity O(nlog(n)).\nThus, the overall complexity of this algorithm is O(nlog(n)).\n4.2.2 Validation of the Transformation Algorithm\nTo validate the proposed algorithm we analyzed the re-\nsult after the transformation of the FM4 Soundpark into\na browsing graph. The parameters used to transform the\ngraph were kmin= 4,kmax= 7 andkstart = 9. As we\ndo not have yet statistics of the usage before and after the\ntransformation, we follow the standard procedure in MIRresearch and evaluate transformation algorithm in an indi-\nrect way, via a music genre analysis. For all query songs\nqwe count the number of songs in the result set R(q)that\nhave the same genre as the query song and compute the\noverall percentage relative to the number of recommended\nsongs. That way we measure the accuracy of the recom-\nmendations independent of the number of the recommen-\ndations. The accuracy of the recommendations using result\nsets of length k= 5 was35.39%, for k= 6 was34.86%\nand for k= 7 was34.32%. After the transformation us-\ning the above parameters the accuracy was 35.63% with\nan average degree of 5.918per vertex. This preliminary\nresult indicates that there is only a marginal change in rec-\nommendation quality, however a more detailed empirical\nstudy will be done in future. Furthermore to evaluate how\nFigure 3 .The average percentage of songs that can be\nreached by browsing sequences of different length. Before\nthe transformation (for k= 5,6,7) and after the tranfor-\nmation.\nthe reachability of songs has changed we investigated how\nmany songs can be reached in average by a recommenda-\ntion sequence of length l. To do so we computed for each\nsong the number of songs that can be reached by such a se-\nquence. This can be done by traversing the recommenda-\ntion graph using the breadth-ﬁrst search (BFS) algorithm\nup to a maximum depth of l. We then take the average over\nall songs to get a quality indicator for the whole network.\nAs one can see from ﬁgure 3 after the transformation more\nsongs can be reached when browsing the resulting graph\nthan before.\n5. APPLICATION AND FUTURE WORK\nBased on the graph-theoretic studies performed on the FM4\nSoundpark Recommender, we are now investigating ways\nof turning the Soundpark into a Browsing Graph. Given\nthe purpose of the system – to make new music artists\nknown to a wide public – reachability of as many artists\n(or works) as possible would be a prime feature. This is\nnot quite straightforward and will involve some interesting\nresearch questions. Several aspects have to be addressed:\nRecommendation quality: Clearly, the quality of the rec-\nommendations changes as a recommendation graph\n133Poster Session 1\n(which is based on content-based similarity relations)\nis transformed into a browsing graph (which sacri-\nﬁces certain recommendation links in order to sat-\nisfy the browsing constraints). Whether or not that\nunduly degrades the quality of the recommendation\nservice can only be studied empirically. We will ad-\ndress this issue by means of a large-scale user study,\nwhich is yet to be designed (see below).\nIncremental updates: The FM4 music database grows on\na daily basis. Every day, dozens of new songs, mostly\nby new artists, are added to the database and inte-\ngrated into the recommender system in nightly batch\nupdate sessions. Thus, the browsing graph transfor-\nmation will also have to be run at regular intervals.\nAs an alternative, we will look into the possibility of\nincremental update algorithms for browsing graphs.\nTime-varying recommendations: A speciﬁc aspect of the\ngrowing database is that the system’s recommenda-\ntions may change from day to day. That is, if the user\nselects the same seed song on two consecutive days,\nshe may get different recommendations of songs that\nare supposedly ‘similar’. This may be a problem in\ncertain applications, but perhaps not in the case of\nthe Soundpark. Soundpark users have been taught\nto regard the recommendation service as a means\nto explore the Soundpark and ﬁnd new things that\nthey would not otherwise ﬁnd. From the user feed-\nback we currently have, we can conclude that many\nof the users are quite open-minded about occasional\n‘strange’ recommendations, regarding them as ‘in-\nteresting’ or ‘funny’ ideas by the computer, rather\nthan annoying mistakes. Thus, they might ﬁnd time-\nvarying recommendations (if they ever notice them)\nto be enriching rather than irritating.\nModiﬁcations to the Soundpark recommender system\nwill be accompanied with a large scale user study . We have\naccess to two kinds of user feedback: the browsing ses-\nsions themselves (click data) as logged by the Soundpark\nserver, and an on-line user forum, where users discuss their\nimpressions of the system (among other things). Questions\nto be studied include, e.g., whether improved reachability\nconditions really increase the number of artists that are lis-\ntened to by users; whether and how one can quantify dif-\nferences in recommendation quality between recommen-\ndation and browsing graphs; and general aspects of user\nbrowsing behaviour that may help in designing better rec-\nommenders in the future (for instance: how long is a typ-\nical browsing sequence? do users follow more than one\nrecommendation in a given recommendation list? etc.).\nIn this way, the FM4 Soundpark may then become one\nof the ﬁrst real-world music recommendation system that\nis (a) purely content-based, that is, based on musical simi-\nlarity as estimated by the system itself, and (b) speciﬁcally\ndesigned to maximize the percentage of music items that\ncan be found via similarity-based browsing.6. CONCLUSIONS\nIn this paper we have shown that designing music recom-\nmender systems is not as straight forward as it seems. Es-\npecially reachability is an important property if a music\nrecommendation system should also allow users to explore\na music archive via browsing. A bad system design might\nhave the consequence that a portion of all songs in the\ndatabase cannot be discovered as they are not accessible at\nall. To overcome these limitations we took a ﬁrst attempt to\nmodify the graph representation of a recommender system\nin such a way that browsing the resulting recommendation\nnetwork is more convenient. We believe that improving\nthe accessibility of songs in a music archives can signiﬁ-\ncantly increase the usability of music services and might\neven help to alleviate the long tail phenomenon by ensur-\ning the accessibility of ’niche’ products.\n7. ACKNOWLEDGEMENTS\nThis research was supported by the Austrian Research Fund\n(FWF) under grant L511-N15, and by the Austrian Re-\nsearch Promotion Agency (FFG) - project number 815474.\n8. REFERENCES\n[1] O. Celma and P. Cano. From hits to niches? or how\npopular artists can bias music recommendation and\ndiscovery. In 2nd Workshop on Large-Scale Recom-\nmender Systems (ACM KDD) , Las Vegas, USA, 2008.\n[2] O. Celma and P. Herrera. A new approach to evaluating\nnovel recommendations. In 2008 ACM Conference on\nRecommender Systems , Lausanne, Switzerland, 2008.\n[3] C. Anderson. The Long Tail: Why the Future of Busi-\nness Is Selling Less of More . Hyperion, July 2006.\n[4] E. Brynjolfsson, Y . J. Hu, and M.l D. Smith. From\nniches to riches: Anatomy of the long tail. Sloan Man-\nagement Review , 47(4), 2006.\n[5] S. Kullback and R. A. Leibler. On information and suf-\nﬁciency. Annals of Mathematical Statistics , 1951.\n[6] M. Levy and M. Sandler. Lightweight measures for\ntimbral similarity of musical audio. In Proc. of the 1st\nACM Workshop on Audio and Music Computing Mul-\ntimedia , Santa Barbara, USA, 2006.\n[7] M. Mandel and D. Ellis. Song-level features and svms\nfor music classiﬁcation. In Proc. of the 6th Int. Conf.\non Music Information Retrieval , September 2005.\n[8] A. Flexer, D. Schnitzer, M. Gasser, and G. Widmer.\nPlaylist generation using start and end songs. In Proc.\nInt. Sym. on Music Information Retrieval , 2008.\n[9] J.-J. Aucouturier and F. Pachet. A scale-free distribu-\ntion of false positives for a large class of audio similar-\nity measures. Pattern Recogn. , 41(1):272–284, 2008.\n[10] A.-L. Barabsi R. Albert. Statistical mechanics of com-\nplex networks. Review of Modern Physics , 2002.\n134"
    },
    {
        "title": "A Periodicity-based Theory for Harmony Perception and Scales.",
        "author": [
            "Frieder Stolzenburg"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414904",
        "url": "https://doi.org/10.5281/zenodo.1414904",
        "ee": "https://zenodo.org/records/1414904/files/Stolzenburg09.pdf",
        "abstract": "Empirical results demonstrate, that human subjects rate harmonies, e.g. major and minor triads, differently with respect to their sonority. These judgements of listeners have a strong psychophysical basis. Therefore, harmony perception often is explained by the notions of dissonance and tension, computing the consonance of one or two intervals. In this paper, a theory on harmony perception based on the notion of periodicity is introduced. Mathematically, periodicity is derivable from the frequency ratios of the tones in the chord with respect to its lowest tone. The used ratios can be computed by continued fraction expansion and are psychophysically motivated by the just noticeable differences in pitch perception. The theoretical results presented here correlate well to experimental results and also explain the origin of complex chords and common musical scales.",
        "zenodo_id": 1414904,
        "dblp_key": "conf/ismir/Stolzenburg09",
        "keywords": [
            "harmonies",
            "sonority",
            "psychophysical basis",
            "dissonance",
            "tension",
            "consonance",
            "periodicity",
            "frequency ratios",
            "continued fraction expansion",
            "pitch perception"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAPERIODICITY-BASED THEORY FORHARMONYPERCEPTION AND\nSCALES\nFriederStolzenburg\nHochschule Harz,Automation &Computer SciencesDepartment , 38855 Wernigerode, GERMANY\nfstolzenburg@hs-harz.de\nABSTRACT\nEmpirical results demonstrate, that human subjects rate\nharmonies,e.g.majorandminortriads,differentlywithre -\nspect to their sonority. These judgements of listeners have\nastrongpsychophysicalbasis.Therefore,harmonypercep-\ntion often is explained by the notions of dissonance and\ntension,computingtheconsonanceofoneortwointervals.\nInthispaper,atheoryonharmonyperceptionbasedonthe\nnotion of periodicity is introduced. Mathematically, peri -\nodicity is derivable from the frequency ratios of the tones\ninthechordwithrespecttoitslowesttone.Theusedratios\ncan be computed by continued fraction expansion and are\npsychophysically motivated by the just noticeable differ-\nencesinpitchperception.Thetheoreticalresultspresent ed\nherecorrelatewelltoexperimentalresultsandalsoexplai n\nthe originof complex chords and common musical scales.\n1. INTRODUCTION\n1.1 Motivation\nMusic perception and composition seem to be inﬂuenced\nnot only by convention or culture, manifested by musical\nstylesorcomposers,butalsobythepsychophysics oftone\nperception [1–3]. Thus, in order to better understand the\nprocessofmusicalcreativityandinformationretrieval,t he\nfollowing questions should be addressed:\n•What are underlying (psychophysical) principles of\nmusicperception?\n•Howcantheperceivedsonorityofchordsandscales,\ninparticular of westernmusic, be explained?\nTherefore, in the rest of this section (Sect. 1), we will\nintroduce basic musical notions and results. After that, we\nwillbrieﬂyreviewexistingpsychophysicaltheoriesonhar -\nmony perception (Sect. 2), which are often based on the\nnotions dissonance and tension, taking harmonic overtone\nspectra into account. In contrast to this, the approach pre-\nsented here (Sect. 3) is simply based on the periodicity of\nchords. Applying this theory to common musical chords\nand also scales (Sect. 4), shows a very good correlation to\nempirical results, that e.g. most subjects prefer major to\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbear this noticeand thefull citation ontheﬁrst page.\nc/circlecopyrt2009International Society forMusic InformationRetrieval .minor chords. Finally, we will highlight the psychophys-\nical basis of the proposed approach, by reviewing some\nrecent results from neuro-science on periodicity detectio n\nof thebrain, and end upwithconclusions (Sect. 5).\n1.2 BasicMusical Notions\nBeforeweareabletoaddresstheproblemofharmonyper-\nception,weshouldclarifytheterminologyweuse.Forthis,\nwefollowthelinesof[2].Thebasicentitywehavetodeal\nwithisatone:Apuretoneisatonewithasinusoidalwave-\nform.Ithasaspeciﬁcpitch,correspondingtoitsperceived\nfrequency f , usually measured in Hertz (Hz), i.e. periods\nper second. In practice, pure tones almost never appear.\nThe tones produced by real instruments like strings, tubes,\northehumanvoicehaveharmonicorother overtones .The\nfrequenciesofharmonicovertonesareintegermultiplesof\na fundamental frequency f. For the frequency of the n-th\novertone ( n≥1), it holds fn=n·f, i.e.f1=f. The am-\nplitudes of the overtones deﬁne the spectrum of a tone or\nsound and account for itsloudness and speciﬁc timbre.\nAharmony in an abstract sense can be identiﬁed by\na set of tones forming an interval, chord, or scale. Two\ntones deﬁne an interval, which is the distance between\ntwo pitch categories. The most prominent interval is the\noctave, corresponding to a frequency ratio of 2 /1. Since\nthesamenamesareassignedtonotesanoctaveapart,they\nare assumed to be octave equivalent . An octave is usually\ndivided into 12 semitones in western music, correspond-\ning to a frequency ratio of12√\n2 in equal temperament (cf.\nSect.3.3).Thus,intervalsmayalsobedeﬁnedbythenum-\nber of semitones between two tones. A chordis a com-\nplexmusicalsoundcomprisingthreeormoresimultaneous\ntones, while a scaleis a set of musical notes, whose cor-\nrespondingtonesusuallysoundconsecutively.Bothcanbe\nidentiﬁed by the numbers of semitones intheharmony.\nAtriadis a chord consisting of three tones. Classical\ntriads are built from major and minor thirds, i.e., the dis-\ntance between successive pairs of tones are 3 or 4 semi-\ntones. For example, the major triad consists of the semi-\ntones {0,4,7}, whichisthe rootposition ofthischord. An\ninversion of a chord is obtained by transposing the cur-\nrently lowest tone by an octave. Fig. 1 (a) shows the three\ninversions of the E major chord, including the root posi-\ntion. Fig. 1 (b)–(e) shows all triads that can be build from\nthirds including their inversion, always with e′as lowest\ntone. Fig. 1 (f) shows the suspended chord, built from per-\nfect fourths (5 semitones). Its last inversion, consisting of\nthesemitones {0,5,10}, reveals this.\n87Poster Session 1\nG\n(a) triads¯4¯¯4¯¯¯¯¯4¯\n(b) major¯4¯¯¯6¯¯¯¯4¯\n(c)minor¯¯¯¯4¯4¯¯¯6¯\n(d)diminished¯¯2¯¯¯2¯¯2¯2¯\n(e) augmented¯4¯4¯\n(f)suspended¯¯¯¯4¯¯¯¯¯\nFigure 1. Triads and their inversions.\n2. THEORIES ON HARMONY PERCEPTION\nChord classes lead to different musical modes. The ma-\njor chord is often associated with emotional terms like\nhappy,strong, orbright, and, in contrast to this, the minor\nchordwithtermslike sad,weak,ordark.Empiricalresults\n(see e.g. [4])reveal a preference ordering on the perceived\nsonority of the triads as follows: major ≺minor ≺dimin-\nished ≺augmented. Since all these triads are built from\nthirds, thirds do not provide an explanation of this pref-\nerence ordering on its own. Therefore, let us now review\nexisting theories on harmony perception, discussing some\nof their meritsand drawbacks.\n2.1 Explanation byOvertones\nOvertones can explain the origin of the major triad and\nhence its high perceived sonority. The major triad appears\nearly in the sequence, namely overtones 4, 5, 6 (root posi-\ntion) and —even earlier— 3, 4, 5 (second inversion). But\nitiswell-known,thatovertonesfailtoexplaintheorigino f\nthe minor chord.\n2.2 Dissonance and Tension\nSincetheoriginofharmonyandscalescannotbeexplained\nwell by overtones, newer explanations base upon the no-\ntionsofdissonance[2,5]andtension[6].Ingeneral, disso-\nnanceis the opposite to consonance, meaning how well\ntones sound together. Although this approach correlates\nbetter to the empirical results on harmony perception, it\ndoes not explain the low perceived sonority of the dimin-\nished or the augmented triad, which are built from two\nminor or major thirds, respectively. Therefore, [6] adopts\nthe argument from psychology that neighboring intervals\nofequivalentsizeareinstableandproduceasenseoftonal\ntension, that is resolved by pitch changes leading to un-\nequal intervals. Since lowering any tone in an augmented\ntriadbyonesemitoneleadstoamajortriadandraisingtoa\nminor triad, [6] assumes sound symbolism, where the ma-\njor triad is associated with social strength and the minor\ntriad with social weakness. But on the contrary, a minor\ntriad becomes a major triad by raising the third. In ad-\ndition, it is unclear whether suspended triads, built from\ntwoperfectfourths,alsohavealowperceivedsonority.Fi-\nnally, most of the empirical experiments on harmony per-\nception present only single chords to the tested subjects.\nThis means, thereisactually nopitch movement at all.\n3. A PERIODICITY-BASED THEORY\nThe approaches discussed so far more or less take the fre-\nquency spectrum of a sound as their starting point. Obvi-(a)\n(b)\n(c)\n(d)\nFigure 2. Sinusoids of themajor triad.\nously, analyzing the frequency spectrum is closely related\nto analyzing the time domain (periodicity). Fourier trans-\nformationallowstotranslatebetweenbothmathematically .\nHowever, subjective pitch detection, i.e., the capability of\nour auditory system to identify the repetition rate (peri-\nodicity) of a complex tone sensation, only works for the\nlowerbutmusicallyimportantfrequencyrangeuptoabout\n1.500Hz [3]. In consequence, a missing fundamental tone\ncan be assigned toeach interval. The tone withthe respec-\ntive frequency, called virtual pitch of the interval, is not\npresentasanoriginaltonecomponent.Ithasnothingtodo\nwith (ﬁrst-order) beats and is perceived not directly in the\near,but inthebrain.\n3.1 Periodicity Pitch ofChords\nFor intervals, i.e. two tones, the concept of virtual pitch\nhas been studied many times in the literature (see [3] and\nreferences therein). The idea in this paper now is to trans-\nfer this concept to chords by considering relative peri-\nodicity, i.e. the period length of complex sinusoids rel-\native to the period length of the frequency of the low-\nest tone component (cf. [7, Sect. 7.1]). For example, the\nAmajor triad in just intonation consists of three tones\nwith(absolute)frequencies f1=440Hz,f2=550Hz,and\nf3=660Hz. The respective frequency ratios wrt. the low-\nest tone (a′) areF1=1/1,F2=5/4 (third), and F3=3/2\n(ﬁfth),correspondingtothesemitones {0,4,7}.Fig.2(a)–\n(c) show the sinusoids for the three pure tone components\nandFig.2(d)theirsuperposition,i.e.thegraphofthefunc -\ntion sin (ω1t) +sin(ω2t) +sin(ω3t), where ωi=2πfiare\ntherespective angular frequencies, and tisthe time.\nAs one can see, the period length of the chord is (only)\n8810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nfour times the period length of the lowest tone for this ex-\nample. In the following, we call this ratio h. It depends on\nthefrequencyratios {a1/b1,...,ak/bk}ofthegivenchord.\nWe assume, that each frequency ratio Fiis a fraction ai/bi\n(in its lowest terms), because otherwise no ﬁnite period\nlength can be found in general, and it holds Fi≈fi/f1for\n1≤i≤k.Thismeans,allfrequenciesarerelativizedtothe\nlowest frequency f1, andF1=1. The value of hthen can\nbecomputedas lcm(b1,...,bk),i.e.,itisthe leastcommon\nmultiple(lcm)ofthedenominatorsofthefrequencyratios.\nThiscanbeseenasfollows:Sincetherelativeperiodlength\nofthelowesttone T1=1/F1is1,wehavetoﬁndthesmall-\nestintegernumberthatisanintegermultipleofallrelativ e\nperiod lengths Ti=1/Fi=bi/aifor 1<i≤k. Since after\naiperiodsofthe i-thtone,wearriveattheinteger bi,hcan\nbe computed as the leastcommon multipleof all bi.\n3.2 A Hypothesis on Harmony Perception\nWe now set up the following hypothesis on harmony per-\nception: The perceived sonority of a chord, called har-\nmonicityin this context, decreases with the value of h. For\nthe major triad in root position we have h=4 (see above),\nwhichisquitelow.Therefore,itspredictedsonorityishig h.\nThiscorrelateswelltotheempiricalresults,ingeneralbe t-\nter than the approaches discussed in the previous section\n(Sect.2),aswewillseelateron(inSect.4).Inaddition,th e\nperiodicity-basedtheorypresentedhereiscomputational ly\nsimple, because it needs no assumptions on parameters,\nsuch as harmonic overtone spectra. Neither complex sum-\nmation nor computing local extrema is required. Only the\nfrequency ratios of the tone components in the chord are\nneeded as input parameters. But we still have to answer\nthe question, which frequency ratios should be used in the\ncomputation of h. Since this is done in a special way here,\nwe present this now inmoredetail.\n3.3 Tuning and Frequency Ratios\nThe frequencies for the k-th semitone in equal tempera-\nmentwith twelve tones per octave can be computed as\nfk=12√\n2k·f1,wheref1isthefrequencyofthelowesttone.\nThe respective frequency ratios are shown in Tab. 1 (a).\nThe values grow exponentially and not linearly, following\ntheWeber-Fechner law in psychophysics, which says that,\nif the physical magnitude of stimuli grows exponentially,\nthen the perceived intensity grows only linearly. In equal\ntemperament, all keys sound equal. This is essential for\nplaying in different keys on one instrument and for mod-\nulation, i.e. changing from one key to another within one\npiece of music. Since this seems to be universal, at least\nin western music, we will adopt the equal temperament as\nreference system forother tunings.\nThe frequency ratios in equal temperament are irra-\ntional numbers (except for the ground tone and its oc-\ntaves),butforperiodicitydetectiontheymustbefraction s,\nas mentioned above. Let us thus consider other tunings\nwith rational frequency ratios. The oldest tuning with this\nproperty is probably the Pythagorean tuning , shown in\nTab. 1 (b). Here, frequency relationships of all intervalshave the form 3m/2nfor some integers mandn, i.e., they\nare based on ﬁfths, strictly speaking, a stack of perfect\nﬁfths (frequency ratio 3 /2), applying octave equivalence.\nHowever,althoughhugenumbersappearinthenumerators\nand denominators of the fractions in Pythagorean tuning,\nthe relative errors compared to equal temperament (shown\ninbrackets inTab. 1)grow up tomorethan 1%.\nIn fact, the Pythagorean tuning does not follow results\nof psychophysics, namely that human subjects can dis-\ntinguish frequency differences for pure tone components\nonly up to a certain resolution, namely 0 .5% under opti-\nmalconditions.Forthemusicallyimportantlowfrequency\nrange, especially the tones in (accompanying) chords, this\nso-called just noticeable difference is worse, namely only\nbelow about 1% [3]. Therefore, we should look for tun-\nings, where the relative error is approximately 1%. In ad-\ndition,thefrequencyratiosshouldbesimpleintegerratio s,\ni.e. fractions with small numerators and denominators. In\norder to achieve the latter, we can look in the harmonic\novertone sequence,when a tone of the chromatic scale ap-\npears for the ﬁrst time, applying again octave equivalence.\nThe result of this procedure, which we will call overtonal\ntuning,leadstofrequencyratiosoftheform m/2nforsome\nintegersmandnas shown in Tab. 1 (c). However, as one\ncan see, the relative error compared to equal temperament\nagain issometimes high.\nIn the literature (see e.g. [5] and references therein),\nother historical and modern tunings are listed, e.g. Kirn-\nbergerIII , see Tab.1 (d). However, they are also only par-\ntially useful in this context, because they do not take into\naccount the fact on just noticeable differences explicitly .\nIn principle, this also holds for the adaptive tunings in [5] ,\nwheresimpleintegerratiosareusedandscalesareallowed\ntovary.Anadaptivetuningcanbeviewedasageneralized\ndynamic just intonation , which ﬁts well to musical prac-\ntice, because the frequencies for one and the same pitch\ncategory may vary signiﬁcantly during the performance of\na piece of music. Trained musicians try to intonate e.g. a\nperfect ﬁfth with the frequency ratio 3 /2, and listeners are\nhardly able to distinguish this frequency ratio from others\nthat are close to the value in equal temperament, namely\n12√\n27≈1.498. In consequence, also the rational tuning ,\nwhich we introduce now, primarily should not be consid-\nered as a tuning, but more as the basis for intonation and\nperception of intervals. We will use the frequency ratios\nof the rational tuning, shown in Tab. 1 (e), in our analyses\nof harmonicity. They are fractions with smallest possible\ndenominator,suchthattherelativeerrorwrt.equaltemper -\nament is just below 1%. They can be computed by means\nofFarey sequences , i.e. ordered sequences of completely\nreduced fractions between 0 and 1 which have denomina-\ntors less than or equal to some (small) n, or by continued\nfractionexpansion.\n3.4 Continued Fraction Expansion\nInmathematics,a(regular) continuedfraction isanexpres-\nsion as shown in Fig. 3 (a), where the ciare integer num-\nbers that must be positive for i>0. For a given rational or\n89Poster Session 1\ninterval k(a)equal temperament (b)Pythagorean (c) overtonal (d) Kir nberger III (e) rational\nprime, unison 0 1.000 1/1 (0.00%) 1/1 (0.00%) 1/1 (0.00%) 1/1 (0.00%)\nminor second 1 1.059 37/211(0.79%) 17/16 (0.29%) 25/24 (–1.68%) 16/15 (0.68%)\nmajor second 2 1.122 9/8 (0.23%) 9/8 (0.23%) 9/8 (0.23%) 9/8 ( 0.23%)\nminor third 3 1.189 39/214(1.02%) 19/16 (–0.14%) 6/5 (0.91%) 6/5 (0.91%)\nmajor third 4 1.260 81/64 (0.45%) 5/4 (–0.79%) 5/4 (–0.79%) 5 /4 (–0.79%)\nperfect fourth 5 1.335 311/217(1.25%) 21/16 (–1.67%) 4/3 (–0.11%) 4/3 (–0.11%)\ntritone 6 1.414 36/29(0.68%) 23/16 (1.65%) 45/32 (–0.56%) 17/12 (0.17%)\nperfect ﬁfth 7 1.498 3/2 (0.11%) 3/2 (0.11%) 3/2 (0.11%) 3/2 ( 0.11%)\nminor sixth 8 1.587 38/212(0.91%) 25/16 (–1.57%) 25/16 (–1.57%) 8/5 (0.79%)\nmajor sixth 9 1.682 27/16 (0.34%) 27/16 (0.34%) 5/3 (–0.90%) 5/3 (–0.90%)\nminor seventh 10 1.782 310/215(1.14%) 7/4 (–1.78%) 16/9 (–0.23%) 16/9 (–0.23%)\nmajor seventh 11 1.888 243/128 (0.57%) 15/8 (–0.68%) 15/8 (– 0.68%) 15/8 (–0.68%)\noctave 12 2.000 2/1 (0.00%) 2/1 (0.00%) 2/1 (0.00%) 2/1 (0.00 %)\nTable 1. Table of relative frequencies fordifferent tunings.\n(a)x≈c0+1\nc1+1\nc2+1\nc3+1\n...\n(b)c0=⌊x⌋cn=⌊1/xn−1⌋\nx0=x−c0xn=1/xn−1−cn\n(c)a−1=1a0=c0an+1=an−1+cn+1an\nb−1=0b0=1bn+1=bn−1+cn+1bn\nFigure 3. Continued fractions and Euclidean algorithm.\nreal number x, the values cican be computed recursively\nbythe(extended) Euclideanalgorithm ,statedinFig.3(b),\nwhere the ﬂoor function ⌊x⌋is used, which yields the\nlargest integer less than or equal to x. The sequence of the\nciinducesasequenceoffractions ai/bi,calledconvergents\nor fraction expansion of x, which can be computed by the\nequations in Fig. 3 (c). Continued fractions obey many in-\nterestingproperties (see[8]),for instance:\n•Any ﬁnite continued fraction represents a rational\nnumber.\n•Every convergent ai/biof a continued fraction is in\nits lowest terms, i.e. , aiandbihave no common di-\nvisors.\n•Each convergent is nearer to xthan the preceding\nconvergent and also than any other fraction whose\ndenominator islessthan that of the convergent.\nThe most important property in this context is the last\none,becauseitprovidesaprocedureforcomputingthefre-\nquencyratiosoftherationaltuningasfollows.Forthe k-th\nsemitone,weconsiderthefractionexpansionof x=12√\n2k,\ni.e.thefrequencyratioinequaltemperament,untiltherel -\native error oftheconvergent y=an/bnwrt.x, i.e.theterm\n|y/x−1|, islessthan 1%.Continued fractions may help us explain the origin of\nthe chromatic twelve-tone scale. For this, we look for a\ntuning in equal temperament with ntones per octave, such\nthattheperfectﬁfthinjustintonation(frequencyratio3 /2)\nis approximated as good as possible. Thus, we develop\na fraction m/nwith 2m/n≈3/2, where mis the number\nof the semitone representing the ﬁfth. Hence, we have to\napproximate x=log2(3/2)≈0.585. In this case, the se-\nquence ofconvergents is0 /1, 1/1, 1/2,3/5, 7/12, 24 /41,\n31/53,...,showing m/n=7/12asdesired,becausesemi-\ntonem=7 gives the perfect ﬁfth in the chromatic scale\nwithn=12 tones per octave.\n4. APPLICATION OFTHETHEORY\n4.1 Comparison of Different Approaches\nLet us now apply the periodicity-based theory to com-\nmonmusicalchordsandcorrelatetheobtainedresultswith\nempirical results. Tab. 2 shows the perceived and com-\nputed relative sonority of basic chord classes (cf. Fig. 1).\nTab. 2 (a) shows the ranking for the perceived sonority ac-\ncording to empirical experiments reported in [4], which\nhave been repeated by many others with similar results.\nUnfortunately, [4] does not consider the suspended triad.\nTherefore, it is not ranked in the table. Tab. 2 (b) provides\nthe ranking for complex tonalness [2], whose numerical\nvalues are shown in brackets. The model according to [2]\nbuilds on earlier work [9]. However, especially the disso-\nnance of the augmented triad is not reﬂected in this model\nby its calculated tonalness: It appears on rank 2, right af-\nter the major triad in root position. Therefore, [2] argues,\nthat this has cultural rather than sensory origin. Tab. 2 (c)\nshowstherankingwrt. instability [6].Thenotionoftension\nused in this model produces the desired low sonority of\nthediminishedandtheaugmentedtriad(cf.Sect.2.2).The\ncorrelation with the empirical results is good, but can stil l\nbe improved, e.g., the minor triad in root position (rank 2)\nscores better than the inversions of the major triad (ranks\n4and 5), which isnot as desired.\nTab. 2 (d)–(e) shows the ranking wrt. the harmonic-\nity values h. As one can see, there is almost a one-to-\n9010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nchord class (a)empirical [4] (b)tonality [2] (c)instabili ty[6] (d)harmonicity (e)harmonicity∗\nmajor {0,4,7} 1 1 (0.48) 1 (0.624) 2 (4) 2 (4.0)\n{0,3,8} 2 6 (0.38) 5 (0.814) 3 (5) 3 (5.0)\n{0,5,9} 3 3 (0.43) 4 (0.780) 1 (3) 1 (3.0)\nsuspended {0,5,7} 8 (1.175) 4 (6) 4 (6.0)\n{0,2,7} 11 (1.219) 5 (8) 5 (8.0)\n{0,5,10} 9 (1.191) 6 (9) 6 (9.0)\nminor {0,3,7} 4 4 (0.42) 2 (0.744) 7 (10) 7 (10.0)\n{0,4,9} 5 7 (0.38) 3 (0.756) 8-9 (12) 8 (12.0)\n{0,5,8} 6 10 (0.32) 6 (0.838) 10-11 (15) 9 (15.0)\ndiminished {0,3,6} 7 9 (0.35) 12 (1.431) 13 (60) 13 (26.0)\n{0,3,9} 8 5 (0.40) 7 (1.114) 10-11 (15) 10 (16.6)\n{0,6,9} 9 8 (0.37) 10 (1.196) 8-9 (12) 12 (19.9)\naugmented {0,4,8} 10 2 (0.44) 13 (1.998) 12 (20) 11 (19.7)\nTable 2. Ranking relative sonorities of common triads.\none correspondence with the empirical results. The num-\nbers in brackets are the respective harmonicity values h\nandh∗, where the latter are averaged over all inversions.\nFor this, we compute the harmonicity of the given chord\n(cf. Sect. 3), e.g. the ﬁrst inversion of the diminished tria d\n{0,3,9}, that ish0=lcm(1,5,3) =15. In addition, we\nadopt each tone as reference tone, not only the lowest\ntone.Thus,weconsideralsothechordswiththesemitones\n{−3,0,6}and{−9,−6,0}.Forsemitonesassociatedwith\na negative number n, we take the frequency ratio of semi-\ntone 12 −naccording toTab. 1(e) and halve it,i.e., wedo\nnot apply octave equivalence here. Therefore, we get the\nfrequency ratios {5/6,1/1,17/12}and{3/5,17/24,1/1}\nwith harmonicity values h1=12 andh2=120, respec-\ntively. Since periodicity of chords is related to the lowest\ntone, we multiply the hvalues by the lowest frequency ra-\ntiointhechord,obtaining h′\n0=15,h′\n1=5/6·12=10,and\nh′\n2=3/5·120=72.Wethenaveragethevirtualchordfre-\nquencies f1/h,wherehappearsinthedenominator.Hence,\nwe calculate the harmonic average of all harmonicity val-\nuesh′\n0,h′\n1,andh′\n2,which yields h∗≈16.6.\nTab. 2 (a) and (e) differ only in two respects: First, the\nmost consonant chord according to harmonicity (rank 1)\nis the second inversion of the major triad with semitones\n{0,5,9}andnottherootposition.Itscalculatedharmonic-\nity ish=3, which however coincides with the fact, that\nthe second inversion appears before the root position in\ntheharmonicovertonesequence(cf.Sect.2.1).Second,the\naugmented triad appears late as expected (rank 11 of 13),\nbuttherootpositionandthesecondinversionofthedimin-\nished triad appear still later. However, the continued frac -\ntion expansion for the tritone (semitone 6, frequency ratio√\n2),occurringinbothtriads,yieldsﬁrst7 /5,whichisonly\nslightly mistuned. This would lead to a signiﬁcantly lower\nhvalue of the two chords – as desired. Thus, in summary,\nthe periodicity-based approach on harmony perception ﬁts\nbest toempirical results.\n4.2 Overtones and Periodicity\nHarmonic overtone spectra are irrelevant for determining\nrelative periodicities. The period length of such complexwaveforms is identical with that of its fundamental tone.\nWe obtain h=1, since the frequencies of harmonic over-\ntones are integer multiples of the fundamental frequency,\nhence all frequency ratios {1/1,2/1,3/1,...}have 1 as\ndenominator. Therefore, harmonicity is independent from\nconcreteamplitudesandphaseshiftsofthesinusoidsofthe\npure tone components. This seems plausible, because har-\nmony perception only partially depends on loudness and\ntimbre of the sound. It should not matter much, whether\na chord is played e.g. on guitar, piano, or pipe organ. Of\ncourse, this argument only holds for tones with harmonic\novertone spectra. If we have inharmonic overtones in a\ncomplex tone such as in gamelan music (cf. [5]), then it\nholdsh>1 for the harmonicity value of a single tone,\ni.e., we have an inherently increased harmonic complexity\n(cf.[2]).\n4.3 From Chords toScales\nTheharmonicityvalue hcanbedeterminedforharmonies,\nconsisting of far more than three tones, without any com-\nputational problems. Thus, let us apply the formulae from\nSect. 3 to general chords and scales. Fig. 4 (a)–(b) shows\nharmonies with 5 tones, that have low hvalues. The pen-\ntachordEmaj7/9withh=8,classicallybuiltfromastack\nof thirds, is standard in jazz music. Alternatively, it may\nbe understood as superposition of the major triads Eand\nB, which are in a tonic-dominant relationship according to\nclassical harmony theory. Fig. 4 (b) shows the pentatonic\nscale (h=24), which could alternatively be viewed as the\nstandard jazz chord E6/9. All harmonies shown in Fig. 4\nhave low, i.e. good harmonicity values h, ranking among\nthe top 5% in their tone multiplicity category. This also\nholds for the diatonic scale (7 tones, h=24) and the blues\nscale (8 tones, h=24) in Fig. 4 (c)–(d). Furthermore, ac-\ncordingtotheir h∗value,allchurchmodes,i.e.thediatonic\nscaleanditsinversions,rankamongthetop11of462pos-\nsible scales with 7 tones. Therefore, the periodicity-base d\ntheory can contribute signiﬁcantly to the discussion about\ntheoriginofscalesofwesternmusic.Thereareothermath-\nematicalexplanationsfortheoriginofscales,e.g.bygrou p\ntheory [10], ignoring however the sensory psychophysical\n91Poster Session 1\nG4444\n(a) pentachord¯¯¯¯¯\n(b)pentatonics¯¯¯¯¯\n(c) diatonic scale¯¯¯¯¯¯¯\n(d) blues scale¯¯6¯ 4¯¯¯¯6¯\nFigure 4. Harmonies (scales)withmore than threetones.\nbasis forthe musical importance of theperfect ﬁfth.\n5. CONCLUSIONS\nAs we have seen in this paper, harmony perception can\nbe explained well by considering relative periodicities of\nchords, that can be computed from the frequency ratios of\ntheintervalsinthechord.Theapproachshowsagoodcor-\nrelation to empirical studies on perceived sonority. Even\nthe origin of scales can be described with this approach.\nIt is mathematically simple, employing Farey sequences\nor the Euclidean algorithm for computing continued frac-\ntions.Theapproachhasastrongpsychophysicallybasis.It\ntakes into account that human pitch perception is limited\nby a just noticeable difference of about 1% and assumes\nthat virtual pitch of chords (chord periodicity) can be de-\ntected.Thelatterisindeedpossible,asresultsfromneuro -\nscience prove, which webrieﬂy review now.\n5.1 Periodicity and Neuro-Science\nFrom a spectral point of view, sounds are combinations of\na fundamental frequency and certain overtones. Spectral\nanalysis is performed in the cochlea. When a pure tone is\ndetected, waves travel along the basilar membrane, which\nthe cochlea houses, reaching a maximum amplitude at a\npoint depending on the frequency of the tone [1–3]. Thus,\ntheearworksasaspectralanalyzer.Thisfunctionoftheear\nis used in the explanations of harmony perception, based\non overtones or dissonance (Sect. 2).\nPeriodicity-based explanations use missing fundamen-\ntal tones, i.e. tones that are physically not present and\nhence cannot perceived by the ear directly. It has been\nwell-known for years that periodicity can be detected in\nthebrain.Forexample,twopuretonesformingamistuned\noctavecauseso-called second-orderbeats ,althoughnoex-\nactoctaveispresent[3].Recently,neuro-sciencefoundth e\nmechanism for being able to perceive periodicity. As a re-\nsultofacombined frequency-time analysis,i.e.somekind\nofauto-correlation bycomb-ﬁltering,pitchandtimbreare\nmapped temporally and also spatially and orthogonally to\neachotherintheauditorymidbrainandauditorycortex[1]\n(see also [11]). [12] reviews neuro-physiological evidenc e\nfor interspike interval-based representations for pitch a nd\ntimbre in the auditory nerve and cochlear nucleus. Tim-\nings of discharges in auditory nerve ﬁbers reﬂect the time\nstructure of acoustic waveforms, such that the interspike\nintervals (i.e. the period lengths) that are produced conve y\ninformationconcerningstimulusperiodicities,thatares till\npresent inshort-termmemory [1].5.2 Summary and Open Questions\nFrom the good correlation of the periodicity-based theory\nwith the empirical results presented here, one may con-\nclude, that there is a strong psychophysical basis for har-\nmony perception and the origin of musical scales. As un-\nderlying principle for this, periodicity detection turns o ut\nto be more important than spectral analysis, although cul-\nturalandotheraspectscertainlymustnotbeneglected.The\nquestion, how different harmonies cause different emo-\ntions or subjective effects like happiness or sadness is not\nyet answered bythis,of course.\n6. REFERENCES\n[1] GeraldLangner.DiezeitlicheVerarbeitungperiodischerSig-\nnale im H ¨orsystem: Neuronale Pr ¨asentation von Tonh ¨ohe,\nKlangundHarmonizit ¨at.Zeitschriftf ¨urAudiologie ,46(1):8–\n21, 2007.\n[2] Richard Parncutt. Harmony: A Psychoacoustical Approach .\nSpringer, Berlin,Heidelberg, New York, 1989.\n[3] Juan G. Roederer. The Physics and Psychophysics of Music:\nAnIntroduction .Springer,Berlin,Heidelberg,NewYork,4th\nedition, 2008.\n[4] L. A. Roberts. Consonant judgments of musical chords by\nmusicians and untrained listeners. Acustica, 62:163–171,\n1986.\n[5] William A. Sethares. Tuning, Timbre, Spectrum, Scale .\nSpringer, London,2ndedition, 2005.\n[6] Norman D. Cook and Takashi X. Fujisawa. The psy-\nchophysics of harmony perception: Harmony is a three-tone\nphenomenon. EmpiricalMusicology Review ,1(2),2006.\n[7] James Beament. How we hear music: The relationship be-\ntweenmusicandthehearingmechanism . TheBoydellPress,\nWoodbridge, UK, 2001.\n[8] CarlD.Olds. ContinuedFractions ,volume9of NewMathe-\nmatical Library . L.W.Singer Company, 1963.\n[9] Ernst Terhardt, Gerhard Stoll, and Manfred Seewann. Algo-\nrithmforextractionofpitchandpitchsaliencefromcomplex\ntonal signals. Journal of the Acoustical Society of America ,\n71(3):679–688, 1982.\n[10] GeraldJ.Balzano.Thegroup-theoreticdescriptionoftwelve-\nfoldandmicrotonalpitchsystems. ComputerMusicJournal ,\n4(4):66–84, 1980.\n[11] Ray Meddis and Michael J. Hewitt. Virtual pitch and phase\nsensivity of a computer model of the auditory periphery: I.\nPitchidentiﬁcation,II.Phasesensivity. JournaloftheAcous-\ntical Society ofAmerica ,89(6):2866–2894, 1991.\n[12] Peter A. Cariani. Temporal coding of periodicity pitch in the\nauditory system: An overview. Neural Plasticity , 6(4):147–\n172, 1999.\n92"
    },
    {
        "title": "An Integrated Approach to Music Boundary Detection.",
        "author": [
            "Min-Yian Su",
            "Yi-Hsuan Yang",
            "Yu-Ching Lin",
            "Homer H. Chen"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417585",
        "url": "https://doi.org/10.5281/zenodo.1417585",
        "ee": "https://zenodo.org/records/1417585/files/SuYLC09.pdf",
        "abstract": "Music boundary detection is a fundamental step of music analysis and summarization. Existing works use either unsupervised or supervised methodologies to detect boundary. In this paper, we propose an integrated approach that takes advantage of both methodologies. In particular, a graph-theoretic approach is proposed to fuse the results of an unsupervised model and a supervised one by the knowledge of the typical length of a music section. To further improve accuracy, a number of novel mid-level features are developed and incorporated to the boundary detection framework. Evaluation result on the RWC dataset shows the effectiveness of the proposed approach.",
        "zenodo_id": 1417585,
        "dblp_key": "conf/ismir/SuYLC09",
        "keywords": [
            "Music boundary detection",
            "fundamental step",
            "unsupervised methodologies",
            "supervised methodologies",
            "graph-theoretic approach",
            "fusing results",
            "typical length of a music section",
            "novel mid-level features",
            "evaluation result",
            "RWC dataset"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nAN INTEGRATED APPROACH TO MUSIC BOUNDARY DETECTION \nMin-Yian Su, Yi-Hsuan Yang, Yu-Ching Lin, Homer Chen \nNational Taiwan University \nsui751004@gmail.com, affige@gmail.com, va gante@gmail.com, homer@cc.ee.ntu.edu.tw \nABSTRACT \nMusic boundary detection is a fundamental step of music \nanalysis and summarization. Existing works use either \nunsupervised or supervised methodologies to detect \nboundary. In this paper, we propose an integrated \napproach that takes advantage of both methodologies. In \nparticular, a graph-theoretic ap proach is proposed to fuse \nthe results of an unsupervised model and a supervised one by the knowledge of the typical length of a music section. \nTo further improve accuracy, a number of novel \nmid-level features are developed and incorporated to the boundary detection framework. Evaluation result on the \nRWC dataset shows the eff ectiveness of the proposed \napproach. \n1. INTRODUCTION \nPopular songs usually comprise several music sections \nsuch as intro, verse, chorus, bridge and outro. A music boundary is the time point where a section transits to \nanother. Identifying such boundaries is important because \nit allows us to divide a song into semantically meaningful sections. This information can also be applied to music \nsummarization [1] and thumbnailing [2] to facilitate \nmusic browsing and struct ure-aware playback [3]. \nBoundary detection also serves  as a front-end processor \nfor music content analysis since it provides a local \ndescription of each section rath er than a global but coarse \nrepresentation of the whole song [5]. \nAlthough there is a rich literature in music theory about \nmusic structure analysis for symbolic music (e.g. [20]), \nmusic boundary detection for music signals is still a \nchallenging task because precise pitch detection in poly- \nphonic music is not yet achievable. Under this condition, most work on music boundary detection utilizes the \nsimilarity between short-term (e.g., 23ms) audio frames \nwithin a song to identify the repetitive parts and divide a \nsong into a number of sections [1–3, 6–8]. A more recent \nwork formulates boundary detection as a clustering problem and considers that the audio frames of each \ncluster belong to the same music section [9]. \nThe accuracy of this unsupervised  approach, however, \nmay be limited because only the information of a song \nitself is exploited. For example, identifying repetitive \nparts cannot correctly identify the boundary between two adjacent music sections that always occur successively in  \na song. On the other hand, clustering-based methods tend \nto produce over-segmented results if the acoustic property \nof the frames in a music section varies greatly. Using histograms to gather statistic  of spectral characteristics of \nneighboring audio frames [9] does not necessarily solve \nthe problem because the hi stograms of two adjacent \nframes are usually similar, making boundary detection \neven more difficult. \nTo address the aforementioned drawbacks, Turnbull et \nal formulate music boundary detection as a supervised \nproblem and train a binary cla ssifier to classify whether a \ntime point is a boundary or not [10]. In this way, we can \nmine more information from a large number of training \nsongs and identify features that are relevant to boundary detection. \nHowever, because a supervised system is pre-trained \nby using the training data and fixed afterwards, it is not as adaptive to test songs as its unsupervised counterpart. The \ndetection accuracy may signi ficantly degrade when the \ncharacteristics of the training  data and a test song are \nconsiderably different. For instance, if the system detects \nboundary according to the energy level in a certain \nfrequency range, the system may not work for a song whose energy in that frequency range maintains high \nthroughout the song. \nBased on the above observations, we propose to take \nadvantage of both methodologies by aggregating the \nresults of an unsupervised model and a supervised one. In \nthis way, we can exploit the discriminative information \nprovided by the training data and the song-specific \nFigure 1 . A schematic diagram of the proposed music \nboundary detection system. \n \nPermission to make digital or hard copies of all or part of this work fo r\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for prof it or commercial advantage and tha t\ncopies bear this notice and the full citation on the first page. \n© 2009 International Society for Music Information Retrieval  \n705Poster Session 4\n  \n \ninformation of a test song at the same time. Moreover, to \nbetter capture the discriminative characteristics of a \nboundary, we further propose a number of novel mid-level features, including nove lty score, dissonance \nlevel and vocal occurrence. Comparing to low-level \nfeatures such as the spectral properties, these mid-level features carry more semantic meaning that improves \nmusic boundary detection. \nA schematic diagram of the proposed system is shown \nin Fig. 1. An input song is partitioned by the beat onsets \nand represented by a set of low-level and mid-level \nfeatures. The probability of each beat onset of being a \nboundary is then computed by both supervised and \nunsupervised methods with the features extracted from the subsequent beat interval. We then model the beat \nonsets as the vertices of a dir ected graph, with the vertex \nweights determined by the probability of being a boundary and the edge weights determined based on the \nmusic knowledge of the typical length of a music section \n[7, 11]. Finally, we formulate music boundary detection as a shortest path problem and identify the true \nboundaries by the Viterbi algorithm [18]. \nThe paper is organized as follows. Section 2 describes \nthe feature representation of  music, including low-level \nand mid-level features. Section 3 elaborates on the system \nframework and the adopted supervised and unsupervised approaches. Experimental result is presented in Section 4. \nSection 5 concludes the paper. \n2. MUSICAL REPRESENTATION \nBefore feature extraction, ea ch song is converted to a \nstandard format (mono channel and 22,050 Hz sampling \nrate) and partitioned into several beat intervals by the beat onset detection algorithm BeatRoot [12]. We adopt beat \ninterval instead of frame as  the basic time unit because \nthe characteristics of a song are more likely to be consistent within a beat in terval and because a music \nboundary tends to occur at a beat onset [7]. \n2.1 Low-level Features  \nFor low-level local features , we use 40-dim Mel-scale \ncepstral coefficients (MFCCs ), 24-dim chromagram, and \n52-dim fluctuation patterns (FPs) [19] to represent the timbre, harmony, and rhythm aspects of music. We \nextract MFCCs and chromagram with a 40ms and \nnon-overlapping sliding window and aggregate the frame-level features within each  beat interval by taking \nthe mean and the standard deviation. FPs are computed \ndirectly for each beat interv al. These features have been \nfound useful for music boundary detection [10]. Note \nthese features only capture the local property of music. \n2.2 Mid-level Features \nBelow we describe three mid-level features: novelty score, \ndissonance level, and vocal occurrence. While the first \none is originally proposed by Cooper et al in [4], it has \nbeen used in an unsupervised setting rather than as a \nmid-level feature in a supervised one. On the other hand, \nthough the latter two features have been studied in the context of music theory [21], few attempts have been made to incorporate them to  the task of music boundary \ndetection for raw audio signals. \n2.2.1 Novelty Score  \nThe novelty score is computed by two steps [4]. First, a \nsimilarity matrix is constructed by measuring the \nsimilarity of the low-level feature vectors of every two beats in a song. In this matrix, the two segments beside \nthe boundary produce two adjacent square regions of high \nwithin-segment similarity along the main diagonal and two rectangular regions of low between-segment \nsimilarity off the main diagonal. As a result, each \nboundary produces a checkerboard  pattern in the matrix \nand the beat interval that bou ndary occurs is the crux of \nthis checkerboard. To identify these patterns, we correlate \na Gaussian-tapered checkerbo ard kernel along the main \ndiagonal of the similarity matrix to compute the so-called \nnovelty scores, which measures both the dissimilarity \nbetween two different adjacent segments beside each potential boundary as well as the similarity within these \nsegments. We define the term segment  here to represent a \nset of consecutive beat intervals and the term section\n as a \nsegment which is semantically meaningful (such as verse, \nchorus or bridge).1 \nIn this work, we compute three novelty scores based on \nthe three low-level features. Because the novelty scores \nof adjacent beats tend to be similar,2 we also divide the \nnovelty score of a certain beat interval by the sum of the \nnovelty scores of neighboring beat intervals and use the \nnormalized score as additional feature, resulting in a total of 6 features for each beat interval. \n2.2.2 Dissonance Level  \nIt is known in musicology that the relaxation or release of \ntension plays an important role in the transition of music \nsections. Because changes in tension often occur when \ndissonance giving way to consonance [13], we develop a novel feature based on the dissonance level of music. We \nfirst define the dissonant intervals  according to the \nrelationship between the pitches of two notes that cause tension (e.g., Tritone and Minor Second [14]), and then \ncompute the dissonance level as the weighted sum of the \ncorresponding dissonant intervals from the unwrapped chromagram of a beat, \n∑∑∑\n∈+\n=\nmmDqmqmmq\ntccck\ny ,    (1) \nwhere yt denotes the dissonance level of a beat t, q \ndenotes the interval that has q semitones between the two \nnotes, D is the set of dissonant intervals, cm is the mth bin \nof the chromagram, and kq is a constant corresponding to \nq, which is empirically set according to the ratio of \nfrequencies of the two pitches in q. The denominator is a \nnormalization term. \n                                                           \n1 While a segment can be of arbitrary length, the length of a \nsection often follows a typical pattern, see Section 3.3. \n2 The novelty scores of adjacent beats are similar because the \nsubmatrices of the similarity matrix of these beats overlap a lot. \n70610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \n \nWe compute the dissonance level for each beat interval \nand obtain a sequence of dissonance levels. We compute \nthe derivative from the resulting sequence as the \ndissonant features to capture  the changes in tension,  \n2p\ntm mp\nt p\nmpmy\ny\nm− =−\n=−Δ=∑\n∑,   (2) \nwhere p denotes the window size. In this work, we set p \nto 1 and 2 and generate a two dimensional dissonance level feature. Fig. 2 illustrates the relationship between \nmusic boundary and dissonance level; clearly the music \nboundaries occur right after peaks of dissonance level (the rise and relax of tension). \n2.2.3 Vocal Occurrence \nIn pop/rock songs, the time points that a vocalist sings \noften correspond to the music boundaries. For example, if \na beat onset falls in the middle of a segment with pure \ninstrument and another segment with singing voice, it is very likely a music boundary. Furthermore, because a \nmusic section is comprised of several music phrases,\n3 a \ntransition of music sections must also be a transition of music phrases. Therefore, if a beat onset falls in a short \ninstrumental interval between two vocal music phrases, it \nis more likely to be a music boundary. \nIn light of the above observation, we train a \nvocal/non-vocal classifier by support vector machine \n(SVM) [15], with MFCC as the feature representation, to \nestimate the probability of the vocal occurrence for each \nbeat interval. If the sum of these probabilities from the beat intervals in a segment exceeds a threshold, we regard \nthe segment as a vocal segment . More specifically, the \nvocal occurrence feature of a certain beat interval is computed as follows. For a beat  interval, if both of its \nneighboring segments are n on-vocal, the vocal occur- \nence is set to 0; if only one of the neighboring segments is non-vocal, the vocal occurren ce is set to 1. When both \nneighboring segments are vocal, we set the vocal \no c c u r r e n c e  a c c o r d i n g  t o  t h e  f o l l o w i n g  f o r m u l a :                                   \n∑+\n≠−=+−=wt\ntjwtjjt\nt vwvz\n, 1 21,   (3) \nwhere zt is the vocal occurrence f eature of beat interval t, \n                                                           \n3 Several music phrases constitute a music section.  vt is the probability estimate of beat interval t generated \nby the vocal/non-vocal SVM classifier, and  ω is the \nwindow size that represents the length of the segment. \nWe vary the value of ω and generate a multi-dimensional \nfeature vector. In this work we set the value of ω to 8 and \n12. An illustrative example is shown in Fig. 3. The first \nred line labels a transition from a non-vocal section (intro) to a vocal section (verse). The green circles label two \nobvious transition points of music phrases, while the \nlatter one is in fact a transition point of music sections. We can see the corr esponding vocal occu rrence feature is \nhighly correlated to music boundaries. A pitfall of this \nfeature is that it may regard  every phrase boundary as a \nsection boundary and result in over segmentation. The \nuse of other features may offset this mistake. \nRepresenting the acoustic properties of music by these \nlow-level and mid-level features, we then employ the \nsystem described below to detect boundaries. \n3. SYSTEM DESCRIPTION \nIn this section, we first introduce the supervised and \nunsupervised approaches adopted in our system. Both \napproaches estimate the possibility of each beat onset of being a music boundary. Second, we describe how we \nintegrate these two estimations with the music knowledge \nof typical section length. \n3.1 Supervised Estimation \nWe train a SVM classifier with polynomial kernel and \nprobability estimates to obtain the possibility of a beat onset being a music boundary. The label for a beat \ninterval is marked 1 if a bo undary occurs at that beat \nonset and 0 otherwise. Besides mid-level features, we also use the low-level featur es to train the classifier \nbecause low-level features also contain some relevant \ninformation. For example, a drum-fill is usually played when a music section ends; this characteristic can be \ndetected by FP. For a test song, the SVM model \nFigure 3 . Top: the possibility of vocal estimated by SVM \nfor a part of Billie Jean by Michael Jackson. The two red lines label a transition from intro to verse and a transition \nfrom verse to bridge. The green circles label two obvious \ntransition points of music phrases. Bottom: corresponding \nvocal occurrence feature. \n \nFigure 2 . The dissonance level of a part of Billie Jean by \nMichael Jackson. The two red lines label a transition from \nverse to bridge and a transition from bridge to chorus. \nThese boundaries occur right after high dissonance levels.\n707Poster Session 4\n  \n \ncomputes the probability of the occurrence of a boundary \nat every beat onset. We utilize this probability as the \noutput of the supervised approach. \n3.2 Unsupervised Estimation \nAs for the unsupervised part, we construct three similarity \nmatrices based on the kinds of low-level features and detect the peaks of the mean of the novelty scores from \nthese matrices. We then use these peaks to divide the test \nsong into a number of segments [4]. The low-level features of a segment are in tegrated to one vector by \ntaking the mean and the standard deviation and a distance \nmatrix among the segments is constructed by computing the pairwise distance between these vectors. The \nnormalized cut algorithm [16] is then performed on the \ndistance matrix to group these segments into acoustic similar clusters. At each beat  interval, we further count \nthe cluster indices of neighboring beat intervals within a \npredefined window size and establish two histograms: one for the beat intervals preceding to the beat onset, and \nthe other for the subsequent beat intervals. The Euclidean \ndistance of the resulting histograms can represent the \npossibility of a music boundary occurs at the designated \nbeat onset, and the ratio of this possibility value of a beat onset to the sum of the possibility values of its \nneighboring ones is regarded as the estimation of the \nunsupervised approach. \n3.3 Integration \nBecause music sections tend to have some typical length \n(e.g., 8 or 16 bars) [7, 11], it should be beneficial to \nincorporate this knowledge to the music boundary detection framework. As Fig. 4 illustrates, we construct a \ndirected graph G = (V, E) to integrate the estimates of \nsupervised and unsupervised models and to take \nadvantages of this music kn owledge. In this graph, a \nvertex represents a beat onse t, with the weight of it \ndetermined by the weighted sum of the estimates of \nsupervised and unsupervised models \n1ii ivu swp k p=+ ,    (4) \nwhere wvi denotes the weight of a vertex i, pui and psi are \nthe probability estimates produced by an unsupervised model and a supervised one respectively, and  k\n1 is a \nparameter balancing the effect of the two models. The \nmusic knowledge of section length is incorporated as follows. If there exists th e possibility that vertices v\ni and \nvj are two successive music boundaries, we form an edge \nbetween these two vertices. Th e weight of the edge is \ndetermined by the music knowledge of the length of a \nmusic section. We gather the statistics from training data \nto obtain the probability of two beats with specific \ntemporal distance being music boundaries. That is, the \nweight of eij equals to the weight of emn if j–i equals to \nn–m. To achieve this goal, a histogram is constructed by \nsimply counting the number of beats of each music \nsection from the training data. \nTherefore, a path in this constructed graph can be \nregarded as a set of music boundaries. We further define \nthe weight of a path B as the sum of the weights of its \nconstituent edges and vertices,  \n2 Bv e vB eBww k w∈∈=+∑∑ ,  (5) \nwhere wv and we are the weights of a vertex and an edge \nin B, and k2 is a constant to balance the effects of vertices \nand edges. We regard wB as the probability of the \nassociated beat onsets bein g correct music boundaries. \nBecause the path with maximum wB consists of \nvertices that are most likely the music boundaries, we \nformulate the problem as a shortest path problem and \nemploy the Viterbi algorithm [18] to solve it,  \n*arg maxBBB w= ,   (6) \nwhere B* denotes the optimal solution. In practice, we \nonly apply Viterbi to a feasible number of paths to reduce the complexity.  \n4. EXPERIMENT \n4.1  Experimental Setup \nWe conduct an empirical evaluation on the RWC music \ndataset [17], which contains 100 pieces of song that are \noriginally produced for experiment; most of the pieces \n(80%) are recorded according to 1990s Japanese chart music, while the rest resemble the 1980s American chart \nmusic. RWC dataset provides clear annotations of music \nboundaries and is adopted in many literatures in music \nboundary detection [6, 10].  \nWe evaluate the performance in terms of precision (the \nproportion of true boundaries among the detected ones), \nrecall (the proportion of true boundaries in the ground \ntruth that are detected by th e system), and f-score (the \nharmonic average of precision  and recall). A detected \nboundary is considered correct if it falls within 1.5 \nseconds of the ground-truth, which is stricter than the one used in prior work [9] and should be reasonable for \nreal-world applications. \nFor the unsupervised method s, we process each of the \n100 songs independently and take the average result. For \nthe supervised methods, we evaluate the system with \nFigure 4 . The directed graph G of a song, which has n \nbeat onsets (vertices) and k—1 possible section lengths \n(possible jumps). The vertex weights are determined by the probability of being a boundary and the edge weights are determined based on the music knowledge of the \ntypical length of a music section [7, 11]. We assume that \nevery music section contains at least one beat interval. \n \n70810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n  \n \nstratified five-fold cross validation: 20 random songs are \nheld out as test data and the rest are used for training. The \nevaluation is iterated five times to get the average result. \n4.2 Results \nWe first evaluate the supervised approach with different \nfeature representations, including low-level and mid-level \nfeatures. To compare the pe rformance against previous \nwork, we also implement the difference feature and its \nderivative proposed in [10]. The difference feature is \ncomputed by sliding a window along the audio signal and comparing the statistic of low-level features in the first \nhalf of the window with the ones in the second half. A \nbeat onset is detected as a boundary if its probability \nestimate assigned by SVM exceeds a threshold. Instead of \nusing a fixed threshold, we adaptively set the threshold of each song to be the mean plus  one standard deviation of \nthe probability estimates of the song. \nThe evaluation result is shown in Table 1. The three \nlow-level features bring about similar accuracy, with FPs \nslightly worse than the other two, implying that the \ncharacteristics of music boundaries are represented more in timbre and rhythm. The di rect concatenation of the \nthree low-level features, which are denoted as local (L) in \nthe table, further improves the f-score to 0.2206.  \nWe then compare four mid-level features, including the \ndifference feature proposed in [10]. It can be found that, \nwith much lower feature dimension, the use of mid-level features achieves similar or superior performance to that \nattained by low-level features. The novelty score, in \nparticular, achieve an f-score of 0.2549 that significantly \noutperform all other low-level or mid-level features. We \ncan also find that the differe nce feature does not perform well, which possibly due to the disregard of the similarity \nof the beats in each segment. \nThe combination of mid-level and low-level features \nonly brings about slight improvement, which somewhat implies that most of the information carried by low-level \nfeatures has already been well represented by the \nmid-level features. The combination of novelty score (N), dissonance level (D), vocal occurrence (V), and local \nfeatures (L) achieves the highest f-score of 0.2641.  \nWe then compare the tw o unsupervised methods \ndescribed in Section 3.2. For the cluster-based method, \nwe simply mark the boundary of two consecutive \nsegments that are associated with different clusters as a \nmusic boundary without smoothing. The result is shown \nin Table 2. As expected, th e clustering-based approach \nexhibits a remarkably high recall but a relatively low \nprecision. For the histogram-based method, we consider \nthe segments whose probability estimates exceed a threshold as boundaries. The threshold value is set in the \nsame way as in the supervis ed methods. The performance \nof the histogram-based method is slightly worse than the clustering-based one, showing that gathering statistics of \nneighboring frames does not improve the precision of \nboundary detection. Moreover, it can be noted that in our evaluation the unsupervised approaches generally \noutperform the supervised counterparts, showing that the \nability of the unsupervised approach to be adaptive to each test song is essentia l in boundary detection.  \nFinally, we evaluate the performance of integrating the \nresult of unsupervised and supervised methodologies. For comparison, we further implement a baseline method that \nsimply sums up the supe rvised and unsupervised \nestimates with the same weight as the one in proposed \ngraph-theoretical fusion method without exploiting the \nmusic knowledge of section length.  \nThe result is also shown in Table 2. It can be found that \nsimply taking the average has achieved a higher f-score \nthan any of the supervised-only or unsupervised one, showing that the two methodologies are indeed \ncomplementary and the fusion of them is plausible. The \nproposed graph-theoretical fusion further improves the f-score to 0.4094, which greatly outperform the taking \naverage baseline, especially in recall. This result shows \nthe integration of the two methodologies and the incorporation of music knowledge are essential to music \nboundary detection.  \nA sample segmentation result is displayed in Fig. 5. In \nthis example, all th e boundaries can be correctly detected \nby the proposed system. Nevertheless, there is an over \nsegmentation problem because the characteristics of the \nsegments of the same music section may be incoherent. Feature # \nfeature Precision Recall F-score\nMFCC 40 0.1910 0.2574 0.2142\nchromagram 24 0.1665 0.2131 0.1842\nfluct. pattern 52 0.1906 0.2190 0.2019\nlocal (L) 116 0.1982 0.2629 0.2206\ndifference [10] 6 0.1602 0.2519 0.1885\nnovelty (N) 6 0.2427  0.2770 0.2549\ndissonance (D) 2 0.2109  0.2505 0.2198\nvocal (V) 2 0.2128  0.2687 0.2240\nN+L 122 0.2354  0.2900 0.2594\nN+D+V 10 0.2322 0.2909 0.2592\nN +D+V+L 126 0.2461  0.2932 0.2641\n \nTable 1.  Evaluation result of different features used in \nsupervised musical boundary detection methods. Approach  Method Pr ecision Recall F-score \nSupervised only N +D+V+L 0.2461  0.2932 0.2641 \nUnsupervised only Cluster-based (normalized cut) [9] 0.2770  0.5166 0.3517 \nHistogram-based 0.3068  0.3428 0.3124 \nDirectly sum  0.3274  0.3470 0.3385 \nIntegrated with section length Viterbi algorithm [18] 0.3800  0.4452 0.4094 \n \n Table 2.  Evaluation result of different musical boundary detection methods. \n709Poster Session 4\n  \n \n \nTo resolve this problem, we are working on incorporating \nmore music knowledge and mid-level features. \n5. CONCLUSION \nIn this paper, we have presented an integrated system \nthat combines the informati on from supervised approach, \nunsupervised approaches, and music knowledge. We formulate music boundary detection as a shortest path \nproblem and employ the Viterbi algorithm to solve it. We \nalso propose a number of no vel mid-level features to \nbetter capture the discriminative characteristics of music \nboundaries. Experiments conducted on the RWC dataset \nshow significant improvement over the state-of-the-art \nsupervised-only and unsupervised-only methods. \n6. ACKNOWLEDGEMENT \nThis work was supported by the National Science \nCouncil of Taiwan under the contract number NSC \n97-2221-E-002-111-MY3. The authors would like to \nthank the anonymous reviewers for valuable comments that greatly improved the quality of this paper. \n7. REFERENCES \n[1] W. Chai, “Semantic segmentation and summari- \nzation of music,” in  IEEE  Signal Processing \nMagazine, Vol. 23, No. 2, pp. 124–132, 2006. \n[2] M. Levy, M. Sandler, and M. Casey, “Extraction of \nhigh-level musical structure from audio data and its \napplication to thumbnail generation,” in Proc. \nICASSP, pp. 1433–1436, 2006. \n[3] M. Goto, “A chorus-section detection method for \nmusical audio signals and its application to a music listening station,” in IEEE Trans. Au dio, Speech and \nLanguage Processing, Vol.14, No.5, pp. 1783–1784, \n2006. \n[4] M. Cooper and J. Foote, “Summarizing popular \nmusic via structural similarity analysis,” in\n Proc. \nIEEE Workshop  Applications of Signal Processing \nto Audio and Acoustics, pp. 127–130, 2003.  \n[5] M .  C a s e y ,  R .  V e l t k a m p ,  M .  G o t o ,  M .  L e m a n ,  C .  \nRhodes, and M. Slaney, “Content-based music information retrieval: current directions and future \nchallenges,” in Proceedings of the IEEE, Vol. 96, \nNo. 4, pp. 668–696, 2008. \n[6] J. Paulus and A. Klapuri, “Music structure analysis \nusing a probabilistic fitness measure and a greedy search algorithm,” in IEEE Trans. Audio, Speech \nand Language Processing , Vol. 17, No. 6, pp. \n1159–1170, 2009.   \n[7] N. C. Maddage, C. Xu, M. S. Kankanhalli, and X. \nShao, “Content-based music structure analysis with \napplications to music semantics understanding,” in Proc. ACM Multimedia , pp. 112–119, 2004. \n[8] L. Lu, M. Wang, and H. Zhang, “Repeating pattern \ndiscovery and structure analysis from acoustic music \ndata,” in Proc. ACM SIGMM Int. Workshop on \nMultimedia Information Retrieval , 2004. \n[9] M. Levy and M. Sandler, “Structural segmentation \nof musical audio by constrained clustering,” in IEEE \nTrans. Audio, Speech and Language Processing , Vol. \n16, No. 2, pp. 318–326, 2008. \n[10] D Turnbull, G. Lanckriet, E. Pampalk, and M. Goto, \n“A supervised approach fo r detecting boundaries in \nmusic using difference features and boosting,” in Proc. ISMIR  2007. \n[11] C. Rhodes et al, “A Markov-chain Monte-Carlo \napproach to musical audio segmentation,” in Proc. \nICASSP , 2006. \n[12] S. Dixon, “Evaluation of the audio beat tracking \nsystem BeatRoot,” in Journal of New Music \nResearch , Vol, 36, No. 1, pp. 39–50, 2007. \n[13] DeLone et al, Aspects of Twentieth-Century Music . \nEnglewood Cliffs, New Jersey: Prentice-Hal l, 1975 \n[14] Wyatt and Keith, Harmony & Theory .  Hal \nLeonard Corporation, pp. 77, 1998. \n[15] N. Maddage et al, “An SVM-based classification \napproach to musical audio,” in Proc. ISMIR, 2003. \n[16] Ji. Shi and J. Malik, “Normalized cuts and image \nsegmentation,” in Proc. CVPR , pp. 731–737, 1997. \n[17] M. Goto, “AIST annotation for RWC music \ndatabase,” in  Proc. ISMIR\n, 2006. \n[18] G. D. Fomey, “The Viterbi algorithm,” in Procee- \ndings of the IEEE  Vol. 61, No. 3, pp.268–278, 1973. \n[19] E. Pampalk, “Computational models of music \nsimilarity and their application in music information retrieval,” in PhD  thesis, Vienna University of \nTechnology , 2006. \n[20] F. Lerdahl and R. Jackendoff, “An overview of \nhierarchical structure in music,” in Machine Models \nof Music , pp. 289–312, 1993. \n[21] D. Pressnitzer, S. McAdams,  “Two phase effects in \nroughness perception,” in The Journal of the \nAcoustical Society of America, Vol. 105, No. 5, \npp.2773–2782, 1999. \nFigure 5 . The segmentation result of Billie Jean by \nMichael Jackson. Top: the b oundaries detected by the \nproposed system. Bottom: the manual annotation. \n710"
    },
    {
        "title": "Slave: A Score-Lyrics-Audio-Video-Explorer.",
        "author": [
            "Verena Thomas",
            "Christian Fremerey",
            "David Damm",
            "Michael Clausen"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418029",
        "url": "https://doi.org/10.5281/zenodo.1418029",
        "ee": "https://zenodo.org/records/1418029/files/ThomasFDC09.pdf",
        "abstract": "We introduce the music exploration system SLAVE, which is based upon previous developments of our group. SLAVE manages multimedia music collections and allows for multimodal navigation, playback, and visualization in an efficient and user-friendly manner. 1 While previously the focus of our system development has been the simultaneous exploration of digitized sheet music and audio, with SLAVE we enhance the functionalities by video and lyrics to achieve a more comprehensive music interaction. In this paper, we concentrate on two aspects. Firstly, we integrate video documents into our framework. Secondly, we introduce a graphical user interface for semi-automatic feature extraction, indexing, and synchronization of heterogeneous music collections. The output of this GUI is used by SLAVE to offer both high quality audio and video playback with time-synchronous display of digitized sheet music and content-based search.",
        "zenodo_id": 1418029,
        "dblp_key": "conf/ismir/ThomasFDC09",
        "keywords": [
            "multimodal navigation",
            "multimedia music collections",
            "multimodal exploration",
            "user-friendly manner",
            "video and lyrics",
            "heterogeneous music collections",
            "graphical user interface",
            "semi-automatic feature extraction",
            "time-synchronous display",
            "digitized sheet music"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nSLAVE:A SCORE-LYRICS-AUDIO-VIDEO-EXPLORER\nVerenaThomas Christian Fremerey David Damm MichaelClausen\nDepartment of Computer Science III\nUniversity of Bonn, Germany\n{thomas,fremerey,damm,clausen }@iai.uni-bonn.de\nABSTRACT\nWe introduce the music exploration system S LAVE, which\nisbaseduponpreviousdevelopmentsofourgroup. S LAVE\nmanagesmultimediamusiccollectionsandallowsformul-\ntimodal navigation, playback, and visualization in an ef-\nﬁcient and user-friendly manner.1While previously the\nfocus of our system development has been the simultane-\nous exploration of digitized sheet music and audio, with\nSLAVEwe enhance the functionalities by video and lyrics\ntoachieveamorecomprehensivemusicinteraction. Inthis\npaper, weconcentrate ontwoaspects. Firstly,weintegrate\nvideo documents into our framework. Secondly, we in-\ntroduce a graphical user interface for semi-automatic fea-\ntureextraction,indexing,andsynchronizationofheterog e-\nneous music collections. The output of this GUI is used\nby SLAVEto offer both high quality audio and video play-\nbackwithtime-synchronousdisplayofdigitizedsheetmu-\nsicand content-based search.\n1. INTRODUCTION\nVarious aspects of a piece of music can be described\nby different types of music documents, such as scans of\nsheet music, symbolic data (e.g., MIDI, MusicXML), text\n(e.g.,lyrics,libretti,musicanalysis),audiorecording s,and\nvideo. In modern digital music libraries large collections\nof these music documents are stored. The availability of\ndigitalmusiccollectionsnaturallyleadstothenecessity of\nproviding tools to automatically process, analyze and pre-\nparethismultimediadataforanefﬁcient anduser-friendly\naccess. Equally, user interfaces for an adequate multi-\nmodal presentation of and interaction with the music doc-\numentsneedtobeprovided. Thelastyearshavewitnessed\nsubstantial progress in developing automated MIR pro-\ncessingprocedurestocomputesynchronizationandindex-\nWe gratefully acknowledge support from the German Research\nFoundation DFG. The work presented in this paper was support ed by the\nPROBADO project (http://www.probado.de/, grant INST 11925/1-1) a nd\nthe ARMADAproject (grantCL64/6-1).\n1While”multimodal”referstotheperceptionofmusicthroughdif fer-\nentmodalities(userperspective),”multimedia”corresponds tothediffer-\nent mediatypes (data perspective).\nPermission to make digital or hard copies of all or part of this w ork for\npersonal or classroom use is granted without fee provided th at copies are\nnotmadeordistributedforproﬁtorcommercialadvantageandth atcopies\nbear this noticeand thefull citation ontheﬁrst page.\nc/circlecopyrt2009International Society forMusic InformationRetrieval .ing information for multimedia music collections [1–4,6].\nIntheareaofdigitalizationofmultimedialibrarycontent s,\nefforts both towards better automatized digitization tech -\nniques and semantic integration of multimedia documents\narenoticeable[5,6]. Furthermorethereareseveralpropos -\nals for user interfaces to access and present digital music\ndatabases inamultimodal manner [1,6–9].\nThe most frequently encountered digitally available\ntypes of music representation are scanned sheet music,\nsymbolic score data and audio recordings. Therefore most\nof the presented techniques and frameworks mainly focus\non one or several of these data types. However, there is\nanother type of music representation, which can provide\nlibrary users, musicians and musicologists with rich infor -\nmation on the pieces of music. Today, most live perfor-\nmances are ﬁlmed and distributed to a broad audience via\nvideo DVD and television. Besides recordings of live per-\nformancesalsospeciﬁcvideoproductionsofpiecesofmu-\nsic are available. Hence, the extension of the functionali-\ntiesofmultimodalframeworkstosupportvideodocuments\nand the development of user interfaces for video integra-\ntionsuggest themselves.\nA holistic presentation using as many different media\nsourcesandtypesaspossiblecansupporttheprocessofex-\nperiencing the music as well as analyzing the music with\nrespect to different aspects. Prospective conductors, for\nexample, might be interested in watching music videos to\nlearn or compare the conducting style of different conduc-\ntors. Providing tools to allow fast and smooth comparison\nbetween and browsing within interpretations are desirable\nforthispurpose.\nIn this paper, we introduce the Score-Lyrics-Audio-\nVideo-Explorer (SLAVE),whichisbaseduponpreviousde-\nvelopmentsofourgroup. Asenhancement,weproposethe\nintegration of videos into S LAVEto converge to a holistic\nexploration of music using various types of music docu-\nments in an integrated manner. Furthermore, we introduce\na graphical user interface for the semi-automatic process-\ning of multimedia music collections to generate indexing\nandsynchronizationstructuresaswellasotherderiveddat a\ntypes. Note that for videos, we solely use the audio track\ntoperform all required calculations.\nThe rest of this paper is organized as follows. The\nsubsequent Section 2 provides information on the under-\nlying techniques of feature extraction and music synchro-\nnization. In Section 3 the workﬂow for processing music\ncollections and a GUI for a user-friendly management of\nthe workﬂow are described. Section 4 presents the inter-\n717Oral Session 8: Lyrics\nFigure 1. Illustration of the scan-video synchronization,\nusing the ﬁrst measures of the 2nd movement of Liszt’s\nFaust Symphony. (a) Scanned sheet music. (b) Chroma-\ngram of the sheet music. (c) Audio chromagram. (d) Au-\ndio track extracted from the video. (e) Music Video. The\nscan-video synchronization (double-headed arrows) is ob-\ntained by chromagram-alignment, see Section 2.2.\nface SLAVEand in particular the integration of video doc-\numentsintothissystem. ThepaperclosesinSection5with\nprospects on futurework.\n2. UNDERLYING TECHNIQUES\nIn this section, we describe the methods needed to pro-\ncess, match and align various types of music documents.\nThe basic idea of the presented processing methods is to\ntransform all music document types into a common fea-\nture representation, which allows for direct comparison\nand alignment independent of the input document types.\nIn this context, chroma-based music features have proven\nto be a good mid-level representation [10,11]. At ﬁrst,\nthe input documents are transformed into sequences of\n12-dimensional chroma vectors, where each vector repre-\nsentsanenergydistributionoverthetwelvepitchclasseso f\nthe equal-tempered scale. In Western music notation, the\nchroma are commonly indicated by the pitch spelling at-\ntributesC,C#,D,...,B.Byconsideringshort-timestatis-\ntics, these chroma features are transformed into the robust\nandscalableCENSfeatures,see[12]fordetails. Asanex-\nample,theCENSsequencesforanextractofscannedsheet\nmusic and the CENS features of the corresponding videosection are displayed in Figure 1 (b) and (c). Throughout\nthis paper, chroma features with a sample rate of 10Hz\nare applied, whereas CENS features of different sample\nrates are generated and used for alignment and indexing\npurposes.\n2.1 Deriving Chroma-based Features\nTo time-align two music documents (e.g., sheet music and\na video recording) describing the same piece of music,\nboth documents aretransformed intoCENS features.\nThe transformation of scanned sheet music into CENS\nfeatures requires several processing steps, see [13]. At\nﬁrst, using standard software for optical music recogni-\ntion (OMR), the scanned score data is analyzed and trans-\nformed into musical note parameters. Subsequently, based\non the gained pitch and timing information, the chroma\nfeaturescanessentiallybecomputedbyidentifyingpitche s\nthat belong to the same chroma class. The CENS se-\nquences are gained from these features as previously de-\nscribed. Duringthefeatureextraction,aconstanttempoof\nthe piece of music represented by the scanned sheet music\nisassumed.\nFor a detailed description on methods for CENS fea-\nture generation of audio recordings, we refer to the litera-\nture [10,12]. In principle, in our application the audio sig -\nnalistransformedintochromafeaturesbyusingshort-time\nFourier transformsincombination withbinning strategies .\nTo enable the generation of CENS features from video\nﬁles,theaudiotrackofthevideorecordingisextractedand\nthefeature computation for audio recordings isapplied.\n2.2 MusicSynchronization\nFigure 1 gives an example of the alignment procedure for\nthe scanned sheet music and a video recording of the ﬁrst\nmeasuresofthe 2ndmovementofLiszt’sFaustSymphony.\nAs described before, the ﬁrst step for the synchronization\noftwomusicdocumentsis,toconvertbothintoacommon\nandmeaningfulfeaturerepresentation. Basedonthesefea-\ntures, multiscale dynamic time warping techniques (Ms-\nDTW)areemployedtodeterminethesynchronizationsbe-\ntween music documents. The essential idea of MsDTW is\nto recursively compute alignment paths for coarse feature\nresolutions and project them to the next higher resolution\nlevel, where they subsequently are reﬁned. Further details\nonthe MsDTW method are available, e.g., in[14,15].\nDuring the described synchronization, we assume that\nthe music documents match with respect to their musical\nstructure so that only local and global tempo-variations\nneed to be considered. In Section 3 we go into details on\nhow to deal with structural differences during score-audio\nand score-video alignment. For information on synchro-\nnizationofstructurallydifferingaudiorecordings,see[ 16].\n3. SEMI-AUTOMATIC DATA PROCESSING\nTo allow for a fast and user-friendly generation of all data\nﬁlesusedbytheS LAVEsystem(Section4),theautomation\nof all required computing steps is desirable. As a ﬁrst step\n71810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 2. TheContentCreator interface forsemi-automatic data processing.\ntowardsfullautomation,wepresentthe ContentCreator in-\nterface,seeFigure2,forthegenerationofsynchronizatio n\ninformation,indexingstructures,andfurtherdatatypesf or\nmusic collections consisting of score scans, audio record-\nings, and videos.\nTheContentCreator interfaceaimsatprovidinganintu-\nitiveGUItosupporttheprocesschainrequiredtogenerate\nall metadata used by the S LAVEsystem. Within this con-\ntext, metadata refers to data ﬁles containing information\nthat ranges from indexing and synchronization structures\nto score and CD cover images used by S LAVEfor visual-\nizationpurposes. AsshowninFigure2,theworkﬂowisdi-\nvidedintoseveralinterdependentstages. Thegenerationo f\ntheresultsforeachstagecanbetriggeredindividually. Th e\nintegratedarrowshelptoclarifythedependenciesbetween\nthe various stages and also display the different paths for\nthegenerationofmetadata. Duetotheselecteddivisionof\ntheworkﬂowintoindividuallytriggeredstages,themanual\nmanipulationofintermediateﬁlesbeforecontinuingtothe\nnextstageispossible. Atthemoment,somestagesmerely\ngeneratedefaultor,duetotheerror-proneOMR,erroneous\ndataﬁles. Atthesepoints,amanualreworkisessentialfor\nasuccessfulworkﬂow. Furtherdetailsonthisissuearead-\ndressed inSection 3.2.\nDuring the usage of the ContentCreator , three differenttypesofdataﬁlescanbedistinguished. Theinputﬁles(la-\nbel1in Figure 2) mark the starting point of the process\nchain and currently consist of scanned pages of a music\nbook, audio recordings, and videos. The second are the\nintermediate data types (labels 2−6). These ﬁles are re-\nquired for the process chain but do not contain informa-\ntion directly used by the S LAVEsystem. By contrast, the\nlast type of ﬁles (labels 7−9and unlabeled boxes) con-\ntainsmetadata. Theoutputstagesnotfurthermentionedin\nthispapergeneratemetadataliketexturedataforrenderin g\nthe sheet music pages, information on length and name of\ntheaudioandvideotracks,anddatastructuresforcontent-\nbased retrieval.\nMethods to save and restore the current state as well as\nthe possibility to export the created metadata for runtime\nusage with S LAVEareprovided.\n3.1 Workﬂow forsemi-automatic musicalignment\nIn the following, we describe the chain of stages involved\nin the process of time-aligning a music book to various\ninterpretations (video and audio).\nAs ﬁrst step, the input data needs to be selected and\nloaded to the application (label 1). TheContentCreator\ninterface enables the management of arbitrary numbers of\naudio and video interpretations of a piece of music (right-\n719Oral Session 8: Lyrics\nhand box) and the synchronization of these interpretations\nto a scanned music book representing the same piece of\nmusic(left-hand box).\nToextractthescoreinformationfromthescannedscore\npages, OMR is performed (label 2) and the resulting data\nare subsequently merged into a single SymbolicScore ﬁle\n(label3). Thisﬁletherebycontainsvariousmusicinforma-\ntion such as note events, key signatures, time signatures,\nstaff information, accidentals, and information on instru -\nmentationandtransposition,requiredforthegenerationo f\nchroma-based features.\nWhenaimingatthealignmentofawholemusicbookto\na set of video or audio recordings, the individual scanned\npages of the music book need to be related to the differ-\nent tracks of the music book. Therefore in the next step, a\nﬁle containing information on the tracks contained in the\nmusic book is generated (label 4). Besides the musical in-\nformation stored in the SymbolicScore ﬁles, generated in\nstage3, a score also contains information on repetitions\nand jumps. This data is extracted from the OMR output\nand saved in the next stage (label 5). Together with the\njump information of the video and audio interpretations,\nthis data contributes fundamentally to the success of the\nsynchronizationprocess. OngeneratingtheCENSfeatures\nof the scanned sheet music, the given jump information\nis employed to ensure structural accordance with the re-\nspective features of the video and audio recordings. Sub-\nsequently,theCENSfeaturesofallloadedaudioandvideo\nﬁles are generated as described inSection 2.1(label 6).\nPrior to the execution of the synchronization, there are\ntwo more steps to be conducted. After splitting the mu-\nsic book into various tracks in stage 4, these need to be\nmapped to the audio and video ﬁles of each loaded inter-\npretation(label 7). Currently,manualreworkisrequired,if\nthe order of the videos or the audio tracks of the interpre-\ntation does not coincide with the track order of the music\nbook. There are already proposals for a feature based, au-\ntomatable creation of mapping information, which can be\nintegrated into the ContentCreator interface, see [1]. Fi-\nnally, the jump and repetition instructions extracted from\nthe score scans might not be consistent with the repeti-\ntions and jumps actually performed in the speciﬁc audio\nor video recording. Therefore, the last stage (label 8) be-\nfore the synchronization consists of the generation of this\ndata for allloaded video and audio interpretations.\nAfter passing all stages described before, the synchro-\nnization information for the music book and the loaded\nvideoandaudiointerpretationsarecomputed(label 9),us-\ning the MsDTW approach mentioned in Section 2.2. The\nCENS features of the music book are computed on de-\nmand, considering the structural information of the audio\norvideotrackusedforthecurrentsynchronizationprocess .\n3.2 Reworks during theworkﬂow\nTheindividualstagesofthepresentedworkﬂowareautom-\natized as far as possible. For stages, where currently no\nadequate computational methods exist to enable automa-\ntion, default data ﬁles, which need to be reworked by the\nuser, are generated. The user can modify those data ﬁlesor can import previously generated data ﬁles into the cur-\nrentlyreworkedstage. Atthemoment,partsofthe Symbol-\nicScoreﬁle(transpositioninformationforthecontainedin-\nstruments,label 3)andtheinterpretationspeciﬁcjumpand\nrepetition information for video and audio recordings (la-\nbel8) might need manual correction. In addition, in stage\n3,4,5, and7manual adjustments might be required for\ncomplex pieces of musicor low qualitymusic book scans.\nTo extend the workﬂow managed by the ContentCre-\natortolargermusiccollections,theappliedtechniquesare\ncurrentlyintegratedintothe P ROBADO libraryservicesys-\ntem buildup at theBavarian State Library, see [1].\n4. THESLAVE SYSTEM\nRecently, various computer tools were created to enable\nthemanagementandpresentationofmultimediamusiccol-\nlections. However,sofarthosetoolsmostlyconcentrateon\nsheetmusicandaudiorecordings. Inthissection,wewant\ntopresenttheS LAVEsystem,whichaimsatauser-friendly\nand holisticexploration of musicinamultimodal manner.\nSLAVEisbaseduponthe SyncPlayer system,presented\nin [7]. The SyncPlayer offers – besides basic audio player\nfunctionalities–thepossibilityofaddingplug-insformu l-\ntimodal music presentation and audio analysis (e.g., a\nplug-inforthevisualizationofthemusicalstructureofth e\ncurrent piece of music). S LAVEprovides a renewed GUI\nand includes some of the techniques already available in\ntheSyncPlayer as well as some new features. The new\nframework is envisioned as user interface for the library\nservice system set up at the Bavarian State Library as part\nof the P ROBADO project. First system developements to-\nwards S LAVEwererecently presented in[1].\nThe framework consists of several user interfaces for\nmultimodal music presentation, navigation and content-\nbased retrieval. The central component is the ScoreViewer\ninterface shown in Figure 3, which offers the visualiza-\ntion of the scanned pages of the underlying music book.\nWhen audio playback is started, the corresponding mea-\nsures within the sheet music are highlighted based on the\nsynchronization information created by the ContentCre-\natorsystem described in Section 3. Some additional fea-\ntures of the ScoreViewer are automatic page turning dur-\ning playback, navigation within the music book, and user-\nfriendly music retrieval based on the query-by-example\nparadigm. The latter is implemented by enabling the user\nto select a region within the sheet music using the mouse\npointer. The issued query is processed by determining the\ncorresponding audio clipping of the currently active in-\nterpretation and performing content-based music retrieva l.\nFor details on the employed matching and indexing tech-\nniques, we referto[17].\nThere might exist several interpretations of the same\npieceofmusic,whichmatchtothegivenmusicbook. The\nnameofthecurrentlyactiveaudiointerpretation,aswella s\nan icon showing a corresponding CD cover are displayed\nin the upper left corner of the ScoreViewer interface. To\nseamlessly switch to a different interpretation, a list con -\ntaining information on all loaded interpretations is avail -\nable byclicking onthe current cover icon.\n72010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure3. TheScoreViewer interfaceformultimodalmusic\npresentation and navigation. During video or audio play-\nback the corresponding musical measures within the sheet\nmusicarehighlighted. Asmoothchangebetweendifferent\ninterpretations of apiece of musicispossible.\nFigure 4. TheVideoViewer shows the currently played\nvideoofthechosenvideointerpretationandallowsbrows-\ningwithinthevideoaswellaswithinthelistofvideoﬁles\nof theinterpretation.\nTo include music videos, we added the VideoViewer in-\nterface which offers basic video player functionalities, s ee\nFigure 4. As for the audio recordings, during video play-\nback, the corresponding measures of the sheet music are\nhighlighted. The smooth change between different video\nand audio interpretations of the piece of music via the\nScoreViewer interface enables the comparison of different\nmusicdocumenttypes. Furthermorethecontent-basedmu-\nsicquerywasextendedtoallowtheusageofvideoextracts\nasqueriesandtoincludevideointerpretationsinthesearc h\nindices and resultlists.\nAlthough, simultaneously looking at both the Video-\nand theScoreViewer might be hard for the user, having\na time aligned view on the score constitutes several ad-\nvantages. In longer video recordings, it might be cumber-\nsometosearchforaspeciﬁcpointintimewithinthevideo,\nwhereas using the score for navigation is easier and faster.\nFurthermore with respect to conductings of classical mu-\nsic,itmightbeofinteresttocomparerecordingsofseveral\ndifferent conductors at the same musical position. Using\nthecapabilityofsmoothswitchingbetweeninterpretation sorperformingacontent-basedqueryusingthesheetmusic\ncan help tofacilitatethesetasks.\n4.1 Further Extensions\nIn this section we want to give a preview on further func-\ntionalitiesandinterfaceswhichwillbeaddedtoS LAVEfor\naholisticmusicpresentation.\n4.1.1 LyricsViewer\nCurrently, S LAVEenables the combined presentation of\nscanned music books, audio recordings and videos. How-\never, text documents like libretti of operas and lyrics of\nsongcyclesareadditionalingredientsofdigitalmusiccol -\nlections. Therefore, our current work aims at the integra-\ntion of a LyricsViewer . Foundations for this development\nare the previously introduced Karaoke Display and Lyrics\nSeeker of our SyncPlayer system [18]. As for the Score-\nViewer, the currently vocalized words of the song text will\nbe highlighted during video or audio playback. Addition-\nally, search mechanisms based on the lyrics will be sup-\nported bythe LyricsViewer interface.\n4.1.2 InterpretationSwitcher\nIn addition to S LAVE, we enhanced the SyncPlayer sys-\ntem [7] (see Figure 5), which is basically a predecessor of\nour new system, tosupport video documents.\nFirst, the audio player component of the SyncPlayer\nwas modiﬁed to allow for the playback of video ﬁles and\nfor the inclusion of videos into playlists. Additionally we\nimplemented the InterpretationSwitcher plug-in which is\nbasically an extension of the AudioSwitcher plug-in [7].\nTheInterpretationSwitcher offers the possibility to switch\nbetweendifferentaudioandnewlyvideointerpretationsof\napieceofmusicduringplayback. Onchangingtoadiffer-\nentinterpretationthecurrentplaybackpositioninthepie ce\nofmusicisretainedandtheplaybackseamlesslycontinues\nwithinthechosen interpretation.\nFigure 5 shows an example of two audio interpreta-\ntions and one video of the second movement of Liszt’s\nFaust Symphony. The sliders enable the user to change\nto an arbitrary playback position within one of the inter-\npretations. The playback symbols to the left of the sliders\nmark, which interpretation is currently playing and enable\nsmooth switching between the interpretations. First steps\ntowardstheintegrationofsimilarfunctionalitiesto S LAVE\narepresented in[19].\n4.2 Applications of SLAVE and theVideoViewer\nTheavailabilityofvideodocumentsintheframeworkpre-\nsented in this paper offers several advantages for musi-\ncians, musicologists, music lovers, and others. As men-\ntioned before, prospective conductors might be able to use\ntheproposedsystemtocomparetheworkofdifferentcon-\nductors within the same piece of music. Looking at more\ncomplex pieces of music – as orchestral works – compar-\ning the orchestration and the arrangement of the orchestra\nmightbeofinterest. Thinkingofoperasevenstagedesign-\ners, make-up artists and costume designers might have an\n721Oral Session 8: Lyrics\nFigure 5.SyncPlayer withInterpretationSwitcher plug-in\nand video player. The InterpretationSwitcher enables the\nselection of several video or audio interpretations of the\nsame piece of music. Using the sliders and the playback\nsymbols onthe left,theuser can smoothly switchbetween\nand browse withinthe interpretations.\ninterest to compare different stagings. The possibility of\nsmooth changes between various interpretations and score\nbased navigation offers thereby signiﬁcant support. Fur-\nthermore,lookingattheareaofdance,choreographersand\ndance theorists might beneﬁt fromthese tools aswell.\nWethereforehopetoexperiencegreatacceptanceofthe\nnewly integrated video capabilities of our framework by\nthe various target groups.\n5. CONCLUSIONS\nInthispaper,wereportedonanewuserinterfaceforsemi-\nautomaticprocessingofvideo,audioandscorecollections\nto generate synchronization information, indexing struc-\ntures and metadata required for a holistic presentation of\nthese heterogeneous music collections. We also presented\nthemultimodalmusicmanagementframework S LAVEand\nthe inclusion of music videos as further music documenttype. Especially, we introduced the possibility of video\nplayback and simultaneous scorehighlighting.\nBesides the extensions described in this paper, the de-\nvelopment of new functionalities and interfaces especiall y\nforvideodocumentsareenvisioned,e.g.,afterthesynchro -\nnization of various videos, during the playback of one ref-\nerence video, the other sources can be shown time aligned\ntothisvideo. Forplayback,onlytheaudiotrackoftheref-\nerenceisused. Thistypeofapplicationwillenableamore\nconvenient and direct comparison of video interpretations .\n6. REFERENCES\n[1] F.Kurth,D.Damm,C.Fremerey,M.M ¨uller,M.Clausen: “A\nFramework for Managing Multimodal Digitized Music Col-\nlections,” Proc. ECDL,Aarhus,Denmark , 2008.\n[2] A. D’Aguanno, G. Vercellesi: “Automatic Music Synchro-\nnization Using Partial Score Representation Based on IEEE\n1599,”JournalofMultimedia ,Vol.4,No.1,pp.19-24,2009.\n[3] A. Klapuri, M. Davy: Signal Processing Methods for Music\nTranscription .Springer, New York, 2006.\n[4] B. Pardo: “Music Information Retrieval,” Special Issue,\nCommun. ACM ,Vol. 49, No.8, pp.28-58,2006.\n[5] BMWi: “C ONTENTUS ,”http://theseus-programm.de/en-\nus/theseus-application-scenarios/contentus/default.aspx .\n[6] C. Landone, J. Harrop, J. Reiss: “Enabling Access to Sound\nArchives through Integration, Enrichment and Retrieval: the\nEASAIER Project,” Proc.ISMIR,Vienna, Austria ,2007.\n[7] C. Fremerey, F. Kurth, M. M ¨uller, M. Clausen: “A Demon-\nstration of the SyncPlayer System,” Proc. ISMIR, Vienna,\nAustria,2007.\n[8] A. Barat, L. A. Ludovico: “Advanced Interfaces for Music\nEnjoyment,” Proc.AVI,Napoli,Italy , 2008.\n[9] J. W. Dunn, D. Byrd, M. Notess, J. Riley, R. Scherle: “Vari-\nations2: Retrieving and Using Music in an Academic Set-\ntings,”Special Issue, Commun. ACM , Vol. 49, No. 8, pp. 53-\n58, 2006.\n[10] M. A. Bartsch, G. H. Wakeﬁeld: “Audio Thumbnailing of\nPopular Music using Chroma-based Representations,” IEEE\nTrans.onMultimedia , Vol. 7, No.1,pp. 96-104,2005.\n[11] N. Hu, R. Dannenberg, G. Tzanetakis: “Polyphonic Audio\nMatching and Alignment for Music Retrieval,” Proc. IEEE\nWASPAA,NewPaltz,NY ,2003.\n[12] M. M ¨uller:Information Retrieval for Music and Motion .\nSpringer, Berlin,2007.\n[13] F. Kurth, M. M ¨uller, C. Fremerey, Y. Chang, M. Clausen:\n“Automated Synchronization of Scanned Sheet Music with\nAudio Recordings,” Proc.ISMIR,Vienna, Austria ,2007.\n[14] S. Salvador, P. Chan: “FastDTW: Toward Accurate Dynamic\nTime Warping in Linear Time and Space,” Proc. KDD Work-\nshopon MiningTemporal andSequential Data ,2004.\n[15] M. M ¨uller, H. Mattes, F. Kurth: “An Efﬁcient Multiscale\nApproach to Audio Synchronization,” Proc. ISMIR,Victoria,\nCDN,2006.\n[16] M. M ¨uller, S. Ewert: “Joint Structure Analysis with Appli-\ncations to Music Annotation and Synchronization,” Proc. IS-\nMIR,Philadelphia, USA ,2008.\n[17] F. Kurth, M. M ¨uller: “Efﬁcient Index-based Audio Match-\ning,”IEEE Transactions on Audio, Speech, and Language\nProcessing ,Vol. 16,No. 2,pp. 382-395,2008.\n[18] M. M ¨uller, F. Kurth, D. Damm, C. Fremerey, M. Clausen:\n“Lyrics-basedAudioRetrievalandMultimodalNavigationin\nMusicCollections,” Proc.ECDL,Budapest, Hungary ,2007.\n[19] D. Damm, C. Fremerey, F. Kurth, M. M ¨uller, M. Clausen:\n“Multimodal Presentation and Browsing of Music,” Proc.\nICMI,Chania, Greece , 2008.\n722"
    },
    {
        "title": "Additions and Improvements in the ACE 2.0 Music Classifier.",
        "author": [
            "Jessica Thompson 0001",
            "Cory McKay",
            "John Ashley Burgoyne",
            "Ichiro Fujinaga"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416048",
        "url": "https://doi.org/10.5281/zenodo.1416048",
        "ee": "https://zenodo.org/records/1416048/files/ThompsonMBF09.pdf",
        "abstract": "This paper presents additions and improvements to the Autonomous Classification Engine (ACE), a framework for using and optimizing classifiers. Given a set of feature values, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality-reduction techniques in order to arrive at a configuration that is well-suited to a given problem. Changes and additions have been made to ACE in order to increase its functionality as well as to make it easier to use and incorporate into other software frameworks. Details are provided on ACE’s remodeled class structure and associated API, the improved command line and graphical user interfaces, a new ACE XML 2.0 ZIP file format and expanded statistical reporting associated with cross validation. The resulting improved processing and methods of operation are also discussed.",
        "zenodo_id": 1416048,
        "dblp_key": "conf/ismir/ThompsonMBF09",
        "keywords": [
            "ACE",
            "framework",
            "classifier",
            "dimensionality-reduction",
            "configuration",
            "problem",
            "functionality",
            "software",
            "framework",
            "statistical reporting"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   ADDITIONS AND IMPROVEMENTS TO THE ACE 2.0 MUSIC CLASSIFIER Jessica Thompson Cory McKay John Ashley Burgoyne Ichiro Fujinaga Music Technology McGill University jessica.thompson@ mail.mcgill.ca CIRMMT McGill University cory.mckay@ mail.mcgill.ca CIRMMT McGill University ashley@        music.mcgill.ca CIRMMT McGill University ich@          music.mcgill.ca ABSTRACT This paper presents additions and improvements to the Autonomous Classification Engine (ACE), a framework for using and optimizing classifiers. Given a set of feature values, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimen-sionality-reduction techniques in order to arrive at a con-figuration that is well-suited to a given problem. Changes and additions have been made to ACE in order to in-crease its functionality as well as to make it easier to use and incorporate into other software frameworks. Details are provided on ACE’s remodeled class structure and associated API, the improved command line and graphi-cal user interfaces, a new ACE XML 2.0 ZIP file format and expanded statistical reporting associated with cross validation. The resulting improved processing and meth-ods of operation are also discussed. 1. INTRODUCTION Automatic classification techniques play an essential role in many music information retrieval (MIR) research ar-eas. These include genre classification, mood classifica-tion, music recommendation, performer identification, composer identification, and instrument identification, to name just a few. Classification software especially adapted to MIR can be of significant benefit. Some im-portant work has already been done in this area, as noted in Section 2. ACE, which is part of the jMIR software suite described below, is a framework that builds upon these systems and adds additional functionality that is generally lacking in both systems designed specifically for music classification as well as general classification systems. ACE 2.0, which is presented in this paper, has been significantly improved since the release in 2005 of ACE 1.1 [1]. Improvements include an entirely restruc-tured and simplified API, a significantly improved com-mand-line interface, a new GUI, new file formats, im-proved processing and significantly expanded statistical reporting.   1.1 jMIR jMIR [2] is a suite of software tools developed for use in MIR research. The jMIR components can be used either independently or as an integrated suite. Although the components can read and write to common file formats such as Weka ARFF, jMIR also uses its own ACE XML file formats that offer a number of advantages over alter-native data-mining formats [1, 3]. jMIR was designed to provide: • a flexible set of tools that can easily be applied to a wide variety of MIR-oriented research tasks; • a platform that can be used to combine research on symbolic, audio and/or cultural data; • easy-to-use and accessible software with a minimal learning curve that can be used by researchers with little or no technological training; • a modular and extensible framework for iteratively developing and sharing new feature extraction and classification technologies; and • software that encourages collaboration between dif-ferent research centers by facilitating the sharing of research data using powerful and flexible file for-mats [4]. jMIR is the only existing software suite that combines a meta-learning component (ACE) into an integrated framework with three different types of musical feature extractors, a metadata correction tool, and ground truth data. jMIR is also the only unified MIR research frame-work that combines all three of symbolic, audio, and cul-tural features. 1.2 ACE XML ACE XML [1, 3] is a set of file formats developed to enable communication between the various jMIR soft-ware components, including ACE. These file formats have been designed to be very flexible and expressive. It is hoped that the MIR research community will eventu-ally adopt them as multi-purpose standardized formats, beyond the limited scope of jMIR. ACE XML has re-cently been significantly revised and expanded in order to help make this possible [3]. 1.3 ACE  ACE is a meta-learning classification system that can automatically experiment with a variety of different di-mensionality-reduction and machine-learning algorithms  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. © 2009 International Society for Music Information Retrieval  \n435Poster Session 3\n   in order to evaluate which ones are best suited to particu-lar problems. ACE can also be used as a simple automatic classification system. ACE is open source and available for free. It is implemented entirely in Java in order to maximize portability. ACE is built on the standardized Weka machine-learning infrastructure [5] and makes direct use of a vari-ety of algorithms and data structures distributed with Weka. This means not only that new algorithms produced by the very active Weka community can be incorporated into ACE immediately, but also that new algorithms spe-cifically designed for MIR research can be developed using the Weka framework. ACE can read features stored in either ACE XML or Weka ARFF files. Two Weka data structures of particular interest that are used by ACE and referred to in this paper are the Weka Instances object, which stores a set of instances in a representation similar to the Weka ARFF file, and the Weka Classifer object, which classifies a set of Weka Instances with a specified classification algorithm.  2. RELATED WORK There are a number of existing software packages that are often used for machine learning, including Weka [5], PRTools [6] and several other MATLAB [7] tool-boxes. There are also several systems that offer meta-learning functionality, including RapidMiner (formerly Yale) [8] and METAL [9]. All of these are general pur-pose systems, however, and do not meet some of the spe-cial needs of MIR, as discussed in [1]. ACE and ACE XML make it possible to represent and use types of in-formation that are particularly relevant to MIR but are not expressible or usable in most alternative systems. For example, jMIR and ACE XML have the ability to: • maintain logical groupings between multi-dimen-sional features; • represent class labels and feature values for poten-tially overlapping sub-sections of instances as well as for instances as a whole; • represent structured class ontologies; and • associate multiple classes with a single instance.  There are also several high-quality toolsets that have been designed specifically for MIR, but they tend to offer less sophisticated processing specifically with respect to machine learning. MIRtoolbox [10] is a powerful modu-lar MATLAB toolbox for designing and extracting audio features. The well-known CLAM [11] and Marsyas [12] focus on audio-related tasks. The M2K [13] graphical patching interface can be used to connect a range of dif-ferent MIR processing components in ways that can take advantage of distributed processing.  3. IMPROVEMENTS NEW TO ACE 2.0 3.1 Architectural Restructuring ACE’s class structure has been redesigned to be more flexible, extensible, and easy to understand. This redesign is intended to facilitate integration with other software. ACE’s main functionality is accessed through an inter-face class called Coordinator. Figure 1 illustrates this organization: the GUI, command-line interface, and ex-ternal software all only directly access this new Coordi-nator class, which then communicates with ACE’s proc-essing classes. This organization ensures that all processing is performed identically, regardless of the source of the request, and makes ACE easier to use. New users wishing to use the ACE API need only understand the Coordinator class in order to be able to use all of ACE’s functionality. The remainder of this section pre-sents ACE’s main classes. \nFigure 1. Structure of ACE’s main processing classes. The Coordinator class provides an ex-clusive interface through which ACE’s main func-tionality can be accessed. Arrows indicate interac-tions between classes. All public methods are listed, but parameters are omitted from method declarations to save space. \n43610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   • Coordinator: The class provides the interface through which ACE’s training, classification, cross validation, and experimentation functionality can be accessed. Only Coordinator calls the classes listed below; all other sources need only call the appropri-ate methods in Coordinator to access the func-tionality of all other processing classes. Loading and preparation of instances as well as dimensionality reduction is performed in this class prior to passing instances to processing classes. • Trainer: Trains a specified type of Weka Classi-fier based on the given training instances. The trained Weka Classifier is stored and saved in an ACE TrainedModel object. • InstanceClassifier: Classifies a set of instances using a trained Weka Classifier. This class reads the TrainedModel object from a specified file and uses it to classify the given instances. In the context of cross validation, a classified Weka Instances object is returned. Classifications can be written to a Weka ARFF file or an ACE XML Instance Label file. • DimensionalityReducer: Reduces the dimen-sionality of the features extracted from a set of in-stances. This class is called by the Coordinator class to reduce the dimensionality of the training data prior to training and cross-validation in order to help avoid the “curse of dimensionality.” This class is also called by the Experimenter class to create an array of multiple dimensionality-reduced versions of an original set of instances. • CrossValidator: Cross-validates the given in-stances with the specified type of Weka Classi-fier using the specified number of partitions. In-stances are partitioned randomly into training and testing data for each fold. CrossValidator makes calls to the Trainer and InstanceClassifier classes to evaluate the performance of a specific classification approach. The specified type of Weka Classifier is trained on the remaining training data and tested on the testing data for each partition. Statistics are stored for each partition and used to generate performance reports that provide much more statistical detail than Weka itself provides. • Experimenter: Tests to find the best performing classification approach by making repeated calls to the CrossValidator class using different parame-ters each time. Different types of classifiers are tested with different types of dimensionality reduc-tion. Experimenter calls Dimensionali-tyReducer to get an array of Weka Instances ob-jects, wherein each cell contains a different dimen-sionality-reduced version of the original instances.  Each type of classifier is cross-validated with each set of dimensionality-reduced instances. A summary of the results for each cross-validation ex-periment for each dimensionality-reduction experi-ment is generated, as well as more detailed results when requested by the user. After the best classifica-tion methodology has been selected, validation is performed using a publication set put aside at the beginning of the experiment. A new Weka Classi-fier of the chosen type is created and trained on the chosen type of dimensionality-reduced instances (all instances are now available for use as training data, except for the publication set). The newly trained Weka Classifier is tested on the publication set and the results are saved. 3.2 Redesigned Cross Validation ACE performs full meta-learning with training, testing, and publication data sets. Previously, cross-validation was performed using the Weka API, but now, ACE im-plements its own cross-validation that improves upon Weka’s. This new implementation, contained in the CrossValidator class, includes output of additional statistics and more transparent data processing. Whereas previously Weka’s cross-validation only allowed access to overall correctness statistics and confusion matrices, ACE’s new implementation includes variances across partitions, individual instance classification results for each partition, confusion matrices for each partition, and data on running times.  3.3 ACE XML 2.0 ZIP and Project Files ACE XML, the file format used to transmit information between the jMIR components, consists of four different file types for storing, respectively, extracted feature val-ues, feature metadata, labeled instances and class ontolo-gies. Although the separation of this data into four differ-ent types of files does have significant advantages [4], large projects consisting of multiple files can become unwieldy. The new ACE XML 2.0 Project and ZIP files present solutions to this problem. The Project file allows users to associate ACE XML files together so that they may be automatically saved or loaded together, and the ZIP for-mat makes it possible to package all files referred to in a Project file into a single compressed ZIP file.  The ACE XML ZIP file is implemented using an ACE XML Project file and a hidden text file with the extension “.sp”. This file contains only one line of text that specifies the name of the single ACE XML Project file compressed within the ACE XML ZIP file. When ACE parses an ACE XML ZIP file, it looks for the .sp file first, and then parses the associated ACE XML Pro-ject file so that the other contents of the ACE XML ZIP file can be properly interpreted. \n437Poster Session 3\n   ACE includes utilities for creating, accessing, and managing ACE XML ZIP files. When ACE unzips an ACE XML ZIP file, it rewrites the ACE XML Project file to reflect the new path names of the newly unzipped files. ACE can also extract or add a single file from/to an ACE XML ZIP file. An ACE XML ZIP file can be used to load or save an ACE project via the ACE command-line interface, GUI or API.  3.4 Improved Command-Line Interface The previous ACE command-line interface has been en-tirely redesigned with clearer and more intuitive com-mands. The command-line interface of software such as ACE is particularly important, as it is often needed to perform batch processing that can last days or weeks. Running ACE from the command line has become easier with the addition of new functionality such as the ability to load an ACE project from an ACE XML Project file or an ACE ZIP file. The user can also now specify the type of classifier or dimensionality-reduction algorithm to be used as well as other options related to the distribution of datasets (e.g., randomization, maximum class member-ship, and maximum class spread). With the verbose op-tion, the user also has the option of printing a more de-tailed report of the performed processing. These im-provements to the command-line interface not only make ACE easier to use, but also provide more precise control of ACE’s processing. 3.5 Graphical User Interface ACE also now includes functionality for GUI-based viewing, editing, and saving of ACE XML files. This functionality is divided between three panes: the Taxon-omy pane, which displays the contents of an ACE XML Class Ontology file; the Features pane, which displays the contents of an ACE XML Feature Description file; and the Instances pane, which displays the combined contents of both ACE XML Feature Value files and ACE XML Instance Label files.  A screen shot of the Taxonomy pane is shown in Fig-ure 2. The displayed structure indicates a genre taxonomy for use in an automatic genre classification task. If an ACE XML Instance Label file is loaded without explic-itly specifying such a class ontology either manually or with an ACE XML Class Ontology file, then a flat ontol-ogy is automatically generated based on the labels used in the ACE XML Instance Label file, and is displayed in the Taxonomy pane. Figure 3 shows a Features pane display-ing a list of audio features. If an ACE XML Feature De-scriptions file is not loaded here prior to loading an ACE XML Feature Values file, feature descriptions are gener-ated automatically based on the features present in the ACE XML Feature Values file. Figure 4 shows how the Instances pane can be used to display class labels that have been associated with particular instances.   \n Figure 2. A sample genre ontology displayed in the Tax-onomy pane of the ACE GUI. \n Figure 3. The Features pane of the ACE GUI displaying metadata about a set of audio features. \n Figure 4. The Instances pane of the ACE GUI. Song ti-tles are associated in this example with particular genre \n43810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n   labels drawn from the genre ontology shown in Figure 2. Note that neither the Display Feature Values nor the Dis-play Misc Info checkboxes are checked, so only instance identifiers and classes are displayed in this particular ex-ample. Figures 5, 6, and 7 illustrate a more complex exam-ple. Figure 5 establishes a new class ontology, in this case a hierarchical music–speech–applause–silence discrimi-nator. Figure 6 shows how the Instances pane can display not only class labels, but also miscellaneous metadata. This figure also demonstrates that instances can be bro-ken into separately-labeled subsections and that each in-stance or subsection may be associated with multiple class labels. Start and stop times indicate the boundaries of the subsections. Figure 7 demonstrates how feature values can also be displayed using the Instances pane. It shows the same data as Figure 6, except that feature val-ues are displayed and miscellaneous metadata is not.  If feature arrays and class labels are loaded for the same subsection, all information for that instance is pre-sented in one row. The specific time of the overlap of class labels within a subsection is indicated within paren-theses after the class name. Subsection rows for any overall instance can be hidden by unchecking the Show Sections checkbox. The Composer and Note columns are metadata loaded from the particular Instance Label ACE XML file associated with this example and can be hidden by clicking on the Display Misc Info checkbox.  \n Figure 5. Another class ontology displayed in the Taxon-omy pane of the ACE GUI. \n Figure 6. Instances with subsections and metadata dis-played in the Instances pane of the ACE GUI. \n Figure 7. Instances with subsections and feature values displayed in the Instances pane of the ACE GUI. 4. CONCLUSION AND FUTURE WORK Improvements have been made to ACE since its original publication, including new capabilities that make it a more complete and easy-to-use meta-learning classifica-tion framework. ACE is an ongoing project, and further improvements will continue to be made. 4.1 Fully Functional GUI The ACE GUI currently serves as a tool for viewing and editing ACE XML files. It will eventually be possible to also use the GUI to perform experiments on data sets, as can currently only be done with the command-line inter-face or API. This functionality will be accessible from two currently unfinished panes: the Experimenter pane and the Preferences pane. The Experimenter pane will allow full access to all of ACE’s machine learning func-tionality. Several sub-panes will be used to display the same output that is printed or saved to files when running experiments from the ACE command-line interface. The Preferences pane of the ACE GUI will allow users to specify preferences related to both interface settings and machine learning parameters. User studies will also be performed in order to validate and improve the GUI de-sign. \n439Poster Session 3\n   4.2 Distributed Work Load Functionality is being built into ACE to allow it to run trials on multiple computers in parallel in order to reducte execution times. Once the distributed aspect of the system is complete, a server-based subsystem will be designed that contains a coordination system and database. Al-though not necessary for using ACE, users will be able to choose to dedicate computers to this server, allowing ACE to run continually. The server will keep a record of the performances of all ACE operations run on a particu-lar user’s cluster and generate statistics for self-evaluation and improvement. ACE will then make use of any idle time to attempt to improve solutions to previously en-countered but currently inactive problems. Ultimately, the user would only be required to specify the total time available (typically days or weeks) for ACE to run its experiments and everything else, including the choice of learning algorithms and their parameters, would be auto-matically determined by ACE. 4.3 Expanded Machine Learning Algorithms In the future, ACE will include learning schemes impor-tant to MIR that are currently missing from the Weka distribution, such as hidden Markov models and recurrent neural networks. Support for Weka’s unsupervised learn-ing functionality will also be incorporated. It would also be beneficial to include tools for constructing blackboard systems, in particular those that can integrate knowledge sources based on expert heuristics. Another potentially beneficial addition would be to implement modules for facilitating post-processing. All of these extensions would add to ACE’s flexibility and breadth of processing. 5. ACKNOWLEDGEMENTS The authors would like to thank the Andrew W. Mellon Foundation and the Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT) for their generous financial support, as well as the members of the Networked Environment for Music Analysis (NEMA) group for their valuable critiques and suggestions. 6. REFERENCES [1] McKay, C., R. Fiebrink, D. McEnnis, B. Li, and I. Fujinaga. 2005. ACE: A framework for optimizing music classification. Proceedings of the International Conference on Music Information Retrieval. 42–9. [2] McKay, C., and I. Fujinaga. 2009. jMIR: Tools for automatic music classification. Proceedings of the International Computer Music Conference [3] McKay, C., J. A. Burgoyne, J. Thompson, and I. Fujinaga. 2009. Using the ACE XML 2.0 file formats to store and share music classification data. Proceedings of the International Conference on Music Information Retrieval. [4] McKay, C., and I. Fujinaga. 2008. Combining features extracted from audio, symbolic and cultural sources. Proceedings of the International Conference on Music Information Retrieval, 597–602. [5] Witten, I., and E. Frank. 2005.  Data mining: Practical machine learning tools and techniques with Java implementations. San Francisco: Morgan Kaufmann. [6] van der Heijden, F., R. P. W. Duin, D. de Ridder, and D. M. J. Tax. 2004. Classification, parameter estimation and state estimation: An engineering approach using MATLAB. New York: Wiley. [7] MathWorks  2008. MATLAB version 7.6.0. Natick, Massachusetts: The MathWorks. [8] Mierswa, I., M. Wrust, R. Klinkenberg, M. Scholz, and T. Euler. 2006. YALE: Rapid prototyping for complex data mining tasks. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. [9] Brazdil, P., C. Soares, and J. Costa. 2003. Ranking learning algorithms. Machine Learning 50 (3): 251–77. [10] Lartillot, O., P. Toiviainen, and T. Eerola. 2008. A Matlab toolbox for music information retrieval. In Data analysis, machine learning and applications, ed. C. Preisach, H. Burkhardt, L. Schmidt-Thieme, and R. Decker. New York: Springer. 261–8. [11] Arumi, P., and X. Amatriain. 2005. CLAM: An ob-ject oriented framework for audio and music. Pro-ceedings of the International Linux Audio Confer-ence. [12] Tzanetakis, G., and P. Cook. 1999. MARSYAS: A framework for audio analysis. Organized Sound 4 (3): 169–75. [13] Downie, S., A. Ehmann, and D. Tcheng. 2005. Music-to-knowledge (M2K): A prototyping and evaluation environment for music information retrieval research. Proceedings of the 28th International ACM SIGIR Conference on Research and Development in Information Retrieval, 676.  \n440"
    },
    {
        "title": "Publishing Music Similarity Features on the Semantic Web.",
        "author": [
            "Dan Tidhar",
            "György Fazekas",
            "Sefki Kolozali",
            "Mark B. Sandler"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417071",
        "url": "https://doi.org/10.5281/zenodo.1417071",
        "ee": "https://zenodo.org/records/1417071/files/TidharFKS09.pdf",
        "abstract": "We describe the process of collecting, organising and publishing a large set of music similarity features produced by the SoundBite [10] playlist generator tool. These data can be a valuable asset in the development and evaluation of new Music Information Retrieval algorithms. They can also be used in Web-based music search and retrieval applications. For this reason, we make a database of features available on the Semantic Web via a SPARQL end-point, which can be used in Linked Data services. We provide examples of using the data in a research tool, as well as in a simple web application which responds to audio queries and finds a set of similar tracks in our database.",
        "zenodo_id": 1417071,
        "dblp_key": "conf/ismir/TidharFKS09",
        "keywords": [
            "Music similarity features",
            "SoundBite playlist generator",
            "Semantic Web",
            "SPARQL end-point",
            "Linked Data services",
            "Web-based music search",
            "Music Information Retrieval algorithms",
            "Research tool",
            "Audio queries",
            "Similar tracks"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nPUBLISHING MUSIC SIMILARITY FEATURES ON THE SEMANTIC WEB\nDan Tidhar, György Fazekas, Sefki Kolozali, Mark Sandler\nCentre for Digital Music\nQueen Mary, University of London\nMile End Road, London E1, UK\n{dan.tidhar, gyorgy.fazekas, sefki.kolozali, mark.sandler}@elec.qmul.ac.uk\nABSTRACT\nWe describe the process of collecting, organising and pub-\nlishing a large set of music similarity features produced\nby the SoundBite [10] playlist generator tool. These data\ncan be a valuable asset in the development and evaluation\nof new Music Information Retrieval algorithms. They can\nalso be used in Web-based music search and retrieval ap-\nplications. For this reason, we make a database of features\navailable on the Semantic Web via a SPARQL end-point,\nwhich can be used in Linked Data services. We provide\nexamples of using the data in a research tool, as well as in\na simple web application which responds to audio queries\nand ﬁnds a set of similar tracks in our database.\n1. INTRODUCTION\nSimilarity-based retrieval is an important subject area in\nmusic information research. Yet, researchers working in\nthis ﬁeld are often limited by the unavailability of large\naudio collections, copyright restrictions, and even more\noften, unreliable metadata associated with songs in a par-\nticular music database or personal library. This paper de-\nscribes a system for collecting and publishing music sim-\nilarity features from a large user base coupled with valu-\nable editorial metadata. Metadata are veriﬁed against Mu-\nsicBrainz,1a large public database of editorial informa-\ntion on the Web, and published together with the match-\ning similarity features on the Semantic Web [1]. We ex-\nplore some research opportunities opened by the system,\nand describe SAWA2recommender, a sample web appli-\ncation which demonstrates how the published data can be\nused. Rather than describing a music recommender in de-\ntail, our primary motivation is in making high quality data\navailable for similarity and recommendation research in a\nstandardised way.\n1http://www.MusicBrainz.org/\n2SAWA stands for Sonic Annotator Web Application. A search and\nrecommendation system built on SAWA and the SoundBite data set is\navailable at: http://www.isophonics.net/sawa/rec\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.The heart of the data collection system is SoundBite\n[10] [15], a tool for similarity-based automatic playlist gen-\neration. Soundbite is available as an iTunes plugin, and is\ncurrently being implemented as a plugin for other audio\nplayers as well. Once installed, it extracts features from\nthe user’s entire audio collection and stores them for fu-\nture similarity calculations. It can then generate playlists\nconsisting of nmost similar tracks to any given seed track\nspeciﬁed by the user. The similarity data currently con-\nsists of 40 values per track, based on the distribution of\nMel-Fequency Cepstral Coefﬁcients (MFCC) as described\nin [10]. The extracted features are also reported to a central\nserver, where they become part of the so called Isophone\ndatabase. This database is used for aggregating informa-\ntion from SoundBite clients, consisting of editorial meta-\ndata and similarity features for each audio track. The entire\nsystem may therefore be regarded as a distributed frame-\nwork for similarity feature extraction. The accumulated\ndata can be valuable to the research community, and may\nalso be used by other audio similarity and recommenda-\ntion systems. In order to facilitate such usage, we publish\na cleaned-up portion of the data on the Semantic Web.\nThe rest of the paper is organised as follows: In sec-\ntion two, we provide brief explanations of some of the key\nterms relevant to the technologies we use. In section three,\nwe describe the published data set, the collection system\narchitecture, the data clean-up process, and the way re-\nsearchers as well as Semantic Web applications can access\nthe data using a SPARQL end-point3. Finally, in section\nfour, we describe our prototype recommender, a publicly\naccessible web application based on this data set.\n2. LINKED DATA AND THE SEMANTIC WEB\nBuilding the Semantic Web involves creating a machine-\ninterpretable web of data in parallel to the existing web\nof documents [1]. By uniformly integrating diverse data\nand services, it aims to enable applications which would\nbe difﬁcult, if not impossible, to build using prevailing in-\ncompatible interfaces and representation formats. An ex-\nample application from the world of music would interlink\ncontent providers (music labels, music sellers, online radio\nstations), meta-databases holding musical and artists infor-\n3A web resource that responds to queries using the SPARQL Protocol\nand RDF Query Language, an SQL-like language for accessing RDF [9]\ndata bases.\n447Poster Session 3\nmation, semantic audio tools and music identiﬁcation ser-\nvices, and perhaps even music collections held on personal\ncomputers. This could revolutionise the way we access or\ndiscover new music. However, creating such a distributed\nnetwork requires that all data sources speak the same lan-\nguage, i.e., are governed by a common schema.\nBecause of the diverse and unbounded nature of infor-\nmation on the general Web (and we believe that musical\ninformation is just as diverse), a major challenge was set\nforth to Semantic Web developers: How to design a stan-\ndard, extensible schema for representing information en-\ncompassing a wide range of human knowledge? The Se-\nmantic Web’s answer to this apparently complex and cir-\ncular problem is in specifying how information is pub-\nlished, rather than trying to arrange everything into rigid\ndata structures.\n2.1 Semantic Web Technologies\nThe key concepts and technologies enabling the develop-\nment of the Semantic Web are the Resource Description\nFramework (RDF) [9], Semantic Web ontologies, and RDF\nquery languages.\nRDF is a conceptual data model. It provides the ﬂexi-\nbility and modularity required for publishing diverse semi-\nstructured data — that is, just about anything on the Se-\nmantic Web. It is based on the simple idea of expressing\nstatements in the form of subject — predicate — object .\nElements of these statements are literals , and resources\nnamed by Uniform Resource Identiﬁers (URI). This pro-\nvides the model with an unambiguous way of referring to\nthings, and – through the HTTP dereferencing mechanism\n– access to additional information a resource may hold.\nSimple RDF statements, however, are not sufﬁcient for ex-\npressing things unambiguously. In order to be precise in\nour statements, we need to be able to deﬁne, and later re-\nfer to concepts and relationships pertinent to a domain or\napplication. Ontologies are the tools for establishing these\nnecessary elements.\nSemantic Web ontologies are built on the same concep-\ntual model that is used for expressing data. However, addi-\ntional vocabularies were created for expressing formal on-\ntologies. RDF is the basis for a hierarchy of languages rec-\nommended by the W3C4. This includes the RDF Schema\nLanguage (RDFS) for deﬁning classes and properties of\nRDF resources and the OWL Web Ontology Language for\nmaking RDF semantics more explicit.5\nBesides a standard way of representing information, ac-\ncess to data also needs to be standardised. The SPARQL\nProtocol and RDF Query Language [6] is a recent recom-\nmendation by the W3C for accessing RDF data stores. A\nWeb interface which accepts and executes these queries is\ncommonly referred to as a SPARQL end-point .\nSPARQL allows access to information in a multitude of\nways. In the simplest case, it is used in a similar manner\n4The World Wide Web Consortium: http://www.w3.org/\n5For example, OWL-DL (description logic) can impose restrictions on\nthe range and domain types of properties, or constraints on cardinality.to querying a relational database using SQL6. A query –\nconsisting of a set of triple patterns – is matched against\nthe database. Results are then composed of variable bind-\nings of matching statements, based on a select clause spec-\niﬁed by the user. This can be used to retrieve informa-\ntion about a particular resource. More complex SPARQL\nqueries are frequently used to aggregate information in a\nparticular way. For example, a user agent may interpret a\nquery and aggregate data from various sources on the ﬂy.\nThe standardisation and increasing support of the SPARQL\nquery language strongly promotes the adoption of RDF as\na prevailing metadata model and language.\n2.2 Linked vs. Structured Data\nThere are already a large number of services exposing struc-\ntured data on the Web. Examples include Google, Yahoo,\nOpenSearch, Amazon, Geonames, and the MediaWiki APIs.\nMusic-related data providers include the Magnatune and\nJamendo labels, and the MusicBrainz database. Most of\nthese services use proprietary XML-based data formats.\nThis is sufﬁcient for structuring data for a given applica-\ntion, yet, because of the fairly ad-hoc deﬁnition of concepts\nin XML schema, these formats do not provide the means\nfor transparent access to a variety of services. The Linked\nData community7offers standardised access to some in-\nformation exposed by the previously listed services, as well\nas other related data sets. In Linked Data services, the re-\nliance on diverse interfaces and result formats is reduced\nby using RDF as a common representation. This also pro-\nvides the means for making data available on the Semantic\nWeb.\nMost existing metadata formats for expressing audio\nfeatures are also based on XML. MPEG-7 [7] and ACE-\nXML [11] are perhaps the most prominent examples. The\nstructural and syntactical requirements for expressing el-\nements and schemes in MPEG-7 are fulﬁlled by using an\nextended XML schema language. Although this allows the\nproduction of machine-parsable data, it does not provide a\nmachine-interpretable representation of the semantics as-\nsociated with MPEG-7 metadata elements. The same prob-\nlem arises with the ACE-XML format developed for the\njMIR package, linking components such as jAudio for fea-\nture extraction, and the ACE classiﬁcation engine. A com-\nmon problem can be recognised in using XML for stan-\ndardised syntax, while the data model remains disjoint and\noften arbitrary, with ad-hoc deﬁnition of terms, and with-\nout the ability to deﬁne meta-level relationships such as the\nequivalence of certain concepts. This hinders the ability to\nintegrate services expressing metadata in these formats, or\nthe reuse of any of the deﬁned terms in other domains. Our\ndata, on the other hand, is expressed using a ﬂexible RDF\nand Web Ontology based data model. It is compatible with\nthe Music Ontology [12], which is already widely used in\nLinked Data applications.\n6Structured Query Language\n7\"Linking open data on the semantic web\",\nhttp://linkeddata.org/\n44810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n2.3 Ontologies\nAs mentioned in section 2.1, only a conceptual model is\nprovided by RDF. Ontologies are used for the actual def-\ninition of pertinent terms and relationships. Recent ef-\nforts [13] toward integrating music-related web services\nand data sources have led to the creation of the Music On-\ntology [12]. It serves as a standard base ontology which\ncan be readily used for describing a wide range of con-\ncepts. These include high-level editorial data about songs\nor artists, production data about musical recordings, and\ndetailed structural information about music using events\nand timelines. The ontology provides the basis for nu-\nmerous extensions, including the Audio Features Ontol-\nogy [14]. The music similarity features published and used\nby the services described in this paper are expressed using\nthese ontologies.\n3. THE SOUNDBITE DATASET\nThe SoundBite dataset consists of MFCC features and Mu-\nsicBrainz identiﬁers for a cleaned-up subset of the data re-\nported back to the central server by the different instances\nof the SoundBite client application. Currently, the database\nincludes metadata for 152,410 tracks produced by 6,938\nunique artists. These numbers are expected to grow as the\nnumber of SoundBite users grows, and the data clean-up\nprocedure is reﬁned. We believe that this dataset can be\nespecially valuable because of its scope and diversity. Fur-\nthermore, it originates from real-world users, and there-\nfore reﬂects at least a part of the users’ community inter-\nests and relevant needs. It is not susceptible to any biases\nwhich might be implicit in datasets which are artiﬁcially-\ncreated for research purposes. We currently do not collect\npersonal data about SoundBite users, although this infor-\nmation might be of interest for other studies. However, at\nthe time of writing this paper, the growing user community\nalready seems sufﬁciently large and varied for the dataset\nto cover the most popular genres. The dataset coverage is\nexpected to further improve as a direct result of user base\ngrowth and further clean-up.\nFigure 1 . Simpliﬁed SoundBite Architecture.\nAs mentioned in section 1, the features extracted by\neach instance of the SoundBite client application are re-\nported back to a central server, where they are stored in a\ndatabase alongside the relevant textual metadata. Figure 1illustrates the interaction between the iTunes application,\nthe SoundBite plugin, and the Isophone server. The rele-\nvant resources on the client side are iTunes music library\nand the corresponding XML ﬁle which describes the col-\nlection. Since textual metadata contained in this XML ﬁle,\nsuch as title and artist, are often inserted or altered by the\nusers themselves, we cannot rely on their accuracy. They\ncertainly cannot be used as unique identiﬁers which are\nnecessary for facilitating public usage of the dataset. Prior\nto publishing, the data need to undergo a clean-up process,\nas described in following sections. Using the MFCC data\nfor automatic playlist creation, as done by the Soundbite\nplugin, requires similarity metrics to be deﬁned on the data.\nThese are not provided as part of the dataset, but are rather\nconsidered part of an algorithm which utilizes the data for a\nparticular application, namely, playlist creation. The pub-\nlished data facilitate the exploration of further similarity\nalgorithms and applications.\n3.1 Data ﬁltering and publishing\nSince the audio tracks to which the features relate reside\nin end-users’ audio collections, they are inaccessible to\nus and we obviously cannot provide them as part of the\ndataset. It is therefore of crucial importance that we do pro-\nvide unique identiﬁers to the audio material, without which\nthe provided features can be of very little use. As a source\nfor such unique identiﬁers, and as an aid in metadata-based\nﬁltering, we use the MusicBrainz database.\nMusicBrainz is a comprehensive public community mu-\nsic meta-database. It can be used to identify songs or CDs,\nand provides valuable data about tracks, albums, artists and\nother related information. MusicBrainz can be accessed ei-\nther through their web site or by using client applications\nvia an application programming interface (API). We use\nthe MusicBrainz service as metadata reference in the ﬁlter-\ning process, and use MusicBrianz ID’s as unique identiﬁers\nwhich are published together with the MFCC’s.\nThe editorial metadata reported back to the server by\nSoundBite (as depicted in ﬁgure 1) include the entire con-\ntent of the iTunes Music Library XML ﬁle. The data clean-\nup procedure currently uses the following metadata items:\n•Track Title\n•Main Artist\n•Album Title\n•Track Duration\n•File Format\n•Bit Rate\nIn the ﬁrst stage of the clean-up process, title, artist,\nand album are matched against the MusicBrainz database.\nThe track’s duration is used for resolving ambiguities, as\nwell as for sanity check (a large difference between the re-\nported duration value and the duration retrieved from Mu-\nsicBrainz may indicate that the other ﬁelds are erroneously\nor maliciously wrong). Each matching track is assigned an\n449Poster Session 3\nID provided by the MusicBrainz database, which serves as\nunique identiﬁer. We found that about 28% of the entries\nin our database had exact matches (artist, title, album, and\napproximate duration) in the MusicBrainz database. The\nremaining 72% are stored for possible future use, but do\nnot currently qualify for publishing. The relatively small\nproportion of tracks that do qualify can be regarded as an\nindication of the poor reliability of textual metadata in end\nusers’ audio collection.\nAs indicated in [16], MFCC features are more robust\nat higher bit rates. Therefore, in the second stage the data\nis further ﬁltered according to maximum bit rate and best\nquality audio ﬁle type (e.g. keeping AACs as opposed\nto MP3s), in order to preserve the highest quality features\nfor each track. Since these parameters are included in the\nmetadata reported to the server, this doesn’t require access\nto the audio ﬁles themselves.\nOnce cleaned-up and ﬁltered as described above, the\nMFCC features and the obtained MusicBrainz ID’s are ex-\nported from the database as RDF’s using the D2R Map-\nping [2], with the appropriate linking to the Audio Fea-\ntures [14] and SoundBite ontologies (see ﬁgure 2). They\nare then made available via a SPARQL end-point on our\nserver8.\nFigure 2 . Accessing the SPARQL endpoint using the\nSoundBite ontology.\n4. APPLICATIONS\nIn this section we describe how our data set can be used\nas basis for the development of new music similarity and\nmusic recommendation algorithms. Additionally, we pro-\nvide an example of a prototype audio search engine. The\nservice uses our database to ﬁnd tracks similar to an audio\nquery and returns editorial metadata about the found set\nobtained from external web-services.\n4.1 Research Platform\nThere has recently been a signiﬁcant amount of research\non music similarity and audio-based genre classiﬁcation.\nBoth ﬁelds use content-based descriptors extracted from\n8http://dbtune.org/iso/audio signals. Apart form being computationally expen-\nsive, audio-similarity features coupled with matching tex-\ntual metadata are not easily obtainable in large quantities.\nThe published Isophone data provide an excellent oppor-\ntunity for further research based on a reliable music col-\nlection with readily-available MFCC features. Obviously,\nsince the available features are calculated prior to being\npublished, the dataset does not accommodate changes to\nthe algorithms which produced them in the ﬁrst place. There\nis, however, plenty of room for experimentation with the\nway the different features are combined to form similar-\nity metrics, and the way they are used on the application\nlevel. We use the dataset in a research platform, which fa-\ncilitates such experiments. We are currently exploring dif-\nferent similarity metrics based on the published features, as\nwell as different ways to combine the features with other\nrelevant data, e.g. in the context of hybrid recommender\nsystems (see, for exmple, [5]). As a proof of concept, and\nto demonstrate how the research community could use the\npublished data, we have implemented a tool which queries\nthe SPARQL endpoint to obtain MFCC’s for given tracks,\nto facilitate the above mentioned research activities.\n4.2 SA WA-recommender\nSAWA-recommender9is a simple query by example search\nservice made available on the Web. Its main goal is to\ndemonstrate an application of the published music simi-\nlarity features. In this section, we outline the use and con-\nstruction of this service.\nA query to SAWA-recommender is formed by one or\nmore audio ﬁles uploaded by the user. It is typically based\non single ﬁle, however, uploading multiple audio ﬁles is\nalso allowed. In the latter case, a small set of songs forms\nthe basis of the query, either by considering similarity to\nany of the uploaded songs (and ranking the results appro-\npriately), or formulating a single common query by jointly\ncalculating the features of the query songs. The calculated\nquery is matched against the Isophone database holding\nsimilarity features and MusicBrainz identiﬁers associated\nwith each song in this database. Finally, the MusicBrainz\nweb API is used to obtain metadata about songs in the re-\nsult set. These are displayed to the user. The metadata\nconsist of basic information such as song title, album title\nand the main artist’s name associated with each song. We\nalso provide direct links to MusicBrainz, as well as Linked\nData services such as BBC Music10artist pages.\nFor each uploaded ﬁle, the system also attempts to iden-\ntify the audio by calculating a MusicDNS11ﬁngerprint\nand associated identiﬁer. This identiﬁer is matched against\nthe MusicBrainz database to obtain editorial data, hence\none can also use the service to ﬁnd more information about\nan audio ﬁle (see ﬁgure 3).\nThe architecture of the web application is depicted in\nﬁgure 4. The system is built on software components de-\n9http://isophonics.net/sawa/rec\n10http://www.bbc.co.uk/music/\n11http://www.musicdns.com/\n45010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 3 . File Identiﬁcation and Selection Interface.\nveloped in the OMRAS2 project12and a small set of com-\nmon open-source libraries.\nThe signal processing back-end of the service is pro-\nvided by Sonic Annotator13together with Vamp audio\nanalysis plugins [3]. These plugins use an application pro-\ngramming interface (API) designed for audio feature ex-\ntraction. They take audio input and return structured nu-\nmerical results. While Vamp plugins perform the feature\nextraction step (implemented in efﬁcient C++ code), Sonic\nAnnotator is the host application that reads audio data and\napplies plugins to one or more ﬁles in batch. This pro-\ngram accepts conﬁguration data and returns audio features\nin RDF according to speciﬁc ontologies [14] [4]. For the\npurpose of this present search system, we conﬁgure Sonic\nAnnotator and a suitable Vamp plugin to extract audio sim-\nilarity features based on MFCCs [10].\nFigure 4 . Search Engine System Architecture.\nThe core of the search system is a Python application\nwhich provides a Web interface and a basic search and\nclassiﬁcation engine. It also manages user sessions and\nuploaded ﬁles. Since users may upload copyrighted mate-\nrial, user sessions are fully isolated, and all audio ﬁles are\nautomatically deleted as the user leaves the service.\n12http://www.omras2.org/\n13Available at: http://omras2.org/SonicAnnotatorThe Web interface is built using the Cherrypy14Python\nlibrary. This allows the implementation of HTTP request\nhandlers as ordinary methods deﬁned within a web appli-\ncation class. Using this library, it is straightforward to ac-\ncept audio ﬁles as well as publishing data received from\nother system components using dynamically generated web\npages.\nQuery processing and database search is performed in\nthree steps. First, we extract features from the uploaded\naudio ﬁles. For optimised search, the query features are\nmatched against a model trained on the whole database.\nFinally, a selected group of songs are ranked based on their\nsimilarity to the query and the results are displayed to the\nuser.\nAlthough simple linear search was suggested for per-\nsonal collections, [10] the size of our current database is\nover 150.000 tracks and it is expected to grow. For this\nreason, we partition the data space by similarity to form\nself-similar groups of songs. These groups or clusters can\nthen be used to index the database. We can limit the search\nspace by choosing the best matching cluster based on its\nproximity to the query song. Hence, the number of direct\nsimilarity calculations is greatly reduced. Since our goal\nis search optimisation rather than classiﬁcation, we choose\nan unsupervised learning algorithm using a self-organising\nmodel, similar to a Self Organising Map [8]. The details of\nthis exceed the scope of our current discussion. However,\nit is important to note that using the symmetrised Kullback-\nLeibler (KL) divergence as basis for training and classiﬁca-\ntion, we could verify the scarcity of hubs reported in [10]\nusing a 100-times larger database of features. The songs\nare roughly equally distributed among the nodes. Only 4%\nof the nodes became hubs (containing a large set of songs)\nand 3% of them contain fewer songs. We also found that\nthis phenomenon is largely independent of the size of the\nmodel (the number of nodes). The fact that the collec-\ntion can be partitioned automatically by grouping similar\nsongs - without obtaining too many over-populated clus-\nters (hubs) - shows that the database is well balanced and\njustiﬁes the choice of metrics and learning algorithm. This\nis also favourable for the search application, since we can\nlimit the number of songs where the similarity has to be ex-\nplicitly calculated and compute the divergence only within\na single class without signiﬁcantly modifying the results\nset. In our current implementation, a local copy of the par-\ntitioned database is used for searching, however, the model\nis trained on the data available at the SPARQL end-point.\nThis is achieved by an appropriate SPARQL query, gen-\nerated and issued in each training iteration. This way, the\nmodel can easily be adjusted if the database is expanded in\nthe future. For producing the ﬁnal results, a limited set of\nsimilar songs is collected and ranked by similarity to the\nquery song(s) using the KL divergence described in [10].\nFinally, the metadata are obtained from MusicBrainz and\ndisplayed to the user.\nSince our similarity assessment follows the same prin-\nciples applied in SoundBite, these results can be seen as\n14Available at: http://www.cherrypy.org/\n451Poster Session 3\ncontent-based recommendations. However, given the size\nof the database they might be useful for identifying un-\nknown songs or song segments. In a commercial situation,\nour service might be useful in ﬁnding an alternative for a\nsong, where a copyright agreement for its use can not be\nobtained.\n5. CONCLUSION\nWe described the SoundBite dataset and its publication on\nthe Semantic Web. We believe that due to its scope and\ndiversity (which are expected to grow even further), it is\na valuable resource for researchers as well as application\ndevelopers. We provided some examples of applying the\ndata in research and prototyping web applications. These\ninitial examples strengthen our beliefs regarding the value\nand potential of this dataset, and we therefore intend to\ncontinue to follow our policy of publishing accumulated\ndata on the Semantic Web. We intend to further develop\nthis particular dataset by collecting more raw data and re-\nﬁning the ﬁltering process, and to continue developing ap-\nplications which utilize the data for research purposes and\npublic use.\n6. ACKNOWLEDGEMENTS\nThe authors acknowledge the support of the School of Elec-\ntronic Engineering and Computer Science, Queen Mary,\nUniversity of London. This work has been carried out at\nthe Centre for Digital Music and supported by the EPSRC-\nfunded ICT project OMRAS- 2 (EP/E017614/1).\n7. REFERENCES\n[1] T. Berners-Lee, J. Handler, and O. Lassila, “The se-\nmantic web”, Scientiﬁc American , pp. 34–43, May\n2001.\n[2] C. Bizer and R. Cyganiak, “D2R Server-Publishing Re-\nlational Databases on the Semantic Web\",\n5th International Semantic Web Conference,\nAthens, GA, USA, 2006.\n[3] C. Cannam, “The Vamp Audio Analysis Plugin API:\nA Programmer’s Guide”,\nhttp://vamp-plugins.org/guide.pdf\nLast accessed: July, 2009.\n[4] C. Cannam, “The Vamp Plugin Ontology”,\nhttp://omras2.org/VampOntology ,\nLast accessed: July, 2009.\n[5] J. Donaldson„ “A hybrid social-acoustic recommenda-\ntion system for popular music\", In proceedings of the\n2007 ACM conference on Recommender systems , Min-\nneapolis, MN, USA, 2007.\n[6] K. Grant Clark, L. Feigenbaum, E. Torres, (eds.)\n“SPARQL Protocol for RDF”\nW3C Recommendation , 15. January 2008http://www.w3.org/TR/2008/\nREC-rdf-sparql-protocol-20080115/\nLast accessed: July, 2009.\n[7] H. Kim, N. Moreau, T. Sikora, “MPEG-7 Audio and\nBeyond: Audio Content Indexing and Retrieval.\"\nWiley and Sons , October 2005.\n[8] T. Kohonen, “Self-Organizing Maps\",\nSpringer , Berlin, 1995.\n[9] O. Lassila, R. Swick, “Resource description frame-\nwork model and syntax speciﬁcation”, 1998.\nhttp://citeseer.ist.psu.edu/article/\nlassila98resource.html\nLast accessed: March, 2009.\n[10] M. Levy and M. Sandler, “Lightweight measures for\ntimbral similarity of musical audio”, In Proceedings of\nthe 1st ACM Workshop on Audio and Music Computing\nMultimedia , (Santa Barbara, California, USA, October\n27, 2006). AMCMM ’06. ACM, New York, NY , 27-36.\n[11] D. McEnnis, C. McKay, I. Fujinaga, P. Depalle, \"jAu-\ndio: A Feature Extraction Library\", in Proc. of the\nInternational Conference on Music Information Re-\ntrieval, London, UK, 2005.\n[12] Y . Raimond., S. Abdallah, and M. Sandler. “The Music\nOntology”, Proceeedings of the 8th International Con-\nference on Music Information Retrieval , Vienna, Aus-\ntria, 2007.\n[13] Y . Raimond. and M. Sandler, “A Web of Musical In-\nformation”, Proceeedings of the 9th International Con-\nference on Music Information Retrieval , Philadelphia,\nUSA September 14-18, 2008.\n[14] Y . Raimond, “Audio Features Ontology Speciﬁcation”,\nhttp://motools.sourceforge.net/doc/\naudio_features.html\nLast accessed: March, 2009.\n[15] M. Sandler, M. Levy, “Signal-based Music Searching\nand Browsing”, ICCE 2007. International Conference\non Consumer Electronics , 10-14 Jan. 2007.\n[16] S. Sigurdsson, K. Brandt Petersen, T. Lehn-Schiler,\n“Mel Frequency Cepstral Coefﬁcients: An Evaluation\nof Robustness of MP3 Encoded Music,” ISMIR 2006\n7th International Conference on Music Information\nRetrieval .\n452"
    },
    {
        "title": "Using Regression to Combine Data Sources for Semantic Music Discovery.",
        "author": [
            "Brian Tomasik",
            "Joon Hee Kim",
            "Margaret Ladlow",
            "Malcolm Augat",
            "Derek Tingle",
            "Rich Wicentowski",
            "Douglas Turnbull"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415558",
        "url": "https://doi.org/10.5281/zenodo.1415558",
        "ee": "https://zenodo.org/records/1415558/files/TomasikKLATWT09.pdf",
        "abstract": "In the process of automatically annotating songs with descriptive labels, multiple types of input information can be used. These include keyword appearances in web documents, acoustic features of the song’s audio content, and similarity with other tagged songs. Given these individual data sources, we explore the question of how to aggregate them. We find that fixed-combination approaches like sum and max perform well but that trained linear regression models work better. Retrieval performance improves with more data sources. On the other hand, for large numbers of training songs, Bayesian hierarchical models that aim to share information across individual tag regressions offer no advantage.",
        "zenodo_id": 1415558,
        "dblp_key": "conf/ismir/TomasikKLATWT09",
        "keywords": [
            "keyword appearances",
            "acoustic features",
            "similarity",
            "fixed-combination approaches",
            "trained linear regression models",
            "data sources",
            "large numbers of training songs",
            "Bayesian hierarchical models",
            "sharing information",
            "tag regressions"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nUSING REGRESSION TO COMBINE DATA SOURCES FOR SEMANTIC\nMUSIC DISCOVERY\nBrian Tomasik, Joon Hee Kim, Margaret Ladlow, Malcolm Augat,\nDerek Tingle, Richard Wicentowski, Douglas Turnbull\nDepartment of Computer Science, Swarthmore College, Swarthmore PA 19081\n{btomasi1, joonhee.kim, mladlow1, maugat1 }@alum.swarthmore.edu\n{dt, richardw, turnbull }@cs.swarthmore.edu\nABSTRACT\nIn the process of automatically annotating songs with de-\nscriptive labels, multiple types of input information can be\nused. These include keyword appearances in web docu-\nments, acoustic features of the song’s audio content, and\nsimilarity with other tagged songs. Given these individ-\nual data sources, we explore the question of how to aggre-\ngate them. We ﬁnd that ﬁxed-combination approaches like\nsum and max perform well but that trained linear regres-\nsion models work better. Retrieval performance improves\nwith more data sources. On the other hand, for large num-\nbers of training songs, Bayesian hierarchical models that\naim to share information across individual tag regressions\noffer no advantage.\n1. INTRODUCTION\nWe are interested in developing a semantic music discov-\nery engine in which users enter text queries and receive a\nranked list of relevant songs. This task requires a semantic\nmusic index , i.e., a mapping between songs and associated\ntags. A tag, such as “afro-cuban roots,” “heavy metal,” or\n“steel-string guitar,” is a short text token which describes\nsome meaningful aspect of the music (e.g., genre, instru-\nmentation, emotion, geographical origins). In this paper,\nour goal will be to compute a real-valued score /hatwideystthat\nexpresses how strongly tag tapplies to song s.\nThere are a number of ways to collect semantic annota-\ntions of music. [1] compare ﬁve such approaches: surveys,\nsocial tagging, games, web documents, and audio content.\nEach of these data sources offers a different perspective,\nand each has its own strengths and weaknesses (e.g., scala-\nbility, popularity bias, accuracy), so we may wish to collect\ninformation from several of them. The question then be-\ncomes how to combine that information into a single score\nfor use in our semantic index.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.In Section 2, we describe three sources of music in-\nformation that we have collected: text mining web docu-\nments, content-based audio analysis, and collaborative ﬁl-\ntering. Section 3 describes various approaches for combin-\ning these sources, including simple ﬁxed rules, as well as\na trained regression model in which combination weights\ndepend on the quality and sparsity of the input data. We\nexplore both ordinary linear and logistic regression, as\nwell as Bayesian hierarchical models that aim to share\ninformation across tags. Section 4 describes our exper-\nimental setup, which includes a ground-truth corpus of\n10,870 songs for two vocabularies (71 Genre tags and 151\nAcoustic tags) collected from Pandora’s Music Genome\nProject.1Section 6 concludes.\n2. MUSIC INFORMATION SOURCES\nWe collect semantic-annotation information from three\nsources: web documents (WD), content-based audio anal-\nysis (CB), and collaborative ﬁltering (CF). For each song\nsand tagt, we use these sources to generate scores—\ndenotedxWD\nst,xCB\nst, andxCF\nst, respectively—indicating how\nwelltdescribess.\n2.1 Web Documents\nTags that appropriately describe a song will tend to appear\nin association with the song’s name in natural-language\ntext documents. We exploit this fact by downloading from\nthe web pages that describe the song and counting how of-\nten the proposed tag appears within them.\nGiven a song s, we generate a database Dsof docu-\nments by querying Google for “song name” “artist name”\nin lower-case (e.g., “enjoy the silence” “depeche mode”).\nWe download all hits in the top 10 and clean the HTML\nﬁles into raw text. This was done for a total of 9,359 songs.\nThen, for each tag t, we compute\nxWD\nst=/summationdisplay\nd∈Dsntd\nNtd,\nwherentdis a measure that roughly expresses how many\ntimestactually appeared in document d, andNtdis the\nnumber of times tcould have appeared in d.Ntdis just\n1Seehttp://www.pandora.com/mgp.shtml\n405Poster Session 3\n|d|/|t|, the number of words in ddivided by the number\nof words int.ntdis a bit more complicated. For long tags,\nsuch as “call and answer vocal harmony (antiphony),” po-\nsitional searches for the entire phrase would not work well.\nOn the other hand, searching for the appearance of any of\nthe words in twould yield too many hits. We compromise\nby computing ntdas the minimum number of hits for any\nword, taken over all words in t. In the case when the words\nintappear indonly in the correct order, ntdwill in fact be\nequal to the number of occurrences of the full phrase t.\n2.2 Content-Based Audio Analysis\nA second potential source of semantic information about a\nsong is the audio content itself. For this purpose we use\nthe supervised multiclass labeling (SML) model recently\nproposed by [2].\nThe audio track of a song is represented as a bag of\nfeature vectorsX={x1,...,xT}, where each xiis a fea-\nture vector that represents a short-time segment of audio,\nandTdepends on the length of the song. We use the ex-\npectation maximization (EM) algorithm to learn a song-\nspeciﬁc Gaussian mixture model (GMM) distribution over\neachX. Then, for each tag in our vocabulary, we learn a\ntag-speciﬁc GMM using the Mixture Hierarchies EM algo-\nrithm [3]. This algorithm combines the set of song-speciﬁc\nGMMs for all the songs that have been associated with the\ntag. Given a novel song s, we compute the likelihood that\nits bag of feature vectors Xswould have been generated by\neach of the tag GMMs. Normalizing these likelihoods us-\ning the technique described in [2] yields our set of scores\nxCB\nst, which can be interpreted as the parameters of a multi-\nnomial distribution over the vocabulary of tags.\nWe use the popular Mel frequency cepstral coefﬁcients\n(MFCCs) as our audio feature representation since it was\nincorporated into all of the top performing autotagging sys-\ntems in the 2008 MIREX tag classiﬁcation task [2, 4–6].\nMFCCs are loosely associated with the musical notion\nof timbre (“color”) of the music because they are a low-\ndimensional representation of the frequency spectrum of a\na short-time audio sample. For each monaural song in the\ndata set, sampled at 22,050 Hz, we compute the ﬁrst 13\nMFCCs for each half-overlapping short-time ( ∼23 msec)\nwindow from 6 ﬁve-second clips spaced at uniform inter-\nvals over the length of the song. Over the time series of\naudio segments, we calculate the ﬁrst and second instan-\ntaneous derivatives (referred to as deltas ) for each MFCC.\nThis results in about 5,000 39-dimensional MFCC+delta\nfeature vectors per 30 seconds of audio content. We sum-\nmarize an entire song by modeling the distribution of\nits MFCC+delta features with a 4-component GMM. We\nmodel each tag with an 8-component GMM.\n2.3 Collaborative Filtering\nOne additional source of semantic information is user\nplaylists: If two songs appear together in a large number\nof listener collections, one possible reason is that the songs\nshare certain attributes (say, “punk inﬂuences”) that the lis-\nteners enjoy. This suggests the idea of tag propagation :Find songs that tend to co-occur in playlists, and transfer\ntags from one of them to the other. A more robust approach\nis to ﬁnd the collection of ksongs (k= 32 here) that have\nthe strongest co-occurence score with a given song s. For\neach tagt, we take the association xCF\nstofswithtto be the\nfraction of those 32 songs to which tapplies. We set this\nnumber to 0 if the fraction is below a threshold of 0.3. The\nreasons for these choices, as well as further details on the\nentire data-collection process and choice of tag sets, appear\nin [7].\nOur data consist of 400,000 user music libraries from\nlast.fm, where a library is taken to be the set of items that\na user listens to at least 1% of the time. It turns out that data\nat the song level is too sparse to generate meaningful co-\noccurence statistics, so we instead work at the artist level.\nWe say that a tag applies to an artist if the tag applies to\nany of that artist’s songs. At the end of the propagation\nprocess, we transfer an artist’s score for a tag to each of its\nsongs. We ﬁnd the 32 closest artists using the following\nsimilarity score. Between artists iandj, we take\nsim(i,j) =p(i,j)/radicalbig\np(i)p(j),\nwherep(i,j)is the fraction of all artist co-occurrences rep-\nresented by artists iandj, andp(i)is the fraction of all\nco-occurrences containing artist i.\n3. COMBINING METHODS\nGiven the data sources described in Section 2, how can\nwe aggregate them? This general question has been well\nstudied and is known variously as combining expert judg-\nments (e.g., [8, 9]), multi-sensor data fusion (e.g., [10]),\ninformation fusion (e.g., [11]), or combining classiﬁers\n(e.g., [12, 13]). Rather than reviewing the entire body of\nliterature on the subject, we focus on two of the most basic\napproaches: Fixed-combination rules and trained combin-\ners, speciﬁcally regression.\n3.1 Fixed Combiners\nFixed combining rules take the output score /hatwideystto be a sim-\nple function of the input scores: e.g., max, min, median,\nsum, or product [14, sec. 3]. Usually the input scores xi\nst,\nwithi∈{WD,CB,CF}, are calibrated so that they corre-\nspond to conﬁdences or probabilities pi\nstthattapplies tos\ngiven the source. This can be done, for instance, by stan-\ndardizing the input scores to have mean 0 and variance 1\nand then taking\npi\nst=1\n1 + exp/parenleftbig\n−αxi\nst/parenrightbig\nfor someα[14, sec. 4.1]. We use α= 1in this paper.\nA disadvantage of this technique, however, is that each\nsource is treated on equal footing, when in fact, one of our\nsources may be far more trustworthy or better informed\n[14, sec. 1]. One method that overcomes this limitation is\nBayesian Model Averaging (BMA) (e.g., [15]), which as-\nsumes that one of the data sources is the “correct” source\n40610th International Society for Music Information Retrieval Conference (ISMIR 2009)\nand takes the ﬁnal probability to be a weighted combina-\ntion of the input probabilities:\npall sources\nst =/summationdisplay\ni∈{WD,CB,CF}pi\nstpi,\nwherepiis the probability that source iis correct. As [16,\nsec. 1] point out, this assumption is often unrealistic, as\nthe truth about whether a tag applies to a song needn’t be\ncaptured by exactly one of our data sources. Still, the idea\nof taking our ﬁnal score /hatwideystto be a weighted combination\nof the input scores—\n/hatwideyst=/summationdisplay\ni∈{WD,CB,CF}βi\ntxi\nst (1)\nfor some weights βi\nt—does seem like a natural way to ac-\ncount for the differential predictive value of different in-\nputs. The question is how to determine the weights.\n3.2 Trained Combiners\nIf we have training data for a subset of songs,2the obvious\nanswer is to use supervised learning. This is the trained\ncombiners approach advocated in [14]. Indeed, (1) has the\nform of a linear-regression model, and we can determine\nthe weights of the sources just by treating them as input\nfeatures and computing their regression coefﬁcients.\nWe try both linear and logistic regression, predicting the\nground truth yst∈{0,1}by the individual scores xi\nst, as\nwell as an intercept and possibly other features of inter-\nest (see Section 3.4). We take our predicted values /hatwideystto\nbe real-valued so that we can more ﬁnely rank-order songs\nthan with 0/1 labels. Regression is a convenient combi-\nnation approach because it potentially allows us to use a\nnumber of standard statistical tools: p-values for the sig-\nniﬁcance of regression coefﬁcients, prediction intervals for\nour output scores, model selection based on residual sum\nof squares, and many more advanced techniques.\n3.3 Hierarchical Regression Models\nOne such technique is borrowing of information across\ntags. Each tag has its own regression model, but we might\nsuspect that these models share signiﬁcant structure: For\ninstance, if collaborative ﬁltering tends to be a highly pre-\ndictive source, we would expect its coefﬁcient to be con-\nsistently large. And the linear combination of sources that\nbest predicts the tag “traditional country” is probably sim-\nilar to the one that best predicts “contemporary country.”\nOne way to capture this intuition is with a Bayesian hi-\nerarchical linear model (e.g., [17]). We’ll illustrate this\nconcept in the case of a single regression coefﬁcient βtfor\na single data source xstwithout an intercept, but similar\n2Another possible scenario is that, rather than having ground-truth la-\nbels for a subset of our songs, we have data that applies to all of our songs\nbut is weakly labeled, i.e., not every song that applies for a given tag is\nlabeled as such. If our input data sources are less sparse, we can use them\nto “ﬁll in zeros” in the ground truth while preserving the labels that the\nground truth had already.equations apply in the multivariate setting. Independent\nregression across the Ttags assumes\nyst=βtxst+/epsilon1st, /epsilon1sti.i.d.∼ N (0,σ2\nt), t= 1,...,T (2)\nfor some variances σ2\nt, with no relationship among the βt\nvalues. We call this the Independent Linear model. The\nIndependent Logistic model is the same, except that yst\nis replaced by the log-odds ln/parenleftBig\npst\n1−pst/parenrightBig\n, wherepstis the\nprobability that yst= 1.\nIn a hierarchical model, we assume in addition to (2)\nthat theβt’s share a common structure:\nβt=β+vt, vti.i.d.∼ N (0,σ2), t= 1,...,T. (3)\nFor instance, if we had three tags with independent regres-\nsion coefﬁcients of 0.1, 0.2, and 0.3, it might be reasonable\nto suppose that β≈0.2withσ≈0.1. We can further\nassume a prior over βand perform Bayesian inference to\nestimate the parameters. The multivariate version of this\nmodel we call Hierarchical Linear , and the correspond-\ning version in which ystis replaced by the log-odds that\nyst= 1we call Hierarchical Logistic .\nWe might also assume that vtin (3), rather than be-\ning normally distributed, is drawn from a mixture of nor-\nmal distributions. For instance, perhaps the web-document\nsource is much better at predicting genre labels than acous-\ntic ones, so that its βtvalues for genre tags cluster around\n0.2, say, while its βtvalues for acoustic tags cluster around\n0.05. In that case, βtcould be modeled by taking β= 0.05,\nwithvthaving peaks at 0 and 0.15. We call this model Mix-\nture Linear k, wherekis the number of centers.3\n3.4 Regression Models\nEquation (1) suggests the basic regression model to use,\nalthough in practice we include an intercept, which we ﬁnd\nalways to be highly statistically signiﬁcant. We can also\nregress on just one or two of the main sources at a time.\nA nice aspect of using regression is that we can in-\nclude extra features in our model (assuming we expect\nthey’ll contribute useful information rather than meaning-\nless noise that will lead us to overﬁt). In particular, we\ninclude scrobble counts from last.fm as a measure of the\npopularity of the artist who wrote the given song. If we\nsuspected that more popular songs had more nonzero yst\nvalues in our ground-truth, we would expect this popular-\nity term to have a high positive regression coefﬁcient. In-\ncluding the term could be seen as a way of controlling for\npopularity bias if we omit the popularity feature when we\npredict/hatwideystfor novel songs. We can also include terms for\nthe interaction of data sources with popularity. A positive\ninteraction coefﬁcient would indicate that the data source\ngives a more conﬁdent prediction that a tag applies to a\nsong when the song’s artist is popular.\n3See Chapters 3 and 5 of [18] for details on each of these three hierar-\nchical models in a more general setting.\n407Poster Session 3\n4. EXPERIMENTAL SETUP\n4.1 Data Set\nOur data set consists of 10,870 songs representing 19 top-\nlevel genres (e.g., rock, classical, electronic) and 180 sub-\ngenres (e.g., grunge, romantic period opera, trance). We\nhave approximately 60 songs per subgenre. Each song is\nassociated with one or more genres and one or more sub-\ngenres. For each song, we also attempt to collect between\n2 and 10 acoustic tags from Pandora’s Music Genome\nProject vocabulary. This vocabulary consists of over 1,000\nunique tags like “dominant bass riff,” “gravelly male vo-\ncalist,” and “acoustic sonority.” These acoustic tags can\nbe thought to be objective in that two trained experts can\nannotate a song using the same tags with high probabil-\nity [19].\n4.2 Cross-Validation Setup\nWe evaluate the retrieval performance of our combined\nscores using ﬁve-fold cross-validation on the Pandora data\nset. Ordinarily, this would involve training our regression\nmodel on 4/5 of the data and testing on the remaining 1/5.\nHowever, we need to be careful here, because our content-\nbased data source also trains on the Pandora data set. The\ndanger is that the content-based system may overﬁt the\ntraining data, and because our regression model would be\nusing the same training data, the model might overweight\nthe content-based source. [14, sec. 5] notes this problem\nand suggests that it be addressed by dividing the training\nset into two parts, which we do as follows.\nWe divide the songs into ﬁve partitions, each with\nroughly 2,000 songs. We apply an artist ﬁlter to the parti-\ntions, with all of the songs by an artist appearing in a single\nfold, to avoid overﬁtting our model to the particular artists\nthat appear in our training set. On three of the partitions\nwe train the content-based system, using it to then obtain\npredictions for the songs in the remaining two. We use\none of those partitions (roughly 2,000 songs) to train our\nregression model, which then makes its predictions on the\nﬁnal partition. We then cycle this process ﬁve times. The\nreason for the uneven split between the two training sets\nis that the content-based system needs to learn many more\nparameters than our regression model, which typically has\nat most ﬁve coefﬁcients.\n4.3 Tag Pruning\nSome tags are labeled with too few songs to be useful for\ntraining when we divide the songs into ﬁve partitions, so\nwe prune them. In particular, the content-based training\nconsiders only tags that have at least 20 positive instances\nin the ground truth over each possible set of three parti-\ntions on which to train. In addition, our regression model\nrequires that each single partition have at least one positive\nground-truth song (since it would be trivial to train a model\nwhen theyst’s are all 0) and at least one positive song in\neach of the three main data sources. After pruning we are\nleft with 71 Genre tags and 151 Acoustic tags.4.4 Implementation Details\nRegression works best when the features are roughly nor-\nmally distributed, so we transform some of the input scores\nfor this purpose. For popularity counts, which range any-\nwhere from 1 to over 15 million, we apply a log transfor-\nmation. For the web-document source, which is based on\ncount data, we apply a square-root transformation [20, p.\n84]. We then standardize each data source by subtracting\nthe mean and dividing by the standard deviation for a given\ntag. Thexi\nst’s referred to in Section 3.1 are these standard-\nized values.\nFor a small number of tags, βi\ntwas estimated as neg-\native for one or two of the input data sources. Because\nwe believe that our main three data sources, while poten-\ntially unhelpful, should not be anti-predictive of the ground\ntruth, we eliminate negative coefﬁcients by setting them\nto 0 when they occur. (Making this adjustment results in\na small but statistically signiﬁcant improvement in mean\naverage precision and area under the ROC curve for both\nGenre and Acoustic tags.) We do allow popularity to have\na negative coefﬁcient, and we remove this restriction en-\ntirely when considering models with interaction terms.\n4.5 Regression Types\nWe implement Independent Linear and Independent Lo-\ngistic regression using the basic lmandglm func-\ntions of the Rlanguage. For the hierarchical re-\ngressions, we use the bayesm package [21], speciﬁ-\ncally the rhierLinearModel ,rhierBinLogit , and\nrhierLinearMixture functions for Hierarchical Lin-\near, Hierarchical Logistic, and Mixture Linear k, respec-\ntively, with all optional parameters set to their default val-\nues. These methods use Markov chain Monte Carlo to\nsample the entire posterior distribution for the βi\nt’s given\nthe data, but we simply take our βi\ntestimate to be the aver-\nage of these draws. Performance is good with as few as a\nfew hundred samples, but we ﬁnd that area under the ROC\ncurve does not level off completely until 5,000 to 10,000\ndraws. For the results in this paper, we sample 15,000\ndraws, which takes on the order of 30 minutes with roughly\n100 tags and 2,000 songs. A parameter sweep of the num-\nberkof means in the Gaussian-mixture prior showed no\nappreciable differences over the range 2 to 50, so we use\nk= 2as the default.\n5. RESULTS AND DISCUSSION\nWe assess performance using the four standard\ninformation-retrieval metrics listed in Table 1 (see [22, sec.\n8.4] for explanation of each). We have also made avail-\nable4a list of the top 5 predicted songs for each tag for\npurposes of qualitative evaluation.\n4See http://www.sccs.swarthmore.edu/users/09/\nbtomasi1/combiner/\n40810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTable 1 .Area under the ROC curve, mean average precision, R-precision, and 10-precision for various settings described further in\nthe text. Rows are ordered by average AUC for Genre tags. Means and standard errors are taken over the tags, applied to the averages\nof ﬁve-fold cross-validation. (To compute standard errors with respect to each individual CV fold, divide the reported standard errors\nby a further√\n5.) The data-source abbreviations are web documents (WD), collaborative ﬁltering (CF), content-based analysis (CB),\npopularity (P), all three main sources in the model (All3), and interactions with each of the three main sources (I).\nRegression Model\n71 Genre Tags 151 Acoustic Tags\nAUC MAP R-Prec 10-Prec AUC MAP R-Prec 10-Prec\nRandom 0.502 ±0.003 0.09±0.01 0.08±0.01 0.08±0.02 0.508±0.003 0.032±0.003 0.030±0.003 0.03±0.00\nWD 0.666 ±0.010 0.25±0.02 0.29±0.02 0.47±0.03 0.616±0.006 0.135±0.007 0.181±0.008 0.29±0.02\nCF 0.732 ±0.010 0.45±0.02 0.45±0.02 0.72±0.04 0.641±0.008 0.154±0.010 0.213±0.011 0.25±0.02\nCB 0.781 ±0.014 0.23±0.02 0.25±0.02 0.38±0.03 0.836±0.008 0.141±0.007 0.161±0.008 0.19±0.01\nWD&CF 0.789 ±0.010 0.50±0.02 0.50±0.02 0.74±0.04 0.724±0.007 0.231±0.010 0.280±0.011 0.40±0.02\nCB&WD 0.819 ±0.010 0.32±0.02 0.34±0.02 0.53±0.03 0.870±0.006 0.220±0.009 0.246±0.009 0.36±0.02\nCB&CF 0.853 ±0.009 0.49±0.02 0.48±0.02 0.73±0.04 0.861±0.007 0.213±0.010 0.244±0.010 0.29±0.01\nAll3&P&I 0.856 ±0.007 0.52±0.02 0.50±0.02 0.74±0.04 0.860±0.006 0.262±0.010 0.288±0.010 0.40±0.02\nAll3 0.871 ±0.007 0.52±0.02 0.50±0.02 0.74±0.04 0.888±0.006 0.276±0.010 0.298±0.010 0.42±0.02\nAll3&P 0.876±0.007 0.52±0.02 0.51±0.02 0.74±0.04 0.887±0.006 0.277±0.010 0.299±0.010 0.42±0.02\nCombination Method\n71 Genre Tags 151 Acoustic Tags\nAUC MAP R-Prec 10-Prec AUC MAP R-Prec 10-Prec\nMin 0.658 ±0.015 0.27±0.02 0.27±0.02 0.60±0.04 0.654±0.009 0.121±0.006 0.161±0.008 0.26±0.01\nProduct 0.826 ±0.009 0.42±0.03 0.41±0.02 0.67±0.04 0.814±0.006 0.197±0.008 0.232±0.009 0.32±0.01\nMedian 0.826 ±0.009 0.43±0.02 0.43±0.02 0.68±0.04 0.820±0.006 0.219±0.009 0.261±0.009 0.35±0.02\nSum 0.851 ±0.007 0.44±0.03 0.44±0.02 0.69±0.04 0.847±0.006 0.220±0.009 0.252±0.009 0.34±0.01\nMax 0.856 ±0.007 0.46±0.02 0.48±0.02 0.59±0.03 0.859±0.006 0.239±0.009 0.274±0.009 0.34±0.01\nInd Log 0.866 ±0.006 0.51±0.03 0.50±0.02 0.72±0.04 0.875±0.005 0.266±0.010 0.293±0.010 0.40±0.02\nHier Log 0.872 ±0.006 0.51±0.03 0.50±0.02 0.73±0.04 0.883±0.006 0.272±0.010 0.296±0.010 0.40±0.02\nHier Mix 0.876±0.007 0.52±0.02 0.51±0.02 0.74±0.04 0.887±0.006 0.277±0.010 0.299±0.010 0.42±0.02\nHier Lin 0.876±0.007 0.52±0.02 0.51±0.02 0.74±0.04 0.887±0.006 0.277±0.010 0.299±0.010 0.42±0.02\nInd Lin 0.876±0.007 0.52±0.02 0.51±0.02 0.74±0.04 0.887±0.006 0.277±0.010 0.299±0.010 0.42±0.02\n5.1 Regression Models\nThe top half of Table 1 reports the performance of the In-\ndependent Linear model on subsets of the data sources, as\nwell as models that include popularity information. The\nRandom method is a regression model in which all sources\nhave coefﬁcients of 0, so that the ﬁnal ranking of songs\nis the same as the (randomized) order in which they were\ninitially seen. Each source alone clearly performs better\nthan random, and each addition of a new source results in\na statistically signiﬁcant improvement in AUC.5This is\nconsistent with the fact that the data sources are relatively\nuncorrelated, having correlation coefﬁcients typically less\nthan 0.3 and often less than 0.1, depending on the tag.\nAccording to the AUC measure, CB is the individually\nmost predictive source, while according to precision, CF\nis. We suspect this reﬂects the fact that CB’s input repre-\nsentation is dense, providing nonzero scores for 91.2% of\nsongs for each tag, while CF’s input contains mostly ze-\nros, with scores for only an average across tags of 2.4% of\nsongs. (WD falls in the middle, with nonzero scores for\nan across-tag average of 13.7% of songs.) When CF has a\n5This is usually apparent from inspection of standard errors, but we\nverify it by checking that p-values are less than 0.05 for paired t-tests on\nthe per-tag AUC values. In fact, the only pairs between which this fails\nto hold are (1) CB and WD&CF for Genre tags, (2) All3 and All3&P\nfor Acoustic tags, and (3) CB&CF and All3&P&I for both tag types. If\nwe apply a conservative Bonferroni correction for the10·9\n2pairs of tests,\na few more pairs become not signiﬁcant, including the transition from\nCB&CF to All3 for Genre tags.nonzero value, it really means something, so that CF’s top\nresults are very precise. Toward the later end of the ranked\nresults list, however, CF is essentially random, while CB\nstill provides useful information.\nIt is interesting to observe that CB’s advantage over CF\nin terms of AUC is larger in the case of acoustic tags than\ngenre tags, perhaps because acoustic tags are inherently\nmore predictable by audio content alone.\nPopularity data was not especially helpful. While its\naddition to the three main sources did result in a statis-\ntically signiﬁcant AUC improvement for Genre tags (p-\nvalue 0.007), it did not for Acoustic tags (p-value 0.4), and\nthe magnitude of difference was relatively small. In some\nsense, this is a welcome result, since it suggests that the\nPandora labels are not biased very much by whether an\nartist is well-known. The interaction model contained too\nmany features and tended to overﬁt, which is unsurprising\ngiven the modest usefulness of the main popularity term.\n5.2 Coefﬁcient Magnitudes\nOur default regression model was Independent Linear with\nthe three main data sources, popularity, and an intercept.\nAveraging the βi\nt’s over all of the tags tgives the following\nprediction equation for Genre tags (the one for Acoustic\ntags is similar):\n/hatwideyst= 0.08 + 0.02xWD\nst+ 0.02xCB\nst+ 0.09xCF\nst+ 0.02xpop\ns.\n409Poster Session 3\nBecause the xi\nst’s represent the transformed and standard-\nized input values (see Section 4.4), the standard error for\neachβi\nstis roughly the same for a given tag,6so that the\nt-statistic of each coefﬁcient is roughly proportional to the\ncoefﬁcient’s magnitude. It’s worth noting, though, that sta-\ntistical signiﬁcance of a coefﬁcient as different from zero\nis not identical with usefulness as a data source. Indeed,\nwe saw in Section 5.1 that CB was individually more pre-\ndictive than CF, at least as measured by AUC, while CB’s\ncoefﬁcient is 0.02 instead of 0.09. The reason may again\nbe that CB provides a denser input representation than CF;\nCF can afford to have a large βCF\nstbecause in the rare cases\nwhen its values are nonzero, they’re strongly informative.\n5.3 Regression Types\nThe bottom half of Table 1 shows various combination\ntechniques. The regression approaches use the model\nAll3&P, while the ﬁxed-combination approaches use just\nthe three main sources. All trained regression models out-\nperform all ﬁxed-combining methods.7This result con-\ntrasts with the ﬁnding by [11] that the simple sum rule out-\nperformed supervised linear-discriminant analysis (similar\nto logistic regression) and decision trees. Still, Sum and\nespecially Max do not fare badly and would not be unrea-\nsonable choices for a simple combining system. That Max\nis close to Independent Logistic regression is perhaps un-\nsurprising, because the ﬁxed-combining methods apply the\nsame sigmoid transformation to the input data that logistic\nregression uses.\nWhile Hierarchical Logistic regression did slightly out-\nperform Independent Logistic, the hierarchical and mix-\nture models showed no apparent effect for linear regres-\nsion. We suspect this is because the number of observa-\ntions (songs) is so large (over 2,100 on average) that the\nBayesian prior terms in those models wash out. To conﬁrm\nthis, we tried artiﬁcially restricting ourselves to 250 songs,\nand in that case, the hierarchical methods did slightly out-\nperform their independent counterparts.\n6. CONCLUSIONS\nWe have shown that combining different sources of\nsong-tag annotation information improves retrieval perfor-\nmance. Fixed-combining methods like Sum and Max do\na ﬁne job for simple systems, but retrieval improves when\nwe use a trained combining method like linear or logistic\nregression. In settings where large numbers of songs are\navailable, basic Independent Linear regression on each tag\nseparately gives results just as good as more sophisticated\nhierarchical models, while allowing for easier implemen-\ntation, faster computation, and greater parallelizability.\n6This is only “roughly” because of small inter-feature correlations.\n7Paired t-tests on the AUC values for individual tags give p-values\nless than 0.05 for all pairs except between (1) Product and Median and\n(2) Sum and Max for Genre tags, and (3) all three of Hier Mix, Hier Lin,\nand Ind Lin for both tag types. For Genre tags, ﬁve more pairs fail to\nreject the null hypothesis if we apply a Bonferroni correction on the10·9\n2pairs of tests, including Sum vs. Independent Logistic (p-value 0.01).7. REFERENCES\n[1] D. Turnbull, L. Barrington, and G. Lanckriet. Five ap-\nproaches to collecting tags for music. ISMIR , 2008.\n[2] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. Se-\nmantic annotation and retrieval of music and sound effects.\nIEEE TASLP , 2008.\n[3] N. Vasconcelos. Image indexing with mixture hierarchies.\nIEEE CVPR , pages 3–10, 2001.\n[4] S. J. Downie. The music information retrieval evaluation ex-\nchange (2005–2007): A window into music information re-\ntrieval research. Acoustical Science and Technology , 2008.\n[5] M. Mandel and D. Ellis. Multiple-instance learning for music\ninformation retrieval. In ISMIR , 2008.\n[6] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green. Auto-\nmatic generation of social tags for music recommendation. In\nNIPS , 2007.\n[7] J. Kim, B. Tomasik, and D. Turnbull. Using artist similarity\nto propagate semantic information. ISMIR , 2009.\n[8] P. A. Morris. Combining expert judgments: A Bayesian ap-\nproach. Management Science , pages 679–693, 1977.\n[9] R. A. Jacobs. Methods for combining experts’ probability as-\nsessments. Neural computation , 7(5):867–888, 1995.\n[10] H. B. Mitchell. Multi-Sensor Data Fusion: An Introduction .\nSpringer, 1 edition, September 2007.\n[11] A. Ross and A. Jain. Information fusion in biometrics. Pattern\nRecognition Letters , 24(13):2115–2125, 2003.\n[12] J. Kittler. Combining classiﬁers: A theoretical framework.\nPattern Analysis & Applications , 1(1):18–27, 1998.\n[13] C. De Stefano, C. D’Elia, A. Marcelli, and A. S. di Freca.\nUsing bayesian network for combining classiﬁers. In ICIAP ,\npages 73–80, 2007.\n[14] R. Duin. The combining classiﬁer: To train or not to train? In\nICPR , volume 16, pages 765–770, 2002.\n[15] J. A. Hoeting, D. Madigan, A. E. Raftery, and C. T. V olin-\nsky. Bayesian model averaging: a tutorial. Statistical science ,\npages 382–401, 1999.\n[16] Z. Ghahramani and H. C. Kim. Bayesian classiﬁer combina-\ntion. Gatsby Computational Neuroscience Unit Tech Report ,\n2003.\n[17] D. V . Lindley and A. F. M. Smith. Bayes estimates for the\nlinear model. Journal of the Royal Statistical Society. Series\nB (Methodological) , pages 1–41, 1972.\n[18] P. E. Rossi and G. M. Allenby. Bayesian statistics and mar-\nketing. Marketing Science , pages 304–328, 2003.\n[19] T. Westergren. Personal notes from Pandora get-together in\nSan Diego, March 2007.\n[20] J. J. Faraway. Practical Regression and ANOVA using\nR. July 2002. http://cran.r-project.org/doc/\ncontrib/Faraway-PRA.pdf .\n[21] P. E. Rossi and R. McCulloch. bayesm. R package ver. 2.2-2,\n2008-06-09, http://faculty.chicagobooth.edu/\npeter.rossi/research/bsm.html .\n[22] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to\nInformation Retrieval . Cambridge University Press, 2008.\n410"
    },
    {
        "title": "Musical Bass-Line Pattern Clustering and Its Application to Audio Genre Classification.",
        "author": [
            "Emiru Tsunoo",
            "Nobutaka Ono",
            "Shigeki Sagayama"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1417141",
        "url": "https://doi.org/10.5281/zenodo.1417141",
        "ee": "https://zenodo.org/records/1417141/files/TsunooOS09.pdf",
        "abstract": "This paper discusses a new approach for clustering musical bass-line patterns representing particular genres and its application to audio genre classification. Many musical genres are characterized not only by timbral information but also by distinct representative bass-line patterns. So far this kind of temporal features have not so effectively been utilized. In particular, modern music songs mostly have certain fixed bar-long bass-line patterns per genre. For instance, while frequently bass-lines in rock music have constant pitch and a uniform rhythm, in jazz music there are many characteristic movements such as walking bass. We propose a representative bass-line pattern template extraction method based on k-means clustering handling a pitchshift problem. After extracting the fundamental bass-line pattern templates for each genre, distances from each template are calculated and used as a feature vector for supervised learning. Experimental result shows that the automatically calculated bass-line pattern information can be used for genre classification effectively and improve upon current approaches based on timbral features.",
        "zenodo_id": 1417141,
        "dblp_key": "conf/ismir/TsunooOS09",
        "keywords": [
            "musical",
            "bass-line",
            "patterns",
            "genre",
            "classification",
            "timbral",
            "information",
            "representative",
            "template",
            "extraction"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMUSICAL BASS-LINE PATTERN CLUSTERING AND ITS APPLICATION\nTO AUDIO GENRE CLASSIFICATION\nEmiru Tsunoo Nobutaka Ono Shigeki Sagayama\nGraduate School of Information Science and Technology\nThe University of Tokyo, Japan\n{tsunoo, onono, sagayama }@hil.t.u-tokyo.ac.jp\nABSTRACT\nThis paper discusses a new approach for clustering mu-\nsical bass-line patterns representing particular genres and\nits application to audio genre classiﬁcation. Many musical\ngenres are characterized not only by timbral information\nbut also by distinct representative bass-line patterns. So far\nthis kind of temporal features have not so effectively been\nutilized. In particular, modern music songs mostly have\ncertain ﬁxed bar-long bass-line patterns per genre. For in-\nstance, while frequently bass-lines in rock music have con-\nstant pitch and a uniform rhythm, in jazz music there are\nmany characteristic movements such as walking bass. We\npropose a representative bass-line pattern template extrac-\ntion method based on k-means clustering handling a pitch-\nshift problem. After extracting the fundamental bass-line\npattern templates for each genre, distances from each tem-\nplate are calculated and used as a feature vector for super-\nvised learning. Experimental result shows that the auto-\nmatically calculated bass-line pattern information can be\nused for genre classiﬁcation effectively and improve upon\ncurrent approaches based on timbral features.\n1. INTRODUCTION\nDue to the increasing size of music collections available on\ncomputers and portable music players, the need for music\ninformation retrieval (MIR) has surged recently. In partic-\nular, automatic genre classiﬁcation from audio is a tradi-\ntional topic of MIR and provides a structured way of eval-\nuating new representations of musical content. In this task,\nnot only instrumental information but also a bass-line in-\nformation is thought to be important. For instance, bass\nparts in most rock songs consist of root notes of the chords\nin a uniform rhythm. In comparison to this, bass parts in\njazz songs have a lot of characteristic movements which\nare called walking bass. If such representative bar-long\nbass-line patterns in music can be captured automatically\nas templates, they can potentially be used to characterize\ndifferent music genres directly from audio signals.\nIn previous research, timbral features, rhythmic features\nand pitch features have been used for audio genre classi-\nﬁcation [1]. In this work, the timbral features were the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.most dominant and the pitch features used were not lim-\nited to the bass register. Studies more related to bass part\nextraction have been presented [2] where musical melodic\nand bass notes were modeled with Gaussian mixture model\n(GMM) and estimated. Using this pitch estimation method,\nMarolt [3] worked on the clustering of melodic lines using\nGMM. Researches using bass-line information for genre\nclassiﬁcation include McKay [4] and Tsuchihashi [5]. How-\never the bass-line features discussed were based on overall\nstatistics and did not represent directly temporal informa-\ntion.\nIn this paper, we discuss an approach for clustering unit\nbass-line patterns from a number of audio tracks and pro-\npose a feature vector based on the distances from templates\nfor the applicationto genre classiﬁcation. First, we empha-\nsize only harmonic sounds of the audio tracks and esti-\nmate measure segments which divide tracks into measures.\nThen we propose a clustering method specialized to bass-\nline patterns based on the k-means clustering algorithm.\nFor the purpose of an application to audio genre classiﬁ-\ncation, the scheme to extract feature vector based on the\nbass-line patterns which contain temporal information is\nsuggested. Finally, the effectiveness of the proposed bass-\nline pattern information for genre classiﬁcation is veriﬁed\nexperimentally.\n2. BASS-LINE PATTERN CLUSTERING\n2.1 Challenges in Bass-line Pattern Clustering\nBar-long bass-line patterns are frequently common and char-\nacteristic of a particular genre. In order to extract those\nrepresentative patterns from audio signals, there are sev-\neral challenges to be cleared. Especially in modern popu-\nlar music, pieces comprise of both harmonic and percus-\nsive sounds and percussive components might disturb the\nbass-line analysis. Additionally, the bar lines of the mu-\nsic pieces need to be estimated. Another problem is that\nunit bass-line patterns are shifted in pitch according to the\nchord played. For example, a pattern consists of only root\nnotes in a uniform rhythm, all notes in this pattern need to\nbe pitch-shifted by the same amount of notes according to\nthe chord, because the root note changes accompany with\nthe chord changes.\nTherefore the problems in extraction of representative\nbar-long patterns can be summarized as the following three\nproblems:\nI. audio signals may contain not only harmonic sounds\nbut also percussive sounds,\n219Poster Session 2\nFigure 1 . The ﬂow diagram of the system.\nII. measure segmentation is to be estimated, and\nIII. bass-line patterns are pitch-shifted according to chords.\nIn the next subsections, we describe our approach to solv-\ning these challenges. Fig. 1 illustrates the ﬂow of the algo-\nrithm.\n2.2 Emphasizing Harmonic Components\nGenerally, harmonic and percussive sounds are mixed in\nthe observed spectrograms of audio pieces (the problem\nI). Therefore in order to perform bass-line pattern analysis\nit is useful to separate these components as a preprocess-\ning step. We utilize the harmonic/percussive sound sepa-\nration (HPSS) technique which we have proposed [6] that\nis based on the difference of general timbral features. By\nlooking at the upper left ﬁgure in Fig. 2, a typical instance\nof spectrogram, one can observe that harmonic compo-\nnents tend to be continuous along the temporal axis in par-\nticular frequencies. On the other hand, percussive compo-\nnents tend to be continuous along the frequency axis and\ntemporally short. Mask functions for separating the two\ncomponents (harmonic and percussive) are calculated fol-\nlowing a maximum a priori (MAP) estimation approach us-\ning the expectation maximization (EM) algorithm. Apply-\ning this approach to the shown spectrogram, harmonic and\npercussive components are separated and harmonic ones\nare emphasized (harmonic and percussive components are\nshown in the upper right and the lower left of Fig. 2 respec-\ntively). In order to capture only the bass part we apply low\npass ﬁltering to the harmonic-only spectrogram.\n2.3 Bar Line estimation Using Percussive Clustering\nMethod\nThere are many possible ways to solve problem II which is\nthe estimation of bar lines. One way is beat tracking [8] in\nwhich onset, chord changes and drum patterns are used as\ncues. Instead, in the case where it is unknown how many\nbeats are in one measure or songs do not always start with\nthe head of the measure, the pattern matching approach is\nrather useful to estimate bar lines. Therefore the rhythm\nmap which we have proposed [9] is employed.\nThe rhythm map is an approach to estimate representa-\ntive bar-long percussive patterns and its segmentation (i.e.\nbar lines) simultaneously. This algorithm also requires the\nFigure 2 . The original spectrogram (upper left), the\nharmonics-emphasized spectrogram (upper right) and the\npercussion-emphasized spectrogram (lower left) of a pop-\nular music piece (RWC-MDB-G-2001 No.6 [7]). The\nlow-pass ﬁltered logarithmic spectrogram calculated us-\ning wavelet transform from harmonics-emphasized spec-\ntrogram is shown in lower right ﬁgure.\nsource separation method shown in previous subsection as\na preprocessing and deals with the percussive-emphasized\nspectrogram. By iterating dynamic programming (DP) match-\ning and updating the templates used for DP matching based\non the k-means-clustering-like update rules, both segmen-\ntation and templates themselves are updated. After conver-\ngence, the multiple percussive patterns in the input song\nare learned and the optimal segmentation is obtained. We\nuse this estimated segmentation as bar lines.\n2.4 Iterative Update of Bass-line Pattern Cluster\nIf there was no pitch shift (problem III) a simple k-means\nclustering approach can be used to estimate the represen-\ntative bar-long bass-line patterns: Distances between each\nbar-long spectrogram pattern and centroid spectrogram pat-\nterns are calculated, and the centroids are updated by aver-\naging the sample patterns. In order to deal with pitch-shift\nproblem, we propose an new approach where every possi-\nble pitch-shift is compared in k-means framework.\nHere we should note that both centroid template spec-\ntrogram and input spectrogram need to be logarithmic along\nfrequency axis in order to consider pitch-shift. Because\nthe musical notes are fashioned logarithmic in linear fre-\nquency domain. This kind of logarithmic spectrogram can\nbe obtained by using wavelet transform. The lower right\nof Fig. 2 shows the logarithmic spectrogram which is pro-\ncessed low-pass ﬁltering after Gabor wavelet transform whose\nfrequency resolution is semitone (100 cents). The low-pass\nﬁltering (actually a band-pass ﬁltering) was done by setting\nhigh and low frequency components to be zero. In this ﬁg-\nure, the energy of bass drum is still dominant even after the\nsource separation. That is because the bass drum sounds\nsometimes have long duration and the duration compo-\nnents also have the same feature with harmonic sounds\nwhich are continuous along the temporal axis. However\nthese energies are thought not to be so harmful because\nwhen averaging spectrogram patterns to update templates,\nthose parts become relatively small.\n22010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nMathematically, we modeled a bass-line template as a\nmatrix and ﬁnd out the most matched template for each\nbar-long spectrogram by calculating distances over all pos-\nsible pitch-shift. The centroid bass-line pattern is deﬁned\nas a matrix whose rows represent semitone numbers and\nwhose columns are time instants. This matrix contains en-\nergy distribution of bar-long harmonic spectrogram. When\nthe input piece has Mmeasures, mth measure processed\nlow-pass ﬁltering can be written as Xm, theI×Nmatrix,\nwhere Nis the number of semitones to capture in low fre-\nquency band and Iis the resolution of time that divides a\nbar-long. On the other hand, kth bass-line pattern template\ncan be written as the I×2Nmatrix Bk. Since there are N\nnotes to pitch-shift potentially, at least Nrows of margin\nhave to be provided.\nAs a distance measure of two bass-line pattern matri-\nces, we use the following distance introduced by Frobenius\nnorm:\nd(X, Y ) =I∑\ni=1N∑\nn=1(xi,n−yi,n)2, (1)\nwhere xi,nandyi,nare the (i,j)th entries of matrices X\nandY. Considering pitch-shift, we can deﬁne the distance\nbetween input spectrogram Xmand template bass-line pat-\nternBkas\nD(Xm, Bk) = min\n1≤n≤Nd(Xm, Bk,n) (2)\nwhere Bk,nis aI×Nsubmatrix of Bkwhich is from nth\nrow to (n+N−1)th row.\nLikek-means algorithm, for every input bass-line pat-\nternXm, the distances are calculated according to Eq. (2)\nand classes and pitch-shift intervals are determined as\nˆkm= argmin\n1≤k≤KD(Xm, Bk) (3)\nand\nˆnm= argmin\n1≤n≤Nd(Xm, Bˆk,n). (4)\nThen the update rule of a template pattern can be written as\nfollowing, just by averaging patterns in a particular class,\nB/prime\nk=∑\nm∈{m|k=ˆkm}SˆnmEXm∑\nm∈{m|k=ˆkm}1, (5)\nwhere Eis anN×2Nextending matrix and Sis a2N×\n2Npitch-shift matrix respectively deﬁned as\nE=\n1 0 . . . 0\n0 1 0\n.........\n0. . . 0 1\n0 0 . . . 0\n.........\n0 0 . . . 0\n(6)\nS=\n0 0 0 . . . 0\n1 0 0 . . . 0\n0 1 0 . . . 0\n...............\n0. . . 0 1 0\n. (7)This update rule satisﬁes following equation as far as\nEuclidean distance metric is used,\n∑\nm∈{m|k=ˆkm}D(Xm, B/prime\nk)≤∑\nm∈{m|k=ˆkm}d(Xm, B/prime\nk,ˆn)\n≤∑\nm∈{m|k=ˆkm}D(Xm, Bk,ˆn),(8)\nand convergence of iterative update is guaranteed. After\nthe convergence, Kcentroid bass-line patterns Bk(k=\n1, . . . , K )are obtained.\n3. BASS-LINE PATTERN FEATURE EXTRACTION\nAs the proposed clustering method applied to audio genre\nclassiﬁcation, there is a problem that the learned templates\ncannot be used directly. Ideally bass-line patterns for a\nparticular genre would be ﬁxed and would be automati-\ncally extracted perfectly. If that was the case then auto-\nmatic genre classiﬁcation could be performed simply by\nlooking at which particular bass-line patterns are used in a\nmusic pieces by calculating the distance deﬁned in Eq. (2).\nHowever in practice there is no guarantee that patterns are\nﬁxed for a particular genre or that their automatic extrac-\ntion will be perfect. Therefore in many cases the bass-line\npatterns of a particular music piece will belong to more\nthan one genre. To address these problems simultaneously\nwe utilize a feature vector based on the distances from each\ntemplate followed by statistical machine learning to auto-\nmatically classify music genre. Supervised learning clas-\nsiﬁers such as support vector machines (SVM) [10] can be\nused for this purpose.\nOne possible way to extract feature vector is calculat-\ning distances between every measure of input spectrogram\nand every template pattern following Eq. (2) and averaging\nthem through whole an input piece. Even though there is\na possibility for a music piece to belong to more than one\ngenre templates, the distances between spectrogram in the\ninput piece and learned templates are still affected, e.g.,\none measure spectrogram in blues song is close enough to\nthe templates learned from blues collection even if its dis-\ntance is not the smallest.\nThe mathematical deﬁnition of the feature vector is fol-\nlowing. After bass-line pattern templates are learned, we\nhaveK·Gtemplates when Ktemplates are learned from G\ngenres. Then the distances between input song which have\nMmeasures and learned templates are calculated. The av-\neraged distances are obtained as follows:\ndl=1\nMM∑\nm=1D(Xm, Bl) (9)\nwhere 1≤l≤KG is the template number. The feature\nvector xcan be written as\nx= (d1, d2, . . . , d KG)T. (10)\nWe use this feature vector for a supervised learning classi-\nﬁcation to classify music genre.\n221Poster Session 2\nFigure 3 . Two examples of learned bass-line templates\nfrom jazz, blues, rock, and hiphop, in descending order.\nWhile a lot of movements of bass notes are shown in jazz\nand blues, bass notes in rock are unchanged, and hiphop\nbass-line templates are quite sparse.\n4. PROCEDURAL SUMMARY OF THE\nALGORITHM\nThe overall algorithm can be summarized as follows:\n1. Preprocessing\n(a) Emphasis of harmonic components using HPSS\n(b) Apply low-pass ﬁltering\n(c) Estimate bar lines using rhythm map\n2. Bass-line Pattern Clustering\n(a) Provide initial (random value) templates\n(b) Match the templates patterns with pitch-shifting\nby Eq. (2)\n(c) Update the template patterns following Eq. (5)\n(d) Iterate steps b and c until the convergence\n3. Genre Classiﬁcation\n(a) Calculate the distances between patterns (Eq. (10))\nand use it as a feature vector characterizing a\nmusic pieces\n(b) Perform classiﬁcation into genres using a ma-\nchine learning technique\n5. EXPERIMENTAL RESULTS\n5.1 Dataset\nExperiments with the proposed algorithms were conducted\non the GTZAN dataset [1]. The dataset had 10 genres:\nFigure 4 . Classiﬁcation accuracy using only bass-line fea-\ntures for each number of templates learned from one genre.\nThe baseline accuracy of random classiﬁer was 10.0%\nblues, classical, country, disco, hiphop, jazz, metal, pop,\nreggae, and rock. The dataset had 100 songs per genre all\nof which were single-channel and sampled at 22.05kHz.\nAll songs were processed harmonic-percussive sound source\nseparation and wavelet transform. Then they were pro-\ncessed low-pass ﬁltering and obtained the spectrogram only\nfrom 82.4Hz (E1) to 330.0Hz (E3). The reason we didn’t\nuse the spectrogram under 82.4Hz was the dominance of\nthe energy of bass drums in that area.\n5.2 Template Learning and Feature Extraction\nFirst, common bass-line pattern templates were learned us-\ning the proposed algorithm for each genre. The proposed\nalgorithm was implemented using audio processing frame-\nwork Marsyas1which is open source software with spe-\nciﬁc emphasis on Music Information Retrieval (MIR) [11].\nTo generalize the learning templates part, we divided the\ndataset 50-50 into two parts randomly and obtained two\nsets of templates for each genre. In this experiment, the\nnumber of iteration was ﬁxed to 15 times because it was\nenough to converge, and the number of the time resolution\nwas ﬁxed to 16.\nThe examples of learned templates of jazz, blues, rock\nand hiphop are shown in Fig. 3. While jazz bass patterns\nhad some movements, rock patterns showed the horizon-\ntally straight lines. In blues bass-line templates, a typical\nswing rhythm is shown, and in hiphop templates, there are\nsparse bass notes because hiphop songs mostly have strong\ndrum sounds but bass sounds are not so melodic.\nAfter learning templates two sets of templates were ob-\ntained, and next, we extracted feature vector (Eq. (10)) us-\ning those template sets. The templates learned from data\n1 were used to extract feature vectors of data 2, and vice\nversa.\n5.3 Classiﬁcation results\nIn order to train a classiﬁer in the feature space, the “Weka”\nmachine learning toolkit [12] was employed. All the re-\nsults shown were based on 10-fold cross validation using a\nlinear SVM as a classiﬁer. The labeled data was split into\n10 folds and each fold was used once for testing with the\nremaining 9 folds used for training the classiﬁer to gener-\nalize the classiﬁcation, for both of divided data sets.\n1http://marsyas.sness.net/\n22210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 5 . Classiﬁcation accuracy using both bass-line and\ntimbral features for each number of templates learned from\none genre. The baseline accuracy of existing features was\n72.4%.\nTable 1 . Genre classiﬁcation accuracy using only bass-line\npattern features and merged with timbral features.\nFeatures data 1 data 2\nBaseline (random classiﬁer) 10.0% 10.0%\nOnly bass-line (400 dim.) 42.0% 44.8%\nExisting (Timbre, 68 dim.) 72.4% 72.4%\nMerged (468 dim.) 74.4% 76.0%\nThe number of templates learned for each particular genre\nwere decided experimentally. Fig. 4 shows the results us-\ning only the bass-line pattern features whose dimension\nwas10Kwhen the number of templates learned from one\ngenre was K. As can be seen the proposed features had\nenough information for genre classiﬁcation and the accu-\nracy improved as the number of templates was increased.\nAn existing state-of-the-art genre classiﬁcation system\nwhich uses 68 dimensional timbral features like MFCC\nand spectrum centroids proposed by Tzanetakis was used\nfor comparison. The system performed well on several au-\ndio classiﬁcation tasks in MIREX 2008 [13]. Merging this\ntimbral features and bass-line features, the classiﬁcation\naccuracies shown in Fig. 5 were obtained. When the num-\nber of templates Kwas40the performance was the best,\nand when we increased the number Kmore than that the\nperformance got worse. That was because the dimension\nof feature space became larger and curse of dimensional-\nity was thought to occur. In particular case the number\nof templates Kwas ﬁxed to 40for each particular genre,\nthe precise result is shown in Table 1 and the confusion\nmatrices are shown in Table 2 and Table 3. These results\nwere higher than existing genre classiﬁcation systems that\nrely on timbral information and verify the effectiveness of\nthe proposed bass-line patterns. One can see that classical\nwas the most distinguished by the system and some reggae\nsongs were mistaken for disco.\n6. CONCLUSIONS\nWe discussed an approach for clustering common bar-long\nbass-line patterns for particular genres, and proposed a fea-\nture vector which represented relations to learned bass-\nline templates. We used HPSS technique to extract har-\nmonic components from audio signals and rhythm map\nto estimate measure segmentation. After processing low-pass ﬁltering, bass-line patterns were clustered using a new\nclustering method based on k-means clustering with pitch-\nshift. For audio genre classiﬁcation, a new feature vec-\ntor was deﬁned as averaged distances from each template.\nExperiments over music pieces from various genres con-\nﬁrmed that the proposed algorithm can improve the accu-\nracy of classiﬁcation systems based on timbral informa-\ntion.\nFuture work includes more experiments with the param-\neters of the algorithms such as the resolution of time in\ntemplates. Additionally, other features than pattern dis-\ntance vector can be devised. Combination with other fea-\ntures like percussive pattern can be done to improve further\ngenre classiﬁcation as well.\nACKNOWLEDGMENTS\nThe authors thank George Tzanetakis for sharing his dataset\nand some useful experimental discussions with us.\n7. REFERENCES\n[1] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of\naudio signals. IEEE Transaction on Speech and Audio Pro-\ncessing , 10(5):293–302, 2002.\n[2] M. Goto. A real-time music-scene-description system:\nPredominant-f0 estimation for detecting melody and bass\nlines in real-world audio signals. Speech Communication ,\n43(4):311–329, 2004.\n[3] M. Marolt. Gaussian mixture models for extraction of\nmelodic lines from audio recordings. In Proc. of ISMIR , pages\n80–83, 2004.\n[4] C. McKay and I. Fujinaga. Automatic genre classiﬁcation us-\ning large high level musical feature sets. In Proc. of ISMIR ,\npages 525–530, 2004.\n[5] Y. Tsuchihashi and H. Katayose. Music genre classiﬁcation\nfrom bass-part using som. In IPSJ SIG Technical Reports ,\nvolume 90, pages 31–36, 2006.\n[6] N. Ono, K. Miyamoto, H. Kameoka, and S. Sagayama. A\nreal-time equalizer of harmonic and percussive componets in\nmusic signals. In Proc. of the 9th Int. Conf. on Music Infor-\nmation Retrieval , pages 139–144, September 2008.\n[7] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka. Rwc mu-\nsic database: Music genre database and musical instrument\nsound database. In Proc. of the 4th Int. Conf. on Music Infor-\nmation Retrieval , pages 229–230, October 2003.\n[8] M. Goto. An audio-based real-time beat tracking system for\nmusic with or without drum-sounds. Journal of New Music\nResearch , 30(2):159–171, June 2001.\n[9] E. Tsunoo, N. Ono, and S. Sagayama. Rhythm map: Extrac-\ntion of unit rhythmic patterns and analysis of rhythmic struc-\nture from music acoustic signals. In Proc. of ICASSP , pages\n185–188, 2009.\n[10] V. Vapnik. The Nature of Statistical Learning Theory .\nSpringer-Verlag, 1995.\n[11] G. Tzanetakis. Marsyas-0.2: A Case Study in Implementing\nMusic Information Retrieval System , chapter 2, pages 31 –\n49. Idea Group Reference, 2007. Shen, Shepherd, Cui, Liu\n(eds).\n[12] I. Witten and E. Frank. Data Mining: Practical Machine\nLearning Tools and Techniques . Morgan Kaufmann, 2005.\n[13] Mirex 2008. http://www.music-ir.org/mirex/2008/.\n223Poster Session 2\nTable 2 . The confusion matrix of data 1 in the case 40\ntemplates learned from each genre\nclassiﬁed as bl cl co di hi ja me po re ro\nbl: blues 38 0 5 0 0 2 1 0 1 3\ncl: classical 048 0 0 0 0 0 0 0 2\nco: country 3 030 1 1 2 1 3 0 9\ndi: disco 0 0 136 1 1 1 2 2 6\nhi: hiphop 0 0 1 634 0 1 2 4 2\nja: jazz 4 2 1 1 041 0 0 0 1\nme: metal 0 0 2 1 0 0 43 0 0 4\npo: pop 1 0 3 3 3 2 0 34 0 4\nre: reggae 0 0 4 4 2 0 0 037 3\nro: rock 3 0 4 3 2 1 3 0 331\nTable 3 . The confusion matrix of data 2 in the case 40\ntemplates learned from each genre\nclassiﬁed as bl cl co di hi ja me po re ro\nbl: blues 38 0 5 2 0 2 2 0 1 0\ncl: classical 047 0 0 0 3 0 0 0 0\nco: country 4 040 0 0 0 0 0 0 6\ndi: disco 3 0 034 1 0 1 2 3 6\nhi: hiphop 1 0 0 341 0 0 0 5 0\nja: jazz 3 1 0 0 043 2 0 0 1\nme: metal 1 0 0 0 0 1 44 0 1 3\npo: pop 4 0 1 2 1 0 0 35 3 4\nre: reggae 4 0 0 9 4 2 0 327 1\nro: rock 6 0 3 5 0 0 3 2 031\n224"
    },
    {
        "title": "Tag Integrated Multi-Label Music Style Classification with Hypergraph.",
        "author": [
            "Fei Wang 0001",
            "Xin Wang 0013",
            "Bo Shao",
            "Tao Li 0001",
            "Mitsunori Ogihara"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416962",
        "url": "https://doi.org/10.5281/zenodo.1416962",
        "ee": "https://zenodo.org/records/1416962/files/WangWSLO09.pdf",
        "abstract": "Automatic music style classification is an important, but challenging problem in music information retrieval. It has a number of applications, such as indexing of and searching in musical databases. Traditional music style classification approaches usually assume that each piece of music has a unique style and they make use of the music contents to construct a classifier for classifying each piece into its unique style. However, in reality, a piece may match more than one, even several different styles. Also, in this modern Web 2.0 era, it is easy to get a hold of additional, indirect information (e.g., music tags) about music. This paper proposes a multi-label music style classification approach, called Hypergraph integrated Support Vector Machine (HiSVM), which can integrate both music contents and music tags for automatic music style classification. Experimental results based on a real world data set are presented to demonstrate the effectiveness of the method.",
        "zenodo_id": 1416962,
        "dblp_key": "conf/ismir/WangWSLO09",
        "keywords": [
            "Automatic music style classification",
            "important",
            "challenging problem",
            "music information retrieval",
            "applications",
            "traditional music style classification approaches",
            "unique style assumption",
            "modern Web 2.0 era",
            "additional indirect information",
            "multi-label music style classification"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nTAG INTEGRATED MULTI-LABEL MUSIC STYLE CLASSIFICATION\nWITH HYPERGRAPH\nFei Wang, Xin Wang, Bo Shao, Tao Li\nFlorida International University\n{feiwang,xwang009,bshao001,taoli }@cs.fiu.eduMitsunori Ogihara\nUniversity of Miami\nogihara@cs.miami.edu\nABSTRACT\nAutomatic music style classiﬁcation is an important, but\nchallenging problem in music information retrieval. It has\na number of applications, such as indexing of and search-\ning in musical databases. Traditional music style classiﬁ-\ncation approaches usually assume that each piece of music\nhas a unique style and they make use of the music con-\ntents to construct a classiﬁer for classifying each piece into\nits unique style. However, in reality, a piece may match\nmore than one, even several different styles. Also, in this\nmodern Web 2.0 era, it is easy to get a hold of additional,\nindirect information (e.g., music tags) about music. This\npaper proposes a multi-label music style classiﬁcation ap-\nproach, called Hypergraph integrated Support Vector Ma-\nchine (HiSVM ), which can integrate both music contents\nand music tags for automatic music style classiﬁcation.\nExperimental results based on a real world data set are pre-\nsented to demonstrate the effectiveness of the method.\n1. INTRODUCTION\nMusic styles (e.g., Dance, Urban, Pop, and Country) are\none of the top-level descriptions of music content. Con-\nsequently, automatic Music Style Classiﬁcation (MSC for\nshort) is a key step for modern music information retrieval\nsystems [7]. There has already been some work toward\nautomatic music style classiﬁcation. For example, Qin\nand Ma [10] introduce an MSC system that takes MIDI\nas data source and mines frequent patterns of different mu-\nsic. Zhang and Zhou [18] present a study on music clas-\nsiﬁcation using short-time analysis along with data mining\ntechniques to distinguish among ﬁve music styles. Zhou et\nal.[19] propose a Bayesian inference based decision tree\nmodel to classify the music into pleasurable and sorrowful\nmusic. Although these methods are highly successful, two\nmajor limitations exist.\n•These are single-label methods in that they can as-\nsign only one style label, but many pieces of music\nTHE WORK IS PARTIALLY SUPPORTED BY NSF GRANTS IIS-\n0546280, DMS-0844513, AND CCF-0830659.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.may map to more than one style.\n•They only make use of the music content informa-\ntion. However, with the rapid development of web\ntechnologies, we can easily obtain much richer in-\nformation of the music (e.g., tags, lyrics, and user\ncomments). How to incorporate these pieces of\ninformation into the MSC process effectively is a\nproblem worthy of researching.\nIn this paper, we propose a multi-label MSC method that\ncan integrate three types of information: (1) audio sig-\nnals (MFCC coefﬁcients, STFT, DWCH); (2) music style\ncorrelations; (3) music tag information and correlations.\nSpeciﬁcally, we construct two hyper-graphs, one on music\nstyle labels and the other on music tags, where the vertices\non the hypergraphs correspond to the data points, and the\nhyperedges correspond to the music styles and the tags,\nrespectively. We ﬁrst integrate those two hypergraphs to\nobtain a uniﬁed hypergraph. Next, assuming that similar\nmusic tends to have similar style labels on the hypergraph,\nwe propose a new, SVM-like multilabel ranking algorithm.\nThe algorithm uses a hypergraph Laplacian regularizer and\ncan be efﬁciently solved by the dual coordinate descent\nmethod. Finally, we propose a predictor of the size of la-\nbel set to determine the number of labels assigned to for\neach piece of music independently. To demonstrate the\nefﬁciency and effectiveness of our proposed method, we\nconducted a set of experiments applying the method to a\nreal world data.\nThe rest of this paper is organized as follows. In Sec-\ntion 2 we brieﬂy introduce preliminaries on our key con-\ncept, the hypergraph. In Section 3 we describe our HiSVM\nalgorithm. We describe in Section 4 the audio features ex-\ntracted from the data set as well as the style and tag infor-\nmation of the data set. We present the results of experi-\nments in Section 5 and conclude the paper in Section 6.\n2. PRELIMINARIES\nAhypergraph is a generalization of a graph, in which\nedges, called hyperedges, may connect any positive num-\nber of vertices [1, 11]. Formally, a hypergraph Gis a pair\n(V,E)where Vis a set of vertices andE ⊆ 2V− ∅ is a\nset of hyperedges . An edge-weighted hypergraph is one in\nwhich each hyperedge is assigned a weight. We use w(e)\nto denote the weight given to e. The degree of a hyperedge\ne, denoted as δ(e), is the number of vertices in e. For a\n363Oral Session 5: Tags\nstandard graph (sometimes called a “2-graph”) the value\nofδis2for all edges. The degree d(v)of a vertex vis\nd(v) =/summationtext\nv∈e,e∈Ew(e). The vertex-edge incidence matrix\nH∈R|V|×|E|is deﬁned as: h(v, e) = 1 ifv∈eand0\notherwise. We thus have\nd(v) =/summationdisplay\ne∈Ew(e)h(v, e) (1)\nδ(e) =/summationdisplay\nv∈Vh(v, e). (2)\nLetDe(respectively, DvandW) be the diagonal matrix\nwhose diagonal entries are d(v)(respectively, δ(e), and\nw(e)).\nThe graph Laplacian is the discrete analog of the\nLaplace-Beltrami operator on compact Riemannian man-\nifolds [12]. The graph Laplacian has been widely used\nin unsupervised learning (e.g., spectral clustering [9]) and\nsemi-supervised learning (e.g. [16, 20]). Below we will\nsketch a commonly used algorithm by Chung [3], called\ntheClique Expansion Algorithm , for constructing the hy-\npergraph Laplacian.\nThe Clique Expansion Algorithm constructs a tradi-\ntional 2-graph Gc= (Vc,Ec)from the original hypergraph\nG= (V,E)and views the Laplacian of Gcto be the Lapla-\ncian of G. Suppose Vc=VandEc={(u, v)|u, v∈e, e∈\nE}. The edge weight wc(u, v)ofGcis deﬁned by\nwc(u, v) =/summationdisplay\nu,v∈e,e∈Ew(e) (3)\nAn interpretation of this deﬁnition is that the edge weight\nmatrix, Wc, ofGccan be expressed as\nWc=HWHT(4)\nLetDcbe the diagonal matrix such that\nDc(u, u) =/summationdisplay\nvwc(u, v).\nThen the combinatorial Laplacian, Lc, ofGcis given by\nLc=Dc−Wc=Dc−HWHT(5)\nand the normalized Laplacian, Ln, is given by\nLn=I−D−1/2\ncHWHTD−1/2\nc. (6)\nFrom Eq. (5) and (6), we have\nLn=D−1/2\ncLcD−1/2\nc. (7)\nIn our music style classiﬁcation, we construct two hyper-\ngraphs: the style hypergraph Gsand the tag hypergraph Gt.\nThe vertices of GsandGtare simply the data points. The\nhyperedges of Gscorrespond to the style labels, i.e., each\nhyperedge in Gscontains all the data points that belong to\na speciﬁc style category. Similarly, each hyperedge of Gt\ncontains all the data points that own the corresponding tag.\nFigure 1 shows an intuitive example on the music style and\ntag hypergraphs.\n\"Who is he\"\n\"Strip\"Soul\nFigure 1 . An example of the music style (left) and tag\n(right) hypergraph. The nodes on the hypergraphs corre-\nspond to the music “Angola Bond”, “Who is he”, “Dan-\ngerous”, “Pleasure”, and “Strip”. The regions of different\ncolors correspond to the different hyperedges. The hyper-\nedges correspond to music styles in the left panel and to\nmusic tags in the right panel.\n3. MULTI-LABEL LEARNING WITH\nHYPERGRAPH REGULARIZATIONS\nIn this section we will present in detail our proposed multi-\nlabel classiﬁcation algorithm with hypergraph regulariza-\ntion. Suppose there are ntraining samples {(xi, yi)}n\ni=1,\nwhere each instance xiis drawn from some domain X ⊆\nRmand its label yiis a subset of the output label set Y=\n{1,···, k}. For example, if xibelongs to categories 1,3,\nand4, then yi={1,3,4}. We use X= (x1,···,xn)Tto\nrepresent the data feature matrix.\nOur basic strategy is to solve the multi-label learning by\ncombing a label ranking problem and a label number pre-\ndiction problem. That is, for each instance we produce a\nranked list of all possible labels, estimate the number of la-\nbels for the instance, and then select the predicted number\nof labels from the list.\nLabel ranking is the task of inferring a total order over\na predeﬁned set of labels for each given instance [5].\nGenerally, for each category, we deﬁne a linear function\nfi(x) =/angbracketleftwi,x/angbracketright+bi(i= 1,···, k), where /angbracketleft·,·/angbracketrightis the\ninner product and biis a bias term. One often deals with\nthe bias term by appending to each instance an additional\ndimension\nxT←[xT,1],wT\ni←[wT\ni, bi] (8)\nthen the linear function becomes fi(x) =/angbracketleftwi,x/angbracketright. The\ngoal of label ranking is to order {fi(x), i= 1,···, k}for\neach instance xaccording to some predeﬁned empirical\nloss function and complexity measures. Elisseeff and We-\nston [6] apply the large margin idea to multi-label learn-\ning and present an SVM-like ranking system, called Rank-\nSVM, given as follows:\nmin1\n2k/summationdisplay\ni=1/bardblwi/bardbl2+Cn/summationdisplay\ni=11\n|yi||yi|/summationdisplay\n(p,q)∈yi×yiξipq\ns.t. /angbracketleftwp−wq,xi/angbracketright ≥1−ξipq,(p, q)∈yi×yi\nξipq≥0 (9)\n36410th International Society for Music Information Retrieval Conference (ISMIR 2009)\nwhere C≥0is a penalty coefﬁcient that trades off the\nempirical loss and model complexity, yiis the comple-\nmentary set of yiinY,|yi|is the cardinality of the set\nyi, i.e., the number of elements of the set yi, and ξipq(i=\n1,···, n; (p, q)∈yi×yi)are slack variables. The margin\nterm/summationtextk\ni=1/bardblwi/bardbl2controls the model complexity and im-\nproves the model generalization performance. Although\nthis approach performs better than Binary-SVM in many\ncases, it still does not model the category correlations\nclearly. Next, we will describe how to construct a hyper-\ngraph to exploit the category correlations and how to in-\ncorporate the hypergraph regularization into the problem\nin the form of Eq. (9).\n3.1 Basic Framework\nTo model the correlations among different categories effec-\ntively, a hypergraph is built where each vertex corresponds\nto one training instance and a hyperedge is constructed for\neach category which includes all the training instances rel-\nevant to the same category. Here, we apply the Clique Ex-\npansion algorithm to construct the similarity matrix of the\nhypergraph. It means that the similarity of two instances\nis proportional to the sum of the weights of their com-\nmon categories, thereby captures the higher order relations\namong different categories. This kind of hypergraph struc-\nture was used in the feature extraction by spectral learn-\ning [13]. However, we consider how to apply the relation\ninformation encoded in the hypergraph to directly design\nthe multi-label learning model. Intuitively, two instances\ntend to have a large overlap in their assigned categories if\nthey share high similarity in the hypergraph. Formally, this\nsmoothness assumption can be expressed using the hyper-\ngraph Laplacian regularizer, trace (/hatwideFTL/hatwideF). Therefore we\ncan introduce the smoothness assumption into problem Eq.\n(9) and obtain\nmin1\n2k/summationdisplay\ni=1/bardblwi/bardbl2+1\n2λtrace(/hatwideFTL/hatwideF) +\nCn/summationdisplay\ni=11\n|yi||yi|/summationdisplay\n(p,q)∈yi×yiξipq\ns.t. /angbracketleftwp−wq,xi/angbracketright ≥1−ξipq,(p, q)∈yi×yi\nξipq≥0 (10)\nHere/hatwideFis the matrix of label prediction; that is, it is the\nn×kmatrix (fj(xi)),1≤i≤n,1≤j≤k. Also, Lis\nthen×nhypergraph Laplacian and λ/greaterorequalslant0is a constant that\ncontrols the model complexity in the intrinsic geometry of\ninput distribution.\n3.2 Optimization Strategy\nProblem (10) is a linearly constrained quadratic convex op-\ntimization problem. To solve it, we ﬁrst introduce a dual\nset of variables, one for each constraint, i.e., αipq≥0for\n/angbracketleftwp−wq,xi/angbracketright −1 +ξipq≥0andηipqforξipq≥0. Af-\nter some linear algebraic derivation, we obtain the dual ofProblem (10) as\nming(α) =1\n2k/summationdisplay\np=1n/summationdisplay\nh,i=1βphβpixT\nh(I+λXTLX)−1xi\n−n/summationdisplay\ni=1/summationdisplay\n(p,q)∈yi×yiαipq\ns.t. 0≤αipq≤C\n|yi||yi|(11)\nwhere αdenotes the set of dual variables αipqandIis the\n(m+ 1)×(m+ 1) identity matrix.\nOnce the variables αipqthat minimize g(α)are ob-\ntained, we can compute wpby\nwp= (I+λXTLX)−1n/summationdisplay\ni=1βpixi (12)\nwhere\nβpi=/summationdisplay\n(j,q)∈yi×yitp\nijqαijq (13)\ntp\nijq=\n\n1\n−1\n0j=p\nq=p\nifj/negationslash=pandq/negationslash=p(14)\nCompared to the primal optimization problem, the dual\nhask(m+ 1) less variables and includes simple box con-\nstraints. The dual can be solved by the dual coordinate\ndescent algorithm shown in Algorithm 1.\nAlgorithm 1 A dual coordinate descent method for\nHiSVM\nStart with α=0∈Rnα(nα=/summationtextn\ni=1|yi||yi|), and the\ncorresponding wi=0(i= 1,···, k)\nwhile 1do\nfori= 1,···, nand(j, q)∈yi×yido\n1.G= (wp−wq)Txi−1\n2.PG=\n\nG\nmin(0 , G)\nmax(0 , G)if0< αipq<C\n|yi||yi|\nifαipq= 0\nifαipq=C\n|yi||yi|\n3. If|PG| /negationslash= 0,\nα∗\nipq←min/parenleftBig\nmax/parenleftBig\nαipq−G\n2Aii,0/parenrightBig\n,C\n|yi||yi|/parenrightBig\nwp←wp+ (α∗\nipq−αipq)(I+λXTLX)−1xi\nwq←wq−(α∗\nipq−αipq)(I+λXTLX)−1xi\nend for\nif/bardblα∗−α/bardbl//bardblα/bardbl< /epsilon1(e.g./epsilon1= 0.01)then\nBreak\nend if\nα=α∗\nend while\n3.3 Predicting the Size of Label Set\nSo far we have only provided a label ranking algorithm. To\nidentify the ﬁnal labels of data, we need to design an ap-\npropriate threshold for each instance to determine the size\n365Oral Session 5: Tags\nof its corresponding label set. Here, we adopt the strategy\nproposed by Elisseeff and Weston [6], which treats thresh-\nold designing as a supervised learning problem. More con-\ncretely, for each instance x, deﬁne a threshold function\nh(x)and the size of label set s(x) =/bardbl{j|fj(x)>\nh(x), j= 1,···, k}/bardbl. Our goal is to obtain h(x)through\na supervised learning method. For the training data xi,\nits label ranking values, f1(xi),···, fk(xi), can be given\nby the foregoing ranking algorithm, and its corresponding\nthreshold h(xi)is simply deﬁned by\nh(xi) =1\n2(min\nj∈yi{fj(xi)}+ max\nj∈yi{fj(xi)})\nOnce the training data (x1, h(x1)),···,(xu, h(xu))\nare generated, we can use off-the-shelf learning methods\nto learn h(x). In this paper, Linear Support Vector Regres-\nsion [15] has been adopted to solve h(x). We note there\nthat all the label ranking based algorithms toward multi-\nlabel learning can use this postprocessing approach to pre-\ndict the size of label set.\n4. DATA DESCRIPTION\nFor experimental purpose, we created a data set consisting\nof 403 artists. For each artist, we include a representative\nsong and also obtain the style and tag description.\n4.1 Music Content Features\nFor each song, a single vector of 80 components is ex-\ntracted. The single vector contains the following audio\nfeatures:\n1) Mel-Frequency Cepstral Coefﬁcients (MFCC): Mel-\nFrequency Cepstral Coefﬁcients (MFCC) is a feature set\nthat is very popular in speech processing. MFCC is de-\nsigned to capture short-term spectral based features. The\nfeatures of MFCC are computed as follows: First, for\neach frame, the logarithm of the amplitude spectrum based\non short-term Fourier transform is calculated, where the\nfrequencies are divided into thirteen bins using the Mel-\nfrequency scaling. Next, this vector is decorrelated using\ndiscrete cosine transform. The resulting vector is called the\nMFCC vector. In our experiments, we compute the mean\nand variance of each bin over the frame for the two vectors\n(before and after decorrelation). Thus, for each sample,\nMFCC occupies 52 components.\n2) Short-Term Fourier Transform Features (STFT): This\nis a set of features related to timbral textures and is not\ncaptured using MFCC. It consists of the following types\nof features: Spectral Centroid, Spectral Rolloff, Spectral\nFlux, Zero Crossings, and Low Energy. More detailed de-\nscriptions of STFT can be found in [14]. In our experi-\nments, we compute the mean for all types and the variance\nfor all but zero crossings. STFT thus occupies 12 compo-\nnents.\n3) Daubechies Wavelet Coefﬁcient Histograms\n(DWCH): Daubechies wavelet ﬁlters are a set of ﬁl-\nters that are popular in image retrieval (see [4]). The\nDaubechies Wavelet Coefﬁcient Histograms, proposedin [8], are features extracted in the following manner:\nFirst, the Daubechies-8 (db8) ﬁlter with seven levels of\ndecomposition (or subbands) is applied to 30 seconds\nof monaural audio signals. Then, the histogram of the\nwavelet coefﬁcients is computed for each subband. Then\nthe ﬁrst three moments of each histogram, i.e., the average,\nthe variance, and the skewness, are calculated from each\nsubband. In addition, the subband energy, deﬁned as the\nmean of the absolute value of the coefﬁcients, is computed\nfrom each subband. More details of DWCH can be found\nin [8].\n4.2 Music Tag Information\nMusic tags are descriptions given by visitors or music tag\neditors from the website to express their idea on the mu-\nsic artists. Tags can be as simple as a word or as com-\nplicated as a whole sentence. Popular tags are terms like\n“rock,” “black metal,” and “indie pop.” Long tags are like\n“I love you baby can I have some more.” The tags are\nnot as formal as style description created by music experts,\nbut they give us ideas of how large population music lis-\nteners think about the music artists. In our experiments,\ntag data was collected from the popular music website\nhttp://www.last.fm. In order to understand how important\na tag is, and how accurately it reﬂects the characteristics\nof an artist, the frequencies of all the tags to describe the\nartists (tag counts) were also used in the experiments.\nA total of 8,529 tags were collected. Each artist has at\nmost 100 tags and at least 3 tags. On average, each artist\nis associated with 89.5 tags. Note that, each artist may be\ndescribed by some tags for more than once, for example,\nMichael Jackson has been tagged with “80s” for 453 times.\n4.3 Music Style Information\nStyle data were collected from All Music Guide\n(http://www.allmusic.com). These data are created by mu-\nsic experts to describe the characteristics of music artists.\nStyle terms are nouns like Rock & Roll, Greek Folk, and\nChinese Pop as well as adjectives like Joyous, Energetic,\nand New Romantic. Styles for each artist/track are differ-\nent from the music tags described in the above, since each\nstyle name for one artist appears only once.\nA total of 358 styles were found. Each artist has at most\n12 and at least one style type. On average, every artist is\nassociated with 4.7 style labels.\n5. EXPERIMENTS\nWe performed experiments on HiSVM and four real-world\nmulti-label learning models arising in text categorization,\nimage classiﬁcation, video indexing and gene function pre-\ndiction. Comparisons are made with Binary-SVM and\nRank-SVM [6].\n5.1 Methods and Experimental Setup\nThe data set information we used to evaluate our pro-\nposed approach has been introduced in the previous sec-\ntion, where we use 70% of the data for training (282 pieces\n36610th International Society for Music Information Retrieval Conference (ISMIR 2009)\ntotal), and the remaining 30% for testing (121 pieces total).\nHere, the four models used for multi-label learning are as\nfollows:\n•Binary-SVM. In this model, ﬁrst, for each category,\ntrain a linear SVM classiﬁer independently. Then,\nthe labels for each test instance can be obtained by\naggregating the classiﬁcation results from all the bi-\nnary classiﬁers. Here, we use LIBSVM [2] to train\nthe linear SVM classiﬁers.\n•Rank-SVM [6]. In this model, ﬁrst, using Eq. (9),\nwe implement Algorithm 1 ( λ= 0) to train a lin-\near label ranking system. We then apply the pre-\ndiction method for the size of label set described in\nSection 3.3 to design the threshold model. Finally,\nfor each test instance, we combine the label ranking\nand threshold models, thereby infer its labels.\n•HiSVM. This is our proposed algorithm. The algo-\nrithm is composed of three steps: (1) we implement\nAlgorithm 1 to achieve a linear label ranking system;\n(2) we apply the method in Section 3.3 to design the\nthreshold model; (3) for each test instance, we com-\nbine the label ranking and threshold models to infer\nits labels.\n•HSVM. HSVM is the style Hypergraph regularized\nSVM method, which is the same as the HiSVM\nmethod except that it only makes use of the style\nhypergraph and does not use the tag hypergraph.\n•GSVM. GSVM is similar to HiSVM except we con-\nstruct a traditional 2-graph where each vertex repre-\nsents one training instance in GSVM rather than a\nhypergraph. In order to compute the Laplacian, the\nweight wijof the edge between xiandxjis deﬁned\nas follows\nwij= exp( −ρ/bardblxi−xj/bardbl2) (15)\nwhere ρis a nonnegative constant. Apparently, the\ncategory correlation information is not used during\nthe construction of 2-graph in GSVM.\nSome details of HiSVM are in order. We use Eq. (5) to\nconstruct both the style hypergraph Laplacian Lsand the\ntag hypergraph Laplacian Lt, where the weight w(e)of the\nhyperedge is calculated by\nw(e) = exp( −νde) (16)\nHere νis a nonnegative constant, and deis the average\nintra-class distance (N.B. Each hyperedge corresponds to\none speciﬁc style or tag):\nde=/summationtext\nu,v∈e/bardblxu−xv/bardbl2\nδ(e)(δ(e)−1)(17)\nThe smaller the average intra-class distance, the larger the\ncorresponding hyperedge weight. Finally we combine Ls\nandLtto obtain a uniﬁed hypergraph Laplacian Lby\nL=1\n2(Ls+Lt)Table 1 . A contingency table\nYES is correct NO is correct\nAssigned YES a b\nAssigned NO c d\nwhich is used in the rest of the inferences and experiments.\nIn the above four models, it is necessary to iden-\ntify the best value of model parameters such as C,\nλandνon the training data. Here, the grid search\nmethod with 5-fold cross validation is used to de-\ntermine the best parameter values. For the penalty\ncoefﬁcient Cin the Linear SVM, we tune it from\nthe grid points {10−6,10−5,···,100,101,···,106}; for\nthe tradeoff parameter λ, we tune it from the grid\npoints {10−6,10−5,···,100,101,···,106}; the scale\nparameter νandρare tuned from the grid points\n{2−6,2−5,···,20,21,···,26}.\n5.2 Evaluation Metrics\nWe choose two measures, F1Macro andF1Micro [17],\nas the evaluation metrics for multi-label learning. Suppose\nthere are a total of Sstyle categories. Then for each cate-\ngory, we can construct a contingency table as follows: Let\na(respectively, b) be the number of pieces that are cor-\nrectly assigned (respectively, not correctly assigned) to this\nstyle category, and let c(respectively, d) be the number\nof pieces that are incorrectly rejected (respectively, cor-\nrectly rejected) by this style category (see Table 1). Let\nr=a/(a+c)andp=a/(a+b), where the former is\ncalled the recall and the latter the precision. Then the F1\nscore of this style category can be computed as\nF1=2pr\np+r(18)\nTheF1Macro can be computed by ﬁrst calculating the\nF1scores for the per-category contingency tables and then\naveraging these scores to compute the global means. F1\nMicro can be computed by ﬁrst constructing a global con-\ntingency table, each of whose cell value is the sum of the\ncorresponding cells in the per-category contingency tables,\nand then use this global contingency table to compute the\nMicro F1score.\n5.3 Experimental Results\nTable 2 illustrates the experimental results on our HiSVM\nalgorithm along with the four other methods on the data\nset. The values in Table 2 are the F1Micro values and\nF1Macro values averaged over 50 independent runs to-\ngether with their standard deviations. From the table we\ncan clearly observe the following:\n•Multi-label methods perform better than the simple\nBinary-SVM method.\n•The consideration of label correlations is helpful for\nthe ﬁnal algorithm performance.\n367Oral Session 5: Tags\n020 40 60 8010000.20.40.60.811.21.4\nnumber of iteration stepsrelative errorFigure 2 . The relative error /bardblα∗−α/bardbl//bardblα/bardblvs. itera-\ntion step plot of our proposed dual coordinate descent al-\ngorithm for solving HiSVM.\nTable 2 . Performance comparisons of four models on the\nLast.fm dataset\nMethods F1Macro F1Micro\nBinary-SVM 0.4231±0.0025 0 .4317±0.0103\nRank-SVM 0.4526±0.0114 0 .4733±0.0036\nGSVM 0.5018±0.0054 0 .5244±0.0103\nHSVM 0.5365±0.0120 0 .5509±0.0072\nHiSVM 0.5613±0.0069 0 .5802±0.0116\n•Hypergraph regularization is better than ﬂat two-\ngraph regularization because it can incorporate the\nhigh-order label relationships naturally.\n•The incorporation of tag information is helpful for\nthe ﬁnal classiﬁcation performance.\nFigure 2 shows how the relative error /bardblα∗−α/bardbl//bardblα/bardbl\nvaries with the process of iteration using the dual coordi-\nnate descent method introduced in Algorithm 1. From the\nﬁgure we clearly see that with the process of coordinate de-\nscent, the relative error will decrease and it takes approxi-\nmately 30 steps to converge. This validates the correctness\nof our algorithm experimentally.\n6. CONCLUSION\nWe propose a novel multi-label classiﬁcation method\ncalled Hypergraph integrated SVM (HiSVM) for music\nstyle classiﬁcation. Our method can not only take into ac-\ncount the music style correlations, but also the music tag\ncorrelations. We also propose an efﬁcient dual coordinate\ndescent algorithm to solve it, and ﬁnally experimental re-\nsults on a real world data set are presented to show the\neffectiveness and correctness of our algorithm.\n7. REFERENCES\n[1]C. Berge. Graphs and Hypergraphs. Elsevier, 1973.[2]C.-C. Chang and C.-J. Lin, LIBSVM: a library\nfor support vector machines, 2001, software\navailable at http://www.csie.ntu.edu.tw/\ncjlin/libsvm.\n[3]F. R. K. Chung. The laplacian of a hypergraph. In Ex-\npanding Graphs, DIMACS Series, V ol. 10, AMS, 1993.\n[4]I. Daubechies. Ten lectures on wavelets. SIAM, 1992.\n[5]O. Dekel, C. D. Manning, and Y . Singer. Log-linear\nmodels for label ranking. In Proc. of NIPS, 2003.\n[6]A. Elisseeff and J. Weston. A kernel method for multi-\nlabelled classiﬁcation. In Proc. of NIPS, 2001.\n[7]T. Li and M. Ogihara. Towards intelligent music in-\nformation retrieval. IEEE Transactions on Multimedia\n8(3):564-575, 2006.\n[8]T. Li, M. Ogihara, and Q. Li. A comparative study on\ncontent-based music genre classiﬁcation. In Proceed-\nings of SIGIR, pages 282-289, 2003.\n[9]U. von Luxburg. A tutorial on spectral clustering. Max\nPlanck Institute for Biological Cybernetics, Tech. Rep.,\n2006.\n[10] D. Qin and G. Z. Ma. Music style identiﬁcation sys-\ntem based on mining technology. Computer Engineer-\ning and Design. 26, 3094-3096. 2005.\n[11] S. Chen, F. Wang and C. Zhang: Simultaneous hetero-\ngeneous data clustering based on higher order relation-\nships. ICDM Workshops 2007: 387-392.\n[12] S. Rosenberg. The Laplacian on a Remannian mani-\nfold. London Math. Soc., 1997.\n[13] L. Sun, S. Ji, and J. Ye. Hypergraph spectral learning\nfor multilabel classiﬁcation. In Proc. KDD, 2008.\n[14] G. Tzanetakis and P. Cook. Music genre classiﬁcation\nof audio signals. IEEE Transactions on Speech and Au-\ndio Processing, 10(5):293-302, 2002.\n[15] V . Vapnik. The Nature of Statistical Learning Theory.\nSpringer, 1995.\n[16] F. Wang and C. Zhang. Label Propagation Through\nLinear Neighborhoods. In Proc. ICML, pages 985-992,\n2006.\n[17] Y . Yang. An evaluation of statistical approaches to text\ncategorization. Information Retrieval 1:69–90, 1999.\n[18] Y . B. Zhang and J. Zhou. A study on content-based mu-\nsic classiﬁcation. In Proc. IEEE Signal Processing and\nIts Applications, 2003.\n[19] Y . Zhou, T. Zhang, and J. Sun. Music style classiﬁca-\ntion with a novel Bayesian model. In Proc. Advanced\nData Mining and Appliations, Springer, 2006.\n[20] X. Zhu. Semi-supervised learning literature survey.\nUniversity of Wisconsin-Madision, Technical Report\nTR 1530, 2006.\n368"
    },
    {
        "title": "Automatic Generation of Lead Sheets from Polyphonic Music Signals.",
        "author": [
            "Jan Weil",
            "Thomas Sikora",
            "Jean-Louis Durrieu",
            "Gaël Richard"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1414758",
        "url": "https://doi.org/10.5281/zenodo.1414758",
        "ee": "https://zenodo.org/records/1414758/files/WeilSDR09.pdf",
        "abstract": "A lead sheet is a type of music notation which summarizes the content of a song. The usual elements that are reproduced are the melody, chords, tempo, time signature, style and the lyrics, if any. In this paper we propose a system that aims at transcribing both the melody and the associated chords in a beat-synchronous framework. A beat tracker identifies the pulse positions and thus defines a beat grid on which the chord sequence and the melody notes are mapped. The harmonic changes are used to estimate the time signature and the down beats as well as the key of the piece. The different modules perform very well on each of the different tasks, and the lead sheets that were rendered show the potential of the approaches adopted in this paper.",
        "zenodo_id": 1414758,
        "dblp_key": "conf/ismir/WeilSDR09",
        "keywords": [
            "lead sheet",
            "music notation",
            "summarizes",
            "song",
            "elements reproduced",
            "beat-synchronous framework",
            "melody",
            "chords",
            "beat tracker",
            "time signature"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nAUTOMATIC GENERATION OF LEAD SHEETS\nFROM POLYPHONIC MUSIC SIGNALS\nJan Weil, Thomas Sikora\nCommunication Systems Group\nTechnische Universit ¨at BerlinJ.-L. Durrieu, Ga ¨el Richard\nInstitut Telecom\nTelecom ParisTech\nCNRS LTCI\nABSTRACT\nA lead sheet is a type of music notation which summa-\nrizes the content of a song. The usual elements that are\nreproduced are the melody, chords, tempo, time signature,\nstyle and the lyrics, if any. In this paper we propose a sys-\ntem that aims at transcribing both the melody and the as-\nsociated chords in a beat-synchronous framework. A beat\ntracker identiﬁes the pulse positions and thus deﬁnes a beat\ngrid on which the chord sequence and the melody notes are\nmapped. The harmonic changes are used to estimate the\ntime signature and the down beats as well as the key of the\npiece. The different modules perform very well on each of\nthe different tasks, and the lead sheets that were rendered\nshow the potential of the approaches adopted in this paper.\n1. INTRODUCTION\nThe lead sheet format is a convenient form of music nota-\ntion for songs. It is mostly used for popular music and fa-\nmously represented by collections of Jazz standards, e.g.,\nThe Real Book . It allows the musician to see all the impor-\ntant elements necessary to perform a song in a very com-\npact format. It mostly consists of a single staff; the melody\nis notated in Western music standard, with the associated\nlyrics under the staff and the chord sequence noted above\nit. The lead sheet also often speciﬁes the style, i.e., the\nway the melody has to be played, e.g., straight or swung\nrhythm, and the way the accompaniment should be gener-\nated from the chords. Of course, it also deﬁnes the time\nsignature, the key and the tempo.\nVery few works have been oriented towards producing\nusable music scores directly from audio. In [1], the au-\nthors estimate the melody, the bass line, and the chords.\nHowever, the results are not temporally quantized, so the\noutput is not completely suited for lead sheet generation\nitself. This temporal quantization is indeed a non-trivial\nproblem and we propose a potential solution in this paper.\nThe proposed lead sheet transcription system can be\nbroken down into four seperate modules which exchange\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.\nBeat Tracking\nBeat Positions\nMelody Estimation\nMelody NotesSeparated AccompanimentHarmonic Analysis\nKeyMeasure GridChord Sequence\nSheet Rendering\nLead SheetFigure 1 . Modules of the proposed system along with the\nintermediate results they exchange. Dashed lines mark po-\ntential future dependencies.\nintermediate results. These modules are depicted in Fig. 1.\nThe beat tracker provides a continuous pulse grid which\nforms the temporal basis for the other modules. The algo-\nrithm favours faster tempi such that the risk of phase errors\nis minimized and ensures a continuos beat grid. The reader\nis refered to [2] for details about the chosen approach. In\nthis article, we directly use the output of this algorithm.\nTheithbeat position in seconds is denoted bi. The har-\nmonic module estimates beat-aligned chord sequences, the\nmost likely measure grid, and the key of the piece. The\nmeasure grid is in turn used to reﬁne the chord sequence by\nmaking chord change probabilities depend on the position\nin the measure. The chord detection module is based on the\napproach described in [3]. The melody module ﬁrst sepa-\nrates the main melody and the accompaniment building on\nthe approach presented in [4]. The model is extended such\nthat the fundamental frequencies of the main melody and\nthe musical (MIDI) notes of the melody are jointly esti-\nmated. The rendering module determines the appropriate\ntime signature, quantizes the note onsets and durations of\nthe melody to sub-divisions of the beat level, divides both\nmelody and chords in measure blocks, and applies pitch\nspelling depending on the estimated key.\nIn the following section we describe the chord detection\nscheme and how the down-beat positions are estimated us-\ning the detected chord sequence. After that the key es-\ntimation method is introduced. The melody extraction is\ndiscussed in Sec. 4. In Sec. 5, we describe how the lead\nsheets are rendered. Finally, we present the results as well\nas our conclusions and perspectives.\n603Poster Session 4\n2. ESTIMATION OF CHORDS AND MEASURES\n2.1 Chord detection\nThe chord detection module can be considered one of the\nnumerous followers of the approaches described in [5] and\n[3], which are based on Hidden Markov Models (HMM).\nWe model the chords as states of the HMM using a chord\nalphabet comprising major and minor chords, i.e., the er-\ngodic model has S= 24 statesωk,k∈[1,S]. The chord\nsequence is given as the most likely sequence of states\ngiven the observed feature sequence; this is known as the\ndecoding problem which is solved using the Viterbi algo-\nrithm. Training and decoding is done in a 10-fold cross-\nvalidation setup.\n2.1.1 Feature extraction\nBeat-synchronous chroma vectors computed from the au-\ndio data form the observable features. The audio signal\nis mixed to a single channel and downsampled to 11025\nHz. We compute a constant-Q spectrogram [6] from note\nE2 (82.4 Hz) to note D#6 (1.24 kHz) using a hop size of\n512 samples1. Due to the chosen lowest frequency the\nlength of the longest window is 4096 samples. Chroma\nvectors are computed by summing up the magnitude of the\ntransform for each of the 12 pitch classes over all four oc-\ntaves. We then use the result from the beat tracking module\nto average all feature vectors within beat boundaries. Let\nxc(i)denote the 12-dimensional chroma vector represent-\ning the time segment between beat positions biandbi+1,\ni= 1,2,....\n2.1.2 Training\nThe observation distribution is modeled as a multivariate\nGaussian per state with mean vectors µkand (full) co-\nvariance matrices Σk,k∈[1,S]. The prior probabilities\nare considered uniformly distributed. Both the transition\nprobabilities and the observation distribution are computed\nfrom the training sets using beat-quantized annotation data\nin a similar fashion as described for methods 1 and D in [7].\n2.1.3 Initial chord sequence decoding\nIn the ﬁrst stage, the chord sequence is decoded using the\nclassic Viterbi algorithm. Let q1(i)denote the initial es-\ntimate of the decoded chord symbol which is assumed to\nhave emittedxc(i). Based on this initially decoded se-\nquence we estimate the measure grid.\n2.2 Estimating the measure grid\nWe assume that the probability of chord changes depends\non the position in a measure and that, generally, chords are\nmore likely to change at the beginning of measures [8].\nWe also assume a constant time signature; we do not, how-\never, assume a 4/4 meter (although it clearly dominates our\ndatabase). We consider a set of measure grid candidates of\nwidthν∈[3,4,...,8], i.e., each third, fourth, ..., eighth\n1Note that, for the database we used, we can consider all pieces per-\nfectly tuned to A4 = 440 Hz\n1 2 3 400.51\n1 2 3 4 5 600.51\n1 2 3 4 5 6 7 800.51\nbeat in measureFigure 2 . Probability of chord changes depending on the\nposition in the measure for 4, 6, and 8 beats in a measure,\nrespectively.\nbeat is assumed a down-beat. For each νwe have to con-\nsiderνpotential phase candidates φ∈[1,ν], i.e., the ﬁrst\ndown-beat is b1,b2, ..., orbν. For each of these grid width\nand phase candidate pairs, we compute the score\ns(ν,φ) =Tcc(ν,φ)−Fcc(ν,φ), (1)\nwhereTcc(ν,φ)denotes the number of grid points which\nfall on beat positions with a chord change, i.e., q1(i−1)/negationslash=\nq1(i), andFcc(ν,φ)denotes the number of grid points with-\nout chord changes. The pair (νo,φo) = arg max s(ν,φ)\ndetermines the chosen measure grid. Note that νdoes not\nnecessarily correspond to the numerator of the time signa-\nture as the beat we tracked may actually reﬂect half-time\nor double-time tempo.\n2.3 Reﬁned chord sequence decoding\nWe use the measure grid estimate to compute the reﬁned\nchord sequence q2(i)by making the transition probabili-\nties depend on the position in the measure. Based on the\ndown-beat information given by the annotation we com-\npute the distribution of chord change positions relative to\nthe measures from the training set. For the database we\nused, which will be discussed in Sec. 6, there are three\npossible values of ν: 4 (4/4 meter), 6 (6/8 meter), and 8\n(4/4 meter; beat represents 8th notes). Fig. 2 depicts an\nexample for the resulting probability proﬁles. As antici-\npated, chords are most likely to change on the beginning\nof a measure. We now propose a modiﬁed Viterbi decod-\ning procedure. As we assume a continuous beat and mea-\nsure grid, we can compute the current beat position in a\nmeasurebm= (i−φo) (modνo) + 1 . Now the transition\nprobability matrix is modiﬁed in the following manner: the\ndiagonal, i.e., the probability to remain in the current state,\nis set to 1−pcc(bm), wherepcc(bm)denotes the probabil-\nity of a chord change at beat position bmin the measure.\nThe remaining non-diagonal elements are scaled such that\nthey add up to pcc(bm). Decoding the HMM using these\nvarying transition probabilities gives the reﬁned chord se-\nquenceq2(i).\n60410th International Society for Music Information Retrieval Conference (ISMIR 2009)\n3. KEY ESTIMATION\nTo estimate the key one can compute an average chroma\nproﬁle and correlate it to key-speciﬁc templates [9]. In-\nstead, we propose to compute the mean vector of the chord\nlikelihoods using the trained Gaussian distributions for the\nchord states. We compare both approaches. We train key\ntemplate proﬁles for major and minor keys which are cir-\ncularly shifted to form all 24 possible key proﬁles. To this\nend,xc(i)is circularly shifted such that the key is mapped\nto the root C for all pieces in the training set. The chroma-\nbased templates µK1(m)for both key modes m, major\nand minor, are computed as the mean vector of all shifted\nchroma vectors representing mode m. These templates\nhave dimension 12. For the chord-based templates, the\nmulti-variate Gaussian distribution is evaluated to compute\nthe likelihoods P(xc|ωk). The 24-dimensional mean vec-\ntor of these chord likelihoods for both modes m, normal-\nized to add up to one, gives the second set of key templates\nµK2(m). To estimate the key of a piece we compute both\nthe mean chroma vector and the normalized mean chord\nlikelihoods. Then we compute the dot product of these\ntest proﬁles and all 12 shifted variants of the two key tem-\nplates as a measure of correlation. Note that for µK2(m)\nthe two halves of the likelihood vectors representing ma-\njor and minor chords must be shifted independently. The\nkey for which the template maximizes the dot product is\nchosen. This is done for both µK1(m)andµK2(m)to\ncompare the results.\n4. MAIN MELODY ESTIMATION\n4.1 Global model for main melody sequence\nOur model for melody estimation is based on the model\nproposed in [4]. In order to achieve a meaningful quan-\ntization of the desired melody line, we adapted the note\nduration model initially proposed in [10].\nThe observation audio signal xis considered as the instan-\ntaneous mixture of two contributions, the main instrument\nvoicevplaying the main melody and the accompaniment\nor background music m, i.e.,x=v+m. This relation\nstays valid for the short time Fourier transform matrices\nX,VandMof these signals. We assume that the sig-\nnal was decomposed into Nframes, with Fourier trans-\nforms ofFpositive frequency bins. We model the complex\nFourier transforms as complex proper centered Gaussians,\nfor which we more speciﬁcally model the variances.\nOn one hand, for the accompaniment M, the “Nonneg-\native Matrix Factorization” (NMF) model is retained. The\nresulting variance SMn(f)forMn(f), at framenand fre-\nquencyfis then given by:\nSMn(f) =R/summationdisplay\nr=1WM(f,r)HM(r,n), (2)\nwhereRis the number of elements in the spectrum dictio-\nnaryWMandHMis the activation coefﬁcient matrix asso-\nciated toWM. In matrix notation, with the variance matrix\nSMsuch thatSM(f,n) =SMn(f):SM=WMHM.\nlevelFilterK(n−1) K(n) K(n+ 1)Main\nInstrument\nlevelVn−1(f) Vn(f) Vn+1(f)Fundamental\nFrequency\nlevelF0(n−1) F0(n) F0(n+ 1)Note level E(n) E(n−1) E(n+ 1)\nFigure 3 . Generative model for the main instrument\nsource/ﬁlter model.\nOn the other hand, the main instrument voice Vis mod-\nelled through a source/ﬁlter model. The source part is\ndriven by a three-layer generative model, shown on the up-\nper part of Fig. 3. The ﬁlter part is modelled thanks to a\ntwo-layer model (lower part of Fig. 3). Note that the main\ninstrument level Vis also a hidden layer which, along with\nthe accompaniment level M, gives the mixture observation\nlevelX.\nThe source level comprises two hidden levels. First,\nthe fundamental frequency level F0(n)controls the pitch\nof the main instrument. These variables are dependent on\nthe second layer, the note level. The evolution between\nthe states of the note level E(n)and the fundamental fre-\nquency states are explained in Sec. 4.2.\nThe ﬁlter layer is simpler, because here, we are more in-\nterested in the note and frequency levels. Therefore, we al-\nlow more ﬂexibility in the evolution of the ﬁlter part and do\nnot model any constraint on the corresponding sequence.\nThe main instrument level is then generated with the ﬁl-\nter and fundamental frequency levels. The variance matrix\nSVforVn(f), such thatSV(f,n) =SVn(f), is given by:\nSV= (WΦ/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownright\nWΓHΓHΦ)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nFilter part.∗(WF0HF0)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nSource part, (3)\nwhereWΓis aF×Pdictionary of Psmooth atomic el-\nements,WF0a dictionary of NF0spectral combs for the\nvoiced source part and HΓthe coefﬁcient matrix such that\nthe actual ﬁlter dictionary WΦ=WΓHΓ. The activation\ncoefﬁcient matrices for the ﬁlter and the source parts re-\nspectively are HΦandHF0.\nThe optimal note sequence E={E(1),...,E (N)}is\nestimated within a Maximum Likelihood (ML) framework:\nˆE,ˆF0,ˆK=argmaxE,F0,Klogp(X,E,F 0,K). (4)\nSuch an estimation is computationally too intensive, and\nwe propose in the next section some simpliﬁcations to es-\ntimate the different levels of the problem.\n605Poster Session 4\n4.2 Model Approximations\nIn order to estimate the desired note sequence, we ﬁrst ne-\nglect the constraint of having only one ﬁlter per frame. We\nthen limit the problem to:\nˆE,ˆF0=argmaxE,F0logp(X,E,F 0), (5)\nThe right-hand side of Eq. (5) can be further expressed as:\nlogp(X,E,F 0) = log p(X|F0) + log p(F0|E) + log p(E),\nwhere, as shown on Fig. 3, we use that the sequence Xis\nindependent from Econditional on F0. Furthermore, we\nassume that:\nlogp(X|F0)≈/summationdisplay\nnlog˜HF0(F0(n),n). (6)\nIn (6), the observation likelihood conditional on the melody\nfundamental frequency is approximated with a modiﬁed\nversion ˜HF0of the source activation coefﬁcient matrix HF0\ncalculated on the data as described in [4]. During this ﬁrst\nestimation round, the observation frames are assumed in-\ndependent. We set ˜HF0=HF0and then normalize each\ncolumn of ˜HF0by its maximum value.\nThe log-likelihood of the fundamental frequency sequence,\nconditional on the note state sequence, logp(F0|E)is equal\nto:\nN/summationdisplay\nn=2logp(F0(n)|F0(n−1),E(n)) + log p(F0(1)|E(1))\nStrictly speaking, F0(n)should also depend on E(n−1),\nbut for simplicity, we drop this dependency. We further\nassume that p (F0(n)|F0(n−1),E(n))is proportional to\nthe product:\np(F0(n)|F0(n−1))×p(F0(n)|E(n)).\np(F0(n)|F0(n−1))is aprior that simulates smooth f0\nvariations. p (F0(n)|E(n))penalizes the distance between\nthe fundamental frequency and the “expected” frequency\nfor the note state E(n). These functions are set to:\np(F0(n) =f2|F0(n−1) =f1)∝exp(−α|log2(f2\nf1)|),\np(F0(n) =f0|E(n) =e)∝exp(−β|log2(f0/fe)|2),\nwherefeis the “standard” frequency for note E=e.\nAt last, we use the “segmental” duration model in [10] for\nthe note state evolution:\nlogp(E1:n) = log p(En|E1:n−1) log p(E1:n−1).(7)\nThe interested reader may ﬁnd more information on this\nmodel in [10] , especially on the exact equations for the\ndurations as well as on the beam searching algorithm that\nallows to ﬁnd an optimal path for the sequence E.\nTo put it in a nutshell, we proceed as follows:\n1. First assuming the independence of neighbouring\nframes, the parameters for the fundamental frequency\nand the ﬁlters are globally estimated.2. We then extract pitch candidates for the main melody\nfrom the matrix HF0and use them to restrain the\nrange of pitches to be tested when looking for the\noptimal path.\n3. Finally, we ﬁnd the optimal path of sequences Eand\nF0using a beam search strategy, maximizing the ap-\nproximated likelihood Eq. (5).\n4.3 Generating a usable melody track transcription\nThe note sequence must be further quantized to produce\na musical score. The fundamental frequencies are quan-\ntized onto the Western musical scale using the model for\nthe sequence E. The temporal quantization is yielded to\nthe rendering module such that the time signature can be\nconsidered.\n5. LEAD SHEET GENERATION\nEventually, all the pieces of information are put together to\nrender a readable transcription. Depending on νoand the\nestimated tempo we choose an appropriate time signature.\nBoth the chords and the melody are processed in measure\nchunks. The onsets and the duration of the melody notes\nare quantized to a subdivision of quarter notes. These are\nusually eighth notes, which gives a good tradeoff between\nquantization errors and spurious notes. Depending on the\nestimated key, a simple pitch spelling algorithm is applied\nfor both notes and chords. Basically, we choose note and\nchord names such that the distance on the circle of ﬁfths is\nminimized.\n6. RESULTS AND EV ALUATION\nIn order to assess the different modules of the transcrip-\ntion system, we need a database for which the chords, the\nbeat, and the melody line are annotated. Assembling such a\ndatabase by manually annotating audio recordings is highly\ntime-consuming. We found using the Band-In-A-Box2\n(BIAB) format a convenient way of generating the annota-\ntion in a semi-automatic way. BIAB is software which gen-\nerates musical accompaniment given a sequence of chords,\na tempo, and a style; it also supports melody tracks. Thus,\nBIAB ﬁles contain all the information which is relevant\nfor the lead sheet generation task. Actually, BIAB even\nfeatures lead sheet printouts, which gives a convenient ref-\nerence for the subjective assessment of the results.\nOur database comprises 278 ﬁles adding up to about\n16.5 hours of audio material. It is a subset of the Pop &\nRock database gathered by members of the Yahoo BIAB\nuser group. Details are available on-line [11]. The ﬁles are\nrendered substituting the oboe for the singing voice, which\nis an instrument that shares a number of acoustic properties\nwith the human voice. We used a modiﬁed version of one\nof the BIAB parsers available on-line to extract the relevant\ninformation from the BIAB ﬁles.\n2http://www.band-in-a-box.com/\n60610th International Society for Music Information Retrieval Conference (ISMIR 2009)\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5020406080portion of db [%]\nratio detected temp/ground−truth tempoFigure 4 . Histogram of the ratio detected tempo / ground-\ntruth tempo over the entire database.\n6.1 Beat tracking evaluation\nWe use the same metric as in [2] to evaluate the beat track-\ning module. The performance measure is the fraction of\nthe longest continuous portion of the piece for which all\nbeats are detected. A ground-truth beat is considered cor-\nrectly tracked if the absolute distance to the nearest de-\ntected beat is smaller than 17.5 % of the period. If the ratio\nof the detected tempo to the ground-truth tempo is either\ntwo or three, we only consider every second or third beat,\nrespectively, during the evaluation and choose the starting\nbeat which maximizes the performance (see [2] for de-\ntails). Fig. 4 depicts the histogram of the ratio detected\ntempo / ground-truth tempo . There is a single ﬁle for which\nthe ratio is 1.5 which must be considered wrong. The aver-\nage beat tracker performance is 94.1 %. For 91.4 % of all\npieces we correctly track more than 90 % of the beats.\n6.2 Down-beat tracking evaluation\nThe down-beat information implicitly given in the BIAB\nﬁles cannot be trusted. Historically, BIAB’s support for\nmeters other than 4/4 is weak and sometimes the system\nis abused, e.g., a 6/8 meter would be recorded as a slower\n4/4 meter where each beat of the 4/4 meter collects three\nbeats of the 6/8 meter. Generally, the beat given in BIAB\nﬁles is not guaranteed to correspond to the tactus period,\ni.e., the denominator of the time signature. It may reﬂect\nhalf-time tempo, double-time tempo, or ternary meters. To\nassess the proposed measure grid estimation approach we\nhave to take these peculiarities into account. In compliance\nwith the beat tracker performance measure we consider a\ndown-beat correctly detected if the absolute distance to the\nclosest ground-truth downbeat is less than 17.5 % of the\nperiod estimated by the beat tracking module. We com-\npute the down-beat performance measure as the fraction\nof the longest continuous portion for which all down-beats\nwere correctly detected. This is a particularly conservative\nmeasure as it combines both the result of the beat tracking\nmodule and the estimated measure grid based on detected\nchord change points. The average down-beat performance\nis 87.3 %.\n6.3 Chord estimation evalution\nWe use basically the same evaluation measure as applied\nto the 2008 MIREX chord detection task3. All annotated\nchord symbols are mapped to their root triads resulting in\n3MIREX 2008 Evaluation Campaign, website:\nhttp://www.music-ir.org/mirex/2008/ﬁve chord classes: major, minor, diminished, augmented,\nand suspended. (Note that 98.3 % of the chord symbols in\nour database fall into the major and minor categories.) This\nresults in 5·12 + 1 possible states, including the no-chord\nstate, which is used in the two pickup bars. The evalua-\ntion measure is the overlap in seconds between the detected\nchord sequence and the ground-truth sequence mapped to\nthe 61 possible states as described above. The average\noverlap for the entire database is 76.4 % for the initial chord\ndetection phase and 79.3 % for the reﬁned estimation us-\ning transition probabilities depending on the position in the\nmeasure. The average overlap quantized to beats, which\nis more relevant to the transcription task, is 80.0 %; it is\n82.7 % when the pickup bars are discarded.\n6.4 Key estimation evaluation\nFor transcription purposes, a confusion of relative major\nand minor keys does not matter as the key signature re-\nmains the same. To evaluate the key estimation algorithms\nwe thus compute the difference in the numbers of sharp\nor ﬂat symbols, i.e., the smallest distance on the circle of\nﬁfths either clockwise (positive) or counterclockwise (neg-\native). Fig. 5 shows the histogram of the key error mea-\nsure for both key estimation approaches over the entire\ndatabase. Both approaches correctly estimate the key sig-\nnature for the majority of the pieces. However, the portion\nof the database for which the absolute key signature error\nis not greater than one is 80.2 % using the chroma proﬁles\nand 93.5 % using the mean chord likelihoods. The chroma-\nbased approach is prone to confuse minor keys with their\nrelative major keys (+3), e.g., A major instead of A mi-\nnor, or with the key of the (major) dominant in the case\nof harmonic minor (+4), e.g., E major instead of A minor.\nExamining the statistics reveals that the variance remains\nsigniﬁcant for the chroma proﬁles. One could try to use\na Gaussian classiﬁer instead but, here, the method using\nthe mean chord likelihoods works very well. In Pop and\nRock music the chord range of the diatonic scale is often\nextended to include chords of keys which are close on the\ncircle of ﬁfths, e.g., a major chord on the minor 7th degree\nof a major scale (subdominant of the subdominant); this\nexplains absolute key signature errors of one.\n6.5 Melody tracking evaluation\nFor the melody estimation, we selected 11 songs that ﬁt our\ndeﬁnition of the main melody. For each song, the melody\nestimation algorithm returns the transcribed notes of the\nmelody, with their MIDI note number, onset and offset\ntimes. A transcribed note is considered correct if there is a\nnote in the reference with the same MIDI note number of\nwhich the onset time is close to the one of the transcribed\nnote. The absolute difference between these onset times\nshould be less than 150 ms. We compute precision, recall,\nand f-measure, and we provide the score obtained using the\nperceptually motivated measures in [12]. On our database,\nwe obtain average recall, precision and f-measure of, re-\nspectively, 63 %, 68 % and 63 %. The average perceptive\nF-measure is 69 %. Fig. 6 shows the box and whiskers for\n607Poster Session 4\n−6 −5 −4 −2 −1 0 1 2 3 4020406080portion of db [%]\n−5 −4 −3 −2 −1 0 1 2 3 4020406080portion of db [%]\nnumber of incorrect key signature symbolsFigure 5 . Histogram of the key signature error in steps on\nthe circle of ﬁfths for the chroma-based (top) and the chord\nlikelihood-based method (bottom).\nR P F Perc. F20406080percentage\nFigure 6 . Box and whiskers plot of the results for melody\nestimation: Recall (R), Precision (P), F-measure (F) and\nperceptive F-measure (Perc. F).\nthe 11 songs. The outlier corresponds to a song for which\nthe melody was too fast and too variable for the melody\ntracker to follow. The results are promising; however, the\ndatabase we used was rather small and experiments on a\nbigger and more realistic database should be held in the\nfuture.\n7. CONCLUSIONS AND PERSPECTIVES\nWe have proposed a lead sheet generation system. The\ntempo, time signature, chords, key, and melody were han-\ndled by several modules that can interact with each other.\nThe chord sequence helps in determining the time signa-\nture, which in turn can be used to reﬁne the chord sequence\nand also deﬁnes the minimum note duration for quantiz-\ning the melody. Our approach groups several modules that\nachieve state-of-the-art performance on each sub-task. As-\nsessing the overall quality of the generated transcription\nis not trivial and subjective evaluation should be held for\nthat purpose. For some examples available on-line [11] the\nresulting score is close to musician expectations. Some\nassumptions make the system targeted at Western music\ngenres like Pop and Rock as represented by the chosen\ndatabase. Evaluation of the sub-systems on real audio data\nremains to be done. The system could be further improved\nby allowing more joint estimations. A global model could\ncover all the aspects of the problem for which all the pa-\nrameters for the different modules are jointly estimated.\nHowever, as for the melody module, such a model mightbe too complicated to be directly solved. Instead, this inte-\ngration can be approximated for instance by including the\ndetected beat positions in the melody note duration model.\nThe melody estimation and separation can also be used to\nimprove the chord sequence estimation.\n8. ACKNOWLEDGEMENTS\nThis work was supported by the European Commission un-\nder contract FP6-027026 (KSpace), by the European Com-\nmunity’s Seventh Framework Programme [FP7/2007-2011]\nunder grant agreement no216444 (PetaMedia) and by\nOSEO, French State agency for innovation, as part of the\nQuaero Programme.\n9. REFERENCES\n[1] M. P. Ryynanen and A. P. Klapuri. Automatic tran-\nscription of melody, bass line, and chords in poly-\nphonic music. CMJ , 32(3):72–86, 2008.\n[2] J. Weil, J.-L. Durrieu, G. Richard, and T. Sikora. Beat\ntracking using the delta-phase matrix. Technical report,\nInstitut Telecom, Telecom ParisTech, CNRS LTCI,\n2009.\n[3] J.P. Bello and J. Pickens. A robust mid-level represen-\ntation for harmonic content in music signals. In ISMIR ,\npages 304–311, 2005.\n[4] J.-L. Durrieu, G. Richard, and B. David. An iterative\napproach to monaural musical mixture de-soloing. In\nICASSP , pages 105–108, 2009.\n[5] A. Sheh and D.P.W. Ellis. Chord segmentation and\nrecognition using EM-trained hidden Markov models.\nInISMIR , pages 185–191, 2003.\n[6] J.C. Brown and M.S. Puckette. An efﬁcient algorithm\nfor the calculation of a constant Q transform. JASA ,\n92:2698–2698, 1992.\n[7] H. Papadopoulos and G. Peeters. Large-scale study of\nchord estimation algorithms based on chroma repre-\nsentation and hmm. In CBMI’07 , pages 53–60, 2007.\n[8] H. Papadopoulos and G. Peeters. Simultaneous estima-\ntion of chord progression and downbeats from an audio\nﬁle. In ICASSP , pages 121–124, 2008.\n[9] E. G ´omez. Tonal Description of Music Audio Signals .\nPhD thesis, Universitat Pompeu Fabra, 2006.\n[10] E. Vincent. Musical source separation using time-\nfrequency source priors. IEEE Trans. on Audio,\nSpeech, and Lang. Proc. , 14(1):91–98, 2006.\n[11] Accompanying website. http://www.nue.tu-\nberlin.de/research/leadsheets/.\n[12] A. Daniel, V . Emiya, and B. David. Perceptually-based\nevaluation of the errors usually made when automat-\nically transcribing music. In ISMIR , pages 550–555,\n2008.\n608"
    },
    {
        "title": "Robust and Fast Lyric Search based on Phonetic Confusion Matrix.",
        "author": [
            "Xin Xu",
            "Masaki Naito",
            "Tsuneo Kato",
            "Hisashi Kawai"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1418227",
        "url": "https://doi.org/10.5281/zenodo.1418227",
        "ee": "https://zenodo.org/records/1418227/files/XuNKK09.pdf",
        "abstract": "This paper proposes a robust and fast lyric search method for music information retrieval. Current lyric search systems by normal text retrieval techniques are severely deteriorated in the case that the queries of lyric phrases contain incorrect parts due to mishearing and misremembering. To solve this problem, the authors apply acoustic distance, which is computed based on a confusion matrix of an ASR experiment, into DP-based phonetic string matching. The experimental results show that the search accuracy is increased by more than 40% compared with the normal text retrieval method; and by 2% ∼4% compared with the conventional phonetic string matching method. Considering the high computation complexity of DP matching, the authors propose a novel two-pass search strategy to shorten the processing time. By pre-selecting the probable candidates by a rapid index-based search for the first pass and executing a DP-based search among these candidates during the second pass, the proposed method reduces processing time by 85.8% and keeps search accuracy at the same level as that of a complete search by DP matching with all lyrics.",
        "zenodo_id": 1418227,
        "dblp_key": "conf/ismir/XuNKK09",
        "keywords": [
            "Lyric Search",
            "Phonetic Confusion Matrix",
            "Acoustic Distance",
            "Phonetic String Matching",
            "Music Information Retrieval",
            "Dynamic Programming",
            "Query Mishearing",
            "Search Accuracy",
            "Two-Pass Search",
            "Automatic Speech Recognition"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nROBUST AND FAST LYRIC SEARCH BASED ON PHONETIC\nCONFUSION MATRIX\nXin Xu, Masaki Naito, Tsuneo Kato\nKDDI R&D Laboratories, Inc.\nsh-jo, naito, tkato@kddilabs.jpHisashi Kawai\nNational Institute of Information and\nCommunications Technology\nhisashi.kawai@nict.go.jp\nABSTRACT\nThis paper proposes a robust and fast lyric search method\nfor music information retrieval. Current lyric search sys-\ntems by normal text retrieval techniques are severely dete-\nriorated in the case that the queries of lyric phrases contain\nincorrect parts due to mishearing and misremembering. To\nsolve this problem, the authors apply acoustic distance,\nwhich is computed based on a confusion matrix of an ASR\nexperiment, into DP-based phonetic string matching. The\nexperimental results show that the search accuracy is in-\ncreased by more than 40% compared with the normal textretrieval method; and by 2% ∼4% compared with the con-\nventional phonetic string matching method. Considering\nthe high computation complexity of DP matching, the au-\nthors propose a novel two-pass search strategy to shorten\nthe processing time. By pre-selecting the probable candi-\ndates by a rapid index-based search for the ﬁrst pass and\nexecuting a DP-based search among these candidates dur-\ning the second pass, the proposed method reduces process-\ning time by 85.8% and keeps search accuracy at the samelevel as that of a complete search by DP matching with all\nlyrics.\n1. INTRODUCTION\nAn easy-to-use music information retrieval (MIR) system\nplays an essential role in realizing satisfactory music distri-\nbution services. Current commercial MIR systems accept\ndiverse queries by text, humming, singing, and acoustic\nmusic signals. Among these types of queries, text queries\nof lyric phrases are commonly used (lyric search) [1]. As\nmany MIR systems apply full-text search engines to search\nlyric, the issue of lyric search has been widely acceptedas a solved issue by state-of-art text retrieval techniques.\nHowever, the authors’ preliminary investigations on real\nworld queries suggested that, users are likely to input in-\ncorrect lyric phrases (incorrect queries in this paper) into\nMIR systems resulting in a failed lyric search. The incor-\nrect lyric phrases are due to unreliable human memory or\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.mishearing, as users remember the lyric phrases when they\nare impressed by hearing a part of a song without a lyric\nsheet. The analysis found that incorrect queries which re-\nplaces a word with another word of a similar pronunciation\nreaches 19%. This phenomenon is called “acoustic confu-\nsion” here.\nIn text retrieval ﬁeld, some fuzzy algorithms, such as\nLatent Semantic Indexing (LSI) and partial matching, were\nused by major commercial Web search engines [2] to im-\nprove the robustness against incorrect queries. However,\nXu’s research veriﬁed that these algorithms were not help-\nful for acoustic confusion [3].\nTo solve this problem peculiar to lyric search, a search\nmethod is expected to be able to identify a lyric contain-\ning a part that is most similar in acoustic respect to the\nquery. Phonetic string matching, which is used in such ap-\nplications as name retrieval [4], was considered to be clos-\nest to the expected method. It uses edit distance betweenphoneme strings to search words with similar sound. How-\never, edit distance is the minimum number of operations\nneeded to transform one string into the other [5]. It does\nnot present the degree of acoustic confusability between\nphonemes. For example, /aki/ is easily misheard as /agi/\nas opposed to /aoi/, though the edit distances are identi-\ncal. This is because the phonemes, “k” and “g”, tend to be\nconfused mutually, compared with “k” and “o”.\nIn order to take the degree of acoustic confusability be-\ntween phonemes into account for string matching, the au-\nthors apply a new distance, called acoustic distance, to\nphonetic string matching. Acoustic distance is obtained by\nDP matching with the costs derived from phonetic confu-sion probabilities between the phoneme strings of a query\nand a lyric. It is motivated by the ideas in Spoken Doc-\nument Retrieval (SDR )and Spoken Utterance Retrieval\n(SUR )[6,7]. Phonetic confusion probabilities are derived\nfrom a phonetic confusion matrix that is obtained from a\npreliminary automatic speech recognition (ASR) experi-\nment.\nAs it is found by authors’ preliminary investigations\nthat queries of lyric phrases are not segmented by word or\nsentence boundary, edge-free DP matching between two\nphoneme strings of a query and a lyric is used to calcu-\nlate acoustic distance. The computation complexity of DP\nmatching O{DP}cannot be ignored here because lyrics\nalways contain long phoneme strings. Conventional pho-\nnetic string matching applied a complete search by DP\n417Poster Session 3\nmatching with all lyrics, so the computation complexity is\nregarded as O{DP}∗It, whereItis the number of lyrics\nto search. Since commercial MIR systems usually provide\nhundreds of thousands of lyrics, the computation complex-\nity is too high to realize a real time search.\nTherefore, a lyric search method with a two-pass search\nstrategy is proposed to speed up the search process. In\nthe ﬁrst pass, the proposed method pre-selects the proba-\nble lyric candidates by a rapid approximate search basedon the accumulation of pre-computed and indexed partial\nacoustic distances. Then, a complete search by DP match-\ning with the remaining lyrics is carried out during the sec-\nond pass, which decreases the value of I\ntcontributing to\nthe computation complexity.\nThe experimental results show that the application of\nphonetic confusion probability improves search accuracy\nin lyric search. Moreover, the processing time is greatly\nreduced by using two-pass search strategy.\nThe remainder of this paper is organized as follows: the\nanalysis of real world queries is described in Section 2.\nThe deﬁnition of acoustic distance and the proposed methodare introduced in Section 3. The experiments are carried\nout to evaluate the proposed method in Section 4. The pa-\nper is summarized in Section 5.\n2. ANALYSIS OF REAL WORLD LYRIC QUERIES\nTo analyze the queries of lyric phrases for MIR in the real\nworld, the authors investigated some question & answer\ncommunity web sites, where many questions were found\nthat used lyric phrases to request the names of songs and\nsingers. As 1140 queries of lyric phrases were collected,the authors compared each query with its corresponding\nlyric to distinguish whether lyric phrases in the query are\ncorrect or not (correct query or incorrect query) and how\nthey were mistaken. The lyrics and queries are written in\nJapanese or English, or a mixture of both.\nFigure 1 shows the distribution of incorrect queries in\nthe different types and correct queries within the collected\ndata. The incorrect queries, which occupy around 79%, are\nclassiﬁed into the following types:\n•Confusion of notations: Chinese characters in the\nqueries are substituted for syllabary characters (Hi-ragana or Katakana in Japanese), or vice versa.\n•Function-word-error: Only the function words, which\nhave little lexical meaning, such as prepositions, pro-\nnouns, auxiliary verbs, are mistaken in the queries.\n•Content-word-error: The content words such as a\nnoun, verb, or adjective, that have a stable lexical\nmeaning, are mistaken in the queries.\nIn the current full-text search methods, function-word-\nerror and confusion of notations can be handled using astop word list to ﬁlter out the function words [8], and a\nhybrid index of words and syllables [9].\nFigure 1 . The distribution of incorrect queries in the differ-\nent types and correct queries within the collected queries\nOn the other hand, as the content words play more im-\nportant roles in determining the search intension [8], content-\nword-error queries were further categorized into three sub-types by the authors, viz., namely “acoustic confusion”,\n“meaning confusion” and “others”. The percentages and\nexamples are listed in Table 1. The mistaken parts are\nmarked in bold.\nAcoustic confusion is deﬁned as a replacement of a word\nwith that of a similar pronunciation; or a replacement of the\nunknown-spelling words with syllable strings of a similar\npronunciation. For the ﬁrst example of acoustic confusion\nqueries in Table 1, “/kotoganai/” and “/kotobawanani/” have\nsimilar pronunciations while the text strings have no com-mon parts. In the second example, the Japanese syllable\nstring is used as a query whose pronunciation is similar to\nthe English phrase, “You’ve been out riding fences for so\nlong now” in the target lyric. It was supposed to happen\nwhen users were not able to spell the foreign words that\nthey heard in a song.\nMeaning confusion is deﬁned as a replacement of a word\nwith its synonym or near-synonym. As shown in Table 1\nthe ﬁrst example of meaning confusion queries, “/anata/”\nis mistaken for “/kimi/”. Both of the terms refer to the\nsame meaning “you” in Japanese. For the second example,\n“/tsuki/” and “/hoshi/”, which mean “moon” and “star”, areconfused.\nThe type of “others” contains word insertion, word dele-\ntion and other errors in the queries. From the analysis of\ncollected examples, it is known that mistakes in “others”\ntype are derived from arbitrary reasons, which include in-\ndividual experiences or memories, special environments,\nand other reasons. The analysis did not ﬁnd a relationship\nbetween the mistakes and the lyrics.\nAs the acoustic confusion queries occupy about 45% of\ncontent-word-error queries (19% of the collected queries),\nit remains an important issue for lyric search. Based on\nthe description above, identifying a lyric containing a part\nthat is most similar in the acoustic aspect of the query is abetter solution for acoustic confusion than focusing on the\ntextual or semantic aspects.\n41810th International Society for Music Information Retrieval Conference (ISMIR 2009)\n/;#23#23#23\n/g40/g91/g68/g80/g83/g79/g72/g86/;#23#23#23 /g55/g92/g83/g72/g86/;#23#23#23 /g82/g73/;#23#23#23\n/g84/g88/g72/g85/g76/g72/g86/;#23#23#23/g51/g72/g85/g70/g72/g81/g87/g68/g74/g72 /;#23#23#23/;#23#23#23\n/g38/g82/g85/g85/g72/g70/g87/;#23#23#23/g79/g92/g85/g76/g70/;#23#23#23 /g48/g76/g86/g87/g68/g78/g72/g81/;#23#23#23/g84/g88/g72/g85/g76/g72/g86/;#23#23#23\n/g55/g72/g91/g87/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23/g45/g68/g83/g68/g81/g72/g86/g72/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 ᅢߥ߈੐੐੐੐޿ߥ߇޿ߥ߇޿ߥ߇޿ߥ߇#;/23#23#23/;#23#23#23/;#23#23#23/;#23#23#23 ᅢߥ߈⸒⪲⸒⪲⸒⪲⸒⪲ߪߪߪߪ૗૗૗૗/;#23#23#23\n/g51/g85/g82/g81/g88/g81/g70/g76/g68/g87/g76/g82/g81/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g86/g88/g78/g76/g81/g68/g78/g82/g87/g82/g74/g68/g81/g68/g76/g78/g82/g87/g82/g74/g68/g81/g68/g76/g78/g82/g87/g82/g74/g68/g81/g68/g76/g78/g82/g87/g82/g74/g68/g81/g68/g76 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23 /g86 /g88 /g78 /g76 /g81 /g68 /g78/g82/g87/g82/g69/g68/g90/g68/g81/g68/g81/g76/g78/g82/g87/g82/g69/g68/g90/g68/g81/g68/g81/g76/g78/g82/g87/g82/g69/g68/g90/g68/g81/g68/g81/g76/g78/g82/g87/g82/g69/g68/g90/g68/g81/g68/g81/g76 /;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /g40/g91/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23\n/g48/g72/g68/g81/g76/g81/g74/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /g55/g75/g72/g85/g72/;#23#23#23/g76/g86/;#23#23#23/g81/g82/g87/g75/g76/g81/g74/g81/g82/g87/g75/g76/g81/g74/g81/g82/g87/g75/g76/g81/g74/g81/g82/g87/g75/g76/g81/g74 /;#23#23#23/g44/;#23#23#23/g79/g76/g78/g72/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23 /g58/g75/g68/g87/;#23#23#23/g68/g85/g72/;#23#23#23/g92/g82/g88/;#23#23#23/g73/g68/g89/g82/g85/g76/g87/g72 /;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23/g90/g82/g90/g82/g90/g82/g90/g82/g85/g71/g86/g85/g71/g86/g85/g71/g86/g85/g71/g86/g34/;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23\n/g55/g72/g91/g87/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /g60/g82/g88/g183/g89/g72/;#23#23#23 /g69/g72/g72/g81/;#23#23#23/g82/g88/g87/;#23#23#23/g85/g76/g71/g76/g81/g74/;#23#23#23/g73/g72/g81/g70/g72/g86/;#23#23#23/g60/g82/g88/g183/g89/g72/;#23#23#23 /g69/g72/g72/g81/;#23#23#23/g82/g88/g87/;#23#23#23/g85/g76/g71/g76/g81/g74/;#23#23#23/g73/g72/g81/g70/g72/g86/;#23#23#23/g60/g82/g88/g183/g89/g72/;#23#23#23 /g69/g72/g72/g81/;#23#23#23/g82/g88/g87/;#23#23#23/g85/g76/g71/g76/g81/g74/;#23#23#23/g73/g72/g81/g70/g72/g86/;#23#23#23/g60/g82/g88/g183/g89/g72/;#23#23#23 /g69/g72/g72/g81/;#23#23#23/g82/g88/g87/;#23#23#23/g85/g76/g71/g76/g81/g74/;#23#23#23/g73/g72/g81/g70/g72/g86/;#23#23#23\n/g73/g82/g85/;#23#23#23/g86/g82/;#23#23#23/g79/g82/g81/g74/;#23#23#23/g81/g82/g90/g73/g82/g85/;#23#23#23/g86/g82/;#23#23#23/g79/g82/g81/g74/;#23#23#23/g81/g82/g90/g73/g82/g85/;#23#23#23/g86/g82/;#23#23#23/g79/g82/g81/g74/;#23#23#23/g81/g82/g90/g73/g82/g85/;#23#23#23/g86/g82/;#23#23#23/g79/g82/g81/g74/;#23#23#23/g81/g82/g90/;#23#23#23ࡦ࡯ࠪࡦ࠘࠼࠙࡜ࡊ࠙࠽࡯ࡌ࡯࡙ࡦ࡯ࠪࡦ࠘࠼࠙࡜ࡊ࠙࠽࡯ࡌ࡯࡙ࡦ࡯ࠪࡦ࠘࠼࠙࡜ࡊ࠙࠽࡯ࡌ࡯࡙ࡦ࡯ࠪࡦ࠘࠼࠙࡜ࡊ࠙࠽࡯ࡌ࡯࡙\n࠙࠽ࠣࡦ࠮࡯࠰ࠬࠢ࠙࠽ࠣࡦ࠮࡯࠰ࠬࠢ࠙࠽ࠣࡦ࠮࡯࠰ࠬࠢ࠙࠽ࠣࡦ࠮࡯࠰ࠬࠢ#;/ 23#23#23\n/g51/g85/g82/g81/g88/g81/g70/g76/g68/g87/g76/g82/g81/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g92/g88/g88/g69/g76/g76/g69/g76/g76/g49/g49/g68/g88/g87/g82/g85/g68/g76/g71/g76/g49/g49/g74/g88/g73/g72/g49/g49/g86/g92/g88/g88/g69/g76/g76/g69/g76/g76/g49/g49/g68/g88/g87/g82/g85/g68/g76/g71/g76/g49/g49/g74/g88/g73/g72/g49/g49/g86/g92/g88/g88/g69/g76/g76/g69/g76/g76/g49/g49/g68/g88/g87/g82/g85/g68/g76/g71/g76/g49/g49/g74/g88/g73/g72/g49/g49/g86/g92/g88/g88/g69/g76/g76/g69/g76/g76/g49/g49/g68/g88/g87/g82/g85/g68/g76/g71/g76/g49/g49/g74/g88/g73/g72/g49/g49/g86\n/g75/g76/g93/g88/g73/g82/g82/g86/g82/g82/g85/g82/g49/g49/g74/g88/g81/g68/g88/g75/g76/g93/g88/g73/g82/g82/g86/g82/g82/g85/g82/g49/g49/g74/g88/g81/g68/g88/g75/g76/g93/g88/g73/g82/g82/g86/g82/g82/g85/g82/g49/g49/g74/g88/g81/g68/g88/g75/g76/g93/g88/g73/g82/g82/g86/g82/g82/g85/g82/g49/g49/g74/g88/g81/g68/g88/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23 /;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g92/g88/g88/g69/g72/g72/g81/g68/g88/g83/g88/g85/g68/g88/g71/g88/g49/g49/g86/g75/g76/g76/g49/g49/g78/g88/g86/g88/g92/g88/g88/g69/g72/g72/g81/g68/g88/g83/g88/g85/g68/g88/g71/g88/g49/g49/g86/g75/g76/g76/g49/g49/g78/g88/g86/g88/g92/g88/g88/g69/g72/g72/g81/g68/g88/g83/g88/g85/g68/g88/g71/g88/g49/g49/g86/g75/g76/g76/g49/g49/g78/g88/g86/g88/g92/g88/g88/g69/g72/g72/g81/g68/g88/g83/g88/g85/g68/g88/g71/g88/g49/g49/g86/g75/g76/g76/g49/g49/g78/g88/g86/g88\n/g86/g82/g82/g86/g72/g49/g49/g74/g88/g81/g68/g88/g86/g82/g82/g86/g72/g49/g49/g74/g88/g81/g68/g88/g86/g82/g82/g86/g72/g49/g49/g74/g88/g81/g68/g88/g86/g82/g82/g86/g72/g49/g49/g74/g88/g81/g68/g88/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/g36/g70/g82/g88/g86/g87/g76/g70/;#23#23#23\n/g70/g82/g81/g73/g88/g86/g76/g82/g81/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23\n/;#23#23#23\n/g40/g91/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23\n/g48/g72/g68/g81/g76/g81/g74/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /g60/g82/g88/g183/g89/g72/;#23#23#23 /g69/g72/g72/g81/;#23#23#23 /g82/g88/g87/;#23#23#23 /g85/g76/g71/g76/g81/g74/;#23#23#23 /g73/g72/g81/g70/g72/g86/;#23#23#23 /g73/g82/g85/;#23#23#23 /g86/g82/;#23#23#23\n/g79/g82/g81/g74/;#23#23#23/g81/g82/g90/;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/g81/g82/;#23#23#23/g68/g70/g87/g88/g68/g79/;#23#23#23/g80/g72/g68/g81/g76/g81/g74/;#23#23#23 /;#23#23#23/;#23#23#23/;#23#23#23\n/g55/g72/g91/g87/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23/g45/g68/g83/g68/g81/g72/g86/g72/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 ำำำำߪߦ૗߽ߢ⹤ߣࠃࠆߖ#;/ 23#23#23 ߪߦߚߥ޽ߚߥ޽ߚߥ޽ߚߥ޽ ૗߽ߢ⹤ߣࠃࠆߖ#;/ 23#23#23\n/g51/g85/g82/g81/g88/g81/g70/g76/g68/g87/g76/g82/g81/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g78/g78/g78/g78/g76/g80/g76/g76/g80/g76/g76/g80/g76/g76/g80/g76/g81/g76/g90/g68/g81/g68/g49/g49/g71/g72/g80/g82/g75/g68/g81/g68/g86/g72/g85/g88/g92/g82/g87/g82/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23 /g68/g81/g68/g87/g68/g68/g81/g68/g87/g68/g68/g81/g68/g87/g68/g68/g81/g68/g87/g68/g81/g76/g90/g68/g81/g68/g49/g49/g71/g72/g80/g82/g75/g68/g81/g68/g86/g72/g85/g88/g92/g82/g87/g82/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /g40/g91/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23\n/g48/g72/g68/g81/g76/g81/g74/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /g44/;#23#23#23/g70/g68/g81/;#23#23#23/g86/g68/g92/;#23#23#23/g68/g81/g92/g87/g75/g76/g81/g74/;#23#23#23/g87/g82/;#23#23#23 /g92/g82/g88/g92/g82/g88/g92/g82/g88/g92/g82/g88/;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23 /g44/;#23#23#23/g70/g68/g81/;#23#23#23/g86/g68/g92/;#23#23#23/g68/g81/g92/g87/g75/g76/g81/g74/;#23#23#23/g87/g82/;#23#23#23 /g92/g82/g88/g92/g82/g88/g92/g82/g88/g92/g82/g88/;#23#23#23\n/g55/g72/g91/g87/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23/g45/g68/g83/g68/g81/g72/g86/g72/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 ᦬᦬᦬᦬ߦ㗿ࠍ޿#;/23#23#23/;#23#23#23/;#23#23#23/;#23#23#23 ᤊᤊᤊᤊߦ㗿ࠍ޿#;/23#23#23\n/g51/g85/g82/g81/g88/g81/g70/g76/g68/g87/g76/g82/g81/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g87/g86/g88/g78/g87/g86/g88/g78/g87/g86/g88/g78/g87/g86/g88/g78/g76/g76/g76/g76/g81/g76/g81/g72/g74/g68/g76/g82/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g75/g82/g86/g75/g76/g75/g82/g86/g75/g76/g75/g82/g86/g75/g76/g75/g82/g86/g75/g76/g81/g76/g81/g72/g74/g68/g76/g82/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23/;#23#23#23/g48/g72/g68/g81/g76/g81/g74/;#23#23#23\n/g70/g82/g81/g73/g88/g86/g76/g82/g81/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23#23#23\n/;#23#23#23\n/g40/g91/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23\n/g48/g72/g68/g81/g76/g81/g74/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /g83/g85/g68/g92/;#23#23#23/g87/g82/;#23#23#23/g87/g75/g72/;#23#23#23 /g80/g82/g82/g81/g80/g82/g82/g81/g80/g82/g82/g81/g80/g82/g82/g81/;#23#23#23/g83 /g85 /g68 /g92 /;#23#23#23 /g87 /g82 /;#23#23#23 /g87 /g75 /g72 /;#23#23#23/g86/g87/g68/g85/g86/g87/g68/g85/g86/g87/g68/g85/g86/g87/g68/g85/;#23#23#23\n/g55/g72/g91/g87/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23/g45/g68/g83/g68/g81/g72/g86/g72/;#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 ᤊࠄ߆ࠄ߆ࠄ߆ࠄ߆᧪᧪᧪᧪ߚߚߚߚሶߩ⷗ࠆᄞߪ#;/ 23#23#23 ᤊߩሶߩࡦࡆ࡚࠴ࡦࡆ࡚࠴ࡦࡆ࡚࠴ࡦࡆ࡚࠴ ⷗ࠆᄞߪ#;/ 23#23#23\n/g51/g85/g82/g81/g88/g81/g70/g76/g68/g87/g76/g82/g81/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g75/g82/g86/g75/g76/g78/g68/g85/g68/g78/g76/g87/g68/g78/g68/g85/g68/g78/g76/g87/g68/g78/g68/g85/g68/g78/g76/g87/g68/g78/g68/g85/g68/g78/g76/g87/g68 /g78/g82/g81/g82/g80/g76/g85/g88/g92/g88/g80/g72/g90/g68/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g75/g82/g86/g75/g76/g81/g82/g78 /g82/g70/g75/g82/g69/g76/g49/g49/g70/g75/g82/g69/g76/g49/g49/g70/g75/g82/g69/g76/g49/g49/g70/g75/g82/g69/g76/g49/g49 /g81/g82/g80/g76/g85/g88/g92/g88/g80/g72/g90/g68/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23\n/g50/g87/g75/g72/g85/g86/;#23#23#23 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23/;#23#23#23 /g40/g91/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23\n/g48/g72/g68/g81/g76/g81/g74/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/;#23#23#23 /g55/g75/g72/;#23#23#23/g71/g85/g72/g68/g80/;#23#23#23/g87/g75/g68/g87/;#23#23#23/g87/g75/g72/;#23#23#23/g70/g75/g76/g79/g71/;#23#23#23/g90/g75/g82/;#23#23#23 /g70/g68/g80/g72/;#23#23#23/g70/g68/g80/g72/;#23#23#23/g70/g68/g80/g72/;#23#23#23/g70/g68/g80/g72/;#23#23#23\n/g73/g85/g82/g80/g73/g85/g82/g80/g73/g85/g82/g80/g73/g85/g82/g80/;#23#23#23/g87/g75/g72/;#23#23#23/g86/g87/g68/g85/;#23#23#23/g75/g68/g86/;#23#23#23/g55/g75/g72/;#23#23#23 /g71/g85/g72/g68/g80/;#23#23#23 /g87/g75/g68/g87/;#23#23#23 /g70/g75/g76/g79/g71/;#23#23#23 /g38/g38/g38/g38/g75/g82/g69/g76/g81/g75/g82/g69/g76/g81/g75/g82/g69/g76/g81/g75/g82/g69/g76/g81/;#23#23#23/g82/g73/;#23#23#23 /g87/g75/g72/;#23#23#23\n/g86/g87/g68/g85/;#23#23#23/g75/g68/g86/;#23#23#23\nTable 1 . The distribution of mistaken types within content-word-error cases\n3. AN EFFICIENT SEARCH METHOD BASED ON\nACOUSTIC DISTANCE\n3.1 Introduction of Acoustic DistanceThe authors introduce acoustic distance to quantify the de-\ngree of acoustic confusion. Acoustic distance is calculated\nby DP matching with cost values derived from phonetic\nconfusion probabilities, instead of the constant cost values\nused for edit distance.\nFirst, a phonetic confusion matrix is obtained by run-\nning a phoneme speech recognizer over training data and\nby aligning the recognition results of phoneme strings with\nreference phoneme strings.\nFor the elements of the confusion matrix, n(p,q)means\nthe number of phoneme qobtained as recognition results\nby the actual utterances of phoneme p.A s “φ” represents\na null,n(φ,p)means the number of the misrecognized\nphoneme p(insertion) and n(p,φ)means the number of\nthe deleted phoneme p(deletion). Mrepresents the set of\nphonemes including null.\nFor each phoneme p, the phonetic confusion probabili-\nties of an insertion P\nins(p), deletion Pdel(p)and substitu-\ntion for phoneme qP sub(p,q)are calculated on the basis\nof the confusion matrix elements, by Eq.1 ∼3.\nPins(p)=n(φ,p)/summationtext\nk∈Mn(k,p)(1)\nPdel(p)=n(p,φ)/summationtext\nk∈Mn(p,k)(2)\nPsub(p,q)=n(p,q)/summationtext\nk∈Mn(p,k)(3)\nAs a large value of Pins(p)represents a high confus-\nability for an insertion of p, it corresponds to a low costof an insertion operation for pin string matching based on\nDP. Therefore the value of insertion cost Cins(p), is cal-\nculated by Eq.4. In the same way, the value of deletion\ncostCdel(p)and substitution cost Csub(p,q), are calcu-\nlated from the corresponding phonetic confusion probabil-\nities by Eq.5 and Eq.6.\nCins(p)=1−Pins(p) (4)\nCdel(p)=1−Pdel(p) (5)\nCsub(p,q)=1−Psub(p,q) (6)\nSecond, with the calculated cost values, edge-free DP\nmatching between the phoneme strings S1,S2is carried\nout by Eq.7 ∼9. Here, S[x]isxth phoneme of phoneme\nstringSandlen(S)means the length of S(S1,S2∈S).\nD(i,j)designates the minimum distance from the start-\ning point to the lattice point (i,j).DS1,S2is the accumu-\nlated cost of DP matching between S1andS2, which is\ndeﬁned as the acoustic distance. It reﬂects acoustic confu-\nsion probability for each phoneme.\n1. Initialization:\nD(0,j) = 0(0≤j≤len(S2)); (7)\n2. Transition:\nD(i,j) = min⎧\n⎪⎪⎨\n⎪⎪⎩D(i,j−1) +Cins(S2[j])\nD(i−1,j−1) +Csub(S1[i],S2[j])\nD(i−1,j−1),(S1[i]=S2[j])\nD(i−1,j)+Cdel(S1[i])\n(8)\n419Poster Session 3\n3. Determination\nDS1,S2=min{D(len(S1),j)}(0<j≤len(S2));\n(9)\n3.2 Searching Method based on Acoustic Distance by\nDP matching\nBased on the criterion that a lyric containing a part that has\nthe minimum acoustic distance from the query should be\nthe user’s target, a method with a complete search by DP\nmatching with all lyrics is described as follows:\n1. The lyrics LItare converted into syllable strings us-\ning a morphological analysis tool such as Mecab [10].The syllable strings are converted into phoneme strings\nby referring to a syllable-to-phoneme translation ta-\nble. Consequently, a phoneme string S\nL(k)repre-\nsents a lyric L(k)(L(k)∈LIt).\n2. Once a query Qis provided, it is converted into a\nphoneme string SQin the same way as step 1. By\nEq.7∼9, the acoustic distance DSQ,SL(k)between\nthe query and all lyrics LItis calculated.\n3. Lyrics LItare ranked in the order of the acoustic\ndistanceDSQ,SL(k), and then the lyrics with lower\ndistance values are provided as search results.\n3.3 Searching Method based on Acoustic Distance by\nTwo-pass Searching\nConsidering that the complete search by DP matching with\nall lyrics requires high computation complexity, a method\nwith two-pass search strategy is proposed and realized with\nfollowing steps:\n•Preliminary indexing: An inverted index construc-\ntion is preliminarily incorporated for the ﬁrst pass\nsearch. A list of linguistically existing units of N\nsuccessive syllables (syllable N-gram)A1···An\nare collected from the text corpus. The units are\norganized as index units for fast access, as shown\nin Table 2. The acoustic distance DSAn,SL(k)be-\ntween the phoneme strings of AnandL(k)are pre-\ncomputed by Eq.7 ∼9 and stored in the index matrix.\nIt can be regarded as an index of acoustic confusion.\n•First pass search: By accessing the index described\nabove, a fast search is realized by using the four\nsteps below, and the ﬂowchart is illustrated in Figure 2:\n1. The input query Qis converted into a syllable\nstringvby Macab.\n2. By Eq.10 the syllable string is converted into\nsyllableN-gram sets, V1,...,Vm,...,VM.\nHere,v[m]is themth syllable of v.\nVm={v[m],v[m+1 ],···,v[m+N−1]};\n(10)3.V1,...,V m,...,V Mare matched with the in-\ndex units A1,...,A n,.... By accumulating\nthe pre-computed and indexed distance values\nDSAn,SL(k), the approximate acoustic distance\nR(k)is calculated by Eq.11.\nR(k)=/summationdisplay\nm=1,···,MDSAn,SL(k),(Vm=An)\n(11)\n4. To narrow the search space of lyrics, L(k)with\nhigherR(k)is pruned off, and a lyric set LIc\ncontaining Ic(Ic<I t) best lyric candidates is\npreserved for the second pass.\n•Second pass search: A complete search by DP match-\ning with the lyrics in LIcis carried out.\nBased on the processes above, the computing complex-\nity of the proposed method is reduced to O{FPS}+O{DP}∗\nIc.A sO{FPS}is the computing complexity of the ﬁrst\npass search which is much less than O{DP}andIcis\nmuch less than It, it provides a faster response for a real-\ntime MIR system.\n4. EVALUATION OF SEARCH ACCURACY AND\nPROCESSING TIME\n4.1 Experimental Set Up and Test SetTo evaluate the search performance of the proposed search\nmethod, experiments were carried out. A database of 10000\nlyrics was collected containing both Japanese and Englishlyrics. The test set consisted of 220 incorrect queries that\nwere mistaken in acoustic confusion. They were from the\ncollected queries mentioned in Section 2. The lyrics corre-\nsponding to the queries were included in the database. The\nresults of the experiment were obtained using a personal\ncomputer (Intel Core2Duo CPU 3.0GHz, 4G RAM). Four\nmethods described as follows, were compared.\n•“Baseline”: A normal partial matching method us-\ning full-text retrieval engine “Lucene”, which is based\non inverted index construction [11]. In this method,\nas the query of lyric phrases is divided into Nsuc-\ncessive character substrings, the lyric containing more\nsubstrings is regarded as a more suitable candidate.\n•“Method based on Edit Distance of Phoneme (EDP)”:\nThe search method based on the edit distance of phonemestrings, which is described in [4].\n•“Method based on Acoustic Distance by DP match-\ning (ADDP)”: The search method using a complete\nsearch described in Section 3.2. The phonetic con-\nfusion matrix for calculating acoustic distance is ob-\ntained using the same speech recognition experiment\nas in [12]. Although the confusion matrix should be\nbased on singing voice, a huge amount of singingdata is not available. In this research, telephone speech\ndata of Japanese phonetically balanced sentences were\n42010th International Society for Music Information Retrieval Conference (ISMIR 2009)\nsyllable 3-gramLyric No.L(1) ··· L(k) ···\nA1[a-i-u] 0.34 ··· 0.23 ···\n··· ··· ··· ··· ···\nAn[na-ko-to] 0.88 ···DSAn,SL(k)···\nTable 2 . A sample of index item by syllable 3-gram\nCharacter to Syllable \nconversionQuery in Characters : /g16888/g4450/g16912/g16941/g2640/g16911/g16941/g16903 (Japanese) /g16889\n…0.45……1…0.23L(k)\n… … … …\n0.88 An\n… … … …\n1 A1000\n… … … …………\n1…0.34L(1) … Lyric  No.\nsyllable 3-gram\nA35… …… A1 Syllable 3-gram \ncompositionQuery in Syllables : /g16888su-ki-na-ko-to-ga-na-i /g16889\nga-na-i V6… …na-ko-to Vm… …su-ki-na V1A set of syllable 3-gram\nAccumulating pre-computed \nand indexed distancesINDEX\nLyrics with approximate \nacoustic distance R(k)\nPruning\nA lyric set of I/g17187/g17187/g17187/g17187candidatescIL)(, kLSnASD\nFigure 2 . Flowchart of the ﬁrst pass search\nused as the training data for the acoustic models of\nASR.\n•“Method based on Acoustic Distance by Two-pass\nSearching (ADTS)”: The proposed method using\nthe two-pass search strategy as described in Section 3.3.\nConsidering the balance of index size and search ac-curacy, here Nof syllable N-gram index is set to\n3. The syllable 3-grams are collected from the lyric\nand newspaper corpus. 100,000 entries of syllable\n3-grams, which cover 90% of all syllable 3-grams in\nthe collected text corpus, are prepared in the index.\nAs all the syllable 3-grams which exist in the queries\nare prepared, no search errors come from out-of-\nvocabulary syllable 3-grams in the experiments.\n4.2 Improvements of Search Accuracy by Applying\nAcoustic Distance\nThe comparison of the results between “EDP” and “ADDP”\nare shown in Figure 3. The vertical axis means the hit rate,\nwhile the horizontal axis shows the top Tcandidates of the\nranked lyrics, which is called T-best. Here, the hit rate\nofT-best is deﬁned as the rate of the total number of hits\nwithin top Tcandidates to the total number of search ac-\ncesses. “ADDP” improves the hit rates by 2% ∼4%, as\ntheTofT-best is ranged from 1 to 100. It indicates that\nthe proposed acoustic distance gives better effects than edit\ndistance.4.3 Evaluation of Search Accuracy and Time\nComplexity\nThe search accuracy and time complexity of four methods\nare shown in Figure 4 and Table 3 respectively.\nNote that, the value of I\ncfor “ADTS” is determined by\na preliminary experiment based on the same test set. It\nonly uses the ﬁrst pass search of “ADTS” for lyric search\nto investigate the relationship between hit rates and T-best\nto chose the best threshold value for Ic. Because the hit\nrates almost saturated when Tis larger than 800, Icis set\nto 800 in this paper.\nWith a well-designed data structure, “Baseline” achieved\nthe fastest response among four methods. However, since\nthe normal text retrieval techniques cannot solve the acous-\ntic confusion problem in lyric search, other three methodsbased on phonetic string matching achieved higher search\naccuracy than “Baseline” by more than 40%. On the other\nhand, though “ADDP” and “EDP” provide high perfor-\nmances of search accuracy, the processing times for one\nquery are over 9 seconds, which are not practical in real\nworld search. By applying a fast search in the ﬁrst pass to\nnarrow the search space, “ADTS” shortens the processingtime into 1.85 seconds, which is 14.2% of “ADDP”, with\nonly 0.5% ∼5% deterioration of search accuracy due to\nthe loss happened in the index-based pruning of “ADTS”.\nAttributing to the application of acoustic distance, “ADTS”\nkeep almost the same hit rates as “EDP” and achieves 2%improvement when Tis larger than 20, by using only 19%\ntime of “EDP”.\n421Poster Session 3\n5. CONCLUSION\nThis paper proposed a robust and fast lyric search method\nbased on the introduced acoustic distance and a two-passsearch strategy using an index-based approximate prese-\nlection for the ﬁrst pass and a DP-based string matching\nin the second pass. In the case of incorrect queries caused\nby acoustic confusion, the proposed method achieved sig-\nniﬁcantly higher search accuracy than the normal text re-\ntrieval method by more than 40%. An improvement by\n2%∼4% is also achieved compared with the conventional\nphonetic string matching method. Furthermore, the pro-\nposed method realized a real time operation by reducing85.8% processing time with a slight loss in search accuracy\ncompared with a complete search by DP matching with all\nlyrics. It is proved to be the most practical solution for\nacoustic confusion queries on the balance of high search\naccuracy and light computation complexity.\n6. REFERENCES\n[1] Downie and Cunningham: “Toward a theory of mu-\nsic information retrieval queries: System design impli-\ncations,” Proceedings of the Third International Con-\nference on Music Information Retrieval (ISMIR 2002) ,\npp. 299–300, 2002.\n[2] Denys Poshyvanyk et al.: “Combining Probabilistic\nRanking and Latent Semantic Indexing for Feature\nIdentiﬁcation,” the 14th IEEE International Confer-\nence on Program Comprehension , pp. 137–148, 2006.\n[3] Xin Xu, Masaki Naito, Tsuneo Kato, Hisashi Kawai:\n“An Introduction of a Fuzzy Text Retrieval System ForMusic Information Retrieval,” Information Processing\nSociety of Japan SIG Notes , No.127, pp. 41–46, 2008.\n[4] J. Zobel and P. Dart: “Phonetic string matching:\nLessons from information retrieval,” Proceedings of\nthe 19th International Conference on Research and\nDevelopment in Information Retrieval , pp. 166–172,\n1996.\n[5] E. Ristad and P. Yianilos: “Learning string edit dis-\ntance,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 20, pp. 522–532, 1998.\n[6] Ville T. Turunen, Mikko Kurimo: “Indexing confusion\nnetworks for morph-based spoken document retrieval,”\nProceedings of the 30th annual international ACM SI-\nGIR conference on Research and development in infor-\nmation retrieval , pp. 631–638, 2007.\n[7] Takaaki Hori et.al: “Open-Vocabulary Spoken Utter-\nance Retrieval Using Confusion Networks,” Proceed-\nings of the 2007 International Conference on Acous-\ntics, Speech, and Signal Processing (ICASSP) , pp. 73–\n76, 2007.\n[8] Christopher Fox: “A stop list for general text,” ACM\nSIGIR Forum , Vol. 24, No. 1–2, pp. 19-21, Fall\n1989/Winter 1990.4550556065707580\n123456789102030405060708090100\nT-bestHit Rate (%)\nADDP\nEDP\nFigure 3 . The improvement of search accuracy by acoustic\ndistance\n20304050607080\n123456789102030405060708090100\nT-bestHit Rate (%)\nBaseline\nADTS\nEDP\nADDP\nFigure 4 . Search accuracy of four search methods\nMethods Baseline ADTS EDP ADDP\nTime (second) 0.006 1.85 9.72 13.01\nTable 3 . Average processing times of one query\n[9] Nina Kummer, Christa Womser-Hacker and Noriko\nKando: “MIMOR@NTCIR 5: A Fusion-based Ap-\nproach to Japanese Information Retrieval,” Proceed-\nings of NTCIR-5 Workshop Meeting, Tokyo, Japan ,\n2005.\n[10] http://mecab.sourceforge.net\n[11] Erik Hatcher, Otis Gospodnetic: Lucene In Action ,\nManning Publications Co., 2004.\n[12] Makoto Yamada, Tsuneo Kato, Masaki Naito and\nHisashi Kawai: “Improvement of Rejection Perfor-\nmance of Keyword Spotting Using Anti-Keywords De-\nrived from Large Vocabulary Considering Acousti-\ncal Similarity to Keywords,” Proceedings of INTER-\nSPEECH , pp. 1445–1448, 2005.\n422"
    },
    {
        "title": "Improving Musical Concept Detection by Ordinal Regression and Context Fusion.",
        "author": [
            "Yi-Hsuan Yang",
            "Yu-Ching Lin",
            "Ann Lee 0002",
            "Homer H. Chen"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1416650",
        "url": "https://doi.org/10.5281/zenodo.1416650",
        "ee": "https://zenodo.org/records/1416650/files/YangLLC09.pdf",
        "abstract": "To facilitate information retrieval of large-scale music databases, the detection of musical concepts, or auto-tagging, has been an active research topic. This paper concerns the use of concept correlations to improve musical concept detection. We propose to formulate concept detection as an ordinal regression problem to explicitly take advantage of the ordinal relationship between concepts and avoid the data imbalance problem of conventional multi-label classification methods. To further improve the detection accuracy, we propose to leverage the co-occurrence patterns of concepts for context fusion and employ concept selection to remove irrelevant or noisy concepts. Evaluation on the cal500 dataset shows that we are able to improve the detection accuracy of 174 concepts from 0.2513 to 0.2924.",
        "zenodo_id": 1416650,
        "dblp_key": "conf/ismir/YangLLC09",
        "keywords": [
            "music databases",
            "auto-tagging",
            "concept correlations",
            "ordinal regression",
            "data imbalance",
            "conventional multi-label",
            "context fusion",
            "concept selection",
            "cal500 dataset",
            "improved detection accuracy"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nIMPROVING MUSICAL CONCEPT DETECTION BY ORDINAL\nREGRESSION AND CONTEXT FUSION\nYi-Hsuan Yang, Yu-Ching Lin, Ann Lee, Homer Chen\nNational Taiwan University\nafﬁge@gmail.com, vagante@gmail.com, an918tw@yahoo.com.tw, homer@cc.ee.ntu.edu.tw\nABSTRACT\nTo facilitate information retrieval of large-scale music data-\nbases, the detection of musical concepts, or auto-tagging,\nhas been an active research topic. This paper concerns the\nuse of concept correlations to improve musical concept de-\ntection. We propose to formulate concept detection as an\nordinal regression problem to explicitly take advantage of\nthe ordinal relationship between concepts and avoid the\ndata imbalance problem of conventional multi-label clas-\nsiﬁcation methods. To further improve the detection ac-\ncuracy, we propose to leverage the co-occurrence patterns\nof concepts for context fusion and employ concept selec-\ntion to remove irrelevant or noisy concepts. Evaluation on\nthe cal500 dataset shows that we are able to improve the\ndetection accuracy of 174 concepts from 0.2513 to 0.2924.\n1. INTRODUCTION\nMusic plays an important role in human’s history, even\nmore so in the digital age. Never before has such a large\ncollection of music been created and accessed daily by\npeople. Bridging the semantic gap–the chasm between raw\ndata (signals) and high-level semantics (meanings)–is es-\nsential for exploiting the growing music content. Toward\nthis goal, recent research has focused on building detectors\nfor detecting musical concepts such as genre, emotion, and\ninstrumentation using a pre-deﬁned lexicon and a sufﬁcient\nnumber of annotated examples [1–8]. Once trained, these\ndetectors can be used to semantically tag and index music\ncontent in a fully automatic fashion. A user can then query\nmusic by semantic description [2], such as “ﬁnd me a song\nthat is brit poppy and alternative, features male vocal, and\nhas a nice distorted electric guitar solo.”\nEarly attempts to musical concept detection formulated\nthe problem as a multi-label binary classiﬁcation problem\nand trained detector independently for each concept [1–3].\nThe training data is annotated by human subjects and the\nrelationship between ground truth and audio features is\nlearnt by machine. Subsequent efforts went one step for-\nward and utilized the correlation between concepts (either\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for Music Information Retrieval.\nFigure 1 . A schematic diagram of the proposed musical\nconcept detection system.\npositive or negative) to improve concept detection. Duan\net al. proposed a collective annotation scheme that trains\nadditional models for the pairs of concepts that have strong\ncorrelations [4]. Bertin-Mahieux et al. studied a second-\nstage learning and a correlation reweighting scheme to boost\nthe result of concept detection [5]. Aucouturier et al. [6]\nused decision tree to reﬁne the result of individual detec-\ntors. Chen et al. built anti-models to exploit the negative\ncorrelations of concepts [7]. Modeling concept correlation\nhas been shown effective for improving musical concept\ndetection.\nIt is, however, noted that most existing works focus on\nthe reﬁnement of the individual detectors by training ad-\nditional models rather than focus on the direct incorpora-\ntion of concept correlation in training the individual de-\ntectors. Evidently, there are different levels of correlation\nbetween concepts, by which we can divide the training\ndata into more than two categories; some of the training\npieces should be more relevant to a target concept than\nother pieces. Consider the following toy example. We are\ntraining a concept detector of “happy” based on three train-\ning pieces a,bandc, which are annotated with “happy,”\n“tender” and “sad” respectively. Conventional approaches\nformulate the problem as a ﬂatbinary classiﬁcation, using\naas positive example and b,cas negative examples. How-\never, since “happy” is semantically closer to “tender,” there\nshould be an ordinal scale among them, a/followsb/followsc, where\n/followsdenotes a relevance relationship. Such ordinal informa-\ntion is neglected by treating bandcthe same.\nIn this paper, we propose to formulate concept detec-\n147Poster Session 1\ntion as an ordinal regression problem [9–11] and train a\nconcept model to estimate the relevance score of a song\nwith respect to a target concept. A higher relevance score\nrepresents a higher probability of the song being anno-\ntated with the concept. The advantage of this approach\nis two-fold. First, we can make better use of the training\ndata (whose collection process is fairly time-consuming\nand labor-intensive) by explicitly leveraging the ordinal re-\nlationship between concepts. Second, conventional classi-\nﬁcation algorithms are hampered by the so-called data im-\nbalance problem: the performance of a classiﬁer degrades\nsigniﬁcantly when the number of training data is not uni-\nformly distributed across classes. For example, when 95%\nof the training data is negative, a classiﬁer can achieve a\n95% accuracy by simply classifying everything as nega-\ntive, which is highly undesirable. This problem is usually\nobserved for infrequent concepts such as “genre-swing”\nand “instrument-organ.” Ordinal regression is free of this\nproblem because the objective function of learning is not\nminimizing classiﬁcation errors and because the training\npieces that are annotated with semantically close concepts\ncan still be leveraged in learning.\nThe second contribution of the paper is the investiga-\ntion of context fusion and concept selection to improve\nthe detection result. The basic idea of context fusion is to\nleverage the co-occurrence patterns between target seman-\ntic and peripherally related concepts to improve the result\nof an initial model. It has been successfully applied to im-\nprove visual concept detection and image search [12–14].\nBecause of the assumption that the result is presented in an\nordered form, context fusion can be combined with ordi-\nnal regression in an elegant way. We also study a concept\nselection method to remove irrelevant concepts to improve\ncontext fusion. The number of selected concepts is target\nconcept-dependent. For a concept that lacks strongly cor-\nrelated concepts, context fusion is not applied.\nA schematic diagram of the overall system is shown in\nFig. 1. In the train phase, the annotations, features, and\nconcept correlations are utilized to train the individual con-\ncept detectors by ordinal regression. We then exploit the\ncontextual patterns among concepts to train a context de-\ntector for each concept. The concepts utilized in context\nfusion are selected based on concept correlations. In the\ntest phase, we extract the features of the test data and then\napply concept detection and context fusion in cascade to\ngenerate the detection result.\nThe paper is organized as follows. In Section 2 we de-\nscribe the corpus adopted in this work and the concept cor-\nrelations therein. The correlations are then used in Section\n3 for ordinal regression and in Section 4 for context fusion.\nWe report the experimental results in Section 5. Section 6\nconcludes the paper.\n2. CORPUS AND CONCEPT CORRELATION\nWe use the Computer Audition Lab 500-Song (cal500) data\nset [2] in this study for it is publicly available.1The col-\n1Available at: http://cosmal.ucsd.edu/cal/projects/AnnRet\nFigure 2 . Concept frequency distribution of a subset (413\nsongs) of cal500 [2]. Note the concepts have been sorted\nby the number of positive examples.\nlection is made of 502 recent Western songs by 502 differ-\nent artists chosen to cover a large amount of acoustic varia-\ntion. 66 paid students were recruited to annotate the songs\nwith a ﬁxed vocabulary of 135 musical concepts, with each\nsong annotated by at least three respondents. A song is an-\nnotated with a concept if there is at least 80% agreement\nbetween the respondents. The concept lexicon spans six\nsemantic categories: 29 instruments, 22 vocal character-\nistics, 36 genres, 18 emotions, 15 acoustic qualities, and\n15 usage terms. The concepts of emotions and acoustic\nqualities are further broken down into bipolar ones (e.g.,\n“emotion-happy” and “emotion-NOT happy”), resulting in\na total of 174 concepts [2].\nWe collect the audio ﬁles of 413 songs of cal500 and\nanalyze the frequency of each concept. As Fig. 2 shows,\nrare concepts form a long tail in the concept frequency dis-\ntribution. While frequent concepts (e.g., “song-recorded”\nand “instrument-male lead vocals”) have more than 300\npositive examples, 37 concepts have less than 10 positive\nexamples. A preliminary evaluation also shows the detec-\ntion accuracy of the infrequent concepts are particularly\nlow. Because of this data imbalance problem, we also use\na subset of concepts that have more than 50 positive exam-\nples in this study. The resulting lexicon, which consists of\n69 concepts, is denoted as cal500-lite hereafter.\nGiven a concept lexicon C={c1, c2, . . . , c |C|}andN\nannotated examples D={d1, d2, . . . , d N}, we can mea-\nsure the pairwise correlation ρmnbetween two concepts\ncmandcnfrom the annotations A, which are represented\nby a|C|× Nbinary matrix, with Ami= 1indicating that\ndiis annotated with cm. We compute ρmnby the Pearson’s\ncorrelation coefﬁcient of AmandAn,\nρmn=E((Am−µAm)(An−µAn))\nσAmσAn. (1)\nρmn∈[−1,1]andρmn>0iff there is a positive correla-\ntion. Interestingly, we ﬁnd the correlation values generally\nfollow a Laplacian-like distribution: the number of con-\ncept pairs decreases exponentially along with the absolute\nvalues of correlation. See Fig. 3.\n14810th International Society for Music Information Retrieval Conference (ISMIR 2009)\nFigure 3 . Distribution of the correlation values of cal500.\n3. ORDINAL REGRESSION\n3.1 Brief Review\nUnlike classiﬁcation, ordinal regression deﬁnes a number\nof classes that exhibits an ordinal scale among them. For\nexample, the preference of a song can be categorized to\n“very dislike,” “dislike,” “neutral,” “like,” and “very like.”\nThe outcome space can be denoted as Y={r1, . . . , r K},\nwith ordinal classes rK/followsYrK−1/followsY. . ./followsYr1, where\nKis the number of classes. A closely related problem\nis ranking, which presents ordered results to a user in re-\nsponse to a query. A common example is the ranking of\nsearch results from the search engine (e.g., Google). Both\nordinal regression and ranking assign each object a rele-\nvance score , by which the object is ranked. The difference\nis ordinal regression needs a further step that determines\nthe class membership of each object with respect to the\ndiscrete ordinal classes.\nIn the seminal work of Herbrich et al., the ordinal classes\nwere modeled by intervals on the real line [9]. A discrim-\ninative function f:X /mapsto→ Rwas trained to predict the\nrelevance score ˆyi=f(xi) = ( w·xi), where xiis a\nfeature vector of an object and wis a vector of weights.\nHowever, because the outcome space Yis discrete, Her-\nbrich et al. determined the rank boundary θ(rk)between\nclasses rkandrk+1on the real line according to the fol-\nlowing heuristics,\nθ(rk) =1\n2(f(x1) +f(x2)), (2)\n(x1,x2) = arg min\n(xi,xj)∈Θ(k)[f(xi)−f(xj)], (3)\nwhere Θ(k)is the set of object pairs (xi,xj)withyi=rk,\nyj=rk+1, and ( ˆyi−ˆyj)(yi−yj)≥0. In other words,\nthe optimal threshold θ(rk)for rank rklies in the middle of\nthe estimates of the closest objects of rank rkandrk+1that\ncan be correctly ranked by f(·). After the estimation of\nthe boundaries θ(rk)a new object is assigned to an ordinal\nclass according to the following equation,\ng(xi) =rk⇔f(xi)∈[θ(rk−1), θ(rk)]. (4)\nTo learn f(·), Herbrich et al. viewed the problem as\nthe classiﬁcation of object pairs into two categories (cor-\nrectly ranked and incorrectly ranked) and trained a supportvector machine (SVM) to minimize the classiﬁcation error/summationtextN\ni,j( ˆyi−ˆyj)(yi−yj). Though this algorithm, generally\ncalled rankSVM, offers advantages, it is time-consuming\nas the operation on every possible pair is O(N2).\nAlternatively, we employ the listNet [11] algorithm in\nthis work. It uses score lists directly as learning instances\nand minimizes the listwise loss between the ground truth\nranking list and the estimated one. In this way, the opti-\nmization is performed directly on the list and the compu-\ntation complexity is reduced to O(N). More speciﬁcally,\nto deﬁne a listwise loss function, the top-one probability\nis employed to transform a list of relevance scores into a\nprobability distribution. The top one probability P(yi)of\ntheith object, deﬁned as follows, represents the probability\nof the object being ranked on the top,\nP(yi) =Φ(yi)/summationtextN\ni=1Φ(yi)=exp(yi)/summationtextN\ni=1exp(yi), (5)\nwhere Φ(·)is an increasing and strictly positive function\nsuch as the exponential function. Modeling the list of scores\nas a probabilistic distribution, a metric such as the cross\nentropy can be used to measure the distance (listwise loss)\nbetween the ground truth list and the estimated one,\nL(y,ˆ y) =−N/summationdisplay\ni=1P(yi) log( P(f(xi))), (6)\nwhere y={yi}N\ni=1andˆ y={f(xi)}N\ni=1. The algorithm\nthen learns the weighting vector wby updating it at a learn-\ning rate ηby gradient descent,\nw←w−η×∆w, (7)\n∆w=∂L(y,ˆ y)\n∂w=N/summationdisplay\ni=1(P(f(xi))−P(yi))xi.(8)\nIt has been shown that listNet is more efﬁcient and effec-\ntive than rankSVM for a variety of ordinal regression and\nranking problems, such as image/video search [13].\n3.2 Concept Model Training by Ordinal Regression\nGiven the ground truth Amof concept cmand the feature\nrepresentation ofD, typically a binary classiﬁer bm(·)is\ntrained by treating D+\nm={di|Ami= 1}as positive ex-\namples andD−\nm={di|Ami= 0}as negative examples.\nHowever, such a dichotomy of the training data loses many\nvaluable information embedded in Am, as we have illus-\ntrated in Section 1. We can in fact divide the training data to\nmultiple ( K≥2) ordinal classes according to concept cor-\nrelations and then employ listNet to train a concept model\nfm(·)for each concept cm,\nˆymi=fm(xi) = (w·xi). (9)\nTwo such implementations are employed in this work,\nK= 2 andK= 4. The ﬁrst one simply dichotomizes\nDas the binary classiﬁcation setting. That is, Dr2m=D+\nm\nandDr1m=D−\nm. We then set ymi=rkifdi∈ Drkm.\n149Poster Session 1\nIn this way, the concept correlations are not explicitly uti-\nlized, but thanks to the ordinal regression algorithm (which\nminimizes a listwise loss instead of clariﬁcation error) the\ndata imbalance problem is avoided. The second implemen-\ntation uses K= 4and partitionsDto four classes accord-\ning to the following rules, which are listed in descending\norder of precedence,\n• Dr4m=D+\nm\n• Dr1m={di|Ani= 1, ρmn≤l, di∈D−\nm}\n• Dr3m={di|Ani= 1, ρmn≥u, di∈D−\nm\\Dr1m}\n• Dr2m=D−\nm\\/uniontext{Dr1m,Dr3m}\nIn other words,Dr1mconsists of songs that are annotated\nwith any of the concepts Cr1mthat are strongly negatively\ncorrelated with cm, andDr3mconsists of songs that are an-\nnotated with any of the concepts Cr3mthat are strongly posi-\ntively correlated with cm. In this work, we set l=µρ−σρ,\nu=µρ+σρ, where µρ/similarequal0.01andσρ/similarequal0.11are the mean\nand the standard deviation of all the correlation values of\nthe concept corpus (see Fig. 3).\nTable 2 shows some highly correlated concepts for four\ndifferent target concepts. It can be found that most of the\ncorrelated concepts are intuitively correct.\n4. CONTEXT FUSION\nThe nature of concept detection makes it possible to dis-\ncover co-occurrence patterns through mining ground truth\nannotations and utilize the patterns to improve concept de-\ntection. For example, if a song has the concepts “song-high\nenergy” and “song-heavy beat,” it is very likely that it also\nhas the concept “song-fast tempo.” If the relevance score\nof “song-fast tempo” is somehow detected low (maybe the\ndetector is less reliable), we can modify the result by in-\ncreasing the value. We refer to such a model that learns the\nco-occurrence patterns as the context model.\nFollowing the idea of discriminative model fusion (DMF)\n[12, 13], we train a context model for each concept based\non the output of the concept models. For each song, the\n|C|concept models are employed to predict the relevance\nscore of song diwith respect to each concept; this re-\nsults in a|C|-dimensional model vector vi={ˆyni}|C|\nn=1=\n{f1(xi),···, f|C|(xi)}. We use the model vectors to train\nthe context model ˜fm(·)for each concept cmby minimiz-\ning the loss between {ymi}N\ni=1and{˜fm(vi)}N\ni=1using list-\nNet. We then replace ˆymiwith ˜fm(vi). That is,\nˆymi←˜fm(vi) = (˜ w·vi) =|C|/summationdisplay\nn=1˜wnfn(xi). (10)\nTherefore, ˜fm(vi)can be regarded as the weighted com-\nbination of the relevance scores of diwith respect to other\nconcepts. Intuitively, the absolute value of ˜wnwould be\nlarge if cnis highly correlated with cm. A total of|C|con-\ntext models are trained.TRAINING PHASE\nINPUT: training data D, A,{xi}N\ni=1, parameters K, θ\ncompute correlations {ρmn}|C|\nm,nby Eq. 1.\nform= 1to|C|\npartitionDtoKclasses by Amand{ρmn}|C|\nn=1\nsetymi=rkifdi∈Drkm\ntrainfm(·)by minimizing L({ymi},{fm(xi)})\nend\nform= 1to|C|\nconstruct vmi={fn(xi)}n:abs(ρmn)≥θ\ntrain ˜fm(·)by minimizing L({ymi},{˜fm(vi)})\nend\nOUTPUT: concept and context models {fm,˜fm}|C|\nm=1\nTEST PHASE\nINPUT: test data{xz}\nform= 1to|C|\npredict ˆy/prime\nmz=fm(xz)\nend\nform= 1to|C|\nconstruct vmz={ˆy/prime\nmz}n:abs(ρmn)≥θ\npredict ˆymz=˜fm(vmz)\nend\nOUTPUT: concept scores {ˆymz}(one can get binary\nresult with the boundary θ(rK−1); see Eqs. 2–4)\nTable 1 . Pseudo codes of the concept detection framework.\nWe also study concept selection by removing concepts\nwhose absolute correlation values to the target concept are\nbelow a threshold θ. That is,\nvmi={fn(xi)}n:abs(ρmn)≥θ, (11)\nwhere abs(·) is an operator that takes the absolute value. In-\ntuitively, the number of selected concepts |vmi|decreases\nasθis set larger and the actual number of |vmi|depends\noncmbut not on di. When θ= 0, no concept selection is\nperformed and all the concepts are utilized; when θ= 1,\nno context fusion is conducted. For a concept that does not\nhave strongly correlated concepts, |vmi|would equal zero\nand we do not apply context fusion to it.\nThe algorithmic descriptions of the proposed concept\ndetection framework is shown in Table 1.\n5. EXPERIMENTAL RESULT\n5.1 Experiment Setup\nFor fair comparison, each songs is converted to a standard\nformat (22,050 Hz sampling frequency, 16 bits precision\nand mono channel) and represented by a 30-second seg-\nment starting from the initial 30th second of the song, a\ncommon practice in music classiﬁcation.\nFor feature representation of a song we use the com-\nputer program MA toolbox [15] to extract Mel-frequency\ncepstral coefﬁcients (MFCC), one of the most popular fea-\nture representation for audio signal processing. It is com-\nputed by taking the cosine transform of the short-term log\n15010th International Society for Music Information Retrieval Conference (ISMIR 2009)\ntarget concept cm=Cr4m strongly positively correlated concepts Cr3m strongly negatively correlated concepts Cr1m\nemotion: angry/agressive emotion: exciting/thrilling, powerful/strong emotion: calming, laid-back, happy,\ngenre: metal/hard rock, hip hop/rap, punk loving, positive, tender\ninstrument: drum machine, electric guitar (distorted) instrument: piano\nsong: fast tempo, heavy beat, high energy; song: positive feelings, texture acoustic\nemotion: sad emotion: calming/soothing, emotional/passionate emotion: arousing, carefree, happy\ninstrument: female lead vocals instrument: drum set, male lead vocals\nsong: quality, texture acoustic song: high energy, positive feelings\nusage: going to sleep, intensive listening usage: cleaning the house\ngenre: jazz emotion: calming, laid back, pleasant, tender, touching emotion: not calming, not loving\ngenre: bebop, contemporary R&B, cool jazz, swing genre: rock\ninstrument: piano, saxophone, trombone, trumpet instrument: male lead vocals\nsong: very danceable emotion: arousing, carefree, exciting, happy, light emotion: calming, laid-back, sad, tender\ngenre: dance pop, funk, swing, hip-hop/rap, pop, R&B genre: alternative, soft rock, rock\nusage: at a party, exercising\nTable 2 . Using the rules described in Section 3.2, we can obtain the highly correlated concepts for a target concept and\npartition the training data to four classes. This table shows some (only partial) highly correlated concepts for four concepts.\npower spectrum expressed on a nonlinear perceptual-related\nmel-frequency scale. We use the default 23ms frame size\nwith half overlapping to compute a bag of 20-dimensional\nMFCC vectors and then collapse the sequence of feature\nvectors into a single feature vector by taking the mean and\nstandard deviation. As prior works [2], we also take the\nﬁrst-order derivatives of the MFCC vectors to capture tem-\nporal information, resulting in a 80-dimensional feature\nvector xifor each song.\nWe randomly hold out 100 songs as the test set and\nuse the remaining 313 songs for training. The evaluation\nprocess is repeated 100 times to compute the average ac-\ncuracy, which is measured by average precision (AP), the\napproximation of the area under the recall/precision curve\n[10]. Let ˆpm={rank(ˆymi)}N\ni=1, where rank (ˆymi)is the\nranking order of diinDaccording to ˆymi, we have\nAP(ˆpm, Am) =1\nrel/summationdisplay\nj:Amj=1Prec @j, (12)\nwhere rel=|i:Ami= 1|is the number of relevant ob-\njects (true positives) of concept cm, and Prec @jis the per-\ncentage of relevant objects in the top jobjects in predicted\nranking ˆpm. AP equals 1 when all the relevant objects are\nranked at top. Since AP only shows the performance of a\nconcept, we evaluate the performance in terms of mean av-\nerage precision (MAP), the mean of APs for all concepts.\n5.2 Evaluate Ordinal Regression\nWe ﬁrst compare the performance of ordinal regression and\nmulti-label classiﬁcation. We use listNet for ordinal re-\ngression and SVM for multi-label classiﬁcation.2Table\n3 shows the MAP of different learning algorithms. It can\nbe found that listNet( K=2) signiﬁcantly outperforms SVM\n(p-value <0.01) for both the cal500 and cal500-lite lexi-\ncons, showing the effectiveness of ordinal regression. Set-\n2We use SVM for its superior performance in classiﬁcation problems.\nWe implement it based on the LIBSVM library [16]. The parameters are\ntuned by a cross validation procedure to achieve better result: for SVM,\nwe set the cost parameter Cto 1000 and the gamma γin the RBF kernel\nto 0.01; for listNet, we set the learning step ηto 0.05.SVM listNet( K=2) listNet( K=4)\ncal500 0.2513 0.2769 0.2787\ncal500-lite 0.4323 0.4687 0.4727\nTable 3 . Evaluation of ordinal regression.\nSVM listNet( K=4)\nthe 40 most freq. cpts 0.5113 0.5523 (+8.02%)\nthe medium freq. cpts 0.2182 0.2460 (+12.74%)\nthe 40 least freq. cpts 0.0690 0.0818 (+18.47%)\naverage 0.2513 0.2787 (+10.89%)\nTable 4 . The accuracy of concept detection for the cal500\nconcepts of different concept frequencies.\ntingK= 4and leveraging concept correlation to the train-\ning process further improves the accuracy. The relative\ngain of listNet( K=4) over SVM is +10.89% and +9.35%\nfor cal500 and cal500-lite, respectively.\nTo investigate the detection accuracy of ordinal regres-\nsion for concepts of different frequencies, we break down\nthe cal500 concept lexicon to three groups: the 40 most\nfrequent ones, the 40 least frequent ones, and the others.\nTable 4 shows the MAP of the concept groups of SVM\nand listNet( K=4). The correlations between concept fre-\nquency, accuracy of concept detection, and the relative per-\nformance gain of listNet( K=4) over SVM are salient. The\ndetection accuracy is generally higher for frequent con-\ncepts, while the relative performance gain of listNet( K=4)\nis generally higher for rare concepts. This implies that the\ndata imbalance problem is mitigated by listNet.\n5.3 Evaluate Context Fusion and Concept Selection\nWe then evaluate the performance of context fusion (us-\ning DMF) with and without concept selection. We use\nlistNet to train both the concept models and context mod-\nels and vary the value of the concept selection threshold\nθ. Results shown in Table 5 lead to the following obser-\n151Poster Session 1\ncal500 cal500-lite\nlistNet( K=4) 0.2787 0.4727\nlistNet( K=4)+DMF( θ=0) 0.2829 0.4873\nlistNet( K=4)+DMF( θ=0.1) 0.2911 0.4882\nlistNet( K=4)+DMF( θ=0.2) 0.2924 0.4854\nlistNet( K=4)+DMF( θ=0.3) 0.2856 0.4824\nlistNet( K=4)+DMF( θ=0.5) 0.2784 0.4754\nTable 5 . Evaluation of context fusion with different values\nof threshold θ(smaller θselects more concepts).\nvations. First, with mild concept selection, context fu-\nsion greatly improves concept detection. The MAP reaches\n0.2924 (+4.92%) for cal500 and 0.4882 (+3.28%) for cal500-\nlite. This degree of performance gain is similar to that of\napplying context fusion to visual concept detection [13].\nSecond, without concept selection ( θ=0), the performance\nof context fusion for cal500-lite is similar to the optimal\none 0.4882, which may result from the fact that the de-\ntection accuracy of cal500-lite is generally high and thus\ndirectly leveraging all concepts is effective. On the con-\ntrary, due to the rather inconsistent accuracy, the detection\nof cal500 calls for concept selection to remove irrelevant\nconcepts. Finally, setting θtoo large removes most of the\nconcepts and degrades accuracy. A mild value of θexhibits\nthe best result.\nTable 6 shows the MAP of different semantic categories\nwith and without context fusion. It can be found that con-\ntext fusion with concept selection consistently improves all\nthe semantic categories, especially for “emotion,” “genre,”\nand “usage.” In particular, because the detection accuracy\nof “genre” and “usage” are relatively low, concept selec-\ntion is prerequisite for context fusion to be effective. In\naddition, due to the lack of strongly correlated concepts,\ncontext fusion does not improve the category “vocal.” An-\nother interesting observation is the selected concepts often\nbelong to “emotion,” “song,” or the same semantic cate-\ngory as the target concept. This evaluation demonstrates\nthe importance of context fusion and concept selection.\n6. CONCLUSION\nIn this paper, we have presented a novel framework of\nutilizing concept correlations to improve musical concept\ndetection. A concept model is trained by an ordinal re-\ngression algorithm, which effectively utilizes the ordinal\nrelationships among concepts and avoids the data imbal-\nance problem of the commonly-used classiﬁcation meth-\nods. A context model is then trained to improve the detec-\ntion result by leveraging the co-occurrence patterns among\nconcepts. We also employ a concept selection method to\nkeep irrelevant concepts from being used in context fusion.\nExperimental results show that ordinal regression outper-\nforms the conventional multi-label classiﬁcation method\nby a great margin; a +10.89% relative gain in mean av-\nerage precision is achieved. With mild concept selection,\ncontext fusion further improves the detection accuracy to\n0.2924 for the 174 musical concepts of cal500.listNet +DMF( θ=0) +DMF( θ=0.2)\nemotion 0.4272 0.4369 (+2%) 0.4522 ( +6% )\ngenre 0.1731 0.1769 (+2%) 0.1890 ( +9% )\ninstrument 0.2321 0.2345 (+1%) 0.2383 (+3%)\nsong 0.4233 0.4302 (+2%) 0.4345 (+3%)\nusage 0.1753 0.1750 ( 0%) 0.1952 ( +11% )\nvocal 0.1981 0.1998 (+1%) 0.1997 (+1%)\naverage 0.2787 0.2829 (+2%) 0.2924 (+5%)\nTable 6 . The accuracy of concept detection for the cal500\nconcepts of different semantic categories.\n7. ACKNOWLEDGEMENT\nThis work was supported by the National Science Council\nof Taiwan under contract NSC 97-2221-E-002-111-MY3.\n8. REFERENCES\n[1] B. Whitman and R. Rifkin: “Musical query-by-description as\na multicalss leanring problem,” MMSP , pp. 153–156, 2002.\n[2] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet:\n“Semantic annotation and retrieval of music and sound ef-\nfects,” IEEE Trans. Audio, Speech and Language Processing ,\nVol. 16, No. 2, pp. 467–476, 2008.\n[3] M. I. Mandel and D. P. W. Ellis: “Multiple-instance learning\nfor music information retrieval,” ISMIR , 2008.\n[4] Z.-Y. Duan, L. Lu, and C.-S. Zhang: “Collective annota-\ntion of music from multiple semantic categories,” ISMIR ,\npp. 237–242, 2008.\n[5] T. Bertin-Mahieux et al: “Autotagger: A model for predicting\nsocial tags from acoustic features on large music databases,”\nJ. New Music Research , Vol. 37, No. 2, pp. 115–135, 2008.\n[6] J.-J. Aucouturier, F. Pachet, P. Roy, and A. Beuriv ´e: “Sig-\nnal+context=better classiﬁcation,” ISMIR , 2007.\n[7] Z.-S. Chen, J.-M. Zen, and J.-S. Jang: “Music annotation and\nretrieval system using anti-models,” AES Convention , 2008.\n[8] E. Law et al, “Tagatune: a game for music and sound anno-\ntation,” ISMIR , pp. 361–364, 2007.\n[9] R. Herbrich et al: “Support vector learning for ordinal regres-\nsion,” ICANN , pp. 97–102, 1999.\n[10] Y. Yue et al: “A support vector method for optimizing average\nprecision,” SIGIR , pp. 271–278, 2007.\n[11] F. Xia et al: “Listwise approach to learning to rank: Theory\nand algorithm,” ICML , pp. 1192–1199, 2008.\n[12] J. Smith, M. Naphade, and A. Natsev: “Multimedia semantic\nindexing using model vectors,” ICME , pp. 445–448, 2003.\n[13] Y.-H. Yang et al: “Online reranking via ordinal informative\nconcepts for context fusion in concept detection and video\nsearch,” IEEE Trans. Circuits and Sys. for Video Tech. , 2009.\n[14] M. Naphade et al: “Large-scale concept ontology for multi-\nmedia,” IEEE Multimedia Magazine , Vol. 13, No. 3, pp. 86–\n91, 2006.\n[15] E. Pampalk: “A Matlab toolbox to compute\nmusic similarity from audio,” ISMIR , 2004.\nhttp://www.ofai.at/ elias.pampalk/ma/.\n[16] C.-C. Chang and C.-J. Lin: “LIBSVM: a library for support\nvector machines,” 2001.\n152"
    },
    {
        "title": "Continuous pLSI and Smoothing Techniques for Hybrid Music Recommendation.",
        "author": [
            "Kazuyoshi Yoshii",
            "Masataka Goto"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1415204",
        "url": "https://doi.org/10.5281/zenodo.1415204",
        "ee": "https://zenodo.org/records/1415204/files/YoshiiG09.pdf",
        "abstract": "This paper presents an extended probabilistic latent semantic indexing (pLSI) for hybrid music recommendation that deals with rating data provided by users and with contentbased data extracted from audio signals. The original pLSI can be applied to collaborative filtering by treating users and items as discrete random variables that follow multinomial distributions. In hybrid recommendation, it is necessary to deal with musical contents that are usually represented as continuous vectorial values. To do this, we propose a continuous pLSI that incorporates Gaussian mixture models. This extension, however, causes a severe local optima problem because it increases the number of parameters drastically. This is considered to be a major factor generating “hubs,” which are items that are inappropriately recommended to almost all users. To solve this problem, we tested three smoothing techniques: multinomial smoothing, Gaussian parameter tying, and artist-based item clustering. The experimental results revealed that although the first method improved nothing, the others significantly improved the recommendation accuracy and reduced the hubness. This indicates that it is important to appropriately limit the model complexity to use the pLSI in practical.",
        "zenodo_id": 1415204,
        "dblp_key": "conf/ismir/YoshiiG09",
        "keywords": [
            "extended probabilistic latent semantic indexing",
            "hybrid music recommendation",
            "rating data",
            "content-based data",
            "Gaussian mixture models",
            "local optima problem",
            "hubs",
            "multinomial smoothing",
            "Gaussian parameter tying",
            "artist-based item clustering"
        ],
        "content": "10th International Society for Music Information Retrieval Conference (ISMIR 2009)\nCONTINUOUS PLSI AND SMOOTHING TECHNIQUES\nFOR HYBRID MUSIC RECOMMENDATION\nKazuyoshi Yoshii Masataka Goto\nNational Institute of Advanced Industrial Science and Technology (AIST)\n{k.yoshii,m.goto }@aist.go.jp\nABSTRACT\nThis paper presents an extended probabilistic latent seman-\ntic indexing (pLSI) for hybrid music recommendation thatdeals with rating data provided by users and with content-\nbased data extracted from audio signals. The original pLSI\ncan be applied to collaborative ﬁltering by treating usersand items as discrete random variables that follow multi-nomial distributions. In hybrid recommendation, it is nec-\nessary to deal with musical cont ents that are usually repre-\nsented as continuous vectorial values. To do this, we pro-pose a continuous pLSI that incorporates Gaussian mix-\nture models. This extension, however, causes a severe lo-\ncal optima problem because it increases the number of pa-\nrameters drastically. This is considered to be a major fac-tor generating “hubs,” which are items that are inappropri-\nately recommended to almost all users. To solve this prob-\nlem, we tested three smoothing techniques: multinomialsmoothing, Gaussian parameter tying, and artist-based item\nclustering. The experimental results revealed that although\nthe ﬁrst method improved nothing, the others signiﬁcantlyimproved the recommendation accuracy and reduced thehubness. This indicates that it is important to appropriately\nlimit the model complexity to use the pLSI in practical.\n1. INTRODUCTION\nThe musical tastes of users of online music distribution ser-\nvices that provide millions of items are strongly inﬂuencedby the characteristics of the music automatically recom-mended by those services. Users often have difﬁculty re-\ntrieving unknown items they might like. In such case, users\nconsider recommendations and get aware of what kinds ofitems are their favorites. When only popular items are al-\nways recommended, users are not exposed to items they\nmight enjoy more and get used to enjoying only the “safe”recommendations. This in turn strengthens the tendency torecommend only popular items. In other words, there is a\nsevere limitation in serendipity of music discovery. In fact,\nthis negative-feedback phenomenon has been observed inmany services based on collaborative ﬁltering.\nWe aim to enhance the serendipity by transforming the\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted w ithout fee provided that copies are\nnot made or distributed for proﬁt or c ommercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page.\nc/circlecopyrt2009 International Society for M usic Information Retrieval.… Topic 1 Topic 2 Topic 3\na\n b\n c\n z\n3\n 1\n 2\n 9\n1\n clikes\n2\n b likes\n aRating data Content-based data\n…\n…\n……\nRecommender Train continuous-pLSI model\n cwho likesThe objective is to recommend \nitems to \nis likely to select \nis recommended to \n3\n z likes\n b\n9likes\n z\n c\na Features\nb\n Features\nc\nFeatures\nz\nFeatures\n…\nTopic 3\nz\nEstimation of musical tastesExample of recommendation\nTopic 5\nFigure 1 . Hybrid recommender based on continuous pLSI.\npassive experience in which users only receive “default”\nrecommendations into an interactive experience in which\nusers can freely customize (personalize) those recommen-\ndations. To achieve this, it is necessary to let users clearlyunderstand and express their own musical tastes that are es-timated as bases of making default recommendations. The\nconventional reasoning like “You like A, so you would like\nB because other users who like like A also like B” is a rela-\ntiveexpression of musical tastes. We aim to obtain a direct\nexpression of each user’s musical tastes that is easy to use\nas a basis for interactive recommendation.\nA promising way to do this is to use probabilistic latent\nsemantic indexing (pLSI) based on a multi-topic model,\nwhich has been originally used for document modeling [1].\nThe model includes latent variables corresponding to theconcepts of topics. How likely a document and a word co-\noccur is predicted by stochastically associating each docu-\nment and word with a topic. Documents and words that arestrongly associated with the same topic are likely to occurjointly. The model can be applied to collaborative ﬁltering\nbased on rating histories by treating documents and words\nas users and items [2]. Given a user, we can predict howlikely each item is purchased by estimating how likely the\nuser chooses each topic. The musical tastes of users can be\nexpressed as the strength of user-topic associations.\nAs shown in Figure 1, we propose continuous pLSI for\nhybrid recommendation that enhances serendipity by com-\nbining rating data with content-based data extracted from\nmusical audio signals. Speciﬁcally, Gaussian mixture mod-els (GMMs) are built into the collaborative ﬁltering model\nof pLSI in order to address continuous vectorial data. Un-\nlike the major collaborative methods relying on heuristics[3,4], the pLSI model can be extended in a consistent man-ner because it is ﬂexible and has a theoretical basis.\n339Oral Session 4: Music Recommendation and Playlist Generation\nThe continuous pLSI, however, suffers from a serious\nlocal optima problem because a number of parameters lin-\nearly increases according to d ata size. This causes the hub\nphenomenon [5], in which speciﬁc items are almost alwaysrecommended to users regardless of their rating histories.Thus, the serendipity of recommendations is insufﬁcient.\nAlthough a similar probabilistic model was proposed for\ngenre classiﬁcation [6], this problem was not addressed.\nTo solve this problem, we propose three smoothing tech-\nniques: multinomial smoothing, Gaussian parameter tying,\nand artist-based item clustering. The ﬁrst technique is ex-\npected to avoid overﬁtting and the other two reduce themodel complexity. We compared the effectiveness of these\ntechniques experimentally.\nThe rest of this paper is organized as follows. Section 2\nintroduces related work. Section 3 explains a model of thecontinuous pLSI. Section 4 describes the three smoothing\ntechniques. Section 5 reports our experiments. Section 6\nsummarizes the key ﬁndings of this paper.\n2. RELATED WORK\nMusic recommendation is an important topic today in the\nﬁeld of music information processing. Conventional stud-ies on recommendation have been intended to deal with\ntextual data (documents and words). In addition, many re-\nsearchers have proposed various ideas to make the mostof content-based data that is automatically extracted frommusical audio signals. For example, Logan [7] proposed a\ncontent-based recommender based on the cosine distance\nbetween a user’s favorite items and non-rated items. Magnoand Sable [8] reported subjective experiments showing that\na content-based recommender competes against Last.fm (a\ncollaborative recommender) and Pandora (a recommenderbased on manual annotations) in terms of user satisfaction.These reports indicate the synergistic effect of integrating\nrating data with content-based data. Hybrid recommenders\nhave been actively investigated recently. Celma et al. [9]\nused both content-based similarity and user proﬁles given\nin RSS feeds to choose suitable items. Tiemann et al. [10]\nintegrated two weak learners (social and content-based rec-ommenders) by using an ensemble learning method.\nAnother important issue is how to present recommenda-\ntions to users. Donaldson and Knopke [11] visualized the\nrelationships of recommended items in a two dimensionalspace. Lamere and Maillet [12] proposed a transparent andsteerable interface for a recommender based on crowds of\nsocial tags. A common concept of these studies seems to\nbe that users had better actively explore or control recom-mendations. This would result in enhanced serendipity.\nThe existence of hubs has recently been recognized as\na serious problem. Interestingly, this problem was not re-\nported in the ﬁeld of text-based recommendation. In musicrecommendation and retrieval, GMMs are generally used\nto represent the distributions of acoustic features. Aucou-\nturier et al. [5] pointed out that this kind of modeling tends\nto create hubs that are wrongly evaluated as similar to allother items. Berenzweig [13] concluded that the hub phe-\nnomenon is related to the curse of dimensionality. Chordiaet al. [14] discussed content-based recommendation based\non the Earth-Movers distance between GMMs of individ-\nual items. They empirically found that a homogenization\nmethod can improve performance [15]. Hoffman et al. [16]\ntried to solve this problem by using the hierarchical Dirich-let process (HDP) for modeling content-based data. Unlike\nthe GMM, the HDP represents each item as a mixture of\nan unﬁxed number of Gaussians. The number is automat-ically adjusted to match the data complexity. In addition,\nthe same set of Gaussians is used to model all items, with\nonly the mixture weights varying from item to item. Thisis similar to Gaussian parameter tying.\n3. CONTINUOUS PLSI\nThis section explains a continuous pLSI model and a train-\ning method suitable for efﬁcient parallel processing.\n3.1 Problem Statement\nWe deﬁne several symbols from a probabilistic viewpoint.\nLetU={u\n1,u2,···,u|U|}be the set of all users, where\n|U|is the number of them, and let V={v1,v2,···,v|V|}\nbe the set of all items, where |V|is the number of them. Let\nuandvbediscrete random variables respectively taking\nthe values of one member of Uand one member of V.L e t\nX={x1,x2,···,x|V|}denote content-based data that\nis a set of D-dimensional feature vectors extracted from\nindividual items. Let xbe a continuous random variable\nin theD-dimensional space. Probabilistic distributions are\nrepresented as p(variable )orp(variable1 |variable2 ),e . g . ,a\ndiscrete distribution p(u)or a conditional continuous dis-\ntribution p(x|u). For example, probabilities or probability\ndensities are given by p(u=ui)orp(x=xj|u=ui),\nwhich are simply described as p(ui)orp(xj|ui).\nAs to available rating data, we mainly assume implicit\nratings such as purchase histories or listening counts, whichare recorded automatically ev en when users do not explic-\nitly express their preferences for individual items. In gen-\neral, the number of implicit ratings tends to be much larger\nthan that of explicit ratings. We thus think that the formerare more suitable to probabilistic approaches because for\nthem the sparseness problem is less serious.\nThe total available data (combinations of rating data and\ncontent-based data) is given by O={(u\n(1),v(1),x(1)),\n···,(u(N),v(N),x(N))},w h e r e (u(n),v(n),x(n))( 1≤n\n≤N)is a user-item-feature co-occurrence that user u(n)\nhas purchased (viewed or listened to) item v(n)with fea-\nturex(n)andNis the number of co-occurrences. Let\nc(u,v)be the number of times that co-occurrence (u,v,x)\nwas observed. Obviously, N=/summationtext\nu,vc(u,v). An easy\nway to utilize explicit ratings (e.g., numerical rating scores\nsuch as the numbers of “stars” in an one-to-ﬁve scale rat-ing system adopted by Amazon.com) is to set the value of\nc(u,v)to one if a user ulikes item v, i.e., the rating score\nis greater than a neutral score ( three stars). Alternatively,\nwe could use rating scores for weighting c(u,v).\nThe ﬁnal objective is to estimate the probabilistic distri-\nbution p(v|u), which indicates how likely it is that user u\nlikes item v. Recommendations are then made by ranking\nitems not rated by user uin a descending order of p(v|u).\n34010th International Society for Music Information Retrieval Conference (ISMIR 2009)\n3.2 Model Formulation\nThe graphical representation of a continuous pLSI model\nis shown in Figure 2. This is an extended version of three-\nway aspect models [17, 18] in which all variables are dis-crete. We assume that users, items, and features are condi-\ntionally independent through latent topics. In other words,\nonce a latent topic is speciﬁed, there is no mutual informa-tion between three kinds of variables. Although this seemsa strong assumption, it is a reasonable way to avoid the lo-\ncal optima problem. Introducing a dependency edge from\nitems to features in order to model the real world accuratelywould increase the number of parameters drastically.\nThe pLSI model can explain the process generating co-\noccurrence (u\n(n),v(n),x(n)).L e tZ={z1,···,z|Z|}be\nas e to f topics ,w h e r e |Z|is the number of them. Let zbe a\nlatent variable that takes the value of one of Z. Each topic\ncan be regarded as a softcluster that is simultaneously as-\nsociated with users and items. That is, each user and eachitem stochastically belong to one of the topics. The model\nthus treats triplet (u\n(n),v(n),x(n))as incomplete data that\nis latently associated with z(n)∈Z. The complete data is\ngiven by quartet (u(n),v(n),x(n),z(n)). An interpretation\nof the generative process is that user u(n)stochastically se-\nlects topic z(n)according to his or her taste p(z(n)|u(n)),\nandz(n)stochastically generates item vand its features x\nin turn. For convenience, we let Sbe{z(1),···,z(n)}.\nA unique feature of the continuous pLSI is that p(x|z)\nis modeled with a Gaussian mixture model (GMM) in or-der to deal with continuous observation x.L e tMbe the\nnumber of mixtures (Gaussian components). Each topic\nz\nk∈Zhas a GMM deﬁned by the mixing proportions\nof Gaussians {wk,1,···,wk,M}and their means and co-\nvariances {μk,1,···,μk,M}and{Σk,1,···,Σk,M}.A s\nin the original pLSI, p(u),p(z|u),a n dp(v|z)are multino-\nmial distributions. We practically use an equivalent deﬁ-nition of the model obtained by focusing on p(z),p(u|z),\nandp(v|z). The parameters of these multinomial distribu-\ntions are simply given by (conditional) probability tables\nof target variables. Let θbe the set of all parameters of |Z|\nGMMs and |Z|(1 +|U|+|V|)multinomial distributions.\n3.3 Model Training\nThe training method we explain here uses the Expectation-\nMaximization (EM) algorithm [19] and is a natural exten-sion of previous methods [17, 18] (c.f., discrete HMM v.s.\ncontinuous HMM). Instead of maximizing the incomplete\nlog-likelihood, logp(O), the EM algorithm maximizes the\nexpected complete log-likelihood E\nS[logp(S,O)]itera-\ntively, where Ez[f(z)]means an expected value of func-\ntionf(z)with respect to p(z);Ez[f(z)] =/summationtext\nzp(z)f(z).\nThe complete likelihood of (u,v,x,z)is given by\np(u,v,x,z)=p(z)p(u|z)p(v|z)p(x|z). (1)\nThis can be easily calculated for given observations when\nparameters θare obtained.\nIn the E-step we deﬁne a Q function as\nQ(θ|θcurrent )=ES[logp(S,O)] (2)\n=/summationdisplay\nu,vc(u,v)/summationdisplay\nzp(z|u,v,x)l o gp(u,v,x,z), (3)u\nx vz)|(zup\n)|(zvp )|(zxpUser\nItem FeatureTopic\n(continuous ) (discrete )(discrete )\nMultinomial\nMultinomial Gaussian  mixture  modelUser’s taste\nFigure 2 . Graphical representation of continuous pLSI.\nwhere p(z|u,v,x)is a posterior distribution of latent vari-\nablezand can be calculated by using the current parame-\ntersθcurrent as follows:\np(z|u,v,x)=p(u,v,x,z)\n/summationtext\nzp(u,v,x,z). (4)\nIn the M-step we update the current parameters by max-\nimizing Eqn. (3). Note that logp(u,v,x,z)can be decom-\nposed into logp(z)+l og p(u|z)+l og p(v|z)+l og p(x|z).\nThis means that the parameters of each distribution can beupdated independently. To update p(z), for example, we\nonly focus on a term related to p(z)as follows:\nQ\np(z)=/summationdisplay\nu,vc(u,v)/summationdisplay\nzp(z|u,v,x)l o gp(z).(5)\nUsing a Lagrange multiplier λfor a constraint of probabil-\nity standardization, we deﬁne a new function Fp(z)as\nFp(z)=Qp(z)+λ/parenleftBigg\n1−/summationdisplay\nzp(z)/parenrightBigg\n. (6)\nWe then calculate the differential of Eqn. (6) with respect\ntop(z)and set it to zero as follows:\n∂Fp(z)\n∂p(z)=1\np(z)/summationdisplay\nu,vc(u,v)p(z|u,v,x)−λ≡0.(7)\nThe updated distribution p(z)can be obtained by\np(z)=/summationtext\nu,vc(u,v)p(z|u,v,x)\n/summationtext\nu,v,zc(u,v)p(z|u,v,x). (8)\nThe other two multinomial distributions p(u|z)andp(v|z)\ncan be similarly updated as follows:\np(u|z)=/summationtext\nvc(u,v)p(z|u,v,x)\n/summationtext\nu,vc(u,v)p(z|u,v,x), (9)\np(v|z)=/summationtext\nuc(u,v)p(z|u,v,x)\n/summationtext\nu,vc(u,v)p(z|u,v,x). (10)\nTo update continuous distribution p(x|z), we focus on\nQp(x|z)=/summationdisplay\nu,vc(u,v)/summationdisplay\nzp(z|u,v,x)l o gp(x|z)(11)\n=/summationdisplay\nu,vc(u, v)K/summationdisplay\nk=1p(zk|·)l o gM/summationdisplay\nm=1p(yk,m)p(x|zk,yk,m),(12)\nwhere to improve legibility we wrote p(z|u,v,x)asp(z|·).\n341Oral Session 4: Music Recommendation and Playlist Generation\nyk∈{yk,1,···,yk,M}is a latent variable that indicates\nwhich Gaussian in the GMM of topic zkgenerates x.p(yk)\nrepresents a probability distribution over MGaussians, i.e.,\np(yk,m)=wk,m,a n dp(x|zk,yk,m)is the likelihood that\nfeature xis generated from a Gaussian indicated by yk,m.\nBecause the logarithmic opera tion for the summation makes\nEqn. (12) hard to maximize directly, we focus on the ex-\npected value of Qp(x|z)with respect to yk:\nEyk[Qp(x|z)]=/summationdisplay\nu,vc(u, v)K/summationdisplay\nk=1p(zk|·)\nM/summationdisplay\nm=1p(yk,m|x,zk)/parenleftBig\nlogwk,m+l o g N(x|µk,m,Σk,m)/parenrightBig\n,(13)\nwhere p(yk,m|x,zk)is a posterior probability given by\np(yk,m|x,zk)=p(yk,m)p(x|zk,yk,m)\n/summationtextM\nm=1p(yk,m)p(x|zk,yk,m).(14)\nTo obtain optimized wk,m, we deﬁne the following func-\ntion by introducing a Lagrange multiplier β:\nFwk=Eyk[Qp(x|z)]+β/parenleftBigg\n1−M/summationdisplay\nm=1wk,m/parenrightBigg\n. (15)\nCalculating the partial partial differential of Eqn. (15) with\nrespect to wk,mand setting it to zero, we obtain\nwk,m=/summationtext\nu,vc(u, v)p(zk|·)p(yk,m|x,zk)\n/summationtextM\nm=1/summationtext\nu,vc(u, v)p(zk|·)p(yk,m|x,zk). (16)\nSetting the partial differential of Eqn. (13) to zero, the mean\nand variance μk,mandΣk,mare obtained by\nµk,m=/summationtext\nu,vc(u, v)p(zk|·)p(yk,m|x,zk)x\n/summationtext\nu,vc(u, v)p(zk|·)p(yk,m|x,zk),(17)\nΣk,m=/summationtext\nu,vc(u, v)p(zk|·)p(yk,m|x,zk)(x−µk,m)2\n/summationtext\nu,vc(u, v)p(zk|·)p(yk,m|x,zk).(18)\nGiven a user ui, recommendations are made by evalu-\natingp(v|ui)=/summationtext\nzp(v|z)p(z|ui),w h e r e p(z|ui)is pro-\nportional to p(z)p(ui|z)and indicates the musical tastes of\nuseru: how likely it is user uiselects (likes) topic z.\n3.4 MapReducing EM Algorithm\nComputational efﬁciency, a very important issue in mu-\nsic recommendation when the database and model becomelarge, is especially critical when used data cannot be loadedon the memory of a single machine. Elegant implementa-\ntions, however, have scarcely been addressed.\nA remarkable advantage of pLSI-based recommenders\nis that we can easily implemen t them in parallel process-\ning environments that consist of multiple machines such\nas clusters. Google News, for example, uses a distributedcomputation framework called MapReduce [20].\nWe can implement the continuous pLSI by using MPI or\nHadoop [21]. Suppose we have G\nUGVmachines (CPUs).\nLet{U1,···,UGU}and{V1,···,VGV}be exclusive sets\nof users and items, where U1∩···∩ UGU=UandV1∩\n···∩VGV=V. To update p(z), for example, we calculate\ncz(Ui,Vj)=/summationdisplay\nu∈Ui,v∈Vjc(u,v)p(z|u,v,x). (19)This can be separately calculated in each machine. To cal-\nculate p(z|u,v,x), we need only p(z),p(u|z)(u∈Ui),\np(v|z)(v∈Vj),a n dp(x|z). The number of parameters of\nthese distributions is much smaller than the total numberof parameters. Finally, we can get an integrated result by\np(z)∝/summationdisplay\n1≤i≤GU,1≤j≤GVcz(Ui,Vj). (20)\n4. SMOOTHING TECHNIQUES\nTo avoid overﬁtting, one needs to use appropriate smooth-\ning techniques. In our study, we use three techniques to im-\nprove accuracy and reduce hubness: multinomial smooth-ing, Gaussian parameter tying, and artist-based item clus-tering. The ﬁrst relaxes the excessive inclination of multi-\nnomial parameters, and the others limit model complexity.\n4.1 Multinomial Smoothing\nWe add a conjugate prior called a Dirichlet distribution to\na Q function as a regularization term. To estimate p(z),f o r\nexample, we consider the following function:\nQ\n/prime\np(z)=Qp(z)+Dir(α), (21)\nwhere αis a set of Kparameters of a Dirichlet distribution.\nThis results in the additive smoothing method. We set allparameters to 1.0001 . Maximizing Q\n/prime\np(z),w eg e t\np(z)=/summationtext\nu,vc(u, v)p(z|u, v,x)+α−1\n/summationtext\nz/parenleftBig/summationtext\nu,vc(u, v)p(z|u, v,x)+α−1/parenrightBig. (22)\nThe updating formulas of the other multinomial distribu-\ntions are similarly given by\np(u|z)=/summationtext\nvc(u, v)p(z|u, v,x)+α−1\n/summationtext\nu/parenleftbig/summationtext\nvc(u, v)p(z|u, v,x)+α−1/parenrightbig, (23)\np(v|z)=/summationtext\nuc(u, v)p(z|u, v,x)+α−1\n/summationtext\nv/parenleftbig/summationtext\nuc(u, v)p(z|u, v,x)+α−1/parenrightbig. (24)\n4.2 Gaussian Parameter Tying\nWe force all GMMs to share the same set of Gaussians and\ndiffer from each other in the mixing proporti ons of those\nGaussians. In the context of HMMs, this is called a tiedmixture model. The new updating formulas are given by\nµk,m=/summationtext\nu,v,mc(u, v)p(zk|·)p(yk,m|x,zk)x\n/summationtext\nu,v,mc(u, v)p(zk|·)p(yk,m|x,zk),(25)\nΣk,m=/summationtext\nu,v,mc(u, v)p(zk|·)p(yk,m|x,zk)(x−µk,m)2\n/summationtext\nu,v,mc(u, v)p(zk|·)p(yk,m|x,zk).(26)\n4.3 Artist-based Item Clustering\nWe replace item-based distribution p(v|z)with artist-based\ndistribution p(a|z), where variable arepresents one of the\nartists in the database. Let Abe a set of items sung by artist\na, That is, let all items be grouped according to their artist\nnames. We train an artist-based model for users, artists,\nand features by iteratively updating p(a|z)as follows:\np(a|z)=/summationtext\nu,v∈Ac(u,v)p(z|u,v,x)\n/summationtext\nu,vc(u,v)p(z|u,v,x). (27)\n34210th International Society for Music Information Retrieval Conference (ISMIR 2009)\nScore\n 54321\nCounts\n 5336 1458 457 211 333\nRatio\n 68.5% 18.7% 5.86% 2.71% 4.27%\nTable 1 . Distribution of rating scores.\nTo recommend items rather than artists, we then con-\nstruct an item-based model by replacing p(a|z)withp(v|z).\nTo do this, we use an incremental training method [18] that\nre-estimates a distribution of unknown items p(v|z)with-\nout affecting other trained distributions p(z),p(u|z),(x|z):\np(v|z)=/summationtext\nuc(u,v)p(z)p(u|z)p(x|z)\n/summationtext\nzp(z)p(u|z)p(x|z)\n/summationtext\nu,vc(u,v)p(z)p(u|z)p(x|z)\n/summationtext\nzp(z)p(u|z)p(x|z). (28)\n5. EVALUATION\nWe experimentally evaluated the continuous pLSI in terms\nof accuracy and hubness by using various combinations of\nthe smoothing techniques.\n5.1 Data\nThe music items we used were Japanese songs recorded in\nsingle CDs that were ranked in weekly top-20 sales rank-ings from Apr. 2000 to Dec. 2005. To use these items,we need real implicit ratings c(u,v)such as purchase his-\ntories and listening counts, but most online services do\nnot release such data to the public. We therefore insteadcollected explicit ratings (numbers of “stars” ranging from\none to ﬁve) from Amazon.co.jp by using ofﬁcial APIs [22]\nthat let us download almost all the information availablefrom Amazon.co.jp [22]. For reliable evaluation, we ex-cluded users who had rated fewer than two items and ex-\ncluded items that had been rated less than two times. As\na result, |U|was 1872 and |V|was 1400. The number of\nartists was 471. If a rating score given to item v\njby user ui\nwas greater than three (th e neutral score), we set c(ui,vj)\nto the score. Otherwise, we set c(ui,vj)to zero. In other\nwords, we considered only positive ratings. A similar ap-proach has been used previously [23]. Note that, as shown\nin Table 1, the distribution of rating scores was strongly\nskewed. The density of 6794 positive ratings (scores 4 and5) was 0.259% in the user-item co-occurrence table.\nWith regard to the content-based data, we focused on\nvocal features because all the items included singing voices\nthat strongly affected the musical tastes of users. To extract\nthese features from polyphonic audio signals, we used amethod proposed by Fujihara et al. [24]. We calculated a\n13-dimensional feature vect or at each frame where singing\nvoices were highly likely to be i ncluded, concatenated the\nmean and variance of the feature vectors in each item into a26-dimensional vector, and then used principal component\nanalysis to compress the dimensionality to 20 ( D=2 0 ).\n5.2 Protocols\nTo test all combinations of the three smoothing techniques,\nwe prepared eight models of the continuous pLSI. For con-\nvenience, throughout Section 5 , the multinomial smooth-\nDisabled SM1 SM2 SM1&2\nDisabled\n 4.65 4.29 6.18 6.57\nSM3\n 7.10 6.72 19.4 19.3\nTable 2 . Expected utility of recommendations: Higher\nscores indicate better performance.\nDisabled SM1 SM2 SM1&2\nDisabled\n 5.94 5.81 6.39 6.36\nSM3\n 5.98 5.81 6.36 6.34\nTable 3 . Entropy of recommendations: Higher scores in-\ndicate better performance (fewer hubs).\ning, Gaussian parameter tying, and item clustering are re-\nspectively called SM1, SM2, and SM3. The number of\nlatent variables was 256 ( |Z|= 256 ). Although the num-\nber of mixtures was 32, when SM1 was disabled it was setto 1 in order to avoid overﬁtting.\nWe conducted 10-fold cross v alidation by splitting the\npositive explicit ratings into ten groups. Nine groups were\nused for making recommendations with the eight models.The other group was considered to be not observed and\nwas used for evaluating the recommendations.\n5.3 Measures\nRecommendation results given as ranked lists of items were\nevaluated in terms of accuracy and hubness.\nTo calculate accuracy, we used the expected utility of a\nranked list [25], which for each user is deﬁned as\nR\nu=|V|−#(rated items)/summationdisplay\nr=1max( score u,r−3,0)\n2(r−1)/(γ−1), (29)\nwhere score u,ris the rating score that user uactually gave\nther-th ranked item although the item was considered a\nnon-rated item (the score was hidden) in model training.When score\nu,rwas not available, its value was set to 3.\nγis a viewing half-life based on the assumption that the\nprobability that a user views an r-th ranked item is twice\nthe probability that the user views an (r+γ)-th ranked\nitem. We set γto 5 as in the literature [25]. Ruwas not\nsensitive to the value of γ. The total score is given by\nR= 100/summationtext\nuRu\n/summationtext\nuRmaxu(0≤R≤100), (30)\nwhere Rmax\nuis the maximum achievable utility if all items\nwith available scores given by user uhad been at the top of\nthe ranked list in order of those scores. Basically, higher\nvalues indicate better performance, but note that the prob-ability of recommending known items is high.\nWe propose the following hubness measure based the\nentropy of recommendations:\nH=−|V|/summationdisplay\nj=1t(j)\n|U|log/parenleftbiggt(j)\n|U|/parenrightbigg\n, (31)\nwhere t(j)is the number of times that item vjwas rec-\nommended with the highest (top 1) ranking. A larger H\n(higher entropy) indicates a smaller bias in how many times\neach item is recommended.\n343Oral Session 4: Music Recommendation and Playlist Generation\n5.4 Results\nAs shown in Table 2, the accuracies of recommendations\nwere greatly improved by using SM3. This can be ex-\nplained from two aspects: th e relationship between items\nand features and that between items and users. First, the\nitems of each artist tend to be similar to each other in their\nmusical features. Second, most users of Amazon.co.jp tendto like any of the items of the few artists they like. Thiswould be a common tendency of the users of many online\nmusic distribution services. Therefore, SM3 reduced the\ncomplexity of the model while preserving almost all theinformation of the rating data.\nSM2 improved the accuracy of recommendations made\nregardless of the combinations in which it was used. Inter-estingly, recommendations obtained by jointly using SM2and SM3 were much more accurate than those made when\nthese techniques were used independently. SM1, on the\nother hand, slightly reduced th e accuracy because it is based\non additive smoothing. It is known that its approximation\nerrors are larger than those of the other smoothing methods\nsuch as the Good-turing method.\nTable 3 shows hubness of recommendations. SM2 sig-\nniﬁcantly reduced the hubness while the SM1 and SM3\nhad no gains. This is consistent with the results reported\nby Hoffman et al. [16], who found that HDP and vector\nquantization (VQ) did not produce many hubs. VQ can be\nconsidered as a hard clustering version of the tied GMM,\nwhich is a softclustering model.\nWe conclude that combining SM2 and SM3 is the best\napproach to improving performance. In our experiments,\nit yielded accuracy comparab le with that of conventional\nmethods of collaborative ﬁltering.\n6. CONCLUSION\nThis paper has presented a continuous-pLSI-based model\nfor hybrid music recommendation. The model uses GMMsto represent distributions of acoustic features extracted from\nmusical audio signals. As in the original pLSI, users and\nitems are assumed to follow multinomial distributions. Wedeveloped an algorithm for parameter estimation and im-\nplemented it in a parallel processing environment. Experi-\nmentally testing the abilities of three smoothing techniques—multinomial smoothing, Gaussian parameter tying, andartist-based item clustering—, we found that using the sec-\nond and third techniques to adjust model complexity sig-\nniﬁcantly improved the accuracy of recommendations andthat the second technique could also reduce hubness.\nIn the future, we plan to introduce conjugate priors of all\ndistributions (GMMs and multinomial distributions) intothe continuous pLSI to enable full Bayesian estimation.Extending latent Dirichlet allocation (LDA) [23] and HDP-\nLDA [26] are worth considering.\nAcknowledgement : This study was partially supported by\nGrant-in-Aid for Young Scientists (Start-up) 20800084.\n7. REFERENCES\n[1] T. Hofmann and J. Puzicha. Probabilistic latent semantic in-\ndexing. In SIGIR , pages 50–57, 1999.\n[2] T. Hofmann and J. Puzicha. Latent class models for collabo-\nrative ﬁltering. In IJCAI , pages 688–693, 1999.[3] P. Resnick, N. Iacovou, M. Sushak, and P. Bergstrom. Grou-\npLens: An open architecture for collaborative ﬁltering of net-news. In CSCW , pages 175–186, 1994.\n[4] D. Lemire and A. Maclachlan. Slope one predictors for on-\nline rating-based collaborative ﬁltering. In SDM , pages 21–\n23, 2005.\n[5] J.-J. Aucouturier, F. Pache, and M. Sandler. The way it\nsounds: Timbre models for analysis and retrieval of poly-phonic music signals. IEEE Transactions of Multimedia ,\n7(6):1028–1035, 2005.\n[6] P. Ahrendt, J. Larsen, and C. Goutte, Co-occurrence models\nin music genre classiﬁcation. In MLSP , pages 24–252, 2005.\n[7] B. Logan. Music recommendation from song sets. In ISMIR ,\npages 425–428, 2004.\n[8] T. Magno and C. Sable. A comparison of signal-based mu-\nsic recommendation to genre labels, collaborative ﬁltering,musicological analysis, human recommendation, and randombaseline. In ISMIR , pages 161–166, 2008.\n[ 9 ] O .C e l m a ,M .R a m ´ ırez, and P. Herrera. Foaﬁng the music: A\nmusic recommendation system based on RSS feeds and userpreferences. In ISMIR , pages 464–467, 2005.\n[10] M. Tiemann, S. Pauws, and F. Vignoli. Ensemble learning\nfor hybrid music recommendation. In ISMIR , pages 179–180,\n2007.\n[11] J. Donaldson and I. Knopke. Music recommendation map-\nping and interface based on structural network entropy. InISMIR , pages 181–182, 2007.\n[12] P. Lamere and F. Maillet. Creating transparent, steerable rec-\nommendations. Late breaking in ISMIR, 2008.\n[13] A. Berenzweig. Anchors and Hubs in Audio-based Music\nSimilarity . PhD thesis, Columbia University, 2007.\n[14] P. Chordia, M. Godfrey, and A. Rae. Extending content-based\nmusic recommendation: The case of Indian classical music.InISMIR , pages 571–576, 2008.\n[15] M. Godfrey and P. Chordia. Hubs and homogeneity: improv-\ning content-based music modeling. In ISMIR , pages 307–\n312, 2008.\n[16] M. Hoffman, D. Blei, and P. Cook. Content-based musical\nsimilarity computation using the hierarchical Dirichlet pro-cess. In ISMIR , pages 349–354, 2008.\n[17] A. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Prob-\nabilistic models for uniﬁed collaborative and content-based\nrecommendation in sparse-data environments. In UAI, pages\n437–444, 2001.\n[18] K. Yoshii, M. Goto, K. Komatani, T. Ogata, and H.G. Okuno.\nAn efﬁcient hybrid music recommender system using anincrementally-trainable probabilistic generative model. IEEE\nTransactions on ASLP , 16(2):435–447, 2008.\n[19] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood\nfrom incomplete data via the EM algorithm. Royal Statistical\nSociety , B-39(1):1–38, 1977.\n[20] A. Das, M. Datar, and A. Garg. Google news personalization:\nScalable online collaborative ﬁltering. In WWW , pages 271–\n280, 2007.\n[21] Hadoop. http ://hadoop.apache.org/core.\n[22] Amazon web services. http://aws.amazon.com.\n[23] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation.\nMachine Learning Research , 3:993–1022, 2003.\n[24] H. Fujihara, T. Kitahara, M. Goto, K. Komatani, T. Ogata,\nand H.G. Okuno. Singer identiﬁcation based on accompani-ment sound reduction and reliable frame selection. In ISMIR ,\npages 329–336, 2005.\n[25] J. Breese, D. Heckerman, and C. Kadie. Empirical analysis\nof predictive algorithms for collaborative ﬁltering. In\nUAI,\npages 43–52, 1998.\n[26] Y . Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical Dirich-\nlet processes. JASA , 101(476):1566-1581, 2006.\n344"
    },
    {
        "title": "Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009, Kobe International Conference Center, Kobe, Japan, October 26-30, 2009",
        "author": [
            "Keiji Hirata 0001",
            "George Tzanetakis",
            "Kazuyoshi Yoshii"
        ],
        "year": "2009",
        "doi": "10.5281/zenodo.1285647",
        "url": "https://doi.org/10.5281/zenodo.1285647",
        "ee": "https://zenodo.org/records/1285647/files/annotated_jingju_arias_1.0.zip",
        "abstract": "The Annotated Jingju Arias Dataset is a collection of 34 jingju arias manually segmented in various levels using the software Praat v5.3.53. The selected arias contain samples of the two main shengqiang in jingju, name xipi and erhuang, and the five main role types in terms of singing, namely, dan, jing, laodan, laosheng and xiaosheng.\n\nThe dataset includes a Praat TextGrid file for each aria with the following tiers (all the annotations are in Chinese):\n\n\n\taria: name of the work (one segment for the whole aria)\n\tMBID: MusicBrainz ID of the audioi recording(one segment for the whole aria)\n\tartist: name of the singing performer (one segment for the whole aria)\n\tschool: related performing school (one segment for the whole aria)\n\trole-type: role type of the singing character(one segment for the whole aria)\n\tshengqiang:boundaries and label of theshengqiangperformed in the aria (including accompaniment)\n\tbanshi: boundaries and label of the banshi performed in the aria (including accompaniment)\n\tlyrics-lines: boundaries and annotation of each line of lyrics\n\tlyrics-syllables: boundaries and annotation of each syllable\n\tluogu: boundaries and label of each of the performed percussion patterns in the aria\n\n\nThe ariasInfo.txt file contains a summary of the contents per aira of the whole dataset.\n\nA subset of this dataset comprising 20 arias has been used for the study of the relationship between linguistic tones and melody in the following papers:\n\n\nShuoZhang, Rafael Caro Repetto, and Xavier Serra (2014) Study of the Similarity between Linguistic Tones and Melodic Pitch Contours in Beijing Opera Singing. In Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, October 2731, pp. 343348.\n\n\n\n______ (2015) Predicting Pairwise Pitch Contour Relations Based on Linguistic Tone Information in Beijing Opera Singing. In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Mlaga, Spain, October 2630, pp. 107113.\n\n\nHere is the list of the arias from the dataset used in these papers.\n\nThe whole dataset has been used for the automatic analysis of the structure of jingju arias and their automatic segmentation in the following master&#39;s thesis:\n\n\nYile Yang(2016) Structure Analysis of Beijing Opera Arias. Masters thesis, Universitat Pompeu Fabra, Barcelona.\n\n\nUsing this dataset\n\nIf you use this dataset in a publication, please cite the above publications.\n\nWe are interested in knowing if you find our datasets useful! If you use our dataset please email us at mtg-info@upf.edu and tell us about your research.\n\nContact\n\nThe audio recordings used for these annotations are available for research purposes. Please contact Rafael Caro Repetto\n\nrafael.caro@upf.edu\n\n\n\nhttp://compmusic.upf.edu/node/349",
        "zenodo_id": 1285647,
        "dblp_key": "conf/ismir/2009"
    }
]